\documentclass[twoside,11pt]{article}

%\usepackage{blindtext}
\usepackage{amsmath, setspace, enumerate}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{jmlr2e}
\usepackage{lastpage}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage{algorithm, comment}
\usepackage{algpseudocode}

\usepackage{multirow}
\usepackage{subcaption}
\usepackage{color, colortbl}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{pro}{Proposition}
\newtheorem{rem}{Remark}
\newtheorem{assum}{Assumption}
%\titlespacing*{\section}{0pt}{3pt plus 3pt minus 1pt}{3pt plus 3pt minus 1pt}
%\titlespacing*{\subsection}{0pt}{3pt plus 3pt  minus 1pt}{3pt plus 3pt minus 1pt}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\E}{\mathbb{E}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\normx[1]{\left\Vert#1\right\Vert}
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
%\setlength{\parskip}{6pt}
%\setlength{\parindent}{0pt}


\jmlrheading{23}{2023}{1-\pageref{LastPage}}{7/23; Revised x/2x}{x/xx}{21-0000}{Giddens$^*$ and Zhou$^*$ and Krull and Brinkman and Song and Liu$^\dagger$ \\
$^*$ co-first authors; $^\dagger$ corresponding author}


%\ShortHeadings{DP-wERM and DP-OWL}
%\firstpageno{1}


\begin{document}
\title{A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning\vspace{-12pt}}

\author{\name Spencer Giddens$^*$ \email sgiddens@nd.edu \\
       \addr Department of Applied and Computational Mathematics and Statistics\\
       University of Notre Dame\\
       Notre Dame, IN 46556, USA
       \AND
       \name Yiwang Zhou$^*$\email yiwang.zhou@stjude.org \\
       \addr Department of Biostatistics \\ St. Jude Children's Research Hospital \\
       Memphis, TN 38105, USA
       \AND
       \name Kevin R. Krull\email kevin.krull@stjude.org \\
       \addr Department of Psychology and Biobehavioral Sciences\\ St. Jude Children's Research Hospital \\
       Memphis, TN 38105, USA 
       \AND
       \name Tara M. Brinkman\email tara.brinkman@stjude.org \\
       \addr Department of Psychology and Biobehavioral Sciences\\ St. Jude Children's Research Hospital\\
       Memphis, TN 38105, USA 
       \AND
       \name Peter X.K. Song\email pxsong@umich.edu \\
       \addr Department of Biostatistics \\ University of Michigan\\
       Ann Arbor, MI 48109, USA 
       \AND
       \name Fang Liu$^\dagger$\email fliu2@nd.edu \\
        \addr Department of Applied and Computational Mathematics and Statistics\\
       University of Notre Dame\\
       Notre Dame, IN 46556, USA}


\editor{My editor}

% Uncomment to remove the date
\date{}


\maketitle

\newpage
\begin{abstract}
It is common practice to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM).
While these models can be highly accurate in prediction, sharing the results from these models trained on sensitive data may be susceptible to privacy attacks.
Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data.
Previous work has primarily concentrated on applying DP to unweighted ERM.
We  consider weighted ERM (wERM), an important generalization, where each individual's contribution to the objective function can be assigned varying weights. We propose the first differentially private algorithm for general wERM, with theoretical DP guarantees.
Extending the existing DP-ERM procedures to wERM creates a pathway for deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL). 
We evaluate the performance of the DP-wERM framework applied to OWL in both simulation studies and in a real clinical trial.
All empirical results demonstrate the feasibility of training OWL models via wERM with DP guarantees while maintaining sufficiently robust model performance, providing strong evidence for the practicality of implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data.  
%OWL is widely used in clinical studies but also has applications in personalized advertising and recommender systems.
%This work has broad implications in medical studies and other settings that use wERM models for making predicting while needing to protect the privacy of the individuals whose data is used in training such models.
\end{abstract}

\begin{keywords} 
Differential privacy; global sensitivity; individualized treatment rule; support vector machines; weighted empirical risk minimization.
\end{keywords}



\newpage
%-------------------------------------------
\section{Introduction}\label{sec:intro}
%{\color{red} I would like to switch the first two sections; 1.2 first and 1.1 second.  Perhaps 1.1 can be absorbed into 1.2.  I think there lacks a key insight on the need of data privacy in OWL. This insight/need should be stated in the opening sentences in this paper. Below is my try for the opening paragraph.  In addition, 1.3 is too long and needs to be shortened.  Overall the integration of three subsections needs an improvement.  }

% {\color{red} Outcome weighted learning (OWL) has gained its great popularity as one of the most effective data analytics to employ the arsenals of machine learning to deliver individualized medical or business decisions. In the current literature, establishing OWL is carried out under the assumption that data scientists have full access to training data with no protection of data privacy. This assumption is indeed impractical because in real-world applications many data sources such as electronic health records and national survey data contain sensitive information, so ensuring a certain level of data privacy is inevitable. To address this privacy requisite, in this paper we aim to make a practically important, non-trivial extension of OWL, and the resulting machinery is termed as \emph{differentially private outcome weighted learning (DP-OWL)}. This extension is technically viable due to the recent advances in differentially private support vector machines (SVMs) that play a central role in performing optimization required in the derivation of OWL.}


Outcome weighted learning (OWL) \citep{Zhao2012} has gained great popularity as one of the most effective machine learning (ML) techniques for delivering individualized medical or business decisions.
%Clinical trials are a common data source used to train OWL models In in the context of precision medicine.
For example, in a two-armed randomized trial, where participants are randomly assigned to one of two treatments, a ``benefit'' metric is measured to assess the effectiveness of the treatment for each participant.
The OWL loss function represents the benefit-weighted average shortfall between the randomly assigned treatment and predicted labels, minimization of which will lead to a classifier that can be used to predict the optimal treatment with the highest expected benefit for an individual. %given the collected data (baseline characteristics and prognostic variables,  treatment assignment, and benefit measurement) 

The current literature on OWL assumes that data scientists have unrestricted access to training data.
This assumption proves impractical in real-world applications where sensitive health information exists in data sources such as randomized clinical trials, electronic health records, and national surveys, necessitating the protection of data privacy for ethical and legal purposes when training OWL models. 

We propose a general framework of \emph{differentially private weighted empirical risk minimization (DP-wERM)}, of which DP-OWL is a special case, to release privacy-preserving outputs from wERM.
DP-OWL leverages the recent advancements in differentially private support vector machines (SVMs), which play a vital role in the optimization of OWL models.
DP-OWL aims to provide a practical solution for training OWL models  on sensitive data, facilitating information sharing with mathematical privacy guarantees.

\subsection{Background}\label{subsec:background}
Individualized treatment rules (ITRs) are fundamental in the field of  precision medicine \citep{national2011toward, collins2015new}, an innovative framework for maximizing the clinical benefit that takes into account individual heterogeneity to tailor treatments for subgroups of patients \citep{Qian2011} instead of relying on a one-size-fits-all paradigm.
% various approaches have been proposed in the literature to construct ITRs \citep{watkins1989learning, murphy2003optimal}.  
%This approach contrasts with broadly applying the same treatment to an entire population presenting with the same set of symptoms and can lead to better patient outcomes \citep{Qian2011}. 
In addition to precision medicine, ITRs are also applicable to personalized advertising \citep{Wang2015, Sun2015} and recommender systems \citep{Schnabel2016, Lada2019}. 

Various methods have been proposed in the literature to establish ITRs. OWL is a common framework for constructing ITRs by directly optimizing the population-level expected outcomes.
\citet{Zhao2012} showed that estimating the optimal ITR is equivalent to a classification problem of treatment groups, in which participants in different groups are weighted proportionally to their observed clinical benefits. %In OWL, weighted support vector machine (SVM) is invoked to estimate the optimal ITR.  
OWL has also been extended to search for individualized continuous doses \citep{chen2016personalized}, derive ITRs for multiple treatment options \citep{zhou2018outcome}, and establish dynamic treatment regimes \citep{eguchi2022outcome}. 
\citet{Zhou2017} proposed residual weighted learning (RWL) to improve the finite sample performance of OWL by weighting individual participants with the residuals of their benefits obtained by a regression model and solving the optimization problem using a difference of convex algorithm.
% To address these challenges, residual weighted learning (RWL) \citep{Zhou2017} was proposed to derive ITRs by weighting observations using residuals of the outcome from a regression fit and solve the optimization problem using a difference of convex (d.c.) algorithm. 
Matched learning (M-learning) \citep{Wu2020} further improves the OWL-based framework by utilizing matching instead of inverse probability weighting to balance participants in different treatments, making the algorithm more applicable for the analysis of observational studies (e.g., large-scale electronic health records). We focus on OWL in this work with formal privacy guarantees and will explore incorporating privacy guarantees in other types of ITR frameworks in future work. %, which typically have much larger sample sizes than clinical trials. 


\begin{comment}
Traditional privacy protection techniques such as anonymization %(i.e., removing identifiers like names, addresses, etc., or quasi-identifiers such as age, gender, etc.) 
have been shown to be insufficient in ensuring privacy.
For example, %\citet{Narayanan2008} demonstrated, using the anonymized 2006 Netflix Prize dataset, that uncovering sensitive, individual-level information from anonymized datasets is possible with access only to limited, publicly accessible side information.
\citet{Sweeney2015} demonstrated re-identification attacks using Washington State health records.
ML models learned from anonymized datasets have also been shown to be vulnerable to attacks inferring membership in and sensitive attributes of the dataset \citep{Shokri2017, Zhao2021}.
\end{comment}

Differential privacy (DP) \citep{Dwork2006} is a state-of-the-art privacy notion that provides a mathematically rigorous framework for ensuring the privacy of sensitive datasets.
Privacy literature has repeatedly shown that ad hoc de-identification and anonymization of sensitive datasets, even if done carefully, is insufficient to guarantee privacy.
There are several examples across various disciplines where attackers leveraged anonymized data to infer masked or removed values \citep{Narayanan2008, Sweeney2015, Ahn2015}.
Aggregated statistics are also vulnerable, as demonstrated by an attack on the 2010 US Census \citep{Desfontain2021}.
Sometimes (even black-box) access to trained ML models permits inferring membership in the training data \citep{Shokri2017} and attribute values in some cases \citep{Zhao2021}.
DP has emerged as a popular framework in privacy research, offering precise mathematical guarantees of privacy that do not make any assumptions on the would-be attacker's methods or auxiliary knowledge.
In this framework, carefully calibrated random noise is injected through a randomized mechanism into statistics or function outputs derived from sensitive data, lowering the probability of learning personal information used to generate the outputs.


\subsection{Related Work}\label{subsec:related_work}
In work completed concurrently to, but independently from ours, \citet{Spicker2024} developed a method for differentially private OWL.
Their method is an extension of the DP SVM approach \citep{Rubenstein2012}, which is less general than the DP-ERM approach \citep{chaudhuri2011} in that it is based in the SVM framework and not directly extensible to other ERM problems.
Comparatively, our DP-wERM framework is more general.

To our knowledge, no other privacy-preserving counterparts have been developed for the models constructing ITRs.
Though some recent work estimates the individual treatment effects themselves in a DP-satisfying manner \citep{Betlei2021, Niu2022}, our work is different in that it learns a treatment assignment rule with DP guarantees.

In terms of DP-ERM, \citet{Chaudhuri2009} analyzed the application of DP to releasing logistic regression coefficients.
\citet{chaudhuri2011} generalized DP logistic regression and developed DP counterparts to binary classification problems in a general unweighted ERM framework.
Subsequent works have incrementally improved DP-ERM analysis, such as by tightening bounds on excess risk or making the methods more computationally efficient \citep{Bassily2014, Kasiviswanathan2016}.
\citet{Kifer2012} extended the framework in \citet{chaudhuri2011} to the approximate DP framework with a broader set of regularizers.


%----------------------------------------
\subsection{Our Work and Contribution} 
Motivated by the problem of training OWL models using sensitive datasets, 
we formulate a general wERM problem and present a procedure to achieve DP guarantees for the outputs from wERM, of which OWL is special case.
wERM is an extension of unweighted ERM, where individual weights are applied to the loss function evaluated at each observation in the training dataset. 

We prove that, under certain regularity conditions, our wERM algorithm is differentially private. 
We discuss how to enhance the utility of our algorithm for a given privacy budget through hyperparameter tuning and feature selection.
We also provide an algorithm for optimal hyperparameter tuning using a dataset independent of the sensitive dataset on which the wERM is trained and examine its robustness even when the independent dataset deviates from the sensitive dataset in some aspects.
To our knowledge, our procedure represents the first extension to the wERM case of the unweighted ERM models with DP guarantees examined in \citet{Chaudhuri2009, chaudhuri2011, Kifer2012}, among others. 

We apply our DP-wERM algorithm in a simulation study and a real randomized clinical trial to train OWL with privacy guarantees.
The results demonstrate that our algorithm has the potential to build useful OWL models for optimal ITR derivation and fairly accurate estimation of the empirical treatment value relative to their nonprivate counterparts while preserving privacy for study participants.



%-----------------------------------------
\section{Preliminaries}
\label{sec:preliminaries}

In this section, we formalize the notions of wERM, OWL, and DP.
By default, an unspecified norm is the $\ell_2$ norm (i.e., $\norm{\cdot} = \norm{\cdot}_2$) and boldfaced symbols are vector-valued.

\subsection{Weighted Empirical Risk Minimization (wERM)}
\label{subsec:wERM}
Let $(\mathbf{x}_i, y_i) \in (\mathcal{X},\mathcal{Y})$ represent the $p$ features or predictors in observation $i$ and the corresponding outcome, respectively, for $i=1,\ldots,n$.
Let $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ denote a non-negative loss function over the label space $\mathcal{Y}$.
The goal in ERM is to determine a predictor function $f$ that minimizes the loss function over the data. Linear regression, logistic regression, and SVM are all examples of ERM.
 We assume $f$ is parameterized by $\boldsymbol{\theta}$. It is also common to employ a regularizer $R$ that penalizes the complexity of  $f_{\boldsymbol{\theta}}$;
%We denote the regularizer  by $R: \mathcal{F} \rightarrow \mathbb{R}$, where $\mathcal{F}$ represents the set of considered predictor functions $f: \mathcal{X} \rightarrow \mathcal{Y}$. 
the solution to a regularized unweighted ERM is 
\begin{equation}\label{eqn:ERM}
\argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}}\left\{ \frac{1}{n}\sum_{i=1}^n \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}= \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}}\left\{ \frac{1}{n}\sum_{i=1}^n \ell_i(\boldsymbol{\theta}) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}, 
\end{equation}
where $\gamma>0$ is a tunable regularization constant and $\mathcal{F}$ is an appropriate predictor function space that governs specific instances of ERM.
%For example, ERM reduces to linear regression if $\mathcal{F}$ is defined to be the set of all functions of the form $f_{\boldsymbol{\theta}, b}(\mathbf{x}) = \mathbf{x}^T\boldsymbol{\theta}$. %where $\boldsymbol{\theta}$ feature weights, and $b$ is a bias term.

In some applications, each of the $n$ loss function terms $\ell_i(\boldsymbol{\theta})$ in the ERM problem in Eqn \eqref{eqn:ERM} may be weighted differently, leading to weighted ERM (wERM).
\begin{defn}[weighted empirical risk minimization (wERM)]
\label{def:wERM}
Define $(\mathbf{x}_i, y_i)$, $\ell$, $R$, $\gamma$, and $\mathcal{F}$ as above. 
Let $w_i$ denote the weight associated with individual loss $\ell_i(\boldsymbol{\theta}) = \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$.
A wERM problem is defined as
    \begin{equation}\label{eqn:wERM}
        \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}} \left\{\frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}= \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}}\left\{ \frac{1}{n}\sum_{i=1}^n w_i \ell_i(\boldsymbol{\theta}) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}.
    \end{equation}
\end{defn}
\noindent An example of wERM is OWL, as introduced next.

%---------------------------------
\subsection{Outcome Weighted Learning (OWL)}
\label{subsec:OWL}
%{\color{red} Notation in this section is not well aligned with that given in the previous section 2.1. I guess, here we don't have that lable outcome $y_i$ given in the standard setting as treatment is part of $x_i$ not outcome; in factor, the outcome is benefit $B_i$. This point is not clearly stated. As a matter of factor, we have a model with treatment-attribute interactions to predict outcome $B$. In the previous section, $f(x_i)$ directly predicts label $y_i$, while here we don't have this direct prediction scheme; rather we use sign($f(x)$) to make prediction. Thus, the $f$ function means different things, I guess.  } 

Consider a dataset $D=\{(\mathbf{x}_i, A_i, B_i)\,;\,i=1,\dots,n\}$ collected from either a randomized clinical trial or an observational study with a total of $n$ participants. 
A $p$-dimensional vector $\mathbf{x}=(x_1,\dots,x_p)^\top\in\mathcal{X}\subseteq\mathbb{R}^p$ contains features used for ITR derivation. WLOG, we assume a binary treatment $A\in\mathcal{A}=\{-1,1\}$ that each participant with probability $P(A|\mathbf{x})$ at the beginning of the study. 
In a randomized clinical trial, complete randomization implies $P(A=1|\mathbf{x})=P(A=-1|\mathbf{x})=0.5$. In an observational study, treatment assignment depends on the features $\mathbf{x}$, and the probability $P(A|\mathbf{x})$ can be estimated using methods such as logistic regression. Noted is that $A$ does not necessarily represent the underlying optimal treatment assignment that is the most beneficial (measured by $B$) for a subjects in ITR settings. In other words, there are no labels in the training data for ITR problems  -- a key difference between ITR and traditional ML classification problems. The goal of ITR is not to estimate a treatment assignment function that predicts the treatment an individual actually receives, but rather to leverage the benefit information $B$ to produce a function that will assign individuals to their underlying optimal treatment assignment in the future; %that is, to derive a treatment assignment function $T: \mathcal{X}\rightarrow \mathcal{A}$, which is a mapping from the feature space to the space of possible treatments.  $B_i$ refers to the treatment benefit for individual $i$ that measures the therapeutic effects of the treatments. 



OWL \citep{Zhao2012} is a seminal work enabling the estimation of an optimal ITR $T^*$ that maximizes the expected clinical benefit $\mathbb{E}[\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A=T(\mathbf{x}))]$, where $\mathbbm{1}(\cdot)$ is an indicator function and $P(A|\mathbf{x})$ is the propensity score -- the probability of receiving treatment $A$ given $\mathbf{x}$.   WLOG, we assume that the larger the value of $B$, the greater the treatment benefit.
The maximization problem above is equivalent to  $T^*\in \argmin_{T\in\mathcal{T}} \mathbb{E}[\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A\neq T(\mathbf{x}))]$, where $\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A\neq T(\mathbf{x}))$ is a weighted classification error.
Therefore, OWL pertains to a weighted classification problem.
Let $\mathcal{S}=\{(\mathbf{x}_i, A_i, B_i)\,;\,i=1,\dots,n\}$ be a set of observations and let $T(\mathbf{x})=1$ if $f(\mathbf{x})>0$ and $T(\mathbf{x})=-1$ otherwise for some predictor function $f$.
Then the optimization problem becomes an ERM problem
%$$\textstyle T^*\in \argmin_{T\in\mathcal{T}} \frac{1}{n} \sum_{i=1}^n \frac{B_i}{P(A_i|\mathbf{x}_i)}\mathbbm{1}(A_i\neq T(\mathbf{x}_i)),$$
%which is equivalent to 
$$\textstyle f^*\in \argmin_{f\in\mathcal{F}} \frac{1}{n} \sum_{i=1}^n \frac{B_i}{P(A_i|\mathbf{x}_i)}\mathbbm{1}(A_i\neq \text{sign}(f(\mathbf{x}_i))).$$ 
The loss function is a weighted sum of 0-1 loss that is neither convex nor continuous.
To solve the optimization problem, OWL uses a convex surrogate hinge loss $\text{max}(0,x)$ to replace the 0-1 loss. 
To penalize the complexity of the predictor function $f$, an $\ell_2$ penalty is often used, leading to 
optimization problem:
\begin{equation}\label{eqn:OWL}
    \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}} \left\{\frac{1}{n}\sum_{i=1}^n \frac{B_i}{P(A_i|\mathbf{x}_i)}\text{max}(0,1-A_i f_{\boldsymbol{\theta}}(\mathbf{x}_i))+\frac{\gamma}{n}\norm{\boldsymbol{\theta}}\right\},
\end{equation}
where $f_{\boldsymbol{\theta}}(\cdot)$ is a predictor function parameterized in $\boldsymbol{\theta}$. It is straightforward to see that OWL as formulated in Eqn \eqref{eqn:OWL}  is a special case of wERM from Definition \ref{def:wERM}, where  $A$ corresponds to $y$ in Eqn \eqref{eqn:wERM},
\begin{align}\label{eqn:wl}
w_i=\frac{B_i}{P(A_i|\mathbf{x}_i)}, \mbox{ and }
\ell_i(\boldsymbol{\theta})=\text{max}(0,1-A_i f_{\boldsymbol{\theta}}(\mathbf{x}_i)).
\end{align}

%-------------------------------------------------
\subsection{Differential Privacy (DP)}
\label{subsec:DP}
We use the DP framework to achieve privacy guarantees when releasing results from OWL models.
The DP framework ensures that, regardless of the inclusion or exclusion of any single individual's data, the results of a DP-satisfying mechanism acting on a sensitive dataset are sufficiently similar.
We formalize DP by first defining the notion of neighboring datasets.

\begin{defn}[neighboring datasets
    \textnormal{\citep{Dwork2006}}]
\label{def:neighbor} Two datasets $D, \tilde{D} \in \mathcal{D}$ are considered \textnormal{neighboring datasets}, denoted by  $d(D, \tilde{D})=1$, if $D$ can be obtained from $\tilde{D}$ by modifying a single individual's data\footnote{The modification referred to in Definition \ref{def:neighbor} can be deletion/removal or substitution/replacement.}.
    %If the distance $d$ between two datasets is defined to be the number of records differing between them, $D$ and $\tilde{D}$ are neighboring datasets if and only if $d(D, \tilde{D})=1$.
\end{defn}
\begin{defn}[$(\epsilon, \delta)$-differential privacy
    \textnormal{\citep{Dwork2006b}}]
\label{def:DP} A randomized mechanism $\mathcal{M}$ satisfies \textnormal{$(\epsilon, \delta)$-differential privacy} if for all $S \subset \textnormal{Range}(\mathcal{M})$ and $d(D, \tilde{D})=1$,
    \begin{equation}\label{eqn:DP}
        P(\mathcal{M}(D) \in S) \le e^\epsilon P(\mathcal{M}(\tilde{D}) \in S) + \delta,
    \end{equation}
    where $\epsilon>0$, and $\delta\in[0,1)$ are privacy loss or privacy budget parameters.
    When $\delta=0$, it becomes \textnormal{$\epsilon$-DP}.
\end{defn}

Participants in the neighboring datasets are arbitrary, and therefore the DP guarantees hold for all participants simultaneously.
Per Eqn \eqref{eqn:DP}, the privacy of each participant is protected since the resulting output will be similar whether their data is included or not.
The tunable parameter $\epsilon$ controls the similarity of the output distributions on the neighboring datasets.  Smaller $\epsilon$ implies more privacy.
Generally, $\epsilon\le1$ is considered to provide strong privacy guarantees, though it is not unusual to see larger values of $\epsilon$ used in applications.
It is common to interpret $\delta$ as the probability that $\epsilon$-DP fails.
The value $\delta$ is often set on the order of $o(1/\mbox{poly}(n))$.

DP possesses appealing theoretical properties that facilitate its implementation in practical settings.
The post-processing theorem, for example, ensures that the output of a DP mechanism cannot be further manipulated to weaken the DP guarantees.
Additionally, in many situations, it is common to generate multiple statistics or outputs from a sensitive dataset.
%For example, one may seek to compute both a mean and a standard deviation from a dataset or output gradients from the optimization of a deep neural network.
The quantification of the overall privacy loss over the applications of multiple DP mechanisms to the same dataset is known as privacy loss composition and has been well-studied in the literature \citep{Dwork2006b, mcsherry2007mechanism, Dwork2010, abadi2016deep, Bun2016, Mironov2017, Dong2019}.
%There exist  several composition theorems \citep{Dwork2006, Dwork2006b, Dwork2010} that allow the quantification of  DP guarantees of the composition of several DP mechanisms. The recent extensions of the original DP definitions in Definition \ref{def:DP} are driven by the motivation to have tighter upper bounds on privacy loss over composition \citep{Bun2016, Mironov2017, Dong2019}.
%Our algorithm DP-OWL has only one output and no composition is needed.

A common way of achieving DP guarantees for information release is to add noise, appropriately calibrated to a given privacy loss or budget, directly to the output before release.
The scale of the added noise often relates to the global sensitivity of the output.
Global sensitivity is originally defined using the $\ell_1$ norm by \citet{Dwork2006}.
In this paper, we use a more general definition.

\begin{defn}[$\ell_p$-global sensitivity\citep{Liu2019}]
    Let $\mathbf{s}$ be an output calculated from a dataset.
    The \textnormal{$\ell_p$-global sensitivity} of $\mathbf{s}$ is 
    \begin{equation}
        \Delta_{p,\mathbf{s}} = \max_{d(D,\tilde{D}) = 1}\norm{\mathbf{s}(D) - \mathbf{s}(\tilde{D})}_p, \mbox{  where $\norm{\cdot}_p$ is the $\ell_p$ norm for $p>0$.}
    \end{equation}
\end{defn}

The global sensitivity bounds the amount by which the output $\mathbf{s}$ can vary between two neighboring datasets.
%As an example, from a dataset of $n$ observations on a single attribute with values bounded within $[5, 10]$ globally, the $\ell_p$-global sensitivity of the sample mean is $(10 - 5)/n$, representing the largest possible change in the sample mean from modifying a single individual value.
%In this example, the $\ell_p$-global sensitivity is the same for all $p$, as the statistic is a scalar.
In general, the larger the global sensitivity of an output, the larger the noise scale of a DP mechanism $\mathcal{M}$ is to achieve privacy guarantees at pre-specified $(\epsilon, \delta)$.
For example, the Laplace mechanism is a commonly used mechanism that satisfies $\epsilon$-DP \citep{Dwork2006} by adding noise drawn from a Laplace distribution with mean 0 and scale $\Delta_{1, \mathbf{s}}/\epsilon$ to the output of a target function/computed statistic $\mathbf{s}$. As a trade-off, mechanisms satisfying DP generally incur a cost in data utility.
Privacy loss parameters $(\epsilon, \delta)$ can be prespecified to yield a satisfactory trade-off between privacy protection and data utility.


%-----------------------------------------------------
\section{Differentially Private wERM for Binary Classification}
\label{sec:methods}

We now present a general procedure for privacy-preserving wERM via output perturbation and prove that it satisfies $\epsilon$-DP. WLOG, 
we restrict our attention to the space of linear predictor functions $\mathcal{F} = \{f_{\boldsymbol{\theta}}\,:\,f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}^\intercal\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$ in Definition \ref{def:wERM}. Nonlinear cases (e.g., the radial kernel for SVM) can be approximated with linear predictor functions in unweighted  ERM problems (e.g., \citet{chaudhuri2011} approximates the radial kernel SVM via random projections \citep{Rahimi2007, Rahimi2008} in the DP-ERM framework); the same applies to the weighted  ERM problems. %Furthermore, the proposed framework can indirectly handle nonlinear predictor functions.
A predicted label $\hat{y}$ for a given $\mathbf{x}$ can be obtained from the estimated $\hat{f}=f_{\hat{\boldsymbol{\theta}}}(\mathbf{x})= \mathbf{x}^\intercal\hat{\boldsymbol{\theta}}$, such as sign($\hat{f}$) if $y\in\{1,-1\}$, where $\hat{\boldsymbol{\theta}}$ is an optimal solution to the wERM problem.
%We discuss the modifications necessary to the OWL procedure in order for it to be a special case of the DP-satisfying wERM algorithm.This collectively results in proving an approximation to OWL that satisfies DP.


%-----------------------------------------------------
\subsection{Regularity Conditions}
\label{subsec:regularity_conditions}
To develop a privacy-preserving version for the wERM problem in Definition \ref{def:wERM}, some regularity conditions are required, as listed in Assumption \ref{assump}.
The conditions not related to the weights $\mathbf{w}$ are the same as the conditions in \citet{chaudhuri2011}.

\begin{assum}[Regularity conditions for DP-wERM]\label{assump}\hspace{1in}\vspace{-3pt}
\begin{enumerate}
\setlength{\itemsep}{-2pt}
        \item[{A1.}] $\norm{\mathbf{x}_i}\le 1$ and $w_i\in(0,W]$ for all $i$; 
        \item[{A2.}] The set of predictor functions are linear predictors $\mathcal{F} = \{f_{\boldsymbol{\theta}}\,:\,f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}^\intercal\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$; 
        \item[{A3.}] Loss function $\ell$ is convex and everywhere first-order differentiable;
        \item[{A4.}] For any observed label $y\in\{-1,1\}$ and any predicted value $\hat{f} = f_{\hat{\boldsymbol{\theta}}}(\mathbf{x})$, 
        the loss function $\ell(\hat{f}, y)$ must be expressible as a function of the product $z=y\hat{f}$; in other words, $\ell(\hat{f}, y) = \tilde{\ell}(z)$ for some function $\tilde{\ell}$.
        Also, $|\tilde{\ell}^\prime(z)| \le 1\;\forall\; z$;
        \item[{A5.}] The regularizer $R$ is $1$-strongly 
        convex \footnote{Technically, $R$ could be $\Lambda$-strongly convex, but this distinction does not impact our results. We instead assume 1-strong convexity WLOG as this simplifies the proof of Theorem \ref{thm:wERM_sens}.} and everywhere first-order differentiable.
          %{\color{red}, $R$ is in general defined on a functional space, so its convexity and differeniabilty are not the same as those given in the Calculus. Of course, since the class of linear predictor functions is considered in this paper, this $R$ function is in fact defined on $\theta$ in the Euclidean space. This is a point worth a comment. }
    \end{enumerate}
\end{assum}
% \item[{A2.}]$w_i\sim f(\mu_w,\sigma^2_w)$ independently across $i=1,\ldots,n$ with support $\in(0,W]$, where $\mu_w=\mathbb{E}(w_i)$ and $\sigma^2_w=\mathbb{V}(w_i)$; 

Some of the above conditions (e.g., A1 $\norm{\mathbf{x}_i}\le1$) are satisfied by minor modifications to datasets.
%However, knowledge of upper and lower bounds on the possible feature values can be used to preprocess the dataset so that the condition is met. 
%If the upper and lower bounds can be reasonably well inferred or approximated without reference to the dataset itself (e.g., the human age can be reasonably assumed to be bounded between $[0, 125]$ years old; some studies are designed to run in adults, which leads to natural and publicly known bounds of $[18, 45]$), this preprocessing can be done without incurring additional privacy cost.
%If for some features the bounds are difficult to obtain without referencing the data, a portion of the privacy budget could be allocated to utilize the data to select the bounds.
While we focus on predictor functions $f_{\hat{\boldsymbol{\theta}}}(\mathbf{x})$ without an explicit bias (intercept) term, an implicit bias term can still be used by augmenting $\mathbf{x}$ to include a constant term.
The above conditions on $\ell$ and $R$ can be achieved by weighted versions of common classification methods with only minor modifications (if any).  For example, the loss function in logistic regression automatically satisfies conditions A3 and A4.
The hinge loss function commonly used for SVM can be easily smoothed to an approximation that also satisfies conditions A3 and A4. 
The popular $\ell_2$-norm regularizer satisfies condition A5.


\subsection{DP-wERM Algorithm} \label{subsec:DP-wERM}
Under Assumption~\ref{assump}, we develop the DP-wERM procedure, as presented in Algorithm \ref{alg:DP-wERM} when solving general wERM problems with $\epsilon$-DP guarantees.
The derivation of the global sensitivity $(C+2W)/\gamma$ on line 3 of Algorithm~\ref{alg:DP-wERM} is provided in Section \ref{sec:DP_guarantees}.
Noise $\mathbf{z}^*=\zeta\frac{\mathbf{z}}{\norm{\mathbf{z}}}$ generated in lines 4 to 7 of the algorithm is equivalent to drawing random sample from distribution $f(\mathbf{z}^*)\propto \exp(-\frac{\epsilon}{\Delta}\norm{\mathbf{z}^*})$ \citep{Chaudhuri2009}. 
%a uniform sample from a $p$-dimensional hypersphere \citep{Muller1959}.
The output predictor function $f^*$ on line 8 satisfies $\epsilon$-DP through sanitizing the parameters $\hat{\boldsymbol{\theta}}$ with DP guarantees.\vspace{-8pt}

\begin{algorithm}[H]
\caption{Differentially private weighted empirical risk minimization (DP-wERM)}\label{alg:DP-wERM}
\begin{algorithmic}[1]
    \State \textbf{Input}: Privacy budget $\epsilon>0$; dataset $D = (\mathbf{x}_i, y_i, w_i) \in \mathbb{R}^p \times \{-1, 1\} \times \mathbb{R}$ of feature-label-weight triples with $\norm{\mathbf{x}_i}\le 1$ and weights $0 < w_i \le W$ for all $i$; regularization constant $\gamma>0$; constant $C$ (see Theorem \ref{thm:wERM_sens});  $f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}^\intercal\boldsymbol{\theta}$; loss $\ell$ and regularizer $R$. %each satisfying regularity conditions A3-A5.
    \State \textbf{Output}: Privacy preserving predictor function $f^*$
    \State Set $\hat{\boldsymbol{\theta}} \gets \{\argmin_{\boldsymbol{\theta} \in \mathbb{R}^p} \frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta})\}$ \Comment{\texttt{wERM without DP}}
    \State Set $\Delta \gets (C+2W)/\gamma$ \Comment{\texttt{$\ell_2$-global sensitivity of $\hat{\boldsymbol{\theta}}$}}
    \State Sample $\zeta$ from a Gamma distribution with shape $p$ and rate $\epsilon/\Delta$
    \State Sample $\mathbf{z}$ from the $p$-dimensional multivariate standard Gaussian distribution
    \State Set $\hat{\boldsymbol{\theta}}^* \gets \hat{\boldsymbol{\theta}} + \zeta\frac{\mathbf{z}}{\norm{\mathbf{z}}}$ \Comment{\texttt{Randomized mechanism to achieve $\epsilon$-DP}}
    \State \Return $f^*=f_{\hat{\boldsymbol{\theta}}^*}= \mathbf{x}^\intercal\hat{\boldsymbol{\theta}}^*$ for  any $\mathbf{x}\in \mathbb{R}^p$ with $\norm{\mathbf{x}}\le 1$
\end{algorithmic}
\end{algorithm}\vspace{-12pt}


%In the setting of OWL, the loss function can be formulated as a weighted SVM problem. SVM is still an ERM problem, so is weighted SVM relative to wERM. To achieve DP guarantees for SVM, using the well-known ``kernel trick''  that transforms the input data to a higher dimensional space and finds a linear separating hyperplane in that space to classify data would leak  

%Nonlinear separators (in the original dataset dimension) can also be constructed by 

\subsection{Feature Engineering via Dimensionality Reduction or Variable Selection}
\label{subsec:dimensionality_deduction}
The dimension of $\boldsymbol{\theta}$ in the predictor function $f$ increases with the number of predictors in $\mathbf{x}$. %(in the case of OWL, they basically have the same dimensionality).
The larger the number of predictors, the larger the amount of noise injected into $\hat{\boldsymbol{\theta}}$ is for a pre-specified privacy budget in Algorithm \ref{alg:DP-wERM}, leading to noisier privacy-preserving $\hat{\boldsymbol{\theta}}^*$ and $f^*$.
It is generally good practice to perform variable selection or dimensionality reduction to limit the number features and thus parameters  through  dimension reduction or variable selection techniques.
%One benefit of performing variable selection instead of dimensionality reduction is that it preserves the interpretability of the predictive model, which is not necessarily the case in the alternative approach. 

The feature engineering step can be included an internal pre-processing step -- which variables are selected and what the newly created features are do not need to be released together with the learned function $f^*$ from Algorithm \ref{alg:DP-wERM}.
If the data curator wishes to make the procedure more transparent rather than a ``black box'', the selected variables or newly engineered features can be released.
When there exists prior knowledge or subject matter expertise that informs the relevant predictors or features, or an independent dataset that can be used to select predictors or perform dimensionality reduction, the feature engineering step would not incur additional privacy cost.
If no such knowledge or datasets exist, one can allocate a small amount of the overall privacy budget or divide the whole dataset into two non-overlapping portions for privacy-preserving variable selection \citep{Kifer2012, Thakurta2013, Steinke2017} or dimensionality reduction \citep{Chaudhuri2013, Dwork2014PCA, Gonem2018}. 
%Alternatively, we could , one used for privacy-preserving variable selection/dimensionality reduction and the other for DP-wERM at the same privacy budget.


%However, for a real-world dataset, it is possible that there is sufficient correlation between features that selecting few enough features to achieve good performance of DP-OWL is difficult. In such cases, it may be valuable to first use correlation-reducing variable transformation techniques such as principal component analysis (PCA) before the feature selection step. These techniques were not utilized for our simulations because the features are already independent as an assumption of the simulation settings, but they may be valuable in a real-world implementation.

\vspace{-3pt}\subsection{Hyperparameter Tuning}\label{subsec:hyperparameter_tuning}\vspace{-3pt}
The regularization constant $\gamma$ in the non-DP setting is often tuned using cross-validation on the same dataset used to fit the model, but this approach cannot be directly applied to the DP case without considering the complication of privacy loss.
The DP literature on statistical and ML procedures often treats the issue of hyperparameter tuning as a secondary issue, implicitly assuming the hyperparameters for those procedures are known \emph{a priori} or can be obtained from other independent data sources.
For example, \citet{chaudhuri2011} provide a hyperparameter tuning method, but privacy loss from hyperparameter tuning in their experiments is not accounted for in the results, essentially assuming a public dataset of the same distribution as the private one.
Similarly, \citet{Kifer2012} does not address hyperparameter selection and neither does \cite{abadi2016deep}  discuss how hyperparameters in deep neural networks are selected when training the neural networks with DP guarantees.

If no prior knowledge exists to inform the choice of hyperparameters, the sensitive dataset itself can be used to tune hyperparameters and caution must be used so as not to violate DP guarantees.
Two approaches can be used for the tuning task: (1) allocating a portion of the total privacy budget or (2) allocating a portion of the dataset.
The former approach permits use of the whole dataset for both hyperparameter tuning and model training, while the latter allows for tuning at the same privacy budget as used for training the DP-wERM on the rest of the data.
The hyperparameter tuning method suggested by \citet{chaudhuri2011} follows the second approach.
If the training dataset is not large enough, both approaches are likely to return noisy ``optimal'' hyperparameters that may not have good utility.

Our experiments show that the optimal value for the regularization constant $\gamma$ for DP-wERM varies with the privacy budget $\epsilon$ and the training set size $n$.
Thus, it is important to tune $\gamma$ in order to obtain the best possible performance for DP-wERM.
To aid this process, we provide an $(\epsilon, n)$-adaptive procedure in Algorithm \ref{alg:reg_const_selection} for tuning $\gamma$. 
 \vspace{-12pt}

%Assume we have a public dataset of size $N$ and that we will hold out a minimum of $m$ out of the $N$ data points as a testing dataset for evaluation purposes. We first draw a random sample of the target sample size $n$ from the public dataset and train a DP-wERM model via Algorithm \ref{alg:DP-wERM} using the target privacy budget $\epsilon$. Then we evaluate the model on the remaining data points, which form the testing dataset. If $n>N-m$, meaning there will not be sufficient testing data points for evaluation, then $m$ evaluation data points are first randomly selected from the public dataset and held out, then a random sample of target sample size $n$ is bootstrapped from the remaining data points. This sample, train, evaluate procedure is repeated 1000 times for each of a set of considered regularization constant values, and the value producing the largest mean empirical value function for the given combination of $\epsilon$ and $n$ is chosen.
\begin{algorithm}[H]
\caption{$(n,\epsilon)$-adaptive hyperparameter tuning using an independent dataset}\label{alg:reg_const_selection}
\begin{algorithmic}[1]
\State \textbf{Input}: training data size $n$; privacy budget $\epsilon$;  hyperparameter tuning dataset $D_0$ of size $n_0$; minimum validation set size $m$ ($m<n_0$); candidate regularization constant set $\{\gamma_1, \ldots, \gamma_k\}$; number of repeats $r$; evaluation metric $v(f^*, D_0)$ for fitted model $f^*$.
\State \textbf{Output}: $(n,\epsilon)$-adaptive regularization hyperparameter $\gamma$
\For{$i = 1, 2, \ldots, k$}
\For{$j = 1, 2, \ldots, r$}
\If{$n>n_0-m$}
    \State Set $D_{j,\textnormal{valid}}$ to be a random subset of $D_0$ of size $m$, sampled without replacement
    \State Set $D_{j,\textnormal{train}}$ to be a set of size $n_0$, sampled with replacement from $D_0\setminus D_{j,\textnormal{valid}}$
\Else
    \State Set $D_{j,\textnormal{train}}$ to a set of size $n_0$,  sampled  without replacement from $D_0$
    \State Set $D_{j,\textnormal{valid}}=D\setminus D_{j,\textnormal{train}}$
\EndIf
\State Run Algorithm \ref{alg:DP-wERM} given $\epsilon$, $D_{j,\textnormal{train}}$, and $\gamma_i$ to obtain $f^*_{j,i}$
\State Compute and store $v(f^*_{j,i}, D_{j,\textnormal{valid}})$
\EndFor
\State Set $\bar{v}_i=r^{-1}\sum_{j=1}^r v(f^*_{j,i}, D_{j,\textnormal{valid}})$
\EndFor
\State Set $i^* = \argmax_i \{\bar{v}_1, \bar{v}_2, \ldots, \bar{v}_k\}$ \Comment{\texttt{WLOG, suppose better model has larger metric}}
\State \Return $\gamma=\gamma_{i^*}$
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:reg_const_selection} can be used when a subset of the sensitive dataset or an independent dataset $D_0$ from the sensitive dataset is available for tuning $\gamma$.
When the size of this dataset$n_0$ is small, such as in the former case or if $D_0$ is a small pilot study in the latter case, it is likely the steps in lines 6 and 7 will be used.
If $D_0$ is not ``perfect'' (e.g., its $\mathbf{X}_0$ might not completely overlap with $\mathbf{X}$ in the sensitive dataset or contain the latter as a subset, or $Y_0$ is a surrogate marker of $Y$ in the sensitive dataset) but is large in amount ($n_0>n+m)$, then lines 9 and 10  Algorithm \ref{alg:reg_const_selection} will be run.
Regardless, if $D_0$ is an independent data source from $D$, tuning $\gamma$ with $D_0$ via Algorithm \ref{alg:reg_const_selection} would not incur privacy loss on the sensitive dataset that is used for training the wERM algorithm. The evaluation metric function $v$ in Algorithm \ref{alg:reg_const_selection} should be defined to reflect an important performance metric for the original wERM problem, such as the the optimal treatment prediction accuracy or the empirical treatment value in OWL.
In the simulation studies and the case study, we set $v$ to be the optimal treatment prediction accuracy, though setting $v$ to be the empirical treatment value was found to nearly identical results.

As part of our simulation studies, we explore the robustness of Algorithm \ref{alg:reg_const_selection} in Section \ref{sec:sensitivity} for hyperparameter tuning when $D_0$ deviates from the sensitive dataset $D$ in various ways such as the number of records, the optimal treatment function, and the distribution of independent variables. 

%--------------------------------------
\vspace{-3pt}\section{DP Guarantees of DP-wERM Algorithm}
\label{sec:DP_guarantees}\vspace{-3pt}
To prove that Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP, we first introduce the concept of $\Lambda$-strong convexity, which is a necessary condition for establishing DP.
\begin{defn}[$\Lambda$-strong convexity]
    Let $g: \mathbb{R}^p \rightarrow \mathbb{R}$.
    If for all $\alpha \in (0, 1)$ and for all $\mathbf{x}, \mathbf{y}\in\mathbb{R}^p$,
    \begin{equation}
       \textstyle g(\alpha\mathbf{x} + (1-\alpha)\mathbf{y}) \le \alpha g(\mathbf{x}) + (1-\alpha)g(\mathbf{y}) - \frac{1}{2}\Lambda\alpha(1-\alpha)\norm{\mathbf{x}-\mathbf{y}}^2,
    \end{equation}
    then $g$ is said to be \textnormal{$\Lambda$-strongly convex} for $\Lambda>0$.
\end{defn}
It is straightforward to show that (i) if $g$ is $\Lambda$-strongly convex, then $ag$ is $a\Lambda$-strongly convex for any $a\in\mathbb{R}$; (ii) if $f$ is convex and $g$ is $\Lambda$-strongly convex, then $f+g$ is $\Lambda$-strongly convex.
Below we re-state Lemma~\ref{lem:Chaudhuri_lemma} as originally presented in \citet{chaudhuri2011} for the sake of self-containment; readers may refer to its proof in the original paper.
\begin{lem}
    \label{lem:Chaudhuri_lemma}
    \textnormal{\citep{chaudhuri2011}} Let $h_1: \mathbb{R}^p \rightarrow \mathbb{R}$ and $ h_2 : \mathbb{R}^p \rightarrow \mathbb{R}$ be everywhere first-order differentiable functions.
    Assume $h_1(\boldsymbol{\theta})$ and $h_1(\boldsymbol{\theta}) + h_2(\boldsymbol{\theta})$ are both $\Lambda$-strongly convex.
    If $\hat{\boldsymbol{\theta}}_1 = \argmin_{\boldsymbol{\theta}}h_1(\boldsymbol{\theta})$ and $\hat{\boldsymbol{\theta}}_2 = \argmin_{\boldsymbol{\theta}}\{h_1(\boldsymbol{\theta}) + h_2(\boldsymbol{\theta})\}$, then we have
    \begin{equation}
        \textstyle\norm{\hat{\boldsymbol{\theta}}_1 - \hat{\boldsymbol{\theta}}_2} \le \frac{1}{\Lambda}\max_{\boldsymbol{\theta}}\norm{\nabla h_2(\boldsymbol{\theta})}.
    \end{equation}
\end{lem}

Given Assumption~\ref{assump} and Lemma \ref{lem:Chaudhuri_lemma}, we are now equipped to derive the $\ell_2$-global sensitivity of $\hat{\boldsymbol{\theta}}$ in the wERM optimization and to establish the main result (Theorem \ref{thm:wERM_sens}).
Compared to the unweighted ERM case \citep{chaudhuri2011}, the establishment of DP guarantees for wERM is more complex as weight $w_i$ in Eqn \eqref{eqn:wl} affects the global sensitivity of the solution to the wERM problem, as presented in the proof of Theorem \ref{thm:wERM_sens}.
\begin{thm}[\textbf{main result}]\label{thm:wERM_sens}
Let $\mathcal{L}$ represent a wERM problem in Eqn \eqref{eqn:wERM} satisfying the regularity conditions A1-A5. Then\\
%Assume the weight $w_i$ in Eqn(4) does not depend on the information from individuals $i'\ne i$ for $i'=1,\ldots,n$ in dataset $(\mathbf{x, y, w})$.
1) the $\ell_2$ global sensitivity of estimated parameters $\hat{\boldsymbol{\theta}}$ in the predictor function $f_{\boldsymbol{\theta}}$ is $\Delta_{2, \boldsymbol{\theta}} = (C+2W)/\gamma$. $C\ge 0$ is a constant, the value of which depends how $\mathbf{w}=(w_1,\ldots,w_n)$ is obtained; \\
2) Algorithm~\ref{alg:DP-wERM} satisfies $\epsilon$-DP.
\end{thm}

\begin{proof}
Let $D$ and $\tilde{D}$ be neighboring datasets consisting of feature-label-weight triples. 
Without loss of generality, assume they differ on individual $n$.
That is, $\mathbf{d}_i = (\mathbf{x}_i, y_i) = \tilde{\mathbf{d}}_i= (\tilde{\mathbf{x}}_i, \tilde{y}_i)$ for $i =1,2,\ldots, n-1$, and $\mathbf{d}_n  \ne  \tilde{\mathbf{d}}_n$, where $\mathbf{d}_n= (\mathbf{x}_n, y_n)$ and $\tilde{\mathbf{d}}_n=(\tilde{\mathbf{x}}_n, \tilde{y}_n)$. Define
\begin{align}
    g_1(\boldsymbol{\theta}) & = \textstyle\frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \notag\\
    \tilde{g}_1(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^n \tilde{w}_i\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_i), \tilde{y}_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \notag\\
    g_2(\boldsymbol{\theta})
    &= \textstyle\tilde{g}_1(\boldsymbol{\theta}) - g_1(\boldsymbol{\theta}) \notag\\
    & = \textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i),y_i)+\frac{1}{n}\{\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\} \label{eq:g2}
    %\\& \textstyle \leq \frac{C}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)+\frac{1}{n}\left(\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right)
\end{align}
as $(\mathbf{x}_i, y_i) = (\tilde{\mathbf{x}}_i, \tilde{y}_i)$ for $i =1,2,\ldots, n-1$.
%The last equation holds given that the weight $w_i$ in Eqn \eqref{eqn:wl} does not depend on the information from individuals $i'\ne i$ in dataset $(\mathbf{x, y, w})$.
%The last equation holds since on a compact set $\norm{\mathbf{x}}\leq 1$ and $y\in\{-1,1\}$, we have $|\ell(\cdot)|\leq C$ for $\mathbf{x}$ and $y$.
By regularity condition A3, $\ell$ is convex and everywhere first-order differentiable, and by regularity condition A5, $R$ is 1-strongly convex and everywhere differentiable.
Since $w_i>0$ and $\tilde{w}_i>0$, $g_1$ and $\tilde{g}_1 = g_1 + g_2$ are both $\frac{\lambda}{n}$-strongly convex and everywhere differentiable.
By regularity conditions A2 and A4, we can re-write $\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$ as $\tilde{\ell}(y_i\mathbf{x}_i^\intercal\boldsymbol{\theta})$, which implies
\begin{align}
\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)+\textstyle\frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\}\notag \\
&=\textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_i \mathbf{x}_i^\intercal\boldsymbol{\theta})+\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\notag\\
&=\textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)y_i\mathbf{x}^\intercal_i\tilde{\ell}^\prime(y_i\mathbf{x}^\intercal_i\boldsymbol{\theta})+\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\notag\\
\|\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})\|&\leq \textstyle \|\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\|+
\|\frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\|\label{eq:delta_g_ineq1}\\
&\leq \textstyle \|\frac{1}{n-1}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\|+
\|\frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\|\notag\\
&\le\textstyle \|\frac{1}{n-1}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\|+
\frac{1}{n}\!\left(\|\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta})\|\!+\!
\|w_n y_n\mathbf{x}^\intercal_n\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})\|\right)  \notag\\\
&= \textstyle\|\frac{1}{n-1}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\|\notag\\
&\quad\textstyle+\frac{1}{n}\left(|\tilde{w}_n| |\tilde{y}_n| \norm{\tilde{\mathbf{x}}_n} |\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta})| + |w_n| |y_n| \norm{\mathbf{x}^\intercal_n} |\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})|\right).
\label{eq:delta_g_ineq2}
\end{align} 
The inequality in Eqn \eqref{eq:delta_g_ineq1}  holds because $|\tilde{\ell}^\prime(z)| \le 1$, $|y_i|\le 1$, and $\norm{\mathbf{x}_i}\le 1$ for all $i$. 
%&\textstyle \frac{1}{n}\sum_{i=1}^n(\tilde{w}_i-w_i)-\frac{1}{n}(\tilde{w}_n-w_n)+\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\notag\\
%=&\textstyle  \frac{1}{n}\sum_{i=1}^n\!\tilde{w}_i-\frac{1}{n}\sum_{i=1}^n\!w_i-\frac{1}{n}(\tilde{w}_n\!-\!w_n)\!+\!\frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta})\!-\! w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}.\!\! \label{eq:grad-g2}
To derive an upper bound for Eqn \eqref{eq:delta_g_ineq2}, we examine two scenarios below.

Case 1: When $\mathbf{w}$ is observed (or predicted from a model trained on data independent of $D$ and $\tilde{D}$), $w_i=\tilde{w}_i$  (or $\hat{w}_i=\hat{\tilde{w}}_i$ if predicted) for $i=1,\ldots,n-1$, implying that the first term in Eqn \eqref{eq:delta_g_ineq2} is 0.
%as long as the unknown parameters in $P(A_i|\mathbf{x}_i)$ are the same across all $i$'s, 
By assumptions A1 and A4, the second term in Eq \eqref{eq:delta_g_ineq2}  %we show below that the $\ell_2$-norm of the second term in Eqn (\ref{eq:delta_g_ineq2}) is of order $O(1/n)$. That is,
\begin{align}\label{eqn:bound1}
%le\frac{1}{n}\norm{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n\boldsymbol{\theta})}
%\!\!\!&\textstyle\frac{1}{n}\|\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta}) - w_n y_n\mathbf{x}_n\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})\|\!\le\!\frac{1}{n}\!\left(\|\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta})\|\!+\!\|w_n y_n\mathbf{x}^\intercal_n\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})\|\right)  \notag\\\
%\!\!\!&= \textstyle\frac{1}{n}\left(|\tilde{w}_n| |\tilde{y}_n| \norm{\tilde{\mathbf{x}}_n} |\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta})| + |w_n| |y_n| \norm{\mathbf{x}^\intercal_n} |\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})|\right)
\le \textstyle \frac{1}{n}\left(|\tilde{w}_n| + |w_n|\right)\le \frac{2W}{n}.
\end{align}

Case 2: When $\mathbf{w}$ is not observed but predicted from the sensitive dataset itself, then the predictions $\hat{w}_i$ and $\hat{\tilde{w}}_i$ would be different  for $i=1,\ldots,n$; Eqn \eqref{eq:delta_g_ineq2} becomes 
\begin{align}\textstyle  
\le\|\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)\|+\frac{2W}{n}.\label{eq:eq:delta_g_ineq3}
 \end{align}
A loose upper bound for Eqn \eqref{eq:delta_g_ineq2}  is \vspace{-5pt}
\begin{align}\textstyle W+\frac{2W}{n},\label{eq:bound2}
\end{align}
\vspace{-17pt} 

\noindent assuming each difference $|\hat{\tilde{w}}_i-\hat{w}_i|$ for $i=1,\ldots,n-1$ is upper bounded by $W$. This bound is vastly conservative and counter-intuitive as the influence of a single data point on the estimates of $\mathbf{w}$ would diminish as $n\to\infty$, but Eqn \eqref{eq:bound2} approaches a non-zero constant $W$. As $n$ increases, the influence of a single data point (observation $n$ in $D$ and $\tilde{D}$) on the prediction of $w_i=\tilde{w}_i$ for $i=1,\ldots,n-1$ will subside and it is expected that   $\hat{w}_i-\hat{\tilde{w}}_i\to 0$.

To tighten the bound, assume $\mathbf{w}$ is predicted by an M estimation approach that includes the most common estimation frameworks, such as maximum likelihood estimation, least squared estimation, robust estimators (e.g. Huber's estimator), and many common ERM problems. We leverage the established asymptotics of M estimators; that is, under certain regularity conditions \citep{huber1967behavior,  van2000asymptotic, stefanski2002calculus, ma2005robust, duchi2021statistics}, %(e.g., convexity, differentiability, and boundedness of the loss function,  Lipschitz continuous gradient, sub-Gaussian noise in observations, and differentiability of the function in the application of the delta method, iid) 
$\hat{w}_i-w_i=o_p(n^{-1/2})$ and $\hat{\tilde{w}}_i-\tilde{w}_i=o_p(n^{-1/2})$; furthermore, $\sqrt{n}\hat{w}_i\overset{d}{\sim}\mathcal{N}(w_i, \sigma^2_i)$ and $\sqrt{n}\hat{\tilde{w}}_i\overset{d}{\sim}\mathcal{N}(\tilde{w}_i, \sigma^2_i)$. For observations $i=1,\ldots,n-1$ that are shared between $D$ and $\tilde{D}$, $w_i=\tilde{w}_i$ and %$\hat{w}_i-w_i = O(n^{-1/2})$ and $\hat{\tilde{w}}_i-w_i = O(n^{-1/2})$, and thus 
\begin{equation} \label{eq:w0}
\sqrt{n}(\hat{w}_i-\hat{\tilde{w}}_i)\overset{d}{\sim} \mathcal{N}(0, \sigma^2_{d,i}) \mbox{ for } i =1,\ldots,n-1, 
\end{equation} 
where $\sigma^2_{d,i}/n$ denotes the variance of $\hat{w}_i-\hat{\tilde{w}}_i$. 

While $\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)$ is an average over $n-1$ data points, the regular CLT as $n\to\infty$ does not apply as $\{\hat{w}_i-\hat{\tilde{w}}_i\}_{i=1,\ldots,n-1}$ are not mutually independent. % and the correlation would depend on the model used, the correlations among the predictors, and existence of leverage points, etc. 
Let $rn$ ($r\in(0,1]$) be the ``effective'' sample size -- the number of ``independent'' data points after ``correcting'' for the correlations among $\{\hat{w}_i-\hat{\tilde{w}}_i\}_{i=1,\ldots,n-1}$ and define $\sigma_d\triangleq\sqrt{\sum_{i=1}^{n-1}\sigma^2_{d,i}/(n-1)}$, then 
\begin{equation} \label{eq:w}
\frac{\sqrt{rn(n-1)}}{\sigma_d}\left(\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)\right)\approx
\frac{n\sqrt{r}}{\sigma_d} \left(\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)\right)\overset{d}{\sim} \mathcal{N}(0, 1).
\end{equation} 
Leveraging the normality in Eqn \eqref{eq:w}, we can tighten the bound for $\|\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)\|$ by ignoring the probability mass in the distribution of $\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)$ beyond $k(\sqrt{r}n)^{-1}\sigma_d$ to upper bound $\|\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)\|$ at $k\sqrt{r}n^{-1}\sigma_d$), where $k$ is a constant (e.g. $\ge2.5$),  a commonly used technique for calculating global sensitives for statistics when it comes to normally distributed data \citep{liu2019statistical,liu2020model,bowen2020comparative}. Plugging  $k(\sqrt{r}n)^{-1}\sigma_d$ in Eqn \eqref{eq:delta_g_ineq2}, we have
\begin{equation}\label{eq:bound3}
\le \frac{kr^{-1/2}\sigma_d+2W}{n}.
\end{equation}


%First, per assumption A2 and the CLT, $\frac{1}{n}\sum_{i=1}^n\!\tilde{w}_i-\mu_w=O(n^{-1/2})$ and $\frac{1}{n}\sum_{i=1}^n w_i-\mu_w=O(n^{-1/2})$; second, per assumption A2, $\frac{1}{n}(\tilde{w}_n\!-\!w_n)\in\frac{1}{n}[-W,W]$; third,
%All taken together, $\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} \leq \frac{2W}{n}$  for large $n_0$.
%When $n\to \infty$, the propensity score difference due to one data perturbation is arguably ignorable. Thus, $\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} \leq \frac{2W}{n}$ when $n\to \infty$. 
Lastly, define $\boldsymbol{\hat{\theta}}_1 = \argmin_{\boldsymbol{\theta}} g_1(\boldsymbol{\theta})$ and $\boldsymbol{\hat{\theta}}_2 = \argmin_{\boldsymbol{\theta}}\{g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})\}$. All taken together with Lemma \ref{lem:Chaudhuri_lemma}, 
\begin{align}
\pushQED{\qed}
\Delta_{2, \boldsymbol{\theta}} &= \max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\boldsymbol{\hat{\theta}}_1 - \boldsymbol{\hat{\theta}}_2}_2=\textstyle \frac{n}{\gamma} \max\limits_{\boldsymbol{\theta}}\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} \leq  \left(\frac{n}{\gamma}\right) \left(\frac{C+2W}{n}\right) = \frac{C+2W}{\gamma}, \mbox{where }\label{eqn:GS}\\
& C= \begin{cases}
0 & \mbox{if $\mathbf{w}$ is observed or estimated from a model trained on data}\\
& \mbox{independent of the sensitive dataset}; \\
kr^{-1/2}\sigma_d  & \mbox{if $\mathbf{w}$ is estimated from the sensitive dataset for large $n$;}\\
nW & \mbox{a vastly conservative option.}
\end{cases}\label{eqn:C}
\end{align}
After $\Delta_{2, \boldsymbol{\theta}}$ is obtained, the proof on the satisfaction of $\epsilon$-DP in Algorithm \ref{alg:DP-wERM} is identical to the proof of Theorem 6 in \citet{chaudhuri2011}, by replacing the sensitivity there with $\Delta_{2, \boldsymbol{\theta}}$ in Eqn \eqref{eqn:GS}.
\end{proof}\vspace{-12pt}
\begin{remark}
A precise value of $\sigma_d$ may not be easily attainable. In that case, one may plug in an upper bound of $\sigma_d$. For example, since $w_i\in(0,W]$, so are $\hat{w}_i$ and $\hat{\tilde{w}}_i$, and thus $\hat{w}_i-\hat{\tilde{w}}_i\in[-W,W]$, when $n$ is is large, $\sigma_d\le 2W/2=W$ \citep{shiffler1980upper}; thus 
\begin{equation}\label{eqn:C1}
    C=kr^{-1/2}W.
\end{equation} 
If it is expected that the value of $\sigma_d$ would not drastically change from one dataset to another for the same outcome $Y$ measured on the same population, we may use other datasets to estimate $\sigma_d$. We leave the thorough investigation on the formulation of $\sigma_d$ to future work. 
%If an upper bound for $\sigma_d$ is not available, then the conservative  sensitivity  $\Delta_{2, \boldsymbol{\theta}}=(nW+2W)/\gamma$ with $C=nW$ (Eq \eqref{eqn:bound1})  can be used as the last resort. However, this large sensitivity can result in a large amount of DP noise being injected and low-utility-output from the DP-wERM.
\end{remark}
\begin{remark}
The bound in Eqn \eqref{eq:bound3} is based on the asymptotic properties of M-estimators and expected to perform well for large $n$ under the necessary regularity conditions. There is also literature that focuses on the finite sample behaviors of M estimators in different optimization problems and data settings (including data with outliers and heavy tails), under various regularity conditions. Theoretical results obtained in the finite-sample setting are typically framed in terms of an upper bound on the excess population or empirical risk or on the deviation of   parameter estimates from the optimal population estimates  (measured in some types of $l$-norms)  with a certain probability at a given lower bound on $n$ \citep{feng2014robust, zhou2018new,lecue2020robust, chinot2020robust, ostrovskii2021finite}. Not only is the upper bound formulation often problem-specific and rather complicated, but also the upper bounds, if calculated from the sensitive dataset itself, would risk  privacy leakage and require sanitization themselves, a separate and potentially challenging issue in its own right. Furthermore, the bounds are only met with a non-zero certain probability, which is a function of $n$ and other constants intrinsic to the the estimation problem itself. For these reasons, we would not deem  at this moment  that these finite-sample results are suitable theoretically or practically for bounding  $\frac{1}{n-1}\sum_{i=1}^{n-1}(\hat{\tilde{w}}_i-\hat{w}_i)$ (the first term in Eqn \eqref{eq:delta_g_ineq2}) to meet the $\epsilon$-DP requirement and will leave further exploration of this possibility to the future.
\end{remark}



\begin{comment}
%In summary, Theorem \ref{thm:wERM_sens} holds regardless of whether $w_i$ is independent of the sensitive dataset or not.If $w_i$ is data-dependent and the assumption in Eqn (\ref{eq:w}) is questionable, the values of $P(A_i|\mathbf{x}_i)$, whether known or estimated, of $k\ge0$ more individuals are different in their $w_i$ values in addition to the individual whose record is different between the two neighboring datasets, then $g_2(\boldsymbol{\theta})$ in Eqn \eqref{eq:g2} would be the summation of the $k\in[1,n]$ differences between $\tilde{g}(\boldsymbol{\theta})$ and $g(\boldsymbol{\theta})$ evaluated at those $k+1$ individuals, Eqn \eqref{eqn:bound1} would become $2W/n+2kW'/n$, where $W'\le W$, representing the upper global bound in the change of the weights of the $k$ individuals other than the individual whose record gets altered, and the sensitivity in Eqn \eqref{eqn:GS} would become $2W/\gamma+2kW'/\gamma$.

\begin{thm}[\textbf{main result}]\label{thm:wERM_sens}
Let $\mathcal{A}$ represent a wERM problem in Eqn \eqref{eqn:wERM} satisfying the regularity conditions A1-A5.
Assume the weight $w_i$ in Eqn \eqref{eqn:wl} does not depend on the information from individuals $i'\ne i$ for $i'=1,\ldots,n$ in dataset $(\mathbf{x, y, w})$.
The $\ell_2$-global sensitivity of the parameters in the predictor function $f_{\boldsymbol{\theta}}$ of the wERM problem on data $(\mathbf{x, y, w})$ is $\Delta_{2, \boldsymbol{\theta}} = \frac{2W}{\gamma}$, and Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP.
\end{thm}


\begin{proof}
Without loss of generality, assume they differ on individual $n_0$.
That is, $D_i = (\mathbf{x}_i, y_i, w_i) = (\tilde{\mathbf{x}}_i, \tilde{y}_i, \tilde{w}_i) = \tilde{D}_i$ for $i =1,2,\ldots, n-1$ and $D_n = (\mathbf{x}_n, y_n, w_n) \ne (\tilde{\mathbf{x}}_n, \tilde{y}_n, \tilde{w}_n) = \tilde{D}_n$.
Define
\begin{align}
    g_1(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \\
    \tilde{g}_1(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^n \tilde{w}_i\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_i), \tilde{y}_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \\
    g_2(\boldsymbol{\theta}) &= \textstyle\tilde{g}_1(\boldsymbol{\theta}) - g_1(\boldsymbol{\theta}) = \frac{1}{n}\left(\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right).\label{eq:g2}
\end{align}
The last equation holds given that the weight $w_i$ in Eqn \eqref{eqn:wl} does not depend on the information from individuals $i'\ne i$ in dataset $(\mathbf{x, y, w})$.
By regularity condition 3, $\ell$ is convex and everywhere first-order differentiable, and by regularity condition 5, $R$ is 1-strongly convex and everywhere differentiable.
Since $w_i>0$ and $\tilde{w}_i>0$, $g_1$ and $\tilde{g}_1 = g_1 + g_2$ are both $\frac{\lambda}{n}$-strongly convex and everywhere differentiable.
By regularity conditions 2 and 4, we can re-write $\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$ as $\tilde{\ell}(y_i\mathbf{x}_i\boldsymbol{\theta})$, which implies
\begin{equation}
    \begin{aligned}
        \nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\left(\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right) \\
        &=\textstyle \frac{1}{n}\left(\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n\boldsymbol{\theta})\right).
    \end{aligned}
\end{equation}

By regularity conditions 1 and 4, 
\begin{align}\label{eqn:bound1}
\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} &\textstyle\le \frac{1}{n}\norm{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n\boldsymbol{\theta})} \notag\\
&\textstyle= \frac{1}{n}\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n y_n\mathbf{x}_n\tilde{\ell}^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})}\notag\ \\
&\textstyle\le \frac{1}{n}\left(\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta})} + \norm{w_n y_n\mathbf{x}_n\tilde{\ell}^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})}\right)  \notag\\\
&\textstyle= \frac{1}{n}\left(|\tilde{w}_n| |\tilde{y}_n| \norm{\tilde{\mathbf{x}}_n} |\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta})| + |w_n| |y_n| \norm{\mathbf{x}_n} |\tilde{\ell}^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})|\right)\notag\ \\
&\le \frac{1}{n}\left(\tilde{w}_n + w_n\right)\le \frac{2W}{n}.
\end{align}

Finally, define $\boldsymbol{\hat{\theta}}_1 = \mathcal{A}(D) = \argmin_{\boldsymbol{\theta}} g_1(\boldsymbol{\theta})$ and $\boldsymbol{\hat{\theta}}_2 = \mathcal{A}(\tilde{D}) = \argmin_{\boldsymbol{\theta}} \tilde{g}_1(\boldsymbol{\theta}) = \argmin_{\boldsymbol{\theta}}g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})$.
By Lemma \ref{lem:Chaudhuri_lemma},
\begin{align}
\pushQED{\qed}
\Delta_{2, \boldsymbol{\theta}} &= \textstyle\max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\mathcal{A}(D) - \mathcal{A}(\tilde{D})}_2 = \max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\boldsymbol{\hat{\theta}}_1 - \boldsymbol{\hat{\theta}}_2}_2\notag \\
&=\textstyle \frac{n}{\gamma} \max\limits_{\boldsymbol{\theta}}\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} = \left(\frac{n}{\gamma}\right) \left(\frac{2W}{n}\right) = \frac{2W}{\gamma}.\label{eqn:GS}
\end{align}

With the global sensitivity, the proof on the satisfaction of $\epsilon$-DP in Algorithm \ref{alg:DP-wERM} DP is identical to the proof of Theorem 6 in \citet{chaudhuri2011}, by replacing the sensitivity from that work with the one in Eqn \eqref{eqn:GS}.
\end{proof}

\textbf{Remark}: The assumption that  weight $w_i=B_i/P(A_i|\mathbf{x}_i)$ in Eqn \eqref{eqn:wl} does not depend on the information of individuals $i'\ne i$ in dataset $(\mathbf{x, A, B})$ holds in general.
First, $B_i$ is always observed (treatment outcome) and does not depend on other individuals other than individual $i$ himself/herself.
As for $P(A_i|\mathbf{x}_i)$, it is the probability that an individual $i$ receives treatment $A_i$, aka his/her propensity score.
Supposedly, it is a value either determined by study design, such as in randomized clinical trials, where $P(A_i|\mathbf{x}_i)$ is the allocation ratio between two treatments (e.g. 0.5) or depends on the individual's own attributes $\mathbf{x}_i$ (e.g., prescription given by the person's doctor in EHR data).
In both the simulation studies and the case study in this work, we deal with data from randomized clinical trials, and $P(A_i|\mathbf{x}_i)$ is given per study design.
If $P(A_i|\mathbf{x}_i)$ is unknown (so is $w_i$) and needs to be estimated, the estimation takes place after the formulation of Eqn \eqref{eq:g2}.
Furthermore, the actual values of $P(A_i|\mathbf{x}_i)$ and $w_i$ for each individual $i$, whether known or unknown (but estimated), do not matter in the sensitivity calculation as long as the global bounds of $w_i$, which are independent of the dataset locally, are given.
If the true values of $P(A_i|\mathbf{x}_i)$, whether known or unknown, of $k\ge0$ additional individuals are affected other than the individual whose record gets altered in the dataset, then $g_2(\boldsymbol{\theta})$ in Eqn \eqref{eq:g2} would be the summation of the $k\in[1,n]$ differences between $\tilde{g}(\boldsymbol{\theta})$ and $g(\boldsymbol{\theta})$ evaluated at those $k+1$ individuals, Eqn \eqref{eqn:bound1} would become $2W/n+2kW'/n$, where $W'\le W$, representing the upper global bound in the change of the weights of the $k$ individuals other than the individual whose record gets altered, and the sensitivity in Eqn \eqref{eqn:GS} would become $2W/\gamma+2kW'/\gamma$.

The estimation of $P(A_i|\mathbf{x}_i)$ can be estimated from a different dataset other than the dataset $(\mathbf{x, A, B})$ to which OWL is applied, then $k=1$. Lastly, if $P(A_i|\mathbf{x}_i)$ is estimated using the dataset $(\mathbf{x, A, B})$, such as through fitting a logistic regression, then its value would be affected even if the information of one individual in the dataset is altered, then $k=n$. Or one may choose to use a subset of $(\mathbf{x, A, B})$  to estimate $P(A_i|\mathbf{x}_i)$ instead of the whole dataset then k would take a value between $\in(0,n)$. On the other hand, one may argument
\end{comment}

%----------------------------------------------
\section{Differentially Private Outcome Weighted Learning (DP-OWL)} \label{subsec:DP-OWL}

As OWL is a special case of wERM,  Algorithm \ref{alg:DP-wERM} is applicable to train a privacy-preserving OWL model that satisfies $\epsilon$-DP, so are the remarks on variable selection, dimensionality reduction, and hyperparameter tuning.  The algorithm generates privacy-preserving outputs from OWL by operating on the primal weighted SVM problem with a linear kernel. For SVMs with a nonlinear kernel, it can approximated with a linear kernel.
For example, \citet{chaudhuri2011} approximate nonlinear kernels using random projections \citep{Rahimi2007,Rahimi2008}, which is the approach mentioned at the beginning of Section \ref{sec:DP_guarantees} and adopted by Algorithm \ref{alg:DP-wERM}. 
This contrasts with the typical method of solving the problem by optimizing the dual problem using the popular ``kernel trick'' to efficiently compute the solution for a nonlinear basis (e.g., Gaussian or radial kernels and polynomial kernels).
This is because the dual problem poses special problems for DP as releasing the learned SVM would also release the information of the predictions of individuals in the training data\footnote{Approxmating nonlinear kernals with linear ones is one approach to solve SVMs with DP guarantees. Other approaches exist; for example, \citet{Rubenstein2012} solve the SVM dual problem directly with DP, bypassing the privacy concerns of the kernel trick.}.

Since the hinge loss function in Eqn \eqref{eqn:OWL} is not smooth, we approximate the hinge loss in OWL with the Huber loss \citep{Chapelle2007} so that the regularity conditions A3 and A4 in Assumption \ref{assump} for DP-wERM are satisfied. 
\begin{defn}[Huber loss]
    \textnormal{\citep{Chapelle2007}} Given $h>0$,  the \textnormal{Huber loss} $\ell_{\textnormal{Huber}}: \mathbb{R} \rightarrow \mathbb{R}$ is defined as
    \begin{equation}\label{eqn:huber}
        \ell_{\textnormal{Huber}}(z) = \begin{cases} 
        0, & \textnormal{if } z > 1+h \\
        \frac{1}{4h}(1+h-z)^2, & \textnormal{if } |1 - z| \le h \\
        1 - z, & \textnormal{if } z < 1 - h \\
        \end{cases},
    \end{equation}
\end{defn}
In the context of wERM, $z$ in Eqn \eqref{eqn:huber} is replaced by $y\hat{f}$.
It is straightforward to show that the Huber loss is convex and everywhere first-order differentiable.
Additionally, the absolute value of the derivative $|\ell^\prime_{\textnormal{Huber}}(z)|$ is maximized at 1. %; that is, $|\ell^\prime_{\textnormal{Huber}}(z)| \le 1$ for all $z$. 
In the experiments in Section \ref{sec:sim_studies} and \ref{sec:case_study}, we set the Huber loss parameter $h=0.5$, which is considered a ``typical value'' \citep{Chapelle2007}.

Corollary \ref{cor:DP-OWL} states Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP when it is run in the case of OWL with the Huber loss.
The corollary is a direct extension from Theorem \ref{thm:wERM_sens} as all regularity conditions in Assumption \ref{assump} are satisfied. 
\begin{cor}[\textbf{DP guarantees of OWL via the DP-wERM algorithm}]\label{cor:DP-OWL}
Consider the approximate OWL problem
\begin{equation}
    \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}} \frac{1}{n} \sum_{i=1}^n\frac{B_i}{P(A_i|\mathbf{x}_i)} \ell_{\textnormal{Huber}}(A_if_{\boldsymbol{\theta}}(\mathbf{x}_i)) + \frac{\gamma}{n}\norm{\boldsymbol{\theta}},
\end{equation}
where $\norm{\mathbf{x}_i}\!\le\!1$, $\mathcal{F}\!= \!\{f_{\boldsymbol{\theta}}:f_{\boldsymbol{\theta}}(\mathbf{x}) \!=\!\mathbf{x}^\intercal\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$, and  $\frac{B_i}{P(A_i|\mathbf{x}_i)}=w_i\in(0,W]$.
Then the $\ell_2$ global sensitivity of estimate $\hat{\boldsymbol{\theta}}$ is $(C+2W)/\gamma$, where $C\ge 0$ is a constant, the value of which depends how $\mathbf{w}=(w_1,\ldots,w_n)$ is obtained; and Algorithm \ref{alg:DP-wERM} used for solving the OWL problem satisfies $\epsilon$-DP.
\end{cor} 
Similar to the cases listed in the proof of Theorem \ref{thm:wERM_sens}, the constant $C$ depends on how $w_i$ is obtained, or how $P(A_i|\mathbf{x}_i)$ is obtained since $B_i$ is observed in the OWL setting.
First, $P(A_i|\mathbf{x}_i)$ can be a known constant.
For example, when the dataset comes from a randomized clinical trial, $P(A_i|\mathbf{x}_i)$ depends on the allocation ratio between two treatments and is a known constant per the study design.
Second, $P(A_i|\mathbf{x}_i)$ is unknown but can be predicted from a model trained on a different dataset $D'$ than the sensitive dataset $D$ on which the wERM problem is based.
Suppose $D'$ contains the same set of covariates $\mathbf{X}$ and treatment variable $A$ as $D$, then one can train a classifier (e.g., logistic regression) on $D'$ to predict $\Pr(A_i)$ given $\mathbf{x}_i$ for $i=1,\ldots,n-1$ in $D$.
In these two cases, $C=0$ (Eqn \eqref{eqn:C}).
Finally, when $P(A_i|\mathbf{x}_i)$ is unknown and is predicted using the same sensitive dataset on which the wERM problem is based, then $C$ may be set at $kr^{-1/2}\sigma_d$ (or $kr^{-1/2}W$ if the upper bound $W$ is plugged in for $\sigma_d$) or $nW$ as in Eqn \eqref{eqn:C}.  %there can be other values of $C$, specifically in the OWL problem, depending on how $P(A_i|\mathbf{x}_i)$ is estimated.For example, even the one data point that differ between the two datasets $D$ and $\tilde{D}$ are more extreme values relative to the rest of the data points, robust logistic regression \cite{feng2014robust} can be applied, where $l_2$ error of the regression efficient estimates of dimension $p$ is upper bounded by $O(a + (a+\sqrt{a})\sqrt{p/n}+a\sqrt{\log(p)/n+\log(n)/n})$ with probability , where $a=1/n$ represent a single extreme value among $n$ data point,

\begin{remark}
Both the simulation studies and the case study in this work use data from a randomized clinical trial with a 1:1 allocation ratio and thus $P(A_i|\mathbf{x}_i)$ is 0.5; that is, $P(w|A,X)$ is a pre-specified known value, and thus $C=0$ and $\Delta_{2, \boldsymbol{\theta}}=2W/\gamma$.
\end{remark}

%In summary, Theorem \ref{thm:wERM_sens} holds regardless of whether $w_i$ is independent of the sensitive dataset or not.If $w_i$ is data-dependent and the assumption in Eqn (\ref{eq:w}) is questionable, the values of $P(A_i|\mathbf{x}_i)$, whether known or estimated, of $k\ge0$ more individuals are different in their $w_i$ values in addition to the individual whose record is different between the two neighboring datasets, then $g_2(\boldsymbol{\theta})$ in Eqn \eqref{eq:g2} would be the summation of the $k\in[1,n]$ differences between $\tilde{g}(\boldsymbol{\theta})$ and $g(\boldsymbol{\theta})$ evaluated at those $k+1$ individuals, Eqn \eqref{eqn:bound1} would become $2W/n+2kW'/n$, where $W'\le W$, representing the upper global bound in the change of the weights of the $k$ individuals other than the individual whose record gets altered, and the sensitivity in Eqn \eqref{eqn:GS} would become $2W/\gamma+2kW'/\gamma$.


%-------------------------------------------------------------------
\section{Simulation Study}\label{sec:sim_studies}
We conducted a simulation study to examine the prediction performance of the DP-OWL algorithm with privacy budgets $\epsilon\in\{0.1, 0.5, 1, 2, 5, 20, 50, 150, 300, 500, 800, 1000\}$ and training dataset size $n\in\{200, 500, 800, 1000, 1500, 2000, 2500\}$.
The larger values of $\epsilon$ are simulated to gain an understanding of asymptotic performance of the DP-wERM algorithm, though we recognize that they are not practical scenarios.
As a baseline for comparison, we also run OWL without DP, which can be thought of as having a privacy budget of $\epsilon=\infty$.
The prediction accuracy is measured by both the correct underlying optimal treatment assignment and the empirical treatment value that estimates the expected clinical benefit from the treatment assignment.
We first introduce the simulation setting and then discuss the pre-processing step of variable selection and privacy-preserving hyperparameter tuning.


%-----------------------------------------------------
\subsection{Simulation Setting}
\label{subsec:sim_settings}

Our simulation setting replicates a typical phase II clinical trial used to estimate treatment effects.
The participants are randomly assigned to one of the possible treatments and an outcome for each participant is measured, along with
baseline characteristics or predictors $\mathbf{X}$.
%Medical studies that involve human subjects often collect sensitive individual information. As a result, legal and ethical concerns regarding individual privacy often restrict access to the data. These simulations seek to demonstrate how DP can be harnessed to provide an extra layer of privacy for this sensitive data.
%The following settings define the distributions from which the simulation values are drawn.
We draw $\mathbf{x}_i = (x_{i,1}, x_{i,2}, \ldots, x_{i,10})$ from a uniform distribution (i.e., $x_{i,j} \sim U[0, 1]$ independently for all $i=1,\ldots,n$ and $j=1,\ldots,10$). 
We examine two treatments $A\in\{-1,1\}$ and each individual is randomly assigned with a probability of 0.5 to either treatment.
Therefore, the propensity $P(A_i=1|\mathbf{x}_i) = P(A_i=-1|\mathbf{x}_i) = 0.5$.
We assume the underlying optimal treatment is the sign of the function $f(\mathbf{x}_i) = 1 + x_{i, 1} + x_{i, 2} - 1.8x_{i, 3} - 2.2x_{i, 4}$.
%Note $\E[f(\mathbf{x}_i)] = 0$.
Treatment benefit (the outcome) $B_i$ for  $i=1,\ldots,n$ is drawn independently from  $\mathcal{N}(\mu_i, \sigma^2)$ with $\mu_i = 0.01 + 0.02x_{i, 4} + 3A_if(\mathbf{x}_i)$ and $\sigma=0.5$.
If a simulated $B_i<0$, then it is shifted to be $B_i + |\min\{B_i\}| + 0.001\;\forall$ to ensure that $w_i = \frac{B_i}{P(A_i|\mathbf{x}_i)}> 0$ as required. 
Based on this definition, an individual who receives the optimal treatment per the randomization is likely to receive a larger treatment benefit, while those assigned to the non-optimal treatment  are likely to receive a smaller benefit.

We evaluate privacy-preserving OWL models trained via DP-wERM in two ways.
First, since the true optimal treatment is known given this is a simulation study, we can evaluate the accuracy of the optimal treatment assignment via the learned DP-OWL models.
Second, we calculate the empirical treatment value, an estimate of the expected clinical benefit $E[\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A=T(\mathbf{x}))]$, which is defined as
\begin{equation}\label{eqn:empirical_treatment_value}
    V(\hat{f}) = \textstyle\left(\sum_{i=1}^n\frac{\mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))}{P(A_i|\mathbf{x}_i)}B_i\right)\left(\sum_{i=1}^n \frac{\mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))}{P(A_i|\mathbf{x}_i)}\right)^{-1},
\end{equation}
where $\hat{f}$ is the estimated  privacy-preserving predictor function via Algorithm \ref{alg:DP-wERM} and $\mathbbm{1}$ is the indicator function.
Since $P(A_i|\mathbf{x}_i)=0.5$ for all $i=1,\ldots,n$ in this simulation, $V(\hat{f})= \left(\sum_{i=1}^n\mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))B_i\right)\left(\sum_{i=1}^n \mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))\right)^{-1}$.


%-------------------------------------------
\subsection{Variable Selection, Data Pre-processing, and Hyperparameter Tuning}
\label{subsec:feature_selection}
To reduce the number of predictors that go into the DP-wERM optimization, we perform variable selection, assuming that there exists a publicly available dataset we can use for that purpose.
This public dataset contains $(\mathbf{x}_i, A_i, B_i)$ for $i=1,\ldots,1000$, simulated in the same way as outlined in Section \ref{subsec:sim_settings}.
We run 10-fold cross-validation to fit a LASSO-regularized weighted logistic regression model for variable selection, where $A_i$ is the outcome and the weights are $w_i = \frac{B_i}{P(A_i|\mathbf{x}_i)}$.
The results suggest that the first four predictors $X_1$ to $X_4$ are relevant, matching our expectations based on the assumed underlying optimal treatment function from the simulation settings.
%The set $\{X_1, X_2, X_3, X_4\}$ is used as the input to the subsequent OWL model.
To meet the regularity condition $\norm{\mathbf{x}_i}\le1$  for DP-wERM, we preprocess each observation $\mathbf{x}_i$ for $i=1,\ldots,n$ by dividing it by $\sqrt{5}$ (4 selected features and an additional intercept or bias term) to obtain $\tilde{\mathbf{x}}_i = \mathbf{x}_i/\sqrt{5}$, which is used in the DP-OWL algorithm.
%The implicit assumption here is that it is public knowledge each $x_{i,j}$ is bounded between $[0,1]$ and the number of features including the implicit bias term is $\le5$.
%Since this is done without reference to the sensitive dataset itself, we can scale the feature vectors as required without utilizing a portion of the privacy budget.

The (global) upper bound $W$ for the weights $w_i=\frac{B_i}{P(A_i|\mathbf{x}_i)}$ is also a hyperparameter that needs to be specified.
A larger $W$ corresponds to a larger scale for the additive noise in Algorithm \ref{alg:DP-wERM}, so it is ideal to select $W$ to be as close as possible to $\max_i\{w_i\}$ from the sensitive dataset, but using it directly would incur privacy loss unless it is sanitized with a portion of the privacy budget (e.g. via the PrivateQuantile procedure \citep{smith2011privacy}). Since $\Pr(A_i|\mathbf{x}_i)\equiv0.5$ in this simulation study, we only need to bound $B_i$ which may have a natural bound or we can clip it at a prespecified constant $C_B$. We set $C_B=15$ and $B$ values $>15$ are clipped to 15, leading to $W=15/0.5=30$.

To tune the regularization constant $\gamma$, we use Algorithm \ref{alg:reg_const_selection} on the same public dataset as used for variable selection, at each combination of privacy budget $\epsilon$ and sample size $n$ used. We set $m=500$, $n_0=1000, r=1000$, and the empirical treatment value from Eqn \eqref{eqn:empirical_treatment_value} as our evaluation metric $v$ in Algorithm \ref{alg:reg_const_selection}.
%$X_1$ to $X_4$ from the variable selection procedure are used in training the models in Algorithm \ref{alg:reg_const_selection}.
The independent dataset we use for regularization constant tuning is similar to the sensitive dataset as they are simulated from the underlying population.
In practice, the independent datasets may differ from the sensitive dataset in various aspects.
We run a sensitivity study in Section \ref{sec:sensitivity} to examine the robustness of Algorithm 2 for hyperparameter tuning.

%-----------------------------------
\subsection{Results}\label{subec:results}
We ran the DP-wERM algorithm on 200 simulated datasets for each $n$ and $\epsilon$ combination and obtained the average (95\% confidence intervals) optimal assignment accuracy rates and average empirical treatment values on a test dataset of size $5,000$ in each repeat. 


The results are presented in Figure \ref{fig:sim_results}. %(more detailed information is presented in a table in the supplementary materials).  
As expected, the accuracy rate and empirical value improve as $n$ or $\epsilon$ increases, approaching the performance for the non-private case, where the accuracy rate is 90\% to 95\%, depending on $n$, and the empirical value is around 10.5.
The DP-wERM algorithm can achieve an 80\% optimal treatment assignment accuracy rate or larger for $\epsilon\ge2$ when  $n\ge1000$ or  $\epsilon\ge1$ when $n\ge2000$.
If $\epsilon$ increases to 5, 80\% or larger accuracy rates can be obtained for $n$ as small as 500.
%As $n$ or $\epsilon$ gets smaller, the assignment accuracy rate goes down to 50-60\%.
Similar trends are observed for the empirical value prediction.
At $\epsilon = 5$, the predicted values range 10 to 10.5 for $n\ge500$, around $10$ for $n\ge800$ at $\epsilon=2$ and decrease to the range of 8 to 9 for smaller $n$ or $\epsilon$.
%For sample sizes $n>500$, we begin to see performance near non-DP levels in our simulation studies around $\epsilon=50$, while for sample sizes smaller than this, it may take until $\epsilon=100$ or more to see performance near non-DP levels.

% Figure environment removed

We also made several modifications to the DP-OWL algorithm in an attempt to improve its performance at smaller $n$ or $\epsilon$.
These attempts aimed to reduce the noise scale by scaling or discretizing $B$ values, increasing $\gamma$t, and applying privacy amplification via subsampling \citep{balle2018privacy}.
None of these attempts resulted in improved performance. 

As mentioned in Sec \ref{subsec:related_work}, \citet{Spicker2024} developed a DP-OWL algorithm based in the SVM framework, independently and concurrently to our work.  While our simulation studies were constructed differently from those in \citet{Spicker2024}, it is interesting that the accuracy results presented above that were obtained via our more general DP-wERM procedure are comparable for many combinations of $n$ and $\epsilon$. It would be interesting to compare our works side-by-side in the future.   


\vspace{-3pt}\subsection{Sensitivity Analysis}\label{sec:sensitivity}\vspace{-3pt}
In the simulation study presented above, Algorithm \ref{alg:reg_const_selection} was used to obtain an optimal $\gamma$ on an independent public dataset similar to the sensitive dataset.
Since access to such a dataset is unlikely in practice, we conducted additional experiments to assess the sensitivity of hyperparameter selection when the independent dataset differs from the sensitive one in sample size, optimal treatment function coefficients, feature distribution, and number of predictors. 

Due to space considerations, the detailed results are presented in the Supplementary Materials, but the main findings are summarized as follows.
The results in each case showed that for each combination of $n$ and $\epsilon$, the optimal regularization constant $\gamma$ that would be chosen based on these ``imperfect'' independent datasets is consistently similar to the optimal value for the sensitive dataset.
In other words, even if the independent dataset differs from the sensitive dataset in some key aspects, the independent dataset can often still be used in Algorithm \ref{alg:reg_const_selection} as a good proxy for selecting the regularization constant.

%--------------------------------------
\section{Case Study}
\label{sec:case_study}\vspace{-3pt}
We applied the proposed DP-OWL algorithm to a real randomized clinical trial that was conducted to study the effectiveness of melatonin on insomnia and neurocognitive impairment in childhood cancer survivors \citep{lubas2022randomized}. 
The main objective of this analysis is to derive an ITR that guides childhood cancer survivors to take melatonin to maximize its effect on insomnia or neurocognitive impairment while protecting the sensitive personal data used to derive the ITR, including benefit scores, demographic information, and medical histories.

%Long-term survivors of childhood cancer are at risk of treatment-related neurocognitive and sleep problems \citep{spiegler2004change}. 
%Although these late effects may pose a significant impact on the psychological and social functioning of these childhood cancer survivors, there are few effective interventions for these problems. 
Melatonin is an endogenously secreted hormone that has a sleep-promoting effect and has been associated with enhanced cognitive performance \citep{zhdanova1997melatonin, cardinali2012therapeutic, furio2007possible}. A clinical trial was conducted at St. Jude Children's Research Hospital to study the effects of exogenous melatonin on alleviating problems related to insomnia and/or neurocognitive impairment in adult survivors of childhood cancer \citep{lubas2022randomized}. 
Survivors were recruited from the St. Jude Lifetime Cohort Study (SJLIFE) \citep{howell2021cohort}. 
Based on the baseline clinical evaluation of neurocognitive function and sleep status, participants were classified into three strata: 1) neurocognitive impairment without insomnia; 2) both neurocognitive impairment and insomnia; and 3) insomnia without neurocognitive impairment.  In this work, we focus on the effect of melatonin on improving the neurocognitive performance of the participants and thus consider the  participants in the first two strata only for ITR derivation. 

At the beginning of the trial, participants were randomized to receive 3 mg of time-released melatonin or placebo. 
Individual baseline  characteristics used in our ITR analysis include age at diagnosis (in years), age at enrollment (in years), sex (female or male), race (White or other), and diagnosis (leukemia or non-leukemia). 
%To be clear, these parameters were not set for intervention study enrollment.
Neurocognitive evaluations of the participants were completed at baseline and at month 6 post intervention. 
Previous results from this trial suggested that a larger proportion of the survivors with neurocognitive impairment on melatonin showed a treatment response for nonverbal reasoning compared to placebo. 
Therefore, we took the assessment of nonverbal reasoning as our endpoint of interest and the benefit value $B$ was defined as the difference in the scores if the endpoint taken at month 6 and baseline. 

A total of 246 participants were included in our analysis, with 120 on melatonin and 126 on placebo.  The participants were all adult survivors of childhood cancer. We set  the lower and upper bounds of enrollment age to 18 and 60 years, respectively.
Patients admitted to St. Jude are pediatric oncology patients, typically diagnosed at ages younger than 20 years, in general.
Thus, we set the lower and upper bounds of age at diagnosis at 0 and 20, respectively.
The other three predictors, sex, race, and diagnosis, were binary and coded as 0 or 1.
$B_i$ was converted to age-adjusted Z-scores, which followed a $N(0,1)$ distribution.
The shifted benefit $B_i'$ for each survivor $i$ as in $B'_i=B_i + |\min\{B_i\}| + 0.001$ was used as the final benefit measure. When applying the proposed DP-wERM algorithm to derive a privacy-preserving ITR via OWL, we set the upper bound for the weight $w_i=\frac{B'_i}{P(A_i|\mathbf{x}_i)}$ at $W=4/0.5=8$, where $4$ represents a $B'$ value $2$ standard deviation from its mean and a reasonable upper bound for $B'$ and the propensity score $P(A_i|\mathbf{x}_i)=0.5$. The sensitivity of the OWL parameter estimare per Corollary \ref{cor:DP-OWL} is $2W/\gamma= 16/\gamma$. 
We examined several privacy budgets $\epsilon=\{0.1, 0.5, 1, 2, 5\}$ and varied the regularization constant $\gamma=\{30, 50, 90, 150\}$ to assess the sensitivity of the derived ITR with DP guarantees and the robustness of the hyperparameter turning in deriving ITR. This tuning parameter range was selected based on the simulation results in Section \ref{sec:sim_studies}, which suggest that the optimal choice of tuning parameters depends on the sample size $n$. Since the case study sample size $n=246$ is comparable to the $n=200$ scenario in the simulation study, we considered this range reasonable. %We chose to examine multiple tuning parameters rather than a single one suggested by the simulation for $n=200$ to evaluate .
We run 200 replicates at each combination of $\epsilon$ and $\gamma$ to examine the stability of the DP-wERM algorithm.

Table~\ref{tab:etv} presents the means and SDs of the privacy-preserving empirical treatment values for the benefit of interest in each $(\epsilon, \gamma)$ scenario.
The ITR derived by the non-private OWL yields an empirical value of 2.101 (SD = 0.104), while the ITRs estimated by DP-wERM generate similar empirical values, all around 2.09 to 2.10, suggesting that the utility of the privacy-preserving ITRs is well preserved in terms of the estimated clinical benefit.
In addition to the empirical value, we also summarize the number of participants allocated to melatonin or placebo based on the DP ITRs.  Table~\ref{tab:etv} presents the proportion of concordance pairs out of 246 participates in the melatonin:placebo allocation based on the original and the DP ITRs as well as the p-values from the
chi-squared association tests. The concordance percentage is about 80\% to 95\% for $\epsilon\ge0.5$ and the $p$-values from the tests are all very close to 0, indicating that the privacy-preserving ITRs gave similar patient allocations as to those obtained by OWL without DP. The results in Table~\ref{tab:etv} also suggest that privacy-preserving results are very consistent across different $\gamma$ values at any given $\epsilon$. 
%%and calculated the probability of assigning a certain participant to melatonin over the 200 replicates and allocated that participant to melatonin if this probability exceeded 50\%. 

\begin{table}[!htb]
\centering\vspace{-6pt}
\caption{Mean and standard deviation (SD) privacy-preserving empirical treatment value  for the outcome, optimal treatment allocation ratio, and the percentage of concordance pairs obtained by DP-wERM  over 200 repeats in the melatonin trial ($n=246$). \vspace{-6pt}\label{tab:etv}}
\resizebox{0.825\textwidth}{!}{
\begin{tabular}{ccccccc}
\hline
$\epsilon$ & $\gamma$ & Mean  & SD & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Allocation\\ (melatonin:placebo)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Concordance \\Pairs Proportion\end{tabular}} & $p$-value$^\dagger$  \\ \hline
0.1 & 30    & 2.097 & 0.094 & 164:82 & 0.752 & $7.65\times 10^{-10}$ \\
0.1 & 50    & 2.093 & 0.091 & 156:90 & 0.703 & $1.37\times 10^{-6}$ \\
0.1 & 90    & 2.093 & 0.091 & 156:90 & 0.703 & $1.37\times 10^{-6}$ \\
%0.1 & 100   & 2.093 & 0.091 & 156:90 & 0.703 & $1.37\times 10^{-6}$ \\
0.1 & 150   & 2.093 & 0.091 & 156:90 & 0.703 & $1.37\times 10^{-6}$ \\
\hline
0.5 & 30    & 2.088 & 0.094 & 174:72 & 0.809 & $7.58\times10^{-15}$ \\
0.5 & 50    & 2.094 & 0.101 & 172:74 & 0.793 & $4.96\times10^{-13}$ \\
0.5 & 90    & 2.094 & 0.101 & 172:74 & 0.793 & $4.96\times10^{-13}$ \\
%0.5 & 100   & 2.094 & 0.101 & 172:74 & 0.793 & $4.96\times10^{-13}$ \\
0.5 & 150   & 2.094 & 0.101 & 172:74 & 0.793 & $4.96\times10^{-13}$ \\
\hline
1   & 30    & 2.086 & 0.115 & 182:64 & 0.850 & $<2.2\times10^{-16}$ \\
1   & 50    & 2.095 & 0.111 & 187:59 & 0.886 & $<2.2\times10^{-16}$ \\
1   & 90    & 2.095 & 0.110 & 187:59 & 0.886 & $<2.2\times10^{-16}$ \\
%1   & 100   & 2.096 & 0.110 & 187:59 & 0.886 & $<2.2\times10^{-16}$ \\
1   & 150   & 2.095 & 0.110 & 187:59 & 0.886 & $<2.2\times10^{-16}$ \\
\hline
2   & 30    & 2.098 & 0.111 & 188:58 & 0.890 & $<2.2\times10^{-16}$ \\
2   & 50    & 2.098 & 0.110 & 188:58 & 0.890 & $<2.2\times10^{-16}$ \\
2   & 90    & 2.098 & 0.110 & 188:58 & 0.890 & $<2.2\times10^{-16}$ \\
%2   & 100   & 2.098 & 0.110 & 188:58 & 0.890 & $<2.2\times10^{-16}$ \\
2   & 150   & 2.098 & 0.110 & 188:58 & 0.890 & $<2.2\times10^{-16}$ \\
\hline
5   & 30    & 2.099 & 0.103 & 194:52 & 0.947 & $<2.2\times10^{-16}$ \\
5   & 50    & 2.099 & 0.103 & 194:52 & 0.947 & $<2.2\times10^{-16}$ \\
5   & 90    & 2.099 & 0.103 & 194:52 & 0.947 & $<2.2\times10^{-16}$ \\
%5   & 100   & 2.099 & 0.103 & 194:52 & 0.947 & $<2.2\times10^{-16}$ \\
5   & 150   & 2.099 & 0.103 & 194:52 & 0.947 & $<2.2\times10^{-16}$ \\
\hline
\multicolumn{7}{l}{\small{The mean of the original empirical treatment value is 2.101 (SD = 0.104) with  a}}\\  
\multicolumn{7}{l}{\small{melatonin:placebo  allocation ratio of 197:49 for all 5 examined $\gamma$ values. Given the study}}\\ 
\multicolumn{7}{l}{\small{results were already published \citep{lubas2022randomized}}, the original results are provided as a}\\   
\multicolumn{7}{l}{\small{utility benchmark for the DP results.}}\\   
\multicolumn{7}{l}{$^\dagger$ \small{from the chi-squared independence test of the 2 allocation ratios obtained by OWL}}\\
\multicolumn{7}{l}{\small{with vs without DP; p-value $\le0.05$ indicates high concordance between the privacy-}} \\
\multicolumn{7}{l}{\small{preserving and nonprivate ITRs.}}\\
\hline
\end{tabular}}\vspace{-6pt}
\end{table}

Compared to the results at $n=200\sim500$ and $\epsilon\le5$ in the simulation study, the privacy-preserving results in this case study ($n=246)$ are more closely aligned with the corresponding original results.
One possible explanation is that the simulation study was designed to mimic real-world clinical trial scenarios with two competing treatments, where different individuals can benefit from one or the other.
However, in the case study, placebo is expected to offer minimal benefit (with an expected $B$ close to 0), while melatonin is likely to be the optimal treatment for most subjects.
If the derived ITR is correct, it would predominantly assign melatonin to each subject, which is indeed the case as reflected by a melatonin:placebo allocation ratio of 197:49 and a mean empirical treatment value of 2.101 (close to the mean of $B$) in the original results.
In other words, the case study presents a simpler ITR problem compared to the simulation study, with a stronger signal for the optimal treatment that is less likely to be obscured by the noise introduced by the DP mechanism.
Nevertheless, the overall privacy-utility trend still holds -- smaller $\epsilon$ results in greater deviations from the original results.

%--------------------------------------
\section{Discussion}
\label{sec:discussion}
We have presented and proved a general algorithm for achieving $\epsilon$-DP for wERM.
This work extends a previous work by \citet{chaudhuri2011} to allow for weighted loss functions. %, where the weights depends on the sensitive dataset but may or may not be related to the outcome.
We applied the general DP-wERM algorithm to OWL, which is often used in medical studies.
The results from our simulation and a real randomized clinical trial demonstrate that with a reasonably small privacy budget and a reasonably large sample size, satisfactory optimal treatment prediction and empirical treatment value can be achieved while simultaneously providing privacy guarantees for individuals in the study dataset.
Though strong privacy protection with DP is generally thought to be achieved with $\epsilon\le 1$ in the literature, larger values of $\epsilon$ in practice are often adopted in real life.\footnote{The US Census Bureau used $\epsilon=19.61$ to release statistical information from the 2020 census \citep{USCensus}; Apple self-reports $\epsilon$ values between $2$ (for the Health app) and $8$ (for determining if videos should autoplay with/without sound in Safari) \citep{Apple2017}; Google's next word prediction models guarantee $(\epsilon, 10^{-10})$-DP with $\epsilon$ as large as 13.69 \citep{Xu2024}.
These and several other real-world DP applications are listed by \citet{Desfontain2024}.}

As seen in Figure \ref{fig:sim_results}, the DP-OWL algorithm struggles with small sample sizes or privacy budgets.
An intuitive explanation follows.
DP provides privacy protection by ensuring the outputs of a DP mechanism are similar between two neighboring datasets, meaning that a single individual cannot influence the output ``too much''.
OWL, in contrast, aims to fit a model for individualized treatment assignment by weighting the input by how much it should influence the fitted model output.
Thus, in some sense, DP and OWL have competing goals that must be balanced to adequately satisfy the demands of each.
%\citet{chaudhuri2011} presents two algorithms for achieving DP for vanilla, unweighted ERM problems.One is the perturbation of the output coefficients from the fitted model given the data and an assumed model, which is the method utilized in this paper.The other method is to perturb the objective function itself with DP guarantees, the output from which is also privacy-preserving per the immunity to post-processing property of DP.
Nevertheless, the problem warrants further exploration to improve performance.
\citet{chaudhuri2011} showed that the objective function perturbation method for unweighted ERM problems generally provided higher utility than the output perturbation method at the same privacy budget.
Thus, it may be worth exploring how to adapt the objective perturbation algorithm to the wERM case and analyzing its performance. 
Another potential direction is to explore relaxing the DP guarantees from $\epsilon$-DP to $(\epsilon, \delta)$-DP or other types of DP relaxations such as Renyi DP \citep{Mironov2017}, zero-concentrated DP \citep{Bun2016}, or Gaussian DP \citep{Dong2019}, to examine if and how much the performance of DP-OWL improves.
It would also be interesting to compare the results of our work to the results obtained in the concurrently developed work by \citep{Spicker2024}.

This work represents an early effort in developing differentially private ITR methods.
While the developed DP-wERM is general, we have only explored its application to the OWL framework in a static treatment setting.
There exist other ITR methods, such as RWL and M-learning, that may be more powerful or general than OWL; it will be meaningful to explore algorithms that achieve $\epsilon$-DP for these ITR frameworks, as well as for other applications of ITRs such as dose-finding clinical trials, multi-treatment settings, and scenarios with multi-stage decision making, as introduced in Section~\ref{subsec:background}. 
As a preliminary study, we derive the $\ell_2$ global sensitivity of the output from M-learning (see the supplementary materials).  %various methods have been proposed as extensions of OWL to derive optimal ITRs. 
%Estimated predictor functions derived by OWL may be affected by a simple shift of the outcome and tends to keep treatment assignments that subjects actually received.  To address these challenges, residual weighted learning (RWL) \citep{Zhou2017} was proposed to derive ITRs by weighting observations using residuals of the outcome from a regression fit and solve the optimization problem using a difference of convex (d.c.) algorithm. 
%Matching-learning (M-learning) \citep{Wu2020} further improves the OWL-based framework by utilizing matching to balance subjects in different treatments instead of inverse probability weighting (IPW), making the algorithm more applicable for analysis of observational studies, which typically have much larger sample sizes than clinical trials. 
It would also be of interest to explore the performance of the DP-OWL algorithm in other application regimes where ITRs are commonly applied, such as electronic health records, personalized advertising, and recommender systems.
These applications use observational data collected from various sources (e.g., hospitals, clinics, insurance companies, social media, and e-commerce websites) and tend to be much larger in size compared to controlled clinical trials, implying that the performance of DP-OWL can be potentially much better.

As clinical and medical study data is extremely sensitive, being able to provide privacy protection guarantees for participants when collecting and sharing data is extremely valuable.
By applying privacy-guarantee frameworks when analyzing data and training models, as we have demonstrated in this work, study participants would be more comfortable participating in a study and researchers and investigators would feel more confident about sharing study results without compromising participants' privacy.

The general DP-wERM algorithm is coded in the \texttt{DPpack} R package \citep{DPpack, Giddens2023}.
The code used in the simulation and case studies is available at \url{https://github.com/sgiddens/DP-OWL}.
%The raw data from the melatonin clinical trial is not shareable due to privacy concerns.

%--------------------------------------
\acks{The authors acknowledge the helpful insights gleaned from several discussions with Dr. Jie Ding and Mr. Ganghua Wang. This work is supported by the University of Notre Dame Schmitt Fellowship and Lucy Graduate Scholarship to Giddens, University of Notre Dame Technology Ethics Center Research Assistantship, and was partially supported by an NSF grant DMS2113564 to Song.}

%--------------------------------------
\subsection*{Supplementary Materials}
The supplementary materials contain additional results from the simulation studies, detailed results on the sensitivity analysis of Algorithm 2 for hyperparameter tuning, and the derivation of the $\ell_2$ global sensitivity of the output from M-learning. 

\bibliography{references}

\end{document}