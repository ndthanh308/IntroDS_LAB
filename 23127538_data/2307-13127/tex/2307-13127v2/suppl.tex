\documentclass[12pt, letterpaper]{article}
%verbose=true,
%\usepackage{arxiv}
\usepackage[margin=1in]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{doi}
\usepackage{amsmath,setspace, enumerate}
\usepackage{amssymb}
\usepackage{algorithm, comment}
\usepackage{algpseudocode, titlesec}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{color, colortbl}
\usepackage{float}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{pro}{Proposition}
\newtheorem{rem}{Remark}
\newtheorem{assum}{Assumption}
\titlespacing*{\section}{0pt}{3pt plus 3pt minus 1pt}{3pt plus 3pt minus 1pt}
\titlespacing*{\subsection}{0pt}{3pt plus 3pt  minus 1pt}{3pt plus 3pt minus 1pt}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\E}{\mathbb{E}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\normx[1]{\left\Vert#1\right\Vert}
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}



\title{Supplementary materials to ``A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning''}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

% \author{ \href{https://orcid.org/0000-0000-0000-0000}{% Figure removed\hspace{1mm}David S.~Hippocampus}\thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
% 	Department of Computer Science\\
% 	Cranberry-Lemon University\\
% 	Pittsburgh, PA 15213 \\
% 	\texttt{hippo@cs.cranberry-lemon.edu} \\
% 	%% examples of more authors
% 	\And
% 	\href{https://orcid.org/0000-0000-0000-0000}{% Figure removed\hspace{1mm}Elias D.~Striatum} \\
% 	Department of Electrical Engineering\\
% 	Mount-Sheikh University\\
% 	Santa Narimana, Levand \\
% 	\texttt{stariate@ee.mount-sheikh.edu} \\
% 	%% \AND
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% }
\author{\small
Spencer Giddens$^1$\footnote{co-first authors}, Yiwang Zhou$^{2*}$, Kevin R. Krull$^{3,4}$, \vspace{-6pt}\\ 
\small Tara M. Brinkman$^{3,4}$, Peter X.K. Song$^5$, Fang Liu$^1$\footnote{corresponding author: fliu2@nd.edu}\\
\small$^1$ Department of Applied and Computational Mathematics and Statistics\vspace{-6pt}\\
\small University of Notre Dame, Notre Dame, IN 46556\\
\small$^2$ Department of Biostatistics, St. Jude Children's Research Hospital, Memphis, TN 38105\\
\small$^3$ Department of Epidemiology and Cancer Control, \vspace{-6pt}\\ \small St. Jude  Children's Research Hospital, Memphis, TN 38105\\
\small$^4$ Department of Psychology and Biobehavioral Sciences,  \vspace{-6pt}\\ \small St. Jude Children's Research Hospital, Memphis, TN 38105\\
\small$^5$ Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{Technical Report}
% \renewcommand{\undertitle}{Technical Report}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\subsection*{1. $\ell_2$ sensitivity in output perturbation of M-learning}

The appendix presents the $\ell_2$ sensitivity of the output from the M-learning framework \citep{Wu2020}, which is introduced in Section 1.1 of the main text.
The result is an extension of Theorem 1  to  M-learning.
The regularity conditions in the derivation of the sensitivity listed in Assumption 1 apply here. 

M-learning methods perform matching instead of inverse probability weighting as used in OWL for estimating ITRs to alleviate confounding and assess individuals' treatment responses to alternative treatments.
%Matching-based value functions are used to compare matched pairs.
Denote the matched set for subject $i$ as $\mathcal{M}_i$, which consists of subjects with opposite treatments but similar covariates as subject $i$, where similarity is defined under a suitable distance metric. The loss function, formulated with the Huber loss and an $\ell_2$ regularization term on  the model parameters, in M-learning  is
\begin{align*}
L=\sum_{i=1}^n \sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)\ell_{\text{Huber}}(z_{ij})+\gamma||\boldsymbol{\theta}||^2,
\end{align*}
where $g(.)$ is a user-specified monotonically increasing function (e.g. typical choices are $g(.)=1$ or the identity function), $z_{ij}= A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\boldsymbol{\theta}^T\mathbf{x}_i$.
$\tilde{B}_i= B_i -s(\mathbf{x}_i) $ is a surrogate residualized outcome for the observed outcome $B_i$, where $s()$ is a measurable function. It is shown that using $\tilde{B}_i$, instead of simply $B_i$ helps improve the performance of M-learning \cite{Wu2020, wang2024matching}. 

Let $d_g(\boldsymbol{\theta})$ denote the difference of two loss functions evaluated at a pair of neighboring datasets $(\mathbf{x}, \mathbf{A},\mathbf{B})$ and $(\mathbf{x}', \mathbf{A}',\mathbf{B}')$, where $\mathbf{x}=\{x_i\}_{i=1,\ldots,n},  \mathbf{A}=\{A_i\}_{i=1,\ldots,n}, \mathbf{B}=\{B_i\}_{i=1,\ldots,n}$; similarly for $(\mathbf{x}', \mathbf{A}',\mathbf{B}')$.
WLOG, we assume the $n$-th individual in two datasets ($\mathbf{x}, \mathbf{A},\mathbf{B}$) and ($\mathbf{x}', \mathbf{A}',\mathbf{B}'$) are not the same and that the values of $\tilde{B}$ in a subset of individuals $\mathcal{S}$ of the dataset ($|\mathcal{S}|\ge1$ individuals) are affected from altering the $n$-th individual, then 
\begin{align*}
d_g(\boldsymbol{\theta})
=&\!\sum_{i\in\mathcal{S}}\!\bigg\{\!\!\sum_{j\in\mathcal{M}'_i}\!|\mathcal{M'}_i|^{-1}g(|\tilde{B}'_i-\tilde{B}'_j|)\ell_{\text{Huber}}(z'_{ij}))\!-\!\!\!\sum_{j\in\mathcal{M}_i}\!\!|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)\ell_{\text{Huber}}(z_{ij}))\bigg\}, \\
\nabla_{\boldsymbol{\theta}}d_g(\boldsymbol{\theta}) 
=&\sum_{i\in\mathcal{S}}\bigg\{ \sum_{j\in\mathcal{M}'_i}|\mathcal{M'}_i|^{-1}d_g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i-\\
& \qquad\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(\tilde{B}_i-\tilde{B}_j)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\},\\
||\nabla_{\boldsymbol{\theta}}d_g(\boldsymbol{\theta})|| = &\bigg\|
\sum_{i\in\mathcal{S}}\bigg\{\sum_{j\in\mathcal{M}'_i}|\mathcal{M'}_i|^{-1}g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i-\\
& \qquad\quad\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\}\bigg\|\\
\le & |\mathcal{S}|\bigg\|
\sum_{j\in\mathcal{M}'}|\mathcal{M'}_i|^{-1}g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i\bigg\|+\\
&|\mathcal{S}| \bigg\|\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\|\\
\le & |\mathcal{S}| \sum_{j\in\mathcal{M}'_i}|\mathcal{M'}_i|^{-1} \bigg\|
g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i\bigg\|+\\
& |\mathcal{S}|\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1} \bigg\|g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\|.
\end{align*}
Given $A_i\in\{-1,1\}$, $\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\in\{-1,1\}$ and the assumptions of $||\mathbf{x}_i||_2\le 1$ $\forall i$ and $\|\nabla_{\boldsymbol{\theta}}\ell_{\text{Huber}}\|\le1$ (Assumption 1), then
\begin{align*}
||\nabla_{\boldsymbol{\theta}}d_g(\boldsymbol{\theta})|| 
& \le|\mathcal{S}|\sum_{j\in\mathcal{M}_i}\!|\mathcal{M}_i|^{-1} \bigg\|g(|\tilde{B}_i-\tilde{B}_j|)\bigg\|\! + |\mathcal{S}|\sum_{j\in\mathcal{M}'_i}|\mathcal{M}'_i|^{-1} \bigg\|g(|\tilde{B}'_i-\tilde{B}'_j|)\bigg\|\\
&\le\!2|\mathcal{S}|\max_i\max_j\big|g(|\tilde{B}_i-\tilde{B}_j|)\big|.
\end{align*}
Per Lemma 1 in \cite{chaudhuri2011arxiv}, the $\ell_2$ sensitivity of $\hat{\boldsymbol{\theta}}$ is
\begin{equation}\label{eqn:M.sens}
\Delta_{\hat{\boldsymbol{\theta}}}= \frac{2|\mathcal{S}|}{\gamma} \sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|.
\end{equation}
As for the value of $\sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|$, it depends on the function $g$ and the model used to obtain $\tilde{B}$.
If $g()=1$, then $|\mathcal{S}|=1,\sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|=1$ and $\Delta_{\hat{\boldsymbol{\theta}}}=2/\gamma$.
If linear regression is used to estimate $\tilde{B}$ and $g$ is the identity function, then it is reasonable to set $\sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|=6$. 

Finally, privacy-preserving estimate for $\boldsymbol{\theta}$ from an M-learning procedure with $\epsilon$-DP guarantees given the derived sensitivity $\Delta_{\hat{\boldsymbol{\theta}}}$ in Eqn \eqref{eqn:M.sens} is obtained by $\hat{\boldsymbol{\theta}}^*=\hat{\boldsymbol{\theta}}+\mathbf{e}$, where $f(\mathbf{e})\propto e^{-(\epsilon/\Delta_{\hat{\boldsymbol{\theta}}})\|e\|}=e^{-(\epsilon/\Delta_{\hat{\boldsymbol{\theta}}})\sqrt{e_i^2+ \cdots + e^2_p}}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{2. Tabular results from the simulation studies}

\begin{table}[!htp]
   \caption{Simulation results for several combinations of $\epsilon$ and $n$. The case without DP (corresponding to $\epsilon=\infty$) is also shown for reference.}
    \label{tab:sim_results}\vspace{-12pt}
    \begin{center}
\resizebox{0.9\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|cc}
    \hline
    \multicolumn{2}{c}{\textbf{}} & \multicolumn{2}{|c|}{\textbf{Treatment Assignment Accuracy (\%)}} & \multicolumn{2}{c}{\textbf{Treatment Value}} \\
    \hline
    $\epsilon$ & $n$ & Mean & 95\% CI & Mean & 95\% CI \\
    \hline
    \multirow{4}{*}{0.1} & 200 & 50.24 & (49.06, 51.42) & 8.36 & (8.26, 8.46)\\
    & 500 & 50.20 & (48.89, 51.52) & 8.42 & (8.32, 8.52)\\
    & 800 & 50.70 & (49.45, 51.94) & 8.40 & (8.30, 8.50)\\
    & 1000 & 51.08 & (49.72, 52.44) & 8.41 & (8.32, 8.51)\\
    \hline
    \multirow{4}{*}{0.5} & 200 & 52.87 & (51.49, 54.25) & 8.56 & (8.46, 8.66)\\

    & 500 & 58.02 & (56.58, 59.47) & 8.84 & (8.73, 8.95)\\

    & 800 & 60.20 & (58.61, 61.79) & 9.04 & (8.92, 9.16)\\

    & 1000 & 60.95 & (59.22, 62.68) & 9.05 & (8.93, 9.16)\\
    \hline
    \multirow{4}{*}{1} & 200 & 55.53 & (54.23, 56.83) & 8.77 & (8.67, 8.86)\\

    & 500 & 61.03 & (59.36, 62.70) & 9.07 & (8.96, 9.18)\\

    & 800 & 64.08 & (62.22, 65.93) & 9.25 & (9.13, 9.37)\\

    & 1000 & 64.70 & (62.93, 66.48) & 9.28 & (9.17, 9.40)\\
    \hline
    \multirow{4}{*}{2} & 200 & 58.99 & (57.30, 60.68) & 8.90 & (8.79, 9.02)\\

    & 500 & 68.17 & (66.27, 70.08) & 9.49 & (9.38, 9.61)\\

    & 800 & 74.32 & (72.50, 76.14) & 9.91 & (9.80, 10.02)\\

    & 1000 & 79.03 & (77.42, 80.63) & 10.12 & (10.03, 10.21)\\
    \hline
    \multirow{4}{*}{5} & 200 & 67.66 & (65.88, 69.43) & 9.51 & (9.40, 9.63)\\

    & 500 & 78.79 & (77.29, 80.29) & 10.13 & (10.04, 10.21)\\

    & 800 & 81.97 & (80.57, 83.37) & 10.23 & (10.15, 10.31)\\

    & 1000 & 84.80 & (83.62, 85.99) & 10.31 & (10.24, 10.38)\\
    \hline
    \multirow{4}{*}{$\infty$ (No DP)} & 200 & 88.65 & (88.02, 89.27) & 10.41 & (10.36, 10.47)\\

    & 500 & 92.18 & (91.75, 92.60) & 10.52 & (10.47, 10.57)\\

    & 800 & 93.39 & (93.02, 93.75) & 10.54 & (10.49, 10.60)\\

    & 1000 & 94.29 & (93.97, 94.61) & 10.62 & (10.57, 10.68)\\
    \hline
    \end{tabular}}
    \end{center}
 
\end{table}

% % Figure environment removed

\newpage
\subsection*{3. Sensitivity studies on Hyperparameter Tuning via Algorithm 2}

In our simulation study, we tuned the regularization constant via Algorithm 2 on an independent dataset that came from the same underlying population and had the same data characteristics (type of features, optimal treatment function, etc.) as the sensitive dataset we wanted to protect.
In practice, it is unlikely that we would have access to an independent dataset that mimics the sensitive dataset so closely.
Thus, it is fair to question whether the selected regularization constant would perform similarly if the independent dataset differed from the sensitive dataset.
We answer this question by running additional experiments on varying independent datasets to determine if the regularization constant selected via Algorithm 2 is sufficiently close to the optimal choice that would be obtained if the independent dataset came from the same underlying population as the sensitive dataset -- that is, the robustness of hyper-parameter selection via Algorithm 2.

The experiment settings are as follows.
We examine 4 different scenarios on how the independent datasets can be different from the sensitive dataset: 1) varying the number of data points $n_0$ in the independent dataset, 2) varying the coefficients $\boldsymbol{\theta}$ of the optimal treatment function, 3) varying the distribution of the features $\mathbf{X}$, and 4) varying the size $p$ of the optimal treatment function coefficients $\boldsymbol{\theta}$.
In each case, $n_0$ data points in the independent dataset  were split into a training set of size $0.8n_0$  and a validation set of size $0.2n_0$.
For each combination of sample size $n\in\{200,500,1000,2000\}$ and privacy budget $\epsilon\in\{0.5,1,5,20,150\}$, Algorithm 2 is run on the independent training set and used to fit a DP-OWL model with privacy budget $\epsilon$ at regularization constant $\gamma\in\{1,21,41,\ldots,601\}$.
The mean (with 95\% confidence interval) treatment assignment accuracy on the validation set over 1,000 repeats is shown in Figures \ref{fig:suppl_materials_N} to \ref{fig:suppl_materials_P}.
The results for the empirical treatment value, though not plotted, yielded similar conclusions and insights as described below.

In each plot, the black line corresponds to the baseline case of an independent dataset of size $n_0=1,000$ that matches the sensitive dataset in distribution and treatment function parameter and what we would see from the sensitive dataset itself.
It is clear to see that the optimal value for $\gamma$ that yields the highest accuracy varies as a function of $n$ and $\epsilon$.
The region of $\gamma$ values in between the vertical red lines indicates the $\gamma$ values where the accuracy is measured to be within $5\%$ of highest accuracy.
The range is fairly large for most simulated scenarios, indicating that the regularization constant is relatively ``robust'' as strong performance does not require a strict precision in the parameter tuning.
We seek to determine whether the $\gamma$ value that would be chosen based on alternative independent datasets would likely fall within this range.

\emph{Robustness of Algorithm 2 against varying size of the independent dataset}: Figure \ref{fig:suppl_materials_N} plots the results of the regularization constant tuning process for independent datasets of sizes $n_0\in\{100, 500, 1000, 2500\}$.
We observe that the optimal $\gamma$ values for these cases are consistently similar, meaning that even a smaller independent dataset (e.g., $n_0=100$) compared to the size of the sensitive dataset ($n=200$ to 2000) can still be useful for tuning the regularization constant.

\emph{Robustness of Algorithm 2 against different optimal treatment functions}: The results in Figure \ref{fig:suppl_materials_theta} correspond to different sets of coefficients (denoted by $\boldsymbol{\theta}$) for the underlying optimal treatment function from which the independent datasets ($n_0=1,000$)  were simulated. 
$\boldsymbol{\theta}_0$ is the baseline case that matches the sensitive dataset, and $\boldsymbol{\theta}_1$ and $\boldsymbol{\theta}_2$ represent two alternative optimal treatment functions. 
Despite the fact that a model fitted on these data would perform poorly in terms of prediction on the sensitive data (since the target function to learn is different), the optimal regularization constant values are still nearly the same for each corresponding pair of $\epsilon$ and sample size $n$.

\emph{Robustness of Algorithm 2 against different types of independent variables $\mathbf{X}$}: Figure \ref{fig:suppl_materials_dist} examines the case where the distribution of features $\mathbf{X}$ in the independent dataset is not the same as that in the sensitive dataset.
Specifically, each element in $\mathbf{X}$ follows a uniform distribution in the sensitive dataset but a truncated normal distribution (between 0 and 1) with varying means for each element in the independent dataset.
While the shape of plot looks different and shows longer plateaus in the trajectory of the regularization constant in the independent dataset, choosing one at the lower end of the plateau still consistently lands within the ``good window'' for the sensitive dataset.

\emph{Robustness of Algorithm 2 against different numbers of non-zero coefficients}: Figure \ref{fig:suppl_materials_P} visualizes the case where $\boldsymbol{\theta}$  for the optimal treatment assignment function contains a different number of non-zero elements.
In the baseline case, there are $p=4$ non-zero coefficients, the same as the sensitive dataset.
Here, we also examine the cases of $p\in\{2,6,8\}$ non-zero coefficients and observe that the optimal hyperparameter choice for the cases with different values of $p$ is consistently close to the optimal for the baseline case that matches the sensitive dataset.

\newpage
% Figure environment removed

\newpage
% Figure environment removed

\newpage
% Figure environment removed

\newpage
% Figure environment removed

\clearpage
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}