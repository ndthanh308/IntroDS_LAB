% Encoding: UTF-8

@article{orvieto2023resurrecting,
  title={Resurrecting Recurrent Neural Networks for Long Sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
journal={International Conference on Machine Learning},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{smith2022simplified,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal={arXiv preprint arXiv:2208.04933},
  year={2022}
}


@inproceedings{Chung21,
 author = {Chung, Stephen and Siegelmann, Hava},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28431--28441},
 publisher = {Curran Associates, Inc.},
 title = {Turing Completeness of Bounded-Precision Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ef452c63f81d0105dd4486f775adec81-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{Siegelmann92,
author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
title = {On the Computational Power of Neural Nets},
year = {1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {440â€“449},
numpages = {10},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}



@article{funahashi1989approximate,
  title={On the approximate realization of continuous mappings by neural networks},
  author={Funahashi, Ken-Ichi},
  journal={Neural networks},
  volume={2},
  number={3},
  pages={183--192},
  year={1989},
  publisher={Elsevier}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{haar1911theorie,
  title={Zur theorie der orthogonalen funktionensysteme},
  author={Haar, Alfred},
  journal={Mathematische Annalen},
  volume={71},
  number={1},
  pages={38--53},
  year={1911},
  publisher={Springer}
}

@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{jaeger2004harnessing,
  title={Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication},
  author={Jaeger, Herbert and Haas, Harald},
  journal={science},
  volume={304},
  number={5667},
  pages={78--80},
  year={2004},
  publisher={American Association for the Advancement of Science}
}

@article{candes2006stable,
  title={Stable signal recovery from incomplete and inaccurate measurements},
  author={Candes, Emmanuel J and Romberg, Justin K and Tao, Terence},
  journal={Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
  volume={59},
  number={8},
  pages={1207--1223},
  year={2006},
  publisher={Wiley Online Library}
}

@article{korda2018linear,
  title={Linear predictors for nonlinear dynamical systems: Koopman operator meets model predictive control},
  author={Korda, Milan and Mezi{\'c}, Igor},
  journal={Automatica},
  volume={93},
  pages={149--160},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{
park2021minimum,
title={Minimum Width for Universal Approximation},
author={Sejun Park and Chulhee Yun and Jaeho Lee and Jinwoo Shin},
booktitle={International Conference on Learning Representations},
year={2021},
}


@inproceedings{hanson2020universal,
  title={Universal simulation of stable dynamical systems by recurrent neural nets},
  author={Hanson, Joshua and Raginsky, Maxim},
  booktitle={Learning for Dynamics and Control},
  pages={384--392},
  year={2020},
  organization={PMLR}
}

@article{pinkus1999approximation,
  title={Approximation theory of the {MLP} model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}


% from MRU paper

@inproceedings{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{de2020batch,
  title={Batch normalization biases residual blocks towards the identity function in deep networks},
  author={De, Soham and Smith, Sam},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@misc{haiku2020github,
  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
  title = {{H}aiku: {S}onnet for {JAX}},
  url = {http://github.com/deepmind/dm-haiku},
  version = {0.0.9},
  year = {2020},
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{
kostic2022learning,
title={Learning Dynamical Systems via Koopman Operator Regression in Reproducing Kernel Hilbert Spaces},
author={Vladimir R Kostic and Pietro Novelli and Andreas Maurer and Carlo Ciliberto and Lorenzo Rosasco and massimiliano pontil},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@book{jeffreys1998theory,
  title={The theory of probability},
  author={Jeffreys, Harold},
  year={1998},
  publisher={OUP Oxford}
}

@book{jacquot2019modern,
  title={Modern digital control systems},
  author={Jacquot, Raymond G},
  year={2019},
  publisher={Routledge}
}

@misc{wangeffects,
  title={The Effects of Nonlinearity on Approximation Capacity of Recurrent Neural Networks},
  author={Wang, Shida and Li, Zhong and Li, Qianxiao},
  year={2022}
}

@article{gu2022train,
  title={How to train your hippo: State space models with generalized orthogonal basis projections},
  author={Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.12037},
  year={2022}
}

@inproceedings{islam2022long,
  title={Long movie clip classification with state-space video models},
  author={Islam, Md Mohaiminul and Bertasius, Gedas},
  booktitle={ECCV 2022},
  year={2022},
  organization={Springer}
}

@inproceedings{gupta2022diagonal,
  title={Diagonal State Spaces are as Effective as Structured State Spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{dao2022hungry,
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Dao, Tri and Fu, Daniel Y and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2212.14052},
  year={2022}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  year={2020}
}

@inproceedings{ni2022recurrent,
  title={Recurrent model-free rl can be a strong baseline for many pomdps},
  author={Ni, Tianwei and Eysenbach, Benjamin and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  year={2022},
  organization={PMLR}
}

@inproceedings{kapturowski2018recurrent,
  title={Recurrent experience replay in distributed reinforcement learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle={International conference on learning representations},
  year={2018}
}

@misc{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{gupta2022simplifying,
  title={Simplifying and Understanding State Space Models with Diagonal Linear RNNs},
  author={Gupta, Ankit and Mehta, Harsh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2212.00768},
  year={2022}
}

@article{noci2022signal,
  title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse},
  author={Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
  journal={arXiv preprint arXiv:2206.03126},
  year={2022}
}
@article{gu2022parameterization,
  title={On the parameterization and initialization of diagonal state space models},
  author={Gu, Albert and Gupta, Ankit and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.11893},
  year={2022}
}


@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{schmid2010dynamic,
  title={Dynamic mode decomposition of numerical and experimental data},
  author={Schmid, Peter J},
  journal={Journal of fluid mechanics},
  year={2010},
  publisher={Cambridge University Press}
}

@book{kutz2016dynamic,
  title={Dynamic mode decomposition: data-driven modeling of complex systems},
  author={Kutz, J Nathan and Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L},
  year={2016},
  publisher={SIAM}
}

@article{koopman1932dynamical,
  title={Dynamical systems of continuous spectra},
  author={Koopman, Bernard O and Neumann, J v},
  journal={Proceedings of the National Academy of Sciences},
  year={1932},
  publisher={National Acad Sciences}
}

@inproceedings{surana2016koopman,
  title={Koopman operator based observer synthesis for control-affine nonlinear systems},
  author={Surana, Amit},
  booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)},
  year={2016},
  organization={IEEE}
}

@incollection{korda2020koopman,
  title={Koopman model predictive control of nonlinear dynamical systems},
  author={Korda, Milan and Mezi{\'c}, Igor},
  booktitle={The Koopman Operator in Systems and Control},
  year={2020},
  publisher={Springer}
}

@article{mauroy2016global,
  title={Global stability analysis using the eigenfunctions of the Koopman operator},
  author={Mauroy, Alexandre and Mezi{\'c}, Igor},
  journal={IEEE Transactions on Automatic Control},
  year={2016},
  publisher={IEEE}
}

@article{proctor2018generalizing,
  title={Generalizing Koopman theory to allow for inputs and control},
  author={Proctor, Joshua L and Brunton, Steven L and Kutz, J Nathan},
  journal={SIAM Journal on Applied Dynamical Systems},
  year={2018},
  publisher={SIAM}
}

@article{kaiser2021data,
  title={Data-driven discovery of Koopman eigenfunctions for control},
  author={Kaiser, Eurika and Kutz, J Nathan and Brunton, Steven L},
  journal={Machine Learning: Science and Technology},
  year={2021},
  publisher={IOP Publishing}
}

@article{williams2015data,
  title={A data--driven approximation of the koopman operator: Extending dynamic mode decomposition},
  author={Williams, Matthew O and Kevrekidis, Ioannis G and Rowley, Clarence W},
  journal={Journal of Nonlinear Science},
  year={2015},
  publisher={Springer}
}

@book{mauroy2020koopman,
  title={Koopman operator in systems and control},
  author={Mauroy, Alexandre and Susuki, Y and Mezi{\'c}, I},
  year={2020},
  publisher={Springer}
}

@article{korda2018convergence,
  title={On convergence of extended dynamic mode decomposition to the Koopman operator},
  author={Korda, Milan and Mezi{\'c}, Igor},
  journal={Journal of Nonlinear Science},
  year={2018},
  publisher={Springer}
}


@article{li2022approximation,
  title={Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks.},
  author={Li, Zhong and Han, Jiequn and Weinan, E and Li, Qianxiao},
  journal={J. Mach. Learn. Res.},
  year={2022}
}

@article{romero2022towards,
  title={Towards a General Purpose CNN for Long Range Dependencies in ND},
  author={Romero, David W and Knigge, David M and Gu, Albert and Bekkers, Erik J and Gavves, Efstratios and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal={arXiv preprint arXiv:2206.03398},
  year={2022}
}

@article{weintraub2009jordan,
  title={Jordan canonical form: theory and practice},
  author={Weintraub, Steven H},
  journal={Synthesis Lectures on Mathematics and Statistics},
  year={2009},
  publisher={Morgan \& Claypool Publishers}
}

@article{martin2017parallelizing,
  title={Parallelizing linear recurrent neural nets over sequence length},
  author={Martin, Eric and Cundy, Chris},
  journal={arXiv preprint arXiv:1709.04057},
  year={2017}
}

@article{zhou2022film,
  title={Film: Frequency improved legendre memory model for long-term time series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Sun, Liang and Yao, Tao and Jin, Rong and others},
  journal={arXiv preprint arXiv:2205.08897},
  year={2022}
}

@article{bordin2022novel,
  title={Novel machine learning approaches revolutionize protein knowledge},
  author={Bordin, Nicola and Dallago, Christian and Heinzinger, Michael and Kim, Stephanie and Littmann, Maria and Rauer, Clemens and Steinegger, Martin and Rost, Burkhard and Orengo, Christine},
  journal={Trends in Biochemical Sciences},
  year={2022},
  publisher={Elsevier}
}

@article{goel2022s,
  title={It's Raw! Audio Generation with State-Space Models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2202.09729},
  year={2022}
}




@article{meckes2021eigenvalues,
  title={The Eigenvalues of Random Matrices},
  author={Meckes, Elizabeth},
  journal={arXiv preprint arXiv:2101.02928},
  year={2021}
}

@article{mehta2022long,
  title={Long range language modeling via gated state spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2206.13947},
  year={2022}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  year={1990},
  publisher={Wiley Online Library}
}


@inproceedings{pascanu2014construct,
  title={How to construct deep recurrent neural networks: Proceedings of the Second International Conference on Learning Representations (ICLR 2014)},
  author={Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={2nd International Conference on Learning Representations, ICLR 2014},
  year={2014}
}
@book{nakajima2021reservoir,
  title={Reservoir computing},
  author={Nakajima, Kohei and Fischer, Ingo},
  year={2021},
  publisher={Springer}
}

@article{gautschi1975optimally,
  title={Optimally conditioned Vandermonde matrices},
  author={Gautschi, Walter},
  journal={Numerische Mathematik},
  volume={24},
  pages={1--12},
  year={1975},
  publisher={Springer-Verlag}
}

@article{gautschi1987lower,
  title={Lower bounds for the condition number of Vandermonde matrices},
  author={Gautschi, Walter and Inglese, Gabriele},
  journal={Numerische Mathematik},
  volume={52},
  number={3},
  pages={241--250},
  year={1987},
  publisher={Springer}
}

@inproceedings{schafer2006recurrent,
  title={Recurrent neural networks are universal approximators},
  author={Sch{\"a}fer, Anton Maximilian and Zimmermann, Hans Georg},
  booktitle={Artificial Neural Networks--ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16},
  pages={632--640},
  year={2006},
  organization={Springer}
}

@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  journal={http://yann. lecun. com/exdb/mnist/},
  year={1998}
}

@article{cordova1990vandermonde,
  title={Vandermonde matrices on the circle: spectral properties and conditioning},
  author={C{\'o}rdova, Antonio and Gautschi, Walter and Ruscheweyh, Stephan},
  journal={Numerische Mathematik},
  volume={57},
  number={1},
  pages={577--591},
  year={1990},
  publisher={Springer}
}

@article{cooley1965algorithm,
  title={An algorithm for the machine calculation of complex Fourier series},
  author={Cooley, James W and Tukey, John W},
  journal={Mathematics of computation},
  year={1965},
  publisher={JSTOR}
}

@article{youla1961normal,
  title={A normal form for a matrix under the unitary congruence group},
  author={Youla, DC},
  journal={Canadian Journal of Mathematics},
  year={1961},
  publisher={Cambridge University Press}
}

@article{zhinan2002jordan,
  title={The Jordan Canonical Form of a Rational Random Matrix},
  author={Zhinan, Zhang},
  journal={Science Direct Working Paper},
  year={2002}
}

@article{wang2022pretraining,
  title={Pretraining Without Attention},
  author={Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M},
  journal={arXiv preprint arXiv:2212.10544},
  year={2022}
}

@book{axler1997linear,
  title={Linear algebra done right},
  author={Axler, Sheldon},
  year={1997},
  publisher={Springer Science \& Business Media}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  year={2013},
  organization={PMLR}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  year={2017},
  organization={PMLR}
}

%%%% Original RNN papers %%%

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  year={1943},
  publisher={Springer}
}


@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  year={1982},
  publisher={National Acad Sciences}
}



@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

%% orthogonal RNNs

@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  year={2016},
  organization={PMLR}
}

@inproceedings{mhammedi2017efficient,
  title={Efficient orthogonal parametrisation of recurrent neural networks using householder reflections},
  author={Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  booktitle={International Conference on Machine Learning},
  year={2017},
  organization={PMLR}
}

@inproceedings{vorontsov2017orthogonality,
  title={On orthogonality and learning recurrent networks with long term dependencies},
  author={Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
  booktitle={International Conference on Machine Learning},
  year={2017},
  organization={PMLR}
}

@inproceedings{maduranga2019complex,
  title={Complex unitary recurrent neural networks using scaled cayley transform},
  author={Maduranga, Kehelwala DG and Helfrich, Kyle E and Ye, Qiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2019}
}


@article{hochreiter1991untersuchungen,
title={Untersuchungen zu dynamischen neuronales Netzen},
author={Hochreiter, Sepp},
journal={Diploma thesis, Institut f{"u}r Informatik, Technische Universit{"a}t M{"u}nchen},
year={1991}
}

@article{bengio1994learning,
title={Learning long-term dependencies with gradient descent is difficult},
author={Bengio, Y and Simard, Patrice and Frasconi, Paolo},
journal={IEEE transactions on neural networks},
year={1994},
publisher={IEEE}
}

@article{hochreiter1997long,
title={Long short-term memory},
author={Hochreiter, Sepp and Schmidhuber, J{"u}rgen},
journal={Neural computation},
year={1997},
publisher={MIT Press}
}

@article{salinas2020deepar,
  title={DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author={Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal={International Journal of Forecasting},
  year={2020},
  publisher={Elsevier}
}
@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  year={2021},
  publisher={Elsevier}
}

@article{smyl2018m4,
  title={M4 forecasting competition: Introducing a new hybrid ES-RNN model},
  author={Smyl, Slawek and Ranganathan, Jai and Pasqua, Andrea},
  journal={https://eng.uber.com/m4-forecasting-competition},
  year={2018}
}

@article{bahdanau2014neural,
title={Neural machine translation by jointly learning to align and translate},
author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
journal={arXiv preprint arXiv:1409.0473},
year={2014}
}

@article{kocisk_y2018comparative,
title={A comparative study of attention mechanisms in deep learning models for natural language processing},
author={Kocisk{'y}, Tom{'a}{\v{s}} and Bojar, Ond{\v{r}}ej and Haddow, Barry and Federmann, Christian},
journal={arXiv preprint arXiv:1810.00347},
year={2018}
}

@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={International conference on machine learning},
  year={2018},
  organization={PMLR}
}

@article{wisdom2016full,
  title={Full-capacity unitary recurrent neural networks},
  author={Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
  journal={Advances in neural information processing systems},
  year={2016}
}

@inproceedings{hyland2017learning,
  title={Learning unitary operators with help from u (n)},
  author={Hyland, Stephanie L and R{\"a}tsch, Gunnar},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{erichson2021lipschitz,
title={Lipschitz Recurrent Neural Networks},
author={N. Benjamin Erichson and Omri Azencot and Alejandro Queiruga and Liam Hodgkinson and Michael W. Mahoney},
booktitle={International Conference on Learning Representations},
year={2021}
}

@article{hasani2022liquid,
  title={Liquid structural state-space models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}

@article{gu2021combining,
  title={Combining recurrent, convolutional, and continuous-time models with linear state space layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  year={2021}
}

@article{ma2022mega,
  title={Mega: moving average equipped gated attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2209.10655},
  year={2022}
}

@inproceedings{hasani2021liquid,
  title={Liquid time-constant networks},
  author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}


@inproceedings{nguyen2022s4nd,
  title={S4nd: Modeling images and videos as multidimensional signals with state spaces},
  author={Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon and Shah, Preey and Dao, Tri and Baccus, Stephen and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  year={2015},
}

@book{siegelmann2012neural,
  title={Neural networks and analog computation: beyond the Turing limit},
  author={Siegelmann, Hava T},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{vogel2002computational,
  title={Computational methods for inverse problems},
  author={Vogel, Curtis R},
  year={2002},
  publisher={SIAM}
}

@article{voelker2019legendre,
  title={Legendre memory units: Continuous-time representation in recurrent neural networks},
  author={Voelker, Aaron and Kaji{\'c}, Ivana and Eliasmith, Chris},
  journal={Advances in neural information processing systems},
  year={2019}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@inproceedings{lezcano2019cheap,
  title={Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group},
  author={Lezcano-Casado, Mario and Mart{\i}nez-Rubio, David},
  booktitle={International Conference on Machine Learning},
  year={2019},
  organization={PMLR}
}

@inproceedings{jing2017tunable,
  title={Tunable efficient unitary neural networks (eunn) and their application to rnns},
  author={Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Solja{\v{c}}i{\'c}, Marin},
  booktitle={International Conference on Machine Learning},
  year={2017},
  organization={PMLR}
}



@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{chung2014empirical,
title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
author={Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
journal={arXiv preprint arXiv:1412.3555},
year={2014}
}

@inproceedings{helfrich2018orthogonal,
  title={Orthogonal recurrent neural networks with scaled cayley transform},
  author={Helfrich, Kyle and Willmott, Devin and Ye, Qiang},
  booktitle={International Conference on Machine Learning},
  year={2018},
  organization={PMLR}
}



@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{ginibre1965statistical,
  title={Statistical ensembles of complex, quaternion, and real matrices},
  author={Ginibre, Jean},
  journal={Journal of Mathematical Physics},
  year={1965},
  publisher={American Institute of Physics}
}

@article{kilian1996dynamic,
  title={The dynamic universality of sigmoidal neural networks},
  author={Kilian, Joe and Siegelmann, Hava T},
  journal={Information and computation},
  year={1996},
  publisher={Elsevier}
}

@article{chung2021turing,
  title={Turing Completeness of Bounded-Precision Recurrent Neural Networks},
  author={Chung, Stephen and Siegelmann, Hava},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}




@article{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@article{conneau2019cross,
  title={Cross-lingual Language Model Pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Justin and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Shankar and Sastry, Gaurav and Askell, Alden and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{raffel2019exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Houlsby, Neil and Gelly, Sylvain and Zhang, Xiaohua and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author={Baevski, Alexey and Auli, Michael},
  journal={arXiv preprint arXiv:2006.11477},
  year={2020}
}

@article{huang2018music,
  title={Music Transformer: Generating Music with Long-Term Structure},
  author={Huang, Zhiqiu and Li, Yuxuan and Duan, Nan and Chen, Yan and Li, Haizhou},
  journal={arXiv preprint arXiv:1809.04281},
  year={2018}
}

@inproceedings{biggio2021neural,
  title={Neural symbolic regression that scales},
  author={Biggio, Luca and Bendinelli, Tommaso and Neitz, Alexander and Lucchi, Aurelien and Parascandolo, Giambattista},
  booktitle={International Conference on Machine Learning},
  year={2021},
  organization={PMLR}
}

@article{lu2022survey,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Lu, Pan and Qiu, Liang and Yu, Wenhao and Welleck, Sean and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2212.10535},
  year={2022}
}

@article{polu2022formal,
  title={Formal mathematics statement curriculum learning},
  author={Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2202.01344},
  year={2022}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}


@inproceedings{choromanski2020rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{chen2021skyformer,
  title={Skyformer: Remodel self-attention with gaussian kernel and {Nystrom} method},
  author={Chen, Yifan and Zeng, Qi and Ji, Heng and Yang, Yun},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{li2022makes,
  title={What Makes Convolutional Models Great on Long Sequence Modeling?},
  author={Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
  journal={arXiv preprint arXiv:2210.09298},
  year={2022}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2015}
}
@inproceedings{brock2020characterizing,
  title={Characterizing signal propagation to close the performance gap in unnormalized ResNets},
  author={Brock, Andrew and De, Soham and Smith, Samuel L},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{2019t5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {arXiv preprint arXiv:1910.10683},
  year = {2019}
}


@article{lee2021fnet,
  title={Fnet: Mixing tokens with fourier transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

@article{le2015simple,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{kalchbrenner2016neural,
  title={Neural machine translation in linear time},
  author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1610.10099},
  year={2016}
}

@article{liu2019mkd,
  title={Mkd: a multi-task knowledge distillation approach for pretrained language models},
  author={Liu, Linqing and Wang, Huan and Lin, Jimmy and Socher, Richard and Xiong, Caiming},
  journal={arXiv preprint arXiv:1911.03588},
  year={2019}
}

@article{nallapati2016abstractive,
  title={Abstractive text summarization using sequence-to-sequence rnns and beyond},
  author={Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
  journal={arXiv preprint arXiv:1602.06023},
  year={2016}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  year={2010},
  organization={Makuhari}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  year={2022}
}

@inproceedings{steil2004backpropagation,
  title={Backpropagation-decorrelation: online recurrent learning with O (N) complexity},
  author={Steil, Jochen J},
  booktitle={2004 IEEE international joint conference on neural networks},
  year={2004},
  organization={IEEE}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@misc{bradbury2018jax,
  title={{JAX}: composable transformations of Python+ NumPy programs},
  author={Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and others},
  year={2018}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}