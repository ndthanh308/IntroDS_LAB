\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Candes et~al.(2006)Candes, Romberg, and Tao]{candes2006stable}
Emmanuel~J Candes, Justin~K Romberg, and Terence Tao.
\newblock Stable signal recovery from incomplete and inaccurate measurements.
\newblock \emph{Communications on Pure and Applied Mathematics: A Journal
  Issued by the Courant Institute of Mathematical Sciences}, 59\penalty0
  (8):\penalty0 1207--1223, 2006.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Chung and Siegelmann(2021)]{Chung21}
Stephen Chung and Hava Siegelmann.
\newblock Turing completeness of bounded-precision recurrent neural networks.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 28431--28441. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/ef452c63f81d0105dd4486f775adec81-Paper.pdf}.

\bibitem[C{\'o}rdova et~al.(1990)C{\'o}rdova, Gautschi, and
  Ruscheweyh]{cordova1990vandermonde}
Antonio C{\'o}rdova, Walter Gautschi, and Stephan Ruscheweyh.
\newblock Vandermonde matrices on the circle: spectral properties and
  conditioning.
\newblock \emph{Numerische Mathematik}, 57\penalty0 (1):\penalty0 577--591,
  1990.

\bibitem[Dao et~al.(2022)Dao, Fu, Saab, Thomas, Rudra, and
  R{\'e}]{dao2022hungry}
Tri Dao, Daniel~Y Fu, Khaled~K Saab, Armin~W Thomas, Atri Rudra, and
  Christopher R{\'e}.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock \emph{arXiv preprint arXiv:2212.14052}, 2022.

\bibitem[Funahashi(1989)]{funahashi1989approximate}
Ken-Ichi Funahashi.
\newblock On the approximate realization of continuous mappings by neural
  networks.
\newblock \emph{Neural networks}, 2\penalty0 (3):\penalty0 183--192, 1989.

\bibitem[Gautschi(1975)]{gautschi1975optimally}
Walter Gautschi.
\newblock Optimally conditioned vandermonde matrices.
\newblock \emph{Numerische Mathematik}, 24:\penalty0 1--12, 1975.

\bibitem[Gautschi and Inglese(1987)]{gautschi1987lower}
Walter Gautschi and Gabriele Inglese.
\newblock Lower bounds for the condition number of vandermonde matrices.
\newblock \emph{Numerische Mathematik}, 52\penalty0 (3):\penalty0 241--250,
  1987.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022s}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It's raw! audio generation with state-space models.
\newblock \emph{arXiv preprint arXiv:2202.09729}, 2022.

\bibitem[Gu et~al.(2021)Gu, Goel, and Re]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gu et~al.(2022)Gu, Gupta, Goel, and R{\'e}]{gu2022parameterization}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock \emph{arXiv preprint arXiv:2206.11893}, 2022.

\bibitem[Gupta et~al.(2022)Gupta, Mehta, and Berant]{gupta2022simplifying}
Ankit Gupta, Harsh Mehta, and Jonathan Berant.
\newblock Simplifying and understanding state space models with diagonal linear
  rnns.
\newblock \emph{arXiv preprint arXiv:2212.00768}, 2022.

\bibitem[Haar(1911)]{haar1911theorie}
Alfred Haar.
\newblock Zur theorie der orthogonalen funktionensysteme.
\newblock \emph{Mathematische Annalen}, 71\penalty0 (1):\penalty0 38--53, 1911.

\bibitem[Hanson and Raginsky(2020)]{hanson2020universal}
Joshua Hanson and Maxim Raginsky.
\newblock Universal simulation of stable dynamical systems by recurrent neural
  nets.
\newblock In \emph{Learning for Dynamics and Control}, pages 384--392. PMLR,
  2020.

\bibitem[Hasani et~al.(2021)Hasani, Lechner, Amini, Rus, and
  Grosu]{hasani2021liquid}
Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu.
\newblock Liquid time-constant networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Hornik(1991)]{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Jaeger and Haas(2004)]{jaeger2004harnessing}
Herbert Jaeger and Harald Haas.
\newblock Harnessing nonlinearity: Predicting chaotic systems and saving energy
  in wireless communication.
\newblock \emph{science}, 304\penalty0 (5667):\penalty0 78--80, 2004.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Cai, Zhang, Chen, and
  Dey]{li2022makes}
Yuhong Li, Tianle Cai, Yi~Zhang, Deming Chen, and Debadeepta Dey.
\newblock What makes convolutional models great on long sequence modeling?
\newblock \emph{arXiv preprint arXiv:2210.09298}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Han, Weinan, and
  Li]{li2022approximation}
Zhong Li, Jiequn Han, E~Weinan, and Qianxiao Li.
\newblock Approximation and optimization theory for linear continuous-time
  recurrent neural networks.
\newblock \emph{J. Mach. Learn. Res.}, 2022{\natexlab{b}}.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Martin and Cundy(2017)]{martin2017parallelizing}
Eric Martin and Chris Cundy.
\newblock Parallelizing linear recurrent neural nets over sequence length.
\newblock \emph{arXiv preprint arXiv:1709.04057}, 2017.

\bibitem[Nakajima and Fischer(2021)]{nakajima2021reservoir}
Kohei Nakajima and Ingo Fischer.
\newblock \emph{Reservoir computing}.
\newblock Springer, 2021.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu,
  and De]{orvieto2023resurrecting}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,
  Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Pinkus(1999)]{pinkus1999approximation}
Allan Pinkus.
\newblock Approximation theory of the {MLP} model in neural networks.
\newblock \emph{Acta numerica}, 8:\penalty0 143--195, 1999.

\bibitem[Sch{\"a}fer and Zimmermann(2006)]{schafer2006recurrent}
Anton~Maximilian Sch{\"a}fer and Hans~Georg Zimmermann.
\newblock Recurrent neural networks are universal approximators.
\newblock In \emph{Artificial Neural Networks--ICANN 2006: 16th International
  Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16},
  pages 632--640. Springer, 2006.

\bibitem[Siegelmann and Sontag(1992)]{Siegelmann92}
Hava~T. Siegelmann and Eduardo~D. Sontag.
\newblock On the computational power of neural nets.
\newblock In \emph{Proceedings of the Fifth Annual Workshop on Computational
  Learning Theory}, COLT '92, page 440â€“449, New York, NY, USA, 1992.
  Association for Computing Machinery.

\bibitem[Smith et~al.(2022)Smith, Warrington, and
  Linderman]{smith2022simplified}
Jimmy~TH Smith, Andrew Warrington, and Scott~W Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock \emph{arXiv preprint arXiv:2208.04933}, 2022.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Yan, Gu, and Rush]{wang2022pretraining}
Junxiong Wang, Jing~Nathan Yan, Albert Gu, and Alexander~M Rush.
\newblock Pretraining without attention.
\newblock \emph{arXiv preprint arXiv:2212.10544}, 2022.

\end{thebibliography}
