\documentclass{hld2023} % Include author names
%\documentclass[anon]{hld2023} % Author names withheld

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\newtheorem{assumption}{Assumption}
\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}
\renewcommand{\theassumption}{\Alph{assumption}}
\newcommand{\razp}[1]{{\color{red}[razp: #1]}}
\newcommand{\caglar}[1]{{\color{cyan}[caglar: #1]}}

\title[On the Universality of Linear Recurrences followed by Nonlinear Projections]{On the Universality of Linear Recurrences\\ Followed by Nonlinear Projections}
%\title[Approximation guaranteed for deep LRUs ]{Deep Linear Recurrent Units Can Approximate \\Arbitrary Sequence-to-Sequence Maps}

% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Authors with the same address:
\hldauthor{%
\begin{center}
\Name{Antonio Orvieto$^*$, Soham De$^\dagger$, Caglar Gulcehre$^\dagger$, Razvan Pascanu$^\dagger$, Samuel L. Smith$^\dagger$} \\
\addr {\qquad ETH Zurich$^*$ and DeepMind London$^\dagger$} 
\end{center}
 }

% Authors with different addresses:
%\optauthor{%
% \Name{Author Name1} \Email{abc@sample.com}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.com}\\
% \addr Address 2%
%}
%Universality of linear recurrence followed by nonlinear projections
\input{defs}
\begin{document}

\maketitle

\vspace{-8mm}

\begin{abstract}%
In this note~(\textit{work in progress towards a full paper}) we show that a family of sequence models based on recurrent \textit{linear} layers~(including S4, S5 and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. 
The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP. 
\end{abstract}

%\begin{keywords}%
%  List of keywords%
%\end{keywords}

\section{Introduction}
%The development of efficient and high-performing architectures for understanding sequences is a captivating area of deep learning research, with promising applications in fields ranging from music generation to language modeling and genome analysis. While commonly-used transformer-based models for sequence generation and understanding, rooted in the attention mechanism~\cite{vaswani2017attention}, are easy to optimize and scale on modern hardware, their inference and training time is quadratic in the input sequence length. This limitation poses challenges when dealing with extremely long sequences, requiring additional techniques that inevitably limit performance~\cite{wang2020linformer, chen2021skyformer}. Conversely, traditional sequence processing blocks like LSTMs~\cite{hochreiter1997long} and GRUs~\cite{cho2014learning}, which are still commonly used in applications like reinforcement learning and neuroscience, offer rapid inference times that scale linearly with the sequence length. While faster than attention-based models at inference time, these are not designed to be trained efficiently on parallel hardware and suffer from well-known issues such as vanishing gradients.

Transformers \cite{vaswani2017attention} have supplanted LSTMs \cite{hochreiter1997long} and GRUs \cite{cho2014learning} as the dominant sequence to sequence model for deep learning. However, a promising line of research initiated by the S4 model \cite{gu2021efficiently} has begun a come-back of recurrent sequence models in simplified \textit{linear} form. A growing family of `state-space' models (SSMs) \cite{hasani2021liquid,smith2022simplified, gupta2022simplifying, gu2022parameterization,li2022makes, orvieto2023resurrecting} has emerged which interleave recurrent linear layers with other components of popular deep learning models such as position-wise multi-layer perceptrons (MLPs), residual connections \citep{he2016deep}, and normalization layers \citep{ioffe2015batch, ba2016layer}. These models achieve state-of-the-art results on the long-range arena~\cite{tay2020long}, and have been successfully applied to boost performance and scalability in many applications~\cite{goel2022s,wang2022pretraining,dao2022hungry}. They have gained significant attention in the literature due to two key factors. First, their computational complexity scales linearly in sequence length, while transformers scale quadratically \cite{vaswani2017attention, katharopoulos2020transformers}. Second, unlike LSTMs and GRUs, they can be efficiently parallelized during training using parallel scans~\cite{martin2017parallelizing, smith2022simplified}.



%Among the many variants and improvements over the S4 state-space model~\cite{hasani2021liquid,smith2022simplified, gupta2022simplifying, gu2022parameterization,li2022makes, orvieto2023resurrecting}, the recently proposed Linear Recurrent Unit~(LRU)~\cite{orvieto2023resurrecting} is conceptually very similar to linear RNNs, and yet it achieves state-of-the-art performance on the long-range arena~\cite{tay2020long} by means of simple stacks of diagonal linear recurrent layers, with all non-linearities placed outside of the recurrent layers within position-wise MLPs. LRUs, as well as all S4 variants cited above, offer the same inference-time complexity as LSTMs and GRUs, while being scalable on modern hardware leveraging parallel scans~\cite{martin2017parallelizing}.

%While recent works are mostly dedicated to scaling state space models or linear RNNs and improving their empirical performance~\cite{dao2022hungry, wang2022pretraining}, 
The success of these models is surprising, since it was previously widely thought that non-linearities within the recurrence are necessary for RNNs to model complex functions~\cite{schafer2006recurrent}. In this paper, we therefore step back and study the theoretical expressivity of models based on stacks of linear RNNs interleaved with position-wise MLPs. In contrast to non-linear RNNs for which we have a multitude of results in terms of expressivity, from Turing-completeness~\cite{Chung21,Siegelmann92} to universality~\cite{schafer2006recurrent, hanson2020universal}, surprisingly little is known for architectures of this type relying on linear recurrences. 

We prove that linear RNN layers interleaved with position-wise MLPs can approximate arbitrarily well any \emph{sufficiently regular} sequence to sequence map over finite length sequences. Our key insight is that, if the linear RNN can preserve a lossless compressed representation of the entire sequence seen so far, the MLP will have access to all of the information in the input. Since infinite-width MLPs are universal approximators~\cite{pinkus1999approximation}, it can process this information to achieve the desired map.

%In this work, we limit our attention on the setting when we have finite length sequences, confirming that interleaved with MLPs, linear recurrent blocks such as LRUs and S4 preserve universal approximator property, i.e they can approximate arbitrarily well any \emph{sufficiently regular} sequence-to-sequence map over a finite length stream. While the MLP has the universal approxmator property, it only sees the state of the linear recurrent model which is fed one time step at a time. Our key insight is that the linear RNN can preserve a lossless compressed representation of the entire sequence seen so far, hence the MLP has acceess to the whole sequence and is able to approximate any map.

%Our findings show that, once interleaved with MLPs, linear recurrent blocks such as LRUs and S4 variants can approximate regular sequence-to-sequence maps with arbitrary precision. Our discussion supports the already clear power of such models, which achieve state-of-the-art performance in many long-range reasoning tasks. 

% Razvan: I'm not sure we need to list the contributions, though we can add that in the final version if we have space!
%In this note, we make the following contributions:
%\begin{enumerate}
%    \item 
%\end{enumerate}

% Figure environment removed


\section{Formal problem definition and known results}
\vspace{-2mm}
Consider length-$L$ sequences of real $M$-dimensional inputs: $(u_i)_{i=1}^L\in\R^{M\times L}$. A non-linear causal sequence-to-sequence map produces a sequence of real $M$-dimensional\footnote{For simplicity yet without loss in generality, we assume input and output dimensions coincide.} outputs $(y_i)_{i=1}^L$ as 
%\razp{Did we take this formalism from somewhere (i.e. so we can say similar to X we define.. I wonder how to think of this when dealing with infinite sequences .. not sure what happens mathematically to $T_k$ as $k \to \infty$}
%\antonio{I did not take it from anywhere, but I think its provably the most general thing one can have. I have a new section~\ref{app:alternative_def} on another alternative.}
\begin{equation}
    y_k = T_k[(u_i)_{i=1}^k],
    \label{eq:seq-to-seq}
    \vspace{-1mm}
\end{equation}
where $T_k:\R^{M\times k}\to\R^{M}$ is a non-linear map. Here, we consider an approximation of this map using deep networks with linear diagonal RNNs interleaved with position-wise MLPs~(see Fig.~\ref{fig:architecture}).
\vspace{-2mm}
\paragraph{Architecture.} The sequence of real $M$-dimensional inputs $(u_i)_{i=1}^L$ is first processed by a linear RNN of hidden size $N$, initialized e.g. in the style of~\cite{orvieto2023resurrecting}, or as in \cite{gu2021efficiently, smith2022simplified}\footnote{For our purposes, fixing the linear RNN at random inizialization is enough (hence why we need to specify it). Our setting is therefore similar to reservoir computing, yet with a modern linear RNN input $\to$ reservoir map~\cite{nakajima2021reservoir}.}
%\razp{This is a proof by construction, where learning is not taken into account. So we should not specify how it is trained or anything like that. In the proof we don't take SGD into account}\antonio{Good point, I modified slightly}
: from $x_{-1}=0$,
\begin{equation}
    x_k = \Lambda x_{k-1} + B u_k,
    \label{eq:hidden}
\end{equation}
where $\Lambda = \diag(\lambda_1,\lambda_2,\dots,\lambda_N)\in\C^{N\times N}$ is a diagonal complex matrix and $B\in\C^{N\times M}$ is the input projection matrix. As explained in detail in~\cite{orvieto2023resurrecting}, the use of complex matrices is convenient for the purpose of retaining full equivalence with dense real linear systems of the form $x_k = \tilde A x_{k-1} + \tilde B u_k$, where $\tilde A,\tilde B$ are real dense matrices. Compared to the dense or non-linear setting, equation~\eqref{eq:hidden} leads to faster training and inference on parallelizable hardware~(see discussion in~\cite{smith2022simplified, orvieto2023resurrecting}).

% % Figure environment removed


The sequence of $N$-dimensional hidden states $(x_k)_{k=1}^L$ is then projected back on the space of real numbers to get the sequence $(\hat y_k)_{k=1}^L$, where $\hat y_k = \Re[Cx_k]$ with $C = \C^{M\times N}$. Finally, a position-wise MLP is applied to $\hat y$ to get the output sequence: $y_k = \hat\phi(\hat y_k) = \phi (x_k)$, where $\phi:\C^N\to\R^N$ contains all position-wise operations: $\phi(x) := \hat\phi(\Re[C x])$.

What we just described is a model of a single block of modern SSM architectures including diagonal variants of the S4 model \cite{gu2021efficiently} or LRUs \cite{orvieto2023resurrecting}. We note that these architectures typically also include batch/layer normalization and residual connections, which we do not discuss in this note.

% All in all, we can quickly write the architecture\footnote{In this note, for simplicity, we do not discuss the effects of batch/layer normalization and of residual connections.} as
% \begin{equation}
%     y_k = \phi_D\circ\LRU_{D}\circ\cdots\circ\phi_{2}\circ\LRU_2\circ\phi_1\circ\LRU_1((u_i)_{i=1}^k),
%     \label{eq:approx}
% \end{equation}
% where LRU$_i$ maps the input sequence to the corresponding sequence of hidden states at layer $i$, and the $\phi_i$ act position-wise. In this paper, we claim equation~\eqref{eq:approx} under some conditions can parametrize general dynamical systems of the form of equation~\eqref{eq:seq-to-seq}. In particular, we show that non-linearities in the recurrence are not necessary for modeling non-linear dynamical systems, once an MLP is placed after the linear recurrent unit.
\vspace{-2mm}
\paragraph{Universality of MLPs and non-linear RNNs.} The approximation properties of deep neural networks with ReLU activations are well studied. While recent advances concern the effect of depth~\cite{lu2017expressive}, the study by Pinkus~\cite{pinkus1999approximation}, as well as previous works~\cite{funahashi1989approximate,hornik1989multilayer,hornik1991approximation}, already established the power of neural networks with a single hidden layer, which can approximate arbitrary continuous non-linear maps on compacts as the size of the hidden layer grows to infinity. This result can be also used in the context of nonlinear RNN approximation of dynamical systems~(e.g. in neuroscience), where the state can be seen as part of an MLP~(see e.g.~\cite{hanson2020universal}): compared to eq.~\eqref{eq:hidden} we have $x_k = \sigma(A x_{k-1} + B u_k)$, where $\sigma$ is is a nonlinearity. Meanwhile linear RNNs, where $x_k = A x_{k-1} + B u_k$, have often been considered of minor interest, equivalent in approximation power to convolutions~\cite{li2022approximation}. However, we focus on a restricted scenario of dealing with finite length sequences, and show that, when sufficiently wide, the linear RNN \emph{does not} form a bottleneck, and the architecture maintains universality through the application of the pointwise MLP, as done in recent SSMs achieving state-of-the-art results~\cite{gu2021efficiently, smith2022simplified, orvieto2023resurrecting}. We give a detailed overview in App.~\ref{app:rw}.


\vspace{-2mm}
\section{Random Linear RNNs compress the input sequence}
Unrolling eq.~\eqref{eq:hidden} we get, $x_{0} = Bu_0$,  $x_1 = \Lambda Bu_0 + Bu_1$, $x_2 = \Lambda^2Bu_0 + \Lambda Bu_1 + B u_2$ \dots
\begin{align}
    \label{eq:lin_rnn_unroll}
    x_k =\sum_{j=0}^{k-1} \Lambda^jBu_{k-j}.
    \vspace{-1mm}
\end{align}
One linear RNN+MLP layer therefore computes the sequence $(y_k)_{k=1}^L$ as $y_k = \phi\left(\sum_{j=0}^{k-1} \Lambda^jBu_{k-j}\right)$, where $\phi:\C^{N}\to\R^M$ is a non-linear map parameterized by an MLP. Under some assumptions, we show in this section that $\sum_{j=0}^{k-1} \Lambda^jBu_{k-j}$ captures all information about the input up to step $k$. As a result, as we show later in~\S\ref{sec:MLP}, $\phi$ can effectively parametrize any \emph{regular enough} non-linear sequence-to-sequence map from past inputs to outputs: $\phi\left(\sum_{j=0}^{k-1} \Lambda^jBu_{k-j}\right) = T_k((u_i)_{i=1}^k)$.
\vspace{-2mm}
\subsection{Warm-up}
\label{sec:vandermonde}
We start with a simple one-dimensional example. Let $M=1$ and $B = (1,1,\dots, 1)^\top\in\R^{N\times M}$. Then, eq.~\eqref{eq:lin_rnn_unroll} can be written as a matrix multiplication, recalling $\Lambda=\diag(\lambda_1,\dots,\lambda_N)$, $\lambda_i\in\C$:
\begin{equation}
    x_k   =
    \begin{pmatrix}
    \lambda_1^{k-1}&\lambda_1^{k-2} &\cdots& \lambda_1&1\\
    \lambda_2^{k-1}&\lambda_2^{k-2} &\cdots& \lambda_2&1\\
    \vdots&\vdots &\ddots&\vdots&\vdots\\
    \lambda_N^{k-1}&\lambda_N^{k-2} &\cdots& \lambda_N&1\\
    \end{pmatrix}
    \begin{pmatrix}
    u_1 \\ u_{2} \\ \vdots \\ u_k
    \end{pmatrix}= V_k u_{1:k}.
    \label{eq:vandermonde}
\end{equation}
As long as $N\ge k$, we can hope to recover $u_{1:k}$ by pseudoinversion of the Vandermonde matrix $V_k$:
\begin{equation}
    u_{1:k} = V_k^+ x_k. 
\end{equation}
Indeed, note that at the boundary setting $N=k$~(number of equations $=$ number of unknowns), the matrix $V_N$ is invertible since $\det(V_N) = \prod_{1\le i< j\le N}(\lambda_i-\lambda_j)\ne 0$  with probability one under e.g. uniform on the complex unit disk initialization (proposed in \cite{orvieto2023resurrecting}, see Lemma~\ref{lemma:sampling_exp} in the appendix).
%Since $x$ is $N$-dimensional regardless of the timestamp, we want to have a procedure for reconstructing $k$ given the components of $x$. 
\paragraph{Reconstruction of the timestamp.} At the linear RNN output, for each timestamp $k$, one is given the corresponding hidden state $x$ to be used to retrieve $u_{1:k}$. The estimated $k$ will be then used to set the Vandermonde matrix and get $u_{1:k}$. Fortunately, architectures such as S4, S5 and LRU include an encoding layer before the linear RNN core. Let us consider an embedding such that $u_k$ is mapped to $(1, u_k)$ for all $k$. Let $B = ((1,0,0,\dots, 0),(0,1,1,\dots,1))^\top\in\R^{(N+1)\times M}$, we get $x_k  =\left(\sum_{i=0}^{k-1}\lambda_0^i, \ \tilde x_k\right)$, where $\tilde x_k$ follows eq.~\eqref{eq:vandermonde}. To reconstruct $k$ from $\tilde x_k$ note that $x_{k,0}=\sum_{i=0}^{k-1}\lambda_0^i = \frac{\lambda_0^k-1}{\lambda_0-1}$ so, $k = \log_{\lambda_0}(x_{k,0}(\lambda_0-1)+1)$. 
%\razp{Btw, if we did not enforce the recurrent weights to be diagonal, that this could have been folded in the recurrent weight. I.e. even if you do not have the linear encoder in Fig 1, you can say that you write the recurrence as a product of two matrices, one being B the other the diagonal. That is if we want to talk about linear RNN in general. We could make a note like \emph{Note that in the more generic case, when $\Lambda$ is non-diagonal, we do not need an explict matrix $B$, as we can write $\Lambda = \Lambda' B$}}\antonio{Correct, but the new $\Lambda$ would not be a square matrix! Or an I missing something?}

\paragraph{Taming ill-conditioning with complex numbers.} From the Vandermonde determinant formula, it is clear that $V_k$ is likely ill-conditioned under random initialization, which makes it hard to implement the inversion in practice. However, while if $\Lambda$ is real the condition number of $V_k$ grows exponentially with $N$~\cite{gautschi1987lower}, in the complex case it is possible to make the condition number of $V_k$ exactly 1 by e.g. choosing the $N$th-roots of unity as eigenvalues of $\Lambda$~\cite{gautschi1975optimally,cordova1990vandermonde}. This fact provides a \textit{precise justification for the use of complex numbers in the recurrent computation}. We explore this topic experimentally in  App.~\ref{app:rec_vandermonde}, where we also show performance increase in reconstruction of MNIST digits~\cite{lecun1998mnist} when we move initialization close to the boundary of the unit disk~(see Fig.~\ref{fig:MNIST_rec_random_van_main}), as observed in practice in all recent works on state-space models~\cite{gu2021efficiently,gupta2022simplifying,smith2022simplified, orvieto2023resurrecting}. We note that the discussion here opens up interesting directions for future investigations, such as the derivation of an optimal $\Lambda$ for reconstruction using the Vandermonde inverse. For the time being, from our experiments in~\S\ref{app:rec_vandermonde}, we conclude that a setting which safely leads to perfect reconstruction without ill-conditioning is, empirically, $N\gg L$ with initialization of $\Lambda$ close to the unit disk. 

% Figure environment removed


\subsection{Role of sparsity in a data-dependent basis}
\label{sec:sparsity}
%\razp{I would rephrase saying, lets turn to the more challenging scenario where we do not rely on $N\to\infty$, but we rather have $N < L$, which is a more realistic setting.}\antonio{ok done!}

While the set $(V^+_k)_{k=1}^L$ is numerically stable only for $N$ considerably larger than $L$, we note that it is not surprising that, in order to memorize a sequence of $L$~(one-dimensional) inputs one needs a hidden state which is at least as big as $L$. From an information theory perspective, one can only have\footnote{In modern RNN models~\citep{orvieto2023resurrecting, gu2021efficiently, smith2022simplified} we indeed have $N<L$.} $N<L$ if the set of inputs is a structured subset of $\R^L$ -- i.e. only if the input can be compressed. It is useful to note that, in the setting of the last subsection, the role of the RNN has mainly been ensuring no information is lost: the final hidden state provides a direct copy of the inputs, with no further reasoning. Towards fixing this, leveraging ideas from sparse coding, compressed sensing~\cite{candes2006stable}, and echo state networks~\cite{jaeger2004harnessing}, we introduce the following assumption. \vspace{-1mm}
\begin{assumption}
Consider a proper collection of ordered basis matrices $(\psi^i)_{i=1}^P$, with $\psi^i\in\R^{M\times L}$ for all $i=1,2,\cdots, P$. Let $u = (u_{:,1},u_{:,2},\cdots, u_{:,L})\in\R^{M\times L}$, be a sample sequence from the input sequence distribution. For any $k\in\{1,2,\dots,L\}$ one can always write $u_{:,1:k} = \sum_{i=1}^P \alpha^{u,k}_i\psi^i_{:,1:k}$, where $\psi^i_{:,1:k}$ are the first $k$ columns of $\psi^i$, and $(\alpha^{u,k}_i)_{i=1}^P$ is a sequence of real numbers. We do not assume such decomposition is unique, nor we assume we have access to the basis functions.
\label{ass:sparse}
\end{assumption}
\vspace{-1mm}
The idea behind leveraging this representation is that one can assume $P\ll L$, and reduce the task of estimating $u$ from $x$, to the task of estimating $\alpha$ from $x$. In formulas, since for every $k\in\mathbb{N}$, $u_{1:k} = \Psi_k \cdot \alpha^{u_{1:k}}$, with $\Psi_k\in\R^{k\times P}$ and $\alpha^{u_{1:k}}\in\R^P$, we have
\begin{equation}
    \tilde x_k  = V_k u_{1:k} = \underbrace{V_k \cdot \Psi_k}_{\Omega_k}\cdot \alpha^{u_{1:k}},
    \vspace{-2mm}
\end{equation}
where $\Omega_k\in\C^{N-1\times P}$. Under the assumption that $\Omega_k$ has column rank greater than row rank, i.e. we need $N>P+1$, we are be able to solve the system for $\alpha^{u_{1:k}}$. As before, since $x_{k,0}$, the first coordinate of $x_k$, can be used to recover $k$, the procedure above gives a unique way to recover an arbitrary-length signal $u$ \textit{sparse in the basis} $\Psi$, through a simple operation on the linear RNN output.
\vspace{-2mm}
\paragraph{$\Psi$ can also be learned.} One does not need to assume a specific $\Psi$ a priori: as long as we have input sparsity, one can guarantee approximation with a linear operator $\Omega_k^+$ on the state $x_k$, which can be learned and may be task-dependent. Note that even if the input is strictly-speaking not sparse, not all information might be relevant for prediction. In a way, sparsity mathematically quantifies the belief that the information content relevant in a certain task has fewer bits than the original signal.

\vspace{-2mm}
\paragraph{Sparseness improves conditioning.} In the last section, we discussed ill conditioning of $V_k$. We show with a simulation in the appendix~(Fig.~\ref{fig:better_conditioning}) that if $P\ll L$ then for the matrix $\Omega_k$ we have no computational issues when calculating its pseudo-inverse~(reconstruction map is ``easy'').

\vspace{-2mm}
\paragraph{Multidimensional setting.} We provide a discussion for the multidimensional setting in Appendix~\ref{app:multidim_rec}.




%\razp{Isn't there room to expand Assumption A (for ICLR submission) to infinite length sequences, and then really talk about dynamical systems? I.e. you make a basis of functions (almost like a Fourier Transform perspective) and you decompose your signal in that basis. And as long as that basis is small, you just need to figure out the coeficients and have some kind of counter, and the MLP can do everything else.}\antonio{Yes, one in general can have infintite sequences! Let's talk more about it. Fourier or any other basis would be fine.}


\section{Role of the MLP}
\label{sec:MLP}
In the last subsection, we showed how linear recurrences can be used as compression algorithms, with guarantees of perfect reconstruction under suitable assumptions on the RNN size and information content in the input. Mathematically, in the settings described in the last subsection, there exists a function $\gamma$ such that $x_k\overset{\gamma}{\to}((u_i)_{i=1}^k,k)$ for all $k=1,2,\dots ,L$.

Then, intuitively, since we can perfectly reconstruct the input sequence from $x_k$, we can let the MLP parametrize any non-linear function on this subsequence, indexed by $k$. In formulas:
\begin{equation}
    y_k = T_k[(u_i)_{i=1}^k] = \bar T((u_i)_{i=1}^k,k) = \bar T(\gamma(x_k)) = \phi(x_k).
    \label{eq:mlp}
\end{equation}
We can then let $\phi$ be parametrized by an MLP. Indeed, as it is well known~(App.~\ref{app:rw}), as the MLP size grows, it can approximate arbitrary non-linear map on compact sets~(in our case, $\bar T\circ\gamma$). To make the argument behind eq.~\eqref{eq:mlp}~(see App.~\ref{sec:mlp_proof}) precise, we need the following assumption.

\begin{assumption}
The input sequences $(u_i)_{i=1}^L$ live on a compact set $U\subset\R^{M\times L}$. The maps $T_k:\R^{M\times k}\to\R^M$, producing the output sequence $(y_i)_{i=1}^L$ as $y_k = T_k[(u_i)_{i=1}^k]$, are continuous on $U$.
\label{ass:system}
\end{assumption}
We can then summarize the findings of this paper in an informal way as follows.
\begin{theorem}[Informal]
Assume finite-length sequence data is generated from a sequence-to-sequence model such that Assumption~\eqref{ass:system} holds. Consider a randomly initialized linear recurrent network of size $N$, with $N$ large enough depending on the sparseness of the input set~(see Assumption~\eqref{ass:sparse} and discussion on initialization in \S\ref{sec:sparsity}). Then, there exists a wide enough MLP which, if applied pointwise to the RNN hidden states, leads to perfect reconstruction of the system's output. 
\end{theorem}
We provide a numerical validation of our claim on a non-linear controlled Lotka-Volterra system in App.~\ref{app:mlp_exp}. Our current research objective is to provide a non-asymptotic version of the theorem above~(potentially in the deep setting), as well as more thorough experimental evaluations, addressing more precisely potential limitations and issues such as bad conditioning~(see \S\ref{sec:vandermonde}).

% \section{Conclusion}
% In this short paper, we proved that a single linear RNN layer, if sufficiently wide, can store all of the information contained in a finite length input sequence. Randomly initialized complex diagonal linear recurrences can therefore be seen as compression algorithms, which efficiently learn relevant features in the input sequence, such as sparse representation coefficients. We then showed that general sequence-to-sequence maps can be parametrized by MLPs, acting on the recurrent model hidden state. While our paper raises many interesting questions for more precise investigations, we believe our findings already support the excellent performance these models achieve in practice.

\bibliography{sample}
\newpage
\appendix
\section{Approximation theory of MLPs and RNNs}
\label{app:rw}

We recall a few useful well-known results on universality of MLPs and non-linear RNNs.


\begin{theorem}[Universality of MLPs with one hidden layer --- Pinkus' Theorem~\cite{pinkus1999approximation}]. Consider the set
$$\mathcal{M}(\sigma)=\text{span}\{\sigma(w^\top x - \theta): \theta\in\mathbb{R}, w\in\mathbb{R}^n\},$$
where $\sigma:\R\to\R$ is a continuous non-linearity. Then, as long as $\sigma$ is not polynomial~(e.g. it is a ReLU or sigmoid), for any continuous $\bar f\in\R^n\to\R$, any compact set $K\subset\R^n$, and any $\epsilon>0$, there exists $f\in\mathcal{M}(\sigma)$ such that $\max_{x\in K}|f(x)-\bar f(x)|\le \epsilon$.
\label{thm:pinkus}
\end{theorem}

It is easy to realize that this result directly implies that one-hidden-layer MLPs can also approximate any non-linear continuous function $f:\R^n\to\R^m$ on compacts, as the width grows to infinity. This result can be used to show universality of non-linear RNNs for the task of modeling non-linear dynamical systems. The most powerful result in this line of research is formulated in continuous-time.

\begin{theorem}[Universality of non-linear RNNs~\cite{hanson2020universal}]
Consider the continuous-time non-linear dynamical system with form
\begin{equation}
    \dot{\bar{x}}(t) = \bar f(\bar x(t), u(t)),\quad \bar y(t) = h(\bar x(t)),
    \label{eq:non-linear-RNN-ct}
\end{equation}
with $\bar x(t)\in\R^{\bar N}$, $u(t)\in\R^M$. Under some technical assumptions~(bounded input, non-chaotic $f$), for any $\epsilon>0$ there exists a non-linear RNN
\begin{equation}
    \dot x(t) = -\frac{1}{\tau} x(t) + \sigma(A x(t) + Bu(t)), \quad y(t) = Cx(t),
    \label{eq:non-linear-RNNapprox-ct}
\end{equation}
for some non-polynomial $\sigma$, $\tau>0$, $A\in\R^{N\times N}$, $B\in\R^{N\times M}$, $C\in\R^{M\times N}$ that approximates the solution of equation~\eqref{eq:non-linear-RNN-ct} up to error $\epsilon$ uniformly in time, on compact sets of inputs.
\end{theorem}
The result above typically involves taking $N$~(RNN width) to infinity.

\begin{proof}
We briefly outline the idea behind the proof, and invite the reader to refer to~\cite{hanson2020universal} for details. Approximating the solution to equation~\eqref{eq:non-linear-RNN-ct} is equivalent to approximating the infinitesimal solution generator, which is a non-linear function of $(x, u)$. By Pinkus's theorem~(Thm.~\ref{thm:pinkus}), this generator can be approximated by a one-layer MLP, that is exactly the right-hand side of equation~\eqref{eq:non-linear-RNNapprox-ct}.
\end{proof}

Simply taking out the non-linearity from the recurrence in equation~\eqref{eq:non-linear-RNNapprox-ct} restricts the function class to convolutions. To start, recall that the linear continuous-time RNN on one-dimensional inputs
\begin{equation*}
    \dot x(t) = A x(t) + B u(t), \quad y(t) = Cx(t),
\end{equation*}
with $A\in\R^{N\times N}$, $B \in\R^{N\times M}$, $C \in\R^{1\times N}$
has solutions given by a convolution.
\begin{equation*}
    x(t) = \int_0^t C^\top e^{As}B u(t-s) ds =: \int_0^t \rho(s)^\top u(t-s) ds =: \sum_i(\rho^i\star u^i)_t.
\end{equation*}
Let us call $\hat{\mathcal{H}_N}$ the class of functionals parametrizable with linear RNNs with hidden state of dimension $N$, and $\hat{\mathcal{H}} = \cup_{N\in\mathbb{N}_+}\mathcal{H}_N$. 
\begin{equation*}
    \mathcal{H}_N :=\left\{ \{H_t
: t \in \R\}, H_t(u) = \int_{0}^{t}C^\top e^{As} Bu(t-s) ds, C\in\R^{1\times N}, A\in\R^{N\times N}, B\in \R^{N\times M}\right\}.
\end{equation*}
It turns out that convolutions are dense in the class of linear functionals. Let $\mathcal{U} = C_0(\R,\R^d)$ with norm $\|u\| = \sup_{t\in\R} \|u(t)\|_\infty$.
\begin{theorem}[Linear functionals in $C_0(\R,\R^d)$ are convolutions \cite{li2022approximation}]
Let $\{H_t
: t \in \R\}$ be a
family of continuous, linear, causal, regular, and time-homogeneous functionals on $\mathcal{U}$, i.e. such that
\begin{enumerate}
    \item (Continuous)  $\forall t\in\R$, $\sup_{\|u\|<1} H_t(u)<\infty$.
    \item (Linear) $\forall t\in\R$, $u,v\in \mathcal{U}$ and $\nu,\lambda\in \R$, we have $H_t(\lambda u + \nu v) = \lambda H_t(u) + \nu H_t(v)$.
    \item (Causal) For all $u,v\in\mathcal{U}$ such that $u(s)=v(s)$ for all $s\le t$, he have $H_t(v)=H_t(u)$.
    \item (Regular) Let $(u^n)$ be a sequence in $\mathcal{U}$ s.t. $u^n(s)\to 0$ for almost every $s\in\R$, then, for all $t\in\R$, we have $\lim_{n\to\infty} H_t(u^n)=0$.
    \item (Time Homogeneous) For all $u\in \mathcal{U}$ let $u^{\tau}_t = u(t-\tau)$, then $H_{t}(u^{\tau}) = H_{t+\tau}(u)$. 
\end{enumerate}
Then, for any $\{H_t
: t \in \R\}$ there exist a function (a kernel) $\bar \rho:\R_+\to\R^M$ such that for all $t\in\R$.
\begin{equation*}
    H_t(u) = \int_{0}^\infty \rho(s)^{\top} u(t-s) ds = \sum_i(\bar \rho^i\star u^i)_t.
\end{equation*}
\end{theorem}


\begin{theorem}[Linear RNNs can parametrize any convolution \cite{li2022approximation}]
Let $\{H_t
: t \in \R\}$ be a
family of continuous, linear, causal, regular, and time-homogeneous functionals on $\mathcal{U}$. Then,
for any $\epsilon > 0$ there exists $\{\hat H_t: t \in \R\}\in\hat{\mathcal{H}}$ such that
$$
    \sup_{t\in\R}\sup_{\|u\|<1} \|H_t(u)-\hat H_t(u)\|\le \epsilon.
$$
\end{theorem}
The result above typically involves taking $N$~(RNN width) to infinity.

\section{Reconstruction of sparse multidimensional inputs}
\label{app:multidim_rec}

As initialization of $\Lambda$, we are going to make use of the following lemma from~\cite{orvieto2023resurrecting},
\begin{lemma}
 Let $u_1,u_2$ be independent uniform random variables on the interval $[0,1]$. Let $0\le r_{\min}\le r_{\max}\le1$. Compute $\nu = -\frac{1}{2}\log\left(u_1(r_{\max}^2-r_{\min}^2)+r_{\min}^2\right)$ and $\theta = 2\pi u_2 $. Then $\exp(-\nu+i\theta)$ is uniformly distributed on the ring in $\mathbb{C}$ between circles of radii $r_{\min}$ and $r_{\max}$. We denote this distribution as $\mathbb{T}(r_{\min}, r_{\max})$.
 \label{lemma:sampling_exp}
 \end{lemma}

Assumption~\ref{ass:sparse} implies the result below, which is complemented by the simulation in Fig.~\ref{fig:prop1}\&\ref{fig:prop2} in the appendix. Fig.~\ref{fig:architecture} illustrates the result through a concrete example on the MNIST dataset.
\begin{proposition}[LRU memory] Let $M$ dimensional input sequences $u\in\R^{M\times L}$, with $L\le L_{\max}$ be sampled from a distribution $\mathcal{U}$. Assume that $\mathcal{U}$ is such that Assumption~\ref{ass:sparse} holds true almost surely. Let $E:\R^{M}\to\R^{H}$, $H=M+1$, be an affine position-wise encoder that appends the feature $1$ for all multidimensional inputs: $u\overset{E}{\to}\tilde u$. Let then $\tilde u\in\R^{H\times L}$ be the input to the following recursive LRU computation: $x_k = \Lambda x_{k-1} + B\tilde u_{:,k}$, with $B\in \C^{N\times H}$ a specific matrix~(see Eq.\eqref{eq:matrix_B}), $\Lambda=\diag(\lambda_1,\lambda_2,\dots, \lambda_N)$ and each $\lambda_i\sim\mathbb{T}(r_{\min}, r_{\max})\subseteq\C$ according to Lemma~\ref{lemma:sampling_exp}. If $N $ big enough but still way smaller than $L_{\max}$
%(~\antonio{To be precise is a bit hard, working on it})
, there exists a linear map that from each $x_k$~(seen as a generic vector in $\C^N$, with no access to its index) can recover the coefficients $\alpha^{u,k}$ for the decomposition $u_{:,1:k} = \sum_{i=1}^P \alpha^{u,k}_i\psi^i_{:,1:k}$, with probability one. Moreover, there exists a non-linear continuous map that from $x_k$ can recover $k$.
\label{prop:approx}
\end{proposition}


\begin{proof}
Since, while making use of Assumption~\ref{ass:sparse}, we want to write $u$ as a matrix multiplication of basis functions times coefficients, it is useful to work with vectorized quantities. Let us define
\begin{equation}
    \vect(u_{:,1:k}) := \begin{pmatrix}
u_{1,1:k}^\top \vspace{1mm}\\
 u_{2,1:k}^\top \\
 \vdots \\
 u_{M,1:k}^\top
    \end{pmatrix}\in\R^{kM},
\end{equation}
Next, let us define the matrix
\begin{equation}
    \Psi_k := (\vect(\psi^1_{:,1:k}),\vect(\psi^2_{:,1:k}),\cdots, \vect(\psi^P_{:,1:k}))\in\R^{kM\times P}.
\end{equation}
Then, Assumption~\ref{ass:sparse} implies
\begin{equation}
    \vect(u_{:,1:k}) = \Psi_k \alpha^{u,k}
\end{equation}
where $\alpha^{u,k} = (\alpha_1^{u,k}, \alpha_2^{u,k},\cdots, \alpha_P^{u,k})^\top\in\R^{P\times 1}$, for all $k$. Note that the coding vector $\alpha$ has fixed size, independent of $k$.

We assume the encoder $E:\R^{M}\to\R^{H}$, $H=M+1$, preceding the linear recurrent unit~(LRU) already appended the feature $1$ positionwise to each multidimensional input, such that $\tilde u_{:,i} = (1,u_{:,i})\in\R^{H}$ for all $i\in\{1,2,\dots,L\}$ --- we will use this dummy feature to recover the inner state position in the sequence. We denote by $\tilde u = (\tilde u_{:,1}, \tilde u_{:,2},\cdots, \tilde u_{:,L})\in\R^{H\times L}$ the LRU input. 
\begin{equation}
    \tilde u =
    \begin{pmatrix}
    1 & 1 & 1&\cdots& 1\\
    u_{1,1} &u_{1,2} & u_{1,3}&\cdots&u_{1,L}\\
    \vdots&\vdots & \vdots&\ddots&\vdots\\
    u_{M,1} &u_{M,2} & u_{M,3}&\cdots&u_{M,L}\\
    \end{pmatrix}
\end{equation}
All in all, we can write
\begin{equation}
    \tilde u_{:,1:k} = (1_{k\times 1}, u_{:,1:k})^\top,\quad \vect(u_{:,1:k}) = \Psi_k \alpha^{u,k}
\end{equation}

The LRU is $x_k = \Lambda x_{k-1} + B \tilde u_k$, where $x_k\in\R^N$ and $\tilde u_k\in\R^{H}$. We pick $N = RM+1$~(where $R>0$ we will fix later), and the following input projection $B\in\C^{N\times H}$:
\begin{equation}
    B = \begin{pmatrix}
1& 0 & \cdots & 0 & 0\\
0 & 1_{R\times 1} & \cdots & 0_{R\times 1} & 0_{R\times 1}\\
0 & 0_{R\times 1} & \cdots & 0_{R\times 1} & 0_{R\times 1}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0_{R\times 1} & \cdots & 1_{R\times 1} & 0_{R\times 1}\\
0 & 0_{R\times 1} & \cdots & 0_{R\times 1} & 1_{R\times 1}\\
\end{pmatrix}.
\label{eq:matrix_B}
\end{equation}
The result of the LRU computation is a block diagonal version of the Vandermonde multiplication we had in the simplified setting
\begin{equation}
    \tilde x_k  =
    \begin{pmatrix}
  \sum_{i=0}^{k-1}\lambda_0^i
  & \rvline &  & \rvline & & \rvline & &  \rvline & \\
\hline
& \rvline & V_{k,1} & \rvline & & \rvline & &  \rvline & \\
\hline
  & \rvline &  &\rvline & V_{k,2}& \rvline & &  \rvline & \\
\hline
  & \rvline &  &\rvline & & \rvline & \ddots &  \rvline & \\
\hline
  & \rvline &  &\rvline & & \rvline && \rvline  &  V_{k,M} \\
\end{pmatrix}
    \begin{pmatrix}
    1\\
    \vspace{1mm}
u_{1,1:k}^\top \\
    \vspace{1mm}
 u_{2,1:k}^\top \\
 \vdots \\
 u_{M,1:k}^\top
    \end{pmatrix} = \tilde V_k  \begin{pmatrix}1\\
    \vect(u_{:,1:k})\end{pmatrix},
\end{equation}
where $\tilde V_k=\diag(1,V_k)$, $V_k = \diag(V_{k,1},V_{k,2},\cdots,V_{k,M})$, and $V_{k,j}$ is the Vandermonde matrix of width $k$ corresponding to the block $\Lambda_j$ in the diagonal matrix $\Lambda$.
\begin{align}
&\Lambda = \diag(\lambda_0, \Lambda_1,\Lambda_2,\cdots,\Lambda_M)\in\C^{(RM+1)\times(RM+1)},\\
    &\Lambda_j = \diag(\lambda_{1,j},\lambda_{2,j},\cdots,\lambda_{R,j})\in\C^{R\times R},\\
    &V_{k,j} = \begin{pmatrix}
    \lambda_{1,j}^{k-1}&\lambda_{1,j}^{k-2} &\cdots& \lambda_{1,j}&1\\
    \lambda_{2,j}^{k-1}&\lambda_{2,j}^{k-2} &\cdots& \lambda_{2,j}&1\\
    \vdots&\vdots &\ddots&\vdots&\vdots\\
    \lambda_{R,j}^{k-1}&\lambda_{R,j}^{k-2} &\cdots& \lambda_{R,j}&1\\
    \end{pmatrix}\in\C^{R\times k}.
\end{align}
We use Assumption~\ref{ass:sparse} to write
\begin{equation}
    x_k =  \tilde V_k \begin{pmatrix}1\\
    \vect(u_{:,1:k})\end{pmatrix} = \begin{pmatrix}\sum_{i=0}^{k-1}\lambda_0^i\\
     (V_k \Psi_k) \alpha^{u,k}\end{pmatrix},
\end{equation}
The timestamp $k$ can be recovered in a trivial way, under the assumption that $\lambda_0\in\C$ is such that $|\lambda_0|\le 1$:
\begin{equation*}
    x_{k,0}=\sum_{i=0}^{k-1}\lambda_0^i = \frac{\lambda_0^k-1}{\lambda_0-1}\quad\implies\quad k = \log_{\lambda_0}(x_{k,0}(\lambda_0-1)+1).
\end{equation*}
Next, we show how to recover the coding coefficients $\alpha^{u,k}$. Let us define
\begin{equation}
    \Omega_k = V_k\Psi_k,\qquad\text{such that}\quad x_k = \Omega_k \alpha^{u,k}.
\end{equation}
Now, to get $\alpha^{u,k}$ one can compute a~(left) pseudoinverse
\begin{equation}
    \alpha^{u,k} = \Omega_k^+ x_k.
\end{equation}
Since $\Omega_k\in\C^{(N-1)\times P}$, with $N = RM+1$, for the system to be overdetermined, a necessary condition is $N-1=RM > P$, i.e $R\ge \lceil P/M\rceil$. To be able to recover $\alpha^{u,k}$, we need $\Omega$ to have row rank bigger or equal to the column rank. In practice, for numerical feasibility, one needs $\Sigma_k$ not to be ill-conditioned. In Figures~\ref{fig:prop1},\&~\ref{fig:prop2} we show this is achievable by using $\Lambda$ initialized very close to the unit disk, i.e. under the LRU initialization. Under the assumption that $\Omega_k$ is not too ill-conditioned, one can perfectly recover $u_{1:k}$ with a linear map $\vec(u_{1:k}) = \Psi_k\Omega_k^+ x_k$.

\end{proof}



\section{Proofs for the MLP}
\label{sec:mlp_proof}
The following lemma reduces the problem of learning $L$ maps $T_1,\dots, T_L$ to that of learning a single map $\bar T$ on a compact subset of $\R^{ML +1}$. In essence, we are going to put the index $k$ as input, and relax the domain to a compact subset of the Euclidean space by leveraging causality.
\begin{lemma}[Domain Relaxation]
Under Assumption~\ref{ass:system}, there exist a \textbf{continuous} map $\tilde T$ on the compact $\left(U\times [1,L]\right)\subset\R^{M\times L}\times\R\equiv\R^{ML+1} $ which for any $(u_i)_{i=1}^L\in U$ and any desired $k\in\{1,\dots,L\}\subset [0,L]$, produces the desired output $y_k$:
\begin{equation}
    \bar T[(u_i)_{i=1}^L, k] = T_k[(u_i)_{i=1}^L],\qquad \forall k\in\{1,2,\dots, L\}.
\end{equation}
Moreover $\tilde T$ is causal, that is, it has the property that if $(v_i)_{i=1}^k = (u_i)_{i=1}^k$ then $\bar T[(u_i)_{i=1}^L, t] = \bar T[(v_i)_{i=1}^L, t]$ for any $t\le k$.
\label{lemma:domain_relax}
\end{lemma}

The proof is presented in the appendix. The main result of this paper directly follows from the lemma above using Pinkus Theorem~(Thm~.\ref{thm:pinkus}), applied to the output of the RNN, which is expressive enough thanks to Proposition~\ref{prop:approx}.

\begin{proof}
Note that one can write
\begin{equation}
    y_k = \tilde T[(u_i)_{i=1}^L, k],
\end{equation}
where $\tilde T:\left(\R^{(M\times L)}\times \{1,2,\dots,L\}\right)\to\R^M$ is such that $\tilde T[(u_i)_{i=1}^L, k] = T_k[(u_i)_{i=1}^k]$ for all $u\in U$. More precisely, $\tilde T[\cdot,k]:\R^{M\times L}\to\R^M$ is the trivial extension of $T_k:\R^{k\times L}\to\R^M$ to cover the whole $\R^{M\times L}$.

Further, note that the map $\tilde T$ has domain $\left(\R^{(M\times L)}\times \{1,2,\dots,L\}\right)\subset \left(\R^{(M\times L)}\times \mathbb{N}\right)$. In addition, $\tilde T$ is causal: for all $k\in\{1,2,\dots, L\}$
\begin{equation}
   (v_i)_{i=1}^k = (u_i)_{i=1}^k\implies \bar T[(u_i)_{i=1}^L, k] = \bar T[(v_i)_{i=1}^L, k].
\end{equation}


To talk about continuity, one needs to provide an extension of this map to $\left(\bar \R^{(M\times L)}\times [1,L]\right)\subseteq(\R^{M\times L}\times\R)$. It is fortunately trivial to do this, by definining
\begin{equation}
    \tilde T[(u_i)_{i=1}^L, t] := (\lceil t\rceil-t)\bar T[(u_i)_{i=1}^L, \lfloor t\rfloor] + (t-\lfloor t\rfloor)\bar T[(u_i)_{i=1}^L, \lceil t\rceil].
\end{equation}
This concludes the proof of the lemma.
\end{proof}
We are now ready to present the formal argument behind eq.~\ref{eq:mlp}. At step $k$ of the RNN output, the MLP ~(which needs to be the same for each token, i.e. for each $k$) is given $\tilde x_k = (x_{k,0},x_k)\in\R^{N}$, which contains the RNN hidden state $x_k$ as well as the positional encoding feature $x_{k,0}$. We want to construct a continuous mapping $\tilde x_k\to y_k$. Our strategy is to combine Proposition~\ref{prop:approx} with Lemma~\ref{lemma:domain_relax}: first we reconstruct the input from the hidden state, and then we map this result to the output.
\begin{equation*}
    (x_{k,0},x_k)\quad\overset{f_1}{\underset{\text{Prop.~\ref{prop:approx}}}{\longmapsto}}\quad ((u_i)_{i=1}^L,k)\quad\overset{\tilde T}{\underset{\text{Lemma.~\ref{lemma:domain_relax}}}{\longmapsto}}\quad y_k
\end{equation*}
We would like $f(\cdot):=\tilde T(f_1(\cdot))$ to be continuous. 


\paragraph{Step 1: Causality gives flexibility in the reconstruction.} We know by Lemma~\ref{lemma:domain_relax} that there exist a continuous map $\tilde T$ on the compact $\left(U\times [1,L]\right)\subset\R^{M\times L}\times\R\equiv\R^{ML+1} $ which for any $(u_i)_{i=1}^L\in U$ and any desired $k\in\{1,\dots,L\}\subset [0,L]$, produces the correct output $y_k$. Moreover, now crucially, $\tilde T$ is causal, that is, it has the property that if $(v_i)_{i=1}^k = (u_i)_{i=1}^k$ then $\bar T[(u_i)_{i=1}^L, t] = \bar T[(v_i)_{i=1}^L, t]$ for any $t\le k$. This implies that, for reconstruction at step $k$, we can feed in $\tilde u_k = (u_{:,1:k},a_{:,k+1:L})\in\R^{M\times L}$, where $a_{:,k+1:L}\in\R^{M\times(L-k)}$ is a vector of arbitrary numbers, and we have $\bar T[(u_i)_{i=1}^L, t] = \bar T[(\tilde u_i)_{i=1}^L, t]$. This will become very useful in the next step. Schematically, what we need is
\begin{equation*}
    (x_{k,0},x_k)\quad\overset{f_1}{\underset{\text{Prop.~\ref{prop:approx}}}{\longmapsto}}\quad ((u_i)_{i=1}^k\cup(a_i)_{i=k+1}^L,k) \quad\overset{f_2}{\underset{\text{Lemma.~\ref{lemma:domain_relax}}}{\longmapsto}}\quad y_k
\end{equation*}

\paragraph{Step 2: Fixed-size input reconstruction.} By Proposition~\ref{prop:approx}, leveraging Assumption~\ref{ass:sparse}, there exist~(for big enough $N$) a mapping from the linear RNN output at time step $k$ which is able to recover precisely the first $k$ inputs $u_{:,1:k}$ with the following procedure, which we then would need to translate to a continuous mapping on a fixed~($k$-independent) compact domain:
\begin{enumerate}
    \item First, we can use the first coordinate of the hidden state to recover the index $k$:
    \begin{equation}
    k = \log_{\lambda_0}(x_{k,0}(\lambda_0-1)+1).
    \end{equation}
    \item Then, given the index $k$, there exist matrices $\Psi_k\in\R^{Mk\times P}$ and $\Omega_k^+\in\C^{P\times (N-1)}$~(see proof of Prop.~\ref{prop:approx}) such that
    \begin{equation}
        \vect(u_{:,1:k}) = \Psi_k\Omega_k^+ x\in\R^{Mk}.
    \end{equation}
    \item In the last step , we are not restricted to recovering the first $k$ columns of $u$: we can instead compute 
\begin{equation}
    \vect(\tilde u_k) = \Psi_L\Omega_k^+ x\in\R^{ML},
\end{equation}
and note that by definition of $\Psi$, we have that $\tilde u_{:,1:k}=u_{:,1:k}$. That is, given the coding coefficients recovered from the first $k$ inputs, we can actually provide an estimate of the entire signal. We have no hope this reconstruction is precise, but we actually no not care. Indeed, the result is of the type $(u_{:,1:k},a_{:,k+1:L})$ --- where we just need the first $k$ columns to be correct. 
\end{enumerate}
We successfully provided a fixed-dimension reconstruction which one can feed into the mapping $\tilde T$ of Lemma~\ref{lemma:domain_relax}. As a last step, we need to relax the reconstruction mapping to be continuous.

\paragraph{Last step: making the reconstruction continuous.} First, we define the variables 
\begin{align}
    &t := \log_{\lambda_0}(x_{k,0}(\lambda_0-1)+1)\in\R,\\
    &\Omega_t^+ := (\lceil t\rceil-t)\Omega_{\lfloor t\rfloor}^+ + (t-\lfloor t\rfloor) \Omega_{\lceil t\rceil}^+.
\end{align}
It is clear that when considering $t=k$ the quantities above reduce to the ones from the previous step. However, the above reasoning implies existence of a continuous reconstruction mapping. The proof is then concluded by use of Pinkus Theorem~(Thm.~\ref{thm:pinkus}).

\section{Alternative definition for sequence-to-sequence models}
\label{app:alternative_def}
In the paper, we consider sequence-to-sequence models $(u_k)_{k=1}^L\to(y_k)_{k=1}^L$ generated as $y_k = T_k[(u_i)_{i=1}^k]$, where $(T_k)_{k=1}^L$ is a collection of functions. An alternative definition for a sequence-to-sequence model --- which we show to be a special case for our definition --- is the following nonlinear state-space model
\begin{equation}
    x_{k} = T(x_{k-1}, u_k),\qquad y_k = g(x_k),
    \label{eq:state-space}
\end{equation}
similar to the nonlinear ODE proposed in~\cite{hanson2020universal} to model the ground truth. Note that, applying the formula recursively, we have~(assuming $x_{-1}=0$). 
\begin{align}
    &y_{0} = g(T(0, u_0)):= T_0(u_0)\\
    &y_{1} = g(T(T(0, u_0), u_1)):=T_1((u_i)_{i=0}^1)\\
    &y_{2} = g(T(T(T(0, u_0), u_1), u_2)):=T_k((u_i)_{i=0}^2)\\
    &\dots
\end{align}
As one can see from the equations above, the generation is a special case of the definition we use in this paper.

\newpage
\section{Additional experiments}
\subsection{Reconstruction using the Vandermonde inverse~(support to \S\ref{sec:vandermonde})} 
\label{app:rec_vandermonde}
We consider linear diagonal RNNs with the $N$ diagonal entries of $\Lambda$ sampled inside the unit disk in $\C$, uniformly in angle in between radii $r_{\min}$ and 1. We consider the hidden state $x_L\in\C^N$ computed after $L$ RNN steps, \textit{i.e. after the sequence is completely processed}. We want to recover the input sequence from the hidden state using the Vandermonde inverse $V_L^+$~(see \S\ref{sec:vandermonde}). If $N\ge L$, since under random initialization the determinant of any set of $L$ columns of $V_L$ is positive, we can in theory achieve perfect reconstruction. In practice, $V_L$ is ill conditioned --- especially if $r_{\min}$ is not close to $1$. This causes some problem in the pseudoinverse computation, which may result in imperfect reconstruction. 

In Figure~\ref{fig:MNIST_rec_random_van} we provide evidence on the MNIST dataset~(flattened image $=$ 784 tokens): we observe that, if eigenvalues are sampled with $r_{\min}=0$, by multiplying $x_L$ by $V_L^+$ we are only able to recover recent history. Instead, moving closer to the unit disk allows almost perfect reconstruction at $N=784$, and a satisfactory result already at $N=512$.

% Figure environment removed

 In Figure~\ref{fig:MNIST_van_error_avg} we clearly show that the average reconstruction error~(average over $10k$ images samples and 10 random re-samplings of the linear RNN) is decreasing both as a function of the hidden state size~(see discussion in \S\ref{sec:vandermonde}) and of $r_{\min}$~($r_{\max}=1$). The same pattern is observed for the condition number of $V_L^\top V_L$. On the same figure, we show how the error is distributed over timestamps: it is clear that, for $r_{\min}\ll1$, the reconstruction only covers the last few hundreds of tokens -- a property which is liked to the bad condition number observed in this setting.

% Figure environment removed

 Last, in Figure~\ref{fig:MNIST_rec_unit_van} we show what happens when picking the $N$ diagonal entries of $\Lambda$ to be the $N$-th complex roots of $1$: as shown in~\cite{cordova1990vandermonde}, in this setting the Vandermonde condition number is $1$. We observe that we can indeed reconstruct perfectly the output for $N=784$. However, for smaller values of $N$, the reconstruction presents undesired artifacts.

% Figure environment removed

\subsection{Reconstruction under sparsity~(support to \S\ref{sec:sparsity})}
\label{app:rec_sparse}


In Figures~\ref{fig:prop1} and \ref{fig:prop2} we test the discussion in~\S\ref{sec:sparsity} in a controlled setting. We consider one-dimensional length $L=4096$ random inputs sparse in a basis of $P=32$ Haar wavelets~\cite{haar1911theorie}. The linear diagonal RNN has $\Lambda\in\C^{N\times N}$ with eigenvalues sampled uniformly at random from $\mathbb{T}(0.95,1)$~(Fig.~\ref{fig:prop1}) or $\mathbb{T}(0.99,1)$~(Fig.~\ref{fig:prop2}). We use matrix $B = (1,1,\dots, 1)^\top$. Plotted is the rank of $V_k,\Psi_k$, $\Sigma_k = V_k\Psi_k$ as $k$ increases~(see notation in \S\ref{sec:sparsity}). We show how the reconstruction error behaves when reconstructing $u_{1:k} = \Psi_k\alpha^{u,k}$ with $\alpha^{u,k} = \Omega_k^+ x_k$. In the figures, we plot the error for reconstruction of the tokens $(u_i)_{i=1}^k$ from $x_k$, for all $k\le L$. As $N$ gets bigger the reconstruction error gets uniformly negligible. In particular, if we initialize in $\mathbb{T}(0.95,1)$ then the minimum $N$ we need for perfect reconstruction of around $N=256$. If instead we initialize closer to the unit circle, then $N=64$ is enough for perfect reconstruction. This finding is similar to the one presented in~\S\ref{app:rec_vandermonde}. 

On a more fundamental level, we study the condition number of the matrix $\Omega^T\Omega$, where $\Omega = V_L\Psi_L$. This condition number quantifies the numerical stability of the pseudoinverse $\Omega^+$, used in reconstruction. In Figure~\ref{fig:better_conditioning}, we show the logarithm of the condition number for a sequence of length $512$ sparse in a basis of $P$ eigenfunctions. As $r_{\min}$ gets close to $1$ the condition number decreases as we saw in \S\ref{app:rec_vandermonde}. Crucially however, the condition number also decreases as $P$ decreases.


% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{MLP approximation}
\label{app:mlp_exp}
In this appendix we give experimental evidence for the claim in \S\ref{sec:MLP}: after the input is processed by a linear recurrence, it is possible to compute any sequence-to-sequence map through application of a point-wise MLP to the RNN hidden state. To this end, we consider inputs sampled from a 2-dimensional Fourier basis, and compute the outputs as Euler discretizations of the following Lotka-Volterra non-linear controlled ODE:
\begin{align}
      &\dot z_1(t) = z_1(t)(a-b z_2(t))\\
      &\dot z_2(t) = -z_2(t)(c-d z_1(t))+u(t).
\end{align}
We set the input length to $2048$ and use hyperparameters $a = 1, b = 0.6, c = 1, d = 0.3, z_1(0) = 1, z_2(0) = 0.5$ to uniquely define the ground truth under discretization with stepsize $0.0075$. To elaborate the input, we choose an RNN hidden state of dimension 128. We fix the linear RNN to random initialization following Lemma~\ref{lemma:sampling_exp} with $r_{\min}=0.99$, $r_{\max}=1$. For the MLP~(which we train), we define this to have 3 layers~(2 hidden layers) with 512 neurons each. For training the MLP, we sample $1024$ input-output trajectories and use $70\%$ for training anf $30\%$ for validation.


In Figure~\ref{fig:lv} we show example of input-output pairs as well as validation performance. As clear from the results, the MLP was able to translate the input tokens representations into the correct values of the output sequence. We note that the same MLP is applied to each timestamp, therefore the MLP is effectively implementing eq.~\eqref{eq:mlp}.

% Figure environment removed


\end{document}
