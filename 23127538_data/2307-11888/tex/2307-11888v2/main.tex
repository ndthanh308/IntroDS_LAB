%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\usepackage{enumitem}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2024}
\input{math_commands}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Universality of Linear Recurrences Followed by Non-linear Projections}

\begin{document}

\twocolumn[
\icmltitle{Universality of Linear Recurrences Followed by Non-linear Projections: \\
{Finite-Width Guarantees and Benefits of Complex Eigenvalues}}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Antonio Orvieto}{xxx}
\icmlauthor{Soham De}{yyy}
\icmlauthor{Caglar Gulcehre}{zzz}
\icmlauthor{Razvan Pascanu}{yyy}
\icmlauthor{Samuel L. Smith}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, Tübingen AI Center, Tübingen, Germany}
\icmlaffiliation{yyy}{Google Deepmind}
\icmlaffiliation{zzz}{EPFL}


\icmlcorrespondingauthor{Antonio Orvieto}{antonio@tue.ellis.eu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.28in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
\vspace{-0.9mm}
Deep neural networks based on linear complex-valued RNNs interleaved with position-wise MLPs are gaining traction as competitive approaches to sequence modeling. Examples of such architectures include state-space models~(SSMs) like S4, LRU, and Mamba: recently proposed models that achieve promising performance on text, genetics, and other data that requires long-range reasoning. Despite experimental evidence highlighting these architectures' effectiveness and computational efficiency, their expressive power remains relatively unexplored, especially in connection to specific choices crucial in practice -- e.g., carefully designed initialization distribution and use of complex numbers. In this paper, we show that combining MLPs with both real or complex linear diagonal recurrences leads to arbitrarily precise approximation of regular causal sequence-to-sequence maps. At the heart of our proof, we rely on a separation of concerns: the linear RNN provides a lossless encoding of the input sequence, and the MLP performs non-linear processing on this encoding. While we show that using real diagonal linear recurrences is enough to achieve universality in this architecture, we prove that employing complex eigenvalues near unit disk -- i.e., empirically the most successful strategy in SSMs -- greatly helps the RNN in storing information. We connect this finding with the vanishing gradient issue and provide experimental evidence supporting our claims.
\end{abstract}

\vspace{-6mm}

\textit{Note: The preliminary version of this manuscript~(v1, ICML workshop version) only contains a subset of the results we present here. Further, in combination of our current revision, we published a follow-up~\cite{cirone2024theoretical} on the expressive power of gated SSMs such as Mamba~\cite{gu2023mamba} and Hawk/Griffin~\cite{de2024griffin}. }

\vspace{3mm}


\section{Introduction}

Attention \citep{vaswani2017attention} has supplanted LSTMs \citep{hochreiter1997long} and GRUs \citep{cho2014learning} as the dominant sequence-to-sequence mechanism for deep learning. However, a promising line of research sparked by the S4 model \citep{gu2021efficiently} has initiated a come-back of recurrent sequence models in simplified \textit{linear} form. A growing family of `state-space' models (SSMs) \citep{gu2021efficiently,hasani2022liquid,smith2022simplified, gu2022parameterization,li2022makes, orvieto2023resurrecting,gu2023mamba} has emerged which interleave recurrent linear complex-valued\footnote{The performance of SSMs is greatly affected by the choice of number field: using complex diagonal recurrences greatly improves performance on long-range reasoning tasks. See discussion in~\cite{gu2022parameterization,orvieto2023resurrecting} and in particularly the S4D-Lin initialization.} layers with other components such as position-wise multi-layer perceptrons (MLPs), gates~\citep{dauphin2017language}, residual connections \citep{he2016deep}, and normalization layers \citep{ioffe2015batch, ba2016layer}. 

SSMs achieve state-of-the-art results on the long-range arena~\citep{tay2020long} and have reached outstanding performance in various domain including vision~\citep{nguyen2022s4nd}, audio~\citep{goel2022sashimi}, biological signals~\citep{gu2021efficiently}, reinforcement learning~\citep{lu2023structured} and online learning~\citep{zucchet2023online}. SSMs, possibly augmented with a selectivity mechanism, are also successfully applied to language~\citep{katsch2023gateloop,gu2023mamba,de2024griffin}, and are sometimes used in combination with softmax attention~\citep{fu2023hungry, wang2023pretraining} or linear attention~\citep{peng2023rwkv, sun2023retentive, katsch2023gateloop, yang2023gated}. They gained significant interest in the literature due to two key factors. First, their computational complexity scales linearly in sequence length, while transformers scale quadratically \citep{vaswani2017attention, katharopoulos2020transformers}. Second, unlike LSTMs and GRUs, training can be efficiently parallelized~\cite{martin2017parallelizing}.

% Figure environment removed

The success of these models is surprising, since it was previously widely thought that non-linearities within the recurrence are necessary for RNNs to model complex functions~\citep{schafer2006recurrent}. In this paper, we therefore step back and study the theoretical expressivity of models based on stacks of vanilla\footnote{For an analysis of selective SSMs such as Mamba~\cite{gu2023mamba} and Griffin~\cite{de2024griffin}, please check our follow-up~\cite{cirone2024theoretical}.} linear complex-valued RNNs interleaved with position-wise MLPs. In contrast to non-linear RNNs for which we have a multitude of expressivity results, from Turing-completeness~\citep{Chung21,Siegelmann92} to universality~\citep{schafer2006recurrent, hanson2020universal}, less is known for architectures relying on linear recurrences, with nonlinearities placed outside of the sequential processing.

\vspace{-5mm}
\paragraph{Results on SSMs expressivity.} The Mütz-Szász Theorem~\citep{muntz1914approximationssatz, szasz1916approximation} can be used to show that linear diagonal real-valued RNNs can approximate causal convolutions of the input with an arbitrary filter in the width limit~\citep{li2022approximation}. However, linear filtering alone cannot provide satisfactory approximations of non-linear causal sequence-to-sequence maps. Yet, interestingly, \citet{hanson2019universal} showed that staking an exponential~(in the sequence length) number of filters, chained with ReLUs, can approximate arbitrary time-homogeneous nonlinear causal operators. Recently, in the more realistic finite-depth regime of SSMs, \citet{wang2023state} proved that interleaving five linear RNNs with nonlinearities leads, in the width limit, to the approximation of any continuous mappings from an input sequence to a scalar target. This result uses the Kolmogorov-Arnold Theorem~\citep{kolmogorov1957representation}. Approximating nonlinear causal sequence-to-sequence mappings~(in the sense of Definition~\ref{def:seq}) is, of course, harder: \citet{wang2023state} provided an interesting connection with Volterra-series expansions~\citep{boyd1985fading} of time-invariant causal operators: multiplying the output of $K$ infinitely wide linear RNNs provides a $K$-th order approximation of smooth non-linear sequence to sequence mappings. While this result establishes a connection to the rich literature in dynamical systems, it concerns a specific architecture design strategy~(product of $K$ RNN outputs), which is far from SSMs practice. Further, the discussion in~\citet{wang2023state} does not characterize the effect of choosing complex-valued recurrences~(as crucially done in most SSMs, including S4)  on expressivity. In this paper, we adopt a different strategy compared to~\citet{wang2023state}, and aim instead at characterizing the information content in the RNN hidden state, further studying how this information is processed by the MLP across timestamps.

\vspace{-3mm}
\paragraph{Contributions.} We prove that a single linear diagonal RNN layer followed by a position-wise MLPs can approximate arbitrarily well any \emph{sufficiently regular}~(see Definition~\ref{def:seq}) nonlinear sequence to sequence map over finite length sequences. Our key insight is that, if the linear RNN can preserve a lossless compressed representation of the entire sequence seen so far, the MLP has access to all of the information in the input. Since MLPs are universal approximators~\cite {pinkus1999approximation}, they can process this information to achieve the desired mapping~(see Fig.~\ref{fig:architecture}). Along our reasoning, we give several insights on the effects of initialization and characterize the role of complex numbers on memory.
\vspace{-3mm}
\begin{enumerate}[left=0.1em, itemsep=0.5pt]
    \item In \S\ref{sec:vandermonde}, using the properties of Vandermonde matrices, we show that at a fixed timestamp $k$ one can precisely (zero error) reconstruct the value of each seen input token from a random diagonal linear RNN hidden state. We discuss how wide the RNN should be for this property to hold, and how the result adapts to the setting of compressible inputs. 
    \item In \S\ref{sec:MLP}, we prove our claim: linear RNNs followed by position-independent MLPs with one hidden layer can approximate regular sequence-to-sequence maps. Starting from the results of \S\ref{sec:vandermonde}, we use Barron's theory~\citep{barron1993universal} to provide guarantees on the MLP width. This result involves technical steps such as the computation of the Barron constant of the interpolation of non-linear maps. Crucially, we find that the MLP width is affected by the ease of reconstruction from the linear RNN state, quantified by the condition number of the reconstruction map, which is heavily dependent on the RNN initialization.
    \item In \S\ref{sec:discussion}, we leverage our framework to understand some interesting features of SSMs --- such as the need for complex eigenvalues in the recurrence and initialization near the unit circle in $\C$. In the same section,  we validate our claims and intuition on initialization on synthetic datasets and on long-range-arena tasks~\citep{tay2020long}.
\end{enumerate}
\vspace{-2mm}
Due to space limitations, proofs and additional related works are presented in the appendix.


\section{Preliminaries}
\label{sec:pre}
Consider length-$L$ sequences of real $M$-dimensional inputs: $\bar v=(v_i)_{i=1}^L\in\mathcal{V}\subseteq\R^{M\times L}$. We often refer to each $v_i$ as a ``token''\footnote{In formal NLP~\cite{cotterell2023formal}, $\mathcal{V}$ is a finite vocabulary. Here, we discuss the setting where $\mathcal{V}$ is possibly dense in $\R^{M\times L}$. }. A \textit{sequence-to-sequence map} is a deterministic transformation of input sequences that produces output sequences of the same length: $\bar y=(y_i)_{i=1}^L\in\mathcal{Y}\subseteq\R^{S\times L}$. We say that a sequence-to-sequence map is \textit{causal} if for every $k$, $y_k$ is blind to the tokens $(v_i)_{i=k+1}^L$.  

\begin{restatable}[Sequence-to-sequence]{definition}{seq-to-seq}
\label{def:seq}
A causal sequence-to-sequence map with length-$L$ sequential $M$-dimensional inputs $\bar v=(v_i)_{i=1}^L\in\mathcal{V}\subseteq\R^{M\times L}$ and length-$L$ sequential $S$-dimensional outputs $\bar y=(y_i)_{i=1}^L\in\mathcal{Y}\subseteq\R^{S\times L}$ is a sequence of non-linear continuous functions $T=(T_k)_{k=1}^L$, $T_k:\R^{M\times k}\to\R^S$ $\forall k\in[L]$. $T$ acts as follows: 
\vspace{-2mm}
$$(v_i)_{i=1}^L\overset{T}{\mapsto} (y_i)_{i=1}^L \ \ , \ \ \text{s.t.} \ \ y_k = T_k((v_i)_{i=1}^k).$$
\end{restatable}
\vspace{-2mm}
We are going to assume without restating this that each $T_k$ is a Barron function~(Def.~\ref{def:barron}), with a well-defined integrable Fourier transform~(used in Thm.~\ref{thm:comb_barron}). We consider approximations $\hat T$ to $T$ using a neural network. Schematically:
$$\hat T = g \circ \rnn \circ e,$$
\vspace{-4mm}
where:
\begin{itemize}
\item $e:\R^M\to\R^H$ is a linear embedding layer with biases, acting tokenwise. We denote the encoded tokens sequence $\bar u = (u_i)_{i=1}^L\in\R^{H\times L}$, where $u_k = e(v_k)\in\R^H$, for all $k\in[L]$.
    \item $\rnn$ is a linear RNN processing the encoded input tokens $(u_i)_{i=1}^L$ producing a sequence of $N$-dimensional hidden states $(x_i)_{i=1}^L\in\R^{N\times L}$.
    \item $g:\R^{N}\to\R^S$ is a non-linear function, acting tokenwise, parametrized by an MLP. We have that $\hat{y_k} = g(x_k)\in\R^S$, for all $k\in[L].$
\end{itemize}
\vspace{-3mm}
The combination of $g$ with $\rnn$ and $e$, which we denote as $g \circ  \rnn \circ e$, produces outputs $\hat{\bar{y}} = (\hat y_i)_{i=1}^L\in\R^{P\times L}$ which we hope to be close to $\bar y = T(\bar v)$.

The Linear RNN $R$ processes inputs recurrently. This operation is parametrized by matrices $A$ and $B$. 
\begin{restatable}[Linear RNN]{definition}{lrnn}
$\rnn_{A,B}:\R^{H\times L}\to\R^{N\times L}$ processes an (encoded) sequence $\bar u = (u_i)_{i=1}^L$ producing an output sequence of hidden states $\bar x=(x_i)_{i=1}^L\in\R^{N\times L}$ by means of the following recursion:
\begin{equation}
\label{eq:lrnn}
   x_{k} = A x_{k-1} + B u_k, 
\end{equation}
where $A\in\R^{N\times N}$, $B\in\R^{N\times H}$, and $x_{0}=0\in\R^N$.
\end{restatable}
\vspace{-2mm}
\paragraph{Diagonal Linear RNNs.} Linear RNNs have been shown to be very successful token-mixing components in deep architectures such as SSMs~\citep{gu2021efficiently, orvieto2023resurrecting}. A crucial feature of SSMs --- making them more appealing compared to non-linear variants such as LSTMs~\citep{hochreiter1997long} or GRUs~\citep{cho2014learning}--- is that the forward pass is cheap to compute by means of parallel scans~\citep{blelloch1990prefix}. At the root of such fast computation is the diagonalizability property of linear RNNs.\footnote{In principle, also non-diagonal linear RNNs can be parallelized. Yet, parallelizing non-diagonal RNNs involves multiplying dense matrices. To achieve a speedup over recurrent computation on modern hardware, the linear RNN has to be diagonal.} Indeed, over the space of $N\times N$ non-diagonal real matrices, the set of non-diagonalizable~(in the complex domain) matrices has measure zero~\citep{bhatia2013matrix}. Hence, up to arbitrarily small perturbations, $A$ is diagonalizable over the complex numbers, i.e. one can write $A = Q\Lambda Q^{-1}$, where $\Lambda = \diag(\lambda_1, \dots, \lambda_{N}) \in \mathbb{C}^{N \times N}$ gathers the eigenvalues of $A$, and the columns of $Q\in\C^{N\times N}$ are the corresponding eigenvectors. %Eq.~(\ref{eq:lrnn}) is equivalent to the following diagonal complex recurrence, up to a linear token-wise transformation of the hidden-states.
\begin{align*}
    & x_k= Q\Lambda Q^{-1} x_{k-1} + B u_k\\
    \implies & (Q^{-1}x_{k}) = \Lambda (Q^{-1} x_{k-1}) + (Q^{-1} B) u_k
\end{align*}
By renaming $x_k\leftarrow Q^{-1}x_{k}$ and $B\leftarrow Q^{-1}B$, one arrives at the complex-valued diagonal recursion 
\begin{equation}
\label{eq:ldrnn}
   x_{k} = \Lambda x_{k-1} + B u_k.
\end{equation}
The corresponding map $\rnn_{\Lambda,B}:\R^{N\times L}\to\C^{N\times L}$ is such that $\rnn_{A,B} = Q\circ \rnn_{\Lambda,B}$, where $Q$ is applied tokenwise. Since we are interested in architectures of the form $\hat f = g \circ \rnn \circ e$, the linear transformation $Q$ can be merged into $g$, at the price of having inputs for $g$ with \textit{doubled dimension}~(real and imaginary parts\footnote{In practice, some SSM variants drop the imaginary part.}): $g:\R^{2N}\to\R^P$.
Without loss in generality, we therefore assume from now on our linear RNNs are in diagonal form. While the role of complex numbers in this setting is clearly motivated by the construction, we later show in \S\ref{sec:discussion} that using complex eigenvalues induces peculiar memorization properties.
\vspace{-1mm}

\paragraph{MLPs and universality.} MLPs with one hidden-layer~(1HL-MLPs) are universal non-linear function approximators~\citep{barron1993universal, pinkus1999approximation}. In this paper, we consider $g$ parametrized by a 1HL-MLP. $g$ is a proxy for a general non-linear function $f$, which is regular in the sense that it is \textit{not oscillating too quickly} --- meaning that its Fourier transform $F(\omega)$ exhibits fast decay as $\|\omega\|\to\infty$.

% \begin{equation}
%     \mathcal{F}(\omega):= \frac{1}{(2\pi)^n}\int_{\R^n}f(x) e^{-i\langle \omega,x\rangle}dx.
% \end{equation}
\begin{restatable}[Barron function]{definition}{barron}
\label{def:barron}
Let $\mathcal{F}(\omega)$ be the Fourier transform of $f:\R^n\to\R$. $f$ belongs to the Barron class if $C_f = \int_{\R^n} \|\omega\|_2 |\mathcal{F} (\omega)|d\omega<\infty$.
\end{restatable}

\vspace{-2mm}
We have the important result due to~\cite{barron1993universal}.

\begin{restatable}[Universality of 1HL-MLPs]{theorem}{barronthm}
\label{thm:barron_thm}
Consider $g(x)$ parametrized by a 1HL-MLP (with $D$ hidden neurons): $g(x) = \sum_{k=1}^D \tilde c_k\sigma(\langle \tilde a_k , x\rangle + \tilde b_k)+ \tilde c_0$, where $\sigma$ is any sigmoidal function\footnote{$\lim_{x\to-\infty} \sigma(x) = 0$ and $\lim_{x\to\infty} \sigma(x) = 1$.}. Let $f:\R^n\to\R$ be continuous with Barron constant $C_f$. If $D\ge 2 r^2 C_{f}^2 \epsilon^{-2}$, then there exist parameters such that $\sup_{\|x\|\le r} |f(x)-g(x)|\le\epsilon$.
\end{restatable}
\vspace{-2mm}
Note that ReLU activations also work in the setting of the theorem above, up to a factor $2$ in the bound, since $\text{ReLU}(x)-\text{ReLU}(x-1)$ is sigmoidal.
\begin{restatable}[Multidimensional Output]{remark}{barron_rmk_multi}
\label{rmk:multidim_barron}
The result above is stated for the scalar output case. The $S$-dimensional outputs result can be easily derived by stacking neurons in the hidden layer, that becomes of dimension $D\leftarrow DS$. Therefore if $D\ge 2 r^2 C_{f}^2 S\epsilon^{-2}$, then $\sup_{\|x\|\le r} \|f(x)-g(x)\|_1\le\epsilon S$ (since errors accumulate). Let us then call $\epsilon \leftarrow \epsilon S$ the desired accuracy; the number of neurons needed to achieve that is $D\ge 2 r^2 C_{f}^2 S^3\epsilon^{-2}$.
\end{restatable}


\section{Universality Result}
\label{sec:universal}
In this section, we show that, as the model $\hat T = g \circ R \circ e$ grows in width, there exist network parameters such that $\hat T \approx T$. We state this result informally below.

\begin{restatable}[Universality]{theorem}{main_thm}
\label{thm:universal}
Let the inputs set $\mathcal{V}\subset\R^{M\times L}$ be bounded and $\epsilon>0$ be the desired accuracy in approximating $T$. Let $\rnn$ be a diagonal linear real or complex RNN with width $N\ge \dim(\mathcal V)$ and let the MLP width $D\ge O(L/\epsilon^2)$. Then, $\hat T = g \circ R \circ e$ approximates pointwise $T$ with error $\epsilon$:  $$\sup_{\bar v\in \mathcal{V}}\|\hat T (\bar v)-T(\bar v)\|\le \epsilon.$$
\end{restatable}
\vspace{-2mm}
Here, by $\dim(\mathcal{V})$ we mean the \textit{vector-space dimension} of $\mathcal{V}$. In the worst-case scenario where inputs are not structured, $\dim(\mathcal V) = LM$. In practice, we observe that one can work with smaller dimension in the hidden state~(Fig.~\ref{fig:MNIST_PF_rec_MLP}).
The proof comprises two steps:
\vspace{-3mm}
\begin{itemize}[left=0.1em, itemsep=1pt]
    \item Step 1: Linear RNNs can perform lossless compression~(\S\ref{sec:vandermonde}): from the RNN output $x_k$ one can perfectly~(no error) reconstruct the input $(v_i)_{i=1}^k$, assuming $N$ is large enough~($N\ge \dim(\mathcal V)$).
    \item Step 2: The MLP head $g$ on top of $x_k$ can reconstruct the ground-truth map $T_k$: $T_k((v_i)_{i=1}^k)\simeq g(x_k)$, assuming the number of hidden MLP neurons $D$ is large enough~($D\ge O(L/\epsilon^2)$).
\end{itemize}
\vspace{-2mm}
While step 2 is mainly technical, step 1 --- dealing specifically with the RNN --- is less involved and leads to valuable insights into the architecture.
\begin{restatable}{remark}{compression}
\label{rmk:compression}
A few comments on the result are needed:
\vspace{-3mm}
\begin{itemize}[left=0.1em, itemsep=1pt]
    \item In Thm.~\ref{thm:universal}, both the size of the RNN and the MLP agree with basic information theory reasoning: (a) if inputs are random and unstructured, the hidden state cannot perform compression: has to store $\mathcal{O}(L)$ arbitrary real numbers, requiring $\mathcal{O}(L)$ hidden dimensions; (b) the MLP size is $\mathcal{O}(L)$ because, in the worst-case, it has to model $L$ distinct maps $T_1, T_2,\dots, T_L$~(Def.~\ref{def:seq}). This complexity can be reduced by assuming temporal smoothness.
    \item The setting of tokens living in a finite set~\cite{cotterell2023formal} is drastically different and not discussed in this work. An interesting result using a construction derived from the first version of this paper can be found in~\citet{ding2023recurrent}. Additional recent results can be found in~\citet{jelassi2024repeat}.
    \item A simple application of Barron's Theorem~(Thm.~\ref{thm:barron_thm}) does not suffice to prove Step 2. In SSMs, the same MLP function is applied at each timestamp, ant therefore has to model the interpolation of $T_1, T_2,\dots, T_L$. Adapting Barron's theory to this time-dependent setting is our main technical contribution.
\end{itemize}
\end{restatable}
This section is dedicated to the proof of Thm~\ref{thm:universal}, with the main intuition relevant for our discussion is presented in \S\ref{sec:main_idea}. Valuable insights on initialization and role of complex numbers can be derived from our proof strategy. These are discussed in \S\ref{sec:discussion}. 
\subsection{Linear RNNs can perfectly memorize inputs}
\label{sec:vandermonde}
We first state the main result of this subsection.
\begin{restatable}[Power of Linear RNNs, informal]{theorem}{pow_rnns}
\label{thm:perfect}
Under no assumption on the encoded inputs set $\mathcal{U}\subseteq \R^{H\times L}$, if the hidden-state dimension scales with $HL$, then linear RNN-based processing comes with \textbf{no information loss}: inputs $(u_i)_{i=1}^k$ can be perfectly recovered from $x_k$, for any $k=1,2,\dots, L$. The hidden-state dimension can be reduced to $\mathcal{O}(P)$ if $\dim(\mathcal{U})=:P<HL$ --- i.e. if $\mathcal{U}$ is linearly \textbf{compressible}.
\end{restatable}
\vspace{-2mm}
We encourage the reader to go through this subsection to get a proof idea. Insights and practical considerations will be then addressed thoroughly in~\S\ref{sec:discussion}.\\
In this subsection, with ``input'' we refer to the encoded sequence $\bar u = e(\bar v)\in\R^{H\times L}$. Note that, as typically $H\ge M$, accurate reconstruction of $\bar u$ implies accurate reconstruction of $\bar v$. We start by unrolling Eq.~(\ref{eq:ldrnn}): $x_{1} = Bu_1$,  $x_2 = \Lambda Bu_1 + Bu_2$, $x_3 = \Lambda^2Bu_1 + \Lambda Bu_2 + B u_3$, and 
\begin{align}
    \label{eq:lin_rnn_unroll}
    x_k =\sum_{j=0}^{k-1} \Lambda^jBu_{k-j}.
\end{align}
It is easy to realize that this operation stacks convolutions of the sequence $ B \bar u\in\C^{N\times L}$ with $N$ independent one-dimensional filters parametrized by $\Lambda=\diag(\lambda_1,\dots,\lambda_N)$, and with form $(1, \lambda_i,\lambda_i^2,\dots, \lambda_i^{L-1})$ for $i=1,2,\dots N$. 
\subsubsection{Main idea}
\label{sec:main_idea}
Let $H=M=1$, the encoder $e$ be the identity, and $B = (1,1,\dots, 1)^\top$. Then, eq.~(\ref{eq:lin_rnn_unroll}) can be written as
\begin{equation}
    x_k   =
    \begin{pmatrix}
    \lambda_1^{k-1}&\lambda_1^{k-2} &\cdots& \lambda_1&1\\
    \lambda_2^{k-1}&\lambda_2^{k-2} &\cdots& \lambda_2&1\\
    \vdots&\vdots &\ddots&\vdots&\vdots\\
    \lambda_N^{k-1}&\lambda_N^{k-2} &\cdots& \lambda_N&1\\
    \end{pmatrix}
    \begin{pmatrix}
    u_1 \\ u_{2} \\ \vdots \\ u_k
    \end{pmatrix}= V_k u_{1:k}^\top.
    \label{eq:vandermonde}
\end{equation}
where $u_{1:k}= v_{1:k} = (v_i)_{i=1}^k \in\R^{1\times k}$, and $V_k$ is a Vandermonde matrix. As long as $N\ge k$, we can hope to recover $u_{1:k}$ by pseudoinversion of the Vandermonde:
\begin{equation}
    v_{1:k}^\top = u_{1:k}^\top = V_k^+ x_k,
\end{equation}
Indeed, note that if $N=k$~(number of equations $=$ number of unknowns), the matrix $V_k$ is invertible under the assumption that all $\lambda_i$ are distinct, since~(see e.g.~\cite{bhatia2013matrix}): 
$\det(V_k) = \prod_{1\le i< j\le k}(\lambda_i-\lambda_j)\ne 0$. If $N>k$ then under again the assumption that eigenvalues are distinct, the linear operation on the hidden state $V_k^+ x_k$ produces the input sequence without errors. To guarantee $N\ge k$ for all $k$, it is clear one has to choose $N\ge L$. We summarize below.

\begin{restatable}[Bijectivity]{proposition}{main_simple}
Let $M=1$, $H=1$ and let the encoder $e$ be the identity. Consider input projection $B = (1,1,\dots, 1)^\top$ and recurrent matrix $\Lambda=\diag(\lambda_1,\dots,\lambda_N)$ with $\lambda_i\ne\lambda_j$ for all $i\ne j$. Fix $k\in[L]$, let $R_{\Lambda,B}^k:\R^{H\times k}\to\R^N$ denote the map $(u_{i})_{i=1}^k\mapsto x_k$ produced by $R_{\Lambda,B}$. If $N\ge L$, $R_{\Lambda,B}^k$ is bijective, with linear inverse.
\end{restatable}
\vspace{-3mm}
The reconstruction map~(parametrized by $V_k^+$) is time-dependent and has \textit{time-dependent} output dimension $\R^k$. Since the RNN output we consider in this paper is a \textit{time-independent MLP} head $g$, it is essential for the hidden state to also contain information about \textbf{time}. This is trivial to achieve with linear RNNs: consider $e:\R^M\to\R^{M+1}$ such that $v_k$ is mapped to $u_k = (1, v_k)$ for all $k$. Let $B = ((1,0,0,\dots, 0),(0,1,1,\dots,1))^\top$. Consider $\lambda_0=1$. Then, the first hidden state dimension will coincide with the index $k$, since $x_{k,0} = x_{k-1,0} + 1$ for all $k$. The construction above, allows the RNN to count the step it is on. In \S\ref{sec:MLP}, we will use this fact to conclude the theoretical discussion. 

\vspace{-3mm}
\paragraph{Multidimensional setting.} Since the RNN is linear, one can design $M$ independent RNNs acting on separate input dimensions. Such RNNs can be combined into one single block using a properly designed $B$ matrix.

\vspace{-3mm}
\paragraph{Solution by circulant recurrences.} It is easy to construct a non-diagonalizable RNN solution to the problem of memorizing length-$L$ sequences: picking the recurrent matrix to be circulant. However, circulant matrices are not diagonalizable, so the recurrence would not be in the hypothesis class of modern state-space models (S4D/S5/LRU). As our main objective is to study SSMs using diagonal recurrences, it is not satisfactory to base the discussion on this corner setting~(see also \S\ref{sec:discussion}). Our result provides instead guarantees for linear diagonal random RNNs.

\subsubsection{RNNs can compress inputs, when possible}
\label{sec:sparse}
What we discussed in Theorem~\ref{thm:perfect} is a \textbf{worst-case setting} that requires the RNN hidden state $N$ to be of size proportional to the input sequence $L$. This is not surprising since we made no assumptions about the inputs $\bar u$: it is indeed not possible in general to store $\mathcal{O}(L)$ real numbers in a vector of size smaller than $\mathcal{O}(L)$ by means of a linear Vandermonde projection --- unless such inputs are structured~(cf. Rmk.~\ref{thm:universal}). However, the RNN hidden state dimension scales efficiently under sparseness.

\begin{restatable}[Sparse inputs]{assumption}{sparse} Let $\Psi = (\psi^i)_{i=1}^P$, with $\psi^i\in\R^{M\times L}$ for all $i=1,2,\cdots, P$ be an ordered list of basis functions. For every input sequence $\bar v$, there exists coefficients $\alpha^v:=(\alpha^{v}_i)_{i=1}^P\in\R^P$ such that $\bar v  = \sum_{i=1}^P \alpha^{v}_i\psi^i$. We do not assume such decomposition is unique, nor to have access to the basis functions.
\label{ass:sparse}
\end{restatable}
\vspace{-2mm}
Let us discuss this assumption in the one-dimensional setting $M=1$. Definitions and results carry out effortlessly to the multidimensional setting. In matrix form, Assumption~\ref{ass:sparse} can be written as: there exist a real matrix $\Psi\in\R^{L\times P}$ such that, for all $\bar v\in\mathcal{V}\subset\R^{1\times L}$ there exist $\alpha^v\in\R^P$ such that $\bar v^\top = \Psi \alpha^v$. Let $\Psi_k\in\R^{k\times P}$ denote the first $k$ columns of $\Psi$. The last equation implies $v_{1:k}^\top = \Psi_k \alpha^v_k$, where $\alpha_k$ is one among the estimates of the coefficients $\alpha^v$ holding until iteration $k$~(e.g. one frequency component might be visible only after a specific $k$). Eq.~(\ref{eq:vandermonde}) then implies:
\begin{equation}
    x_k  = V_k v_{1:k}^\top = V_k\Psi_k \alpha^v_k,
\end{equation}
Under the assumption that $\Gamma_k := V_k\Psi_k$ is full rank, $\alpha^v_k = (\Gamma_k^\top \Gamma_k)^{-1} \Gamma_k^\top x_k$. We therefore get:
\begin{equation}
    v_{1:k} = \Omega_k x_k, \quad \Omega_k:=\Psi_k(\Gamma_k^\top \Gamma_k)^{-1} \Gamma_k^\top \in\C^{k\times N}.
\end{equation}

\vspace{-4mm}
\paragraph{General definition of $\boldsymbol{\Omega_k}$.} In the non-sparse setting, it is easy to see that $\Omega_k=V_k^+$. Therefore, from this point in the paper \textit{we will keep denoting as $\Omega_k$ the linear reconstruction map}. Moreover, to make $\Omega_k$ act on real numbers, we consider instead its real/imaginary inputs representation in $\R^{2N\times N}$.

\vspace{-4mm}
\paragraph{$\boldsymbol{\Psi}$ may be unknown.} One does not need to assume a specific $\Psi$ a priori: as long as we assume input sparsity, we can guarantee approximation with a learned linear map on the state, which may be task-dependent. Even if the input is strictly-speaking not sparse, not all information might be relevant for prediction. In a way, sparsity mathematically quantifies the belief that the information content relevant in a certain task has lower dimension compared to the original signal.


\subsection{MLP Reconstruction}
\label{sec:MLP}
Our goal is to approximate the object $T=(T_i)_{i=1}^L$, that maps $(v_i)_{i=1}^L\overset{T}{\mapsto} (y_i)_{i=1}^L$ such that $y_k = T_k((v_i)_{i=1}^k)$ for all $k=1,2,\dots, L$. In the usual SSMs pipeline, this approximation takes the form $\hat T = g \circ \rnn \circ e$, where compositions are tokenwise~(MLP $g$ and embedding $e$ applied to each token), and $R$ is a linear diagonal RNN, mixing tokens in the temporal direction.\\
Let $x_k$ be the $k$-th RNN output. In the settings described in the last subsection, for every $k\in[L]$ there exists a linear function $\Omega_k'$~(identity on the first coordinate $x_{k,0}=k$, and $\Omega_k$ on the other coordinates) such that $\Omega_k' x_k = (k,(v_i)_{i=1}^k)$. Note that under no assumption on the set of inputs, $\Omega_k$ coincides with the Vandermonde inverse $V_k^+$. The last operation, $(k,v_{1:k})\mapsto y_k$ needs to be performed by the \textit{time-independent} map $g$. 

\vspace{-2mm}
\paragraph{Step 1: fixed timestamp.} Let us focus on getting the MLP to approximate $T_k$, $k$ fixed, from the RNN output.  $g$ has to satisfy $g(x_k) \overset{!}{=} T_k(\Omega_k(x_k)) =: f_k(x_k)$.
The following result~(based on Rmk.\ref{rmk:multidim_barron}) estimates the width the MLP $g$ needs to have to approximate the equality above.
\begin{restatable}[MLP, single timestamp]{proposition}{mainMLPsing}
\label{prop:single_timestamp}
The number of hidden neurons in the MLP $g$ needed to approximate $f_k:=T_k\circ\Omega_k$ at level $\epsilon$ from the RNN hidden state $x_k$ is bounded by $4 r^2 C_{T_k}^2 \|\Omega_k\|^2_2 S^3\epsilon^{-2}$ where $r$ bounds the hidden state magnitude, and $C_{T_k}$ is the Barron constant of $T_k$.
\end{restatable}
\vspace{-2mm}
The result is easy to parse: to approximate the output at a specific timestamp $k$, the width needed is similar to that of an MLP $(v_i)_{i=1}^k\overset{g_k}{\mapsto} \hat y_k$, which would be $4 r^2 C_{T_k}^2 S^3\epsilon^{-2}$~(see Thm.~\ref{thm:barron_thm}). The additional factor $\|\Omega_k\|^2_2$ appears because $g$ operates on the hidden state and not from the input. Hence, the MLP has to pay a \textbf{penalty for reconstruction}. The result is based on the computation of the Barron constant for the map $f_k$, which is upper bounded by $\|\Omega_k\|_2C_{T_k}$~(see appendix). In the next paragraph, we study how the complexity is affected by the additional requirement that the map $g$ should be able to interpolate in-between $T_k$s, based on the first coordinate in the hidden state $x_{k,1}=k$.

\vspace{-3mm}
\paragraph{Step 2: arbitrary timestamp.} To construct a time-independent global approximation, we use recent results from harmonic analysis~\citep{tlas2022bump} to construct a proper domain relaxation and operate with a single MLP. This computation leads to a formula for the Barron complexity for function sequences, where an input dimension determines the function the MLP should implement. In addition to the definition $C_f = \int_{\R^n} \|\omega\|_2 |\mathcal{F} (\omega)|d\omega$, define the Fourier norm $C_f' = \int_{\R^n} |\mathcal{F} (\omega)|d\omega$. 

\begin{restatable}[Combination of Barron Maps]{theorem}{mainbarronk}
\label{thm:comb_barron}
Let $f:[1, 2, \dots, L] \times \R^n\to\R$ be such that each $f(k, \cdot)$ is Barron with constant $C_{f_k}$ and Fourier norm $C_{f_k}'$. There exist an extension $\tilde f:\R^{n+1}\to\R$ of $f$, s.t. $\tilde f$ is Barron with constant $\tilde C \le \mathcal{O}(\sum_{k=1}^L C_{f_k} + C_{f_k}') = \mathcal{O}(L)$ and $\tilde f(x,k) = f(x,k), \forall x\in\R^n$ and $k\in 1, 2, \dots, L$.
\end{restatable}
This result, which essentially confirms the need, under no further assumptions on $T$, to blow up the number of hidden neurons with the sequence length~(unless some smoothness is assumed, see Remark~\ref{rmk:compression}), directly leads to our main result~(Thm.~\ref{thm:universal}) under the assumption that the RNN has perfect memory, guaranteed if $N\ge\dim(\mathcal{V})$~(Thm.~\ref{thm:perfect}).

\textit{Proof of Thm.~\ref{thm:universal}.} Let $f_k := T_k\circ \Omega_k:\R^{2N}\to\R^S$. By Prop.~\ref{prop:single_timestamp}, $C_{f_k}=\|\Omega_k C_{T_k}\|$. $C_{f_k}'$ scales similarly. We have $f_k(x_k) = T_k(v_{1:k})$ thanks to Thm.~\ref{thm:perfect}. We then apply Thm.~\ref{thm:comb_barron} to the sequence of $f_k$~(one for each timestamp): there exist an interpolating function $\tilde f$~(time as first component followed by the $2N$ arguments of $f_k$) with Barron constant $\mathcal{O}(\sum_{k=1}^L C_{f_k} + C_{f_k}') = \mathcal{O}(L)$. We let this be the function the 1-HL MLP $g$ has to approximate, and apply Thm.~\ref{thm:barron_thm}.





% Figure environment removed 

% Figure environment removed



\section{Validation and Discussion}
\label{sec:discussion}
In this section, we revisit and validate our claims, and further discuss the practical insights associated with input reconstruction from the linear RNN hidden state.


\subsection{Conditioning of Vandermonde inverse and role of complex numbers}
\label{sec:exp_rec_vander}


In \S\ref{sec:main_idea}, we discussed the main idea behind reconstruction from the linear RNN hidden state. Let us restrict attention to the last timestamp: under no sparseness assumption~(discussed at the end of this subsection) $x_L = V_k \bar v^\top$, with $V_L\in\C^{N\times L}$, hence $\bar v^\top = V_L^+x_L$, where $V_L^+ = (V_L^\top V_L)^{-1}V_L^\top =:\Omega_L$. Indeed, if the eigenvalues $(\lambda_i)_{i=1}^N$ are distinct and $N\ge L$, $V_L^\top V_L\in\C^{L\times L}$ is invertible since the Vandermonde determinant formula ensures $V_L$ has full column rank\footnote{By contradiction, assume $V_L^\top V_L$ has the zero eigenvalue. This implies there exist $x$ such that $V_L^\top V_L x = 0$, hence $x^\top V_L^\top V_L x = \|V_L x\| = 0$, which is true if and only if $V_L x=0$, meaning $V_L$ is not full column rank.}. Such computation does not take into account a \textit{numerical issue}: if $V_L$ is \textbf{ill-conditioned}, $\|V_L^+\|_2\to\infty$, preventing successful reconstruction. Instead, when initializing or learning the RNN, not only can we assume the eigenvalues are distinct, but we can also control them. Let us pick $\lambda_i s$ uniformly on the complex ring in between $[r_{\min},r_{\max}]\subseteq[0,1]$: For $i\in[N]$, $\lambda_i$ i.i.d with
\begin{equation*}
    \lambda_i\sim \mathbb{T}[r_{\min}, r_{\max}] := \{\lambda\in\C \ | \ r_{\min}\le|\lambda|\le r_{\max}\}.
\end{equation*}
Initialization of $\Lambda$ \textbf{close to the unit circle} is often used in the context of modern state-space models to unlock long-range reasoning~\citep{orvieto2023resurrecting}. Our theory gives grounding to this strategy: we see in Fig.~\ref{fig:vander_study} that if $r_{\min}\to 1$, then the Vandermonde becomes well conditioned and reconstruction becomes successful~(Fig.~\ref{fig:MNIST_rec_random_van_main} + appendix).

% Figure environment removed 








One can also link this effect to vanishing gradients: if $|\lambda_i|\to0$ then the hidden state ``forgets'' inputs at a rate $\lambda^k$. This is the reason why in Fig.~\ref{fig:MNIST_rec_random_van_main} we observe successful reconstruction only for the recent past if $r_{\min}=0$. 




\vspace{-2mm}
\paragraph{Role of complex numbers.} If $\Lambda$ is real the condition number of $V_k$ grows exponentially with $N$~\citep{gautschi1987lower}. In the complex setting it is possible to make the condition number of $V_k$ exactly 1 by e.g. choosing the $N$th-roots of unity as eigenvalues of $\Lambda$~\citep{gautschi1975optimally,cordova1990vandermonde}. This fact provides a \textit{precise justification for the use of complex numbers in the recurrent computation}: diagonal real recurrent RNNs can be also implemented fast with parallel scans~\citep{smith2023simplified}, but are less expressive and suffer from inherent information loss.

\vspace{-3mm}
\paragraph{Comparison with FFT.} The case of equally-spaced eigenvalues on the disk coincides with the computation of the Fourier Transform. Perfect reconstruction in this setting is guaranteed by the inverse Fourier Transform Theorem~\citep{folland2009fourier}. While this would motivate the use of unitary RNNs~\citep{arjovsky2016unitary, helfrich2018orthogonal}, as we will see in \S\ref{sec:odes} best performing learned architectures do not have this property: hidden-state features with vanishing memory are often the best option. 

\vspace{-3mm}
\paragraph{Comparison with HiPPO.} The S4 linear recurrence~\citep{gu2021efficiently} is motivated by HiPPO theory~\citep{gu2020hippo}: a design principle for the recurrence eigenvalues that induces perfect memory of past tokens given some measure and a basis of reconstruction polynomials. Our approach is more general, we show that it is not needed to fix an input measure or to hand-pick eigenvalues: random initialization on a ring can already lead to successful reconstruction.

\vspace{-3mm}
\paragraph{Sparseness improves conditioning.} In the last section, we discussed how input sparseness~(on a basis of $P$ functions) results in a reduced hidden state dimension requirement. Specifically, if $\dim(\mathcal{V})=P$, i.e. there exists $\Psi\in\R^{L\times P}$ such that for any $\bar v\in\mathcal{V}$ there exists $\alpha^v\in\R^P$ such that $\bar v^\top = \Psi \alpha^v$, then an RNN with width $N=P$ achieves perfect recall for inputs in $\mathcal{V}$. In Fig.~\ref{fig:effect_P} we show that sparseness also improves conditioning of the reconstruction map $\Omega_L$, appearing in Prop.~\ref{prop:single_timestamp}. 

% Figure environment removed


\subsection{Learned architecture for reconstruction}
\label{sec:learned_rec}
In the previous subsection we studied the reconstruction map computed from the parameters of a \textbf{random} linear RNN. While our theory provides satisfactory approximation guarantees in this setting, performance is much stronger if we allow the linear RNN to be \textit{\textbf{trained} jointly with the reconstruction map}. We use here a simple LRU block~(at inference time, a linear RNN~\citep{orvieto2023resurrecting}) followed by a linear reconstruction head $\Omega$. This framework, perfectly in line with our settings in this paper, allows the RNN eigenvalues and the decoder to synchronously adapt to the specific data structure. Out of curiosity, we also test here the performance of non-linear reconstruction from the RNN hidden state with a 1-HL MLP with $2048$ hidden units\footnote{This is not the function $g$ that allows approximation of $T_k$. Here the MLP serves as non-linear decoder.}. We use data from the long-range arena~\citep{tay2020long}, specifically sequential MNIST~(784 tokens) and PathFinder~(1024 tokens) and train our model\footnote{We train on a single Nvidia RTX A5000 for 100 epochs.} under the usual guidelines from random initialization~(see discussion in~\cite{orvieto2023resurrecting}). 
\vspace{-3mm}
\begin{enumerate}[left=0.1em, itemsep=1pt]
    \item The reconstruction error~(input of reconstruction map is the last hidden state) vanishes as the RNN hidden dimension increases. At $N=256$, RNN width used in practice on the long-range arena~\citep{gu2021efficiently, orvieto2023resurrecting}, reconstruction is nearly perfect. For $N=1024$, the error is negligible~(as predicted by Thm.~\ref{thm:perfect}) since $N\ge L$.
    \item Reconstruction is surprisingly accurate at small values of $N$ if the decoder is non-linear. Instead, for $N=1024$, the error is smallest with a linear decoder, a finding we believe is rooted in optimization of wide MLPs. 
\end{enumerate}

%\vspace{3mm}

\subsection{Approximation of non-linear ODEs.}
\label{sec:odes}
In \S\ref{sec:exp_rec_vander}\&\ref{sec:learned_rec} we studied the information content of linear RNNs hidden states and verified empirically that near-perfect input reconstruction is possible, even if $N<L$. We now conclude the paper with results verifying our main claim~(Thm.~\ref{thm:universal}): a single MLP applied to the RNN hidden states, independent of the timestamp, is able to reconstruct the output of regular sequence-to-sequence maps~(Def.~\ref{def:seq}). An example of a sequence-to-sequence map is the flow of a controlled differential equation, of form $\dot z_t = f(z_t, v_t), y_t = h(z_t)$, where $(v_t)_{t}$ is the input, $f$ is a non-linear multidimensional function, and $h$ projects the multidimensional state $z_t$ into a one-dimensional output. Such systems can model complex interactions and stand at the root of physics. Due to space limitations, we limit our discussion here to the problem of learning a \textit{protein transduction~(PT) system}~\citep{vyshemirsky2008bayesian}.\textit{The reader can find results for other non-linear ODEs in the appendix}. 

Here, $z_t$ is 5-dimensional, and $f$ includes multiplicative interactions between components of $z_t$ as well as feedback~(see equations in the appendix). We include an input to the right-hand-side of the first ODE while integrating with Runge-Kutta 45, and take $h$ as the projection of the first component of $z_t$. We learn to mimic the PT system with a linear RNN with $N=128$, followed by a 1-HL MLP with $D=256$. We sample $10k$ smooth inputs and train on the simulated outputs for $L=2048$ integration steps. We test on $1k$ additional trajectories. The results in Fig.~\ref{fig:ptpz} clearly show that the architecture studied in this paper is able to learn the PT sequence-to-sequence map: plotted are two out-of-sample trajectories~(not used during training), showcasing that our model prediction matches the PT dynamics up to a small error~(as confirmed by the low test loss). We also include a depiction of the learned eigenvalues, to confirm that the RNN has indeed vanishing memory --- but this does not necessarily imply information loss.


% Figure environment removed







\vspace{-1mm}
\section{Conclusion}
In this paper we studied the expressive power of architectures combining linear RNNs with position-wise MLPs. These models recently exhibited remarkable performance in the context of long-range reasoning. In our theoretical framework, these two blocks work in symbiosis while having distinct roles: the linear RNN provides a compressed~(where possible) representation of the input sequence, while the MLP performs non-linear processing. We believe our \textit{separation of concerns} principle takes us one step further in understanding how to design and conceptualize deep state-space models: non-linear sequential processing can be performed by a time-independent component~(the MLP). Further, the use of complex numbers in the recurrence unlocks lossless compression via a well-conditioned reconstruction map. In addition, our analysis provides guidelines for asymptotic scaling of architecture components, and can lead to novel initialization strategies based on local objectives~(e.g. successful reconstruction). In combination to network design, our work leaves open many interesting avenues for future research, such as a precise proof of Turing completeness, and the finite-alphabet setting. Regarding the expressivity of recent selective RNNs such as Mamba and Hawk/Griffin, an extension of the theory found here using the Signature approach can be found in~\citet{cirone2024theoretical}.

\section{Acknowledgements}
Antonio Orvieto acknowledges the financial support of the Hector Foundation.

% \section{Impact Statement}
% The authors do not foresee any societal consequences or ethical concerns arising from their work. This is a theoretical paper.

% \razp{I would add in the conclusion some more clear future directions if we end up having some space. Particularly I would say:
% \begin{itemize}
%     \item Our construction relies on the separation of concerns between the linear RNN and the MLP. As future work, it will be interesting to see whether this separation can be further exploited, for example when devising initialization or local objectives for learning.
%     \item Our construction can provide intuition for deciding on the size of the MLP and RNN. In particular, the MLP needs to be (considerably) wider then the hidden state of the RNN, since it needs to represent the different non-linear behaviour of each step. 
%     \item There is still an open question of formalizing under which conditions the linear RNN can efficiently compress the sequence. And in particular, if information needs to be thrown out in this encoding, given typical initialization of LRU, which information is the system biased towards forgetting.
%     \item Lastly our constrcution regards sequence to sequence mappings for finite and fixed length sequences. It is still an open question of how well can these systems approximate dynamical systems in general or whether they can be Turing complete, under similar assumptions as done for non-linear RNNs.
% \end{itemize}
% }

% \section*{Checklist}

%  \begin{enumerate}


%  \item For all models and algorithms presented, check if you include:
%  \begin{enumerate}
%   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes]
%   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes]
%   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes]
%  \end{enumerate}


%  \item For any theoretical claim, check if you include:
%  \begin{enumerate}
%   \item Statements of the full set of assumptions of all theoretical results. [Yes]
%   \item Complete proofs of all theoretical results. [Yes]
%   \item Clear explanations of any assumptions. [Yes]     
%  \end{enumerate}


%  \item For all figures and tables that present empirical results, check if you include:
%  \begin{enumerate}
%   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes, we provide two notebooks verifying the main reconstruction claims. Code for numerical experiments using learned reconstructions and projections will be available upon acceptance]
%   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes]
%          \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes]
%          \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes]
%  \end{enumerate}

%  \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
%  \begin{enumerate}
%   \item Citations of the creator If your work uses existing assets. [Not Applicable]
%   \item The license information of the assets, if applicable. [Not Applicable]
%   \item New assets either in the supplemental material or as a URL, if applicable. [Not Applicable]
%   \item Information about consent from data providers/curators. [Not Applicable]
%   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
%  \end{enumerate}

%  \item If you used crowdsourcing or conducted research with human subjects, check if you include:
%  \begin{enumerate}
%   \item The full text of instructions given to participants and screenshots. [Not Applicable]
%   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
%   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
%  \end{enumerate}

%  \end{enumerate}
\bibliography{main}
\bibliographystyle{icml2024}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 
\appendix
\input{app}



\end{document}
