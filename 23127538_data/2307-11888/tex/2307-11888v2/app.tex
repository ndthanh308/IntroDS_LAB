\onecolumn
\begin{center}
{\Large \textbf{Appendix}}
\end{center}

\section{RELATED WORKS}

\paragraph{Issues with attention for long-range reasoning.} Efficient processing of long sequences is an important open question in deep learning. Attention-based transformers~\citep{vaswani2017attention} provide a scalable approach but suffer from \textit{quadratically increasing complexity in inference/memory} as the sequence length grows. While many approaches exist to alleviate this issue, e.g. efficient memory management~\citep{dao2022flashattention,dao2023flashattention} and architectural modifications~\citep{wang2020linformer, kitaev2020reformer, child2019generating, beltagy2020longformer}, the sequence length in large language models is usually kept to $2k/4k$ tokens for this reason~(e.g. Llama2~\citep{touvron2023llama}). In addition, in some long-range reasoning tasks~\citep{tay2020long} attention does not seem to provide the correct \textit{inductive bias}, leading to poor performance in addition to high computational costs.
\vspace{-3mm}
\paragraph{Success of modern recurrent layers.} Due to the issues outlined above, the community has witnessed in the last year the rise of new, drastically innovative, \textit{recurrent} alternatives to the attention mechanism, named state-space models~(SSMs). The first SSM, S4, was introduced by \cite{gu2021efficiently}, and since then, a plethora of variants have been proposed:  LiquidS4~\citep{hasani2022liquid}, DSS~\citep{gupta2022diagonal},
S4D~\citep{gu2022parameterization}, S5~\citep{smith2022simplified}, RWKV~\citep{peng2023rwkv} and RetNet~\citep{sun2023retentive} among others. These models achieve remarkable performance, surpassing all modern attention-based transformer variants by an average $20\%$ accuracy on challenging sequence classification tasks~\citep{tay2020long}. SSMs have reached outstanding results in various domains beyond toy datasets~\citep{nguyen2022s4nd,goel2022sashimi,gu2021efficiently,lu2023structured,zucchet2023online}. SSMs also were successfully applied to language modeling, and are sometimes used in combination with attention~\citep{fu2023hungry, wang2023pretraining, ma2022mega}. At inference time, all SSMs coincide with a stack of linear Recurrent Neural Networks, interleaved with MLPs and normalization. Linearity of the RNNs allows fast parallel processing with FFTs~\citep{gu2022parameterization} or parallel scans~\citep{smith2023simplified}.  
\vspace{-3mm}

\paragraph{The linear recurrent unit~(LRU).} Among modern architectures for long-range reasoning based on recurrent modules, the simplest is perhaps Linear Recurrent Unit~(LRU)~\citep{orvieto2023resurrecting}: while SSMs rely on the discretization of a structured continuous-time latent dynamical system, the LRU is directly designed for discrete-time systems~(token sequences), and combines easy hyperparameter tuning with solid performance and scalability. The only differences between the LRU and the standard RNN update $x_{k} = A x_{k-1} + B u_k$~($u$ is the input at a specific layer and $x$ is the hidden-state, then fed into a position-wise MLP) are (1) the system operates in the complex domain~(required for expressivity, see discussion in~\cite{orvieto2023resurrecting})~(2) to enhance stability and better control how quickly gradients vanish, $A$~(diagonal) is learned using polar parametrization and log-transformed magnitude and phase. Finally, (3) the recurrence is normalized through an extra optimizable parameter that scales the input to stabilize signal propagation. The parametrization of linear RNNs of~\citep{orvieto2023resurrecting} was found to be effective also in surpassing deep LSTMs and GRUs~\citep{zucchet2023gated}. We use the LRU codebase\footnote{\url{https://github.com/NicolasZucchet/minimal-LRU/tree/main/lru}} as a starting point for our experiments, when the linear RNN is learned.
\vspace{-3mm}
\paragraph{Approximation theory for MLP and non-linear RNNs.} The approximation properties of deep neural networks with ReLU activations are well studied. While recent advances concern the effect of depth~\citep{lu2017expressive}, the study by \citet{pinkus1999approximation}, as well as previous works~\citep{funahashi1989approximate,hornik1989multilayer,hornik1991approximation, barron1993universal}, already established the power of neural networks with a single hidden layer, which can approximate arbitrary continuous non-linear maps on compacts as the size of the hidden layer grows to infinity. The cleanest result is perhaps the one of \citet{barron1993universal}, that we heavily use in this paper.\\
In the context of non-linear RNN approximation of dynamical systems~(e.g. in neuroscience), the state-to-state computation can be seen as part of an MLP~(see e.g.~\citet{hanson2020universal}): we have $x_k = \sigma(A x_{k-1} + B u_k)$, where $\sigma$ is is a non-linearity. As a result, wide non-linear RNNs can in principle approximate non-linear dyamical systems, as we discuss in detail in App.~\ref{app:rw_MLP}.\\
Meanwhile, linear RNNs, where $x_k = A x_{k-1} + B u_k$, have often been considered of minor interest, as they equivalent in approximation power to convolutions~\citep{li2022approximation}~(see App.~\ref{app:rw_MLP}). In this paper we take a different approach: we show that when sufficiently wide, the linear RNNs \emph{do not} form a bottleneck, and the architecture maintains universality through the application of the pointwise MLP on the hidden state, as done in recent SSMs achieving state-of-the-art results~\citep{gu2021efficiently, smith2022simplified, orvieto2023resurrecting}. As motivated thoroughly in the paper, this architecture unlocks parallel computation, in contrast to what is possible with directly placing non-linearities in the recurrence.

\section{Approximation theory for (non-linear) RNNs}
\label{app:rw_MLP}

We recall a result on universality of MLPs already stated in the main paper.

\barron*
\barronthm*


\subsection{Guarantees for RNNs with recurrent non-linearities} Research on universality of non-linear RNNs dates back to~\citep{siegelmann1992computational}. We present here a more recent result by \citet{hanson2020universal}.

\begin{theorem}[Universality of non-linear RNNs] Consider the continuous-time non-linear dynamical system with form
\begin{equation}
    \dot{\bar{x}}(t) = \bar f(\bar x(t), u(t)),\quad \bar y(t) = h(\bar x(t)),
    \label{eq:non-linear-RNN-ct}
\end{equation}
with $\bar x(t)\in\R^{\bar N}$, $u(t)\in\R^M$. Under some technical assumptions~(bounded input, non-chaotic $f$), for any $\epsilon>0$ there exists a non-linear RNN
\begin{equation}
    \dot x(t) = -\frac{1}{\tau} x(t) + \sigma(A x(t) + Bu(t)), \quad y(t) = Cx(t),
    \label{eq:non-linear-RNNapprox-ct}
\end{equation}
for some non-polynomial $\sigma$, $\tau>0$, $A\in\R^{N\times N}$, $B\in\R^{N\times M}$, $C\in\R^{M\times N}$ that approximates the solution to Eq.~\ref{eq:non-linear-RNN-ct} up to error $\epsilon$ uniformly in time, on compact sets of inputs.
\end{theorem}
The result above typically involves taking $N$~(RNN width) to infinity.

\textit{Proof.} We briefly outline the idea behind the proof, and invite the reader to refer to~\cite{hanson2020universal} for details. Approximating the solution to Eq.~~\ref{eq:non-linear-RNN-ct} is equivalent to approximating the infinitesimal solution generator, which is a non-linear function of $(x, u)$. By Barron's Theorem~(Thm.~\ref{thm:barron_thm}), this generator can be approximated by a one-layer MLP, that is exactly the right-hand side of Eq.~\ref{eq:non-linear-RNNapprox-ct}.
\proofend

\subsection{Guarantees for linear RNNs} Simply taking out the non-linearity from the recurrence in Eq.~\ref{eq:non-linear-RNNapprox-ct} restricts the function class to convolutions. To start, recall that the linear continuous-time RNN on one-dimensional inputs
\begin{equation*}
    \dot x(t) = A x(t) + B u(t), \quad y(t) = Cx(t),
\end{equation*}
with $A\in\R^{N\times N}$, $B \in\R^{N\times M}$, $C \in\R^{1\times N}$
has solutions given by a convolution.
\begin{equation*}
    x(t) = \int_0^t C^\top e^{As}B u(t-s) ds =: \int_0^t \rho(s)^\top u(t-s) ds =: \sum_i(\rho^i\star u^i)_t.
\end{equation*}
Let us call $\hat{\mathcal{H}_N}$ the class of functionals parametrizable with linear RNNs with hidden state of dimension $N$, and $\hat{\mathcal{H}} = \cup_{N\in\mathbb{N}_+}\mathcal{H}_N$. 
\begin{equation*}
    \mathcal{H}_N :=\left\{ \{H_t
: t \in \R\}, H_t(u) = \int_{0}^{t}C^\top e^{As} Bu(t-s) ds, C\in\R^{1\times N}, A\in\R^{N\times N}, B\in \R^{N\times M}\right\}.
\end{equation*}
It turns out that convolutions are dense in the class of linear functionals. Let $\mathcal{U} = C_0(\R,\R^d)$ with norm $\|u\| = \sup_{t\in\R} \|u(t)\|_\infty$.
\begin{theorem}[Linear functionals in $C_0(\R,\R^d)$ are convolutions \citep{li2022approximation}]
Let $\{H_t
: t \in \R\}$ be a
family of continuous, linear, causal, regular, and time-homogeneous functionals on $\mathcal{U}$, i.e. such that
\begin{enumerate}
    \item (Continuous)  $\forall t\in\R$, $\sup_{\|u\|<1} H_t(u)<\infty$.
    \item (Linear) $\forall t\in\R$, $u,v\in \mathcal{U}$ and $\nu,\lambda\in \R$, we have $H_t(\lambda u + \nu v) = \lambda H_t(u) + \nu H_t(v)$.
    \item (Causal) For all $u,v\in\mathcal{U}$ such that $u(s)=v(s)$ for all $s\le t$, he have $H_t(v)=H_t(u)$.
    \item (Regular) Let $(u^n)$ be a sequence in $\mathcal{U}$ s.t. $u^n(s)\to 0$ for almost every $s\in\R$, then, for all $t\in\R$, we have $\lim_{n\to\infty} H_t(u^n)=0$.
    \item (Time Homogeneous) For all $u\in \mathcal{U}$ let $u^{\tau}_t = u(t-\tau)$, then $H_{t}(u^{\tau}) = H_{t+\tau}(u)$. 
\end{enumerate}
Then, for any $\{H_t
: t \in \R\}$ there exist a function (a kernel) $\bar \rho:\R_+\to\R^M$ such that for all $t\in\R$.
\begin{equation*}
    H_t(u) = \int_{0}^\infty \rho(s)^{\top} u(t-s) ds = \sum_i(\bar \rho^i\star u^i)_t.
\end{equation*}
\end{theorem}


\begin{theorem}[Linear RNNs can parametrize any convolution \citep{li2022approximation}]
Let $\{H_t
: t \in \R\}$ be a
family of continuous, linear, causal, regular, and time-homogeneous functionals on $\mathcal{U}$. Then,
for any $\epsilon > 0$ there exists $\{\hat H_t: t \in \R\}\in\hat{\mathcal{H}}$ such that
$$
    \sup_{t\in\R}\sup_{\|u\|<1} \|H_t(u)-\hat H_t(u)\|\le \epsilon.
$$
\label{thm:linear_rnn}
\end{theorem}
The result above typically involves taking $N$~(RNN width) to infinity.

\paragraph{This paper.} There is a sizable gap between the power of nonlinear and linear RNNs. We show in this paper that placing a nonlinearity at the output of linear RNNs~(unlocking parallel computation) allows approximation of arbitrary regular non-linear sequence-to-sequence mappings.

% \section{Reconstruction of sparse multidimensional inputs}
% \label{app:multidim_rec}

% As initialization of $\Lambda$, we are going to make use of the following lemma from~\cite{orvieto2023resurrecting},
% \begin{lemma}
%  Let $u_1,u_2$ be independent uniform random variables on the interval $[0,1]$. Let $0\le r_{\min}\le r_{\max}\le1$. Compute $\nu = -\frac{1}{2}\log\left(u_1(r_{\max}^2-r_{\min}^2)+r_{\min}^2\right)$ and $\theta = 2\pi u_2 $. Then $\exp(-\nu+i\theta)$ is uniformly distributed on the ring in $\mathbb{C}$ between circles of radii $r_{\min}$ and $r_{\max}$. We denote this distribution as $\mathbb{T}(r_{\min}, r_{\max})$.
%  \label{lemma:sampling_exp}
%  \end{lemma}

% Assumption~\ref{ass:sparse} implies the result below, which is complemented by the simulation in Fig.~\ref{fig:prop1}\&\ref{fig:prop2} in the appendix. Fig.~\ref{fig:architecture} illustrates the result through a concrete example on the MNIST dataset.
% \begin{proposition}[LRU memory] Let $M$ dimensional input sequences $u\in\R^{M\times L}$, with $L\le L_{\max}$ be sampled from a distribution $\mathcal{U}$. Assume that $\mathcal{U}$ is such that Assumption~\ref{ass:sparse} holds true almost surely. Let $E:\R^{M}\to\R^{H}$, $H=M+1$, be an affine position-wise encoder that appends the feature $1$ for all multidimensional inputs: $u\overset{E}{\to}\tilde u$. Let then $\tilde u\in\R^{H\times L}$ be the input to the following recursive LRU computation: $x_k = \Lambda x_{k-1} + B\tilde u_{:,k}$, with $B\in \C^{N\times H}$ a specific matrix~(see Eq.\eqref{eq:matrix_B}), $\Lambda=\diag(\lambda_1,\lambda_2,\dots, \lambda_N)$ and each $\lambda_i\sim\mathbb{T}(r_{\min}, r_{\max})\subseteq\C$ according to Lemma~\ref{lemma:sampling_exp}. If $N $ big enough but still way smaller than $L_{\max}$
% %(~\antonio{To be precise is a bit hard, working on it})
% , there exists a linear map that from each $x_k$~(seen as a generic vector in $\C^N$, with no access to its index) can recover the coefficients $\alpha^{u,k}$ for the decomposition $u_{:,1:k} = \sum_{i=1}^P \alpha^{u,k}_i\psi^i_{:,1:k}$, with probability one. Moreover, there exists a non-linear continuous map that from $x_k$ can recover $k$.
% \label{prop:approx}
% \end{proposition}


% Since, while making use of Assumption~\ref{ass:sparse}, we want to write $u$ as a matrix multiplication of basis functions times coefficients, it is useful to work with vectorized quantities. Let us define
% \begin{equation}
%     \vect(u_{:,1:k}) := \begin{pmatrix}
% u_{1,1:k}^\top \vspace{1mm}\\
%  u_{2,1:k}^\top \\
%  \vdots \\
%  u_{M,1:k}^\top
%     \end{pmatrix}\in\R^{kM},
% \end{equation}
% Next, let us define the matrix
% \begin{equation}
%     \Psi_k := (\vect(\psi^1_{:,1:k}),\vect(\psi^2_{:,1:k}),\cdots, \vect(\psi^P_{:,1:k}))\in\R^{kM\times P}.
% \end{equation}
% Then, Assumption~\ref{ass:sparse} implies
% \begin{equation}
%     \vect(u_{:,1:k}) = \Psi_k \alpha^{u,k}
% \end{equation}
% where $\alpha^{u,k} = (\alpha_1^{u,k}, \alpha_2^{u,k},\cdots, \alpha_P^{u,k})^\top\in\R^{P\times 1}$, for all $k$. Note that the coding vector $\alpha$ has fixed size, independent of $k$.

% We assume the encoder $E:\R^{M}\to\R^{H}$, $H=M+1$, preceding the linear recurrent unit~(LRU) already appended the feature $1$ positionwise to each multidimensional input, such that $\tilde u_{:,i} = (1,u_{:,i})\in\R^{H}$ for all $i\in\{1,2,\dots,L\}$ --- we will use this dummy feature to recover the inner state position in the sequence. We denote by $\tilde u = (\tilde u_{:,1}, \tilde u_{:,2},\cdots, \tilde u_{:,L})\in\R^{H\times L}$ the LRU input. 
% \begin{equation}
%     \tilde u =
%     \begin{pmatrix}
%     1 & 1 & 1&\cdots& 1\\
%     u_{1,1} &u_{1,2} & u_{1,3}&\cdots&u_{1,L}\\
%     \vdots&\vdots & \vdots&\ddots&\vdots\\
%     u_{M,1} &u_{M,2} & u_{M,3}&\cdots&u_{M,L}\\
%     \end{pmatrix}
% \end{equation}
% All in all, we can write
% \begin{equation}
%     \tilde u_{:,1:k} = (1_{k\times 1}, u_{:,1:k})^\top,\quad \vect(u_{:,1:k}) = \Psi_k \alpha^{u,k}
% \end{equation}

% The LRU is $x_k = \Lambda x_{k-1} + B \tilde u_k$, where $x_k\in\R^N$ and $\tilde u_k\in\R^{H}$. We pick $N = RM+1$~(where $R>0$ we will fix later), and the following input projection $B\in\C^{N\times H}$:
% \begin{equation}
%     B = \begin{pmatrix}
% 1& 0 & \cdots & 0 & 0\\
% 0 & 1_{R\times 1} & \cdots & 0_{R\times 1} & 0_{R\times 1}\\
% 0 & 0_{R\times 1} & \cdots & 0_{R\times 1} & 0_{R\times 1}\\
% \vdots & \vdots & \ddots & \vdots & \vdots\\
% 0 & 0_{R\times 1} & \cdots & 1_{R\times 1} & 0_{R\times 1}\\
% 0 & 0_{R\times 1} & \cdots & 0_{R\times 1} & 1_{R\times 1}\\
% \end{pmatrix}.
% \label{eq:matrix_B}
% \end{equation}
% The result of the LRU computation is a block diagonal version of the Vandermonde multiplication we had in the simplified setting
% \begin{equation}
%     \tilde x_k  =
%     \begin{pmatrix}
%   \sum_{i=0}^{k-1}\lambda_0^i
%   & \rvline &  & \rvline & & \rvline & &  \rvline & \\
% \hline
% & \rvline & V_{k,1} & \rvline & & \rvline & &  \rvline & \\
% \hline
%   & \rvline &  &\rvline & V_{k,2}& \rvline & &  \rvline & \\
% \hline
%   & \rvline &  &\rvline & & \rvline & \ddots &  \rvline & \\
% \hline
%   & \rvline &  &\rvline & & \rvline && \rvline  &  V_{k,M} \\
% \end{pmatrix}
%     \begin{pmatrix}
%     1\\
%     \vspace{1mm}
% u_{1,1:k}^\top \\
%     \vspace{1mm}
%  u_{2,1:k}^\top \\
%  \vdots \\
%  u_{M,1:k}^\top
%     \end{pmatrix} = \tilde V_k  \begin{pmatrix}1\\
%     \vect(u_{:,1:k})\end{pmatrix},
% \end{equation}
% where $\tilde V_k=\diag(1,V_k)$, $V_k = \diag(V_{k,1},V_{k,2},\cdots,V_{k,M})$, and $V_{k,j}$ is the Vandermonde matrix of width $k$ corresponding to the block $\Lambda_j$ in the diagonal matrix $\Lambda$.
% \begin{align}
% &\Lambda = \diag(\lambda_0, \Lambda_1,\Lambda_2,\cdots,\Lambda_M)\in\C^{(RM+1)\times(RM+1)},\\
%     &\Lambda_j = \diag(\lambda_{1,j},\lambda_{2,j},\cdots,\lambda_{R,j})\in\C^{R\times R},\\
%     &V_{k,j} = \begin{pmatrix}
%     \lambda_{1,j}^{k-1}&\lambda_{1,j}^{k-2} &\cdots& \lambda_{1,j}&1\\
%     \lambda_{2,j}^{k-1}&\lambda_{2,j}^{k-2} &\cdots& \lambda_{2,j}&1\\
%     \vdots&\vdots &\ddots&\vdots&\vdots\\
%     \lambda_{R,j}^{k-1}&\lambda_{R,j}^{k-2} &\cdots& \lambda_{R,j}&1\\
%     \end{pmatrix}\in\C^{R\times k}.
% \end{align}
% We use Assumption~\ref{ass:sparse} to write
% \begin{equation}
%     x_k =  \tilde V_k \begin{pmatrix}1\\
%     \vect(u_{:,1:k})\end{pmatrix} = \begin{pmatrix}\sum_{i=0}^{k-1}\lambda_0^i\\
%      (V_k \Psi_k) \alpha^{u,k}\end{pmatrix},
% \end{equation}
% The timestamp $k$ can be recovered in a trivial way, under the assumption that $\lambda_0\in\C$ is such that $|\lambda_0|\le 1$:
% \begin{equation*}
%     x_{k,0}=\sum_{i=0}^{k-1}\lambda_0^i = \frac{\lambda_0^k-1}{\lambda_0-1}\quad\implies\quad k = \log_{\lambda_0}(x_{k,0}(\lambda_0-1)+1).
% \end{equation*}
% Next, we show how to recover the coding coefficients $\alpha^{u,k}$. Let us define
% \begin{equation}
%     \Omega_k = V_k\Psi_k,\qquad\text{such that}\quad x_k = \Omega_k \alpha^{u,k}.
% \end{equation}
% Now, to get $\alpha^{u,k}$ one can compute a~(left) pseudoinverse
% \begin{equation}
%     \alpha^{u,k} = \Omega_k^+ x_k.
% \end{equation}
% Since $\Omega_k\in\C^{(N-1)\times P}$, with $N = RM+1$, for the system to be overdetermined, a necessary condition is $N-1=RM > P$, i.e $R\ge \lceil P/M\rceil$. To be able to recover $\alpha^{u,k}$, we need $\Omega$ to have row rank bigger or equal to the column rank. In practice, for numerical feasibility, one needs $\Sigma_k$ not to be ill-conditioned. In Figures~\ref{fig:prop1},\&~\ref{fig:prop2} we show this is achievable by using $\Lambda$ initialized very close to the unit disk, i.e. under the LRU initialization. Under the assumption that $\Omega_k$ is not too ill-conditioned, one can perfectly recover $u_{1:k}$ with a linear map $\vec(u_{1:k}) = \Psi_k\Omega_k^+ x_k$.

\newpage
\section{Details on Multidimensional Input Reconstruction}

In the main text, we showed that linear diagonal RNN computations on one-dimensional input sequences can be written in matrix form using a Vandermonde matrix~(Sec.~\ref{sec:vandermonde}). For convenience of the reader, we repeat the reasoning here: let $H=M=1$, the encoder $e$ be the identity, and $B = (1,1,\dots, 1)^\top$. Then, eq.~(\ref{eq:lin_rnn_unroll}) can be written as
\begin{equation*}
    x_k   =
    \begin{pmatrix}
    \lambda_1^{k-1}&\lambda_1^{k-2} &\cdots& \lambda_1&1\\
    \lambda_2^{k-1}&\lambda_2^{k-2} &\cdots& \lambda_2&1\\
    \vdots&\vdots &\ddots&\vdots&\vdots\\
    \lambda_N^{k-1}&\lambda_N^{k-2} &\cdots& \lambda_N&1\\
    \end{pmatrix}
    \begin{pmatrix}
    u_1 \\ u_{2} \\ \vdots \\ u_k
    \end{pmatrix}= V_k u_{1:k}^\top.
\end{equation*}
where $u_{1:k}= v_{1:k} = (v_i)_{i=1}^k \in\R^{1\times k}$, and $V_k$ is a Vandermonde matrix. As long as $N\ge k$, we can hope to recover $u_{1:k}$ by pseudoinversion of the Vandermonde:
\begin{equation*}
    v_{1:k}^\top = u_{1:k}^\top = V_k^+ x_k,
\end{equation*}

Here, we give  details on the design of input projections such that the RNN output from multidimensional inputs can also be seen as matrix multiplication.  Let us define
\begin{equation*}
    \vect(v_{1:k}) := \begin{pmatrix}
v_{1,1:k}^\top \vspace{1mm}\\
 v_{2,1:k}^\top \\
 \vdots \\
 v_{M,1:k}^\top
    \end{pmatrix}\in\R^{kM},
\end{equation*}

The matrix $B$ we are going to use in our linear diagonal RNN is
\begin{equation*}
    B = \begin{pmatrix}
 1_{N'\times 1} & \cdots & 0_{N'\times 1} & 0_{N'\times 1}\\
 0_{N'\times 1} & \cdots & 0_{N'\times 1} & 0_{N'\times 1}\\
  \vdots & \ddots & \vdots & \vdots\\
 0_{N'\times 1} & \cdots & 1_{N'\times 1} & 0_{N'\times 1}\\
 0_{N'\times 1} & \cdots & 0_{N'\times 1} & 1_{N'\times 1}\\
\end{pmatrix},
\end{equation*}
where we select $N = MN'$. With this choice, the linear diagonal RNN output can be written as
\begin{equation*}
    x_k  =
    \begin{pmatrix}
V_{k,1} & \rvline & & \rvline & &  \rvline & \\
\hline
  &\rvline & V_{k,2}& \rvline & &  \rvline & \\
\hline
  &\rvline & & \rvline & \ddots &  \rvline & \\
\hline
  &\rvline & & \rvline && \rvline  &  V_{k,M} \\
\end{pmatrix}
    \begin{pmatrix}
v_{1,1:k}^\top \\
    \vspace{1mm}
 v_{2,1:k}^\top \\
 \vdots \\
 v_{M,1:k}^\top
    \end{pmatrix} = \tilde V_k  \begin{pmatrix}1\\
    \vect(v_{:,1:k})\end{pmatrix},
\end{equation*}

where $V_k = \diag(V_{k,1},V_{k,2},\cdots,V_{k,M})$, and $V_{k,j}\in\C^{N'\times k}$ is the Vandermonde matrix corresponding to the block $\Lambda_j$ in the diagonal recurrent matrix $\Lambda$:
\begin{align*}
&\Lambda = \diag( \Lambda_1,\Lambda_2,\cdots,\Lambda_M)\in\C^{(N'M)\times(N'M)},\\
    &\Lambda_j = \diag(\lambda_{1,j},\lambda_{2,j},\cdots,\lambda_{N',j})\in\C^{N'\times N'},\\
    &V_{k,j} = \begin{pmatrix}
    \lambda_{1,j}^{k-1}&\lambda_{1,j}^{k-2} &\cdots& \lambda_{1,j}&1\\
    \lambda_{2,j}^{k-1}&\lambda_{2,j}^{k-2} &\cdots& \lambda_{2,j}&1\\
    \vdots&\vdots &\ddots&\vdots&\vdots\\
    \lambda_{N',j}^{k-1}&\lambda_{N',j}^{k-2} &\cdots& \lambda_{N',j}&1\\
    \end{pmatrix}\in\C^{N'\times k}.
\end{align*}
This construction effectively decouples each input dimension and reduces the discussion to the one-dimensional setting: invertibility of $V_k$ is guaranteed by invertibility of each block, provided $N'\ge L$ and that eigenvalues are distinct. Slight changes can be made to keep also track of the timestamp~(see Sec.~\ref{sec:main_idea}) and to adapt to the sparse setting~(see Sec.~\ref{sec:sparse}).

\newpage
\section{Expressivity Proofs}

One of our main steps involved computation of the Barron constant of the function mapping the RNN hidden state to the output.
\mainMLPsing*

Since $\Omega_k$ is a matrix, the result can be proved by computing the Barron constant of a function where the argument is the output of a linear map.


\begin{restatable}[Change of variables]{lemma}{cov}
Let $A\in\R^{p\times n}$ and $f(x) = g(A x)$, then 
$$C_f = \|A\|_2 C_g.$$
\end{restatable}

\textit{Proof.} The inverse Fourier transform formula directly leads to
\begin{equation}
    f(x) = \int_{\R^p} e^{i \langle p, A x\rangle} \mathcal{G}(\xi)d\xi.
\end{equation}
Let us now compute the Fourier Transform of $f$.
\begin{align}
    \F(\omega) &= \int_{\R^n} e^{-i\langle \omega, x \rangle} f(x) dx\\
    &= \int_{\R^n} e^{-i\langle \omega, x \rangle} \left[\int_{\R^p} e^{i \langle \xi, A x\rangle} \mathcal{G}(\xi)d\xi\right] dx\\
    &= \int_{\R^n}\int_{\R^p} e^{-i\langle \omega, x \rangle}  e^{i \langle  A^\top \xi, x\rangle} \mathcal{G}(\xi)d\xi dx\\
    &= \int_{\R^p}\left[\int_{\R^n} e^{i \langle A^\top\xi  - \omega, x\rangle}dx\right] \mathcal{G}(\xi)d\xi.
\end{align}

Recall now the definition of the Dirac delta:
\begin{equation}
\delta(z) = \frac{1}{2\pi}\int_{\R}e^{i\nu z}d\nu.
\end{equation}
Therefore
\begin{equation}
    \F(\omega) =\int_{\R^p} \delta(A^\top\xi  - \omega)\mathcal{G}(\xi)d\xi.
\end{equation}
Note that this is a singular measure in $\R^n$: lives in a linear $p$-dimensional subspace. Further, note that
\begin{align}
    C_f &= \int_{\R^n}\|\omega\|_2\cdot|\F(\omega)|d\omega\\
    &= \int_{\R^n}\|\omega\|_2\cdot\big|\int_{\R^p} \delta(A^\top\xi  - \omega)\mathcal{G}(\xi)d\xi\big|d\omega\\
    &\le \int_{\R^n}\|\omega\|_2\cdot\int_{\R^p} \delta(A^\top\xi  - \omega)|\mathcal{G}(\xi)|d\xi d\omega\\
    &\le \int_{\R^p}\left[\int_{\R^n}\|\omega\|_2 \delta(A^\top\xi  - \omega) d\omega\right] |\mathcal{G}(\xi)|d\xi\\
    &= \int_{\R^p}\|A^\top\xi\|_2\cdot  |\mathcal{G}(\xi)|d\xi\\
    &= \|A^\top\|_2\int_{\R^p}\|\xi\|_2\cdot  |\mathcal{G}(\xi)|d\xi.
\end{align}
The proof is done, since $\|A^\top\|_2 = \|A\|$~(same singular values), and $C_g = \int_{\R^p}\|\xi\|_2\cdot  |\mathcal{G}(\xi)|d\xi$.
\proofend


We now proceed proving the complexity for interpolation of sequences of Barron functions. This directly implies our main result~(Thm.\ref{thm:universal}).


\mainbarronk*

\textit{Proof.} Let us consider the following definition for $\tilde f$:
\begin{equation}
    \tilde f(x,t) = \sum_{k=1}^L f(x,k) h(t-k),
\end{equation}
where $h:\R\to\R$ is a filter~(see discussion after the proof) with support in $[-1,1]$. Compactness in the support of $h$ leads to the desired property $\tilde f(x,k) = f(x, t)$, for all $k = 1,2,\dots L$. Let us now compute the Fourier transform of $\tilde f$. Frequencies are of the form $\omega=(w,\nu)$, with $w\in\R^n$, $\nu\in\R$. 
\begin{align}
    \tF(\omega) &= \frac{1}{(2\pi)^{n+1}} \int_\R \int_{\R^n} \tilde f(x,t) e^{-i\langle w,x\rangle} e^{-i\nu t} dx dt\\
    &= \frac{1}{(2\pi)^{n+1}} \int_\R \int_{\R^n} \left[\sum_{k=1}^L f(x,k) h(t-k)\right] e^{-i\langle w,x\rangle} e^{-i\nu t} dx dt\\
    &= \frac{1}{(2\pi)^{n+1}} \sum_{k=1}^L \left[ \int_{\R^n} f(x,k)  e^{-i\langle w,x\rangle}  dx\right]\cdot \left[\int_\R h(t-k) e^{-i\nu t} dt\right]\\
    &= \sum_{k=1}^L \tF_k(w) \mathcal{H}(\nu) e^{i\nu k},
\end{align}
where $\mathcal{H}$ is the Fourier transform of $h$, and the factor $e^{i\nu k}$ comes from the shift $h(\cdot -k)$. All in all, we get
\begin{equation}
    \tF(w,\nu) = \mathcal{H}(\nu) \sum_{k=1}^L \tF_k(w)  e^{i\nu k}.
\end{equation}
Trivially, 
\begin{equation}
    |\tF(w,\nu)| \le |\mathcal{H}(\nu)| \sum_{k=1}^L |\tF_k(w)|.
\end{equation}

Therefore:
\begin{align}
    \tilde C &= \int_{\R^{n+1}} \|\omega\|_2 |\tilde{\mathcal{F}} (\omega)| d\omega\\ &\le \int_{\R^n}\int_\R \|(w,\nu)\|_2 \cdot \left[ |\mathcal{H}(\nu)|\sum_{k=1}^L |\tF_k(w)|\right] dw d\nu\\
&=\sum_{k=1}^L \int_{\R^n}\int_\R \|(w,\nu)\|_2 \cdot |\mathcal{H}(\nu)|\cdot|\tF_k(w)| dw d\nu.
    \end{align}
    
Using the triangle inequality:
\begin{align}
    \tilde C &\le \int_{\R^n}\int_\R (\|w\|_2 + |\nu| )\cdot |\mathcal{H}(\nu)|\cdot|\tF_k(w)| dw d\nu\\ &= \int_{\R^n}\int_\R \|w\|_2\cdot |\mathcal{H}(\nu)|\cdot|\tF_k(w)| dw d\nu + \int_{\R^n}\int_\R |v|\cdot |\mathcal{H}(\nu)|\cdot|\tF_k(w)| dw d\nu\\
    &= C_{f_k} C'_{h}+ C_h C_{f_k}'.
\end{align}
This concludes the proof since~(see next paragraph) $C_h$ and $C'_h$ are problem-independent bounded constants.
\hfill $\square$

The proof of the theorem above concludes stating that $C_h$ and $C'_h$ are problem-independent bounded constants. Recall that, in our proof, $h:\R\to\R$ has compact support in $[-1,1]$. We can design $h$ such that its Fourier transform has fast decay:

\begin{restatable}[\cite{tlas2022bump}]{theorem}{tlas}
For any $\delta \in (0,1)$ and any $c >0$  there is a function $h(t)$ which is $C^\infty$, real, even, nonnegative, supported in $[-1,1]$ and whose Fourier transform $\mathcal{H}(\nu)$ is monotone decreasing for $\nu \geq 0$ and satisfies the following double inequality 
\begin{equation}
      \exp(- (1+\epsilon) c \nu^\delta ) \lesssim  \mathcal{H}(\nu)  \lesssim  \, \, \exp(- (1- \epsilon) c \nu^\delta),
\end{equation}
for any $\epsilon > 0$.
\end{restatable}
This result, rooted in the Beurling-Malliavin multiplier theorem\cite{mashreghi2006beurling}, ensures that we can design $h$ on a compact support with exponentially decaying frequencies. This implies that both integrals
\begin{equation}
    C_h = \int_{\R} |\nu| |\mathcal{H} (\nu)|d\nu,\qquad 
    C_h' = \int_{\R} |\mathcal{H} (\nu)|d\nu
\end{equation}
are bounded and independent on the specific form of the function $f$ we want to approximate.




% \subsection{Approximation of a target from last token representation}

% Consider the task of approximating a \textbf{target scalar function} over the whole sequence. That is, assume that 
% \begin{equation}
%     y = f_L(u_{1:L}) = f_L(u_1,u_2,\dots, u_L).
% \end{equation}
% Let us assume this function is Barron, that is, $C_f<\infty$. As such, using $u_{1:L}$ as input of a 1HL-MLP, one needs $\frac{4r^2 C_f^2}{\epsilon^2}$ hidden neurons to approximate $f$ at level $\epsilon$ on the compact set $\|u_{1:L}\|\le r$.

% Let us now consider instead approximating $f$ using an MLP head on the last token representation provided by a linear RNN. We now ask -- \textit{how many hidden neurons do we need to reconstruct $f$ directly from $x_L$}?

% As a first simplified setting, assume the RNN hidden state is as wide as the input length: $N = L$. In this case, $x_L = V_L u_{1:L}$ where the Vandermonde matrix $V_L\in\R^{L\times L}$ under the LRU initialization is invertible with probability 1 --- i.e. has non-vanishing determinant.

% We have the following result
% \begin{theorem}
% In the setting we just described, the number of hidden neurons needed to approximate $f$ at level $\epsilon$ from the last RNN hidden state $x_L$ is
% \begin{equation}
%     H\ge \frac{4 r^2 C_f^2 \|V_L^{-1}\|^2_2}{\epsilon^2},
% \end{equation}
% where $r$ bounds the hidden state magnitude.
% \label{thm:approx_barron_last}
% \end{theorem}

% Recall the change of variables formula:
% \begin{lemma}
% Let $A$ be an invertible matrix, then
% \begin{equation}
%     \int_{\R^n} g(Ax)dx = \frac{1}{|\det(A)|}\int_{\R^n} g(u)du.
% \end{equation}
% \end{lemma}

% This formula directly implies the next result:
% \begin{theorem}[Fourier Transform Stretch]
% Let $f$ be $L^1(\R^n)$. Let $W\in\R^{n\times n}$ be an invertible matrix and let $W^{-\top}$ be the inverse of its transpose. We have
% \begin{equation}
%     \mathcal{F}(f(W\cdot))(w) = \frac{1}{|\det(W)|}  \hat f(W^{-\top} w)
% \end{equation}
% \label{thm:fourier_of_stretch}
% \end{theorem}

% \textit{Proof of Thm.~\ref{thm:approx_barron_last}}
% In our setting, $f(u_{1:L}) = f(V_L^{-1} x_L):= \bar f_L(x_L)$. Theorem~\ref{thm:fourier_of_stretch} implies
% \begin{equation}
%     \hat{\bar {f}}_L(w) = \mathcal{F}(\bar f_L))(w) = \frac{1}{|\det(V_L^{-1})|}  \hat f(V_L^{\top} w)
% \end{equation}

% Let us then proceed computing the Barron constant of $f_L$
% \begin{equation}
%     C_{\bar f_L} = \frac{1}{|\det(V_L^{-1})|} \int_{\R^d} \|w\|_2 |\hat f(V_L^{\top} w)| dw<\infty.
% \end{equation}
% Consider the change of variables $u = V_L^{\top} w$. Then,
% \begin{align}
%     C_{\bar f_L} &= \frac{1}{|\det(V_L^{-1})|} \int_{\R^d} \|V_L^{-\top} V_L^{\top}w\|_2 |\hat f(V_L^{\top} w)|dw\\ &= \frac{1}{|\det(V_L^{-1})||\det(V_L^\top)|} \int_{\R^d} \|V_L^{-\top} u\|_2 |\hat f(u)|du\\
%     &= \int_{\R^d} \|V_L^{-\top} u\|_2 |\hat f(u)|du\\
%     &= \int_{\R^d} \|V_L^{-\top}\|_2 \| u\|_2 |\hat f(u)|du\\
%     &\le \|V_L^{-\top}\|_2 C_f\\
%     &= \|V_L^{-1}\|_2 C_f
% \end{align}
% This concludes the proof.

% \proofend

% Intuitively, the theorem we just proved shows that estimating $f$ from the hidden state is easy, if the maximum eigenvalue of $V_L^{-1}$ is not too large -- that we can obtain with diagonal recurrences with eigenvalues close to the unit disk.


% \begin{theorem} Set $\tilde C_f = \frac{1}{L}\sum_{k =0}^{L-1} C_{f_k} \|V_k^{-1}\|_2$ and $L_{1}^{f,a} = \frac{1}{L}\sum_{k =0}^{L-1} 2\sqrt{\frac{a}{\pi}} \|\hat{ \bar{f}}_k\|_{L_1}$.
% In the setting we just described, with level of smoothing $a$ the number of hidden neurons needed to approximate $\tilde f(x,k)$ at level $\epsilon$ from the last RNN hidden state is
% \begin{equation}
%     H\ge \frac{4 r^2 L (\tilde C_f + L_{1}^{f,a})}{\epsilon^2},
% \end{equation}
% where $r$ bounds the hidden state magnitude and $L$ the makixum sequence length.
% \label{thm:approx_barron_all}
% \end{theorem}



% \paragraph{Warmup.} Consider as a warm-up the case where we have $f(x) = g(a^\top x)$, where $x, a\in\R^d$. Given $C_g$ the Barron constant for $g:\R\to\R$, what is the Barron constant for $f$?

% First, note that one can write
% \begin{equation}
%     f(x) = \int_{\R} e^{i\nu \langle a,x\rangle}\mathcal{G}(\nu)d\nu
% \end{equation}
% Indeed, recall the Dirac identity
% \begin{equation}
%     \delta(z) = \frac{1}{2\pi}\int_{\R}e^{i\nu z}d\nu.
% \end{equation}
% From this, we get
% \begin{align}
%     f(x) & = g(\langle a,x\rangle)\\
%     & = \int_{\R}\delta(\langle a,x\rangle-y) g(y) dy\\
%     & = \frac{1}{2\pi}\int_{\R}\int_\R e^{i (\langle a,x\rangle-y) \nu} g(y) d\nu  dy\\
%     & = \frac{1}{2\pi}\int_{\R}\int_\R e^{-iy\nu} e^{i\langle a,x\rangle \nu} g(y) d\nu  dy\\
%     & = \int_{\R} e^{i\langle a,x\rangle \nu}\left[\frac{1}{2\pi}\int_\R e^{-iy\nu}  g(y) dy\right]  d\nu\\
%     & = \int_{\R} e^{i\nu \langle a,x\rangle}\mathcal{G}(\nu)d\nu
% \end{align}

% \begin{restatable}[Change of variables]{lemma}{cov}

% \end{restatable}

% \textit{Proof.}
% let $A_i$ be the i-th row of $A$ and $y_i$ be the $i$-th element of $y$.
% \begin{align}
%     f(x) & = g(Ax)\\
%     & = \int_{\R^m}\left[\prod_{i=1}^m\delta(A_ix-y_i)\right] g(y) dy\\
%     & = \frac{1}{(2\pi)^m}\int_{\R^m}\left[\prod_{i=1}^m\int_\R e^{i (A_ix-y_i) \nu_i} d\nu_i\right] g(y)   dy\\
%     & = \frac{1}{(2\pi)^m}\int_{\R^m}\int_{\R^m} e^{i \langle v, A x-y\rangle} g(y)  d v  dy\\
%     & = \int_{\R^m} e^{i \langle v, A x\rangle} \left[\frac{1}{(2\pi)^m}\int_{\R^m}  e^{-i \langle v, y\rangle} g(y)   dy\right]  d v\\
%     & = \int_{\R^m} e^{i \langle v, A x\rangle} \mathcal{G}(v)dv.
% \end{align}
% \proofend

\newpage
\section{Additional experiments}
\subsection{Reconstruction using the Vandermonde inverse~(support to Section~\ref{sec:vandermonde})} 
\label{app:rec_vandermonde}
We consider linear diagonal RNNs with the $N$ diagonal entries of $\Lambda$ sampled inside the unit disk in $\C$, uniformly in angle in between radii $r_{\min}$ and 1. We consider the hidden state $x_L\in\C^N$ computed after $L$ RNN steps, \textit{i.e. after the sequence is completely processed}. We want to recover the input sequence from the hidden state using the Vandermonde inverse $V_L^+$~(see Sec.~\ref{sec:vandermonde}). If $N\ge L$, since under random initialization the determinant of any set of $L$ columns of $V_L$ is positive, we can in theory achieve perfect reconstruction. In practice, $V_L$ is ill conditioned --- especially if $r_{\min}$ is not close to $1$. This causes some problem in the pseudoinverse computation, which may result in imperfect reconstruction. 

In Figure~\ref{fig:MNIST_rec_random_van} we provide evidence on the MNIST dataset~(flattened image $=$ 784 tokens): we observe that, if eigenvalues are sampled with $r_{\min}=0$, by multiplying $x_L$ by $V_L^+$ we are only able to recover recent history. Instead, moving closer to the unit disk allows almost perfect reconstruction at $N=784$, and a satisfactory result already at $N=512$.

% Figure environment removed

 In Figure~\ref{fig:MNIST_van_error_avg} we clearly show that the average reconstruction error~(average over $10k$ images samples and 10 random re-samplings of the linear RNN) is decreasing both as a function of the hidden state size~(see discussion in Sec.~\ref{sec:vandermonde}) and of $r_{\min}$~($r_{\max}=1$). The same pattern is observed for the condition number of $V_L^\top V_L$. On the same figure, we show how the error is distributed over timestamps: it is clear that, for $r_{\min}\ll1$, the reconstruction only covers the last few hundreds of tokens -- a property which is liked to the bad condition number observed in this setting.

% Figure environment removed

 Last, in Figure~\ref{fig:MNIST_rec_unit_van} we show what happens when picking the $N$ diagonal entries of $\Lambda$ to be the $N$-th complex roots of $1$: as shown in~\cite{cordova1990vandermonde}, in this setting the Vandermonde condition number is $1$. We observe that we can indeed reconstruct perfectly the output for $N=784$. However, for smaller values of $N$, the reconstruction presents undesired artifacts.

% Figure environment removed

\subsection{Reconstruction under sparsity~(support to \S\ref{sec:sparse})}
\label{app:rec_sparse}


In Figures~\ref{fig:prop1} and \ref{fig:prop2} we test the discussion in Sec.~\ref{sec:sparse} in a controlled setting. We consider one-dimensional stream of $L=4096$ random inputs sparse in a basis of $P=32$ Haar wavelets~\citep{haar1911theorie}. The linear diagonal RNN has $\Lambda\in\C^{N\times N}$ with eigenvalues sampled uniformly at random from $\mathbb{T}(0.95,1)$~(Fig.~\ref{fig:prop1}) or $\mathbb{T}(0.99,1)$~(Fig.~\ref{fig:prop2}), where
$$\mathbb{T}[r_{\min}, r_{\max}] := \{\lambda\in\C \ | \ r_{\min}\le|\lambda|\le r_{\max}\}.$$
We use matrix $B = (1,1,\dots, 1)^\top$. Plotted is the rank of $V_k,\Psi_k$, $\Omega_k = V_k\Psi_k$ as $k$ increases~(see notation in Sec.~\ref{sec:sparse}). We show how the reconstruction error behaves when reconstructing $u_{1:k} = \Psi_k\alpha^{u}_k$ with $\alpha^{u}_k = \Omega_k^+ x_k$. In the figures, we plot the error for reconstruction of the tokens $(u_i)_{i=1}^k$ from $x_k$, for all $k\le L$. As $N$ gets larger the reconstruction error gets uniformly negligible. In particular, if we initialize in $\mathbb{T}(0.95,1)$ then the minimum $N$ we need for perfect reconstruction of around $N=256$. If instead we initialize closer to the unit circle, then $N=64$ is enough for perfect reconstruction. This finding is similar to the one presented in Sec.~\ref{app:rec_vandermonde}. 

On a more fundamental level, we study the condition number of the matrix $\Omega^T\Omega$, where $\Omega = V_L\Psi_L$. This condition number quantifies the numerical stability of the pseudoinverse $\Omega^+$, used in reconstruction. In Figure~\ref{fig:better_conditioning}, we show the logarithm of the condition number for a sequence of length $512$ sparse in a basis of $P$ eigenfunctions. As $r_{\min}$ gets close to $1$ the condition number decreases as we saw in \S\ref{app:rec_vandermonde}. Crucially however, the condition number also decreases as $P$ decreases.


% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Approximation of sequence-to-sequence maps~(ODE Systems)}
We consider approximating sequence-to-sequence maps $(v_i)_{i=1}^L\overset{T}{\mapsto} (y_i)_{i=1}^L$ defined by Runge-Kutta discretization of the flow of a controlled differential equation $\dot z_t = f(z_t, v_t), y_t = h(z_t)$, where $(v_t)_{t}$ is the input, $f$ is a non-linear multidimensional function, $h$ projects the multidimensional state $z_t$ into a one-dimensional output. An example is the \textbf{Protein Transduction~(PT)} system~\citep{vyshemirsky2008bayesian}:
 \begin{align*}
     &\dot z_1(t) = -k_1 z_1(t) - k_2 z_1(t) z_3(t) + k_3 z_4(t) + v(t)\\
     &\dot z_2(t) = k_1 z_1(t)\\
     &\dot z_3(t) = - k_2 z_1(t) z_3(t) + k_3 z_4(t) + V \frac{z_5(t)}{K_m + z_5(t)}\\
     &\dot z_4(t) = k_2 z_1(t) z_3(t) - (k_3+k_4)z_4(t)\\
     &\dot z_5(t) = k_4 z_4(t) - V\frac{z_5(t)}{K_m + z_5(t)}
 \end{align*}
\vspace{-2mm}

We identify $y_k = z_1(\Delta k)$, where $\Delta = 0.01$, and $v_k = v(\Delta k)$. We sample $(v_i)_{i=1}^L$~($L=2048$ in PT) from a linear combination of 16 base wavelets of low frequency (\textit{bias: input is random but has slow variations}), and set the ground truth $(y_i)_{i=1}^L$ to be the result of Runge-Kutta integration with stepsize $\Delta$. As hyperparameters, we use $k_1 = 0.07, k_2 = 0.6, k_3 =
0.05, k_4 = 0.3, V = 0.017, K_m = 0.3, z_1(0)=1, z_2(0) = 0, z_3(0) = 1, z_4(0) =  z_5(0) = 0$ as prescribed by~\citet{vyshemirsky2008bayesian}. Results using approximation of one linear RNN followed by a 1HL-MLP~(shared across timestamps) are shown in Fig.~\ref{fig:ptpz} and discussed in the main text. To train, we use $10k$ random (low frequency) trajectories and test on $1k$ trajectories with same distribution. Experiments run on a single A5000 using an LRU in JAX~(\url{https://github.com/NicolasZucchet/minimal-LRU/tree/main/lru}).

In this appendix, we additionally discuss performance in approximating the solution of two other controlled ODEs. Settings are same as used for PT unless stated otherwise. The first ODE~(results in Fig.~\ref{fig:res_lv}) is a \textbf{Lotka-Volterra (LV)} system~\citep{lotka1925elements, volterra1928variations}:
\begin{align*}
\dot z_1(t) &= z_1(t)(a-b\cdot z_2(t)) \\
\dot z_2(t) &= -z_2(t)(c-d \cdot z_1(t))+v(t)
\end{align*}
where $a = 1.0, b= 0.6, c=1.0, d=0.7$, and we initialize $z_1(0) = 1.0 , z_2(0) = 0.5$ as used in~\citet{dondelinger2013ode}. For integration, we use a stepsize of $\Delta =0.01$ and $y_k = z_1(\Delta k)$. Here, sequence length is again $L=2048$.

Finally, we consider an extremely \textit{challenging scenario}: the \textbf{Lorentz system~(LZ)}~\citep{lorenz1963deterministic}, which notoriously has chaotic solutions~(named strange attractor, linked to the butterfly effect):
\begin{align*}
\dot z_1(t) &= \sigma \cdot (z_2(t)-z_1(t)) \\
\dot z_2(t) &= (r-z_3(t))z_1(t)-z_2(t)+ v(t) \\
\dot z_3(t) &= z_2(t)z_1(t)-b\cdot z_3(t)
\end{align*}
With our parameter choices $\sigma = 10, r= 26, b =8/3$ and initialization $z_1(0) = -0.89229143 , z_2(0) = 1.08417925, z_3(0) = 2.34322702$~(parameter source: Wikipedia, visited September 2023), the system has a chaotic behavior as shown in Figure~\ref{fig:lorentz_ill}. We choose an integration timestep $\Delta = 0.002$ and, due to the non-linear chaotic and possibly unstable nature of the controlled attractor, consider $L=512$ in this setting.
\vspace{-3mm}
\paragraph{Note on training.} Training one layer of linear RNN + MLP on these ODEs exhibits huge variation across seeds~(see Fig.~\ref{fig:seed_variability}). Since in this paper we want to show that \textit{there exist} a Linear RNN+MLP configuration able to model non-linear sequence to sequence maps, we consider the following setup: we run each experiment and hyperparameter sweep on seeds $1-6$, and only report the best performance on the test data (train loss always lower). Based on our experience with the LRU, we conclude that instability is due to the small dimension of our model: performance on standard tasks is more stable as depth increases~\citep{orvieto2023resurrecting}. To test our theory, we limit ourselves to a linear RNN with either $N=128$ or $N=256$, followed by an MLP with one hidden layer. We grid-search hyperparameters for each model configuration and report test error for the best-performing models. Usual best-performing stepsizes are $0.003$ and $0.01$. In Lotka-Volterra experiments~(Fig.~\ref{fig:res_lv}) we train for $200$ epochs, while we train on the Lorentz System~(Fig.~\ref{fig:res_lz}) for $1000$ epochs. No weight decay, dropout or normalizations are applied. \textbf{Results are discussed in the relative figure captions and nicely validate our claims}.  \textit{Code for reproducing the experiments will be provided upon acceptance of this paper}.


% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

