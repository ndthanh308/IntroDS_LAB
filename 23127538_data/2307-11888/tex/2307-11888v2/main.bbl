\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Arjovsky, M., Shah, A., and Bengio, Y.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{International conference on machine learning}. PMLR, 2016.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Barron(1993)]{barron1993universal}
Barron, A.~R.
\newblock Universal approximation bounds for superpositions of a sigmoidal function.
\newblock \emph{IEEE Transactions on Information theory}, 39\penalty0 (3):\penalty0 930--945, 1993.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bhatia(2013)]{bhatia2013matrix}
Bhatia, R.
\newblock \emph{Matrix analysis}, volume 169.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Blelloch(1990)]{blelloch1990prefix}
Blelloch, G.~E.
\newblock Prefix sums and their applications, 1990.

\bibitem[Boyd \& Chua(1985)Boyd and Chua]{boyd1985fading}
Boyd, S. and Chua, L.
\newblock Fading memory and the problem of approximating nonlinear operators with volterra series.
\newblock \emph{IEEE Transactions on circuits and systems}, 32\penalty0 (11):\penalty0 1150--1161, 1985.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio]{cho2014learning}
Cho, K., Van~Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using rnn encoder-decoder for statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Chung \& Siegelmann(2021)Chung and Siegelmann]{Chung21}
Chung, S. and Siegelmann, H.
\newblock Turing completeness of bounded-precision recurrent neural networks.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  28431--28441. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/ef452c63f81d0105dd4486f775adec81-Paper.pdf}.

\bibitem[Cirone et~al.(2024)Cirone, Orvieto, Walker, Salvi, and Lyons]{cirone2024theoretical}
Cirone, N.~M., Orvieto, A., Walker, B., Salvi, C., and Lyons, T.
\newblock Theoretical foundations of deep selective state-space models.
\newblock \emph{arXiv preprint arXiv:2402.19047}, 2024.

\bibitem[C{\'o}rdova et~al.(1990)C{\'o}rdova, Gautschi, and Ruscheweyh]{cordova1990vandermonde}
C{\'o}rdova, A., Gautschi, W., and Ruscheweyh, S.
\newblock Vandermonde matrices on the circle: spectral properties and conditioning.
\newblock \emph{Numerische Mathematik}, 57\penalty0 (1):\penalty0 577--591, 1990.

\bibitem[Cotterell et~al.(2023)Cotterell, Svete, Meister, Liu, and Du]{cotterell2023formal}
Cotterell, R., Svete, A., Meister, C., Liu, T., and Du, L.
\newblock Formal aspects of language modeling.
\newblock \emph{arXiv preprint arXiv:2311.04329}, 2023.

\bibitem[Dao(2023)]{dao2023flashattention}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock \emph{arXiv preprint arXiv:2307.08691}, 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Dao, T., Fu, D., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier]{dauphin2017language}
Dauphin, Y.~N., Fan, A., Auli, M., and Grangier, D.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}. PMLR, 2017.

\bibitem[De et~al.(2024)De, Smith, Fernando, Botev, Cristian-Muraru, Gu, Haroun, Berrada, Chen, Srinivasan, Desjardins, Doucet, Budden, Teh, Pascanu, Freitas, and Gulcehre]{de2024griffin}
De, S., Smith, S.~L., Fernando, A., Botev, A., Cristian-Muraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y., Srinivasan, S., Desjardins, G., Doucet, A., Budden, D., Teh, Y.~W., Pascanu, R., Freitas, N.~D., and Gulcehre, C.
\newblock Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.

\bibitem[Ding et~al.(2023)Ding, Orvieto, He, and Hofmann]{ding2023recurrent}
Ding, Y., Orvieto, A., He, B., and Hofmann, T.
\newblock Recurrent distance-encoding neural networks for graph representation learning.
\newblock \emph{arXiv preprint arXiv:2312.01538}, 2023.

\bibitem[Dondelinger et~al.(2013)Dondelinger, Husmeier, Rogers, and Filippone]{dondelinger2013ode}
Dondelinger, F., Husmeier, D., Rogers, S., and Filippone, M.
\newblock Ode parameter inference using adaptive gradient matching with gaussian processes.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  216--228. PMLR, 2013.

\bibitem[Folland(2009)]{folland2009fourier}
Folland, G.~B.
\newblock \emph{Fourier analysis and its applications}, volume~4.
\newblock American Mathematical Soc., 2009.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Re]{fu2023hungry}
Fu, D.~Y., Dao, T., Saab, K.~K., Thomas, A.~W., Rudra, A., and Re, C.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Funahashi(1989)]{funahashi1989approximate}
Funahashi, K.-I.
\newblock On the approximate realization of continuous mappings by neural networks.
\newblock \emph{Neural networks}, 2\penalty0 (3):\penalty0 183--192, 1989.

\bibitem[Gautschi(1975)]{gautschi1975optimally}
Gautschi, W.
\newblock Optimally conditioned vandermonde matrices.
\newblock \emph{Numerische Mathematik}, 24:\penalty0 1--12, 1975.

\bibitem[Gautschi \& Inglese(1987)Gautschi and Inglese]{gautschi1987lower}
Gautschi, W. and Inglese, G.
\newblock Lower bounds for the condition number of vandermonde matrices.
\newblock \emph{Numerische Mathematik}, 52\penalty0 (3):\penalty0 241--250, 1987.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022sashimi}
Goel, K., Gu, A., Donahue, C., and R{\'e}, C.
\newblock It's raw! audio generation with state-space models.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023mamba}
Gu, A. and Dao, T.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1474--1487, 2020.

\bibitem[Gu et~al.(2021)Gu, Goel, and Re]{gu2021efficiently}
Gu, A., Goel, K., and Re, C.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gu et~al.(2022)Gu, Gupta, Goel, and R{\'e}]{gu2022parameterization}
Gu, A., Gupta, A., Goel, K., and R{\'e}, C.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Gupta et~al.(2022)Gupta, Gu, and Berant]{gupta2022diagonal}
Gupta, A., Gu, A., and Berant, J.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Haar(1911)]{haar1911theorie}
Haar, A.
\newblock Zur theorie der orthogonalen funktionensysteme.
\newblock \emph{Mathematische Annalen}, 71\penalty0 (1):\penalty0 38--53, 1911.

\bibitem[Hanson \& Raginsky(2019)Hanson and Raginsky]{hanson2019universal}
Hanson, J. and Raginsky, M.
\newblock Universal approximation of input-output maps by temporal convolutional nets.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Hanson \& Raginsky(2020)Hanson and Raginsky]{hanson2020universal}
Hanson, J. and Raginsky, M.
\newblock Universal simulation of stable dynamical systems by recurrent neural nets.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  384--392. PMLR, 2020.

\bibitem[Hasani et~al.(2022)Hasani, Lechner, Wang, Chahine, Amini, and Rus]{hasani2022liquid}
Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D.
\newblock Liquid structural state-space models.
\newblock In \emph{The International Conference on Learning Representations}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Helfrich et~al.(2018)Helfrich, Willmott, and Ye]{helfrich2018orthogonal}
Helfrich, K., Willmott, D., and Ye, Q.
\newblock Orthogonal recurrent neural networks with scaled cayley transform.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2018.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Hornik(1991)]{hornik1991approximation}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{hornik1989multilayer}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Jelassi et~al.(2024)Jelassi, Brandfonbrener, Kakade, and Malach]{jelassi2024repeat}
Jelassi, S., Brandfonbrener, D., Kakade, S.~M., and Malach, E.
\newblock Repeat after me: Transformers are better than state space models at copying.
\newblock \emph{arXiv preprint arXiv:2402.01032}, 2024.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5156--5165. PMLR, 2020.

\bibitem[Katsch(2023)]{katsch2023gateloop}
Katsch, T.
\newblock Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Kolmogorov(1957)]{kolmogorov1957representation}
Kolmogorov, A.~N.
\newblock On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition.
\newblock In \emph{Doklady Akademii Nauk}, pp.\  953--956. Russian Academy of Sciences, 1957.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Cai, Zhang, Chen, and Dey]{li2022makes}
Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D.
\newblock What makes convolutional models great on long sequence modeling?
\newblock \emph{arXiv preprint arXiv:2210.09298}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Han, Weinan, and Li]{li2022approximation}
Li, Z., Han, J., Weinan, E., and Li, Q.
\newblock Approximation and optimization theory for linear continuous-time recurrent neural networks.
\newblock \emph{J. Mach. Learn. Res.}, 2022{\natexlab{b}}.

\bibitem[Lorenz(1963)]{lorenz1963deterministic}
Lorenz, E.~N.
\newblock Deterministic nonperiodic flow.
\newblock \emph{Journal of atmospheric sciences}, 20\penalty0 (2):\penalty0 130--141, 1963.

\bibitem[Lotka(1925)]{lotka1925elements}
Lotka, A.~J.
\newblock \emph{Elements of physical biology}.
\newblock Williams \& Wilkins, 1925.

\bibitem[Lu et~al.(2023)Lu, Schroecker, Gu, Parisotto, Foerster, Singh, and Behbahani]{lu2023structured}
Lu, C., Schroecker, Y., Gu, A., Parisotto, E., Foerster, J., Singh, S., and Behbahani, F.
\newblock Structured state space models for in-context reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L.
\newblock The expressive power of neural networks: A view from the width.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ma et~al.(2022)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer]{ma2022mega}
Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L.
\newblock Mega: moving average equipped gated attention.
\newblock \emph{arXiv preprint arXiv:2209.10655}, 2022.

\bibitem[Martin \& Cundy(2017)Martin and Cundy]{martin2017parallelizing}
Martin, E. and Cundy, C.
\newblock Parallelizing linear recurrent neural nets over sequence length.
\newblock \emph{arXiv preprint arXiv:1709.04057}, 2017.

\bibitem[Mashreghi et~al.(2006)Mashreghi, Nazarov, and Havin]{mashreghi2006beurling}
Mashreghi, J., Nazarov, F., and Havin, V.
\newblock Beurling--malliavin multiplier theorem: the seventh proof.
\newblock \emph{St. Petersburg Mathematical Journal}, 17\penalty0 (5):\penalty0 699--744, 2006.

\bibitem[M{\"u}ntz(1914)]{muntz1914approximationssatz}
M{\"u}ntz, C.~H.
\newblock \emph{{\"U}ber den approximationssatz von Weierstrass}.
\newblock Springer, 1914.

\bibitem[Nguyen et~al.(2022)Nguyen, Goel, Gu, Downs, Shah, Dao, Baccus, and Ré]{nguyen2022s4nd}
Nguyen, E., Goel, K., Gu, A., Downs, G.~W., Shah, P., Dao, T., Baccus, S.~A., and Ré, C.
\newblock S4nd: Modeling images and videos as multidimensional signals using state spaces.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{orvieto2023resurrecting}
Orvieto, A., Smith, S.~L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Cao, Cheng, Chung, Grella, GV, et~al.]{peng2023rwkv}
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K.~K., et~al.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023.

\bibitem[Pinkus(1999)]{pinkus1999approximation}
Pinkus, A.
\newblock Approximation theory of the {MLP} model in neural networks.
\newblock \emph{Acta numerica}, 8:\penalty0 143--195, 1999.

\bibitem[Sch{\"a}fer \& Zimmermann(2006)Sch{\"a}fer and Zimmermann]{schafer2006recurrent}
Sch{\"a}fer, A.~M. and Zimmermann, H.~G.
\newblock Recurrent neural networks are universal approximators.
\newblock In \emph{Artificial Neural Networks--ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16}, pp.\  632--640. Springer, 2006.

\bibitem[Siegelmann \& Sontag(1992{\natexlab{a}})Siegelmann and Sontag]{Siegelmann92}
Siegelmann, H.~T. and Sontag, E.~D.
\newblock On the computational power of neural nets.
\newblock In \emph{Proceedings of the Fifth Annual Workshop on Computational Learning Theory}, COLT '92, pp.\  440–449, New York, NY, USA, 1992{\natexlab{a}}. Association for Computing Machinery.

\bibitem[Siegelmann \& Sontag(1992{\natexlab{b}})Siegelmann and Sontag]{siegelmann1992computational}
Siegelmann, H.~T. and Sontag, E.~D.
\newblock On the computational power of neural nets.
\newblock In \emph{Proceedings of the fifth annual workshop on Computational learning theory}, pp.\  440--449, 1992{\natexlab{b}}.

\bibitem[Smith et~al.(2022)Smith, Warrington, and Linderman]{smith2022simplified}
Smith, J.~T., Warrington, A., and Linderman, S.~W.
\newblock Simplified state space layers for sequence modeling.
\newblock \emph{arXiv preprint arXiv:2208.04933}, 2022.

\bibitem[Smith et~al.(2023)Smith, Warrington, and Linderman]{smith2023simplified}
Smith, J.~T., Warrington, A., and Linderman, S.~W.
\newblock Simplified state space layers for sequence modeling.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2023retentive}
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.
\newblock Retentive network: A successor to transformer for large language models.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem[Sz{\'a}sz(1916)]{szasz1916approximation}
Sz{\'a}sz, O.
\newblock {\"U}ber die approximation stetiger funktionen durch lineare aggregate von potenzen.
\newblock \emph{Mathematische Annalen}, 77\penalty0 (4):\penalty0 482--496, 1916.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{tay2020long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Tlas(2022)]{tlas2022bump}
Tlas, T.
\newblock Bump functions with monotone fourier transforms satisfying decay bounds.
\newblock \emph{Journal of Approximation Theory}, 278:\penalty0 105742, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Volterra(1928)]{volterra1928variations}
Volterra, V.
\newblock Variations and fluctuations of the number of individuals in animal species living together.
\newblock \emph{ICES Journal of Marine Science}, 3\penalty0 (1):\penalty0 3--51, 1928.

\bibitem[Vyshemirsky \& Girolami(2008)Vyshemirsky and Girolami]{vyshemirsky2008bayesian}
Vyshemirsky, V. and Girolami, M.~A.
\newblock Bayesian ranking of biochemical system models.
\newblock \emph{Bioinformatics}, 24\penalty0 (6):\penalty0 833--839, 2008.

\bibitem[Wang et~al.(2023)Wang, Yan, Gu, and Rush]{wang2023pretraining}
Wang, J., Yan, J.~N., Gu, A., and Rush, A.~M.
\newblock Pretraining without attention, 2023.

\bibitem[Wang \& Xue(2023)Wang and Xue]{wang2023state}
Wang, S. and Xue, B.
\newblock State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory.
\newblock \emph{arXiv preprint arXiv:2309.13414}, 2023.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Yang et~al.(2023)Yang, Wang, Shen, Panda, and Kim]{yang2023gated}
Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y.
\newblock Gated linear attention transformers with hardware-efficient training.
\newblock \emph{arXiv preprint arXiv:2312.06635}, 2023.

\bibitem[Zucchet et~al.(2023{\natexlab{a}})Zucchet, Kobayashi, Akram, von Oswald, Larcher, Steger, and Sacramento]{zucchet2023gated}
Zucchet, N., Kobayashi, S., Akram, Y., von Oswald, J., Larcher, M., Steger, A., and Sacramento, J.
\newblock Gated recurrent neural networks discover attention.
\newblock \emph{arXiv preprint arXiv:2309.01775}, 2023{\natexlab{a}}.

\bibitem[Zucchet et~al.(2023{\natexlab{b}})Zucchet, Meier, Schug, Mujika, and Sacramento]{zucchet2023online}
Zucchet, N., Meier, R., Schug, S., Mujika, A., and Sacramento, J.
\newblock Online learning of long-range dependencies, 2023{\natexlab{b}}.

\end{thebibliography}
