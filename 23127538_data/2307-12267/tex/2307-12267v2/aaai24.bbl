\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abid, Farooqi, and Zou(2021)}]{abid2021persistent}
Abid, A.; Farooqi, M.; and Zou, J. 2021.
\newblock Persistent anti-muslim bias in large language models.
\newblock In \emph{Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
  and Society}, 298--306.

\bibitem[{Aljanabi(2023)}]{aljanabi2023chatgpt}
Aljanabi, M. 2023.
\newblock ChatGPT: Future directions and open possibilities.
\newblock \emph{Mesopotamian Journal of CyberSecurity}, 2023: 16--17.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.~D.; Dhariwal, P.;
  Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:
  1877--1901.

\bibitem[{Buschek, Z{\"u}rn, and Eiband(2021)}]{buschek2021impact}
Buschek, D.; Z{\"u}rn, M.; and Eiband, M. 2021.
\newblock The impact of multiple parallel phrase suggestions on email input and
  composition behaviour of native and non-native english writers.
\newblock In \emph{Proceedings of the 2021 CHI Conference on Human Factors in
  Computing Systems}, 1--13.

\bibitem[{Castro~Nascimento and Pimentel(2023)}]{castro2023large}
Castro~Nascimento, C.~M.; and Pimentel, A.~S. 2023.
\newblock Do Large Language Models Understand Chemistry? A Conversation with
  ChatGPT.
\newblock \emph{Journal of Chemical Information and Modeling}, 63(6):
  1649--1655.

\bibitem[{Choi et~al.(2023)Choi, Hickman, Monahan, and
  Schwarcz}]{choi2023chatgpt}
Choi, J.~H.; Hickman, K.~E.; Monahan, A.; and Schwarcz, D. 2023.
\newblock Chatgpt goes to law school.
\newblock \emph{Available at SSRN}.

\bibitem[{Clark et~al.(2021)Clark, August, Serrano, Haduong, Gururangan, and
  Smith}]{clark2021all}
Clark, E.; August, T.; Serrano, S.; Haduong, N.; Gururangan, S.; and Smith,
  N.~A. 2021.
\newblock All That’s ‘Human’Is Not Gold: Evaluating Human Evaluation of
  Generated Text.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, 7282--7296.

\bibitem[{Dugan et~al.(2023)Dugan, Ippolito, Kirubarajan, Shi, and
  Callison-Burch}]{dugan2023real}
Dugan, L.; Ippolito, D.; Kirubarajan, A.; Shi, S.; and Callison-Burch, C. 2023.
\newblock Real or fake text?: Investigating human ability to detect boundaries
  between human-written and machine-generated text.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, 12763--12771.

\bibitem[{Ethayarajh and Jurafsky(2022)}]{ethayarajh2022human}
Ethayarajh, K.; and Jurafsky, D. 2022.
\newblock How human is human evaluation? Improving the gold standard for NLG
  with utility theory.

\bibitem[{Fagni et~al.(2021)Fagni, Falchi, Gambini, Martella, and
  Tesconi}]{fagni2021tweepfake}
Fagni, T.; Falchi, F.; Gambini, M.; Martella, A.; and Tesconi, M. 2021.
\newblock TweepFake: About detecting deepfake tweets.
\newblock \emph{Plos one}, 16(5): e0251415.

\bibitem[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith}]{gehman2020realtoxicityprompts}
Gehman, S.; Gururangan, S.; Sap, M.; Choi, Y.; and Smith, N.~A. 2020.
\newblock RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language
  Models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, 3356--3369.

\bibitem[{Hassani and Silva(2023)}]{hassani2023role}
Hassani, H.; and Silva, E.~S. 2023.
\newblock The role of ChatGPT in data science: how ai-assisted conversational
  interfaces are revolutionizing the field.
\newblock \emph{Big data and cognitive computing}, 7(2): 62.

\bibitem[{Ippolito et~al.(2020)Ippolito, Duckworth, Callison-Burch, and
  Eck}]{ippolito2020automatic}
Ippolito, D.; Duckworth, D.; Callison-Burch, C.; and Eck, D. 2020.
\newblock Automatic Detection of Generated Text is Easiest when Humans are
  Fooled.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, 1808--1822.

\bibitem[{Jawahar, Abdul-Mageed, and
  Laks~Lakshmanan(2020)}]{jawahar2020automatic}
Jawahar, G.; Abdul-Mageed, M.; and Laks~Lakshmanan, V. 2020.
\newblock Automatic Detection of Machine Generated Text: A Critical Survey.
\newblock In \emph{Proceedings of the 28th International Conference on
  Computational Linguistics}, 2296--2309.

\bibitem[{Jin et~al.(2018)Jin, He, Hui, and Sun}]{jin2018tdnn}
Jin, C.; He, B.; Hui, K.; and Sun, L. 2018.
\newblock TDNN: a two-stage deep neural network for prompt-independent
  automated essay scoring.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 1088--1097.

\bibitem[{Latif et~al.(2023)Latif, Mai, Nyaaba, Wu, Liu, Lu, Li, Liu, and
  Zhai}]{latif2023artificial}
Latif, E.; Mai, G.; Nyaaba, M.; Wu, X.; Liu, N.; Lu, G.; Li, S.; Liu, T.; and
  Zhai, X. 2023.
\newblock Artificial general intelligence (AGI) for education.

\bibitem[{Lee, Liang, and Yang(2022)}]{lee2022coauthor}
Lee, M.; Liang, P.; and Yang, Q. 2022.
\newblock Coauthor: Designing a human-ai collaborative writing dataset for
  exploring language model capabilities.
\newblock In \emph{Proceedings of the 2022 CHI Conference on Human Factors in
  Computing Systems}, 1--19.

\bibitem[{Li et~al.(2023)Li, Sha, Yan, Lin, Rakovi{\'c}, Galbraith, Lyons,
  Ga{\v{s}}evi{\'c}, and Chen}]{li2023can}
Li, Y.; Sha, L.; Yan, L.; Lin, J.; Rakovi{\'c}, M.; Galbraith, K.; Lyons, K.;
  Ga{\v{s}}evi{\'c}, D.; and Chen, G. 2023.
\newblock Can large language models write reflectively.
\newblock \emph{Computers and Education: Artificial Intelligence}, 4: 100140.

\bibitem[{Lipton, Elkan, and Naryanaswamy(2014)}]{lipton2014thresholding}
Lipton, Z.~C.; Elkan, C.; and Naryanaswamy, B. 2014.
\newblock Optimal thresholding of classifiers to maximize F1 measure.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014.
  Proceedings, Part II 14}, 225--239. Springer.

\bibitem[{Liu, Kusner, and Blunsom(2020)}]{liu2020survey}
Liu, Q.; Kusner, M.~J.; and Blunsom, P. 2020.
\newblock A survey on contextual embeddings.

\bibitem[{Lyu et~al.(2023{\natexlab{a}})Lyu, Tan, Zapadka, Ponnatapura, Niu,
  Myers, Wang, and Whitlow}]{lyu2023translating}
Lyu, Q.; Tan, J.; Zapadka, M.~E.; Ponnatapura, J.; Niu, C.; Myers, K.~J.; Wang,
  G.; and Whitlow, C.~T. 2023{\natexlab{a}}.
\newblock Translating radiology reports into plain language using ChatGPT and
  GPT-4 with prompt learning: results, limitations, and potential.
\newblock \emph{Visual Computing for Industry, Biomedicine, and Art}, 6(1): 9.

\bibitem[{Lyu et~al.(2023{\natexlab{b}})Lyu, Li, Yang, de~Rijke, Ren, Zhao,
  Yin, and Ren}]{lyu2023feature}
Lyu, Y.; Li, P.; Yang, Y.; de~Rijke, M.; Ren, P.; Zhao, Y.; Yin, D.; and Ren,
  Z. 2023{\natexlab{b}}.
\newblock Feature-level debiased natural language understanding.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, 13353--13361.

\bibitem[{Ma, Liu, and Yi(2023)}]{ma2023abstract}
Ma, Y.; Liu, J.; and Yi, F. 2023.
\newblock Is this abstract generated by ai? a research for the gap between
  ai-generated scientific text and human-written scientific text.

\bibitem[{Martens and Maalej(2019)}]{martens2019towards}
Martens, D.; and Maalej, W. 2019.
\newblock Towards understanding and detecting fake reviews in app stores.
\newblock \emph{Empirical Software Engineering}, 24(6): 3316--3355.

\bibitem[{Mitchell et~al.(2023)Mitchell, Lee, Khazatsky, Manning, and
  Finn}]{mitchell2023detectgpt}
Mitchell, E.; Lee, Y.; Khazatsky, A.; Manning, C.~D.; and Finn, C. 2023.
\newblock Detectgpt: Zero-shot machine-generated text detection using
  probability curvature.

\bibitem[{Perone, Silveira, and Paula(2018)}]{perone2018evaluation}
Perone, C.~S.; Silveira, R.; and Paula, T.~S. 2018.
\newblock Evaluation of sentence embeddings in downstream and linguistic
  probing tasks.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever
  et~al.}]{radford2018improving}
Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et~al. 2018.
\newblock Improving language understanding by generative pre-training.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019language}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et~al.
  2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Schroff, Kalenichenko, and Philbin(2015)}]{Schroff_2015_CVPR}
Schroff, F.; Kalenichenko, D.; and Philbin, J. 2015.
\newblock FaceNet: A Unified Embedding for Face Recognition and Clustering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}.

\bibitem[{Snell, Swersky, and Zemel(2017)}]{snell2017prototypical}
Snell, J.; Swersky, K.; and Zemel, R. 2017.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, 4080--4090.

\bibitem[{Uchendu et~al.(2020)Uchendu, Le, Shu, and
  Lee}]{uchendu-etal-2020-authorship}
Uchendu, A.; Le, T.; Shu, K.; and Lee, D. 2020.
\newblock Authorship Attribution for Neural Text Generation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 8384--8395. Online: Association for
  Computational Linguistics.

\bibitem[{Wang et~al.(2018)Wang, Liu, Luo, and Wang}]{wang2018lstm}
Wang, J.-H.; Liu, T.-W.; Luo, X.; and Wang, L. 2018.
\newblock An LSTM approach to short text sentiment classification with word
  embeddings.
\newblock In \emph{Proceedings of the 30th conference on computational
  linguistics and speech processing (ROCLING 2018)}, 214--223.

\bibitem[{Wang et~al.(2021)Wang, Wang, Yao, and Dou}]{wang2021hierarchical}
Wang, Y.; Wang, S.; Yao, Q.; and Dou, D. 2021.
\newblock Hierarchical Heterogeneous Graph Representation Learning for Short
  Text Classification.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, 3091--3101.

\bibitem[{Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato,
  Huang, Cheng, Glaese, Balle, Kasirzadeh et~al.}]{weidinger2021ethical}
Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.; Huang, P.-S.;
  Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; et~al. 2021.
\newblock Ethical and social risks of harm from language models.

\bibitem[{Xiao et~al.(2023)Xiao, Xu, Zhang, Wang, and Xia}]{xiao2023evaluating}
Xiao, C.; Xu, S.~X.; Zhang, K.; Wang, Y.; and Xia, L. 2023.
\newblock Evaluating Reading Comprehension Exercises Generated by LLMs: A
  Showcase of ChatGPT in Education Applications.
\newblock In \emph{Proceedings of the 18th Workshop on Innovative Use of NLP
  for Building Educational Applications (BEA 2023)}, 610--625.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Rashkin, Bisk, Farhadi,
  Roesner, and Choi}]{zellers2019defending}
Zellers, R.; Holtzman, A.; Rashkin, H.; Bisk, Y.; Farhadi, A.; Roesner, F.; and
  Choi, Y. 2019.
\newblock Defending against neural fake news.
\newblock \emph{Advances in neural information processing systems}, 32.

\end{thebibliography}
