%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS

%  add your own packages  --------------
\usepackage{subfigure}
\usepackage{color,xcolor}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{bm}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{newfloat}
\usepackage{listings}
%  add your own packages  --------------

\usepackage{aaai24}  % DO NOT CHANGE THIS
%\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Towards Automatic Boundary Detection for Human-AI Hybrid Essay in Education}
\author{
    %Authors
    % All authors must be in the same font size and format.
    %Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    %AAAI Style Contributions by Pater Patel Schneider,
    Zijie Zeng,
    Lele Sha,
    Yuheng Li,
    Kaixun Yang,
    Dragan Ga\v{s}evi\'{c} \text{and}
    Guanliang Chen\thanks{Corresponding author}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm }Centre for Learning Analytics, Monash University, Australia\\
    \{Zijie.Zeng, Lele.Sha1, Yuheng.Li, Kaixun.Yang1, Dragan.Gasevic, Guanliang.Chen\}@monash.edu}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Human-AI collaborative writing has been greatly facilitated with the help of modern large language models (LLM), e.g., ChatGPT. While admitting the convenience brought by technology advancement, educators also have concerns that students might leverage LLM to partially complete their writing assignment and pass off the human-AI hybrid text as their original work. Driven by such concerns, in this study, we investigated the automatic detection of Human-AI hybrid text in education, where we formalized the hybrid text detection as a boundary detection problem, i.e., identifying the transition points between human-written content and AI-generated content. We constructed a hybrid essay dataset by partially removing sentences from the original student-written essays and then instructing ChatGPT to fill in for the incomplete essays. Then we proposed a two-step detection approach where we (1) Separated AI-generated content from human-written content during the embedding learning process; and (2) Calculated the distances between every two adjacent prototypes (a prototype is the mean of a set of consecutive sentences from the hybrid text in the embedding space) and assumed that the boundaries exist between the two prototypes that have the furthest distance from each other. Through extensive experiments, we summarized the following main findings: (1) The proposed approach consistently outperformed the baseline methods across different experiment settings; (2) The embedding learning process (i.e., step 1) can significantly boost the performance of the proposed approach; (3) When detecting boundaries for single-boundary hybrid essays, the performance of the proposed approach could be enhanced by adopting a relatively large prototype size\footnote{The number of sentences needed to calculate a prototype.}, leading to a $22$\% improvement (against the second-best baseline method) in the in-domain setting and an $18$\% improvement in the out-of-domain setting.

%an important hyperparameter of the proposed approach.
%Human-AI collaborative writing has been greatly facilitated with the help of modern large language models (LLM), e.g., GPT-3, GPT-4, and ChatGPT. While admitting the convenience brought by technology advancement, education practitioners also have concerns that students might inappropriately leverage LLM to partially complete their writing assignment and pass off the human-AI collaborative text (hybrid text) as their original work. Driven by such concerns, in this study, we set out to investigate the techniques for the automatic detection of hybrid text in education. Noticing the hybrid nature of human-AI collaborative text, we formalized the hybrid text detection as a boundary detection problem instead of a binary classification problem, i.e., identifying the transition points (boundaries) between human-written content and AI-generated content from the given hybrid text. We acquired our hybrid essay dataset by first partially removing sentences from the original student-written essays and then instructing ChatGPT to fill in for the incomplete essays. Then we proposed a two-step automatic boundary detection approach where we (1) Separated AI-generated content from human-written content during the embedding learning (encoder training) process; and (2) Calculated the distances between every two adjacent prototypes (a prototype is the mean of a set of consecutive sentences from the hybrid text in the embedding space) and assumed that the boundaries exist between the two prototypes that have the furthest distance from each other. Through extensive experiments based on the hybrid essay dataset, where we evaluated the proposed approach and other baseline methods, we summarized the following main findings: (\romannumeral1) The proposed approach demonstrated advantages over baseline methods on detecting boundaries from the educational hybrid essays; (\romannumeral2) The embedding learning process (i.e., step 1) can significantly boost the detecting performance of the proposed approach; (\romannumeral3) When detecting boundaries for hybrid text with fewer boundaries (e.g., single-boundary hybrid text), the performance of the proposed approach could be further enhanced by adopting a relatively large value for the prototype size, which is an important hyperparameter of the proposed approach.
\end{abstract}

\section{Introduction}\label{sec:intro}

The neural large language models (LLM) nowadays are capable of generating controllable, natural-sounding texts that are free of grammatical errors most of the time. However, the growing generative abilities of LLM have arguably been a sword with two blades. As pointed out by \citet{ma2023abstract,zellers2019defending}, LLM can potentially be used to generate seemingly correct but counterfactual texts that might affect public opinion, e.g., fake news \citep{zellers2019defending}, fake app reviews \citep{martens2019towards}, fake social media text \citep{fagni2021tweepfake}, and other harmful text \citep{weidinger2021ethical, abid2021persistent, gehman2020realtoxicityprompts}. Concerns have also been raised among education practitioners that the powerful generative ability of LLM may facilitate students' misconduct of leveraging LLM to complete their writing assignments (e.g., essay writing), with their writing and critical thinking skills being undeveloped \citep{ma2023abstract,dugan2023real,mitchell2023detectgpt}. Driven by the above risks and concerns, many studies focused on differentiating human-written content from AI-generated content have been conducted~\citep{mitchell2023detectgpt, ma2023abstract, zellers2019defending, uchendu-etal-2020-authorship, fagni2021tweepfake, ippolito2020automatic}. For example, to advance the techniques for detecting AI-generated content on social media (e.g., Twitter and Facebook), \citet{fagni2021tweepfake} constructed the TweepFake dataset, based on which they evaluated a series of $13$ detection methods. Their findings suggested that the latest sophisticated generative methods (e.g., GPT series models) are capable of generating texts more human-like than the earlier generative methods, such as RNNs.  

% Figure environment removed


While most existing studies \citep{ma2023abstract, clark2021all, mitchell2023detectgpt, jawahar2020automatic, martens2019towards} formalized the AI-content detection as a binary classification problem and assumed that a sample text is either entirely human-written or entirely AI-generated, \citet{dugan2023real,cutler2021automatic} noticed the trends in Human-AI collaborative writing \citep{buschek2021impact,lee2022coauthor} and reflected on the binary classification setting, pointing out that a text could be written by human and AI model collectively (i.e., hybrid text \citep{ma2023abstract}). They argued that due to the hybrid nature of Human-AI collaborative text, simply yielding the probability of a hybrid text being human-written or AI-generated is less informative than identifying the possible AI-generated parts from the whole hybrid text (i.e., boundary detection). As illustrated in Figure \ref{fig1}, where we tested a commercial AI content detector released by Copyleaks\footnote{https://copyleaks.com/ai-content-detector} with a hybrid essay written first by human (first half) and then by ChatGPT (second half), the detector suggested that the hybrid input was human-written but with mere confidence of 51.1\%, which provided only very limited information and could hardly serve as rigid evidence of whether the given text was indeed human-written (or AI-generated), highlighting the need for an alternative perspective regarding the detection of Human-AI collaborative hybrid text. In this study, we focused on the detection of the Human-AI collaborative hybrid text in education, where we followed \citet{dugan2023real} to formalize the hybrid text detection as a boundary detection problem, i.e., identify the transition points (i.e., boundaries) between human-written content and AI-generated content from the given hybrid text. We formally defined our research question (RQ) as:
\begin{itemize}
    \item To what extent can the boundaries between human-written content and AI-generated content be automatically detected within educational hybrid essays?
\end{itemize}


To investigate this research question, we first constructed a Human-AI hybrid essay dataset\footnote{https://www.kaggle.com/c/asap-aes} by partially removing sentences from open-source student-written essays and leveraging ChatGPT to fill in for the incomplete essays. Then we proposed our two-step approach to (1) Separate AI-generated content from human-written content during the embedding learning (encoder training) process; and (2) Calculate the distances between every two adjacent prototypes (a prototype means the average embeddings of a set of consecutive sentences from the hybrid text) and assumed that the boundaries exist between the two prototypes that have the furthest distance from each other. Through extensive experiments, we summarized the following main findings: (1) The proposed approach consistently outperformed the baseline methods across different experiment settings; (2) The embedding learning process in step 1 can significantly boost the performance of the proposed approach; (3) When detecting boundaries for single-boundary hybrid essays, the performance of the proposed approach could be enhanced by adopting a relatively large prototype size, resulting in a $22$\% improvement (against the second-best baseline method) in the in-domain setting and a $18$\% improvement in the out-of-domain setting. %Our dataset and the codes are available on github\footnote{https://github.com/ANONYMOUSGITHUBLINK}.





\section{Related Work}\label{sec:background}

\subsection{Large Language Model and AI content Detection}

The recent large language models (LLM) have been able to generate fluent and natural-sounding text. Among all LLMs, the Generative Pre-trained Transformer (GPT) \citep{radford2018improving, radford2019language,brown2020language} series language models developed by OpenAI, are known to be the state-of-the-art and have achieved great success in NLP tasks. ChatGPT, a variant of GPT, is specifically optimized for conversational response generation \citep{hassani2023role} and could generate human-like responses to specific input text (i.e., prompt text) in different styles or tones, according to the role we require it to play \citep{aljanabi2023chatgpt}. On one hand, generating human-like texts is one of the capabilities we expect from LLM. On the other hand, there still exist some scenarios where there might be potential risks if we fail to identify whether a given text is human-written or AI-generated. For example, \citet{ma2023abstract,zellers2019defending} pointed out that LLM can be used to generate misleading information that might affect public opinion, e.g., fake news \citep{zellers2019defending}, fake app reviews \citep{martens2019towards}, fake social media text \citep{fagni2021tweepfake}, and other harmful text \citep{weidinger2021ethical}. Education practitioners are also concerned about students' leveraging LLM to (partially) complete their writing assignments (e.g., essay writing), without developing their writing and critical thinking skills \citep{ma2023abstract,dugan2023real,mitchell2023detectgpt}. Driven by the need to identify AI content, many online detecting tools have been developed, e.g., the detectors developed by WRITER\footnote{https://writer.com/ai-content-detector/}, Copyleaks\footnote{https://copyleaks.com/ai-content-detector}, OpenAI\footnote{https://platform.openai.com/ai-text-classifier}, GPTzero\footnote{https://gptzero.me/}, and Turnitin\footnote{https://help.turnitin.com/ai-writing-detection.htm}. Concurrent with these tools are the AI content detection studies, which focused on either (a). investigating humans' abilities to detect AI content \citep{ippolito2020automatic, clark2021all, brown2020language, ethayarajh2022human, dugan2023real}, e.g., \citet{ethayarajh2022human} reported an interesting finding that human annotators rated GPT-3 generated text more human-like than the authentic human-written text; or (b). automating the detection of AI content \citep{ma2023abstract, clark2021all, mitchell2023detectgpt, jawahar2020automatic, martens2019towards}. For example, \citet{ma2023abstract} investigated a series of features for detecting AI-generated scientific text, i.e., writing style, coherence, consistency, and argument logistics. They found that AI-generated scientific text significantly differentiated from human-written scientific text in terms of writing style. DetectGPT~\citep{mitchell2023detectgpt} first computes the log probability for both the candidate sample $x$ and its perturbations $\widetilde{x}_i$ under the source model $p$. Then it compares the log probability of $x$ against that of the perturbations and employs the average log ratio as the indicator of whether candidate $x$ originates from the source model $p$.

\subsection{Human-AI Collaborative Writing and Boundary Detection}
With the modern large language models, Human-AI collaborative writing is becoming easier and easier. For example, \citet{buschek2021impact} attempted to leverage GPT-2 to generate phrase-level suggestions for human writers in their email writing tasks. Similar studies were conducted in \citet{lee2022coauthor}, where they alternatively used the more powerful GPT-3 for providing sentence-level suggestions for human-AI interactive essay writing. Noticing the trends in human-AI collaborative writing, studies focused on detecting boundaries from hybrid text have been conducted. For example, \citet{cutler2021automatic} addressed boundary detection as a classification problem and trained a series of classifiers to predict the boundaries. However, their approaches were restricted to single-boundary hybrid texts and required the number of sentences to be fixed across all hybrid texts, preventing their application to hybrid texts in real-world scenarios\footnote{Hybrid texts in real-world scenarios can contain an arbitrary number of sentences and boundaries.}. \citet{dugan2023real} conducted a study to investigate human's ability to detect the boundary from the single-boundary hybrid texts. Their results showed that, although humans' abilities to detect boundaries could be boosted after certain training processes, the overall detecting performances were hardly satisfactory, i.e., the participants could only correctly identify the boundary $23.4\%$ of the time. Driven by the trends in Human-AI collaborative writing, and the need for education practitioners to identify AI content from hybrid texts in real-world scenarios, in this study, we investigated the automatic detection of hybrid essays in education and formalized the detection of hybrid text as a boundary detection problem, considering its collaborative nature involving multiple authors.


%\footnote{We use 'general hybrid text' to refer to hybrid text with an arbitrary number of boundaries and an arbitrary number of sentences.} are rarely explored. One relevant study was conducted by \citet{cutler2021automatic}, which addressed boundary detection as a classification problem and trained classifiers to predict the boundary. However, their approach was restricted to single-boundary hybrid texts and required the number of sentences to be consistent among all hybrid texts\footnote{For example, they trained a classifier on the hybrid text dataset where each text contained exactly $10$ sentences and this classifier can only predict the boundary for the hybrid text with exactly $10$ sentences.}. We considered that such an approach can not be adapted to general hybrid text in real-world scenarios and thus we did not include it as our baseline method.



\section{Method}

\subsection{Hybrid Essay Dataset Construction}%\label{sec:data}


\smallskip
\noindent\textbf{Source Data and Pre-processing.}
%\subsubsection{}
We identified the essay dataset for the Automated Student Essay Assessment competition\footnote{https://www.kaggle.com/c/asap-aes} as the suitable source material to construct our educational hybrid essay dataset. The source essays were written by junior high school students (Grade 7 to 10) in the US. There were a total of eight prompts in this dataset. Each essay was a student-written response to one of these prompts. To ensure a level of quality and informativeness, we only preserved the source essays with more than 100 words. We further noticed that for some source essays, entities such as Date, location, and the name of the person involved were anonymized and replaced with a string starting with '@' for the sake of privacy protection. For example, the word 'Boston' in the original source essay could be replaced with '@LOCATION'. We also filtered out essays containing such anonymized entities to prevent our model from incorrectly associating data bias \citep{lyu2023feature} with the label, i.e., associating the presence of '@' with the authorship (human-written or AI-generated) of the text in our case. 
%Finally, we obtained a total of 4082 essays for the later hybrid essay generation.

\begin{table*}[!htb]
 \caption{Descriptions of the fill-in tasks. \textbf{H} and \textbf{G} in Hybrid Text Structure are short for \textbf{H}uman-written text and \textbf{G}enerated text, respectively. For example, $<$H$\rightarrow$G$\rightarrow$H$>$ in Task 3 means that the expected hybrid essay should be started/ended with human-written sentences, while the text in between the starting and the ending should be generated by ChatGPT.} \label{table1}
 \begin{center}

\resizebox{0.92\textwidth}{!}{
     
     \begin{tabular}{l|p{4.7cm}|p{4.7cm}|p{4.7cm}p{4.7cm}} 
    \toprule
     & Task 1 & Task 2 & Task 3 \\ 
    \midrule
    Description & Task 1 requires ChatGPT to generate continuation based on the specified beginning
     text. & Task 2 requires ChatGPT to generate the essay using the specified ending. & Task 3
     requires ChatGPT to fill in between the specified beginning and specified ending. \\ 
    \midrule
    Hybrid Text Structure & H$\rightarrow$G & G$\rightarrow$H & H$\rightarrow$G$\rightarrow$H \\ 
    \midrule
    \#Boundaries & 1 & 1 & 2 \\ 
    \midrule
     & Task 4 & Task 5 & Task 6 \\ 
    \midrule
    Description & Task 4
     requires that the generated essay should include the specified human-written
     text between the beginning and the ending. & Task
     5 requires ChatGPT to fill in for the incompleted essay where some in-between
     text and the ending text have been removed. & Task
     6 requires ChatGPT to fill in for the incompleted essay where some in-between
     text and the beginning text have been removed. \\ 
    \midrule
    Hybrid Text Structure & G$\rightarrow$H$\rightarrow$G & H$\rightarrow$G$\rightarrow$H$\rightarrow$G & G$\rightarrow$H$\rightarrow$G$\rightarrow$H \\ 
    \midrule
    \#Boundaries & 2 & 3 & 3 \\
    \bottomrule
    \end{tabular}

}
 \end{center}
\end{table*}

%\subsubsection{}
\smallskip
\noindent\textbf{Hybrid Essay Generation.}
We employed ChatGPT as the generative AI model for our hybrid essay generation, considering its outstanding generative ability and easy accessibility. To construct a hybrid essay from a source essay $R$, we randomly removed a few sentences from $R$ and instructed ChatGPT to perform a fill-in task over the incompleted essay $R'$. Specifically, we designed six fill-in tasks to cover the common use cases of leveraging ChatGPT to generate hybrid essays in real-life circumstances. As shown in Table \ref{table1}, the missing parts that ChatGPT was required to fill were different from task to task. For example, in task 1, ChatGPT was prompted to continue writing the essay based on the given beginning text while in task 3, ChatGPT was required to fill in between the specified beginning text and ending text.

\smallskip
\noindent\textbf{Prompt.}
For each hybrid essay, the adopted prompting text generally consisted of two parts: (1) The instructions\footnote{These can be found in https://www.kaggle.com/competitions/asap-aes/data.} that a writer should refer to when compositing the essay. Note that we strictly adopted these instructions as the first part of the prompting text; (2) The second part is the task-relevant prompting text that detailed the concrete requirements regarding the structure of the hybrid essay:
\begin{itemize}
    \item Task 1: Please begin with $<$BEGINNING TEXT$>$.
    \item Task 2: Please ensure to use $<$ENDING TEXT$>$ as the ending.
    \item Task 3: Please begin with $<$BEGINNING TEXT$>$ and continue writing the second part. For the ending, please use $<$ENDING TEXT$>$ as the ending.
    \item Task 4: Please ensure to include $<$IN-BETWEEN TEXT$>$ in between the starting text and the ending text.
\end{itemize}

We found that ChatGPT can generate the hybrid essays as expected most of the time when simple instructions (prompting text for task 1, 2, 3, and 4) were given. However, for complicated tasks (task 5 and 6) where multiple missing text pieces needed to be filled (e.g., task 5 required ChatGPT to fill for the in-between text and the ending text), ChatGPT tended to generate unexpected output (failing to correctly fill in at the expected space) with a single and unified prompt. Therefore, for task 5 and 6, we alternatively broke the fill-in task into two steps, where we found the chance of generating unexpected output could be significantly reduced. The prompting text for each step is as follows:
\begin{itemize}
    \item Step 1: We strictly follow the prompting text for task 3 to generate an initial hybrid essay, which is denoted as $H_1\rightarrow G\rightarrow H_2$, meaning that the first part ($H_1$) and the third part ($H_2$) are \textbf{H}uman-written while the second part ($G$) is \textbf{G}enerated by ChatGPT.
    \item Step 2: We randomly remove the first few sentences from $H_1$ and obtain $H_1'$. Then we use the prompt 'Please use $H_1'\rightarrow G\rightarrow H_2$ as the ending' to obtain the final hybrid essay as required by task 6, which could be denoted as $G'\rightarrow H_1'\rightarrow G\rightarrow H_2$. Similarly, when we remove the last few sentences from $H_2$ and obtain $H_2'$, we can prompt ChatGPT with 'Please begin with $H_1\rightarrow G\rightarrow H_2'$ to obtain the hybrid essay as required by task 5, denoted as $H_1\rightarrow G\rightarrow H_2'\rightarrow G'$.
\end{itemize} 

Due to the randomness of the ChatGPT \citep{lyu2023translating,wu2023brief}, we could still have a certain probability of getting invalid output from ChatGPT even with the prompting texts described above. To deal with this, we simply discarded the invalid output and instructed ChatGPT to generate a hybrid essay for another four attempts at most. If all attempts failed for a specific source essay, we skipped this source essay and continued generating using the remaining source essays. To ensure the authenticity of the human-written text, we still preserved the typo\footnote{We noticed that ChatGPT tended to automatically fix typos within the human-written text.} within the human-written part of the hybrid essay and ignore the typo-fixing modification. The statistics of the final hybrid essay dataset are described in Table \ref{table2}.

\begin{table}[!htb]
 \caption{Statistics of the hybrid essay dataset.} \label{table2}
 \begin{center}

\resizebox{0.42\textwidth}{!}{
     
     %\begin{tabular}{l|p{3.7cm}|p{3.7cm}|p{3.7cm}p{3.7cm}} 
    \begin{tabular}{l|ccc|c} %{l|p{3.7cm}|p{3.7cm}|p{3.7cm}p{3.7cm}} 
    %\begin{tabular}{p{3.0cm}|ccc|c} 
        \toprule
        \multirow{2}{*}{} & \multicolumn{3}{c|}{\#Boundaries} & \multirow{2}{*}{All} \\ 
        \cline{2-4}
         & 1 & 2 & 3 & \\ 
         %\midrule
        %\#Source essay & 4034 & 3880 & 2510 & 4049 \\ 
        \midrule
        \#Hybrid essay & 7488 & 6429 & 3219 & 17136 \\ 
        \midrule
        \#Words per essay & 275.3 & 279.5 & 332.6 & 287.6 \\ 
        \midrule
        \#Sentences
         per essay & 12.9 & 13.4 & 16.1 & 13.7 \\ 
        \midrule
        \begin{tabular}[c]{@{}l@{}}Average length\\ of AI-generated\\ sentences\end{tabular} & 22.7 & 21.8 & 21.7 & 22.2 \\ 
        \midrule
        \begin{tabular}[c]{@{}l@{}}Average length\\ of human-written\\ sentences\end{tabular} & 22.7 & 22.6 & 21.2 & 22.4 \\ 
        \midrule
        \begin{tabular}[c]{@{}l@{}}Ratio of AI-generated\\ sentences per essay\end{tabular} & 67.4\% & 58.8\% & 73.2\% & 65.3\% \\
        \bottomrule
    \end{tabular}

}
 \end{center}
\end{table}



\subsection{Task and Evaluation}

For a given hybrid text $<s_{1}, s_{2},...,s_{n}>$ consisting of $n$ sentences where each sentence is either human-written or AI-generated, the automatic boundary detection task requires an algorithm to identify all indexes $i$, where sentence $s_i$ and sentence $s_{i+1}$ are written by different authors, e.g., the former sentence by human and the later sentence by generative language model (e.g., ChatGPT), or vice versa. To evaluate the ability of an algorithm to detect boundaries from a hybrid text, we first describe the following two concepts:
\begin{itemize}
    \item $L_{topK}$: The list of top-K boundary sentences suggested by the algorithm;
    \item $L_{Gt}$: The ground-truth list that contains the actual boundary sentences.
\end{itemize}
We considered the harmonic mean \citep{lipton2014thresholding} of precision and recall for evaluation, i.e., the F1 metric \citep{lipton2014thresholding}:

\begin{equation}\label{eq:1}
    F1@K = 2\cdot  \dfrac{|L_{topK}\cap L_{Gt}|}{ |L_{topK}|+ |L_{Gt}| }.
\end{equation}
Note that $K$ denotes the number of boundary candidates proposed by the algorithm. We set $K=3$ in our study due to the maximum number of boundaries in our dataset being 3. 

\subsection{Boundary Detection Approaches}

To the best of our knowledge, the automatic boundary detection approaches for hybrid text are rarely explored. The only relevant study was conducted by \citet{cutler2021automatic}, in which the evaluated approaches were restricted to single-boundary hybrid texts and required a fixed number of sentences across all hybrid texts\footnote{For example, they trained a classifier on the hybrid text dataset where each text contained exactly $10$ sentences and this classifier can only predict the boundary for the hybrid text with exactly $10$ sentences.}. We considered that such approaches can not be adapted to general hybrid text in real-world scenarios and thus we did not include them as baseline methods. Inspired by \citet{perone2018evaluation,liu2020survey}, which pointed out that the quality of representations (embeddings) can significantly affect the performance of downstream tasks. we introduce the following two-step automatic boundary detection approach (also illustrated in Figure \ref{fig2}):

\begin{itemize}
    \item \textbf{TriBert (Our two-step approach)}: 
    \begin{enumerate}
        \item We first adopt the pre-trained sentence encoder implemented in the Python package SentenceTransformers\footnote{https://www.sbert.net/} as the initial encoder. We then fine-tune the initial encoder with the triplet Bert-networks \citep{Schroff_2015_CVPR} architecture, which is described as follows: for a sentence triplet $(a, x^+, x^-)$, where $a$ is the anchor sentence whose label is the same as that of the positive sentence $x^+$, but different from the label of the negative sentence $x^-$. The network (i.e., Bert encoder) is trained such that the distance between the embeddings of $a$ and $x^+$ ($d_1$ in step $1$ in Figure \ref{fig2}) is smaller than the distance between the embeddings of $a$ and $x^-$ ($d_2$ in step $1$ in Figure \ref{fig2}). Step 1 aims for separating AI-generated content from human-written content during the encoder training process.
        \item Let $S_i^{p-}$ be the averaged embeddings (also termed as prototype \citep{snell2017prototypical}) of sentence $s_i$ and its $(p-1)$ preceding sentences and $S_{i+1}^{p+}$ be the averaged embeddings of sentence $s_{i+1}$ and its $(p-1)$ following sentences. To identify the possible boundaries, we first calculate the distances between every two adjacent prototypes $S_i^{p-}$ and $S_{i+1}^{p+}$, where $i\in\{1,2,...,k\}$ and hyperparameter $p$ denotes the number of sentences used to calculate the prototype, i.e., the prototype size. Note that $k+1$ is the number of sentences of the hybrid text. Then we assume that the boundaries exist between the two adjacent prototypes that have the furthest distance from each other. We searched the learning rate from $ \{ 1e-6, 5e-6, 1e-5\}$ and reported the results of prototype size $ p$ in $\{ 1, 2, 3, 4, 5, 6\}$.

    \end{enumerate}
    
\end{itemize}

% Figure environment removed
We describe all the other baseline boundary detection approaches as follows:

\begin{itemize}
    
    \item \textbf{Bert}: This method jointly fine-tunes the pre-trained Bert-base encoder and the final dense classification layer. Then the trained model is used to classify each sentence from the hybrid input text. Finally, $i$ is predicted as a boundary if sentence $s_i$ and sentence $s_{i+1}$ are of different predicted labels, e.g., if $s_i$ is predicted as human-written while $s_{i+1}$ is predicted as AI-generated, then this method assumes that $i$ is a possible boundary. This Bert-based classifier was adapted from the pre-trained Bert-based model implemented in the Transformers\footnote{https://github.com/huggingface/transformers} package.
    
    %For this method, we searched the learning rate from $ \{ 1e-6, 5e-6, 1e-5\}$.
    
    \item \textbf{LR}: This method directly employs the sentence encoder implemented in SentenceTransformers for generating sentence embeddings. Then, with the embeddings of each sentence from the hybrid text as input, this method trains a logistic regression binary classifier. Finally, we follow the same process as described in \textit{Bert} approach to identify the possible boundaries.
    
    
    \item \textbf{Random}: This method randomly suggests a list of candidate boundaries, serving as a baseline showing the boundary detection performance using random guessing.
\end{itemize}


\section{Experiments}
We conducted both the in-domain evaluation (where models are trained and tested on the same domains) and the out-of-domain evaluation (where training domains have no overlap with the testing domains).

\subsection{Training, Validating, and Testing}

\smallskip
\noindent\textbf{In-domain Study.} For the hybrid essays of each prompt, we first grouped the hybrid essays by the source essays from eight prompts. Then we specify that 70\% groups of hybrid essays from each prompt are assigned to the training set. The remaining groups were equally assigned to validation and testing with ratios of 15\% and 15\%, respectively. The process of grouping by source essays was meant to ensure that the source essays for testing would not overlap with the source essays for training, avoiding the situation that essay $E_1$ and $E_2$ were generated from the same source essay $E$ but happened to be assigned to the training set and test set, respectively. Note that in this case, $E_1$ and $E_2$ could theoretically share some common human-written sentences, leading to testing data being exposed in the training stage. 

\smallskip
\noindent\textbf{Out-of-Domain Study.} We followed \citet{jin2018tdnn} to adopt the prompt-wise cross-validation for out-of-domain evaluation, i.e., eight-fold cross-validation in the case of our study because we have hybrid essays from eight different prompts. Specifically, for each fold,  we have the hybrid essays from the target prompt as testing data while hybrid essays from the remaining seven prompts were used for model training. Additionally, 30\% of the training data were held for validation.

\subsection{Parameters, Implementation, and Other Details} For simplicity, we define the completion of training $n$ samples (in our study we used $n=5000$) as one training epoch and we tested the models on the validation data after each training epoch. Note that the learning rate will be reduced by $20\%$ after each epoch and early stopping will be triggered at epoch $t$ (i.e., after epoch $t$ but before starting epoch $t+1$) if the model performance on epoch $t$ shows no improvement over compared to epoch $t-1$. Then we used the best models (selected based on validation results) to predict for the testing data and reported the results using F1 metrics as described in Equation (\ref{eq:1}).
%All experiments were run on NVIDIA Tesla T4 GPU with 16 GB RAM and the codes are available via \url{https://github.com/UNKNOWNGITHUBLINK.}


\section{Results}%\footnote{For simplicity, all performance comparisons in this section were based on F1 score.}}

We presented the results of the boundary detection approaches in Table \ref{table3}, based on which we structured the following analysis and findings.

\begin{table*}[!htb]
 \caption{Results of different methods on the boundary detection task. The evaluation metric adopted here is \textbf{F1} score. Note that 'NT' in \textit{TriBert} (NT, $p=k$) is short for '\textbf{N}ot \textbf{T}rained', which means the encoder was directly used without fine-tuning. We use \#Bry to denote the number of boundaries. Each reported entry is a mean over \textbf{three} independent runs with the same hyperparameters. The best results are in \textbf{bold}.} \label{table3}
 \begin{center}

\resizebox{0.90\textwidth}{!}{

\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c} 
\toprule
Method & \multirow{16}{*}{In-Domain} & \#Bry=1 & \#Bry=2 & \#Bry=3 & All & \multirow{16}{*}{Out-of-Domain} & \#Bry=1 & \#Bry=2 & \#Bry=3 & All \\ 
\cmidrule{1-1}\cmidrule{3-6}\cmidrule{8-11}
Bert & & 0.398 & 0.601 & 0.730 & 0.536 & & 0.369 & 0.545 & 0.632 & 0.486 \\
LR & & 0.265 & 0.377 & 0.404 & 0.332 & & 0.159 & 0.248 & 0.241 & 0.208 \\
\cmidrule{1-1}\cmidrule{3-6}\cmidrule{8-11}
TriBert (p=1) & & 0.430 & 0.646 & \textbf{0.752} & 0.571 & & 0.379 & 0.559 & \textbf{0.640} & 0.497 \\
TriBert (p=2) & & 0.455 & \textbf{0.692} & 0.622 & \textbf{0.575} & & 0.417 & \textbf{0.597} & 0.545 & \textbf{0.510} \\
TriBert (p=3) & & 0.477 & 0.672 & 0.565 & 0.566 & & 0.421 & 0.567 & 0.463 & 0.484 \\
TriBert (p=4) & & \textbf{0.486} & 0.641 & 0.514 & 0.549 & & \textbf{0.436} & 0.551 & 0.428 & 0.477 \\
TriBert (p=5) & & 0.480 & 0.586 & 0.487 & 0.519 & & 0.424 & 0.511 & 0.402 & 0.452 \\
TriBert (p=6) & & 0.477 & 0.526 & 0.466 & 0.492 & & 0.419 & 0.487 & 0.395 & 0.439 \\ 
\cmidrule{1-1}\cmidrule{3-6}\cmidrule{8-11}
TriBert (NT, p=1) & & 0.185 & 0.290 & 0.313 & 0.248 & & 0.188 & 0.278 & 0.306 & 0.244 \\
TriBert (NT, p=2) & & 0.189 & 0.324 & 0.301 & 0.260 & & 0.205 & 0.316 & 0.302 & 0.266 \\
TriBert (NT, p=3) & & 0.195 & 0.316 & 0.296 & 0.258 & & 0.206 & 0.305 & 0.288 & 0.259 \\
TriBert (NT, p=4) & & 0.191 & 0.282 & 0.263 & 0.238 & & 0.201 & 0.292 & 0.265 & 0.248 \\
TriBert (NT, p=5) & & 0.179 & 0.282 & 0.259 & 0.231 & & 0.191 & 0.287 & 0.262 & 0.240 \\
TriBert (NT, p=6) & & 0.183 & 0.286 & 0.247 & 0.232 & & 0.189 & 0.276 & 0.249 & 0.233 \\
\cmidrule{1-1}\cmidrule{3-6}\cmidrule{8-11}
Random & & 0.130 & 0.204 & 0.209 & 0.173 & & 0.139 & 0.230 & 0.222 & 0.188 \\
\bottomrule
\end{tabular}

}
 \end{center}
\end{table*}

\smallskip
\noindent\textbf{In-Domain (ID) and Out-of-Domain (OOD) Detection. }
We observed that for \textit{LR}, \textit{Bert}, and \textit{TriBert} ($p=1,2,...$), the ID performance is generally higher than the OOD performance. This observation is not surprising because, in the ID setting, the domains of the test data have all been seen during the training while in the OOD setting, all test domains are unseen during the training stage. Note that for methods involving no training (or fine-tuning) process, i.e., \textit{Random} and \textit{TriBert} (NT\footnote{We use 'NT' to denote 'Not Trained'.}, $p=1,2,..$), the performance difference between the ID and OOD setting is negligible because all test domains were unseen regardless of the current training setting being ID or OOD.

\smallskip
\noindent\textbf{TriBert v.s. Baseline Approaches. }
We noticed that, although \textit{LR} performed better than \textit{Random} in both ID and OOD settings, its performance was also poorer compared to \textit{Bert} and \textit{TriBert} ($p=1,2,...$). Our explanation is that the shallow structure (only one output layer) of \textit{LR} and the limited number of learnable parameters (i.e., 768 parameters) hampers \textit{LR} from further learning complex concepts from the input sentences. Besides, \textit{TriBert} ($p=1$) outperformed \textit{Bert} in both ID and OOD settings across all levels, i.e., the overall level and breakdown levels (\#Bry$=1,2,3$), which demonstrated the advantage of \textit{TriBert}'s idea of calculating the dissimilarity (Euclidean distance) between every two adjacent prototypes and assuming that the boundaries exist between the most dissimilar (i.e., having the largest distance from each other) adjacent prototypes. 
%By contrast, Bert treated the sentences individually and simply performed binary classification over each sentence of the hybrid text, ignoring the information that can be obtained through comparing adjacent sentences.

\smallskip
\noindent\textbf{The Effect of Learning Embeddings through Separating AI-Content from Human-written Content. }
We observed that \textit{TriBert} (NT, $p=1, 2, ...$), which went through no further embedding learning (fine-tuning) and relied only on the pre-trained encoder from SentenceTransformers for sentence embedding, performed better than \textit{Random}. However, we also witnessed a significant performance improvement from the untrained \textit{TriBert} (NT, $p=1, 2, ...$) to the fine-tuned \textit{TriBert} ($p=1, 2, ...$), which demonstrated the necessity of fine-tuning the encoder (embedding learning) through separating AI-Content from human-written content before applying the encoder for boundary detection.

\smallskip
\noindent\textbf{The Effect of Varying the Prototype Size of TriBert. }
To better understand the role of prototype size $p$ in \textit{TriBert}, we first introduce two effects that can enhance/degrade the prototype in \textit{TriBert} when varying the prototype size $p$. Let's suppose that we have a set of $k$ consecutive sentences (sharing the same authorship) based on which the prototype will be calculated, denoted as $<s_{i}, s_{i+1},...,s_{i+k-1}>$. Then we would like to introduce a new adjacent sentence $s_{i+k}$ to this sentence pool, i.e., $p$ is growing from $k$ to $k+1$. Note that in this case, the introduction of the new adjacent sentence $s_{i+k}$ can either benefit the prototype (positive effect) or degrade the prototype (negative effect):

\begin{itemize}
    \item \textbf{Positive Effect}: If the newly introduced adjacent sentence $s_{i+k}$ shares the same authorship with the existing sentences from the pool, the prototype is enhanced and can yield better representation, increasing \textit{TriBert}'s performance.
    \item \textbf{Negative Effect}: If the authorship of $s_{i+k}$ is different from that of the existing sentences, $s_{i+k}$ is considered as noise because the prototype calculated based on hybrid content can represent neither AI content nor human-written content, i.e., the quality of the prototype is degraded.
\end{itemize}  
From the results of the overall level (the rightmost columns, denoted as \textit{All}), we observed that \textit{TriBert} achieved the best performance with prototype size $p=2$, for which we the following explanation: when $p<2$, the benefit of increasing $p$ outweighs the risk of bringing noise to the prototype calculation, i.e., the positive effect overcomes the negative effect and plays the dominant role as $p$ grows; However, when $p>=2$, the negative effect overcomes the positive effect and plays the dominant role as $p$ grows, which means \textit{TriBert}'s performance declines as $p$ grows.

Furthermore, when we dived into the results of the breakdown level (The results of \#Bry $=1,2,3$), we noticed that the best prototype size $p$ tends to be large (or small) if the number of boundaries is small (or large), i.e., the best $p$ for \#Bry $=1,2,3$ are $4$, $2$, and $1$, respectively. Our explanation for this observation is as follows: When we try to sample a set of consecutive sentences $S_k$ (Note that the prototype will be calculated based on $S_k$) from a hybrid text $T$, the more boundaries there are within $T$, the more likely we are to find hybrid content from the selected sentences. For example, consider the hybrid essay $A=<H-H-H-G-G>$ (Here $H$ denotes a human-written sentence and $G$ denotes an AI-generated sentence) with one boundary ($b=1$) located between the third and the fourth sentence. Let $s_i$ and $s_{i+1}$ be two consecutive sentences randomly sampled from $A$. The probability of $s_i$'s authorship being different from $s_{i+1}$'s is $1/4=25\%$. Similarly, this probability is $4/4=100\%$ for the hybrid essay $B=<H-G-H-G-H>$ sharing the same length with $A$ but with three more boundaries ($b=4$). As a result, \textit{TriBert} has a higher chance to sample hybrid content (which triggers the negative effect) for prototype calculation when predicting for hybrid text groups of \#Bry=$2,3$ compared to when predicting for the group with small boundary number, i.e., \#Bry=$1$. It is also noteworthy that one could alleviate the negative effect by using smaller $p$. As can be seen, the best $p$ for the group of \#Bry=$2,3$ is $1$ and $2$, respectively. However, when predicting for the group with a small boundary number (the \#Bry=$1$ group), \textit{TriBert} is more likely to sample sentences sharing consistent authorship (which triggers the positive effect) for prototype calculation, i.e., the prototype is calculated based on purely AI-content (or purely human-written content). In this case, a large prototype size $p$ is preferred for \textit{TriBert} to improve the detecting performance. As we can see, the best $p$ for \#Bry=$1$ group is when $p=4$, i.e., the proposed \textit{TriBert} with $p=4$ outperformed the second-best baseline \textit{Bert} by an improvement of $22$\% in the in-domain setting and $18\%$ in the out-of-domain setting, respectively.

%When detecting boundaries for hybrid essays with a single boundary, the performance of the proposed approach could be further enhanced by adopting a relatively large prototype size, outperforming the second-best baseline by a performance improvement of $22$\% in the in-domain setting and $18\%$ in the out-of-domain setting.





\section{Conclusion and Future Work}\label{sec:conclusion 5}
Human-AI collaborative writing is getting easier with the help of modern large language models (e.g., ChatGPT). However, the detection techniques for detecting AI content from hybrid text have rarely been explored, raising concerns among educators that students might leverage LLM to partially complete their writing assignments without being detected. To address this, we conducted the first study on automatic detection for Human-AI collaborative hybrid text in education, where we formalize the task of hybrid text detection as a boundary detection. Specifically, we propose a two-step boundary detection approach to (1) Separate AI-generated content from human-written content during the embedding learning (fine-tuning) process; and (2) Calculate the (dis)similarity between adjacent prototypes and assume that the boundaries exist between the most dissimilar adjacent prototypes. We conducted extensive empirical experiments, in which our proposed approach (with necessary training) demonstrated advantages over the other baseline methods across different experiment settings. Furthermore, we noticed that for the hybrid text with fewer boundaries (e.g., one boundary), the proposed approach performed well with a large prototype size; When the number of boundaries is large or unclear, a small prototype size is preferred. The above findings can shed light on how to utilize the proposed approach for better detection performance, e.g., if the hybrid texts are known to be written first by students and then by generative language models (i.e., single-boundary hybrid texts), it will be beneficial to start with a relatively large prototype size. One limitation of this study is that the number of boundaries in our hybrid text dataset is no more than $3$. For future work, we would like to evaluate the performance of the proposed boundary detection approach to hybrid texts with more than $3$ boundaries. 
%Another direction for future work is to evaluate the proposed method on hybrid text from other domains, e.g., scientific text \citep{ma2023abstract}.

\bibliography{aaai24}

\end{document}
