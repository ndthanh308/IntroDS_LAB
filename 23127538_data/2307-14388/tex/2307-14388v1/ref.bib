@inproceedings{free_lunch,
title = "No free lunch in data privacy",

year = "2011",
doi = "10.1145/1989323.1989345",
language = "English (US)",
isbn = "9781450306614",
series = "Proceedings of the ACM SIGMOD International Conference on Management of Data",
publisher = "Association for Computing Machinery",
pages = "193--204",
booktitle = "Proceedings of SIGMOD 2011 and PODS 2011",
}

@inproceedings{Kifer2012ARA,
  title={A rigorous and customizable framework for privacy},
  author={Daniel Kifer and Ashwin Machanavajjhala},
  booktitle={ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  year={2012}
}

@article{Kifer2014PufferfishAF,
  title={Pufferfish: A framework for mathematical privacy definitions},
  author={Daniel Kifer and Ashwin Machanavajjhala},
  journal={ACM Trans. Database Syst.},
  year={2014},
  volume={39},
  pages={3:1-3:36}
}

@ARTICLE{Composition,
  author={Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
  journal={IEEE Transactions on Information Theory}, 
  title={The Composition Theorem for Differential Privacy}, 
  year={2017},
  volume={63},
  number={6},
  pages={4037-4049},
  doi={10.1109/TIT.2017.2685505}}

@INPROCEEDINGS{Quantify_DP,
  author={Cao, Yang and Yoshikawa, Masatoshi and Xiao, Yonghui and Xiong, Li},
  booktitle={2017 IEEE 33rd International Conference on Data Engineering (ICDE)}, 
  title={Quantifying Differential Privacy under Temporal Correlations}, 
  year={2017},
  volume={},
  number={},
  pages={821-832},
  doi={10.1109/ICDE.2017.132}}

  @inproceedings{DP_mec,
author = {Xiao, Yonghui and Xiong, Li},
title = {Protecting Locations with Differential Privacy under Temporal Correlations},
year = {2015},
isbn = {9781450338325},
address = {New York, NY, USA},
doi = {10.1145/2810103.2813640},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1298–1309},
numpages = {12},
keywords = {location-based services, differential privacy, location privacy, sensitivity hull, planar isotropic mechanism},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@INPROCEEDINGS{Quantify_LP,
  author={Shokri, Reza and Theodorakopoulos, George and Le Boudec, Jean-Yves and Hubaux, Jean-Pierre},
  booktitle={2011 IEEE Symposium on Security and Privacy}, 
  title={Quantifying Location Privacy}, 
  year={2011},
  volume={},
  number={},
  pages={247-262},
  doi={10.1109/SP.2011.18}}

  @inproceedings{prolong_DP,
author = {Theodorakopoulos, George and Shokri, Reza and Troncoso, Carmela and Hubaux, Jean-Pierre and Le Boudec, Jean-Yves},
title = {Prolonging the Hide-and-Seek Game: Optimal Trajectory Privacy for Location-Based Services},
year = {2014},
isbn = {9781450331487},
address = {New York, NY, USA},
doi = {10.1145/2665943.2665946},
abstract = {Human mobility is highly predictable. Individuals tend to only visit a few locations with high frequency, and to move among them in a certain sequence reflecting their habits and daily routine. This predictability has to be taken into account in the design of location privacy preserving mechanisms (LPPMs) in order to effectively protect users when they expose their whereabouts to location-based services (LBSs) continuously. In this paper, we describe a method for creating LPPMs tailored to a user's mobility profile taking into her account privacy and quality of service requirements. By construction, our LPPMs take into account the sequential correlation across the user's exposed locations, providing the maximum possible trajectory privacy, i.e., privacy for the user's past, present location, and expected future locations. Moreover, our LPPMs are optimal against a strategic adversary, i.e., an attacker that implements the strongest inference attack knowing both the LPPM operation and the user's mobility profile.The optimality of the LPPMs in the context of trajectory privacy is a novel contribution, and it is achieved by formulating the LPPM design problem as a Bayesian Stackelberg game between the user and the adversary. An additional benefit of our formal approach is that the design parameters of the LPPM are chosen by the optimization algorithm.},
booktitle = {Proceedings of the 13th Workshop on Privacy in the Electronic Society},
pages = {73–82},
numpages = {10},
keywords = {location privacy, bayesian stackelberg game, optimal location obfuscation, location transition privacy, trajectory privacy, privacy-utility tradeoff},
location = {Scottsdale, Arizona, USA},
series = {WPES '14}
}


@ARTICLE{6949097,
  author={Zhu, Tianqing and Xiong, Ping and Li, Gang and Zhou, Wanlei},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Correlated Differential Privacy: Hiding Information in Non-IID Data Set}, 
  year={2015},
  volume={10},
  number={2},
  pages={229-242},
  doi={10.1109/TIFS.2014.2368363}}

  @INPROCEEDINGS{9488899,
  author={Ye, Qingqing and Hu, Haibo and Li, Ninghui and Meng, Xiaofeng and Zheng, Huadi and Yan, Haotian},
  booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
  title={Beyond Value Perturbation: Local Differential Privacy in the Temporal Setting}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/INFOCOM42981.2021.9488899}}

@article{CGM,
author = {Bao, Ergute and Yang, Yin and Xiao, Xiaokui and Ding, Bolin},
title = {CGM: An Enhanced Mechanism for Streaming Data Collection with Local Differential Privacy},
year = {2021},
issue_date = {July 2021},
volume = {14},
number = {11},
issn = {2150-8097},
doi = {10.14778/3476249.3476277},
abstract = {Local differential privacy (LDP) is a well-established privacy protection scheme for collecting sensitive data, which has been integrated into major platforms such as iOS, Chrome, and Windows. The main idea is that each individual randomly perturbs her data on her local device, and only uploads the noisy version to an untrusted data aggregator. This paper focuses on the collection of streaming data consisting of regular updates, e.g., daily app usage. Such streams, when aggregated over a large population, often exhibit strong autocorrelations, e.g., the average usage of an app usually does not change dramatically from one day to the next. To our knowledge, this property has been largely neglected in existing LDP mechanisms. Consequently, data collected with current LDP methods often exhibit unrealistically violent fluctuations due to the added noise, drowning the overall trend, as shown in our experiments.This paper proposes a novel correlated Gaussian mechanism (CGM) for enforcing (ϵ, δ)-LDP on streaming data collection, which reduces noise by exploiting public-known autocorrelation patterns of the aggregated data. This is done through non-trivial modifications to the core of the underlying Gaussian Mechanism; in particular, CGM injects temporally correlated noise, computed through an optimization program that takes into account the given autocorrelation pattern, data value range, and utility metric. CGM comes with formal proof of correctness, and consumes negligible computational resources. Extensive experiments using real datasets from different application domains demonstrate that CGM achieves consistent and significant utility gains compared to the baseline method of repeatedly running the underlying one-shot LDP mechanism.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2258–2270},
numpages = {13}
}

@inproceedings{Conti_release_LDP,
author = {Wang, Tianhao and Chen, Joann Qiongna and Zhang, Zhikun and Su, Dong and Cheng, Yueqiang and Li, Zhou and Li, Ninghui and Jha, Somesh},
title = {Continuous Release of Data Streams under Both Centralized and Local Differential Privacy},
year = {2021},
isbn = {9781450384544},
address = {New York, NY, USA},
doi = {10.1145/3460120.3484750},
abstract = {We study the problem of publishing a stream of real-valued data satisfying differential privacy (DP). One major challenge is that the maximal possible value in the stream can be quite large, leading to enormous DP noise and bad utility. To reduce the maximal value and noise, one way is to estimate a threshold so that values above it can be truncated. The intuition is that, in many scenarios, only a few values are large; thus truncation does not change the original data much. We develop such a method that finds a suitable threshold with DP. Given the threshold, we then propose an online hierarchical method and several post-processing techniques.Building on these ideas, we formalize the steps in a framework for the private publishing of streaming data. Our framework consists of three components: a threshold optimizer that privately estimates the threshold, a perturber that adds calibrated noise to the stream, and a smoother that improves the result using post-processing. Within our framework, we also design an algorithm satisfying the more stringent DP setting called local DP. Using four real-world datasets, we demonstrate that our mechanism outperforms the state-of-the-art by a factor of $6-10$ orders of magnitude in terms of utility (measured by the mean squared error of the typical scenario of answering a random range query).},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1237–1253},
numpages = {17},
keywords = {continuous observation, local differential privacy, data stream, differential privacy},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}


@inproceedings{Dwork_DPcontinualOb,
author = {Dwork, Cynthia and Naor, Moni and Pitassi, Toniann and Rothblum, Guy N.},
title = {Differential Privacy under Continual Observation},
year = {2010},
isbn = {9781450300506},
address = {New York, NY, USA},
doi = {10.1145/1806689.1806787},
abstract = {Differential privacy is a recent notion of privacy tailored to privacy-preserving data analysis [11]. Up to this point, research on differentially private data analysis has focused on the setting of a trusted curator holding a large, static, data set; thus every computation is a "one-shot" object: there is no point in computing something twice, since the result will be unchanged, up to any randomness introduced for privacy. However, many applications of data analysis involve repeated computations, either because the entire goal is one of monitoring, e.g., of traffic conditions, search trends, or incidence of influenza, or because the goal is some kind of adaptive optimization, e.g., placement of data to minimize access costs. In these cases, the algorithm must permit continual observation of the system's state. We therefore initiate a study of differential privacy under continual observation. We identify the problem of maintaining a counter in a privacy preserving manner and show its wide applicability to many different problems.},
booktitle = {Proceedings of the Forty-Second ACM Symposium on Theory of Computing},
pages = {715–724},
numpages = {10},
keywords = {private data analysis, privacy},
location = {Cambridge, Massachusetts, USA},
series = {STOC '10}
}

@ARTICLE{7918623,
  author={Calmon, Flavio du Pin and Makhdoumi, Ali and Médard, Muriel and Varia, Mayank and Christiansen, Mark and Duffy, Ken R.},
  journal={IEEE Transactions on Information Theory}, 
  title={Principal Inertia Components and Applications}, 
  year={2017},
  volume={63},
  number={8},
  pages={5011-5038},
  doi={10.1109/TIT.2017.2700857}}


  @INPROCEEDINGS{8262832,
  author={Wang, Hao and Calmon, Flavio P.},
  booktitle={2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={An estimation-theoretic view of privacy}, 
  year={2017},
  volume={},
  number={},
  pages={886-893},
  doi={10.1109/ALLERTON.2017.8262832}}
  
  @ARTICLE{8438536,
  author={Asoodeh, Shahab and Diaz, Mario and Alajaji, Fady and Linder, Tamás},
  journal={IEEE Transactions on Information Theory}, 
  title={Estimation Efficiency Under Privacy Constraints}, 
  year={2019},
  volume={65},
  number={3},
  pages={1512-1534},
  doi={10.1109/TIT.2018.2865558}}

  @inproceedings{Pufferfish_mec,
author = {Song, Shuang and Wang, Yizhen and Chaudhuri, Kamalika},
title = {Pufferfish Privacy Mechanisms for Correlated Data},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3064025},
doi = {10.1145/3035918.3064025},
abstract = {Many modern databases include personal and sensitive correlated data, such as private information on users connected together in a social network, and measurements of physical activity of single subjects across time. However, differential privacy, the current gold standard in data privacy, does not adequately address privacy issues in this kind of data.This work looks at a recent generalization of differential privacy, called Pufferfish, that can be used to address privacy in correlated data. The main challenge in applying Pufferfish is a lack of suitable mechanisms. We provide the first mechanism -- the Wasserstein Mechanism -- which applies to any general Pufferfish framework. Since this mechanism may be computationally inefficient, we provide an additional mechanism that applies to some practical cases such as physical activity measurements across time, and is computationally efficient. Our experimental evaluations indicate that this mechanism provides privacy and utility for synthetic as well as real data in two separate domains.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1291–1306},
numpages = {16},
keywords = {pufferfish privacy, differential privacy, privacy},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@ARTICLE{7498650, 
author={W. Wang and L. Ying and J. Zhang}, 
journal={IEEE Transactions on Information Theory}, 
title={On the Relation Between Identifiability, Differential Privacy, and Mutual-Information Privacy}, 
year={2016}, 
volume={62}, 
number={9}, 
pages={5018-5029}, 
keywords={data privacy;Hamming distance;differential privacy level;identifiability level;mutual-information optimal mechanism;mutual-information privacy;unified privacy-distortion framework;Data analysis;Data privacy;Databases;Distortion;Mutual information;Privacy;Random variables;Differential privacy;Hamming distance;identifiability;mutual information;rate-distortion}, 
doi={10.1109/TIT.2016.2584610}, 
ISSN={0018-9448}, 
month={September},}


@inproceedings{10.1145/2714576.2714577,
author = {Ma, Chris Y.T. and Yau, David K.Y.},
title = {On Information-Theoretic Measures for Quantifying Privacy Protection of Time-Series Data},
year = {2015},
isbn = {9781450332453},
address = {New York, NY, USA},
doi = {10.1145/2714576.2714577},
abstract = {Privacy protection of time-series data, such as traces of household electricity usage reported by smart meters, is of much practical importance. Solutions are available to improve data privacy by perturbing clear traces to produce noisy versions visible to adversaries, e.g., in battery-based load hiding (BLH) against non-intrusive load monitoring (NILM). A foundational task for research progress in the area is the definition of privacy measures that can truly evaluate the effectiveness of proposed protection methods. It is a difficult problem since resilience against any attack algorithms known to the designer is inconclusive, given that adversaries could discover or indeed already know stronger algorithms for attacks. A more basic measure is information-theoretic in nature, which quantifies the inherent information available for exploitation by an adversary, independent of how the adversary exploits it or indeed any assumed computational limitations of the adversary. In this paper, we analyze information-theoretic measures for privacy protection and apply them to several existing protection methods against NILM. We argue that although these measures abstract away the details of attacks, the kind of information the adversary considers plays a key role in the evaluation, and that a new measure of offline conditional entropy is better suited for evaluating the privacy of perturbed real-world time-series data, compared with other existing measures.},
booktitle = {Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security},
pages = {427–438},
numpages = {12},
keywords = {privacy measure, correlated time-series, conditional entropy, privacy protection},
location = {Singapore, Republic of Singapore},
series = {ASIA CCS '15}
}

@ARTICLE{9153837,
  author={Erdemir, Ecenaz and Dragotti, Pier Luigi and Gündüz, Deniz},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Privacy-Aware Time-Series Data Sharing With Deep Reinforcement Learning}, 
  year={2021},
  volume={16},
  number={},
  pages={389-401},
  doi={10.1109/TIFS.2020.3013200}}


  @ARTICLE{9715086,
  author={Zhang, Wenjing and Jiang, Bo and Li, Ming and Lin, Xiaodong},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Privacy-Preserving Aggregate Mobility Data Release: An Information-Theoretic Deep Reinforcement Learning Approach}, 
  year={2022},
  volume={17},
  number={},
  pages={849-864},
  doi={10.1109/TIFS.2022.3152361}}

  @ARTICLE{LIP1,
  author={Jiang, Bo and Li, Ming and Tandon, Ravi},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Local Information Privacy and Its Application to Privacy-Preserving Data Aggregation}, 
  year={2022},
  volume={19},
  number={3},
  pages={1918-1935},
  doi={10.1109/TDSC.2020.3041733}}

  @ARTICLE{LIP2,
  author={Jiang, Bo and Seif, Mohamed and Tandon, Ravi and Li, Ming},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Context-Aware Local Information Privacy}, 
  year={2021},
  volume={16},
  number={},
  pages={3694-3708},
  doi={10.1109/TIFS.2021.3087350}}

  @inproceedings{Centrlized_IP,
  title={Privacy against statistical inference},
  author={du Pin Calmon, Fl{\'a}vio and Fawaz, Nadia},
  booktitle={50th annual Allerton conference on communication, control, and computing (Allerton)},
  pages={1401--1408},
  month = {October},
  year={2012}
}


@inproceedings{10.1145/3314111.3319915,
author = {Steil, Julian and Hagestedt, Inken and Huang, Michael Xuelin and Bulling, Andreas},
title = {Privacy-Aware Eye Tracking Using Differential Privacy},
year = {2019},
isbn = {9781450367097},
address = {New York, NY, USA},
doi = {10.1145/3314111.3319915},
abstract = {With eye tracking being increasingly integrated into virtual and augmented reality (VR/AR) head-mounted displays, preserving users' privacy is an ever more important, yet under-explored, topic in the eye tracking community. We report a large-scale online survey (N=124) on privacy aspects of eye tracking that provides the first comprehensive account of with whom, for which services, and to what extent users are willing to share their gaze data. Using these insights, we design a privacy-aware VR interface that uses differential privacy, which we evaluate on a new 20-participant dataset for two privacy sensitive tasks: We show that our method can prevent user re-identification and protect gender information while maintaining high performance for gaze-based document type classification. Our results highlight the privacy challenges particular to gaze data and demonstrate that differential privacy is a potential means to address them. Thus, this paper lays important foundations for future research on privacy-aware gaze interfaces.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
articleno = {27},
numpages = {9},
keywords = {data sharing, privacy protection, user modeling, online survey, eye movements, gaze behaviour},
location = {Denver, Colorado},
series = {ETRA '19}
}
@article{Miller_2020, doi = {10.1038/s41598-020-74486-y}, year = 2020, month = {oct}, publisher = {Springer Science and Business Media {LLC}}, volume = {10}, number = {1}, author = {Mark Roman Miller and Fernanda Herrera and Hanseul Jun and James A. Landay and Jeremy N. Bailenson}, title = {Personal identifiability of user tracking data during observation of 360-degree {VR} video}, journal = {Scientific Reports} }

@incollection{Dwork20061,
author="Dwork, Cynthia",
editor="Bugliesi, Michele
and Preneel, Bart
and Sassone, Vladimiro
and Wegener, Ingo",
title="Differential Privacy",
bookTitle="Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Part II",
year="2006",
pages="1--12",
abstract="In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.",
isbn="978-3-540-35908-1",
doi="10.1007/11787006_1"
}

@incollection{Dwork2006,
author="Dwork, Cynthia
and McSherry, Frank
and Nissim, Kobbi",
title="Calibrating Noise to Sensitivity in Private Data Analysis",
bookTitle="Theory of Cryptography: Third Theory of Cryptography Conference",
year="2006",
pages="265--284",
isbn="978-3-540-32732-5",
doi="10.1007/11681878_14"
}

@incollection{Dwork2008,
author="Dwork, Cynthia",
editor="Agrawal, Manindra
and Du, Dingzhu
and Duan, Zhenhua",
title="Differential Privacy: A Survey of Results",
booktitle="Theory and Applications of Models of Computation: 5th International Conference, TAMC",
year="2008",
pages="1--19",
isbn="978-3-540-79228-4",
doi="10.1007/978-3-540-79228-4_1"
}


@ARTICLE{7867732,
  author={Alrawais, Arwa and Alhothaily, Abdulrahman and Hu, Chunqiang and Cheng, Xiuzhen},
  journal={IEEE Internet Computing}, 
  title={Fog Computing for the Internet of Things: Security and Privacy Issues}, 
  year={2017},
  volume={21},
  number={2},
  pages={34-42},
  doi={10.1109/MIC.2017.37}}

  @unknown{medical,
author = {Beaulieu-Jones, Brett and Wu, Zhiwei and Williams, Chris and Lee, Ran and Bhavnani, Sanjeev and Byrd, James and Greene, Casey},
year = {2018},
month = {12},
pages = {},
title = {Privacy-preserving generative deep neural networks support clinical data sharing},
doi = {10.1101/159756}
}


@INPROCEEDINGS{context,
  author    = {Chong Huang and
               Peter Kairouz and
               Xiao Chen and
               Lalitha Sankar and
               Ram Rajagopal},
  title     = {Context-Aware Generative Adversarial Privacy},
  bookTitle={Entropy},
  year      = {2017},
  eprint    = {19, 656.},
}


@inproceedings{relation,
author = {Cuff, Paul and Yu, Lanqing},
title = {Differential Privacy as a Mutual Information Constraint},
year = {2016},
isbn = {9781450341394},
doi = {10.1145/2976749.2978308},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {43–54},
numpages = {12},
keywords = {information theory, differential privacy},
location = {Vienna, Austria},
series = {CCS ’16}
}

@inproceedings{Machanavajjhala:2008:PTM:1546682.1547184,
 author = {Machanavajjhala, Ashwin and Kifer, Daniel and Abowd, John and Gehrke, Johannes and Vilhuber, Lars},
 title = {Privacy: Theory Meets Practice on the Map},
 booktitle = {Proceedings of the 2008 IEEE 24th International Conference on Data Engineering},
 series = {ICDE '08},
 year = {2008},
 isbn = {978-1-4244-1836-7},
 pages = {277--286},
 numpages = {10},
 doi = {10.1109/ICDE.2008.4497436},
 acmid = {1547184},
} 

@inproceedings{inproceedingsCensus,
author = {Abowd, John},
title = {The {U.S.} Census Bureau Adopts Differential Privacy},
bookTitle = {24th International Conference on Knowledge Discovery  Data Mining (ACM SIGKDD), London, UK},
year = {2018},
month = {07},
pages = {2867-2867},
doi = {10.1145/3219819.3226070}
}

@inproceedings{Rappor,

title = {RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response},
author  = {Úlfar Erlingsson and Vasyl Pihur and Aleksandra Korolova},
year  = {2014},
booktitle = {Proceedings of the 21st ACM CCS}
}

@inproceedings{Cormode:2018:PSL:3183713.3197390,
 author = {Cormode, Graham and Jha, Somesh and Kulkarni, Tejas and Li, Ninghui and Srivastava, Divesh and Wang, Tianhao},
 title = {Privacy at Scale: Local Differential Privacy in Practice},
 booktitle = {Proceedings of the 2018 International Conference on Management of Data},
 series = {SIGMOD '18},
 year = {2018},
 isbn = {978-1-4503-4703-7},
 location = {Houston, TX, USA},
 pages = {1655--1658},
 numpages = {4},
 doi = {10.1145/3183713.3197390},
 acmid = {3197390},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data collection, differential privacy, local differential privacy, privacy},
} 
@article{LPSApple,
author={Differential Privacy Team},
title={Learning with privacy at scale},
journal = {https: //machinelearning.apple.com/2017/12/06/
learning-with-privacy-at-scale.html},
}
