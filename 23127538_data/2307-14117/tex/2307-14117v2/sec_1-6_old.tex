\section{Introduction}
\label{sec:intro}

% Chit chat 


% Figure environment removed

A core strategy to improve social conversation models is through human feedback. There has been remarkable progress on learning from feedback, including reinforcement learning with human feedback \citep{stiennon2020learning,bai2022training}, where a large number of human annotations are needed to ensure a good reward function. The sources of feedback include binary ratings of a bot turn, users' feedback of a dialogue turn or dialogue episode in the form of numerical scores, rankings or natural language comments, and so on. 
In most works, these signals are collected explicitly using crowdworkers, as organic users may not want to be burdened with providing explicit signals, or else may provide unreliable information \cite{ju2022learning}.



In this work, we consider the setting where we have a large number of dialogue episodes of deployment-time dialogue which consist of natural conversations between the model and organic users. We want to see if we can obtain any implicit signal from these organic user conversations, and leverage these signals to improve the dialogue model. The rationale is two-fold. First, the organic users most closely approximate the data distribution
%user group in future dialogue model 
for future deployment; yet they may not  % want to %organically 
provide explicit annotations. %dense explicit annotations. 
Second, relying on implicit signals from existing conversation episodes does not incur extra cost that would otherwise be spent on crowdsourcing. More specifically, in this work we investigate the following: can we fine-tune the chatbot so as to optimize for \emph{implicit} feedback signals like the number of, length, sentiment, or reaction of future human responses?





To investigate this issue, we use publicly released de-identified data \citep{xu2023improving} from the BlenderBot online deployment  %from August 2022 to January 2023 
\citep{shuster2022blenderbot}. Utilizing this data  we train sample-and-rerank models, comparing various implicit feedback signals.
Through both automatic and human judgments, our new models 
%that leverage implicit signals
are found to be preferable to the baseline responses. Furthermore, as our implicit feedback signals are coarse {proxy} metrics of the quality of bot generations, we ask whether encouraging these metrics would lead to undesirable behaviors. The answer is yes, depending on the chosen signal: in particular, optimizing for longer conversation lengths
can make the model produce controversial takes or respond in an unfriendly or confrontational way. Optimizing for positive reaction or sentiment on the other hand has {\em the opposite} effect, and decreases these behaviors compared to the baseline.
Overall, we find that implicit feedback from humans is a useful training signal that can improve overall performance, but the precise signal used  
has important behavioral consequences.
% care must be taken to find the best recipe to extract them.



\section{Related Work}


Researchers and practitioners have strived to build better neural open-domain dialogue models for years \citep{chen2017survey,gao2018neural,khatri2018advancing}. 
DialoGPT~\citep{zhang-etal-2020-dialogpt} and BlenderBot~\citep{shuster2022blenderbot} have released the models as well as the training pipelines which have enabled follow-up dialogue projects from the community \citep{bang-etal-2021-assessing,adewumi2022smaaprat}. In particular, for BlenderBot, dialogue interaction data has been released \cite{xu2023improving,shi2022life}, which makes the study of implicit signals in our work possible. 

The class of training strategies that are most relevant to this work -- decoding utterances for future success -- is discussed in \citet{li2017learning}, in which they interpolate the MLE-trained token-level conditional probability with a value function that is trained to predict the property of a completed sequence (e.g., length, BLEU/ROUGE against the reference) given a partial sequence. This overall idea is extended in \citet{zemlyanskiy-sha-2018-aiming} where a chatbot learns to generate utterances that have the maximal information gain about the human in the future, as well as \citet{kulikov2019multi} that propose to generate the current bot utterance that leads to the most probable sequence of future utterances. The concurrent work \citet{irvine2023rewarding} uses conversation engagement metrics (e.g., approximated by retry rate,\footnote{A user can click ``retry'' in their software so that the bot re-generates the response.} manually-annotated engagement metrics, whether the conversation is ending soon) to optimize for engaging bot responses. 
In contrast, our work highlights both the strengths as well as the challenges of using implicit feedback, 
and in particular that conversation engagement metrics have negative consequences that can be alleviated through other choices of implicit signal.



\section{Approach}
\label{sec:approach}


\subsection{Implicit Feedback Signals}

Our goal is to extract learning signals from a (relatively large) set of human-bot conversational episodes. 
We assume such a set has
already been collected.
We represent a conversation episode between a human and a bot as ${\bf{x}} = (\vx_1^b, \vx_1^h, \vx_2^b, \vx_2^h, \dots)$ with $T$ utterances by the bot (denoted with superscript ``$b$'', where the bot is assumed to speak first) and $T'$ utterances by the human (denoted with ``$h$'').\footnote{% Given that we assume that the bot initiates the dialogue, $T'$ equals either $T$ or $T-1$. 
For simplicity, we do not include the static context (e.g., memories of the other speaker's experiences) in the notation, but the context (if exists) is always included in the input.}  
We use ${\bf{x}}_{<t}$ to denote the conversation history before bot's $t$-th turn: $\vx_1^b, \vx_1^h, \dots, \vx_{t-1}^b, \vx_{t-1}^h$. 
% \subsection{Signals} 
Next, we define the implicit feedback-derived scoring function $r_\phi(\vx_t^b, {\bf{x}}_{<t})$ that predicts the quality of the bot's $t$-th turn $\vx_t^b$ given past utterances. The input to $r_\phi$ is the first $t$ bot utterances and the first $t-1$ human utterances; the output is a score in $[0,1]$ representing the probability that $\vx_t^b$ is good, according to one of the below criteria. Crucially, for the training data (but not for test data) we have access to entire conversations ${\bf{x}}$ (with $T+T'$ utterances for a given episode). We  can hence 
use future human turns to gather implicit feedback to judge the quality of $\vx_t^b$, which we hence use to define training labels $y(\vx_t^b|{\bf{x}})$ in order to learn
$r_\phi$. 
We consider several candidate implicit signals, which we describe next.

\paragraph{Existence of next human turn.} Intuitively, if the human user quits the conversation after the bot's $t$-th turn $\vx_t^b$, then it is likely that $\vx_t^b$ is 
of poor quality. % This signal is a special case ($k=0$) of the length-based signal.
Conversely if humans continue to converse, and do not quit, this prolonged 
engagement can be seen as a proxy for satisfaction \cite{o2008user}.  Therefore, set the reference label $y(\vx_t^b|{\bf{x}})$ for training $r_\phi(\vx_t^b, {\bf{x}}_{<t})$ to 1 if the next human turn exists, and 0 otherwise.
We use ``replied'' to represent this signal in later sections. 

\paragraph{Next human turn length.} 
If a human is unwilling to invest time into the conversation their responses may be shorter. Given the crude intuition that a long human turn likely implies that the previous bot turn is good, let  $y(\vx_t^b|{\bf{x}})$   be 1 if the next human turn has $\geq k$ words ($k$ is a hyperparameter); 0 otherwise. Granted, the intuition is not always true in practice (e.g., a human response could be a tirade against previous bot turns). We use ``length'' to represent this signal. 


\paragraph{Number of words in all future human utterances or number of future human turns.}
We can also build variants of the previous two signals that instead take into account multiple future turns to build the scoring function.
For turn lengths, let  $y(\vx_t^b|{\bf{x}})$ be 1 if the length of all future human turns is larger than a threshold $k$. Otherwise, set the score to 0. 
For number of turns, let  $y(\vx_t^b|{\bf{x}})$  be 1 if there are $\geq k$ human utterances in the future. Intuitively, if a bot turn approaches the end of the conversation episode, then it may be an inferior one.


\paragraph{Sentiment in the next human utterance.} % combined with length.} 
We use a recent positive/neutral/negative sentiment analysis model trained on tweets \citep{camacho-collados-etal-2022-tweetnlp}. % \footnote{\texttt{cardiffnlp/twitter-roberta-base-sentiment-latest} on Hugging Face.} 
Intuitively, we want humans to react positively in future responses. For sentiment and reaction signals, we find that the classifiers struggle at classifying very short utterances. At the same time, very short human responses likely mean that humans are unwilling to meaningfully engage in the conversation. We thus experiment with these two options: (1) Set the reference label  $y(\vx_t^b|{\bf{x}})$  to 1 if sentiment of $\vx_t^h$ is positive or neutral, and length is $\geq$ 5 words; 0 otherwise. (2) Set the reference label to 1 if sentiment is positive and length is $\geq$ 5 words; 0 otherwise. 

\paragraph{Reaction in the next human utterance.} We use a model \citep{hartmann2022emotionenglish} fine-tuned from DistilRoBERTa \citep{sanh2019distilbert}; % \footnote{\texttt{j-hartmann/emotion-english-distilroberta-base} on Hugging Face.} 
the seven output categories are anger, disgust, fear, joy, neutral, sadness, and surprise. 
We train a classifier that predicts whether the human next turn would have the ``joy'' reaction and $\geq$ 5 words at the same time.\footnote{We also tried the following: the classifier predicts whether the human next turn's top predicted reaction is anger/disgust or non-anger/disgust, but we find that this feature cannot be well-predicted (dev accuracy $\sim$55\%).} 
Let  $y(\vx_t^b|{\bf{x}}) = 1$ if the reaction of $\vx_t^h$ is joy and length is $\geq$ 5 words; 0 otherwise. We use ``joy \& length'' to represent this signal in later sections. 







\subsection{Generation Using Implicit Signals} \label{sec:implicit_generation}
% \label{sec:algo}

% The remainder of the section describes the generation algorithm. 
We use the simple sample-and-rerank approach, which has been shown to perform similarly (albeit with a larger computation cost which is not the focus of our discussion) as RL-based approaches in machine translation \citep{pang-etal-2022-amortized} and learning from pairwise feedback in language modeling \citep{dubois2023alpacafarm}. Given a conversation history, the first step is to sample 20 candidate responses. To sample, we use factual top-$p$ sampling \citep{lee2022factuality} given that \citet{shuster2022blenderbot} have shown that it achieves a good balance between generation diversity and factuality for social conversations.\footnote{The high level idea is that in factual top-$p$ sampling, $p$ varies by time-step $t$ which leads to more factual generations.} The second step is to rerank these generations using a reranker model, i.e., the classifier $r_\phi$ trained using the deployment data with implicit feedback labels $y$. We then pick the candidate generation with the highest reranker score. 



\begin{table*}[ht!]
\setlength{\tabcolsep}{2.1pt}
\centering
\small
\begin{tabular}{lccccc}
    \toprule
    & \makecell[c]{classifier accuracy \\ under balanced \\ dev set} & \makecell[c]{score of generations \\ (scored by classifier) \\ (baseline / new)} & \makecell[c]{annotator pref. \\ (baseline / new / tie)} & sig. & \makecell[c]{mean \\ length}  \\
    \midrule
    baseline & -- & -- & -- & -- & 19.7 \\
    ranked by probability & -- & -- & 27.0 / 30.0 / 43.0 & -- & 18.1 \\
    baseline + replied & 0.678 & 0.957 / 0.999 & 33.0 / 32.0 / 35.0 & -- & 20.2 \\
    \makecell[l]{baseline + length ($k$=20)} & 0.761 & 0.332 / 0.708 & 31.0 / 43.0 / 26.0 & $\ast\ast$ & 21.9  \\  % 200 examples % there was a typo in this line
    \makecell[l]{baseline + length ($k$=5)} & 0.624 & 0.587 / 0.740 & 31.0 / 36.0 / 33.0 & -- & 24.2 \\
    \makecell[l]{baseline + non-neg. sentiment \& length ($k$=5)} & 0.603 & 0.524 / 0.634 & 29.0 / 37.5 / 33.5 & $\ast$ & 21.9 \\  % 200
    \makecell[l]{baseline + positive sentiment \& length ($k$=5)} & 0.670 & 0.506 / 0.742 & 31.5 / 38.0 / 30.5 & -- & 19.4 \\
    baseline + joy \& length ($k$=5) & 0.675 & 0.486 / 0.766 & 27.0 / 36.5 / 36.5 & $\ast\ast$ & 19.4  \\
    \bottomrule
\end{tabular}
\caption{Performance of generated dialogue responses using different implicit feedback signals. 
Sig.: ** if $p$-value $\in [0, 0.05)$, * if $p$-value $\in [0.05, 0.1)$, -- otherwise. See \S\ref{app:additional-info-eval} for more info. 
}
\label{tab:main}
\end{table*}

% \afterpage{\afterpage{
% \addtocounter{footnote}{-1}
% \footnotetext{test}
% \addtocounter{footnote}{1}
% \footnotetext{test}
% }}

\begin{table*}[ht!]
\setlength{\tabcolsep}{2.1pt}
\centering
\small
\begin{tabular}{lcccccccc}
    \toprule
    & \makecell[c]{\% \\ seek info} & \makecell[c]{\% \\ off-topic} & \makecell[c]{\% off-topic \\  \& seek info} & \makecell[c]{\% \\ insincere} & \makecell[c]{\% \\ controversial} & \makecell[c]{\% \\ unfriendly} \\
    \midrule
    baseline & 32.5 & 11.5 & 3.0 & 20.0 & 17.0 &  9.0\\
    ranked by probability & 43.0 & 13.5 & 4.0 & 16.0 & 16.0 &  7.0\\
    baseline + replied & 47.5 & 16.0 & 5.0 & 21.0 & 24.5 &  12.5\\
    \makecell[l]{baseline + length ($k$=20)} & 46.0 & 15.0 & 4.5 & 20.0 & 17.0 &  12.5 \\
    \makecell[l]{baseline + length ($k$=5)} & 56.0 & 13.0 & 8.0 & 19.0 & 19.0 &  9.5 \\
    % \makecell[l]{+ length \\ ~~~~thr=5} \\
    \makecell[l]{baseline + non-neg. sentiment \& length} & 60.0 & 14.5 & 8.0 & 21.0 &  13.0 & 6.0  \\
    \makecell[l]{baseline + positive sentiment \& length} & 41.0 & 11.0 & 3.5 & 20.0 & 9.5 &  6.0 \\
    baseline + joy \& length & 49.0 & 12.0 & 8.0 & 22.5 & 8.5 & 6.0\\
    \bottomrule
\end{tabular}
\caption{Properties of generations from the baseline and different implicit feedback models. 
}
\label{tab:properties}
\end{table*}



\section{Experiments and Results}

\subsection{Experimental Setup}

% \paragraph{Classifiers.}

We base our experiments off the publicly released BlenderBot deployment data \citep{xu2023improving}, in order to build our implicit feedback models. The dataset used in this work contains 3.1M bot utterances and 3.1M human utterances collected from August 2022
to January 2023.  The classifiers (i.e., implicit feedback-based rerankers) are based on a pretrained RoBERTa with 24 layers, 16 attention heads, token embedding size 1024, and FFN size 4096. Table~\ref{tab:main} examines the accuracy of the classifiers based on different implicit feedback signals under a balanced dev set (where the number of examples is equal across classes). For the ``number of words in all future human utterances'' signal and the ``number of future human turns'' signal, the best accuracy is 0.595 (experimented with threshold $k=$ 30, 50, 100) and 0.587 (with threshold 2, 3, 5, 10), respectively. We have also attempted restricting the classification problem to conversations with at least 3, 5, or 10 human turns -- the accuracy is still $<$0.6.  We consider the accuracy to be too low for generation experiments, so we discard these signals in the rest of our experiments.

% \paragraph{Base dialogue model.} 
Our baseline model is the publicly released BlenderBot model (\texttt{r2c2\_blenderbot\_3B}) with around 3B parameters, pretrained on both dialogue and language modeling tasks, and fine-tuned on dialogue tasks \citep{shuster2022language}. The model has 2 encoder layers, 24 decoder layers, 32 attention heads, FFN size 10240, and embedding size 2560. 
We also report results for the method ``ranked by probability'' -- we simply rerank using the sequence-level probabilities during sample-and-rerank. We want to see whether our approaches based on the implicit feedback classifiers outperform using the naive ranking criteria (i.e., by probabilities). 


\subsection{Evaluation Methods}
\label{sec:eval-methods}

Given a conversation history and two candidate responses (baseline and new model responses), we ask a large language model (LLM), in this case  \texttt{gpt-3.5-turbo-0613}, to judge which one of the two responses is better or if they tie, with 8-shot chain-of-thought (CoT) prompts shown in \S\ref{app:prompts} (see the ``comparison'' paragraph). Experts (the authors of this paper) also carefully annotate 200 comparisons. We find that LLM vs. expert example-based agreement is not high (see \S\ref{app:agreement} for details; see Table~\ref{tab:llm-results-main} for results), so we also conduct human annotation via crowdworkers. Nevertheless, as we will see, we do find general agreement between crowdworkers and LLM evaluation at the level of averaging over many examples.





\paragraph{Crowdworker evaluation of pairwise comparison.} We ask crowdworkers to decide which one of the two responses is better or if they tie. Each judgment is done by five crowdworkers.\footnote{The final answer is the majority vote. If there is no majority vote, then the final answer is ``(a) and (b) tie.''} We add 10\% catch questions (which are questions with known unambiguous answers) and if the crowdworker gets too many incorrect,\footnote{$>$20\% if the worker has done $\geq$ 5 annotations (of catch questions), $>$50\% if the worker has done $<$ 5 annotations.} then we discard all their ratings and relaunch the annotation jobs for the corresponding examples. % \footnote{If the two candidate generations are exactly the same, we automatically label the result as ``tie'' and do not include the annotation batch.} 
The crowdworker--expert agreement is much better than the LLM--expert agreement: \S\ref{app:agreement} shows that 86\% answers match, and 6.5\% strongly disagree.\footnote{Strongly disagree: crowdworkers choosing ``(a) better than (b)'' and experts choosing ``(b) better than (a),'' or vice versa. The ``tie'' annotations are not considered.} 

\paragraph{Evaluating properties of generated responses.}

We also investigate what behaviors (including potentially undesirable ones) the generated responses have. The properties are as follows. \textbf{Seek info}: whether the response is seeking information (e.g., ``tell me about the dune''); \textbf{off-topic}: whether the response is off-topic and irrelevant to the conversation; \textbf{controversial}: whether the response contains anything controversial; \textbf{insincere}: whether the response is insincere (being deceitful, not being genuine, not being serious about the conversation); \textbf{unfriendly}: whether the response is being unfriendly or confrontational toward the other speaker. 
We use \texttt{gpt-3.5-turbo-0613} (with 8-shot CoT prompting for most cases) to conduct evaluation. These questions are intuitively straightforward, and we observe that the LLM--expert evaluation outputs match $>$ 90\% of the time.\footnote{For each behavior, we first attempt zero-shot prompting. Automatic evaluation is done on 50 examples; experts annotate these 50 examples as well. If the LLM--expert agreement is $> 90\%$, then we use zero-shot prompting. Otherwise, we use 8-shot CoT prompting (4 positive and 4 negative examples -- in each in-context example we include the reasoning process before arriving at a final answer). We end up using 8-shot CoT prompting for all annotations except for ``controversial.''} See \S\ref{app:prompts} for the prompts used. 




\subsection{Results}


\paragraph{Overall results.}

Overall results are given in 
Table~\ref{tab:main}. Firstly, they show that our implicit feedback models achieve {better classifier scores} compared to the baseline. This observation is guaranteed given our sample-and-rerank algorithm, but we believe it is still instructive to see how large the gap is.

Annotators find that many of the implicit feedback signal choices outperform the baseline and the ``ranked by probability'' method (refer to \S\ref{app:additional-info-eval}). 
In particular, 
``length ($k$=20),'' %``length ($k$=5),'' 
``non-neg. sentiment \& length,'' and ``joy \& length'' are all better than the baseline (results are significant using Wilcoxon signed-rank test). % (results are significant using a one-sample $t$-test). 
For example, responses generated using the ``length ($k$=20)'' signal correspond to an 12-point lead (compared to the baseline responses) in terms of human preferences, and the ``joy \& length'' signal corresponds to a 9.5-point lead. 
%, even though the classifier accuracy is often low.\footnote{
We also observe that the LLM preferences follow roughly the same trend as the human annotators,
see Table~\ref{tab:llm-results-main}.
We provide further supporting results and analysis in \S\ref{app:additional-info-eval}.



\paragraph{Behavior analysis.}
%Undesirable behaviors.}

While we see several choices of implicit feedback metrics can improve overall performance, we observe both positive and negative consequences in terms of observed behavior depending on the implicit signal chosen. 
Evaluation of behavioral properties is provided in Table~\ref{tab:properties}.

Implicit signals that approximately optimize conversation length 
(``replied'', ``length ($k$=5)'', ``length ($k$=20)'') tend to increase the  amount of \textbf{controversial} and/or generations that are deemed \textbf{unfriendly}. 
In contrast, positive sentiment and joy optimizing signals (``sentiment \& length'', ``joy \& length'') tend to {\em decrease} 
both of these behaviors compared to the baseline. The ``replied'' signal produces the most controversial messages -- possibly to provoke the user into responding one more time.
``length ($k$=20)'' and ``replied'' signals  lead to a larger number of unfriendly generations, possibly by antagonizing the other speaker so they are too provoked to not respond. 
The ``joy \& length'' signal on the other hand halves the amount of controversial messages (from 17\% to 8.5\%) compared to the baseline, avoiding these types of messages.


We also observe that most implicit signals lead to an increased amount of  \textbf{information seeking}. 
Further, some signals, especially for ``replied'' and ``length ($k$=20),'' may go \textbf{off-topic} at a slightly higher rate than the baseline. In fact, for generations using signals ``length ($k$=5),'' ``non-neg. sentiment \& length'' and ``joy \& length,'' there is a higher rate in seeking off-topic information. A possible explanation is that the model could ask slightly irrelevant questions so as to keep the human user engaged.

 


\section{Discussion and Conclusion}



In summary, we find that optimizing for implicit feedback signals from human responses is effective, providing improved models over the baseline.
However, the choice of implicit signal to extract has important behavioral consequences. Conversation length-based signals tend to increase controversial and unfriendly messages, while sentiment or reaction-based signals tend to do the opposite, decreasing the frequency of this behavior compared to the baseline. 
%For controversial messages to some extent this is a system choice. 
%To asome extent this is a system choice.
While we provide no formal evaluation to support this, decreasing controversial messages potentially prevents the 
discussion of 
%bot and the human user from discussing 
serious matters, for example, sharing indignance on issues like social justice or discussing unfortunate everyday situations. On the other hand, encouragement of these messages increases the chance of upsetting or even harmful conversations.

We note, however, that if we discount generations that are off-topic, controversial, unfriendly, or insincere, and only evaluate on the rest of the examples, then the human annotation would prefer our implicit feedback models over the baseline even more (see 
\S\ref{app:additional-info-eval}).
%Appendix \autoref{app:additional-info-eval}. 
Hence, future work could try to extract signals towards that goal, or  consider additional safeguards or mitigations. % those scenarios. 

While we have used the sample-and-rerank  algorithm in our experiments, a natural extension is to use implicit signals in other learning approaches such as RL, where in that case it might be important to reduce reward gaming behaviors \citep{pang2022reward}. %Alternatively, one could investigate non-RL approaches that learn from binary feedback, such as Cringe loss which improves upon approaches like unlikelihood \citep{welleck2020neural} and Director \citep{arora-etal-2022-director}. 
It may also be important to study the use of implicit feedback signals in an iterative framework, whereby the new improved model is re-deployed and feedback recollected.
For example, we find many of the implicit feedback models we explored increase information-seeking messages, which is not always beneficial \cite{dinan2020second}. If those methods have overcompensated and now produce an excessive amount of such messages, redeployment can provide feedback to correct this and iteratively improve the model.




\if 
One efficiency-related limitation of the current approach is the sample-and-rerank decoding algorithm. The decoding algorithm is not the focus of our discussion, but to significantly improve the efficiency: given that our classifiers are far from perfect, we can either work on reducing reward gaming behaviors \citep{pang2022reward} or investigate non-RL approaches that learn from binary feedback. One option is Cringe loss which improves upon approaches like unlikelihood \citep{welleck2020neural} and Director \citep{arora-etal-2022-director}. 

\fi