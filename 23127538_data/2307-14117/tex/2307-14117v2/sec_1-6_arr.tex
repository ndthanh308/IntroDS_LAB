\section{Introduction}
\label{sec:intro}

A core strategy to improve social conversation models is through human feedback. There has been remarkable progress in learning from feedback, including reinforcement learning with human feedback \citep{stiennon2020learning,bai2022training}, where a large number of human annotations are needed to ensure a good reward function. For social conversation models, the feedback usually involves binary ratings \citep{xu2023improving}, numerical scores \citep{shalyminov-etal-2018-neural,hancock-etal-2019-learning}, ranking \citep{ghazarian-etal-2023-mercy}, or natural language comments of a dialogue turn or episode \citep{li2017dialogue,yuan2023system}. 
These signals are most often collected explicitly using crowdworkers, as organic users may not want to be burdened with providing explicit signals, or else may provide unreliable information \cite{ju2022learning}.

% Figure environment removed

In this work, we consider the setting where we have a large number of dialogue episodes of deployment-time dialogue which consist of natural conversations between the model and organic users. We want to see if we can obtain any implicit signal from these organic user conversations, and leverage these signals to improve the dialogue model. The rationale is two-fold. First, the organic users most closely approximate the data distribution
%user group in future dialogue model 
for future deployment; yet they may not  % want to %organically 
provide explicit annotations. %dense explicit annotations. 
Second, relying on these implicit signals does not incur extra cost that would otherwise be spent on crowdsourcing. More specifically, in this work we investigate the following: Can we improve the chatbot by optimizing for \textit{simple {implicit} feedback signals} like the number of, length, sentiment, or reaction of future human responses? In particular, we do not leverage any explicit annotation (e.g., binary or numerical rating, explicit natural language feedback, etc.).

We use publicly released de-identified data \citep{xu2023improving} from the BlenderBot online deployment 
\citep{shuster2022blenderbot}. Utilizing this data, we obtain sample-and-rerank models, comparing various implicit feedback signals.  
Through both automatic and human judgments, some of our new models 
%that leverage implicit signals
are preferable to baseline responses. Next, as our implicit feedback signals are coarse {proxy} metrics of the quality of bot turns, we ask whether encouraging these metrics would lead to undesirable behaviors. The answer is yes, depending on the chosen signal: in particular, optimizing for longer conversation lengths
can make the model produce controversial takes or respond in an unfriendly or confrontational way. Optimizing for positive reaction or sentiment on the other hand has {\em the opposite} effect, and decreases these behaviors compared to the baseline.
Overall, implicit feedback from humans is a useful training signal that can improve overall performance, but the precise signal used  
has important behavioral consequences.



\section{Related Work}

Researchers and practitioners have strived to build better neural open-domain dialogue models for years \citep{chen2017survey,gao2018neural,khatri2018advancing,xu2023improving}. 
DialoGPT~\citep{zhang-etal-2020-dialogpt} and BlenderBot~\citep{shuster2022blenderbot} have released the models as well as the training pipelines which have enabled follow-up dialogue projects from the community \citep{bang-etal-2021-assessing,adewumi2022smaaprat,lee-etal-2023-framework}. In particular, for BlenderBot, dialogue interaction data has been released \cite{xu2023improving,shi2022life}, which makes the study of implicit signals in our work possible. 

The class of training strategies that are most relevant to this work -- decoding utterances for future success -- is discussed in \citet{li2017learning}, in which they interpolate the MLE-trained token-level conditional probability with a value function that is trained to predict the property of a completed sequence (e.g., length, BLEU/ROUGE against the reference) given a partial sequence. This overall idea is extended in \citet{zemlyanskiy-sha-2018-aiming} where a chatbot learns to generate utterances that have the maximal information gain about the human in the future, as well as \citet{kulikov2019multi} that propose to generate the current bot utterance that leads to the most probable sequence of future utterances. \citet{irvine2023rewarding} use conversation engagement metrics (e.g., approximated by retry rate, % \footnote{A user can click ``retry'' in their software so that the bot re-generates the response.} 
manually-annotated engagement metrics) 
to optimize for engaging bot responses; in contrast, our work highlights both the strengths as well as the challenges of using implicit feedback, 
and in particular that conversation engagement metrics have negative consequences that can be alleviated through other choices of implicit signal.



\section{Approach}
\label{sec:approach}

\subsection{Implicit Feedback Signals}

Our goal is to extract learning signals from a large set of human-bot conversational episodes. 
Assume such a set has
already been collected.
A conversation episode is represented as ${\bf{x}} = (\vx_1^b, \vx_1^h, \vx_2^b, \vx_2^h, \dots)$ with $T$ utterances by the bot (denoted with superscript ``$b$''; bot is assumed to speak first) and $T'$ utterances by the human (denoted with ``$h$''). % \footnote{% Given that we assume that the bot initiates the dialogue, $T'$ equals either $T$ or $T-1$. 
% \footnote{For simplicity, we do not include the static context (e.g., memories of the other speaker's experiences) in the notation, but the context (if exists) is always included in the input.}  
Let ${\bf{x}}_{<t}$ denote the conversation history before bot's $t$-th turn: $\vx_1^b, \vx_1^h, \dots, \vx_{t-1}^b, \vx_{t-1}^h$. 
% \subsection{Signals} 

Next, we define the implicit feedback-derived scoring function $r_\phi(\vx_t^b, {\bf{x}}_{<t})$ that predicts the quality of the bot's $t$-th turn $\vx_t^b$ given past utterances. The input to $r_\phi$ is the first $t$ bot utterances and the first $t-1$ human utterances; 
the output is a real number in $[0,1]$ that scores $\vx_t^b$ according to one of the below criteria. Crucially, for the training data (but not for test data) we have access to the entire conversation ${\bf{x}}$ (with $T+T'$ utterances for a given episode). We can hence use future human turns to gather implicit feedback to judge the quality of $\vx_t^b$, which we hence use to define training labels $y(\vx_t^b)$ in order to learn
the scoring function $r_\phi$. 
We consider several candidate implicit signals, which we describe next -- these signals are \textit{coarse proxy} metrics of the quality of bot turns, and we aim to investigate the effect of optimizing them.

\textbf{Existence of next human turn.} Intuitively, if the human user quits the conversation after the bot's $t$-th turn $\vx_t^b$, then \textit{it is likely} that $\vx_t^b$ is 
of poor quality. % This signal is a special case ($k=0$) of the length-based signal.
Conversely, if humans continue to converse, and do not quit, this prolonged 
engagement can be seen as a proxy for satisfaction \cite{o2008user,see-manning-2021-understanding}.  Therefore, we set the reference label $y(\vx_t^b)$ for training $r_\phi(\vx_t^b, {\bf{x}}_{<t})$ to 1 if the next human turn exists, and 0 otherwise. 
We use ``replied'' to represent this signal in later sections. 

\textbf{Next human turn length.} 
If a human is unwilling to invest time into the conversation, their responses may be shorter. Given the crude intuition that a long human turn \textit{likely} implies that the previous bot turn is good, let $y(\vx_t^b)$   be 1 if the next human turn has $\geq k$ words ($k$ is a hyperparameter); 0 otherwise. Granted, the intuition is not always true in practice (e.g., a human response could be a tirade against previous bot turns); we only use the signals in this section as \textit{coarse proxy} metrics of bot turn's quality. We use ``length'' to represent this signal. 

In the same vein, we have also attempted to leverage the \textbf{number of words in all future human utterances} or \textbf{number of future human turns} -- we leave this discussion to \S\ref{app:other-signals} as we are not able to train an effective scoring function.

\textbf{Sentiment in the next human utterance.} 
We use a recent positive/neutral/negative sentiment model trained on tweets \citep{camacho-collados-etal-2022-tweetnlp}. % \footnote{\texttt{cardiffnlp/twitter-roberta-base-sentiment-latest} on Hugging Face.} 
Intuitively, we want humans to react positively in future responses. For sentiment and reaction signals, we find that the classifiers struggle at classifying very short utterances. At the same time, very short human responses likely mean that humans are unwilling to meaningfully engage. % in the conversation. 
We thus experiment with two options: (1) Set reference label  $y(\vx_t^b)$  to 1 if sentiment of $\vx_t^h$ is positive or neutral, and length is $\geq$ 5 words; 0 otherwise. (2) Set reference label to 1 if sentiment is positive and length is $\geq$ 5 words; 0 otherwise. 

\textbf{Reaction in the next human utterance.} We use an existing model \citep{hartmann2022emotionenglish} 
% finetuned from DistilRoBERTa \citep{sanh2019distilbert}; % \footnote{\texttt{j-hartmann/emotion-english-distilroberta-base} on Hugging Face.} 
with output categories: anger, disgust, fear, joy, neutral, sadness, and surprise. 
Similar to the previous paragraph, we train a classifier that predicts whether the human next turn would have the ``joy'' reaction and $\geq$ 5 words at the same time.\footnote{We also attempted the following: the classifier predicts whether the human next turn's top predicted reaction is anger/disgust or non-anger/disgust, but we find that this feature cannot be well-predicted (dev accuracy $\sim$55\%).} 
Let  $y(\vx_t^b) = 1$ if the reaction of $\vx_t^h$ is joy and length is $\geq$ 5 words; 0 otherwise. This signal is denoted by ``joy \& length.''



\subsection{Models Using Implicit Signals} \label{sec:implicit_generation}

We use the sample-and-rerank approach, which has been shown to perform similarly (albeit with a larger inference cost which is not the focus of our discussion) as RL-based approaches in machine translation \citep{pang-etal-2022-amortized} and learning from pairwise feedback in language modeling \citep{dubois2023alpacafarm}. Given a conversation history, first, sample 20 candidate responses. We use factual-top-$p$ sampling \citep{lee2022factuality} given that \citet{shuster2022blenderbot} have shown that it achieves a good balance between generation diversity and factuality for social conversations.\footnote{The high level idea is that in factual top-$p$ sampling, $p$ varies by time-step $t$ which leads to more factual generations.} 
Next, rerank these generations using a reranker model, i.e., the classifier $r_\phi$ trained using the deployment data with implicit feedback labels $y$. We then pick the candidate generation with the highest reranker score. 

\input{table_merged}



\section{Experiments and Results}

\subsection{Experimental Setup}

We base our experiments off the publicly released BlenderBot deployment data \citep{xu2023improving} in order to build implicit feedback models. The dataset used in this work contains 3.1M bot utterances and 3.1M human utterances collected from August 2022
to January 2023.  The classifiers (i.e., rerankers) are based on a pretrained RoBERTa-large. 
Our baseline is the publicly released BlenderBot model (\texttt{r2c2\_blenderbot\_3B}) with around 3B parameters, pretrained on dialogue and language modeling tasks, and fine-tuned on dialogue tasks \citep{shuster-etal-2022-language}. % The model has 2 encoder layers, 24 decoder layers, 32 attention heads, FFN size 10240, and embedding size 2560. 
We also report results for the method ``ranked by probability:'' we simply rerank using the sequence-level probabilities during sample-and-rerank; we want to see whether our approaches based on the implicit feedback classifiers outperform using this naive ranking criterion. 



\subsection{Evaluation Methods}
\label{sec:eval-methods}

Given a conversation history and two candidate responses (baseline and new model responses), we ask a large language model (LLM), in this case \texttt{gpt-3.5-turbo-0613}, to judge which one of the two responses is better or if they tie, with 8-shot chain-of-thought (CoT) prompts. 
Experts (authors of this paper) also carefully annotate 200 comparisons with example order and response order randomized. We find that LLM vs. expert example-based agreement is not high; see \S\ref{app:agreement} for more details -- the LLM does not excel on our evaluation task, despite existing work showing superior LLM annotation performance on certain other tasks \citep[e.g., ][]{gilardi2023chatgpt}. 
%(see \S\ref{app:agreement} for agreement details; see \autoref{tab:main-app} for results). 

Therefore, we conduct human annotation via crowdworkers, using majority vote over 5 workers per comparison,\footnote{The final answer is the majority vote. If there is no majority vote (e.g., if five votes are ``(a) wins,'' ``(a) wins,'' ``(b) wins,'' ``tie,'' ``tie''), then the final answer is ``(a) and (b) tie.''} with 10\% catch questions with known unambiguous answers to filter for quality. We find that the human annotation vs. expert agreement is much higher than LLM vs. expert. But we do find general agreement between crowdworkers and LLM evaluation at the level of averaging over many examples.  See \S\ref{app:additional-info-eval} for more details on human annotation and comparison with LLMs.

\paragraph{Behaviors of generated responses.}

We also investigate what behaviors (including potentially undesirable ones) the generations have. The properties are as follows. \textbf{Seek info}: whether the response is seeking information (e.g., ``tell me about the dune''); \textbf{off-topic}: whether the response is off-topic and irrelevant to the conversation; \textbf{controversial}: whether the response contains anything controversial; \textbf{insincere}: whether the response is insincere (being deceitful, not being genuine, not being serious about the conversation); \textbf{unfriendly}: whether the response is being unfriendly or confrontational toward the other speaker. 
We use \texttt{gpt-3.5-turbo-0613} (with 8-shot CoT prompts shown in \S\ref{app:prompts}) to conduct this behavior evaluation. 
These questions are intuitively straightforward (compared to the pairwise comparison task described at the beginning of this section), and we observe that the LLM--expert evaluation outputs match $>$90\% of the time. 



\subsection{Results}

\paragraph{Overall results.}

Overall results are given in 
\autoref{tab:main}. 
\if 0
Firstly, they show that our implicit feedback models achieve {better classifier scores} compared to the baseline. This observation is guaranteed given our sample-and-rerank algorithm, but we believe it is still instructive to see how large the gap is. 
\fi 
Annotators find that several of the implicit feedback signals outperform the baseline and the ``ranked by probability'' method (more in \S\ref{app:additional-info-eval}). 
In particular, 
``length ($k$=20),'' %``length ($k$=5),'' 
``non-neg. sentiment \& length,'' and ``joy \& length'' are all significantly better than the baseline using Wilcoxon signed-rank test. 
For example, responses generated using the ``length ($k$=20)'' signal correspond to a 12-point lead compared to the baseline responses, 
and the ``joy \& length'' signal corresponds to an 9.5-point lead. 
We also find that LLM-based evaluation  follows roughly the same trend as human annotators;  
see further supporting results in \S\ref{app:additional-info-eval}.
%(\autoref{tab:main-app}).
%We provide further supporting results and analysis in %\S\ref{app:additional-info-eval}.

\paragraph{Behavior analysis.}

While several choices of implicit feedback  improve overall performance, we observe both positive and negative consequences in terms of observed behavior depending on the implicit signal chosen (\autoref{tab:main} columns 4--9).

Implicit signals that approximately optimize conversation length 
(``replied,'' ``length ($k$=5),'' ``length ($k$=20)'') tend to increase the  amount of \textbf{controversial} and/or generations that are deemed \textbf{unfriendly}. 
In contrast, positive sentiment and joy optimizing signals (``sentiment \& length,'' ``joy \& length'') tend to {\em decrease} 
both of these behaviors compared to the baseline. The ``replied'' signal produces the most controversial messages -- possibly to provoke the user into responding one more time.
The ``length ($k$=20)'' and ``replied'' signals  lead to a larger number of unfriendly generations, possibly by antagonizing the other speaker so they are too provoked to not respond. 
The ``joy \& length'' signal on the other hand halves the amount of controversial messages (from 17\% to 8.5\%) compared to the baseline, avoiding these types of messages.

We also observe that most implicit signals lead to an increased amount of  \textbf{information seeking}. 
Further, some signals, especially for ``replied'' and ``length ($k$=20),'' may go \textbf{off-topic} at a slightly higher rate than the baseline. For generations using signals ``length ($k$=5)'' and ``non-neg. sentiment \& length,'' there is a much higher rate in seeking off-topic information; a possible explanation is that the model could ask slightly irrelevant questions so as to keep the human user engaged.



\section{Conclusion}

In summary, we find that optimizing for certain implicit feedback signals from human responses is effective, providing improved models over the baseline.
However, the choice of implicit signal to extract has important behavioral consequences. Conversation length-based signals tend to increase controversial and unfriendly messages, while sentiment or reaction-based signals tend to do the opposite, decreasing the frequency of this behavior compared to the baseline. 

We note, however, that if we discount generations that are off-topic, controversial, unfriendly, or insincere, and only evaluate on the rest of the examples, then the human annotation would prefer our implicit feedback models over the baseline even more (see the end of \S\ref{app:additional-info-table}). 
Hence, future work could try to extract signals towards that goal (of optimizing toward implicit signals while reducing the amount of undesirable generations), or consider additional safeguards or mitigations while optimizing toward implicit signals.



\section*{Limitations}

While we provide no formal evaluation, decreasing controversial messages potentially prevents the 
discussion of serious matters, for example, sharing indignance on issues involving social justice or discussing unfortunate everyday situations. On the other hand, encouragement of these messages increases the chance of upsetting conversations or even harmful conversations.

Algorithm-wise, while we have used the sample-and-rerank in our experiments, a natural extension which we did not explore in this project is to use implicit signals in other learning approaches such as RL. To use RL, we may need strategies to reduce reward gaming behaviors in text generation \citep{skalse2022defining,pang2022reward} given that our classifiers are imperfect. Alternatively, one could investigate non-RL approaches that learn from preference signals, such as Cringe loss \citep{adolphs-etal-2023-cringe}, direct preference optimization \citep{rafailov2023direct}, and their variants \citep{yuan2024self}. 
Another future direction which we did not explore in this project is to study the use of implicit feedback signals in an iterative framework, whereby the new improved model is re-deployed and feedback recollected.
For example, we find many of the implicit feedback models we explored increase information-seeking messages, which is not always beneficial \cite{dinan2020second}. If those methods have overcompensated and now produce an excessive amount of such messages, redeployment can provide feedback to correct this and \textit{iteratively} improve the model.



\section*{Acknowledgement}

We thank Jing Xu, Da Ju, Mojtaba Komeili, Vishakh Padmakumar, Nitish Joshi, and Leshem Choshen for valuable
discussion. The work is undertaken as part of the
Metaâ€“NYU mentorship program.