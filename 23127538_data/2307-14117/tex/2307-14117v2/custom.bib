%% A

@inproceedings{adewumi2022smaaprat,
  title={Sm{\aa}prat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning},
  author={Adewumi, Oluwatosin and Br{\"a}nnvall, Rickard and Abid, Nosheen and Pahlavan, Maryam and Sabah Sabry, Sana and Liwicki, Foteini and Liwicki, Marcus},
  booktitle={5th Northern Lights Deep Learning Conference (NLDL), Troms{\o}, Norway, January 10-12, 2022},
  volume={3},
  year={2022},
  organization={Septentrio Academic Publishing}
}

@article{adiwardana2020towards,
  title={Towards a human-like open-domain chatbot},
  author={Adiwardana, Daniel and Luong, Minh-Thang and So, David R and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and others},
  journal={arXiv preprint arXiv:2001.09977},
  year={2020}
}

@inproceedings{adolphs-etal-2023-cringe,
    title = "The {CRINGE} Loss: Learning what language not to model",
    author = "Adolphs, Leonard  and
      Gao, Tianyu  and
      Xu, Jing  and
      Shuster, Kurt  and
      Sukhbaatar, Sainbayar  and
      Weston, Jason",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.493",
    doi = "10.18653/v1/2023.acl-long.493",
    pages = "8854--8874",
}

@inproceedings{arora-etal-2022-director,
    title = "Director: Generator-Classifiers For Supervised Language Modeling",
    author = "Arora, Kushal  and
      Shuster, Kurt  and
      Sukhbaatar, Sainbayar  and
      Weston, Jason",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.39",
    pages = "512--526",
}


%% B

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


%% C

@inproceedings{camacho-collados-etal-2022-tweetnlp,
    title={{T}weet{NLP}: {C}utting-{E}dge {N}atural {L}anguage {P}rocessing for {S}ocial {M}edia},
    author={Camacho-Collados, Jose and Rezaee, Kiamehr and Riahi, Talayeh and Ushio, Asahi and Loureiro, Daniel and Antypas, Dimosthenis and Boisson, Joanne and Espinosa-Anke, Luis and Liu, Fangyu and Mart{\'\i}nez-C{\'a}mara, Eugenio and others},
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2022",
    address = "Abu Dhabi, U.A.E.",
    publisher = "Association for Computational Linguistics",
}

@article{chen2017survey,
  title={A survey on dialogue systems: Recent advances and new frontiers},
  author={Chen, Hongshen and Liu, Xiaorui and Yin, Dawei and Tang, Jiliang},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={19},
  number={2},
  pages={25--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{
  choshen2020weakness,
  title={On the Weaknesses of Reinforcement Learning for Neural Machine Translation},
  author={Leshem Choshen and Lior Fox and Zohar Aizenbud and Omri Abend},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=H1eCw3EKvH}
}


%% D

@inproceedings{dinan2020second,
  title={The second conversational intelligence challenge ({ConvAI2})},
  author={Dinan, Emily and Logacheva, Varvara and Malykh, Valentin and Miller, Alexander and Shuster, Kurt and Urbanek, Jack and Kiela, Douwe and Szlam, Arthur and Serban, Iulian and Lowe, Ryan and others},
  booktitle={The NeurIPS'18 Competition: From Machine Learning to Intelligent Conversations},
  pages={187--208},
  year={2020},
  organization={Springer}
}

@inproceedings{
    dubois2023alpacafarm,
    title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback},
    author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori Hashimoto},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=4hturzLcKX}
}


%% G

@inproceedings{gao2018neural,
  title={Neural approaches to conversational AI},
  author={Gao, Jianfeng and Galley, Michel and Li, Lihong},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={1371--1374},
  year={2018}
}

@inproceedings{ghazarian-etal-2023-mercy,
    title = "{MERCY}: Multiple Response Ranking Concurrently in Realistic Open-Domain Conversational Systems",
    author = "Ghazarian, Sarik  and
      Hedayatnia, Behnam  and
      Jin, Di  and
      Liu, Sijia  and
      Peng, Nanyun  and
      Liu, Yang  and
      Hakkani-Tur, Dilek",
    editor = "Stoyanchev, Svetlana  and
      Joty, Shafiq  and
      Schlangen, David  and
      Dusek, Ondrej  and
      Kennington, Casey  and
      Alikhani, Malihe",
    booktitle = "Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigdial-1.58",
    doi = "10.18653/v1/2023.sigdial-1.58",
    pages = "615--631",
    abstract = "Automatic Evaluation (AE) and Response Selection (RS) models assign quality scores to various candidate responses and rank them in conversational setups. Prior response ranking research compares various models{'} performance on synthetically generated test sets. In this work, we investigate the performance of model-based reference-free AE and RS models on our constructed response ranking datasets that mirror real-case scenarios of ranking candidates during inference time. Metrics{'} unsatisfying performance can be interpreted as their low generalizability over more pragmatic conversational domains such as human-chatbot dialogs. To alleviate this issue we propose a novel RS model called MERCY that simulates human behavior in selecting the best candidate by taking into account distinct candidates concurrently and learns to rank them. In addition, MERCY leverages natural language feedback as another component to help the ranking task by explaining why each candidate response is relevant/irrelevant to the dialog context. These feedbacks are generated by prompting large language models in a few-shot setup. Our experiments show the better performance of MERCY over baselines for the response ranking task in our curated realistic datasets.",
}

@article{gilardi2023chatgpt,
  title={{ChatGPT} outperforms crowd-workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={arXiv preprint arXiv:2303.15056},
  year={2023}
}

@article{guo2018topic,
  title={Topic-based evaluation for conversational bots},
  author={Guo, Fenfei and Metallinou, Angeliki and Khatri, Chandra and Raju, Anirudh and Venkatesh, Anu and Ram, Ashwin},
  journal={arXiv preprint arXiv:1801.03622},
  year={2018}
}


%% H 

@misc{hartmann2022emotionenglish,
  author={Hartmann, Jochen},
  year={2022},
  howpublished = {Model accessible via \url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base}},
}


%% I

@article{irvine2023rewarding,
  title={Rewarding Chatbots for Real-World Engagement with Millions of Users},
  author={Robert P. Irvine and Douglas Boubert and Vyas Raina and Adian Liusie and Vineet Mudupalli and Aliaksei Korshuk and Zongyi Joe Liu and Fritz Cremer and Valentin Assassi and Christie-Carol Beauchamp and Xiaoding Lu and Thomas Rialan and William Beauchamp},
  journal={arXiv preprint arXiv:2303.06135},
  year={2023}
}


%% J

@article{ju2022learning,
  title={Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls},
  author={Ju, Da and Xu, Jing and Boureau, Y-Lan and Weston, Jason},
  journal={arXiv preprint arXiv:2208.03295},
  year={2022}
}


%% K 

@article{khatri2018advancing,
  title={Advancing the state of the art in open domain dialog systems through the {Alexa} prize},
  author={Khatri, Chandra and Hedayatnia, Behnam and Venkatesh, Anu and Nunn, Jeff and Pan, Yi and Liu, Qing and Song, Han and Gottardi, Anna and Kwatra, Sanjeev and Pancholi, Sanju and others},
  journal={arXiv preprint arXiv:1812.10757},
  year={2018}
}


@InProceedings{korbak2022controlling,
  title = 	 {Controlling Conditional Language Models without Catastrophic Forgetting},
  author =       {Korbak, Tomasz and Elsahar, Hady and Kruszewski, German and Dymetman, Marc},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {11499--11528},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/korbak22a/korbak22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/korbak22a.html},
  abstract = 	 {Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities ("catastrophic forgetting"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and — in contrast with baseline approaches — does not result in catastrophic forgetting.}
}

@article{kulikov2019multi,
  title={Multi-turn beam search for neural dialogue modeling},
  author={Kulikov, Ilia and Lee, Jason and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1906.00141},
  year={2019}
}

%% L

@inproceedings{
    lee2022factuality,
    title={Factuality Enhanced Language Models for Open-Ended Text Generation},
    author={Nayeon Lee and Wei Ping and Peng Xu and Mostofa Patwary and Pascale Fung and Mohammad Shoeybi and Bryan Catanzaro},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=LvyJX20Rll}
}

@inproceedings{lee-etal-2023-framework,
    title = "A Framework for Vision-Language Warm-up Tasks in Multimodal Dialogue Models",
    author = "Lee, Jaewook  and
      Park, Seongsik  and
      Park, Seong-Heum  and
      Kim, Hongjin  and
      Kim, Harksoo",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.167",
    pages = "2789--2799",
    abstract = "Most research on multimodal open-domain dialogue agents has focused on pretraining and multi-task learning using additional rich datasets beyond a given target dataset. However, methods for exploiting these additional datasets can be quite limited in real-world settings, creating a need for more efficient methods for constructing agents based solely on the target dataset. To address these issues, we present a new learning strategy called vision-language warm-up tasks for multimodal dialogue models (VLAW-MDM). This strategy does not require the use of large pretraining or multi-task datasets but rather relies solely on learning from target data. Moreover, our proposed approach automatically generate captions for images and incorporate them into the model{'}s input to improve the contextualization of visual information. Using this novel approach, we empirically demonstrate that our learning strategy is effective for limited data and relatively small models. The result show that our method achieved comparable and in some cases superior performance compared to existing state-of-the-art models on various evaluation metrics.",
}

@inproceedings{
li2017dialogue,
title={Dialogue Learning With Human-in-the-Loop},
author={Jiwei Li and Alexander H. Miller and Sumit Chopra and Marc'Aurelio Ranzato and Jason Weston},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HJgXCV9xx}
}

@article{li2017learning,
  title={Learning to decode for future success},
  author={Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1701.06549},
  year={2017}
}


%% N

@article{ni2022recent,
  title={Recent advances in deep learning based dialogue systems: A systematic survey},
  author={Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and Cambria, Erik},
  journal={Artificial intelligence review},
  pages={1--101},
  year={2022},
  publisher={Springer}
}


%% P

@inproceedings{
    pan2022the,
    title={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models},
    author={Alexander Pan and Kush Bhatia and Jacob Steinhardt},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=JYtwGwIL7ye}
}

@inproceedings{pang-etal-2022-amortized,
    title = "Amortized Noisy Channel Neural Machine Translation",
    author = "Pang, Richard Yuanzhe  and
      He, He  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 15th International Conference on Natural Language Generation",
    month = jul,
    year = "2022",
    address = "Waterville, Maine, USA and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.inlg-main.11",
    pages = "131--143",
    abstract = "",
}

@inproceedings{pang2022reward,
    title = "Reward Gaming in Conditional Text Generation",
    author = "Pang, Richard Yuanzhe  and
      Padmakumar, Vishakh  and
      Sellam, Thibault  and
      Parikh, Ankur  and
      He, He",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.262",
    pages = "4746--4763",
    abstract = "To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.",
}


@article{parmar2022health,
  title={Health-focused conversational agents in person-centered care: a review of apps},
  author={Parmar, Pritika and Ryu, Jina and Pandya, Shivani and Sedoc, Jo{\~a}o and Agarwal, Smisha},
  journal={NPJ Digital Medicine},
  volume={5},
  number={1},
  pages={21},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

%% S

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{shi2022life,
  title={When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels},
  author={Shi, Weiyan and Dinan, Emily and Shuster, Kurt and Weston, Jason and Xu, Jing},
  journal={arXiv preprint arXiv:2210.15893},
  year={2022}
}

@article{shum2018eliza,
  title={From {Eliza} to {Xiaoice}: challenges and opportunities with social chatbots},
  author={Shum, Heung-Yeung and He, Xiao-dong and Li, Di},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={19},
  pages={10--26},
  year={2018},
  publisher={Springer}
}


@article{shuster2022blenderbot,
  title={Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage},
  author={Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and others},
  journal={arXiv preprint arXiv:2208.03188},
  year={2022}
}

@inproceedings{shuster-etal-2022-language,
    title = "Language Models that Seek for Knowledge: Modular Search {\&} Generation for Dialogue and Prompt Completion",
    author = "Shuster, Kurt  and
      Komeili, Mojtaba  and
      Adolphs, Leonard  and
      Roller, Stephen  and
      Szlam, Arthur  and
      Weston, Jason",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.27",
    doi = "10.18653/v1/2022.findings-emnlp.27",
    pages = "373--393",
}

@article{skalse2022defining,
  title={Defining and characterizing reward gaming},
  author={Skalse, Joar and Howe, Nikolaus and Krasheninnikov, Dmitrii and Krueger, David},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9460--9471},
  year={2022}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}


%% T

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

%% V

@article{veselovsky2023artificial,
  title={Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks},
  author={Veselovsky, Veniamin and Ribeiro, Manoel Horta and West, Robert},
  journal={arXiv preprint arXiv:2306.07899},
  year={2023}
}


%% X

@article{xu2023improving,
  title={Improving Open Language Models by Learning from Organic Interactions},
  author={Xu, Jing and Ju, Da and Lane, Joshua and Komeili, Mojtaba and Smith, Eric Michael and Ung, Megan and Behrooz, Morteza and Ngan, William and Moritz, Rashel and Sukhbaatar, Sainbayar and others},
  journal={arXiv preprint arXiv:2306.04707},
  year={2023}
}

@article{o2008user,
  title={What is user engagement? {A} conceptual framework for defining user engagement with technology},
  author={O'Brien, Heather L and Toms, Elaine G},
  journal={Journal of the American society for Information Science and Technology},
  volume={59},
  number={6},
  pages={938--955},
  year={2008},
  publisher={Wiley Online Library}
}

%% W

@inproceedings{
    welleck2020neural,
    title={Neural Text Generation With Unlikelihood Training},
    author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SJeYe0NtvH}
}

@incollection{wilcoxon1992individual,
  title={Individual comparisons by ranking methods},
  author={Wilcoxon, Frank},
  booktitle={Breakthroughs in Statistics: Methodology and Distribution},
  pages={196--202},
  year={1992},
  publisher={Springer}
}

%% Y

@article{yuan2023system,
  title={System-Level Natural Language Feedback},
  author={Yuan, Weizhe and Cho, Kyunghyun and Weston, Jason},
  journal={arXiv preprint arXiv:2306.13588},
  year={2023}
}



@inproceedings{
    rafailov2023direct,
    title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
    author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=HPuSIXJaa9}
}

@article{yuan2024self,
  title={Self-Rewarding Language Models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}