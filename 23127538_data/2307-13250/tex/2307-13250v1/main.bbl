% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{8970556}
W.~Zhu, X.~Wang, and W.~Gao, ``Multimedia intelligence: When multimedia meets
  artificial intelligence,'' \emph{IEEE Transactions on Multimedia}, vol.~22,
  no.~7, pp. 1823--1835, 2020.

\bibitem{10017364}
X.~Yang, F.~Liu, and G.~Lin, ``Effective end-to-end vision language pretraining
  with semantic visual loss,'' \emph{IEEE Transactions on Multimedia}, pp.
  1--10, 2023.

\bibitem{8768045}
J.~Li, Y.~Wong, Q.~Zhao, and M.~S. Kankanhalli, ``Video storytelling: Textual
  summaries for events,'' \emph{IEEE Transactions on Multimedia}, vol.~22,
  no.~2, pp. 554--565, 2020.

\bibitem{8811730}
W.~Zhang, S.~Tang, Y.~Cao, S.~Pu, F.~Wu, and Y.~Zhuang, ``Frame augmented
  alternating attention network for video question answering,'' \emph{IEEE
  Transactions on Multimedia}, vol.~22, no.~4, pp. 1032--1041, 2020.

\bibitem{9465732}
X.~Zhang, F.~Zhang, and C.~Xu, ``Explicit cross-modal representation learning
  for visual commonsense reasoning,'' \emph{IEEE Transactions on Multimedia},
  vol.~24, pp. 2986--2997, 2022.

\bibitem{hcrn}
T.~M. Le, V.~Le, S.~Venkatesh, and T.~Tran, ``Hierarchical conditional relation
  networks for video question answering,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2020.

\bibitem{jiang2020r}
P.~Jiang and Y.~Han, ``Reasoning with heterogeneous graph alignment for video
  question answering,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence (AAAI)}, 2020.

\bibitem{park2021b}
J.~Park, J.~Lee, and K.~Sohn, ``Bridge to answer: Structure-aware graph
  interaction network for video question answering,'' in \emph{Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  2021.

\bibitem{lgcn}
D.~Huang, P.~Chen, R.~Zeng, Q.~Du, M.~Tan, and C.~Gan, ``Location-aware graph
  convolutional networks for video question answering,'' in \emph{Proceedings
  of the AAAI Conference on Artificial Intelligence}, 2020.

\bibitem{gu2021graph}
M.~Gu, Z.~Zhao, W.~Jin, R.~Hong, and F.~Wu, ``Graph-based multi-interaction
  network for video question answering,'' \emph{IEEE Transactions on Image
  Processing}, vol.~30, pp. 2758--2770, 2021.

\bibitem{seo2021masn}
A.~Seo, G.-C. Kang, J.~Park, and B.-T. Zhang, ``Attend what you need:
  Motion-appearance synergistic networks for video question answering,'' in
  \emph{ACL}, 2021, pp. 6167--6177.

\bibitem{li2022fine}
J.~Li, X.~He, L.~Wei, L.~Qian, L.~Zhu, L.~Xie, Y.~Zhuang, Q.~Tian, and S.~Tang,
  ``Fine-grained semantically aligned vision-language pre-training,''
  \emph{arXiv preprint arXiv:2208.02515}, 2022.

\bibitem{zhang2021DVC}
Z.~Zhang, D.~Xu, W.~Ouyang, and L.~Zhou, ``Dense video captioning using
  graph-based sentence summarization,'' \emph{IEEE Transactions on Multimedia},
  vol.~23, pp. 1799--1810, 2021.

\bibitem{fan2020recurrent}
H.~Fan, L.~Zhu, Y.~Yang, and F.~Wu, ``Recurrent attention network with
  reinforced generator for visual dialog,'' \emph{ACM Transactions on
  Multimedia Computing, Communications, and Applications (TOMM)}, vol.~16,
  no.~3, pp. 1--16, 2020.

\bibitem{zhu2020actbert}
L.~Zhu and Y.~Yang, ``Actbert: Learning global-local video-text
  representations,'' in \emph{Proceedings of the IEEE/CVF conference on
  computer vision and pattern recognition}, 2020, pp. 8746--8755.

\bibitem{wang2021t2vlad}
X.~Wang, L.~Zhu, and Y.~Yang, ``T2vlad: global-local sequence alignment for
  text-video retrieval,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2021, pp. 5079--5088.

\bibitem{zhao2022centerclip}
S.~Zhao, L.~Zhu, X.~Wang, and Y.~Yang, ``Centerclip: Token clustering for
  efficient text-video retrieval,'' \emph{arXiv preprint arXiv:2205.00823},
  2022.

\bibitem{zhu2017uncovering}
L.~Zhu, Z.~Xu, Y.~Yang, and A.~G. Hauptmann, ``Uncovering the temporal context
  for video question answering,'' \emph{International Journal of Computer
  Vision}, vol. 124, no.~3, pp. 409--421, 2017.

\bibitem{DBLP:journals/tip/YangWDDWC22}
X.~Yang, S.~Wang, J.~Dong, J.~Dong, M.~Wang, and T.~Chua, ``Video moment
  retrieval with cross-modal neural architecture search,'' \emph{{IEEE} Trans.
  Image Process.}, vol.~31, pp. 1204--1216, 2022.

\bibitem{zhong2022video}
Y.~Zhong, W.~Ji, J.~Xiao, Y.~Li, W.~Deng, and T.-S. Chua, ``Video question
  answering: datasets, algorithms and challenges,'' \emph{arXiv preprint
  arXiv:2203.01225}, 2022.

\bibitem{yang2021just}
A.~Yang, A.~Miech, J.~Sivic, I.~Laptev, and C.~Schmid, ``Just ask: Learning to
  answer questions from millions of narrated videos,'' in \emph{Proceedings of
  the IEEE/CVF International Conference on Computer Vision}, 2021, pp.
  1686--1697.

\bibitem{zellers2021merlot}
R.~Zellers, X.~Lu, J.~Hessel, Y.~Yu, J.~S. Park, J.~Cao, A.~Farhadi, and
  Y.~Choi, ``Merlot: Multimodal neural script knowledge models,''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, pp.
  23\,634--23\,651, 2021.

\bibitem{yu2021learning}
W.~Yu, H.~Zheng, M.~Li, L.~Ji, L.~Wu, N.~Xiao, and N.~Duan, ``Learning from
  inside: Self-driven siamese sampling and reasoning for video question
  answering,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 26\,462--26\,474, 2021.

\bibitem{Li_2022_CVPR}
D.~Li, J.~Li, H.~Li, J.~C. Niebles, and S.~C. Hoi, ``Align and prompt:
  Video-and-language pre-training with entity prompts,'' in \emph{Proceedings
  of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)}, June 2022, pp. 4953--4963.

\bibitem{miech2019howto100m}
A.~Miech, D.~Zhukov, J.-B. Alayrac, M.~Tapaswi, I.~Laptev, and J.~Sivic,
  ``Howto100m: Learning a text-video embedding by watching hundred million
  narrated video clips,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2019, pp. 2630--2640.

\bibitem{tvqa}
J.~Lei, L.~Yu, M.~Bansal, and T.~L. Berg, ``Tvqa: Localized, compositional
  video question answering,'' \emph{Conference on Empirical Methods in Natural
  Language Processing}, 2018.

\bibitem{li19}
X.~Li, L.~Gao, X.~Wang, W.~Liu, X.~Xu, H.~T. Shen, and J.~Song, ``Learnable
  aggregating net with diversity learning for video question answering,''
  \emph{ACMMM}, pp. 1166--1174, 2019.

\bibitem{jin19}
W.~Jin, Z.~Zhao, M.~Gu, J.~Yu, J.~Xiao, and Y.~Zhuang, ``Multi-interaction
  network with object relation for video question answering,'' \emph{ACMMM},
  pp. 1193--1201, 2019.

\bibitem{comem}
J.~Gao, R.~Ge, K.~Chen, and R.~Nevatia, ``Motion-appearance co-memory networks
  for video question answering,'' in \emph{Proceedings of the IEEE Conference
  on Computer Vision and Pattern Recognition}, 2018.

\bibitem{zhao18}
Z.~Zhao, X.~Jiang, D.~Cai, J.~Xiao, X.~He, and S.~Pu, ``Multi-turn video
  question answering via multi-stream hierarchical attention context network.''
  in \emph{IJCAI}, vol. 2018, 2018, p. 27th.

\bibitem{kim18}
K.-M. Kim, S.-H. Choi, J.-H. Kim, and B.-T. Zhang, ``Multimodal dual attention
  memory for video story question answering,'' in \emph{Proceedings of the
  European Conference on Computer Vision (ECCV)}, 2018.

\bibitem{msvd-qa}
D.~Xu, Z.~Zhao, J.~Xiao, F.~Wu, H.~Zhang, X.~He, and Y.~Zhuang, ``Video
  question answering via gradually refined attention over appearance and
  motion,'' in \emph{Proceedings of the 25th ACM international conference on
  Multimedia}, 2017.

\bibitem{li2022compositional}
J.~Li, J.~Xie, L.~Qian, L.~Zhu, S.~Tang, F.~Wu, Y.~Yang, Y.~Zhuang, and X.~E.
  Wang, ``Compositional temporal grounding with structured variational
  cross-graph correspondence learning,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2022, pp. 3032--3041.

\bibitem{nan2021interventional}
G.~Nan, R.~Qiao, Y.~Xiao, J.~Liu, S.~Leng, H.~Zhang, and W.~Lu,
  ``Interventional video grounding with dual contrastive learning,'' in
  \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern
  recognition}, 2021, pp. 2765--2775.

\bibitem{li2021adaptive}
J.~Li, S.~Tang, L.~Zhu, H.~Shi, X.~Huang, F.~Wu, Y.~Yang, and Y.~Zhuang,
  ``Adaptive hierarchical graph reasoning with semantic coherence for
  video-and-language inference,'' in \emph{Proceedings of the IEEE/CVF
  International Conference on Computer Vision}, 2021, pp. 1867--1877.

\bibitem{da2021hrac}
L.~H. Dang, T.~M. Le, V.~Le, and T.~Tran, ``Hierarchical object-oriented
  spatio-temporal reasoning for video question answering,'' in
  \emph{Proceedings of the 30th International Joint Conference on Artificial
  Intelligence (IJCAI)}, August 2021.

\bibitem{pennington2014glove}
J.~Pennington, R.~Socher, and C.~D. Manning, ``Glove: Global vectors for word
  representation,'' in \emph{Proc. EMNLP}, 2014, pp. 1532--1543.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on Computer Vision
  and Pattern Recognition}, 2016.

\bibitem{carreira2017quo}
J.~Carreira and A.~Zisserman, ``Quo vadis, action recognition? a new model and
  the kinetics dataset,'' in \emph{proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition}, 2017, pp. 6299--6308.

\bibitem{DBLP:conf/nips/SimonyanZ14}
K.~Simonyan and A.~Zisserman, ``Two-stream convolutional networks for action
  recognition in videos,'' in \emph{NeurIPS}, 2014.

\bibitem{ren2016faster}
S.~Ren, K.~He, R.~Girshick, and J.~Sun, ``{Faster R-CNN}: towards real-time
  object detection with region proposal networks,'' \emph{IEEE Transactions on
  Pattern Analysis and Machine Intelligence}, 2016.

\bibitem{kim2018bilinear}
J.-H. Kim, J.~Jun, and B.-T. Zhang, ``Bilinear attention networks,'' in
  \emph{NeurIPS}, 2018.

\bibitem{tgif-qa}
Y.~Jang, Y.~Song, Y.~Yu, Y.~Kim, and G.~Kim, ``Tgif-qa: Toward spatio-temporal
  reasoning in visual question answering,'' in \emph{Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition}, 2017, pp. 2758--2766.

\bibitem{psac}
X.~Li, J.~Song, L.~Gao, X.~Liu, W.~Huang, X.~He, and C.~Gan, ``Beyond rnns:
  Positional self-attention with co-attention for video question answering,''
  in \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  2019.

\bibitem{quest}
J.~Jiang, Z.~Chen, H.~Lin, X.~Zhao, and Y.~Gao, ``Divide and conquer:
  Question-guided spatio-temporal contextual attention for video question
  answering,'' in \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem{xiao2022video}
J.~Xiao, A.~Yao, Z.~Liu, Y.~Li, W.~Ji, and T.-S. Chua, ``Video as conditional
  graph hierarchy for multi-granular question answering.''\hskip 1em plus 0.5em
  minus 0.4em\relax AAAI, 2022.

\bibitem{msrvtt-qa}
J.~Xu, T.~Mei, T.~Yao, and Y.~Rui, ``Msr-vtt: A large video description dataset
  for bridging video and language,'' in \emph{Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition}, 2016, pp. 5288--5296.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{IEEE Conference on
  Computer Vision and Pattern Recognition}, 2009.

\bibitem{krishna2017visual}
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.-J. Li, D.~A. Shamma \emph{et~al.}, ``Visual genome:
  Connecting language and vision using crowdsourced dense image annotations,''
  \emph{International journal of computer vision}, vol. 123, no.~1, pp. 32--73,
  2017.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\end{thebibliography}
