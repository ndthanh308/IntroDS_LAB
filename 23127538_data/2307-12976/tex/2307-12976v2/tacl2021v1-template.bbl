\begin{thebibliography}{49}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang,
  and Weinbach}]{black2022gpt}
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao,
  Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
  Michael Pieler, Usvsn~Sai Prashanth, Shivanshu Purohit, Laria Reynolds,
  Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.bigscience-1.9}
  {{GPT}-{N}eo{X}-20{B}: An open-source autoregressive language model}.
\newblock In \emph{Proceedings of BigScience Episode {\#}5 -- Workshop on
  Challenges {\&} Perspectives in Creating Large Language Models}, pages
  95--136, virtual+Dublin. Association for Computational Linguistics.

\bibitem[{Brown et~al.(2020{\natexlab{a}})Brown, Mann, Ryder, Subbiah, Kaplan,
  Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss,
  Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler,
  Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{NEURIPS2020_1457c0d6}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020{\natexlab{a}}.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[{Brown et~al.(2020{\natexlab{b}})Brown, Mann, Ryder, Subbiah, Kaplan,
  Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss,
  Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler,
  Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020{\natexlab{b}}.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al. 2021.
\newblock \href {https://arxiv.org/abs/2107.03374} {Evaluating large language
  models trained on code}.
\newblock \emph{ArXiv preprint}, abs/2107.03374.

\bibitem[{Cohen et~al.(2023{\natexlab{a}})Cohen, Geva, Berant, and
  Globerson}]{cohen2023crawling}
Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. 2023{\natexlab{a}}.
\newblock \href {https://aclanthology.org/2023.findings-eacl.139} {Crawling the
  internal knowledge-base of language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EACL 2023}, pages 1856--1869, Dubrovnik, Croatia. Association for
  Computational Linguistics.

\bibitem[{Cohen et~al.(2023{\natexlab{b}})Cohen, Geva, Berant, and
  Globerson}]{cohen-etal-2023-crawling}
Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson. 2023{\natexlab{b}}.
\newblock \href {https://aclanthology.org/2023.findings-eacl.139} {Crawling the
  internal knowledge-base of language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EACL 2023}, pages 1856--1869, Dubrovnik, Croatia. Association for
  Computational Linguistics.

\bibitem[{Dai et~al.(2022)Dai, Dong, Hao, Sui, Chang, and
  Wei}]{dai-etal-2022-knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.581} {Knowledge
  neurons in pretrained transformers}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 8493--8502,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{De~Cao et~al.(2021)De~Cao, Aziz, and
  Titov}]{de-cao-etal-2021-editing}
Nicola De~Cao, Wilker Aziz, and Ivan Titov. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.522} {Editing
  factual knowledge in language models}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 6491--6506, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Dhingra et~al.(2022)Dhingra, Cole, Eisenschlos, Gillick, Eisenstein,
  and Cohen}]{dhingra2022time}
Bhuwan Dhingra, Jeremy~R. Cole, Julian~Martin Eisenschlos, Daniel Gillick,
  Jacob Eisenstein, and William~W. Cohen. 2022.
\newblock \href {https://doi.org/10.1162/tacl_a_00459} {Time-aware language
  models as temporal knowledge bases}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:257--273.

\bibitem[{Genin and Huber(2022)}]{sep-formal-belief}
Konstantin Genin and Franz Huber. 2022.
\newblock {Formal Representations of Belief}.
\newblock In Edward~N. Zalta and Uri Nodelman, editors, \emph{The {Stanford}
  Encyclopedia of Philosophy}, {F}all 2022 edition. Metaphysics Research Lab,
  Stanford University.

\bibitem[{Geva et~al.(2023)Geva, Bastings, Filippova, and
  Globerson}]{geva2023dissecting}
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023.
\newblock Dissecting recall of factual associations in auto-regressive language
  models.
\newblock \emph{arXiv preprint arXiv:2304.14767}.

\bibitem[{Geva et~al.(2021)Geva, Schuster, Berant, and
  Levy}]{geva-etal-2021-transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.446} {Transformer
  feed-forward layers are key-value memories}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 5484--5495, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Gupta et~al.(2023)Gupta, Mondal, Sheshadri, Zhao, Li, Wiegreffe, and
  Tandon}]{gupta2023editing}
Anshita Gupta, Debanjan Mondal, Akshay~Krishna Sheshadri, Wenlong Zhao,
  Xiang~Lorraine Li, Sarah Wiegreffe, and Niket Tandon. 2023.
\newblock \href {http://arxiv.org/abs/2305.14956} {Editing commonsense
  knowledge in gpt}.

\bibitem[{Hase et~al.(2023)Hase, Diab, Celikyilmaz, Li, Kozareva, Stoyanov,
  Bansal, and Iyer}]{hase-etal-2023-methods}
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin
  Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2023.
\newblock \href {https://aclanthology.org/2023.eacl-main.199} {Methods for
  measuring, updating, and visualizing factual beliefs in language models}.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter
  of the Association for Computational Linguistics}, pages 2714--2731,
  Dubrovnik, Croatia. Association for Computational Linguistics.

\bibitem[{Heinzerling and Inui(2021)}]{heinzerling2020language}
Benjamin Heinzerling and Kentaro Inui. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.eacl-main.153} {Language
  models as knowledge bases: On entity representations, storage capacity, and
  paraphrased queries}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pages
  1772--1791, Online. Association for Computational Linguistics.

\bibitem[{Hernandez et~al.(2023{\natexlab{a}})Hernandez, Li, and
  Andreas}]{hernandez2023inspecting}
Evan Hernandez, Belinda~Z. Li, and Jacob Andreas. 2023{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2304.00740} {Inspecting and editing
  knowledge representations in language models}.

\bibitem[{Hernandez et~al.(2023{\natexlab{b}})Hernandez, Li, and
  Andreas}]{hernandez2023measuring}
Evan Hernandez, Belinda~Z Li, and Jacob Andreas. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2304.00740} {Measuring and manipulating
  knowledge representations in language models}.
\newblock \emph{ArXiv preprint}, abs/2304.00740.

\bibitem[{Hoelscher-Obermaier et~al.(2023)Hoelscher-Obermaier, Persson, Kran,
  Konstas, and Barez}]{hoelscherobermaier2023detecting}
Jason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and Fazl
  Barez. 2023.
\newblock \href {https://aclanthology.org/2023.findings-acl.733} {Detecting
  edit failures in large language models: An improved specificity benchmark}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 11548--11559, Toronto, Canada. Association for Computational
  Linguistics.

\bibitem[{Jang et~al.(2022)Jang, Ye, Yang, Shin, Han, Kim, Choi, and
  Seo}]{jang2021towards}
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun
  Kim, Stanley~Jungkyu Choi, and Minjoon Seo. 2022.
\newblock \href {https://openreview.net/forum?id=vfsRB5MImo9} {Towards
  continual knowledge learning of language models}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain,
  Perez, Schiefer, Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan
  Perez, Nicholas Schiefer, Zac~Hatfield Dodds, Nova DasSarma, Eli
  Tran-Johnson, et~al. 2022.
\newblock \href {https://arxiv.org/abs/2207.05221} {Language models (mostly)
  know what they know}.
\newblock \emph{ArXiv preprint}, abs/2207.05221.

\bibitem[{Kassner et~al.(2021)Kassner, Tafjord, Sch{\"u}tze, and
  Clark}]{kassner-etal-2021-beliefbank}
Nora Kassner, Oyvind Tafjord, Hinrich Sch{\"u}tze, and Peter Clark. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.697}
  {{B}elief{B}ank: Adding memory to a pre-trained language model for a
  systematic notion of belief}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 8849--8861, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Lazaridou et~al.(2021)Lazaridou, Kuncoro, Gribovskaya, Agrawal,
  Liska, Terzi, Gimenez, de~Masson~d'Autume, Kocisk{\'{y}}, Ruder, Yogatama,
  Cao, Young, and Blunsom}]{lazaridou2021mind}
Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam
  Liska, Tayfun Terzi, Mai Gimenez, Cyprien de~Masson~d'Autume, Tom{\'{a}}s
  Kocisk{\'{y}}, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and
  Phil Blunsom. 2021.
\newblock \href
  {https://proceedings.neurips.cc/paper/2021/hash/f5bf0ba0a17ef18f9607774722f5698c-Abstract.html}
  {Mind the gap: Assessing temporal generalization in neural language models}.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems 2021, NeurIPS 2021,
  December 6-14, 2021, virtual}, pages 29348--29363.

\bibitem[{Levy et~al.(2017)Levy, Seo, Choi, and
  Zettlemoyer}]{levy-etal-2017-zero}
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017.
\newblock \href {https://doi.org/10.18653/v1/K17-1034} {Zero-shot relation
  extraction via reading comprehension}.
\newblock In \emph{Proceedings of the 21st Conference on Computational Natural
  Language Learning ({C}o{NLL} 2017)}, pages 333--342, Vancouver, Canada.
  Association for Computational Linguistics.

\bibitem[{Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K{\"{u}}ttler, Lewis, Yih, Rockt{\"{a}}schel, Riedel, and
  Kiela}]{lewis2021retrievalaugmented}
Patrick S.~H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K{\"{u}}ttler, Mike Lewis, Wen{-}tau Yih,
  Tim Rockt{\"{a}}schel, Sebastian Riedel, and Douwe Kiela. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}
  {Retrieval-augmented generation for knowledge-intensive {NLP} tasks}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[{Liu et~al.(2023)Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig}]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig. 2023.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55(9):1--35.

\bibitem[{Mallen et~al.(2023)Mallen, Asai, Zhong, Das, Khashabi, and
  Hajishirzi}]{mallen-etal-2023-trust}
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and
  Hannaneh Hajishirzi. 2023.
\newblock \href {https://aclanthology.org/2023.acl-long.546} {When not to trust
  language models: Investigating effectiveness of parametric and non-parametric
  memories}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 9802--9822,
  Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Meng et~al.(2022)Meng, Bau, Andonian, and
  Belinkov}]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.
\newblock Locating and editing factual associations in gpt.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:17359--17372.

\bibitem[{Meng et~al.(2023)Meng, Sharma, Andonian, Belinkov, and
  Bau}]{meng2022mass}
Kevin Meng, Arnab~Sen Sharma, Alex~J Andonian, Yonatan Belinkov, and David Bau.
  2023.
\newblock \href {https://openreview.net/forum?id=MkbcAHIYgyS} {Mass-editing
  memory in a transformer}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Mitchell et~al.(2022{\natexlab{a}})Mitchell, Lin, Bosselut, Finn, and
  Manning}]{mitchell2021fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D.
  Manning. 2022{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=0DcZxeWfOPt} {Fast model
  editing at scale}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Mitchell et~al.(2022{\natexlab{b}})Mitchell, Lin, Bosselut, Finn, and
  Manning}]{mitchell2022fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D.
  Manning. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=0DcZxeWfOPt} {Fast model
  editing at scale}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Onoe et~al.(2023)Onoe, Zhang, Padmanabhan, Durrett, and
  Choi}]{onoe2023lms}
Yasumasa Onoe, Michael Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol
  Choi. 2023.
\newblock \href {https://aclanthology.org/2023.acl-long.300} {Can {LM}s learn
  new entities from descriptions? challenges in propagating injected
  knowledge}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 5469--5485,
  Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href {http://arxiv.org/abs/2203.02155} {Training language models to
  follow instructions with human feedback}.

\bibitem[{Peters et~al.(2019)Peters, Neumann, Logan, Schwartz, Joshi, Singh,
  and Smith}]{peters-etal-2019-knowledge}
Matthew~E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi,
  Sameer Singh, and Noah~A. Smith. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1005} {Knowledge enhanced
  contextual word representations}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 43--54, Hong Kong, China.
  Association for Computational Linguistics.

\bibitem[{Petroni et~al.(2019)Petroni, Rockt{\"a}schel, Riedel, Lewis, Bakhtin,
  Wu, and Miller}]{petroni2019language}
Fabio Petroni, Tim Rockt{\"a}schel, Sebastian Riedel, Patrick Lewis, Anton
  Bakhtin, Yuxiang Wu, and Alexander Miller. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1250} {Language models as
  knowledge bases?}
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2463--2473, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8):9.

\bibitem[{Razniewski et~al.(2021)Razniewski, Yates, Kassner, and
  Weikum}]{razniewski2021language}
Simon Razniewski, Andrew Yates, Nora Kassner, and Gerhard Weikum. 2021.
\newblock \href {https://arxiv.org/abs/2110.04888} {Language models as or for
  knowledge bases}.
\newblock \emph{ArXiv preprint}, abs/2110.04888.

\bibitem[{Roberts et~al.(2020)Roberts, Raffel, and Shazeer}]{roberts2020much}
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.437} {How much
  knowledge can you pack into the parameters of a language model?}
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 5418--5426, Online. Association
  for Computational Linguistics.

\bibitem[{Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh}]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L. Logan~IV, Eric Wallace, and Sameer
  Singh. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.346}
  {{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with
  {A}utomatically {G}enerated {P}rompts}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4222--4235, Online. Association
  for Computational Linguistics.

\bibitem[{Si et~al.(2023)Si, Gan, Yang, Wang, Wang, Boyd-Graber, and
  Wang}]{si2022prompting}
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan~Lee
  Boyd-Graber, and Lijuan Wang. 2023.
\newblock \href {https://openreview.net/forum?id=98p5x51L5af} {Prompting
  {GPT}-3 to be reliable}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al. 2023.
\newblock \href {https://arxiv.org/abs/2302.13971} {Llama: Open and efficient
  foundation language models}.
\newblock \emph{ArXiv preprint}, abs/2302.13971.

\bibitem[{Wang et~al.(2021{\natexlab{a}})Wang, Tang, Duan, Wei, Huang, Ji, Cao,
  Jiang, and Zhou}]{wang2020kadapter}
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji,
  Guihong Cao, Daxin Jiang, and Ming Zhou. 2021{\natexlab{a}}.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-acl.121}
  {{K-Adapter}: {I}nfusing {K}nowledge into {P}re-{T}rained {M}odels with
  {A}dapters}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 1405--1418, Online. Association for Computational
  Linguistics.

\bibitem[{Wang et~al.(2021{\natexlab{b}})Wang, Gao, Zhu, Zhang, Liu, Li, and
  Tang}]{wang-etal-2021-kepler}
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi
  Li, and Jian Tang. 2021{\natexlab{b}}.
\newblock \href {https://doi.org/10.1162/tacl_a_00360} {{KEPLER}: A unified
  model for knowledge embedding and pre-trained language representation}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:176--194.

\bibitem[{Yao et~al.(2022)Yao, Huang, Dong, Wei, Chen, and
  Zhang}]{yao2022kformer}
Yunzhi Yao, Shaohan Huang, Li~Dong, Furu Wei, Huajun Chen, and Ningyu Zhang.
  2022.
\newblock Kformer: Knowledge injection inÂ transformer feed-forward layers.
\newblock In \emph{Natural Language Processing and Chinese Computing}, pages
  131--143, Cham. Springer International Publishing.

\bibitem[{Yao et~al.(2023)Yao, Wang, Tian, Cheng, Li, Deng, Chen, and
  Zhang}]{yao2023editing}
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng,
  Huajun Chen, and Ningyu Zhang. 2023.
\newblock \href {http://arxiv.org/abs/2305.13172} {Editing large language
  models: Problems, methods, and opportunities}.

\bibitem[{Zhang et~al.(2021)Zhang, Deng, Cheng, Chen, Zhang, Zhang, and
  Chen}]{ijcai2021p552}
Ningyu Zhang, Shumin Deng, Xu~Cheng, Xi~Chen, Yichi Zhang, Wei Zhang, and
  Huajun Chen. 2021.
\newblock \href {https://doi.org/10.24963/ijcai.2021/552} {Drop redundant,
  shrink irrelevant: Selective knowledge injection for language pretraining}.
\newblock In \emph{Proceedings of the Thirtieth International Joint Conference
  on Artificial Intelligence, {IJCAI-21}}, pages 4007--4014. International
  Joint Conferences on Artificial Intelligence Organization.
\newblock Main Track.

\bibitem[{Zhang et~al.(2022)Zhang, Bosselut, Yasunaga, Ren, Liang, Manning, and
  Leskovec}]{zhang2022greaselm}
Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang,
  Christopher~D. Manning, and Jure Leskovec. 2022.
\newblock \href {https://openreview.net/forum?id=41e9o6cQPj} {Greaselm: Graph
  reasoning enhanced language models}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Zhang et~al.(2019)Zhang, Han, Liu, Jiang, Sun, and
  Liu}]{zhang-etal-2019-ernie}
Zhengyan Zhang, Xu~Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1139} {{ERNIE}: Enhanced
  language representation with informative entities}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 1441--1451, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Zheng et~al.(2023)Zheng, Li, Dong, Fan, Wu, Xu, and
  Chang}]{zheng2023edit}
Ce~Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao
  Chang. 2023.
\newblock \href {http://arxiv.org/abs/2305.12740} {Can we edit factual
  knowledge by in-context learning?}

\bibitem[{Zhong et~al.(2023)Zhong, Wu, Manning, Potts, and
  Chen}]{zhong2023mquake}
Zexuan Zhong, Zhengxuan Wu, Christopher~D. Manning, Christopher Potts, and
  Danqi Chen. 2023.
\newblock \href {http://arxiv.org/abs/2305.14795} {Mquake: Assessing knowledge
  editing in language models via multi-hop questions}.

\end{thebibliography}
