% Figure environment removed


\section{Ripple Effects of Factual Edits}
\label{sec:rethinking}

We focus on evaluating the downstream effect of a given edit, i.e., given an edit $(e,r,o) \rightarrow (e,r,o')$, we expect certain facts related to the edit to change as well. Consider, for example, the edit shown in Fig.~\ref{figure:intro}. Changing the team for which Messi plays might also affect the league he plays in and his country of residence.
Formally, for a given model, assume a knowledge-graph ${\mathcal{K} := \{(e_i, r_i, o_i)\}_{i=1}^{N}}$ of $N$ factual triplets, representing the model's knowledge, and let $\delta: (e,r,o) \rightarrow (e,r,o')$ be an edit request for $\mathcal{K}$.
We define the \textit{ripple effect} of $\delta$ on $\mathcal{K}$, as the set of triplets $\mathcal{R}(\delta)$ that the model implicitly needs to inject, modify, or delete from $\mathcal{K}$ to reflect the world state after the edit. 

Notably, different edits can cause ripple effects of varying magnitudes. For example, changing the country of Rome from Italy to France, will entail many follow-up changes, such as the country in which the Colosseum is located, the language spoken in Rome, and so forth. 
In contrast, updating the siblings of Prince (Fig.~\ref{figure:examples}) is both more realistic and should result in a more local effect. 
We refer to the number of facts affected by a single edit $\delta$ (i.e. $|\mathcal{R}(\delta)|$) as its \textit{severity}.
In general, editing popular entities that appeared frequently during training is likely to introduce more changes, and thus, editing their properties has a higher severity.

\subsection{Evaluation Criteria}
\label{subsec:criteria}

We wish to evaluate how well models capture the ripple effects of factual edits. However, given that ripple effects can potentially span a large number of implied edits, we focus on evaluating modified facts that are within a 2-hop distance from the subject or object of the edit.
Concretely, for an edit $\delta: (e, r, o) \rightarrow (e, r, o^*)$, we evaluate the ripple effect $\mathcal{R}(\delta)$, via the following evaluation criteria (examples are shown in Fig.~\ref{figure:examples}): 

\begin{enumerate}
[leftmargin=*,topsep=2pt,parsep=1pt]
    \item \textbf{\logicalgeneralization{} (LG)}:
    Relations in a knowledge graph satisfy certain logical constraints. For example, the relation \texttt{Sibling} is symmetric and therefore if $(e, \texttt{Sibling} ,o)$ is true then $(o,\texttt{Sibling},e)$ is also true, and vice versa (Fig.~\ref{figure:examples}A). Likewise, the relation \texttt{Location} is transitive so $(e,\texttt{Location},o) \wedge (o,\texttt{Location},z) \Rightarrow (e,\texttt{Location},z)$. We wish to check that such logical implications about the subject $e$, the original object $o$, and the new object $o^*$, hold after editing. We focus and elaborate on specific constraints in \S\ref{sec:rippledit}.
    
    \item \textbf{\compositionality{} (CI)}: As $\delta$ alters one edge in a knowledge graph, we can check the composition of this edge with other edges. Namely, we test if the model can compose the edited fact with other facts about the target object.
    Let $(o, r', z)$ and $(o^*, r', z^*)$ be two facts of the same relation about $o$ and $o^*$,
    respectively. Also, denote by $r''=r\circ r'$ the complex relation expressing the composition of $r$ and $r'$ (e.g., $r''=\texttt{Profession of sibling}$ for $r=\texttt{Sibling}$ and $r'=\texttt{Profession}$).  
    Then, after the edit $\delta$, we expect the following change  $(e, r'', z) \rightarrow (e, r'', z^*)$.
    For example (Fig.~\ref{figure:examples}B), the professions of the siblings of \texttt{Prince} can be modified once a new sibling is injected.
    
    \item \textbf{\forwardcompositionality{} (CII)}:
    We test if the model can compose the edited fact with facts about a different subject $e' \neq e$.
    Formally, let $(e', r', e)$ be a fact about $e'$ with $e$ as its object, and denote by $r''=r'\circ r$ the complex relation expressing the composition of $r'$ and $r$ (see an example in criterion 2). After the edit $\delta$, the following change is expected for the subject $e'$:
    $(e', r'', o) \rightarrow (e', r'', o^*)$. 
    For instance (Fig.~\ref{figure:examples}C), changing the siblings of \texttt{Prince} also modifies the siblings of the founder of \texttt{Paisley Park Records} (i.e., $r''$ is a complex relation expressing ``siblings of the founder'').
    
    \item \textbf{\aliasing{} (SA)}: We test that editing a fact about $e$ induces the same edit to other entities $e'$ that are aliases of $e$, namely, $(e', r, o) \rightarrow (e', r, o^*)$. 
    For instance (Fig.~\ref{figure:examples}D), modifying the siblings of \texttt{Prince}, should also modify the sibling of his alias, \texttt{Prince Roger Nelson}.
    
    \item \textbf{\forgetfulness{} (PV)}: If $r$ is a one-to-many relation, then adding a new object should not affect the other objects encoded about $e$. 
    Hence, in such cases, we expect that any existing triplet $(e, r, o')$ for an object $o' \neq o^*$ would remain following the edit. 
    For example (Fig.~\ref{figure:examples}E), after inserting the sibling \texttt{Nicholas Carminowe} for \texttt{Prince}, the fact that \texttt{Tyka Nelson} is also his sibling should be retained.
    
    \item \textbf{\relationspecificity{} (RS)}: We test that facts about $e$, with relations whose objects are not influenced by $o$, are indeed not affected by the edit.
    For example (Fig.~\ref{figure:examples}F), modifying the sibling of \texttt{Prince} should not change his \texttt{Mother}. Note that these facts complement those evaluated by \logicalgeneralization{}.
\end{enumerate}

\noindent In \S\ref{sec:datacollection}, we describe how we generate factual editing evaluations, based on the above criteria.

\subsection{Related Work}
\label{subsec:currentstate}

\paragraph{Knowledge Editing Methods}
Several methods have been proposed to edit the factual knowledge encoded in a model. 
\citet{de-cao-etal-2021-editing} and \citet{mitchell2022fast} suggested to use hyper-networks to update the model weights. 
In addition, \citet{meng2022locating, meng2022mass} proposed to modify encoded facts by updating the weights of MLP layers, following recent observations that these layers can be cast as key-value memories \cite{geva-etal-2021-transformer} that store factual knowledge \cite{dai-etal-2022-knowledge}.
Other methods learn encodings that update the hidden representations created during model inference \cite{hernandez2023inspecting}, or augment the input context with edits \cite{zhong2023mquake, zheng2023edit}.
In \S\ref{subsec:experimental_settings}, we discuss state-of-the-art KE methods used in this work in greater detail.

Separately from factual KE, recent works have also studied how to inject new facts into a model. Previous methods suggested unsupervised pre-training \cite{roberts2020much, ijcai2021p552}, semi-parametric methods methods where external information is added from a knowledge-base \cite{zhang-etal-2019-ernie, peters-etal-2019-knowledge, lewis2021retrievalaugmented, zhang2022greaselm}, using adapters to store knowledge \cite{wang2020kadapter}, or extending the MLP layers \cite{yao2022kformer}.

\paragraph{Knowledge Editing Evaluation}
Recently, there has been a growing interest in KE evaluation \cite{yao2023editing}.
The prominent benchmarks for evaluating factual KE are the Zero-Shot Relation Extraction
(zsRE) \cite{levy-etal-2017-zero, de-cao-etal-2021-editing} and CounterFact \cite{meng2022locating}. zsRE is a question-answering dataset for relation-specific queries, which includes human generated paraphrases that are used to measure robustness to semantically equivalent inputs. For example, for the triplet  (\texttt{x}, \texttt{Country}, \texttt{y}), zsRE contains queries such as ``\emph{In which country is x?}''. CounterFact offers a more challenging setting, where edits are counterfactuals of a low probability, such as changing the \texttt{City} of \texttt{The Louvre} from \texttt{Paris} to \texttt{Rome}.

Evaluation in zsRE and CounterFact focuses on three primary aspects of (a) \textit{efficacy}: checking that the model generates the target object post-editing, (b) \textit{paraphrasing}: testing robustness in generating the target for paraphrases of the input, and (c) \textit{specificity}: verifying that facts not related to the edit are unaffected.
In addition, CounterFact evaluates the generation quality of the edited model when prompted with the edit's subject, measuring: \textit{consistency}, i.e., similarity with subjects that share the same property as the edited object, and \textit{fluency} in terms of repetitiveness of the generated text.
More broadly, previous work evaluated to which extent LMs have beliefs \cite{sep-formal-belief, kassner-etal-2021-beliefbank, hase-etal-2023-methods}, and \citet{hase-etal-2023-methods} examined if updating beliefs propagates to entailed facts, extending the Wikidata5m dataset \cite{wang-etal-2021-kepler} to test editing specificity.

Recently, \citet{onoe2023lms} introduce the task of \emph{entity knowledge propagation}, aiming to examine the extent to which models are able to reason about emergent entities that did not appear in pre-training. In addition, \citet{hoelscherobermaier2023detecting} show that existing KE methods can have unwanted side effects and suffer from low specificity.
A concurrent work by \citet{zhong2023mquake} introduces MQUAKE, a benchmark that tests the ability of models to perform multi-hop reasoning after edits. 
While each of these benchmarks focuses on a single consequence of editing, \ripple{} provides a general framework for evaluating various types of edit ripple effects.
Last, \citet{gupta2023editing} focus on editing commonsense knowledge and introduce MEMIT-CSKPROBE, a dataset for semantic generalization of commonsense edits. \ripple{} is different from MEMIT-CSKPROBE as it evaluates editing of factual knowledge rather than commonsense knowledge.