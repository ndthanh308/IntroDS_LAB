
\section{Experiments}
\label{sec:experiments}

We use \ripple{} to evaluate recent KE methods, and show that despite substantial progress on existing benchmarks, current methods struggle to introduce consistent changes to the model's knowledge after an edit. Moreover, a simple in-context editing baseline that conditions the generation on the edited fact obtains better results, while leaving ample room for improvement for future research.

% Figure environment removed


\subsection{Evaluation Setting}
\label{subsec:experimental_settings}

\paragraph{Data}
To evaluate how well an editing method handles the ripple effects resulting from editing a given model, the data first needs to be adjusted such that (a) only cases of successful edits are evaluated, and (b) only test queries that the model answered correctly pre-editing are used for evaluation. 
Concretely, for an editing method $\mathcal{F}$ and a model $\mathcal{M}$, an edit request $x: (e,r,o) \rightarrow (e,r,o')$ is included in the evaluation if the following conditions are met when applying $\mathcal{F}$ to $\mathcal{M}$ and $x$: (a) $\mathcal{M}$ successfully generates the original objects for the test queries before applying the edit, and (b) $\mathcal{M}$ successfully generates $o'$ when queried about $e$ and $r$, namely, the edit has successfully been applied. 
For example, we verify that the model can predict the children of $o'$ before asking about $e$'s new siblings. 


\paragraph{Editing methods} 
We evaluate three KE methods:
MEND \cite{mitchell2022fast}, ROME \cite{meng2022locating}, and MEMIT \cite{meng2022mass}. MEND trains a network that modifies gradients to produce local edits.
ROME makes rank-one updates to the weights of the Transformer's MLP layers to modify specific factual associations, and MEMIT is an extension of ROME that is adjusted to editing many facts at once.


\paragraph{Baseline} Motivated by the recent success of LMs to learn in-context and follow instructions \cite{NEURIPS2020_1457c0d6, ouyang2022training, liu2023pre},
specifically for knowledge editing \cite{zhong2023mquake, zheng2023edit},
we experiment with an in-context editing (ICE) baseline for factual editing.
Unlike the above methods, it does not introduce changes to the model parameters, but rather generation is conditioned on the new fact. 
Concretely, given an edit $(e, r, o) \rightarrow (e, r, o^*)$ and a test query $q$, we use the following prompt to obtain an answer from the model: \texttt{``Imagine that <$o^*$> would have been <$P_r$>''}, where $P_r$ is a manually-written proposition of $r$, such as \emph{``The mother of <$e$>''} when $r=$ \texttt{Mother} and $e$ is the subject.
An example is depicted in Fig.~\ref{figure:ice_demonstration}.


\begin{table}[t]
\setlength{\belowcaptionskip}{-8pt}
\setlength\tabcolsep{4pt}
    \centering
    \footnotesize
    \begin{tabular}{lllccrr}
    & \multicolumn{2}{c}{\recentlyemerged{}} & \multicolumn{2}{c}{\fakefacts{}} & \multicolumn{2}{c}{\topviews{}} \\ 
    & Edits & Tests & Edits & Tests & Edits & Tests \\ \midrule
        \gptxl{}
         &$853$ &$29\%$ &$689$ &$33\%$ &$722$ &$71\%$  \\ 
         \gptj{}
         &$801$ &$33\%$ &$717$ &$34\%$ &$760$ &$76\%$  \\ 
         \gptneo{} 
         &$989$ &$45\%$ &$801$ &$46\%$ &$828$ &$86\%$ \\ 
         \llama{} 
         &$847$ &$44\%$ &$796$ &$49\%$ &$784$ &$87\%$   \\ 
         \gptt{} 
         &$822$ &$55\%$ &$760$ &$74\%$ &$665$ &$94\%$  \\
         \bottomrule 
    \end{tabular}
    \caption{(a) Number of edits considered in our evaluation (i.e., that have successfully applied), from each subset, averaged over \rome{}, \memit{} and \mend{}, for the models: \gptxl{}, \gptj{}, \gptneo{} and \llama{}, and the ICE baseline for \gptt{}. (b) Portion of queries, on average, that were used in our evaluation.}
\label{table:filtered_tests_portion}
\end{table}

\paragraph{Models} 
We use 4 recent auto-regressive decoder-only LMs of different sizes: GPT-2 XL \cite{radford2019language} with 
1.5B parameters,
GPT-J \cite{chen2021evaluating} with 6B parameters, LLaMA with 7B parameters, \cite{touvron2023llama}, and GPT-NeoX with 20B parameters \cite{black2022gpt}.
In addition, as our baseline does not require access to the model parameters, we also evaluate it on the closed-source model GPT-3 \texttt{text-davinci-003} with 175B parameters \cite{brown2020language}.
However, for the baseline we do not include results for \gpt{} and \gptj{} as the number of testable edits for these models is rather small ($\leq20\%$ for each of the data subsets).

For all model-method combinations, except for \rome{} with \llama{}, we use the official implementation and hyperparameters from \citet{meng2022locating}. 
We adjust ROME to \llama{} by following the authors' method and codebase.
Table~\ref{table:filtered_tests_portion} shows the number of edits and test queries left, for every model, after filtering out non-successful edits and inapplicable test queries (as described above).

\paragraph{Evaluation} Each model-method pair is evaluated separately, on every subset of \ripple{}. For each evaluation criterion, we first compute the average accuracy over the test queries per example, and then average over all the examples. For a given test query, we let the model generate a maximum of 20 tokens. A generation is considered successful if one of the aliases of the target object appears in the text. In cases of multiple gold target objects (as in \forgetfulness{}), we evaluate each target object separately and consider the generation as correct if it matches at least one object.

\input{tables/results_real}
\input{tables/results_fake}
\input{tables/results_fakepopular}


\subsection{Results}
Tables~\ref{table:recently_emerged_results},~\ref{table:fake_facts_results},~\ref{table:top_views_results} show the evaluation results on the \recentlyemerged{}, \fakefacts{}, and \topviews{} subsets, respectively.
Considering the average scores across all subsets, we observe that existing editing methods struggle to handle the ripple effect induced by editing facts, with low average accuracy of $38-66$ across all models.
This suggests that, while KE methods demonstrate high capability in making local updates to the model's knowledge, these changes are mostly applied at a surface-level without propagating to other related facts.
Moreover, we observe that our ICE baseline obtains the best overall results. Specifically, it outperforms \rome{} by more than 10 points for \gptneo{} and 29 points for \llama{}, on average across subsets. Although \gptt{} with ICE performs best on average, the 7B \llama{} is highly competitive, performing better or similarly on the \recentlyemerged{} and \topviews{} subsets.

Next, comparing results across evaluation criteria shows that some ripple effects are handled better than others. For example, while \aliasing{} accuracy is consistently high ($\geq86.8$ across all settings), the accuracy on other criteria is generally lower and varies greatly between models, methods, and edits (e.g., \logicalgeneralization{} accuracy for \rome{} on \gptj{} is $53.8$ on the \fakefacts{} subset, compared to only $5.5$ on the \topviews{} subset).


% Figure environment removed

\paragraph{Results across model size}
We analyze how editing performance on \ripple{} is influenced by the model size. To this end, we further evaluate \rome{} on smaller versions of \gpt{} -- with 345M (GPT2-M) and 762M (GPT2-L) parameters, and plot the average accuracy over the three subsets as a function of model size.
Fig.~\ref{figure:acc_as_a_function_of_size} presents the results, showing that editing performance increases with model size, with \rome{} obtaining substantially higher accuracy when applied to larger models. 
Nevertheless, our results (Tables~\ref{table:recently_emerged_results},~\ref{table:fake_facts_results},~\ref{table:top_views_results}) show that when using ICE, the 7B \llama{} is competitive with the much larger \gptt{}, suggesting that simply scaling the model size may not be sufficient to fix the drawbacks of current editing methods. 

\paragraph{Results across methods}

\begin{table}[t]
% \setlength{\belowcaptionskip}{-10pt}
\footnotesize
\begin{center}
\begin{tabular}{lccc}
 & \mend{} & \rome{} & \memit{} \\ [0.1cm]
\toprule
\relationspecificity{}          & $34.4$    &$37.6$   &$39.1$  \\
\logicalgeneralization{}    & $39.1$    &$26.5$   &$29.0$  \\
\compositionality{}            & $17.0$    &$37.9$   &$35.3$   \\ 
\forwardcompositionality{}    & $13.6$    &$37.7$   &$39.1$    \\
\bottomrule
\end{tabular}
\end{center}
\caption{Accuracy of \mend{}, \rome{} and \memit{}, using \gptxl{}, averaged over the three \ripple{} splits - \recentlyemerged{}, \fakefacts{} and \topviews{}.}
\label{table:res_across_methods}
\end{table}

Table~\ref{table:res_across_methods} shows the accuracy of \mend{}, \rome{} and \memit{}, on \gptxl{} across our evaluation criteria, averaged over the three subsets. Interestingly, \mend{} outperforms \rome{} and \memit{} in \logicalgeneralization{}, but is worse in \compositionality{} and \forwardcompositionality{}, suggesting that different methods might better capture different types of ripple effects.

% Figure environment removed

\paragraph{Results across data splits}
The subsets of \ripple{} differ in whether edited facts are counterfeit
or real, and in the popularity of the edited entities. These differences allow us to control for the edit severity, as popular entities are expected to introduce larger ripple effects (see \S\ref{sec:rethinking}).
In Fig.~\ref{figure:test_scores_per_split}, we show the accuracy on each subset and evaluation criterion, averaged over the different editing methods.
Comparing \fakefacts{} and \topviews{}, that differ in the popularity of the edited entities, we see that while \logicalgeneralization{} accuracy is substantially higher for \fakefacts{}, \forgetfulness{} accuracy is higher for \topviews{}. This suggests that, although retaining correct knowledge is easier for popular entities, modifying other facts that logically follow from an edit is harder for popular entities, which could be explained by the severity of these edits (i.e. the high number of facts that are semantically related to them).





\begin{table}[t]
\setlength{\belowcaptionskip}{-10pt}
\setlength\tabcolsep{3pt}
    \centering
    \footnotesize
    \begin{tabular}{llccc}
    & & No effect & Abstaining & Noise \\ \midrule
        \multirow{2}{*}{\gptxl{}} 
         & ROME & 27\% & 31\% & 42\%  \\
         & ICE & 32\% & 27\% & 41\%  \\ \midrule
         \multirow{2}{*}{\gptneo{}} 
         & ROME & 24\% & 40\% & 36\%  \\
         & ICE & 10\% & 65\% & 25\%     \\ \midrule
         \multirow{2}{*}{\llama{}}
         & ROME & 20.5\% & 45\% & 34.5\% \\
         & ICE & 11\% & 71\% & 18\%  \\
         \bottomrule
    \end{tabular}
    \caption{Error type distribution on 200 failures of \rome{} and ICE, on \gptxl{}, \gptneo{}, and \llama{}.}
\label{table:error_analysis}
\end{table}


\subsection{Error Analysis}
\paragraph{\rome{} versus ICE}
We qualitatively analyze the effect induced by KE methods to the model's knowledge. To this end, for each of \rome{} and our ICE baseline and each of the models \gpt{}, \gptneo{}, and \llama{}, we sample 200 test queries from \ripple{} on which the model fails post-editing. We then label these failures using three categories: (a) \textit{no effect}, for cases when the model predicts the original object, i.e. the edit introduced no ripple effect, (b) \textit{abstaining}, when the model abstains from answering by generating text like \textit{``unknown''} or \textit{``a mystery''}, and (c) \textit{noise}, when the model generates an incorrect object or unrelated text.
Table~\ref{table:error_analysis} presents the results, showing that in most cases ($\geq68\%$ across all settings) factual editing introduces erroneous changes to the model's knowledge rather than making no change. Interestingly, for both \gptneo{} and \llama{}, where editing performance is better than \gpt{}, \rome{} introduces more incorrect changes while ICE causes the model to abstain from answering. 


\paragraph{\gptt{} versus \llama{} using ICE}
We further looked into the performance on the LG tests, where applying ICE to \gptt{} is notably inferior to ICE on \llama{} (see Tables~\ref{table:recently_emerged_results},~\ref{table:fake_facts_results},~\ref{table:top_views_results}). Specifically, we collected responses from each of the models to 100 random LG queries, and analyzed them using the same categories as described above. 
We observed that \gptt{} abstains from answering the query much more often than \llama{} (49\% of the cases for \gptt{} compared to only 28\% in \llama{}), which could explain the lower performance of ICE on \gptt{} on these queries.
