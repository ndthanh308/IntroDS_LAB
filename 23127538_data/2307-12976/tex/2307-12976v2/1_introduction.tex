

% Figure environment removed


\section{Introduction}

Modern language models (LMs) capture a large volume of factual knowledge in their parameters, which can be effectively utilized in downstream tasks \cite{petroni2019language, roberts2020much, shin2020autoprompt, razniewski2021language, heinzerling2020language, kadavath2022language, cohen2023crawling}.
However, factual beliefs captured by the model may be incorrect or become outdated over time, potentially affecting the model's performance on downstream tasks, its reliability and its usability \cite{dhingra2022time, lazaridou2021mind, jang2021towards}.


This limitation has prompted research on knowledge editing (KE) methods, which modify LMs to fix their factual errors (we provide a formal definition in \S\ref{sec:setting}).
Knowledge editing work has focused on applying factual updates to LMs. Given an entity-relation-object triplet $(e,r,o)$ representing a fact (e.g. \textit{``Lionel Messi plays for the Inter Miami team''}), recent work proposed various methods \cite{mitchell2021fast, meng2022locating, meng2022mass, hernandez2023measuring, si2022prompting} to inject this fact into the parameters of a given LM, while ``overriding'' beliefs the model might have on $e$ and $r$ (e.g. that Messi plays for Paris Saint-Germain).

A key question with KE is how to evaluate the success of such editing operations. The most basic ``sanity-check'' is that the model correctly completes  $(e,r,?)$, as well as other paraphrases of this task, with $o$. However, this is not enough as an evaluation, since one needs to check that the model did not distort other facts. Indeed, the standard evaluation protocol \cite{mitchell2022fast, meng2022locating, meng2022mass} for KE focuses on these two aspects of correctly completing various paraphrases of the new fact, as well as ensuring that other unrelated facts have not been changed.

In this work, we argue that to evaluate model edits, one should go beyond the single fact that was edited and check that other facts that are logically derived from the edit were also changed accordingly. For example, if $z$ is the mother of $e$, then the children of $z$ are the siblings of $e$. Consequently, once we modify the belief of a certain model that $z \rightarrow z'$ is the mother of $e$, then we should also ensure that the model's belief regarding the siblings of $e$ is also correct. 
Fig.~\ref{figure:intro} illustrates another example, 
where editing the \texttt{Team} for which \texttt{Lionel Messi} plays modifies other related facts, such as his country of residence, while other facts should be retained.
We refer to such changes that are implied by a factual edit as \textit{``ripple effects''}. 

To account for ripple effects in the evaluation of factual edits, we propose six concrete evaluation criteria (see \S\ref{sec:rethinking}, Fig.~\ref{figure:examples}), for testing which facts other than the edit itself should be modified or retained post-editing. 
Our tests evaluate how well the model integrates the edit with the rest of its knowledge, through queries that involve logical reasoning, complex composition of facts with the edit as an intermediate step, subject aliasing, and specificity across relations. 

Building upon these criteria, we create \ripple{}, a new benchmark for comprehensive evaluation of KE of LMs (see \S\ref{sec:rippledit}).
\ripple{} includes $5$K entries, each consisting of a factual edit, along with a set of test queries that check if the edit was successful in terms of its ripple effect.
Moreover, \ripple{} contains meta-data for each edit, including information about the timestamp of the edit (i.e., recent versus old), and the popularity of the entities (i.e., head versus tail).

We use \ripple{} to evaluate three popular editing methods on five recent strong LMs (see \S\ref{sec:experiments}). We find that, even though current KE methods are effective in modifying a particular fact, they often fail to capture the ripple effects entailed by that fact, and demonstrate poor performance on most of our evaluation criteria. Moreover, analyzing how editing performance varies across model sizes and entity frequencies, we find that (a) larger models handle ripple effects better, and (b) editing frequent entities results in more logical reasoning errors.

Last, we consider a simple in-context editing baseline for KE, that leverages the casual attention mechanism rather than explicit parametric updates. 
While this method achieves the best results on our benchmark, outperforming current parametric KE methods, there is still ample room for improvement that calls for future research. 


To conclude, our work makes multiple contributions: (a) it highlights key limitations of KE evaluation, specifically regarding ripple effects and introduces comprehensive evaluation criteria to mitigate those limitations, (b) it proposes \ripple{}, a benchmark inspired by these criteria, (c) it evaluates current methods for KE and shows that they do not perform well on this task, while demonstrating that in-context editing is a promising direction for KE. 
We release \ripple{} and our code to facilitate future work on KE.