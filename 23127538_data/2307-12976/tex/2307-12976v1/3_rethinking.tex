% Figure environment removed


\section{Ripple Effects of Factual Edits}
\label{sec:rethinking}

Our focus is on evaluating the downstream effects of a given edit. Namely, given an edit $(e,r,o) \rightarrow (e,r,o')$, we expect certain facts related to the edit to change as well. Consider, for example, the edit shown in Fig.~\ref{figure:intro}. Changing the city in which the \texttt{Eiffel Tower} is located might also affect its country location and time zone. 
Formally, for a given model, assume a knowledge-graph ${\mathcal{K} := \{(e_i, r_i, o_i)\}_{i=1}^{N}}$ of $N$ factual triplets, representing the model's internal knowledge, and let $\delta: (e,r,o) \rightarrow (e,r,o')$ be an edit request for $\mathcal{K}$.
We define the \textit{ripple effect} of $\delta$ on $\mathcal{K}$, as the set of triplets $\mathcal{R}(\delta)$ that the model implicitly needs to inject, modify, or delete from $\mathcal{K}$ to reflect the world state after the edit $\delta$. 

Notably, different edits can cause ripple effects of varying magnitudes. For example, changing the country of Rome from Italy to France, will entail many changes, such as the country in which the Colosseum is located, the language spoken in Rome, inter alia. 
On the other hand, updating the \texttt{Siblings} of \texttt{Prince} (Fig.~\ref{figure:examples}) is both more realistic and should result in a more local effect. 
We refer to the number of facts affected by a single edit $\delta$ (i.e. $|\mathcal{R}(\delta)|$) as the severity of the edit.
In general, editing popular facts that were seen many times during training is likely to introduce more changes, and thus, editing their properties has a higher severity level.

\subsection{Evaluation Criteria}
\label{subsec:criteria}

We wish to evaluate how well models capture the ripple effects of factual edits. However, ripple effects potentially can span a large number of follow-up edits. Therefore, we focus on evaluating modified facts that are within 2-hop distance from subject or object of the original edit.
Concretely, for a single edit $\delta: (e, r, o) \rightarrow (e, r, o^*)$,
we evaluate the ripple effect $\mathcal{R}(\delta)$, via the following evaluation criteria (examples are shown in Fig.~\ref{figure:examples}): 

\begin{enumerate}
[leftmargin=*,topsep=2pt,parsep=1pt]
    \item \textbf{\logicalgeneralization{} (LG)}:
    we test if facts about the subject $e$, the original object $o$, and the new object $o^*$ that are semantically related to the modified fact and expected to change by the edit, were indeed modified. Concretely, we test whether facts $(x, r', z)$, with $x\in \{e, o, o^*\}$ and $r'$ that is semantically related to $r$, where modified consistently with respect to $\delta$. 
    For instance (Fig.~\ref{figure:examples}A), since the relation \texttt{Sibling} is symmetric, the triplet (\texttt{Prince}, \texttt{Sibling}, \texttt{Nicholas Carminowe}) directly implies that the symmetric fact (\texttt{Nicholas Carminowe}, \texttt{Sibling}, \texttt{Prince}) also holds.
    
    \item \textbf{\compositionality{} (CI)}: tests if the model can compose the edited fact with other facts about the target object $o^*$.
    Let $(o, r', z)$ and $(o^*, r', z^*)$ be two facts of the same relation about $o$ and $o^*$, respectively. Also, denote by $r''$ the complex relation expressing the composition of $r$ and $r'$, e.g. $r''=\texttt{Profession of sibling}$ for $r=\texttt{Sibling}$ and $r'=\texttt{Profession}$.  
    Then, after the edit $\delta$, we expect the following change  $(e, r'', z) \rightarrow (e, r'', z^*)$.
    As illustrated in Fig.~\ref{figure:examples}B, we ask about the profession of the siblings of \texttt{Prince}, after modifying his sibling.
    
    \item \textbf{\forwardcompositionality{} (CII)}:
    tests if the model can compose a fact about a different subject $e' \neq e$ with the edited fact.
    Formally, let $(e', r', e)$ be a fact about $e'$ with the subject $e$ as its object, and denote by $r''$ the complex relation expressing the composition of $r'$ and $r$ (see an example above). After the edit $\delta$, the following change is expected for the subject $e'$: $(e', r'', o) \rightarrow (e', r'', o^*)$. 
    As shown in Fig.~\ref{figure:examples}C, after modifying his sibling, we alias \texttt{Prince} as the founder of \texttt{Paisley Park Records}, and inquire about the \texttt{Sibling of Founder} of \texttt{Paisley Park Records}.
    
    \item \textbf{\aliasing{} (SA)}: tests that the edit to the fact about $e$ was also applied to any other entity $e'$ that is an alias for $e$, namely, $(e', r, o) \rightarrow (e', r, o^*)$. 
    For instance, as in Fig.~\ref{figure:examples}D, after modifying the sibling of \texttt{Prince}, we verify whether his sibling was also modified for his alias, \texttt{Prince Roger Nelson}.
    
    \item \textbf{\forgetfulness{} (FN)}: for a one-to-many relation $r$, there could be multiple objects for a given subject. In such cases, adding a new object should not affect the other objects encoded for this subject and relation. Therefore, for an object $o' \neq o^*$ for which there exists a triplet $(e, r, o')$, we use this triplet as a test query.
    For example (Fig.~\ref{figure:examples}E), after inserting the sibling \texttt{Nicholas Carminowe} for \texttt{Prince}, we check the model retains the fact that \texttt{Tyka Nelson} is also one of his siblings.
    
    \item \textbf{\relationspecificity{} (RS)}: we test that facts about the subject $e$, with relations whose objects are not influenced by  the object of $r$, are indeed not affected by the edit.
    As an example, in Fig.~\ref{figure:examples}F, we check whether the model still correctly outputs the name of the \texttt{Mother} of \texttt{Prince}, after modifying his sibling.
\end{enumerate}

In \S\ref{sec:datacollection}, we describe how we utilize a Knowledge Graph to generate factual editing evaluations, based on the above criteria.

\subsection{Related Work}
\label{subsec:currentstate}

\paragraph{Knowledge Editing Methods}
Several methods have been proposed to edit the factual knowledge encoded in a model. 
\citet{de-cao-etal-2021-editing} and \citet{mitchell2022fast} suggested to use hyper-networks to update the model weights. 
In addition, \citet{meng2022locating, meng2022mass} proposed to modify encoded facts by updating the weights of MLP layers, following recent observations that these layers can be cast as key-value memories \cite{geva-etal-2021-transformer} that store factual knowledge \cite{dai-etal-2022-knowledge}. 
% Recently, \citet{hase2023does} showed that MLP layers modified in these works do not necessarily correlated with edit performance. 
Instead of updating the weights, other methods learn encodings that directly update the hidden representations of the models \cite{hernandez2023inspecting}, or augment the input context with retrieved edits \cite{zhong2023mquake}.
In \S\ref{subsec:experimental_settings}, we discuss state-of-the-art KE methods used in this work in greater detail.

Separately from factual knowledge editing, recent works have also studied how to inject new facts into a model. Previous methods suggested unsupervised pre-training \cite{roberts2020much, ijcai2021p552}, semi-parametric methods methods where external information is added from a knowledge-base \cite{zhang-etal-2019-ernie, peters-etal-2019-knowledge, lewis2021retrievalaugmented, zhang2022greaselm}, using adapters to store knowledge \cite{wang2020kadapter}, or more recently directly updating FFN layers \cite{yao2022kformer}.

\paragraph{Knowledge Editing Evaluation}
Recently, there has been a growing interest in KE evaluation.
The main benchmarks currently used to evaluate KE methods are the Zero-Shot Relation Extraction
(zsRE) \cite{levy-etal-2017-zero, de-cao-etal-2021-editing} and CounterFact \cite{meng2022locating}. zsRE is a question-answering dataset for relation-specific queries, which includes human generated paraphrases that can be used to measure  robustness to semantically equivalent inputs. For example, for the triplet  (\texttt{x}, \texttt{Country}, \texttt{y}), zsRE contains queries such as ``\emph{In which country is x?}''. CounterFact offers a more challenging setting, where the edited facts are counterfactuals, that are assigned a low probability by the LLM, such as editing the \texttt{City} of \texttt{The Louvre} from \texttt{Paris} to \texttt{Rome}.

Evaluation in both zsRE and CounterFact primarily focuses on three  important aspects of (a) \textit{efficacy}, that is, the model generates the target object after the edit, (b) \textit{paraphrasing}, testing if the model is robust in generating the target for paraphrases of the input, and (c) \textit{specificity}, i.e., facts that are not related to the edit are unaffected.
In addition to \textit{efficacy} and \textit{specificity}, CounterFact measures the generation quality of the edited model when prompted with the edit's subject on two additional aspects: \textit{consistency} measures the similarity with subjects that share the same property as the edited object, and \textit{fluency} measures repetitiveness of the generated text.

Recently, \citet{onoe2023lms} introduce the task of \emph{entity knowledge propagation}, aiming to examine the extent to which models are able to reason about emergent entities that did not appear in pre-training. In addition, \citet{hoelscherobermaier2023detecting} show that existing KE methods can have unwanted side effects and suffer from low specificity.
\citet{gupta2023editing} focus on editing commonsense knowledge and introduce \texttt{MEMIT-CSKPROBE}, a dataset for semantic generalization of commonsense edits.
A concurrent work by \citet{zhong2023mquake} introduce MQUAKE, a benchmark that tests the ability of models to perform multi-hop reasoning after edits. While each of these benchmarks focuses on a single consequence of editing, \ripple{} enables comprehensive evaluation across six different evaluation criteria. For a detailed review of KE in LLMs see \citet{yao2023editing}.
