% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{hase2023does,
      title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models}, 
      author={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},
      year={2023},
      eprint={2301.04213},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gupta2023editing,
 archiveprefix = {arXiv},
 author = {Anshita Gupta and Debanjan Mondal and Akshay Krishna Sheshadri and Wenlong Zhao and Xiang Lorraine Li and Sarah Wiegreffe and Niket Tandon},
 eprint = {2305.14956},
 primaryclass = {cs.CL},
 title = {Editing Commonsense Knowledge in GPT},
 year = {2023}
}

@inproceedings{zhang-etal-2019-ernie,
 address = {Florence, Italy},
 author = {Zhang, Zhengyan  and
Han, Xu  and
Liu, Zhiyuan  and
Jiang, Xin  and
Sun, Maosong  and
Liu, Qun},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1139},
 pages = {1441--1451},
 publisher = {Association for Computational Linguistics},
 title = {{ERNIE}: Enhanced Language Representation with Informative Entities},
 url = {https://aclanthology.org/P19-1139},
 year = {2019}
}

@inproceedings{peters-etal-2019-knowledge,
 address = {Hong Kong, China},
 author = {Peters, Matthew E.  and
Neumann, Mark  and
Logan, Robert  and
Schwartz, Roy  and
Joshi, Vidur  and
Singh, Sameer  and
Smith, Noah A.},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1005},
 pages = {43--54},
 publisher = {Association for Computational Linguistics},
 title = {Knowledge Enhanced Contextual Word Representations},
 url = {https://aclanthology.org/D19-1005},
 year = {2019}
}

@inproceedings{zhang2022greaselm,
 author = {Xikun Zhang and
Antoine Bosselut and
Michihiro Yasunaga and
Hongyu Ren and
Percy Liang and
Christopher D. Manning and
Jure Leskovec},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/0001BYRLML22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {GreaseLM: Graph REASoning Enhanced Language Models},
 url = {https://openreview.net/forum?id=41e9o6cQPj},
 year = {2022}
}

@inproceedings{lewis2021retrievalaugmented,
 author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
 year = {2020}
}

@inproceedings{ijcai2021p552,
 author = {Zhang, Ningyu and Deng, Shumin and Cheng, Xu and Chen, Xi and Zhang, Yichi and Zhang, Wei and Chen, Huajun},
 booktitle = {Proceedings of the Thirtieth International Joint Conference on
Artificial Intelligence, {IJCAI-21}},
 doi = {10.24963/ijcai.2021/552},
 editor = {Zhi-Hua Zhou},
 note = {Main Track},
 pages = {4007--4014},
 publisher = {International Joint Conferences on Artificial Intelligence Organization},
 title = {Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining},
 url = {https://doi.org/10.24963/ijcai.2021/552},
 year = {2021}
}

@inproceedings{wang2020kadapter,
 address = {Online},
 author = {Wang, Ruize  and
Tang, Duyu  and
Duan, Nan  and
Wei, Zhongyu  and
Huang, Xuanjing  and
Ji, Jianshu  and
Cao, Guihong  and
Jiang, Daxin  and
Zhou, Ming},
 booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
 doi = {10.18653/v1/2021.findings-acl.121},
 pages = {1405--1418},
 publisher = {Association for Computational Linguistics},
 title = {{K-Adapter}: {I}nfusing {K}nowledge into {P}re-{T}rained {M}odels with {A}dapters},
 url = {https://aclanthology.org/2021.findings-acl.121},
 year = {2021}
}

@InProceedings{yao2022kformer,
author="Yao, Yunzhi
and Huang, Shaohan
and Dong, Li
and Wei, Furu
and Chen, Huajun
and Zhang, Ningyu",
editor="Lu, Wei
and Huang, Shujian
and Hong, Yu
and Zhou, Xiabing",
title="Kformer: Knowledge Injection in Transformer Feed-Forward Layers",
booktitle="Natural Language Processing and Chinese Computing",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="131--143",
abstract="Recent days have witnessed a diverse set of knowledge injection models for pre-trained language models (PTMs); however, most previous studies neglect the PTMs' own ability with quantities of implicit knowledge stored in parameters. A recent study [2] has observed knowledge neurons in the Feed Forward Network (FFN), which are responsible for expressing factual knowledge. In this work, we propose a simple model, Kformer, which takes advantage of the knowledge stored in PTMs and external knowledge via knowledge injection in Transformer FFN layers. Empirically results on two knowledge-intensive tasks, commonsense reasoning (i.e., SocialIQA) and medical question answering (i.e., MedQA-USMLE), demonstrate that Kformer can yield better performance than other knowledge injection technologies such as concatenation or attention-based injection. We think the proposed simple model and empirical findings may be helpful for the community to develop more powerful knowledge injection methods{\$}{\$}^{\{}1{\}}{\$}{\$}1(Code available in https://github.com/zjunlp/Kformer).",
isbn="978-3-031-17120-8"
}

@misc{hernandez2023inspecting,
 archiveprefix = {arXiv},
 author = {Evan Hernandez and Belinda Z. Li and Jacob Andreas},
 eprint = {2304.00740},
 primaryclass = {cs.CL},
 title = {Inspecting and Editing Knowledge Representations in Language Models},
 year = {2023}
}

@inproceedings{jang2023temporalwiki,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Jang, Joel  and
Ye, Seonghyeon  and
Lee, Changho  and
Yang, Sohee  and
Shin, Joongbo  and
Han, Janghoon  and
Kim, Gyeonghun  and
Seo, Minjoon},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {6237--6250},
 publisher = {Association for Computational Linguistics},
 title = {{T}emporal{W}iki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models},
 url = {https://aclanthology.org/2022.emnlp-main.418},
 year = {2022}
}

@inproceedings{dai-etal-2022-knowledge,
 address = {Dublin, Ireland},
 author = {Dai, Damai  and
Dong, Li  and
Hao, Yaru  and
Sui, Zhifang  and
Chang, Baobao  and
Wei, Furu},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.581},
 pages = {8493--8502},
 publisher = {Association for Computational Linguistics},
 title = {Knowledge Neurons in Pretrained Transformers},
 url = {https://aclanthology.org/2022.acl-long.581},
 year = {2022}
}

@inproceedings{geva-etal-2021-transformer,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Geva, Mor  and
Schuster, Roei  and
Berant, Jonathan  and
Levy, Omer},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.446},
 pages = {5484--5495},
 publisher = {Association for Computational Linguistics},
 title = {Transformer Feed-Forward Layers Are Key-Value Memories},
 url = {https://aclanthology.org/2021.emnlp-main.446},
 year = {2021}
}

@inproceedings{roberts-etal-2020-much,
 address = {Online},
 author = {Roberts, Adam  and
Raffel, Colin  and
Shazeer, Noam},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.437},
 pages = {5418--5426},
 publisher = {Association for Computational Linguistics},
 title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
 url = {https://aclanthology.org/2020.emnlp-main.437},
 year = {2020}
}

@misc{yao2023editing,
 archiveprefix = {arXiv},
 author = {Yunzhi Yao and Peng Wang and Bozhong Tian and Siyuan Cheng and Zhoubo Li and Shumin Deng and Huajun Chen and Ningyu Zhang},
 eprint = {2305.13172},
 primaryclass = {cs.CL},
 title = {Editing Large Language Models: Problems, Methods, and Opportunities},
 year = {2023}
}

@misc{zhong2023mquake,
 archiveprefix = {arXiv},
 author = {Zexuan Zhong and Zhengxuan Wu and Christopher D. Manning and Christopher Potts and Danqi Chen},
 eprint = {2305.14795},
 primaryclass = {cs.CL},
 title = {MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions},
 year = {2023}
}

@inproceedings{hoelscherobermaier2023detecting,
    title = "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
    author = "Hoelscher-Obermaier, Jason  and
      Persson, Julia  and
      Kran, Esben  and
      Konstas, Ioannis  and
      Barez, Fazl",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.733",
    pages = "11548--11559",
    abstract = "Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.",
}

@inproceedings{onoe2023lms,
    title = "Can {LM}s Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
    author = "Onoe, Yasumasa  and
      Zhang, Michael  and
      Padmanabhan, Shankar  and
      Durrett, Greg  and
      Choi, Eunsol",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.300",
    pages = "5469--5485",
    abstract = "Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs{'} abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM{'}s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.",
}

@inproceedings{mitchell2022memorybased,
 author = {Eric Mitchell and
Charles Lin and
Antoine Bosselut and
Christopher D. Manning and
Chelsea Finn},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/MitchellLBMF22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {15817--15831},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Memory-Based Model Editing at Scale},
 url = {https://proceedings.mlr.press/v162/mitchell22a.html},
 volume = {162},
 year = {2022}
}

@inproceedings{mitchell2022fast,
 author = {Eric Mitchell and
Charles Lin and
Antoine Bosselut and
Chelsea Finn and
Christopher D. Manning},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/MitchellLBFM22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Fast Model Editing at Scale},
 url = {https://openreview.net/forum?id=0DcZxeWfOPt},
 year = {2022}
}

@inproceedings{de-cao-etal-2021-editing,
 address = {Online and Punta Cana, Dominican Republic},
 author = {De Cao, Nicola  and
Aziz, Wilker  and
Titov, Ivan},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.522},
 pages = {6491--6506},
 publisher = {Association for Computational Linguistics},
 title = {Editing Factual Knowledge in Language Models},
 url = {https://aclanthology.org/2021.emnlp-main.522},
 year = {2021}
}

@inproceedings{levy-etal-2017-zero,
 address = {Vancouver, Canada},
 author = {Levy, Omer  and
Seo, Minjoon  and
Choi, Eunsol  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)},
 doi = {10.18653/v1/K17-1034},
 pages = {333--342},
 publisher = {Association for Computational Linguistics},
 title = {Zero-Shot Relation Extraction via Reading Comprehension},
 url = {https://aclanthology.org/K17-1034},
 year = {2017}
}

@inproceedings{cohen-etal-2023-crawling,
 address = {Dubrovnik, Croatia},
 author = {Cohen, Roi  and
Geva, Mor  and
Berant, Jonathan  and
Globerson, Amir},
 booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
 pages = {1856--1869},
 publisher = {Association for Computational Linguistics},
 title = {Crawling The Internal Knowledge-Base of Language Models},
 url = {https://aclanthology.org/2023.findings-eacl.139},
 year = {2023}
}

@misc{bommasani2022opportunities,
 archiveprefix = {arXiv},
 author = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
 eprint = {2108.07258},
 primaryclass = {cs.LG},
 title = {On the Opportunities and Risks of Foundation Models},
 year = {2022}
}

@misc{chowdhery2022palm,
 archiveprefix = {arXiv},
 author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
 eprint = {2204.02311},
 primaryclass = {cs.CL},
 title = {PaLM: Scaling Language Modeling with Pathways},
 year = {2022}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 year = {2020}
}

@inproceedings{petroni2019language,
 address = {Hong Kong, China},
 author = {Petroni, Fabio  and
Rockt{\"a}schel, Tim  and
Riedel, Sebastian  and
Lewis, Patrick  and
Bakhtin, Anton  and
Wu, Yuxiang  and
Miller, Alexander},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1250},
 pages = {2463--2473},
 publisher = {Association for Computational Linguistics},
 title = {Language Models as Knowledge Bases?},
 url = {https://aclanthology.org/D19-1250},
 year = {2019}
}

@inproceedings{roberts2020much,
 address = {Online},
 author = {Roberts, Adam  and
Raffel, Colin  and
Shazeer, Noam},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.437},
 pages = {5418--5426},
 publisher = {Association for Computational Linguistics},
 title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
 url = {https://aclanthology.org/2020.emnlp-main.437},
 year = {2020}
}

@inproceedings{shin2020autoprompt,
 address = {Online},
 author = {Shin, Taylor  and
Razeghi, Yasaman  and
Logan IV, Robert L.  and
Wallace, Eric  and
Singh, Sameer},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.346},
 pages = {4222--4235},
 publisher = {Association for Computational Linguistics},
 title = {{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts},
 url = {https://aclanthology.org/2020.emnlp-main.346},
 year = {2020}
}

@article{elazar2021measuring,
 address = {Cambridge, MA},
 author = {Elazar, Yanai  and
Kassner, Nora  and
Ravfogel, Shauli  and
Ravichander, Abhilasha  and
Hovy, Eduard  and
Sch{\"u}tze, Hinrich  and
Goldberg, Yoav},
 doi = {10.1162/tacl_a_00410},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {1012--1031},
 publisher = {MIT Press},
 title = {Measuring and Improving Consistency in Pretrained Language Models},
 url = {https://aclanthology.org/2021.tacl-1.60},
 volume = {9},
 year = {2021}
}

@article{razniewski2021language,
 author = {Razniewski, Simon and Yates, Andrew and Kassner, Nora and Weikum, Gerhard},
 journal = {ArXiv preprint},
 title = {Language models as or for knowledge bases},
 url = {https://arxiv.org/abs/2110.04888},
 volume = {abs/2110.04888},
 year = {2021}
}

@inproceedings{heinzerling2020language,
 address = {Online},
 author = {Heinzerling, Benjamin  and
Inui, Kentaro},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 doi = {10.18653/v1/2021.eacl-main.153},
 pages = {1772--1791},
 publisher = {Association for Computational Linguistics},
 title = {Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries},
 url = {https://aclanthology.org/2021.eacl-main.153},
 year = {2021}
}

@article{kadavath2022language,
 author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and others},
 journal = {ArXiv preprint},
 title = {Language models (mostly) know what they know},
 url = {https://arxiv.org/abs/2207.05221},
 volume = {abs/2207.05221},
 year = {2022}
}

@inproceedings{cohen2023crawling,
 address = {Dubrovnik, Croatia},
 author = {Cohen, Roi  and
Geva, Mor  and
Berant, Jonathan  and
Globerson, Amir},
 booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
 pages = {1856--1869},
 publisher = {Association for Computational Linguistics},
 title = {Crawling The Internal Knowledge-Base of Language Models},
 url = {https://aclanthology.org/2023.findings-eacl.139},
 year = {2023}
}

@article{dhingra2022time,
 address = {Cambridge, MA},
 author = {Dhingra, Bhuwan  and
Cole, Jeremy R.  and
Eisenschlos, Julian Martin  and
Gillick, Daniel  and
Eisenstein, Jacob  and
Cohen, William W.},
 doi = {10.1162/tacl_a_00459},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {257--273},
 publisher = {MIT Press},
 title = {Time-Aware Language Models as Temporal Knowledge Bases},
 url = {https://aclanthology.org/2022.tacl-1.15},
 volume = {10},
 year = {2022}
}

@inproceedings{lazaridou2021mind,
 author = {Angeliki Lazaridou and
Adhiguna Kuncoro and
Elena Gribovskaya and
Devang Agrawal and
Adam Liska and
Tayfun Terzi and
Mai Gimenez and
Cyprien de Masson d'Autume and
Tom{\'{a}}s Kocisk{\'{y}} and
Sebastian Ruder and
Dani Yogatama and
Kris Cao and
Susannah Young and
Phil Blunsom},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LazaridouKGALTG21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {29348--29363},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Mind the Gap: Assessing Temporal Generalization in Neural Language
Models},
 url = {https://proceedings.neurips.cc/paper/2021/hash/f5bf0ba0a17ef18f9607774722f5698c-Abstract.html},
 year = {2021}
}

@inproceedings{jang2021towards,
 author = {Joel Jang and
Seonghyeon Ye and
Sohee Yang and
Joongbo Shin and
Janghoon Han and
Gyeonghun Kim and
Stanley Jungkyu Choi and
Minjoon Seo},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/JangYYSHKCS22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Towards Continual Knowledge Learning of Language Models},
 url = {https://openreview.net/forum?id=vfsRB5MImo9},
 year = {2022}
}

@article{meng2022locating,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 journal = {Advances in Neural Information Processing Systems},
 pages = {17359--17372},
 title = {Locating and editing factual associations in GPT},
 volume = {35},
 year = {2022}
}

@inproceedings{mitchell2021fast,
 author = {Eric Mitchell and
Charles Lin and
Antoine Bosselut and
Chelsea Finn and
Christopher D. Manning},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/MitchellLBFM22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Fast Model Editing at Scale},
 url = {https://openreview.net/forum?id=0DcZxeWfOPt},
 year = {2022}
}

@inproceedings{
si2022prompting,
title={Prompting {GPT}-3 To Be Reliable},
author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Lee Boyd-Graber and Lijuan Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=98p5x51L5af}
}

@inproceedings{
meng2022mass,
title={Mass-Editing Memory in a Transformer},
author={Kevin Meng and Arnab Sen Sharma and Alex J Andonian and Yonatan Belinkov and David Bau},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=MkbcAHIYgyS}
}

@article{hernandez2023measuring,
 author = {Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
 journal = {ArXiv preprint},
 title = {Measuring and Manipulating Knowledge Representations in Language Models},
 url = {https://arxiv.org/abs/2304.00740},
 volume = {abs/2304.00740},
 year = {2023}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
 journal = {OpenAI blog},
 number = {8},
 pages = {9},
 title = {Language models are unsupervised multitask learners},
 volume = {1},
 year = {2019}
}

@article{chen2021evaluating,
 author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
 journal = {ArXiv preprint},
 title = {Evaluating large language models trained on code},
 url = {https://arxiv.org/abs/2107.03374},
 volume = {abs/2107.03374},
 year = {2021}
}

@inproceedings{black2022gpt,
 address = {virtual+Dublin},
 author = {Black, Sidney  and
Biderman, Stella  and
Hallahan, Eric  and
Anthony, Quentin  and
Gao, Leo  and
Golding, Laurence  and
He, Horace  and
Leahy, Connor  and
McDonell, Kyle  and
Phang, Jason  and
Pieler, Michael  and
Prashanth, Usvsn Sai  and
Purohit, Shivanshu  and
Reynolds, Laria  and
Tow, Jonathan  and
Wang, Ben  and
Weinbach, Samuel},
 booktitle = {Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models},
 doi = {10.18653/v1/2022.bigscience-1.9},
 pages = {95--136},
 publisher = {Association for Computational Linguistics},
 title = {{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model},
 url = {https://aclanthology.org/2022.bigscience-1.9},
 year = {2022}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {ArXiv preprint},
 title = {Llama: Open and efficient foundation language models},
 url = {https://arxiv.org/abs/2302.13971},
 volume = {abs/2302.13971},
 year = {2023}
}

@inproceedings{brown2020language,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 year = {2020}
}

@misc{openai2023gpt4,
 archiveprefix = {arXiv},
 author = {OpenAI},
 eprint = {2303.08774},
 primaryclass = {cs.CL},
 title = {GPT-4 Technical Report},
 year = {2023}
}

@inproceedings{mallen-etal-2023-trust,
 address = {Toronto, Canada},
 author = {Mallen, Alex  and
Asai, Akari  and
Zhong, Victor  and
Das, Rajarshi  and
Khashabi, Daniel  and
Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {9802--9822},
 publisher = {Association for Computational Linguistics},
 title = {When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
 url = {https://aclanthology.org/2023.acl-long.546},
 year = {2023}
}


@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{dong2022survey,
  title={A Survey for In-context Learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{geva2023dissecting,
  title={Dissecting recall of factual associations in auto-regressive language models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  journal={arXiv preprint arXiv:2304.14767},
  year={2023}
}