

% Figure environment removed


\section{Introduction}

Modern language models (LMs) capture a large volume of factual knowledge in their parameters, and this can be effectively utilized in downstream tasks \cite{petroni2019language, roberts2020much, shin2020autoprompt, razniewski2021language, heinzerling2020language, kadavath2022language, cohen2023crawling}.
However, factual beliefs captured by the model may be incorrect or become outdated over time, potentially affecting the performance on the model on downstream tasks, its reliability and its usability \cite{dhingra2022time, lazaridou2021mind, jang2021towards}.


This limitation has prompted research on knowledge editing (KE) methods, which modify LMs in order to fix their factual errors (we provide a formal definition in \S\ref{sec:setting}).
Concretely, knowledge editing work has focused on injecting factual updates to LMs. Given an entity-relation-object triplet $(e,r,o)$ representing a fact (e.g. \textit{``The Eiffel Tower is in the city of Paris''}), recent work proposed various methods \cite{mitchell2021fast, meng2022locating, meng2022mass, hernandez2023measuring, si2022prompting} to inject this fact to a given LM, while ``overriding'' previous beliefs the model might have on $e$ and $r$ (e.g. that the Eiffel Tower is in London).

A key question with KE is how to evaluate the success of such editing operations. The most basic ``sanity-check'' is that the model correctly completes  $(e,r,?)$, as well as other paraphrases of this task, with $o$. However, this is not enough as an evaluation, since one needs to check that the model did not distort other facts. Indeed, the standard evaluation protocol \cite{mitchell2022fast, meng2022locating, meng2022mass}, for KE focuses on these two aspects of correctly completing various paraphrases of the new fact, and ensuring that other unrelated facts haven't been changed.

In this work, we argue that to evaluate model edits, one should go beyond the single fact that was edited and check that other facts that are logically derived from the edit were also changed accordingly. For example, if $z$ is the mother of $e$, then the children of $z$ are the siblings of $e$. Consequently, once we modify the belief of a certain model that $z \rightarrow z'$ is the mother of $e$, then we should also ensure that the model's belief regarding the siblings of $e$ is also correct. 
Fig.~\ref{figure:intro} illustrates another example, where editing the \texttt{City} in which the \texttt{Eiffel Tower} is located, modifies other related facts, such as its country, while other facts should be retained.
We refer to such changes that are implied by a factual edit as \textit{``ripple effects''}.

To account for ripple effects in the evaluation of factual edits, we propose six concrete evaluation criteria (\S\ref{sec:rethinking}, Fig.~\ref{figure:examples}), for testing which facts other than the edit itself should be modified or retained post-editing. 
Our tests allow to evaluate how well the model integrates the edit with the rest of its knowledge, through queries that involve logical reasoning, complex composition of facts with the edit as an intermediate step, and specificity across relations. 

Building upon these criteria, we create \ripple{}, a new benchmark for comprehensive evaluation of KE of LMs (see \S\ref{sec:rippledit}).
\ripple{} includes $5$K entries, each consisting of a factual edit, along with a set of test queries that check if the edit was successful in terms of its ripple effect.
Moreover \ripple{} contains meta-data for each edit, including information about the timestamp of the edit (i.e., recent vs old), and the frequency of the entities (i.e., head vs tail).

We use \ripple{} to evaluate three popular editing methods on five recent LMs (see \S\ref{sec:experiments}). We find that, even though current KE methods are effective in modifying a particular fact, they often fail to capture the ripple effects entailed by that fact, and demonstrate poor performance on most of our evaluation criteria.
In addition, we consider a simple in-context editing baseline for KE, that leverages the casual attention mechanism rather than explicit updates the model parameters. 
While this method achieves improved results on our benchmark, outperforming current parametric KE methods, there is still ample room for improvement that calls for future research. 
Further, we analyze how editing performance varies across different model sizes and entity frequencies, finding that (a) larger models handle ripple effects better, and (b) success on \ripple{} is influenced by entity frequency, where editing frequent entities results in more logical reasoning errors.

To conclude, our work makes the following contributions: (a) it highlights key limitations of KE evaluation, specifically regarding ``ripple effects'', (b) it introduces comprehensive evaluation criteria that aim to mitigate those limitations, (c) it proposes \ripple{}, a benchmark inspired by these criteria, (d) it evaluates current methods for KE and shows that they do not perform well on this task, while demonstrating that in-context editing is a promising direction for KE. 
We release \ripple{} and our code to facilitate future work on KE. 
