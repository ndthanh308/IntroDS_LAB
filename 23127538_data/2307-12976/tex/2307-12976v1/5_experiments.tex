
\section{Experiments}
\label{sec:experiments}

We use \ripple{} to evaluate recent KE methods on multiple LMs, and show that despite  substantial progress on existing benchmarks, current KE methods struggle to introduce consistent changes to the model's knowledge after an edit. Moreover, a simple in-context editing baseline where generation is conditioned on the edited facts is more consistent, leaves ample room for improvement for future methods.

% Figure environment removed


\subsection{Evaluation Setting}
\label{subsec:experimental_settings}

\paragraph{Data}
\ripple{} is meant to be used as a diagnostic dataset to evaluate the ripple effects resulting from an editing operation. Therefore, to evaluate the performance of an editing method on a given model, the data first needs to be adjusted such that (a) only cases of successful edits are evaluated, and (b) only test queries that the model answered correctly pre-editing are used for evaluation. 

Concretely, for a given editing method $\mathcal{F}$, a model $\mathcal{M}$, an edit request $x: (e,r,o) \rightarrow (e,r,o')$, is included in the evaluation if the following conditions are met when applying $\mathcal{F}$ to $\mathcal{M}$ and $x$: (a) $\mathcal{M}$ successfully generates $o'$ when queried about $e$ and $r$, namely, the edits has successfully been applied, and (b) $\mathcal{M}$ successfully generates the correct objects for queries corresponding to the tests before applying the edit. 
For example, we check that the model can predict the children of $o'$ before asking about $e$'s new \texttt{siblings}, and that it predicts the mother of $o'$ before asking about the new \texttt{maternal grandmother} of $e$.

\paragraph{Editing methods} 
We evaluate three KE methods:
MEND \cite{mitchell2022fast}, ROME \cite{meng2022locating}, and MEMIT \cite{meng2022mass}. MEND trains a network that modifies gradients to produce local edits when presented with a desirable input-output pair. ROME rank-one updates to the weights of the Transformer's MLP layers to update specific factual associations. MEMIT is an extension of ROME that is also capable of editing many facts at once.

\begin{table}[t]
\setlength\tabcolsep{4pt}
    \centering
    \footnotesize
    \begin{tabular}{lllccrr}
    & \multicolumn{2}{c}{\recentlyemerged{}} & \multicolumn{2}{c}{\fakefacts{}} & \multicolumn{2}{c}{\topviews{}} \\ 
    & Edits & Tests & Edits & Tests & Edits & Tests \\ \midrule
        \gptxl{}
         &$853$ &$29\%$ &$689$ &$33\%$ &$722$ &$71\%$  \\ 
         \gptj{}
         &$801$ &$33\%$ &$717$ &$34\%$ &$760$ &$76\%$  \\ 
         \gptneo{} 
         &$989$ &$45\%$ &$801$ &$46\%$ &$828$ &$86\%$ \\ 
         \llama{} 
         &$847$ &$44\%$ &$796$ &$49\%$ &$784$ &$87\%$   \\ 
         \gptt{} 
         &$822$ &$55\%$ &$760$ &$74\%$ &$665$ &$94\%$  \\
         \bottomrule 
    \end{tabular}
    \caption{(a) Number of edits we considered in our evaluation (that is, edits that have successfully occurred), from each of the data types, averaged over \rome{}, \memit{} and \mend{}, for the models: \gptxl{}, \gptj{}, \gptneo{} and \llama{}, and using the ICE baseline for \gptt{}. (b) Portion of queries, on average, that have been considered during our evaluation, namely that their conditions have been met.}
\label{table:filtered_tests_portion}
\end{table}

\paragraph{Baseline} Motivated by the recent success of LMs to learn in-context and follow instructions \cite{NEURIPS2020_1457c0d6, ouyang2022training, liu2023pre}, we propose an in-context editing (ICE) baseline for factual editing. Unlike the above methods, it does not introduce changes to the model parameters, but rather generation is conditioned on the new fact. 
Concretely, given an edit $(e, r, o) \rightarrow (e, r, o^*)$ and a test query $q$, we use the following prompt to obtain an answer from the model: \texttt{``Imagine that <$o^*$> would have been <$P_r$>''}, where $P_r$ is a particular manual-phrased proposition corresponding to $r$, such as \emph{``The mother of <$e$>''} when $r=$ \texttt{Mother} and $e$ is the subject.
% (similar process as in \S\ref{paragraph:phrasing_in_nl}).
An example is illustrated in Fig.~\ref{figure:ice_demonstration}.

\input{tables/results_real}
\input{tables/results_fake}
\input{tables/results_fakepopular}

\paragraph{Models} 
We use 4 recent auto-regressive decoder-only LMs of different sizes: GPT-2 XL \cite{radford2019language} with 
% 345M (GPT2-M), 762M (GPT2-L), and 1,542M
1.5B parameters,
GPT-J \cite{chen2021evaluating} with 6B parameters, LLaMA with 7B parameters, \cite{touvron2023llama}, and GPT-NeoX with 20B parameters \cite{black2022gpt}.
In addition, as our baseline does not require access to the model parameters, we also evaluate it on the closed-source models GPT-3 \texttt{text-davinci-003} with 175B parameters \cite{brown2020language}. 

For all model-method combinations, except for \rome{} with \llama{}, we use the official implementation and hyperparameters from \citet{meng2022locating}. 
% We use the implementation and hyperparameters by \citet{meng2022locating} to edit \gptneo{} with ROME, \gptj{} with ROME and MEMIT, and \gptxl{} with all the 3 methods. 
Also, we adjust ROME to \llama{} by following the method and utilizing the authors' codebase.
Tab.~\ref{table:filtered_tests_portion} shows the number of edits and test queries left, for every model, after filtering out non-successful edits and inapplicable test queries (as described above).

\paragraph{Evaluation} Each model-method pair is evaluated separately, on every subset of \ripple{}. For each evaluation criteria, we first compute the average accuracy over the test queries per example, and then average over all the examples. For every test query, we let the model generate a maximum of 20 token. We consider a generation as successful if one of the target object's aliases appears in the text. In cases of multiple gold target objects (as in \forgetfulness{} test queries), we evaluate each target object separately and consider the generation as correct if the generation was correct with respect to at least one object.

\subsection{Results}
Tab.~\ref{table:recently_emerged_results},~\ref{table:fake_facts_results},~\ref{table:top_views_results} show the evaluation results on the \recentlyemerged{}, \fakefacts{}, and \topviews{} subsets, respectively.
Considering the average scores across all subsets, we observe that existing editing methods struggle to handle the ripple effect induced by editing facts, with low average accuracy of $38-66$ across all models.
This suggests that, while KE methods demonstrate high capability in making local updates to the model's knowledge, these changes are mostly applied at a surface-level without propagating to other related facts.

Moreover, comparing results across test criteria shows that some \ripple{} criteria are handled better than others. For example, while results for the \aliasing{} criteria that measures generalization to paraphrases are high (86.8 or higher across all settings), results for the other criteria are lower and vary between models, methods, and splits (e.g for the \logicalgeneralization{} criteria, results are at a very low $5.5$ on the \topviews{} split with \gptj{} and \rome{}, but are much higher at $71.1$ on the \fakefacts{} split with \llama{} and in-context editing).
Next, we analyze our results across the different dimensions, to reveal fine-grained insights on when current KE methods succeed and fail.

Importantly, we observe that our in-context editing baseline obtains the best overall results. Specifically, ICE outperforms \rome{} by more than 10 points for \gptneo{} and 30 points for \llama{}, on average. Although \gptt{} with ICE performs best on average, the 7B \llama{} is highly competitive, performing better or similarly on the \recentlyemerged{} and \topviews{} splits.

% Figure environment removed

\paragraph{Results across model size}
We analyze how editing performance on \ripple{} is influenced by the model size. To this end, we further evaluate \rome{} on smaller versions of \gpt{} -- with 345M (GPT2-M) and 762M (GPT2-L) parameters, and plot the average accuracy over the three subsets of \ripple{} as a function of model size.
Fig.~\ref{figure:acc_as_a_function_of_size} presents the results, showing that editing performance increases in model size, with \rome{} obtaining substantially higher accuracy when applied to larger models. 
Nevertheless, our results (Tab.~\ref{table:recently_emerged_results},~\ref{table:fake_facts_results},~\ref{table:top_views_results}) show that when using ICE, the 7B \llama{} is competitive with the much larger \gptt{}, suggesting that simply scaling the model size may not be sufficient to fix the drawbacks of current editing methods. 

\paragraph{Results across editing methods}

\begin{table}[t]
\setlength{\belowcaptionskip}{-10pt}
\footnotesize
\begin{center}
\begin{tabular}{lccc}
 & \mend{} & \rome{} & \memit{} \\ [0.1cm]
\toprule
\relationspecificity{}          & $34.4$    &$37.6$   &$39.1$  \\
\logicalgeneralization{}    & $39.1$    &$26.5$   &$29.0$  \\
\compositionality{}            & $17.0$    &$37.9$   &$35.3$   \\ 
\forwardcompositionality{}    & $13.6$    &$37.7$   &$39.1$    \\
\bottomrule
\end{tabular}
\end{center}
\caption{Accuracy of \mend{}, \rome{} and \memit{}, using \gptxl{}, averaged over the three \ripple{} splits - \recentlyemerged{}, \fakefacts{} and \topviews{}.}
\label{table:res_across_methods}
\end{table}

Tab.~\ref{table:res_across_methods} shows the results of \mend{}, \rome{} and \memit{}, on \gptxl{} across the \ripple{} evaluation criteria, while averaging over the three data subsets. Interestingly, \mend{} outperforms \rome{} and \memit{} in \logicalgeneralization{}, but is worse in \compositionality{} and \forwardcompositionality{}, suggesting that different methods might better capture different types of ripple effects.

% Figure environment removed

\paragraph{Results across data splits}
Fig.~\ref{figure:test_scores_per_split} displays results across evaluation splits and criteria. 
Splits differ in whether edited facts are counterfactual or real and in the popularity of the edited entities.
When comparing the \recentlyemerged{} split that examines injection of real recent facts to the counterfactual \fakefacts{} and \topviews{} splits, we observe that for \relationspecificity{}, performance is best on the \recentlyemerged{} split.
Comparing the \fakefacts{} and \topviews{} splits, that differ in the popularity of the edited entities, we see that while \logicalgeneralization{} is higher for \fakefacts{}, \forgetfulness{} is higher for \topviews{}. These results suggest that, although retaining correct knowledge is easier for popular entities, updating other facts that logically follow from an edit is harder for popular entities.
