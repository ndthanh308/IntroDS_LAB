This section presents the dataset, training approach, and results of using PELICAN as a classifier in the context of identifying Lorentz-boosted top quarks. Three different versions of PELICAN are discussed, each with a different size in terms both the width of the network and the number of trainable parameters. Lastly, the dependence of the performance on the size of the training dataset is also presented, providing a quantitative relationship between the size of the network, the training dataset efficiency, and the resulting performance.

%------------------------
% DATASET
%------------------------
\subsection{Classification dataset}
%\paragraph{Dataset}

We perform top-tagging on the reference dataset~\cite{KasPleThRu19}, which was also used in Ref. \cite{KasiePlehn19}. This dataset consists of 2M entries, each entry corresponding with a single hadronic top jet or the leading jet from a QCD dijet event. There are 1.2M training entries, 400k validation entries and 400k testing entries. The events were generated with \PythiaEight, and the \Delphes framework~\cite{deFavereau:2013fsa} was used for fast detector simulation in order to incorporate detector effects. For each jet, the 4-momentum of the $200$ leading constituents are stored in Cartesian coordinates $(E,p_x,p_y,p_z)$, in order of decreasing $p_T$. This list is zero-padded, and all jets in the dataset have fewer than 200 constituents. The dataset does not contain any other information on the jet constituents, such as charge or spin.

%------------------------
% TRAINING PROCEDURE
%------------------------
\subsection{Classification training procedure}
%\paragraph{Training} 

The top-tagging model contains five $\mathrm{Eq}_{2\to2}$ blocks of identical shapes. We train three different versions of the model with different widths. The widest model has $132$ input and $78$ output channels on every messaging layer (the equivariant layer then produces $132\times 15$ quantities which get mixed down to $78$ channels by a fully connected linear layer). The output MLP is just one layer that mixes $132$ channels down to $2$ classification weights. The number of jet constituents was capped at 80 (no noticeable performance gain was seen beyond that number). The dropout rate was $0.025$, and the model was optimized using the \textsc{AdamW} optimizer \cite{AdamW} with weight decay of $0.005$. The training on the full dataset went on for 35 epochs with the same learning rate schedule as in Ref.~\cite{LorentzNet22}: 4 epochs of linear warm-up up to learning rate of $0.001$, followed by 28 epochs of \textsc{CosineAnnealingLR} with $T_0$ of $4$ epochs and $T_{\mathrm{mult}}=2$, and then 3 epochs of exponentially decaying learning rate with exponent $\gamma=0.5$ per epoch. The three models were trained on Nvidia H100 GPU's with batch size of 100, taking $0.43$, $0.17$, or $0.08$ seconds per batch, respectively. Inference took $0.17$, $0.07$, or $0.04$ seconds per batch. Batches were shuffled between epochs.
%===========================================
\begin{table}[t]
    \vspace{-0.\intextsep}
    \centering
    \begin{small}
    \begin{tabular}{l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}r}
    \toprule
    Architecture    &   Accuracy    &   AUC         &   $1/\epsilon_B$  &   \# Params \\
    \midrule
    TopoDNN\cite{Pearkes:2017hku}   & 0.916 & 0.972 & 382$\pm$ 5 & 59k \\
    LGN\cite{Bogatskiy:2020tje}             &   0.929(1)    &   0.964(14)   & 424 $\pm$ 82      &   4.5k    \\
    PFN\cite{EFN}             &   0.932       &   0.982       &   891 $\pm$ 18    &   82k     \\
    ResNeXt\cite{KasiePlehn19}         &   0.936       &   0.984       &   1122 $\pm$ 47   &   1.46M   \\
    ParticleNet\cite{ParticleNet}     &   0.938       &   0.985       &   1298 $\pm$ 46   &   498k    \\
    LorentzNet\cite{LorentzNet22}      &   0.942       &   0.9868      & 2195 $\pm$ 173    &   220k    \\     
    \midrule
    $\text{PELICAN}_{132/78}$        &   0.9425(1)   &   0.9870(1)   & 2250 $\pm$ 75    &   208k     \\   
    $\text{PELICAN}_{60/35}$               &   0.9423(1)   &   0.9868(1)   & 2133 $\pm$ 148    &   48k     \\   
    $\text{PELICAN}_{25/15}$               &   0.9410(3)   &   0.9858(4)   & 1879 $\pm$ 103    &   11k     \\   
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Comparison of PELICAN models trained on different fractions of the training dataset. The subscripts indicate the width of the network: e.g.~132/78 means each \textsc{Msg} layer has 132 input and 78 output channels.\label{tab1}}
\end{table}
%===========================================
% \WFclear % see here: https://tex.stackexchange.com/a/442369

%------------------------
% RESULTS
%------------------------
\subsection{Classification results}
%\paragraph{Results} 

Figure~\ref{PELICAN-ROC} shows the \textit{receiver operating characteristic}, here represented by the background rejection as a function of the signal efficiency, for the classification performance. In Table~\ref{tab1} we compare the accuracy, area under the curve (AUC), and background rejection values at $30\%$ signal efficiency between PELICAN and a multiple existing ML top-taggers, including the previous state-of-the-art LorentzNet~\cite{LorentzNet22}. We trained three PELICAN top-taggers with layers of differing widths, with 208k, 48k, and 11k trainable parameters respectively. The results are averaged over $5$ random initialization seeds, and the uncertainties are given by the standard deviation. The large PELICAN model improves upon the LorentzNet result with a comparable number of parameters, and the medium model roughly matches LorentzNet despite having 5 times fewer parameters. Perhaps most remarkably, the small model with 11k parameters beats every pre-LorentzNet competitor despite having at least times fewer parameters, and up to 130 times fewer parameters, than other networks. 

%===========================================
% Figure environment removed
%===========================================

In addition to different model sizes, we also explore sample efficiency. Each of the three models above was trained on $0.5\%$, $1\%$ and $5\%$ of the training data and compared to the original. For these, the training went on for 70 epochs with 60 epochs of \textsc{CosineAnnealingLR} instead of 28, and 6 epochs of exponential decay instead of 3. The results can be found in Table~\ref{tab2}. Notice that at lower amounts of training data the differences in performance between models of different width become much less significant, and at $1\%$ and $0.5\%$ of training data all three models fall within each other's uncertainty ranges. These results suggest that the larger PELICAN networks are likely able to learn a greater range of more subtle features from the training data and thus benefit from seeing a larger training dataset. On the other hand, the primary features are already learnable with just a few percent of the data. In particular, with $5\%$ of the training data and only 11k learnable parameters, the $\text{PELICAN}_{25/15}$ version of the network appears to achieve the similar background rejection performance as ResNeXt, which uses 1.46M parameters learning on the full dataset.
%===========================================
\begin{table}[h]
    \centering
    \begin{small}
    \begin{tabular}{l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}r}
    \toprule
    Model    & \% training data &   Accuracy    &   AUC         &   $1/\epsilon_B$ \\
    \midrule
    $\text{PELICAN}_{132/78}$ & $100\%$  &   0.9425(1)   &   0.9870(1)   & 2250 $\pm$ 75       \\   
                              & $5\%$ &   0.9366(3)   &   0.9841(1)   & 1213 $\pm$ 79 \\   
                              & $1\%$ &   0.9316(6)   &   0.9810(5)   & 789 $\pm$ 49     \\   
                              & $0.5\%$ &   0.9289(11)   &   0.9800(5)   & 633 $\pm$ 28      \\   
    \midrule
    $\text{PELICAN}_{60/35}$  & $100\%$ &   0.9423(1)   &   0.9868(1)   & 2133 $\pm$ 148       \\ 
                              & $5\%$ &   0.9368(2)   &   0.9841(1)   & 1148 $\pm$ 49      \\   
                              & $1\%$ &   0.9323(3)   &   0.9813(4)   & 799 $\pm$ 52     \\   
                              & $0.5\%$ &   0.9289(9)   &   0.9795(5)   & 637 $\pm$ 105     \\   
    \midrule
    $\text{PELICAN}_{25/15}$  & $100\%$  &   0.9410(3)   &   0.9858(4)   & 1879 $\pm$ 103      \\   
                              & $5\%$ &   0.9361(5)   &   0.9835(2)   & 1122 $\pm$ 44      \\   
                              & $1\%$ &   0.9316(1)   &   0.9810(5)   & 798 $\pm$ 116       \\   
                              & $0.5\%$ &   0.9286(11)   &   0.9795(6)   & 615 $\pm$ 133      \\   
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Comparison of PELICAN models trained on different fractions of the training data. \label{tab2}}
\end{table}
%===========================================
