This section presents the dataset, training approach, and results of using PELICAN as a classifier in the context of identifying ``Lorentz-boosted'' (high transverse momentum) top quarks. Three different versions of PELICAN are discussed, each with a different size in terms both the width of the network and the number of trainable parameters. Lastly, the dependence of the performance on the size of the training dataset is also presented, providing a quantitative relationship between the size of the network, the training dataset efficiency, and the resulting performance.

%------------------------
% DATASET
%------------------------
\subsection{Top tagging dataset}
%\paragraph{Dataset}

We perform top-tagging on the reference dataset~\cite{KasPleThRu19}, which was also used in ref.~\cite{KasiePlehn19}. This dataset consists of 2M entries, each entry corresponding with a single hadronic top jet or the leading jet from a QCD dijet event. There are 1.2M training entries, 400k validation entries and 400k testing entries. The events were generated with the \PythiaEight event generator~\cite{Bierlich:2022pfr}, and the \Delphes framework~\cite{deFavereau:2013fsa} was used for fast detector simulation in order to incorporate detector effects. For each jet, the 4-momentum of the $200$ leading constituents are stored in Cartesian coordinates $(E,p_x,p_y,p_z)$, in order of decreasing $p_T$. This list is zero-padded, and all jets in the dataset have fewer than 200 constituents. The dataset does not contain any other information on the jet constituents, such as charge or spin.

%------------------------
% TRAINING PROCEDURE
%------------------------
\subsection{Classification training procedure}
%\paragraph{Training} 

The top-tagging model contains five $\mathrm{Eq}_{2\to2}$ blocks of identical shapes (followed by a sixth $\mathrm{Eq}_{2\to1}$ block). We train three different versions of the model with different widths. The widest model has $132$ input and $78$ output channels on every messaging layer (the equivariant layer then produces $132\times 15$ quantities which get mixed down to $78$ channels by a fully connected linear layer). The output MLP is just one layer that mixes $132$ channels down to $2$ classification weights. The number of jet constituents was capped at 80 (no noticeable performance gain was seen beyond that number). The dropout rate was $0.025$, and the model was optimized using the \textsc{AdamW} optimizer \cite{AdamW} with weight decay of $0.005$. The training on the full dataset went on for 35 epochs with the same learning rate schedule as in ref.~\cite{LorentzNet22}: 4 epochs of linear warm-up up to learning rate of $0.001$, followed by 28 epochs of \textsc{CosineAnnealingLR} with $T_0$ of $4$ epochs and $T_{\mathrm{mult}}=2$, and then 3 epochs of exponentially decaying learning rate with exponent $\gamma=0.5$ per epoch. We ensure that each minibatch contains an equal number of signal and background events, and shuffle the order of the minibatches at every epoch. The three models were trained on Nvidia H100 GPU's with batch size of 100, taking $0.43$, $0.17$, or $0.08$ seconds per batch, respectively. Inference took $0.17$, $0.07$, or $0.04$ seconds per batch. Batches were shuffled between epochs.
%===========================================
\begin{table}[t]
    \vspace{-0.\intextsep}
    \centering
    \begin{small}
    \begin{tabular}{l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}r}
    \toprule
    Architecture    &   Accuracy    &   AUC         &   $1/\epsilon_B$ ($\epsilon_S=0.3$)  &   \# Params \\
    \midrule
    TopoDNN\cite{Pearkes:2017hku}   & 0.916 & 0.972 & 382$\pm$ 5 & 59k \\
    EFN\cite{EFN}    &   0.927       &   0.979       &   729 $\pm$ 13    &   82k     \\
    LGN\cite{Bogatskiy:2020tje}             &   0.929(1)    &   0.964(14)   & 424 $\pm$ 82      &   4.5k    \\
    BIP(XGBoost)\cite{Ortner22}        &   0.929       &   0.978       &   600  $\pm$ 47   & 312       \\
    EFP\cite{EFP}             &   0.932       &   0.980       &   384             & 1k       \\
    BIP(MLP)\cite{Ortner22}        &   0.931       &   0.981       &   853  $\pm$ 68   & 4k       \\
    PFN\cite{EFN}             &   0.932       &   0.982       &   891 $\pm$ 18    &   82k     \\
    DisCo-FFS\cite{Shih22}             &   --       &   0.982       &   1249 $\pm$ 43    &   1.4k     \\
    ResNeXt\cite{KasiePlehn19}         &   0.936       &   0.984       &   1122 $\pm$ 47   &   1.46M   \\
    ParticleNet\cite{ParticleNet}     &   0.938       &   0.985       &   1298 $\pm$ 46   &   498k    \\
    ParT\cite{ParT}      &   0.940       &   0.9858      & 1602 $\pm$ 81    &   2.1M    \\
    LorentzNet\cite{LorentzNet22}      &   0.942       &   0.9868      & 2195 $\pm$ 173    &   220k    \\
    \midrule
    $\text{PELICAN}$        &   0.9426(2)   &   0.9870(1)   & 2250 $\pm$ 75    &   208k     \\ 
    $\text{PELICAN}_{\text{IRC}}$   &   0.9406(2)   &   0.9844(11)   & 1711 $\pm$ 208    &   208k   \\   
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Comparison of different classifiers trained on the full top-tagging dataset. Note that BIP(XGBoost) and EFP are not neural networks. PELICAN's metrics are averaged and the uncertainties are given by the standard deviation over 5 runs with different values of the random seed. $\text{PELICAN}_{\text{IRC}}$ is an IRC-safe modification detailed below in \secref{irc}. Note that Ref.~\cite{ParT} also includes a higher-performing model pre-trained on a large custom dataset. For the direct comparison, this table refers only to the non-pre-trained ParT model. \label{tab1}}
\end{table}

\begin{table}[t]
    \vspace{-0.\intextsep}
    \centering
    \begin{small}
    \begin{tabular}{l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}r}
    \toprule
    Depth $L$ & Width &   Accuracy    &   AUC         &   $1/\epsilon_B$  ($\epsilon_S=0.3$)  &   \# Params \\
    \midrule
      5    &  132/78  &   0.9425(1)   &   0.9870(1)   & 2250 $\pm$ 75    &   208k     \\  % 500_100p
      5    &  60/35  &   0.9424(1)   &   0.9868(1)   & 2148 $\pm$ 125    &   48k     \\   
      5    &  25/15  &   0.9410(3)   &   0.9858(4)   & 1879 $\pm$ 103    &   11k     \\   
    %  5    &  10/6  &   0.9385(3)   &   0.9848(2)   & 1471 $\pm$ 81    &   3k     \\   % 501_100p
      5    &  10/6  &   0.9386(2)   &   0.9850(1)   & 1494 $\pm$ 43    &   3k     \\   % 551_100p
      3    &  6/4  &   0.9358(7)   &   0.9835(2)   & 1145 $\pm$ 74    &   1k     \\   % 548_100p
      2    &  6/3  &   0.9336(5)   &   0.9823(3)   & 901 $\pm$ 59    &   605     \\   % 554_100p
    % 3    &  2/2 &   0.9288(21)   &   0.9799(10)   & 622 $\pm$ 72    &   339   \\   
      1    &  6/3  &  0.9291(5)   &   0.9801(4)   &  669$\pm$41     &   326     \\   % 550_100p
      %1    &  6/3  &  0.9291(9)   &   0.9796(8)   & 637 $\pm$ 56    &   294     \\   % 505_100p
      1    &  -/6    &  0.9258(8)   &   0.9780(6)   & 516 $\pm$ 52    &   248     \\   
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Comparison of PELICAN classifiers of varying shapes trained on the full top-tagging dataset. The depth $L$ is the number of equivariant $\mathrm{Eq}_{2\to 2}$ blocks, and the width consists of two numbers. E.g.~132/78 means that each messaging block takes in 132 channels and outputs 78 channels (and the reverse for the aggregation block). The input width of the output MLP matches the messaging blocks. The last model has the fully connected messaging layers disabled altogether, so only the shape of the aggregation blocks is given. See also \figref{toptag-params}. \label{tab_pelican_model_size}}
\end{table}

%===========================================
% \WFclear % see here: https://tex.stackexchange.com/a/442369

%------------------------
% RESULTS
%------------------------
\subsection{Top tagging results}
%\paragraph{Results} 

\Figref{PELICAN-ROC} shows the \textit{receiver operating characteristic}, here represented by the background rejection as a function of the signal efficiency, for the classification performance. In \tabref{tab1} we compare the accuracy, area under the curve (AUC), and background rejection values at $30\%$ signal efficiency between PELICAN and multiple existing ML top-taggers, including the previous state-of-the-art LorentzNet~\cite{LorentzNet22}. We also include two non-ML taggers: the Energy Flow Polynomials~\cite{EFP}, designed to be IRC-safe; and the Boost Invariant Polynomials~\cite{Ortner22}, designed to be partially Lorentz-invariant and used as inputs to XGBoost~\cite{XGBoost}. These two taggers stand out due to their low numbers of parameters and efficient utilization of physical constraints. We trained three PELICAN top-taggers with layers of differing widths, with 208k, 48k, and 11k trainable parameters respectively. The results are averaged over $5$ random initialization seeds, and the uncertainties are given by the standard deviation. The large PELICAN model improves upon the LorentzNet result with a comparable number of parameters, and the medium model roughly matches LorentzNet despite having 5 times fewer parameters. Perhaps most remarkably, the small model with 11k parameters beats every pre-LorentzNet competitor despite having at least several times fewer, and up to 190 times fewer, parameters than other networks. The metrics for these three and a few even smaller models are presented in \tabref{tab_pelican_model_size}, and we visualize this comparison across all models in \figref{toptag-params}.

%===========================================
% Figure environment removed
%===========================================

In addition to different model sizes, we also explore sample efficiency. Each of the three models above was trained on $0.5\%$, $1\%$ and $5\%$ of the training data and compared to the original. For these, the training went on for 70 epochs with 60 epochs of \textsc{CosineAnnealingLR} instead of 28, and 6 epochs of exponential decay instead of 3. The results can be found in \tabref{tab2}. Notice that at lower amounts of training data the differences in performance between models of different width become much less significant, and at $1\%$ and $0.5\%$ of training data all three models fall within each other's uncertainty ranges. These results suggest that the larger PELICAN networks are likely able to learn a greater range of more subtle features from the training data and thus benefit from seeing a larger training dataset. On the other hand, the primary features are already learnable with just a few percent of the data. In particular, with $5\%$ of the training data and only 11k learnable parameters, the $25/15$-channel wide version of the network achieves similar background rejection performance as ResNeXt, which uses 1.46M parameters learning on the full dataset.
%===========================================

%===========================================
% Figure environment removed
%===========================================

\begin{table}[t]
    \centering
    \begin{small}
    \begin{tabular}{l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}l@{\hspace{2mm}}r}
    \toprule
    Width    & \% training data &   Accuracy    &   AUC         &   $1/\epsilon_B\;(\epsilon_S=0.3)$ \\
    \midrule
    $132/78$ & $100\%$  &   0.9425(1)   &   0.9870(1)   & 2250 $\pm$ 75       \\   
                              & $5\%$ &   0.9368(3)   &   0.9843(1)   & 1270 $\pm$ 65 \\   % 547_5p
                              & $1\%$ &   0.9316(6)   &   0.9810(5)   & 789 $\pm$ 49     \\   
                              & $0.5\%$ &   0.9289(11)   &   0.9800(5)   & 633 $\pm$ 28      \\   
    \midrule
    $60/35$  & $100\%$ &   0.9423(1)   &   0.9868(1)   & 2133 $\pm$ 148       \\         % 546_100p
                              & $5\%$ &   0.9368(2)   &   0.9841(1)   & 1148 $\pm$ 49      \\   
                              & $1\%$ &   0.9323(3)   &   0.9813(4)   & 799 $\pm$ 52     \\   
                              & $0.5\%$ &   0.9289(9)   &   0.9795(5)   & 637 $\pm$ 105     \\   
    \midrule
    $25/15$  & $100\%$  &   0.9411(2)   &   0.9863(1)   & 1885 $\pm$ 109      \\           % 545_100p
                              & $5\%$ &   0.9360(2)   &   0.9837(1)   & 1111 $\pm$ 108      \\   
                              & $1\%$ &   0.9316(1)   &   0.9810(5)   & 798 $\pm$ 116       \\   
                              & $0.5\%$ &   0.9286(11)   &   0.9795(6)   & 615 $\pm$ 133      \\   
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Comparison of PELICAN top-tagging models of different widths (depth is always $L=5$) trained on different fractions of the training data. \label{tab2}}
\end{table}
