%========================
% LORENTZ GROUP
%========================

This section aims to establish a clear connection between the group theory that underlies the PELICAN architecture and the implementation of this approach for both classification and regression, as described in Section~\ref{architecture}. 

In general, given a symmetry group $G$ and two sets $X,Y$ on which an action of $G$ is defined, a mapping $F:X\to Y$ is called $G$\textit{-equivariant} if $F(g\cdot x)=g\cdot F(x)$ for any $x\in X$ and $g\in G$. In particular, if the action of $G$ on $Y$ happens to be trivial (i.e. $g\cdot y=y$ for all $g,y$), then $F$ is called \textit{invariant}. In relativistic physics, equivariant maps are typically represented by tensors with equivariant spacetime indices treated via Einstein notation. For instance, the electromagnetic field tensor $F^{\mu\nu}$ can be viewed as a Lorentz-equivariant mapping from covariant vector fields to contravariant ones. In this work we will be interested in tasks from particle physics that can be reduced to learning a Lorentz-equivariant map. In this section we review several aspects of Lorentz symmetry in the context of such tasks.

%------------------------
% LORENTZ SYMMETRY
%------------------------
\subsection{Lorentz symmetry and jets}


Lorentz symmetry is one of the fundamental symmetries of the Standard Model of particle physics. The full Lorentz group $\mathrm{O}(1,3)$ can be defined as the set of linear transformations of the 4-dimensional spacetime that preserve the Minkowski metric $\eta=\mathrm{diag}(1,-1,-1,-1)$. However, in this work we will restrict ourselves to the \textit{proper orthochronous} subgroup $\SO^+(1,3)$ that preserves spatial and temporal orientations. Lorentz invariance is the mathematical encapsulation of the fact that the outcomes of physical phenomena don't depend on the inertial frame of the observer. In the context of particle accelerators, this boils down to the observation that all initial and final states of a particle interaction are the same in all inertial frames. This is formally reflected in the fact that the Standard Model of particle physics is Lorentz-invariant, and therefore any model of any physically relevant processes described by the Standard Model can be as well.

Several subtle points are worth addressing before applying Lorentz symmetry to experimental tasks in jet physics. Neither the actual particle detectors nor the software simulating particle decays and their detection are Lorentz-invariant. Reasons for this include: non-invariant corrections to perturbative computations in quantum chromodynamics (QCD); non-invariance of jet clustering algorithms; practical limitations of detectors such as finite sptial and temporal resolution, as well as energy and momentum thresholds. Nevertheless, it is still valid to learn Lorentz-invariant models from data obtained this way. Firstly, QCD is globally Lorentz-invariant and boosting the \textit{entire} event does not change the outcome of the decay process. As long as inference is performed on data obtained in conditions similar to the conditions of the perturbative simulation, corrections from effects such as the running of the couplings with varying momentum scales are not a concern either. The same applies to jet clustering algorithms and finite detector resolution: as long as the data used for inference was obtained in the same reference frame as the data used for training, the inference is valid and the outputs are expected to be Lorentz-equivariant. Finally, the fact that the detector itself introduces a fixed reference frame can be fully addressed without breaking the symmetry of the model by including detector geometry among its inputs. This is discussed further in  \secref{beams}. 



%------------------------
% LORENTZ INVARIANCE
%------------------------
\subsection{Lorentz invariance}\label{LI}


The classification tasks considered in this work are exactly Lorentz invariant. The physical implications of this statement are discussed below, but may be stated mathematically in the following way: if the inputs to the network are a collection of 4-vectors (energy-momentum vectors in our case) $p_1,\ldots,p_N$, the output is $F(p_1,\ldots,p_N)$, and $\Lambda\in\SO^+(1,3)$ is a Lorentz transformation, then
\[F\left(\Lambda p_1,\ldots,\Lambda p_N\right)=F\left( p_1,\ldots, p_N\right).\]
There are multiple approaches to constructing an ML model that satisfies such a constraint. The simplest one is to hand-pick a set of invariant observables (such as particle masses, jet masses, relative masses, particle identification labels and charge) and use them as input to a generic NN architecture. 

Another approach inspired by convolutional networks is to preserve group-equivariant latent representations in the hidden layers. In this case, the neuron nonlinearity must be a Lorentz-equivariant operation, and examples of this can be found in both the Lorentz Group Network (LGN)~\cite{Bogatskiy:2020tje} and LorentzNet~\cite{LorentzNet22} architectures. Equivariance with respect to the part of the Lorentz group that fixes the proton beam axis was also used for regression problems in ref.~\cite{CPT}. As in traditional CNN's used in image processing, equivariant latent representations, as opposed to invariant ones, can regularize the network via efficient weight-sharing and improve training.

The PELICAN design adopts a slightly different approach. Given a set of 4-vector inputs $p_1,\ldots,p_N$, we compute a \textit{complete} set of Lorentz invariants on that set. For classical groups, including the Lorentz group, the space of invariants constructed out of a collection of vectors in the fundamental representation consists of functions of only the pairwise invariant dot products (using the appropriate invariant quadratic form for the given symmetry group) and of square determinants (e.g.~of 4 column-vectors for the Lorentz group)~\cite{Weyl46}. Furthermore, if the invariant is required to be symmetric in the vector inputs, then it is \textit{only} a function of the dot products (see also the discussion in ref.~\cite{Gripaios2021}). In short, all totally symmetric Lorentz invariants can be written in the following form:
\[I(p_1,\ldots,p_N)=f\left(\{p_i\cdot p_j\}_{i,j}\right).\label{eq}\]

This is the first key idea used in the PELICAN architecture. The first step performed by the input layer is the computation of the $N\times N$ array of dot products between the input 4-momenta (also known as the Gram matrix). The $N(N-1)/2$ components of the Gram matrix $\{p_i\cdot p_j\}$ cannot be independent, which is apparent from dimension counting. The physical manifold inside this high-dimensional space is defined by the set of constraints $\det M_5=0$ for \textit{every} 5-minor $M_5$ of the Gram matrix (that is, any matrix obtained from the original one by crossing out $N-5$ rows and $N-5$ columns). Moreover, a causally related set of points such as a particle jet will always satisfy $p_i\cdot p_j\geq 0$ for all $i,j$. Therefore a neural network whose input is an $N\times N$ matrix will learn the task only on this $(4N-6)$-dimensional submanifold of $\mathbb{R}^{N^2}$. The outputs of the trained model on the rest of the space will be uncontrollable and physically meaningless.

%------------------------
% PREMUTATION INVARIANCE
%------------------------
\subsection{Permutation equivariance}\label{EUA}


Particle data are often interpreted as a point cloud since there is no natural ordering on the vectors. For such problems it makes sense to use a permutation-invariant or equivariant architecture. One of the simplest approaches is called \textit{Deep Sets}~\cite{ZaKoRaPoSS17}, which has been applied to jet tagging~\cite{EFN} and even heavy-flavor tagging~\cite{ATLAS:2020jip}. The fundamental fact used in Deep Sets is that any permutation-invariant continuous mapping of inputs $x_1,\ldots,x_N$ can be written in the form $\psi\left(\sum_i\varphi(x_i)\right)$, where $\psi$ and $\varphi$ can be approximated by neural networks. 

The main limitation of permutation-invariant architectures such as Deep Sets is the difficulty of training. Since aggregation (summation over the particle index) happens only once, the Deep Sets architecture can struggle with modeling complex higher-order interactions between the particles, as shown rigorously in ref.~\cite{wagstaff22}. The network representing $\psi$ is forced to be a relatively wide fully connected network, which presents difficulties in training.

An alternative to permutation-invariant architectures is provided by permutation-\textit{equivariant} ones. Given a symmetry group $G$ (e.g.~the group of permutations), a representation $(V,\rho)$ is a tuple where $V$ is a set and $\rho:G\times V\to V$ is a map that becomes a bijection $\rho_g=\rho(g,\cdot):V\to V$ for any fixed value of the first argument, $\rho_e=\mathrm{id}$, and $\rho_{g^{-1}}=\rho_g^{-1}$. Given two representations $(V,\rho)$ and $(V',\rho')$ of a group $G$, a map $F:V\to V'$ is called equivariant if it \textit{intertwines} the two representations, that is:
\[
F(\rho_g(v))=\rho_g'\left(F(v)\right),\quad v\in V,\; g\in G.
\]
Equivariance is a key property of all convolutional networks -- for example, in CNN's the convolution operation is inherently equivariant with respect to translations (up to edge effects). 

Similarly, Graph Neural Networks (GNNs) use permutation equivariance with respect to the reordering of the rows and columns of the adjacency matrix for problems where the inputs can be naturally represented by a graph data structure. In this context, we review the standard definition of a message passing layer where the particles are treated as nodes in a graph (e.g.~a fully connected graph), and every layer of the network only updates the activation at every node. If we denote by $f_i$ the data assigned to node $i$, then the message passing layer will typically construct ``messages'' $m_{ij}=m(f_i,f_j)$ and update each node by aggregating the messages from all neighbors of that node and combining the result with the original state of the node: $f_i'=\psi(f_i, \sum_j m_{ji})$. Sometimes the graph also possesses ``edge data'' $D_{ij}$ that can be incorporated into the message-forming stage.

Message passing architectures have been successfully applied to jet tagging, most prominently in refs.~\cite{Bogatskiy:2020tje, LorentzNet22}. Closely related permutation-equivariant transformer architectures were also applied to particle physics in \cite{ParT,CPT}. However, attempts to combine message passing with Lorentz invariance reduced at the input stage as described above run into a major obstacle: the new inputs to the network consist of \textit{nothing but} edge data $d_{ij}=p_i\cdot p_j$. Traditional message passing would require a reduction of this set of inputs to a point cloud (with only one particle index), potentially restricting the set of possible higher-order interactions between the points. To avoid making these unnecessary choices, we employ the general permutation-equivariant layers suggested in refs.~\cite{Maron18, KondorPan}.

In the general setting, permutation equivariance is a constraint on mappings $F$ between arrays $T_{i_1i_2\cdots i_r}$ of any rank $r$, every index $i_k\in \{1,\ldots,N\}$ referring to a particle label, whereby permutations of the particles commute with the map:
\[F\left(\pi\circ T_{i_1i_2\cdots i_r}\right) =\pi\circ F\left(T_{i_1i_2\cdots i_s}\right), \quad \pi\in S_N.\]
Here, the action of permutations is effectively diagonal: $\pi\circ T_{i_1i_2\cdots i_r} =T_{\pi(i_1)\ldots \pi(i_r)}$. GNNs explicitly implement this constraint for rank 1 arrays (node information). A higher-order generalization of the message passing layer can be defined as
\[\text{\bf Equivariant Layer:}\quad T^{(\ell+1)}=\textsc{Agg}\circ \textsc{Msg}\left(T^{(\ell)}\right).\]
In this nomenclature, $\textsc{Msg}$ is a node-wise nonlinear map (\textit{message forming}) shared between all nodes, and $\textsc{Agg}$ is a general permutation-equivariant linear mapping (\textit{aggregation}) acting on the particle indices of $T$. Note that whether $\textsc{Msg}$ is node-wise and whether $\textsc{Agg}$ is linear is somewhat ambiguous based on how one separates the mappings into their components, which is why, in particular, the traditional formulation of message passing allows messages to be functions of pairs of nodes. In practice, our aggregation block will also involve a nonlinear activation function.

%------------------------
% PREMUTATION INVARIANCE
%------------------------
\subsection{Elementary equivariant aggregators}\label{EEA}
%\paragraph{Elementary Equivariant Aggregators}\label{EEA}

The exact structure of the equivariant aggregation layers defined above must still be specified. Since the general case is presented in refs.~\cite{Aggregators, KondorPan}, here we will only present the layers that are required for jet physics tasks. Since the input is an array of rank $2$, the main equivariant layer for us is one that transforms arrays of rank $2$ to other arrays of the same rank: $T_{ij}\mapsto T_{ij}'$. The space of all linear maps of this type turns out to be 15-dimensional. The basis elements of this space can be conveniently illustrated using binary arrays of rank $4$. There are 15 such arrays $B^{a}_{ijkl}, a=1,\ldots,15$, and the action of the equivariant layer can be written as
\[T^{\prime a}_{ij}=\sum_{k,l=1}^N B^{a}_{ijkl} T_{kl}.\label{aggregators}\]

\noindent The 15 aggregators $B^a$ may be visualized as is done in Figure~\ref{fig:aggregators} for $N=2$.
%===========================================
% Figure environment removed
%===========================================
The smallest squares represent components of the input $2\times 2$ array, and the larger $2\times 2$ squares represent components of the output array. Dots represent the non-zero components of the binary tensors $B^a$, and every component of the output tensor is the result of aggregation over all inputs marked by the dots. Output components that lack any dots are set to be a fixed constant, by default zero (the affine versions of these mappings include two such parameters: one constant for the diagonal and another for the remaining components). By ``aggregation'' we mean, in general, any symmetric function, but in practice it is usually a sum or mean. For example, the first aggregator is simply the identity map on matrices: the $ij$'th component of the output array is the result of aggregation over only the $ij$'th component of the input. The second aggregator realizes the transposition of arrays $T'_{ij}=T_{ji}$. The following three aggregators represent various ways of embedding the diagonal of the input array in an equivariant way. It is easy to see that simultaneously swapping the two rows and the two columns of the input is equivalent to doing the same to the output, which confirms equivariance. These first 5 aggregators are ``order zero'' in $N$ because they do not actually perform any aggregation. Instead, they can be thought of as permutation-equivariant skip-connections.

The second group of 8 ``order one'' aggregators aggregate over $N$ components of the input by aggregating either over rows, columns, or the diagonal, and then embedding the result into the output array in all possible equivariant ways. Finally, the last 2 aggregators are the ``order two'' aggregators that aggregate over all $N^2$ components of the input.

If we allow aggregators to be nonlinear, then they can take the following form: the binary array $B^a$ selects a subset of the components of the input array, and then a general symmetric function $S^a$ is applied to that subset:
\[T^{\prime a}_{ij}=S^a\left(\{T_{kl}\mid k,l: B^{a}_{ijkl}\neq 0\}\right).\]
In practice we define $S^a$ as the mean of its inputs followed by an additional scaling by a factor of $N^{\alpha_a}/\bar{N}^{\alpha_a}$ with learnable exponents $\alpha_a$, where $\bar{N}$ is a constant representing the typical number of input vectors expected in the dataset, provided to the model as a hyperparameter.

These equivariant layers and their generalizations have been thoroughly studied since their introduction (see e.g.~refs.~\cite{EqGNNs,Sannai, Maron19OnUniversality,Maron19ProvablyPowerful}) and applied to problems in graph learning and chemistry, e.g.~in refs.~\cite{KondorPan,Kondor2023}. However, many important questions about the universality and expressivity of these networks remain open. For example, networks that can use any $\mathrm{Eq}_{r\to s}$ layers were proven to be universal in refs.~\cite{EqGNNs, Maron19OnUniversality}, but the exact value of the maximum rank (which is 2 in PELICAN) required for universality in a given problem is not known.

%------------------------
% SEGUE TO HEP: EQUIVARIANCE & JETS
%------------------------
\subsection{Equivariance and Jet Physics}

There are several reasons for enforcing the full Lorentz symmetry in our ML models. First and foremost, it is a fundamental symmetry of the space to which the inputs belong. Lorentz transformations represent the effect of switching between different inertial frames, and most fundamental processes in physics are independent of the choice of the observer's inertial frame: if a given collection of particles consists of the products of a decay of a top quark for one observer, then the same is true for all other observers.

Nevertheless, some processes involved in generating and observing high-energy collision events break the Lorentz symmetry in some subtle ways. At the fundamental level, the running of the couplings in QCD can cause Lorentz symmetry breaking in the parton shower distribution functions. Even the amount of final decay products depends on the transversal boost of the initial parton-level particles. However, there is no question that both the original protons and the final (asymptotic) decay products are accurately represented by a collection of 4-vectors subject to the spacetime Lorentz symmetry: the asymptotic outcome of a collision event is independent of the observer's reference frame. 

Another reason for symmetry-restricted modeling is that, from the geometric perspective, only some mathematical operations are permissible when working with objects that transform in a certain way under a symmetry group. A non-equivariant neural network effectively neglects the vector nature of the inputs by treating individual components of the input vectors as scalars. While improving network expressivity, non-equivariance fails to deliver physically interpretable models. Ultimately, a statement about equivariance is a statement about what the basic \textit{features} of the data are -- e.g.~vectors are features, but the individual components of those vectors are not.

More relevant to the applications is the fact that both the simulation and the observation of collisions inevitably involves some degree of \textit{clustering}. A particle detector is made of cells (e.g.~calorimeters) of finite size and as such is unable to distinguish between some particles that are collinear or very close to collinear. Similarly, the standard algorithms for collision simulation typically perform \textit{jet clustering} to closely reproduce the detector behavior. Clustering of course is not a Lorentz-invariant procedure: particle tracks that diverge by a small angle in one frame will diverge by a large angle in another highly boosted frame. However, this limitation of Lorentz-invariant architectures is fairly minor. Since clustering is always done in a fixed laboratory frame, it is still reasonable to impose the full Lorentz symmetry on the resulting 4-vector data. So unless the pre-clustering data itself is coming from multiple significantly different inertial frames, clustering is not interfering with the fundamental symmetry. Simply put, however a given set of 4-vectors is obtained and represented in a specific inertial frame, those vectors will respect the Lorentz symmetry. Finally, one important way to address the issue of network sensitivity to the clustering of collinear particles is through enforcing so called IRC-safety, which will be discussed separately in Section~\ref{irc}.

% These issues are part of the issue of Infrared and Collinear (IRC) Safety, which is a statement that the final ``hard jet'' products of a decay must be invariant under additions on light infrared particles (IR-safety) or under splittings of any present particle into two or more collinear particles with the same total energy-momentum (C-safety). Since any particle detector inherently performs some phase-space integration, it can only observe IRC-safe observables. However, other ``observables'' (such as differential scattering amplitudes) in the theory itself need not be IRC-safe. This opens up the debate about the importance of non-IRC-safe observables in computational methods such as machine learning of collision events. For instance, in \cite{EFN} the non-IRC-safe network performed better than an analogous IRC-safe one, indicating that ``there is discrimination power to be found in IRC-unsafe information''. Hence part of this paper is also dedicated to a comparison of an IR-safe instance of our top tagger with the general non-IRC-safe one and quantifying the IRC-safety of the model.