\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and
  Cordelia Schmid.
\newblock Vivit: {A} video vision transformer.
\newblock In {\em International Conference on Computer Vision}, pages
  6816--6826. {IEEE}, 2021.

\bibitem{Timesformer}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In {\em International Conference on Machine Learning}, pages
  813--824. {PMLR}, 2021.

\bibitem{I3D}
Jo{\~{a}}o Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? {A} new model and the kinetics
  dataset.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  4724--4733. {IEEE}, 2017.

\bibitem{restoration}
Harry Cheng, Yangyang Guo, Jianhua Yin, Haonan Chen, Jiafang Wang, and Liqiang
  Nie.
\newblock Audio-driven talking video frame restoration.
\newblock {\em IEEE Transactions on Multimedia}, pages 1--13, 2021.

\bibitem{VIT_22B}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag
  Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku
  Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin~F. Elsayed, Aravindh
  Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings,
  Mark~Patrick Collier, Alexey~A. Gritsenko, Vighnesh Birodkar, Cristina
  Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic,
  Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah
  Harmsen, and Neil Houlsby.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock {\em CoRR}, abs/2302.05442:1--21, 2023.

\bibitem{ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, pages
  1--12. OpenReview.net, 2021.

\bibitem{MViT}
Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In {\em International Conference on Computer Vision}, pages
  6804--6815. {IEEE}, 2021.

\bibitem{DTR}
Jiawei Fan, Yu Zhao, Xie Yu, Lihua Ma, Junqi Liu, Fangqiu Yi, and Boxun Li.
\newblock {DTR:} an information bottleneck based regularization framework for
  video action recognition.
\newblock In {\em {ACM} International Conference on Multimedia}, pages
  3877--3885. {ACM}, 2022.

\bibitem{TAM}
Quanfu Fan, Chun{-}Fu~(Richard) Chen, Hilde Kuehne, Marco Pistoia, and David~D.
  Cox.
\newblock More is less: Learning efficient video representations by big-little
  network and depthwise temporal aggregation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2261--2270, 2019.

\bibitem{SlowFast}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In {\em International Conference on Computer Vision}, pages
  6201--6210. {IEEE}, 2019.

\bibitem{vqa-tomm}
Yangyang Guo, Liqiang Nie, Harry Cheng, Zhiyong Cheng, Mohan~S. Kankanhalli,
  and Alberto~Del Bimbo.
\newblock On modality bias recognition and reduction.
\newblock {\em ACM Transactions on Multimedia Computing, Communications, and
  Applications}, 19(3):103:1--103:22, 2023.

\bibitem{vqa-tip}
Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Qi Tian, and Min Zhang.
\newblock Loss re-scaling {VQA:} revisiting the language prior problem from a
  class-imbalance view.
\newblock {\em IEEE Transactions on Image Processing}, 31:227--238, 2022.

\bibitem{GuptaInterpolation2020}
Akash Gupta, Abhishek Aich, and Amit~K. Roy{-}Chowdhury.
\newblock {ALANET:} adaptive latent attention network for joint video
  deblurring and interpolation.
\newblock In {\em {ACM} Multimedia}, pages 256--264. {ACM}, 2020.

\bibitem{HeRestoration2020}
Gang He, Chang Wu, Lei Li, Jinjia Zhou, Xianglin Wang, Yunfei Zheng, Bing Yu,
  and Weiying Xie.
\newblock A video compression framework using an overfitted restoration neural
  network.
\newblock In {\em Conference on Computer Vision and Pattern Recognition
  Workshops}, pages 593--597. CVF / {IEEE}, 2020.

\bibitem{ResNEt}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  770--778. {IEEE}, 2016.

\bibitem{ActNet}
Fabian~Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan~Carlos Niebles.
\newblock Activitynet: {A} large-scale video benchmark for human activity
  understanding.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  961--970. {IEEE}, 2015.

\bibitem{Regu_1}
Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu,
  and Marcelo H.~Ang Jr.
\newblock Tada! temporally-adaptive convolutions for video understanding.
\newblock In {\em International Conference on Learning Representations}, pages
  1--13. OpenReview.net, 2022.

\bibitem{STM}
Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan.
\newblock {STM:} spatiotemporal and motion encoding for action recognition.
\newblock In {\em International Conference on Computer Vision}, pages
  2000--2009. {IEEE}, 2019.

\bibitem{JiangInterpolation2018}
Huaizu Jiang, Deqing Sun, Varun Jampani, Ming{-}Hsuan Yang, Erik~G.
  Learned{-}Miller, and Jan Kautz.
\newblock Super slomo: High quality estimation of multiple intermediate frames
  for video interpolation.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  9000--9008. CVF / {IEEE}, 2018.

\bibitem{JinRestoration2020}
Zhi Jin, Muhammad~Zafar Iqbal, Dmytro Bobkov, Wenbin Zou, Xia Li, and
  Eckehard~G. Steinbach.
\newblock A flexible deep {CNN} framework for image restoration.
\newblock {\em IEEE Transactions on Multimedia}, 22(4):1055--1068, 2020.

\bibitem{JuMatching22}
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In {\em European Conference on Computer Vision}, volume 13695, pages
  105--124. Springer, 2022.

\bibitem{Kinectics}
Will Kay, Jo{\~{a}}o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier,
  Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul
  Natsev, Mustafa Suleyman, and Andrew Zisserman.
\newblock The kinetics human action video dataset.
\newblock {\em CoRR}, abs/1705.06950:1--12, 2017.

\bibitem{Regularization_smooth}
Jinhyung Kim, Seunghwan Cha, Dongyoon Wee, Soonmin Bae, and Junmo Kim.
\newblock Regularization on spatio-temporally smoothed feature for action
  recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  12100--12109. CVF / {IEEE}, 2020.

\bibitem{KimRestoration2018}
Tae~Hyun Kim, Mehdi S.~M. Sajjadi, Michael Hirsch, and Bernhard
  Sch{\"{o}}lkopf.
\newblock Spatio-temporal transformer network for video restoration.
\newblock In {\em European Conference on Computer Vision}, pages 111--127.
  Springer, 2018.

\bibitem{HMDB}
Hildegard Kuehne, Hueihan Jhuang, Est{\'{\i}}baliz Garrote, Tomaso~A. Poggio,
  and Thomas Serre.
\newblock {HMDB:} {A} large video database for human motion recognition.
\newblock In {\em International Conference on Computer Vision}, pages
  2556--2563. {IEEE}, 2011.

\bibitem{BLIP2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C.~H. Hoi.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em CoRR}, abs/2301.12597:1--11, 2023.

\bibitem{BLIP}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C.~H. Hoi.
\newblock {BLIP:} bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em International Conference on Machine Learning}, pages
  12888--12900. {PMLR}, 2022.

\bibitem{TA2N}
Shuyuan Li, Huabin Liu, Rui Qian, Yuxi Li, John See, Mengjuan Fei, Xiaoyuan Yu,
  and Weiyao Lin.
\newblock {TA2N:} two-stage action alignment network for few-shot action
  recognition.
\newblock In {\em {AAAI} Conference on Artificial Intelligence}, pages
  1404--1411. {AAAI} Press, 2022.

\bibitem{TEA}
Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang.
\newblock {TEA:} temporal excitation and aggregation for action recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  906--915. CVF / {IEEE}, 2020.

\bibitem{MviTV2}
Yanghao Li, Chao{-}Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Mvitv2: Improved multiscale vision transformers for classification
  and detection.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  4794--4804. {IEEE}, 2022.

\bibitem{OCSampler}
Jintao Lin, Haodong Duan, Kai Chen, Dahua Lin, and Limin Wang.
\newblock Ocsampler: Compressing videos to one clip with single-step sampling.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  13894--13903. IEEE, 2022.

\bibitem{TSM}
Ji Lin, Chuang Gan, and Song Han.
\newblock {TSM:} temporal shift module for efficient video understanding.
\newblock In {\em International Conference on Computer Vision}, pages
  7082--7092. {IEEE}, 2019.

\bibitem{liu2023hs}
Han Liu, Yinwei Wei, Jianhua Yin, and Liqiang Nie.
\newblock Hs-gcn: Hamming spatial graph convolutional networks for
  recommendation.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering},
  35(6):5977--5990, 2023.

\bibitem{VideoSwin}
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  3192--3201. {IEEE}, 2022.

\bibitem{MM_res}
Hongming Luo, Fei Zhou, Kin{-}Man Lam, and Guoping Qiu.
\newblock Restoration of user videos shared on social media.
\newblock In {\em {ACM} International Conference on Multimedia}, pages
  2749--2757. {ACM}, 2022.

\bibitem{ChatGPT}
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--10. MIT Press, 2022.

\bibitem{CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. {PMLR}, 2021.

\bibitem{frame_active}
Aayush Rana and Yogesh~S Rawat.
\newblock Are all frames equal? active sparse labeling for video action
  detection.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--15. MIT Press, 2022.

\bibitem{Two_stream}
Karen Simonyan and Andrew Zisserman.
\newblock Two-stream convolutional networks for action recognition in videos.
\newblock In {\em Annual Conference on Neural Information Processing Systems},
  pages 568--576. MIT Press, 2014.

\bibitem{MAE_B}
Mannat Singh, Quentin Duval, Kalyan~Vasudev Alwala, Haoqi Fan, Vaibhav
  Aggarwal, Aaron Adcock, Armand Joulin, Piotr Doll√°r, Christoph
  Feichtenhofer, Ross Girshick, Rohit Girdhar, and Ishan Misra.
\newblock The effectiveness of mae pre-pretraining for billion-scale
  pretraining.
\newblock {\em CoRR}, abs/2303.13496:1--11, 2023.

\bibitem{UCF101}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock {UCF101:} {A} dataset of 101 human actions classes from videos in the
  wild.
\newblock {\em CoRR}, abs/1212.0402:1--7, 2012.

\bibitem{TanClassification2021}
Yi Tan, Yanbin Hao, Xiangnan He, Yinwei Wei, and Xun Yang.
\newblock Selective dependency aggregation for action classification.
\newblock In {\em {ACM} Multimedia Conference}, pages 592--601. {ACM}, 2021.

\bibitem{C3D}
Du Tran, Lubomir~D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
\newblock Learning spatiotemporal features with 3d convolutional networks.
\newblock In {\em International Conference on Computer Vision}, pages
  4489--4497. {IEEE}, 2015.

\bibitem{R2+1D}
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
  Paluri.
\newblock A closer look at spatiotemporal convolutions for action recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  6450--6459. {IEEE}, 2018.

\bibitem{DataAug_Act}
Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy~J. Ma, Hao Cheng, Pai Peng,
  Feiyue Huang, Rongrong Ji, and Xing Sun.
\newblock Removing the background by adding the background: Towards background
  robust self-supervised video representation learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  11804--11813. CVF / {IEEE}, 2021.

\bibitem{TDN}
Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu.
\newblock {TDN:} temporal difference networks for efficient action recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  1895--1904. CVF / {IEEE}, 2021.

\bibitem{TSN}
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and
  Luc~Van Gool.
\newblock Temporal segment networks: Towards good practices for deep action
  recognition.
\newblock In {\em European Conference on Computer Vision}, pages 20--36.
  Springer, 2016.

\bibitem{ActionClip}
Mengmeng Wang, Jiazheng Xing, and Yong Liu.
\newblock Actionclip: {A} new paradigm for video action recognition.
\newblock {\em CoRR}, abs/2109.08472:1--11, 2021.

\bibitem{WeiRestoration2022}
Xin Wei, Yuyuan Yao, Haoyu Wang, and Liang Zhou.
\newblock Perception-aware cross-modal signal reconstruction: From audio-haptic
  to visual.
\newblock {\em IEEE Transactions on Multimedia}, pages 1--12, 2022.

\bibitem{Regularization_Multigrid}
Chao{-}Yuan Wu, Ross~B. Girshick, Kaiming He, Christoph Feichtenhofer, and
  Philipp Kr{\"{a}}henb{\"{u}}hl.
\newblock A multigrid method for efficiently training video models.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  150--159. CVF / {IEEE}, 2020.

\bibitem{Transfer_VL_ko}
Wenhao Wu, Zhun Sun, and Wanli Ouyang.
\newblock Revisiting classifier: Transferring vision-language models for video
  recognition.
\newblock In {\em AAAI Conference on Artificial Intelligence}, pages 1--10.
  {AAAI} Press, 2023.

\bibitem{S3D}
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
\newblock Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
  in video classification.
\newblock In {\em European Conference on Computer Vision}, pages 318--335.
  Springer, 2018.

\bibitem{MGSampler}
Yuan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu.
\newblock Mgsampler: An explainable sampling strategy for video action
  recognition.
\newblock In {\em International Conference on Computer Vision}, pages
  1493--1502. {IEEE}, 2021.

\bibitem{ZhouInterpolation2021}
Chengcheng Zhou, Zongqing Lu, Linge Li, Qiangyu Yan, and Jing{-}Hao Xue.
\newblock How video super-resolution and frame interpolation mutually benefit.
\newblock In {\em {ACM} Multimedia}, pages 5445--5453. {ACM}, 2021.

\end{thebibliography}
