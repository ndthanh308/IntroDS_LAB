% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{xie_learning_2020}{report}{}
      \name{author}{5}{}{%
        {{hash=106e98fdd4d77120f7f7715961cd511d}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Annie},
           giveni={A\bibinitperiod}}}%
        {{hash=0163beefd49e610ee1f8dbe5dd57b7b4}{%
           family={Losey},
           familyi={L\bibinitperiod},
           given={Dylan\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=6032ffd7718e7534965c1c505cac624c}{%
           family={Tolsma},
           familyi={T\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=058e82495825ae376c6a96a12169e6ee}{%
           family={Finn},
           familyi={F\bibinitperiod},
           given={Chelsea},
           giveni={C\bibinitperiod}}}%
        {{hash=ad742818738efaa3c358f9def3944504}{%
           family={Sadigh},
           familyi={S\bibinitperiod},
           given={Dorsa},
           giveni={D\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {arXiv}%
      }
      \strng{namehash}{2e0ec7701a3a3ea2bdc66dc01c28088b}
      \strng{fullhash}{43dcd61e4cda8c3700213864a3a7aa57}
      \strng{bibnamehash}{43dcd61e4cda8c3700213864a3a7aa57}
      \strng{authorbibnamehash}{43dcd61e4cda8c3700213864a3a7aa57}
      \strng{authornamehash}{2e0ec7701a3a3ea2bdc66dc01c28088b}
      \strng{authorfullhash}{43dcd61e4cda8c3700213864a3a7aa57}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.}
      \field{month}{11}
      \field{note}{arXiv:2011.06619 [cs] type: article}
      \field{number}{arXiv:2011.06619}
      \field{title}{Learning {Latent} {Representations} to {Influence} {Multi}-{Agent} {Interaction}}
      \field{type}{techreport}
      \field{urlday}{8}
      \field{urlmonth}{6}
      \field{urlyear}{2022}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2011.06619
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2011.06619
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics}
    \endentry
    \entry{nguyen_review_2019}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=01d75ec74cde28c61f185a823aa9b5e6}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Hai},
           giveni={H\bibinitperiod}}}%
        {{hash=3ffa972e8bebc3bff033dd19f903b60f}{%
           family={La},
           familyi={L\bibinitperiod},
           given={Hung},
           giveni={H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Naples, Italy}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{7441a56c35554c3c3fc3120892a5b8b2}
      \strng{fullhash}{7441a56c35554c3c3fc3120892a5b8b2}
      \strng{bibnamehash}{7441a56c35554c3c3fc3120892a5b8b2}
      \strng{authorbibnamehash}{7441a56c35554c3c3fc3120892a5b8b2}
      \strng{authornamehash}{7441a56c35554c3c3fc3120892a5b8b2}
      \strng{authorfullhash}{7441a56c35554c3c3fc3120892a5b8b2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Reinforcement learning combined with neural networks has recently led to a wide range of successes in learning policies in different domains. For robot manipulation, reinforcement learning algorithms bring the hope for machines to have the human-like abilities by directly learning dexterous manipulation from raw pixels. In this review paper, we address the current status of reinforcement learning algorithms used in the ﬁeld. We also cover essential theoretical background and main issues with current algorithms, which are limiting their applications of reinforcement learning algorithms in solving practical problems in robotics. We also share our thoughts on a number of future directions for reinforcement learning research.}
      \field{booktitle}{2019 {Third} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})}
      \field{isbn}{978-1-5386-9245-5}
      \field{month}{2}
      \field{title}{Review of {Deep} {Reinforcement} {Learning} for {Robot} {Manipulation}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{590\bibrangedash 595}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/IRC.2019.00120
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/8675643/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/8675643/
      \endverb
    \endentry
    \entry{stouraitis_online_2020}{article}{}
      \name{author}{4}{}{%
        {{hash=435a8c1488eb906a7b0b50d2ae9e6f29}{%
           family={Stouraitis},
           familyi={S\bibinitperiod},
           given={Theodoros},
           giveni={T\bibinitperiod}}}%
        {{hash=efd02dda559ca6219d8925745d82815c}{%
           family={Chatzinikolaidis},
           familyi={C\bibinitperiod},
           given={Iordanis},
           giveni={I\bibinitperiod}}}%
        {{hash=84fd128d0830d14084fa7b35f206a387}{%
           family={Gienger},
           familyi={G\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=7b2c17552ac9061afc4606fc8fc554b9}{%
           family={Vijayakumar},
           familyi={V\bibinitperiod},
           given={Sethu},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{f8ce2ccb87418844794679f930f4abd5}
      \strng{fullhash}{7ced31e5bbe3e0a144a5487498edd90c}
      \strng{bibnamehash}{7ced31e5bbe3e0a144a5487498edd90c}
      \strng{authorbibnamehash}{7ced31e5bbe3e0a144a5487498edd90c}
      \strng{authornamehash}{f8ce2ccb87418844794679f930f4abd5}
      \strng{authorfullhash}{7ced31e5bbe3e0a144a5487498edd90c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Effective collaboration is based on online adaptation of one’s own actions to the actions of their partner. This article provides a principled formalism to address online adaptation in joint planning problems such as Dyadic collaborative Manipulation (DcM) scenarios. We propose an efﬁcient bilevel formulation that combines graph search methods with trajectory optimization, enabling robotic agents to adapt their policy on-the-ﬂy in accordance to changes of the dyadic task. This method is the ﬁrst to empower agents with the ability to plan online in hybrid spaces; optimizing over discrete contact locations, contact sequence patterns, continuous trajectories, and force proﬁles for co-manipulation tasks. This is particularly important in large object co-manipulation that requires changes of grasp-holds and plan adaptation. We demonstrate in simulation and with robot experiments the efﬁcacy of the bilevel optimization by investigating the effect of robot policy changes in response to real-time alterations of the dyadic goals, eminent grasp switches, as well as optimal dyadic interactions to realize the joint task.}
      \field{issn}{1552-3098, 1941-0468}
      \field{journaltitle}{IEEE Transactions on Robotics}
      \field{month}{10}
      \field{number}{5}
      \field{title}{Online {Hybrid} {Motion} {Planning} for {Dyadic} {Collaborative} {Manipulation} via {Bilevel} {Optimization}}
      \field{urlday}{2}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{volume}{36}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{1452\bibrangedash 1471}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1109/TRO.2020.2992987
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9166536/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9166536/
      \endverb
    \endentry
    \entry{obhi_moving_2011}{article}{}
      \name{author}{2}{}{%
        {{hash=6d33d093d490c052340b327f7a9a1b81}{%
           family={Obhi},
           familyi={O\bibinitperiod},
           given={Sukhvinder\bibnamedelima S.},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=a236312b36076def91d9af84fc51bdf8}{%
           family={Sebanz},
           familyi={S\bibinitperiod},
           given={Natalie},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{0c39a3fab06e66883b0497e9aa7f6b9a}
      \strng{fullhash}{0c39a3fab06e66883b0497e9aa7f6b9a}
      \strng{bibnamehash}{0c39a3fab06e66883b0497e9aa7f6b9a}
      \strng{authorbibnamehash}{0c39a3fab06e66883b0497e9aa7f6b9a}
      \strng{authornamehash}{0c39a3fab06e66883b0497e9aa7f6b9a}
      \strng{authorfullhash}{0c39a3fab06e66883b0497e9aa7f6b9a}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{issn}{0014-4819, 1432-1106}
      \field{journaltitle}{Experimental Brain Research}
      \field{month}{6}
      \field{number}{3-4}
      \field{shorttitle}{Moving together}
      \field{title}{Moving together: toward understanding the mechanisms of joint action}
      \field{urlday}{31}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{volume}{211}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{329\bibrangedash 336}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1007/s00221-011-2721-0
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/s00221-011-2721-0
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/s00221-011-2721-0
      \endverb
    \endentry
    \entry{sheridan_eight_nodate}{article}{}
      \name{author}{1}{}{%
        {{hash=e2257dbf6507d592cb5db08f26613f4c}{%
           family={Sheridan},
           familyi={S\bibinitperiod},
           given={Thomas\bibnamedelima B},
           giveni={T\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{e2257dbf6507d592cb5db08f26613f4c}
      \strng{fullhash}{e2257dbf6507d592cb5db08f26613f4c}
      \strng{bibnamehash}{e2257dbf6507d592cb5db08f26613f4c}
      \strng{authorbibnamehash}{e2257dbf6507d592cb5db08f26613f4c}
      \strng{authornamehash}{e2257dbf6507d592cb5db08f26613f4c}
      \strng{authorfullhash}{e2257dbf6507d592cb5db08f26613f4c}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{EIGHT} {ULTIMATE} {CHALLENGES} {F} {HUMAN}-{ROBOT} {COMMUNICATION}}
    \endentry
    \entry{kim_influencing_2022}{misc}{}
      \name{author}{8}{}{%
        {{hash=1118e60ae61122fe363f276cda2fd942}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Dong-Ki},
           giveni={D\bibinithyphendelim K\bibinitperiod}}}%
        {{hash=b43d13252a1efb31508f9952205f4e14}{%
           family={Riemer},
           familyi={R\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=0474cc7d2cd3785ad22074a7b4abeb8b}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Miao},
           giveni={M\bibinitperiod}}}%
        {{hash=d02e0768e79bc374c7db58661d344e29}{%
           family={Foerster},
           familyi={F\bibinitperiod},
           given={Jakob\bibnamedelima N.},
           giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=02e603f0f0d76987cf82f7f4e94462fc}{%
           family={Everett},
           familyi={E\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=746eb35a34355e0442e993cd2f97dd4f}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Chuangchuang},
           giveni={C\bibinitperiod}}}%
        {{hash=778694b9dda05914a1456df10efb774c}{%
           family={Tesauro},
           familyi={T\bibinitperiod},
           given={Gerald},
           giveni={G\bibinitperiod}}}%
        {{hash=8848ce0ee15eec9f66f8d5f9006b8d66}{%
           family={How},
           familyi={H\bibinitperiod},
           given={Jonathan\bibnamedelima P.},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{bdeffff2188c31392f074e2c84c10899}
      \strng{fullhash}{76f5c059c4edf0fdb87ef32cd5b5e6b6}
      \strng{bibnamehash}{bdeffff2188c31392f074e2c84c10899}
      \strng{authorbibnamehash}{bdeffff2188c31392f074e2c84c10899}
      \strng{authornamehash}{bdeffff2188c31392f074e2c84c10899}
      \strng{authorfullhash}{76f5c059c4edf0fdb87ef32cd5b5e6b6}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The main challenge of multiagent reinforcement learning is the difﬁculty of learning useful policies in the presence of other simultaneously learning agents whose changing behaviors jointly affect the environment’s transition and reward dynamics. An effective approach that has recently emerged for addressing this non-stationarity is for each agent to anticipate the learning of other agents and inﬂuence the evolution of future policies towards desirable behavior for its own beneﬁt. Unfortunately, previous approaches for achieving this suffer from myopic evaluation, considering only a ﬁnite number of policy updates. As such, these methods can only inﬂuence transient future policies rather than achieving the promise of scalable equilibrium selection approaches that inﬂuence the behavior at convergence. In this paper, we propose a principled framework for considering the limiting policies of other agents as time approaches inﬁnity. Speciﬁcally, we develop a new optimization objective that maximizes each agent’s average reward by directly accounting for the impact of its behavior on the limiting set of policies that other agents will converge to. Our paper characterizes desirable solution concepts within this problem setting and provides practical approaches for optimizing over possible outcomes. As a result of our farsighted objective, we demonstrate better long-term performance than state-of-the-art baselines across a suite of diverse multiagent benchmark domains.}
      \field{month}{5}
      \field{note}{Number: arXiv:2203.03535 arXiv:2203.03535 [cs]}
      \field{title}{Influencing {Long}-{Term} {Behavior} in {Multiagent} {Reinforcement} {Learning}}
      \field{urlday}{11}
      \field{urlmonth}{10}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2203.03535
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2203.03535
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
    \endentry
    \entry{yang_optimal_2022}{misc}{}
      \name{author}{3}{}{%
        {{hash=d97d320db0ed53f9fb0c48af8cea82b1}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Mesut},
           giveni={M\bibinitperiod}}}%
        {{hash=0f3418775cf07bf978b10e452185a9d8}{%
           family={Carroll},
           familyi={C\bibinitperiod},
           given={Micah},
           giveni={M\bibinitperiod}}}%
        {{hash=ac56afa5bcb9a863ab4555f3ac357ef5}{%
           family={Dragan},
           familyi={D\bibinitperiod},
           given={Anca},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{ea1a3d7be1764d92a44f04395ccc41af}
      \strng{fullhash}{ea1a3d7be1764d92a44f04395ccc41af}
      \strng{bibnamehash}{ea1a3d7be1764d92a44f04395ccc41af}
      \strng{authorbibnamehash}{ea1a3d7be1764d92a44f04395ccc41af}
      \strng{authornamehash}{ea1a3d7be1764d92a44f04395ccc41af}
      \strng{authorfullhash}{ea1a3d7be1764d92a44f04395ccc41af}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{AI agents designed to collaborate with people beneﬁt from models that enable them to anticipate human behavior. However, realistic models tend to require vast amounts of human data, which is often hard to collect. A good prior or initialization could make for more data-efﬁcient training, but what makes for a good prior on human behavior? Our work leverages a very simple assumption: people generally act closer to optimal than to random chance. We show that using optimal behavior as a prior for human models makes these models vastly more data-efﬁcient and able to generalize to new environments. Our intuition is that such a prior enables the training to focus one’s precious real-world data on capturing the subtle nuances of human suboptimality, instead of on the basics of how to do the task in the ﬁrst place. We also show that using these improved human models often leads to better human-AI collaboration performance compared to using models based on real human data alone.}
      \field{month}{11}
      \field{note}{arXiv:2211.01602 [cs]}
      \field{shorttitle}{Optimal {Behavior} {Prior}}
      \field{title}{Optimal {Behavior} {Prior}: {Data}-{Efficient} {Human} {Models} for {Improved} {Human}-{AI} {Collaboration}}
      \field{urlday}{22}
      \field{urlmonth}{11}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2211.01602
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2211.01602
      \endverb
      \keyw{Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
    \endentry
    \entry{bobu_less_2020}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=1f931fbdc8cb9d19903360e15cb7b138}{%
           family={Bobu},
           familyi={B\bibinitperiod},
           given={Andreea},
           giveni={A\bibinitperiod}}}%
        {{hash=10fc910da10ad77461e352eb9d2087cc}{%
           family={Scobee},
           familyi={S\bibinitperiod},
           given={Dexter\bibnamedelimb R.\bibnamedelimi R.},
           giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=0ed0a2dae2379aae8d2dd06fb918d870}{%
           family={Fisac},
           familyi={F\bibinitperiod},
           given={Jaime\bibnamedelima F.},
           giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=d756b35e7b530d260e5a0807d1953354}{%
           family={Sastry},
           familyi={S\bibinitperiod},
           given={S.\bibnamedelimi Shankar},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=4090ff37d461901de2419d0252e265a1}{%
           family={Dragan},
           familyi={D\bibinitperiod},
           given={Anca\bibnamedelima D.},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Cambridge United Kingdom}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{ccad0c33e2e732735ac9567d28878c25}
      \strng{fullhash}{2e53c1289bab7b16c9f9e936cf711dfc}
      \strng{bibnamehash}{2e53c1289bab7b16c9f9e936cf711dfc}
      \strng{authorbibnamehash}{2e53c1289bab7b16c9f9e936cf711dfc}
      \strng{authornamehash}{ccad0c33e2e732735ac9567d28878c25}
      \strng{authorfullhash}{2e53c1289bab7b16c9f9e936cf711dfc}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.}
      \field{booktitle}{Proceedings of the 2020 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}}
      \field{isbn}{978-1-4503-6746-2}
      \field{month}{3}
      \field{shorttitle}{{LESS} is {More}}
      \field{title}{{LESS} is {More}: {Rethinking} {Probabilistic} {Models} of {Human} {Behavior}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{429\bibrangedash 437}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1145/3319502.3374811
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3319502.3374811
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3319502.3374811
      \endverb
    \endentry
    \entry{nomura_prediction_2008}{article}{}
      \name{author}{4}{}{%
        {{hash=6aaad1d7bd3a80358bb42d01cdc207d3}{%
           family={Nomura},
           familyi={N\bibinitperiod},
           given={Tatsuya},
           giveni={T\bibinitperiod}}}%
        {{hash=401c2b6898c4cc146157500b540f459b}{%
           family={Kanda},
           familyi={K\bibinitperiod},
           given={Takayuki},
           giveni={T\bibinitperiod}}}%
        {{hash=ee99c10d32956356e1d6af06400d3d64}{%
           family={Suzuki},
           familyi={S\bibinitperiod},
           given={Tomohiro},
           giveni={T\bibinitperiod}}}%
        {{hash=d6c73d4c042fbdefa0e638cd38402a78}{%
           family={Kato},
           familyi={K\bibinitperiod},
           given={Kensuke},
           giveni={K\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{9ccdeddd6d2982fdd55bf20b3548b7ef}
      \strng{fullhash}{d4481e632d3a19a01add003d2b90caf9}
      \strng{bibnamehash}{d4481e632d3a19a01add003d2b90caf9}
      \strng{authorbibnamehash}{d4481e632d3a19a01add003d2b90caf9}
      \strng{authornamehash}{9ccdeddd6d2982fdd55bf20b3548b7ef}
      \strng{authorfullhash}{d4481e632d3a19a01add003d2b90caf9}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{When people interact with communication robots in daily life, their attitudes and emotions toward the robots affect their behavior. From the perspective of robotics design, we need to investigate the inﬂuences of these attitudes and emotions on human–robot interaction. This paper reports our empirical study on the relationships between people’s attitudes and emotions, and their behavior toward a robot. In particular, we focused on negative attitudes, anxiety, and communication avoidance behavior, which have important implications for robotics design. For this purpose, we used two psychological scales that we had developed: negative attitudes toward robots scale (NARS) and robot anxiety scale (RAS). In the experiment, subjects and a humanoid robot are engaged in simple interactions including scenes of meeting, greeting, self-disclosure, and physical contact. Experimental results indicated that there is a relationship between negative attitudes and emotions, and communication avoidance behavior. A gender effect was also suggested.}
      \field{issn}{1552-3098}
      \field{journaltitle}{IEEE Transactions on Robotics}
      \field{month}{4}
      \field{number}{2}
      \field{title}{Prediction of {Human} {Behavior} in {Human}--{Robot} {Interaction} {Using} {Psychological} {Scales} for {Anxiety} and {Negative} {Attitudes} {Toward} {Robots}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{volume}{24}
      \field{year}{2008}
      \field{urldateera}{ce}
      \field{pages}{442\bibrangedash 451}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/TRO.2007.914004
      \endverb
      \verb{urlraw}
      \verb http://ieeexplore.ieee.org/document/4481184/
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/document/4481184/
      \endverb
    \endentry
    \entry{nikolaidis_human-robot_2017}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=2583c1a0115814748f265c81c137b4fe}{%
           family={Nikolaidis},
           familyi={N\bibinitperiod},
           given={Stefanos},
           giveni={S\bibinitperiod}}}%
        {{hash=8ce00a811cd3e9dc3b166fd7ff631921}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Yu\bibnamedelima Xiang},
           giveni={Y\bibinitperiod\bibinitdelim X\bibinitperiod}}}%
        {{hash=335adf1f352b65a9575abf76521fb446}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=cb1a6e64b31ce80aa88005a357504a99}{%
           family={Srinivasa},
           familyi={S\bibinitperiod},
           given={Siddhartha},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Vienna Austria}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{75dc15f9257fa28607b3c922e6166ec1}
      \strng{fullhash}{bd0cbe213b68814b3bf202182894cfa0}
      \strng{bibnamehash}{bd0cbe213b68814b3bf202182894cfa0}
      \strng{authorbibnamehash}{bd0cbe213b68814b3bf202182894cfa0}
      \strng{authornamehash}{75dc15f9257fa28607b3c922e6166ec1}
      \strng{authorfullhash}{bd0cbe213b68814b3bf202182894cfa0}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 2017 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}}
      \field{isbn}{978-1-4503-4336-7}
      \field{month}{3}
      \field{title}{Human-{Robot} {Mutual} {Adaptation} in {Shared} {Autonomy}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{294\bibrangedash 302}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1145/2909824.3020252
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/2909824.3020252
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/2909824.3020252
      \endverb
    \endentry
    \entry{papoudakis_dealing_nodate}{article}{}
      \name{author}{4}{}{%
        {{hash=d0631db9b5ca2e6150a04082663ca08d}{%
           family={Papoudakis},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
        {{hash=f355421623a9ce0d54965817f05f6ad0}{%
           family={Christianos},
           familyi={C\bibinitperiod},
           given={Filippos},
           giveni={F\bibinitperiod}}}%
        {{hash=b0a84f9ec63b81f5eaf710941897288f}{%
           family={Rahman},
           familyi={R\bibinitperiod},
           given={Arrasy},
           giveni={A\bibinitperiod}}}%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{aad129e7f4a411257e518ba750d651b9}
      \strng{fullhash}{b52a227a8d9f5624b2fbb9e34b6dc5da}
      \strng{bibnamehash}{b52a227a8d9f5624b2fbb9e34b6dc5da}
      \strng{authorbibnamehash}{b52a227a8d9f5624b2fbb9e34b6dc5da}
      \strng{authornamehash}{aad129e7f4a411257e518ba750d651b9}
      \strng{authorfullhash}{b52a227a8d9f5624b2fbb9e34b6dc5da}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modiﬁcations in the training procedure, such as centralized training, to learning representations of the opponent’s policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.}
      \field{title}{Dealing with {Non}-{Stationarity} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}}
    \endentry
    \entry{albrecht_autonomous_2018}{article}{}
      \name{author}{2}{}{%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V.},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=3f42a499da2cddc0bae71b76c81f7c46}{%
           family={Stone},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{9350371115a0d7dab048a4452942a148}
      \strng{fullhash}{9350371115a0d7dab048a4452942a148}
      \strng{bibnamehash}{9350371115a0d7dab048a4452942a148}
      \strng{authorbibnamehash}{9350371115a0d7dab048a4452942a148}
      \strng{authornamehash}{9350371115a0d7dab048a4452942a148}
      \strng{authorfullhash}{9350371115a0d7dab048a4452942a148}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{issn}{00043702}
      \field{journaltitle}{Artificial Intelligence}
      \field{month}{5}
      \field{shorttitle}{Autonomous agents modelling other agents}
      \field{title}{Autonomous agents modelling other agents: {A} comprehensive survey and open problems}
      \field{urlday}{21}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{volume}{258}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{66\bibrangedash 95}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1016/j.artint.2018.01.002
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0004370218300249
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0004370218300249
      \endverb
    \endentry
    \entry{wang_uencing_nodate}{article}{}
      \name{author}{4}{}{%
        {{hash=5d99d491c95f4634e2e1eb29526ad513}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Woodrow\bibnamedelima Z},
           giveni={W\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=a7b4dd0087a3c49f89ad850cb8bede41}{%
           family={Shih},
           familyi={S\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=106e98fdd4d77120f7f7715961cd511d}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Annie},
           giveni={A\bibinitperiod}}}%
        {{hash=ad742818738efaa3c358f9def3944504}{%
           family={Sadigh},
           familyi={S\bibinitperiod},
           given={Dorsa},
           giveni={D\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{9c246f6a9c3457f4f504a397bba5557f}
      \strng{fullhash}{8efa0a194e4b19f7a6f73623d550fef7}
      \strng{bibnamehash}{8efa0a194e4b19f7a6f73623d550fef7}
      \strng{authorbibnamehash}{8efa0a194e4b19f7a6f73623d550fef7}
      \strng{authornamehash}{9c246f6a9c3457f4f504a397bba5557f}
      \strng{authorfullhash}{8efa0a194e4b19f7a6f73623d550fef7}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learning in multi-agent environments is difﬁcult due to the nonstationarity introduced by an opponent’s or partner’s changing behaviors. Instead of reactively adapting to the other agent’s (opponent or partner) behavior, we propose an algorithm to proactively inﬂuence the other agent’s strategy to stabilize –which can restrain the non-stationarity caused by the other agent. We learn a lowdimensional latent representation of the other agent’s strategy and the dynamics of how the latent strategy evolves with respect to our robot’s behavior. With this learned dynamics model, we can deﬁne an unsupervised stability reward to train our robot to deliberately inﬂuence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efﬁciency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation. We show qualitative results on our website.}
      \field{title}{Inﬂuencing {Towards} {Stable} {Multi}-{Agent} {Interactions}}
      \field{pages}{12}
      \range{pages}{1}
    \endentry
    \entry{parekh_rili_2022}{misc}{}
      \name{author}{3}{}{%
        {{hash=3f4a615931c66890e30728ebf29f0588}{%
           family={Parekh},
           familyi={P\bibinitperiod},
           given={Sagar},
           giveni={S\bibinitperiod}}}%
        {{hash=dd142aa00f0b59a7c6207cfc1045cef7}{%
           family={Habibian},
           familyi={H\bibinitperiod},
           given={Soheil},
           giveni={S\bibinitperiod}}}%
        {{hash=0163beefd49e610ee1f8dbe5dd57b7b4}{%
           family={Losey},
           familyi={L\bibinitperiod},
           given={Dylan\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{08a6e1081a5808d1a96c27e5713652dc}
      \strng{fullhash}{08a6e1081a5808d1a96c27e5713652dc}
      \strng{bibnamehash}{08a6e1081a5808d1a96c27e5713652dc}
      \strng{authorbibnamehash}{08a6e1081a5808d1a96c27e5713652dc}
      \strng{authornamehash}{08a6e1081a5808d1a96c27e5713652dc}
      \strng{authorfullhash}{08a6e1081a5808d1a96c27e5713652dc}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{When robots interact with human partners, often these partners change their behavior in response to the robot. On the one hand this is challenging because the robot must learn to coordinate with a dynamic partner. But on the other hand -- if the robot understands these dynamics -- it can harness its own behavior, influence the human, and guide the team towards effective collaboration. Prior research enables robots to learn to influence other robots or simulated agents. In this paper we extend these learning approaches to now influence humans. What makes humans especially hard to influence is that -- not only do humans react to the robot -- but the way a single user reacts to the robot may change over time, and different humans will respond to the same robot behavior in different ways. We therefore propose a robust approach that learns to influence changing partner dynamics. Our method first trains with a set of partners across repeated interactions, and learns to predict the current partner's behavior based on the previous states, actions, and rewards. Next, we rapidly adapt to new partners by sampling trajectories the robot learned with the original partners, and then leveraging those existing behaviors to influence the new partner dynamics. We compare our resulting algorithm to state-of-the-art baselines across simulated environments and a user study where the robot and participants collaborate to build towers. We find that our approach outperforms the alternatives, even when the partner follows new or unexpected dynamics. Videos of the user study are available here: https://youtu.be/lYsWM8An18g}
      \field{month}{7}
      \field{note}{arXiv:2203.12705 [cs]}
      \field{shorttitle}{{RILI}}
      \field{title}{{RILI}: {Robustly} {Influencing} {Latent} {Intent}}
      \field{urlday}{22}
      \field{urlmonth}{11}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2203.12705
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2203.12705
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Robotics}
    \endentry
    \entry{karita_comparative_2019}{inproceedings}{}
      \name{author}{13}{}{%
        {{hash=879774ecbd2eb8c15e31a9bc2dca53c6}{%
           family={Karita},
           familyi={K\bibinitperiod},
           given={Shigeki},
           giveni={S\bibinitperiod}}}%
        {{hash=014a2645817376950f81c70f969a9fe2}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Nanxin},
           giveni={N\bibinitperiod}}}%
        {{hash=d240bae1e3c88f765895ab4d83782eb4}{%
           family={Hayashi},
           familyi={H\bibinitperiod},
           given={Tomoki},
           giveni={T\bibinitperiod}}}%
        {{hash=3d7f41dab838c920fb9511319762ddf6}{%
           family={Hori},
           familyi={H\bibinitperiod},
           given={Takaaki},
           giveni={T\bibinitperiod}}}%
        {{hash=1b6ece64ce935f90d240bf937cc07775}{%
           family={Inaguma},
           familyi={I\bibinitperiod},
           given={Hirofumi},
           giveni={H\bibinitperiod}}}%
        {{hash=4dee82afdc6cf09682d6db3d0779da6a}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Ziyan},
           giveni={Z\bibinitperiod}}}%
        {{hash=627704a16af997fbbfd3488039e9f608}{%
           family={Someki},
           familyi={S\bibinitperiod},
           given={Masao},
           giveni={M\bibinitperiod}}}%
        {{hash=5d44d64aa24886aadf24aea066bbe644}{%
           family={Soplin},
           familyi={S\bibinitperiod},
           given={Nelson\bibnamedelimb Enrique\bibnamedelima Yalta},
           giveni={N\bibinitperiod\bibinitdelim E\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=d041256806edee13f942f8790029e506}{%
           family={Yamamoto},
           familyi={Y\bibinitperiod},
           given={Ryuichi},
           giveni={R\bibinitperiod}}}%
        {{hash=4d0c2b1ef387cabfa88346c2a1ded791}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xiaofei},
           giveni={X\bibinitperiod}}}%
        {{hash=ae898cb785c2eecb4cda42d862e0d5d4}{%
           family={Watanabe},
           familyi={W\bibinitperiod},
           given={Shinji},
           giveni={S\bibinitperiod}}}%
        {{hash=e0a58b052caf9d7c2ae5b9e428b3b4b9}{%
           family={Yoshimura},
           familyi={Y\bibinitperiod},
           given={Takenori},
           giveni={T\bibinitperiod}}}%
        {{hash=1e02b53212e627765f11f57e6fc91fa3}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Wangyou},
           giveni={W\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{14a011c9e6b3d6c764c58fd83e12c2f9}
      \strng{fullhash}{3bdfa770634eb9b78d5c452c866e1a05}
      \strng{bibnamehash}{14a011c9e6b3d6c764c58fd83e12c2f9}
      \strng{authorbibnamehash}{14a011c9e6b3d6c764c58fd83e12c2f9}
      \strng{authornamehash}{14a011c9e6b3d6c764c58fd83e12c2f9}
      \strng{authorfullhash}{3bdfa770634eb9b78d5c452c866e1a05}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sequence-to-sequence models have been widely used in end-toend speech processing, for example, automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS). This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance in neural machine translation and other natural language processing applications. We undertook intensive studies in which we experimentally compared and analyzed Transformer and conventional recurrent neural networks (RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and signiﬁcant performance beneﬁts obtained with Transformer for each task including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to succeed our exciting outcomes.}
      \field{booktitle}{2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})}
      \field{month}{12}
      \field{note}{arXiv:1909.06317 [cs, eess]}
      \field{title}{A {Comparative} {Study} on {Transformer} vs {RNN} in {Speech} {Applications}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{449\bibrangedash 456}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/ASRU46091.2019.9003750
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1909.06317
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1909.06317
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{radford_language_nodate}{article}{}
      \name{author}{6}{}{%
        {{hash=a812c46caad94fc8701be37871f303ba}{%
           family={Radford},
           familyi={R\bibinitperiod},
           given={Alec},
           giveni={A\bibinitperiod}}}%
        {{hash=495187f3a2c93ddb8083bd18a5702527}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
        {{hash=5c2f5c2e6d4a9ec8681377f8a8e5e6af}{%
           family={Child},
           familyi={C\bibinitperiod},
           given={Rewon},
           giveni={R\bibinitperiod}}}%
        {{hash=63b32614460a4ab72dbbbbfe223220ea}{%
           family={Luan},
           familyi={L\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=1e6adbf36ab730cd5fdadb838b4d2667}{%
           family={Amodei},
           familyi={A\bibinitperiod},
           given={Dario},
           giveni={D\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{680d516e36de8e8be8a6aaa2ef1ad9e7}
      \strng{fullhash}{272bcacf72583dff08d1a2eedd467b89}
      \strng{bibnamehash}{272bcacf72583dff08d1a2eedd467b89}
      \strng{authorbibnamehash}{272bcacf72583dff08d1a2eedd467b89}
      \strng{authornamehash}{680d516e36de8e8be8a6aaa2ef1ad9e7}
      \strng{authorfullhash}{272bcacf72583dff08d1a2eedd467b89}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.}
      \field{title}{Language {Models} are {Unsupervised} {Multitask} {Learners}}
    \endentry
    \entry{vaswani_attention_2017}{misc}{}
      \name{author}{8}{}{%
        {{hash=7f28e84700536646dd6620a0db07ad09}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=62efade83d70f0323fe248755e6c90c5}{%
           family={Shazeer},
           familyi={S\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=06649ebab1ea5cac0250746a19764975}{%
           family={Parmar},
           familyi={P\bibinitperiod},
           given={Niki},
           giveni={N\bibinitperiod}}}%
        {{hash=831027ee0ebf22375e2a86afc1881909}{%
           family={Uszkoreit},
           familyi={U\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
        {{hash=27b07e4eacbf4ef7a1438e3badb7dd8d}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Aidan\bibnamedelima N.},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=95595a0fefb86187cbc36e551017d332}{%
           family={Polosukhin},
           familyi={P\bibinitperiod},
           given={Illia},
           giveni={I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{fullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \strng{bibnamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authorbibnamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authornamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authorfullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}
      \field{month}{12}
      \field{note}{Number: arXiv:1706.03762 arXiv:1706.03762 [cs]}
      \field{title}{Attention {Is} {All} {You} {Need}}
      \field{urlday}{2}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.03762
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.03762
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{reddy_where_2019}{misc}{}
      \name{author}{3}{}{%
        {{hash=1bf7c71307d2e36fa4b5e0139d6dca75}{%
           family={Reddy},
           familyi={R\bibinitperiod},
           given={Siddharth},
           giveni={S\bibinitperiod}}}%
        {{hash=4090ff37d461901de2419d0252e265a1}{%
           family={Dragan},
           familyi={D\bibinitperiod},
           given={Anca\bibnamedelima D.},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{65317445fe9d49c86f3795926ec0d6dc}
      \strng{fullhash}{65317445fe9d49c86f3795926ec0d6dc}
      \strng{bibnamehash}{65317445fe9d49c86f3795926ec0d6dc}
      \strng{authorbibnamehash}{65317445fe9d49c86f3795926ec0d6dc}
      \strng{authornamehash}{65317445fe9d49c86f3795926ec0d6dc}
      \strng{authorfullhash}{65317445fe9d49c86f3795926ec0d6dc}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspeciﬁcation: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules – the dynamics – governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user’s internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.}
      \field{month}{1}
      \field{note}{Number: arXiv:1805.08010 arXiv:1805.08010 [cs, stat]}
      \field{shorttitle}{Where {Do} {You} {Think} {You}'re {Going}?}
      \field{title}{Where {Do} {You} {Think} {You}'re {Going}?: {Inferring} {Beliefs} about {Dynamics} from {Behavior}}
      \field{urlday}{12}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1805.08010
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1805.08010
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{laidlaw_boltzmann_2022}{misc}{}
      \name{author}{2}{}{%
        {{hash=4b2ba4511f0dc7349e863ecd69fe2dd6}{%
           family={Laidlaw},
           familyi={L\bibinitperiod},
           given={Cassidy},
           giveni={C\bibinitperiod}}}%
        {{hash=ac56afa5bcb9a863ab4555f3ac357ef5}{%
           family={Dragan},
           familyi={D\bibinitperiod},
           given={Anca},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{592190b2a216a3f2bd9736bd3f06c72a}
      \strng{fullhash}{592190b2a216a3f2bd9736bd3f06c72a}
      \strng{bibnamehash}{592190b2a216a3f2bd9736bd3f06c72a}
      \strng{authorbibnamehash}{592190b2a216a3f2bd9736bd3f06c72a}
      \strng{authornamehash}{592190b2a216a3f2bd9736bd3f06c72a}
      \strng{authorfullhash}{592190b2a216a3f2bd9736bd3f06c72a}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and distribution. The advantage of the latter type, which includes Boltzmann rationality, is the ability to make accurate predictions in new environments without extensive data when humans are actually close to optimal. However, these models fail when humans exhibit systematic suboptimality, i.e. when their deviations from optimal behavior are not independent, but instead consistent over time. Our key insight is that systematic suboptimality can be modeled by predicting policies, which couple action choices over time, instead of trajectories. We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difﬁcult to compute and represent because policies lie in a high-dimensional continuous space, but we leverage tools from generative and sequence models to enable efﬁcient sampling and inference. We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data.}
      \field{month}{4}
      \field{note}{Number: arXiv:2204.10759 arXiv:2204.10759 [cs]}
      \field{shorttitle}{The {Boltzmann} {Policy} {Distribution}}
      \field{title}{The {Boltzmann} {Policy} {Distribution}: {Accounting} for {Systematic} {Suboptimality} in {Human} {Models}}
      \field{urlday}{12}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2204.10759
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2204.10759
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics}
    \endentry
    \entry{chan_human_2021}{misc}{}
      \name{author}{3}{}{%
        {{hash=510dbf787b25c946b28ce73e81955092}{%
           family={Chan},
           familyi={C\bibinitperiod},
           given={Lawrence},
           giveni={L\bibinitperiod}}}%
        {{hash=045bea942c19f5861de09055c6a6187e}{%
           family={Critch},
           familyi={C\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=ac56afa5bcb9a863ab4555f3ac357ef5}{%
           family={Dragan},
           familyi={D\bibinitperiod},
           given={Anca},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{8053aac70c91442ec01036c719d432f1}
      \strng{fullhash}{8053aac70c91442ec01036c719d432f1}
      \strng{bibnamehash}{8053aac70c91442ec01036c719d432f1}
      \strng{authorbibnamehash}{8053aac70c91442ec01036c719d432f1}
      \strng{authornamehash}{8053aac70c91442ec01036c719d432f1}
      \strng{authorfullhash}{8053aac70c91442ec01036c719d432f1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference.}
      \field{month}{11}
      \field{note}{Number: arXiv:2111.06956 arXiv:2111.06956 [cs]}
      \field{shorttitle}{Human irrationality}
      \field{title}{Human irrationality: both bad and good for reward inference}
      \field{urlday}{12}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2111.06956
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2111.06956
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{bing_meta-reinforcement_2022}{article}{}
      \name{author}{4}{}{%
        {{hash=a7e1cd481a680d5ca7664dfa2abb5958}{%
           family={Bing},
           familyi={B\bibinitperiod},
           given={Zhenshan},
           giveni={Z\bibinitperiod}}}%
        {{hash=45551c07c608b3da0b72070a39725b19}{%
           family={Lerch},
           familyi={L\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=ade0b4c1c5cedeaecd33cd784e361ece}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=c45d8c33e4ef56b7d84b362173fe9d96}{%
           family={Knoll},
           familyi={K\bibinitperiod},
           given={Alois},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{78ad7a256df5076ca91c74767f263394}
      \strng{fullhash}{0d4318a6fca1fc9f4f07db6aaca46e74}
      \strng{bibnamehash}{0d4318a6fca1fc9f4f07db6aaca46e74}
      \strng{authorbibnamehash}{0d4318a6fca1fc9f4f07db6aaca46e74}
      \strng{authornamehash}{78ad7a256df5076ca91c74767f263394}
      \strng{authorfullhash}{0d4318a6fca1fc9f4f07db6aaca46e74}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, the subject of deep reinforcement learning (DRL) has developed very rapidly, and is now applied in various fields, such as decision making and control tasks. However, artificial agents trained with RL algorithms require great amounts of training data, unlike humans that are able to learn new skills from very few examples. The concept of meta-reinforcement learning (meta-RL) has been recently proposed to enable agents to learn similar but new skills from a small amount of experience by leveraging a set of tasks with a shared structure. Due to the task representation learning strategy with few-shot adaptation, most recent work is limited to narrow task distributions and stationary environments, where tasks do not change within episodes. In this work, we address those limitations and introduce a training strategy that is applicable to non-stationary environments, as well as a task representation based on Gaussian mixture models to model clustered task distributions. We evaluate our method on several continuous robotic control benchmarks. Compared with state-of-the-art literature that is only applicable to stationary environments with few-shot adaption, our algorithm first achieves competitive asymptotic performance and superior sample efficiency in stationary environments with zero-shot adaption. Second, our algorithm learns to perform successfully in non-stationary settings as well as a continual learning setting, while learning well-structured task representations. Last, our algorithm learns basic distinct behaviors and well-structured task representations in task distributions with multiple qualitatively distinct tasks.}
      \field{issn}{0162-8828, 2160-9292, 1939-3539}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{title}{Meta-{Reinforcement} {Learning} in {Non}-{Stationary} and {Dynamic} {Environments}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 17}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1109/TPAMI.2022.3185549
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9804728/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9804728/
      \endverb
    \endentry
    \entry{arango_multimodal_2021}{misc}{}
      \name{author}{4}{}{%
        {{hash=cbfe4eae76121b23e3d46ac8e89db292}{%
           family={Arango},
           familyi={A\bibinitperiod},
           given={Sebastian\bibnamedelima Pineda},
           giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=778b01bcee5702a980b1c630b8b7fc08}{%
           family={Heinrich},
           familyi={H\bibinitperiod},
           given={Felix},
           giveni={F\bibinitperiod}}}%
        {{hash=824e68257590e0960776fef7d8390ce9}{%
           family={Madhusudhanan},
           familyi={M\bibinitperiod},
           given={Kiran},
           giveni={K\bibinitperiod}}}%
        {{hash=6bd1e34895061f42175a4e3ec60b0ea6}{%
           family={Schmidt-Thieme},
           familyi={S\bibinithyphendelim T\bibinitperiod},
           given={Lars},
           giveni={L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{5df4b512323b04bd82b898b8e8277bc9}
      \strng{fullhash}{be4b2d8522c1b82e1ea110c2777be53b}
      \strng{bibnamehash}{be4b2d8522c1b82e1ea110c2777be53b}
      \strng{authorbibnamehash}{be4b2d8522c1b82e1ea110c2777be53b}
      \strng{authornamehash}{5df4b512323b04bd82b898b8e8277bc9}
      \strng{authorfullhash}{be4b2d8522c1b82e1ea110c2777be53b}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work has shown the eﬃciency of deep learning models such as Fully Convolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with Time Series Regression (TSR) problems. These models sometimes need a lot of data to be able to generalize, yet the time series are sometimes not long enough to be able to learn patterns. Therefore, it is important to make use of information across time series to improve learning. In this paper, we will explore the idea of using meta-learning for quickly adapting model parameters to new short-history time series by modifying the original idea of Model Agnostic Meta-Learning (MAML) [3]. Moreover, based on prior work on multimodal MAML [22], we propose a method for conditioning parameters of the model through an auxiliary network that encodes global information of the time series to extract meta-features. Finally, we apply the data to time series of diﬀerent domains, such as pollution measurements, heart-rate sensors, and electrical battery data. We show empirically that our proposed meta-learning method learns TSR with few data fast and outperforms the baselines in 9 of 12 experiments.}
      \field{month}{11}
      \field{note}{Number: arXiv:2108.02842 arXiv:2108.02842 [cs]}
      \field{title}{Multimodal {Meta}-{Learning} for {Time} {Series} {Regression}}
      \field{urlday}{2}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2108.02842
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2108.02842
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{ditterich_evidence_2006}{article}{}
      \name{author}{1}{}{%
        {{hash=6a66e7d069c01d989487f67fcbdb394f}{%
           family={Ditterich},
           familyi={D\bibinitperiod},
           given={Jochen},
           giveni={J\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{6a66e7d069c01d989487f67fcbdb394f}
      \strng{fullhash}{6a66e7d069c01d989487f67fcbdb394f}
      \strng{bibnamehash}{6a66e7d069c01d989487f67fcbdb394f}
      \strng{authorbibnamehash}{6a66e7d069c01d989487f67fcbdb394f}
      \strng{authornamehash}{6a66e7d069c01d989487f67fcbdb394f}
      \strng{authorfullhash}{6a66e7d069c01d989487f67fcbdb394f}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Computational models based on diffusion processes have been proposed to account for human decision-making behaviour in a variety of tasks. The basic idea is that the brain keeps accumulating noisy sensory evidence until a critical level is reached. This study explores whether such models account for the speed and accuracy of perceptual decisions in a reaction-time random dot motion direction discrimination task, and whether they explain the decision-related activity of neurons recorded from the parietal cortex (area LIP) of monkeys performing the task. While a simple diffusion model can explain the psychometric function and the mean response times of correct responses, it fails to account for the longer response times observed for errors and for the response time distributions. Here I demonstrate that a time-variant version of the diffusion model can explain the psychometric function, the mean response times and the shape of the response time distributions. Such a time-variant mechanism could be implemented in different ways, but the best match between the physiological data and model predictions is provided by a diffusion process with a gain of the sensory signals, which increases over time. It can be shown that such a time-variant decision process allows the monkey to perform optimally (in the sense of maximizing reward rate) given the risk of aborting a trial by breaking fixation before a choice can be reported. The results suggest that the brain trades off speed and accuracy not only by adjusting parameters between trials but also by dynamic adjustments during an ongoing decision.}
      \field{issn}{1460-9568}
      \field{journaltitle}{European Journal of Neuroscience}
      \field{number}{12}
      \field{title}{Evidence for time-variant decision making}
      \field{urlday}{29}
      \field{urlmonth}{6}
      \field{urlyear}{2023}
      \field{volume}{24}
      \field{year}{2006}
      \field{urldateera}{ce}
      \field{pages}{3628\bibrangedash 3641}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1111/j.1460-9568.2006.05221.x
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2006.05221.x
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2006.05221.x
      \endverb
      \keyw{LIP,computation,integration,model,monkey,parietal cortex}
    \endentry
    \entry{jang_categorical_2017}{misc}{}
      \name{author}{3}{}{%
        {{hash=3b21b8aef58f6e35446f2dbc679e5e6e}{%
           family={Jang},
           familyi={J\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=0072cdac4f418553b46ec3ddc61cebad}{%
           family={Gu},
           familyi={G\bibinitperiod},
           given={Shixiang},
           giveni={S\bibinitperiod}}}%
        {{hash=e528fda20644135dfae4f481a342148f}{%
           family={Poole},
           familyi={P\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{56a47665bbe4612de2bb387bea75ea33}
      \strng{fullhash}{56a47665bbe4612de2bb387bea75ea33}
      \strng{bibnamehash}{56a47665bbe4612de2bb387bea75ea33}
      \strng{authorbibnamehash}{56a47665bbe4612de2bb387bea75ea33}
      \strng{authornamehash}{56a47665bbe4612de2bb387bea75ea33}
      \strng{authorfullhash}{56a47665bbe4612de2bb387bea75ea33}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efﬁcient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classiﬁcation.}
      \field{month}{8}
      \field{note}{Number: arXiv:1611.01144 arXiv:1611.01144 [cs, stat]}
      \field{title}{Categorical {Reparameterization} with {Gumbel}-{Softmax}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1611.01144
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1611.01144
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{jiang_transformer_2020}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=b33717bc555fc7091aee0f02c4302693}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Junyan},
           giveni={J\bibinitperiod}}}%
        {{hash=104bf1e744b74b0af79f821a2d8a00cc}{%
           family={Xia},
           familyi={X\bibinitperiod},
           given={Gus\bibnamedelima G.},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=bb90436761c3de8ef2fa3705369083fa}{%
           family={Carlton},
           familyi={C\bibinitperiod},
           given={Dave\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=8f97ef93882c9cdfcdef7425a488377e}{%
           family={Anderson},
           familyi={A\bibinitperiod},
           given={Chris\bibnamedelima N.},
           giveni={C\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=5b95290ab5838e0be058bc9e6b29ca8e}{%
           family={Miyakawa},
           familyi={M\bibinitperiod},
           given={Ryan\bibnamedelima H.},
           giveni={R\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {Barcelona, Spain}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{0b99ee90d326039866ae750c5ecf7a0e}
      \strng{fullhash}{ac3c67b0e75f116594176312fd90efd7}
      \strng{bibnamehash}{ac3c67b0e75f116594176312fd90efd7}
      \strng{authorbibnamehash}{ac3c67b0e75f116594176312fd90efd7}
      \strng{authornamehash}{0b99ee90d326039866ae750c5ecf7a0e}
      \strng{authorfullhash}{ac3c67b0e75f116594176312fd90efd7}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Structure awareness and interpretability are two of the most desired properties of music generation algorithms. Structureaware models generate more natural and coherent music with long-term dependencies, while interpretable models are more friendly for human-computer interaction and co-creation. To achieve these two goals simultaneously, we designed the Transformer Variational AutoEncoder, a hierarchical model that uniﬁes the efforts of two recent breakthroughs in deep music generation: 1) the Music Transformer and 2) Deep Music Analogy. The former learns long-term dependencies using attention mechanism, and the latter learns interpretable latent representations using a disentangled conditional-VAE. We showed that Transformer VAE is essentially capable of learning a context-sensitive hierarchical representation, regarding local representations as the context and the dependencies among the local representations as the global structure. By interacting with the model, we can achieve context transfer, realizing the imaginary situation of “what if" a piece is developed following the music ﬂow of another piece.}
      \field{booktitle}{{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})}
      \field{isbn}{978-1-5090-6631-5}
      \field{month}{5}
      \field{shorttitle}{Transformer {VAE}}
      \field{title}{Transformer {VAE}: {A} {Hierarchical} {Model} for {Structure}-{Aware} and {Interpretable} {Music} {Representation} {Learning}}
      \field{urlday}{2}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{516\bibrangedash 520}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICASSP40776.2020.9054554
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9054554/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9054554/
      \endverb
    \endentry
    \entry{haarnoja_soft_2018}{misc}{}
      \name{author}{4}{}{%
        {{hash=45b14f55dd7ae28cb6cb2e23e4a168b9}{%
           family={Haarnoja},
           familyi={H\bibinitperiod},
           given={Tuomas},
           giveni={T\bibinitperiod}}}%
        {{hash=519033a7338e5ef684c77d4d04748a4a}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Aurick},
           giveni={A\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{79f7561871874c6a791348c3bc9badda}
      \strng{fullhash}{951e86110020d3107317b3e819f3d92d}
      \strng{bibnamehash}{951e86110020d3107317b3e819f3d92d}
      \strng{authorbibnamehash}{951e86110020d3107317b3e819f3d92d}
      \strng{authornamehash}{79f7561871874c6a791348c3bc9badda}
      \strng{authorfullhash}{951e86110020d3107317b3e819f3d92d}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
      \field{month}{8}
      \field{note}{Number: arXiv:1801.01290 arXiv:1801.01290 [cs, stat]}
      \field{shorttitle}{Soft {Actor}-{Critic}}
      \field{title}{Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1801.01290
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1801.01290
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{huang_cleanrl_nodate}{article}{}
      \name{author}{7}{}{%
        {{hash=a485c9f2edbeda28fc6122a6d8ef9913}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Shengyi},
           giveni={S\bibinitperiod}}}%
        {{hash=32e389f8fcbcf0f82233cdfb17fc7c2e}{%
           family={Dossa},
           familyi={D\bibinitperiod},
           given={Rousslan\bibnamedelimb Fernand\bibnamedelima Julien},
           giveni={R\bibinitperiod\bibinitdelim F\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=f1dee49ff1f7c58167e78dd52894041e}{%
           family={Ye},
           familyi={Y\bibinitperiod},
           given={Chang},
           giveni={C\bibinitperiod}}}%
        {{hash=d8aa41e941d28e7bf283e3e19f317610}{%
           family={Braga},
           familyi={B\bibinitperiod},
           given={Jeﬀ},
           giveni={J\bibinitperiod}}}%
        {{hash=4c84046e785d39de01f660f64cbadb3e}{%
           family={Chakraborty},
           familyi={C\bibinitperiod},
           given={Dipam},
           giveni={D\bibinitperiod}}}%
        {{hash=0fb774b6fae5ad2ed906e0161fa9f0bd}{%
           family={Mehta},
           familyi={M\bibinitperiod},
           given={Kinal},
           giveni={K\bibinitperiod}}}%
        {{hash=efc401b24715eff187740eb8bed66d7b}{%
           family={Araujo},
           familyi={A\bibinitperiod},
           given={Joao\bibnamedelimb G\bibnamedelima M},
           giveni={J\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{cabf9a50be29c11f91512df6de456273}
      \strng{fullhash}{a3214f03a7e95b36b5d38b0aeafce5bb}
      \strng{bibnamehash}{cabf9a50be29c11f91512df6de456273}
      \strng{authorbibnamehash}{cabf9a50be29c11f91512df6de456273}
      \strng{authornamehash}{cabf9a50be29c11f91512df6de456273}
      \strng{authorfullhash}{a3214f03a7e95b36b5d38b0aeafce5bb}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{CleanRL is an open-source library that provides high-quality single-ﬁle implementations of Deep Reinforcement Learning (DRL) algorithms. These single-ﬁle implementations are selfcontained algorithm variant ﬁles such as dqn.py, ppo.py, and ppo atari.py that individually include all algorithm variant’s implementation details. Such a paradigm signiﬁcantly reduces the complexity and the lines of code (LOC) in each implemented variant, which makes them quicker and easier to understand. This paradigm gives the researchers the most ﬁne-grained control over all aspects of the algorithm in a single ﬁle, allowing them to prototype novel features quickly. Despite having succinct implementations, CleanRL’s codebase is thoroughly documented and benchmarked to ensure performance is on par with reputable sources. As a result, CleanRL produces a repository tailor-ﬁt for two purposes: 1) understanding all implementation details of DRL algorithms and 2) quickly prototyping novel features. CleanRL’s source code can be found at https://github.com/vwxyzjn/cleanrl.}
      \field{title}{{CleanRL}: {High}-quality {Single}-ﬁle {Implementations} of {Deep} {Reinforcement} {Learning} {Algorithms}}
    \endentry
    \entry{papoudakis_benchmarking_2021}{misc}{}
      \name{author}{4}{}{%
        {{hash=d0631db9b5ca2e6150a04082663ca08d}{%
           family={Papoudakis},
           familyi={P\bibinitperiod},
           given={Georgios},
           giveni={G\bibinitperiod}}}%
        {{hash=f355421623a9ce0d54965817f05f6ad0}{%
           family={Christianos},
           familyi={C\bibinitperiod},
           given={Filippos},
           giveni={F\bibinitperiod}}}%
        {{hash=c32fcecebc45af27f83fbcfea1c93b31}{%
           family={Schäfer},
           familyi={S\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
        {{hash=9da4f3f413d514d731efe34067976909}{%
           family={Albrecht},
           familyi={A\bibinitperiod},
           given={Stefano\bibnamedelima V.},
           giveni={S\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e6a7a7ce69e533efd778054e8286d10c}
      \strng{fullhash}{fb48708063b34a78b84029004ce47bdb}
      \strng{bibnamehash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authorbibnamehash}{fb48708063b34a78b84029004ce47bdb}
      \strng{authornamehash}{e6a7a7ce69e533efd778054e8286d10c}
      \strng{authorfullhash}{fb48708063b34a78b84029004ce47bdb}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonlyused evaluation tasks and criteria, making comparisons between approaches difﬁcult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multiagent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for ﬂexible conﬁguration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.}
      \field{month}{11}
      \field{note}{Number: arXiv:2006.07869 arXiv:2006.07869 [cs, stat]}
      \field{title}{Benchmarking {Multi}-{Agent} {Deep} {Reinforcement} {Learning} {Algorithms} in {Cooperative} {Tasks}}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.07869
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.07869
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning}
    \endentry
    \entry{haarnoja_soft_2018-1}{misc}{}
      \name{author}{4}{}{%
        {{hash=45b14f55dd7ae28cb6cb2e23e4a168b9}{%
           family={Haarnoja},
           familyi={H\bibinitperiod},
           given={Tuomas},
           giveni={T\bibinitperiod}}}%
        {{hash=519033a7338e5ef684c77d4d04748a4a}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Aurick},
           giveni={A\bibinitperiod}}}%
        {{hash=e28d4ee199593959d8c29980a64f1974}{%
           family={Abbeel},
           familyi={A\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=c9545da54b33744da25943cdf66eadac}{%
           family={Levine},
           familyi={L\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{79f7561871874c6a791348c3bc9badda}
      \strng{fullhash}{951e86110020d3107317b3e819f3d92d}
      \strng{bibnamehash}{951e86110020d3107317b3e819f3d92d}
      \strng{authorbibnamehash}{951e86110020d3107317b3e819f3d92d}
      \strng{authornamehash}{79f7561871874c6a791348c3bc9badda}
      \strng{authorfullhash}{951e86110020d3107317b3e819f3d92d}
      \field{extraname}{2}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
      \field{month}{8}
      \field{note}{Number: arXiv:1801.01290 arXiv:1801.01290 [cs, stat]}
      \field{shorttitle}{Soft {Actor}-{Critic}}
      \field{title}{Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}}
      \field{urlday}{9}
      \field{urlmonth}{11}
      \field{urlyear}{2022}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1801.01290
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1801.01290
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{loaiza-ganem_continuous_2019}{misc}{}
      \name{author}{2}{}{%
        {{hash=f03f19255363c89a97a6b6d92bb517ee}{%
           family={Loaiza-Ganem},
           familyi={L\bibinithyphendelim G\bibinitperiod},
           given={Gabriel},
           giveni={G\bibinitperiod}}}%
        {{hash=2f7d2033f2af8255e45b666491f83c85}{%
           family={Cunningham},
           familyi={C\bibinitperiod},
           given={John\bibnamedelima P.},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{ad041a9525f6694328a9bd01b1315a09}
      \strng{fullhash}{ad041a9525f6694328a9bd01b1315a09}
      \strng{bibnamehash}{ad041a9525f6694328a9bd01b1315a09}
      \strng{authorbibnamehash}{ad041a9525f6694328a9bd01b1315a09}
      \strng{authornamehash}{ad041a9525f6694328a9bd01b1315a09}
      \strng{authorfullhash}{ad041a9525f6694328a9bd01b1315a09}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Variational autoencoders (VAE) have quickly become a central tool in machine learning, applicable to a broad range of data types and latent variable models. By far the most common first step, taken by seminal papers and by core software libraries alike, is to model MNIST data using a deep network parameterizing a Bernoulli likelihood. This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0,1] valued, not \{0,1\} as supported by the Bernoulli likelihood. Here we show that, far from being a triviality or nuisance that is convenient to ignore, this error has profound importance to VAE, both qualitative and quantitative. We introduce and fully characterize a new [0,1]-supported, single parameter distribution: the continuous Bernoulli, which patches this pervasive bug in VAE. This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets, including sharper image samples, and suggests a broader class of performant VAE.}
      \field{month}{12}
      \field{note}{Number: arXiv:1907.06845 arXiv:1907.06845 [cs, stat]}
      \field{shorttitle}{The continuous {Bernoulli}}
      \field{title}{The continuous {Bernoulli}: fixing a pervasive error in variational autoencoders}
      \field{urlday}{30}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.06845
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.06845
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
  \enddatalist
\endrefsection
\endinput

