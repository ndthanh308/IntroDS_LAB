
@article{huang_cleanrl_nodate,
	title = {{CleanRL}: {High}-quality {Single}-ﬁle {Implementations} of {Deep} {Reinforcement} {Learning} {Algorithms}},
	abstract = {CleanRL is an open-source library that provides high-quality single-ﬁle implementations of Deep Reinforcement Learning (DRL) algorithms. These single-ﬁle implementations are selfcontained algorithm variant ﬁles such as dqn.py, ppo.py, and ppo atari.py that individually include all algorithm variant’s implementation details. Such a paradigm signiﬁcantly reduces the complexity and the lines of code (LOC) in each implemented variant, which makes them quicker and easier to understand. This paradigm gives the researchers the most ﬁne-grained control over all aspects of the algorithm in a single ﬁle, allowing them to prototype novel features quickly. Despite having succinct implementations, CleanRL’s codebase is thoroughly documented and benchmarked to ensure performance is on par with reputable sources. As a result, CleanRL produces a repository tailor-ﬁt for two purposes: 1) understanding all implementation details of DRL algorithms and 2) quickly prototyping novel features. CleanRL’s source code can be found at https://github.com/vwxyzjn/cleanrl.},
	language = {en},
	author = {Huang, Shengyi and Dossa, Rousslan Fernand Julien and Ye, Chang and Braga, Jeﬀ and Chakraborty, Dipam and Mehta, Kinal and Araujo, Joao G M},
}

@article{ditterich_evidence_2006,
	title = {Evidence for time-variant decision making},
	volume = {24},
	issn = {1460-9568},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2006.05221.x},
	doi = {10.1111/j.1460-9568.2006.05221.x},
	abstract = {Computational models based on diffusion processes have been proposed to account for human decision-making behaviour in a variety of tasks. The basic idea is that the brain keeps accumulating noisy sensory evidence until a critical level is reached. This study explores whether such models account for the speed and accuracy of perceptual decisions in a reaction-time random dot motion direction discrimination task, and whether they explain the decision-related activity of neurons recorded from the parietal cortex (area LIP) of monkeys performing the task. While a simple diffusion model can explain the psychometric function and the mean response times of correct responses, it fails to account for the longer response times observed for errors and for the response time distributions. Here I demonstrate that a time-variant version of the diffusion model can explain the psychometric function, the mean response times and the shape of the response time distributions. Such a time-variant mechanism could be implemented in different ways, but the best match between the physiological data and model predictions is provided by a diffusion process with a gain of the sensory signals, which increases over time. It can be shown that such a time-variant decision process allows the monkey to perform optimally (in the sense of maximizing reward rate) given the risk of aborting a trial by breaking fixation before a choice can be reported. The results suggest that the brain trades off speed and accuracy not only by adjusting parameters between trials but also by dynamic adjustments during an ongoing decision.},
	language = {en},
	number = {12},
	urldate = {2023-06-29},
	journal = {European Journal of Neuroscience},
	author = {Ditterich, Jochen},
	year = {2006},
	keywords = {LIP, computation, integration, model, monkey, parietal cortex},
	pages = {3628--3641},
}

@article{obhi_moving_2011,
	title = {Moving together: toward understanding the mechanisms of joint action},
	volume = {211},
	issn = {0014-4819, 1432-1106},
	shorttitle = {Moving together},
	url = {http://link.springer.com/10.1007/s00221-011-2721-0},
	doi = {10.1007/s00221-011-2721-0},
	language = {en},
	number = {3-4},
	urldate = {2023-03-31},
	journal = {Experimental Brain Research},
	author = {Obhi, Sukhvinder S. and Sebanz, Natalie},
	month = jun,
	year = {2011},
	pages = {329--336},
}

@article{sheridan_eight_nodate,
	title = {{EIGHT} {ULTIMATE} {CHALLENGES} {F} {HUMAN}-{ROBOT} {COMMUNICATION}},
	language = {en},
	author = {Sheridan, Thomas B},
}

@misc{loaiza-ganem_continuous_2019,
	title = {The continuous {Bernoulli}: fixing a pervasive error in variational autoencoders},
	shorttitle = {The continuous {Bernoulli}},
	url = {http://arxiv.org/abs/1907.06845},
	abstract = {Variational autoencoders (VAE) have quickly become a central tool in machine learning, applicable to a broad range of data types and latent variable models. By far the most common first step, taken by seminal papers and by core software libraries alike, is to model MNIST data using a deep network parameterizing a Bernoulli likelihood. This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0,1] valued, not \{0,1\} as supported by the Bernoulli likelihood. Here we show that, far from being a triviality or nuisance that is convenient to ignore, this error has profound importance to VAE, both qualitative and quantitative. We introduce and fully characterize a new [0,1]-supported, single parameter distribution: the continuous Bernoulli, which patches this pervasive bug in VAE. This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets, including sharper image samples, and suggests a broader class of performant VAE.},
	language = {en},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Loaiza-Ganem, Gabriel and Cunningham, John P.},
	month = dec,
	year = {2019},
	note = {Number: arXiv:1907.06845
arXiv:1907.06845 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	language = {en},
	number = {4},
	urldate = {2023-03-30},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	note = {arXiv:1906.02691 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {307--392},
}

@misc{jang_categorical_2017,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efﬁcient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classiﬁcation.},
	language = {en},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = aug,
	year = {2017},
	note = {Number: arXiv:1611.01144
arXiv:1611.01144 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{papoudakis_benchmarking_2021,
	title = {Benchmarking {Multi}-{Agent} {Deep} {Reinforcement} {Learning} {Algorithms} in {Cooperative} {Tasks}},
	url = {http://arxiv.org/abs/2006.07869},
	abstract = {Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonlyused evaluation tasks and criteria, making comparisons between approaches difﬁcult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multiagent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for ﬂexible conﬁguration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.},
	language = {en},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Papoudakis, Georgios and Christianos, Filippos and Schäfer, Lukas and Albrecht, Stefano V.},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2006.07869
arXiv:2006.07869 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	language = {en},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {Number: arXiv:1801.01290
arXiv:1801.01290 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{noauthor_notitle_nodate,
}

@inproceedings{karita_comparative_2019,
	title = {A {Comparative} {Study} on {Transformer} vs {RNN} in {Speech} {Applications}},
	url = {http://arxiv.org/abs/1909.06317},
	doi = {10.1109/ASRU46091.2019.9003750},
	abstract = {Sequence-to-sequence models have been widely used in end-toend speech processing, for example, automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS). This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance in neural machine translation and other natural language processing applications. We undertook intensive studies in which we experimentally compared and analyzed Transformer and conventional recurrent neural networks (RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and signiﬁcant performance beneﬁts obtained with Transformer for each task including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to succeed our exciting outcomes.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Karita, Shigeki and Chen, Nanxin and Hayashi, Tomoki and Hori, Takaaki and Inaguma, Hirofumi and Jiang, Ziyan and Someki, Masao and Soplin, Nelson Enrique Yalta and Yamamoto, Ryuichi and Wang, Xiaofei and Watanabe, Shinji and Yoshimura, Takenori and Zhang, Wangyou},
	month = dec,
	year = {2019},
	note = {arXiv:1909.06317 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {449--456},
}

@inproceedings{nguyen_review_2019,
	address = {Naples, Italy},
	title = {Review of {Deep} {Reinforcement} {Learning} for {Robot} {Manipulation}},
	isbn = {978-1-5386-9245-5},
	url = {https://ieeexplore.ieee.org/document/8675643/},
	doi = {10.1109/IRC.2019.00120},
	abstract = {Reinforcement learning combined with neural networks has recently led to a wide range of successes in learning policies in different domains. For robot manipulation, reinforcement learning algorithms bring the hope for machines to have the human-like abilities by directly learning dexterous manipulation from raw pixels. In this review paper, we address the current status of reinforcement learning algorithms used in the ﬁeld. We also cover essential theoretical background and main issues with current algorithms, which are limiting their applications of reinforcement learning algorithms in solving practical problems in robotics. We also share our thoughts on a number of future directions for reinforcement learning research.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {2019 {Third} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	publisher = {IEEE},
	author = {Nguyen, Hai and La, Hung},
	month = feb,
	year = {2019},
	pages = {590--595},
}

@article{polydoros_survey_2017,
	title = {Survey of {Model}-{Based} {Reinforcement} {Learning}: {Applications} on {Robotics}},
	volume = {86},
	issn = {0921-0296, 1573-0409},
	shorttitle = {Survey of {Model}-{Based} {Reinforcement} {Learning}},
	url = {http://link.springer.com/10.1007/s10846-017-0468-y},
	doi = {10.1007/s10846-017-0468-y},
	abstract = {Reinforcement learning is an appealing approach for allowing robots to learn new tasks. Relevant literature reveals a plethora of methods, but at the same time makes clear the lack of implementations for dealing with real life challenges. Current expectations raise the demand for adaptable robots. We argue that, by employing model-based reinforcement learning, the—now limited—adaptability characteristics of robotic systems can be expanded. Also, model-based reinforcement learning exhibits advantages that makes it more applicable to real life use-cases compared to model-free methods. Thus, in this survey, modelbased methods that have been applied in robotics are covered. We categorize them based on the derivation of an optimal policy, the definition of the returns function, the type of the transition model and the learned task. Finally, we discuss the applicability of model-based reinforcement learning approaches in new applications, taking into consideration the state of the art in both algorithms and hardware.},
	language = {en},
	number = {2},
	urldate = {2023-03-30},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Polydoros, Athanasios S. and Nalpantidis, Lazaros},
	month = may,
	year = {2017},
	pages = {153--173},
}

@article{papoudakis_dealing_nodate,
	title = {Dealing with {Non}-{Stationarity} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modiﬁcations in the training procedure, such as centralized training, to learning representations of the opponent’s policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
	language = {en},
	author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V},
}

@article{bing_meta-reinforcement_2022,
	title = {Meta-{Reinforcement} {Learning} in {Non}-{Stationary} and {Dynamic} {Environments}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9804728/},
	doi = {10.1109/TPAMI.2022.3185549},
	abstract = {In recent years, the subject of deep reinforcement learning (DRL) has developed very rapidly, and is now applied in various fields, such as decision making and control tasks. However, artificial agents trained with RL algorithms require great amounts of training data, unlike humans that are able to learn new skills from very few examples. The concept of meta-reinforcement learning (meta-RL) has been recently proposed to enable agents to learn similar but new skills from a small amount of experience by leveraging a set of tasks with a shared structure. Due to the task representation learning strategy with few-shot adaptation, most recent work is limited to narrow task distributions and stationary environments, where tasks do not change within episodes. In this work, we address those limitations and introduce a training strategy that is applicable to non-stationary environments, as well as a task representation based on Gaussian mixture models to model clustered task distributions. We evaluate our method on several continuous robotic control benchmarks. Compared with state-of-the-art literature that is only applicable to stationary environments with few-shot adaption, our algorithm first achieves competitive asymptotic performance and superior sample efficiency in stationary environments with zero-shot adaption. Second, our algorithm learns to perform successfully in non-stationary settings as well as a continual learning setting, while learning well-structured task representations. Last, our algorithm learns basic distinct behaviors and well-structured task representations in task distributions with multiple qualitatively distinct tasks.},
	language = {en},
	urldate = {2023-03-30},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bing, Zhenshan and Lerch, David and Huang, Kai and Knoll, Alois},
	year = {2022},
	pages = {1--17},
}

@article{nomura_prediction_2008,
	title = {Prediction of {Human} {Behavior} in {Human}--{Robot} {Interaction} {Using} {Psychological} {Scales} for {Anxiety} and {Negative} {Attitudes} {Toward} {Robots}},
	volume = {24},
	issn = {1552-3098},
	url = {http://ieeexplore.ieee.org/document/4481184/},
	doi = {10.1109/TRO.2007.914004},
	abstract = {When people interact with communication robots in daily life, their attitudes and emotions toward the robots affect their behavior. From the perspective of robotics design, we need to investigate the inﬂuences of these attitudes and emotions on human–robot interaction. This paper reports our empirical study on the relationships between people’s attitudes and emotions, and their behavior toward a robot. In particular, we focused on negative attitudes, anxiety, and communication avoidance behavior, which have important implications for robotics design. For this purpose, we used two psychological scales that we had developed: negative attitudes toward robots scale (NARS) and robot anxiety scale (RAS). In the experiment, subjects and a humanoid robot are engaged in simple interactions including scenes of meeting, greeting, self-disclosure, and physical contact. Experimental results indicated that there is a relationship between negative attitudes and emotions, and communication avoidance behavior. A gender effect was also suggested.},
	language = {en},
	number = {2},
	urldate = {2023-03-30},
	journal = {IEEE Transactions on Robotics},
	author = {Nomura, Tatsuya and Kanda, Takayuki and Suzuki, Tomohiro and Kato, Kensuke},
	month = apr,
	year = {2008},
	pages = {442--451},
}

@inproceedings{kwon_when_2020,
	address = {Cambridge United Kingdom},
	title = {When {Humans} {Aren}'t {Optimal}: {Robots} that {Collaborate} with {Risk}-{Aware} {Humans}},
	isbn = {978-1-4503-6746-2},
	shorttitle = {When {Humans} {Aren}'t {Optimal}},
	url = {https://dl.acm.org/doi/10.1145/3319502.3374832},
	doi = {10.1145/3319502.3374832},
	abstract = {In order to collaborate safely and efficiently, robots need to anticipate how their human partners will behave. Some of today’s robots model humans as if they were also robots, and assume users are always optimal. Other robots account for human limitations, and relax this assumption so that the human is noisily rational. Both of these models make sense when the human receives deterministic rewards: i.e., gaining either \$100 or \$130 with certainty. But in realworld scenarios, rewards are rarely deterministic. Instead, we must make choices subject to risk and uncertainty—and in these settings, humans exhibit a cognitive bias towards suboptimal behavior. For example, when deciding between gaining \$100 with certainty or \$130 only 80\% of the time, people tend to make the risk-averse choice—even though it leads to a lower expected gain! In this paper, we adopt a well-known Risk-Aware human model from behavioral economics called Cumulative Prospect Theory and enable robots to leverage this model during human-robot interaction (HRI). In our user studies, we offer supporting evidence that the Risk-Aware model more accurately predicts suboptimal human behavior. We find that this increased modeling accuracy results in safer and more efficient human-robot collaboration. Overall, we extend existing rational human models so that collaborative robots can anticipate and plan around suboptimal human behavior during HRI.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the 2020 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Kwon, Minae and Biyik, Erdem and Talati, Aditi and Bhasin, Karan and Losey, Dylan P. and Sadigh, Dorsa},
	month = mar,
	year = {2020},
	pages = {43--52},
}

@inproceedings{bobu_less_2020,
	address = {Cambridge United Kingdom},
	title = {{LESS} is {More}: {Rethinking} {Probabilistic} {Models} of {Human} {Behavior}},
	isbn = {978-1-4503-6746-2},
	shorttitle = {{LESS} is {More}},
	url = {https://dl.acm.org/doi/10.1145/3319502.3374811},
	doi = {10.1145/3319502.3374811},
	abstract = {Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the 2020 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Bobu, Andreea and Scobee, Dexter R. R. and Fisac, Jaime F. and Sastry, S. Shankar and Dragan, Anca D.},
	month = mar,
	year = {2020},
	pages = {429--437},
}

@inproceedings{nikolaidis_human-robot_2017,
	address = {Vienna Austria},
	title = {Human-{Robot} {Mutual} {Adaptation} in {Shared} {Autonomy}},
	isbn = {978-1-4503-4336-7},
	url = {https://dl.acm.org/doi/10.1145/2909824.3020252},
	doi = {10.1145/2909824.3020252},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the 2017 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Nikolaidis, Stefanos and Zhu, Yu Xiang and Hsu, David and Srinivasa, Siddhartha},
	month = mar,
	year = {2017},
	pages = {294--302},
}

@misc{poiani_meta-reinforcement_2021,
	title = {Meta-{Reinforcement} {Learning} by {Tracking} {Task} {Non}-stationarity},
	url = {http://arxiv.org/abs/2105.08834},
	abstract = {Many real-world domains are subject to a structured non-stationarity which affects the agent’s goals and the environmental dynamics. Metareinforcement learning (RL) has been shown successful for training agents that quickly adapt to related tasks. However, most of the existing meta-RL algorithms for non-stationary domains either make strong assumptions on the task generation process or require sampling from it at training time. In this paper, we propose a novel algorithm (TRIO) that optimizes for the future by explicitly tracking the task evolution through time. At training time, TRIO learns a variational module to quickly identify latent parameters from experience samples. This module is learned jointly with an optimal exploration policy that takes task uncertainty into account. At test time, TRIO tracks the evolution of the latent parameters online, hence reducing the uncertainty over future tasks and obtaining fast adaptation through the meta-learned policy. Unlike most existing methods, TRIO does not assume Markovian task-evolution processes, it does not require information about the non-stationarity at training time, and it captures complex changes undergoing in the environment. We evaluate our algorithm on different simulated problems and show it outperforms competitive baselines.},
	language = {en},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Poiani, Riccardo and Tirinzoni, Andrea and Restelli, Marcello},
	month = may,
	year = {2021},
	note = {Number: arXiv:2105.08834
arXiv:2105.08834 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{jiang_transformer_2020,
	address = {Barcelona, Spain},
	title = {Transformer {VAE}: {A} {Hierarchical} {Model} for {Structure}-{Aware} and {Interpretable} {Music} {Representation} {Learning}},
	isbn = {978-1-5090-6631-5},
	shorttitle = {Transformer {VAE}},
	url = {https://ieeexplore.ieee.org/document/9054554/},
	doi = {10.1109/ICASSP40776.2020.9054554},
	abstract = {Structure awareness and interpretability are two of the most desired properties of music generation algorithms. Structureaware models generate more natural and coherent music with long-term dependencies, while interpretable models are more friendly for human-computer interaction and co-creation. To achieve these two goals simultaneously, we designed the Transformer Variational AutoEncoder, a hierarchical model that uniﬁes the efforts of two recent breakthroughs in deep music generation: 1) the Music Transformer and 2) Deep Music Analogy. The former learns long-term dependencies using attention mechanism, and the latter learns interpretable latent representations using a disentangled conditional-VAE. We showed that Transformer VAE is essentially capable of learning a context-sensitive hierarchical representation, regarding local representations as the context and the dependencies among the local representations as the global structure. By interacting with the model, we can achieve context transfer, realizing the imaginary situation of “what if" a piece is developed following the music ﬂow of another piece.},
	language = {en},
	urldate = {2023-03-02},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Jiang, Junyan and Xia, Gus G. and Carlton, Dave B. and Anderson, Chris N. and Miyakawa, Ryan H.},
	month = may,
	year = {2020},
	pages = {516--520},
}

@article{padakandla_reinforcement_2020,
	title = {Reinforcement {Learning} in {Non}-{Stationary} {Environments}},
	volume = {50},
	issn = {0924-669X, 1573-7497},
	url = {http://arxiv.org/abs/1905.03970},
	doi = {10.1007/s10489-020-01758-5},
	abstract = {Abstract Reinforcement learning (RL) methods learn optimal decisions in the presence of a stationary environment. However, the stationary assumption on the environment is very restrictive. In many real world problems like traﬃc signal control, robotic applications, etc., one often encounters situations with non-stationary environments and in these scenarios, RL methods yield sub-optimal decisions. In this paper, we thus consider the problem of developing RL methods that obtain optimal decisions in a non-stationary environment. The goal of this problem is to maximize the long-term discounted reward accrued when the underlying model of the environment changes over time. To achieve this, we ﬁrst adapt a change point algorithm to detect change in the statistics of the environment and then develop an RL algorithm that maximizes the long-run reward accrued. We illustrate that our change point method detects change in the model of the environment eﬀectively and thus facilitates the RL algorithm in maximizing the long-run reward. We further validate the eﬀectiveness of the proposed solution on non-stationary random Markov decision processes, a sensor energy management problem and a traﬃc signal control problem.},
	language = {en},
	number = {11},
	urldate = {2023-03-02},
	journal = {Applied Intelligence},
	author = {Padakandla, Sindhu and J, Prabuchandran K. and Bhatnagar, Shalabh},
	month = nov,
	year = {2020},
	note = {arXiv:1905.03970 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3590--3606},
}

@misc{arango_multimodal_2021,
	title = {Multimodal {Meta}-{Learning} for {Time} {Series} {Regression}},
	url = {http://arxiv.org/abs/2108.02842},
	abstract = {Recent work has shown the eﬃciency of deep learning models such as Fully Convolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with Time Series Regression (TSR) problems. These models sometimes need a lot of data to be able to generalize, yet the time series are sometimes not long enough to be able to learn patterns. Therefore, it is important to make use of information across time series to improve learning. In this paper, we will explore the idea of using meta-learning for quickly adapting model parameters to new short-history time series by modifying the original idea of Model Agnostic Meta-Learning (MAML) [3]. Moreover, based on prior work on multimodal MAML [22], we propose a method for conditioning parameters of the model through an auxiliary network that encodes global information of the time series to extract meta-features. Finally, we apply the data to time series of diﬀerent domains, such as pollution measurements, heart-rate sensors, and electrical battery data. We show empirically that our proposed meta-learning method learns TSR with few data fast and outperforms the baselines in 9 of 12 experiments.},
	language = {en},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Arango, Sebastian Pineda and Heinrich, Felix and Madhusudhanan, Kiran and Schmidt-Thieme, Lars},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2108.02842
arXiv:2108.02842 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{bing_meta-reinforcement_2022-1,
	title = {Meta-{Reinforcement} {Learning} in {Non}-{Stationary} and {Dynamic} {Environments}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9804728/},
	doi = {10.1109/TPAMI.2022.3185549},
	abstract = {In recent years, the subject of deep reinforcement learning (DRL) has developed very rapidly, and is now applied in various fields, such as decision making and control tasks. However, artificial agents trained with RL algorithms require great amounts of training data, unlike humans that are able to learn new skills from very few examples. The concept of meta-reinforcement learning (meta-RL) has been recently proposed to enable agents to learn similar but new skills from a small amount of experience by leveraging a set of tasks with a shared structure. Due to the task representation learning strategy with few-shot adaptation, most recent work is limited to narrow task distributions and stationary environments, where tasks do not change within episodes. In this work, we address those limitations and introduce a training strategy that is applicable to non-stationary environments, as well as a task representation based on Gaussian mixture models to model clustered task distributions. We evaluate our method on several continuous robotic control benchmarks. Compared with state-of-the-art literature that is only applicable to stationary environments with few-shot adaption, our algorithm first achieves competitive asymptotic performance and superior sample efficiency in stationary environments with zero-shot adaption. Second, our algorithm learns to perform successfully in non-stationary settings as well as a continual learning setting, while learning well-structured task representations. Last, our algorithm learns basic distinct behaviors and well-structured task representations in task distributions with multiple qualitatively distinct tasks.},
	language = {en},
	urldate = {2023-03-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bing, Zhenshan and Lerch, David and Huang, Kai and Knoll, Alois},
	year = {2022},
	pages = {1--17},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {Number: arXiv:1706.03762
arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{stouraitis_online_2020,
	title = {Online {Hybrid} {Motion} {Planning} for {Dyadic} {Collaborative} {Manipulation} via {Bilevel} {Optimization}},
	volume = {36},
	issn = {1552-3098, 1941-0468},
	url = {https://ieeexplore.ieee.org/document/9166536/},
	doi = {10.1109/TRO.2020.2992987},
	abstract = {Effective collaboration is based on online adaptation of one’s own actions to the actions of their partner. This article provides a principled formalism to address online adaptation in joint planning problems such as Dyadic collaborative Manipulation (DcM) scenarios. We propose an efﬁcient bilevel formulation that combines graph search methods with trajectory optimization, enabling robotic agents to adapt their policy on-the-ﬂy in accordance to changes of the dyadic task. This method is the ﬁrst to empower agents with the ability to plan online in hybrid spaces; optimizing over discrete contact locations, contact sequence patterns, continuous trajectories, and force proﬁles for co-manipulation tasks. This is particularly important in large object co-manipulation that requires changes of grasp-holds and plan adaptation. We demonstrate in simulation and with robot experiments the efﬁcacy of the bilevel optimization by investigating the effect of robot policy changes in response to real-time alterations of the dyadic goals, eminent grasp switches, as well as optimal dyadic interactions to realize the joint task.},
	language = {en},
	number = {5},
	urldate = {2023-03-02},
	journal = {IEEE Transactions on Robotics},
	author = {Stouraitis, Theodoros and Chatzinikolaidis, Iordanis and Gienger, Michael and Vijayakumar, Sethu},
	month = oct,
	year = {2020},
	pages = {1452--1471},
}

@misc{janner_offline_2021,
	title = {Offline {Reinforcement} {Learning} as {One} {Big} {Sequence} {Modeling} {Problem}},
	url = {http://arxiv.org/abs/2106.02039},
	abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simpliﬁes a range of design decisions, allowing us to dispense with many of the components common in ofﬂine RL algorithms. We demonstrate the ﬂexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and ofﬂine RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
	language = {en},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
	month = nov,
	year = {2021},
	note = {arXiv:2106.02039 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{chen_decision_2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	shorttitle = {Decision {Transformer}},
	url = {http://arxiv.org/abs/2106.01345},
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that ﬁt value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free ofﬂine RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	language = {en},
	urldate = {2023-01-07},
	publisher = {arXiv},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {Number: arXiv:2106.01345
arXiv:2106.01345 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{janner_offline_2021-1,
	title = {Offline {Reinforcement} {Learning} as {One} {Big} {Sequence} {Modeling} {Problem}},
	url = {http://arxiv.org/abs/2106.02039},
	abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simpliﬁes a range of design decisions, allowing us to dispense with many of the components common in ofﬂine RL algorithms. We demonstrate the ﬂexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and ofﬂine RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
	language = {en},
	urldate = {2023-01-07},
	publisher = {arXiv},
	author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2106.02039
arXiv:2106.02039 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zerveas_transformer-based_2020,
	title = {A {Transformer}-based {Framework} for {Multivariate} {Time} {Series} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2010.02803},
	abstract = {In this work we propose for the ﬁrst time a transformer-based framework for unsupervised representation learning of multivariate time series. Pre-trained models can be potentially used for downstream tasks such as regression and classiﬁcation, forecasting and missing value imputation. By evaluating our models on several benchmark datasets for multivariate time series regression and classiﬁcation, we show that our modeling approach represents the most successful method employing unsupervised learning of multivariate time series presented to date; it is also the ﬁrst unsupervised approach shown to exceed the current state-of-the-art performance of supervised methods. It does so by a signiﬁcant margin, even when the number of training samples is very limited, while offering computational efﬁciency. Finally, we demonstrate that unsupervised pre-training of our transformer models offers a substantial performance beneﬁt over fully supervised learning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples through the unsupervised objective.},
	language = {en},
	urldate = {2023-01-07},
	publisher = {arXiv},
	author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
	month = dec,
	year = {2020},
	note = {Number: arXiv:2010.02803
arXiv:2010.02803 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{salinas_deepar_2020,
	title = {{DeepAR}: {Probabilistic} forecasting with autoregressive recurrent networks},
	volume = {36},
	issn = {01692070},
	shorttitle = {{DeepAR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301888},
	doi = {10.1016/j.ijforecast.2019.07.001},
	abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
	language = {en},
	number = {3},
	urldate = {2023-01-03},
	journal = {International Journal of Forecasting},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
	month = jul,
	year = {2020},
	pages = {1181--1191},
}

@misc{liu_non-stationary_2022,
	title = {Non-stationary {Transformers}: {Exploring} the {Stationarity} in {Time} {Series} {Forecasting}},
	shorttitle = {Non-stationary {Transformers}},
	url = {http://arxiv.org/abs/2205.14415},
	abstract = {Transformers have shown great power in time series forecasting due to their global-range modeling ability. However, their performance can degenerate terribly on non-stationary real-world data in which the joint distribution changes over time. Previous studies primarily adopt stationarization to attenuate the nonstationarity of original series for better predictability. But the stationarized series deprived of inherent non-stationarity can be less instructive for real-world bursty events forecasting. This problem, termed over-stationarization in this paper, leads Transformers to generate indistinguishable temporal attentions for different series and impedes the predictive capability of deep models. To tackle the dilemma between series predictability and model capability, we propose Non-stationary Transformers as a generic framework with two interdependent modules: Series Stationarization and De-stationary Attention. Concretely, Series Stationarization uniﬁes the statistics of each input and converts the output with restored statistics for better predictability. To address the over-stationarization problem, Destationary Attention is devised to recover the intrinsic non-stationary information into temporal dependencies by approximating distinguishable attentions learned from raw series. Our Non-stationary Transformers framework consistently boosts mainstream Transformers by a large margin, which reduces MSE by 49.43\% on Transformer, 47.34\% on Informer, and 46.89\% on Reformer, making them the state-of-the-art in time series forecasting. Code is available at this repository: https://github.com/thuml/Nonstationary\_Transformers.},
	language = {en},
	urldate = {2023-01-02},
	publisher = {arXiv},
	author = {Liu, Yong and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
	month = oct,
	year = {2022},
	note = {Number: arXiv:2205.14415
arXiv:2205.14415 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@article{tang_probabilistic_nodate,
	title = {Probabilistic {Transformer} for {Time} {Series} {Analysis}},
	abstract = {Generative modeling of multivariate time series has remained challenging partly due to the complex, non-deterministic dynamics across long-distance time steps. In this paper, we propose deep probabilistic methods that combine state-space models (SSMs) with transformer architectures. In contrast to previously proposed SSMs, our approaches use attention mechanism to model non-Markovian dynamics in the latent space and avoid recurrent neural networks entirely. We also extend our models to include several layers of stochastic variables organized in a hierarchy for further expressiveness. Compared to transformer models, ours are probabilistic, non-autoregressive, and capable of generating diverse long-term forecasts with accounted uncertainty. Extensive experiments show that our models consistently outperform competitive baselines on various tasks and datasets, including time series forecasting and human motion prediction.},
	language = {en},
	author = {Tang, Binh and Matteson, David S},
}

@article{chang_hierarchical_nodate,
	title = {Hierarchical {Abstraction} for {Combinatorial} {Generalization} in {Object} {Rearrangement}},
	abstract = {Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of underlying entities that take the value of object states. Worse, these entities are often unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured inputs. By constructing a factorized transition graph over clusters of object representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent’s model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on a set of simulated rearrangement and stacking tasks.},
	language = {en},
	author = {Chang, Michael and Dayan, Alyssa L and Meier, Franziska and Griffiths, Thomas L and Levine, Sergey and Zhang, Amy},
}

@misc{bera_podnet_2020,
	title = {{PODNet}: {A} {Neural} {Network} for {Discovery} of {Plannable} {Options}},
	shorttitle = {{PODNet}},
	url = {http://arxiv.org/abs/1911.00171},
	abstract = {Learning from demonstration has been widely studied in machine learning but becomes challenging when the demonstrated trajectories are unstructured and follow different objectives. This short-paper proposes PODNet, Plannable Option Discovery Network, addressing how to segment an unstructured set of demonstrated trajectories for option discovery. This enables learning from demonstration to perform multiple tasks and plan high-level trajectories based on the discovered option labels. PODNet combines a custom categorical variational autoencoder, a recurrent option inference network, option-conditioned policy network, and option dynamics model in an end-to-end learning architecture. Due to the concurrently trained option-conditioned policy network and option dynamics model, the proposed architecture has implications in multi-task and hierarchical learning, explainable and interpretable artiﬁcial intelligence, and applications where the agent is required to learn only from observations.},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Bera, Ritwik and Goecks, Vinicius G. and Gremillion, Gregory M. and Valasek, John and Waytowich, Nicholas R.},
	month = feb,
	year = {2020},
	note = {Number: arXiv:1911.00171
arXiv:1911.00171 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6, Statistics - Machine Learning},
}

@misc{zheng_disentangling_2019,
	title = {Disentangling {Latent} {Space} for {VAE} by {Label} {Relevant}/{Irrelevant} {Dimensions}},
	url = {http://arxiv.org/abs/1812.09502},
	abstract = {VAE requires the standard Gaussian distribution as a prior in the latent space. Since all codes tend to follow the same prior, it often suffers the so-called ”posterior collapse”. To avoid this, this paper introduces the class speciﬁc distribution for the latent code. But different from cVAE, we present a method for disentangling the latent space into the label relevant and irrelevant dimensions, zs and zu, for a single input. We apply two separated encoders to map the input into zs and zu respectively, and then give the concatenated code to the decoder to reconstruct the input. The label irrelevant code zu represent the common characteristics of all inputs, hence they are constrained by the standard Gaussian, and their encoder is trained in amortized variational inference way, like VAE. While zs is assumed to follow the Gaussian mixture distribution in which each component corresponds to a particular class. The parameters for the Gaussian components in zs encoder are optimized by the label supervision in a global stochastic way. In theory, we show that our method is actually equivalent to adding a KL divergence term on the joint distribution of zs and the class label c, and it can directly increase the mutual information between zs and the label c. Our model can also be extended to GAN by adding a discriminator in the pixel domain so that it produces high quality and diverse images.},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Zheng, Zhilin and Sun, Li},
	month = mar,
	year = {2019},
	note = {Number: arXiv:1812.09502
arXiv:1812.09502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lynch_learning_nodate,
	title = {Learning {Latent} {Plans} from {Play}},
	language = {en},
	author = {Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre},
}

@misc{dunion_temporal_2022,
	title = {Temporal {Disentanglement} of {Representations} for {Improved} {Generalisation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2207.05480},
	abstract = {Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image, which can lead to drastic changes in the agent’s latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We ﬁnd empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, we also ﬁnd that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Dunion, Mhairi and McInroe, Trevor and Luck, Kevin Sebastian and Hanna, Josiah and Albrecht, Stefano V.},
	month = nov,
	year = {2022},
	note = {Number: arXiv:2207.05480
arXiv:2207.05480 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{ilse_diva_nodate,
	title = {{DIVA}: {Domain} {Invariant} {Variational} {Autoencoders}},
	abstract = {We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in ﬁelds like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.},
	language = {en},
	author = {Ilse, Maximilian and Tomczak, Jakub M and Louizos, Christos and Welling, Max},
}

@article{zhang_mode-adaptive_2018,
	title = {Mode-adaptive neural networks for quadruped motion control},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3197517.3201366},
	doi = {10.1145/3197517.3201366},
	abstract = {Quadruped motion includes a wide variation of gaits such as walk, pace, trot and canter, and actions such as jumping, sitting, turning and idling. Applying existing data-driven character control frameworks to such data requires a significant amount of data preprocessing such as motion labeling and alignment. In this paper, we propose a novel neural network architecture called Mode-Adaptive Neural Networks for controlling quadruped characters. The system is composed of the motion prediction network and the gating network. At each frame, the motion prediction network computes the character state in the current frame given the state in the previous frame and the user-provided control signals. The gating network dynamically updates the weights of the motion prediction network by selecting and blending what we call the expert weights, each of which specializes in a particular movement. Due to the increased flexibility, the system can learn consistent expert weights across a wide range of non-periodic/periodic actions, from unstructured motion capture data, in an end-to-end fashion. In addition, the users are released from performing complex labeling of phases in different gaits. We show that this architecture is suitable for encoding the multi-modality of quadruped locomotion and synthesizing responsive motion in real-time.},
	language = {en},
	number = {4},
	urldate = {2022-12-07},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, He and Starke, Sebastian and Komura, Taku and Saito, Jun},
	month = aug,
	year = {2018},
	pages = {1--11},
}

@article{haruno_multiple_nodate,
	title = {Multiple {Paired} {Forward}-{Inverse} {Models} for {Human} {Motor} {Learning} and {Control}},
	abstract = {Humans demonstrate a remarkable ability to generate accurate and appropriate motor behavior under many different and oftpn uncprtain environmental conditions. This paper describes a new modular approach to human motor learning and control, baspd on multiple pairs of inverse (controller) and forward (prpdictor) models. This architecture simultaneously learns the multiple inverse models necessary for control as well as how to select the inverse models appropriate for a given em'ironm0nt. Simulations of object manipulation demonstrates the ability to learn mUltiple objects, appropriate generalization to novel objects and the inappropriate activation of motor programs based on visual cues, followed by on-line correction, seen in the "size-weight illusion".},
	language = {en},
	author = {Haruno, Masahiko and Wolpert, Daniel M and Kawato, Mitsuo},
	pages = {7},
}

@misc{yang_optimal_2022,
	title = {Optimal {Behavior} {Prior}: {Data}-{Efficient} {Human} {Models} for {Improved} {Human}-{AI} {Collaboration}},
	shorttitle = {Optimal {Behavior} {Prior}},
	url = {http://arxiv.org/abs/2211.01602},
	abstract = {AI agents designed to collaborate with people beneﬁt from models that enable them to anticipate human behavior. However, realistic models tend to require vast amounts of human data, which is often hard to collect. A good prior or initialization could make for more data-efﬁcient training, but what makes for a good prior on human behavior? Our work leverages a very simple assumption: people generally act closer to optimal than to random chance. We show that using optimal behavior as a prior for human models makes these models vastly more data-efﬁcient and able to generalize to new environments. Our intuition is that such a prior enables the training to focus one’s precious real-world data on capturing the subtle nuances of human suboptimality, instead of on the basics of how to do the task in the ﬁrst place. We also show that using these improved human models often leads to better human-AI collaboration performance compared to using models based on real human data alone.},
	language = {en},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Yang, Mesut and Carroll, Micah and Dragan, Anca},
	month = nov,
	year = {2022},
	note = {arXiv:2211.01602 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{parekh_rili_2022,
	title = {{RILI}: {Robustly} {Influencing} {Latent} {Intent}},
	shorttitle = {{RILI}},
	url = {http://arxiv.org/abs/2203.12705},
	abstract = {When robots interact with human partners, often these partners change their behavior in response to the robot. On the one hand this is challenging because the robot must learn to coordinate with a dynamic partner. But on the other hand -- if the robot understands these dynamics -- it can harness its own behavior, influence the human, and guide the team towards effective collaboration. Prior research enables robots to learn to influence other robots or simulated agents. In this paper we extend these learning approaches to now influence humans. What makes humans especially hard to influence is that -- not only do humans react to the robot -- but the way a single user reacts to the robot may change over time, and different humans will respond to the same robot behavior in different ways. We therefore propose a robust approach that learns to influence changing partner dynamics. Our method first trains with a set of partners across repeated interactions, and learns to predict the current partner's behavior based on the previous states, actions, and rewards. Next, we rapidly adapt to new partners by sampling trajectories the robot learned with the original partners, and then leveraging those existing behaviors to influence the new partner dynamics. We compare our resulting algorithm to state-of-the-art baselines across simulated environments and a user study where the robot and participants collaborate to build towers. We find that our approach outperforms the alternatives, even when the partner follows new or unexpected dynamics. Videos of the user study are available here: https://youtu.be/lYsWM8An18g},
	language = {en},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Parekh, Sagar and Habibian, Soheil and Losey, Dylan P.},
	month = jul,
	year = {2022},
	note = {arXiv:2203.12705 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{parekh_rili_2022-1,
	title = {{RILI}: {Robustly} {Influencing} {Latent} {Intent}},
	shorttitle = {{RILI}},
	url = {http://arxiv.org/abs/2203.12705},
	abstract = {When robots interact with human partners, often these partners change their behavior in response to the robot. On the one hand this is challenging because the robot must learn to coordinate with a dynamic partner. But on the other hand -- if the robot understands these dynamics -- it can harness its own behavior, influence the human, and guide the team towards effective collaboration. Prior research enables robots to learn to influence other robots or simulated agents. In this paper we extend these learning approaches to now influence humans. What makes humans especially hard to influence is that -- not only do humans react to the robot -- but the way a single user reacts to the robot may change over time, and different humans will respond to the same robot behavior in different ways. We therefore propose a robust approach that learns to influence changing partner dynamics. Our method first trains with a set of partners across repeated interactions, and learns to predict the current partner's behavior based on the previous states, actions, and rewards. Next, we rapidly adapt to new partners by sampling trajectories the robot learned with the original partners, and then leveraging those existing behaviors to influence the new partner dynamics. We compare our resulting algorithm to state-of-the-art baselines across simulated environments and a user study where the robot and participants collaborate to build towers. We find that our approach outperforms the alternatives, even when the partner follows new or unexpected dynamics. Videos of the user study are available here: https://youtu.be/lYsWM8An18g},
	language = {en},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Parekh, Sagar and Habibian, Soheil and Losey, Dylan P.},
	month = jul,
	year = {2022},
	note = {arXiv:2203.12705 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{haarnoja_soft_2018-1,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	language = {en},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {Number: arXiv:1801.01290
arXiv:1801.01290 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lanier_feasible_2022,
	title = {Feasible {Adversarial} {Robust} {Reinforcement} {Learning} for {Underspecified} {Environments}},
	url = {http://arxiv.org/abs/2207.09597},
	abstract = {Robust reinforcement learning (RL) considers the problem of learning policies that perform well in the worst case among a set of possible environment parameter values. In real-world environments, choosing the set of possible values for robust RL can be a difﬁcult task. When that set is speciﬁed too narrowly, the agent will be left vulnerable to reasonable parameter values unaccounted for. When speciﬁed too broadly, the agent will be too cautious. In this paper, we propose Feasible Adversarial Robust RL (FARR), a novel problem formulation and objective for automatically determining the set of environment parameter values over which to be robust. FARR implicitly deﬁnes the set of feasible parameter values as those on which an agent could achieve a benchmark reward given enough training resources. By formulating this problem as a two-player zero-sum game, optimizing the FARR objective jointly produces an adversarial distribution over parameter values with feasible support and a policy robust over this feasible parameter set. We demonstrate that approximate Nash equilibria for this objective can be found using a variation of the PSRO algorithm. Furthermore, we show that an optimal agent trained with FARR is more robust to feasible adversarial parameter selection than with existing minimax, domain-randomization, and regret objectives in a parameterized gridworld and three MuJoCo control environments.},
	language = {en},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Lanier, J. B. and McAleer, Stephen and Baldi, Pierre and Fox, Roy},
	month = oct,
	year = {2022},
	note = {Number: arXiv:2207.09597
arXiv:2207.09597 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{christodoulou_soft_2019,
	title = {Soft {Actor}-{Critic} for {Discrete} {Action} {Settings}},
	url = {http://arxiv.org/abs/1910.07207},
	abstract = {Soft Actor-Critic is a state-of-the-art reinforcement learning algorithm for continuous action settings that is not applicable to discrete action settings. Many important settings involve discrete actions, however, and so here we derive an alternative version of the Soft Actor-Critic algorithm that is applicable to discrete action settings. We then show that, even without any hyperparameter tuning, it is competitive with the tuned model-free state-of-the-art on a selection of games from the Atari suite.},
	language = {en},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Christodoulou, Petros},
	month = oct,
	year = {2019},
	note = {Number: arXiv:1910.07207
arXiv:1910.07207 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{albrecht_game-theoretic_2015,
	title = {A {Game}-{Theoretic} {Model} and {Best}-{Response} {Learning} {Method} for {Ad} {Hoc} {Coordination} in {Multiagent} {Systems}},
	url = {http://arxiv.org/abs/1506.01170},
	abstract = {The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal ﬂexibility and eﬃciency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to ﬁnd optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher ﬂexibility and eﬃciency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner’s Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal eﬃciency and a signiﬁcantly higher welfare and winning rate.},
	language = {en},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
	month = jun,
	year = {2015},
	note = {Number: arXiv:1506.01170
arXiv:1506.01170 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
}

@misc{kim_influencing_2022,
	title = {Influencing {Long}-{Term} {Behavior} in {Multiagent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2203.03535},
	abstract = {The main challenge of multiagent reinforcement learning is the difﬁculty of learning useful policies in the presence of other simultaneously learning agents whose changing behaviors jointly affect the environment’s transition and reward dynamics. An effective approach that has recently emerged for addressing this non-stationarity is for each agent to anticipate the learning of other agents and inﬂuence the evolution of future policies towards desirable behavior for its own beneﬁt. Unfortunately, previous approaches for achieving this suffer from myopic evaluation, considering only a ﬁnite number of policy updates. As such, these methods can only inﬂuence transient future policies rather than achieving the promise of scalable equilibrium selection approaches that inﬂuence the behavior at convergence. In this paper, we propose a principled framework for considering the limiting policies of other agents as time approaches inﬁnity. Speciﬁcally, we develop a new optimization objective that maximizes each agent’s average reward by directly accounting for the impact of its behavior on the limiting set of policies that other agents will converge to. Our paper characterizes desirable solution concepts within this problem setting and provides practical approaches for optimizing over possible outcomes. As a result of our farsighted objective, we demonstrate better long-term performance than state-of-the-art baselines across a suite of diverse multiagent benchmark domains.},
	language = {en},
	urldate = {2022-10-11},
	publisher = {arXiv},
	author = {Kim, Dong-Ki and Riemer, Matthew and Liu, Miao and Foerster, Jakob N. and Everett, Michael and Sun, Chuangchuang and Tesauro, Gerald and How, Jonathan P.},
	month = may,
	year = {2022},
	note = {Number: arXiv:2203.03535
arXiv:2203.03535 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{albrecht_game-theoretic_nodate,
	title = {A {Game}-{Theoretic} {Model} and {Best}-{Response} {Learning} {Method} for {Ad} {Hoc} {Coordination} in {Multiagent} {Systems} ({Extended} {Abstract})},
	abstract = {The ad hoc coordination problem is to design an ad hoc agent which is able to achieve optimal ﬂexibility and eﬃciency in a multiagent system that admits no prior coordination between the ad hoc agent and the other agents. We conceptualise this problem formally as a stochastic Bayesian game in which the behaviour of a player is determined by its type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises a set of user-deﬁned types to characterise players based on their observed behaviours. We evaluate HBA in the level-based foraging domain, showing that it outperforms several alternative algorithms using just a few user-deﬁned types. We also report on a human-machine experiment in which the humans played Prisoner’s Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms. The results show that HBA achieved equal eﬃciency but a signiﬁcantly higher welfare and winning rate.},
	language = {en},
	author = {Albrecht, Stefano V and Ramamoorthy, Subramanian},
	pages = {2},
}

@misc{albrecht_game-theoretic_2015-1,
	title = {A {Game}-{Theoretic} {Model} and {Best}-{Response} {Learning} {Method} for {Ad} {Hoc} {Coordination} in {Multiagent} {Systems}},
	url = {http://arxiv.org/abs/1506.01170},
	abstract = {The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal ﬂexibility and eﬃciency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to ﬁnd optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher ﬂexibility and eﬃciency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner’s Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal eﬃciency and a signiﬁcantly higher welfare and winning rate.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
	month = jun,
	year = {2015},
	note = {Number: arXiv:1506.01170
arXiv:1506.01170 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
}

@misc{bernhard_robust_2020,
	title = {Robust {Stochastic} {Bayesian} {Games} for {Behavior} {Space} {Coverage}},
	url = {http://arxiv.org/abs/2003.11281},
	abstract = {A key challenge in multi-agent systems is the design of intelligent agents solving real-world tasks in close interaction with other agents (e.g. humans), thereby being confronted with a variety of behavioral variations and limited knowledge about the true behaviors of observed agents. The practicability of existing works addressing this challenge is being limited due to using ﬁnite sets of hypothesis for behavior prediction, the lack of a hypothesis design process ensuring coverage over all behavioral variations and sample-inefﬁciency when modeling continuous behavioral variations. In this work, we present an approach to this challenge based on a new framework of Robust Stochastic Bayesian Games (RSBGs). An RSBG deﬁnes hypothesis sets by partitioning the physically feasible, continuous behavior space of the other agents. It combines the optimality criteria of the Robust Markov Decision Process (RMDP) and the Stochastic Bayesian Game (SBG) to exponentially reduce the sample complexity for planning with hypothesis sets deﬁned over continuous behavior spaces. Our approach outperforms the baseline algorithms in two experiments modeling timevarying intents and large multidimensional behavior spaces, while achieving the same performance as a planner with knowledge of the true behaviors of other agents.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Bernhard, Julian and Knoll, Alois},
	month = jul,
	year = {2020},
	note = {Number: arXiv:2003.11281
arXiv:2003.11281 [cs]},
	keywords = {Computer Science - Multiagent Systems, I.2.11, I.2.8, I.2.9},
}

@article{orlov-savko_factorial_2022,
	title = {Factorial {Agent} {Markov} {Model}: {Modeling} {Other} {Agents}' {Behavior} in presence of {Dynamic} {Latent} {Decision} {Factors}},
	abstract = {Autonomous agents operating in the real world often need to interact with other agents to accomplish their tasks. For such agents, the ability to model behavior of other agents – both human and artificial – without complete knowledge of their decision factors is essential. Towards realizing this ability, we present Factorial Agent Markov Model (FAMM), a model to represent behavior of other agents performing sequential tasks. In contrast with most existing models, FAMM allows for behavior of other agents to depend on multiple, time-varying latent decision factors and does not assume rationality. To enable learning of FAMM parameters by observing behavior of other agents, we provide a set of variational inference algorithms for the unsupervised, semi-supervised, and supervised settings. These Bayesian learning algorithms for the FAMM enable agents to model other agents using execution traces and domain-specific priors. We demonstrate the utility of FAMM and corresponding learning algorithms using three synthetic domains and benchmark them against existing algorithms for modeling agent behavior. Our numerical experiments demonstrate that, despite the presence of multiple and time-varying latent states, our approach is capable of learning predictive models of other agents with semi-supervision.},
	language = {en},
	author = {Orlov-Savko, Liubove},
	year = {2022},
	pages = {9},
}

@article{orlov-savko_factorial_2022-1,
	title = {Factorial {Agent} {Markov} {Model}: {Modeling} {Other} {Agents}' {Behavior} in presence of {Dynamic} {Latent} {Decision} {Factors}},
	abstract = {Autonomous agents operating in the real world often need to interact with other agents to accomplish their tasks. For such agents, the ability to model behavior of other agents – both human and artificial – without complete knowledge of their decision factors is essential. Towards realizing this ability, we present Factorial Agent Markov Model (FAMM), a model to represent behavior of other agents performing sequential tasks. In contrast with most existing models, FAMM allows for behavior of other agents to depend on multiple, time-varying latent decision factors and does not assume rationality. To enable learning of FAMM parameters by observing behavior of other agents, we provide a set of variational inference algorithms for the unsupervised, semi-supervised, and supervised settings. These Bayesian learning algorithms for the FAMM enable agents to model other agents using execution traces and domain-specific priors. We demonstrate the utility of FAMM and corresponding learning algorithms using three synthetic domains and benchmark them against existing algorithms for modeling agent behavior. Our numerical experiments demonstrate that, despite the presence of multiple and time-varying latent states, our approach is capable of learning predictive models of other agents with semi-supervision.},
	language = {en},
	author = {Orlov-Savko, Liubove},
	year = {2022},
	pages = {9},
}

@misc{ma_how_2022,
	title = {How {Far} {I}'ll {Go}: {Offline} {Goal}-{Conditioned} {Reinforcement} {Learning} via \$f\$-{Advantage} {Regression}},
	shorttitle = {How {Far} {I}'ll {Go}},
	url = {http://arxiv.org/abs/2206.03023},
	abstract = {Offline goal-conditioned reinforcement learning (GCRL) promises general-purpose skill learning in the form of reaching diverse goals from purely offline datasets. We propose \${\textbackslash}textbf\{Go\}\$al-conditioned \$f\$-\${\textbackslash}textbf\{A\}\$dvantage \${\textbackslash}textbf\{R\}\$egression (GoFAR), a novel regression-based offline GCRL algorithm derived from a state-occupancy matching perspective; the key intuition is that the goal-reaching task can be formulated as a state-occupancy matching problem between a dynamics-abiding imitator agent and an expert agent that directly teleports to the goal. In contrast to prior approaches, GoFAR does not require any hindsight relabeling and enjoys uninterleaved optimization for its value and policy networks. These distinct features confer GoFAR with much better offline performance and stability as well as statistical performance guarantee that is unattainable for prior methods. Furthermore, we demonstrate that GoFAR's training objectives can be re-purposed to learn an agent-independent goal-conditioned planner from purely offline source-domain data, which enables zero-shot transfer to new target domains. Through extensive experiments, we validate GoFAR's effectiveness in various problem settings and tasks, significantly outperforming prior state-of-art. Notably, on a real robotic dexterous manipulation task, while no other method makes meaningful progress, GoFAR acquires complex manipulation behavior that successfully accomplishes diverse goals.},
	language = {en},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Ma, Yecheng Jason and Yan, Jason and Jayaraman, Dinesh and Bastani, Osbert},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.03023
arXiv:2206.03023 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{brewitt_grit_2021,
	title = {{GRIT}: {Fast}, {Interpretable}, and {Verifiable} {Goal} {Recognition} with {Learned} {Decision} {Trees} for {Autonomous} {Driving}},
	shorttitle = {{GRIT}},
	url = {http://arxiv.org/abs/2103.06113},
	abstract = {It is important for autonomous vehicles to have the ability to infer the goals of other vehicles (goal recognition), in order to safely interact with other vehicles and predict their future trajectories. This is a difﬁcult problem, especially in urban environments with interactions between many vehicles. Goal recognition methods must be fast to run in real time and make accurate inferences. As autonomous driving is safetycritical, it is important to have methods which are human interpretable and for which safety can be formally veriﬁed. Existing goal recognition methods for autonomous vehicles fail to satisfy all four objectives of being fast, accurate, interpretable and veriﬁable. We propose Goal Recognition with Interpretable Trees (GRIT), a goal recognition system which achieves these objectives. GRIT makes use of decision trees trained on vehicle trajectory data. We evaluate GRIT on two datasets, showing that GRIT achieved fast inference speed and comparable accuracy to two deep learning baselines, a planning-based goal recognition method, and an ablation of GRIT. We show that the learned trees are human interpretable and demonstrate how properties of GRIT can be formally veriﬁed using a satisﬁability modulo theories (SMT) solver.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Brewitt, Cillian and Gyevnar, Balint and Garcin, Samuel and Albrecht, Stefano V.},
	month = aug,
	year = {2021},
	note = {arXiv:2103.06113 [cs]},
	keywords = {Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@misc{albrecht_interpretable_2021,
	title = {Interpretable {Goal}-based {Prediction} and {Planning} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2002.02277},
	abstract = {We propose an integrated prediction and planning system for autonomous driving which uses rational inverse planning to recognise the goals of other vehicles. Goal recognition informs a Monte Carlo Tree Search (MCTS) algorithm to plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS utilise a shared set of deﬁned maneuvers and macro actions to construct plans which are explainable by means of rationality principles. Evaluation in simulations of urban driving scenarios demonstrate the system’s ability to robustly recognise the goals of other vehicles, enabling our vehicle to exploit non-trivial opportunities to signiﬁcantly reduce driving times. In each scenario, we extract intuitive explanations for the predictions which justify the system’s decisions.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Albrecht, Stefano V. and Brewitt, Cillian and Wilhelm, John and Gyevnar, Balint and Eiras, Francisco and Dobre, Mihai and Ramamoorthy, Subramanian},
	month = mar,
	year = {2021},
	note = {arXiv:2002.02277 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{papoudakis_agent_2021,
	title = {Agent {Modelling} under {Partial} {Observability} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2006.09447},
	abstract = {Modelling the behaviours of other agents is essential for understanding how agents interact and making effective decisions. Existing methods for agent modelling commonly assume knowledge of the local observations and chosen actions of the modelled agents during execution. To eliminate this assumption, we extract representations from the local information of the controlled agent using encoderdecoder architectures. Using the observations and actions of the modelled agents during training, our models learn to extract representations about the modelled agents conditioned only on the local observations of the controlled agent. The representations are used to augment the controlled agent’s decision policy which is trained via deep reinforcement learning; thus, during execution, the policy does not require access to other agents’ information. We provide a comprehensive evaluation and ablations studies in cooperative, competitive and mixed multi-agent environments, showing that our method achieves higher returns than baseline methods which do not use the learned representations.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano V.},
	month = nov,
	year = {2021},
	note = {arXiv:2006.09447 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{mirsky_survey_2022,
	title = {A {Survey} of {Ad} {Hoc} {Teamwork} {Research}},
	url = {http://arxiv.org/abs/2202.10450},
	abstract = {Ad hoc teamwork is the research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution: First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the ﬁeld so far, and identiﬁes the immediate and long-term open problems that need to be addressed in ad hoc teamwork.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V.},
	month = aug,
	year = {2022},
	note = {arXiv:2202.10450 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{vaswani_attention_2017-1,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2022-08-28},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {Number: arXiv:1706.03762
arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{duan_rl2_2016,
	title = {{RL}\${\textasciicircum}2\$: {Fast} {Reinforcement} {Learning} via {Slow} {Reinforcement} {Learning}},
	shorttitle = {{RL}\${\textasciicircum}2\$},
	url = {http://arxiv.org/abs/1611.02779},
	abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, beneﬁting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a “fast” reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL2, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (“slow”) RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination ﬂags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the “fast” RL algorithm on the current (previously unseen) MDP. We evaluate RL2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-armed bandit problems and ﬁnite MDPs. After RL2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the largescale side, we test RL2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
	language = {en},
	urldate = {2022-08-28},
	publisher = {arXiv},
	author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
	month = nov,
	year = {2016},
	note = {Number: arXiv:1611.02779
arXiv:1611.02779 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{wang_learning_2017,
	title = {Learning to reinforcement learn},
	url = {http://arxiv.org/abs/1611.05763},
	abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is conﬁgured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
	language = {en},
	urldate = {2022-08-28},
	publisher = {arXiv},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
	month = jan,
	year = {2017},
	note = {Number: arXiv:1611.05763
arXiv:1611.05763 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{terry_pettingzoo_nodate,
	title = {{PettingZoo}: {A} {Standard} {API} for {Multi}-{Agent} {Reinforcement} {Learning}},
	abstract = {This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle (“AEC”) games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning (“MARL”), by making work more interchangeable, accessible and reproducible akin to what OpenAI’s Gym library did for single-agent reinforcement learning. PettingZoo’s API, while inheriting many features of Gym, is unique amongst MARL APIs in that it’s based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.},
	language = {en},
	author = {Terry, J K and Black, Benjamin and Grammel, Nathaniel},
	pages = {12},
}

@article{terry_pettingzoo_nodate-1,
	title = {{PettingZoo}: {A} {Standard} {API} for {Multi}-{Agent} {Reinforcement} {Learning}},
	abstract = {This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle (“AEC”) games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning (“MARL”), by making work more interchangeable, accessible and reproducible akin to what OpenAI’s Gym library did for single-agent reinforcement learning. PettingZoo’s API, while inheriting many features of Gym, is unique amongst MARL APIs in that it’s based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.},
	language = {en},
	author = {Terry, J K and Black, Benjamin and Grammel, Nathaniel},
	pages = {12},
}

@article{albrecht_autonomous_2018,
	title = {Autonomous agents modelling other agents: {A} comprehensive survey and open problems},
	volume = {258},
	issn = {00043702},
	shorttitle = {Autonomous agents modelling other agents},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218300249},
	doi = {10.1016/j.artint.2018.01.002},
	language = {en},
	urldate = {2022-08-21},
	journal = {Artificial Intelligence},
	author = {Albrecht, Stefano V. and Stone, Peter},
	month = may,
	year = {2018},
	pages = {66--95},
}

@misc{papoudakis_dealing_2019,
	title = {Dealing with {Non}-{Stationarity} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1906.04737},
	abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modiﬁcations in the training procedure, such as centralized training, to learning representations of the opponent’s policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
	language = {en},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V.},
	month = jun,
	year = {2019},
	note = {Number: arXiv:1906.04737
arXiv:1906.04737 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{ahmed_deep_2022,
	title = {Deep {Reinforcement} {Learning} for {Multi}-{Agent} {Interaction}},
	url = {http://arxiv.org/abs/2208.01769},
	abstract = {The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artiﬁcial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a speciﬁc focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sampleefﬁcient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions.},
	language = {en},
	urldate = {2022-08-21},
	publisher = {arXiv},
	author = {Ahmed, Ibrahim H. and Brewitt, Cillian and Carlucho, Ignacio and Christianos, Filippos and Dunion, Mhairi and Fosong, Elliot and Garcin, Samuel and Guo, Shangmin and Gyevnar, Balint and McInroe, Trevor and Papoudakis, Georgios and Rahman, Arrasy and Schäfer, Lukas and Tamborski, Massimiliano and Vecchio, Giuseppe and Wang, Cheng and Albrecht, Stefano V.},
	month = aug,
	year = {2022},
	note = {Number: arXiv:2208.01769
arXiv:2208.01769 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{bhatia_learning_nodate,
	title = {Learning when {Objectives} are {Hard} to {Specify}},
	language = {en},
	author = {Bhatia, Kush},
	pages = {185},
}

@misc{reddy_where_2019,
	title = {Where {Do} {You} {Think} {You}'re {Going}?: {Inferring} {Beliefs} about {Dynamics} from {Behavior}},
	shorttitle = {Where {Do} {You} {Think} {You}'re {Going}?},
	url = {http://arxiv.org/abs/1805.08010},
	abstract = {Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspeciﬁcation: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules – the dynamics – governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user’s internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.},
	language = {en},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey},
	month = jan,
	year = {2019},
	note = {Number: arXiv:1805.08010
arXiv:1805.08010 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{laidlaw_boltzmann_2022,
	title = {The {Boltzmann} {Policy} {Distribution}: {Accounting} for {Systematic} {Suboptimality} in {Human} {Models}},
	shorttitle = {The {Boltzmann} {Policy} {Distribution}},
	url = {http://arxiv.org/abs/2204.10759},
	abstract = {Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and distribution. The advantage of the latter type, which includes Boltzmann rationality, is the ability to make accurate predictions in new environments without extensive data when humans are actually close to optimal. However, these models fail when humans exhibit systematic suboptimality, i.e. when their deviations from optimal behavior are not independent, but instead consistent over time. Our key insight is that systematic suboptimality can be modeled by predicting policies, which couple action choices over time, instead of trajectories. We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difﬁcult to compute and represent because policies lie in a high-dimensional continuous space, but we leverage tools from generative and sequence models to enable efﬁcient sampling and inference. We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data.},
	language = {en},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Laidlaw, Cassidy and Dragan, Anca},
	month = apr,
	year = {2022},
	note = {Number: arXiv:2204.10759
arXiv:2204.10759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@misc{chan_human_2021,
	title = {Human irrationality: both bad and good for reward inference},
	shorttitle = {Human irrationality},
	url = {http://arxiv.org/abs/2111.06956},
	abstract = {Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference.},
	language = {en},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Chan, Lawrence and Critch, Andrew and Dragan, Anca},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2111.06956
arXiv:2111.06956 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{foerster_counterfactual_2017,
	title = {Counterfactual {Multi}-{Agent} {Policy} {Gradients}},
	url = {http://arxiv.org/abs/1705.08926},
	abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efﬁciently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions ﬁxed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efﬁciently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with signiﬁcant partial observability. COMA signiﬁcantly improves average performance over other multi-agent actorcritic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
	language = {en},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	month = dec,
	year = {2017},
	note = {arXiv:1705.08926 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{rashid_qmix_2018,
	title = {{QMIX}: {Monotonic} {Value} {Function} {Factorisation} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	shorttitle = {{QMIX}},
	url = {http://arxiv.org/abs/1803.11485},
	abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint actionvalues conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX signiﬁcantly outperforms existing value-based multi-agent reinforcement learning methods.},
	language = {en},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Rashid, Tabish and Samvelyan, Mikayel and de Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
	month = jun,
	year = {2018},
	note = {arXiv:1803.11485 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{wang_uencing_nodate,
	title = {Inﬂuencing {Towards} {Stable} {Multi}-{Agent} {Interactions}},
	abstract = {Learning in multi-agent environments is difﬁcult due to the nonstationarity introduced by an opponent’s or partner’s changing behaviors. Instead of reactively adapting to the other agent’s (opponent or partner) behavior, we propose an algorithm to proactively inﬂuence the other agent’s strategy to stabilize –which can restrain the non-stationarity caused by the other agent. We learn a lowdimensional latent representation of the other agent’s strategy and the dynamics of how the latent strategy evolves with respect to our robot’s behavior. With this learned dynamics model, we can deﬁne an unsupervised stability reward to train our robot to deliberately inﬂuence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efﬁciency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation. We show qualitative results on our website.},
	language = {en},
	author = {Wang, Woodrow Z and Shih, Andy and Xie, Annie and Sadigh, Dorsa},
	pages = {12},
}

@misc{roy_promoting_2020,
	title = {Promoting {Coordination} through {Policy} {Regularization} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.02269},
	abstract = {In multi-agent reinforcement learning, discovering successful collective behaviors is challenging as it requires exploring a joint action space that grows exponentially with the number of agents. While the tractability of independent agent-wise exploration is appealing, this approach fails on tasks that require elaborate group strategies. We argue that coordinating the agents’ policies can guide their exploration and we investigate techniques to promote such an inductive bias. We propose two policy regularization methods: TeamReg, which is based on interagent action predictability and CoachReg that relies on synchronized behavior selection. We evaluate each approach on four challenging continuous control tasks with sparse rewards that require varying levels of coordination as well as on the discrete action Google Research Football environment. Our experiments show improved performance across many cooperative multi-agent problems. Finally, we analyze the effects of our proposed methods on the policies that our agents learn and show that our methods successfully enforce the qualities that we propose as proxies for coordinated behaviors.},
	language = {en},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Roy, Julien and Barde, Paul and Harvey, Félix G. and Nowrouzezahrai, Derek and Pal, Christopher},
	month = nov,
	year = {2020},
	note = {arXiv:1908.02269 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{liu_coach-player_2021,
	title = {Coach-{Player} {Multi}-{Agent} {Reinforcement} {Learning} for {Dynamic} {Team} {Composition}},
	url = {http://arxiv.org/abs/2105.08692},
	abstract = {In real-world multi-agent systems, agents with different capabilities may join or leave without altering the team’s overarching goals. Coordinating teams with such dynamic composition is challenging: the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Speciﬁcally, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks. We demonstrate zeroshot generalization to new team compositions. Our method achieves comparable or better performance than the setting where all players have a full view of the environment. Moreover, we see that the performance remains high even when the coach communicates as little as 13\% of the time using the adaptive communication strategy. Code is available at https://github.com/ Cranial-XIX/marl-copa.git.},
	language = {en},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Liu, Bo and Liu, Qiang and Stone, Peter and Garg, Animesh and Zhu, Yuke and Anandkumar, Animashree},
	month = sep,
	year = {2021},
	note = {arXiv:2105.08692 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{gu_online_2022,
	title = {{ONLINE} {AD} {HOC} {TEAMWORK} {UNDER} {PARTIAL} {OBSERVABILITY}},
	abstract = {Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the ﬂy. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full observability, ﬁxed and predeﬁned teammates’ types. This paper relaxes these assumptions with a novel reinforcement learning framework called ODITS, which allows the autonomous agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a ﬁnite set of predeﬁned types, ODITS automatically learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, we introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS signiﬁcantly outperforms various baselines in widely used ad hoc teamwork tasks.},
	language = {en},
	author = {Gu, Pengjie and Zhao, Mengchen and Hao, Jianye and An, Bo},
	year = {2022},
	pages = {17},
}

@misc{papoudakis_dealing_2019-1,
	title = {Dealing with {Non}-{Stationarity} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1906.04737},
	abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modiﬁcations in the training procedure, such as centralized training, to learning representations of the opponent’s policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V.},
	month = jun,
	year = {2019},
	note = {arXiv:1906.04737 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{zintgraf_deep_2022,
	title = {Deep {Interactive} {Bayesian} {Reinforcement} {Learning} via {Meta}-{Learning}},
	url = {http://arxiv.org/abs/2101.03864},
	abstract = {Agents that interact with other agents often do not know a priori what the other agents’ strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents’ strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Zintgraf, Luisa and Devlin, Sam and Ciosek, Kamil and Whiteson, Shimon and Hofmann, Katja},
	month = apr,
	year = {2022},
	note = {arXiv:2101.03864 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{zintgraf_varibad_2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	shorttitle = {{VariBAD}},
	url = {http://arxiv.org/abs/1910.08348},
	abstract = {Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent’s uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	month = feb,
	year = {2020},
	note = {arXiv:1910.08348 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{roy_promoting_2020-1,
	title = {Promoting {Coordination} through {Policy} {Regularization} in {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.02269},
	abstract = {In multi-agent reinforcement learning, discovering successful collective behaviors is challenging as it requires exploring a joint action space that grows exponentially with the number of agents. While the tractability of independent agent-wise exploration is appealing, this approach fails on tasks that require elaborate group strategies. We argue that coordinating the agents’ policies can guide their exploration and we investigate techniques to promote such an inductive bias. We propose two policy regularization methods: TeamReg, which is based on interagent action predictability and CoachReg that relies on synchronized behavior selection. We evaluate each approach on four challenging continuous control tasks with sparse rewards that require varying levels of coordination as well as on the discrete action Google Research Football environment. Our experiments show improved performance across many cooperative multi-agent problems. Finally, we analyze the effects of our proposed methods on the policies that our agents learn and show that our methods successfully enforce the qualities that we propose as proxies for coordinated behaviors.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Roy, Julien and Barde, Paul and Harvey, Félix G. and Nowrouzezahrai, Derek and Pal, Christopher},
	month = nov,
	year = {2020},
	note = {arXiv:1908.02269 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@article{agarwal_deep_nodate,
	title = {Deep {Reinforcement} {Learning} at the {Edge} of the {Statistical} {Precipice}},
	abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a ﬁnite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few-run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the ﬁeld. We illustrate this point using a case study on the Atari 100k benchmark, where we ﬁnd substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the ﬁeld’s conﬁdence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance proﬁles to account for the variability in results, as well as present more robust and efﬁcient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our ﬁndings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable2, to prevent unreliable results from stagnating the ﬁeld.},
	language = {en},
	author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G},
	pages = {17},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {Number: arXiv:1707.06347
arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{mnih_playing_nodate,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {en},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	pages = {9},
}

@article{haarnoja_off-policy_nodate,
	title = {Off-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	language = {en},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	pages = {10},
}

@article{noauthor_neural_nodate,
	title = {Neural {Discrete} {Representation} {Learning}},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	language = {en},
	pages = {10},
}

@inproceedings{noauthor_learning_nodate,
	title = {Learning {Latent} {Representations}},
}

@inproceedings{sadigh_influencing_nodate,
	title = {Influencing {Toward} {Multi}-{Agent} {Interactions}},
	author = {Sadigh, Dorsa},
}

@article{berseth_smirl_2021,
	title = {{SMiRL} {Surprise} minimising reinforcement learning},
	abstract = {Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artiﬁcial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment’s prevailing sources of entropy. This might include avoiding other hostile agents, or ﬁnding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-speciﬁc reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning.},
	language = {en},
	author = {Berseth, Glen and Finn, Chelsea and Geng, Daniel and Devin, Coline and Jayaraman, Dinesh and Rhinehart, Nicholas and Levine, Sergey},
	year = {2021},
	pages = {17},
}

@article{noauthor_notitle_nodate-1,
}

@inproceedings{wang_influencing_2022,
	title = {Influencing {Towards} {Stable} {Multi}-{Agent} {Interactions}},
	url = {https://proceedings.mlr.press/v164/wang22f.html},
	abstract = {Learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent’s or partner’s changing behaviors. Instead of reactively adapting to the other agent’s (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent’s strategy to stabilize – which can restrain the non-stationarity caused by the other agent. We learn a low-dimensional latent representation of the other agent’s strategy and the dynamics of how the latent strategy evolves with respect to our robot’s behavior. With this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation. We show qualitative results on our website.},
	language = {en},
	urldate = {2022-06-08},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Wang, Woodrow Zhouyuan and Shih, Andy and Xie, Annie and Sadigh, Dorsa},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1132--1143},
}

@techreport{xie_learning_2020,
	title = {Learning {Latent} {Representations} to {Influence} {Multi}-{Agent} {Interaction}},
	url = {http://arxiv.org/abs/2011.06619},
	abstract = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
	number = {arXiv:2011.06619},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Xie, Annie and Losey, Dylan P. and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
	month = nov,
	year = {2020},
	note = {arXiv:2011.06619 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{losey_learning_2022,
	title = {Learning latent actions to control assistive robots},
	volume = {46},
	issn = {0929-5593, 1573-7527},
	url = {https://link.springer.com/10.1007/s10514-021-10005-w},
	doi = {10.1007/s10514-021-10005-w},
	abstract = {Assistive robot arms enable people with disabilities to conduct everyday tasks on their own. These arms are dexterous and high-dimensional; however, the interfaces people must use to control their robots are low-dimensional. Consider teleoperating a 7-DoF robot arm with a 2-DoF joystick. The robot is helping you eat dinner, and currently you want to cut a piece of tofu. Today’s robots assume a pre-deﬁned mapping between joystick inputs and robot actions: in one mode the joystick controls the robot’s motion in the x–y plane, in another mode the joystick controls the robot’s z–yaw motion, and so on. But this mapping misses out on the task you are trying to perform! Ideally, one joystick axis should control how the robot stabs the tofu, and the other axis should control different cutting motions. Our insight is that we can achieve intuitive, user-friendly control of assistive robots by embedding the robot’s high-dimensional actions into low-dimensional and human-controllable latent actions. We divide this process into three parts. First, we explore models for learning latent actions from ofﬂine task demonstrations, and formalize the properties that latent actions should satisfy. Next, we combine learned latent actions with autonomous robot assistance to help the user reach and maintain their high-level goals. Finally, we learn a personalized alignment model between joystick inputs and latent actions. We evaluate our resulting approach in four user studies where non-disabled participants reach marshmallows, cook apple pie, cut tofu, and assemble dessert. We then test our approach with two disabled adults who leverage assistive devices on a daily basis.},
	language = {en},
	number = {1},
	urldate = {2022-06-08},
	journal = {Autonomous Robots},
	author = {Losey, Dylan P. and Jeon, Hong Jun and Li, Mengxi and Srinivasan, Krishnan and Mandlekar, Ajay and Garg, Animesh and Bohg, Jeannette and Sadigh, Dorsa},
	month = jan,
	year = {2022},
	pages = {115--147},
}
