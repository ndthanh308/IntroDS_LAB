% \subsection{Analysis}

% Figure environment removed

\subsection{Analysis}

Co-pass was used to evaluate the efficacy of the methods, with 10 random seeds being employed for each experiment. The findings from four of the experiments are presented in Figure \ref{fig:four_plots_fig}. Notably, the proposed approach outperforms the state-of-the-art algorithms across the experiment, and approaches the oracle's level of performance. In these experiments, it is possible for the robot agent to reach the maximum reward as the human agent's behaviour is deterministic given the interaction history, so a perfect prediction of the human actions is possible.

\textbf{Fixed behaviour within episodes.} In the first experiment, the human agent's latent state changed between episodes but not during an episode. Figure \ref{fig:four_plots_fig} (first panel) shows all approaches other than SAC (which does not model the dynamics of the human agent) converged to the optimal policy. Notably, BeTrans showed the fastest convergence.

\textbf{Adding noise to the observations.} In the second experiment, Figure \ref{fig:four_plots_fig} (second panel) shows that BeTrans is robust in modelling behavioural dynamics even in the presence of significant levels of noise. 

\textbf{Changing behaviour within episodes.} In the third experiment, we evaluated our approach in scenarios where the latent state governing the simulated human behaviour changed during episodes (third panel of Figure \ref{fig:four_plots_fig}). In this case, BeTrans leverages its flexibilty to converge to an optimal policy. This contrasts with LILI and RNN-LILI, which assume constant human behaviour during episodes, and SAC (which does not model human dynamics).

\textbf{Longer historical-dependencies.} For the fourth experiment (rightmost panel of Figure \ref{fig:four_plots_fig}), the human agent's behaviour depended on a longer history of interaction (the previous 10 episodes). For these longer historical dependencies, there was a greater discrepancy between the performance of the BeTrans and the baseline methods. We postulate that this is due to the transformer's superior performance in capturing long-term historical-dependencies, due to its utilisation of positional encodings. This enables the model to effectively process long-range dependencies compared to the MLP and RNN-based approaches.

\textbf{Changing environmental parameters}. It took BeTrans 31\% longer to converge to an optimal policy when the number of timesteps, corresponding to reward sparsity, doubled from 5 to 10.  All methods converged to the optimal policy when the task difficulty ratio was less than 2 as predictions of the human behaviour were not required.

\textbf{Continuous and discrete latent variables}. We investigated scenarios where the transformer output both discrete and continuous latent variables. A gumbel-softmax was used in the discrete case. The RL policy learned 24-46\% faster with discrete than continuous variables for the custom environment without noise. This implies that noise was picked up in the latent variable representation when using the smooth distance metric in the continuous case. Thus, discretising the latent variable can act as a regulariser. In the discrete case, the method required the output dimension to be greater than or equal to the number of possible human goals. This corresponds to the number of target locations where the human agent can pass the ball in this case.

An illustration of 'co-pass' in action is depicted in Figure \ref{fig:snapshots} where several snapshots of the ball travelling between the two paddles are shown. Figure \ref{fig:traces} shows the traces when evaluating an untrained and a trained robot agent policy with a human agent over five episodes, where the maximum number of passes in each episode is five. The trained robot agent is able to predict where the ball will go and intercepts it more frequently. The greater number of lines show that more passes took place with the trained robot agent.

% Figure environment removed

% Figure environment removed


% \begin{multicols}{2}
%   \lipsum[1-2]
% \end{multicols}

% \begin{multicols}{2}
%   \lipsum[3-4]
% \end{multicols}