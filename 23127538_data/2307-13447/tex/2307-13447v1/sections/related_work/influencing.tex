To successfully complete tasks in pHRC, it may be critical for the robot to influence the human. There are two purposes of this. The first is to influence the human to limit the possibilities of their behaviour so that the robot can make accurate predictions. The second is to influence toward more desirable limiting behaviours so that the robot can maximise its reward. The robot can shape the human’s policy updates (changes in behaviour). We focus on leverage the dependencies of the robot’s actions on the human’s behaviour to shape the convergence process for repeated interactions. Related to our approach is LILI, SILI and RILI. These methods learn a high-level latent strategy for the human and condition a single-agent RL policy on this learned latent strategy. The method leverages recurring neural network (RNNs) or feed-forward neural networks (MLPs) as part of the encoder and decoder blocks to learn the latent strategy.