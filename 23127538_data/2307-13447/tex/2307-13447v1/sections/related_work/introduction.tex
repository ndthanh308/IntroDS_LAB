A key challenge in enabling robots to learn effective policies in pHRC scenarios relates to the human changing their behaviour over time (non-stationarity in reinforcement learning). There has been work in multiagent reinforcement learning (MARL) to tackle non-stationarity, but the approaches use a centralised framework which is not possible in physical human-robot collaboration settings when there is a human that needs to be treated as part of the environment and modelled separately. One way to model a human is by assuming that they are governed by a latent state. This allows the non-stationarity problem to be tackled through the paradigm of zero-shot meta learning, where each behaviour (latent state) is treated as a separate task and the robot needs to instantly adapt to new tasks.  Different methods exist to predict the current human behaviour, or latent state, from history information. \theo{citations are needed in all the above, roughly 1 per statement, hence around 4-5 for the text above}


\theo{the following has a separate main message than the above, hence I separated.}
Transformers have been shown to learn effectively the semantic relationships between sequential elements. Transformers, therefore, could provide a  promising approach for predicting future behaviour. However, transformers, alongside reinforcement learning methods, require significant amounts of data for training. One solution to address the issue of generating training data is to use synthetic humans. There are several methods available to create synthetic humans capable of mimicking real human behaviour and thereby allowing data to be generated for the purpose of training the robot. These issues are addressed in turn below. \theo{citations are needed in all the above, roughly 1 per statement, hence around 2-3 for the text above}


\subsection{What LILI cannot do}
\begin{itemize}
    \item Cannot handle noise in observations (only perfect observations, none stochastic observation model)
    \item Performs poor with long episode lengths ($\geq$ 10 timesteps)
    \item Performs poor with respect to longer histories (related to the number of episodes)
    \item Requires a fixed number of timesteps for the ball to travel between paddles
    \item Do not assume when the human changes behaviour (it can be at any timestep)
    \item Can take variable length inputs to predict the human behaviour (which will help in ad hoc coordination)
    
\end{itemize}