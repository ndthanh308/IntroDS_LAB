Modelling the human by learning a low-level policy may be computationally inefficient. There may be an implicit structure in the human’s policy space that can be captured in a lower dimension. Prior work has captured this low-dimensional term as a latent strategy. This can be depicted as the high level strategy of the human, or their goal. Previous work has treated different latent variables as different tasks, which the robot’s policy can distinguish between. Each task represents a fundamentally different human behaviour. LILI, SILI and RILI \cite{wang_uencing_nodate, parekh_rili_2022, xie_learning_2020} learn a high-level latent strategy for the human and condition a single-agent RL policy on this strategy. LILI, SILI and RILI lever recurring neural network (RNNs) or feed-forward neural networks (MLPs) within encoder and decoder blocks to learn the latent strategy. \theo{here mention MLPs and RNNs drawbacks}
