Reinforcement learning requires large amounts of data to be generated for the robot to adapt to non-stationary humans. Synthetic humans offer one way of dealing with this issue. There have been several approaches adopted to creating synthetic human models for training robots. One way is to create the synthetic human from programming rules. \theo{citation needed }
This allows synthetic humans to be created easily, but the simulation typically does not match real human behaviour. An alternative approach uses supervised methods to learn a policy capable of mapping human actions to what is observed (imitation learning). This circumvents assumptions about human behaviour, but requires large amount of data, has distribution shift, and struggles to generalise. \theo{citations are needed in all the above}
Another technique is to assume that the human acts with respect to a reward, and to use that reward to predict the behaviour (or vice versa) \cite{yang_optimal_2022,reddy_where_2019,laidlaw_boltzmann_2022}.
\theo{citate also Daniel's latest paper}
There are several assumptions that are made when mapping from behaviour to reward. The first is to assume that humans act perfectly rationally. The second is to assume that humans act optimally but with added noise. This gives the assumption of Boltzmann rationality which is the prevalent approach, but ignores a range of systematic biases \cite{chan_human_2021}.\\

In this paper, synthetic humans will be generated with different systematic biases for the robot to train with. The focus is on developing a robot policy that can collaborate with humans with different dynamics.