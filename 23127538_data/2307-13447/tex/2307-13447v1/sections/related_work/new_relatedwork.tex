\subsection{Related work}


% Reinforcement learning (RL) facilitates optimal decision-making for robots in static settings. A primary obstacle in achieving effective policies for human-robot collaboration (pHRC) arises due to the non-stationary nature of human behavior. As humans can be considered part of the environment in pHRC, their dynamic behavior introduces non-stationarity, leading traditional RL approaches to produce sub-optimal results in such scenarios \cite{papoudakis_dealing_nodate, xie_learning_2020}. This challenge will be addressed in this paper.

\subsubsection*{Non-Stationarity in RL}

Single-agent RL facilitates optimal decision-making in static environments. In human-robot collaboration scenarios, the human is typically considered part of the environment. This makes the environment (transition function) time-dependent, due to the non-stationary human behavior. As a result, traditional RL approaches produce sub-optimal results in such scenarios~\cite{papoudakis_dealing_nodate, xie_learning_2020}. This challenge is the focus of the current work.

\subsubsection*{Multiagent RL}
Multiagent reinforcement learning (MARL) aims to tackle non-stationarity~\cite{albrecht_autonomous_2018}, but it typically uses a centralised framework. This approach is not suitable for human-robot collaboration settings as it is not possible to control both agents. In current work, the human agent is treated as part of the environment and modelled separately. 

\subsubsection*{Learning Latent Strategies}

% Modeling human behavior using low-level policies may be computationally inefficient, as there could be an underlying structure in human policy space with lower dimensionality. Prior research has captured these low-dimensional terms as latent strategies representing high-level human goals. 

One way to model a human agent is by assuming that their behaviour is governed by a latent state. This approach assumes there is an underlying structure in the human policy space with a low dimensionality (\textit{e.g.} high-level human goals) that can capture the low-level policies executed by a human.
Robots can differentiate between different human behaviors by treating the latent variables as distinct tasks ~\cite{wang_uencing_nodate, parekh_rili_2022, xie_learning_2020}. This paradigm approach is also followed in our work. 

% LILI, SILI, and RILI are operational examples of this latent strategy approach 
LILI, SILI, and RILI \cite{wang_uencing_nodate, parekh_rili_2022, xie_learning_2020} learn high-level latent strategies, utilising  feed-forward (MLP) or recurrent (RNN) neural networks in encoder-decoder blocks, and condition single-agent RL policies on them.
However, MLPs struggle with sequential modeling due to their lack of memory and an inability to handle variable-length sequences. On the other hand, RNNs are effective in capturing short-term dependencies, but struggle with modeling long-term dependencies in sequential data~\cite{karita_comparative_2019}. These state-of-the-art approaches acknowledge that the learnt policies can be too brittle to handle noisy interactions with a human, and may struggle with long-term dependencies and behaviours that vary within episodes. Our current approach utilises transformers to address the issues highlighted above.

% \subsection{What LILI cannot do}
% \begin{itemize}
%     \item Cannot handle noise in observations (only perfect observations, none stochastic observation model)
%     \item Performs poor with long episode lengths ($\geq$ 10 timesteps)
%     \item Performs poor with respect to longer histories (related to the number of episodes)
%     \item Requires a fixed number of timesteps for the ball to travel between paddles
%     \item Do not assume when the human changes behaviour (it can be at any timestep)
%     \item Can take variable length inputs to predict the human behaviour (which will help in ad hoc coordination)
    
% \end{itemize}


%  \theo{here mention MLPs and RNNs drawbacks}


% \subsubsection*{Meta-RL}
% This allows the non-stationarity problem to be tackled through the paradigm of zero-shot meta learning, where each behaviour (latent state) is treated as a separate task and the robot needs to instantly adapt to new tasks \cite{bing_meta-reinforcement_2022, arango_multimodal_2021}.  

% Meta-RL ("learning to learn") utilises a set of tasks with a common form during a training phase to enable quick adaption to similar but new tasks unseen during training. One approach - which will be adopted in this paper - is to consider meta-RL as an expansion of multi-task RL where the task information must be inferred. Meta-RL refers to learning a multi-task robot policy  $\pi_{\theta}$ and inferring the task $z$. The Meta-RL settings for adapting to new tasks that is relevant to the non-stationary issue in this paper is 'zero-shot' adaptation where the agent may encounter a new task at every timestep and react immediately \cite{bing_meta-reinforcement_2022}. In this paper we use the zero-shot meta-learning approach to enable the robot to deal with non-stationarity.

\subsubsection*{Transformers}
These deep learning models are adept at processing sequential data and are proficient in discerning semantic relationships and capturing the long-term dependencies that cause difficulties for traditional RNNs~\cite{radford_language_nodate, vaswani_attention_2017}. These properties make transformers a promising approach for predicting future human agent behaviour from historical patterns, hence we utilise them in our work.

\subsubsection*{Human Modelling}
A common drawback of deep learning models (such as MLPs, RNNs, transformers and RL methods) is that significant amounts of data are required for training \cite{radford_language_nodate, nguyen_review_2019}. One approach for coping with this issue is to develop a model that can simulate human behaviours. These models 
can include programming rules, imitation learning, and probabilistic models with or without Boltzmann rationality~\cite{yang_optimal_2022,reddy_where_2019,laidlaw_boltzmann_2022, chan_human_2021}. In the current work, 
our human agent model includes different systematic biases that allow us to simulate human agents with varying behaviours.

% Yet, these ignore a range of systematic biases t
 % simulated humans will be generated with different systematic biases for the robot to train with. The focus is on developing a robot policy that can collaborate with simulated humans with different dynamics. \theo{not clear what different dynamics are}

% There are several methods available to create simulated humans capable of mimicking real human behaviour and thereby allowing data to be generated for the purpose of training the robot. 
% These include from programming rules, imitation learning and by making assumptions about human behaviour such as Boltzmann rationality, but this ignores a range of systematic biases \cite{yang_optimal_2022,reddy_where_2019,laidlaw_boltzmann_2022, chan_human_2021}.

% In this paper, simulated humans will be generated with different systematic biases for the robot to train with. The focus is on developing a robot policy that can collaborate with simulated humans with different dynamics. \theo{not clear what different dynamics are}

% One way is to create the simulated human from programming rules. \theo{citation needed }
% This allows simulated humans to be created easily, but the simulation typically does not match real human behaviour. An alternative approach uses supervised methods to learn a policy capable of mapping human actions to what is observed (imitation learning). This circumvents assumptions about human behaviour, but requires large amount of data, has distribution shift, and struggles to generalise. \theo{citations are needed in all the above}
% Another technique is to assume that the human acts with respect to a reward, and to use that reward to predict the behaviour (or vice versa) \cite{yang_optimal_2022,reddy_where_2019,laidlaw_boltzmann_2022}.
% \theo{citate also Daniel's latest paper}
% There are several assumptions that are made when mapping from behaviour to reward. The first is to assume that humans act perfectly rationally. The second is to assume that humans act optimally but with added noise. This gives the assumption of Boltzmann rationality which is the prevalent approach, but ignores a range of systematic biases \cite{chan_human_2021}.\\

% In this paper, simulated humans will be generated with different systematic biases for the robot to train with. The focus is on developing a robot policy that can collaborate with humans with different dynamics.