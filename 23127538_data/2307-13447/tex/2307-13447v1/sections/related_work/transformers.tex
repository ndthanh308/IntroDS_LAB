Transformers offer a promising approach for predicting future behaviour given sequential history information \cite{vaswani_attention_2017}. These models have shown strong performance in capturing long-range dependencies, and have generally improved performance over recurring neural network models (RNNs) for processing sequential data. Our approach uses a transformer due to its ability to identify sequential correlations. 