% Even in simple simulated environments, RL techniques require a large amount of data to learn an effective robot policy \cite{nguyen_review_2019}. It is therefore necessary to generate simulated humans representing real human behaviour when training robots. 

We developed a  probabilistic human agent model with  attributes that encapsulate the key challenges that arise when interacting with different humans whose behaviour, $\pi (a_H | s, t)$, changes over time. 
The three key attributes of our human agent model are: (i) a time-invariant component; (ii) a low-frequency history-dependent (hysteresis) component; (iii) a high-frequency history-dependent component. Each attribute had a corresponding underlying population-level distribution weight $w$. Each simulated human agent was created by sampling a weight from these three distributions $(w_{c}, w_{lf}, w_{hf})$. These weights define how the human agent behaviour evolves as a function of its experience over time. 
% illustrating the probabilistic model of the simulated human is shown Fig\cref{fig:transition model}. 
The ground truth latent state evolves as a function of the history-dependent human dynamics, as described below:
\begin{equation}
z_t \sim \rho_z(z_t | s_{t-L : t}, a_{t-L : t} ; w_{human}), 
\end{equation}
where 
\begin{equation}\label{eq: weight}
    w_{human} \in (w_{c}, w_{lf}, w_{hf})
\end{equation}
The time-invariant latent factor ($z_{c}$) varies between different humans agents but is constant over time for a given human. The time-dependent factors ($z_{lf}, z_{hf}$) evolve over time. The low-frequency component ($z_{lf}$) depends on the previous episode, and the high-frequency component ($z_{hf}$) depends on the previous action. The three components of the human behaviour are combined together with different weights according to: 
\begin{equation} \label{eq: 2}
\Tilde{p}(z) = \sum^N_i w_i p_i (z) = w_c z_c + w_{lf} z_{lf} + w_{hf} z_{hf}
\end{equation}
to simulate different humans agents. The human agent behaviour is defined so there is one-to-one mapping between the ground truth latent state and where the human agent hits the ball. This is written formally as: 
% \begin{equation}
% \pi (a_H | s, z) = constant \; \forall s \in S
% \end{equation}
\begin{equation}
a_{H,0} \sim P(a_{H,0} | z) = \frac{i}{\sum_i z_i} [z_1, z_2, z_3, z_4]
\end{equation}
where each latent component $z_i$ represents a different goal location.

This model simulates human behaviour to generate human-like data. Existing research indicates that human behaviour is influenced by time-invariant elements plus components dependent on varying frequencies (such as low and high frequency factors) outlined in the human model (\ref{human_model_section})  \cite{ditterich_evidence_2006}. However, this simplified model may not capture the contextual relationships inherent in human behaviour. 
% Nonetheless, it serves as a proof-of-concept, demonstrating the types of properties that a robot can handle effectively.

The weight terms in \eqref{eq: 2}, \eqref{eq: weight} determine to which extent each component affects the human's behaviour. For example, a low-frequency weight $w_{lf}$ set to 0 indicates that the low-frequency component has no affect on the decisions made by the human model.

% The look-back window has sufficient information to predict the next latent state.

% % Figure environment removed


% Our work is not focused on the human agent model, yet 

% and a cooperative robot agent needed to overcome.

% % Figure environment removed