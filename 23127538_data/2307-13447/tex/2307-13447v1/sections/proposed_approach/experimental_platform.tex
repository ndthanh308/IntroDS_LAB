% A custom environment was created to deal with the dearth of suitable existing multi-agent environments \cite{papoudakis_benchmarking_2021}, and the need to have complete control over the environmental parameters. 
There are a number of multi-agent environments~\cite{papoudakis_benchmarking_2021} available, but we created our own custom environment ("co-pass") that allowed us to have complete parametric control over factors such as:

\begin{itemize}
    \item \textit{Reward sparsity}: number of timesteps it takes the ball to travel between paddles
    \item \textit{Task difficulty}: ratio of the number of locations where the human can pass the ball against the time it takes the ball to travel between paddles
    \item \textit{Observability}: amount of noise in the observations of the ball positions in the environment, and whether the ball is observed 
    \item \textit{Simulated human}: The human policy as a function of its historical dependencies
\end{itemize}
Co-pass provided a test bed for the algorithms and a platform for investigating our hypotheses (our primary hypothesis being that BeTrans would effectively model historical dependencies). This allowed us to evaluate whether the robot agent could adapt to changing human behaviour during an episode in the presence of observation noise.

% Our simulation platform provided a test bed for the algorithms and a platform for investigating the hypotheses that our proposed approach would a effectively model sequential dependencies, allowing the robot to capably deal to adapt to changing human behaviours during an episode in the presence of observation noise, under the assumption of deterministic dynamics and the learned latent variable being Markovian or that the game is constrained so that the robot can learn the optimal policy even if the learned latent state that is non-Markovian. 
The experimental design was a simulated collaborative 'ball passing' task between a robot and human agent (as shown in Fig. \ref{fig:game schematic}).  
% In the initial instance, 
We discretised the state space and varied both the number of target locations and the timesteps over which the ball travelled between the paddles. The dynamics were deterministic with the ball changing direction when it hit a paddle and the subsequent trajectory being a function of the paddle orientation. We sampled the position of the ball from a standard normal distribution (with the mean at its 'true' position) when injecting noise into the observations of the ball's position. A point was awarded for each successful intercept of the ball. The goal was to accumulate as many points as possible within a set time frame (equivalent to the tennis rally example provided in the introduction).

% The customisable parameters of the novel environment allow a systematic investigation into where current RL techniques break down and enable the relative performance of BeTrans to be established.

% Figure environment removed