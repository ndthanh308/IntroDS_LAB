 % Figure environment removed

% \theo{I need to go over this again, slowly and carefully to be able to comment on it.}

We utilised an auto-regressive transformer architecture to learn a Markovian latent state representation $\hat{v_t}$ that captures the changes in the human agent behaviour (see \eqref{eq:infer latent}). The transformer model is based on GPT-2 \cite{radford_language_nodate}. This includes a causal attention mask to ensure predictions are based only on previous (in time) data. We used the smallest GPT-2 model with 12 multi-attention heads \cite{radford_language_nodate} as it is sufficient for this problem and can perform faster inference - though larger GPT-2 models provide greater accuracy. %due to the faster inference time of the smaller model. 
The self-attention mechanism is used to learn various contextual nuances of the behaviour. %in past behaviour. 
We found that for our relatively simple environment this model gave adequate precision.

% \theo{refer to \cref{fig:transformer_model} and the algorithm where is appropriate in the following text}
Figure \ref{fig:BeTrans} illustrates theapproach. At each time step a tuple ($s_t,a_t,r_t,s_{t+1},d_{t})$ is passed into the architecture as separate tokens so the experiences form a long trajectory of states $s_t$, actions $a_t$, rewards $r_t$, next states $s_{t+1}$ and done tokens $d_t$ (across many episodes). $\tau = (s_0, a_0, r_1, s_1, d_1, s_1, ... , s_{t-1}, a_{t-1}, r_t, s_t, d_t)$ where $t$ is the number of time steps. The states are normalised between -1 and 1 and the actions are one-hot encoded. Each token is embedded into a $H=192$ dimensional vector. A separate linear layer is used to embed each token type (i.e. $s,a,r,d$) in its corresponding $H$ dimensional vector. The model utilises the tokens up to the state to predict the latent state $v$ and subsequent action $a$. It uses all tokens up to and including the action to predict the reward, next state, and whether the task is finished ('done') $(r, s', d)$ (see Figure \ref{fig:BeTrans}). 
 In a similar manner to the GPT-2 model, absolute positional embeddings are added \cite{radford_language_nodate}.

The transformer was trained in two phases - a pre-training phase and a fine-tuning phase. In the pre-training phase, the losses from predicting $(s,a,r,s',d)$ were used to update the transformer's weights $\theta_{BT}$. The purpose of this phase was to train the transformer to learn the underlying structure of the data. The data used to generate the replay buffer for pre-training were generated by taking random actions for 10,000 time steps. In the fine-tuning phase, the predicted latent variable $v$ is fed into the Dynamics Network (DN) along with the corresponding state(s) $s$ and action(s) $a$ at that time step to predict the next state(s) $s'$ and reward(s) $r$. The DN is a multi-layer perceptron (MLP) and consists of two output heads for $s'$ and $r$. The DN consists of two-hidden layers of size 256. In the case where $v$ is Markovian for each timestep, the DN takes in $(s_t, v_t)$ and predicts $(s_{t+1}, r_{t+1})$. In the case where $v$ is non-Markovian given the previous timestep, the transformer predicts a forecast horizon of states and rewards $(s_{t+1:t+N}$, $r_{t+1:t+N})$, where $N$ is the horizon window. We assume that the human agent's behaviour is constant during a horizon window. The reconstruction loss is $\mathcal{J}_{sr} = \mathbb{E}[(s - \hat{s})^2 + (r - \hat{r})^2]$.
%This allows our method to disambiguate changing human agent goals (behaviours).

We investigated both continuous and discrete latent representations. For discrete latent variables, the Gumbel-Softmax distribution was used to enable backpropagation \cite{jang_categorical_2017}. For continuous latent variables, the Kullback-Leibler (KL) divergence was used  to reduce overfitting~\cite{jiang_transformer_2020}. This was important as the flexibility provided by the transformer's architecture could inadvertently incorporate extraneous noise.  The KL divergence was taken between the approximate posterior distribution and prior distribution $p(v) = \mathcal{N}(0,\mathcal{I})$. The posterior distribution was a diagonal Gaussian where the mean and standard deviation came from the transformer's two latent variable output heads (the transformer has just one output head for the case where the latent variable is discrete). The loss $\mathcal{J}_{DN} = \mathcal{J}_{sr} + \mathcal{J}_{KL}$ was used to update the transformer $\phi_{BT}$ and Dynamics Network's weights $\theta_{DN}$.

During training, data were sampled from the replay buffer (which stores the experiences) with a batch size of 32. A block size $L$ of 125 was chosen for the transformer so that it could look-back 25 timesteps (see (\ref{eq:infer latent})). Different block sizes could be chosen depending on the desired look-back length. The proposed approach leverages the transformer's employment of parallelism, positional encoding, and self-attention mechanisms to effectively capture historical dependencies to learn the underlying latent variable.

