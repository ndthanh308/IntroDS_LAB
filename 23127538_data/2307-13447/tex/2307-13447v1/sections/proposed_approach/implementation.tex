\subsubsection*{RL with latent strategies}
For the RL element of our approach, we used the soft-actor critic (SAC) algorithm \cite{haarnoja_soft_2018}, which is an off-policy algorithm conditioned on the latent state. The actor $\pi$ with parameters $\theta_\pi$ and critic $Q$ network with parameters $\theta_Q$ were trained with the SAC loss $\mathcal{J}_{\pi}$ and $\mathcal{J}_{Q}$ \cite{haarnoja_soft_2018}. The hyperparameters in this base algorithm were taken from CleanRL \cite{huang_cleanrl_nodate}. It was found that the default hyperparameters were sufficient for the method to learn an effective policy with an appropriately tuned reward function. In SAC, experiences (i.e. $s,a,r,s',d)$) are sampled at random from the replay buffer. The transformer infers from the history window of desired length (in this case the last 25 timesteps) the corresponding latent variable $v$ at a given timestep. During training, every 1000 timesteps, five batches are sampled from the replay buffer, the SAC ($\mathcal{J}_{\pi}$, $\mathcal{J}_{Q}$) and DN $\mathcal{J}_{sr}$ losses are calculated, and the transformer and SAC weights are updated - see Algorithm~\ref{alg:BeTrans_algorithm}. The learning rate for both the policy $\alpha_\pi$ and critic networks $\alpha_Q$ was set to $3 \times 10^{-4}$. The learning rate for the behavioural transformer $\alpha_{BT}$ and DN $\alpha_{DN}$ was $4 \times 10^{-4}$ and $1 \times 10^{-3}$ respectively.

% % Figure environment removed

% % Figure environment removed

% \begin{multicols}{2}
%   \lipsum[3-4]
% \end{multicols}

% Pseudocode.

\begin{algorithm}[t]
\caption{Behavioural Transformer}\label{alg:BeTrans_algorithm}
\begin{algorithmic}
\Require $\text{Learning rates } \alpha_Q, \alpha_\pi, \alpha_{\text{BT}}, \alpha_{\text{DN}}$
\State $\text{Randomly initialise } \theta_Q, \theta_\pi, \text{ and } \theta_{DN}$
\State $\text{Initialise $\phi_{BT}$} \text{ from pre-training}$
\State $\text{Initialise empty replay buffer } \beta$
\For{$\text{episode} = 1, 2, ... $} \Comment{Collect data}
\For{$t = 1, 2, ... $}
\State $\hat{z_t} \gets f(h_R)$
\State $a_t \sim \pi_{\theta} (a | s, \hat{z_t})$
\EndFor
\State $\text{Assign } \beta[i] \gets \tau^i$
\If{$\text{update }$}\algorithmiccomment{Update GPT and SAC}
\For{$\text{update in } 1, 2, ...$}
\State{sample from batch $\beta$}
\State $\phi_{BT} \gets \phi_{BT} - \alpha_{BT} \nabla_{\theta_{BT}} (\mathcal{J}_{sr} + \mathcal{J}_{KL})$
\State $\psi_{DN} \gets \psi_{DN} - \alpha_{DN} \nabla_{\theta_{DN}} \mathcal{J}_{sr}$
\State $\theta_Q \gets \theta_Q - \alpha_Q \nabla_{\theta_Q} \mathcal{J_Q} $
\State $\theta_\pi \gets \theta_\pi - \alpha_\pi \nabla_{\theta_Q} \mathcal{J_\pi}$
\EndFor
% \EndIf
% \If{$\text{update RL policy}$}
% \For{$\text{RL update in } 1, 2, ...$}\algorithmiccomment{SAC update}
% \State $\text{batch} \sim \beta$
% \State $\theta_Q \gets \theta_Q - \alpha_Q \nabla_{\theta_Q} \mathcal{J_Q} $
% \State $\theta_\pi \gets \theta_\pi - \alpha_\pi \nabla_{\theta_Q} \mathcal{J_\pi}$
% \EndFor
\EndIf
\EndFor

\end{algorithmic}
\end{algorithm}
