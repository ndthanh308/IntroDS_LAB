% We assume that the human behaviour is governed by a latent state $z$. Under this assumption, the human policy becomes Markovian with respect to the latent state $z$ of the human and the current physical state $s$. 
% We assume that the human behaviour is governed by a latent state $z$. Under this assumption, the human policy depends on the latent state $z$ and the current physical state $s$. 
% In other words, it is the conditional: $\pi_H (a_H | s, z ; \theta)$. 


% we model latent variable $z$ as a history-dependent stochastic function  $\rho_z$, where 

% At each timestep the simulated human agent changes behaviour based to a latent variable $z_t$ (see Figure~\ref{fig:transition model}) that is history-dependent stochastic function  $\rho_z$, where $z_{t+1} \sim \rho_z (h_{H,L} ; w_{H} )$. 
%  % The probability distribution $p_z(z_{t+1} | h_{H,L})$ is different for different humans. 

We assume that the human behaviour is governed by a latent state $z$. Under this assumption, the human policy depends on the latent state $z$ and the current physical state $s$ (see Figure~\ref{fig:transition model}),  written with the conditional: $\pi_H (a_H | s, z)$. On this basis, the transition dynamics can be re-written to be conditioned on the latent state:
% Eq. \ref{eq: deterministic1} and \ref{eq: determinstic2}:
\begin{multline}\label{eq: deterministic1}
    p(s',r | s, a_R, a_H) = p(s', r | s, a_R, a_H, z) \\ \text{if} \hspace{0.2cm} s',r \perp\perp z | s, a, a_H
\end{multline}
\begin{multline}\label{eq: determinstic2}
    \implies p(s',r | s, a_R, a_H, z) = p(s', r | s, a_R, z) \\ \text{if} \hspace{0.2cm} s',r \perp\perp a_H | s, a_R, z.
\end{multline}
This makes the environmental dynamics a function of the human's latent state $z$. RL methods assume that the environment dynamics are Markovian, but the variable $z$ may not be Markovian. Hence, the latent state $v$ (see~\cref{subsec:comp_form}) is learned in a Markovian manner so that these methods can be applied successfully, converge to optimal policies, and capture the different behaviours. This approach provides time-invariant state transition dynamics $P(s', v', r | s, a, v)$, and allows the definition of a new state $s_v = [s, v]$ that reduces the transition dynamics to the single-agent MDP problem $P(s_v', r | s_v, a)$. This enables the application of classical RL techniques. In other words,
the problem  reduces to solving a single-agent RL problem, whilst inferring a Markovian latent state representation $\hat{v_t}$, where: 
% $\hat{v_t}$ 
% , Eq. \ref{eq:infer latent}, \ref{eq:robot_history_L}:
\begin{equation} \label{eq:infer latent}
    \hat{v_t} = f(z_{t-L}:z_t) = f(h_{R,L})
\end{equation}
and $\hat{v_t}$ is a function of the robot's history, that is:
\begin{equation} \label{eq:robot_history_L}
    h_{R,L} = \{s_{t-L:t}, a_{t-L:t-1}, r_{t-L:t-1}\}.
\end{equation}



% Thus, a latent variable $v$ needs to be learned (Eq. \ref{eq:infer latent}), which is a function of the human's latent states $z$ and makes the transition dynamics Markovian given the previous state $s$, robot action $a$ and learned latent state $v$.

% In our approach, we assume that the latent variable $z$ is a function of the history $h_{H,L}$ of $L$ previous states $s$, robot actions $a$, and human actions $a_H$, formally 
% $z_{t+1} \sim \rho_z (h_{H,L} ; w_{H} )$  