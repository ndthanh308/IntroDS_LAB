We have created a framework that can learn optimal robot policies in human-robot agent collaboration scenarios where the human agent changes behaviour. We validated our method on a novel environment with customisable parameters and over a range of simulated human agents. The approach was trained against simulated human agents with systematic biases, but whose behaviour is Markovian given sufficient history. Our novel BeTrans methodology allows the robot agent to adapt to changing human behaviours.

We provide a theoretical framework to show that our approach allows the robot to learn a representation which makes the environmental dynamics time-invariant, thereby allowing the application of single-agent RL methods. The human behaviour is assumed to be governed by a latent state, and the method adapts to state transitions during episodes and can handle noise. BeTrans improves on existing state-of-the-art latent variable RL methods.

\textbf{Limitations and future work:} Our method allows the robot to effectively learn a latent variable and interact with humans whose behaviour can change at any timestep - but assumes that the behaviour is Markovian and the dynamics are deterministic. The assumption that the latent state is Markovian represents a simplification of the problem. Single-agent RL methods are based on a Markov Decision Process that assumes each state has sufficient information to predict the probability distribution over the next states given an action. Thus, if the latent state (human's goal) is non-Markovian, the policy cannot predict the next goal so it becomes a multi-task problem. In a multi-task problem, the robot can learn an optimal policy that maximises the reward for a given goal, but it cannot predict how its actions will affect goal transitions. If the robot cannot make this prediction then it is unable to influence the human's goal (as the human's goal transitions may be a function of the robot's actions). %- it could only just maximise the reward given a goal} 
% \theo{the last sentence is nice but it is a bit of jump form the multi-task problem. Maybe introduce one more sentence to make the transition from multi-task problem to influence human goals problem smoother.}

In future work, we plan to extend our approach to deal with non-Markovian behaviours and non-determinstic dynamics. We also plan to train our method using actual human data while performing a ball passing task. Furthermore, in the next phase of work we will 
%and investigate whether it is advantageous to learn a latent variable rather than directly input history information into the learning process. 
validate our method with actual humans interacting with robots.