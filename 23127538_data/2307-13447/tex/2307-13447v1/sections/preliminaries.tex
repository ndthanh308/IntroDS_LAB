

% \subsubsection*{Meta-RL}
% This allows the non-stationarity problem to be tackled through the paradigm of zero-shot meta learning, where each behaviour (latent state) is treated as a separate task and the robot needs to instantly adapt to new tasks \cite{bing_meta-reinforcement_2022, arango_multimodal_2021}.  

% Meta-RL ("learning to learn") utilises a set of tasks with a common form during a training phase to enable quick adaption to similar but new tasks unseen during training. One approach - which will be adopted in this paper - is to consider meta-RL as an expansion of multi-task RL where the task information must be inferred. Meta-RL refers to learning a multi-task robot policy  $\pi_{\theta}$ and inferring the task $z$. The Meta-RL settings for adapting to new tasks that is relevant to the non-stationary issue in this paper is 'zero-shot' adaptation where the agent may encounter a new task at every timestep and react immediately \cite{bing_meta-reinforcement_2022}. In this paper we use the zero-shot meta-learning approach to enable the robot to deal with non-stationarity.

The domain of the current work is human-robot agent cooperation where the human agent is included in the environment. Our focus is to obtain robot agent policies, using RL, that are able to cooperate with human agents that change behaviour over time, \textit{i.e.} solve a non-stationarity RL problem.
% We aim to develop robot policies using RL that can cooperate with human agents that change behaviour over time, \textit{i.e.} solve a non-stationarity RL problem.

Formally, our approach follows the Meta-RL ("learning to learn") paradigm~\cite{bing_meta-reinforcement_2022} where the non-stationarity problem can be described as a zero-shot meta learning problem. This approach represents each behaviour with a latent state and treats them as separate tasks, while the robot agent adapts instantly within each task (\textit{i.e.} behaviour) ~\cite{bing_meta-reinforcement_2022, arango_multimodal_2021}.  
The Meta-RL setting that is relevant to the non-stationary issue requires 'zero-shot' adaptation where the agent may encounter a new task (\textit{i.e.} behaviour) at every timestep and react immediately \cite{bing_meta-reinforcement_2022}. The non-stationarity challenge faced by a robot agent when cooperating with a human agent is formalised below as zero-shot meta-learning. 





% To achieve a robot agent needs to infer task information .

% We build on previous work to create a solution to the key challenge of humans updating their behaviour over time when a human is introduced into the environment. The introduction of a human in pHRC leads to environmental dynamics that change over time and are a function of the robot's policy.  


% Meta-RL ("learning to learn") utilises a set of tasks with a common form during a training phase to enable quick adaption to similar but new tasks unseen during training. One approach - which will be adopted in this paper - is to consider 
% meta-RL as an expansion of multi-task RL where the task information must be inferred. Meta-RL refers to learning a multi-task robot policy  $\pi_{\theta}$ and inferring the task $z$. The Meta-RL settings for adapting to new tasks that is relevant to the non-stationary issue in this paper is 'zero-shot' adaptation where the agent may encounter a new task at every timestep and react immediately \cite{bing_meta-reinforcement_2022}. In this paper we use the zero-shot meta-learning approach to enable the robot to deal with non-stationarity.
% This problem and the framework from which the problem is tackled is formalised below.

\subsubsection*{Single-agent RL}
The environmental dynamics relate to the probability of reaching the next state $s'$ and the corresponding reward $r$, from the current state $s$ and robot action $a$. In single-agent RL, the environmental dynamics $P(s', r | s, a )$ comprise the reward $P(r | s, a )$ and the state transition dynamics $P(s' | s, a )$, and are constant over time.


\subsubsection*{RL with a human agent}
We consider a human agent behaviour model that depends on time, denoted as $a_H \sim \pi_H (t)$. In order to consider the human agent behaviour within the policy of the robot agent, we include the human agent in the environment, hence the environmental dynamics (transition function) are $P(s',r| s, a, a_H)$. Given that the human behaviour changes (i.e. $P(a_H | t)$ is a function of time), the environmental dynamics $P(s', r | s, a, t)$ are also a function of time, hence "non-stationary". 

 % by relating to the probability distribution over the human actions at each state. 
 % The RL framework needs to be extended when a human is acting in the environment. 
 % Including the human action in the environmental dynamics gives $P(s,r' | s, a, a_H)$.


\subsubsection*{Human agent behaviour} To model a time-dependent human agent we assume that the human actions are governed by a probability distribution conditioned on a history $h_{H,L}$. This is written formally as: 
\vspace{-2mm}
\begin{equation}\label{eq: action_sample}
a_H \sim \pi_H(a_H | h_{H,L}),
% a_H = \pi_H(h_{H,L})
\vspace{-2mm}
\end{equation}
where 
\vspace{-2mm}
\begin{equation}\label{eq: human_history}
h_{H,L} = \{s_{t-L:t}, a_{t-L:t-1}, a_{H,t-L:t-1}\},
\end{equation}
and $s$ are states, $a_H$ are human actions, $a$ are robot actions and subscript $L$ indicates a window of L last timesteps. 
The human behaviour is assumed to be Markovian given the look-back window, $L$, hence  
\begin{align}
% P(s_{t+1}|s_{L},a_{L},a_{H,t}) &= 1 \\
% \implies 
P(a_H=a_{H,t} | s_{L},a_{L},s_{t+1}) &= 1,
\end{align}
and this implies that  $a_{H,t}$ is known, given $\{s_{L}, a_t, s_{t+1}\}$.
This means that  the human action can be inferred from a lookback window of states $s_L$ and robot actions $a_{L}$, formally written as: 
\begin{equation}\label{eq: deterministic_simplify}
P(a_{H,t} | s_{L}, a_{L}).
\end{equation}

% $h_{H,L} = \{(s_{t-L}, a_{t-L}, a_{H,t-L}), ... , (s_{t=1}, a_{t-1}, a_{H,t=1}), (s_t, -, -)\}.$

% Thus, in this case the human action $a_{H,t}$ can be predicted from a sequence of states $s_{0,t}$ and robot actions $a_{0,t-1}$.
% \begin{equation}\label{eq: deterministic_simplify}
% P(a_{H,t} | s_{0:t}, a_{0:t-1}, a_{H,0:t-1}) = P(a_{H,t} | s_{0:t}, a_{0:t-1})
% \end{equation}


% \begin{equation}\label{General}
% \begin{split}
% P(s_{t+1}|s_t,a_{t},a_{H,t}) = 1, \\ 
% \implies P(a_H=a_{H,t} | s_{t},a_{t},s_{t+1})=1,\\
% \implies P(a_{H,t} | s_{0:t}, a_{0:t-1}, a_{H,0:t-1}) = P(a_{H,t} | s_{0:t}, a_{0:t-1}),
% \end{split}
% \end{equation}

% \begin{equation}
% P(s_{t+1}|s_t,a_{t},a_{H,t}) = 1 \implies P(a_H=a_{H,t} | s_{t},a_{t},s_{t+1})=1
% \end{equation}

% \begin{equation}
% \implies P(a_{H,t} | s_{0:t}, a_{0:t-1}, a_{H,0:t-1}) = P(a_{H,t} | s_{0:t}, a_{0:t-1})
% \end{equation}


\subsubsection*{Optimal robot agent policy}

The robot agent takes a series of actions and observes a sequence of states and rewards. This allows it to obtain a trace up to time $t$ denoted as $h_R = \{s_{0:t}, a_{0:t-1}, r_{0:t-1}\}~\refstepcounter{equation}(\theequation)\label{eq: robot_history}$.
The aim of the robot agent is to maximise the total expected reward of the policy $r(s_t,a_t,a_H)$ over $N$ interactions: 
% \begin{equation}\label{eq: robot_history}
% h_R = \{s_{0:t}, a_{0:t-1}, r_{0:t-1}\}
% \end{equation}
\begin{equation}
\label{eq:RL_objective}
\theta^* = \arg \max_{\theta} \sum^N_j \mathbb{E}_{\pi_{\theta} (\tau \sim p(\tau_t))} [\sum_{t=0}^T r(s_t, a_t, a_{H,t})],
\end{equation}
where $\theta^*$ are the optimal parameters of the robot agent policy and $p(\tau_t)$ is the probability for a trajectory of length $t$ given by: 
\begin{multline} \label{eq:2}
    p(\tau_t) = \rho(s_0) \prod_{t=0}^T \pi(a_t | h_{R,t-L:t}) \pi_H(a_{H,t} | h_{H,t-L:t}) \\ T(s_{t+1} | s_t, a_t, a_{H,t}),
\end{multline}
where $\rho_0$ is the initial state distribution, $\pi$ is the robot's policy, $\pi_H$ is the human's policy and $T$ is the environmental dynamics. In Figure~\ref{fig:transition model} we provide the directed acyclic graph (DAG) that captures the overall system and illustrates the transition model.

% \theo{I would suggest to re-write the following, trying to be more specific in terms of the example. Use specific points in time from the cooking interaction and focus on trying to explain the utility/functionality of each term in (7) via grounded instances of the cooking interaction scenario.}
The proposed framework can be beneficial in instances of human-robot collaboration in tasks such as cooking. The state $s$ could include the user's location in the kitchen, the ingredients currently in use, the user's movements etc. The actions $a$ could range from handing over utensils, to stirring a mixture, to observing without intervening. The robot should be trained to interpret the user's needs. For instance, if the human is cutting vegetables, will they require a bowl for the cut pieces? If they are about to serve a piece of lasagna will they need a spatula? This need can be captured in the history of length $L$ ($h_{H,t-L:t}$ \eqref{eq: 2}) and from this the robot can predict whether to offer a bowl or spatula and provide it at the right moment. The reward $r$ can include the efficacy of the robot's actions in relation to the human's needs, and its ability to match the human's pace and avoid unnecessary interruptions. Overall, this framework can enable the robot to predict human needs in a cooking environment and thereby enhance the cooking process.

% The proposed framework can be beneficial in instances of human-robot collaboration such as cooking. In this scenario the robot needs to effectively assist the human in the cooking process by predicting and adapting to the human's intentions and goals. For example, it can allow the robot learn to adapt to the pace of cooking as desired by the human, and the human's preferences in terms of what dish to cook. Thus, the robot can align its policy with the chosen recipe and cooking methods and predict when to interrupt and pass items and when to not to ensure a smooth collaboration.}
 
% \subsubsection{Transition Model} The probability for a trajectory of length $N$ is given by Eq. \ref{eq:2} and depicted in Figure \ref{fig:transition model} where $\rho_0$ is the initial state distribution, $\pi$ is the robot's policy, $\pi_H$ is the human's policy and $T$ is the environmental dynamics.
% \begin{multline} \label{eq:2}
%     p(\tau) = \rho(s_0) \prod_{t=0}^T \pi(a_t | h_{R,t-L:t}) \pi_H(a_{H,t} | h_{H,t-L:t}) \\ T(s_{t+1} | s_t, a_t, a_{H,t})
% \end{multline}
% \begin{multline} \label{eq:2}
%     p_\pi (\tau) = \rho(s_0) \prod_{t=0}^T \pi(a_t | h_{R,t-L:t}) \pi_H(a_{H,t} | h_{H,t-L:t}) \\ T(s_{t+1} | s_t, a_t, a_{H,t})
% \end{multline}


% Figure environment removed
