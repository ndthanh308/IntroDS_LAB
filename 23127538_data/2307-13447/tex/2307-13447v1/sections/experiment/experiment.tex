
We performed experiments with a range of simulated human agents and customised environment parameters. We tested the efficacy of our approach by comparing our algorithm against four baselines over a range of parameters within the custom environment. The human agent behaviour was constant during an episode (always passing the ball to one of the target locations) but changed between episodes. The environmental conditions that we considered were; (i) introducing noise in the observations of the ball position; (ii) changing human behaviour during an episode; and (iii) selecting the human's goals based on a function of longer history.

We also performed experiments across a range of environmental parameters, which are outlined in section \ref{simulation_platform}. We investigated the impact of learning a continuous or discrete latent variable on the performance of the proposed method.
% \theo{refer here the Changing environmental parameters and the Continuous and discrete latent variables.}

\subsection{Simulation Platform}
\input{sections/proposed_approach/experimental_platform.tex}\label{simulation_platform}

\subsection{Experiment}

We compared our approach against the hypothetical ideal method and three baselines: 
\begin{itemize}
    \item \textbf{Oracle:} This approach 'knows' the ground truth latent state of the human and provides an upper bound on the performance of a method 
    \item \textbf{SAC:} This approach does not explicitly model the dynamics of a human \cite{haarnoja_soft_2018-1}
    % \item \textbf{History SAC:} This approach observes the history of previous states at each timestep.
    \item \textbf{LILI:} This approach uses a variational autoencoder to learn the dynamics of a human and provides a baseline comparison \cite{xie_learning_2020}
    \item \textbf{RNN-LILI:} This method is analogous to LILI, but employs a RNN as the encoder block, similar to \cite{parekh_rili_2022}
    \item \textbf{BeTrans:} Our approach - using a conditional transformer to learn the dynamics of a human
\end{itemize}

% Our approaches was compared to the baselines in the custom environment co-pass and several experiments were performed.
In the first experiment, the episodes had a fixed length and the human agent latent state (goal) - which determined where the human agent would pass the ball to - was constant during episodes and only changed between episodes. In the second experiment, noise was added to observations of the ball position, by sampling from a normal distribution with a variance of 1 unit in the x-position and 0.5 units in the y-position of the ball, where 1 unit represents the x-distance traveled by the ball between the paddles in one timestep and the y-distance between two consecutive target locations, respectively. For the third experiment, the latent state governing the human behaviour changed during episodes (thus changing where the human agent would pass the ball). In the fourth experiment, longer history lengths were included to investigate BeTrans's ability to cope with long-term dependencies. 

For these experiments, the robot agent was trained with simulated human agents whose weights $(w_{c}, w_{lf}, w_{hf})$ were sampled from a Continuous Bernoulli distribution~\cite{loaiza-ganem_continuous_2019}. These determined how the human behaviour evolved, as detailed in~\cref{human_model_section}.

Co-pass's parameters were changed to investigate the effect they had on performance. These changes included reward sparsity, defined as the number of steps between the paddles (varied between 1 and 20), and the number of target positions where the human could pass the ball (varied between 3 and 40). The ratio between the number of targets and steps between the paddles (task difficulty) was kept greater than 2 to ensure that predictions about the human agent  were relevant for successful collaboration.

% The human's behaviour was not assumed to be constant, and could change at any point when interacting with the robot agent. However, to ensure the Markovain property, the human behaviour at one timestep only depended on its behaviour on the previous timestep $z_{t+1} \sim P(z_{t+1} | s_t, z_t, a_t)$. 