% \subsection*{Problem Formulation}
\subsection*{Computational formulation}
\label{subsec:comp_form}

The problem described above is formulated as learning an optimal policy in Markov Decision Process environments where the changes are a function of the robot’s policy. The tasks are modelled as a Markov Decision Process (MDP), $M_V=(S,A,R_v,P_v,\rho_0,\gamma)$, where $s \in S$ is the state space observed by the robot, $a \in A$ is the robot’s action space, $R_v : S \times A \times S \rightarrow R$ is the reward function, $T_v : S \times A \rightarrow \Delta(S)$ is the state transition probability function, $\rho_0$ is the initial state distribution, and $\gamma \in [0,1]$ is the discount factor. 

Following the meta-RL paradigm described above, each task (behaviour) is assumed to be represented by a latent vector (with parameters $v \in V \subset R^d$) that is Markovian and captures the changes in behaviour. This makes the environment dynamics a function of $v$, $P(s', r | s, a, v)$. The class of MDPs with this parameterisation is defined by $\mathcal{M} : = \{M_v : v \in V\}$. 

% At each timestep the simulated human agent changes behaviour, demoted with $z_t$ (see Figure~\ref{fig:transition model}) according to the history-dependent stochastic human dynamics $\rho_z$, where $z_{t+1} \sim \rho_z (h_{H,L} ; w_{H} )$.  %and $h_{H,L}=(s_{t-L:t},a_{t-L:t})$. 
% This changes the environment model from $M_{v_t}$ to $M_{v_{t+1}}$. 
% The change in behaviour is assumed to be Markovian. 
The robot agent selects its action $a_t \sim \pi_\theta (a | s_t, \hat{v_t})$, receives reward $r$, observes next state $s'$, and aims to maximise the expected cumulative reward defined in~\eqref{eq:RL_objective}.

 % The objective is to maximise the expected cumulative reward with an agent over multiple (up to $\infty$) repeated interactions.

% \begin{equation}
% \theta^* = \arg \max_{\theta} \sum_{j}^{\infty} {\mathbb{E}_{\pi_\theta {(\tau \sim p(\tau_t))}} {[R(\tau)]}}
% \end{equation}

 % This gives the formulation for learning a policy to maximise rewards in the context of dynamic, non-stationary human interactions.

% \theo{a concluding text is missing here}

% \subsection{Transition Model}
% \input{sections/problem_statement/transition_model.tex}

% This represents the set of possible tasks that the robot can encounter. 