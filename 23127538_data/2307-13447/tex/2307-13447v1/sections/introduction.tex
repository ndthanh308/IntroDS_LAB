

Reinforcement learning (RL) provides a powerful framework for robots to learn policies, but such learning has focused on single-agent tasks and few robots can interact effectively with humans \cite{xie_learning_2020, nguyen_review_2019, stouraitis_online_2020}.
In contrast to existing robotic agents, humans are proficient in collaborating with other humans. Yet, our understanding of the joint action mechanisms is still the subject of research~\cite{obhi_moving_2011}. A key aspect of joint action, identified by Sheridan~\cite{sheridan_eight_nodate}, is to acquaint both human and robot agents with models of their partners so that  predicting the behaviour of the partner agent is possible. 

RL in a multi-agent setting requires the robot agent to account for the human's behaviour \cite{kim_influencing_2022}. The difficulty is that human behaviour is typically non-stationary and shows individual differences (requiring ad-hoc coordination). This means that robot agents need to predict human actions based on the observation history, but this has proved extremely challenging to date \cite{xie_learning_2020, yang_optimal_2022}. In the current work, we focus on how a robot agent could predict the behaviour of a collaborative human agent with non-stationary behaviour.

To understand such a challenge better, let us consider two players passing a ball between themselves. The speed of the ball combined with the perceptual and motor lags of the humans means that each player must predict where the other player will hit the ball~\cite{bobu_less_2020}. The human's behavior is affected by long and short-term historical dependencies, such as the pose of the partner, previous hit, \textit{etc}. To accurately predict the goal of the next hit by the human, both short term (\textit{e.g.} previous partner pose) and long term (\textit{e.g.} direction of motion of the partner) information should be used~\cite{nomura_prediction_2008}. To effectively deal with this dynamic environment, it is imperative for each player to update their predictions and their strategies according to their partner's behaviour~\cite{nikolaidis_human-robot_2017}. 

% However, the presence of noise can introduce unpredictability, complicating these predictions.

% Conversely, a tennis coach will use her predictions to help a novice player engage in a rally.
To study such a cooperative problem, we created a 'virtual' ball hitting game to explore the effectiveness of a meta-learning approach. Our approach uses a transformer to learn a latent variable that allows the robot agent to dynamically update its behaviour. We explored whether this could help a robot agent predict where a simulated human agent would hit the ball and thereby collaborate successfully (despite non-stationarity). This manuscript details the following contributions that resulted from these endeavours:

\begin{itemize}
  % \item Formalisation a zero-shot meta-learning theoretic framework for dealing with non-stationary behaviour.
  \item Formalisation of the 'human-robot cooperative RL problem with non-stationary human agent behaviours' as a zero-shot meta-learning theoretic framework.
  % \item Development of a transformer-based approach to interact effectively with humans whose behaviour changes over time.
  \item Development of a transformer-based RL approach to; (i) infer the latent state of a human agent whose behaviour changes over time, and (ii) enable a robot agent to effectively collaborate  with a human agent. 
  % behaviour changes over time
  % trained with a classical RL method to collaborate effectively with a human agent that behaviour changes over time and .
  \item Demonstration that the behavioural transformer approach - BeTrans - can effectively cooperate with simulated human agents and outperform state-of-the-art methods, evaluated in a novel custom environment.
\end{itemize}

Next, we summarize the relevant related work and highlight how this differs to our proposed approach.