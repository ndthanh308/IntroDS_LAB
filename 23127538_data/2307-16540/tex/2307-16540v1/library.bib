@article{Wei2022,
author = {Wei, Ziyun and Trummer, Immanuel},
doi = {10.14778/3574245.3574272},
journal = {PVLDB},
number = {4},
pages = {905--917},
title = {{SkinnerMT: Parallelizing for Efficiency and Robustness in Adaptive Query Processing on Multicore Platforms}},
volume = {16},
year = {2022}
}
@article{Wang2021,
author = {Wang, Junxiong and Trummer, Immanuel and Basu, Debabrota},
journal = {{PVLDB}},
number = {13},
pages = {3402--3414},
title = {{UDO: Universal Database Optimization using Reinforcement Learning}},
url = {http://arxiv.org/abs/2104.01744},
volume = {14},
year = {2021}
}
@article{Li2018,
abstract = {Database knob tuning is important to achieve high performance (e.g., high throughput and low latency). However, knob tuning is an NP-hard problem and existing methods have several limitations. First, DBAs cannot tune a lot of database instances on different environments (e.g., different database vendors). Second, traditional machine-learning methods either cannot find good configurations or rely on a lot of high-quality training examples which are rather hard to obtain. Third, they only support coarse-grained tuning (e.g., workload-level tuning) but cannot provide fine-grained tuning (e.g., query-level tuning). To address these problems, we propose a query-aware database tuning system QTune with a deep reinforcement learning (DRL) model, which can efficiently and effectively tune the database configurations. QTune first featurizes the SQL queries by considering rich features of the SQL queries. Then QTune feeds the query features into the DRL model to choose suitable configurations. We propose a Double-State Deep Deterministic Policy Gradient (DS-DDPG) model to enable query-aware database configuration tuning, which utilizes the actor-critic networks to tune the database configurations based on both the query vector and database states. QTune provides three database tuning granularities: querylevel, workload-level, and cluster-level tuning. We deployed our techniques onto three real database systems, and experimental results show that QTune achieves high performance and outperforms the state-of-the-art tuning methods.},
author = {Li, Guoliang and Zhou, Xuanhe and Li, Shifu and Gao, Bo},
doi = {10.14778/3352063.3352129},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - QTune A QueryAware database tuning system with deep reinforcement learning.pdf:pdf},
issn = {21508097},
journal = {{PVLDB}},
number = {12},
pages = {2118--2130},
title = {{QTune: A QueryAware Database Tuning System with Deep Reinforcement Learning}},
volume = {12},
year = {2018}
}
@article{VanAken2021,
abstract = {Modern database management systems (DBMS) expose dozens of configurable knobs that control their runtime behavior. Setting these knobs correctly for an application's workload can improve the performance and efficiency of the DBMS. But because of their complexity, tuning a DBMS often requires considerable effort from experienced database administrators (DBAs). Recent work on automated tuning methods using machine learning (ML) have shown to achieve better performance compared with expert DBAs. These ML-based methods, however, were evaluated on synthetic workloads with limited tuning opportunities, and thus it is unknown whether they provide the same benefit in a production environment. To better understand ML-based tuning, we conducted a thorough evaluation of ML-based DBMS knob tuning methods on an enterprise database application. We use the OtterTune tuning service to compare three state-of-the-art ML algorithms on an Oracle installation with a real workload trace. Our results with OtterTune show that these algorithms generate knob configurations that improve performance by 45% over enterprise-grade configurations. We also identify deployment and measurement issues that were overlooked by previous research in automated DBMS tuning services.},
author = {{Van Aken}, Dana and Yang, Dongsheng and Brillard, Sebastien and Fiorino, Ari and Zhang, Bohan and Bilien, Christian and Pavlo, Andrew},
doi = {10.14778/3450980.3450992},
file = {:Users/immanueltrummer/Documents/Mendeley/p1241-aken.pdf:pdf},
issn = {21508097},
journal = {{PVLDB}},
number = {7},
pages = {1241--1253},
title = {{An Inquiry into Machine Learning-Based Automatic Configuration Tuning Services on Real-World Database Management Systems}},
volume = {14},
year = {2021}
}
@article{Hilprecht2019a,
abstract = {In this paper we introduce a partitioning advisor for analytical workloads based on Deep Reinforcement Learning. In contrast to existing approaches for automated partitioning design, an RL agent learns its decisions based on experience by trying out different partitionings and monitoring the rewards for different workloads. In our experimental evaluation with a distributed database and various complex schemata, we show that our learned partitioning advisor is thus not only able to find partitionings that outperform existing approaches for automated data partitioning but is also able to find non-obvious partitionings.},
OPTarchivePrefix = {arXiv},
OPTarxivId = {arXiv:1904.01279v1},
author = {Hilprecht, Benjamin and Binnig, Carsten and R{\"{o}}hm, Uwe},
doi = {10.1145/3329859.3329876},
OPTeprint = {arXiv:1904.01279v1},
isbn = {9781450368025},
issn = {07308078},
journal = {SIGMOD},
title = {{Towards Learning a Partitioning Advisor with Deep Reinforcement Learning}},
year = {2019}
}
@inproceedings{Trummer2022,
author = {Trummer, Immanuel},
booktitle = {SIGMOD},
doi = {10.1145/3514221.3517843},
file = {:Users/immanueltrummer/Documents/Mendeley/3514221.3517843.pdf:pdf},
pages = {190--203},
title = {{DB-BERT: A Database Tuning Tool that ``Reads the Manual''}},
year = {2022}
}
@inproceedings{Zhu2017,
annote = {Implementation: Quickstep DBMS

Motivation
* Query optimizer make sometimes big mistakes
* Focus is on robustness, immune to cardinality errors
* Aims to avoid changes to the query optimizer

Approach: lookahead information passing (LIP)
- implemented within a main-memory analytics DBMS
- adaptive strategy that converges to best join order
- aggressive use of semi-join-like filtering techniques
- first apply predicates on dimension tables and hash
- robustness criterion: difference between best and worst cost, normalized by table size and selectivity spread, is bounded by a certain threshold

Approach details
- divides fact table into small batches
- optimal join order places selective predicates first
- idea: forward information from later predicates
- approximate processing via bloom filters
- followed by clean-up phase using hash tables
- adaptive reordering based on selectivity estimates

Bloom filter
- can configure number of bits and hash functions
- set target threshold for false positive probability
- for item set bits determined by all hash functions
- test membership by evaluating all hash functions
- probability for false positive is probability of collision in all hash functions

Theoretical analysis
- fast convergence for selectivities from 5 to 25 {\%}
- bound convergence of selectivity estimates
- latter bound based on Chebychev distribution
- ignore overhead until convergence
- assumption on selectivity for robustness bounds

Experiments
- based on the STAR schema benchmark
- compares robustness with vs. without LIP
- metric: execution times for different start plans

Restrictions
- left-deep pipeline-able join trees with hash joins
- STAR queries with natural joins
- theoretical analysis assumes independence
- assumes that predicates have different selectivity
- neglects effect of different hash table sizes
- assumes that optimal plan starts with fact table
- assume data randomly distributed in same batch
- requires to tune a couple of parameters (Bloom bits)},
author = {Zhu, Jianqiao and Potti, Navneet and Saurabh, Saket and Patel, Jignesh M.},
booktitle = {SIGMOD},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Looking ahead makes query plans robust.pdf:pdf},
mendeley-groups = {Bounded Regret Execution},
pages = {889--900},
title = {{Looking ahead makes query plans robust}},
year = {2017}
}
@inproceedings{Aberger2018,
author = {Aberger, Christopher and Lamb, Andrew and Olukotun, Kunle and Re, Christopher},
booktitle = {ICDE},
doi = {10.1109/ICDE.2018.00048},
keywords = {Worst case optimal join,business intelligence querying,join processing,linear algebra querying},
pages = {449--460},
title = {{Levelheaded: A Unified Engine for Business Intelligence and Linear Algebra Querying}},
year = {2018}
}
@article{Krebs2002,
abstract = {This paper looks at the difficulty in mapping covert networks. Analyzing networks after an event is fairly easy for prosecution purposes. Mapping covert networks to prevent criminal activity is much more difficult. We examine the network surrounding the tragic events of September 11th, 2001. Through public data we are able to map a portion of the network centered around the 19 dead hijackers. This map gives us some insight into the terrorist organization, yet it is incomplete. Suggestions for further work and research are offered},
OPTarchivePrefix = {arXiv},
OPTarxivId = {cond-mat/0309488},
author = {Krebs, Valdis E},
OPTeprint = {0309488},
file = {:Users/immanueltrummer/Documents/Mendeley/ACLURM002810.pdf:pdf},
isbn = {0029-8549},
issn = {0226-1776},
journal = {Connections},
keywords = {3,43-52},
number = {3},
pages = {43--52},
pmid = {17950380},
primaryClass = {cond-mat},
title = {{Mapping Networks of Terrorist Cells}},
url = {http://www.insna.org/pubs/connections/v24.html},
volume = {24},
year = {2002}
}
@article{Khodadadi2021,
abstract = {The k-clique problem is identifying the largest complete subgraph of size k on a network, and it has many applications in Social Network Analysis (SNA), coding theory, geometry, etc. Due to the NP-Complete nature of the problem, the meta-heuristic approaches have raised the interest of the researchers and some algorithms are developed. In this paper, a new algorithm based on the Bat optimization approach is developed for finding the maximum k-clique on a social network to increase the convergence speed and evaluation criteria such as Precision, Recall, and F1-score. The proposed algorithm is simulated in Matlab{\textregistered} software over Dolphin social network and DIMACS dataset for k = 3, 4, 5. The computational results show that the convergence speed on the former dataset is increased in comparison with the Genetic Algorithm (GA) and Ant Colony Optimization (ACO) approaches. Besides, the evaluation criteria are also modified on the latter dataset and the F1-score is obtained as 100{\%} for k = 5.},
author = {Khodadadi, Akram and Saeidi, Shahram},
doi = {10.1186/s40649-021-00087-y},
file = {:Users/immanueltrummer/Documents/Mendeley/s40649-021-00087-y.pdf:pdf},
issn = {21974314},
journal = {Computational Social Networks},
keywords = {Ant colony optimization,Bat optimization algorithm,Genetic algorithm,K-clique problem},
number = {1},
publisher = {Springer International Publishing},
title = {{Discovering the Maximum k-Clique on Social Networks Using Bat Optimization Algorithm}},
url = {https://doi.org/10.1186/s40649-021-00087-y},
volume = {8},
year = {2021}
}
@inproceedings{Kocsis2006,
annote = {Kocsis, 2006


Rollout based Monte-Carlo Planning Template
* iteratively generate episodes from initial state until planning time has run out
* each episode is a sequence of actions and states
* after each episode, the values of states and actions are updated
* after planning time has run out, the action with maximal value is selected


Details of Baseline Algorithm
* selects next action to take with uniform random distribution
* action and state values are computed bottom-up
* state value is maximum over all actions
* action value is average over successor states


UCT algorithm
* Upper Confidence Bounds on Trees
* solves exploration-exploitation dilemma
* selects action with maximal upper confidence bound
* provably good convergence towards optimal actions},
author = {Kocsis, Levente and Szepesv{\'{a}}ri, C},
booktitle = {European Conf. on Machine Learning},
file = {::},
mendeley-groups = {UCT,Optimization,Synthesis},
pages = {282--293},
title = {{Bandit Based Monte-Carlo Planning}},
url = {http://www.springerlink.com/index/D232253353517276.pdf},
year = {2006}
}

@inproceedings{Neumann,
author = {Neumann, Thomas and Kemper, Alfons},
booktitle = {BTW},
file = {::},
isbn = {9783885796350},
issn = {16175468},
mendeley-groups = {Query Optimization},
pages = {383--402},
title = {{Unnesting Arbitrary Queries}},
url = {http://www.btw-2015.de/res/proceedings/Hauptband/Wiss/Neumann-Unnesting_Arbitrary_Querie.pdf},
year = {2015}
}

@book{Sutton2018,
author = {Sutton, Richard S. and Barto, Andrew G.},
doi = {10.1016/s1364-6613(99)01331-5},
isbn = {0262193981},
issn = {13646613},
pages = {532},
pmid = {18255791},
title = {{Reinforcement Learning, Second Edition: An Introduction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
year = {2018}
}


@inproceedings{Yu2020b,
abstract = {Join order selection (JOS) - the problem of finding the optimal join order for an SQL query - is a primary focus of database query optimizers. The problem is hard due to its large solution space. Exhaustively traversing the solution space is prohibitively expensive, which is often combined with heuristic pruning. Despite decades-long effort, traditional optimizers still suffer from low scalability or low accuracy when handling complicated SQL queries. Recent attempts using deep reinforcement learning (DRL), by encoding join trees with fixed-length handtuned feature vectors, have shed some light on JOS. However, using fixed-length feature vectors cannot capture the structural information of a join tree, which may produce poor join plans. Moreover, it may also cause retraining the neural network when handling schema changes (e.g., adding tables/columns) or multialias table names that are common in SQL queries.In this paper, we present RTOS, a novel learned optimizer that uses Reinforcement learning with Tree-structured long short-term memory (LSTM) for join Order Selection. RTOS improves existing DRL-based approaches in two main aspects: (1) it adopts graph neural networks to capture the structures of join trees; and (2) it well supports the modification of database schema and multi-alias table names. Extensive experiments on Join Order Benchmark (JOB) and TPC-H show that RTOS outperforms traditional optimizers and existing DRL-based learned optimizers. In particular, the plan RTOS generated for JOB is 101% on (estimated) cost and 67% on latency (i.e., execution time) on average, compared with dynamic programming that is known to produce the state-of-the-art results on join plans.},
author = {Yu, Xiang and Li, Guoliang and Chai, Chengliang and Tang, Nan},
booktitle = {ICDE},
doi = {10.1109/ICDE48307.2020.00116},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2020 - Reinforcement learning with tree-LSTM for join order selection.pdf:pdf},
isbn = {9781728129037},
issn = {10844627},
pages = {1297--1308},  
title = {{Reinforcement learning with tree-LSTM for join order selection}},
volume = {2020-April},
year = {2020}
}

@inproceedings{boncz2018jcc,
  title={JCC-H: Adding Join Crossing Correlations with Skew to TPC-H},
  author={Boncz, Peter and Anatiotis, Angelos-Christos and Kl{\"a}be, Steffen},
  booktitle={TPCTC, Revised Selected Papers},
  pages={103--119},
  year={2018}
}


@inproceedings{Krishnan2018,
author = {Krishnan, Sanjay and Yang, Zongheng and Goldberg, Ken and Hellerstein, Joseph and Stoica, Ion},
booktitle = {aiDM},
pages = {1--6},
title = {{Learning to Optimize Join Queries with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1808.03196},
year = {2020}
}


@inproceedings{Marcus,
author = {{Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska}, Tim},
booktitle = {SIGMOD Rec.},
doi = {10.1145/3542700.3542702},
file = {:Users/immanueltrummer/Documents/Mendeley/3542700.3542703.pdf:pdf},
isbn = {9781450383431},
issn = {01635808},
number = {1},
pages = {5},
title = {{Bao: Making Learned Query Optimization Practical}},
volume = {51},
year = {2022}
}


@article{Marcus2018a,
author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
doi = {10.14778/3342263.3342644},
issn = {21508097},
journal = {{PVLDB}},
number = {11},
pages = {1705--1718},
title = {{Neo: A Learned Query Optimizer}},
volume = {12},
year = {2018}
}


@article{Freitag2020,
author = {Freitag, Michael and Bandle, Maximilian and Schmidt, Tobias and Kemper, Alfons and Neumann, Thomas},
doi = {10.14778/3407790.3407797},
issn = {21508097},
journal = {{PVLDB}},
number = {11},
pages = {1891--1904},
title = {{Adopting Worst-Case Optimal Joins in Relational Database Systems}},
volume = {13},
year = {2020}
}

@article{palla2005uncovering,
  title={Uncovering the Overlapping Community Structure of Complex Networks in Nature and Society},
  author={Palla, Gergely and Der{\'e}nyi, Imre and Farkas, Ill{\'e}s and Vicsek, Tam{\'a}s},
  journal={Nature},
  volume={435},
  number={7043},
  pages={814--818},
  year={2005},
  publisher={Nature Publishing Group UK London}
}

@article{palla2007quantifying,
  title={Quantifying social group evolution},
  author={Palla, Gergely and Barab{\'a}si, Albert-L{\'a}szl{\'o} and Vicsek, Tam{\'a}s},
  journal={Nature},
  volume={446},
  number={7136},
  pages={664--667},
  year={2007},
  publisher={Nature Publishing Group UK London}
}

@article{yu2006predicting,
  title={Predicting interactions in protein networks by completing defective cliques},
  author={Yu, Haiyuan and Paccanaro, Alberto and Trifonov, Valery and Gerstein, Mark},
  journal={Bioinformatics},
  volume={22},
  number={7},
  pages={823--829},
  year={2006},
  publisher={Oxford University Press}
}

@inproceedings{Aberger2016,
abstract = {There are two types of high-performance graph processing engines: low- and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code; hence ensuring that efficiency is the burden of the user. In high-level engines users write in query languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier to use but are orders of magnitude slower than the low-level engines. We present EmptyHeaded, a high-level engine that supports a rich datalog- like query language capable of expressing standard graph benchmarks and achieves performance comparable to that of low-level engines. EmptyHeaded uses a new class of join algorithms that satisfy strong theoretical guarantees but have thus far not achieved performance comparable to specialized graph processing engines. To achieve high performance, EmptyHeaded introduces a new join engine architecture, including a novel query optimizer and data layouts that leverage single-instruction multiple data (SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level approaches by up to three orders of magnitude on graph pattern queries, PageRank, and Single-Source Shortest Paths (SSSP), and is an order of magnitude faster than many standard low-level baselines. We validate that EmptyHeaded competes with the best-of-breed low-level engine (Galois), achieving comparable performance on PageRank and at most 3x worse performance on SSSP.},
annote = {Claim: first worst-case optimal join processing engine that competes with specialized engines on standard graph workloads

Related
* Worst-case optimal join algorithms
- AGM bound: maximal output size
--> Natural join query - create hypergraph
--> or: hyper-graph from graph pattern query
--> E.g., graph pattern is triangle or clique
--> attributes correspond to vertices
--> hyper-edges represent relations
--> fractional edge cover assignes edges to weights
--> for each node: weights sum of incident edges >=1
--> AGM: product of relation cardinality ^ weight
- 2-way joins sub-optimal for triangle queries
--> NPRR first worst-case optimal algorithm

* Query language similar to LogicBlox
- Conjunctive queries, aggregation, simple recursion
* Compiler: Generalized Hyoertree Decomposition
- Hyper tree: each node is join and projection
- Run worst-case optimal multiway join for each node
- Generalizes LogicBlox query plans
--> LogicBlox: only single-node GHD
--> Better asymptotic performance by generalization
- Use Yannakakis algorithm
--> Evaluates acyclic queries in time O(N + OUT)
--> N: input size, OUT: output size
- Optimization strategy: brute-force over GHTs
--> Basically searching for attribute order
--> Query plan is compiled into C++ code
--> Corresponds to nested loops
--> Based on fast intersection operation
--> I.e., complexity bounded by smaller set

* Exploits SIMD parallelism
- Challenging due to data skew
- Dense data: Bitset; Sparse data: integers
- Intersection algorithms for each layout combination
- Optimizer selects best layout, works well
* Empty-headed is in-memory DBMS
- Exploits multi-core and SIMD parallelism

Experiments
* Compares against prior graph processing engines
-PowerGraph, Snap-R, Ligra, Galois, SociaLite
- PostgreSQL is beaten by orders of magnitude
- Same for GraphX, Grail is similar to PowerGraph
* Benchmarks include Google+, Twitter, patents ...

Debate
* Are special-purpose engines for graphs required?
- Prior work shows that relational engines can compete with specialized graph DBs in distributed (not shared-memory) settings
- SAP Hana includes specialized components for graph processing},
OPTarchivePrefix = {arXiv},
OPTarxivId = {1503.02368},
author = {Aberger, Christopher R. and Tu, Susan and Olukotun, Kunle and R{\'{e}}, Christopher},
booktitle = {SIGMOD},
doi = {10.1145/2882903.2915213},
OPTeprint = {1503.02368},
file = {:Users/immanueltrummer/Documents/Mendeley/emptyheaded-sigmod2016.pdf:pdf},
isbn = {9781450335317},
issn = {07308078},
keywords = {generalized hypertree decomposi-,ghd,graph processing,single instruction multiple,tion,worst-case optimal join},
mendeley-groups = {Graph Data},
pages = {431--446},
pmid = {24655651},
title = {{EmptyHeaded: A Relational Engine for Graph Processing}},
OPTurl = {http://arxiv.org/abs/1503.02368},
year = {2016}
}


@inproceedings{Aref2015,
abstract = {The LogicBlox system aims to reduce the complexity of software development for modern applications which enhance and automate decision-making and enable their users to evolve their capabilities via a "self-service" model. Our perspective in this area is informed by over twenty years of experience building dozens of missioncritical enterprise applications that are in use by hundreds of large enterprises across industries such as retail, telecommunications, banking, and government. We designed and built LogicBlox to be the system we wished we had when developing those applications. In this paper, we discuss the design considerations behind the LogicBlox system and give an overview of its implementation, highlighting innovative aspects. These include: LogiQL, a unified and declarative language based on Datalog; the use of purely functional data structures; novel join processing strategies; advanced incremental maintenance and live programming facilities; a novel concurrency control scheme; and built-in support for prescriptive and predictive analytics.},
annote = {* LogiQL
- Extensional (Base) vs. intensional (Derived) predicates
- Base predicates correspond to relaitons (data)
- Derived predicates are specified via derivation rules
--> Rules of the form R <- F where F a formula
--> (Alternatively, of the form R = t <- F)
--> F is conjunction of (negated) atoms
--> Atoms cover base/derived/built-in predicates
--> Built-in predicates cover arithmetic operations etc.
--> Also covers aggregation
- Integrity constraints are of the form F->G (two formulas)
--> Covers for instance type constraints
--> Also covers inclusion constraints

* Support for predictive and prescriptive analytics
- Business analytics: 
descriptive analytics -> 
predictive analytcs -> 
prescriptive analytics
- support prescriptive analytics via MILP
- support predictive analytics via ML

* Query optimization for LFTJ
- Based on data samples
- Decide on attribute order
- Decide on parallelization (domain partitioning)
- Collect samples via rules

* Updating data, incremental updates
- Data structures are "mutable until shared"
- Threads keep working on mutable data structure
- Later, other threads rely on unmutability
- Efficient view maintenance (view $\sim$ derived predicate)
--> Reasons about delta in LFTJ paths

* Concurrency control
- Support full serializability without locking
- Based on transaction repairs
--> Detect dependencies between transactions
--> Assume we detected dependency between T1 and T2
--> Shift result of T2 based on result of T1
--> Use efficient incremental view update mechanism
-->},
author = {Aref, Molham and {Ten Cate}, Balder and Green, Todd J. and Kimelfeld, Benny and Olteanu, Dan and Pasalic, Emir and Veldhuizen, Todd L. and Washburn, Geoffrey},
booktitle = {SIGMOD},
doi = {10.1145/2723372.2742796},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Aref et al. - 2015 - Design and implementation of the LogicBlox system.pdf:pdf},
isbn = {9781450327589},
issn = {07308078},
keywords = {Datalog,Incremental maintenance,Leapfrog triejoin,Live programming,LogiQL,LogicBlox,Predictive analytics,Transaction repair},
mendeley-groups = {Worst-Case Optimal Joins},
pages = {1371--1382},
title = {{Design and Implementation of the LogicBlox System}},
year = {2015}
}


@article{Trummer2021c,
abstract = {SkinnerDB uses reinforcement learning for reliable join ordering, exploiting an adaptive processing engine with specialized join algorithms and data structures. It maintains no data statistics and uses no cost or cardinality models. Also, it uses no training workloads nor does it try to link the current query to seemingly similar queries in the past. Instead, it uses reinforcement learning to learn optimal join orders from scratch during the execution of the current query. To that purpose, it divides the execution of a query into many small time slices. Different join orders are tried in different time slices. SkinnerDB merges result tuples generated according to different join orders until a complete query result is obtained. By measuring execution progress per time slice, it identifies promising join orders as execution proceeds.Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We upper-bound expected execution cost regret, i.e., the expected amount of execution cost wasted due to sub-optimal join order choices. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations.We experimentally compare SkinnerDB's performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark, TPC-H, and JCC-H, as well as benchmark variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice.},
author = {Trummer, Immanuel and Wang, Junxiong and Wei, Ziyun and Maram, Deepak and Moseley, Samuel and Jo, Saehan and Antonakakis, Joseph and Rayabhari, Ankush},
doi = {10.1145/3464389},
file = {:Users/immanueltrummer/Documents/Mendeley/3464389.pdf:pdf},
issn = {15574644},
journal = {ACM Transactions on Database Systems},
keywords = {Query optimization,adaptive processing,reinforcement learning},
number = {3},
title = {{SkinnerDB: Regret-bounded Query Evaluation via Reinforcement Learning}},
volume = {46},
year = {2021}
}


@article{Menon2020,
abstract = {Just-in-time (JIT) query compilation is a technique to improve analytical query performance in database management systems (DBMSs). But the cost of compiling each query can be significant relative to its execution time. This overhead prohibits the DBMS from employing well-known adaptive query processing (AQP) methods to generate a new plan for a query if data distributions do not match the optimizer's estimations. The optimizer could eagerly generate multiple sub-plans for a query, but it can only include a few alternatives as each addition increases the compilation time. We present a method, called Permutable Compiled Queries (PCQ), that bridges the gap between JIT compilation and AQP. It allows the DBMS to modify compiled queries without needing to recompile or including all possible variations before the query starts. With PCQ, the DBMS structures a query's code with indirection layers that enable the DBMS to change the plan even while it is running. We implement PCQ in an in-memory DBMS and compare it against non-adaptive plans in a microbenchmark and against state-of-the-art analytic DBMSs. Our evaluation shows that PCQ outperforms static plans by more than 4× and yields better performance on an analytical benchmark by more than 2× against other DBMSs.},
author = {Menon, Prashanth and Ngom, Amadou and Ma, Lin and Mowry, Todd C. and Pavlo, Andrew},
doi = {10.14778/3425879.3425882},
file = {:Users/immanueltrummer/Documents/Mendeley/p101-menon.pdf:pdf},
issn = {21508097},
journal = {{PVLDB}},
number = {2},
pages = {101--113},
title = {{Permutable Compiled Queries: Dynamically Adapting Compiled Queries without Recompiling}},
volume = {14},
year = {2020}
}


@techreport{Tzoumas2008,
annote = {Main idea: Use reinforcement learning for adaptive query processing

Related: claims that there is no prior work applying reinforcement learning to adaptive query processing

* focus on two cases: multiple selections on one table and natural join queries
* focus on scenario where Eddies are used to process data at tuple granularity supporting plan adaptions
* multiple selections on one table: 
- states are the set of predicates tested for a given tuple
- actions are associated with the different predicates that remain to be evaluated
- invocation of tuple output and tuple scan is determined by fixed rule
- reward is based on selectivity and operation cost

Discussion
* do not give formal regret bounds
* explicitly mention that other reinforcement learning policies could be used with their approach},
author = {Tzoumas, Kostas and Sellis, Timos and Jensen, Christian S},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Tzoumas, Sellis, Jensen - 2008 - A reinforcement learning approach for adaptive query processing.pdf:pdf},
mendeley-groups = {Bounded Regret Execution},
title = {{A Reinforcement Learning Approach for Adaptive Query Processing}},
year = {2008}
}

@article{boncz2008breaking,
  title={Breaking the Memory Wall in MonetDB},
  author={Boncz, Peter A and Kersten, Martin L and Manegold, Stefan},
  journal={Communications of the ACM},
  volume={51},
  number={12},
  pages={77--85},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@article{postgres,
  title={The Design of Postgres},
  author={Stonebraker, Michael and Rowe, Lawrence A},
  journal={Sigmod Rec.},
  volume={15},
  number={2},
  pages={340--355},
  year={1986},
  publisher={ACM New York, NY, USA}
}
  
@inproceedings{Quanzhong2007b,
abstract = {Traditional query processing techniques based on static query optimization are ineffective in applications where statistics about the data are unavailable at the start of query execution or where the data characteristics are skewed and change dynamically. Several adaptive query processing techniques have been proposed in recent years to overcome the limitations of static query optimizers through either explicit re-optimization of plans during execution or by using a row-routing based approach. In this paper, we present a novel method for processing pipelined join plans that dynamically arranges the join order of both inner and outer-most tables at run-time. We extend the Eddies concept of "moments of symmetry" to reorder indexed nested-loop joins, the join method used by all commercial DBMSs for building pipelined query plans for applications for which low latencies are crucial. Unlike row-routing techniques, our approach achieves adaptability by changing the pipeline itself which avoids the bookkeeping and routing decision associated with each row. Operator selectivities monitored during query execution are used to change the execution plan at strategic points, and the change of execution plans utilizes a novel and efficient technique for avoiding duplicates in the query results. Our prototype implementation in a commercial DBMS shows a query execution speedup of up to 8 times.},
author = {Quanzhong, Li and Minglong, Shao and Markl, Volker and Beyer, Kevin and Colby, Latha and Lohman, Guy},
booktitle = {ICDE},
doi = {10.1109/ICDE.2007.367848},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Quanzhong et al. - 2007 - Adaptively reordering joins during query execution(3).pdf:pdf},
isbn = {1424408032},
issn = {10844627},
mendeley-groups = {Adaptive Query Processing},
pages = {26--35},
title = {{Adaptively Reordering Joins During Query Execution}},
year = {2007}
}


@article{Deshpande2004,
author = {Deshpande, Amol},
doi = {10.1145/974121.974129},
journal = {SIGMOD Rec.},
mendeley-groups = {Adaptive Query Processing},
number = {1},
pages = {44--49},
title = {{An Initial Study of Overheads of Eddies}},
volume = {33},
year = {2004}
}


@inproceedings{Avnur2000,
abstract = {In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments. In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of sym- metry during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from dif- ferent sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates promising results, with eddies performing nearly as well as a static optimizer/executor in static scenarios, and providing dramatic improvements in dynamic execution environments. ?????},
annote = {* Assumes that an initial query plan is constructed
Philosophy: favor adaptivity over best-case performance
* Eddy joins multiple inputs
* Associates tuples with state
- which operations were performed on them?
- which operations are applicable next?
* Try different strategies to select next operation
- learning via ticketing
* Mentions reinforcement learning as future work},
author = {Avnur, Ron and Hellerstein, Jm},
booktitle = {SIGMOD},
doi = {10.1145/342009.335420},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Avnur, Hellerstein - 2000 - Eddies continuously adaptive query processing.pdf:pdf},
isbn = {1581132182},
issn = {01635808},
mendeley-groups = {Adaptive Query Processing,Parametric Query Optimization},
pages = {261--272},
title = {{Eddies: Continuously Adaptive Query Processing}},
url = {http://dl.acm.org/citation.cfm?id=335420},
year = {2000}
}


@article{Lohman2014,
author = {Lohman, Guy},
journal = {SIGMOD Blog},
title = {{Is Query Optimization a “Solved” Problem?}},
year = {2014}
}

@inproceedings{Selinger1979,
abstract = {P23-34},
author = {Selinger, PG G and Astrahan, MM M and Chamberlin, D D and Lorie, R A and Price, T G},
booktitle = {SIGMOD},
file = {::},
mendeley-groups = {Query Optimization},
pages = {23--34},
title = {{Access Path Selection in a Relational Database Management System}},
url = {http://dl.acm.org/citation.cfm?id=582095.582099},
year = {1979}
}


@article{Gubichev2015,
annote = {- Many orders of magnitude cost difference between cheapest plan and average plan
- Cardinality estimates are often wrong and can cause bad plans
- Exhaustive algoroithms better than heuristics and simple randomized algorithms despite cardinality estimation errors
- Call join-crossing correlations "an open frontier for research"},
author = {Gubichev, Andrey and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Gubichev et al. - 2015 - How good are query optimizers, really.pdf:pdf},
journal = {{PVLDB}},
mendeley-groups = {Query Optimization,Cost Prediction},
number = {3},
pages = {204--215},
title = {{How Good are Query Optimizers, Really?}},
volume = {9},
year = {2015}
}

@inproceedings{trummer2019skinnerdb,
  title={SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning},
  author={Trummer, Immanuel and Wang, Junxiong and Maram, Deepak and Moseley, Samuel and Jo, Saehan and Antonakakis, Joseph},
  booktitle={{SIGMOD}},
  pages={1153--1170},
  year={2019}
}

@misc{snapnets,
  author       = {Jure Leskovec and Andrej Krevl},
  title        = {{SNAP Datasets}: {Stanford} Large Network Dataset Collection},
  howpublished = {\url{http://snap.stanford.edu/data}},
  month        = jun,
  year         = 2014
}

@inproceedings{nguyen2015join,
  author    = {Dung T. Nguyen and
               Molham Aref and
               Martin Bravenboer and
               George Kollias and
               Hung Q. Ngo and
               Christopher R{\'{e}} and
               Atri Rudra},
  title     = {Join Processing for Graph Patterns: An Old Dog with New Tricks},
  booktitle = {{GRADES}},
  pages     = {2:1--2:8},
  year      = {2015},
  url       = {https://doi.org/10.1145/2764947.2764948},
  doi       = {10.1145/2764947.2764948},
}

@article{DBLP:journals/sigmod/NgoRR13,
  author    = {Hung Q. Ngo and
               Christopher R{\'{e}} and
               Atri Rudra},
  title     = {Skew strikes back: new developments in the theory of join algorithms},
  journal   = {{SIGMOD} Rec.},
  volume    = {42},
  number    = {4},
  pages     = {5--16},
  year      = {2013},
  url       = {https://doi.org/10.1145/2590989.2590991},
  doi       = {10.1145/2590989.2590991},
}

@inproceedings{DBLP:conf/pods/KhamisNR16,
  author    = {Mahmoud Abo Khamis and
               Hung Q. Ngo and
               Atri Rudra},
  editor    = {Tova Milo and
               Wang{-}Chiew Tan},
  title     = {{FAQ:} Questions Asked Frequently},
  booktitle = {Proceedings of the 35th {ACM} {SIGMOD-SIGACT-SIGAI} Symposium on Principles
               of Database Systems, {PODS} 2016, San Francisco, CA, USA, June 26
               - July 01, 2016},
  pages     = {13--28},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2902251.2902280},
  doi       = {10.1145/2902251.2902280},
}

@article{DBLP:journals/jacm/NgoPRR18,
  author    = {Hung Q. Ngo and
               Ely Porat and
               Christopher R{\'{e}} and
               Atri Rudra},
  title     = {Worst-Case Optimal Join Algorithms},
  journal   = {J. {ACM}},
  volume    = {65},
  number    = {3},
  pages     = {16:1--16:40},
  year      = {2018},
  url       = {https://doi.org/10.1145/3180143},
  doi       = {10.1145/3180143},
}

@inproceedings{DBLP:conf/pods/Khamis0S17,
  author    = {Mahmoud Abo Khamis and
               Hung Q. Ngo and
               Dan Suciu},
  editor    = {Emanuel Sallinger and
               Jan Van den Bussche and
               Floris Geerts},
  title     = {What Do Shannon-type Inequalities, Submodular Width, and Disjunctive
               Datalog Have to Do with One Another?},
  booktitle = {Proceedings of the 36th {ACM} {SIGMOD-SIGACT-SIGAI} Symposium on Principles
               of Database Systems, {PODS} 2017, Chicago, IL, USA, May 14-19, 2017},
  pages     = {429--444},
  publisher = {{ACM}},
  year      = {2017},
  url       = {https://doi.org/10.1145/3034786.3056105},
  doi       = {10.1145/3034786.3056105},
}

@inproceedings{DBLP:conf/icdt/Veldhuizen14,
  author    = {Todd L. Veldhuizen},
  title     = {Triejoin: {A} Simple, Worst-Case Optimal Join Algorithm},
  booktitle = {{ICDT}},
  pages     = {96--106},
  year      = {2014},
  url       = {https://doi.org/10.5441/002/icdt.2014.13},
  doi       = {10.5441/002/icdt.2014.13},
}

@article{DBLP:journals/sigmod/OlteanuS16,
  author    = {Dan Olteanu and
               Maximilian Schleich},
  title     = {Factorized Databases},
  journal   = {{SIGMOD} Rec.},
  volume    = {45},
  number    = {2},
  pages     = {5--16},
  year      = {2016},
  url       = {https://doi.org/10.1145/3003665.3003667},
  doi       = {10.1145/3003665.3003667},
}

@article{DBLP:journals/tods/OlteanuZ15,
  author    = {Dan Olteanu and
               Jakub Z{\'{a}}vodn{\'{y}}},
  title     = {Size Bounds for Factorised Representations of Query Results},
  journal   = {{ACM} Trans. Database Syst.},
  volume    = {40},
  number    = {1},
  pages     = {2:1--2:44},
  year      = {2015},
  url       = {https://doi.org/10.1145/2656335},
  doi       = {10.1145/2656335},
}

@inproceedings{DBLP:conf/cidr/JinAS22,
  author    = {Guodong Jin and
               Nafisa Anzum and
               Semih Salihoglu},
  title     = {GRainDB: {A} Relational-core Graph-Relational {DBMS}},
  booktitle = {{CIDR}},
  publisher = {www.cidrdb.org},
  year      = {2022},
  url       = {https://www.cidrdb.org/cidr2022/papers/p57-jin.pdf},
}

@inproceedings{DBLP:conf/datalog/Aref19,
  author    = {Molham Aref},
  title     = {Relational Artificial Intelligence},
  booktitle = {{Datalog}},
  volume    = {2368},
  pages     = {1},
  year      = {2019},
  url       = {http://ceur-ws.org/Vol-2368/invited1.pdf},
}

@inproceedings{DBLP:conf/sigmod/SchleichOC16,
  author    = {Maximilian Schleich and
               Dan Olteanu and
               Radu Ciucanu},
  title     = {Learning Linear Regression Models over Factorized Joins},
  booktitle = {{SIGMOD}},
  pages     = {3--18},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2882903.2882939},
  doi       = {10.1145/2882903.2882939},
}

@inproceedings{DBLP:conf/icdt/000122,
  author    = {Hung Q. Ngo},
  title     = {On an Information Theoretic Approach to Cardinality Estimation (Invited
               Talk)},
  booktitle = {{ICDT}},
  volume    = {220},
  pages     = {1:1--1:21},
  year      = {2022},
  url       = {https://doi.org/10.4230/LIPIcs.ICDT.2022.1},
  doi       = {10.4230/LIPIcs.ICDT.2022.1},
}

@article{DBLP:journals/jacm/GottlobLVV12,
  author    = {Georg Gottlob and
               Stephanie Tien Lee and
               Gregory Valiant and
               Paul Valiant},
  title     = {Size and Treewidth Bounds for Conjunctive Queries},
  journal   = {J. {ACM}},
  volume    = {59},
  number    = {3},
  pages     = {16:1--16:35},
  year      = {2012},
  url       = {https://doi.org/10.1145/2220357.2220363},
  doi       = {10.1145/2220357.2220363},
}

@article{DBLP:journals/siamcomp/AtseriasGM13,
  author    = {Albert Atserias and
               Martin Grohe and
               D{\'{a}}niel Marx},
  title     = {Size Bounds and Query Plans for Relational Joins},
  journal   = {{SIAM} J. Comput.},
  volume    = {42},
  number    = {4},
  pages     = {1737--1767},
  year      = {2013},
  url       = {https://doi.org/10.1137/110859440},
  doi       = {10.1137/110859440},
}

@article{udo, author = {Wang, Junxiong and Trummer, Immanuel and Basu, Debabrota}, title = {UDO: Universal Database Optimization Using Reinforcement Learning}, year = {2021}, issue_date = {September 2021},  volume = {14}, number = {13}, issn = {2150-8097}, doi = {10.14778/3484224.3484236}, 
journal = {{PVLDB}}, month = {sep}, pages = {3402–3414}, numpages = {13} }

@article{DBLP:journals/jacm/Marx13,
  author    = {D{\'{a}}niel Marx},
  title     = {Tractable Hypergraph Properties for Constraint Satisfaction and Conjunctive
               Queries},
  journal   = {J. {ACM}},
  volume    = {60},
  number    = {6},
  pages     = {42:1--42:51},
  year      = {2013},
  url       = {https://doi.org/10.1145/2535926},
  doi       = {10.1145/2535926},
}

@inproceedings{bait,
author = {Wu, Wentao and Wang, Chi and Siddiqui, Tarique and Wang, Junxiong and Narasayya, Vivek and Chaudhuri, Surajit and Bernstein, Philip A.},
title = {Budget-Aware Index Tuning with Reinforcement Learning},
year = {2022},
isbn = {9781450392495},
url = {https://doi.org/10.1145/3514221.3526128},
doi = {10.1145/3514221.3526128},
booktitle = {{SIGMOD}},
pages = {1528–1541},
numpages = {14}
}

@inproceedings{udodemo, author = {Wang, Junxiong and Trummer, Immanuel and Basu, Debabrota}, title = {Demonstrating UDO: A Unified Approach for Optimizing Transaction Code, Physical Design, and System Parameters via Reinforcement Learning}, year = {2021}, isbn = {9781450383431},  url = {https://doi.org/10.1145/3448016.3452754}, doi = {10.1145/3448016.3452754}, 
booktitle = {SIGMOD}, year= 2021, pages = {2794–2797}
}

@article{DBLP:journals/pvldb/BakibayevKOZ13,
  author    = {Nurzhan Bakibayev and
               Tom{\'{a}}s Kocisk{\'{y}} and
               Dan Olteanu and
               Jakub Zavodny},
  title     = {Aggregation and Ordering in Factorised Databases},
  journal   = {{PVLDB}},
  volume    = {6},
  number    = {14},
  pages     = {1990--2001},
  year      = {2013},
  url       = {http://www.vldb.org/pvldb/vol6/p1990-zavodny.pdf},
  doi       = {10.14778/2556549.2556579},
}

@article{DBLP:journals/sigmod/Khamis0R17,
  author    = {Mahmoud Abo Khamis and
               Hung Q. Ngo and
               Atri Rudra},
  title     = {Juggling Functions Inside a Database},
  journal   = {{SIGMOD} Rec.},
  volume    = {46},
  number    = {1},
  pages     = {6--13},
  year      = {2017},
  url       = {https://doi.org/10.1145/3093754.3093757},
  doi       = {10.1145/3093754.3093757},
}

@inproceedings{Ngo,
annote = {* Goal: beyond worst-case complexity for indexed data
- Assume that indices are consistent with GAO
- GAO = Global Attribute Order

* Complexity of O(C+Z)
- C is size of certificate (may be smaller than input data)
- Z is size of join output
- Neglect factors that depend on query size

* Algorithm overview
- Repeatedly get probing point, considering constraints
- Probe indexes on base relation, two possibilities:
--> Find result tuple: add tuple and constraint
--> Find gap: add corresponding constraint

* Constraint data structure
- Organize output space as a tree
- Tree levels correspond to attributes
- Order is determined by global attribute order
- Inner nodes contain equality attributes and constraints

* Efficiently calculating probing points
- Lazy approach: keep constraint insertion cheap
- Getting probing points can be more expensive
- For each tree node, find next interval or point

* Complexity depends on global attribute order
- Mention optimizing it as future research},
author = {Ngo, Hung Q and R{\'{e}}, Christopher},
booktitle = {PODS},
file = {:Users/immanueltrummer/Library/Application Support/Mendeley Desktop/Downloaded/Ngo, R{\'{e}} - Unknown - Beyond Worst-case Analysis for Joins with Minesweeper ˚ Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450323758},
keywords = {adaptive algorithm,beta-acyclic queries,bounded treewidth,cate,certifi-,instance optimality,join algorithms,triangle query},
mendeley-groups = {Worst-Case Optimal Joins},
pages = {234--245},
title = {{Beyond Worst-case Analysis for Joins with Minesweeper Categories and Subject Descriptors}},
year = {2014}
}

@techreport{ADOPT:2023:extended,
  author      = "Junxiong Wang and Immanuel Trummer and Ahmet Kara and Dan Olteanu",
  title       = "ADOPT: Adaptively Optimizing Attribute Orders for Worst-Case Optimal Join Algorithms via Reinforcement Learning",
  OPTinstitution = "",
  year        = "2023",
  type        = "",
  number      = "",
  address     = "\url{https://github.com/jxiw/ADOPT/blob/main/report/ADOPT.pdf}",
  month       = "",
  OPTnote        = "\url{https://github.com/jxiw/ADOPT/blob/main/report/ADOPT.pdf}",
  annote      = ""
}