\subsection{Estimating Order Quality}
\label{sub:estimation}

%Those values serve as quality samples, judging the quality of an attribute order when processing one specific partition of the data. While quality estimates may vary for the same order, across different invocations, ADOPT converges to the order with maximal average quality over time. 



The reinforcement learning, described in Section~\ref{sub:rl}, is guided by reward values. Next, we discuss the definition of the reward function. Before that, we introduce an auxiliary function, measuring the volume of a cube as the product of range sizes over all dimensions:

\begin{equation}
    Volume(c)=\prod_{i}(c.u_i-c.l_i)
\end{equation}

With a slight abuse of notation, we  write $Volume(q)$ to denote the volume of the cube, spanned by all join attributes of a query $q$. 

In order to fully process a query, ADOPT must cover the cube representing the entire space of attribute value combinations. Hence, the more volume of that cube we cover per time unit, the faster query processing is. \revision{Even for a fixed attribute order, the volume processed per time unit may vary across different parts of the data (e.g., since the number of result tuples per volume varies). However, the fastest order processes most volume in average, averaging over the entire data set, and the UCT algorithm converges to decisions with highest average reward, even if the reward function is noisy~\cite{Kocsis2006}.} This implies that volume covered is a useful measure of progress. The reward function, presented next, follows that intuition. Given a set of processed cubes $P$ for query $q$, it uses the aggregate volume covered, scaled to the total volume to process (scaling ensures reward values between zero and one, consistent with the requirements of the UCT algorithm):

\begin{equation}
    Reward(P,\revision{q})=(\sum_{p\in P}Volume(p))/Volume(q)
\end{equation}

%\junxiong{However, $Reward(P, c)$ is not a precious measurement of the execution progress. During the execution, ADOPT splits cubers into smaller unbalanced cubes and small cubes are more likely to finish during the timeout. Thus, the progress of a target cube $c$ can not reflect the quality of selected join order. To tackle this issue, we scale this reward using the fraction between the volume of target cube and the volume of entire cube. The final reward $r$ is calculate as }

% \begin{equation}
%     r =  Reward(P,c) Volume(c) / Volume(Entire Cube)
% \end{equation}

% \junxiong{Directly using $Reward(P, c)$ as final reward $r$ to update the statistics of UCT leads  }