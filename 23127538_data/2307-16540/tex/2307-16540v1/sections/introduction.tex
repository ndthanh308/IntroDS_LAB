\section{Introduction}

The area of join processing has recently been revolutionized by  worst-case optimal join algorithms~\cite{DBLP:conf/icdt/Veldhuizen14,DBLP:journals/jacm/NgoPRR18}. LeapFrog TrieJoin (LFTJ) is a prime example of a worst-case optimal join algorithm~\cite{DBLP:conf/icdt/Veldhuizen14}. 
Such algorithms guarantee asymptotically worst-case optimal performance. Those formal guarantees set them apart from traditional join algorithms, which are known to be sub-optimal~\cite{DBLP:journals/sigmod/NgoRR13}. In practice, they often translate into orders of magnitude runtime improvements, specifically for cyclic queries, compared to traditional approaches. They are incorporated in  several recent query engines: for factorized databases~\cite{DBLP:journals/tods/OlteanuZ15,DBLP:journals/sigmod/OlteanuS16}, graph processing~\cite{Aberger2016,DBLP:conf/cidr/JinAS22} and general query processing~\cite{Freitag2020}, in-database machine learning~\cite{DBLP:conf/sigmod/SchleichOC16}, and in the commercial systems LogicBlox~\cite{Aref2015} and RelationalAI~\cite{DBLP:conf/datalog/Aref19}.

\revision{As pointed out in prior work~\cite{DBLP:conf/icdt/Veldhuizen14}, in practice, the} performance of worst-case optimal join algorithms often depends heavily on the order in which join attributes (i.e., groups of join columns that are linked by equality constraints) are processed. \revision{Yet this is not reflected in the formal analysis of worst-case optimal join algorithms~\cite{DBLP:conf/icdt/Veldhuizen14}. Worst-case optimality is defined with regards to worst-case assumptions about the database content. Under these assumptions, different attribute orders have asymptotically equivalent time complexity. On the other side, given the actual database content, some attribute orders may perform much better than others in practice. Similar to the classical join ordering problem, it is therefore important to aim for the instance-optimal order, e.g., using data statistics.} 

%Even though all orders are equally good to achieve optimality in the {\em worst case}, their performance may differ significantly in practice. Not unlike the join order for traditional join algorithms, the optimal attribute order depends on the database state. For a fixed database state, the performance gap between good and bad attribute orders is significant, even for moderately sized data sets. Whereas worst-case optimality guarantees (asymptotically) optimal performance, relative to a \textit{worst-case database state}, it does not guarantee optimality with regards to the \textit{actual state}. Hence, the theoretical guarantees offered by worst-case optimal algorithms do not remove the need for careful attribute order selection.

% Figure environment removed

\begin{example}
\rm
Figure~\ref{fig:intro} illustrates the need for accurate attribute order selection. It compares LFTJ execution times (scaled to the time of the fastest order) for different attribute orders and the same query that asks for the number of cliques of five distinct nodes. 120 attribute orders (on the x-axis) are ranked by execution time. \nop{A more detailed description of the experimental setup is deferred to Section~\ref{sec:experiments}.} The performance gap between the best and worst attribute orders is more than 16x. The choice of an attribute order has thus significant impact on performance and near-optimal orders are sparse.
\end{example}

Execution engines using worst-case optimal join operators (e.g., the LogicBlox system~\cite{Aref2015}) typically select attribute orders via a query optimizer. Similar to traditional query optimizers selecting join orders, such optimizers exploit data statistics and simplifying cost models to pick an attribute order. This approach is however risky. Erroneous cost estimates (e.g., due to data skew not represented in data statistics) can lead to highly sub-optimal attribute order choices. Incorrect cost estimates are known to cause significant overheads in traditional query optimization~\cite{Lohman2014}. The experiments in Section~\ref{sec:experiments} show that this case appears in the context of optimization for worst-case optimal join algorithms as well. \revision{In particular, this applies to queries on non-uniform data with an elevated number of predicates, increasing the potential for inter-predicate correlations that make size and cost predictions hard.}

%In particular, optimizing queries with an elevated number of join attributes is difficult especially for large queries on non-uniform data where cost estimation is difficult. The following example enumerates concrete use cases that have these properties.}

\begin{example}
\label{ex:usecases}
\rm
\revision{In social network analysis, analysts are often interested in finding people who are mutually connected in cliques via links in the graph representing the social network~\cite{Khodadadi2021, Krebs2002, palla2005uncovering}. Specifically, prior analysis often considers cliques of up to five or six~\cite{Khodadadi2021, palla2005uncovering}, or more~\cite{Krebs2002} members. The experiments in Section~\ref{sec:experiments} show that such queries already create challenges in cost prediction, making methods that are robust to prediction errors preferable.}
\end{example}

%The guarantees, offered by worst-case optimal join algorithms, refer however to a worst-case scenario. They do not refer to the actual database state. Hence, worst-case optimal join algorithms do not guarantee optimal execution time


% % Figure environment removed



To overcome these challenges, we propose an adaptive execution strategy for worst-case optimal join algorithms. The goal of adaptive processing is to enable attribute order switches, during query processing. The processing time is divided into episodes and in each episode we may choose an attribute order for the execution of the query over a fragment of the input data. By measuring execution speed for different attribute orders, the adaptive processing framework converges to near-optimal attribute orders over time. To the best of our knowledge, this is the first adaptive processing strategy for worst-case optimal join algorithms.

Adaptive processing for query processing based on attribute orders leads, however, to new challenges, discussed in the following. 

First, we must limit overheads due to attribute order switching. In particular, we must avoid redundant processing when applying multiple attribute orders to the same data. We solve this challenge by a task manager, capturing execution progress achieved by different attribute orders. Join result tuples are characterized by a value combination for join attributes. Hence, we generally represent execution progress by (hyper)cubes within the Cartesian product of value ranges over all join attributes. Having processed a cube implies that all contained result tuples, if any, have been generated. Data processing threads query the task manager to retrieve cubes not covered previously. Also, the task manager is updated whenever new results become available. It ensures that different threads process non-overlapping cubes, independently of the current attribute order. Query processing ends once all processed cubes, in aggregate, cover the full space of join attribute value ranges.

%is queried before each attribute order switch. It returns a hypercube, within the space formed by the Cartesian product of value ranges of all join attributes, that has not been processed according to any prior attribute order. Hence, all query result tuples contained within the hypercube (if any) have not been generated yet. Until the next switch, join processing is therefore restricted to tuples within the hypercube. Before switching to a new attribute order, all execution progress achieved until that point is registered in the aforementioned data structure. The unexplored subspace from the current hypercube is expressed by a disjoint union of a small number of  hypercubes and returned back to the data structure for future processing.

Second, we need a metric to compare different attribute orders, based on run time feedback. This metric must be applicable even when executing attribute orders for a very short amount of time. The number of result tuples generated per time unit may appear to be a good candidate metric. However, it is not informative in case of small results. Instead, we opt for a metric analyzing the size of the hypercube (within the Cartesian product of join attribute values) covered per time unit. Even if no result tuples are generated, this metric rewards attribute orders that quickly discard subsets of the output space.

Third, we must choose, in each episode, which attribute order to select next. This choice is challenging as it is subject to the so called \textit{exploration-exploitation dilemma}. Choosing attribute orders that obtained good scores in past invocation (exploitation) may seem beneficial to generate a full query result as quickly as possible. However, executing attribute orders about which little is known (exploration) may be better. It may lead to even better attribute orders that can be selected in future episodes. To balance between these two extremes in a principled manner, we employ methods from the area of reinforcement learning. Under moderately simplifying assumptions, based on the guarantees offered by these methods, we can show that ADOPT converges to optimal attribute orders.

% In summary, our original, scientific contributions
% are the following.

We have integrated our approach for adaptive processing with worst-case optimal join algorithms into ADOPT (ADaptive wOrst-case oPTimal joins), a novel, analytical SQL processing engine. We compare ADOPT to various baselines, including traditional database systems such as PostgreSQL and MonetDB, prior methods for adaptive processing such as SkinnerDB~\cite{Trummer2021c}, as well as commercial and open-source database engines that use worst-case optimal join algorithms. We evaluate all systems on acyclic and cyclic queries from public benchmarks, \revision{TPC-H, JCC-H~\cite{boncz2018jcc},} join order~\cite{Gubichev2015} and SNAP graph data~\cite{snapnets,nguyen2015join} workloads. \revision{For complex queries on skewed data, ADOPT outperforms all competitors.} In particular, it improves over a commercial database engine using the same worst-case optimal join algorithm as ADOPT. This demonstrates the benefit of adaptive attribute order selections.

%For small queries, adaptive processing does not result in performance improvements over competitors. However, for larger and more complex queries, ADOPT outperforms all competitors. 

In summary, the contributions in this paper are the following:

\begin{itemize}
    \item We propose the first adaptive processing strategy for worst-case optimal join algorithms using reinforcement learning.
    \item We describe specialized data structures, progress metrics, and learning algorithms that make adaptive processing in this scenario practical.
    \item We formally analyze worst-case optimality guarantees and convergence properties.
    \item We compare ADOPT experimentally against various baselines, showing that it outperforms them for a variety of acyclic and cyclic queries and datasets.
\end{itemize}

%implement our adaptive strategy in a system called ADOPT. We

The remainder of this paper is organized as follows. Section~\ref{sec:overview} presents an overview of the ADOPT system. Section~\ref{sec:algorithm} describes the algorithm used for adaptive processing in detail. Section~\ref{sec:analysis} analyzes our approach formally.
Section~\ref{sec:experiments} shows experimentally that ADOPT outperforms a range of competitors for both acyclic and cyclic queries.
Section~\ref{sec:related} discusses prior related work.
Appendices \ref{app:statistics_graph_datasets}-\ref{sec:attributescalability} report on further details on the experimental results and the datasets used in the experiments. 
Appendix \ref{sec:illustrating_LFTJ}  details the original LFTJ algorithm ADOPT is based on while
Appendix \ref{sec:lftjinadopt} explains the LFTJ variant used for ADOPT.
