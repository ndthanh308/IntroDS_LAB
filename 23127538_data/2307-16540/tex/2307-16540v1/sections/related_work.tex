\section{Related Work}
\label{sec:related}

%ADOPT relates to prior work on worst-case optimal join algorithms, as well as prior work in query optimization.

The choice of an attribute order, for worst-case optimal join algorithms, resembles the problem of join order selection for traditional join algorithms~\cite{Selinger1979}. Both tuning decisions have significant impact on processing performance. At the same time, it is hard to find good attribute orders before query processing starts, mainly due to challenges in estimating execution cost for specific orders (e.g., due to challenges in estimating sizes of intermediate results). The latter problem has been well documented for traditional query optimizers~\cite{Gubichev2015, Lohman2014}. Our experiments demonstrate that it appears in the context of worst-case optimal join algorithms as well.

Adaptive processing~\cite{Avnur2000, Deshpande2004, Quanzhong2007b, Tzoumas2008, Wei2022, Zhu2017} has been proposed as a \nop{possible} remedy to this problem, allowing the engine to switch to a different join order during query execution based on run time feedback. While early work has focused on stream data processing~\cite{Avnur2000, Deshpande2004, Quanzhong2007b, Tzoumas2008} (where query execution times are assumed to be longer), adaptive processing has recently also gained traction for classical query processing~\cite{Menon2020, trummer2019skinnerdb}. 
SkinnerDB~\cite{trummer2019skinnerdb} is the closest in spirit to ADOPT: \revision{both use reinforcement learning and adaptive processing. However, ADOPT uses an anytime version of a worst-case optimal join algorithm, whereas SkinnerDB's join algorithm is not optimal. The learning problems (i.e., actions and states of the corresponding MDPs) differ between the systems as ADOPT optimizes attribute orders whereas SkinnerDB orders tables. Most importantly: ADOPT introduces a novel data structure, characterizing precisely the cubes in the space of attribute value combinations that have not been processed yet, along with operators for updating it after each episode. This data structure avoids redundant work across episodes and attribute orders as well as across threads. This property is crucial to be able to maintain optimality guarantees for equi-joins when switching between attribute orders. Instead, SkinnerDB uses a tree-based data structure that reduces but does not completely avoid redundant work across join orders that are dissimilar. As the amount of redundant work is hard to bound, it is difficult to maintain worst-case optimality guarantees with such mechanisms.}

%they both use the reinforcement learning algorithm UCT~\cite{Kocsis2006}. However, unlike SkinnerDB, ADOPT achieves worst-case optimal processing for equi-join queries, using a combination of specialized join algorithms, a novel data structure \revision{for storing sets of unprocessed cubes with operators for removing and adding cubes}, and specialized learning methods. %, unlike SkinnerDB, which motivates new data structures, parallelization methods, and  creating speADOPT puts forward a novel data structure for measuring progress that is significantly better than the prefix sharing approach used in SkinnerDB, since it avoids redundant work across time slices (Section~\ref{sub:variants}). Catering to a  worst-case optimal join algorithm further implies specific challenges, such as its specific parallelization and management of  sorting orders and indices of the tables for different attribute orders.

%There is theoretical evidence that using different execution strategies for different parts of the data can lead to asymptotically lower complexity than using only one execution strategy for the entire input data~\cite{DBLP:journals/jacm/Marx13}: The logical data partitioning put forward in this line of theoretical work is carefully crafted to observe the degree constraints in the data (e.g., whether the number of tuples with a certain value for an attribute exceeds a threshold). To achieve asymptotically low complexity, data partitioning and execution strategies are interleaved. PANDA~\cite{DBLP:conf/pods/Khamis0S17} is the first algorithm to employ  this theoretical approach, yet it is not  implemented and its runtime depends on large poly-logarithmic factors in the data size. In contrast, ADOPT picks execution strategies (as variable orders) at random and logically partitions the data without observing degrees. This is simple and effective, as exploiting data degree constraints and synthesizing execution strategies tailored at them may take non-trivial time.

Our work uses reinforcement learning to select attribute orders. It  relates to recent works that employ learning for database tuning in general~\cite{udodemo, bait, Hilprecht2019a, Li2018, Trummer2022, VanAken2021, Wang2021} and, specifically, for query optimization~\cite{Krishnan2018, Marcus, Marcus2018a, Yu2020b}. Our work differs as it focuses on learning and specialized data structures for worst-case optimal join algorithms.

Prior work on query optimization for worst-case optimal joins investigates "model-free" information-theoretic cardinality estimation. A seminal work, which enabled reasoning about worst-case optimal join computation, established tight bounds on the worst-case size of join results~\cite{DBLP:journals/siamcomp/AtseriasGM13}, the so-called AGM bound that is defined as the cost of the optimal solution of a linear program derived from the joins and the sizes of the input tables. This is further refined in the presence of functional dependencies~\cite{DBLP:journals/jacm/GottlobLVV12} and for succinct factorized representations of query results~\cite{DBLP:journals/tods/OlteanuZ15}. The latest development 
extends this line of work with data degree constraints and histograms~\cite{DBLP:conf/icdt/000122}. Classical approaches to query optimization based on heuristics~\cite{Freitag2020} and data statistics~\cite{Aberger2016, Aref2015} have also been considered.
To the best of our knowledge, ADOPT is the first adaptive approach for optimization in the context of worst-case optimal join algorithms. Our approach is free from cost-based heuristics.