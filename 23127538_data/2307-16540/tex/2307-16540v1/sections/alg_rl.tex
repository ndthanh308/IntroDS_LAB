\subsection{Learning Attribute Orders}
\label{sub:rl}

ADOPT uses reinforcement learning to learn near-optimal attribute orders, over the course of a single query execution. At the beginning of each time slice, ADOPT selects an attribute order that maximizes the tradeoff between exploration and exploitation. It uses the Upper Confidence Bounds on Trees (UCT) algorithm~\cite{Kocsis2006} to choose an attribute order. This requires mapping the scenario (of attribute order selection) into a Markov-Decision Problem. Next, we discuss the algorithm as well as the problem model.

An episodic Markov Decision Process (MDP) is generally defined by a tuple $\langle s_0,S,A,T,R\rangle$ where $S$ is a set of states, $s_0\in S$ the initial state in each episode, $A$ a set of actions, and $T:S\times A\rightarrow S$ a transition function, linking states and action pairs to target states. Component $R$ represents a reward function, assigning states to a reward value. In our scenario, the transition function is deterministic while the reward function is probabilistic (i.e., states are associated with a probability distribution over possible rewards, rather than a constant reward that is achieved, every time the state is visited). The transition function is partial, meaning that certain actions are not available in certain states. Implicitly, we assume that all states without available actions are end states of an episode. After reaching and end state, the current episode ends and the next episode starts (from the initial state $s_0$ again). Given an MDP, the goal in reinforcement learning~\cite{Sutton2018} is to find a policy, describing behavior that results in maximal (expected) reward. In order to leverage reinforcement learning algorithms for our scenario, we must therefore map attribute order selection into the MDP formalism.

Our goal is to learn a policy that describes an attribute order. The policy generally recommends actions to take in a specific state. Here, we introduce one action for each query attribute. States are associated with attribute order prefixes (i.e., each state represents an order for a subset of attributes). To simplify the notation, we will refer to states by the prefix they represent, to actions by the attribute they correspond to. The transition function connects a first state $s_1$ to a second state $s_2$ via action $a$, if the second state can be reached by appending the attribute, represented by the action, to the prefix represented by the first state. More precisely, using the notation introduced before, the transition function links the state-action pair $\langle s_1,a\rangle$ to state $s_2=s_1\circ a$ (where $\circ$ represents concatenation). Each state represents a prefix of an attribute order in which each attribute appears at most once. Hence, the actions available in a state correspond to attributes that do not appear in the prefix represented by the state. This means that all states representing a complete attribute order are end states, implicitly. As a further restriction, we do not allow actions representing attributes that do not connect to any attributes in the prefix represented by the current state. This is similar to the heuristic of avoiding Cartesian product joins, used almost uniformly in traditional query optimizers. The reward function is set to zero for all states, except for end states. States of the latter category represent complete attribute orders. Upon reaching such a state, ADOPT executes the corresponding attribute order for a limited number of steps, measuring execution progress. The process by which execution process is measured is described in the following subsections. 


ADOPT applies the UCT algorithm to solve the resulting MDP. As the MDP represents the problem of attribute ordering, linking rewards to execution progress, solving the MDP (i.e., finding a policy with maximal expected reward) yields a near-optimal attribute order. The UCT algorithm represents the state space as a search tree. Nodes represent states while tree edges represent transitions. Tree nodes are associated with statistics, establishing confidence bounds on the average reward associated with the sub-tree rooted at that node. Confidence bounds are updated as new reward samples become available. In each episode, the UCT algorithm selects a path from the search tree root to one of the leaf nodes. At each step, the UCT algorithm selects the child node with maximal upper confidence bound (hence the name of the algorithm). This approach converges to optimal policies~\cite{Kocsis2006}. After selecting a path to a leaf and calculating the associated reward, the UCT algorithm updates confidence bounds for each node on that path.

\revision{ADOPT grows the UCT search tree gradually over the course of query execution. At the start of execution, the tree only contains the root node. Then, in each episode, the tree is expanded by at most one node. Which nodes are added depends on the selected attribute orders. Each attribute order corresponds to a sequence of states in the MDP (a state represents an attribute order, each state appending one attribute, compared to its predecessor). In the fully grown search tree, each state is associated with one node. If, for the currently selected attribute order, some of the states do not have associated nodes in the tree yet, ADOPT expands the tree by adding a node for the first such state. ADOPT uses the partial tree to select attribute orders as follows. Given a state for which all possible successor states have associated nodes in the tree (i.e., reward statistics are available), ADOPT uses the aforementioned principle and selects the attribute that maximizes the upper confidence bound on reward values. If some of the successor states do not have associated nodes yet, ADOPT transitions to a randomly selected state among them (which will create a corresponding node). As a special case, if no nodes are available for any of the successor states, ADOPT selects the next attribute with uniform random distribution.}

%In our scenario, it is crucial to avoid generating the entire search tree at once, as this may cause non-negligible overheads. The number of possible attribute orders for a given query can be very large. Instead, we use a UCT variant that builds the search tree gradually. Specifically, starting from a tree containing only the root node, the algorithm expands the search tree by at most one node per sample. If the algorithm reaches an MDP state that has no associated node in the search tree, it performs random transitions until reaching an end state. In our scenario, this means that the remaining attributes are selected in random order. The resulting reward value corresponds to a uniform random sample for rewards in the corresponding sub-tree. If the algorithm encounters states with no associated tree nodes, it creates the first ``missing'' nodes and adds it to the tree. As time progresses, the tree becomes most refined along paths associated with interesting attribute orders.

\tikzstyle{uctnode}=[draw, circle, shade, top color=gray!10, bottom color=gray!20, blur shadow={shadow blur steps=5}, minimum width=1cm]
\tikzstyle{episode}=[color=red]

% Figure environment removed

\begin{example}
\label{ex:expansion}
\rm
\revision{Given a query with three attributes (A, B, and C), assume that ADOPT selects the following attribute orders in the first episodes (some orders are selected in multiple episodes): ABC, BCA, CBA, ABC, ACB, CBA, CAB, CBA. Figure~\ref{fig:uct} shows the UCT search tree after those episodes. Nodes represent partial attribute orders and edges represent the addition of one attribute. Next to each node, in red, the figure shows the number of the episode in which the node was added. Initially (episode zero), the tree contains only the root node. In the first episode, ADOPT selects order ABC, adding a node for the first prefix (A) without corresponding node in the tree. Later, in episode four, ADOPT selects order ABC and, again, adds a node for the first prefix (AB) for which no node has been created. Once nodes are added, ADOPT starts collecting reward statistics for all attribute orders extending the corresponding prefix. These statistics are used to select attribute orders in future episodes.}
\end{example}

% \begin{algorithm}[t!]
% \caption{Reward Function.\label{alg:reward}}
% \renewcommand{\algorithmiccomment}[1]{// #1}
% \begin{small}
% \begin{algorithmic}[1]
% \State \textbf{Input:} Selected hypercube $c$, processed hypercube $p$
% \State \textbf{Output:} Reward $r$
% \Function{Reward}{$c, p$}
% % \State $c_i \gets c.u_i - c.l_i, \forall i = 1 \to n$
% % \State $\delta_i \gets p.u_i - p.l_i, \forall i = 1 \to n$
% \For{$i \gets 1$ to $m$}
%     \State $c_i \gets c.u_i - c.l_i$
%     \State $\delta_i \gets p.u_i - p.l_i$
% \EndFor
% \State $r \gets \sum_{1\leq i \leq m} (\delta_i / \Pi_{1 \leq k \leq m} (c_k) ) \frac{c.volume}{join\_space.volume}$
% % \State \Comment{Scale reward with the relative hypercube volume ratio}
% % \State $r \gets \frac{c.volume}{join\_space.volume}$ 
% \State \Return $r$
% \EndFunction
% \end{algorithmic}
% \end{small}
% \end{algorithm}

