
\subsection{Main Function}
\label{sub:main}

\begin{algorithm}[t!]
\caption{Main function of ADOPT, processing queries.\label{alg:main}}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{small}
\begin{algorithmic}[1]
\State \textbf{Input:} Query $q$, number of threads $n$, per-episode budget $b$
\State \textbf{Output:} Query result
\Function{ADOPT}{$q, n, b$}
\State \Comment{Filter input tables via unary predicates}
\State $\{R_1,\ldots,R_m\}\gets$\Call{Prep.UnaryFilter}{$q$}
\State \Comment{Initialize join result set}
\State $R\gets\emptyset$
\State \Comment{Initialize reinforcement learning}
\State \Call{RL.Init}{$q$}
\State \Comment{Initialize constraint store}
\State \Call{TM.Init}{$q,n$}
\State \Comment{Iterate until result is complete}
\While {$\lnot$ \Call{TM.Finished}{}}
\State \Comment{Select attribute order via UCT algorithm}
\State $o \gets$ \Call{RL.Select}{}
\State \Comment{Use order for limited join steps}
\State $reward\gets$\Call{AnytimeWCOJ}{$q,o,n,b,R$}
\State \Comment{Update UCT statistics with reward}
\State \Call{RL.Update}{$o,reward$}
\EndWhile
\State \Comment{\revision{Return result after post-processing}}
\State \Return{\revision{\textproc{Post}$(q,R)$}}
\EndFunction
\end{algorithmic}
\end{small}
\end{algorithm}

ADOPT uses Algorithm~\ref{alg:main} to process \revision{simple SPJAG queries (i.e., without sub-queries). In addition to the query,} the algorithm also takes as input a number of data processing threads and a number of computational steps spent to evaluate a selected attribute order.

First, ADOPT filters the tables with the unary predicates \revision{(Line~5)}. \revision{ADOPT supports hash indexes on single columns and uses them, if available, to retrieve rows satisfying unary equality predicates. Without indexes, it scans and filters data, exploiting multi-threading. After that,} the only remaining predicates are then join predicates \revision{(including equality and other join predicates)}. Next, the algorithm initializes the set of join result tuples, the reinforcement learning algorithm by specifying the search space of attribute orders (which depends on the query), and the task manager with the input query and the number of processing threads \revision{(Lines~6 to 11)}. 
Internally, the task manager initializes the hypercube representing the total amount of work for each thread. More precisely, it divides the cube, representing the Cartesian product of all join attribute ranges, into equal shares for each thread. 

The task manager keeps track of cubes processed by the worker threads. Hence, query processing finishes once all processed cubes, in aggregate, cover the full input space. Iterations continue \revision{(Lines~13 to 20)} until that termination condition is satisfied. In each iteration, ADOPT first selects an attribute order via reinforcement learning \revision{(Line~15)}. Then, it executes that order, in parallel, for a fixed number of steps \revision{(Line~17)}. By executing the attribute order, \revision{the result set ($R$) may get updated. Note that $R$ only contains complete result tuples (mapping each attribute to a value) or partial values for aggregates. However, it does not contain any intermediate result tuples. Besides updating results}, executing an attribute order yields reward values, representing execution progress per time unit. Those reward values are used to update statistics \revision{(Line~19)}, maintained internally by the reinforcement learning optimizer, to guide attribute order selections in future iterations. Once \revision{the join finishes}, the algorithm \revision{performs post-processing (e.g., calculating per-group aggregates for group-by queries, based on join results in $R$) and returns the result (Line~22).}

%result tuples may get added into the result set ($R$). \revision{Note that only complete result tuples (i.e., combinations of values for all attributes) are added to that set but no intermediate result tuples.} 