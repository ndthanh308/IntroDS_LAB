\appendix
\section{Statistics about Graph Datasets}
\label{app:statistics_graph_datasets}
Table~\ref{tab:graphtable} describes the graph datasets used in our experiments. 
%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\centering
\caption{Graphs used in the experiments.}
\begin{tabular}{lrr}
\toprule[1pt]
\textbf{Graph} & \textbf{\#Vertices} & \textbf{\#Edges} \\
\midrule[1pt]
ego-Facebook & 4,039 & 88,234 \\ 
ego-Twitter & 81,306 & 2,420,766 \\  
soc-Pokec & 1,632,803 & 30,622,564  \\
soc-LiveJournal1 & 4,847,571 & 689,937,732 \\
\bottomrule[1pt]
\end{tabular}
\label{tab:graphtable}
%\vspace*{-1em}
\end{table}
%%%%%%%%%%%%%%%%%%%%

\section{Relative performance comparison of ADOPT over System-X}
\label{app:comparison_system_X}
\pgfplotstableread[col sep=comma,]{data/clique_relative.csv}\cliquerelative
\pgfplotstableread[col sep=comma,]{data/cycle_relative.csv}\cyclerelative

% \captionsetup[figure]{font=small}

% Figure environment removed

Figure~\ref{fig:relative} examines the relative performance of ADOPT over System-X\nop{, the two systems that implement LFTJ}. The speedup of ADOPT over System-X increases with the query parameter $n$; System-X times out for large $n$. This speedup reaches: 4x for both 5- and 6-clique on ego-Twitter and soc-Pokec; 8x for 5- and 6-clique on soc-Livejournal1; 2x for 5-cycle on ego-Twitter and 4x on both soc-Pokec and soc-Liverjournal1. A reason for this speedup increase is the difficulty of System-X's optimizer to pick a good attribute order for increasingly larger queries. The average performance of the attribute orders of ADOPT is better than the one attribute order of System-X.

%\color{red}
\section{Disk and Memory Consumption}
\label{sec:diskmemory}

%%%%%%%%%%%%%%%%%%%% disk space %%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Disk space of different systems on each benchmark.}
% \vspace{-1em}
\centering
\begin{tabular}{l|rrrrrrr}
\toprule[1pt] Systems & JOB & ego-Facebook & ego-Twitter & soc-Pokec & soc-Livejournal1 & TPC-H & JCC-H \\
\midrule[1pt]
ADOPT       & 3.2G & 836K & 48M  & 405M & 1.1G & 8.5G & 8.5G \\
System-X    & 2.8G & 1.6M & 46M  & 387M & 759M & -    & -    \\
EmptyHeaded & -    & 6.0M & 44M  & 496M & 1.3G & -    & -    \\
PostgreSQL  & 5.4G & 11M  & 101M & 1.1G & 2.3G & 20GB & 20GB \\
MonetDB     & 2.9G & 3.5M & 58M  & 247M & 554M & 8.5G & 8.5G \\
SkinnerDB  & 3.1G & 834K & 47M  & 403M & 882M & 8.4G & 8.4G \\
\bottomrule[1pt]
\end{tabular}
\label{tab:disk}
\end{table*}

%%%%%%%%%%%%%%%%%%%% memory space %%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Memory space of different systems on each benchmark.}
% \vspace{-1em}
\centering
\begin{tabular}{l|rrrrrrr}
\toprule[1pt] Systems & JOB & ego-Facebook & ego-Twitter & soc-Pokec & soc-Livejournal1 & TPC-H & JCC-H \\
\midrule[1pt]
ADOPT       & 22G & 10G	 & 17G	& 22G  & 45G  & 52G  & 57G  \\
% ADOPT Sort Arrays  & 2G  & 689K & 19M  & 233M & 5G   & 2G   & 2G   \\
% ADOPT Unprocessed Cubes  & 37K & 445K & 878K & 3M   & 11M  & 104K & 105K \\
% ADOPT Uct Nodes   & 21M & 4M   & 4M   & 4M   & 4M   & 109K & 110K \\ 
System-X    & 38G & 16G	 & 16G	& 16G  & 26G  & -    & -    \\
EmptyHeaded & -   & 68G	 & 74G	& 85G  & 89G  & -    & -    \\
PostgreSQL  & 28G & 15G	 & 17G	& 25G  & 110G & 56G  & 110G \\
MonetDB     & 42G & 122G & 243G	& 345G & 345G & 18G  & 280G \\
SkinnerDB   & 38G & 26G  & 69G  & 89G  & 125G  & 86G  & 86G \\ 
\bottomrule[1pt]
\end{tabular}
\label{tab:memory}
\end{table*}

\begin{table}[b]
\caption{Breakdown of ADOPT's main memory consumption for each benchmark: total memory consumption, memory for storing sort orders, unprocessed cubes, and the UCT search tree.}
% \vspace{-1em}
\centering
\begin{tabular}{l|rrrr}
\toprule[1pt] Benchmark & Total & Sort & Cubes & UCT \\
\midrule[1pt]
JOB & 22G & 2G & 37K & 21M \\
ego-Facebook & 10G & 689K & 445K & 4M \\
ego-Twitter & 17G & 19M & 878K & 4M \\
soc-Pokec & 22G & 233M & 3M & 4M \\
soc-Livejournal1 & 45G & 5G & 11M & 4M \\
TPC-H & 52G & 2G & 104K & 109K \\
JCC-H & 57G & 2G & 105K & 110K \\
\bottomrule[1pt]
\end{tabular}
\label{tab:memorybreakdown}
\end{table}

We report the disk space and memory usage of different systems in Table~\ref{tab:disk} and ~\ref{tab:memory} respectively. We use the command \texttt{du -sh} to measure disk space of the respective data folder. For measuring maximal memory consumption for each benchmark, we use the \texttt{ps -p pid -o rss=} command. ADOPT is implemented in Java. Hence, we increased the default settings for the Xmx and Xms parameters of the Java virtual machine as follows: \texttt{-Xmx15G -Xms15G} on ego-Facebook, \texttt{-Xmx20G -Xms20G} on ego-Twitter, \texttt{-Xmx25G -Xms25G} on soc-Pokec, \texttt{-Xmx50G -Xms50G} on soc-Livejournal1, and \texttt{-Xmx80G -Xms80G} on JOB, TPC-H, and JCC-H. For all other systems, to maximize their performance in terms of run time (results reported in Table~\ref{tab:overall}), we increased buffer space to 350~GB, the amount of main memory available on our test machine (e.g., for Postgres, we increased the setting for the shared\_buffer\_pool parameter). Note that all systems typically exploit only a small part of the total buffer space available to them. As discussed in Section~\ref{sec:experiments} in more detail, some systems were only evaluated on a subset of benchmarks (missing values are marked by ``-'' in the tables).

% For ADOPT, we use \texttt{-Xmx15G -Xms15G} on ego-Facebook, \texttt{-Xmx20G -Xms20G} on ego-Twitter, \texttt{-Xmx25G -Xms25G} on soc-Pokec, \texttt{-Xmx50G -Xms50G} on soc-Livejournal1, \texttt{-Xmx80G -Xms80G} on JOB, TPC-H and JCC-H. 

Table~\ref{tab:memory} shows that ADOPT consumes an amount of main memory that is approximately comparable to System~X, i.e., ADOPT's main memory consumption is within a factor of 0.6 to 1.7 of the corresponding value for System~X for all evaluated benchmarks. This seems reasonable as the execution engine of System~X is the most similar to the one of ADOPT (due to the use of LFTJ variants). EmptyHeaded, another worst-case optimal system, consumes more main memory than ADOPT. MonetDB consumes significantly more main memory than ADOPT on the graph benchmarks. Here, using non worst-case optimal joins, MonetDB produces large intermediate results for cyclic queries that are stored in main memory. The use of worst-case optimal join algorithms avoids these overheads. Also, MonetDB consumes more main memory on benchmarks that use skewed data (in particular JCC-H). Here, large intermediate results can be avoided by using the right join order. However, due to data skew, reliably identifying near-optimal query plans without adaptive processing is hard. On the other hand, MonetDB consumes only moderate amounts of main memory on TPC-H. Here, worst-case optimal join algorithms are unnecessary to avoid large intermediate results and query planning is easier (due to uniform data distributions). SkinnerDB incurs high memory overhead on some of the graph benchmarks due to cyclic queries. Postgres incurs high memory overheads for some of the graph benchmarks and for the JCC-H benchmark, due to binary joins and non-adaptive optimization. 

ADOPT stores several data structures in main memory that are specific to its adaptive approach: different sort orders for each table to support the LFTJ (each sort order is stored as one integer array with row indexes, of the same length as the table), the set of unprocessed cubes, maintained by the task manager (and represented as variable $U$ in Algorithm~\ref{alg:cubes}), and data structures used by the reinforcement learning algorithm, in particular the UCT search tree with associated reward statistics. Table~\ref{tab:memorybreakdown} shows the amount of memory consumed by each of these data structures for each benchmark (in addition to the total main memory consumption). For most benchmarks, sort orders consume most main memory, among all auxiliary data structures, reaching up to 11\% of total memory consumption for one benchmark. Main memory consumption for storing cubes and UCT statistics is lower by several orders of magnitude, making their contribution to total main memory consumption negligible.

% Table~\ref{tab:memorybreakdown} shows a breakdown of main 

% Furthermore, Table~\ref{tab:memorybreakdown} also presents the memory breakdown of ADOPT. Our memory analysis encompasses three key data structures, sorted arrays, unprocessed cubes, and UCT nodes. The memory of the sorted array is determined by calculating the total number of elements across all arrays multiplied by 4 bytes. For the unprocessed cubes, the memory is computed by multiplying the maximum number of unprocessed cubes by the dimension of the cubes, and then by 2 (since we have lower bound and upper bound) and 4 bytes. Similarly, the memory of the UCT nodes is calculated by multiplying the maximum number of UCT nodes by the size of a single UCT node and 4 bytes. It is evident that sorting arrays requires more memory compared to the other two data structures. However, sorting arrays still does not account for the majority of the overall memory cost.

% We measure the memory of sorted array by calculating \texttt{\#total elements(in all arrays) * 4 Byte}, the memory of unprocessed cubes by computing \texttt{\#maximal unprocessed cubes * dimension of cubes * 2 * 4 Byte}, and the memory of UCT by computing \texttt{\#maximal UCT nodes * size of one UCT node * 4 Byte},

% However, many of them still cannot finish some large clique/cycle queries in six hours, see Figure~\ref{fig:clique_cycle_results}.

% From Table ~\ref{tab:memory}, database systems which use standard pair-wise joins (e.g., MonetDB) have significantly higher memory consumption compared to LFTJ based systems (e.g., ADOPT, System-X) on graph queries due to the materialization of large intermediate results. 

% To ensure that PostgreSQL can utilize all available memory, we have configured the shared\_buffer\_pool to 350G. However, PostgreSQL does not utilize all available memory and is unable to complete the large clique/cycle query within the six-hour timeout.

\section{Index Creation Time}
\label{sec:indexcreation}

%%%%%%%%%%%%%%%%%%%% index time %%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Index creation time (in seconds) of different systems for each benchmark it was evaluated on (- if the corresponding system was not evaluated on the benchmark).}
% \vspace{-1em}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|rrrrrrr}
\toprule[1pt] Systems & JOB & ego-Facebook & ego-Twitter & soc-Pokec & soc-Livejournal1 & TPC-H & JCC-H \\
\midrule[1pt]
ADOPT             & 63 & 0.2  & 1.6 & 28  & 64  & 155 & 153  \\
% ADOPT Sort Index  & 42 & 0.2  & 1.6 & 28  & 64  & 97 & 94  \\
% ADOPT Hash Index  & 21 & 0    & 0   & 0   & 0   & 58 & 59  \\
EmptyHeaded & > 432000  & 17  & 24  & 38  & 115 & -   & -   \\
PostgreSQL  & 78 & 0.36 & 14  & 144 & 328 & 169 & 172 \\
SkinnerDB   & 23 & 0.4  & 1.1 & 19  & 25  & 82  & 84 \\
\bottomrule[1pt]
\end{tabular}
}
\label{tab:index}
\end{table*}

\begin{table}[b]
\caption{Breakdown of index generation overheads for ADOPT: total indexing time, time for generating sort orders, and time for generating hash indexes.}
% \vspace{-1em}
\centering
\begin{tabular}{l|rrr}
\toprule[1pt] Benchmark & Total & Sorting & Hashing \\
\midrule[1pt]
JOB & 63 & 42 & 21 \\
ego-Facebook & 0.2 & 0.2 & 0 \\
ego-Twitter & 1.6 & 1.6 & 0 \\
soc-Pokec & 28 & 28 & 0 \\
soc-Livejournal1 & 64 & 64 & 0 \\
TPC-H & 155 & 97 & 58 \\
JCC-H & 153 & 94 & 59 \\
\bottomrule[1pt]
\end{tabular}
\label{tab:indexbreakdown}
\end{table}

%Table~\ref{tab:index} reports index creation overheads

We report on index creation overheads, referring to the indexes created before the experiments discussed in Section~\ref{sub:baselines}. For Postgres, we created indexes on all primary and foreign key columns. In addition, for TPC-H and JCC-H, we additionally index the o\_orderdate column of the Orders table and build an index with composite search key on the l\_shipdate, l\_discount, and l\_quantity columns of the Lineitem table (the additional indexes improved performance). For SkinnerDB, we run the ``index all'' command, indexing all suitable columns and thereby optimizing its query evaluation times. For ADOPT, we index the same columns as SkinnerDB via hash indexes (supporting evaluation of unary equality predicates), except for join columns (which SkinnerDB indexes via hash indexes as well). Additionally, ADOPT creates data structures representing different sort orders of base tables (see Appendix~\ref{sec:lftjinadopt} for details). System~X and MonetDB create indexes automatically, based on observed queries (note that MonetDB supports the ``create index'' command but it is only treated as a suggestion, according to the online manual\footnote{\url{https://www.monetdb.org/documentation-Sep2022/user-guide/sql-summary/\#create-index}}). To give those two systems the opportunity to create suitable indexes, we ran each benchmark once before starting the actual measurements. As those systems interleave query execution and index creations, making it hard to measure index creation time separately, we do not report indexing overheads for those systems. EmptyHeaded creates all relevant indexes in a pre-processing step. 

%For PostgreSQL, we set the \texttt{\textbackslash timing} flag to measure the index creation time. The index creation time for EmptyHeaded is automatically logged and displayed in the console. As for ADOPT and SkinnerDB, the index creation is measured by calling \texttt{System.currentTimeMillis()} before and after building indices and computing the time difference. 


Table~\ref{tab:index} reports corresponding results. ADOPT has typically higher index creation overheads than SkinnerDB due to the added overhead for creating sort orders. On the other hand, ADOPT's index creation overheads are below the ones of Postgres. EmptyHeaded incurs higher index creation overheads, compared to ADOPT, as it creates indexes for all permutations of table columns. On JOB, this approach incurs very high index generation overheads, making the approach impractical. Table~\ref{tab:indexbreakdown} shows a breakdown of ADOPT's index generation overheads into two components: time for generating indexes representing row orders (to support LFTJ) and time for generating hash indexes on single columns (to support evaluation of unary equality predicates). Clearly, time for generating row orders dominates, even though hashing time is non-negligible for the TPC-H and JCC-H benchmarks. 

%On JOB, EmptyHeaded constructs tries for each column permutation, including string columns, of every table. As a result, it is unable to complete within a reasonable time. We do not report measurements for MonetDB and SystemX, as both interleave query evaluation with index creations, determining indexes to create automatically\footnote{\url{https://www.monetdb.org/documentation-Sep2022/user-guide/sql-summary/\#create-index}}. This makes it hard to separate execution time from index creation time.
% MonetDB and System-X automatically build indices and materialize caches, making precise measurement challenging. 


%Queries usually exhibit slower performance in their initial runs and significantly improve in subsequent runs. The performance that we report in Table~\ref{tab:overall} and Figure~\ref{fig:clique_cycle_results} is their stable performance after running queries multiple times. Based on the results in Table~\ref{tab:index}, ADOPT demonstrates reasonable time consumption for index creation compared to all other baselines.

%Moreover, we breakdown the index creation time in ADOPT to the creation time of the hash index and the creation time of the sort index. In graph benchmarks, we only build sort index, while in JOB, TPC-H and JCC-H benchmarks, we create both the sort index and the hash index (to support complex unary predicates or aggregation function). From Table~\ref{tab:index},  it is evident that the time required for sorting the indices surpasses the time taken for creating the hash index.

 
% From table~\ref{tab:index}, ADOPT spends reasonable time on indices creation compared with all baselines.


\input{sections/ablation}

\input{sections/attribute}

%\newpage
\input{sections/example}

