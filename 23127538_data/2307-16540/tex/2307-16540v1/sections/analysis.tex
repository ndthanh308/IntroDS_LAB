\section{Analysis}
\label{sec:analysis}

% In this section, we prove three deseirable properties of ADOPT. After proving correctness (Section~\ref{sub:correct}), we show that ADOPT maintains worst-case optimality guarantees (Section~\ref{sub:wcoproof}), then we prove that ADOPT converges to optimal attribute orders (Section~\ref{sub:convergence}).

In Section~\ref{sub:convergence}, we prove that ADOPT converges to optimal attribute orders. Two further properties, correctness and worst-case optimality, are analyzed in Sections 
\ref{sub:correct} and respectively \ref{sub:wcoproof}. 


 \subsection{Convergence to Optimal Orders}
 \label{sub:convergence}
First, we show that ADOPT must finish processing once the accumulated rewards reach a precise threshold.
% Different reward functions can be used. For instance, given a variable order, denote by $l$ and $u$ lower and upper bound on the value domain of the first variable. The LFTJ algorithm iterates over variable values in ascending order. Let $d=e-s$ the distance between the value for the first variable (e.g., the maximal value over all iterators) at the end ($e$) and at the start ($s$) of a time slice. Let $r=d/(u-b)$ be the relative distance covered (relative to the entire value range). For the following lemma, we assume that relative distance is used as reward function. 

\begin{theorem}\label{th:threshold}
Join processing finishes once the sum of accumulated rewards over all threads and episodes reaches one.
\end{theorem}
\begin{proof}
Reward is proportional to the volume of the cube covered, scaled to the size of the full cube. Hence, accumulating a reward sum of one means that a volume equal to the full cube has been processed. Furthermore, ADOPT avoids covering overlapping cubes by different threads and in different episodes (independently of the attribute order). Hence, once the accumulated reward reaches one, processed cubes must cover the full cube.
\end{proof}

This implies that the reward function is a good measure of attribute order quality indeed.

\begin{theorem}
The attribute order with the highest average reward per episode minimizes the number of computational steps.\label{th:rewardgood}
\end{theorem}
\begin{proof}
For any attribute order $o$, processing finishes once the accumulated rewards reach one (Theorem~\ref{th:threshold}). Therefore, the average reward  $r_o$ per episode for $o$ is inversely proportional to the number of episodes $e_o$ needed by $o$, i.e.\ $r_o=1/e_o$. Also, the number of computational steps per episode is constant. Therefore, minimizing the number of episodes needed maximizes the average reward.
\end{proof}

This implies convergence to optimal attribute orders.

\begin{corollary}
ADOPT converges to an optimal attribute order.
\end{corollary}
\begin{proof}
Following Theorem~\ref{th:rewardgood}, the order with the highest average reward is also the fastest one to process. Furthermore, the UCT algorithm used by ADOPT converges to a solution with maximal expected reward~\cite{Kocsis2006}. Hence, ADOPT converges to an attribute order that minimizes the number of processing steps.
\end{proof}


\subsection{Correctness}
\label{sub:correct}

We show that ADOPT generates a complete and correct result.

\begin{theorem}
ADOPT does not produce incorrect join results.\label{th:noincorrect}
\end{theorem}
\begin{proof}
ADOPT inserts join results in Line~5 of Algorithm~\ref{alg:joinOneCube}. For each attribute, Algorithm~\ref{alg:joinOneCube} only iterates over values that appear in all relations with that attribute (Line~10 in Algorithm~\ref{alg:joinOneCube}). Hence, join results must satisfy all equality join conditions. Furthermore, Algorithm~\ref{alg:joinOneCube} is only applied to input tuples satisfying all unary predicates (due to the filter in Line~5 in Algorithm~\ref{alg:main}). Hence, join results satisfy all applicable predicates and are correct.
\end{proof}

%Join results are incorrect if they ADOPT considers only combinations of tuples from the join tables that satisfy all unary predicates (due to the pre-processing stage). Hence, tuples combinations that do not satisfy join predicates are the only possibility to generate incorrect output. However, the anytime LFTJ algorithm only advances from one attribute to the next, if all selected tuples have the same value for the current attribute. In that case, the associated equality join condition is satisfied. Hence, result tuples are only generated if all join conditions are satisfied. Hence, the output is correct.

\begin{lemma}
All results contained within processed cubes, returned by Algorithm~\ref{alg:joinOneCube}, have been inserted into the result set.\label{lm:processedcorrect}
\end{lemma}
\begin{proof}
Assume there is a vector $r$ of join attribute values, matching all join predicates, that is contained in a processed cube $p$ but not in the join result. There is an attribute $a$ such that $r$ equals the last selected attribute values $v$ up to attribute $a$ (in attribute order), then takes a value below the last selected value for the next attribute. However, Algorithm~\ref{alg:joinOneCube} does not advance from one value to the next for an attribute, before considering all value combinations for the remaining attributes. Hence, $r$ must have been added to the result, leading to a contradiction.
\end{proof}

\begin{lemma}
The task manager only removes processed cubes.\label{lm:removalcorrect}
\end{lemma}
\begin{proof}
In each invocation of  \textproc{TS.Remove}, the task manager removes only the target cube. Assume a vector $l$ of attribute values, within the target cube, is ``lost'', i.e.\ it is neither contained in any processed cube nor in any of the newly added, unprocessed cubes. Denote by $v$ the last selected values for all attributes in the join invocation, immediately preceding removal, and by $o$ the corresponding attribute order. Assume $l$ matches the values in $v$ for some prefix (possibly of size zero) of order $o$. Denote by $a$ the first attribute in $o$ for which $l$ does not match $v$. Denote by $p$ the processed cube added when reaching $a$ in the loop from Line~30 in Algorithm~\ref{alg:joinOneCube}. If $l_a<v_a$ then $l$ must be contained in $p$. However, if $l_a>v_a$, $l$ must be contained in the unprocessed cube added when reaching $p$ in the loop from Line~26 in Algorithm~\ref{alg:cubes}, leading to a contradiction.
\end{proof}

\begin{theorem}
ADOPT produces a complete join result.\label{th:complete}
\end{theorem}
\begin{proof}
The join phase terminates only once no unprocessed cubes are left. Result tuples contained in processed cubes are inserted into the result set (Lemma~\ref{lm:processedcorrect}) and no unprocessed cubes are erroneously removed (Lemma~\ref{lm:removalcorrect}). Hence, processing cannot terminate before all result tuples are inserted.
\end{proof}

%  more target cubes can be retrieved from the constraint store. This is only possible once the set of cubes, processed via joins, covers the cube representing the full Cartesian product among all joined tables. However, the Cartesian product of all joined tables forms a super set of any valid join result. Therefore, the join result must be complete after termination.

The next result follows immediately.

\begin{corollary}
ADOPT produces a correct join result.
\end{corollary}
\begin{proof}
ADOPT produces all tuples in the join result (Theorem~\ref{th:complete}) without generating any incorrect tuple (Theorem~\ref{th:noincorrect}).
\end{proof}

% \begin{theorem}
% WCOL produces the correct query result.
% \end{theorem}


\subsection{Worst-Case Optimality}
\label{sub:wcoproof}

We analyze whether ADOPT maintains worst-case optimality guarantees. We focus on join processing overheads, neglecting without loss of generality the preparation overheads. These overheads include the sorting of the relations to support LFTJ leapfrogging following the orders of attributes picked by ADOPT. Our analysis is based on the worst-case optimality properties of LFTJ and makes the same assumptions as the corresponding proof~\cite{DBLP:conf/icdt/Veldhuizen14} (e.g., restriction to equality joins). We consider the number of threads a constant. Additionally, our analysis makes the following assumption.

%Hence, it is valid under the same assumptions as the LFTJ analysis Also, we focus on queries that contain only equality joins but no other join predicates. 

\begin{assumption}\label{as:constantepisodes}
We assume that the number of episodes is bounded by a constant that does not depend on the data size.
\end{assumption}

% \begin{assumption}
% We assume that all management overheads due to adaptive processing, including time required for selecting target cubes and initializing execution state at the end of each time slice, are negligible compared to join processing overheads.
% \end{assumption}

% The latter assumption is reasonable for the following reason. Management overheads are incurred only when the attribute order changes. However, we can freely choose the number of steps between join order changes. If management overheads are non-negligible, we can simply increase the step count. 

The latter assumption can be ensured by increasing the number of steps per episode, proportional to the maximal output size. 

\begin{lemma}\label{lemma:adopt-cubes}
The number of cubes processed by ADOPT does not depend on the data size.
\end{lemma}
\begin{proof}
Initially, the number of unprocessed cubes is proportional to the number of threads (i.e., constant). Each invocation of Procedure~\textproc{CS.Remove} may create up to $m$ cubes where $m$ is the number of query attributes (i.e., a constant). Each episode may process multiple cubes, i.e.\ invoke that function multiple times. However, whenever the target cube was fully processed, no unprocessed cubes are added. There can be at most one target cube per episode and thread that was not fully processed. Hence, the number of unprocessed cubes added per episode is bounded by a constant. Due to Assumption~\ref{as:constantepisodes}, the number of generated (and processed) cubes is therefore bounded by a constant as well.
\end{proof}

\begin{lemma}\label{lemma:adopt-dominate}
The time complexity of ADOPT's join phase is dominated by the complexity of \textproc{JoinOneCube}.
\end{lemma}
\begin{proof}
The per-episode complexity of all operations of the reinforcement learning algorithm are bounded by the number of join attributes. Similarly, the number of operations required to retrieve or remove cubes is bounded by the number of join  attributes. Hence, the time complexity for join processing dominates.
\end{proof}

We are now ready to prove our main result.

\begin{theorem}
ADOPT is worst-case optimal.
\end{theorem}
\begin{proof}
The number of cubes processed by ADOPT does not depend on the input data size (Lemma~\ref{lemma:adopt-cubes}). Furthermore, time complexity for joins dominates (Lemma~\ref{lemma:adopt-dominate}). ADOPT uses the LFTJ algorithm to process cubes. This algorithm is worst-case optimal~\cite{DBLP:conf/icdt/Veldhuizen14}. The time for processing the largest cube, which occurs over the execution of a query, is upper-bounded by the time required by LFTJ for processing the entire query cube. The number of cubes is independent of the data size. Hence, total processing overheads are asymptotically equivalent to the time required by LFTJ, therefore worst-case optimal.
\end{proof}

%Note that increasing the number of episodes (by tuning the number of steps per episode) benefits convergence, while it leads potentially to larger overheads, compared to the non-adaptive LFTJ. We plan to investigate this tradeoff further in future work.%We optimize this tradeoff empirically in our experiments.

% \begin{lemma}
% Query processing time is minimal when using the variable order with maximum average reward.\label{lem:maxReward}
% \end{lemma}

% \begin{proof}
% For any variable order $o$, we terminate after covering the entire value domain for the first variable. This corresponds to an accumulated reward of one, according to the relative distance metric. Denote by $t_o$ the number of time slices required to process the entire query with $o$. The average reward per time slice is $1/t_o$ for order $o$. Hence, the order requiring the minimal number of time slices has maximal reward. Also, processing time is proportional to the number of time slices (as duration per slice is constant).
% \end{proof}

% Different reward functions could be used equivalently (e.g., the relative number of tuples covered in a specific table). To simplify the following analysis, we assume that there is exactly one optimal variable order. Also, we assume that the UCT algorithm~\cite{Kocsis2006} is used for learning.

% \begin{corollary}
% Algorithm~\ref{alg:learningLFTJ} converges to an optimal variable order.
% \end{corollary}

% \begin{proof}
% This is a direct consequence of Lemma~\ref{lem:maxReward} as the UCT algorithm converges to maximum reward choices~\cite{Kocsis2006}.
% \end{proof}

% Denote by $t_W^*$ the number of time slices, required to process the input query with the optimal variable order using the LFTJ. Denote by $m$ the number of joined tables, by $n$ the largest table cardinality, and by $k$ the maximal number of variables per table.

% \begin{theorem}
% As $n$ grows, the time complexity of Algorithm~\ref{alg:learningLFTJ} approaches $O(t_W^*+n\cdot\log(n)\cdot m\cdot k!)$.
% \end{theorem}

% \begin{proof}
% Algorithm~\ref{alg:learningLFTJ} converges to optimal join orders, given enough time (large $t^*_W$). Hence, execution time after pre-processing converges to $t_W^*$. Pre-processing requires creating tries on each input table to support all possible variable orders. We create at most $k!$ tries per table, the sorting cost per table is $O(n\cdot\log(n))$, and we treat $m$ tables.
% \end{proof}

% For comparison: the prior SkinnerDB version is in $O(t_S^*+m\cdot n\cdot k)$ time where $t_S^*$ is the number of time slices for the optimal join order with standard join algorithms (not worst-case optimal). Pre-processing is more expensive when creating tries to support different variable orders (as opposed to one hash index per join column). 

% Note: I think we can go down from $m!$ tries to $2^m$ at least. Gonna think more about that ...