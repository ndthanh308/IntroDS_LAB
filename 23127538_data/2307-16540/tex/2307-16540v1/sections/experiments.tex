
\section{Experimental Evaluation}
\label{sec:experiments}



% we vary the benchmark scenario and test the robustness of our system.
% compare ADOPT to multiple baselines (Section~\ref{sub:baselines}). In Section~\ref{sub:variants}, we compare the hypercube approaches proposed by us and prefix share approaches proposed by ~\cite{trummer2019skinnerdb}.

%ADOPT outperforms its competitors on all graph benchmarks by 2-30x. For the JOB benchmark, ADOPT is close (0.91x) to MonetDB and outperforms the others by 1.4-6.3x.


We confirm experimentally that ADOPT outperforms a range of competitors for both acyclic and cyclic queries from the join order benchmark~\cite{Gubichev2015}, \revision{standard decision support benchmarks (i.e., TPC-H and JCC-H ~\cite{boncz2018jcc}),} and graph data~\cite{snapnets,nguyen2015join} workloads. The robustness of ADOPT's query evaluation becomes more evident for queries with an increasingly larger number of joins and with filter conditions whose joint selectivity is hard to assess correctly at optimization time.
\nop{
 We confirm experimentally that our query engine ADOPT outperforms a variety of competitors in both runtime (Sec.~\ref{sub:baselines}) and robustness (Sec.~\ref{sub:robustness}) for graph data workloads involving cyclic queries on a number of publicly-available datasets that are typically used in the literature \cite{snapnets} for benchmarking graph queries~\cite{nguyen2015join}.
 }
The superior performance of ADOPT over its competitors is due to the interplay of its four key features: worst-case optimal join evaluation; reinforcement learning that eventually converges to near-optimal attribute orders (Sec.~\ref{sub:optimalattributeorder}); hypercube data decomposition (Sec.~\ref{sub:variants}); and domain parallelism (Sec.~\ref{sub:parallelization}). \revision{Appendices~\ref{app:comparison_system_X}-\ref{sec:attributescalability}  report further experiments on: the performance comparison of ADOPT and System-X, memory consumption, sorting and synchronization overhead, and scalability with the number of join attributes per table.}

\nop{
We introduce our experimental setups in Section~\ref{sub:expsetup} and benchmark ADOPT and multiples systems on various graph datasets in ~\ref{sub:baselines}. In Section~\ref{sub:variants}, we compare our hypercube approach and alternatives using the prefix share progress tracker and the offset progress tracker. In Section~\ref{sub:ablation}, we study the attribute order of ADOPT and break down the execution time. In Section~\ref{sub:parallelization}, we scale up ADOPT system and evaluate its speedup.
}

\subsection{Experimental Setup}
\label{sub:expsetup}

We benchmark the query engines on acyclic and cyclic queries.

\subsubsection*{Benchmark for acyclic queries} 
The join order benchmark (JOB)~\cite{Gubichev2015} consists of 113 queries over the highly-correlated IMDB real-world dataset. This benchmark shows an orders-of-magnitude performance gap between different join orders for the same query. \revision{TPC-H (JCC-H \cite{boncz2018jcc}) is a benchmark used for decision support, comprising of 22 queries that incorporate standard SQL predicates. In TPC-H, data is synthetically generated with uniform distribution, whereas in JCC-H, the data is highly skewed, which makes JCC-H a harder benchmark to optimize. In our experiments, we use TPC-H/JCC-H with scaling factor ten. We omit four queries in TPC-H (JCC-H) queries, Q2, Q13, Q15, and Q22, for lack of support for non-integer join columns, outer joins, views, and substring functions.}




\subsubsection*{Benchmark for cyclic queries}
We follow prior work on benchmarking worst-case optimal join algorithms against traditional join plans~\cite{nguyen2015join} and consider the evaluation of clique and cycle queries over the binary edge relations of four graph datasets from the SNAP network collection~\cite{snapnets}. The considered queries are as follows:


\begin{itemize}
    \item $n$-clique: Compute the cliques of  $n$ distinct vertices. Such a clique has an edge between any two of its vertices. For instance, the 3-clique is the triangle:
    
    $edge(a,b), edge(b,c), edge(a,c), a<b<c$
    
    \item $n$-cycle: Compute the cycles of  $n$ distinct nodes. Such a cycle has an edge between the $i$-th and the $(i+1)$-th vertices for $1\leq i< n$ and an edge between the first and the last vertices. For instance, the $4$-cycle query is: 
    
    $edge(a,b), edge(b,c), edge(c,d), edge(a,d), a<b<c<d$
    
    % \item $(m,n)$-lollipop: Count the number of distinct $m$-length paths conected to an $n$-clique, where the start nodes are sampled at random. The $(2,3)$-lollipop with the start nodes from the unary relation $v_1$ is:
    
    % $v_1(a), edge(a, b), edge(b, c), edge(c, d), edge(d, e), edge(c, e)$
    
    % \item $n$-barbell: Count the number of distinct subgraphs consisting of two non-overlapping $n$-vertex cliques connected by an edge. The  $3$-barbell query is: 
    
    % $edge(a,b), edge(b,c), edge(a,c), edge(a, d), edge(d,e),$ 
    
    % $edge(e,f), edge(d,f)$
\end{itemize}

The inequalities in the above queries enforce that each node in the clique/cycle is distinct. 
Instead of returning the list of all distinct cliques/cycles, all systems are instructed to return their count. ADOPT  counts the result tuples as they are computed.
The reason for returning the count is to avoid the time to list the result tuples and only report the time to compute them.

\subsubsection*{Systems}
ADOPT is implemented in JAVA (jdk 1.8). It uses 10,000 steps per episode and UCT exploration ratio 1E-6. The competitors are: the open-source engines MonetDB~\cite{boncz2008breaking} (Database Server Toolkit v11.39.7, Oct2020-SP1) and PostgreSQL 10.21~\cite{postgres} that employ traditional join plans; a commercial engine System-X (implemented in C++)\nop{LogicBlox (version 4.40.0)} that uses the worst-case optimal LFTJ  algorithm~\cite{DBLP:conf/icdt/Veldhuizen14}; the open-source engine EmptyHeaded that uses a worst-case optimal join algorithm~\cite{Aberger2016}; and SkinnerDB~\cite{trummer2019skinnerdb} (implemented in Java jdk 1.8) that uses reinforcement learning to learn an optimal join order for traditional query plans.




% 1.00	1.00	1.00	1.00	1.00
% 6.38	5.09	2.90	2.55	2.42
% 0.00	1.54	2.64	4.69	2.09
% 7.11	15.35	17.94	7.23	3.84
% 0.91	14.99	22.03	6.38	3.65
% 1.44	15.71	33.00	10.29	3.85

%PostgreSQL was allocated enough memory to evaluate its query plans in memory. 

\subsubsection*{Setup}
We run each experiment five times and report the average execution time. We used a server with 2 Intel Xeon Gold 5218 CPUs with 2.3 GHz (32 physical cores)/384GB RAM/512GB hard disk. ADOPT, EmptyHeaded, MonetDB, SkinnerDB, and System-X were set to run in memory. By default, all engines use 64 threads\nop{ (using hyperthreading); for ADOPT, we also investigate its runtime as a function of the number of threads (Sec.~\ref{sub:parallelization})}. \revision{For all systems, we create indexes to optimize performance (index creation overheads are reported separately in Appendix~\ref{sec:indexcreation}). For systems such as MonetDB that create indexes automatically, based on properties of observed queries, we perform one warm-up run before starting our measurements.} 


%Each system incurs overhead for ingesting the data into their own internal format. ADOPT stores data as sorted index arrays and this preparation has a low overhead (up to 60 seconds) and is not reported. Its memory overhead is also very low: Besides the data arrays, each hypercube needs a start and end value index per attribute. Each thread either consumes an entire hypercube or returns up to $n$ disjoint hypercubes (for $n$ join attributes). EmptyHeaded takes more than five days to construct indices for non-binary relations.

