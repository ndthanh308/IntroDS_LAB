% -----------------------------
% -----------------------------
% -----------------------------

%%%%%%
%%%%%% References
%%%%%%

@article{RAMEZANIDOORAKI2021103671,
title = {An innovative bio-inspired flight controller for quad-rotor drones: Quad-rotor drone learning to fly using reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {135},
pages = {103671},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103671},
url = {https://www.sciencedirect.com/science/article/pii/S092188902030511X},
author = {Amir {Ramezani Dooraki} and Deok-Jin Lee},
keywords = {Reinforcement learning, Autonomous system, Bio-inspired artificial intelligence, Policy optimization, Artificial neural network, Bio-inspired controller, Machine learning},
abstract = {}
}
@article{Hwangbo_2017,
	doi = {10.1109/lra.2017.2720851},
	url = {https://doi.org/10.1109%2Flra.2017.2720851},
	year = 2017,
	month = {oct},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {2},
	number = {4},
	pages = {2096--2103},
	author = {Jemin Hwangbo and Inkyu Sa and Roland Siegwart and Marco Hutter},
	title = {Control of a Quadrotor With Reinforcement Learning},
	journal = {{IEEE} Robotics and Automation Letters}
}
}

@Article{machines10070500,
AUTHOR = {Ramezani Dooraki, Amir and Lee, Deok-Jin},
TITLE = {A Multi-Objective Reinforcement Learning Based Controller for Autonomous Navigation in Challenging Environments},
JOURNAL = {Machines},
VOLUME = {10},
YEAR = {2022},
NUMBER = {7},
ARTICLE-NUMBER = {500},
URL = {https://www.mdpi.com/2075-1702/10/7/500},
ISSN = {2075-1702},
ABSTRACT = {},
DOI = {10.3390/machines10070500}
}
@Article{app11073257,
AUTHOR = {Pi, Chen-Huan and Ye, Wei-Yuan and Cheng, Stone},
TITLE = {Robust Quadrotor Control through Reinforcement Learning with Disturbance Compensation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3257},
URL = {https://www.mdpi.com/2076-3417/11/7/3257},
ISSN = {2076-3417},
ABSTRACT = {},
DOI = {10.3390/app11073257}
}
@article{ChenHuanPi_LowLevel,
title = {Low-level autonomous control and tracking of quadrotor using reinforcement learning},
journal = {Control Engineering Practice},
volume = {95},
pages = {104222},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2019.104222},
url = {https://www.sciencedirect.com/science/article/pii/S0967066119301923},
author = {Chen-Huan Pi and Kai-Chun Hu and Stone Cheng and I-Chen Wu},
keywords = {Reinforcement learning, Policy gradient, Quadrotor},
abstract = {}
}

@article{DBLP:journals/corr/PathakAED17,
  author       = {Deepak Pathak and
                  Pulkit Agrawal and
                  Alexei A. Efros and
                  Trevor Darrell},
  title        = {Curiosity-driven Exploration by Self-supervised Prediction},
  journal      = {CoRR},
  volume       = {abs/1705.05363},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.05363},
  eprinttype    = {arXiv},
  eprint       = {1705.05363},
  timestamp    = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/PathakAED17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{sun2022aggressive,
      title={Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning}, 
      author={Qiyu Sun and Jinbao Fang and Wei Xing Zheng and Yang Tang},
      year={2022},
      eprint={2203.14033},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{10.5555/3305890.3305962,
author = {Ostrovski, Georg and Bellemare, Marc G. and van den Oord, A\"{a}ron and Munos, R\'{e}mi},
title = {Count-Based Exploration with Neural Density Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2721–2730},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@article{LearningGentleObject,
  author       = {Sandy H. Huang and
                  Martina Zambelli and
                  Jackie Kay and
                  Murilo F. Martins and
                  Yuval Tassa and
                  Patrick M. Pilarski and
                  Raia Hadsell},
  title        = {Learning Gentle Object Manipulation with Curiosity-Driven Deep Reinforcement
                  Learning},
  journal      = {CoRR},
  volume       = {abs/1903.08542},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.08542},
  eprinttype    = {arXiv},
  eprint       = {1903.08542},
  timestamp    = {Mon, 01 Apr 2019 14:07:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-08542.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{intrinsicmotivation,
  
AUTHOR={Oudeyer, Pierre-Yves and Kaplan, Frederic},   
	 
TITLE={What is intrinsic motivation? A typology of computational approaches},      
	
JOURNAL={Frontiers in Neurorobotics},      
	
VOLUME={1},           
	
YEAR={2007},      
	  
URL={https://www.frontiersin.org/articles/10.3389/neuro.12.006.2007},       
	
DOI={10.3389/neuro.12.006.2007},      
	
ISSN={1662-5218},   

}

@article{GOTTLIEB2013585,
title = {Information-seeking, curiosity, and attention: computational and neural mechanisms},
journal = {Trends in Cognitive Sciences},
volume = {17},
number = {11},
pages = {585-593},
year = {2013},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2013.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661313002052},
author = {Jacqueline Gottlieb and Pierre-Yves Oudeyer and Manuel Lopes and Adrien Baranes},
}

@ARTICLE{motionplanninghumanoid,
  
AUTHOR={Frank, Mikhail and Leitner, Jürgen and Stollenga, Marijn and Förster, Alexander and Schmidhuber, Jürgen},   
	 
TITLE={Curiosity driven reinforcement learning for motion planning on humanoids},      
	
JOURNAL={Frontiers in Neurorobotics},      
	
VOLUME={7},           
	
YEAR={2014},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnbot.2013.00025},       
	
DOI={10.3389/fnbot.2013.00025},      
	
ISSN={1662-5218},   

}


@article{Schulman2015-TRPO,
  author    = {John Schulman and
               Sergey Levine and
               Philipp Moritz and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {Trust Region Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1502.05477},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.05477},
  archivePrefix = {arXiv},
  eprint    = {1502.05477},
  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanLMJA15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Schulman2017-PPO,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{Mnih2016a,
	title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
	author = 	 {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {1928--1937},
	year = 	 {2016},
	editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
	url = 	 {http://proceedings.mlr.press/v48/mniha16.html},

}

@misc{schulman2015highdimensional,
    title={High-Dimensional Continuous Control Using Generalized Advantage Estimation},
    author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
    year={2015},
    eprint={1506.02438},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Kakade2002,
	author = {Kakade, Sham and Langford, John},
	title = {Approximately Optimal Approximate Reinforcement Learning},
	booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
	series = {ICML '02},
	year = {2002},
	isbn = {1-55860-873-7},
	pages = {267--274},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=645531.656005},
	acmid = {656005},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
} 

@misc{MDP,
	title = {Markov Decision Process},
	howpublished = {\url{https://en.wikipedia.org/wiki/Markov_decision_process}},
	note = {Accessed: 2023-03-16}
}

@misc{ROS,
	title = {Robot Operating System},
	howpublished = {\url{https://www.ros.org/}},
	note = {Accessed: 2023-03-16}
}

@Inbook{Furrer2016,
author="Furrer, Fadri
and Burri, Michael
and Achtelik, Markus
and Siegwart, Roland",
editor="Koubaa, Anis",
chapter="RotorS---A Modular Gazebo MAV Simulator Framework",
title="Robot Operating System (ROS): The Complete Reference (Volume 1)",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="595--625"
}

@misc{Gazebo,
	title = {Gazebo Simulator},
	howpublished = {\url{http://gazebosim.org/}},
	note = {Accessed: 2023-03-16}
}

@article{DBLP:journals/corr/HouthooftCDSTA16,
  author       = {Rein Houthooft and
                  Xi Chen and
                  Yan Duan and
                  John Schulman and
                  Filip De Turck and
                  Pieter Abbeel},
  title        = {Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian
                  Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1605.09674},
  year         = {2016},
  url          = {http://arxiv.org/abs/1605.09674},
  eprinttype    = {arXiv},
  eprint       = {1605.09674},
  timestamp    = {Mon, 03 Sep 2018 12:15:29 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HouthooftCDSTA16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{MolchanovRL_LowLevel,
  author       = {Artem Molchanov and
                  Tao Chen and
                  Wolfgang H{\"{o}}nig and
                  James A. Preiss and
                  Nora Ayanian and
                  Gaurav S. Sukhatme},
  title        = {Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies
                  to Multiple Quadrotors},
  journal      = {CoRR},
  volume       = {abs/1903.04628},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.04628},
  eprinttype    = {arXiv},
  eprint       = {1903.04628},
  timestamp    = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-04628.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Balaji2017,
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6405–6416},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@ARTICLE{InnateMotivationRobotSwarms,
  author={Kaiser, Tanja Katharina and Hamann, Heiko},
  journal={IEEE Transactions on Robotics}, 
  title={Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments}, 
  year={2022},
  volume={38},
  number={6},
  pages={3582-3601},
  doi={10.1109/TRO.2022.3181004}}

@ARTICLE{CuriosityRobotNavigation,
  author={Cai, Kuanqi and Chen, Weinan and Wang, Chaoqun and Zhang, Hong and Meng, Max Q.-H.},
  journal={IEEE Robotics and Automation Letters}, 
  title={Curiosity-based Robot Navigation under Uncertainty in Crowded Environments}, 
  year={2023},
  volume={8},
  number={2},
  pages={800-807},
  doi={10.1109/LRA.2022.3232270}}
