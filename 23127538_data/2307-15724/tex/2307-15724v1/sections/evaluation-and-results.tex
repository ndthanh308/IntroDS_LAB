\label{evaluation_section}
% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed
%


% Figure environment removed

\subsection{Reward Maximization and Quad-copter Low-level Control}
Reward maximization is the main goal in reinforcement learning-based algorithms. Figure \ref{averaged_reward_averaged_failed_fly_fig} illustrates the performance of the competing algorithms. Looking at the left side of the figure, one can observe that only PPO+HCM (i.e., our proposed curiosity-based algorithm) is able to maximize the reward over time. In other words, only PPO+HCM can learn to perform according to the goals mentioned in Section \ref{environment_section} in the environment. The right side of the same figure displays some information about Failed Fly. A `Failed Fly' is when the quad-copter is initiated randomly in the environment, the algorithm cannot learn to control it and, as a result, the quad-copter crashes. Again, only PPO+HCM can control the quad-copter and reduce the number of `Failed Fly' over time, which is another sign that the algorithm is successful in the environment.
\par
Considering the problem our algorithm tries to solve is controlling a quad-copter, the averaged position and attitude errors are collected and displayed in Figure \ref{averaged_errors_fig}. The information shown in the figure can be divided into two categories 1) Attitude Control Information, and 2) Position Control Information. Reducing errors for Attitude Control implies that the algorithm can control the quad-copter and not crash. However, it does not give any information about how far or near the quad-copter is from the desired position. That information can be retrieved from the Position Control-related diagrams (i.e., the left side of the Figure \ref{averaged_errors_fig}). Overall, both Attitude and Position control information illustrates the capability of our proposed curiosity-based method (i.e., HCM) while showing the lack of ability of other algorithms in terms of control and flying toward the desired position and attitude.
\par
So far, our result showed this paper's algorithm capability in controlling the quad-copter agent and flying toward the desired position and attitude (i.e., the first part of the objectives of the environment described in Section \ref{environment_section}). However, to illustrate the algorithm's capabilities, some information regarding the obstacles is necessary (i.e., the second part of the environment objectives), which is mentioned in the following.


\subsection{Goal and Obstacles Distances}
As described in Section \ref{environment_section}, the algorithm should control a quad-copter agent and fly it toward a desired location while avoiding three obstacles. Figure \ref{averaged_distances_goal_obstacle_fig} is comprised of two diagrams. The left diagram shows the distance between the quad-copter and the position of the goal, and the right image shows the distance between the quad-copter and obstacles in the environment. The obstacle distance is the mean of the three Euclidean distances between the quad-copter and each one of the obstacles in the environment. Both diagrams in Figure \ref{averaged_distances_goal_obstacle_fig} show the capability of PPO+HCI in terms of decreasing the distance between the quad-copter and the location of the goal while slightly increasing and then maintaining the distance between the quad-copter and obstacles. These two diagrams illustrate that the quad-copter controlled by PPO+HCP reaches the goal while avoiding the obstacles.


\subsection{Curiosity Visualization}
Considering the description in Section \ref{curiosity_visualization_section} regarding the visualization of the curiosity effect, Figure \ref{curiosity_visualization_fig} and \ref{curiosity_visualization_fig_inc} show the effect of curiosity in evolving exploration patterns when the number of episodes increases. The figures are comprised of two rows; the first row shows the PPO exploration pattern, and the second row shows the PPO+HCM exploration pattern. For PPO, as can be seen in the figures, the pattern does not evolve and is almost static where it does not move toward the center of the box (i.e., toward the desired position); this observation is also supported by the Position Control Information section of Figure \ref{averaged_errors_fig}, as the algorithm does not reduce the position error by time. For the PPO+HCM, on the other hand, as the illustrated in the second row of the figures, the pattern of exploration is changing. By increment of the number of episodes, the algorithm explores more trajectories that ends up towards the center of the box (i.e., desired position), and also, more concentration can be seen in that area.

\subsection{Trajectories of Optimal Policy}
Figure \ref{optimal_policy_trajectories_fig} shows two sample trajectories generated by an optimal policy trained using the curiosity-based algorithm proposed in this paper (i.e., PPO+HCM) when the quad-copter is initiated in random initial positions and attitudes and three obstacles are initiated in three random positions. The trajectories are visualized using the right-hand coordinate system where the red arrow shows the Yaw orientation of the quad-copter. As can be seen, the algorithm can control the quad-copter and pass the obstacles while controlling the vehicle's Yaw toward the desired location. 
