In this section, we discuss some matters that need further explanation.

\subsection{Algorithm Selection Strategy}
We used the PPO as the baseline algorithm for the performance test because it is the algorithm that is mostly used for learning to fly such as in \cite{RAMEZANIDOORAKI2021103671} \cite{Hwangbo_2017} \cite{MolchanovRL_LowLevel} and is a powerful on-policy algorithm. Moreover, we also selected SAC to show the result of a powerful off-policy RL algorithm that incorporates a memory replay. Our goal in this paper was to show that integrating curiosity with reinforcement learning-based flight controller makes solving complex low-level flight control problems possible. Thus, we selected PPO+ICM for testing the performance. Finally, we showed the capability of the proposed algorithm (i.e., PPO+HCM) compared with the competing ones and the importance of our contribution at the algorithm level. 


\subsection{The Gap between the Simulation and Real World}
One area for improvement with the simulation-based learning algorithm is the gap between the simulation and the real world. One way to address this problem is by reducing the difference between the simulation and the real-world environments. However, this paper focuses on the learning part and the effect of adding curiosity to the learning policy for low-level flight control. For this reason, we did not enter the area of real-world tests in this paper, especially considering that it is already proven in \cite{Hwangbo_2017} \cite{MolchanovRL_LowLevel} \cite{ChenHuanPi_LowLevel} that a reinforcement learning-based policy can be used in a real-world quad-copter for low-level control.


\subsection{Exploration, Exploitation, and Curiosity}
It is a well-known capability of curiosity to increase and orient the exploration towards surprise, or in better words, to explore the environment meaningfully. While reinforcement learning algorithms, by default, use a trade-off between exploitation and random exploration (exploration mechanisms such as epsilon greedy in Q-learning or noise injected to the output action in methods such as DDPG, SAC, and PPO), they do purely random explorations. However, curiosity, as implemented in our work, generates an intrinsic reward that increases in the states unknown to the agent (by measuring the agent prediction error on those states) and decreases in the states that the agent visited frequently. Thus, curiosity can be seen as a parameterized neural network architecture that rewards the agent more in surprising states, motivating the agent to explore those states more. As a result, curiosity aims to make the exploitation part of the RL more efficient while still using the default exploration mechanism. 


\subsection{Computational Time}
Considering adding an extra head to PPO networks and having ten sub-modules in the proposed curiosity algorithm, the learning part is much heavier than regular RL-based low-level flight control learning. However, that is only for the learning time, which usually happens on a powerful machine. For the execution time or deployment on an actual quad-copter, our approach is similar to the mentioned approaches because only one network (i.e.,  Policy Network) is needed.
