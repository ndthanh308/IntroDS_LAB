Humans and intelligent creatures can learn in different ways; among them is learning by experience. Further, they use a spectrum of motivations: some are triggered internally (intrinsic motivations), and some are triggered externally and by the environment (extrinsic motivations). An intelligent creature learns to act in the direction of responding to its motivations. 
Observing this paradigm in nature, machine learning and control communities created the framework of the Markov Decision Process (MDP) and Reinforcement Learning (RL) algorithms to replicate this optimization process in robots and machines. Further, several computational models of intrinsic motivations, such as curiosity, have been implemented in the past decades.
\par
At the same time, considering the advances in computer hardware, different intelligent algorithms for autonomous control of ground, aerial, underwater, surface, and legged robots have been created during the past decade. Specifically, these advancements were significant for multi-copter drones where the weight of the whole robot is significant in terms of its ability to fly and maneuver capabilities in three axes. Nowadays, it is possible to see the application of Unmanned Aerial Vehicles (UAVs) such as quad-copters in several areas. Autonomous inspection, search and rescue missions, and navigation in unknown environments are examples of high-level control where an algorithm takes high-level decisions and passes it to a low-level controller for execution using robots' actuators. Autonomous learning of aggressive maneuvers, drone racing, and fault-tolerant control are examples of low-level flight controllers where the algorithm directly controls the actuators.
\par
Combining the framework of learning-based algorithms, such as reinforcement learning and deep reinforcement learning, with UAVs pushed their autonomous control to new frontiers, allowing them to autonomously learn to control both in high-level and low-level state spaces. 
\par
This paper proposes a new reinforcement learning-based low-level flight controller that learns by parameterized intrinsic (a computational model of curiosity) and extrinsic (external immediate and auxiliary rewards) motivations to directly control the quad-copter's motor speeds and flies toward the desired position while avoiding obstacles. Our contributions are summarized as follows:
\begin{itemize}
  \item We propose a new approach for learning low-level flight policy using parameterized curiosity module.
  \item In our proposed approach, we consider both passing through obstacles and controlling the Yaw direction towards the desired location.
  \item To achieve the mentioned contributions, we propose a new approach for calculating the curiosity reward based on the prediction error.
\end{itemize}
\par
In the rest of this paper, first, we discuss the related literature and the difference between our work and other related works. Next, we describe the proposed methodology including the reinforcement learning approach, the curiosity module, the simulation environment, and the visualization of the curiosity effect. Then, we describe the experimental evaluation and provide results along with a discussion about important matters related to our work. Finally, we provide concluding remarks.