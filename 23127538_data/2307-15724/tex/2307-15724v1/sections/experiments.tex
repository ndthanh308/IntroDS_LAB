\label{experiments_section}
In order to illustrate the performance of the proposed algorithm we compared it with other powerful algorithms which serve as the baselines:
\begin{itemize}
  \item PPO is used as the baseline on-policy algorithm for testing and performance comparison in our tests. 
  \item SAC is used as an off-policy algorithm with memory replay to compare its performance with the other on-policy algorithms mentioned in this section.
  \item PPO+ICM is the PPO algorithm combined with ICM \cite{DBLP:journals/corr/PathakAED17} module (for the curiosity reward). 
  \item PPO+HCM is the PPO algorithm combined with our proposed curiosity approach (i.e., HCM).
\end{itemize}
The PPO algorithm we used here as the baseline is a highly tuned PPO algorithm against our fly environment with exploration noise generated by a Gaussian distribution with a mean of 0 and standard deviation of 1.0. The default parameters used for the SAC algorithm (the stable baseline version). 
%
We tested the performance of the competing algorithms by running our scenario multiple times. Each algorithm was run six times, and the min, max and average results are calculated. Our code can be found in the corresponding GitHub repository\footnote{\url{https://github.com/a-ramezani/CDRL-L2FC_u_HCM}}.