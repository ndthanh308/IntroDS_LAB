\section{Related Work}
\label{sec:related}

\noindent\textbf{Object Detection:} Detector networks can be categorized into one-stage~\cite{tian2019fcos,tan2020efficientdet,liu2016ssd,carion2020end,redmon2018yolov3,lin2017focal} and two-stage~\cite{girshick2015fast,he2017mask,ren2015faster,lin2017feature} detectors. One-stage detectors which directly predict the output objects are comparatively faster while the two-stage detectors are generally superior in performance. The two-stage methods first extract regions of interests (RoIs) using a network~\cite{ren2015faster} and then obtain the final classification and regression outputs using a multi-layer network on the RoIs. Since these architectures perform poorly in incremental settings, we extend the two-stage Faster R-CNN~\cite{ren2015faster} network such that it can learn new object classes over time.

% \vspace{5pt}

\noindent\textbf{Incremental Learning:} Class-incremental learning~\cite{masana2022class,de2021continual} and catastrophic forgetting~\cite{mccloskey1989catastrophic} has been explored extensively for image classification~\cite{castro2018end, li2017learning, rebuffi2017icarl} problems. The previous works can be categorized into rehearsal-based, parameter-isolation and regularization-based methods. Rehearsal-based methods store training samples~\cite{rebuffi2017icarl,castro2018end,li2022class} from previous tasks or generates training data~\cite{kemker2018fearnet,shin2017continual,wu2018memory}. Parameter-isolation methods~\cite{mallya2018packnet,mallya2018piggyback,yoon2018lifelong,liu2021l3doc} modify the initial network to accommodate new classes. Prior-focused regularization methods constrain learning on new classes and penalizing updating on weights~\cite{aljundi2018memory,kirkpatrick2017overcoming} or gradients~\cite{lopez2017gradient} while data regularization methods perform distillation~\cite{hinton2015distilling} between the intermediate features~\cite{douillard2020podnet,douillard2022dytox,li2017learning,hou2019learning} or attention maps~\cite{dhar2019learning} of the teacher model and the current student model to reduce forgetting. Other methods use embedding networks~\cite{Yu_2020_CVPR} or classifier drift correction~\cite{belouadah2019il2m} to address the changing class distributions. In our work, we focus on rehearsal-based and regularization-based methods.

% \vspace{5pt}
% Figure environment removed



\noindent\textbf{Incremental Object Detection:} 
% One of the initial works~\cite{shmelkov2017incremental} on incremental object detection uses knowledge distillation on the outputs of a Fast R-CNN~\cite{girshick2015fast} network. 
Most of the recent works on incremental object detection use the Faster R-CNN~\cite{ren2015faster} architecture and performs distillation on the intermediate features~\cite{yang2022multi,liu2020multi,peng2020faster,hao2019end,cermelli2022modeling,zhou2020lifelong}, the region proposal network~\cite{cermelli2022modeling,peng2020faster,zhou2020lifelong} and head layers~\cite{feng2022overcoming}. 
%,chen2019new
Relatively few works~\cite{li2019rilod,peng2023diode,shieh2020continual} used one-stage architectures for incremental learning. Although the background shift issue was partially addressed in~\cite{zhou2020lifelong} by preventing previous class regions to be sampled as background but it was highlighted recently in~\cite{cermelli2022modeling,peng2020faster}.~\cite{cermelli2022modeling} proposed an unbiased classifier training loss and classifier distillation loss to explicitly tackle the background shift. EWC~\cite{kirkpatrick2017overcoming} has been adapted by~\cite{liu2020incdet} for object detection. 
% Rehearsal-based methods replay images~\cite{gupta2022ow,joseph2021incremental,shieh2020continual} or intermediate feature representations~\cite{acharya2020rodeo} to maintain good performance on previous classes. 
While some methods replay images for finetuning~\cite{joseph2021towards,gupta2022ow} after training and for meta-learning~\cite{joseph2021incremental}, very few methods replay whole images~\cite{shieh2020continual} or stored feature representations~\cite{acharya2020rodeo} during training. For instance segmentation, \cite{ghiasi2021simple} explored copying random instances from one image to another.
Our work deals with bounding box replay methods to better address the challenges of IOD.

 