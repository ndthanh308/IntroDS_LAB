\input{tables/voc-single}
\input{tables/voc-multiple}


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Settings}
\noindent\textbf{Datasets:} We evaluate the proposed method on two publicly available datasets namely PASCAL VOC 2007~\cite{everingham2009pascal} and MS COCO 2017~\cite{lin2014microsoft}. PASCAL VOC 2007 contains 20 object classes and 9,963 images, 50\% of which is used for training and validation and the remaining 50\% for testing following~\cite{everingham2009pascal}. MS COCO 2017, as a challenging dataset, has 80 different object classes and provides 83,000 images for training, 40,000 for validation and 41,000 for testing. 

% \vspace{0.5em}

\noindent\textbf{IOD Protocols:} Following previous works on this topic~\cite{shmelkov2017incremental,joseph2021incremental,cermelli2022modeling}, we obey the same experimental protocols. Each training task contains all images which have at least one bounding box from a new class. The annotations are available only for the new classes while the previous and future classes are not annotated. This setting is practical and can also have repetitions of images across tasks.
% since an image having a new class can also have previous class objects and was therefore already used for training in previous tasks.


\noindent\textbf{Implementation Details:} Similar to~\cite{shmelkov2017incremental,cermelli2022modeling,joseph2021incremental,yang2022multi,liu2020multi,peng2020faster,hao2019end,chen2019new,zhou2020lifelong}, we use the Faster R-CNN~\cite{ren2015faster} architecture with a Resnet-50~\cite{he2016deep} backbone pretrained on ImageNet~\cite{deng2009imagenet}. 
We train the network with SGD optimizer, momentum of 0.9 and weight decay of $10^{-4}$. We use a learning rate of 5 $\times 10^{-3}$ for the initial task and 2 $\times 10^{-3}$ for the subsequent tasks. We used 15K iterations for 5 or 10 class increments in a task and 5K iterations when adding 1 or 2 new classes.
We set the memory size as 2,000 for all the experiments on PASCAL VOC 2007, 10,000 for 70-10 and 5,000 for 40-40 settings on MS COCO 2017 respectively. Our method uses a stack to store boxes, which are randomly selected and placed (while considering overlap criteria) during each iteration. 
To balance the number of old and new objects, we determine the 1:1:2 ratio for mixup, mosaic, and new images based on comparisons across different settings.
% We use a ratio of 1:1:2 for mixup images, mosaic images and new images (without augmentation) respectively.


\noindent\textbf{Evaluation:} 
We evaluate the methods in terms of mean average precision at 0.5 IoU threshold for
% and compare the mAP obtained from different methods on 
PASCAL VOC 2007. For MS COCO 2017, we also report the mAP at different IoU ranging from 0.5 to 0.95 IoU (mAP@[50:95]), at 0.50 IoU (mAP@50) and at 0.75 IoU (mAP@75). 

\input{tables/coco}

\subsection{Quantitative Evaluation}
Following previous works~\cite{cermelli2022modeling,joseph2021incremental,peng2020faster,yang2022multi,zhou2020lifelong,shmelkov2017incremental}, we evaluate our method on settings with different number of initial classes and one or more incremental tasks. We compare our method with two baselines, the Fine-tuning when the model is trained with the data incrementally without any regularization or data replay, and the Joint training when the model is trained on the entire dataset with all the annotations. All results are obtained after training of the last task.

\vspace{-1em}

\input{tables/ablation1}

\subsubsection{PASCAL VOC 2007}
For PASCAL-VOC 2007, we perform our experiments on 19-1, 15-5, 10-10 and 5-15 single incremental task settings adding 1, 5, 10, 15 classes respectively. For multi-step incremental settings, we evaluate on 10-5, 5-5, 10-2, 15-1 and 10-1 settings where we add 5, 5, 2, 1 and 1 classes respectively at every step till all the 20 classes are seen. 



\vspace{2 pt}

\noindent\textbf{Single-step increments:}
We benchmark our ABR method against the existing methods on~\cref{tab:voc_exp_ss}. We notice that Fine-tuning suffers from catastrophic forgetting across all settings. 
% MMA~\cite{cermelli2022modeling} is the best performing method among the available methods. 
ABR outperforms all other methods across all the settings, significantly improving over MMA on the new classes by 4.5 mAP on 15-5, 8.9 mAP on 10-10 and 9.8 mAP on 5-15. We argue that the enhanced stability and plasticity is due to the augmented box replay of previous classes and our effective attention distillation. Our improvements over the methods storing exemplars~\cite{gupta2022ow,joseph2021incremental,joseph2021towards} confirm the importance of the box replay for IOD. 

\vspace{2 pt}


\noindent\textbf{Multi-step increments:}
The catastrophic forgetting and the background shift problem is more crucial on the longer incremental settings as seen in the performance from~\cref{tab:voc_exps_ms}. Fine-tuning suffers from almost complete forgetting on the initial classes. 
% ABR performs significantly better than the other methods across all multi-step settings. 
ABR improves over the closest competitor MMA by 3.9 mAP on 10-5, 3.5 mAP on 10-2, 3.4 mAP on 15-1 and 7.7 mAP on the longest and most challenging setting 10-1. It is interesting to observe that most methods struggle on the 5-5 setting with only 5 initial classes while ABR improves over MMA by 19.5 mAP. 
% This proves that when the model fails to obtain good generalization ability in the initial task, the negative impact of the comparison methods to inhibit the adaption of new classes will gradually intensify with learning, and eventually lead to poor plasticity. 
This implies that the existing methods require more classes in the initial task to achieve better generalization and thus, fails to adapt to new classes when the first task has lesser classes in 5-5 setting. 
On the most difficult setting of 10-1 with 10 increments, ABR outperforms MMA by 4.1 mAP on the previous classes and 11.1 mAP on the new classes. Note that for multiple increment settings, the improvement in the performance of incremental classes is not only due to better learning of new classes but also due to lesser forgetting of the intermediate task classes after moving to new tasks. 




\subsubsection{MS COCO 2017} 
For MS COCO 2017, we perform experiments on 40-40 and 70-10 settings adding 40 and 10 classes respectively. As shown in~\cref{tab:coco_exps}, Fine-tuning suffers from catastrophic forgetting on both settings. While Faster ILOD and MMA has improved over Fine-tuning, our method improves average mAP@[50:95] over MMA by 1.5 on 40-40 setting and by 0.9 on 70-10 setting. These results signify lesser forgetting and better adaptation to new classes with our method.

% our method far outperformed Fine-tuning across various metrics in two setting. 

% In summary, we can find that most methods focus on mitigating catastrophic forgetting, but often at the cost of sacrificing performance on new tasks to preserve previous information. Instead, the augmented box replay can also be considered as an augmentation technology, which relatively provides more diverse background information for the adaption of new classes, thereby improving the plasticity of the model which is very important for real application scenarios. In addition, the attentive RoI distillation further overcomes the catastrophic forgetting and enhance the stability performance in various setting.





\subsection{Analysis and Ablation Study}
We investigate the role of the network components, replay selection strategies, augmentation types in~\cref{tab:ablation1} on the VOC 10-10 and 10-5 settings. We take the baseline model with the RCN classification and distillation loss proposed by~\cite{cermelli2022modeling}. We show that our attentive RoI distillation improves over the RPN distillation used by~\cite{cermelli2022modeling,peng2020faster} owing to better exploitation of location and feature information of the RoIs. 
In replay strategies, we implemented the herding strategy~\cite{rebuffi2017icarl} for selecting boxes to replay. Our method improves 1\%$\sim$1.5\% mAP over the herding strategy. We can observe that our proposed prototype box selection can better capture more representative prototype samples for previous classes. Further, we add mixup and mosaic replay individually and observe that both strategies improve the performance on previous and new classes. The best performance is achieved when both mixup and mosaic replay are performed with the new images.


% We study the role of different components in the proposed method in~\cref{tab:ablation1} on the 10-10 and 10-5 settings of PASCAL VOC 2007. We take the baseline model with the RCN classification and distillation loss proposed by~\cite{cermelli2022modeling}. We show that our attentive RoI distillation improves over the RCN distillation used by~\cite{cermelli2022modeling,peng2020faster} owing to better exploitation of location and feature information of the RoIs. Further, we add mixup and mosaic replay individually and observe that both the strategies improve the performance on previous and new classes. The best performance is achieved when both mixup and mosaic replay are performed with the new images.\JW{Do we compare random bounding box selection with our selection method ?}


We investigate the role of the memory size and train ABR with different memory size of previous class boxes. \cref{fig:bf} plots the mAP@50 results with increasing memory size. It is observed that the performance increases with increasing memory size or replay of more previous objects. It can be observed that after the memory size $>$ 2000, the growth rate of mAP tends to be more stable. Therefore, in the main experiments, we use a memory size of 2000.
% The best mAP is achieved at 2000 memory size which is used in our main experiments.

\input{tables/memory}
\cref{tab:memory} presents a comparison between image replay and our proposed ABR method. The same number of objects ensures that the original information about the previous categories stored in the memory buffer is consistent, and the same storage space controls practicality in real-world applications. As shown in \cref{tab:memory}, despite having the same number of objects, image replay performs worse than augmented box replay in recognizing new classes. This confirms that replaying original images can lead to foreground shift and limit the adaptation of new classes. On the other hand, our memory buffer contains about 4 times as many original objects for previous classes as image replay.
% which provides more diverse original information for these classes.



% \vspace{-1em}

% Figure environment removed


\subsection{Visualization}
\cref{fig:mixup} shows some examples of images generated by mixup replay in VOC 10-10 setting. It can be seen intuitively that the mixup strategy makes the box reasonably integrated into the new images and minimizes the occlusion with the new objects. In addition, the background information compared to the new objects is greatly enriched.
% In the inference results in \cref{fig:predication}, we observe that our proposed method effectively alleviates the forgetting and reduces the bias towards novel classes. 
The inference results are available in supplementary material.


% Figure environment removed
\vspace{-5pt}