\subsection{Analysis Attentive RoI Distillation (ARD)}

While existing methods have utilized attention distillation primarily on feature maps, we advance this approach by integrating location information of Region of Interest (RoI) proposals. By doing so, our model gains the capability to distill both feature and localization information from the replayed and new objects, leading to an overall performance enhancement.

\cref{fig:sm-attmap} showcases some additional attention maps, highlighting how our Attention-based RoI Distillation (ARD) loss effectively retains attention on the old class (e.g., bicycle). This observation confirms ARD's competence in alleviating catastrophic forgetting, a phenomenon that impacts model performance when learning new tasks.

Through the inclusion of location-awareness in attention distillation, our proposed ARD method exemplifies its potential to mitigate catastrophic forgetting and reinforce the preservation of crucial knowledge from previous tasks, resulting in improved overall model performance.


% Figure environment removed