\section{Introduction}
% Figure environment removed

\label{sec:intro}
The field of deep learning has witnessed remarkable progress recently, and state-of-the-art object detection models~\cite{carion2020end,ren2015faster,redmon2018yolov3,he2017mask,girshick2015fast} have been developed that performs exceptionally well on benchmark datasets. However, these models are typically designed to learn from data in a static manner, assuming that all object classes are available at once during training. In real-world scenarios, new object classes may emerge over time, making it necessary to update the model with new data. The inability to learn incrementally is a significant limitation for object detectors, particularly in cases of limited data storage capacity or data privacy concerns~\cite{de2021continual,masana2022class}. Therefore, developing incremental object detection (IOD) methods has become an essential and challenging task in real-world applications. 



SOTA object detectors experience a phenomenon known as catastrophic forgetting~\cite{mccloskey1989catastrophic}, where their performance on previous classes degrades after learning new classes. This issue is commonly observed in incremental settings~\cite{de2021continual} and can be mitigated by balancing model stability (retaining previous information) and plasticity (learning new information without forgetting previous knowledge). 
While most studies in incremental learning are based on image classification~\cite{rebuffi2017icarl,li2017learning,kirkpatrick2017overcoming,aljundi2018memory}, recently it has been studied in the context of object detection~\cite{cermelli2022modeling,shmelkov2017incremental,hao2019end,chen2019new,peng2020faster} and semantic segmentation~\cite{douillard2021plop,zhang2022representation,goswami2023attribution}. A critical aspect in IOD is the background shift, also known as missing annotations~\cite{cermelli2022modeling, peng2020faster} which occurs due to the presence of multiple class objects in an image. Objects belonging to previous or future tasks in incremental object detection are often not annotated and assigned to the background class, as annotations are only available for classes in the current task. 

One of most efficient approaches in incremental classification is rehearsal-based strategy with storing images~\cite{rebuffi2017icarl,castro2018end}.
However, directly applying the replay images into IOD will cause the unlabelled objects of current classes in the replay images to be treated as background by the model. Consequently, the new objects will be background in replay images, while regarded as foreground in the new images. This leads to a contradiction between the foreground annotations in the exemplars and the current images as illustrated in~\cref{fig:introduction}. We refer to this problem as foreground shift which affects the plasticity of the current model.


To overcome the foreground shift for image replay in IOD, we propose a novel method called Augmented Box Replay (ABR). ABR uses mixup and mosaic box augmentation strategies to replay previous objects as an alternative to image replay for training in the current task. Compared to storing images in memory, ABR stores approximately four times as many object instances with the same storage requirements. To more effectively address catastrophic forgetting, we introduce a novel Attentive RoI Distillation loss that utilizes spatial attention from region-of-interest (RoI) features to align the most informative features of the previous and new models and correct the anchor position deviations of proposal pairs.


The proposed method is experimentally evaluated on Pascal-VOC and COCO datasets, and significantly outperforms SOTA methods in multiple settings. Our main contributions are three-fold: 
\vspace{-5pt}
\begin{itemize}
  \setlength\itemsep{0em}
    \item  This paper is the first to identify the critical foreground shift issue which has hampered the usage of replay methods for IOD. We propose Augmented Box Replay as a solution that reduces the memory requirements, eliminates the foreground shift, and improves the model stability and plasticity.
    \item We propose an innovative Attentive RoI Distillation loss to focus the current model on important location and feature information from the previous model and further reduce catastrophic forgetting.
    \item Our method outperforms state-of-the-art methods across multiple datasets and settings, showcasing its practicality and effectiveness. Especially, on the more challenging longer task sequences and the difficult scenario  with a small initial task, our method obtains significant performance gains (see~\cref{fig:longsteps}). 
\end{itemize}
\vspace{-5pt}


% Figure environment removed
\vspace{-5pt}
