\section{Conclusion}
In this paper, we studied the experience replay method for incremental object detection problem and introduced the critical issue of foreground drift during old image replay. We hypothesize that the foreground drift is the reason that replay methods, which are dominant in incremental learning for image classification, have been little studied for IOD.

% In this paper, we studied the experience replay method for incremental object detection problem. In addition to the catastrophic forgetting and background shift issues discussed in previous works, the crucial problem of foreground drift when old images are replayed in IOD is introduced. We hypothesize that the foreground drift is the reason that replay methods, which are dominant in incremental learning for image classification, have been little studied for IOD.
% The foreground drift affects the learning of new classes and the plasticity of the model. 
To tackle this problem, our proposed method ABR stores bounding boxes from old classes and replays them with new images using mixup and mosaic augmentation strategies. This overcomes the foreground drift situation since only the old classes are stored and replayed and not the unlabeled new classes from old images. In addition to box replay, the proposed attentive RoI distillation uses both the location and feature information for the RoIs extracted from the RPN and enables retention of meaningful knowledge of old classes. Further, our method reduces the memory overhead significantly. We demonstrate that ABR outperforms existing methods across all settings on representative datasets.

This work lays the foundation for bounding box replay instead of the traditional image or feature replay methods for object detection tasks. 
% Further studies can be conducted in this direction to advance the concept of box replay. 
% In addition, we aim to study the implications of foreground drift for incremental semantic segmentation, which also faces the background shift problem. Future work includes extending our approach to the currently popular transformer methods (CITE).
Future research should explore the implications of the foreground shift in incremental semantic segmentation and extend our approach to popular transformer methods~\cite{liu2023continual}.