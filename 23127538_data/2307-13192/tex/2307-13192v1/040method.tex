\section{Counterfactual Explanation Policies}
\label{sec:pol_cf}

Reinforcement learning is a powerful decision-making tool that provides a systematic abstraction for defining environment dynamics with reward preferences and also provides the algorithms to come up with policies for maximizing returns in that environment. In the context of RL agents, we pose the counterfactual question as follows -- given a policy $\pi$ performing at level $J_{\pi}$ in an MDP $\mathcal{M}$, what infinitesimal change in $\pi$ would lead us to a target return of $\rtarget$? Here, we refer to such variation of $\pi$ as \textit{Counterfactual Explanation Policy} conditioned on $\rtarget$. In posing the counterfactual question in terms of target returns, we aim to get contrastive insights into what minimal changes to the current policy can result in improving/worsening its performance to a desired level. To estimate counterfactual explanation policy $\pi_{\text{cf}}$, we first define the return penalty for achieving the target return as:
\begin{equation}
    \label{eqn:return_penalty}
    \begin{aligned}
    L_{\text{ret}} = {} & \norm{J_{\pi} - \rtarget}_p, \text{where $\norm{\cdot}_p$ is $\ell_p$-norm.}
    \end{aligned}
\end{equation}
In order to ensure that the counterfactual policy is similar to the original policy, we limit the changes while achieving the target return. We use the KL-divergence loss to measure the distance between the original policy $\pi_0$ and a given policy $\pi$, \ie, 
\begin{equation}
    \label{eqn:loss_kl}
    L_{\text{KL}} = D_{\text{KL}}(\pi_0 || \pi)
\end{equation}
Next, we can find the counterfactual policy $\pi_{\text{cf}}$ by minimizing the following objective:
\begin{equation}
    \label{eqn:loss_cf_policies}
    \begin{aligned}
        \pi_{\text{cf}} =  {} & \argmin_{\pi}~~ L_{\text{ret}} + k \cdot L_{\text{KL}} \\ = {} & \argmin_{\pi}~~ \norm{J_{\pi} - \rtarget}_p + k \cdot D_{\text{KL}}(\pi_0 || \pi),
    \end{aligned}
\end{equation}
where $k$ is a regularization coefficient that ensures that the output counterfactual policy is close to $\pi_0$.
\subsection{Counterfactual Explanation Policy Optimization}
\label{sec:cf_pol_optimization}
In practice, RL policies are represented using function approximators~\citep{fun_approx_1, fun_approx_2}, \eg a policy is represented using a neural network in deep RL. 
Let $\pi_{\theta}$ denote the policy approximated using a neural network function with parameters $\theta$. We can rewrite Eqn.~\ref{eqn:loss_cf_policies} as:
\begin{equation}
    \label{eqn:loss_cf_policies_theta}
    \begin{aligned}
        \pithetacf =  {} & \argmin_{\theta}~~ L_{\text{ret}, \theta} + k \cdot L_{\text{KL}, \theta} \\ = {} &  \argmin_{\theta}~~ \norm{\Jpitheta - \rtarget}_p + k \cdot D_{\text{KL}}(\pi_{\theta_0} || \pi_\theta)
    \end{aligned}
\end{equation}
The above optimization requires the gradients of the counterfactual objectives \textit{w.r.t.} $\theta$, \ie $\nabla_{\theta} (L_{\text{ret}, \theta} + k \cdot L_{\text{KL}, \theta})$, which can be written as $\nabla_{\theta}L_{\text{ret}, \theta} + k \cdot \nabla_{\theta} D_{\text{KL}}(\pi_{\theta_0} || \pi_\theta)$.

Computing the gradients of Eqn.~\ref{eqn:loss_cf_policies_theta} is not directly feasible using automated differentiation provided in standard deep learning libraries~\citep{pytorch,tensorflow}. Therefore, we detail the derivation of how to estimate the gradients of the two loss terms in Eqn.~\ref{eqn:loss_cf_policies_theta}.

\xhdr{Estimating $\mathbf{\nabla_{\theta}L_{\text{ret}, \theta}}$}
Without loss of generality, we calculate the gradients of the return penalty using $\ell_{1}$-norm for the return loss objective $L_{\text{ret}, \theta}$, \ie
\begin{equation}
    \label{eqn:loss_ret_grad_1}
    \begin{aligned}
    \nabla_{\theta}L_{\text{ret}, \theta} =  \nabla_{\theta}|\Jpitheta - \rtarget| = {} & \text{sgn}(\Jpitheta - \rtarget) \cdot \nabla_{\theta}\Jpitheta,
    \end{aligned}
\end{equation}
where $\nabla_{\theta}\Jpitheta$ becomes the standard policy gradient~\citep{polgrad}.

We approximate $\Jpitheta$ with the average return gathered through the on-policy rollout of $\pi_\theta$ in the given environment. Formally, for $N$ episodes sampled using $\pi_\theta$, \ie $\{\tau_i\}_{i=1}^N \sim \pi_{\theta}$ we have:
\begin{equation}
    \label{eqn:jpitheta_approx}
    \Jpitheta \approx \frac{1}{N}\sum_{i=1}^{N}\sum_{t=0}^{T_i-1}{\gamma^{t}r(s_t, a_t, s_{t+1})}
\end{equation}
Further, using the policy gradient theorem~\citep{polgrad,sutton_book}, we can write $\nabla_\theta \Jpitheta  = \mathbb{E}_{\pi_\theta}[Q_{\pi_\theta}(s, a)\nabla_\theta \text{log}(\pi_\theta(s, a))]$. We approximate $Q_{\pi_\theta}(s, a)$ term by the return obtained starting from state $s$ and action $a~{\sim}~\pi_\theta(\cdot|s)$ for a sampled episode, \ie $Q_{\pi_\theta}(s, a)~{\approx}~\sum_{j=t}^{T-1}~{\gamma^{j-t}r(s_j, a_j, s_{j+1} | s_t = s, a_t = a)}$. Therefore,
\begin{fleqn}
  \begin{equation}
    \label{eqn:polgrad_approx}
    \begin{aligned}[b]
        & \nabla_\theta \Jpitheta  \approx \frac{1}{\sum_{i=1}^{N}{T_i}} \cdot\\
        & \sum_{i=1}^{N}{\sum_{t=0}^{T_i-1}[\sum_{j=t}^{T_i-1}{\gamma^{j-t}r(s_j, a_j, s_{j+1} | s_t, a_t)] \cdot \nabla_\theta \text{log}(\pi_\theta(s_t, a_t))}}
    \end{aligned}
  \end{equation}
\end{fleqn}

Note that the $\Jpitheta$ computation can be made more sample efficient by off-policy estimation~\cite{munos2016safe, jiang2016doubly} of $\Jpitheta$ on rollouts performed using $\pi_{\theta_0}$~\cite{a2c_1,trpo,ppo}. However, for the sake of simplicity of exposition and implementation, we use On-policy Monte Carlo Policy Gradients, which can later be modified to their sample efficient versions depending on the environmental requirements.

\xhdr{Estimating $\mathbf{\nabla_\theta D_{\text{KL}}(\pi_{\theta_0}||\pi_\theta)}$} As discussed in Eqn.~\ref{eqn:loss_kl}, we calculate KL-divergence between two policy distributions as the distance penalty for counterfactual policies. As estimating the exact KL-divergence between two policies (\ie $\mathbb{E}_s[D_{\text{KL}}(\pi_0(\cdot|s)||\pi(\cdot|s))]$) is computationally expensive for large state-action spaces, we approximate it by averaging the KL-divergence calculated over states visited during sampling of $N$ episodes. Using states in $\{\tau_i\}_{i=1}^N \sim \pi_{\theta}$, we write:
\begin{equation}
\label{eqn:dkl_approx}
    D_{\text{KL}}(\pi_{\theta_0}||\pi_\theta) \approx \frac{\sum_{i=1}^{N} \sum_{t=0}^{T_i-1}D_{\text{KL}}(\pi_{\theta_0}(\cdot|s_t)||\pi_{\theta}(\cdot|s_t))}{\sum_{i=1}^{N}{T_i}}
\end{equation}
and finally approximate $\nabla_\theta D_{\text{KL}}(\pi_{\theta_0}||\pi_\theta)$ as: 
\begin{equation}
\label{eqn:grad_dkl_approx}
    \nabla_\theta D_{\text{KL}}(\pi_{\theta_0}||\pi_\theta) \approx \frac{\sum_{i=1}^{N} \sum_{t=0}^{T_i-1}\nabla_\theta D_{\text{KL}}(\pi_{\theta_0}(\cdot|s_t)||\pi_{\theta}(\cdot|s_t))}{\sum_{i=1}^{N}{T_i}}
\end{equation}

\xhdr{Additional considerations} Generating counterfactual explanation policies using the above gradients poses a significant practical challenge -- the KL term conservatively restricts the policy to the neighborhood of the original policy $\pi_{\theta_0}$ hindering the actual aim of reaching $\rtarget$. 

Intuitively, we can tune the regularization hyperparameter $k$ on the KL-weight \textit{w.r.t.} the $\rtarget$ at the start of the optimization. Nevertheless, we update the policy pivot $\pi_{\theta_0}$ iteratively to $[\pi_{\theta_1}, \pi_{\theta_2}, \dots]$ after every fixed $m$ number of gradient updates of $\pi_\theta$ to ensure it achieves the desired $\rtarget$ return. Finally, at the $i^{\text{th}}$ iteration, after $m$ steps of gradient updates of the objective:
\begin{equation}
    \label{eqn:cf_iterative_defn}
    \pi_{\theta_\text{cf}} = \argmin_{\theta} (|\Jpitheta - \rtarget| + k \cdot D_{\text{KL}}(\pi_{\theta_{i-1}} || \pi_\theta)),
\end{equation}
we change the pivot policy from $\pi_{\theta_{i-1}}$ to $\pi_{\theta_{i}}$ using $\pi_\theta$ at that step. The above process is continued till $|\Jpitheta{-}\rtarget|$ is less than a certain threshold $\delta$. In Algorithm~\ref{alg:pi_cf_estimation}, we describe the complete step-by-step algorithm of \method, our proposed framework.

% Pseudo-code of our counterfactual algorithm
\begin{algorithm}[h]
\footnotesize
\caption{\method: Counterfactual Explanation Policy Optimization}
\label{alg:pi_cf_estimation}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
% \vspace{1mm}

% \Input{} % Define the input variables
\textbf{Given:}~~$\pi_{\theta_{0}}, \rtarget, \delta, k, m, N, \eta $\\

% Initializations go here
$\pi_{\theta'} \xleftarrow[]{} \pi_{\theta_0}.\text{copy}()$ \tcp{Create copy of original policy, which will be updated}
\For{i = $0, 1, \dots$}{
    \For{j = $0, 1, \dots, m - 1$}{
        $\{\tau\}_{i=1}^{N} \sim \pi_{\theta'}$ \tcp{Sample $N$ episodes using policy under updation}

        $J_{\pi_{\theta'}} \xleftarrow[]{} \text{estimatePerformance}(\{\tau\}_{i=1}^{N})$ \tcp{using Equation~\ref{eqn:jpitheta_approx}}
        \tcc{Check if stopping criterion is met}
        \If{$|J_{\pi_{\theta'}} - \rtarget| < \delta$}{
            $\pi_{\theta_\text{cf}} \xleftarrow[]{} \pi_{\theta'}$
            
            \text{Exit both the loops.}
        }

        $
        \nabla_{\theta}L_{\text{cf},\theta} \xleftarrow[]{} \nabla_{\theta}L_{\text{ret}, \theta}\rvert_{\theta=\theta'} + k \cdot \nabla_{\theta} L_{\text{KL}}(\pi_{\theta_i} || \pi_\theta)\rvert_{\theta=\theta'}
        $ \tcp{Compute the objective gradient using Equations~\ref{eqn:polgrad_approx} and ~\ref{eqn:grad_dkl_approx}}
        % $L_{cf, surr} = \mathbb{E}_{\{\tau\}}[-k \cdot \pi_i(s, a) \cdot  log (\pi'(s, a))  + 2 \cdot (J_{\pi'} - \rtarget) \cdot (Q_{\pi'}(s,a) - \rtarget) \cdot log (\pi'(s,a)) ]$
        $\theta' \xleftarrow[]{} \theta' - \eta \cdot \nabla_{\theta}{L_{\text{cf}, \theta}}\rvert_{\theta=\theta'}$ \tcp{Update parameters of $\pi_{\theta'}$}
    }
    
    $\pi_{\theta_{i+1}} \xleftarrow{} \pi_{\theta'}$ \tcp{Update the KL-pivot policy to $\pi_{\theta'}$}
}
% \Output{} % Output of the algorithm
\textbf{return}~~counterfactual policy $\pi_{\theta_\text{cf}}$
\end{algorithm}

