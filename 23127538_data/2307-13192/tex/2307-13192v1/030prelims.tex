\section{Preliminaries}
\label{sec:prelims}

\xhdr{Supervised Learning} Consider a standard classification setup with a given set of training examples, 
\[\mathbb{X} = \{(\mathbf{x}^{(i)}, y^{(i)})~|~\mathbf{x}^{(i)} \in \mathbb{R}^d, y^{(i)} \in C\},\] % i \in \{1, 2, \dots, N\}, C = \{1, 2, \dots, K\}\},\]
where $i \in \{1, 2, \dots, N\}$, $C = \{1, 2, \dots, K\}\}$, $\mathbf{x}^{(i)}$ is a $d$-dimension vector, $N$ is the number of training examples, and $C$ denotes set of $K$ classes. Let a classifier $f$ be trained on $\mathbb{X}$ to predict the correct label for any unseen input $\textbf{x}$.

\xhdr{Counterfactual Explanations}
XAI literature~\citep{cfxai_survey,cfml_survey} defines counterfactual explanations as a technique to analyze  ``what if'' scenarios for model predictions, \eg a counterfactual explanation for a loan application model rejecting an individual's loan application could be ``if you increased your income by \$500, your application would have been accepted''.  Note that in the present work we work in a non-causal setup, where following the previous works~\citep{wachter2017counterfactual}, we define a counterfactual explanation for a given model prediction of input $\mathbf{x}$ as the \textit{minimal variation in the features of $\mathbf{x}$} that changes the prediction of the underlying model from $y$ to $y_{\text{target}}$. 

Formally, in for the aforementioned supervised learning setup,  \textit{counterfactual explanation} of model prediction $\hat{y}_0 = f(\mathbf{x}_0)$ conditioned on a target class $y_{\text{target}} \in C$ is given by,
\begin{equation}
    \mathbf{x}_{\text{cf}} = \argmin_{\mathbf{x}}[ \text{CE}(f(\mathbf{x}),~y_{\text{target}}) + k\cdot \norm{\mathbf{x}_{\text{0}} - \mathbf{x}}_2^2 ],
    \label{eqn:cf_sl}
\end{equation}
% \looseness=-1
where cross-entropy (CE) loss guides explanation for achieving $y_{\text{target}}$ by modifiying $\mathbf{x}_0$ to $\mathbf{x}_{\text{cf}}$ and the mean-squared distance  between $\mathbf{x}_{\text{cf}}$ and $\mathbf{x}_0$ ensures the proximity between the two, regulated via coefficient $k$.

\xhdr{Reinforcement Learning} Consider a finite horizon Markov Decision Process (MDP)~\citep{mdp} defined as $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, d_0, \gamma)$, where $\mathcal{S}$ denotes the state-space,  $\mathcal{A}$ denotes action-space, $P: (\mathcal{S} \times \mathcal{A} \times \mathcal{S}) \xrightarrow{} [0, 1]$ denotes state transition function, $R: (\mathcal{S} \times \mathcal{A} \times \mathcal{S}) \xrightarrow{} \mathbb{R}$ denotes the reward function, $d_0: \mathcal{S} \xrightarrow[]{} [0,1]$ represents distribution over starting states, and $\gamma \in (0, 1]$ denotes the discount factor. Let $\pi: \mathcal{S} \times \mathcal{A} \xrightarrow[]{} [0, 1]$ denote the learnt agent policy. Then, we measure the performance $J_{\pi}$ of the policy in terms of expected return as follows:
\begin{equation}
    J_{\pi} = \mathbb{E}_{(s_0, a_0, s_1, a_1, \dots, s_T)}\Big[\sum_{t=0}^{T-1}{\gamma^{t}r(s_t, a_t, s_{t+1})}\Big],
    \label{eqn:j_pi}
\end{equation}
where $s_0 \sim d_0$, $a_t \sim \pi(a_t | s_t)$, and $T$ denotes the episode terminating time-step.