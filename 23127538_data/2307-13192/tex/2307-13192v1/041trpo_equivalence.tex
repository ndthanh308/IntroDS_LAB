\subsection{Connection between Counterfactual Explanation Policies and Trust Region Methods}
\label{sec:trpo_connection}
Trust region-based RL policy optimization methods like TRPO~\citep{trpo}, ACKTR~\citep{acktr} and PPO~\citep{ppo} have been foundational in the unprecedented success of deep RL~\citep{dota2, gpt4}. The primary aspect behind their optimization involves iteratively updating the policy within a \textit{trust} region, leading to a monotonic improvement in the policy's performance. Formally, the objective is defined as:
\begin{equation}
    \label{eqn:tr_1}
    \pi_{\theta_{i+1}} =  \argmax_{\theta}~~ J_{\pi_\theta}\text{~~such that~~} D_{\text{KL}}(\pi_{\theta_i} || \pi_{\theta}) \leq \delta
\end{equation}
The above objective can be written in its Lagrangian form using penalty instead of the constraint as: 
\begin{equation}
    \label{eqn:tr_2}
    \begin{aligned}
    \pi_{\theta_{i+1}} =  {} & \argmax_{\theta}~~ J_{\pi_\theta} - \lambda\cdot D_{\text{KL}}(\pi_{\theta_i} || \pi_{\theta}), \\
    {} & \text{where $\lambda$ is treated as a hyper-parameter.}
    \end{aligned}
\end{equation}

Next, we show the equivalence between counterfactual explanation policy and the policy obtained using a trust region-based policy gradient method.
\begin{theorem}(Equivalence)
\label{thm:equivalence}
Let $\rtarget$ be the maximum possible return in the MDP under consideration. Then, for any $L_{\text{ret}}$ estimated using $\ell_1$-norm, the generated counterfactual explanation policy through iterative KL-pivoting is equivalent to optimizing the policy for best return using a trust region-based policy gradient method.
\end{theorem}
\begin{proof}
For a given $\rtarget$, let us assume that the desired return from a policy is equal to the maximum possible return, \ie $\rtarget = R_{\text{max}}$, and the return penalty calculated using $\ell_1$-norm, we rewrite the counterfactual generation objective using KL-pivoting~ Eqn.~\ref{eqn:cf_iterative_defn} as:
\begin{equation}
\label{eqn:proof_1}
    \pi_{\theta_{\text{cf}}} = \argmin_{\theta} (|J_{\pi_\theta} - R_{\text{max}}| + k \cdot D_{\text{kl}}(\pi_{\theta_i} || \pi_\theta)) 
\end{equation}
We have $J_{\pi_\theta}=\mathbb{E}_{\pi_\theta}[\sum_{t=0}^{T-1}\gamma^t r(s_t, a_t, s_{t+1})| s_0 {\sim} d_0, a_t \sim \pi_\theta(\cdot| s_t))]$ as defined in Eq.~\ref{eqn:j_pi}. Now, let $R{=}\sum_{t=0}^{T-1}\gamma^t r(s_t, \\a_t, s_{t+1})$ denote the discounted return for the episode $(s_0, a_0, s_1, a_1, \dots, s_T)$, then $J_{\pi_\theta}=\mathbb{E}_{\pi_\theta}[R]$. Further,
\[ 
    R \leq R_\text{max} \implies J_{\pi_\theta} = \mathbb{E}_{\pi_\theta}[R] \leq \mathbb{E}_{\pi_\theta}[R_\text{max}] = R_\text{max}
\]
Hence, $J_{\pi_\theta} - R_\text{max} \leq 0$, or $R_\text{max} - J_{\pi_\theta} \geq 0$, which allows us to write $|J_{\pi_\theta} - R_\text{max}|$ simply as $(R_\text{max} - J_{\pi_\theta})$. Rewriting Eqn.~\ref{eqn:proof_1}, we get:
\begin{equation}
\label{eqn:proof_2}
\begin{aligned}
\pi_{\theta_{\text{cf}}}
= {} & \argmin_{\theta}~~ R_\text{max} - J_{\pi_\theta} + k \cdot D_{\text{KL}}(\pi_{\theta_i} || \pi_\theta) \\
= {} & \argmin_{\theta}~~ - J_{\pi_\theta} + k \cdot D_{\text{KL}}(\pi_{\theta_i} || \pi_\theta) \\
= {} & \argmax_{\theta}~~ J_{\pi_\theta} - k \cdot D_{\text{KL}}(\pi_{\theta_i} || \pi_\theta)
\end{aligned}
\end{equation}
% \looseness=-1
Comparing the above equations with Eqn.~\ref{eqn:tr_2} and treating the hyper-parameters $k$ and $\lambda$ interchangeably, we find that both these objectives of policy updation are the same, which completes our proof.
\end{proof}
Consequently, the trust region-based policy gradient methods can be interpreted as finding a \textit{counterfactual explanation policy} of the original policy, which achieves the maximum possible return (\ie the best performance). This also opens up possibility for using sophisticated ways to choose distance regulation parameter $k$ similar to $\lambda$ following ~\cite{cpi}.