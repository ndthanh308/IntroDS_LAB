\section{Conclusion and Discussion}
\label{sec:conclusion}
In this work, we present a systematic framework for contrastive analysis of reinforcement learning policies. In doing so, we propose \method for generating counterfactual explanations for a given RL policy. We  demonstrate the flexibility of \method across multiple RL benchmarks and find insights into RL policy learning. We carry out a detailed theoretical analysis to connect the counterfactual estimation with eminent trust region optimization methods in RL. Further, results across five OpenAI gym environments show that \method generates faithful counterfactuals for (un)learning skills while keeping close to the original policy. To the best of our knowledge, our work presents the first technique to generate counterfactual explanation policies. Overall, we believe our work paves the way toward a new direction in the methodical contrastive analysis of RL policies and brings more transparency to RL decision-making. In our present work, we mainly used on-policy policy gradient-based RL optimization techniques, which are not very sample efficient, and we assume the stochastic nature of the policies which might not be the case always. It would be interesting to explore the utility of \method in unlearning options (skills) in hierarchical RL~\citep{hrl,hrl_survey}. Also, \method gives a glimpse into return contours in RL which could be further explored for enhanced optimization.