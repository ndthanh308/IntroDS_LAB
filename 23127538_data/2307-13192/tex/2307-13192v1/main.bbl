\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{tensorflow}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv}, 2016.

\bibitem[Asadi et~al.(2023)Asadi, Fakoor, Gottesman, Kim, Littman, and
  Smola]{asadi2023faster}
Asadi, K., Fakoor, R., Gottesman, O., Kim, T., Littman, M.~L., and Smola, A.~J.
\newblock Faster deep reinforcement learning with slower online network, 2023.

\bibitem[Barto \& Mahadevan(2003)Barto and Mahadevan]{hrl}
Barto, A.~G. and Mahadevan, S.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock \emph{Discrete event dynamic systems}, 2003.

\bibitem[Bellman(1957)]{mdp}
Bellman, R.
\newblock A markovian decision process.
\newblock \emph{Journal of mathematics and mechanics}, 1957.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{dota2}
Berner, C., Brockman, G., Chan, B., Cheung, V., D{\k{e}}biak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv}, 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openaigym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv}, 2016.

\bibitem[Coppens et~al.(2019)Coppens, Efthymiadis, Lenaerts, Now{\'e}, Miller,
  Weber, and Magazzeni]{coppens2019distilling}
Coppens, Y., Efthymiadis, K., Lenaerts, T., Now{\'e}, A., Miller, T., Weber,
  R., and Magazzeni, D.
\newblock Distilling deep reinforcement learning policies in soft decision
  trees.
\newblock In \emph{IJCAI workshop on Explainable Artificial Intelligence},
  2019.

\bibitem[Degris et~al.(2012)Degris, White, and Sutton]{a2c_1}
Degris, T., White, M., and Sutton, R.~S.
\newblock Off-policy actor-critic.
\newblock \emph{arXiv}, 2012.

\bibitem[Deshmukh et~al.()Deshmukh, Dasgupta, Krishnamurthy, Jiang, Agarwal,
  Theocharous, and Subramanian]{deshmukhexplaining}
Deshmukh, S.~V., Dasgupta, A., Krishnamurthy, B., Jiang, N., Agarwal, C.,
  Theocharous, G., and Subramanian, J.
\newblock Explaining rl decisions with trajectories.
\newblock In \emph{ICLR}.

\bibitem[Ding et~al.(2022{\natexlab{a}})Ding, Wei, Zhang, and
  Jovanovic]{pmlr-v162-ding22b}
Ding, D., Wei, C.-Y., Zhang, K., and Jovanovic, M.
\newblock Independent policy gradient for large-scale {M}arkov potential games:
  Sharper rates, function approximation, and game-agnostic convergence.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{ICML}, volume 162 of \emph{PMLR}. PMLR, 17--23 Jul
  2022{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v162/ding22b.html}.

\bibitem[Ding et~al.(2022{\natexlab{b}})Ding, Nakai, and Gong]{protein}
Ding, W., Nakai, K., and Gong, H.
\newblock Protein design via deep learning.
\newblock \emph{Briefings in bioinformatics}, 2022{\natexlab{b}}.

\bibitem[Fawzi et~al.(2022)Fawzi, Balog, Huang, Hubert, Romera-Paredes,
  Barekatain, Novikov, R~Ruiz, Schrittwieser, Swirszcz, et~al.]{matrix}
Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain,
  M., Novikov, A., R~Ruiz, F.~J., Schrittwieser, J., Swirszcz, G., et~al.
\newblock Discovering faster matrix multiplication algorithms with
  reinforcement learning.
\newblock \emph{Nature}, 2022.

\bibitem[Gershman(2017)]{causal_rl}
Gershman, S.~J.
\newblock Reinforcement learning and causal models.
\newblock \emph{The Oxford handbook of causal reasoning}, 2017.

\bibitem[Greydanus et~al.(2018)Greydanus, Koul, Dodge, and
  Fern]{greydanus2018visualizing}
Greydanus, S., Koul, A., Dodge, J., and Fern, A.
\newblock Visualizing and understanding atari agents.
\newblock In \emph{ICML}, 2018.

\bibitem[Hirano et~al.(2022)Hirano, Sakaji, and Izumi]{hirano2022policy}
Hirano, M., Sakaji, H., and Izumi, K.
\newblock Policy gradient stock gan for realistic discrete order data
  generation in financial markets, 2022.

\bibitem[Iyer et~al.(2018)Iyer, Li, Li, Lewis, Sundar, and
  Sycara]{iyer2018transparency}
Iyer, R., Li, Y., Li, H., Lewis, M., Sundar, R., and Sycara, K.
\newblock Transparency and explanation in deep reinforcement learning neural
  networks.
\newblock In \emph{AIES}, 2018.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{ICML}. PMLR, 2016.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{cpi}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, 2002.

\bibitem[Kakade(2001)]{polgrad}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{NeurIPS}, 2001.

\bibitem[Karimi et~al.(2020)Karimi, Barthe, Balle, and Valera]{karimi2019model}
Karimi, A.-H., Barthe, G., Balle, B., and Valera, I.
\newblock Model-agnostic counterfactual explanations for consequential
  decisions.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Khan et~al.(2020)Khan, Tolstaya, Ribeiro, and
  Kumar]{pmlr-v100-khan20a}
Khan, A., Tolstaya, E., Ribeiro, A., and Kumar, V.
\newblock Graph policy gradients for large scale robot control.
\newblock In Kaelbling, L.~P., Kragic, D., and Sugiura, K. (eds.),
  \emph{Proceedings of the Conference on Robot Learning}, PMLR. PMLR, 30
  Oct--01 Nov 2020.
\newblock URL \url{https://proceedings.mlr.press/v100/khan20a.html}.

\bibitem[Koedinger et~al.(2013)Koedinger, Brunskill, Baker, McLaughlin, and
  Stamper]{tutor}
Koedinger, K.~R., Brunskill, E., Baker, R.~S., McLaughlin, E.~A., and Stamper,
  J.
\newblock New potentials for data-driven intelligent tutoring system
  development and optimization.
\newblock \emph{AI Magazine}, 2013.

\bibitem[Lu et~al.(2023)Lu, Qiu, Chang, Wu, Zhu, Rajpurohit, Clark, and
  Kalyan]{lu2023dynamic}
Lu, P., Qiu, L., Chang, K.-W., Wu, Y.~N., Zhu, S.-C., Rajpurohit, T., Clark,
  P., and Kalyan, A.
\newblock Dynamic prompt learning via policy gradient for semi-structured
  mathematical reasoning, 2023.

\bibitem[Maggipinto et~al.(2020)Maggipinto, Susto, and Chaudhari]{9341559}
Maggipinto, M., Susto, G.~A., and Chaudhari, P.
\newblock Proximal deterministic policy gradient.
\newblock In \emph{IROS}, 2020.
\newblock \doi{10.1109/IROS45743.2020.9341559}.

\bibitem[Mahajan et~al.(2019)Mahajan, Tan, and Sharma]{mahajan2019preserving}
Mahajan, D., Tan, C., and Sharma, A.
\newblock Preserving causal constraints in counterfactual explanations for
  machine learning classifiers.
\newblock \emph{arXiv}, 2019.

\bibitem[Melo et~al.(2008)Melo, Meyn, and Ribeiro]{fun_approx_2}
Melo, F.~S., Meyn, S.~P., and Ribeiro, M.~I.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, 2008.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{atari_rl}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv}, 2013.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and
  Nguyen]{unlearning_survey}
Nguyen, T.~T., Huynh, T.~T., Nguyen, P.~L., Liew, A. W.-C., Yin, H., and
  Nguyen, Q. V.~H.
\newblock A survey of machine unlearning.
\newblock \emph{arXiv}, 2022.

\bibitem[Nitanda(2014)]{NIPS2014_d554f7bb}
Nitanda, A.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and
  Weinberger, K. (eds.), \emph{NeurIPS}. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2014/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf}.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Parikh et~al.(2014)Parikh, Boyd, et~al.]{parikh2014proximal}
Parikh, N., Boyd, S., et~al.
\newblock Proximal algorithms.
\newblock \emph{Foundations and trends{\textregistered} in Optimization}, 2014.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{pytorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Pateria et~al.(2021)Pateria, Subagdja, Tan, and Quek]{hrl_survey}
Pateria, S., Subagdja, B., Tan, A.-h., and Quek, C.
\newblock Hierarchical reinforcement learning: A comprehensive survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 2021.

\bibitem[Puiutta \& Veith(2020)Puiutta and Veith]{puiutta2020explainable}
Puiutta, E. and Veith, E.~M.
\newblock Explainable reinforcement learning: A survey.
\newblock In \emph{Machine Learning and Knowledge Extraction: 4th IFIP TC 5, TC
  12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE
  2020, Dublin, Ireland, August 25--28, 2020, Proceedings 4}. Springer, 2020.

\bibitem[Puri et~al.(2019)Puri, Verma, Gupta, Kayastha, Deshmukh,
  Krishnamurthy, and Singh]{puri2019explain}
Puri, N., Verma, S., Gupta, P., Kayastha, D., Deshmukh, S., Krishnamurthy, B.,
  and Singh, S.
\newblock Explain your move: Understanding agent actions using specific and
  relevant feature attribution.
\newblock \emph{arXiv}, 2019.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{JMLR}, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1364.html}.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{ICML}. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv}, 2017.

\bibitem[Sekhari et~al.(2021)Sekhari, Acharya, Kamath, and Suresh]{unlearning}
Sekhari, A., Acharya, J., Kamath, G., and Suresh, A.~T.
\newblock Remember what you want to forget: Algorithms for machine unlearning.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{go_rl}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 2016.

\bibitem[Stepin et~al.(2021)Stepin, Alonso, Catal{\'a}, and
  Pereira-Fari{\~n}a]{cfxai_survey}
Stepin, I., Alonso, J.~M., Catal{\'a}, A., and Pereira-Fari{\~n}a, M.
\newblock A survey of contrastive and counterfactual explanation generation
  methods for explainable artificial intelligence.
\newblock \emph{IEEE Access}, 2021.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton_book}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{fun_approx_1}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{NeurIPS}, 1999.

\bibitem[Theocharous et~al.(2015)Theocharous, Thomas, and
  Ghavamzadeh]{marketing}
Theocharous, G., Thomas, P.~S., and Ghavamzadeh, M.
\newblock Ad recommendation systems for life-time value optimization.
\newblock In \emph{WWW}, 2015.

\bibitem[Ustun et~al.(2019)Ustun, Spangher, and Liu]{Ustun2019ActionableRI}
Ustun, B., Spangher, A., and Liu, Y.
\newblock Actionable recourse in linear classification.
\newblock In \emph{FAacT}, 2019.

\bibitem[Van~Looveren \& Klaise(2019)Van~Looveren and
  Klaise]{van2019interpretable}
Van~Looveren, A. and Klaise, J.
\newblock Interpretable counterfactual explanations guided by prototypes.
\newblock \emph{arXiv}, 2019.

\bibitem[Verma et~al.(2018)Verma, Murali, Singh, Kohli, and
  Chaudhuri]{verma2018programmatically}
Verma, A., Murali, V., Singh, R., Kohli, P., and Chaudhuri, S.
\newblock Programmatically interpretable reinforcement learning.
\newblock In \emph{ICML}. PMLR, 2018.

\bibitem[Verma et~al.(2020{\natexlab{a}})Verma, Dickerson, and
  Hines]{verma2020counterfactual}
Verma, S., Dickerson, J., and Hines, K.
\newblock Counterfactual explanations for machine learning: A review.
\newblock \emph{arXiv}, 2020{\natexlab{a}}.

\bibitem[Verma et~al.(2020{\natexlab{b}})Verma, Dickerson, and
  Hines]{cfml_survey}
Verma, S., Dickerson, J.~P., and Hines, K.~E.
\newblock Counterfactual explanations for machine learning: A review.
\newblock \emph{ArXiv}, abs/2010.10596, 2020{\natexlab{b}}.

\bibitem[Wachter et~al.(2018)Wachter, Mittelstadt, and
  Russell]{wachter2017counterfactual}
Wachter, S., Mittelstadt, B., and Russell, C.
\newblock Counterfactual explanations without opening the black box: automated
  decisions and the gdpr.
\newblock \emph{Harvard Journal of Law \& Technology}, 31\penalty0 (2), 2018.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{acktr}
Wu, Y., Mansimov, E., Grosse, R.~B., Liao, S., and Ba, J.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock \emph{NeurIPS}, 2017.

\end{thebibliography}
