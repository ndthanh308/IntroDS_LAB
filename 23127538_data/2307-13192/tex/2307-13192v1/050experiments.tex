\section{Experiments and Results}
\label{sec:expts}
Here, we study the counterfactual explanations of policies trained using policy gradient methods, particularly the actor-critic methods~\citep{sutton_book}, as they converge more faithfully compared to vanilla policy gradients and also maintain the simplicity required to analyze them in a contrastive fashion rigorously.

\xhdr{Setup} We employ the widely used actor-critic algorithm of Advantage Actor-Critic (A2C)~\citep{a2c_1}, and only in the case of complex environments, we shift to Proximal Policy Optimization (PPO)~\citep{ppo} for training the original policies in our empirical analysis. We use the standard implementations provided in stable-baselines library~\footnote{Documentation for the library: \url{https://stable-baselines3.readthedocs.io/en/master/}}~\citep{stable-baselines3} for RL training using the above-mentioned algorithms. We conduct our analysis in five OpenAI gym environments~\citep{openaigym}, \textit{viz.} i) \textit{CartPole-v1}, ii) \textit{Acrobot-v1}, iii) \textit{Pendulum-v1}, iv) \textit{LunarLander-v2} and v) \textit{BipedalWalker-v3}~\footnote{Additional environment details can be found at \url{https://gymnasium.farama.org/environments/}}. 

\xhdr{Implementation details}
We set the threshold return values of $\delta$ for the five environments to $\{10, 2.5, 37.5, 5, 10\}$ according to the order of magnitude of returns in their respective environment,  the number of KL-pivoting policy iterations $m$ as 10 for all environments (except BipedalWalker, where $m=5$), and the number of trajectory rollouts $N$ to 10 for all four environments except BipedalWalker, where $N=2$. Further, we chose the distance regularizer parameter $k$ values as $\{10, 1, 10^5, 10, 1\}$ for the five environments in the same order as discussed above. For computation purposes, we use a single NVIDIA A100 (40GB) GPU. We share the code for generating counterfactual explanation policies using \method in the supplementary material.
\begin{table*}[ht]
\fontsize{7.5pt}{7.5pt}\selectfont
\centering
\setlength{\tabcolsep}{0.5pt}
\renewcommand{\arraystretch}{1.3}
\caption{\textbf{\method Optimization on Control Environments.} Counterfactual explanations for a given policy $\pi_0$ with performance $J_{\pi_0}$ and three target returns. Shown are the number of outer policy updates ($n_{\pi}$) for generating counterfactuals and the resulting performance of the counterfactual policy $\pi_{\theta_{cf}}$. \method faithfully estimates the counterfactuals across all the $\pi_0$-$\rtarget$ pairs and diverse environments.
}
\label{tab:control_env_results}
\begin{tabular}{lccccccccc}
\toprule
\multicolumn{10}{c}{CartPole-v1} \\
\cmidrule{1-10}
$J_{\pi_{\theta_o}}$ & \multicolumn{3}{c}{235.6} & \multicolumn{3}{c}{368.2} & \multicolumn{3}{c}{500.0} \\ 
$R_\text{target}$ & 50 & 250 & 450 & 50 & 250 & 450 & 50 & 250 & 450 \\
$n_{\pi}$ & 278.0\std{22.9} & 10.0\std{6.5} & 303.3\std{67.1} & 108.0\std{16.6} & 1340.0\std{278.8} & 68.7\std{5.0} & 833.3\std{56.3} & 721.3\std{14.7} & 557.3\std{10.0} \\
\rowcolor{gray!30}
$J_{\pi_{\theta_\text{cf}}}$ & 48.6\std{4.9} & 245.0\std{4.4} & 450.6\std{7.5} & 48.2\std{3.4} & 245.5\std{1.9} & 453.5\std{0.5} & 56.4\std{2.4} & 246.0\std{4.7} & 452.1\std{1.0} \\
\midrule
\multicolumn{10}{c}{Acrobot-v1} \\
\cmidrule{1-10}
$J_{\pi_{\theta_o}}$ & \multicolumn{3}{c}{-146.7} & \multicolumn{3}{c}{-89.0} & \multicolumn{3}{c}{-84.3} \\
$R_\text{target}$ & -120 & -100 & -80 & -120 & -100 & -80 & -120 & -100 & -80 \\
$n_{\pi}$ & 44.0\std{18.8} & 31.3\std{1.9} & 18.0\std{12.8} & 62.0\std{34.8} & 38.0\std{37.3} & 6.0\std{2.8} & 117.3\std{95.3} & 26.0\std{36.8} & 8.7\std{5.7} \\
\rowcolor{gray!30}
$J_{\pi_{\theta_\text{cf}}}$ & -119.5\std{1.3} & -100.5\std{2.1} & -80.6\std{1.6} & -119.1\std{0.5} & -99.9\std{0.2} & -80.6\std{0.5} & -120.1\std{1.5} & -99.8\std{1.8} & -81.6\std{0.3} \\
\midrule
\multicolumn{10}{c}{Pendulum-v1} \\
\cmidrule{1-10}
$J_{\pi_{\theta_o}}$ & \multicolumn{3}{c}{-853.5} & \multicolumn{3}{c}{-792.6} & \multicolumn{3}{c}{-568.0} \\
$R_\text{target}$ & -1000 & -750 & -500 & -1000 & -750 & -500 & -1000 & -750 & -500 \\
$n_{\pi}$ & 1.3\std{0.9} & 15.3\std{14.8} & 174.7\std{38.0} & 6.0\std{3.3} & 10.0\std{11.4} & 198.7\std{157.4} & 113.3\std{139.1} & 22.0\std{25.7} & 6.0\std{3.1} \\
\rowcolor{gray!30}
$J_{\pi_{\theta_\text{cf}}}$ & -996.9\std{19.6} & ~-773.5\std{6.4} & ~-507.9\std{19.7} & ~-992.1\std{25.9} & -777.1\std{6.2} & -507.3\std{7.1} & -997.6\std{15.1} & -764.9\std{19.8} & ~-501.0\std{8.8} \\
\bottomrule
\end{tabular}
\end{table*}
% Figure environment removed
\subsection{Results on \method Optimization}
\looseness=-1
We conduct experiments to understand and verify the optimization of our proposed \method framework. In doing so, we present the results of \method optimization on A2C-trained agents of \textit{CartPole-v1}, \textit{Acrobot-v1}, and \textit{Pendulum-v1} environments. We sample three distinct policy checkpoints from the A2C training of each RL environment and generate counterfactual policies using \method for these checkpoints and three different target returns $\rtarget$ (chosen with respect to the RL environment). We train each tuple of the RL environment, A2C checkpoint, and target return for three different seeds to reduce variance arising from stochasticity. Our results in Table~\ref{tab:control_env_results} show that \method faithfully achieves target returns for diverse starting checkpoints across all environments, \ie \method converges to the target return value generating $\pi_{\text{cf}}$ that obtains return very close to the given $\rtarget$. Further, we find an intuitive trend in the number of outer policy (KL-pivoting) updates, where we 
observe a lesser number of outer policy updates when the $\rtarget$ value is closer to the performance of the original policy $J_{\pi_{\theta_0}}$. Our results on these standard control environments form the basis for further investigation of more complex environments having a discrete or a continuous action space, which we explore in the next section.

\subsection{Contrastive Insights into RL policies}
% using \method}
Next, we present our analysis on generating counterfactual explanations using more complex environments.

\looseness=-1
\xhdr{1) Lunar Lander} We train an RL agent on \textit{LunarLander-v2 }using A2C, and intervene the training to retrieve the policy $\pi_0$ for our contrastive analysis. The original policy, on average, achieves a return of 50 (refer Figure~\ref{fig:lunarlander_quantitative}). We then generate its counterfactuals for target returns of $\{100, 150\}$ to understand how the policy can be improved further and also for target returns of $\{0, -50\}$ to understand how the policy can become worse. We present landing scenarios of the policy $\pi_0$ on three different surfaces in Figure~\ref{fig:lunarlander_qualitative} and their respective contrastive explanations for improvement and deterioration. Our results for $\pi_0$ rollouts show interesting agent characteristics like ``slow start'', ``a quick fall after covering half the distance,'' and ``landing near the right flag''. We find an improved version of the original policy in the generated counterfactuals with $\rtarget=100$ (Fig.~\ref{fig:lunarlander_qualitative}; column 2), where we observe that a uniform descent and shifting the landing slightly to the left between the two flags can improve the original policy $\pi_0$. Further, the counterfactual with  $\rtarget=150$ (Fig.~\ref{fig:lunarlander_qualitative}; column 3) shows uniform descent with decelerated landing can lead to even higher gains. In contrast, $\rtarget=0$ (Fig.~\ref{fig:lunarlander_qualitative}; column 4) shows how by starting very fast and making \textit{free fall} before landing outside the space between flags, $\pi_0$ can go worse. Similarly, $\rtarget=-50$ (Fig.~\ref{fig:lunarlander_qualitative}; column 5) show how the policy could worsen/collapse by making the agent land further right. Notably, the generated counterfactuals for targeted deterioration of performance using \method can be interpreted as a robust way to unlearn~\citep{unlearning, unlearning_survey} RL skills as it might be required to forget certain aspects of learning(\eg in Fig.~\ref{fig:lunarlander_qualitative}, the slow start of $\pi_0$).

\xhdr{2) Bipedal Walker} For the BipedalWalker-v3 environment, we train a PPO agent that demonstrates early success in achieving walking behavior. We conduct experiments to analyze the trained Bipedal agent contrastively by estimating counterfactuals at target returns of 50 and 150 (refer Fig.~\ref{fig:bipedal_convergence}). We demonstrate the qualitative results in Figure~\ref{fig:bipedal_qualitative}, where we find that the given policy $\pi_0$ has a peculiar walk, kneeling on the right leg and taking stride with the left one. In contrast, our generated counterfactual policy using $\rtarget=150$ shows improved (upright and faster) walk of the Bipedal agent. Further, when we reduce the target return to $\rtarget=50$, we observe that the kneeling gets intensified and the agent starts to drag itself to the finishing line, making the agent to walk slow and also fall (as shown in Scenario 1).

Across both environments, we observe that \method generates counterfactual policies that look similar to the original policy, keeping the essential characteristics of the

% Figure environment removed
policy the same. This enables us to easily contrast between different versions of the same policies.
% Figure environment removed