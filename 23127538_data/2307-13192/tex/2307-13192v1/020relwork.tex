\section{Related Work}
\label{sec:related_work}
This work lies at the intersection of counterfactual explanations, explainability in RL, and proximal gradients. Next, we discuss the related work for each of these topics.

% \looseness=-1
\xhdr{Counterfactual Explanations} Several techniques have been recently proposed to generate counterfactual explanations for providing recourses to individuals negatively impacted by complex black-box model decisions~\citep{wachter2017counterfactual,Ustun2019ActionableRI,van2019interpretable,mahajan2019preserving,karimi2019model}. In particular, these techniques aim to provide model explanations in the form of minimal changes to an input instance that changes the original model. Broadly, these methods are categorized based on access to the predictive model (white- vs. black-box), enforce sparsity (more interpretable explanations), and whether the counterfactuals belong to the original data manifold~\cite{verma2020counterfactual}. To this end, ~\citet{wachter2017counterfactual} is one of the most widely used methods that use a gradient-based method to obtain counterfactual explanations for models using a distance-based penalty. In this work, we extend the counterfactual explanations to RL policies. 

\xhdr{Explainability in RL} Explainable RL (XRL)~\citep{puiutta2020explainable} methods are a sub-field of explainable artificial intelligence (XAI) that aims to interpret and explain the complex behaviour of RL agents. Recent works in XRL are broadly categorized into i) gradient-based methods~\citep{greydanus2018visualizing}, which analyzed Atari RL agents that use raw visual input to make their decision and identify salient information in the image that the policy uses to select an action, ii) attribution-based methods~\citep{deshmukhexplaining, puri2019explain, iyer2018transparency}, which explain an agentâ€™s behaviour by attributing its actions to global or local observation data, and iii) surrogate-based methods, which distill complex RL policy into simpler models such as decision trees~\citep{coppens2019distilling} or to human understandable decision language~\citep{verma2018programmatically}. While all existing XRL methods generate explanations using state features or trajectories, none explore counterfactual explanations. To the best of our knowledge, for the first time, we explore counterfactual explanation policies for contrastive analysis of policies in terms of what minimal change to the policy would get us to desired output returns.

% \looseness=-1
\xhdr{Proximal Gradient Methods in Optimization} General idea behind proximal gradient methods is to solve an optimization problem $\min_x f(x)$ in the neighborhood of certain $x_{k-1}$ at $k^{th}$ optimization iteration by modifying the objective as $\min_x f(x) + \lambda \cdot \norm{x - x_{k-1}}_p$~\citep{parikh2014proximal,NIPS2014_d554f7bb}. This objective modification is termed as proximal operator. In machine learning, these operators find great significance in regularization, constraint-based optimization, convex structuring of objectives, etc. They have several applications in a multitude of fields ranging from risk-aware forecasting systems, and mathematical reasoning to reinforcement learning~\citep{9341559,asadi2023faster, pmlr-v162-ding22b, hirano2022policy, pmlr-v100-khan20a, lu2023dynamic}. One major advantage of proximal gradient methods is in their ability to handle noisy and incomplete data, which is common in real-world applications. Subsequently, in RL, they have been referred by state-of-the-art algorithms like PPO~\citep{ppo} to develop ways to achieve monotonic gains. In this light and in contrast, we incorporate a proximal operator to ensure counterfactual RL policies are closer to the original input policy.