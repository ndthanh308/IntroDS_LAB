\section{Introduction}
\label{sec:intro}
Reinforcement learning (RL) has been used successfully to train autonomous agents capable of achieving better than human-level performance in simulated environments like Go~\citep{go_rl} and a suite of Atari games~\citep{atari_rl}. They are increasingly finding new applications across computational analysis~\citep{matrix},  marketing~\citep{marketing},  education~\citep{tutor}, and biomedical research~\citep{protein} and. Recent breakthroughs in large-language models (LLMs)~\citep{gpt4} are primarily attributed to key RL components that have improved the generative capability of state-of-the-art LLMs. With RL frameworks being deployed at scale as well as performing autonomously, it becomes imperative to incorporate explainability in them, resulting in increased user trust in autonomous decision-making. Explaining the decisions of black-box RL agents for a given environment state is non-trivial as it not only involves explaining the final agent action but also includes complex decision-making and planning behind the output action.

A myriad of RL explainability methods with various attribution techniques has recently been proposed~\citep{greydanus2018visualizing,deshmukhexplaining,iyer2018transparency,puri2019explain}. In particular, they focus on identifying input states and past experiences (trajectories) that led the RL agent to learn complex behaviors. While these methods output important input state features (agent's observation) and trajectories, they fail to explain the minimal change in the trained policy leading to a desired outcome or (un)learning of a specific skill. Intuitively, this requires generating \textit{counterfactuals} for a given desired outcome (\ie identifying \textit{what} and \textit{how much} to change a given RL policy to obtain a target return for its current state). While some previous works have explored causal reinforcement learning~\citep{causal_rl}, there is little to no research on systematically explaining the mechanism of the complex policies learned by a given RL agent using counterfactual explanations.

\xhdr{Present Work} We propose \method, a framework for counterfactual analysis of RL policies. In our framework, we generate explanations by asking the question: ``What least change to the current policy would improve or worsen it to a new policy with a specified target return?'' To estimate such counterfactual policies, we present an objective that aims to obtain a new policy with an average performance equal to that of a specified return while limiting its modifications with respect to the given policy. The generated policies provide direct insights into how a policy can be modified to achieve better results as well as what to avoid in order not to deteriorate the performance. Further, we theoretically prove the connection between popular trust region-based optimization methods in RL with \method, bringing a new perspective of looking at RL optimization using a prominent explainability tool. Formally, the \method learns minimal changes in the current policy without changing its general behavior. To optimize counterfactual explanation policies, we specify a novel objective function that can be solved using basic on-policy Monte Carlo policy gradients. In our experiments across diverse RL environments, we show how our algorithm reliably achieves counterfactual for any the set target return for a given policy.

\xhdr{Our Contributions} We present our contributions as follows: 1) We formalize the problem of counterfactual explanation policy for explaining RL policies. 2) We propose \method, an explanatory framework for generating contrastive explanations for RL policies that identify \textit{minimal} changes in the current policy, which would lead to improving/worsening the performance of a given policy. 3) We derive a theoretical equivalence between the \method objective with the widely used trust region-based policy gradient methods. 4) We demonstrate the flexibility of \method through empirical evaluations of explanations generated for five OpenAI gym environments. Qualitative and quantitative results show that \method successfully generates a counterfactual policy for (un)learning skills while keeping close to the original policy.