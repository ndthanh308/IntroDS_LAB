{
  "title": "Counterfactual Explanation Policies in RL",
  "authors": [
    "Shripad V. Deshmukh",
    "Srivatsan R",
    "Supriti Vijay",
    "Jayakumar Subramanian",
    "Chirag Agarwal"
  ],
  "submission_date": "2023-07-25T01:14:56+00:00",
  "revised_dates": [],
  "abstract": "As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "primary_category": "cs.AI",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13192",
  "pdf_url": null,
  "comment": "ICML Workshop on Counterfactuals in Minds and Machines, 2023",
  "num_versions": null,
  "size_before_bytes": 6802520,
  "size_after_bytes": 166036
}