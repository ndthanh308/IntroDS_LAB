
We have introduced two methods to determine Taylor series (and consequently, derivatives) of
expectation values. 
In the reweighting based method (section~\ref{sec:reweighting}) the
samples obtained by the Monte Carlo method are corrected by the
reweighting factors, eq.~(\ref{eq:rwformula}), to take into account the
dependence on the parameters $\tilde \theta$. 
On the other hand, the Hamiltonian approach (section~\ref{sec:nspt})
produces samples that automatically carry the dependence on the
parameters $\tilde \theta$. It is instructive to see the relation of
each method with the reparametrization trick. 
In order to get some intuition,  we can examine a very simple toy model. 
Imagine that we are interested in the distribution function
\begin{equation}
  \label{eq:Gd}
  p_\sigma(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}}
\end{equation}
and in how expectation values depend on $\sigma$ around $\sigma=1$. 
Of course, this can be trivially solved using a change of variables.
If $\{x_A\}_{A=1}^{N}$ are samples for $ \sigma=1$ then
\begin{equation}
  y_A = \sigma x_A\,,
\end{equation}
are samples for any other value of sigma. 
In particular one can write the truncated polynomial
\begin{equation}
  \label{eq:changeG}
  \tilde x_A = x_A + x_A\epsilon\,, \quad (\epsilon = \sigma-1)\,.
\end{equation}
and now evaluating any expectation value using $\{\tilde x_A\}$ as samples
will produce its corresponding Taylor series. 
In this sense the change of variables can be seen as a particular
transformation to the samples (from $x_A$ to $\tilde x_A$) such that
expectation values evaluated with these samples $\tilde x_A$ give
automatically Taylor series of observables. 

Now consider the reweighting approach to this simple problem. 
One would define $\tilde \sigma = 1+\epsilon$, and determine the
reweighing factors Eq.~(\ref{eq:rwformula}). They read
\begin{equation}
  \tilde w^\alpha(\epsilon) = e^{- \frac{(x^\alpha)^2}{2} \left[ \frac{1}{(1+\epsilon)^2} - 1 \right]    }
\end{equation}
On the other hand, if one performs the change of variables
Eq.~(\ref{eq:changeG}) \emph{before} applying the reweighting formula,
the reweighting factors are given by
\begin{equation}
  \tilde w^\alpha(\epsilon) = e^{- \frac{(x^\alpha)^2}{2} \left[ \frac{1}{(1+\epsilon)^2} - \frac{1}{(1+\epsilon)^2}  \right]    -\log (1+\epsilon)} = \frac{1}{1+\epsilon}\,.
\end{equation}
Note that these reweighting factors are constant (i.e. 
independent on $x$). 
They cancel from the computation of the any expectation value:
\begin{equation}
  \langle O(x) \rangle \approx \frac{\sum_\alpha \tilde w^\alpha O_\alpha}{\sum_\alpha \tilde w^\alpha}\ = \frac{1}{N}\sum_\alpha O_\alpha\,, \qquad \left( O_\alpha = O(\tilde x_\alpha) \right)\,.
\end{equation}

One can therefore see the reparametrization trick as a particular
application of the general reweighting formula, where the change of
variables leads to constant reweighting factors.

We claim that the Hamiltonian approach is just a method to find this
change of variables for complicated distributions and to any order.  
In order to see how this happens, we need to work out the solution of
the equations of motion for our toy model Eq.~(\ref{eq:Gd}).
They read
\begin{eqnarray}
\ddot x_0 &=& - \frac{x_0}{\sigma^2} \,, \\
\ddot x_1 &=& - \frac{x_1}{\sigma^2} + 2\frac{x_0}{\sigma^3}\,.
\end{eqnarray}
It is clear that the equation for $x_0(t)$ is just the usual
harmonic oscillator, with solution
\begin{equation}
  x_0(t) = x_0(0)\cos \left( \frac{t}{\sigma} \right) +
             \sigma \pi_0(0)\sin \left( \frac{t}{\sigma} \right) \,.
\end{equation}
For the next order we have a driven harmonic oscillator (without
damping term). Note, however, that since the frequency of the driven
force is the same as the natural frequency of the oscillator ($\omega
= 1/\sigma$), we have a resonant phenomena: the amplitude of the
oscillations to first orders will increase with the trajectory length.
% Figure environment removed
It is clear that since the HMC algorithm only integrates the eom
up to a finite time $\tau$, and since we take the average over the samples,
this phenomena does not represent any
issue for the convergence: any trajectory length will produce correct
results according with our expectations. 
On the other hand, it is also clear that the variance of the
observables computed with these samples will depend significantly on
the trajectory length: of the many solutions found by the Hamiltonian
approach (corresponding to different trajectory lengths), some will
produce results with smaller variances. 

Figure~\ref{fig:ls} shows that for certain trajectory length, the
Hamiltonian approach described here just ``finds'' the transformation
given by Eq.~(\ref{eq:changeG}): the zeroth and first order are
very similar. Any other trajectory length will still produce the correct
expectation values, but with a significantly larger variance. 
In the case of the SMD algorithm we observe a similar phenomena, but
in this case it is the parameter $\gamma$ the one that has to be tuned.

This little example shows two important lessons: 1) the Hamiltonian
approach can be considered just a change of variables from the
original samples $x^{\alpha}\rightarrow\tilde y^\alpha(x^\alpha)$ such that once applied to
the reweighting formula eq.~(\ref{eq:rwformula}) gives \emph{constant}
reweighting factors, and 2)
The variance obtained for the derivatives depends on the particular
change of variables. 
In section~\ref{sec:lamphi4} we will comment on the differences in
variance between the two methods in detail.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
