
Sampling algorithms based on Hamiltonian dynamics are nowadays a
central tool in many different areas. 
Probably the best known example is the Hybrid Monte Carlo (HMC)
algorithm.
Originally developed in the context of Lattice
QCD~\cite{DUANE1987216}, today it is also a cornerstone in Bayesian inference.

The HMC algorithm belongs to the class of Metropolis-Hastings
algorithms that allows to obtain samples of arbitrarily complex
distribution functions with a high acceptance rate.  In order to sample
the distribution function
\begin{equation}
  p_\theta(x) = \frac{1}{\mathcal Z}\exp \left\{ -S(x;\theta) \right\}\,, \qquad \left(  \mathcal Z = \int {\rm d} x\, e^{-S(x:\theta)} \right) \,,
\end{equation}
where $x^\alpha\in \mathbb R^d$, we introduce some momentum variables
$\pi^\alpha$, conjugate to $x^\alpha$, and 
consider the sampling of the modified distribution function
\begin{align}
  q_\theta(\pi,x) = \frac{1}{\mathcal Z'} \exp \left\{ -H(\pi, x;\theta) \right\}\,, && \left(  \mathcal Z' = \int {\rm d} x {\rm d} \pi\, e^{-H(\pi,x:\theta)} \right)\,.
\end{align}
Assuming that the momenta are distributed as a standard Gaussian, the Hamiltonian is defined by
\begin{equation}
  H(\pi,x;\theta) = \frac{1}{2}\sum_{\alpha=1}^d\pi^\alpha\pi^\alpha + S(x;\theta)\,.
\end{equation}

It is clear that expectation values of quantities that depend only on
the variables $x$ are the same if they are computed using
$p_\theta(x)$ or $q_\theta(\pi,x)$ (i.e. 
$\mathbb E_{p_\theta}[f(x)] = \mathbb E_{q_\theta}[f(x)]$). 
On the other hand the distribution $q_\theta(\pi,x)$  can be
sampled with a high acceptance rate just by 1) throwing randomly
distributed Gaussian momenta $\pi(0)\sim e^{-\pi^2/2}$, 2) solving the
Hamilton equations of motion (eom)
\begin{eqnarray}
  \label{eq:eom}
  \dot x^\alpha &=& \pi^\alpha\,, \\
  \dot \pi^\alpha &=& - \frac{\partial H}{\partial x^\alpha} = - \frac{\partial S}{\partial x^\alpha}\,,
\end{eqnarray}
for a time interval from $(\pi(0),x(0))$ to $(\pi(\tau),x(\tau)$, and
3) performing a Metropolis-Hastings accept/reject step with probability 
$e^{-\Delta H}$ (see Figure \ref{fig:hmc}). The values of $x(\tau)$ are distributed according to
the probability density $p_\theta(x)$ (for a
proof see the original reference~\cite{DUANE1987216}). The trajectory
length $\tau$ can be chosen 
arbitrarely, although in order to guarantee the ergodicity of the
algorithm it is required that this length is chosen randomly from some
distribution (exponential or uniform are the most common choices). 
This point is usually not relevant, but for the case of ``free''
theories (i.e. 
Gaussian distributions $p_\theta$), it is well known that a constant trajectory
length can lead to wrong results (see~\cite{RHMC2017}).

\begin{wrapfigure}{l}{0.4\textwidth}
\centering
    % Figure removed
    \caption{The HMC algorithm makes a new sample proposal $x(\tau)$
      from a previous sample $x(0)$ of the
      target distribution $p_\theta(x)$ by numerically integrating the equations of
      motion of a fictitious Hamiltonian. 
      AD techniques can be applied to determine the dependence of the
      trajectory on model parameters $\theta$.} 
    \label{fig:hmc}
\end{wrapfigure}
In the last step $\Delta H = H(\pi(\tau),x(\tau);\theta) -
H(\pi(0),x(0);\theta)$ is just the energy violation. 
Since energy is conserved in Hamiltonian systems, this violation of
energy conservation is entirely due to the
fact that the eom in eq.~(\ref{eq:eom}) are solved
numerically, and not exactly. 
Nevertheless this integration of the eom can be made very precise with
a modest computational effort, allowing to reach arbitrarily high
acceptance rates.

The Stochastic Molecular Dynamics (SMD) algorithm is closely related with the HMC, and also based on a
Hamiltonian approach. 
In this case, after each integration step (from $t$ to
$t+\delta t$) of the equations of motion, 
the momenta is partially refreshed according to the equations
\begin{equation}
  \pi \to c_1\pi + \sqrt{1-c_1^2}\, \eta\,, 
\end{equation}
where $\eta $ is a new random momenta with Gaussian distribution, and
$c_1 = e^{-\gamma \delta t}$ is a parameter that can be chosen
arbitrarily via the value of $\gamma$. 
The SMD algorithm has some advantages from the theoretical point of
view, specially in the context of simulating field theories on the
lattice~\cite{Luscher:2011qa}

\subsubsection{Implementation of AD and convergence}
\label{sec:impl-conv}

The typical implementation of either the HMC or the SMD algorithm
involves the numerical integration of the eqs.~(\ref{eq:eom}).
This is performed by using a sequence of steps
\begin{eqnarray}
\label{eq:schemes}    
  \mathcal I_{\pi,h}: \quad \pi \to \pi - h \frac{\partial S}{\partial x}\,\\     
  \mathcal I_{x,h}: \quad x \to x + h\pi\nonumber \,.
\end{eqnarray}
An example is the well known \emph{leapfrog} integrator, that is
obtained by applying the series of steps $\mathcal
I_{\pi,\delta t/2}\mathcal I_{x,\delta t}\mathcal
I_{\pi,\delta t/2}$. Better precision can be obtained by using higher
order schemes (see~\cite{OMELYAN2003272}).



The application of AD to solve the eom eq.~(\ref{eq:eom}) follows
basically the same procedure, with the difference that 
\begin{enumerate}
\item Both coordinates and conjugate momenta variables are promoted to
  truncated polynomials
  \begin{eqnarray}
    x^\alpha &\longrightarrow& \tilde x^\alpha = \sum_i x_{i}^\alpha\epsilon^i\,, \\
    \pi^\alpha &\longrightarrow& \tilde \pi^\alpha = \sum_i \pi_{i}^\alpha\epsilon^i\,.
  \end{eqnarray}
  At the same time the model parameters are also promoted using
  $\tilde \theta_i = \theta_i + \epsilon_i$.  Only the lowest order
  $\pi^\alpha_0$ is initially set with Gaussian momenta
  \begin{equation}
    \tilde\pi^\alpha_0(0) \sim e^{-\pi^\alpha_0 \pi^\alpha_0/2}\,,
  \end{equation}
  while higher orders are initialized to zero.

  
\item The eom eq.~(\ref{eq:eom}) are solved consistently (i.e.
  order by order in $\epsilon$). Note that the non trivial eom can be
  written at each order as
  \begin{equation}
    \label{eq:dotpi}
    \dot\pi^\alpha_{n} = - \frac{\partial^2 S}{\partial x^\alpha \partial x^\beta} x^\beta_{n} + \text{lower order terms}\,.
  \end{equation}
  and therefore the eom eq.~(\ref{eq:eom}) can be solved numerically
  using the same basic building blocks defined by eq.~(\ref{eq:schemes}).
\item Since now the energy violation $\Delta H$ is a truncated
  polynomial, the usual accept/reject step cannot be carried out. 
  This means that one has to extrapolate the HMC results to
  zero step size $\delta t \to 0$. 
  In practice it is enough to work at sufficiently small step size
  such that any possible bias is well below our statistical uncertainties~\cite{brida_smd-based_2017}.
  
\end{enumerate}
The generalization of the SMD algorithm follows the same basic rules.

Note that the HMC algorithm, despite being in the class of
Metropolis-Hastings algorithms, is a completely deterministic
algorithm in the limit that the eom are integrated exactly. 
This explains the strategy: we are using the tools of AD to determine the Taylor
expansion of $x(\tau)$ with respect to the model parameters $\theta$. 

This approach to the perturbative sampling is intimately related with
the techniques of Numerical Stochastic Perturbation Theory (NSPT)~\cite{DiRenzo:1994av},
especially the Hamiltonian versions described
in~\cite{brida_newmethod_2017,brida2015numerical}.  The crucial
difference is that NSPT is usually applied to determine the deviations
with respect to the ``free'' (i.e. 
Gaussian) approximation, implementing a numerical approach to lattice
perturbation theory, whereas in our case we determine the dependence
with respect to arbitrary parameters $\theta$ (possibly more than one!) present in the
distribution function $p_\theta(x)$.

In practice the numerical implementation of this procedure is
straightforward, and just amounts to numerically solving the eom using
the algebraic rules of truncated 
polynomials (see section~\ref{sec:ad}).

These steps will result in a series of samples $\{\tilde
x_A\}_{A=1}^{N}$ (which are truncated polynomials, cf. expr.(\ref{eq:truncpol})). 
The usual MC evaluation of expectation values 
\begin{equation}
  \frac{1}{N} \sum_A O(\tilde x_A) = \sum_i \bar O_i\epsilon^i\,,
\end{equation}
will give a truncated polynomial that contains the dependence of
expectation values with respect to the model parameters $\theta$.

The convergence of expectation values is guaranteed if the Hessian
\begin{equation}
  H^{\alpha\beta} = \frac{\partial^2 S}{\partial x^\alpha \partial x^\beta}
  \label{eq:Hessian}
\end{equation}
is positive definite (see~\cite{brida_smd-based_2017}).
This condition is always true in the context of perturbative
applications in lattice field theory, since in this case it is
equivalent to having a stable vacuum.  
However, in models defined with compact variables and for the
case or expansions around arbitrary backgrounds, the
convergence of the process is not always guaranteed. 
An important example of this case are the simulation of Yang-Mills
theories on the lattice, where one would in general expect this
process not to converge.
On the other hand, note that in applications of Bayesian inference,
the convergence condition on the Hessian is guaranteed for unimodal
posteriors.  




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
