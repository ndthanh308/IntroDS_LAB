
In this section we explore a few applications of the techniques
introduced in section~\ref{sec:mth}. First we consider the application
of the reweighting technique to an optimization problem. 
Second, we consider the application in Bayesian inference to obtain
the dependence of predictions on the parameters that characterize the
prior distribution.

\subsection{Applications in optimization}
\label{sec:opt}

As an example application of an optimization process we will consider
the probability density function
\begin{equation}
  p_\theta(x) = \frac{1}{\mathcal Z}\exp \left\{ -S(x;\theta) \right\}\,, \qquad \left(  \mathcal Z = \int {\rm d} x\, e^{-S(x:\theta)} \right) \,.
\end{equation}
with
\begin{equation}
  S(x; \theta) = \frac{1}{\theta_1^2+1} \left( x_1^2 + x_1^4 \right) + \frac{1}{2}x_2^2 + \theta_2 x_1x_2\,.
\end{equation}

The shape of $S(x;\theta)$ is inspired in the action of a quantum
field theory in zero dimensions, where $x_1$ and $x_2$ are two fields
with coupling $\theta_2$, while $\theta_1$ is related to the mass of
the field $x_1$.
Expectation values with respect to $p_\theta(x)$ are functions of
the parameters $\theta$.  

% Figure environment removed

As an example we consider the problem of minimizing $\mathbb
E_\theta[x_1^2 + x_2^2]$ (i.e. 
finding the values for $\theta$ that make $\mathbb
E_\theta[x_1^2+x_2^2]$ minimum). 
We have implemented two flavours of Stochastic Gradient Descent (SGD): the first -basic- one, having a constant learning rate, and the second one being the well-known
%both a basic stochastic gradient descent (with constant learning rate) and the
ADAM algorithm \cite{kingma2017adam}. It is worth noting at this
point that as a general concept, SGD implies a stochastic (but
unbiased) evaluation of the gradients of the objective function at
every iteration. While in typical applications in the ML community,
where the task is to fit some dataset, this is done by evaluating the
gradients at different random batches of the data, the present example
is different in that no data is involved. In this case, every
iteration of the SGD evaluates the gradients on the different Monte
Carlo samples used to approximate the objective function  $\mathbb
E_\theta[x_1^2 + x_2^2]$.  

%These algorithms require stochastic evaluations both of the functionand its gradient at arbitrary values of the parameters $\theta$. 
%Here we perform these evaluations via Monte Carlo sampling: we use a
Here we consider a simple implementation of the Metropolis Hastings algorithm in order to
first produce the samples $\{x^{\alpha}\}_{\alpha=1}^N \sim p_\theta(x)$. 
Second, we determine the reweighted expectation value truncated at
first order
\begin{equation}
  \frac{\sum w(x^{\alpha};\tilde\theta) \left[ (x^{\alpha}_1)^2+ (x_2^{\alpha})^2 \right]}{\sum w(x^{\alpha};\tilde \theta)} \approx \bar O + \bar O_i \epsilon_i\,,  
  \qquad \left( w(x^{\alpha};\theta) = e^{S(x^{\alpha};\theta) - S(x^{\alpha};\tilde \theta)}  \right)\,,
\end{equation}
where $\tilde \theta_i =  \theta_i + \epsilon_i$. 
This quantity gives an stochastic estimate of the function value
\begin{equation}
  \bar O = \frac{1}{N}\sum_{i=1}^N [x_1^{\alpha}]^2 + [x_2^{\alpha}]^2\,,
\end{equation}
and its derivatives
\begin{equation}
 \bar O_i \approx \frac{\partial \mathbb E_\theta[x_1^2+x_2^2]}{\partial \theta_i}\,.
\end{equation}

Figure~\ref{fig:sgd} shows the result of the optimization process. 
As the iteration count increases the function is driven to its minima,
while the values of the parameters approach the optimal values
$\theta_1^{\rm opt} = \theta_2^{\rm opt} = 0$. 

It is worth mentioning that in this particular example only $1000$ samples
were used at each step to estimate the loss function and its
derivatives. 
If one decides to use a larger number of samples (say $10^5$), the
value of the parameter $\theta_2$ is determined with a much better precision. 
Note that the direction associated
with $\theta_2$ is much flatter, and therefore its value affects much
less value of the loss function.

\subsection{An application in Bayesian inference}
\label{sec:bayesian}
The purpose of statistical inference is to determine properties of the
underlying statistical distribution of a dataset
$D=\{x_{i},y_{i}\}_{i=1}^{N}$. In many
  cases, the independent variables $x_i$ are fixed, and all the
  stochasticity is captured by the dependent variables $y_i$. As
  such,  
the data is assumed to be sampled from a certain model, specified by
the \textit{likelihood}, 
$p(y|x,\phi)$, which depends on a set of parameters $\phi$.
The Bayesian paradigm attributes a level of confidence to the model by
introducing the \textit{prior} 
$p_{\theta}(\phi)$, \textit{i.e.} an a priori distribution of the
models parameters, where in this context $\theta$ play the role of the
hyper-parameters specifying the prior. Following Bayes' rule, the
\textit{posterior} distribution $p_{\theta}(\phi|D)$ is computed
as\footnote{The normalization factor, 
  $p_{\theta}(D)$, called the evidence, or marginal likelihood, is $\phi$-independent
  and represents the probability distribution of the observed data, given the model.}:
\begin{equation}
  \label{eq:bayes}
  p_{\theta}(\phi|D) \propto p(D|\phi) p_{\theta}(\phi)~.
%  ~~~~
%  p(D|\phi) = \prod_{i=1}^N p(y_i|x_i,\phi)~
\end{equation}
The likelihood of the whole dataset, $p(D|\phi)$, is computed assuming independent data points following a Gaussian distribution:
\begin{equation}
  p(D|\phi) = \prod_{i=1}^{N}\mathcal N(y_{i}|f(x_{i};\phi),\sigma_{i})\,,
  \label{eq:likelihood}
\end{equation}
where $\sigma_i$ are the  uncertainties of the corresponding observations $y_i$ (and assumed here to be given), while the mean of the Gaussian is given by $f(x_i;\phi)$. 
% The posterior in expr.(\ref{eq:bayes}) is the distribution of $\phi$ given the observed data and assumptions.
From a practical standpoint, in addition to the normalization being,
in general, unknown, the usual complexity of the posterior
distribution makes this possibly highly dimensional integral difficult
to compute. The use of Monte Carlo techniques, in particular of the
HMC, is typical in this context.
We focus below on two types of predictions: 1) The variance of the
model parameters $\delta\phi_j^2 = \mathbb{E}_{p_\theta}[\phi_j^2] -
(\mathbb{E}_{p_\theta}[\phi_j])^2$, where $j=1,...,d$, being $d$ the
dimension of $\phi$, and 2) the variance of the output mean $\delta
f_t^2 = \mathbb{E}_{p_\theta}[f_t^2] -
(\mathbb{E}_{p_\theta}[f_t])^2$, where $f_t$ is a shorthand notation
for the output mean $f(x_t;\phi)$, evaluated at a new ``test''
datapoint $x_t$ \footnote{Note that $E_{p_\theta}[f_t]$ is analogous
  to the so-called ``predictive distribution'' of Bayesian inference,
  however here we focus on the expected value of the prediction mean,
  instead of the expected value of the likelihood of $y(x_t)$
  itself.}. 


We are interested in studying the dependence of these quantities on the choice of
hyperparameters $\theta$ that characterize the prior distributions. In
particular we will consider the case of Gaussian priors, and determine
the dependence of our predictions with the width of this Gaussian.

\subsubsection{Model and data set}

We generate a synthetic dataset (cf. Figure \ref{fig:dataset}) by defining the points on an irregular grid in the range $x_i\in[-1.0;1.0]$, such that
\begin{equation}
  y_i = f(x_i;\phi_{\rm true}) + \sigma_i\epsilon~,
\end{equation}
where the mean is a 3rd degree polynomial, $f(x;\phi)=\phi_0+\phi_1x + \phi_2x^2 + \phi_3x^3$, with $\phi_{\rm true} = (1,1,1,1)$; $\epsilon\sim{\cal N}(0,1)$ is sampled from a standard Gaussian, and we consider a heteroscedastic dataset by defining a noise $\sigma_i$ dependent on $x_i$. We adopt the same model in order to  make inference on the parameters $\phi$. 

% Figure environment removed

%The likelihood reads
%\begin{equation}
%  p(D|\phi) = \prod_{i=1}^{N}\mathcal N(y_{i}|f(x_{i},\phi),\sigma_{i})\,,
%\end{equation}
%where $\mathcal N(\mu|\sigma)$ is the usual Gaussian distribution of
%mean $\mu$ and variance $\sigma^2$. As a model we choose a third
%degree polynomial $f(x,\phi) = \phi_{0} + \phi_{1}x + \phi_{2}x^{2}+\phi_{3}x^{3}$. 
%Note that this is the model that was also used to obtain the dataset.

The prior distribution is also chosen as a Gaussian, $\phi\sim {\cal N}(\mu_p,\sigma_p)$. 
For simplicity we choose the priors centered on the ``correct'' values
of the model (i.e. $\mu_p =\phi_{\rm true}$), while we keep the width 
$\sigma_p$ as a hyperparameter to study the dependence on\footnote{This is a simplified setup for the sake of illustration, given the methodological scope of this work. Nonetheless, it is straightforward to apply the method to the situation where we are interested in studying the dependence on both parameters $\mu_p$ and $\sigma_p$ simultaneously, or in general on the joint set of hyperparameters of the model. }.

For any choice of the prior width $\sigma_p$ we can obtain a prediction by
generating $N$ samples $\{\phi^{(\alpha)}\}^N_{\alpha=1}$ according to the distribution
$p_{\theta}(\phi|D)$ computed from \cref{eq:bayes}.  
  
\subsubsection{Reweighting approach}
\label{sec:bayesianhmc}

The reweighting method takes $N$ samples
$\{\phi_{i}^{({\alpha})}\}_{\alpha=1}^{N}$ obtained at
$\sigma_{p}=\sigma_{p}^{*}$ and computes the reweighted average using
$\tilde\sigma_{p}=\sigma_{p}^{*}+\epsilon$ in \cref{eq:rw}.  

For each sample $\phi^{(\alpha)}$, the reweighting factor becomes a polynomial expansion in
$(\sigma-\sigma_{p}^{*})$  
\begin{equation}
  \label{eq:rw bi}
  \tilde w_{\alpha}(\epsilon) = \frac{p_{\mu,\sigma_{p}^{*}+\epsilon}(\phi_{\alpha}|D)}{p_{\mu,\sigma_{p}^{*}}(\phi_{\alpha}|D)}.
\end{equation}
Notice that the zeroth order of \cref{eq:rw bi} is one, such that the zeroth order result corresponds to the usual Monte Carlo point estimate for $\delta\phi_{0}(\sigma_{p}^{*})$.

In order to generate these samples, we used the standard HMC algorithm. 
The equations of motion are
\begin{align}
  &H_{\theta}(\phi,\pi) = \frac{\pi^{2}}{2} - \log(p_{\theta}(\phi|D)),\\
	&\dot\phi_{j} = \pi_{j},\\
	&\dot \pi_{j} = - \frac{1}{\sigma_{p}^{2}}(\phi_{j} - (\mu_p)_j) + \sum_{i=0}^{N}\frac{1}{\sigma_{i}^{2}}\left( y_{i} - f(x_{i},\phi) \right)(x_{i})^{j},
\end{align}
where $\pi=\{\pi_{0},\pi_{1},\pi_{2},\pi_{3}\}$ are the momenta conjugated to $\phi$.
Note that all $\phi$-independent terms can be dropped from the
equations of motion, namely the normalization of $p_{\theta}(\phi|D)$
is not needed.
The eom were solved numerically using a fourth-order symplectic
integrator \cite{OMELYAN2003272} providing a high acceptance rate in
the Metropolis-Hastings step even with a coarse integration.  

The chosen integration step-size was $\varepsilon = 0.001$, while the
trajectory length was uniformly sampled in the interval $[0,100]\times
\varepsilon$\footnote{Due to the quadratic form of the Hamiltonian,
  the phase space of this system is cyclic. The algorithm is ergodic
  only if the trajectory length is
  randomized \cite{RHMC2017}.}.
%Taking into account the conclusions from \cref{sec:nspt}, the average trajectory length is approximately tuned such that the variance is minimized.  

% In the following, all of the Monte Carlo chains correspond to half million thermalized trajectories.

%All the predictions are a function of the hyperparameter $\sigma^*$ and
%we would, generically, be interested in this dependence.
%As for the quantity to study we focus on the uncertainty of the
%average value for $\phi_{i}$, $\delta\phi_{i} =
%\mathbb{E}_{p_{\theta}}[\phi_{i}^{2}]-\mathbb{E}_{p_{\theta}}[\phi_{i}]^{2}$,
%and analogously the uncertainty for the prediction of a new point
%$x_{n}$.  

\subsubsection{Hamiltonian perturbative expansion}

Following the procedure in \cref{sec:nspt}, the Monte Carlo samples
$\{(\tilde\phi_{j})^{\alpha}\}_{\alpha=1}^{N},~j=0,1,2,3$ were
obtained with the modified HMC algorithm for some values of
$\sigma_{p}^{*}$. 
We used the same parameters for the HMC as described in the previous
section. In particular our acceptances were so close to 100\% that any
bias due to the missing accept/reject step is negligible. 
We checked this hypothesis by further performing another simulation
with a coarser value of the integration step and finding completely
compatible results.


\subsubsection{Results}

\begin{table}[t]
  \centering
  \caption{Results for the expansion coefficients of the variance,
    ${\delta\phi^{2}_{j,n}}$ for $\sigma_{p}^{*}=0.3$
    from the reweighting and hamiltonian expansion.}
\scalebox{0.9}{
  \begin{tabular}{cccccccc}
	% \toprule
     & & \multicolumn{6}{c}{$n$} \\\cmidrule{3-8}
     & & 0 & 1 & 2 & 3 & 4 & 5  \\
    \midrule
\multirow{2}{*}{$\delta\phi^{2}_{0,n}$} & RW &    0.00014705(86) &    0.0001384(63) &    -0.000248(29) &     0.000367(62) &     -0.00071(51) &      -0.0003(12)  \\
                  & HAD &   0.00014705(86) &    0.0001365(34) &   -0.0002850(60) &     0.000311(20) &     0.000178(77) &     -0.00115(26)  \\

    \midrule
\multirow{2}{*}{$\delta\phi^{2}_{1,n}$} & RW &     0.01099(15) &       0.0285(12) &      -0.0450(58) &        0.032(13) &         0.04(10) &        -0.61(25)  \\
                  & HAD &     0.01099(15) &      0.02787(69) &      -0.0518(11) &       0.0248(38) &        0.189(16) &       -0.700(46)  \\
    \midrule
\multirow{2}{*}{$\delta\phi^{2}_{2,n}$} & RW &      0.008938(74) &      0.00830(28) &      -0.0283(10) &       0.0850(39) &       -0.234(18) &        0.603(78) \\
                  & HAD &     0.008938(74) &      0.00817(15) &     -0.02789(42) &       0.0849(13) &      -0.2505(44) &        0.726(15)  \\
    \midrule
\multirow{2}{*}{$\delta\phi^{2}_{3,n}$} & RW &     0.03617(59) &      0.1205(51) &       -0.182(24) &        0.050(61) &         0.63(42) &         -4.0(12)  \\
                  & HAD &     0.03617(59) &       0.1177(30) &      -0.2052(42) &        0.020(16) &        1.132(66) &        -4.02(19)  \\
    \bottomrule
    \label{tab:variance phi0}
  \end{tabular}
  }
\end{table}

Here we compare the predictions for the average model parameters
$\phi$ and their dependence on the prior width $\sigma$. In particular
we focus on the variance of the model parameters $\delta\phi^{2}_j$,
since these are the quantities most sentitive to the prior width (i.e. 
very thin priors result in small variance for the model
parameters). We have fixed $\sigma^* = 0.3$, but similar conclusions
are obtained for other values.  

The results of the Monte Carlo average for $\delta\tilde\phi^{2}_i$ and
its derivatives with respect to $\sigma$ are
shown in \cref{tab:variance phi0}. 
Results labeled ``RW'' use the reweighting method, while results
labeled ``HAD'' use the Hamiltonian approach. 

It is obvious that results using the Hamiltonian approach are more
precise:
the uncertainties in the derivatives, $\delta\phi^{2}_{i,n},n\neq 0$, are
smaller for the Hamiltonian approach, despite the statistics being the
same. The difference is larger for higher order derivatives: the
approach based on reweighting struggles to get a signal for the fourth
and fifth derivatives, while the Hamiltonian approach is able to
obtain even the fifth derivative with a few percent precision. 
This fits our expectations (see section~\ref{sec:hamilt-appr-repar}). 
\noindent\newline\newline
On the other hand, for our second quantity of analysis $\delta f_t^2$ (i.e. the variance of the prediction mean), Figure~\ref{fig:ypred} shows the results of the dependence on $\sigma_p$, where we have fixed $x_t=0.5$.

%dependence of the variance of the
%parameter prediction
%\begin{equation}
%  y_{\text{pred}}(x_{n})=\mathbb{E}_{p_{\theta}}[f(x_{n},\phi)] = \int d\phi p_{\theta}(\phi|D) f(x,\phi).
%\end{equation}
%at $x = 0.5$ with respect to $\sigma_P$.
The Hamiltonian approach gives visually results with a reduced variance,
similar to the results presented in table~\ref{tab:variance phi0}.

% Figure environment removed



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
