
The tools of automatic differentiation represent a cornerstone in
modern optimization algorithms and many machine learning applications. 
The extension of these techniques to functions evaluated using Monte
Carlo processes is non trivial. In these cases the underlying
probability distribution depends on some parameters, and Monte Carlo
techniques are used to draw samples for specific values of the parameters. 
The dependence of the samples with the parameter values is difficult
to determine.
Nonetheless, there are many applications for these techniques. 
In this work we have considered several of them, from optimizations of
expectation values, to the study of the dependence of Bayesian
predictions with respect to prior parameters or applications in
lattice field theory.

We have presented two different approaches to the
determination of Taylor series of quantities
estimated via Monte Carlo sampling.  
The first approach is based on reweighting and can be considered a
generalization of the score function estimator, valid for derivatives
of arbitrary orders, and unnormalized probability
distribution functions.
The second approach is based on Hamiltonian methods to sampling
(HMC being the most popular option), and produces samples  that carry
the information of the dependence on the 
action parameters. The convergence of the stochastic process in this
last approach is not always guaranteed, but we have provided
sufficient conditions for the convergence.

We have shown some applications of these methods. 
First, in the context of optimization, we have applied the stochastic
gradient descent to find the optimal parameters of some expectation
value (see section~\ref{sec:opt}). 
Second, in Bayesian inference we have shown how these methods can be
used to estimate the dependence on Bayesian predictions on
the ``hyperparameters'' that describe the prior distribution (cf section~\ref{sec:bayesian}).

Finally, in the context of Lattice Field Theory, we have studied in
detail the case of $\lambda-\phi^4$ in four space-time dimensions. 
The dependence of observables with respect to the parameters of the
action (the bare mass in lattice units $\hat m$ and the bare coupling
$\lambda$) can be accurately determined using these techniques. 

A detailed comparison of both methods shows that results obtained with
the Hamiltonian approach are much more precise. 
We have argued that the Hamiltonian approach can be seen as a change
of variables from the reweighing formula where the reweighing factors
are constant: 
In the Hamiltonian approach all dependence with respect to the
parameters is present in the samples. 
The absence of disconnected contributions (in the lattice jargon)
makes the variance of Taylor series computed with the Hamiltonian
approach much more precise. For example, our study on $\lambda-\phi^4$
shows that for the same statistics one gets results 100 times more precise. 

The Hamiltonian approach has his own drawbacks. 
On one hand the convergence of the stochastic process is not guaranteed. 
In particular for the case of Lattice QCD, that is formulated in terms
of compact variables, the convergence cannot be guaranteed. 
On the other hand, the method cannot be applied to samples that have
already been generated, unlike the reweighting method.

Nevertheless the investigations of this work open the door to an
interesting possibility: that one can find change of variables that
eliminate (or significantly reduce) the disconnected contributions of
the reweighing approach. 
Machine Learning techniques, in particular the tools related with
normalizing flows, potentially can provide a significant gain in the
computation of derivatives of expectation values.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
