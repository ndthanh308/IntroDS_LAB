%     % Table generated by Excel2LaTeX from sheet 'Sheet1'
%     \begin{table}[htbp]
%       \centering
%       \caption{Performance Comparison of Various Models on the COCO-MMRD Dataset.}
%       \scalebox{0.45}{
%       \begin{tabular}{llllll}
%         \toprule
%         \textbf{Model}                                                                                      & \textbf{type}                                                    & \textbf{\begin{tabular}[c]{@{}l@{}}Tuned\\ Params\end{tabular}} & \textbf{BLUE}  &  \textbf{Similarity} & \textbf{Rouge} \\ 
%         \midrule
%         Human                                                                                               & Human                                                            & -------                                                         & 80.35          & 90.25                                      & 82.73                                 \\ 
%         \midrule
%      \begin{tabular}[c]{@{}l@{}}Mutimodal-CoT\\ $_{Flan-T5-Large}$ \cite{DBLP:journals/corr/abs-2302-00923}\end{tabular} & \begin{tabular}[c]{@{}l@{}}VQA\\ Mutimodal model\end{tabular}    & 783M                                                            &  66.89          &        79.77                       &        69.04                        \\ 
%         \midrule
%          UnifiedQA$_{Base}$ \cite{khashabi-etal-2020-unifiedqa}    & Text-to-text  & 223M                                                            & 43.76          & 63.97                                      & 39.46                                 \\
%         UnifiedQA$_{Base}$ w/ COT \cite{DBLP:conf/nips/LuMX0CZTCK22}                                                                 & Text-to-text                                                        & 223M                                                  & 64.71 & 75.75                             & 66.62                        \\ 

%         \midrule
%         LLAVA \cite{DBLP:journals/corr/abs-2304-08485}            & \begin{tabular}[c]{@{}l@{}}Mutimodal \\ LLM model\end{tabular}   & 13B                                                             & 59.38          & 74.59                                      & 62.15                                 \\ 
%         \midrule
%         Enigma-COT$_{Base}$                                                                 & Ours                                                             & 229M                                                            & 65.52          & 78.03                                      & 67.67                                 \\
%         Enigma-COT$_{Flan-T5-large}$                                                                  & Ours                                                             & 793M                                                            & \textbf{68.07} & \textbf{80.41}                             & \textbf{70.17}                        \\ 
%         \bottomrule
%         \end{tabular}
%     }
%   \label{tab:addlabel5}%
% \end{table}%

\begin{table}[htbp]
\small
\centering
\caption{Performance Comparison of Various Models on the COCO-MMRD Dataset.}
{\renewcommand\baselinestretch{1.2}\selectfont
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
Model & Type & Tuned Params & BLUE & Similarity & Rouge \\ 
\midrule
Human & Human & - & 80.35 & 90.25 & 82.73 \\ 
\hline
Mutimodal-CoT$_{\textrm{Flan-T5-Large}}$ & VQA & 783M &  66.89 & 79.77 & 69.04 \\ 
\hline
UnifiedQA$_{\textrm{Base}}$& Text-to-text  & 223M & 43.76 & 63.97 & 39.46 \\
UnifiedQA$_{\textrm{Base}}$ w/ COT & Text-to-text & 223M & 64.71 & 75.75 & 66.62 \\ 
\hline
LLAVA           & LLM & 13B & 59.38 & 74.59 & 62.15 \\ 
\hline
Enigma-COT$_{\textrm{Base}}$ & Ours & 229M & 65.52 & 78.03 & 67.67 \\
Enigma-COT$_{\textrm{Flan-T5-large}}$ & Ours & 793M & \textbf{68.07} & \textbf{80.41} & \textbf{70.17} \\ 
\bottomrule
\end{tabular}\par}
}
\label{tab:addlabel5}
\end{table}