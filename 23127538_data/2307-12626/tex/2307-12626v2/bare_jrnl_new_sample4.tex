\documentclass[lettersize,journal]{IEEEtran}
\usepackage[table]{xcolor}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tabularx} % 用于创建可调整列宽的表格
\usepackage{booktabs} % 用于创建漂亮的表格线
\usepackage{multirow} % 用于合并单元格
\usepackage{leftidx}
\usepackage{hyperref}

\begin{document}

\title{Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework}

\author{Jingxuan Wei, Cheng Tan, Zhangyang Gao, Linzhuang Sun,  Siyuan Li, \\
Bihui Yu, Ruifeng Guo, Stan Z. Li$^\dagger$,~\IEEEmembership{Fellow,~IEEE}
\thanks{Jingxuan Wei and Cheng Tan contributed equally as co-first authors. Gao, Sun, Li, Yu and Guo contributed to the project through selection, direction, code development, and writing assistance. Prof. Stan Z. Li is the corresponding authors.}
\thanks{Cheng Tan, Zhangyang Gao and Siyuan Li are with the AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China. Email: tancheng, gaozhangyang, lisiyuan@westlake.edu.cn}
\thanks{Jingxuan Wei,Linzhuang Sun are affiliated with the Shenyang Institute of Computing Technology, Chinese Academy of Sciences, Shenyang, China. Their institution of higher education is the University of Chinese Academy of Sciences, Beijing, China. Email: weijingxuan20, sunlinzhuang21@mails.ucas.edu.cn}
\thanks{Bihui Yu and Ruifeng Guo are with the Shenyang Institute of Computing Technology, Chinese Academy of Sciences, Shenyang, China. Email: yubihui@sict.ac.cn, guofuifeng@163.com.}
\thanks{Stan Z. Li is with the AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China. Email: stan.zq.li@westlake.edu.cn.}}

% \author{Cheng Tan, Zhangyang Gao,Stan Z. Li$^\dagger$ ,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Multimodal reasoning is a critical component in the pursuit of artificial intelligence systems that exhibit human-like intelligence, especially when tackling complex tasks. While the chain-of-thought (CoT) technique has gained considerable attention, the existing ScienceQA dataset, which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning(COCO-MMR) dataset, a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.  The data and code are available at \href{https://github.com/weijingxuan/COCO-MMR}{https://github.com/weijingxuan/COCO-MMR}.
\end{abstract}

\begin{IEEEkeywords}
Multimodal reasoning, chain-of-thought, scientific questions and explanations, open-ended questions, dataset and techniques.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{A}{dvancing} artificial intelligence systems towards human-like intelligence necessitates robust multimodal reasoning capabilities, especially when addressing complex tasks that require a holistic understanding of visual and textual information~\cite{DBLP:conf/nips/2022,tan2022hyperspherical,DBLP:journals/corr/abs-2201-08239,DBLP:journals/tcsv/ChengJWZ22,DBLP:journals/tcsv/HeP20}. In recent years, the chain-of-thought (CoT) technique~\cite{DBLP:conf/nips/LuMX0CZTCK22,DBLP:journals/corr/abs-2302-00923,DBLP:journals/corr/abs-2301-13379} has emerged as a promising approach for tackling multimodal reasoning challenges. CoT involves reasoning through a series of steps, where each step builds upon previous ones, mimicking the cognitive process humans employ when solving complex problems.

Despite the growing interest in CoT, the existing ScienceQA dataset~\cite{DBLP:conf/nips/LuMX0CZTCK22} comprising multimodal scientific questions and explanations extracted from elementary and high school textbooks has faced doubts due to the following reasons: (1) Limited scale: A relatively small scale of about 21k questions, which may limit the capability of learning models in capturing the diversity and complexity of reasoning. (2) Reliance on multiple-choice questions: Its reliance on multiple-choice questions may hinder the ability to generate open-ended responses that require coherent and comprehensive explanations. (3) Restriction to scientific question reasoning: The dataset is primarily focused on scientific question reasoning and lacks coverage of daily life scenarios. This confinement restricts the evaluation of models in broader real-world contexts.

To address these limitations and further advance the field of multimodal reasoning, we introduce COCO-MMR, a new dataset that is three times larger than ScienceQA, containing approximately 62k questions, rationales, and answers generated from the large object dataset COCO~\cite{lin2014microsoft}. One notable aspect of COCO-MMR is its departure from the convention of multiple-choice questions found in previous datasets. Instead, we introduce open-ended questions that require CoT models to generate detailed and coherent responses, offering a more challenging problem setting that better captures the reasoning capabilities of these models. Moreover, COCO-MMR is designed to cover diverse daily life scenarios, thereby extending the reasoning task into a broader real-world context beyond scientific questions. This helps evaluate whether models can apply commonsense knowledge and reasoning broadly to understand various concepts and situations in everyday life as humans do.

In addition to curating the comprehensive dataset, we propose a novel multimodal reasoning framework, Enigma-COT, that incorporates two innovative techniques: multi-hop cross-modal attention for boosting visual representation learning and sentence-level contrastive learning for strengthening the text encoder. The multi-hop cross-modal attention mechanism enhances the fusion of visual and textual information by considering multiple levels of interactions between different modalities. This enables models to capture more nuanced relationships and dependencies between visual and textual elements, leading to improved reasoning performance. Moreover, our sentence-level contrastive learning technique encourages the text encoder to capture finer-grained semantic relationships within sentences, enhancing the model's ability to reason based on subtle linguistic cues and textual context.

We conduct extensive experiments and employ a diverse set of metrics to ensure a fair and insightful comparison, thus effectively evaluating the effectiveness of our dataset and proposed techniques. The comprehensive experiments and detailed analysis we undertake provide compelling evidence for the efficacy of the COCO-MMR dataset and the proposed techniques. The results not only showcase the advantages of our approach over existing datasets but also offer novel insights into the realm of multimodal reasoning. By demonstrating the superior performance of our proposed method, we pave the way for further advancements in multimodal reasoning. Our work contributes to the development of models that exhibit enhanced reasoning capabilities across a wide range of real-world scenarios. We hope that these findings may propel the field forward, inspiring future research and innovation in multimodal reasoning.

\section{Related Work}

\subsection{Visual Question Answering}

Visual Question Answering (VQA) presents a challenging task that necessitates a model's comprehension of both natural language questions and their associated images, ultimately generating appropriate text responses~\cite{DBLP:journals/tcsv/XieFCHL22,DBLP:journals/tcsv/WangWLLZW23}. Situated at the intersection of computer vision~\cite{DBLP:journals/tcsv/DavaniS21,gao2022simvp,tan2022simvp,cao2022survey,DBLP:journals/tcsv/JinZFZLLZ22,zheng2023cvt} and natural language processing (NLP), VQA was initially proposed by~\cite{DBLP:conf/iccv/AntolALMBZP15}. A key challenge in VQA lies in the model's ability to understand both natural language queries and visual information, merging these distinct types of data to produce accurate answers.

Early VQA systems traditionally treated the question and image separately~\cite{WU201721,DBLP:journals/tmm/YuZLQHTW20}, integrating their representations in later stages to generate answers~\cite{DBLP:conf/iccv/AntolALMBZP15,DBLP:conf/icip/ChowdhuryNFS17,DBLP:conf/icip/HuangKJLJT18}. However, these approaches often overlooked the intrinsic interaction between the question and the image. To tackle this limitation, recent research has adopted advanced multimodal fusion strategies, including attention mechanisms, to more effectively integrate question and image information~\cite{DBLP:journals/tmm/Al-HalahG21,DBLP:journals/tcsv/OuCW21,DBLP:conf/miccai/SeenivasanIKR22,DBLP:journals/tcsv/TangLJPZDK23}. Nonetheless, these methods primarily focus on direct VQA tasks and may neglect challenges involving deep scientific knowledge and complex reasoning.

\subsection{Multimodal Reasoning}

Multimodal reasoning ~\cite{DBLP:journals/frai/WellsB21,DBLP:journals/tcsv/WenP21,DBLP:journals/jair/RasXGD22,DBLP:journals/tmm/WuLZLZQC23,DBLP:journals/tcsv/LiMDFT23} have garnered significant attention in reasoning tasks. These concepts revolve around understanding the decision-making process of models and generating interpretable reasoning paths. \textit{Lu et al.} ~\cite{DBLP:conf/nips/LuMX0CZTCK22} proposed the ScienceQA dataset and introduced a method based on reasoning chains. This work attempts to generate explainable reasoning chains with their scientific question dataset, and reveals a multi-step reasoning process to produce coherent reasoning chains when deriving the correct answer.  \textit{Zhang et al.} ~\cite{DBLP:journals/corr/abs-2302-00923} was the first to work on multimodal reasoning chains and utilize the multimodal information of the ScienceQA dataset by separately generating reasoning chains and answering in two stages. LLaMA-Adapter~\cite{gao2023llamaadapterv2} presents a lightweight adaptation method to efficiently finetune LLaMA~\cite{touvron2023llama} to achieve competitive multimodal reasoning performance with extremely low computational cost.

While previous approaches have demonstrated strong performance on the ScienceQA dataset, we question whether they can achieve comparable results on a more challenging benchmark that thoroughly exploits their reasoning abilities. In this paper, we introduce the COCO-MMR dataset, which increases the difficulty of the multi-modal reasoning task in three key aspects: larger data scale, more complex question forms, and broader question scope.

% Figure environment removed

\section{COCO-MMR}

Our proposed COCO-MMR dataset differs from the ScienceQA dataset principally in the question forms and scope. As shown in Figure \ref{fig:dataset_comparison}, the ScienceQA dataset furnishes more granular information in the text and prompts the model to choose from provided options. In contrast, the COCO-MMR dataset demands that the model generate open-ended answers based solely on the question without additional context or options, posing a greater challenge. Furthermore, the COCO-MMR dataset encompasses a wider range of question types and scenarios, including those from daily life, which aligns more closely with real-world situations.

\subsection{Data Collection}

We illuminate the collection and processing methodology of our proposed COCO-MMR dataset here. The collected dataset derives from the COCO Caption dataset~\cite{chen2015microsoft} and the COCO VQA dataset~\cite{goyal2017making}.

\paragraph{Caption Dataset}

% Figure environment removed

In the COCO caption dataset, the original captions from the COCO dataset were preserved and utilized as the foundation for developing cognitive reasoning inference processes, which were then combined with prompts. The main aim of these cognitive reasoning inferences was to generate answers that could be inferred as captions, using a combination of questions and images.

To facilitate the generation of thinking logic inferences by MiniGPT-4~\cite{zhu2023minigpt}, our data collection process employed a prompt template. As shown in Figure~\ref{fig:dataset_collection} (a), the template was structured as follows: "The caption of this picture reads: $[\texttt{Caption}]$. Based on the content of the picture, use thinking logic to explain why this picture is captioned as such." This prompt can effectively empower the MiniGPT-4 model to generate the desired and well-articulated rationales.

In total, we generated 30,000 triplets consisting of questions, rationales, and answers. Each data point underwent manual verification to ensure the quality and relevance of our dataset, confirming its alignment with the corresponding image.

\paragraph{VQA Dataset}

Similar to the COCO caption dataset, the COCO VQA dataset retained the original questions and generated the thinking logic inference process combined with prompts. The final objective was to generate rationales derived from a combination of questions and images.

As shown in Figure~\ref{fig:dataset_collection} (b), The data collection for the VQA dataset also utilized a prompt template: "I have a question about this picture: $[\texttt{Question}]$. If you have answered, and your ANSWER is: $[\texttt{Answer}]$. Then please, based on the content of the picture, the question and answer, use thinking logic to explain why it is this answer." This prompt guides the generation of informative rationales.

Initially, the VQA dataset contained 35,000 data points. After a meticulous manual filtering and detection process, we retained 32,351 data points that ensured the integrity of the dataset and the relevance of each data point to its corresponding image. The complete COCO-MMR dataset consists of 62,351 data points, which are divided into 56,115 training, 3,117 validation, and 3,119 test samples. To summarize, our data collection process was designed to create a dataset that encourages models to generate thoughtful and open-ended responses. The COCO-MMR dataset serves as a resource for evaluating models in multimodal reasoning.

\subsection{Dataset Analysis}
To procure a richer understanding of the dataset's structure, we've enumerated multiple features and executed a thorough study pertaining to each.

% Figure environment removed
\paragraph{Data Length}
We performed a statistical analysis on the character lengths of questions, rationales, and answers within the COCO-MMR dataset. As Figure \ref{fig:question_length} illustrates, the majority of question lengths in the dataset lie within a 2 to 10 character interval. Within this interval, the length of 10 characters frequently emerges, predominantly due to the consistent style of the title-related questions. The rationale character length exhibits a broader spread, varying between 50 to 250 characters. Lastly, the character lengths of answers predominantly cluster within a concise 1-20 character span.

\paragraph{Support Rate}
% Figure environment removed
We introduce the concept of "Support Rate" as a measure of dataset reliability. For each triplet comprised of a question, rationale, and final answer, we evaluate whether the rationale and question adequately support the derivation of the final answer. If they are, it's categorized as "supportive" and conversely, as non-supportive. We combine the question and rationale as input and use a BERT-based textual entailment model to determine if the answer substantiates the hypothesis. Finally, we compute the proportion of supportive triplets (question, rationale, answer) in each dataset split. This proportion manifests as the support rate. The respective support rates for each split are portrayed in Figure~\ref{fig:Support_Rate}. As can be observed from the results, the majority of the rationales obtained through our data collection approaches are reasonable. This demonstrates that our COCO-MMR dataset is effective for the multimodal reasoning task.

\paragraph{Human Evaluation Analysis}

To conduct a human evaluation, we randomly chosed 200 data entries and assigned them to five volunteers for assessment. The primary evaluation criteria focused on determining if the reasoning could convincingly justify the derivation of the final answer from the given question. Interestingly, the results revealed that 92.20\% of the rationales effectively aided in deriving the final answer from the question. The disparity between this human evaluation and the support rate stems from the fact that the correct answers can be inferred from simple questions without relying on reliable rationales.

% Figure environment removed

In addition, the volunteers classified the selected 200 entries according to question type, with categories primarily including "Caption questions" and "VQA questions". The VQA questions were further divided into subcategories such as 'True/False Questions,' 'Numeric Questions,' 'Color Questions,' 'Person Questions,' 'Time Questions,' 'Action Questions,' and 'Others.' The distribution of these question types is illustrated in Figure~\ref{fig:Human_Evaluation}. It can be seen that the COCO-MMR dataset encompasses a wide variety of question types, enabling the extension of reasoning tasks to a broader range of real-world scenarios, thus posing a greater challenge for evaluation.

\section{Enigma-COT}

We build a multimodal reasoning framework, Enigma-COT, to provide a strong baseline model for multimodal reasoning. Following~\cite{DBLP:journals/corr/abs-2302-00923}, our framework has two stages: rationale generation and answer inference. These stages use the same model architecture but have different inputs and outputs. In the first stage, we provide text and vision inputs to generate rationales. In the second stage, we combine the original text with the generated rationale. We then use the updated text and the original vision input to infer the answer. The model architecture is shown in Figure~\ref{fig:model}.

% Figure environment removed

\subsection{Multi-Hop Cross-Modal Attention}

We propose multi-hop cross-modal attention that iteratively optimizes crucial information, resembling the repetitive thinking process of humans. The $k$-th hop attention $A^{k}$ is defined as follows:
\begin{equation}
A^{k} = \textrm{MHA}^{k}(H_t^{k-1}, H_i, H_i),
\end{equation}
where $\textrm{MHA}(\cdot)$ represents the multi-head attention layer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}. The $(k-1)$-th hop text hidden state $H_t^{k-1}$ is the query, while the image embedding $H_i$ acts as both the key and value.

The $k$-th hop text hidden state $H_t^{k}$ is obtained by aggregating the information of the previous hidden state $H_t^{k-1}$ and the attention $A^{k}$:
\begin{equation}
\begin{aligned}
H_t^{k} &= (1 - G^{k}) \odot H_t^{k-1} + G^{k} \odot A^{k}, \\
G^{k} &= \sigma(W^{k}[H_t^{(k-1)}, A^{k}]),
\end{aligned}
\end{equation}
where $\sigma$ denotes the sigmoid function, $\odot$ denotes element-wise multiplication, and $W^{(k)}$ is the $k$-th gating weight. The gating operation balances the text representation and the attention, enabling the model to better preserve the original information while effectively capturing and utilizing the key information captured by the current attention.

\subsection{Sentence-level Contrastive Learning}

While the multi-hop cross-modal attention is designed to enhance the fusion of visual and textual information, we also propose sentence-level contrastive learning to strengthen the text encoder. We maximize the similarity between the image embedding $H_i$ and the decoded text embedding $H_t$ at the sentence level, and control the similarity by a temperature parameter $\tau$:
\begin{equation}
  \mathrm{S}(H_i, H_t) = \frac{H_i \odot H_t^T}{\tau},
\end{equation}
and the sequence-level contrastive loss is:
\begin{equation}
  \mathcal{L}_{con} = -\log(\mathrm{softmax}(S(H_i, H_j))).
\end{equation}

The overall training objective is the linear combination of vanilla token-level cross-entropy loss and sentence-level contrastive loss:
\begin{equation}
  \mathcal{L} = \mathcal{L}_{ce} + \lambda \mathcal{L}_{con},
\end{equation}
where $\lambda$ is a parameter to balance the two losses.

\section{Experiments}

\begin{table*}[htbp]
  \small
  \centering
  \caption{Accuracy on the ScienceQA Dataset. Evaluation across eight categories: natural science, social science, linguistics, textual context, image context, no context, grades 1-6, and grades 7-12. The best results in each category are highlighted in bold.}
  {\renewcommand\baselinestretch{1.1}\selectfont
  \resizebox{0.86\textwidth}{!}{
  \begin{tabular}{l|c|cccccccc|c}
  \toprule
  Model                                   & \begin{tabular}[c]{@{}c@{}}Tuned\\ Params\end{tabular} & NAT              & SOC              & LAN              & TXT              & IMG              & NO               & G1-6             & G7-12            & AVG   \\
  \midrule
  Human~\cite{DBLP:conf/nips/LuMX0CZTCK22} & -     & 90.23 & 84.97 & 87.48 & 89.6  & 87.5  & 88.10  & 91.59 & 82.42 & 88.40 \\
  \hline
  MCAN~\cite{DBLP:conf/cvpr/Yu0CT019} & 95M   & 56.08 & 46.23 & 58.09 & 59.43 & 51.17 & 55.4  & 51.65 & 59.72 & 54.54 \\
  Top-Down~\cite{DBLP:conf/cvpr/00010BT0GZ18} & 70M   & 59.50  & 54.33 & 61.82 & 62.90  & 54.88 & 59.79 & 57.27 & 62.16 & 59.02 \\
  BAN~\cite{DBLP:journals/titb/YuPFCS23} & 112M  & 60.88 & 46.57 & 66.64 & 62.61 & 52.6  & 65.51 & 56.83 & 63.94 & 59.37 \\
  DFAF~\cite{DBLP:conf/cvpr/GaoJYLHWL19} & 74M   & 64.03 & 48.82 & 63.55 & 65.88 & 54.49 & 64.11 & 57.12 & 67.17 & 60.72 \\
  ViLT~\cite{DBLP:conf/icml/KimSK21} & 113M  & 60.48 & 63.89 & 60.27 & 63.2  & 61.38 & 57    & 60.72 & 61.90  & 61.14 \\
  Patch-TRM~\cite{lu2021iconqa} & 90M   & 65.19 & 46.79 & 65.55 & 66.96 & 55.28 & 64.95 & 58.04 & 67.50  & 61.42 \\
  VisualBERT~\cite{li2019visualbert,DBLP:conf/icml/KimSK21} & 111M  & 59.33 & 69.18 & 61.18 & 62.71 & 62.17 & 58.54 & 62.96 & 59.92 & 61.87 \\
  Mutimodal-CoT$_{\textrm{Base}}$~\cite{DBLP:journals/corr/abs-2302-00923} & 223M  & 87.52 & 77.17 & 85.82 & 87.88 & 82.90  & 86.83 & 84.65 & 85.37 & 84.91 \\
  Mutimodal-CoT$_{\textrm{Flan-T5-large}}$~\cite{DBLP:journals/corr/abs-2302-00923} & 783M  & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & 93.02 \\
  \hline
  UnifiedQA$_{\textrm{Base}}$~\cite{khashabi-etal-2020-unifiedqa} & 223M  & 68.16 & 69.18 & 74.91 & 63.78 & 61.38 & 77.84 & 72.98 & 65.00    & 70.12 \\
  UnifiedQA$_{\textrm{Base}}$ w/ CoT~\cite{DBLP:conf/nips/LuMX0CZTCK22} & 223M  & 71.00    & 76.04 & 78.91 & 66.42 & 66.53 & 81.81 & 77.06 & 68.82 & 74.11 \\
  \hline
  % LLaMA-Adapter$_{\textrm{T}}$ \cite{gao2023llamaadapterv2} & 1.2M  & 79.00    & 73.79 & 80.55 & 78.30  & 70.35 & 83.14 & 79.77 & 75.68 & 78.31 \\
  LLaMA-Adapter~\cite{gao2023llamaadapterv2} & 1.8M  & 84.37 & 88.30  & 84.36 & 83.72 & 80.32 & 86.90  & 85.83 & 84.05 & 85.19 \\
  LLaVa~\cite{DBLP:journals/corr/abs-2304-08485} & 13B  & 90.36 & 95.95  & 88.00 & 89.49 & 88.00 & 90.66  & 90.93 & 90.90 & 90.92 \\
  % LLaVa \cite{DBLP:journals/corr/abs-2304-08485} & 13B  & 91.56 & 96.74  & 91.09 & 90.62 & 88.99 & 93.52  & 92.73 & 92.16 & 92.53 \\
  \hline
\rowcolor{gray!30} Enigma-COT$_{\textrm{Base}}$ & 229M  & 88.28 & 78.74 & 85.64 & 88.51 & 84.28 & 86.90  & 85.43 & 85.89 & 85.59 \\
\rowcolor{gray!30} Enigma-COT$_{\textrm{Flan-T5-large}}$ & 793M  & \textbf{97.51} &  \textbf{84.70} &  \textbf{94.73} &  \textbf{96.68} &  \textbf{91.37} &  \textbf{95.89} &  \textbf{94.46} &  \textbf{93.47} & \textbf{94.11} \\
  \bottomrule
  \end{tabular}}\par}
\label{tab:addlabel1}
\end{table*}

In this section, we provide detailed information about the experiments conducted on the ScienceQA and COCO-MMR.

\subsection{Experimental Setup}
Enigma-COT is initialized by utilizing UnifiedQA \cite{khashabi-etal-2020-unifiedqa} and FLAN-T5 \cite{DBLP:journals/corr/abs-2210-11416}. We incorporate an early stopping strategy to stop the training when the validation loss achieves stability. The maximum number of training epochs is $50$, with a batch size of $32$ and a learning rate of $5\times 10^{-5}$. All experiments were conducted using four NVIDIA Tesla A100 80G GPUs.

% For the ScienceQA dataset, we configure both the rationale generation and answer generation stages with a maximum input sequence length of 512, while the output lengths are defined as 512 and 64 respectively. When it comes to the COCO-MMR dataset, the rationale generation and answer generation stages are regulated with a maximum input sequence length of 100 and 400 respectively, and the output lengths are set to 300 and 100, correspondingly. All experiments were conducted using four NVIDIA Tesla A100 80G GPUs.

\subsection{Baseline Models}
\paragraph{For the ScienceQA dataset}
In alignment with \cite{DBLP:journals/corr/abs-2302-00923,gao2023llamaadapterv2}, the baseline models encompass: (i) human evaluation \cite{DBLP:conf/nips/LuMX0CZTCK22}, (ii) visual question answering models \cite{DBLP:conf/cvpr/Yu0CT019,DBLP:conf/cvpr/00010BT0GZ18,DBLP:journals/titb/YuPFCS23,DBLP:conf/cvpr/GaoJYLHWL19,DBLP:conf/icml/KimSK21,lu2021iconqa,li2019visualbert}, (iii) text-to-text generation (LM) models \cite{khashabi-etal-2020-unifiedqa,DBLP:conf/nips/LuMX0CZTCK22}, and (iv) Multimodal LLM models \cite{gao2023llamaadapterv2,DBLP:journals/corr/abs-2302-00923}. The performance of all baseline models is encapsulated in Table \ref{tab:addlabel1}.

\paragraph{For the COCO-MMR dataset} We select several representative approaches above to evaluate them on the COCO-MMR dataset, including: (i) human evaluation, (ii) visual question answering models~\cite{DBLP:journals/corr/abs-2302-00923}, (iii) text-to-text generation (LM) models~\cite{khashabi-etal-2020-unifiedqa,DBLP:conf/nips/LuMX0CZTCK22}, and (iv) Multimodal LLM models \cite{DBLP:journals/corr/abs-2304-08485}. 

For the methods of human evaluation, we recruited 20 volunteers (with educational backgrounds ranging from bachelor's to doctoral degrees). Each volunteer was randomly assigned 50 questions from the complete test set to answer. We created a straightforward reading webpage for humans to read the questions and images and write their responses. Participants only needed to write the answers based on the question and corresponding image. Monetary rewards were given upon test completion.

% For the detailed evaluation metrics of these models, please refer to Appendix~\ref{metrics}.
\subsection{Evaluation Metrics}
\label{metrics}
For the evaluation of our multimodal reasoning chain task, we utilize four main metrics: AVG Accuracy, BLEU, Similarity, and Rouge. AVG Accuracy serves as the ultimate evaluation criterion for the ScienceQA dataset, while Rouge is the primary metric for the rationale generation phase of the ScienceQA dataset and for the COCO-MMR dataset as well.

\textbf{Accuracy}
We apply accuracy on the ScienceQA dataset to measure the model's precision in predicting answers after generating rationales. The dataset spans eight aspects: natural science, social science, linguistics, textual context, image context, no context, and for grade levels 1-6 and 7-12. We calculated the accuracy for each of these aspects and their average accuracy. The accuracy for each aspect is determined by the ratio of the number of correctly predicted answers for questions within that aspect over the total number of questions predicted by the model within that aspect, represented by the equation:

\begin{equation}
\text{Accuracy} = \frac{\text{Number of Correctly Predicted Samples}}{\text{Total Number of Predicted Samples}}
\end{equation}

The average accuracy is the mean of all these individual accuracies, denoted by the following equation:

\begin{equation}
\text{Average Accuracy} = \frac{1}{n}\sum_{i=1}^{n} \text{Accuracy}_i
\end{equation}

Here, $n$ refers to the number of aspects (in this instance, $n=8$), and $\text{Accuracy}_i$ is the accuracy of the $i$th aspect.

\textbf{BLEU (Bilingual Evaluation Understudy)}: BLEU is an automatic evaluation metric based on n-gram precision, originally used for machine translation tasks. In our task, BLEU mainly evaluates the lexical overlap between the generated rationale or answer and the reference text. Given a reference text $r$ and a candidate text $c$, BLEU is defined as follows:

\begin{equation}
\text{BLEU} = \text{BP} \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)
\end{equation}

where $p_n$ is the $n$-gram precision, $N$ is the maximum order of n-gram, and $w_n$ is the weight of the n-gram precision, usually set to $1/N$. BP (Brevity Penalty) is used to penalize too short candidate texts and is defined as follows:

\begin{equation}
\text{BP} = \left\{ \begin{array}{ll}
1 & \mbox{if } c > r \\
\exp(1 - r / c) & \mbox{otherwise}
\end{array} \right.
\end{equation}

\textbf{Similarity}: Similarity is mainly used to evaluate the semantic similarity of the generated rationale or answer. In our task, we use cosine similarity as the similarity metric, defined as follows:

\begin{equation}
\text{Similarity} = \frac{\textbf{c} \cdot \textbf{r}}{||\textbf{c}||_2 ||\textbf{r}||_2}
\end{equation}

where $\textbf{c}$ and $\textbf{r}$ represent the embedding vectors of the candidate text and reference text, respectively.

\textbf{ROUGE (Recall-Oriented Understudy for Gisting Evaluation)}: ROUGE is an evaluation metric widely used in automatic summarization and machine translation tasks, mainly used to measure the recall performance of the generated text and the reference text. In our task, we mainly use the ROUGE-L metric, which evaluates the similarity between texts based on the longest common subsequence (LCS). ROUGE-L is defined as follows:

\begin{equation}
\text{ROUGE-L} = \frac{\text{LCS}(c, r)}{\max(\text{length}(c), \text{length}(r))}
\end{equation}

where $\text{LCS}(c, r)$ is the length of the longest common subsequence between the candidate text $c$ and the reference text $r$.


\subsection{Experimental Results}

Table~\ref{tab:addlabel1} presents the comparative experimental results on the ScienceQA dataset. The results demonstrate that humans achieved an average accuracy rate of 88.40\%. Notably, both the Mutimodal-CoT and LLaVa models surpassed human performance, achieving accuracy rates of 93.02\% and 90.92\% respectively. Remarkably, the Enigma-COT model achieved an average precision of 94.11\%, representing a significant breakthrough on this benchmark dataset. These findings emphasize the advanced capabilities of our proposed multimodal model, Enigma-COT, in effectively addressing the intricacies of multimodal reasoning by employing the innovative chain-of-thought technique.

Furthermore, the aforementioned research results clearly indicate that the ScienceQA dataset, with its limited scale, reliance on multiple-choice questions, and constraints in scientific reasoning, is not adequately prepared to embrace future advancements in multimodal reasoning. Therefore, there is a pressing need to introduce a more complex dataset.

\begin{table*}[htbp]
  \small
  \centering
  \caption{The performance comparison on the COCO-MMR Dataset.}
  {\renewcommand\baselinestretch{1.3}
%   \selectfont\resizebox{\textwidth}{!}{
  \begin{tabular}{lccccc}
  \toprule
  Model & Type & Tuned Params & BLUE & Similarity & Rouge \\ 
  \midrule
  Human & Human & - & 80.35 & 90.25 & 82.73 \\ 
  \hline
  Mutimodal-CoT$_{\textrm{Flan-T5-Large}}$~\cite{DBLP:journals/corr/abs-2302-00923} & VQA & 783M &  66.89 & 79.77 & 69.04 \\ 
  \hline
  UnifiedQA$_{\textrm{Base}}$~\cite{khashabi-etal-2020-unifiedqa} & Text-to-text  & 223M & 43.76 & 63.97 & 39.46 \\
  UnifiedQA$_{\textrm{Base}}$ w/ COT~\cite{DBLP:conf/nips/LuMX0CZTCK22} & Text-to-text & 223M & 64.71 & 75.75 & 66.62 \\ 
  \hline
  LLAVA \cite{DBLP:journals/corr/abs-2304-08485}            & LLM & 13B & 59.38 & 74.59 & 62.15 \\ 
  \hline
  \rowcolor{gray!30}Enigma-COT$_{\textrm{Base}}$ & Ours & 229M & 65.52 & 78.03 & 67.67 \\
  \rowcolor{gray!30}Enigma-COT$_{\textrm{Flan-T5-large}}$ & Ours & 793M & \textbf{68.07} & \textbf{80.41} & \textbf{70.17} \\ 
  \bottomrule
  \end{tabular}
%   \par}
  }
  \label{tab:addlabel5}
  \end{table*}
  

% Figure environment removed

Table~\ref{tab:addlabel5} provides a performance comparison among the representative approaches on the COCO-MMR dataset. Following~\cite{DBLP:journals/corr/abs-2302-00923}, the principal evaluation metrics encompass BLUE, Similarity, and Rouge scores. Within the COCO-MMR dataset, the Rouge score for humans is recorded as 82.73\%. The scores for Multi-modal-CoT, UnifiedQA, LLaVa, and Enigma-COT are 69.04\%, 66.62\%, 62.15\%, and 70.17\% respectively. Even though Enigma-COT attains the top performance, a substantial discrepancy still persists between its results and human performance. This suggests that despite some progress made by machine learning methods on this inference tasks, the complex cognition and understanding abilities of humans remain inimitable. This presents new challenges for future research. There is a need to further optimize our models in order to lessen the performance gap between human and machine on this particular task.
Figure~\ref{fig:example1} displays an example of the predictions made by the Enigma-COT model on the COCO-MMR dataset. As depicted, the dataset incorporates open-ended questions, requiring the model to comprehend the image and the question, and, leveraging common-sense knowledge, establish connections between the rooms in the image and various locations within a home. Finally, based on the inferred results, the model generates an answer indicating the specific location in the home represented by the image.
% In addition, a detailed analysis of further prediction results is carried out in Appendix~\ref{Example_Analysis}. For the analysis of Enigma-COT in the rational generation, please refer to Appendix~\ref{Experimental_Results_2}.

\subsection{Ablation Study}
Table \ref{tab:addlabel2} represents the ablation study conducted on the ScienceQA dataset. Using Enigma-COT$_{\textrm{Base}}$ as the base model, the removal of the Multi-Hop Cross-Modal Attention module resulted in a more pronounced performance drop compared to removing the Sentence-level Contrastive Learning module.
\input{table2}
Table \ref{tab:addlabel7} represents the ablation study conducted on the COCO-MMR dataset. Similarly, it can be observed that the Multi-Hop Cross-Modal Attention module is more crucial for improving the model's performance.
\input{table7}

% \subsection{Case Analysis of the ScienceQA Dataset}

\subsection{Case Analysis}
\label{Example_Analysis}
To gain a deeper understanding of the performance of the Enigma-COT model and the nature of the COCO-MMR dataset, we manually selected and analyzed a random sample of 500 data entries.

We found that samples with a correctly predicted Answer do not necessarily contain a valid Rationale. In Figure \ref{fig:example2}, the original Rationale was invalid, resulting in an invalid Rationale during the reasoning stage, yet the Answer was correct. Moreover, another situation emerged, as shown in Figure \ref{fig:example3}, where the original Rationale was correct, but the Rationale during the reasoning stage was incorrect, yet the predicted Answer was correct. These results suggest that the Chain of Thought (COT) might not be useful for inferring the Answer in some data instances and can be ignored.

We also found that in samples where the predicted Answer was incorrect, the original Rationale was correct, and the Rationale during the reasoning stage was incorrect, resulting in an incorrect predicted Answer, as shown in Figure \ref{fig:example4}. Additionally, another scenario presented itself where the original Rationale was correct, the Rationale during the reasoning stage was correct, but the predicted Answer was incorrect, as depicted in Figure \ref{fig:example5}.

% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Applications and Potential Impacts of the Dataset}
\label{Applications_and_Potential}
From the perspective of application scenarios, COCO-MMR, as a multimodal reasoning dataset, has a wide range of uses. It can be employed in general domains to train models to understand and generate more complex and diversified responses. This not only promotes the complexity and diversity of AI but also propels its application in real-world scenarios. Moreover, the COCO-MMR dataset can also be used to improve the interpretability of models, allowing us to comprehend how models infer answers from questions and images. This plays a vital role in enhancing the reliability and trustworthiness of models.

Regarding potential impacts, the COCO-MMR dataset may profoundly influence the development of AI. Specifically, it can assist us in developing superior dialogue systems, search engines, and even intelligent assistants, which will significantly increase the efficiency of our daily lives and work. However, we must be vigilant about the serious problems that may arise from the misuse of these technologies. For instance, some may exploit these technologies to fabricate fake news or conduct phishing on the internet. Therefore, as we advance, we also need to be alert to prevent its potential adverse impacts and proactively take measures to mitigate them.


\section{Conclusion}
In addressing the multimodal reasoning chain problem, our focus is on enabling models to utilize common sense knowledge and reasoning in a manner akin to humans, thereby understanding a broad array of concepts and situations encountered in daily life. We introduce a challenging multimodal reasoning dataset, COCO-MMR, that incorporates an open-ended question format, along with a pioneering model named Enigma-COT. Our experimental results demonstrate the effectiveness of our proposed dataset and model. We anticipate that our efforts will further the advancement of models boasting robust reasoning abilities applicable to diverse real-world scenarios and catalyze future exploration in multimodal reasoning.


\section*{Acknowledgments}
This work was supported by the National Key R\&D Program of China (2022ZD0115100), the National Natural Science Foundation of China (U21A20427), the Competitive Research Fund (WU2022A009) from the Westlake Center for Synthetic Biology and Integrated Bioengineering, and the Liaoning Province Applied Basic Research Program (20220303).


\bibliographystyle{IEEEtran}
\bibliography{bare_jrnl_new_sample4}


% \newpage
% 
\section*{Biography Section}
\begin{IEEEbiography}[{% Figure removed}]{Jingxuan Wei} is currently pursuing the Ph.D. degree at the University of Chinese Academy of Sciences, trained by Shenyang Institute of Computing Technology, Chinese Academy of Sciences. He has participated in multimodal research internships and academic exchanges at various companies and institutions, including Kuaishou Technology, ByteDance, and Microsoft Research Asia. Additionally, he has been actively involved in the practical application of numerous foundation models. His primary research interest lies in multimodal learning.
\end{IEEEbiography}
\begin{IEEEbiography}[{% Figure removed}]{Cheng Tan} is currently pursuing the Ph.D. degree with the School of Engineering, Westlake University, Hangzhou, China. He has authored several papers in top-tier conferences and journals, such as the IEEE Transactions on Knowledge and Data Engineering (TKDE), IEEE Transactions on Neural Networks and Learning Systems (TNNLS), ACM Multimedia (MM), International Conference on Learning Representations (ICLR), and Conference on Computer Vision and Pattern Recognition (CVPR). His main research interest is self-supervised learning.
\end{IEEEbiography}
\begin{IEEEbiography}[{% Figure removed}]{Zhangyang Gao} is currently a Ph.D. candidate at the School of Engineering, Westlake University, Hangzhou, China. He has authored several papers in top-tier conferences and journals, such as the IEEE Transactions on Knowledge and Data Engineering (TKDE), IEEE Transactions on Neural Networks and Learning Systems (TNNLS), International Conference on Learning Representations (ICLR), and Conference on Computer Vision and Pattern Recognition (CVPR). His main research interests include unsupervised video tasks and drug discovery.
\end{IEEEbiography}
\begin{IEEEbiography}[{% Figure removed}]{Linzhuang Sun} is currently a Ph.D candidate at the University of Chinese Academy of Sciences, Beijing, China. He has authored several papers in high-level conference, like the International Conference on Advanced Data Mining and Applications(ADMA). He also has worked as an intern at BAAI and Baidu before and after, participating in large model development. And his main research interests include multimodal learning and sentiment analysis.
\end{IEEEbiography}
\begin{IEEEbiography}[{% Figure removed}]{Siyuan Li} received the B.S. degree from Nanjing University, Nanjing, China in 2021. He is currently pursuing the Ph.D. degree with the School of Engineering, Westlake University, Hangzhou. 
He has authored several papers in top-tier conferences, such as European Conference on Computer Vision (ECCV), and International Conference on Machine Learning (ICML). His main research interest is self-supervised learning.
\end{IEEEbiography}
% \vspace{1pt}
\begin{IEEEbiography}[{% Figure removed}]{Bihui Yu} is a researcher and doctoral advisor at the University of Chinese Academy of Sciences, Beijing, China. He has presided over and participated in many provincial and national important projects, like the Network culture market dynamic supervision service system research and development and application demonstration. He also has authored several high level papers and invention patents. And his research interests include knowledge engineering, big data and multimodal learning.
\end{IEEEbiography}
% \vspace{1pt}
\begin{IEEEbiography}[{% Figure removed}]{Ruifeng Guo} , PhD supervisor, is the director of Shenyang Institute of Computing Technology, Chinese Academy of Sciences. He has served as the head of numerous projects, including the development of advanced numerical control devices based on the domestic "Loongson" CPU chip and the establishment of the National Open Numerical Control System Supporting Technology Innovation Platform. His exceptional contributions cater to the major strategic and industrial needs of the nation. In the past two years, he has led his team to achieve numerous accolades, including the prestigious National Science and Technology Progress Second Prize. His research interests encompass machine learning, high-end numerical control, and the application of various cross-disciplinary fields related to artificial intelligence.
\end{IEEEbiography}
% \vspace{1pt}
\begin{IEEEbiography}[{% Figure removed}]{Stan Z. Li} (Fellow, IEEE) received the B.Eng. degree from Hunan University, China, the M.Eng. degree from the National University of Defense Technology, China, and the Ph.D. degree from the University of Surrey, U.K. He was an Associate Professor (tenure) at Nanyang Technological University, Singapore. He was with Microsoft Research Asia, Beijing, China, as a Research Lead, from 2000 to 2004. He was the Director of the Center for Biometrics and Security Research (CBSR), Chinese Academy of Sciences, Beijing, from 2004 to 2019. He joined Westlake University as a Chair Professor of artificial intelligence in February 2019. He has authored over 400 papers in international journals and conferences and has authored and edited ten books, with over 60,000 Google Scholar citations. Among these are Markov Random Field Models in Image Analysis (Springer), Handbook of Face Recognition (Springer), and Encyclopedia of Biometrics (Springer). His research interests include fundamental research in machine learning, data science, and applied research in multiple AI-related interdisciplinary fields (computer vision, smart sensors, life science, material science, and environmental science).

Dr. Li served as an Associate Editor of the IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) and organized more than 100 international conferences or workshops.
\end{IEEEbiography}

% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}



% \bibliographystyle{acl_natbib}

% \vfill

\end{document}


