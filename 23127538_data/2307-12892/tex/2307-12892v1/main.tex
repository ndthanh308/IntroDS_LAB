\pdfoutput=1
\documentclass{article}
\usepackage{mathtools, amssymb, mathrsfs, amsthm}
\usepackage{multirow}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[colorlinks = true, citecolor = blue, urlcolor = blue]{hyperref}
\usepackage{xcolor}
\usepackage{indentfirst}
\usepackage{chngcntr}
\usepackage{apptools}
\usepackage{cleveref}



\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{proposition}{section}}

\newtheorem{problem}{Problem Statement} 
\newtheorem{lemma}{Lemma} 
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\theoremstyle{definition}  
\newtheorem{condition}{Condition} 
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\newcommand{\Cov}{\textup{Cov}}
\newcommand{\Var}{\textup{Var}}
\newcommand{\Proj}{\textup{Proj}}
\newcommand{\Col}{\textup{Col}}
\newcommand{\Row}{\textup{Row}}
\newcommand{\Span}{\textup{Span}}
\newcommand{\Diag}{\textup{Diag}}
\newcommand{\tr}{\textup{Tr}}
\newcommand{\rank}{\textup{rank}}

\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\E}{\boldsymbol{E}}
\newcommand{\A}{\boldsymbol{A}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{D}}
\newcommand{\N}{\boldsymbol{N}}
\newcommand{\M}{\boldsymbol{M}}
\newcommand{\I}{\boldsymbol{I}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\W}{\boldsymbol{W}}
\newcommand{\V}{\boldsymbol{V}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\vec}{\textup{vec}}
\renewcommand{\S}{\mathbb{S}}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\H}{\boldsymbol{H}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\title{A Statistical View of Column Subset Selection}
\author{Anav Sood\\ Stanford University \and Trevor Hastie\\ Stanford University}
\date{\today}
\begin{document}

\maketitle

\begin{abstract}
We consider the problem of selecting a small subset of representative variables from a large dataset. In the computer science literature, this dimensionality reduction problem is typically formalized as Column Subset Selection (CSS). Meanwhile, the typical statistical formalization is to find an information-maximizing set of Principal Variables. This paper shows that these two approaches are equivalent, and moreover, both can be viewed as maximum likelihood estimation within a certain semi-parametric model. Using these connections, we show how to efficiently (1) perform CSS using only summary statistics from the original dataset; (2) perform CSS in the presence of missing and/or censored data; and (3) select the subset size for CSS in a hypothesis testing framework. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
In modern data applications, it is common to reduce a large dataset into a smaller one, either for storage efficiency or ease of downstream analysis. As methods become more complex, it is increasingly desireable for this reduced dataset to be interpretable. 

In this paper, we consider the dimensionality reduction task of selecting a subset of variables that is most representative of the entire dataset. We establish an equivalence between two popular methods for this task, \emph{Column Subset Selection (CSS)} and \emph{Principal Variables}, and provide a generative view of both methods. 

\subsection{Motivation}

The textbook approach to dimensionality reduction is Principal Components Analysis (PCA) \cite{Jolliffe2002}. PCA projects the observed data onto a small set of variance-maximizing axes. Ultimately, the reduced dataset consists of linear combinations of the original variables, and these resulting linear combinations are reffered to as principal components. 

The diverse range of perspectives on PCA has fostered many useful theoretical and methodological insights. For example, although PCA’s objective is typically written in terms of unit-level data, it can also be written solely in terms of the data’s covariance \cite{Pearson}. This covariance characterization turns out to provide a more fundamental view of the problem, and \cite{Okamoto} strengthens PCA's theoretical foundation by using it to establish the method's simultaneous optimality for a large class of objectives. Another example is probabilistic PCA (PPCA), a generative variant of PCA. PPCA broadens PCA’s scope by enabling the application of standard statistical tools (e.g. likelihood ratio testing, expectation-maximization, Bayesian inference) \cite{Tipping}. 

Unfortunately, the linear combinations that PCA outputs are typically dense and difficult to interpret. The field of interpretable dimensionality reduction has emerged largely in response to this phenomenon and is spearheaded by methods that output sparse linear combinations of the original variables  \cite{d'Aspremont, Jolliffe2003, Witten, Zou}. These sparse variants of PCA provide more interpretable results by simplifying the relationship between the reduced and original dataset. Methods like CSS and Principal Variables that select a subset of the original variables simplify this relationship as much as possible and, consequently, can be viewed as PCA’s sparsest and most interpretable alternatives. 

This paper aims to provide for CSS the same diverse set of viewpoints that are available for PCA. In particular, we show that Principal Variables solves the exact same problem as CSS, but is characterized in terms of covariances rather than unit-level data, and we provide a generative variant of CSS that facilitates the application of statistical tools. These viewpoints of CSS lead to novel methodology for a number of open problems in interpretable dimensionality reduction.  

\subsection{Background}

In what follows, we give a detailed description of CSS, Principal Variables, and the relationship (or lack thereof) between their corresponding literatures. 

In the CSS problem we aim to choose $k$ of $p$ columns from a data matrix $\X \in \R^{n \times p}$ that best linearly reconstruct the rest:
\begin{equation}\label{eq:css}
\argmin_{S \subseteq [p]: |S| = k} \min_{\B \in  \R^{k \times p}} \|\X -  \X_{\bullet S} \B\|_F^2
\end{equation}
Here, $\X_{\bullet S}$ denotes the sub-matrix of $\X$ containing columns with indices in $S$. While computing the exact optimum in \eqref{eq:css} is NP-complete \cite{Shitov}, a vast approximation literature has produced tractable algorithms \cite{Boutsidis2009, Civril,  Deshpande, Drineas, Farahat, Ordozgoiti2018, Tropp} with successful applications in epidemiology \cite{Fink, Nowakov}, networking \cite{Tripathi}, business \cite{Boutsidis2008}, imaging \cite{Kromer, Strauch, WangC}, and low-rank matrix approximation \cite{Boutsidis2011,  Dan}. 

Principal Variables \cite{McCabe} puts a non-degenerate $N(0, \bSigma)$ model on the observed data and suggests selecting a subset of ``principal variables'' according to various information criteria. The most popular of the four criteria suggests selecting the subset $S$ that solves 
\begin{equation}\label{eq:mccabe_pv}
\argmin_{S \subseteq [p]: |S| = k} \tr(\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{-1}\bSigma_{S, -S})
\end{equation}
where $-S$ is the complement of $S$. In practice, an estimate $\hat{\bSigma}$ of the covariance matrix is used in \eqref{eq:mccabe_pv}. Like the CSS literature, the Principal Variables literature has produced approximation algorithms \cite{Brusco, Camida,  Guo, Masaeli, Wei} that have enjoyed application in a number of fields, including psychology \cite{Fehrman, Ladd}, imaging \cite{Chang}, and the natural sciences  \cite{Eitrich, Isaac}. 

Expectedly, each thread of literature has something fruitful to offer: principal variables have strong statistical motivation and practitioners can find them in settings where only covariance or correlation estimates are available. We provide examples of such settings in \Cref{sec:equivalence}. The CSS literature places larger emphasis on providing algorithms with theoretical guarantees and minimal computational and storage complexity. 

Despite similarities between the methods, the CSS and principal variables literatures have existed entirely independently. For example, \cite{Masaeli} poses regularized matrix reconstruction problem in the style of \eqref{eq:css}, but cites Principal Variables as motivation and makes no reference to any CSS literature. 
 

\subsection{Our Contributions}

Our main contribution is establishing an equivalence between CSS and Principal Variables, and via this equivalence, providing a novel generative view of both methods.  In particular, we show that performing CSS on $\X$ is equivalent to finding the principal variables according to $\bSigma = \X^{\top} \X/n$, and both are equivalent to maximum likelihood estimation in the following semi-parametric model:
\begin{align}
\begin{split}
&X_S \sim F\\
&X_{-S} \mid X_S \sim N(\mu_{-S} + \W(X_S - E_{F}[X_{S}]), \sigma^2 \I_{p-k}). 
\end{split} \label{eq:pcss_model}
\end{align}

This equivalence and generative view suggest new methodology for several problems in interpretable dimensionality reduction: \newline 

\noindent \textbf{Scalable CSS without unit-level data:} We introduce fast algorithms for performing CSS when only summary statistics are available. To demonstrate our algorithms' efficacy, we provide a real world example where BlackRock, our industry affiliate, wants to perform CSS but only has access to a covariance estimate. Our insights enable us to perform CSS in this setting, and our novel algorithms enable us to run the appropriate experiments, which would take days using current methods, in less than three minutes. \newline 

\noindent \textbf{CSS with missing and/or censored data:} The equivalence between CSS and Principal Variables suggests a natural workflow for performing CSS when data is missing and/or censored: model the missingness and/or censorship, produce a covariance estimate that is reasonable under this model, and perform CSS using this covariance estimate. As an illustrative example, we use this workflow to propose a simple and novel procedure for performing CSS with missing-at-random data. In a simulation study where we sample data from the model \eqref{eq:pcss_model} and omit entries at random, our method always selects the correct subset while the best competing method selects it less than half the time. We also confirm that our method outperforms existing methods on a real dataset. \newline 

\noindent \textbf{Selecting the Subset Size for CSS:} Supposing we observe data drawn from a generalization of the model \eqref{eq:pcss_model}, we propose a novel and theoretically motivated procedure for selecting an appropriate subset size. We first verify that our procedure performs well in some difficult simulated settings, and we then demonstrate how it can be used to shorten surveys in an automated way. As an example, we apply our procedure to the Big Five Inventory (BFI), a 44-item personality questionnaire \cite{John}. In less than a tenth of a second, it suggests cutting the survey in more than half. Importantly, the questions our procedure selects appear to sufficiently capture the relevant information in the survey.\newline 

We provide a python package, \texttt{pycss}, that allows practioners to use our methods as well as notebooks that allow for easy replication of our results at \texttt{\href{https://github.com/AnavSood/CSS}{github.com/AnavSood/CSS}}.

\subsection{Notation and Preliminaries}

We use the following linear algebra notation. All matrices are bolded capital letters. We denote the Moore-Penrose inverse, or pseudo-inverse, of $\A \in \R^{p \times q}$ by $\A^+$, and the $i$-th row and $j$th column of $\A$ by $\A_{i \bullet}$ and $\A_{\bullet j}$ respectively. For subsets $S \subseteq [p] = \{1, \dots, p \}$ and $T \subseteq [q]$, $\A_{S, T}$ is the $|S| \times |T|$ sub-matrix of $\A$ with rows and columns indexed by $S$ and $T$ respectively. Indexing always precedes matrix operations, e.g., $\A^{+}_{S,T} = (\A_{S,T})^{+}  \neq (\A^{+})_{S,T}$. We denote the determinant of a square matrix $\A \in \R^{p \times p}$ by $|\A|$. For symmetric square matrices $\A$ and $\B$, we write $\A \succeq \B$ (resp. $\A \succ \B$) if $\A - \B$ is positive semi-definite (resp. positive definite), and we denote the set of positive-semi definite matrices as $\S^{p \times p}_{+}$. The $p \times p$ identity matrix is $\I_p$. For a vector $x \in \R^p$, the vector $x_{S} \in \R^{|S|}$ has entries indexed by $S$. When appropriate, $0$ and $1$  (resp. $\0$ and $\1$) refer to a vector (resp. matrix) of zeros and ones.

We use the following statistics notation. We typically denote random vectors as unbolded capital letters, e.g., $X \in \R^{p}, Y \in \R^{q}$, and $X \sim (\mu, \bSigma)$ means that $X$ has mean $\mu$ and covariance $\bSigma$. Generally $\bSigma_{Y}$ is $\Cov(Y)$, but we sometimes write $\bSigma$ for $\Cov(X)$ specifically (when unambiguous). Samples drawn from a distribution are assumed independent and identically distributed (i.i.d). A model refers to a family of distributions. 

We will assume that all random vectors $X \in \R^p$ we consider satisfy $E[\|X\|_2^2] < \infty$. 

All proofs are in the Appendix. 


\section{The Equivalence of CSS and Principal Variables}
\label{sec:equivalence}
In this section, we establish a mathematical equivalence and conceptual connection between CSS and Principal Variables. Henceforth, selecting principal variables using $\bSigma$ refers to the following subset search:
\begin{equation}
\label{eq:pv}
\argmin_{S \subseteq [p]: |S| = k} \tr(\bSigma - \bSigma_{\bullet S}\bSigma_{S}^{+}\bSigma_{S \bullet} ).
\end{equation}
The subset search \eqref{eq:pv} generalizes the original Principal Variables criterion \eqref{eq:mccabe_pv} by allowing $\bSigma$ to be singular. 

\Cref{prop:css_equiv_pv} establishes an exact mathematical equivalence between CSS \eqref{eq:css} and Principal Variables \eqref{eq:pv}. This equivalence implies that performing CSS on a centered data matrix $\X$ is identical to selecting principal variables using the sample covariance.
\begin{proposition}[CSS and Principal Variables are equivalent]
\label{prop:css_equiv_pv}
Consider a matrix $\X \in \R^{n \times p}$ and define $\hat{\bSigma} =  \X^\top\X/n$. For any size-$k$ subset $S \subseteq [p]$, the CSS objective \eqref{eq:css} with $\X$ and the Principal Variables objective \eqref{eq:pv} with $\hat{\bSigma}$ are equal:
\begin{equation*}
\min_{\B \in  \R^{k \times p}} \frac{1}{n}\|\X - \X_{\bullet S}\B\|_2^2 = \tr(\hat{\bSigma} - \hat{\bSigma}_{\bullet S} \hat{\bSigma}_{S}^{+}\hat{\bSigma}_{S \bullet}).
\end{equation*}
\end{proposition}
 

Conceptually, \Cref{prop:css_equiv_pv} suggests viewing CSS as an estimation problem in which the estimand is the population principal variable set. This draws a clear parallel between CSS and PCA: both methods seek to estimate some function of the population covariance, and the typical unit-level data characterization of each method does so by plugging in the sample covariance as an estimate.

In what follows, we formalize this point estimation view of CSS. Suppose that $\X \in \R^{n \times p}$ is a data matrix whose rows are centered i.i.d samples of a $p$-dimensional random vector $X \sim (\mu, \bSigma)$. It is then immediate that the CSS solution \eqref{eq:css} is the plug-in estimate for the following estimand:
\begin{equation}\label{eq:pop_css_centered}
\argmin_{S \subseteq [p]: |S| = k} \min_{\B \in  \R^{p \times k}} E[\|(X - \mu) -  \B (X_S -\mu_{S})\|_2^2]
\end{equation}
\Cref{prop:pop_css_equiv_pv} establishes that the population CSS subset search \eqref{eq:pop_css_centered} is equivalent to selecting principal variables using $\bSigma$. Furthermore, it demonstrates that, even when $X$ does not follow a non-degenerate Gaussian distribution, the subset selected by Principal Variables is characterized by an interpretable and desirable optimality criterion: optimal linear reconstruction of the remaining variables (after centering). 

\begin{proposition}[The CSS estimand]
\label{prop:pop_css_equiv_pv}
Consider a $p$-dimensional random vector $X \sim (\mu, \bSigma)$. For any size-$k$ subset $S \subseteq [p]$, the population CSS problem \eqref{eq:pop_css_centered} has the same objective as the Principal Variables objective with $\bSigma$:
\begin{equation*}
\min_{\B \in  \R^{p \times k}} E[\|(X - \mu) - \B (X_{S} -\mu_{S}) \|_2^2] = \tr(\bSigma - \bSigma_{\bullet S}\bSigma_{S}^{+}\bSigma_{S \bullet}).
\end{equation*}
\end{proposition}

Characterizing CSS in terms of covariances and reframing it as an estimation problem yields a number of benefits and insights:

\begin{enumerate}
    \item A more general class of estimators for the population CSS solution \eqref{eq:pop_css_centered} results from solving \eqref{eq:pv} with any estimate $\hat{\bSigma}$ of $\bSigma$, not just sample covariance. Henceforth we will refer to solving \eqref{eq:pv} with $\hat{\bSigma}$ as performing CSS with the covariance $\hat{\bSigma}$.
    \item It is immediately clear that practitioners can perform CSS in settings where they cannot access unit-level data but can access covariance or correlation estimates. For example, only the correlation matrix was available for four of the nine real data examples in \cite{Camida}, and in genome wide association studies practitioners often work with correlation estimates in place of inaccessible unit-level data \cite{Mak, Zihua}.
    \item Characterizing CSS as an estimation problem stresses the importance of selecting a subset that generalizes well. This is in line with recent work that emphasizes CSS's out-of-sample performance and examines the benefits of performing CSS with $\ell_2$ regularization \cite{Ordozgoiti2019}. Our equivalence suggests performing CSS with regularized covariance estimates as an alternative regularization scheme. 
    \item The NP-completeness of CSS implies the NP-completeness of solving \eqref{eq:pv}. Equivalently, if $X \sim N(0, \bSigma)$, $\bSigma \succeq 0$, finding a size $k$ subset $S \subset [p]$ that minimizes $\tr(\Cov(X|X_{S}))$ is NP-complete. 
    \item If $\X \in \R^{n \times p}$ has rank $r \ll \min(n, p)$, we can find $\X_{r} \in \R^{r \times p}$ such that $\X_{r}^\top\X_{r} = \X^\top\X$ via the singular value decomposition (SVD) of $\X$. The time complexity of doing so is $O(npr)$, and computing and then performing CSS on $\X_r$ may be much faster than performing CSS directly on $\X$. 
\end{enumerate}


\section{A Generative View of CSS}
\label{sec:gen_view}

In this section we show that CSS arises naturally from a certain probabilistic model and then discuss a useful generalization of this model. 

\subsection{CSS as Maximum Likelihood Estimation}

In what follows, we show that CSS and Principal Variables can both be viewed as maximum likelihood estimation within a certain semi-parametric model.  We call this viewpoint \emph{Probabilistic Column Subset Selection (PCSS)}. As the name suggests, PCSS provides a generative view of CSS the same way that PPCA does for PCA.  

We introduce the $k$-dimensional \emph{PCSS model}. It posits that each data vector $X \in \R^p$ arises from the following two-step sampling procedure:
\begin{align}
\tag{\ref{eq:pcss_model}, revisited}
\begin{split}
&X_S \sim F\\
&X_{-S} \mid X_S \sim N(\mu_{-S} + \W(X_S - E_{F}[X_{S}]), \sigma^2 \I_{p-k}). 
\end{split} 
\end{align}
In words, we first sample the $k$ principal variables $X_S$ from some distribution $F$, and then generate the remaining variables $X_{-S}$ as linear combinations of these principal variables plus spherical Gaussian noise. The parameters of this model are the set of principal variables $S$, their distribution $F$, the remaining variables' mean $\mu_{-S}$, the regression coefficients $\W$, and the residual variance $\sigma^2 > 0$. The size of the principal variable set, $k$, is fixed and not a parameter in the model. 

The relationship between PCSS and CSS is explained by \Cref{thm:css_is_mle}. It says that, in our PCSS model, any CSS solution is a maximum likelihood estimate (MLE) for $S$ over a non-parametric family of principal variable distributions $F$.  

\begin{theorem}[CSS solution is an MLE]
\label{thm:css_is_mle}
    Let $x^{(1)}, \dots, x^{(n)} \in \R^p$ be samples with sample covariance $\hat{\bSigma}$. For any $M <\infty $, let $\mathcal{P}_M$ be the set of distributions in the $k$-dimensional PCSS model \eqref{eq:pcss_model} where $F$ admits a probability density bounded by $M$. Then, so long as no $k$ variables achieve perfect in-sample linear reconstruction of the remaining variables, any CSS solution
    \begin{equation*}
    \hat{S} \in \argmin_{U \subset [p], |U| = k} \tr(\hat{\bSigma} - \hat{\bSigma}_{\bullet U}\hat{\bSigma}^{+}_{U}\hat{\bSigma}_{U\bullet})
    \end{equation*}
    is a maximum likelihood estimator for $S$ in the model $\mathcal{P}_M$. Moreover, each $\hat{S}$ is a maximum hybrid likelihood estimator (see \cite[Equation 9.1]{Owen}) for $S$ in the original model \eqref{eq:pcss_model} where $F$ is completely unrestricted.
\end{theorem}

The density bound $M < \infty$ is only in place to ensure that the likelihood is finite. Otherwise every subset $S$ results in infinite likelihood. Likewise, if some size-$k$ subset of variables achieves perfect in-sample linear reconstruction of the remaining (and thus attains the minimum CSS objective value of zero), then the likelihood for this subset tends to $\infty$ as we send $\sigma^2 \rightarrow 0$ and technically no MLE exists. 

\subsection{The Subset Factor Model}

By generalizing the probabilistic model \eqref{eq:pcss_model} in which the CSS solution is the MLE, we can motivate subset versions of other existing probabilistic methods.

In this vein, we introduce the $k$-dimensional \emph{subset factor model}, a generalization of our $k$-dimensional PCSS model that drops all distributional assumptions and allows the covariance of $X_{-S} \mid X_S$ to be a diagonal matrix $\D \succ \0$: 
\begin{align}
\begin{split}
&X_S \sim F \qquad \epsilon \sim (0, \D) \qquad \epsilon_1 \perp , \dots, \perp \epsilon_{p-k} \qquad \Cov(X_S, \epsilon) = \0 \\
&X_{-S} = \W(X_{S} - E_{F}[X_S]) + \mu_{-S} + \epsilon.
\end{split} \label{eq:subset_factor_model}
\end{align}
By dropping distributional assumptions and allowing a previously spherical conditional covariance to be diagonal, our subset factor model generalizes our PCSS model the same way the factor model \cite[Section 9.2.1]{Mardia} generalizes the model used for PPCA. This analogy would be even more apt if we dropped the independence condition $\epsilon_1 \perp, \dots, \perp \epsilon_{p-k}$ in \eqref{eq:subset_factor_model}, but this condition is necessary for our application in \Cref{sec:model_selection}. 

Unsurprisingly, our subset factor model has a close relationship with the factor model. In a $k$-dimensional factor model, each variable $X_i$ is a linear combination of latent common factors $Z \sim (0, \I_k)$ plus a mean-zero, unique factor $\epsilon_i$ and a non-random shift $\mu_i$. The unique factors $\epsilon_i$ have positive variance and are uncorrelated with $Z$ and each other. Similarly, in our $k$-dimensional subset factor model, an unknown subset of the observed variables act as common factors (once centered), and the remaining variables are linear combinations of these common factors plus a unique factor and non-random shift. 

The subset factor model offers a natural compromise between PCA and Factor Analysis. Factor Analysis enjoys two major benefits over PCA: (1) it provides an interpretable generative model for the data; and (2) the common factors $Z$ always explain all the non-unique variation in $X$, while principal components typically do not. Factor Analysis, however, suffers from a slew of unidentifiability and indeterminancy issues \cite{Anderson1956, Guttman, Steiger1994}. Proponents of components-based methods are particularly critical of factor indeterminacy, the fact that the common factors are not uniquely determined, even up to rotation \cite{Schonemann1976, Schonemann1978, Steiger1979,  Velicer}. In contrast, components (linear combinations of variables) are always determined.  

A natural way to compromise between the two methods is to assume a factor model where the factors are themselves components. Then, via an interpretable generative model, one can recover determined factors that explain all the non-unique variation in $X$. Unfortunately, this is known to be impossible when all the unique factors are required to have positive variance \cite{Schonemann1976}.  However, if we allow some unique factors to have zero variance, \Cref{thm:compromise} tells us this is equivalent to assuming a subset factor model. 
\begin{theorem}[The subset factor model compromise]
    \label{thm:compromise}
    Suppose $X \sim (\mu, \bSigma)$, $\bSigma \succ \0$ follows a $k$-dimensional factor model where we allow the $\epsilon_i$ to have zero variance. Then the common factors are components (i.e., $Z = \B(X - \mu)$ for some $\B \in \R^{k \times p}$) if and only if $X$ follows a $k$-dimensional subset factor model \eqref{eq:subset_factor_model}. 
\end{theorem}
The conclusion of \Cref{thm:compromise} still holds even if we drop the restriction that $\epsilon_1 \perp, \dots, \perp \epsilon_{p-k}$ in \eqref{eq:subset_factor_model}. 



\section{Applications}
\label{sec:applications}
In this section we illustrate some useful applications of the viewpoints provided earlier.  
\subsection{Scalable CSS without Unit-level Data}
\label{sec:scalable_css}
Our first application is scalable CSS without unit-level data. We provide novel algorithms that allow us to efficiently perform CSS when we can only access covariance estimates and a real data example that illustrates these algorithms' usefulness. These algorithms are made possible by the equivalence drawn in \Cref{sec:equivalence}. We also discuss some other reasonable approaches that perform surprisingly poorly on our real data example.  

\subsubsection{Algorithms}
\label{sec:algorithms}

As it stands, it is unclear how to efficiently perform CSS with just a covariance $\bSigma$. The Principal Variables literature suggests some sophisticated approximation algorithms, but it lacks efficient implementations of them. The CSS literature provides efficient approximation algorithms, but it is unclear how to apply them when we can only access covariance estimates. \Cref{prop:css_equiv_pv} implies that we could find $\X$ such that $\bSigma = \X^\top\X/n$ and apply existing CSS algorithms to $\X$. Unfortunately, factoring $\bSigma$ in this way has larger time complexity than some CSS algorithms themselves.

Hence, we derive versions of Farahat's greedy \cite{Farahat} and Ordozgoiti's swapping \cite{Ordozgoiti2018} algorithms that take a covariance $\bSigma$ as input. These two algorithms are among the best-performing and fastest algorithms in the CSS literature, and they can be easily modified to perform subset search according to other objectives. Working directly with $\bSigma$ results in simpler presentation and derivation of these algorithms, and our new algorithms maintain the same time complexity as the originals when $n \geq p$. For the interested reader, we discuss how to modify our algorithms to perform subset search according to the other three Principal Variables criteria in \Cref{sec:other_algs_appdx}. 

The key insight in deriving our algorithms is writing the CSS objective in terms of residual covariance matrices. For two mean-zero random vectors $X \in \R^{p}$ and $Y \in \R^{q}$ we define the residual from the regression of $Y$ on $X$ as 
\begin{equation*}
    R(Y, X) = Y - \B^* X \qquad \B^* \in \argmin_{\B \in \R^{q \times p}} E[\|Y - \B X \|_2^2].
\end{equation*}
The discussion in \Cref{sec:hilbert_space_appdx} establishes that $R(Y, X)$ is well defined. 

\Cref{lem:residual_cov_update} summarizes what we need to know about residual covariances. In its statement, we adopt the convention that $0 \cdot \infty = 0$.
\begin{lemma}[Efficient residual covariance update]
\label{lem:residual_cov_update}
    Consider a random vector $X \sim (0, \bSigma)$ and a subset $U \subset [p]$ that does not contain the $i$-th variable. Then the residual covariance for the subset $U$ is given by $\bSigma_{R(X, X_{U})} = \bSigma - \bSigma_{\bullet U}\bSigma_{U}^{+} \bSigma_{U\bullet}$, and the residual covariance for the subset $U \cup \{i\}$ is given by 
    \[\bSigma_{R(X, X_{U \cup \{i\}})} = \bSigma_{R(X, X_U)}  - \frac{\beta \beta^\top}{\beta_i} \cdot I_{\beta_i > 0}\]
    where $\beta = (\bSigma_{R(X, X_U)})_{\bullet i} =  \bSigma_{\bullet i} - \bSigma_{\bullet U}\bSigma_{U}^{+}\bSigma_{U, i}$. That is, we can update $\bSigma_{R(X, X_{U})}$ when adding variables to $U$ in $O(p^2)$ time, and removing variables from $U$ in $O(p^2)$ time if $\bSigma_{U}^{+}$ is known. 
\end{lemma}
\Cref{lem:residual_cov_update} illustrates that CSS amounts to finding a subset $S$ that minimizes $\tr(\bSigma_{R(X, X_{S})})$, and it also allows us to quickly update $\tr(\bSigma_{R(X, X_{S})})$ when we add or remove variables from $S$. Define the function  
\begin{equation}
\label{eq:alg_function}
    f(i, \A) = -\|\A_{\bullet i}\|_2^2/\A_{ii} \cdot I_{\A_{ii} > 0}.
\end{equation}
Considering some $U \subset [p]$, computations in \Cref{sec:alg_correctness_appdx} show that any $i \not \in U$ which minimizes $f(i, \bSigma_{R(X, X_{U})})$ also minimizes $\tr(\bSigma_{R(X, X_{U  \cup \{i\}})})$. This is sufficient for establishing the correctness of the greedy and swapping algorithms we provide below. \newline 

\noindent \textbf{Greedy Subset Selection:} Our greedy algorithm (\Cref{alg:greedy}) starts with the empty subset $S^{(0)} = \{ \}$ and, on the $t$-th iteration, finds $S^{(t)}$ by adding the variable to $S^{(t-1)}$ that minimizes $\tr(\bSigma_{R(X, X_{S^{(t)}})})$. It terminates after $k$ iterations and returns $S^{(k)}$. Alternatively, we can allow it to iterate until $\tr(\bSigma_{R(X, X_{S^{(t)}})})$ is sufficiently small. \Cref{alg:greedy}'s time complexity is $O(p^2k)$. A nice property of greedy subset search is that greedily selected subsets of different sizes are nested: upon finding the greedily selected size-$k$ subset, we have necessarily found the greedily selected subsets of size $k-1, \dots, 1$ in the process. \newline 

\begin{algorithm}[H]
\begin{algorithmic}
\Require covariance matrix $\bSigma$, number of columns to select $k$

\vspace{.15cm}

\Procedure{greedy\_subset\_selection($\bSigma, k$)}{} 

\State $S^{(0)} \gets \{ \}$ 
\State $\bSigma_{R(X, X_{S^{(0)}})} \gets \bSigma$ 
\For{$t= 1, \dots, k$}
    \State $i^* \gets \argmin_{i \not \in S^{(t-1)}} f(i, \bSigma_{R(X, X_{S^{(t-1)}})}) $   \Comment{$f$ is from (\ref{eq:alg_function}), arbitrarily break ties}
    \State $S^{(t)} \gets S^{(t-1)} + i^* $, 
    \State Compute $\bSigma_{R(X, X_{S^{(t)}})}$ from $\bSigma_{R(X, X_{S^{(t-1)}})}$ using \Cref{lem:residual_cov_update}
\EndFor
\State \textbf{return:} $S^{(k)}$
\EndProcedure
\end{algorithmic}
\caption{Greedy Subset Selection}
\label{alg:greedy}
\end{algorithm}

\noindent \textbf{Subset Selection via Swapping}: On each iteration of our swapping algorithm (\Cref{alg:swap}) we loop through our currently selected subset $S$ and swap each selected variable out for the variable that most reduces $\tr(\bSigma_{R(X, X_{S})})$. If no swap reduces $\tr(\bSigma_{R(X, X_{S})})$ for a particular selected variable, then we keep that variable in $S$. This is reminiscent of coordinate descent, a procedure that searches for a minimum of $f: \R^q \rightarrow \R$ by iteratively optimizing each coordinate while holding the others fixed. \Cref{alg:swap} terminates after an iteration where no swaps are made, at which point the algorithm has converged to a kind of local optimum. Since each swap strictly reduces the objective value, \Cref{alg:swap} must converge in finite time. Its time complexity is $O(p^2k)$ per iteration. In practice, we run the algorithm multiple times with different random initializations and take the best resulting subset.  

\begin{algorithm}[H]
\begin{algorithmic}
\Require covariance matrix $\bSigma$, number of columns to select $k$, initial selected subset $S \in [p]^k$ 

\vspace{.15cm}

\Procedure{swapping\_subset\_selection($\bSigma, k, S$)}{} 
\State $S_{copy} \gets 0 \in \R^{k}$
\State $\bSigma_{S}^{+} \gets$ compute pseudo-inverse of $\bSigma_{S}$
\State $\bSigma_{R(X, X_{S})} \gets \bSigma - \bSigma_{\bullet S} \bSigma_{S}^{+} \bSigma_{ S \bullet}$  
\While{$S \neq S_{copy}$}
    \State $S_{copy} \gets S$
    \For{$j=1, \dots, k$}
        \State $U \gets S_{-j}$  \Comment{ $S_{-j} \in R^{k-1}$ is $S$ with $S_j$ removed}
        \State $\bSigma_{U}^{+} \gets$ compute $\bSigma_{U}^{+}$ from $\bSigma_{S}^{+}$ using \cite[3.2.7]{Petersen} \Comment{Takes $O(k^2)$ time}
        \State $\bSigma_{R(X, X_{U})}\gets  $ compute $\bSigma_{R(X, X_{U})}$ from $\bSigma_{R(X, X_{S})}$ using \Cref{lem:residual_cov_update}
        \If{ $S_j \not \in \argmin_{i \not \in U} f(i, \bSigma_{R(X, X_{U})} )  $ }  \Comment{$f$ is from (\ref{eq:alg_function})}
            \State $i^{*} \gets \argmin_{i \not \in U} f(i, \bSigma_{R(X, X_{U})} )$  \Comment{Arbitrarily break ties}
            \State $S_j \gets i^*$  \Comment{This alters $S$ in place}
            \State $\bSigma_{S}^{+} \gets$ compute $\bSigma_{S}^{+}$ from $\bSigma_{U}^{+}$ using \cite[3.2.7]{Petersen} \Comment{Takes $O(k^2)$ time}
            \State $\bSigma_{R(X, X_{S})}\gets  $ compute $\bSigma_{R(X, X_{S})}$ from $\bSigma_{R(X, X_{U})}$ using \Cref{lem:residual_cov_update}  
        \EndIf
    \EndFor
\EndWhile
\State \textbf{return:} $S$
\EndProcedure
\end{algorithmic}
\caption{Subset Selection by Swapping}
\label{alg:swap}
\end{algorithm}

\subsubsection{An Example from BlackRock}
\label{sec:blackrock}

We provide a real world application where BlackRock, an industry affiliate of ours, seeks to perform CSS and only has access to a covariance estimate. BlackRock is currently the largest asset manager in the world. For privacy reasons, the data has been anonymized.

BlackRock has a collection of $p = 774$ thematic investment portfolios and would like to find a small subset of them that are representative of the remaining. Specifically, they are interested in performing CSS. Thematic portfolios invest in assets that are tied together by an interpretable common thread, e.g., renewable energy companies or start-ups in India. It is important to select a subset of portfolios (rather than find linear combinations of them) so that this thematic structure is maintained. 

BlackRock does not have unit-level returns for each portfolio, but they do have a covariance estimate. After standardizing the returns of each portfolio to have unit-variance, we perform CSS using our greedy and swapping algorithms for $k=1, \dots, 30$. For our swapping algorithm, we try 25 random initializations. The results are presented in \Cref{fig:BlackRock}, where we plot the (estimated) average $R^2$ from the regression of each individual portfolio on the selected subset. For the size six subset found by our swapping algorithm, this average $R^2$ is above 90\%. The swapping algorithm consistently outperforms the greedy one, and for smaller subset sizes it is able to get the same performance with one less variable. For sake of comparison, we also plot the average $R^2$ from the regression of each portfolio on the top $k$ principal components. The principal components maximize this $R^2$ among all possible linear combinations of the portfolios. We find that the performance gap between PCA and CSS is well worth the trade-off in interpretability. For smaller subset sizes, CSS achieves the same performance as PCA while using a subset of size only one larger than the number of principal components. 

It is important to use efficient algorithms for problems of this scale. The Principal Variables literature, which mainly considers $p < 100$, suggests recomputing the objective from scratch for each new considered subset \cite{Camida}. For the BlackRock data, using Algorithm \ref{alg:greedy} to greedily select a size-$30$ subset takes less than a tenth of a second, while this naive approach takes more than six minutes. For subset selection via swapping, Algorithm \ref{alg:swap} typically runs in less than half a second, while this naive approach often takes over an hour (depending on the number of iterations until convergence). We remark that although the swapping algorithm finds better subsets, the greedy algorithm has comparable performance and is notably faster, especially since it only needs to be run once for the largest $k$ of interest.  

% Figure environment removed

\subsubsection{Other Approaches}

Another approach to finding approximate CSS solutions is formulating a convex relaxation of the original objective \cite{Balzano, WangY, Masaeli}. The group lasso \cite{Yuan} provides one such natural convex relaxation:
\begin{equation}
\label{eq:group_lasso}
\argmin_{\B \in  \R^{p \times p}} \|\X -  \X \B\|_F^2 + \lambda \sum_{i=1}^p \|\B_{i\bullet} \|_2
\end{equation}
For large values of $\lambda$, the solution to \eqref{eq:group_lasso} has many zeroed-out rows, i.e., many variables are not used in the reconstruction.  This suggests the following procedure for CSS: tune $\lambda$ in \eqref{eq:group_lasso} to yield exactly $k$ non-zero rows and select the variables corresponding to these rows. Another approach,  Convex Principal Feature Selection (CPFS) \cite{Masaeli}, solves \eqref{eq:group_lasso} but with an $\ell_{\infty}$ penalty on the rows of $\B$ instead of a $\ell_2$ one.

We can characterize \eqref{eq:group_lasso} solely in terms of covariances and therefore also apply this group lasso approach to BlackRock's data. Defining $\hat{\bSigma} = \X^T\X/n$, \eqref{eq:group_lasso} is equivalent to 
\begin{equation*}
\argmin_{\B \in  \R^{p \times p}} \tr(\hat{\bSigma} - 2\B^T\hat{\bSigma} + \B^{T}\hat{\bSigma}\B) + \frac{\lambda}{n} \sum_{i=1}^p \|\B_{i\bullet} \|_2
\end{equation*}

Surprisingly, on BlackRock's data, this group lasso approach performs worse than selecting subsets at random! \Cref{fig:BlackRock} displays the group lasso's performance as well as the best performance from 25 randomly selected subsets. The subset chosen by the group lasso performs notably worse than the best random subset. We were unable to tractably apply CPFS to the whole dataset, so we tried it on a random sub-sample of one hundred portfolios. On this sub-sample, CPFS did even worse than the group lasso (results not shown). Empirically, we found that the group lasso and CPFS systematically pulled in variables that were highly correlated with those that had already been selected. Our results are another example of convex variable selection methods performing poorly in settings with high correlation \cite{Freijeiro}. 


\subsection{CSS with Missing and/or Censored Data}
\label{sec:missing_data}

The equivalence drawn in \Cref{sec:equivalence} suggests a natural workflow for performing CSS when some data is missing and/or censored: model the missingness and/or censorship, produce a covariance estimate that is reasonable under this model, and perform CSS using this covariance estimate. 

As an illustrative example of this workflow, we use it to suggest a procedure for performing CSS with missing-at-random data. Suppose we observe i.i.d samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ and some values are missing-at-random. Let the set $\mathcal{I}_{s}$ be the indices of samples for which the $s$-th variable is not missing, and $\mathcal{I}_{st}$ be the indices of samples for which the $s$ and $t$-th variables are not missing. To get a reasonable covariance estimate $\hat{\bSigma}$, we estimate the pairwise covariances using the not-missing values and then project onto the positive semi-definite cone:
\begin{equation*}
    \hat{\bSigma} = \argmin_{\A \in \S_{+}^{p \times p}} \|\A - \hat{\boldsymbol{\Psi}} \|_F^2 \qquad  \hat{\boldsymbol{\Psi}}_{st} = \frac{1}{|\mathcal{I}_{st}|} \sum_{i \in \mathcal{I}_{st}} \left(x_s^{(i)} - \frac{1}{|\mathcal{I}_{s}|}\sum_{j \in \mathcal{I}_{s}} x_s^{(j)}\right) \left(x_t^{(i)} - \frac{1}{|\mathcal{I}_{t}|}\sum_{j \in \mathcal{I}_{t}} x_t^{(j)}\right)
\end{equation*}
Projecting a symmetric matrix onto the positive semi-definite cone amounts to setting its negative eigenvalues to zero. Finally, we apply the algorithms from \Cref{sec:algorithms} to $\hat{\bSigma}$.

Using both simulated and real data, we demonstrate that our procedure outperforms the two existing methods for CSS with missing data. Initial work on this problem by \cite{Balzano} uses a greedy Block Orthogonal Matching Pursuit (BOMP) algorithm to approximately solve a CSS-like objective that ignores the missing values. In \cite[Equation 40]{WangY}, the authors 
mask the missing values and solve a group lasso penalized problem similar to \eqref{eq:group_lasso}. As a remark, we point out that one could put a parametric assumption on the principal variable distribution $F$ in the model \eqref{eq:pcss_model} and apply the expectation-maximization (EM) algorithm. We found this to be too sensitive to the distributional assumptions on $F$ and the EM algorithm's initialization to work well in practice. 

\subsubsection{A Simple Simulation Study}
\label{sec:missing_sim}
We run a simulation study to evaluate the quality of each method. In each trial, we draw $n=200$ samples from a particular distribution in our PCSS model \eqref{eq:pcss_model} and randomly omit each observed value with probability $0.05$. The distribution we sample from has $p = 20$ unit-variance variables and a ground truth subset size of $k=4$. It is described in full detail in \Cref{sec:missing_data_dist_appdx}.

For each trial, we select a size-four subset using each method. We also select a random size-four subset as a benchmark. For our method, we use our swapping algorithm and try ten random initializations. For the group lasso, we find the regularization strength that selects a size-four subset via a binary search. We run one thousand trials and use three metrics to measure the quality of each method: (1) whether or not the selected subset $\hat{S}$ is exactly equal to the ground truth subset $S$; (2) the size of the intersection of $\hat{S}$ and $S$; and (3) the selected subset's population CSS objective $\tr(\bSigma - \bSigma_{\bullet\hat{S}}\bSigma_{\hat{S}}^{-1} \bSigma_{\hat{S}\bullet})$, where $\bSigma$ is the population covariance. Recovery of the ground truth subset $S$ is indeed a reasonable performance metric as, in our example, $S$ achieves the lowest population CSS objective among all size-$4$ subsets.  

Table \ref{table:missing_sim} documents the results from our simulation. Our method selects the correct subset every time while BOMP selects it less than half the time and group lasso fails to select it even once. As expected, our method also achieves the lowest average CSS objective value by a significant amount. 


\begin{table}
\centering
\begin{tabular}{ |p{2cm}||p{4cm}|p{4.2cm}|p{2.5cm}|}
 \hline
Method & \% Corr. Subset Selected & \# Corr. Variables Selected & CSS Obj.   \\
 \hline
 Our Method  &  $\boldsymbol{1.000}$  &  $\boldsymbol{4.000 \pm 0.000}$   & $\boldsymbol{2.400 \pm 0.000}$ \\
 BOMP &   $0.456$  &  $2.191 \pm 0.056$   & $3.223 \pm 0.025$\\
 Group Lasso &  $0.000$  &  $1.713 \pm 0.016$  & $5.806 \pm 0.037$\\
 Random & $0.001$ & $0.804 \pm 0.023$ & $6.065 \pm 0.045$ \\
 \hline
\end{tabular}
\caption{The proportion of times the correct subset was selected (\% Corr. Subset Selected), the average size of the intersection of the selected subset and correct subset (\# Corr. Variables Selected), and the population CSS objective (CSS Obj.) for the thousand simulated trials described in \Cref{sec:missing_sim}. The $\pm$ reports one standard error. Bold indicates best performance.}
\label{table:missing_sim}
\end{table}


\subsubsection{An Example with Ozone Level Detection Data}
\label{sec:missing_real}

We consider the Ozone Level Detection Dataset from the UCI machine learning repository \cite{Dua}. The dataset has $n=2535$ samples of $p=73$ variables. So that we can compute a ground truth CSS objective value for any selected subset, we drop samples with already missing values and are left with $n=1847$. 

One hundred times, we randomly omit observed values with probability $q=0.05, 0.1, 0.2$ and select a size $k=5, 10, 20$ subset using our method and BOMP. For our method, we use our swapping algorithm and only try one random initialization. We neglect the group lasso approach because it is too slow for a problem of this scale. Given its noted bad performances, we expect it would have done poorly. To measure a selected subset's quality, we compute the CSS objective value it would have attained on the fully observed dataset. As a benchmark for how well we can expect to do with no missing data, we run our swapping algorithm on the fully observed dataset with one hundred different random initializations and record the best result. 

We provide the results from our experiment in \Cref{fig:ozone_results}. Our method significantly outperforms BOMP in every setting and performs similarly to our no-missing-data benchmark when $k$ and $q$ are not too large. Also, our method's performance appears to be less variable than BOMP's. 

% Figure environment removed


\subsection{Selecting the Subset Size for CSS} \label{sec:model_selection}

Our final application is, given some observed data, determining the smallest subset size needed to capture the data's underlying structure. Our formalization of this problem relies entirely on the generative view discussed in \Cref{sec:gen_view}. The procedure we propose showcases how this generative view facilitates the application of statistical tools in subset selection problems. 

Formally, we aim to find $k^*$, the smallest non-negative integer $k$ for which the $k$-dimensional subset factor model \eqref{eq:subset_factor_model} holds. Recall that the $k$-dimensional subset factor model consists of distributions satisfying \eqref{eq:subset_factor_model} for some size-$k$ factor set $S$:
\begin{align}
\tag{\ref{eq:subset_factor_model}, revisited}
\begin{split}
&X_S \sim F \qquad \epsilon \sim (0, \D) \qquad \epsilon_1 \perp , \dots, \perp \epsilon_{p-k} \qquad \Cov(X_S, \epsilon) = \0 \\
&X_{-S} = \W(X_{S} - E_{F}[X_S]) + \mu_{-S} + \epsilon.
\end{split} 
\end{align}
We focus on the subset factor model over the PCSS model \eqref{eq:pcss_model} because it is more general and hence more representative of the structure we expect to see in real data. An interested reader can find the analogous discussion for the PCSS model in \Cref{sec:model_selection_pcss_appdx}.  Prior to proceeding, we point out that subset factor models of increasing subset size are nested. This is because any distribution satisfying \eqref{eq:subset_factor_model} for a set $S$ also satisfies \eqref{eq:subset_factor_model} for any superset of $S$. 

Our approach is to output $\hat{k}$, the smallest $k$ for which we cannot falsify that the data generating distribution belongs to the $k$-dimensional subset factor model. Formally, $\hat{k}$ is the smallest $k$ for which we fail to reject the null $H_{0, k}: |S| \leq k$ that the data is drawn from a distribution 
satisfying \eqref{eq:subset_factor_model} for some size-$k$ set $S$.  

To start, we present an idealized procedure that guarantees a notion of error control. This ideal procedure controls the probability that the suggested subset size is too large, i.e., that $\hat{k} > k^*$.  Unfortunately, the procedure is computationally intractable, and we ultimately use it to motivate a tractable procedure that performs well in practice. A more technical and detailed discussion of this ideal procedure can be found in \Cref{sec:model_selection_appdx}.

For a fixed $k$, our idealized procedure rejects the null $H_{0, k}: |S| \leq k$ when the statistic $T_k$ is large:
\begin{equation}
\label{eq:test_stat}
T_k = \min_{\substack{U \subset [p]: |U| = k}} T(U),  \qquad  T(U) = n \log \left(\frac{|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U})|}{|\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U}| } \right) 
\end{equation}
The statistic $T_k$ is the generalized likelihood ratio test statistic for testing the null that the data is drawn from a Gaussian distribution in the $k$-dimensional subset factor model (see  \Cref{sec:model_selection_test_stat_appdx} for details). Although the test statistic $T_k$ is designed for Gaussians, we use it while making no such distributional assumptions. Intuitively, the statistic $T_k$ is large when the residual covariance from the regression of $X_{-U}$ on $X_{U}$ is highly non-diagonal for every size-$k$ subset $U$. If this is the case, the $k$-dimensional subset factor model cannot possibly hold. 

In what follows we determine an appropriate critical value for the aforementioned test. By definition, the null $H_{0, k} : |S| \leq k$ is true exactly when $k \geq k^*$. Bearing this in mind, fix some $k \geq k^*$. For simplicity, suppose that the data generating distribution admits a density and restrict the subset factor model so that the unique factors $\epsilon_j$ are Gaussian and independent of the principal variables. In this simpler setting, whenever the data generating distribution satisfies \eqref{eq:subset_factor_model} for some size-$k$ set $S$, the statistic $T(S)$ has distribution exactly 
\begin{equation}
\label{eq:null_dist}
    n \sum_{j=2}^{p-k} \log \left(1 + \frac{\tilde{\chi}^{2}_{j - 1}}{\chi^2_{n- k - j}} \right).
\end{equation}
Thus, rejecting when $T_k =\min_{U: |U| = k} T(U)$ is larger than the critical value 
\begin{equation}
\label{eq:crit_value}
Q_{n,p,k}(1 - \alpha) = \textup{Quantile} \left\{ 1 - \alpha, n \sum_{j=2}^{p-k} \log \left(1 + \frac{\tilde{\chi}^{2}_{j - 1}} 
{\chi^2_{n- k - j}}\right) \right\}
\end{equation}
is a level-$\alpha$ test for the null $H_{0, k} : |S| \leq k$ . In \eqref{eq:null_dist} and \eqref{eq:crit_value}, $\{ \chi^2_{\ell} \}, \{ \tilde{\chi}^{2}_{\ell}\}$ are mutually independent chi-squared random variables with degrees of freedom specified by their subscript. Surprisingly, the same critical value guarantees asymptotic type-I error control in the fully general case.

\Cref{thm:error_control} establishes that, by using the above critical value, our ideal procedure does not suggest a subset size that is too large with high probability. 

\begin{theorem}[Error control]
\label{thm:error_control}
    Consider $n > p$ samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ from a distribution $P$ and let $k^*$ be the smallest non-negative integer $k$ for which $P$ belongs to the $k$-dimensional subset factor model \eqref{eq:subset_factor_model}. If $\hat{k}$ is the smallest $k$ for which the test statistic $T_k$ from \eqref{eq:test_stat} is not strictly larger than the critical value $Q_{n, p, k}(1 - \alpha)$ defined in \eqref{eq:crit_value}, then 
    \begin{equation*}
        \limsup_{n \rightarrow \infty} P(\hat{k} > k^*) \leq \alpha.
    \end{equation*}
    That is, our ideal procedure suggests a subset size that is too large with probability at most $\alpha$ in large samples. Furthermore, if we restrict the subset factor model \eqref{eq:subset_factor_model} to have Gaussian unique factors $\epsilon_j$ that are independent of the principal variables $X_S$, then the above conclusion holds in finite samples, i.e., $P(\hat{k} > k^*) \leq \alpha$. 
\end{theorem}

With the result of \Cref{thm:error_control} in mind, we make two remarks about the output $\hat{k}$ of our ideal procedure. First, if the test statistic $T_k$ typically exceeds the critical value $Q_{n, p, k}(1-\alpha)$ whenever $k < k^*$ (i.e., the test is powerful for the nulls $H_{0, k}: |S| \leq k$ ,  $k < k^*$), our ideal procedure should return $k^*$ with high probability. Second, whenever our ideal procedure returns $\hat{k} > k^*$, the outputted $\hat{k}$ should seldom be much larger than $k^*$. Since $T_k$ minimizes $T(\cdot)$ over all size-$k$ subsets, the proposed critical value becomes increasingly conservative when the data generating distribution satisfies \eqref{eq:subset_factor_model} for many size-$k$ subsets. As $k$ exceeds $k^*$, the proportion of size-$k$ subsets for which \eqref{eq:subset_factor_model} is satisfied grows quickly to one. Also, for $k$ up to $\ceil{(k^* + p)/2}$, there exist subsets $S$ for which \eqref{eq:subset_factor_model} is satisfied with progressively larger symmetric differences, and their corresponding statistics $T(S)$ are accordingly less positively correlated. These facts combined suggest that the probability of rejecting $H_{0, k}: |S| \leq k$ shrinks quickly as $k$ increases past $k^*$. 

Since computing the test statistic $T^*_k$ requires an exhaustive subset search, our idealized procedure is admittedly computationally intractable. Still, we can use it as motivation to design a tractable procedure that works well in practice. 

Our practical approach is to find a size-$k$ subset $\hat{S}$ that approximately minimizes $T(\cdot)$, and then to reject the null $H_{0, k}: |S| \leq k$ when $T(\hat{S})$ exceeds the same critical value $Q_{n, p, k}(1-\alpha)$ as before. We again output the smallest $k$ for which we fail to reject. Our approach is analogous to how, when the likelihood is not convex, a statistician may use an approximately maximized likelihood to run a likelihood ratio test (e.g. when selecting the number of factors to use in a factor model \cite{Beaujean}). \Cref{sec:other_algs_appdx} describes how to modify  \Cref{sec:algorithms}'s algorithms to search for this subset $\hat{S}$.

While our practical procedure does not guarantee error control, we still expect it to output $\hat{k}$ that will rarely be much larger than $k^*$. Again fix $k \geq k^*$. So long as our algorithms happen upon just one of the size-$k$ subsets for which \eqref{eq:subset_factor_model} is satisfied, we expect that $T(\hat{S})$ will be smaller than $Q_{n, p, k}(1-\alpha)$ with probability at least $1 - \alpha$ (or much more when $k$ is larger than $k^*$). As mentioned before, the proportion of size-$k$ subsets for which \eqref{eq:subset_factor_model} is satisfied quickly approaches one as $k$ increases, and it thus becomes correspondingly more likely that our approximation algorithms will happen upon such a subset.

Ultimately, if our procedure suggests selecting a size-$\hat{k}$ subset, we suggest selecting the size-$\hat{k}$ subset $\hat{S}$ that approximately minimizes $T(\cdot)$. When considering the subset factor model, a subset that fits the data well is one that results in a reasonably diagonal residual covariance. The best subset we can find by this standard is $\hat{S}$. Also, upon restricting the subset factor model to multivariate Gaussian distributions, $\hat{S}$ acts as an approximation for the MLE of $S$ (see \Cref{sec:model_selection_selected_subset_appdx} for details). 

In what follows, we verify that our practical procedure suggests reasonable subset sizes in some difficult simulated settings. Then, we demonstrate how it can be applied to dramatically reduce the length of a personality survey. 

\subsubsection{A Simulated Example}
\label{sec:model_selection_sim}

We consider a difficult simulated example that reflects the real data example to be presented in \Cref{sec:model_selection_survey}. For each trial, we draw $n=200$ samples from a mean-zero distribution in the subset factor model  \eqref{eq:subset_factor_model} with $p = 50$ variables and a size $k = 20$ population factor set $S$. For context, there are over 47 trillion size-20 subsets of 50 variables. We consider two cases: one where the unique factors are Gaussian, and one where they are a collection of independent Rademacher, $t$-distributed, and centered exponential random variables. Within these cases we consider distributions with varying amounts of ``signal'', which we define to be the average population $R^2$ from the regression of the variables not in $S$ on those in $S$. \Cref{sec:model_selec_dist_appdx} provides exact descriptions of the distributions we use, and \Cref{sec:model_selec_results_appdx} documents additional results for other sample sizes $n$.

For each trial we apply our procedure at level $\alpha=0.05$ and record (1) the size of the selected subset $\hat{S}$; (2) the size of the intersection of $\hat{S}$ and $S$; and (3) the population sum of square canonical correlations between the variables in $\hat{S}$ and $S$. The sum of square canonical correlations measures the degree to which the variables in $S$ and $\hat{S}$ span similar subspaces (see \cite[Section 3.1]{Schneeweiss} for more detail), and it attains its maximum value of $k=20$ when $S \subseteq \hat{S}$. To search for the subset that minimizes $T(\cdot)$ we use our swapping algorithm and try ten random initializations. 

We provide the results from our simulations in \Cref{fig:model_selec_sim}. As expected, our method under selects when the signal size is too small. In these cases the covariance looks reasonably diagonal to begin with, so it is difficult to gauge whether more variables are needed to achieve a diagonal residual covariance. For reasonable to large signal sizes the procedure typically suggests selecting 20 or 21 variables, and it seldom suggests selecting more than 22. In these settings, selected subset $\hat{S}$ is almost always a superset of the population subset $S$. The results when the unique factors are non-Gaussian versus Gaussian look nearly identical, suggesting that our procedure has reasonable small sample performance even when the unique factors are highly non-Gaussian. 

% Figure environment removed

\subsubsection{Reducing Survey Length via Our Procedure}
\label{sec:model_selection_survey}

In recent years, psychologists have been increasingly interested in creating shorter versions of existing surveys. A survey paper from 2013 \cite{Kruyen} found that, among the 2273 articles that appeared in six peer-reviewed psychological journals from 2005 to 2010, 170 articles contained abbreviated surveys. Developing a novel shortened survey was one of the main contributions in 84 of these articles.

Despite the increasing demand for shortened surveys, there is no standardized way of shortening them. Psychologists default to a number of arbitrary heuristic approaches that require extensive human labor and are susceptible to personal biases. Furthermore, reliability, the one concrete and standard measure of a shortened survey's quality, is highly sensitive to the restrictive assumptions of classical test theory \cite{Allen}. These assumptions are often violated in modern, multi-dimensional settings \cite{McNeish}. 

We propose using our procedure to shorten surveys. Essentially, we suggest selecting a subset of questions that, once accounted for, render the information in the remaining questions independent. To demonstrate our procedure's effectiveness, we consider the Big Five Inventory (BFI) \cite{John}, a $p = 44$ question personality survey. There has been much interest in reducing the BFI survey, as evidenced by articles proposing a 10-question version \cite{Rammstedt}, a 15-question version \cite{Gerlitz}, and multiple 20-question versions \cite{Engvik, Gouveia, Tucakovic}.  Each question in the survey is attributed to one of the five personality factors \cite{Digman}: extraversion, agreeableness, conscientiousness,  neuroticism, and openness. 

The BFI dataset we consider has responses from $n=228$ undergraduate students at a large US public university and can be found on CRAN \cite{Zhang} in the \texttt{EFAutilities} package. We apply our procedure at level $\alpha=0.05$ and search for the minimizing subset using our swapping algorithm with just one random initialization (using more than one random initialization gives identical results). Within less than a tenth of a second, our procedure selects a size-19 subset of the original 44 questions, cutting the survey in more than half.

Close examination of the selected questions suggests that they indeed capture the relevant information in the survey. First, the selected questions are distributed fairly evenly across the personality factors. \Cref{table:factor_attribution} provides a breakdown of how many questions are attributed to each factor in both the original and reduced surveys. Second, performing a 5-factor exploratory factor analysis reveals that the reduced survey maintains the original survey's factor structure (psychologists may refer to this as ``construct validity''). \Cref{table:factor_loadings} displays the loadings from this factor analysis.

\begin{table}
\centering
\begin{tabular}{ |p{3cm}||p{3.8cm}|p{3.2cm}|}
 \hline
Factor & \# in Reduced Survey & \# in Original Survey   \\
 \hline
 Extraversion  & 4 &  8 \\
 Agreeableness  & 4  &  9 \\
 Conscientiousness  & 5 &  9 \\
 Neuroticism  &  3 &  8  \\
 Openness &  3 &  10 \\
 \hline
\end{tabular}
\caption{The number of questions attributed to each of the five personality factors for both the original and reduced surveys from \Cref{sec:model_selection_survey}.}
\label{table:factor_attribution}
\end{table}

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
  \hline
   & Factor 1 & Factor 2 & Factor 3 & Factor 4 & Factor 5 \\
  \hline
  Extraversion 1 & 0.11 & \textbf{0.77} & 0.10 & -0.06 & -0.03 \\
  Extraversion 2 & -0.14 & \textbf{0.54} & 0.10 & 0.25 & 0.13 \\
  Extraversion 3 & 0.23 & \textbf{-0.54} & 0.07 & 0.19 & -0.05 \\
  Extraversion 4 & -0.02 & \textbf{0.81} & -0.01 & 0.10 & 0.02 \\
  Agreeableness 1 & 0.03 & 0.17 & -0.06 & \textbf{0.47 }& 0.15 \\
  Agreeableness 2 & 0.14 & -0.16 & 0.16 & \textbf{-0.66} & 0.02 \\
  Agreeableness 3 & 0.12 & -0.08 & 0.21 & \textbf{0.67} & 0.07 \\
  Agreeableness 4 & \textbf{0.35} & 0.09 & -0.06 & \textbf{-0.52} & -0.07 \\
  Conscientiousness 1 & 0.10 & 0.11 & -0.01 & 0.23 & \textbf{0.48} \\
  Conscientiousness 2 & 0.09 & 0.11 & 0.14 & -0.01 & \textbf{-0.60} \\
  Conscientiousness 3 & -0.03 & -0.09 & 0.24 & -0.03 & \textbf{0.55} \\
  Conscientiousness 4 & 0.06 & 0.11 & -0.02 & 0.05 & \textbf{0.66} \\
  Conscientiousness 5 & \textbf{0.50}& -0.03 & 0.01 & 0.12 & \textbf{-0.36} \\
  Neuroticism 1 & \textbf{-0.57 }& 0.02 & 0.03 & 0.05 & 0.10 \\
  Neuroticism 2 & \textbf{0.63} & -0.07 & -0.15 & 0.07 & -0.08 \\
  Neuroticism 3 & \textbf{0.79} & 0.03 & -0.02 & -0.13 & 0.09 \\
  Openness 1 & 0.01 & 0.11 & \textbf{0.68} & 0.01 & -0.06 \\
  Openness 2 & -0.10 & 0.04 & \textbf{0.76} & -0.01 & 0.03 \\
  Openness 3 & 0.05 & -0.14 & \textbf{0.50} & 0.07 & 0.00 \\
  \hline
  \end{tabular}
  \caption{Factor loadings (after rotation) from a exploratory factor analysis on the reduced 19-question survey from \Cref{sec:model_selection_survey}. As is common in the psychology literature, loadings with magnitudes larger than 0.3 are bolded.  }
  \label{table:factor_loadings}
\end{table}

\section{Acknowledgements}
We thank Ron Kahn,  Ked Hogan, and  Bobby Luo from BlackRock for sharing ideas and their example with us.  We thank Stephen Boyd, John Cherian, Kevin Guo, Tim Morrison, Yash Nair, Asher Spector, Rob Tibshirani, and James Yang for helpful discussions. Anav Sood would like to especially acknowledge Kevin Guo for helpful discussions surrounding the probabilistic view and associated application. Trevor Hastie was partially supported by grant DMS-2013736 from the National Science Foundation, and grant 5R01 EB
001988-21 from the National Institutes of Health.


\bibliographystyle{plain}
\bibliography{bibliography.bib}

\begin{appendix}

\section{Additional Simulation Details and Results}
\label{sec:sims_appdx}

\subsection{Missing Data Simulation Setting}
\label{sec:missing_data_dist_appdx}

We describe the distribution used in \Cref{sec:missing_sim}. Recall the PCSS model:
\begin{align}
\tag{\ref{eq:pcss_model}, revisited}
\begin{split}
&X_S \sim F\\
&X_{-S} \mid X_S \sim N(\mu_{-S} + \W(X_S - E_{F}[X_{S}]), \sigma^2 \I_{p-k}). 
\end{split} 
\end{align}
We set the ground truth subset to be  $S = \{1, 2, 3, 4\}$ and let $X_{S}$ have a mean-zero multivariate Gaussian distribution with equicorrelated covariance $0.75\cdot \I_{4} + 0.25 \cdot 11^\top$. We set $\sigma^2 = 0.15$. The matrix $\W$ is given below:
\begin{equation*}
   \W = \begin{bmatrix}
        \sqrt{17/90} & \sqrt{17/90} & \sqrt{17/90} & 0 \\
        \sqrt{17/50} &  \sqrt{17/50} & -\sqrt{17/50} & 0 \\
        \sqrt{17/50} & -\sqrt{17/50} &  \sqrt{17/50} & 0 \\
        \sqrt{17/50} & -\sqrt{17/50} & -\sqrt{17/50} &  0 \\
       -\sqrt{17/50} &  \sqrt{17/50} &  \sqrt{17/50} & 0  \\
       -\sqrt{17/50} &  \sqrt{17/50} & -\sqrt{17/50} &  0 \\
       -\sqrt{17/50} & -\sqrt{17/50} & \sqrt{17/50} &  0  \\
       -\sqrt{17/90} & -\sqrt{17/90} & -\sqrt{17/90} &  0 \\
        0        &  \sqrt{17/90} &  \sqrt{17/90} &  \sqrt{17/90} \\
        0        &  \sqrt{17/50} &  \sqrt{17/50} & -\sqrt{17/50} \\
        0        &  \sqrt{17/50} & -\sqrt{17/50} & \sqrt{17/50} \\
        0        &  \sqrt{17/50} & -\sqrt{17/50} & -\sqrt{17/50} \\
        0        & -\sqrt{17/50} &  \sqrt{17/50} &  \sqrt{17/50} \\
        0        & -\sqrt{17/50} &  \sqrt{17/50} & -\sqrt{17/50} \\
        0        & -\sqrt{17/50} & -\sqrt{17/50} &  \sqrt{17/50}] \\
        0        & -\sqrt{17/90} & -\sqrt{17/90} & -\sqrt{17/90} 
    \end{bmatrix}
\end{equation*}

It is designed so the rows have non-zero entries of equal magnitude, rows with the same non-zero entries have a different configuration of signs, and the variance of each variable is one. 

\subsection{Simulation Setting for Selecting Subset Size}
\label{sec:model_selec_dist_appdx}

We describe the distribution used in \Cref{sec:model_selection_sim}. Recall the subset factor model:
\begin{align}
\tag{\ref{eq:subset_factor_model}, revisited}
\begin{split}
&X_S \sim F \qquad \epsilon \sim (0, \D) \qquad \epsilon_1 \perp , \dots, \perp \epsilon_{p-k} \qquad \Cov(X_S, \epsilon) = \0 \\
&X_{-S} = \W(X_{S} - E_{F}[X_S]) + \mu_{-S} + \epsilon.
\end{split} 
\end{align}
We set the ground truth subset to be  $S = \{1, \dots, 20\}$ and let $X_{S}$ have a mean-zero multivariate Gaussian distribution with block diagonal covariance.  Specifically there are five equally sized blocks, each of which is the equicorrelated matrix $0.5\cdot \I_{4} + 0.5 \cdot 11^\top$. The matrix $\W$ is somewhat sparse, and the non-zero entries are $1$ or $-1$ with probability $1/2$. We document the non-zero entries of $\W$ here:

\begin{equation*}
    \footnotesize{
    \W_{1:7, 1:8} = \begin{bmatrix} -1 &  1 &  1 & -1 &  1 &  1 &  1 &  1 \\  
       -1 &  1 &  1 & -1 & -1 &  1 &  1 &  1 \\
       -1 &  1 & -1 &  1 &  1 &  1 &  1 &  1 \\
        1 & -1 &  1 & -1 &  1 & -1 & -1 & -1 \\
       -1 &  1 & -1 &  1 &  1 &  1 &  1 &  1 \\
        1 & -1 & -1 &  1 & -1 & -1 & -1 &  1 \\
        1 &  1 &  1 &  1 &  1 & -1 &  1 &  1  \end{bmatrix}},
    \footnotesize{ 
    \W_{8:14, 5:12} = 
        \begin{bmatrix} 1 & -1 &  1 &  1 &  1 &  1 & -1 & -1 \\
        1 &  1 & -1 & -1 &  1 & -1 &  1 & -1 \\
        1 & -1 & -1 & -1 & -1 & -1 &  1 & -1 \\
       -1 &  1 &  1 & -1 & -1 & -1 &  1 &  1 \\
        1 &  1 & -1 & -1 &  1 &  1 &  1 &  1 \\
        1 &  1 &  1 & -1 & -1 & -1 &  1 &  1 \\
       -1 &  1 &  1 & -1 &  1 &  1 &  1 &  1 \end{bmatrix}}
\end{equation*}

\begin{equation*}
    \footnotesize{\W_{15:21, 9:16} = \begin{bmatrix} 1 &  1 &  1 &  1 & -1 &  1 & -1 & -1 \\
        1 &  1 &  1 & -1 & -1 &  1 &  1 &  1 \\
        1 &  1 & -1 &  1 &  1 &  1 &  1 &  1 \\
        1 & -1 &  1 & -1 &  1 & -1 & -1 & -1 \\
        1 &  1 &  1 & -1 &  1 & -1 & -1 &  1 \\
        1 &  1 & -1 &  1 &  1 &  1 &  1 &  1 \\
        1 & -1 &  1 &  1 &  1 & -1 & -1 &  1 \end{bmatrix}}, 
    \footnotesize{\W_{22:30, 13:20} =
    \begin{bmatrix} 1 &  1 & -1 & -1 &  1 &  1 &  1 & -1 \\
       -1 &  1 & -1 & -1 &  1 & -1 & -1 & -1 \\
        1 &  1 & -1 & -1 &  1 &  1 &  1 &  1 \\
        1 & -1 &  1 & -1 &  1 & -1 &  1 & -1 \\
       -1 &  1 & -1 & -1 & -1 &  1 & -1 &  1 \\
       -1 & -1 & -1 & -1 &  1 &  1 & -1 &  1 \\
       -1 & -1 & -1 & -1 &  1 &  1 & -1 & -1 \\
        1 & -1 & -1 &  1 &  1 &  1 &  1 & -1 \\
       -1 & -1 & -1 & -1 & -1 &  1 & -1 &  1 \end{bmatrix}}
\end{equation*}
To define $\D$, first we define the diagonal matrix $\widetilde{\D}$ where $\widetilde{\D}_{ii} = i \mod 6$. Then we set $\D = s \cdot \tilde{\D}$, where we use $s \in \R$ to control the amount of signal. The values of $s$ we use are $s = 0.254, 0.812, 2.71, 9.2, 30.0$. The unique factors $\epsilon \sim (0, \D)$ are generated independently of the principal variables. When the unique factors are non-Gaussian, ten of them are $\textup{Exp}(1) - 1$ random variables, ten are Rademacher random variables that take value $1$ or $-1$ with probability $1/2$, and ten are $t$-distributed random variables with $3$ degrees of freedom. In the non-Gaussian case we scale the unique factors so their variance matches $\D$. The indices of the exponential random variables are $1, 11, 13, 17, 19, 20, 23, 24, 26, 27$, of the Rademacher random variables are $3,  4,  8,  9, 12, 14, 21, 22, 29, 30$, and of the $t$-distributed random variables are $2,  5,  6,  7, 10, 15, 16, 18, 25, 28$. These indices were chosen from a random permutation. 

We also ran the same simulations in the case where $\X_{S} \sim N(0, \I_{k})$, $\epsilon \sim N(0, \eta \I_k)$ for some $\eta > 0$, $X_S \perp \epsilon$, and the rows of $\W$ are randomly generated unit vectors. The results (not shown) are notably better than the ones we chose to display. 

\subsection{Additional Simulations for Selecting Subset Size}
\label{sec:model_selec_results_appdx}

We give results from additional simulations, where we repeat the simulations from \Cref{sec:model_selection_sim} but for different sample sizes $n$. \Cref{fig:gaussian_model_selec_sim} depicts the results for the Gaussian unique factor case and \Cref{fig:non_gaussian_model_selec_sim} for the non-Gaussian unique factor case. 

% Figure environment removed


% Figure environment removed

\section{More on Selecting the Appropriate Subset Size}
\label{sec:model_selection_appdx}
In this appendix, we provide more details surrounding the procedures presented in \Cref{sec:model_selection}. Recall that we want to identify $k^*$, the smallest $k$ for which the data generating distribution belongs to the subset factor model, and that the procedures we consider output $\hat{k}$, the smallest $k$ for which we are unable to reject the null that a size-$k$ subset is sufficient. In  \Cref{sec:model_selection_finite_sample_appdx} we consider the restricted case where the unique factors are Gaussian and independent of the principal variables, and establish a test for this null with finite sample validity. In \Cref{sec:model_selection_asymptotic_appdx}, we show that the same test maintains asymptotic validity in the unrestricted case. This is immediately sufficient to imply the correctness of \Cref{thm:error_control}, the proof of which is detailed in \Cref{thm:error_control:proof}. In \Cref{sec:model_selection_selected_subset_appdx}, we discuss some desirable properties of the (ideally) outputted subset $\hat{S}$ and some practical considerations surrounding searching for it. Finally, in \Cref{sec:model_selection_pcss_appdx} we give a discussion that establishes the analogous procedure for the PCSS model \eqref{eq:pcss_model}. 

Throughout we consider $n > p$ data points  $x^{(1)}, \dots, x^{(n)} \in \R^p$ that are sampled from some distribution and have sample mean $\hat{\mu}$ and sample covariance $\hat{\bSigma}$. 

\subsection{A Finite-Sample Test when the Unique Factors are Gaussian}
\label{sec:model_selection_finite_sample_appdx}

The goal of this section is to develop a test that is level $\alpha$ in finite samples for the null $\overline{H}_0: |S| \leq k$ that the data is drawn from from a distribution in the $k$-dimensional subset factor model \eqref{eq:subset_factor_model} with Gaussian unique factors $\epsilon$ that are independent of the principal variables $X_S$. This is the set of distributions that satisfy 
\begin{align}
\label{eq:diag_pcss_model}
\begin{split}
&X_S \sim F\\
&X_{-S} \mid X_S \sim N(\mu_{-S} + \W(X_S - E_{F}[X_{S}]), \D). 
\end{split} 
\end{align}
for some size-$k$ subset $S$, where $\D \succ 0$ is an arbitrary diagonal matrix. In other words, it is the PCSS model \eqref{eq:pcss_model} where the covariance of $X_{-S} \mid X_{S}$ is now allowed to be diagonal (rather than just isotropic). 

\subsubsection{Motivating a Test Statistic}
\label{sec:model_selection_test_stat_appdx}

Our test statistic more or less comes from a generalized likelihood ratio test (LRT). Specifically we compare the data's likelihood under a totally unrestricted multivariate Gaussian model to its likelihood under a restricted multivariate Gaussian model, where the restricted multivariate Gaussian model only considers distributions that are also in $k$-dimensional subset factor model. Suppose that $\hat{\bSigma} \succ \0$. It is well known that the minimized negative log-likelihood, scaled by $1/n$, under the unrestricted Gaussian model is given by 
\begin{equation*}
    \frac{1}{2}\log |\hat{\bSigma}| + \frac{p}{2}(1 + \log(2\pi))
\end{equation*}
From  \cite[9.1.2]{Petersen}, the above can be written as 
\begin{equation*}
     \frac{1}{2}\log |\hat{\bSigma}_{U}| + \frac{1}{2}\log |\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U}| + \frac{p}{2}(1 + \log(2\pi)),
\end{equation*}
for any size-$k$ subset $U$. \Cref{sec:gaussian_pcss_mle_proof} in the proof of \Cref{thm:css_is_mle} tells us that the minimized negative log-likelihood, also scaled by $1/n$, under the restricted Gaussian model is 
\begin{equation*}
    \min_{U \subset [p] :|U| = k}\frac{1}{2} \log|\hat{\bSigma}_{U}| + \frac{1}{2}\log(|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U})|) + \frac{p}{2}(1 + \log(2\pi)) 
\end{equation*}
The generalized LRT statistic is two times the difference in log likelihoods, and is thus given by  
\begin{equation*}
    \min_{U \subset [p] :|U| = k} n  \log \left( \frac{|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U})|}{|\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U}|} \right).
\end{equation*}

Our test statistic generalizes the LRT statistic by allowing for singular $\hat{\bSigma}$:
\begin{equation}
\tag{\ref{eq:test_stat}, revisited}
T_k = \min_{\substack{U \subset [p]: |U| = k}} T(U),  \qquad  T(U) = n \log \left(\frac{|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U})|}{|\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U}| } \right) 
\end{equation}
There may be size-$k$ subsets $U$ for which $|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U})| = |\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}^{+}_{U}\hat{\bSigma}_{U, -U}| = 0$, and we adopt the convention that $T(U) = 0$ for such $U$. In the case that $k = 0$, the same reasoning tells us that $T_0 = n\log\left( |\Diag(\hat{\bSigma})|/|\bSigma| \right)$. 

\subsubsection{Determining a Critical Value}
\label{sec:model_selection_crit_val_appdx}

We will come up with a critical value by studying the distribution of $T(S)$, where $S$ is a size-$k$ subset for which the data generating distribution satisfies \eqref{eq:diag_pcss_model}. Since $T_k = \min_{\substack{U \subset [p]: |U| = k}} T(U) \leq T(S)$, rejecting when $T_k$ is larger than the appropriate quantile of $T(S)$ will be sufficient to guarantee type I error control.   

To study the distribution of $T(S)$ we leverage techniques from fixed-X Ordinary Least Squares. It suffices to understand the distribution of $\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S} \hat{\bSigma}_{S}^{+}\hat{\bSigma}_{S, -S}$, i.e, the distribution of the residual covariance from the regression of the $x_{-S}^{(i)}$ on the $x^{(i)}_S$. We are able to show that, conditional on the $x^{(i)}$,  this covariance has a Wishart distribution. Then, via the Wishart distribution's Bartlett decomposition \cite[Corollary 7.2.1]{anderson1958}, we establish that the quantiles of $T(S)$ are bounded above by those of 
\begin{equation}
\tag{\ref{eq:null_dist}, revisited}
    n \sum_{j=2}^{p-k} \log \left(1 + \frac{\tilde{\chi}^{2}_{j - 1}}{\chi^2_{n- k - j}} \right).
\end{equation}
In \eqref{eq:null_dist} $\{ \chi^2_{\ell} \}, \{ \tilde{\chi}^{2}_{\ell}\}$ are mutually independent chi-squared random variables with degrees of freedom specified by their subscript. When $k = p - 1$, \eqref{eq:null_dist} should be interpreted as a point mass at zero, and the same argument holds when $k = 0$. 

Denoting the $(1-\alpha)$-th quantile of \eqref{eq:null_dist} as $Q_{n, p, k}(1-\alpha)$, \Cref{lem:finite_sample_validity} both summarizes this result and also establishes finite sample validity the test that rejects when $T_k > Q_{n, p, k}(1-\alpha)$. 
\begin{lemma}
\label{lem:finite_sample_validity}
    Consider $n > p$ samples  $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies  \eqref{eq:diag_pcss_model} for some size-$k$ set $S$ where $0 \leq k < p$. Then the quantiles of $T(S)$ are bounded above by the quantiles of \eqref{eq:null_dist}. As a consequence, $P(T_k > Q_{n, p, k}(1-\alpha)) \leq \alpha$. 
\end{lemma}

\subsection{An Asymptotic Test for the Subset Factor Model}
\label{sec:model_selection_asymptotic_appdx}

We now show that the test from \Cref{sec:model_selection_finite_sample_appdx} maintains asymptotic validity for the more general null $H_{0, k} : |S| \leq k$ that the data is drawn from a distribution in the $k$-dimensional subset factor model. Recall that the $k$-dimensional subset factor model is the set of distributions satisfying 
\begin{align}
\tag{\ref{eq:subset_factor_model}, revisited}
\begin{split}
&X_S \sim F \qquad \epsilon \sim (0, \D) \qquad \epsilon_1 \perp , \dots, \perp \epsilon_{p-k} \qquad \Cov(X_S, \epsilon) = \0 \\
&X_{-S} = \W(X_{S} - E_{F}[X_S]) + \mu_{-S} + \epsilon.
\end{split} 
\end{align}
for some size-$k$ subset $S$. 

\Cref{lem:asymptotic_dist} tells us that, should the data be drawn from a distribution satisfying \eqref{eq:subset_factor_model} for some size-$k$ set $S$, then $T(S)$ has a pivotal chi-squared limiting distribution. The degrees of freedom match what one would expect from Wilks' theorem \cite{Wilks}, although our setting is much more general. The proof is a careful application of the delta method and standard, but its result is surprising. Typically, we would expect the limiting distribution of a statistic like $T(S)$ to depend on fourth moments and therefore not be pivotal.

\begin{lemma}
    \label{lem:asymptotic_dist}
    Consider samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution that satisfies \eqref{eq:subset_factor_model} for some size-$k$ set $S$ where $0 \leq k < p$. With $p$ and $k$ fixed, $T(S) \rightsquigarrow \chi^2_{(p-k)(p-k-1)/2}$ as the number of samples $n$ tends to infinity. 
\end{lemma}

In \Cref{lem:asymptotic_dist}, $\chi^2_{(p-k)(p-k-1)/2}$ should be interpreted as a point mass at zero when $k = p -1$. 

Using \Cref{lem:asymptotic_dist}, \Cref{lem:asymptotic_validity} establishes the asymptotic validity of the same test from \Cref{sec:model_selection_finite_sample_appdx} under the more general null $H_{0, k} : |S| \leq k$. Using \Cref{lem:asymptotic_dist}, we can easily show that the $(1-\alpha)$-th quantile of \eqref{eq:null_dist} converges to the $(1-\alpha)$-th quantile of the $\chi^2_{(p-k)(p-k-1)/2}$ distribution, which we will denote $Q_{p, k}(1-\alpha)$. Because $T(S)$ has a limiting distribution that admits a density, it is then immediate that the difference between $P(T(S) > Q_{n, p, k}(1-\alpha))$ and $P(T(S) > Q_{p, k}(1-\alpha))$ converges to zero, and this is sufficient to imply the result.  

\begin{lemma}
    \label{lem:asymptotic_validity}
    For samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies \eqref{eq:diag_pcss_model} for some size-$k$ set $S$ where $0 \leq k < p$, $\limsup_{n \rightarrow \infty} P(T_k > Q_{n, p, k}(1 - \alpha)) \leq \alpha$. 
\end{lemma}

Our decision to use the quantiles of \eqref{eq:null_dist} rather than those of the $\chi^2_{(p-k)(p-k-1)/2}$ distribution is intentional, even though the latter guarantee the same asymptotic validity. In many practical settings, $n$ is not sufficiently larger than $p$ for our asymptotic guarantees to be reflected in our finite sample results. Although rejecting according to the quantiles of \eqref{eq:null_dist} only guarantees finite sample validity when the unique factors are Gaussian (and independent of the principal variables), we may hope that this validity is somewhat robust to deviations from Gaussianity. If so, rejecting according to the quantiles of \eqref{eq:null_dist} should improve performance for smaller sample sizes while maintaining the same asymptotic guarantees. 

\Cref{fig:dist_comparison} illustrates the benefits of rejecting according to the quantiles of \eqref{eq:null_dist}. In \Cref{fig:dist_comparison}, we consider settings with $p = 50$ variables, a size $k=20$ factor set $S$, and either $n=200$ or $n=5000$ samples, and plot three different distributions: (1) the limiting chi-squared distribution of $T(S)$; (2) the distribution \eqref{eq:null_dist}; and (3) the distribution of $T(S)$ in an example case where the unique factors are not Gaussian. In this non-Gaussian example, the thirty unique factors consist of ten centered $\text{Poi}(\lambda)$ random variables with $\lambda=1, \dots, 10$, ten centered chi-squared random variables with $1, \dots, 10$ degrees of freedom, and ten uniform random variables that range from $-a$ to $a$ for $a = 1, \dots, 10$. The principal variables are independent standard Gaussians and regression coefficient matrix $\W \in \R^{(p-k) \times k}$ has uniformly random unit vector rows. \Cref{fig:dist_comparison} has two important takeaways. First, at reasonable sample sizes, the limiting chi-squared distribution is a poor approximation for the actual distribution of $T(S)$, and rejecting according to its quantiles will lead to over-selection. Second, even when the unique factors are not Gaussian and the sample size is reasonably small, \eqref{eq:null_dist} is an excellent approximation for the actual distribution of $T(S)$.  

% Figure environment removed


\subsection{Examination of the Selected Subset}
\label{sec:model_selection_selected_subset_appdx}

Ultimately, having settled on selecting a size-$\hat{k}$ subset, we would ideally the size-$\hat{k}$ subset $\hat{S}$ that minimizes $T(U)$. Unlike in \Cref{sec:model_selection}, $\hat{S}$ in this section refers to any actual global minimizer of $T(U)$ that would be found via exhaustive search. In what follows we give a useful re-characterization of $\hat{S}$ and then establish a desirable property about it. 

First, we re-characterize $\hat{S}$ as an MLE. Given that $T(U)$ is motivated by the LRT, this should not come as a surprise. Consider the case that every $(k + 1)$ by $(k +1)$ principal sub-matrix of $\hat{\bSigma}$ is full rank. Then we can use \cite[9.1.2]{Petersen} to compute for any size-$k$ subset $U$ that 

\begin{align*}
    n  \log \left( \frac{|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U})|}{|\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U}|} \right) &= n  \log \left( \frac{|\hat{\bSigma}_{U}||\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U})|}{|\hat{\bSigma}_{U}||\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U} \hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U}|} \right) \\
    &=  n  \log \left( \frac{|\hat{\bSigma}_{U}||\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U})|}{|\hat{\bSigma}|} \right)\\
    &= n \left( \log(|\hat{\bSigma}_U|) + \log(|\Diag(\hat{\bSigma}_{-U} -  \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U} ) |) \right)\\
    &\qquad \qquad - n\log(|\hat{\bSigma}|) 
\end{align*}
So, in this case, every minimizer $\hat{S}$ of $T(U)$ belongs to  
\begin{equation}
    \label{eq:minimizing_subset}
    \argmin_{U \subset [p] : |U| = k} \log(|\hat{\bSigma}_U|) + \log(|\Diag(\hat{\bSigma}_{-U} -  \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{-1}\hat{\bSigma}_{U, -U} ) |)
\end{equation}
Correspondingly, in this case, the subsets belonging to \eqref{eq:minimizing_subset} are exactly the maximum likelihood estimators for $S$ in the restricted Gaussian model that only consists of distributions also in the  $\hat{k}$-dimensional subset factor model (see \Cref{sec:gaussian_pcss_mle_proof} in the proof of \Cref{thm:css_is_mle}). 

Solving \eqref{eq:minimizing_subset} also handles the case that every $(k + 1)$ by $(k +1)$ principal sub-matrix of $\hat{\bSigma}$ is not full rank. In this case, there must be a size-$k$ subset $U$ such that $T(U) = 0$, and we should fail to reject. However, every $(k + 1)$ by $(k +1)$ principal sub-matrix of $\hat{\bSigma}$ is not full rank if and only if the minimized value of \eqref{eq:minimizing_subset} is $-\infty$. Thus solving \eqref{eq:minimizing_subset} is sufficient to handle both cases. 

We can easily modify our algorithms from \Cref{sec:algorithms} to efficiently search for a subset that solves \eqref{eq:minimizing_subset}. We detail how to do this in \Cref{sec:other_algs_appdx}. 

Because $\hat{S}$ results in the most empirically diagonal residual covariance, it is immediate that $\hat{S}$ asymptotically recovers a subset that results in an exactly diagonal population residual covariance, provided that $\hat{k} \geq k^*$. Unfortunately, this does not necessarily mean that $\hat{S}$ will recover a subset $S$ for which the subset factor model \eqref{eq:subset_factor_model} is satisfied, because the subset factor model requires that the unique factors are independent. We can at best guarantee that they are uncorrelated. Nonetheless, aside from some pathological cases, we can  guarantee recovery.  

\Cref{lem:subset_recovery} uses the following less restrictive version of the subset factor model to state our recovery result:

\begin{align}
\begin{split}
&X_S \sim F \qquad \epsilon \sim (0, \D) \qquad \Cov(X_S, \epsilon) = \0 \\
&X_{-S} = \W(X_{S} - E_{F}[X_S]) + \mu_{-S} + \epsilon.
\end{split} \label{eq:uncorrelated_subset_factor_model}
\end{align}
 
\begin{lemma}
    \label{lem:subset_recovery}
    Consider samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies \eqref{eq:uncorrelated_subset_factor_model} for some size-$k$ set $S$. Suppose that $P$ has population covariance $\bSigma$ such that $\bSigma_{S} \succ 0 $. Then, with probability one, $P$ eventually satisfies \eqref{eq:uncorrelated_subset_factor_model} with $\hat{S}$ as the number of samples $n$ tends to infinity. 
\end{lemma}

\subsection{The PCSS Model}
\label{sec:model_selection_pcss_appdx}

We provide an exactly analogous discussion for the PCSS model \eqref{eq:pcss_model}. Recall that the $k$-dimensional PCSS model is the set of distributions satisfying 
\begin{align}
\tag{\ref{eq:pcss_model}, revisited}
\begin{split}
&X_S \sim F\\
&X_{-S} \mid X_S \sim N(\mu_{-S} + \W(X_S - E_{F}[X_{S}]), \sigma^2 \I_{p-k}). 
\end{split} 
\end{align}
for some size-$k$ set $S$. Again, our procedure returns $\hat{k}$, the smallest $k$ for which we fail to reject the null $\widetilde{H}_0: |S| \leq k$ that the data generating distribution belongs to the $k$-dimensional PCSS model. Like before, we provide a test for this null that is valid in finite samples and we comment on some qualities of the selected subset $\hat{S}$. Because the PCSS model makes a Gaussian assumption, we do not need to discuss asymptotics. \newline 

\subsubsection{Motivating a Test Statistic}
Like in \Cref{sec:model_selection_test_stat_appdx}, we consider a generalized LRT. Specifically we compare the data's likelihood under a totally unrestricted multivariate Gaussian model to its likelihood under a restricted multivariate Gaussian model, where the restricted multivariate Gaussian model only considers distributions that are also in $k$-dimensional PCSS model. Again suppose that $\bSigma \succ \0$. Identical reasoning as in \Cref{sec:model_selection_test_stat_appdx} along with computations from the proof of \Cref{thm:css_is_mle} (see \Cref{sec:gaussian_pcss_mle_proof}) imply that the LRT test statistic is 
\begin{equation*}
    \min_{U \subset [p] : |U| = k} n \log \left( \frac{\left(\tr(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U})/ (p-k) \right)^{p-k}} {| \hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U}|} \right)
\end{equation*}
Again, we will consider a generalization of this statistic that allow $\hat{\bSigma}$ to be singular:
\begin{equation*}
    \widetilde{T}_k = \min_{\substack{U \subset [p]: |U| = k}} \widetilde{T}(U),  \qquad  \widetilde{T}(U) = n \log \left( \frac{\left(\tr(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}^{+}_{U}\hat{\bSigma}_{U, -U})/ (p-k) \right)^{p-k}} {| \hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}^{+}_{U}\hat{\bSigma}_{U, -U}|} \right)
\end{equation*}

\subsubsection{Determining a Critical Value}
Like in \Cref{sec:model_selection_crit_val_appdx}, we will consider a size-$k$ subset $S$ for which the data generating distribution satisfies \eqref{eq:pcss_model} and study the distribution of $\widetilde{T}(S)$. For simplicity we will assume that the principal variable distribution $F$ admits a density. Like before, the residual covariance still has a conditional Wishart distribution, and we can use the Wishart's Bartlett decomposition to show that $\widetilde{T}(S)$ has distribution 
\begin{equation}
    \label{eq:pcss_null_dist}
    n\log \left(\left(\frac{\left(\tilde{\chi}^2_{(p-k)(p-k-1)/2} + \sum_{j=1}^{p-k} \chi^2_{n - k - j}\right)}{p-k}\right)^{p-k}\bigg/\left(\prod_{j=1}^{p-k} \chi^2_{n - k - j}\right)\right)
\end{equation}
In \eqref{eq:pcss_null_dist}, $\{ \chi^2_{\ell} \}, \{ \tilde{\chi}^{2}_{\ell}\}$ are mutually independent chi-squared random variables with degrees of freedom specified by their subscript. Defining $\widetilde{Q}_{n, p, k}(1-\alpha)$ as the $(1-\alpha)$-quantile of \eqref{eq:pcss_null_dist}, \Cref{lem:pcss_finite_sample_validity} establishes a valid finite sample test for the null $\widetilde{H}_0 : |S| \leq k$. 

\begin{lemma}
    \label{lem:pcss_finite_sample_validity}
    Consider $n > p$ samples  $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies  \eqref{eq:pcss_model} for some size-$k$ set $S$ where $0 \leq k < p$. Suppose further that $P$ admits a density. Then $\widetilde{T}(S)$ has distribution \eqref{eq:pcss_null_dist} and, as a consequence, $P(\widetilde{T}_k > \widetilde{Q}_{n, p, k}(1-\alpha)) \leq \alpha$.
\end{lemma}

\Cref{prop:pcss_error_control} establishes error control of our procedure for the PCSS model. It is analogous to \Cref{thm:error_control}. 
\begin{proposition}
    \label{prop:pcss_error_control}
    Consider $n > p$ samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ from a distribution $P$ that admits a density and let $k^*$ be the smallest $k$ for which $P$ belongs to the $k$-dimensional PCSS model \eqref{eq:pcss_model}. If $\hat{k}$ is the smallest $k$ for which the test statistic $\widetilde{T}_k$ is not strictly larger than the critical value $\widetilde{Q}_{n, p, k}(1 - \alpha)$, then 
    \begin{equation*}
        P(\hat{k} > k^*) \leq \alpha.
    \end{equation*}
\end{proposition}

\subsubsection{Examining the Selected Subset}

As in \Cref{sec:model_selection_selected_subset_appdx} we ultimately select the subset $\widetilde{S}$ that globally minimizes $\widetilde{T}(U)$. Like before, we can characterize this subset as an MLE. Supposing that every $k$ by $k$ principal sub-matrix of $\hat{\bSigma}$ is full rank and no size-$k$ subset empirically perfectly linearly reconstructs the remaining variables, similar computations to those in \Cref{sec:model_selection_selected_subset_appdx} shows that a size-$k$ subset $U$ minimizes $\widetilde{T}(U)$ if and only if it solves
\begin{equation}
    \label{eq:pcss_minimizing_subset}
    \argmin_{U \subset [p]: |U| = k} \log|\hat{\bSigma}_{U}| + (p-k)\log( \tr(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}^{-1}_{U}\hat{\bSigma}_{U, -U})/(p-k))
\end{equation}
Under the same conditions, it is also true that $U$ solves \eqref{eq:pcss_minimizing_subset} if and only if it is the MLE for $S$ in the restricted multivariate Gaussian model that only contains distributions also in the $k$-dimensional PCSS model (see \Cref{sec:gaussian_pcss_mle_proof} in the proof of \Cref{thm:css_is_mle})

If the conditions from the previous paragraph are not met, it is unfortunately the case that the subset that minimizes $\widetilde{T}(U)$ may not solve \eqref{eq:pcss_minimizing_subset}. The conditions from the previous paragraph are not met, however, if and only if the minimized objective of \eqref{eq:pcss_minimizing_subset} is $-\infty$. Solving \eqref{eq:pcss_minimizing_subset} thus warns us if this is the case, but it is not totally sufficient as was true in \Cref{sec:model_selection_selected_subset_appdx}. 

We show how to search for the subset that solves \eqref{eq:pcss_minimizing_subset} using \Cref{sec:algorithms}'s algorithms in \Cref{sec:other_algs_appdx}. 

Analogous to \Cref{lem:subset_recovery}, \Cref{lem:pcss_subset_recovery} uses the following generalization of the PCSS model to state a subset recovery result:
\begin{align}
\begin{split}
&X_S \sim F \qquad \epsilon \sim (0, \sigma^2I) \qquad \Cov(X_S, \epsilon) = \0 \\
&X_{-S} = \W(X_{S} - E_{F}[X_S]) + \mu_{-S} + \epsilon.
\end{split} \label{eq:uncorrelated_pcss_model}
\end{align}
It says that $\widetilde{S}$ eventually will result in a isotropic population residual covariance.  

\begin{lemma}
    \label{lem:pcss_subset_recovery}
    Consider samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies \eqref{eq:uncorrelated_pcss_model} for some size-$k$ set $S$. Suppose further that, at the population level, so set of $k$ variables perfectly linearly reconstruct the remaining. Then, with probability one, $P$ eventually satisfies \eqref{eq:uncorrelated_pcss_model} with $\widetilde{S}$ as the number of samples $n$ tends to infinity. 
\end{lemma}


\section{Efficient Subset Search for other Objectives}
\label{sec:other_algs_appdx}

To modify Algorithm \ref{alg:greedy} and Algorithm \ref{alg:swap} to perform subset search under a new objective we simply need to alter $f$ in (\ref{eq:alg_function}) so that $i \not \in U \subset [p]$ that minimizes $f(i, \cdot)$ also minimizes the new objective over the subsets $U + i$. In this section, we explicitly give $f(i, \cdot)$ that allow us to search for subsets that minimize $T(\cdot)$ (see \Cref{sec:model_selection}),  $\widetilde{T}(\cdot)$ (see \Cref{sec:model_selection_pcss_appdx}), and McCabe's other three criteria. We generalize all of McCabe's criteria by changing inverses to pseudo-inverses, thereby allowing for singular $\bSigma$. Correctness of our proposed $f_{\cdot}(i, \cdot)$ are implied by derivations in \Cref{sec:alg_correctness_appdx}. 

For McCabe's criteria, Table \ref{table:runtimes} contrasts the time complexity of our algorithms with that of naive implementations. In the naive implementation, the practitioner recomputes the objective as written with no optimizations when adding/removing elements to/from the current subset. For both our algorithms and the naive implementation, the time complexity of searching for subsets that minimize $T(\cdot)$ and $\widetilde{T}(\cdot)$ matches that of McCabe's second criterion.  \newline

\begin{table}
\centering
\begin{tabular}{ |p{5cm}||p{3.5cm}|p{3.5cm}|  }
 \hline
 Criterion/Method & Algorithm \ref{alg:greedy} & Naive \\
 \hline
 McCabe's First Criterion  & $O(\min\{p^2k, pk^3\})$   & $O(p^4k)$ or $O(pk^4)$\\
 McCabe's Second Criterion &  $O(p^2k)$  & $O(p^3k^2)$\\
 McCabe's Third Criterion  &$O(p^3k)$ & $O(p^3k^2)$\\
 McCabe's Fourth Criterion &$O(p^3k)$ & $O(p^3k^2)$\\
 \hline
\end{tabular}
\caption{A time complexity comparison of our algorithms to the naive algorithms that, when features are added or removed from the current subset, recompute the objective as written with no optimizations. The reported time corresponds to one complete run of Algorithm \ref{alg:greedy} or one iteration of Algorithm \ref{alg:swap}. McCabe himself pointed out that his first criteria of minimizing $|\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{-1}\bSigma_{S, -S}|$ is equivalent to maximizing $|\bSigma_{S}|$, so we report the naive time complexities for both versions of the objective.}
\label{table:runtimes}
\end{table}

\noindent \textbf{Minimizing $T(\cdot)$:} The discussion in \Cref{sec:model_selection_selected_subset_appdx} tells us that it suffices to find a subset that minimizes $\log |\bSigma_{S}| + \log(|\Diag(\bSigma_{R(X_{-S},X_{S})})|)$. To perform this search, use 
\begin{align*}
    &f(i, U, \bSigma_{R(X, X_{U})}) = \log((\bSigma_{R(X, X_{U})})_{ii})\\
    &\qquad \qquad + \sum_{j \not \in U + i} \log \left((\bSigma_{R(X, X_{U})})_{jj} - (\bSigma_{R(X, X_{U})})^2_{ij}/(\bSigma_{R(X, X_{U})})_{ii} \cdot I_{(\bSigma_{R(X, X_{U})})_{ii} > 0 } \right)
\end{align*}

\noindent \textbf{Minimizing $\widetilde{T}(\cdot)$:} The discussion in \Cref{sec:model_selection_selected_subset_appdx} suggests finding a subset that minimizes $\log |\bSigma_{S}| + (p-k)\log(\tr(\bSigma_{R(X_{-S},X_{S})})/(p-k) )$. If the objective value is not $-\infty$ then solving this problem is sufficient. Otherwise we cannot say anything concrete. To perform this search, use 
\begin{align*}
    &f(i, U, \bSigma_{R(X, X_{U})}) = \log((\bSigma_{R(X, X_{U})})_{ii}) \\
    &\qquad \qquad + (p -k) \cdot \log \left( \tr(\bSigma_{R(X, X_{U})}) - \|(\bSigma_{R(X, X_{U})})_{\bullet i}\|_2^2/(\bSigma_{R(X, X_{U})})_{ii} \cdot  I_{(\bSigma_{R(X, X_{U})})_{ii} > 0 } \right)
\end{align*}

\noindent \textbf{McCabe's First Criterion:} McCabe's first criterion suggests selecting a subset that minimizes $|\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{-1}\bSigma_{S, -S}|$. We generalize this to selecting the subset which minimizes $|\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{+}\bSigma_{S, -S}| = |\bSigma_{R(X, X_{S})}|$. For simplicity, we will assume that there exists a size-$k$ subset such that $|\bSigma_{S}| > 0 $. If this is not the case, then every size $k$ subset has redundant variables which can be perfectly linearly reconstructed by other variables in the subset, and the practitioner should use a smaller subset size.  To perform subset search according to McCabe's first criterion use:
\begin{equation*}
    f(i, \bSigma_{R(X, X_{U})}) = -(\bSigma_{R(X, X_{U})})_{ii}
\end{equation*}
Alternatively, as we add/remove variables to/from $U$, we can keep track of $\bSigma^{+}_{U}$ instead of  $\bSigma_{R(X, X_{U})}$ using \cite[3.2.7]{Petersen} and instead use
\begin{equation*}
    f(i, \bSigma, \bSigma^{+}_{U}) = -(\bSigma_{ii} - \bSigma_{iU}\bSigma_{U}^{+}\bSigma_{Ui})
\end{equation*}
\Cref{lem:residual_cov_update} implies that this is equivalent to our earlier suggestion, but it has smaller time complexity when $k \ll \sqrt{p}$. \newline 

\noindent \textbf{McCabe's Third Criterion:} McCabe's third criterion suggests selecting a subset that minimizes $\| \bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{-1} \bSigma_{S, -S} \|^2_F$. We generalize this to selecting the subset which minimizes $\|\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{+} \bSigma_{S, -S} \|_F = \|\bSigma_{R(X, X_S)}\|^2_F$. To perform subset search according to McCabe's third criterion use:
\[f(i, \bSigma_{R(X, X_{U})}) = [(\|\beta\|_2^2/\beta_i)^2 - 2 \beta^\top\bSigma_{R(X, X_{U})}\beta/\beta_i] \cdot I_{\beta_i > 0}\] 
where $\beta = (\bSigma_{R(X, X_{U})})_{\bullet i}$.\newline 

\noindent \textbf{McCabe's Fourth Criterion:} McCabe's fourth criterion suggests selecting a subset that maximizes $\tr(\bSigma_{-S}^{-1} \bSigma_{-S, S}\bSigma_{S}^{-1} \bSigma_{S, -S})$. This is equivalent to maximizing the sum of squared cannonical correlations between $X_{S}$ and $X_{-S}$. We generalize this to selecting the subset which maximizes $\tr(\bSigma_{-S}^{+} \bSigma_{-S, S}\bSigma_{S}^{+} \bSigma_{S, -S})$ which has the same interpretation in the more general case where $\bSigma$ is singular. Consider a currently selected subset $U$. Fix an $i \not \in U$ and let $V = U + i$. Take $j$ and $h$ to be the rank of $i$ in $V$ and $-U$ respectively. So, for example, $(\bSigma_{V})_{jj} = \bSigma_{ii}$ and $(\bSigma_{-U})_{hh} = \bSigma_{ii}$ Then, to perform subset search in accordance with McCabe's fourth criterion use 
\begin{align*}
&f(i, U, \bSigma, \bSigma_{R(X_{-U}, X_{U})}, \bSigma_{R(X_{U}, X_{-U})}, \bSigma^{+}_{-U}, \bSigma^{+}_{U}) =\\
&\qquad \tr( (\bSigma^{+}_{V})_{-j,-j} \bSigma_{R(X_{U}, X_{-U})}) + \frac{\beta^\top \bSigma^{+}_{V} \beta}{\beta_j} I_{\beta_j > 0}  - I_{(\bSigma_{R(X_{-U}, X_{U})})_{hh} > 0}
\end{align*}
where $\beta =  \bSigma_{V, i} - \bSigma_{V, -V}\bSigma_{-V}^{+}\bSigma_{-V, i}$, and we compute $\bSigma_{V}^+$ from  $\bSigma_{U}^+$ and $\bSigma^+_{-V}$ from $\bSigma^+_{-U}$ using \cite[3.2.7]{Petersen}. We keep track of $\bSigma_{U}^+$ and $\bSigma^+_{-U}$ as we add/remove elements to/from $U$ using \cite[3.2.7]{Petersen} and keep track of $\bSigma_{R(X_{-U}, X_{U})}$ and $\bSigma_{R(X_{U}, X_{-U})}$ using \Cref{lem:residual_cov_update} as usual. 

If $\bSigma \succ \0$, then we can give an exact closed form for the replacement $f$ given above:
\[ f(i, U, \bSigma, \bSigma_{R(X_{-U}, X_{U})}, \bSigma^{-1}_{-U}, \bSigma^{-1}_{U}) =  \frac{\beta_{-j}^\top\bSigma^{-1}_{U}\beta_{-j}}{\beta_{j}} - \frac{(\alpha^\top\beta_{-j})^2}{\delta \beta_{j}} - \frac{\alpha^\top\bSigma_{R(X_U, X_{-U})}\alpha + 2\alpha^\top\beta_{-j} - \beta_{j}}{\delta } \]
where $\alpha = \bSigma_{U}^{-1}\bSigma_{Ui}$, $\delta = \bSigma_{ii} - \bSigma_{iU}\alpha$ and $\beta$ is as before, but now we can explicitly compute $\bSigma_{-V}^+ = \bSigma_{-V}^{-1} = (\bSigma_{-U}^{-1})_{-h, -h} - (\bSigma_{-U})_{-h, h}(\bSigma_{-U})_{h, -h}/(\bSigma_{-U})_{hh}$. Also, as we add elements to $U$, \cite{Khan} now allows us to keep track of $\bSigma_{U}^+ = \bSigma_{U}^{-1}$ and $\bSigma^+_{-U} = \bSigma^{-1}_{-U}$ with simpler closed form updates. 


\section{\texorpdfstring{$L^2$}{L2} Random Variables as a Hilbert Space}
\label{sec:hilbert_space_appdx}

The results of this section are well known, but we recount them in full detail as they are particularly useful for proving the results in this article. Consider a probability space $(\Omega, \mathcal{F}, P)$. Upon identifying random variables which are equal almost surely in the same equivalence class, the random variables in $L^2(\Omega, \mathcal{F}, P)$ form a complete Hilbert space. Equality between random variables denotes membership to the same equivalence class, i.e., almost sure equality. For two real-valued random variables $X, Y \in L^2$, the inner product $\langle X, Y \rangle_{\mathcal{H}}$, given by $E[XY]$, induces a norm $\|X\|_{\mathcal{H}}^2 = E[X^2]$. As abuse of notation, when dealing with random vectors $Z \in \R^k$ with entries $Z_i$ in $L^2$, we let $\|Z\|^2_{\mathcal{H}} = \sum_{i=1}^k \|Z_i\|_{\mathcal{H}}^2$, so $\|Z\|^2_{\mathcal{H}} = \tr(E[ZZ^\top])$. We will denote the covariance of two random vectors $Z_1 \in \R^p$ and $Z_2 \in \R^q$ as $\bSigma_{Z_1 Z_2} \in \R^{p \times q}$. 

Since we're working in a Hilbert space, we can project random variables onto linear subspaces of our Hilbert space. We only project onto finite dimensional subspaces, so we don't concern ourselves with the subtleties of infinite dimensional subspaces. If $\mathcal{G}$ is a subspace of $\mathcal{H}$, we denote the projection of $X$ onto $\mathcal{G}$ as $\Proj_{\mathcal{G}}(X)$. For random vectors, $\Proj_{\mathcal{G}}(Z)$ is the random vector with $i$-th entry $\Proj_{\mathcal{G}}(Z_i)$. We denote the subspace spanned by the entries of $Z$ as $\Span(Z)$, but as an abuse of notation we write $\Proj_{Z}(\cdot)$ instead of $\Proj_{\Span(Z)}(\cdot)$. The following Lemma helps us compute projections:

\begin{lemma}
\label{lem:coefficient_minimiziation} 
For random vectors $X \in \R^p$, $Y \in \R^q$,
\[ \{ E[YX^\top]E[XX^\top]^{+} + \E: \E \in \R^{q \times p} \text{ such that } \E X = 0\} = \argmin_{\B \in \R^{q \times p}} E[\|Y - \B X\|_2^2].  \]
\end{lemma}
\Cref{thm:proj_props} provides some useful facts about these projections. 
\begin{theorem}
\label{thm:proj_props}
Consider random vectors $X \in \R^{p}$, $Y \in \R^{q}$ and define $P = E[YX^\top]E[XX^\top]^+ X$ and $R = Y - P$. Then,
\begin{enumerate}[label=(\roman*)]
    \item $P = \Proj_{X}(Y)$,
    \label{thm:proj_props_i}
    \item $E[PR^\top] = \0$ and $E[XR^\top] = \0$, i.e., each entry of $X$ and $P$ is orthogonal to each entry of $R$,
    \label{thm:proj_props_ii}
    \item $\|Y\|^2_{\mathcal{H}} = \|P\|^2_{\mathcal{H}} + \|R\|^2_{\mathcal{H}} $,
    \label{thm:proj_props_iii}
    \item  $E[PP^\top] = E[YX^\top]E[XX^\top]^{+}E[XY^\top]$ and $E[RR^\top] = E[YY^\top] - E[YX^\top]E[XX^\top]^+E[XY^\top]$. 
    \label{thm:proj_props_iv}
\end{enumerate}
\end{theorem}

The following is a useful corollary of \Cref{thm:proj_props}.

\begin{corollary}
\label{cor:uncorrelated_coeffs}
Considering random vectors $X \in \R^{p}$, $Y \in \R^{q}$, $Y = \W X + \epsilon$ with $E[ X \epsilon^\top] = 0$ if and only if $\W = E[YX^\top]E[XX^\top]^+X + \E$ where $\E \in \R^{q \times p}$ such that $\E X = 0$.
\end{corollary}



\section{Proofs and Derivations}
\label{sec:proofs_appdx}

\subsection{Proof of \texorpdfstring{\Cref{lem:coefficient_minimiziation}}{Lemma}}

Minimizing $E[\|Y - \B X\|_2^2] = E[\tr( (Y - \B X)(Y - \B X)^\top ] = \tr(E[YY^\top] - 2\B E[XY^\top] + \B E[XX^\top]\B^\top)$ with respect to $\B$ is an unconstrained convex minimization problem. Thus $\B$ is a minimizer if and only if it sets the first derivative to $0$. Using \cite[2.5.2]{Petersen} to compute the first derivative, we get $\B$ is a solution if and only if  $\B E[XX^\top] - E[YX^\top] = 0$. 

We check that $\B = E[YX^\top] E[XX^\top]^{+}$ is a solution. If $v \in \R^{p}$ is in the null space of $E[XX^\top]$, then $0 = v^\top E[XX^\top]v = E[(X^\top v)^2]$, so $X^\top v = 0$. Then also $E[YX^\top]v = E[Y(X^\top v)]= 0$. Then, for our choice of $v$, $(E[YX^\top] E[XX^\top]^{+}E[XX^\top] - E[YX^\top])v =0$. If $v \in \R^p$ is not in the null space of $E[XX^\top]$, then $E[XX^\top]^+E[XX^\top]v = v$, so $(E[YX^\top] E[XX^\top]^{+}E[XX^\top] - E[YX^\top])v =0$. Thus $(E[YX^\top]E[XX^\top]^{+}E[XX^\top] - E[YX^\top])v = 0$ for all $v \in \R^p$, so $E[YX^\top]E[XX^\top]^{+}E[XX^\top] - E[YX^\top] = 0$. 

For any $ \E \in \R^{q \times p}$ such that $\E X = 0$ it is obvious that $E[YX^\top]E[XX^\top]^{+} + \E$ is a solution. If $\tilde{\B}$ is also a solution, then  $\tilde{\B} E[XX^\top] - E[YX^\top] = 0$ and thus $(E[YX^\top] E[XX^\top]^{+} - \tilde{\B})E[XX^\top] = 0$. Thus $\tilde{\B} = E[YX^\top] E[XX^\top]^{+} + \E$ for some $ \E \in \R^{q \times p}$ such that $\E E[XX^\top] = \0$. Since also $\E E[XX^\top]\E^\top  = \0$, we have that $\tr(E[\E X (\E X)^\top])) = E[\|\E X \|^2_2]= 0$ so $\E X = 0$.

\subsection{Proof of \texorpdfstring{\Cref{thm:proj_props}}{Theorem}}
 We prove each result in the order the are presented. 
\begin{enumerate}[label=(\roman*)]
    \item The $i$-th entry of $\Proj_{X}(Y)$ is given by $Y_i - (b^*)^\top X$, where $b^*$ minimizes $\|Y_i - b^\top X\|_{H}^2 = E[(Y_i - b^\top X)^2]$ over $b$. By \Cref{lem:coefficient_minimiziation}, $b^*$ must be given by the $i$-th row of $E[YX^\top]E[XX^\top]^{+} + \E$ for some $ \E \in \R^{q \times p}$ such that $\E X = 0$. Thus, up to almost sure equality, that the $i$-th entry of $\Proj_{X}(Y)$ is given by $Y_i - (E[YX^\top]E[XX^\top]^{+})_{i \bullet} X$. 
    \item From \ref{thm:proj_props_i} we know that $P_j$ is $\Proj_X(Y_j)$ and $R_i = Y_i - \Proj_X(Y_i)$. From standard properties of projections in Hilbert spaces, we know that $P_j \in \Span(X)$ and $R_i$ is orthogonal to anything in $\Span(X)$. thus $P_j$ and $R_i$ are orthogonal, meaning exactly that $E[PR^\top] = \0$. Same argument applies for $X$. 
    \item From \ref{thm:proj_props_ii} we know that $E[P_iR_i] = 0$. Thus, since $Y_i = P_i + R_i$ we have $ \|Y_i\|^2_{\mathcal{H}} = E[Y_i^2] = E[R_i^2] + E[P_i^2] =  \|P_i\|^2_{\mathcal{H}}+ \|R_i\|^2_{\mathcal{H}}$.
    \item We can compute $E[PP^\top] = E[YX^\top]E[XX^\top]^{+}E[XX^\top]E[XX^\top]^{+}E[XY^\top]  = E[YX^\top]E[XX^\top]^{+}E[XY^\top]  $. Since $Y = P + R$ and we know $E[PR^\top] = \0$ from \ref{thm:proj_props_ii}, we immediately get $E[YY^\top] = E[PP^\top] + E[RR^\top]$. So,  $E[RR^\top]= E[YY^\top] - E[PP^\top] = E[YY^\top] - E[YX^\top]E[XX^\top]^+E[XY^\top] $.
\end{enumerate}

\subsection{Proof of \texorpdfstring{\Cref{cor:uncorrelated_coeffs}}{Corollary}}
\label{cor:uncorrelated_coeffs:proof}
The reverse direction follows as if $\W = E[YX^\top]E[XX^\top]^+ + \E$, then $\W X = P$ and $\epsilon = R$ as in the statement of \Cref{thm:proj_props}. Thus by \Cref{thm:proj_props}\ref{thm:proj_props_ii},  $E[X \epsilon^\top] = 0$. For the forward direction, by the properties of projections in Hilbert spaces, it is only possible for $\epsilon_i$ to be uncorrelated with $X$ if $\W_{i \bullet}X$ equals $\Proj_{X}(Y_i) = P_i$.  Therefore we must have $\W =  E[YX^\top]E[XX^\top]^+ + \E$ where $\E \in \R^{q \times p}$ such that $\E X = 0$ by \Cref{thm:proj_props}\ref{thm:proj_props_i}. 

\subsection{Proof of \texorpdfstring{\Cref{prop:css_equiv_pv}}{Proposition} and \texorpdfstring{\Cref{prop:pop_css_equiv_pv}}{Proposition}}
By \Cref{lem:coefficient_minimiziation} and \Cref{thm:proj_props}\ref{thm:proj_props_iv}, 
\begin{equation*}
    \min_{\B \in  \R^{p \times k}} E[\|X - \B X_{S}\|_2^2] = \tr(E[XX^\top] - E[XX^\top]_{\bullet S} E[XX^\top]_S^{+}E[XX^\top]_{S \bullet}).
\end{equation*}
Taking $X$ to be mean-zero, this immediately implies \Cref{prop:pop_css_equiv_pv}. To prove \Cref{prop:css_equiv_pv}, let $X$ be uniformly random over the rows of the matrix $\X$ and evaluate the expectations on both sides. 

\subsection{Proof of \texorpdfstring{\Cref{thm:css_is_mle}}{Theorem}} 
We provide a proof of \Cref{thm:css_is_mle}. We then provide the analogous result when we allow the covariance of $X_{-S} | X_{S}$  to be diagonal (see \eqref{eq:diag_pcss_model}). Lastly we provide the analogous results for both these cases when we assume that the principal variable distribution $F$ is Gaussian.  

\subsubsection{PCSS Model}

Suppose we restrict $F$ to have a probability density bounded by $K < \infty$. Our model is parameterized by $\theta = (S, f, \mu_{-S}, \W, \sigma^2)$ where $f$ is the density of $F$. Let $\hat{\mu}$ and $\hat{\bSigma}$ be the sample mean and sample covariance of our data. Fix a size-$k$ subset $S$. The negative log-likelihood, scaled by $1/n$, under our model is given by 
\begin{align*}
    &-\frac{1}{n} \sum_{i=1}^n \ell(x^{(i)}; \theta) \\
    &=-\frac{1}{n} \sum_{i=1}^n \left(\ell(x_{S}^{(i)}; \theta) + \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta) \right)\\
    &= \frac{1}{n} \sum_{i=1}^n  \log(f(x_{S}^{(i)})) +  \frac{1}{2}\cdot\frac{1}{n}\sum_{i=1}^n \left( (p-k) \log(2\pi) + (p-k)\log \sigma^2 + \sigma^{-2}\|x_{-S}^{(i)} - \mu_{S} - \W(x_{S}^{(i)}  - \mu_{-S})\|_2^2)  \right)\\
    &= -\frac{1}{n} \sum_{i=1}^n f(x_{S}^{(i)}) + \frac{p-k}{2} \log \sigma^2 +  \frac{1}{2\sigma^2} \cdot \sum_{j=1}^{p-k} \left( \frac{1}{n}\sum_{i=1}^n ((x_{-S}^{(i)})_j - (\mu_{S})_j - \W_{j \bullet}(x_{S}^{(i)}  - \mu_{-S}))^2 \right)+ \frac{(p-k) \log(2\pi)}{2}.
\end{align*}
We know that $-\frac{1}{n} \sum_{i=1}^n \log(f(x_{S}^{(i)}))$ is bounded below by $-\log(K)$. By adding together $n$ bump functions are narrow enough and attain their maximum height of $K$ at each $x^{(i)}$ and placing any remaining mass in the tails of $f$, we can find an $f$ that attains this bound. Although $\mu_{S} = E_F[X_S]$ is not explicitly known, we can absorb $(\mu_{S})_j$ into $\W_{j \bullet} \mu_{-S}$ for each $j$, so it is an inconsequential parameter. So, without loss of generality we set $\mu_{S} = \hat{\mu}_{S}$ in the above. For $c > 0$, the function $\log x + \frac{c}{x}$ is minimized at $x = c$ and has minimized value $\log(c) + 1$. Thus we should pick $(\mu_{-S})_{j}$ and $\W_{j \bullet}$  to minimize $\frac{1}{n}\sum_{i=1}^n((x_{-S}^{(i)})_{j} - (\mu_{-S})_{j} - \W_{j \bullet}(x_{S}^{(i)} - \hat{\mu}_{S}) )^2$. This is a convex problem. From the first order condition one can compute the minimizing value for $(\mu_{-S})_j$ is $(\hat{\mu}_{-S})_j$, and then apply \Cref{lem:coefficient_minimiziation} to compute that the minimizing value of $\W_{j \bullet}$ is given by $(\hat{\bSigma}_{-S,S})_{j \bullet} \hat{\bSigma}_{S}^{+}$. Plugging in these minimizing values gives
\begin{equation*}
\frac{1}{n}\sum_{i=1}^n((x_{-S}^{(i)})_{j} - (\mu_{-S})_{j} - \W_{j \bullet}(x_{S}^{(i)} - \hat{\mu}_{S}) )^2 = (\hat{\bSigma}_{-S})_{jj} - (\hat{\bSigma}_{-S, S})_{j\bullet}\hat{\bSigma}^{+}_{S}(\hat{\bSigma}_{S, -S})_{\bullet j}. 
\end{equation*}
Since no $k$ variables perfectly linearly reconstruct the remaining, we know at least one of $(\hat{\bSigma}_{-S})_{jj} - (\hat{\bSigma}_{-S, S})_{j\bullet}\hat{\bSigma}^{+}_{S}(\hat{\bSigma}_{S, -S})_{\bullet j}$ is positive. The minimizing value of $\sigma^2$ is thus attained at
\begin{equation*}
  \sigma^2 = \frac{1}{p-k} \sum_{j=1}^{p-k}(\hat{\bSigma}_{-S})_{jj} - (\hat{\bSigma}_{-S, S})_{j\bullet}\hat{\bSigma}^{-1}_{S}(\hat{\bSigma}_{S, -S})_{\bullet j} = \tr(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}^{+}_{S}\hat{\bSigma}_{S, -S})/(p-k).
\end{equation*}
Plugging in all our minimizing values, the negative log likelihood is 
\begin{equation*}
-\log(K) + \frac{p-k}{2} \log\left( \tr(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}^{+}_{S}\hat{\bSigma}_{S, -S})/(p-k)\right) + \frac{p-k}{2}(1 + \log(2\pi)).
\end{equation*}
Thus the $S$ which minimizes the negative log-likelihood is the one which minimizes $\tr(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}^{+}_{S}\hat{\bSigma}_{S, -S}) = \tr(\hat{\bSigma} - \hat{\bSigma}_{\bullet S}\hat{\bSigma}^{+}_{S}\hat{\bSigma}_{S \bullet})$, completing the claim. 

In the case that $F$ is completely unrestricted, our model is parameterized by $\theta = (S, F, \mu_{-S}, \W, \sigma^2)$. Fix a size-$k$ subset $S$. The negative log hybrid likelihood is given by
\begin{align*}
    &-\frac{1}{n} \sum_{i=1}^n \ell(x^{(i)}; \theta) \\
    &=-\frac{1}{n} \left(\sum_{i=1}^n \ell(x_{S}^{(i)}; \theta) - \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta)\right)\\
    &=-\frac{1}{n} \sum_{i=1}^n \log(F(x^{(i)}_{S})) - \frac{1}{n} \sum_{i=1}^n \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta).
\end{align*}
By \cite[Theorem 2.1]{Owen}, the optimal choice of $F$ is that which puts weight $1/n$ on each $x^{(i)}$. For this minimizing $F$, the negative log hybrid likelihood is $  \log(n) - \frac{1}{n} \sum_{i=1}^n \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta)$. The argument is then identical to the one above. 

\subsubsection{Diagonal Conditional Covariance}
We show the analogous result for the when we allow the covariance of $X_{-S} | X_{S}$  to be diagonal (see \eqref{eq:diag_pcss_model}). We will assume that every $(k + 1)$ by $(k + 1)$ principal sub-matrix of $\hat{\bSigma}$ is full rank. 

In place of $\sigma^2 > 0$ we have the diagonal matrix $\D \succ \0$ as a parameter. Fix a subset $S$ and again decompose the likelihood:
\begin{equation*}
   -\frac{1}{n} \sum_{i=1}^n \ell(x^{(i)}; \theta) = -\frac{1}{n} \sum_{i=1}^n \left(\ell(x_{S}^{(i)}; \theta) + \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta) \right). 
\end{equation*}
The maximum likelihood estimate of the density $f$ is the same as before, and finding the remaining maximizers is similar to before:
\begin{align*}
&-\frac{1}{n} \sum_{i=1}^n \ell(x_{-S}^{(i)} | x_{S}^{(i)}; \theta) \\
&=  \frac{1}{2} \sum_{j=1}^{p-k} \left( \log \D_{jj} +  \frac{1}{\D_{jj}} \cdot  \left( \frac{1}{n}\sum_{i=1}^n ((x_{-S}^{(i)})_j - (\mu_{S})_j - \W_{j \bullet}(x_{S}^{(i)}  - \mu_{-S}))^2 \right) \right) + \frac{(p-k) \log(2\pi)}{2}.
\end{align*}
Following the reasoning above, we set $\W_{j \bullet}$  to minimize $\frac{1}{n}\sum_{i=1}^n((x_{-S}^{(i)})_{j} - (\mu_{-S})_{j} - \W_{j \bullet}(x_{S}^{(i)} - \hat{\mu}_{S}) )^2$ resulting in a minimized value of $ (\hat{\bSigma}_{-S})_{jj} - (\hat{\bSigma}_{-S, S})_{j\bullet}\hat{\bSigma}^{-1}_{S}(\hat{\bSigma}_{S, -S})_{\bullet j} > 0$ where the inequality and invertability of $\hat{\bSigma}^{-1}_{S}$ come from our new assumption. Like before, this minimized value is the minimizing choice for $\D_{jj}$, implying that the minimizing $\D$ is given by $\Diag(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}_{S}^{-1}\hat{\bSigma}_{S, -S})$.
The minimized negative log-likelihood is then given by  
\begin{equation*}
-\log(K) + \frac{1}{2}\log(|\Diag(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}_{S}^{-1}\hat{\bSigma}_{S, -S})|) + \frac{p-k}{2}(1 + \log(2\pi)).
\end{equation*}
So a subset minimizes the negative log-likelihood if and only if it minimizes $\log(|\Diag(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}_{S}^{-1}\hat{\bSigma}_{S, -S})|)$.

The argument for minimizing the negative hybrid log-likelihood is then identical to the one above. 

\subsubsection{Gaussian Principal Variable Distribution \texorpdfstring{$F$}{F}}
\label{sec:gaussian_pcss_mle_proof}
Now we derive the analogous results when we require that the principal variable distribution $F$ is a Gaussian, so $X_{S} \sim N(\mu_{S}, \C)$ where $\C \succ \0$. First consider the PCSS model with this restriction. Along with the assumption that no $k$ variables perfectly reconstruct the rest, we require that every $k$ by $k$ principal sub-matrix of $\bSigma$ is full rank. The model is parameterized by $(S, \mu, \C, \W, \sigma^2)$. Fix a subset $S$. The negative log-likelihood, scaled by $\frac{1}{n}$, is given by 
\begin{equation*}
    -\frac{1}{n} \sum_{i=1}^n \ell(x^{(i)}; \theta) = -\frac{1}{n} \sum_{i=1}^n \left(\ell(x_{S}^{(i)}; \theta) + \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta) \right).
\end{equation*}
Fix a size-$k$ subset $S$. We can ignore the dependence of $-\frac{1}{n} \sum_{i=1}^n \ell(x_{-S}^{(i)} | x_{S}^{(i)};\theta)$ on $\mu_{S}$ for the same reason in the proof of \Cref{thm:css_is_mle}. Minimizing $-\frac{1}{n} \sum_{i=1}^n (\ell(x_{S}^{(i)}; \theta)$ with respect to $\C$ and $\mu_{S}$ is equivalent finding the unrestricted MLE for a multivariate Gaussian. Since $\hat{\bSigma}_{S}$ is invertible by our new assumption, the minimizing values of $C$ and $\mu_{S}$ are $\hat{\bSigma}_{S}$ and $\hat{\mu}_S$. The minimizing values of $W, \sigma^2$ are then identical to those in the proof of \Cref{thm:css_is_mle} by the same computations, and the minimized value of the negative log-likelihood is 
\begin{equation*}
    \frac{1}{2} \log|\hat{\bSigma}_{S}| + \frac{p-k}{2} \log\left( \tr(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}^{-1}_{S}\hat{\bSigma}_{S, -S})/(p-k)\right) + \frac{p}{2}(1 + \log(2\pi)). 
\end{equation*}
So the subset which minimizes the negative log-likelihood is one which minimizes $\log|\hat{\bSigma}_{S}| + (p-k)\log( \tr(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}^{-1}_{S}\hat{\bSigma}_{S, -S})/(p-k)$. This is sufficient to prove the claim.  

In the case that the principal variable distribution $F$ is a Gaussian and the covariance of $X_{-S}\mid X_{S}$ is allowed to be diagonal, we again assume that every $(k + 1)$ by $(k + 1)$ principal sub-matrix of $\hat{\bSigma}$ is full rank. Then proof is identical to above. The subset which minimizes the negative log-likelihood is one which minimizes $\log |\bSigma_{S}| + \log(|\Diag(\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S}\hat{\bSigma}_{S}^{-1}\hat{\bSigma}_{S, -S})|)$.


\subsection{Proof of \texorpdfstring{\Cref{thm:compromise}}{Theorem}}
The reverse direction is obvious. We show the forward direction. For simplicity we'll assume $X$ is mean zero. For the general case apply the same argument to $X - \mu$. Since $F \in \R^k$ has covariance $I_k$, if $F = \B X$ then $\B \in \R^{k \times p}$ must be rank $k$. Plugging $F = \B X$ into our factor model equation, we get that $X = \W\B X + \epsilon$, where $\epsilon$ is uncorrelated with $\B X$. By \Cref{cor:uncorrelated_coeffs}, $\W = \bSigma_{X, BX}\bSigma^{-1}_{BX} = \bSigma B^\top$, so $\W$  is full-rank. Our expression $X =\W \B X + \epsilon$ follows the definition of regression component decomposition given in \cite{Schonemann1976}. Referencing \cite[Equation 2.6]{Schonemann1976} and the discussion following it the covariance of $\epsilon$, call it $\D$, has rank exactly $p - k$ non-zero entries. Since $\D$ is diagonal, this means exactly $k$ of its entries are $0$. Let $S \subseteq [p]$ be the subset of indices such if $i \in S$ then $\D_{ii} = 0$ and thus $\epsilon_i = 0$. Then $X_{S} = \W_{S \bullet} \B X$. Since $X_{S}$ and $\B X$ have full rank covariances, $\W_{S \bullet}$ must be full rank and thus invertible. Since $\epsilon$ is uncorrelated with $\B X$ it must also be uncorrelated with $X_{S}$, and further we can write $\B X = \W_{S \bullet}^{-1}X_{S}$. Plugging this into our original factor model we have $X_{-S} = \widetilde{\W}X_{S} + \epsilon$ where $\widetilde{\W} = \W_{-S \bullet}\W_{S \bullet}^{-1}$ and $X_{S}$ is uncorrelated with $\epsilon$.


\subsection{Proof of \texorpdfstring{\Cref{lem:residual_cov_update}}{Lemma}}
By \Cref{lem:coefficient_minimiziation} the residual $R(Y, Z)$ is given by $Y - \bSigma_{YZ} \bSigma_{Z}^+Z$. By standard properties of projections in finite dimensional Hilbert spaces, $R(X, X_{S}) = R(R(X, X_{U}), R(X_{i}, X_{U})) $. We can therefore compute, 
\begin{align*}
\bSigma_{R(X, X_{S})} &= \bSigma_{R(R(X, X_{U}), R(X_{i}, X_{U})))}\\
                     &= \bSigma_{R(X, X_{U})} - \bSigma_{R(X, X_U), R(X_i, X_U)}\bSigma_{R(X_i, X_U) }^{+}\bSigma_{ R(X_i, X_U), R(X, X_U)}\\
                     &= \bSigma_{R(X, X_{U})} - (\bSigma_{R(X, X_U)})_{\bullet i}(\bSigma_{R(X, X_U)})_{i\bullet}/(\bSigma_{R(X, X_U)})_{ii} \cdot I_{(\bSigma_{R(X, X_U)})_{ii} > 0}. 
\end{align*}
Where the second equality follows from \Cref{thm:proj_props}\ref{thm:proj_props_iv}. Also using \Cref{thm:proj_props}\ref{thm:proj_props_iv} we can compute that $(\bSigma_{R(X, X_U)})_{\bullet i}  =  \bSigma_{\bullet i} - \bSigma_{\bullet T}\bSigma_{T}^{+}\bSigma_{T, i}$.

\subsection{Proof of \texorpdfstring{\Cref{lem:finite_sample_validity}}{Lemma}}

Suppose $x^{(1)}, \dots, x^{(n)} \in \R^p$ are drawn from a distribution satisfying (\ref{eq:diag_pcss_model}) for a size-$k$ subset $S$, $k < p$, and population mean $\mu$. 

First we show that the quantiles of $T(S)$ from \eqref{eq:test_stat} are bounded by those of \eqref{eq:null_dist}. We will perform our analysis conditional on $x^{(1)}_{S}, \dots, x^{(n)}_{S}$ (hence, the independence between $X_S$ and $\epsilon$ is crucial). Let $\X_{S} \in \R^{n \times k}$ and $\X_{-S} \in \R^{n \times (p-k)}$ be the matrices with rows $x_{S}^{(i)}$ and $x_{-S}^{(i)}$. Defining $e^{(i)} = (x_{-S}^{(i)} - \mu_{-S}) - \W(x_{S}^{(i)} - \mu_{S})$ and $\hat{e}^{(i)} = (x_{-S}^{(i)} - \hat{\mu}_{-S}) - \hat{\bSigma}_{-S, S}\hat{\bSigma}^{+}_{S}(x_{S}^{(i)} - \hat{\mu}_{S})$, let the matrices $\E, \hat{\E} \in \R^{n \times (p - k)}$ have rows $e^{(i)}$ and $\hat{e}^{(i)}$. Note that $e^{(i)}$ are the unique factors $\epsilon$ for the $i$-th sample. Having conditioned on the $x^{(i)}_{S}$, we are exactly in the setting of fixed-X Ordinary Least Squares (OLS). In particular, we are running a multi-response regression of $\X_{-S}$ on $\X_{S}$ where the responses are independent. The $\hat{\E}$ are the residuals from OLS, while $\E$ are the residuals one would get from using the population regression coefficients. With this in mind, let $\H \in \R^{n \times n}$ be the orthogonal projection matrix onto the span of the columns of $\X_{S}$ and $\1 \in \R^n$, a $n$-dimensional vector of ones. By construction, $\H$ has rank $r \leq k + 1$. Therefore,
\begin{align*}
\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S} \hat{\bSigma}_{S}^{+}\hat{\bSigma}_{S, -S} &= \frac{1}{n} \hat{\E}^\top\hat{\E}\\
&= \frac{1}{n} \X_{-S}^\top(\I_n - \H)\X_{-S}\\
&= \frac{1}{n} ((\X_{S} - \1\mu_{S}^\top)\W^\top + \1\mu_{-S}^{T} - \E)^\top(\I_n - \H)((\X_{S} - \1\mu_{S}^\top)\W^\top + \1\mu_{-S}^\top - \E)\\
&= \frac{1}{n} \E^\top(\I_n - \H)\E.
\end{align*}

Let $\N \in \R^{n \times (p-k)}$ and $\M \in \R^{(n - r) \times (p-k)}$ be random matrices with i.i.d $N(0, 1)$ entries and $\boldsymbol{R} \in \R^{n \times n}$ be a rotation such that $\boldsymbol{R}^\top(\I_n - \H)\boldsymbol{R}$ is diagonal matrix with $1$ on the first $n-r$ diagonal entries and $0$ otherwise. Then we can compute
\begin{align*}
n \log \left(\frac{|\Diag(\E^\top(\I_n - \H)\E)|}{|\E^\top(\I_n - \H)\E|} \right) &= n \log \left( \frac{|\Diag(\D^{-1/2}\E^\top(\I_n - \H)\E \D^{-1/2})|}{|\D^{-1/2}\E^\top(\I_n - \H)\E \D^{-1/2}|} \right)\\
            &\overset{d}{=} n \log \left(\frac{|\Diag(\N^\top(\I_n - \H)\N|)}{|\N^\top(\I_n - \H)\N|} \right)\\
            &\overset{d}{=} n \log\left( \frac{|\Diag(\N^\top \boldsymbol{R}^\top(\I_n - \H)\boldsymbol{R} \N|)}{|\N^\top \boldsymbol{R}^\top(\I_n - \H)\boldsymbol{R}\N|} \right)\\
            &\overset{d}{=} n \log \left( \frac{|\Diag(\M^\top \M)|}{|\M^\top \M|} \right)\\
            &\overset{d}{=} n\log \left( \frac{|\Diag(W_{p- k}(I_{p-k}, n -r))|}{|W_{p- k}(I_{p-k}, n -r)|} \right) \\
            &\overset{d}{=} n \sum_{j=2}^{p-k} \log \left(1 + \frac{\chi^2_{j - 1}}{\chi^2_{n - r - j + 1}}\right).
\end{align*}

where $W_{p-k}(I_{p-k}, n - r)$ is a Wishart distribution and $\{ \chi^2_{\ell} \}, \{ \tilde{\chi}^{2}_{\ell}\}$ are mutually independent chi-squared random variables with degrees of freedom specified by their subscript. The final equality in distribution follows from the Bartlett decomposition of Wisharts \cite[Corollary 7.2.1]{anderson1958}. For clarity, we specify that the final distribution is a point mass at zero when $k = p-1$.

Recall that these equalities in distribution are conditional. Since $r \leq k + 1$, it is clear that the quantiles of $n \sum_{j=2}^{p-k} \log \left(1 + \frac{\chi^2_{j - 1}}{\chi^2_{n - k - j}}\right)$ are always at least as large as those of $n \sum_{j=2}^{p-k} \log \left(1 + \frac{\chi^2_{j - 1}}{\chi^2_{n - r - j + 1}}\right)$. Since the conditional quantiles of $T(S)$ are always bounded above by the quantiles of $n \sum_{j=2}^{p-k} \log \left(1 + \frac{\chi^2_{j - 1}}{\chi^2_{n - k - j}}\right)$, the marginal quantiles must be as well. 

Now we can easily establish the final claim. Recall that $Q_{n, p, k}(1-\alpha)$ is the $(1-\alpha)$-quantile of $n \sum_{j=2}^{p-k} \log \left(1 + \frac{\chi^2_{j - 1}}{\chi^2_{n - k - j}}\right)$.  Since $T_k \leq T(S)$, 
\begin{equation*}
    P(T_k  > Q_{n, p, k}(1-\alpha)) \leq P(T(S) > Q_{n, p, k}(1-\alpha)) \leq \alpha.
\end{equation*}


\subsection{Proof of \texorpdfstring{\Cref{lem:asymptotic_dist}}{Lemma}}

First, some preliminaries. Define the function $\vec(\cdot)$ that maps a symmetric matrix $M \in \R^{q \times q}$ to a vector $m \in \R^{q(q+1)/2}$ that contains all it's unique entries. If $m = \vec(M)$, then we will index $m$ by $m_{ij}$ for $i \leq j$ so that $m_{ij} = M_{ij}$. The inverse map is denoted $\vec^{-1}(\cdot)$. We will use the stochastic calculus notation of \cite[Chapter 2]{Vaart} and also freely refer to results from this chapter. 

Suppose we observe $x^{(1)}, \dots, x^{(n)}$ drawn from a distribution that satisfies \eqref{eq:subset_factor_model} for some size-$k$ subset $S$ where $k < p - 1$. In the case that $k = p -1$, $T(S)$ is a point mass at zero and the result is obvious. Recall $\E$ and $\H$ from the proof of \Cref{lem:finite_sample_validity} and consider the function $g(M) = \log(|\Diag(M)|) - \log(|M|)$. Computations in the proof of \Cref{lem:finite_sample_validity} imply that  $T(S) = n  g( \E^T(\I_n-\H)\E /n)$, and it thus suffices to show that  $n g( \E^T(\I_n-\H)\E / n) \rightsquigarrow \chi^2_{(p-k)(p-k-1)/2}$. We will show this result in steps. \newline 

\noindent \textbf{Step One - CLT for $\boldsymbol{\vec(\E^T\E/n)}$: } Let $a = \vec(\E^T\E/n)$ and $\tilde{a} \in \R^{p(p-1)/2}$ be only the entries of $a$ that correspond to the non-diagonal entries of $\E^T\E/n$. We index $\tilde{a}$ as $\tilde{a}_{ij}$, $i < j$. The standard multivariate CLT tells us that $\sqrt{n}\tilde{a} \rightsquigarrow N(0, \V)$ where $\V \in \R^{p(p-1)/2 \times p(p-1)/2}$ is such that $\V_{ij, st} = \D_{ii}\D_{jj}$ if $i=s$, $j=t$ and $0$ otherwise. Furthermore, by the weak law of large numbers $a_{ii} - \D_{ii} = o_{p}(1)$. \newline 

\noindent \textbf{Step Two - Controlling $\boldsymbol{\vec(\E^T\H\E/n)}$: } We want to control $\E^T \H \E/n$, the difference between $\E^T\E/n$ and $\E^T (\I_n - \H)\E/n$. Particularly we want to show that $(\E^T \H \E/n)_{ij} = o_p(n^{-1/2})$, $i \leq j$. To do so it suffices to show that $\| \E^T \H \E\|^2_F = O_p(1)$. Noting that $\H$ is the sum of the outer product of at most $k +1 $ unit vectors, it suffices to show that $\| \E^T v v^T \E\|^2_F = \| \E^Tv \|_2^4 = O_p(1)$ for an arbitrary sequence of unit vectors $v \in \R^n$. To show this, it suffices to show that $\sum_{i=1}^n e^{(i)}_{\ell} v_i = (\E^T v)_{\ell} = O_p(1)$ for all $1 \leq \ell \leq p$. But $\Var(\sum_{i=1}^n e^{(i)}_{\ell} v_i) = \D_{\ell \ell} \sum_{i=1}^n v_i^2 = \D_{\ell \ell}$ and $\E[\sum_{i=1}^n e^{(i)}_{\ell} v_i] = 0$, so this must be the case by Chebyshev. \newline 

\noindent \textbf{Step Three - CLT for $\boldsymbol{\vec(\E^T(\I_n - \H)\E/n)}$: } Let $a_H = \vec(\E^T(\I_n - \H)\E/n)$ and $\tilde{a}_H \in \R^{p(p-1)/2}$ be only the entries of $a$ that correspond to the non-diagonal entries of $\E^T(\I_n - \H)\E/n$. Step Two implies that $\sqrt{n}(\tilde{a} - \tilde{a}_H) = o_p(1)$, so $\sqrt{n}\tilde{a}_H  \rightsquigarrow N(0, \V)$ by Slutsky's lemma and Step One, where $\V$ is as in Step One. Furthermore, since $a_{ii} - (a_H)_{ii} = o_p(1)$, we also have $(a_H)_{ii} - D_{ii} = o_p(1)$ by Slutsky's. \newline 

\noindent \textbf{Step Four - Delta Method for $\boldsymbol{g(\E^T (\I_n - \H)\E/n)}$: } Define the function $f: \R^{q(q+1)/2} \rightarrow \R$ by $f(m) = g(\vec^{-1}(m))$. We will consider the domain of $f$ to be the open set of $m$ such that $\vec^{-1}(m)$ is a positive definite matrix. On this domain $f$ is $C^{\infty}$. We will take a second order Taylor expansion of the function $f$ around an $d \in \R^{(p-k)(p-k + 1)/2}$ such that $\vec^{-1}(d) = \D$. Note then $d_{ij} = 0$ for $i < j$. We can compute the first and second order partial derivatives of $f$ using \cite[Section 2]{Petersen}:
 
\begin{equation*}
\frac{\partial f(m)}{\partial m_{ij}} =
\begin{cases}
        -2(\vec^{-1}(m))^{-1}_{ij} & i \neq j \\
        m^{-1}_{ii} - (\vec^{-1}(m))^{-1}_{ii} & i = j,
\end{cases}
\end{equation*}


\begin{equation*}
\frac{\partial^2 f(m)}{\partial m_{ij} \partial m_{st}} =
\begin{cases}
        2( (\vec^{-1}(m))^{-1}_{is}(\vec^{-1}(m))_{jt}^{-1} + (\vec^{-1}(m))_{it}^{-1}(\vec^{-1}(m))^{-1})_{js}  &   i \neq j, s \neq t \\
        2(\vec^{-1}(m) )^{-1}_{is}(\vec^{-1}(m) )^{-1}_{js} &  i \neq j, s=t\\
        ((\vec^{-1}(m) )^{-1}_{is})^2 - m^{-2}_{ii} I_{i = s} & i=j, s=t.
        \\
\end{cases}
\end{equation*}
By repeated application of \cite[9.1.2]{Petersen}, it's clear that $f \geq 0$ on its domain. Further $f(d) = 0$. In line with this, all the first order partial derivatives of $f$ are zero at $d$. Many of the second order partial derivatives are also zero. Taylor's theorem  gives the following expansion of $f$ around $d$:
\begin{align*}
    f(m) &= \sum_{i < j} \D^{-1}_{ii} \D^{-1}_{jj}m_{ij}^2 \\
    &\qquad +  \sum_{i_1 \leq j_1, i_2 \leq j_2, i_3 \leq j_3} c_{i_1 j_1 i_2 j_2 i_3 j_3} \frac{\partial f^3(d + t(m - d))}{\partial m_{i_1 j_1} \partial m_{i_2 j_2} \partial m_{i_3 j_3}}  (m_{i_1 j_1} - d_{i_1 j_1}) (m_{i_2 j_2} - d_{i_2 j_2}) (m_{i_3 j_3} - d_{i_3 j_3}).
\end{align*}
for some $t \in [0, 1]$ and constants $c_{i_1 j_1 i_2 j_2 i_3 j_3}$ . It is then clear that 
\begin{equation*}
    n g(\E^T(\I_n - \H)\E/n) = n f(\vec(\E^T(\I - \H)\E/n)) = \sum_{i < j} \D^{-1}_{ii} \D^{-1}_{jj} (\sqrt{n}(a_H)_{ij})^2  + o_p(1),
\end{equation*}
and by continuous mapping theorem $n g(\E^T(\I_n - \H)\E/n) \rightsquigarrow \chi^2_{(p-k)(p-k-1)/2}$.

\subsection{Proof of \texorpdfstring{\Cref{lem:asymptotic_validity}}{Lemma}}

The case that $k = p -1$ is obvious, so we consider $k < p -1$ without loss of generality. 

First we will argue that the $Q_{n, p, k}(1-\alpha)$, the $(1-\alpha)$-quantile of \eqref{eq:null_dist}, converges to $Q_{p, k}(1 - \alpha)$, the $(1-\alpha)$-quantile of a $\chi^2_{(p-k)(p-k-1)/2}$ distribution as the number of samples $n$ tends to infinty. Consider observing $x^{(1)}, \dots, x^{(n)}$ samples drawn from a distribution that satisfies \eqref{eq:subset_factor_model} for a size-$k$ subset $S$, and suppose the principal variable distribution $F$ admits a density. Following the proof of \Cref{lem:finite_sample_validity}, we see that $T(S)$ will have distribution exactly \eqref{eq:null_dist} since $\hat{\bSigma}_{S}$ will be full-rank with probability one. But \Cref{lem:asymptotic_dist} tells us that $T(S) \rightsquigarrow \chi^2_{(p-k)(p-k-1)/2}$. This implies that $Q_{n, p, k}(1-\alpha)$ tends to $Q_{p, k}(1-\alpha)$ as $n \rightarrow 0$. 

Now, by Slutsky's and \Cref{lem:asymptotic_dist} we know that $T(S) - Q_{n, p, k}(1- \alpha) \rightsquigarrow \chi^2_{(p-k)(p-k-1)/2} - Q_{p, k}(1-\alpha)$. Since $0$ is a continuity point of $\chi^2_{(p-k)(p-k-1)/2} - Q_{p, k}(1-\alpha)$ we have that 
\begin{align*}
    \limsup_{n \rightarrow \infty} P(T_k - Q_{n, p, k}(1-\alpha)> 0) &\leq \limsup_{n \rightarrow \infty} P(T(S) - Q_{n, p, k}(1-\alpha)> 0)\\
    &= P(\chi^2_{(p-k)(p-k-1)/2} - Q_{p, k}(1-\alpha) > 0)\\
    &= \alpha.
\end{align*}

\subsection{Proof of \texorpdfstring{\Cref{thm:error_control}}{Theorem}}
\label{thm:error_control:proof}
First we show the asymptotic result. Consider a sample  $x^{(1)}, \dots, x^{(n)}$ from a distribution $P$ and suppose that $k^*$ is the smallest $k$ for which $P$ belongs to the subset factor model \eqref{eq:subset_factor_model}. Note that every distribution belongs to the $(k-1)$-dimensional factor model so $k^* \leq k + 1$. Recall that $\hat{k}$ is the smallest $k$ for which $T_k \leq Q_{n, p, k}(1-\alpha)$. Then, using \Cref{lem:asymptotic_validity}, 
\begin{equation*}
       \limsup_{n \rightarrow \infty} P(\hat{k} > k^*) \leq  \limsup_{n \rightarrow \infty}  P(T_{k^*} > Q_{n, p, k^*}(1-\alpha)) \leq \alpha
\end{equation*}

Now, for the finite sample result. Let $k^*$ be the smallest $k$ for which $P$ satisfies the model \eqref{eq:diag_pcss_model} for some size-$k$ subset $S$, and take $\hat{k}$ as before. It is now possible that $k^* = p$, but $P(\hat{k} > k) = 0$ if so. Thus, without loss of generality, we can suppose that $k^* < p$. Then, from \cref{lem:finite_sample_validity} 

\begin{equation*}
        P(\hat{k} > k^*) \leq  P(T_{k^*} > Q_{n, p, k^*}(1-\alpha)) \leq \alpha.
\end{equation*}

\subsection{Proof of \texorpdfstring{\Cref{lem:subset_recovery}}{Lemma}}

First we show that a distribution $P$ satisfies \eqref{eq:uncorrelated_subset_factor_model} with a size-$k$ set $S$ if and only if, for the population covariance $\bSigma$, $\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{+}\bSigma_{S, -S}$ is diagonal and positive definite. First we show the forward direction. We know that $X_{-S} - \mu_{-S} = \W(X_{S} - \mu_S) + \epsilon$ where $\Cov(X_S, \epsilon) = \0$. Then \Cref{cor:uncorrelated_coeffs} tells us that $X_{-S} - \mu_{-S} = \bSigma_{-S, S}\bSigma_{S}^+(X_{S} - \mu_S) + \epsilon$. Via this, we can compute that $\bSigma_{S}= \bSigma_{-S, S}\bSigma_{S}^+\bSigma_{S, -S} + \D$ where $\D \succ 0$ so $\bSigma_{S} - \bSigma_{-S, S}\bSigma_{S}^+\bSigma_{S, -S}$ is diagonal and positive definite. For the reverse direction \Cref{cor:uncorrelated_coeffs} tells us that $X_{-S} - \mu_{-S} = \bSigma_{-S, S}\bSigma_{S}^{+}(X_{S} - \mu_{S}) + \epsilon$ where $\Cov(X_S, \epsilon) = 0$. A simple computation shows that $\epsilon$ must have diagonal, positive definite covariance and thus the distribution satisfies \eqref{eq:uncorrelated_subset_factor_model} with a size-$k$ set $S$ . 

Now, consider $n > p$  samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies \eqref{eq:uncorrelated_subset_factor_model} for some size-$k$ set $S$ and suppose that $P$ has population covariance $\bSigma$ such that $\bSigma_{S} \succ 0 $. Then it must be the case that $\bSigma \succ 0$. Thus, for every size-$k$ subset $U$, $|\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U}| > 0$. Since $\hat{\bSigma}$ converges almost surely to $\bSigma$ as $n\rightarrow \infty$, we have that, for every size-$k$ subset $U$,
\begin{equation}
\label{eq:sample_test_stat}
    \log\left( \frac{|\Diag(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U})| }{|\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U}|} \right)
\end{equation}
converges almost surely to 
\begin{equation}
\label{eq:pop_test_stat}
    \log\left( \frac{|\Diag(\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{-1}\bSigma_{U, -U})| }{|\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{-1}\bSigma_{U, -U}|} \right)
\end{equation}
As a consequence of \cite[9.1.2]{Petersen}, \eqref{eq:pop_test_stat} is zero if and only if $\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{-1}\bSigma_{U, -U}$ is diagonal and otherwise it is strictly greater than zero. Thus, By the discussion above, it is zero when $P$ satisfies \eqref{eq:uncorrelated_subset_factor_model} with $U$, and otherwise it is strictly greater than zero. Since $\hat{S}$ minimizes $T(U)$ it must also minimize \eqref{eq:sample_test_stat}. And since \eqref{eq:sample_test_stat} converges almost surely to \eqref{eq:pop_test_stat}, eventually $\hat{S}$ will be one of the $U$ for which \eqref{eq:pop_test_stat} is zero, meaning $P$ will eventually satisfy \eqref{eq:uncorrelated_subset_factor_model} with $\hat{S}$. 

\subsection{Proof of \texorpdfstring{\Cref{lem:pcss_finite_sample_validity}}{Lemma} }
We adopt the notation from the proof of \cref{lem:finite_sample_validity}. Similarly we perform our analysis conditional on the $x^{(i)}$. Recall that 

\begin{align*}
\hat{\bSigma}_{-S} - \hat{\bSigma}_{-S, S} \hat{\bSigma}_{S}^{+}\hat{\bSigma}_{S, -S} &= \frac{1}{n} \E^\top(\I_n - \H)\E.
\end{align*}
Since we have assumed that $F$ has a density, we know that $\rank(\H) = k + 1$. We can then similarly compute 

\begin{align*}
\widetilde{T}(S) &= n \log\left( \frac{(\tr(\boldsymbol{E}^T(\I_n - \H)\boldsymbol{E})/(p-k))^{p-k}}{|\boldsymbol{E}^T(\I_n - \H)\boldsymbol{E}|} \right)\\
            &= n \log\left(\frac{(\tr(\sigma^{-1}\boldsymbol{E}^T(\I_n - \H)\boldsymbol{E}\sigma^{-1})/(p-k))^{p-k}}{|\sigma^{-1}\boldsymbol{E}^T(\I_n - \H)\boldsymbol{E}\sigma^{-1}|} \right)\\
            &\overset{d}{=} n \log \left(\frac{(\tr(W_{p- k}(I_{p-k}, n - k - 1))/(p-k))^{p-k}}{|W_{p- k}(I_{p-k}, n - k -1 )|} \right)\\
            & \overset{d}{=} n\log \left(\left(\frac{\tilde{\chi}^2_{(p-k)(p-k-1)/2} + \sum_{j=1}^{p-k} \chi^2_{n - k - j }}{p-k}\right)^{p-k}\bigg/\left(\prod_{j=1}^{p-k} \chi^2_{n - k - j}\right)\right)
\end{align*}
where $W_{p-k}(I_{p-k}, n - r)$ is a Wishart distribution, $\{ \chi^2_{\ell} \}, \{ \tilde{\chi}^{2}_{\ell}\}$ are mutually independent chi-squared random variables with degrees of freedom specified by their subscript, and again the final equality in distribution follows from the Bartlett decomposition of Wisharts \cite[Corollary 7.2.1]{anderson1958}. Although these equalities in distribution are conditional, since the final distribution does not depend on the $x^{(i)}$, it also holds marginally. Note that the distribution is a point mass at zero when $k = p -1$. 


Now we can easily establish the final claim. Recall that $\widetilde{Q}_{n, p, k}(1-\alpha)$ is the $(1-\alpha)$-quantile of 
\begin{equation*}
    n\log \left(\left(\frac{\tilde{\chi}^2_{(p-k)(p-k-1)/2} + \sum_{j=1}^{p-k} \chi^2_{n - k - j }}{p-k}\right)^{p-k}\bigg/\left(\prod_{j=1}^{p-k} \chi^2_{n - k - j}\right)\right)
\end{equation*}
Since $\widetilde{T}_k \leq \widetilde{T}(S)$, 
\begin{equation*}
    P(\widetilde{T}_k  > \widetilde{Q}_{n, p, k}(1-\alpha)) \leq P(\widetilde{T}(S) > \widetilde{Q}_{n, p, k}(1-\alpha)) \leq \alpha.
\end{equation*}

\subsection{Proof of \texorpdfstring{\Cref{prop:pcss_error_control}}{Proposition}}

Let $k^*$ be the smallest $k$ for which $P$ satisfies the model \eqref{eq:diag_pcss_model} for some size-$k$ subset $S$, and recall that $\hat{k}$ is the smallest $k$ for which $\widetilde{T}_k \leq \widetilde{Q}_{n, p, k}(1-\alpha)$. It is possible that $k^* = p$, but then $P(\hat{k} > k) = 0$ if so. Thus, without loss of generality, we can suppose that $k^* < p$. Then, from \cref{lem:pcss_finite_sample_validity},
\begin{equation*}
        P(\hat{k} > k^*) \leq  P(\widetilde{T}_{k^*} > \widetilde{Q}_{n, p, k^*}(1-\alpha)) \leq \alpha.
\end{equation*}

\subsection{Proof of \texorpdfstring{\Cref{lem:pcss_subset_recovery}}{Lemma}}
First we claim that a distribution $P$ satisfies \eqref{eq:uncorrelated_pcss_model} with a size-$k$ set $S$ if and only if, for the population covariance $\bSigma$, $\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{+}\bSigma_{S, -S}$ is isotropic, i.e., equal to $\eta \I_{p-k}$ for some $\eta > 0$. The argument is identical to that in the proof of \Cref{lem:subset_recovery}. 


Now, consider $n > p$  samples $x^{(1)}, \dots, x^{(n)} \in \R^p$ drawn from a distribution $P$ that satisfies \eqref{eq:uncorrelated_pcss_model} for some size-$k$ subset $S$. Further suppose that, at the population level no set of $k$ variables perfectly linearly reconstruct the rest. This implies that $\tr(\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U})$ is never zero for any size-$k$ subset $U$. Then, since $\hat{\bSigma}$ converges almost surely to $\bSigma$ as $n\rightarrow \infty$, we can guarantee that 
\begin{equation}
    \label{eq:pcss_sample_test_stat}
    \log\left( \frac{(\tr(\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U})/(p-k))^{(p-k)} }{|\hat{\bSigma}_{-U} - \hat{\bSigma}_{-U, U}\hat{\bSigma}_{U}^{+}\hat{\bSigma}_{U, -U}|} \right)
\end{equation}
converges almost surely to 
\begin{equation}
    \label{eq:pcss_pop_test_stat}
    \log\left( \frac{ (\tr(\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U})/(p-k))^{(p-k)} }{|\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U}|} \right) 
\end{equation}
We claim that \eqref{eq:pcss_pop_test_stat} is zero if and only if $\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U}$ is of the form $\eta \I_{p-k}$. To see this note that \eqref{eq:pcss_pop_test_stat} is (resp. strictly) larger than zero if and only if 
\begin{equation}
    \label{eq:am_gm}
    \frac{ \tr(\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U})/(p-k) }{|\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U}|^{1/(p-k)}}
\end{equation}
is (resp. strictly) larger than one. But \eqref{eq:am_gm} is always larger than or equal to one because it is the 
ratio of the arithmetic mean to the geometric mean of the eigenvalues of $\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U}$. Equality is only attained when all the eigenvalues are equal, i.e., when  $\bSigma_{-U} - \bSigma_{-U, U}\bSigma_{U}^{+}\bSigma_{U, -U}$ is of the form $\eta \I_{p-k}$, which completes the claim. 

By the discussion above, this quantity will be zero when $P$ satisfies \eqref{eq:uncorrelated_pcss_model} with $U$, and otherwise it will be strictly greater than zero. Since  $\widetilde{S}$ minimizes $\widetilde{T}(U)$, eventually it will be one of the $U$ for which this quantity is zero, and thus eventually $P$ will satisfy \eqref{eq:uncorrelated_subset_factor_model} with $\widetilde{S}$. 

\subsection{Correctness of Subset Search Algorithms}
\label{sec:alg_correctness_appdx}
We provide computations which justify the correctness algorithms in \Cref{sec:algorithms} as well as \Cref{sec:other_algs_appdx}. \newline 

\noindent \textbf{Minimizing $T(\cdot)$:} \cite[9.1.2]{Petersen} and \Cref{lem:residual_cov_update} jointly tell us that $\log|\bSigma_{U+i}| = \log |\hat{\bSigma}_U| + \log((\hat{\bSigma}_{R(X, X_U)})_{ii})$.  Using \Cref{lem:residual_cov_update}, we can compute 
\begin{align*}
&\log|\hat{\bSigma}_{U + i}| + \tr(\log(\Diag(\hat{\bSigma}_{R(X_{-(U + i)},X_{U + i})})))  \\
& \qquad =\log|\hat{\bSigma}_{U}| + \log((\hat{\bSigma}_{R(X, X_U)})_{ii}) + \tr(\log(\Diag((\hat{\bSigma}_{R(X,X_{U + i})})_{-(U + i)}))) \\
& \qquad =\log|\hat{\bSigma}_{U}| + \log((\hat{\bSigma}_{R(X, X_U)})_{ii}) \\
&\qquad \qquad + \tr(\log(\Diag((\hat{\bSigma}_{R(X, X_{U})} -(\hat{\bSigma}_{R(X, X_{U})})_{\bullet i}(\hat{\bSigma}_{R(X, X_{U})})_{i \bullet}/(\hat{\bSigma}_{R(X, X_{U})})_{ii} \cdot I_{(\hat{\bSigma}_{R(X, X_{U})})_{ii} > 0} )_{-(U+i)}))) \\
&\qquad = \log|\hat{\bSigma}_{U}| + \log((\hat{\bSigma}_{R(X, X_U)})_{ii}) + \sum_{j \not \in U + i} \log((\hat{\bSigma}_{R(X, X_U)})_{jj} - (\hat{\bSigma}_{R(X, X_U)})^2_{ij}/(\hat{\bSigma}_{R(X, X_U)})_{ii} \cdot I_{(\hat{\bSigma}_{R(X, X_{U})})_{ii} > 0})
\end{align*}
which is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$. \newline 

\noindent \textbf{Minimizing $\widetilde{T}(\cdot)$:} Following similar reasoning to above, we can compute
\begin{align*}
&\log|\bSigma_{U + i}| + (p-k)\log(\tr(\bSigma_{R(X_{-(U + i)}, X_{U + i})} )/(p-k) )  \\
& \qquad =\log|\bSigma_{U}| + \log((\bSigma_{R(X, X_U)})_{ii}) + (p-k)\log(\tr(\bSigma_{R(X, X_{U + i})} )) - (p-k)\log(p-k) \\
& \qquad =\log|\bSigma_{U}| + \log((\bSigma_{R(X, X_U)})_{ii}) \\
& \qquad \qquad + (p-k)\log(\tr(\bSigma_{R(X, X_U)} -  (\bSigma_{R(X, X_U)})_{\bullet i}(\bSigma_{R(X, X_U)})_{i \bullet}/(\bSigma_{R(X, X_U)})_{ii} \cdot I_{(\bSigma_{R(X, X_U)})_{ii} > 0} )) - (p-k)\log(p-k) \\
&\qquad  = \log|\bSigma_{U}| + \log((\bSigma_{R(X, X_U)})_{ii}) + (p-k)\log(\tr(\bSigma_{R(X, X_U)}) -  \|(\bSigma_{R(X, X_U)})_{\bullet i}\|_2^2/(\bSigma_{R(X, X_U)})_{ii} \cdot I_{(\bSigma_{R(X, X_U)})_{ii} > 0}  ) \\
&\qquad \qquad- (p-k)\log(p-k)
\end{align*}
which is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$. \newline 



\noindent \textbf{McCabe's First Criterion:} If there exists a $k$ by $k$ principal submatrix of $\bSigma$ that is full rank then minimizing $|\bSigma_{-S} - \bSigma_{-S, S}\bSigma_{S}^{+} \bSigma_{S, -S}|$ is equivalent to maximizing $|\bSigma_{S}|$. Consider having a currently selected subset $U$. We know from above that $ |\bSigma_{U + i}| = |\bSigma_{U}|\cdot (\bSigma_{R(X, U)})_{ii}$. This is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$.\newline 

\noindent \textbf{McCabe's Second Criterion:} Consider having currently selected a subset $U$. For $i \not \in U$ let $\beta = (\bSigma_{R(X, X_{U})})_{\bullet i}$. Then from \Cref{lem:residual_cov_update}:
\begin{align*}
     \tr(\bSigma_{R(X, X_{U + i})}) &= \tr(\bSigma_{R(X, X_{U})}  - \frac{\beta\beta^\top}{\beta_i} \cdot I_{\beta_i > 0})\\
     &= \tr(\bSigma_{R(X, X_{U})})  - \frac{||\beta||_2^2}{\beta_i} \cdot I_{\beta_i > 0}
\end{align*}
which is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$.\newline 


\noindent \textbf{McCabe's Third Criterion:} Consider having currently selected a subset $U$. For $i \not \in U$ let $\beta = (\bSigma_{R(X, X_{U})})_{\bullet i}$. Then from \Cref{lem:residual_cov_update}:
\begin{align*}
 \|\bSigma_{R(X, X_{U + i})}\|^2_{F} &= \|\bSigma_{R(X, X_{U})}  - \frac{\beta\beta^\top}{\beta_i} \cdot I_{\beta_i > 0}\|^2_{F}\\
                                    &= \tr((\bSigma_{R(X, X_{U})}  - \frac{\beta\beta^\top}{\beta_i}  \cdot I_{\beta_i > 0})^\top(\bSigma_{R(X, X_{U})}  - \frac{\beta\beta^\top}{\beta_i} \cdot I_{\beta_i > 0}))\\
                                    &=  \|\bSigma_{R(X, X_{U})}\|^2_{F} + \left[ \frac{\|\beta\|_2^4}{\beta^2_{i}} - \frac{2\beta^\top\bSigma_{R(X, X_{U})}\beta}{\beta_i} \right]\cdot I_{\beta_i > 0}
\end{align*}
which is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$. \newline 

\noindent \textbf{McCabe's Fourth Criterion: } McCabe's fourth criterion suggests finding a subset $S$ that maximizes $\tr(\bSigma_{S}^+\bSigma_{S, -S}\bSigma_{-S}^+\bSigma_{-S, S})$.

We briefly justify that our generalized objective has the same interpretation. Fix a subset $S$. It is well known that $\tr(\bSigma_{S}^+\bSigma_{S, -S}\bSigma_{-S}^+\bSigma_{-S, S})$ is the sum of the squared canonical correlations when $\bSigma \succ \0$ \cite{McCabe}. When $\bSigma$ is singular, suppose that the ranks of $\bSigma_{S}$ and $\bSigma_{-S}$ are  $r_1$ and $r_2$ respectively. Then let $Q_1$ and $Q_2$ be rotations so that the last $k - r_1$ and $p- k - r_2$ entries of $Q_1 X_{S}$ and $Q_2 X_{-S}$ are zero. Let $Y_1 \in \R^{r_1}$ and $Y_2 \in \R^{r_2}$ be the first $r_1$ and $r_2$ entries of $X_S$ and $X_{-S}$. By the definition of canonical correlations, the sum of the squared canonical correlations between $X_S$ and $X_{-S}$ and $Y_1$ and $Y_2$ are the same, and it is easy to verify that $\tr(\bSigma_{Y_1}^{-1}\bSigma_{Y_1, Y_2}\bSigma_{Y_2}^{-1}\bSigma_{Y_2, Y_1})$ is equal to $\tr(\bSigma_{S}^+\bSigma_{S, -S}\bSigma_{-S}^+\bSigma_{-S, S})$. 

Now, we compute that
\begin{equation*}
    \tr(\bSigma_{S}^+\bSigma_{S, -S}\bSigma_{-S}^{+}\bSigma_{-S, S}) = \tr(\bSigma_S^{+}(\bSigma_S - \bSigma_{R(X_{S}, X_{-S})}) ) = \rank(\bSigma_{S}) - \tr(\bSigma_S^+ \bSigma_{R(X_{S}, X_{-S})})
\end{equation*}
Consider having currently selected a subset $U$. Take a fixed $i \not \in U$ and let $V = U + i$. Take $j, h$ to be as described in the presentation of the modified algorithm. $\bSigma_{V}$ will have rank larger than $\bSigma_{U}$ by one if feature $i$ cannot be exactly reconstructed by the features in $U$ and the same rank as $\bSigma_{U}$ otherwise. So, $\rank(\bSigma_{V}) = \rank(\bSigma_{U}) + I_{(\bSigma_{R(X_{-U}, X_{U})})_{hh} > 0 }$. Taking $\tilde{\beta} = \bSigma_{\bullet i} -  \bSigma_{\bullet, -V}\bSigma_{-V}^{+}\bSigma_{-V, i}$, we know from \Cref{lem:residual_cov_update} that
\[\bSigma_{R(X, X_{-V})} =  \bSigma_{R(X, X_{-U)})} + \frac{\tilde{\beta} \tilde{\beta}^\top}{\tilde{\beta}_i} \cdot I_{\tilde{\beta}_i > 0} \]
From this, defining $\beta = \tilde{\beta}_{V}$, and recalling that $\beta_j = \tilde{\beta}_i$, we have 
\[\bSigma_{R(X_{V}, X_{-V})} =  \bSigma_{R(X_{V}, X_{-U)}} + \frac{\beta \beta^\top}{\beta_j} \cdot I_{\beta_j > 0} \]
Since $i \in -U$, the $j$th row and column of $\bSigma_{R(X_{V}, X_{-U)})}$ are zeros. Putting everything together 
\begin{align*}
    &\rank(\bSigma_{V}) - \tr(\bSigma_{V}^+ \bSigma_{R(X_{V}, X_{-V})})\\
    &=\rank(\bSigma_{U}) + I_{(\bSigma_{R(X_{-U}, X_{U} )})_{hh} > 0} - \tr((\bSigma^{+}_{V})_{-j, -j}\bSigma_{R(X_{U}, X_{-U)})}) - \frac{\beta^\top \bSigma^+_{V} \beta}{\beta_j} \cdot I_{\beta_j > 0}
\end{align*}
which is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$. 

Now we consider the case where $\bSigma \succ \0$. First, $\bSigma_{S}^{+} = \bSigma_{S}^{-1}$ so our objective simplifies to minimizing $\tr(\bSigma^{-1}_S \bSigma_{R(X_{S}, X_{-S})})$. In this setting, \cite{Khan} tells us that $\bSigma^{-1}_{-V} = (\bSigma_{-U}^{-1})_{-h, -h} - (\bSigma_{-U})_{-h, h}(\bSigma_{-U})_{h, -h}/(\bSigma_{-U})_{hh}$.  Consider $\beta$ from before. Since $\bSigma \succ \0$, $\beta_j > 0$, so $\bSigma_{R(X_{V}, X_{-V})} =  \bSigma_{R(X_{V}, X_{-U)})}  + \beta\beta^\top/\beta_j$. Also from \cite{Khan}, letting $\alpha = \bSigma_{U}^{-1}\bSigma_{Ui}$, $\delta = \bSigma_{ii} - \bSigma_{iU}\alpha$,  we know that $(\bSigma^{-1}_{V})_{-j, -j} = \bSigma^{-1}_{U} - \alpha\alpha^\top/\delta$, $(\bSigma_V^{-1})_{-j, j} = -\alpha/\delta$ and $(\bSigma_V^{-1})_{j, j} = 1/\delta$. Again, recalling that the $j$th row and column of $\bSigma_{R(X_{V}, X_{-U)})}$ are zeros because $i \in  -U$ we can compute 
\begin{align*}
\tr(\bSigma^{-1}_{V}\bSigma_{R(X_{V}, X_{-V})}) &= \tr( (\bSigma^{-1}_{U} - \alpha\alpha^\top/\delta)(\bSigma_{R(X_{U}, X_{-U})} + \beta_{-j}\beta_{-j}^\top/\beta_{j} )  -\alpha\beta_{-j}^\top/\delta) - \alpha^\top\beta_{-j}/\delta + \beta_j/\delta ) \\ 
&= \tr(\bSigma^{-1}_{U} \bSigma_{R(X_{U}, X_{-U})}) +  \frac{\beta_{-j}^{T}\bSigma^{-1}_{U}\beta_{-j}}{\beta_{j}} - \frac{(\alpha^{T}\beta_{-j})^2}{\delta \beta_{j}} - \frac{\alpha^\top\bSigma_{R(X_U, X_{-U})}\alpha + 2\alpha^\top\beta_{-j} - \beta_{j}}{\delta } 
\end{align*}
which is sufficient to imply that the $i \not \in U$ we select minimizes the objective over subsets $U + i$. 






\end{appendix}

\end{document}

