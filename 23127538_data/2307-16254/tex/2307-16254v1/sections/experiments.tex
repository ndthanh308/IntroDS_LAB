\section{EXPERIMENTAL RESULTS}
\label{sec:experiment}
\subsection{Experimental Setup}
\label{ssec:setup}
The experimental setup is shown in Fig.~\ref{fig:fig1} consists of a set of 9 unknown transparent objects belonging to six categories and a Universal Robots UR5 equipped with a sensorised Robotiq 2F140 gripper. The tactile sensor array of the two-finger gripper are sourced from XELA robotics\textsuperscript\textcopyright and Contactile\textsuperscript\textcopyright. The outer and inner side of each finger are sensorised and comprise of $3\times3$ sensor array from the Contactile sensors and $4 \times 4$ sensor array from XELA sensors respectively. The fingertip of the finger sensorised with the XELA sensors also has $6 \times 1$ array. Each taxel of the sensor array provides 3-axis force measurements.
% This configuration allows the robot to acquire tactile data while touching with the outer side and from the fingertip as well as pinch grasp with the sensorised inner sides of the fingers.
% We perform guarded motions of the robot while collecting the tactile data in order to prevent unintended motions in the object. 
The normalised force values of the tactile sensors are measured and contact is established when the force exceeds the baseline threshold $f_{ts} \geq \tau_f$ where $\tau_f = 1.1$.
% The 3D positions of the contacted taxels of the sensor are extracted as contact points and transformed to the world coordinate frame $\mathcal{W}$ using the kinematics of the robot.
% The contact points $P^t_{obs}$ are added to the tactile point cloud $P^t$ after every action. The tactile point cloud $P^t$ consists of $x,y,z$ positions and the normal direction $\hat{n}$ extracted from the normal force vector. The normal information is only used for the baseline GPIS computation and the surface reconstruction.
All operations involving point clouds use the Point Cloud Library\footnote{\url{https://pointclouds.org/}}, occupancy grid computations uses Octomap library\footnote{\url{https://octomap.github.io/}}, and the overall setup uses a ROS-based framework\footnote{\url{https://www.ros.org/}}. All robot experiments are run on a workstation using Ubuntu 18.04 with Intel\textsuperscript\textcopyright Xeon(R) Gold 5222 CPU. The object exploration and reconstruction time is between 5-7 minutes on average as the robot's maximum speed is limited to 250 mm/s for safety regulations.
\subsubsection*{Network Implementation Details} Our proposed network is implemented using the Tensorflow framework and training/ inference are performed on Nvidia Quadro RTX 4000 GPU. We used the ADAM optimiser, learning rate set to $10^{-4}$, momentum 0.9 and batch size 8. All layers of the encoder-decoder uses batch normalisation and the decay rate initialized at 0.5 and gradually increased to 0.99 with decay step size $2\times 10^5$. During training with our synthetic dataset $\mathcal{D}$, random voxel-grid subsampling is done to have input point clouds with point size between 40 and 120. 
% The hyperparameter $\alpha$ for the loss is set to 100.
% For the recognition head, a dropout with keep probability 0.7 is used on the $\mathtt{fc-256}$ layer.
\subsubsection*{Object List} We use the following widely-available transparent objects as unknown objects: bottle 1, bottle 2, can, detergent, cup 1, cup 2, cup 3, wineglass and spray as shown in Tab.~\ref{tab:qualitative_results}.
% The object nomenclature follows \{\textit{category-name}\} \{\textit{instance number}\}.
% For training our proposed network, we extract similar synthetic categorical objects as a subset of ShapeNet dataset~\cite{chang2015shapenet}.
% The real objects are chosen to have varying complexities in shape from simple (bottle) to complex (spray). Various instances of the same category are chosen to show the reconstruction capabilities of our proposed framework ACTOR. The objects also have axes of symmetry which increases the pose estimation challenge. 

\subsection{Active Tactile-based Deep Self-Supervised Category-level Transparent Object Reconstruction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Quantitative recons results fig
% Figure environment removed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Qualitative recons result
\begin{table*}[t!]
\centering
\caption{Qualitative reconstruction results of our proposed method in comparison with Gaussian process implicit surfaces for unknown real test objects. (Best viewed on screen in color).}
\label{tab:qualitative_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll|l|cc|cl|cc@{}}
\toprule
\multicolumn{2}{c}{Object}   & \multicolumn{1}{c}{Tactile PC}  & \multicolumn{2}{c}{Ground Truth} & \multicolumn{2}{c}{GPIS}   & \multicolumn{2}{c}{ACTOR (ours)} \\
         & & N $\sim$120  & PC           & Surface          & Recon. PC & Recon. Surf. & Recon. PC    & Recon. Surf.   \\ \midrule
Bottle 1 & \parbox[c]{1em}{
% Figure removed}    &   \parbox[c]{1em}{
% Figure removed}        & \parbox[c]{1em}{% Figure removed}            &    \parbox[c]{1em}{% Figure removed}        & 
\parbox[c]{1em}{% Figure removed}           & 
\parbox[c]{1em}{% Figure removed}              &
\parbox[c]{1em}{% Figure removed}
& \parbox[c]{1em}{% Figure removed}  
             \\
Bottle 2 &\parbox[c]{1em}{
% Figure removed} &   \parbox[c]{1em}{
% Figure removed}      & \parbox[c]{1em}{% Figure removed}              &   \parbox[c]{1em}{% Figure removed}                &    \parbox[c]{1em}{% Figure removed}       &    \parbox[c]{1em}{% Figure removed}            &   \parbox[c]{1em}{% Figure removed}           &   \parbox[c]{1em}{% Figure removed}                         \\
Can     &\parbox[c]{1em}{
% Figure removed}   &     \parbox[c]{1em}{
% Figure removed}        &      \parbox[c]{1em}{% Figure removed}         &     \parbox[c]{1em}{% Figure removed}        &  \parbox[c]{1em}{% Figure removed}         &     \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}          &     \parbox[c]{1em}{% Figure removed}                       \\
Detergent &\parbox[c]{1em}{
% Figure removed} &      \parbox[c]{1em}{
% Figure removed}    &   \parbox[c]{1em}{% Figure removed}         &      \parbox[c]{1em}{% Figure removed}                  &   \parbox[c]{1em}{% Figure removed}        &    \parbox[c]{1em}{% Figure removed}            &   \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}                \\
Cup 1 &  \parbox[c]{1em}{
% Figure removed} \ \ &    \parbox[c]{1em}{
% Figure removed}      &      \parbox[c]{1em}{% Figure removed}         &  \parbox[c]{1em}{% Figure removed}                          &   \parbox[c]{1em}{% Figure removed}        &  \parbox[c]{1em}{% Figure removed}              &     \parbox[c]{1em}{% Figure removed}           &        \parbox[c]{1em}{% Figure removed}            \\
Cup 2  &  \parbox[c]{1em}{
% Figure removed} &   \parbox[c]{1em}{
% Figure removed}     &     \parbox[c]{1em}{% Figure removed}          &    \parbox[c]{1em}{% Figure removed}                          &   \parbox[c]{1em}{% Figure removed}        &      \parbox[c]{1em}{% Figure removed}          &   \parbox[c]{1em}{% Figure removed}           &        \parbox[c]{1em}{% Figure removed}          \\
Cup 3  &  \parbox[c]{1em}{
% Figure removed} &    \parbox[c]{1em}{
% Figure removed}             &     \parbox[c]{1em}{% Figure removed}          &  \parbox[c]{1em}{% Figure removed}                    &    \parbox[c]{1em}{% Figure removed}       &     \parbox[c]{1em}{% Figure removed}           &     \parbox[c]{1em}{% Figure removed}         & \parbox[c]{1em}{% Figure removed}                \\
Wineglass &\parbox[c]{1em}{
% Figure removed}&   \parbox[c]{1em}{
% Figure removed}        &     \parbox[c]{1em}{% Figure removed}          &   \parbox[c]{1em}{% Figure removed}                    &     \parbox[c]{1em}{% Figure removed}      &     \parbox[c]{1em}{% Figure removed}           &     \parbox[c]{1em}{% Figure removed}         &            \parbox[c]{1em}{% Figure removed}          \\
Spray &  \parbox[c]{1em}{
% Figure removed}  &      \parbox[c]{1em}{
% Figure removed}     &    \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}                      &  \parbox[c]{1em}{% Figure removed}         &     \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}           &  \parbox[c]{1em}{% Figure removed}                  \\ \bottomrule
\end{tabular}%
}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
% In order to initialize the active exploration phase for tactile data collection, a coarse bounding-box information is necessary which can be user-provided or automatically detected using an RGB camera with simple contour segmentation techniques.
The height of the occupancy grid is set constant for every object at 0.4m which is larger than the biggest object. 
% While our network can upsample an input point cloud with even 20 points, 
Reconstruction with acceptable accuracy is obtained with 100 points or more as input. For each object, ten tactile point clouds with point number between 100 and 120 points are extracted using the active exploration strategy and used for reconstruction. The ground-truth point cloud and CAD mesh are obtained by spray-painting the objects and using a scanning device. For evaluation, we use the following performance metrics: Hausdorff distance (HD), Chamfer distance (CD) and Earth Mover distance (EMD). The CD is described in Sec.~\ref{ssec:deep_reconstruction}. Given two points $S_1$ and $S_2$, the Hausdorff distance is defined as~\cite{berger2013benchmark}:
\begin{equation}
    HD(S_1, S_2) = \max\{ \max_{x \in S_1}\min_{y \in S_2}\ \{ ||x - y ||_{2} \}, \max_{y \in S_2}\min_{x \in S_1} \{||y - x ||_{2} \} \}
    \label{eq:hd}
\end{equation}
The HD represents the maximum distance between the two point sets and can be affected by extreme outliers during the reconstruction.
The EMD finds a bijection $\phi: S_1 \rightarrow S_2$ to minimise the average distance between corresponding points in the point clouds as:
\begin{equation}
    EMD(S_1, S_2) = \min_{\phi: S_1 \rightarrow S_2} \frac{1}{|S_1|}\sum_{x \in S_1}  || x - \phi(x)||_2 \quad .
    \label{eq:emd}
\end{equation}
A perfect reconstruction will yield $\{CD, HD, EMD\} \rightarrow 0$ and lower values signify better reconstruction.  \\
We use Gaussian Process Implicit Surfaces (GPIS) as baseline as it is widely used in the literature for tactile-based object reconstruction~\cite{dragiev2011gaussian, yi2016active, bjorkman2013enhancing, gandler2020object, martens2016geometric, suresh2021tactile, jamali2016active}. For implementation, we utilise the GP for machine learning toolbox~\cite{rasmussen2010gaussian} in MATLAB and the Mat\'ern kernel.
% We provide identical input tactile pointclouds to the GPIS method as our proposed method.

The quantitative results of tactile-based reconstruction using our method and baseline GPIS method are shown in Fig.~\ref{fig:quant_plots} and qualitative reconstruction results are presented in Tab.~\ref{tab:qualitative_results}.
From Fig.~\ref{fig:quant_plots}, we note our proposed approach yields lower CD values for all objects. For HD and EMD, apart from the bottle and spray, our method performs better than the baseline approach. On average, our approach is 45\%, 23.5\% and 28\% lower in CD, HD and EMD values compared to baseline GPIS. While the quantitative results focus on local point-distances between the reconstructed and ground-truth point cloud, the qualitative results in Tab.~\ref{tab:qualitative_results} demonstrate the differences in reconstruction accuracy at the object level. GPIS produces warped reconstructed surfaces due to the low number of tactile points. Whereas our method, with the help of the learned model over the category-level synthetic objects, is able to reconstruct the object to an acceptable accuracy even with sparse input data.

\textbf{Active Tactile Reconstruction:} Using our proposed framework ACTOR, we can achieve accurate reconstruction with fewer tactile actions in comparison to the baselines as shown in Fig.~\ref{fig:active_tactile}. 
We define an uniform object exploration and random object exploration strategy as baselines as follows: the bounding box around the object is transformed into a grid with each grid cell of size 3cm $\times$ 3cm (size of the sensor patch). The grid does not encode the probabilistic occupancy as in our ACTOR approach. The robot explores each grid cell in a sequential manner in the uniform strategy. In contrast, for the random strategy, the robot picks a grid cell at random for exploration.
% The current explored grid cells do not influence the next cell to explore in both the baseline approaches. 
In order to have an unbiased comparison between the exploration methods, a maximum of 20 actions are chosen as on average it takes 20 actions to extract atleast 100 tactile points. We begin the model inference from the 4th action onwards to have a minimum of 20 points in the tactile point cloud.
% The chamfer distance with the ground-truth after each action provides the evaluation method.
We note that the uniform strategy requires a large number of tactile actions to completely explore the object in order for reconstruction. The random  strategy has high variance in terms of reconstruction accuracy and stems from the stochastic nature of the exploration while ACTOR deterministically improves reconstruction accuracy with the increasing number of tactile actions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % Active tactile results plot
% % Figure environment removed
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Tactile-based Transparent Object Pose Estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pose Estimation results
% % Figure environment removed
% % Figure environment removed
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% With the reconstructed point cloud as the object point cloud, we perform category-level pose estimation through point cloud registration using the sensed tactile point cloud as the scene point cloud.
As the error in reconstruction propagates to downstream tasks, we perform two experiments: firstly, instance-level estimation using the ground-truth model point cloud as the object point cloud (Fig.~\ref{fig:pose_est}) and secondly, category-level pose estimation using the reconstructed point cloud as the object point cloud (Fig.~\ref{fig:pose_est_cat}). For category-level pose estimation, norm scale error is also reported in addition to rotation and translation.
As our proposed TIQF method is a local registration method, we chose the standard Iterative Closest Point (ICP)~\cite{besl1992method} and Sparse Iterative Closest Point (S-ICP)~\cite{bouaziz2013sparse} as baselines. S-ICP is chosen as it demonstrates higher robustness to outliers and incomplete data as typically found in tactile point clouds. We use the Average Distance of model points with Indistinguishable views metric (ADI)~\cite{hinterstoisser2013model} as a combined measure of the rotation and translational error as we have multiple objects with axis of symmetry. The ADI metric is defined as:    
\begin{equation}
    \mathtt{err}_{adi} = \frac{1}{|\mathcal{O}|}\sum_{\mathbf{p}_1 \in \mathcal{O}} \min_{\mathbf{p}_2 \in \mathcal{O}} || (\mathbf{R}_{gt}\mathbf{p}_1 + \mathbf{t}_{gt}) - (\mathbf{R}_{est}\mathbf{p}_2 + \mathbf{t}_{est}) ||,
    \label{eq:adi}
\end{equation}
where $(\mathbf{R}_{gt}, \mathbf{t}_{gt})$ and $(\mathbf{R}_{est}, \mathbf{t}_{est})$ refers to ground-truth and estimated rotation and translation respectively.
As seen from Fig.~\ref{fig:pose_est},\ref{fig:pose_est_cat}, our proposed approach outperforms the baseline approaches for all input tactile point clouds with varying point numbers demonstrating robustness to point sparsity. The median $\mathtt{err}_{adi} < 1cm$ for our proposed approach even with sparse point clouds with  $N_{P^t} = 20$  and improves with increasing the number of points. The category-level pose estimation errors are higher than instance-level due to the errors in the reconstructed point clouds. However, the accuracy improves by reducing scale error with median $ \mathtt{err}_{adi} < 2cm$ for $N_{P^t} = 120$ with our proposed method. 

% \setlength{\textfloatsep}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Confusion matrix
% \begin{table}[b!]
% \centering
% \caption{Confusion matrix for tactile-based object recognition}
% \label{tab:confusion}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{cccccccc}
% \cline{3-8}
%                                 & \multicolumn{1}{c|}{Bottle}    & \cellcolor[HTML]{343434}{\color[HTML]{FFFFFF} 0.98} & \cellcolor[HTML]{EFEFEF}0.2                         & 0                            & 0                                                   & 0                                                   & \multicolumn{1}{c|}{0}                                                      \\
%                                 & \multicolumn{1}{c|}{Can}       & \cellcolor[HTML]{C0C0C0}0.11                        & \cellcolor[HTML]{656565}{\color[HTML]{FFFFFF} 0.78} & \cellcolor[HTML]{C0C0C0}0.11 & 0                                                   & 0                                                   & \multicolumn{1}{c|}{0}                                                   \\
%                                 & \multicolumn{1}{c|}{Detergent} & \cellcolor[HTML]{EFEFEF}0.08                        & 0                                                   & \cellcolor[HTML]{9B9B9B}0.73 & 0                                                   & 0                                                   & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}0.19}                        \\
%                                 & \multicolumn{1}{c|}{Cup}       & 0                                                   & 0                                                   & \cellcolor[HTML]{EFEFEF}0.04 & \cellcolor[HTML]{343434}{\color[HTML]{FFFFFF} 0.96} & 0                                                   & \multicolumn{1}{c|}{0}                                                   \\
%                                 & \multicolumn{1}{c|}{Wineglass} & 0                                                   & \cellcolor[HTML]{EFEFEF}0.05                        & 0                            & \cellcolor[HTML]{FFFFFF}0                           & \cellcolor[HTML]{343434}{\color[HTML]{FFFFFF} 0.95} & \multicolumn{1}{c|}{0}                                                   \\
%                                 & \multicolumn{1}{c|}{Spray}     & 0                                                   & \cellcolor[HTML]{EFEFEF}0.04                        & \cellcolor[HTML]{C0C0C0}0.13 & 0                                                   & 0                                                   & \multicolumn{1}{c|}{\cellcolor[HTML]{656565}{\color[HTML]{FFFFFF} 0.83}} \\ \cline{3-8} 
% \multirow{-7}{*}{\rotatebox[origin=c]{90}{\textbf{True}}} &                                & Bottle                                              & Can                                                 & Detergent                    & Cup                                                 & Wineglass                                           & Spray                                               \\
% \multicolumn{1}{l}{}           & \multicolumn{1}{l}{}           & \multicolumn{6}{c}{\textbf{Predicted}}                                                                                                                                                                                                                                                                    \\ 
% \end{tabular}%
% }
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % table with and without attention
% \begin{table}[b!]
% \centering
% \caption{Comparison of the performance with and without self-attention in the network.}
% \label{tab:atten}
% % \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}llll@{}}
% \toprule
%                                    & \textbf{HD}  $\downarrow$     & \textbf{CD}   $\downarrow$   & \textbf{EMD}   $\downarrow$   \\
%                                    & \multicolumn{3}{c}{All cols. $\times$ $10^{-3}$} \\ \midrule
% \textbf{ACTOR (without attention)} & 30.65             & 13.96            & 16.66             \\
% \textbf{ACTOR (with attention)}    & 27.01             & 10.26            & 13.27             \\ \bottomrule
% \end{tabular}%
% % }
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Tactile-based Transparent Object Recognition}

% % Another downstream application of ACTOR is recognition of transparent object through tactile sensing. 
% % The synthetic dataset is randomly subsampled to have point clouds between [100,120] points and is used to train the recognition network. 
% The trained recognition model is tested on tactile pointclouds extracted from the objects in Tab.~\ref{tab:qualitative_results}. The resulting confusion matrix is shown in Tab.~\ref{tab:confusion}. We achieve an overall recognition accuracy of 87\%. While most of the categories are recognized with high accuracy, can is confused with bottle and detergent due to similarity in local curvature and detergent and spray categories are confused with each other. This stems from the similarities in shape, particularly if the nozzle of the spray is not explored during active tactile object exploration.



\subsection{Discussion}
Our proposed approach, ACTOR outperforms the GPIS strategy by all our evaluation metrics. We also note the qualitative reconstruction results in Tab.~\ref{tab:qualitative_results}, wherein GPIS fails to capture the shape details of the object while our approach captures the global and local shape accurately (see object spray and wineglass). Our network implicitly learns important feature points and is able to reconstruct the object accurately given few sparse inputs.
% In Tab.~\ref{tab:atten}, we also show the case without using self-attention in the autoencoder and note that the performance  deteriorates (by $\sim$ 10\% HD metric) in comparison to using the self-attention layer.
Our active exploration strategy converges faster to reconstruct the object shape thus improving the sample efficiency.
Furthermore, our proposed category-level pose estimation method outperforms the baseline methods by $\geq$ 25\% ADI error. 

A limitation of the work is the need for category-wise object models for training. A possible future work includes using neural radiance fields (NeRFs)~\cite{wang2021nerf} to generate synthetic models of objects from images that can be used for training. 


















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Legacy
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \begin{table}[]
% \centering
% \caption{}
% \label{tab:my-table}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}llll@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Object}} & \multicolumn{1}{c}{\textbf{ICP}} & \multicolumn{1}{c}{\textbf{SICP}} & \multicolumn{1}{c}{\textbf{TIQF (ours)}} \\
%                                     & \multicolumn{3}{c}{\textbf{ADI (mm)}}                                                                           \\ \midrule
% \textbf{Bottle\_1}                  & 9.44                             & 4.32                              &                                          \\
% \textbf{Bottle\_2}                  & 8.69                             & 8.26                              &                                          \\
% \textbf{Can}                        & 2.22                             & 2.20                              &                                          \\
% \textbf{Cleaner}                    & 12.66                            & 11.16                             &                                          \\
% \textbf{Cup\_1}                     & 8.61                           
% & 8.60                              &                                          \\
% \textbf{Cup\_2}                     & 10.97                            & 11.09                             &                                          \\
% \textbf{Cup\_3}                     & 10.42                            & 10.38                             &                                          \\
% \textbf{Wineglass}                  & 46.12                            & 48.41                             &                                          \\
% \textbf{Spray}                      & 8.40                             & 7.04                              &                                          \\ \bottomrule
% \end{tabular}%
% }
% \end{table}

% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% % \usepackage{multirow}
% % \usepackage{graphicx}
% \begin{table}[t!]
% \centering
% \caption{}
% \label{tab:recons}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}lllllll@{}}
% \toprule
% \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Object}}} & \multicolumn{3}{c}{\textbf{GPIS}}                                                                    & \multicolumn{3}{c}{\textbf{ACTOR (Ours)}}                                                                    \\
% \multicolumn{1}{c}{}                                 & \multicolumn{1}{c}{\textbf{HD} $\downarrow$} & \multicolumn{1}{c}{\textbf{CD}$\downarrow$} & \multicolumn{1}{c}{\textbf{EMD}$\downarrow$} & \multicolumn{1}{c}{\textbf{HD}$\downarrow$} & \multicolumn{1}{c}{\textbf{CD}$\downarrow$} & \multicolumn{1}{c}{\textbf{EMD}$\downarrow$} \\
% &\multicolumn{3}{c}{All cols. $\times 1000$}                                                                    & \multicolumn{3}{c}{All cols. $\times 1000$}                             \\ \midrule
% \textbf{Bottle 1}                                   &    16.402                       &   10.822                        & \multicolumn{1}{l|}{6.542}       &          21.358                       &        6.135                         &      5.657                           \\
% \textbf{Bottle 2}                                   &     19.894                     &  8.224                          & \multicolumn{1}{l|}{5.951}       &          23.368                       &      6.432                            &       6.086                            \\
% \textbf{Can}                                         &          23.621                &   10.831                         & \multicolumn{1}{l|}{6.962}       &             18.804                    &   9.433                              &    15.083                              \\
% \textbf{Detergent}                                     &      52.521                      &    21.207                         & \multicolumn{1}{l|}{17.414}       &      21.592                           &                    9.335             &                 15.355                 \\
% \textbf{Cup 1}                                      &      23.199                      &             13.098               & \multicolumn{1}{l|}{8.917}       &        32.559                         &              14.162                   &           16.839                       \\
% \textbf{Cup 2}                                      &     24.638                       &    12.954                        & \multicolumn{1}{l|}{ 8.453}       &             18.924                    &         8.817                        &            15.046                      \\
% \textbf{Cup 3}                                      &         34.983                   &   15.338                      & \multicolumn{1}{l|}{10.049}       &    26.872                             &        8.216                         &                    12.708              \\
% \textbf{Wineglass}                                   &       61.629                    &  35.476                          & \multicolumn{1}{l|}{26.315}       &      46.358                           &         11.302                        &           14.938                       \\
% \textbf{Spray}                                       &      38.861                    &     16.223                       & \multicolumn{1}{l|}{10.345}       &      38.559                           &     14.329                            &        13.422                          \\ \hline
% \textbf{Average} & & &\multicolumn{1}{l|}{5.951} &  & & \\
% \textbf{SD} & & &\multicolumn{1}{l|}{5.951} &  & & \\\bottomrule
% \end{tabular}%
% }
% \end{table}


% backup
% Qualitative recons result
% \begin{table*}[t!]
% \centering
% \caption{Qualitative reconstruction results of our proposed method in comparison with Gaussian process implicit surfaces for unknown real test objects. (Best viewed on screen in color).}
% \label{tab:qualitative_results}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}lcccccccc@{}}
% \toprule
% Object  &  & Tactile PC  & \multicolumn{2}{c}{GPIS}   & \multicolumn{2}{c}{ACTOR (ours)} & \multicolumn{2}{l}{Ground Truth} \\
%          & & N $\sim$120 & Recon. PC & Recon. Surf. & Recon. PC    & Recon. Surf.    & PC           & Surf.           \\ \midrule
% Bottle 1 & \parbox[c]{1em}{
% % Figure removed}    &   \parbox[c]{1em}{
% % Figure removed}              & 
% \parbox[c]{1em}{% Figure removed}           & 
% \parbox[c]{1em}{% Figure removed}              &
% \parbox[c]{1em}{% Figure removed}
% & \parbox[c]{1em}{% Figure removed}  
% & \parbox[c]{1em}{% Figure removed}            &    \parbox[c]{1em}{% Figure removed}               \\
% Bottle 2 &\parbox[c]{1em}{
% % Figure removed} &   \parbox[c]{1em}{
% % Figure removed}              &    \parbox[c]{1em}{% Figure removed}       &    \parbox[c]{1em}{% Figure removed}            &   \parbox[c]{1em}{% Figure removed}           &   \parbox[c]{1em}{% Figure removed}                & \parbox[c]{1em}{% Figure removed}              &   \parbox[c]{1em}{% Figure removed}                 \\
% Can     &\parbox[c]{1em}{
% % Figure removed}   &     \parbox[c]{1em}{
% % Figure removed}            &  \parbox[c]{1em}{% Figure removed}         &     \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}          &     \parbox[c]{1em}{% Figure removed}            &      \parbox[c]{1em}{% Figure removed}         &     \parbox[c]{1em}{% Figure removed}               \\
% Detergent &\parbox[c]{1em}{
% % Figure removed} &      \parbox[c]{1em}{
% % Figure removed}           &   \parbox[c]{1em}{% Figure removed}        &    \parbox[c]{1em}{% Figure removed}            &   \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}               &      \parbox[c]{1em}{% Figure removed}         &      \parbox[c]{1em}{% Figure removed}              \\
% Cup 1 &  \parbox[c]{1em}{
% % Figure removed}  &    \parbox[c]{1em}{
% % Figure removed}             &   \parbox[c]{1em}{% Figure removed}        &  \parbox[c]{1em}{% Figure removed}              &     \parbox[c]{1em}{% Figure removed}           &        \parbox[c]{1em}{% Figure removed}             &      \parbox[c]{1em}{% Figure removed}         &  \parbox[c]{1em}{% Figure removed}                  \\
% Cup 2  &  \parbox[c]{1em}{
% % Figure removed} &   \parbox[c]{1em}{
% % Figure removed}              &   \parbox[c]{1em}{% Figure removed}        &      \parbox[c]{1em}{% Figure removed}          &   \parbox[c]{1em}{% Figure removed}           &        \parbox[c]{1em}{% Figure removed}           &     \parbox[c]{1em}{% Figure removed}          &    \parbox[c]{1em}{% Figure removed}                \\
% Cup 3  &  \parbox[c]{1em}{
% % Figure removed} &    \parbox[c]{1em}{
% % Figure removed}             &    \parbox[c]{1em}{% Figure removed}       &     \parbox[c]{1em}{% Figure removed}           &     \parbox[c]{1em}{% Figure removed}         & \parbox[c]{1em}{% Figure removed}                  &     \parbox[c]{1em}{% Figure removed}          &  \parbox[c]{1em}{% Figure removed}                  \\
% Wineglass &\parbox[c]{1em}{
% % Figure removed}&   \parbox[c]{1em}{
% % Figure removed}              &     \parbox[c]{1em}{% Figure removed}      &     \parbox[c]{1em}{% Figure removed}           &     \parbox[c]{1em}{% Figure removed}         &            \parbox[c]{1em}{% Figure removed}       &     \parbox[c]{1em}{% Figure removed}          &   \parbox[c]{1em}{% Figure removed}                 \\
% Spray &  \parbox[c]{1em}{
% % Figure removed}  &      \parbox[c]{1em}{
% % Figure removed}           &  \parbox[c]{1em}{% Figure removed}         &     \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}           &  \parbox[c]{1em}{% Figure removed}                  &    \parbox[c]{1em}{% Figure removed}           &    \parbox[c]{1em}{% Figure removed}                \\ \bottomrule
% \end{tabular}%
% }
% \end{table*}