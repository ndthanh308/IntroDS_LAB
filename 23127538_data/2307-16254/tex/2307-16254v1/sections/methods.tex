\section{METHODS}
\label{sec:methods}
\subsection{Problem Definition and Proposed Framework}
The objective is to reconstruct a dense point cloud that precisely represents the shape of unknown transparent objects from sparse point clouds extracted with active tactile interactive perception. To this end, we propose a novel framework termed ACTOR shown in Fig.~\ref{fig:framework}. In Fig.~\ref{fig:framework}(a) we propose a self-surpervised learning approach with an autoencoder network that is trained on subsampled pointclouds from synthetic objects belonging to the same category but not identical as the real objects. In Fig.~\ref{fig:framework}(b), we propose a novel active tactile-based unknown transparent object exploration strategy which is used for inference with our trained model to reconstruct a dense point cloud. We demonstrate downstream tasks such as tactile-based pose estimation.
% and tactile-based object recognition. 

\subsection{Deep Self-Supervised Learning for 3D Object Reconstruction}
\label{ssec:deep_reconstruction}
We generate a dataset $\mathcal{D}$\footnote{\url{https://www.robotact.de/tactile-reconstruction}} of synthetic object models from the ShapeNet repository~\cite{chang2015shapenet} in order to leverage the open-source datasets and avoid expensive real tactile-data collection. The synthetic object models belong to the same category but are different from the real unknown transparent objects. 
We uniformly sample $N_{in} = 2048$ points from the synthetic object meshes. These pointclouds are normalized and scaled to fit into a $[0,1]^3$ cube and added to the dataset, $\mathcal{P}_{in} \in \mathcal{D}$. 
% The generated dataset is provided in the project page\footnote{\url{https://robotac-bmw.github.io/tactile_reconstruction/}}.
In order to generate the input point clouds $\mathcal{P}^{\bullet}_{in}$ to the network, we randomly subsample the $\mathcal{P}_{in}$ by voxel-grid subsampling by the factor $k$ i.e., $\mathcal{P}^{\bullet}_{in} \in \mathbb{R}^{\lceil \frac{1}{k}N_{in} \rceil \times 3}$.  This creates a challenging task for reconstruction with higher values for $k$ as simpler techniques based on interpolation with neighborhood points cannot be used. 

\subsubsection*{Feature-Extraction Encoder}
The network architecture shown in Figure~\ref{fig:framework}(a) is proposed as an autoencoder (AE) that uses a self-supervised approach to reconstruct the original point cloud from a subsampled point cloud. 
The encoder takes subsampled point clouds as inputs and generates a high dimensional feature vector. The feature vector captures the global geometric shape information of the input point cloud. 
In general, any deep network that works on raw input point clouds to provide a high dimensional feature vector can be used as an encoder. In particular,
we use a modified PointNet architecture~\cite{qi2017pointnet} for the encoder. PointNet takes unordered point clouds and generates a global feature descriptor vector of size 1024. The network learns a set of optimization functions that select interesting or informative points of the point cloud. The encoder consists of $[1\times1]$ convolutions with output channels size $(64, 64, 128, 1024)$ with the first convolutional layer with kernel size $[1\times3]$ to encode the input pointcloud of $N\times3$ dimension. The convolution layers are aggregated by a max-pooling layer. We introduce a self-attention layer~\cite{zhang2019self} whose outputs are aggregated with the max-pooled features to provide the global feature vector.  
We have summarized the encoder in Figure~\ref{fig:framework}(a).
% As the encoder provides a high-dimensional global feature vector, we term it as feature-extraction encoder.

\textbf{Self-Attention (SA) Layer:} The SA layer is introduced as it can encode meaningful spatial relationships between features and focus on important local features. From the input layer ($\mathtt{conv2d-1024}$), two separate multi-layer perceptrons (MLPs) are used to get features $\mathbf{G}$ and $\mathbf{H}$ which are subsequently used to get the weights as $\mathbf{W} = softmax(\mathbf{G}^T\mathbf{H})$. The input features are transformed using another MLP to obtain $\mathbf{K}$ and multiplied with the weights as $\mathbf{W}^T\mathbf{K}$.
These vectors are summed with the input vector to produce the output features.
% The SA layer description is shown in Fig.~\ref{fig:self_atten}.  
% \setlength{\columnsep}{0pt}
% \begin{wrapfigure}[12]{r}{0.8\linewidth}
%   \centering
%     % \vspace{-0.5cm}
%     % Figure removed
%   \caption{The self-attention unit.}
%     % \vspace{-0.5cm}
%   \label{fig:self_atten}
% \end{wrapfigure}
% % Figure environment removed

\subsubsection*{Upsampling Decoder}
We design an upsampling decoder that upsamples the input global feature vector to provide the reconstructed dense output point cloud $\mathcal{P}_{out}$. The upsampling decoder is composed by a fully connected layer with output dimension of 1024 and five deconvolutional layers with kernel sizes and output channels shown in Fig.~\ref{fig:framework}(a).  
The decoder produces the output point cloud with point size set to 2048 while training as this is sufficiently dense for reconstruction purposes. 

\subsubsection*{Loss Function}
In order to encourage the upsampled point cloud to be in proximity to the original input point cloud and follow the underlying geometrical surface of the object, we use the Chamfer distance metric~\cite{borgefors1986distance} as the loss. Given the input point cloud prior to subsampling, $\mathcal{P}_{in}$ and the reconstructed output point cloud $\mathcal{P}_{out}$, the loss is defined as:
\begin{align}
    \mathcal{L}_{CD}(\mathcal{P}_{in}, \mathcal{P}_{out}) &= \frac{1}{|\mathcal{P}_{in}|}\sum_{p_1 \in \mathcal{P}_{in}} \min_{p_2 \in \mathcal{P}_{out}} ||p_1 - p_2||_{2} + \\ & \frac{1}{|\mathcal{P}_{out}|}\sum_{p_2 \in \mathcal{P}_{out}} \min_{p_1 \in \mathcal{P}_{in}} ||p_2 - p_1||_{2} \nonumber,
    \label{eq:chamfer_dist}
\end{align}
where $|\bullet|$ refers to the number of points in the point cloud and $||\bullet||_2$ refers to the L2 norm. The loss $\mathcal{L}_{CD}$ represents the average distance between the \textit{closest} points in the two point clouds. We use the weighted loss for learning stability as the reconstruction loss $\mathcal{L}_{rec} = \alpha\mathcal{L}_{CD}$ with $\alpha = 100$ set empirically.
For surface reconstruction from the dense reconstructed point cloud, we use the ball-pivoting algorithm~\cite{bernardini1999ball}.

% \subsubsection*{Recognition Network}
% \label{ssec:recog_net}
% The pretrained encoder layers for reconstruction task are frozen for category-level classification. We employ three fully-connected layers with parameters 512, 256, and $n$ respectively where $n$ represents the number of categories of the objects.
% The softmax cross-entropy loss is used for training the recognition network. The recognition head is shown in Fig.~\ref{fig:framework}(a.I). The subsampled sparse point clouds from our synthetic dataset with different subsampling ratios and data augmentation with random rotations are used. Network implementation details are provided in Sec.~\ref{ssec:setup}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Active Deep Tactile-based Unknown Transparent Object Reconstruction and Pose Estimation}
\subsubsection{Active Tactile-based Transparent Object Reconstruction}
The model trained with only \textit{synthetic data} as described in Sec.~\ref{ssec:deep_reconstruction} is used during the inference with \textit{real-world} transparent objects. The sparse tactile point cloud data is collected autonomously by the robot using an information gain-based active strategy. We define two types of tactile actions for data acquisition: touch and pinch actions as shown in Figure~\ref{fig:occupancy_grid}.
% The action nomenclature is derived from human grasp taxonomy studies~\cite{feix2015grasp}.
The touch action is executed as a guarded horizontal straight-line motion wherein the object is not moved upon contact. The touch action is defined by a tuple $\mathbf{a}^{t} = \{\mathbf{s}^t, \overrightarrow{\mathbf{d}^t} \}$ where $\mathbf{s}^t \in \mathbb{R}^3$ is the start point of the tactile-sensorised gripper and $\overrightarrow{\mathbf{d}^t} \in \mathbb{R}^3$ is the direction of the gripper-motion defined in the world-coordinate frame $\mathcal{W}$. During the pinch action the robot approaches the object in a vertical straight-line motion with a completely open gripper and performs an antipodal enclosure grasp on the object. The fingers of the gripper are closed until the force on the tactile sensors exceeds a predefined threshold.
The pinch action is characterized by $\mathbf{a}^{p} = \{\mathbf{s}^p \}$ where $\mathbf{s}^p \in \mathbb{R}^3 $ is the start position of the gripper motion vertically above the object at a predefined height as shown in Figure~\ref{fig:occupancy_grid}. Given the 2D bounding box of the object (a priori known or through a RGB camera), a probabilistic occupancy grid $\mathcal{OG}_i$ of preset height and resolution $og_{res}$ is defined. Each cell of the occupancy grid $c_i$ is represented by an occupancy probability $p(c_i)$ which is initially set to 0.5. During exploration, if a cell is discovered to belong to the object, the probability is set to 1 and similarly, if the cell belongs to free space, the probability is set to 0. The probabilities are updated through ray intersections based on the virtual sensor model. We define a virtual sensor model of the tactile sensor which casts a set of rays $\mathcal{R} = \{r_1, r_2, \dots, r_{n_{taxel}} \}$ where ${n_{taxel}} $ refers to the number of taxels in the sensor array. The independence assumption of the probability of each grid cell with one another allows us to calculate the overall entropy of the $\mathcal{OG}$ as the summation of the entropy of each cell. The Shannon entropy of the overall occupancy grid is calculated as:
\begin{equation}
    \mathbb{H}(\mathcal{OG}) = \sum_{c_i \in \mathcal{OG}} p(c_i)log(p(c_i)) + (1 - p(c_i))(1 - log(p(c_i))).
    \label{eq:entropy}
\end{equation}
Monte-Carlo sampling of possible tactile actions $N_{nbt}$ are performed for computing the next best tactile (NBT) action. The actions space $\mathcal{A}_{nbt}$ is comprised of an equal number of touch and pinch respectively as $\mathcal{A}_{nbt} = \{a^p, a^t\}_{N_{nbt}}$. The expected measurements $\hat{\mathbf{z}}_t$ for each action $a_t \in \mathcal{A}$ is computed using ray-traversal algorithms~\cite{hornung2013octomap}. 
Given the observed grid cell $c$ and the measurement from sensor observation $z$, the log-odds is updated as $L(c|z) = L(c) + l(z)$ wherein $L(c) = log\frac{p(c)}{1-p(c)}$ and  
\begin{equation}
    l(z) = \left\{
                \begin{array}{ll}
                  log\frac{p_h}{1-p_h}  \quad \mathrm{if} \ z \widehat{=} \textit{ hit} \\
                  log\frac{p_m}{1-p_m} \quad \mathrm{if} \ z \widehat{=} \textit{ miss} 
                \end{array}
              \right.
    \label{eq:log-odds}
\end{equation}
where $p_h$ and $p_m$ are the probabilities of hit and miss which are user-defined values set to 0.7 and 0.4 respectively as in~\cite{hornung2013octomap}. The posterior probability $p(c|z)$ can be computed by inverting $L(c|z)$. The expected information gain by taking an action $a_t \in \mathcal{A}_{nbt}$ with expected measurement $\hat{\mathbf{z}}_t$ is provided by the Kullback-Liebler divergence of the posterior entropy and the prior entropy as: 
\begin{equation}
    E[\mathbb{I}(p(c_i | \mathbf{a}_t,  \hat{z}_t))] = \mathbb{H}(p(c_i)) - \mathbb{H}(p(c_i | \mathbf{a}_t,  \hat{z}_t))
    \label{eq:kl_view}
\end{equation}
Therefore, the action that maximizes the expected information gain is considered as the NBT action:
\begin{equation}
    \mathbf{a}^{nbt*}_t = \argmax_{\mathbf{a} \in \mathcal{A}}(E[\mathbb{I}(p(c_i | \mathbf{a}_t,  \hat{z}_t))])
    \label{eq:kl_view_max}
\end{equation}
Each tactile action extracts contact positions in 3D space and contact forces. The direction of the normal force is used to extract the normal direction $\hat{n}$ of the object surface. The contact points are aggregated into the tactile point cloud $\mathcal{P}^t$. In order to initialize the NBT action calculation, an initial point cloud (with $N_{\mathcal{P}^t} = 20$) is required, which is extracted by randomised touch actions. Further points are collected in an active manner using the NBT criteria. A minimum number of points in the tactile point cloud is required to perform model inference $N_{\mathcal{P}^t} > N_{min}$ which is tuned empirically. The tactile point cloud is provided as input to the trained network and the reconstructed point cloud $\mathcal{P}_{out}$ is obtained . 
% This is used for downstream task Section~\ref{ssec:pose_estimation}. 
% For acceptable reconstruction accuracy around 100 tactile points is required.  

% [TODO:] check for action taxonomy if its correct

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tactile-Based Object Pose Estimation}
\label{ssec:pose_estimation}

We perform the 6D pose estimation through dense to sparse point cloud registration. The sparse scene point cloud $\mathbf{s}_i \in \mathcal{S}$ is represented by the tactile points and the dense object point cloud $\mathbf{o}_i \in \mathcal{O}$ is represented by the reconstructed point cloud in~\ref{ssec:deep_reconstruction} without the need for the object model. Point cloud registration problem with $M$ known correspondences can be formulated as:
\begin{equation}
     \mathbf{s}_i = \mathbf{S}\cdot(\mathbf{R}\mathbf{o}_i) + \mathbf{t} \quad i = 1, \dots M,
     \label{eq:generativemodel}
 \end{equation}
where $\mathbf{S} \in \mathbb{R}^3$ represents scale, $\mathbf{R} \in SO(3)$ represents rotation and $\mathbf{t} \in \mathbb{R}^3$ represents translation which are unknown and to be estimated and $\cdot$ is the element-wise product. 
%% [TODO] : check derivation

We perform the point cloud registration using our novel translation-invariant Quaternion filter (TIQF) presented in~\cite{murali2022active} to determine $\mathbf{R}$, $\mathbf{S}$ and $\mathbf{t}$. 
The scale, rotation and translation are decoupled by finding the relative vectors between corresponding points, i.e., $\forall o_i, o_j \in \mathcal{O}, s_i, s_j \in \mathcal{S}$ the relative vectors are $\mathbf{s}_{ji} = \mathbf{s}_j - \mathbf{s}_i$ and $\mathbf{o}_{ji} = \mathbf{o}_j - \mathbf{o}_i$. Equation~\eqref{eq:generativemodel} is reformulated as:
\begin{align}
    \mathbf{s}_j - \mathbf{s}_i &= (\mathbf{S}\cdot\mathbf{R}\mathbf{o}_j + \mathbf{t}) - (\mathbf{S}\cdot\mathbf{R}\mathbf{o}_i + \mathbf{t}) ,\\
    \mathbf{s}_{ji} &= \mathbf{S}\cdot\mathbf{R}\mathbf{o}_{ji} \quad .
    \label{eq:trans_invariance}
\end{align}

We note that equation~\eqref{eq:trans_invariance} is independent of translation. Taking the L2-norm on both sides for Eq.~\eqref{eq:trans_invariance} and recalling that norm is rotation invariant we get:
\begin{equation}
    \mathbf{||s||}_{ji} = \mathbf{||S||}\cdot\mathbf{||o||}_{ji} \quad .
    \label{eq:rot_invariance}
\end{equation}
The scale $\mathbf{S}$ is estimated by taking the ratio of the axis aligned bounding box (AABB) of the scene and object point clouds, i.e., if $\mathcal{X}_{AABB} = \{ (x_{min}, x_{max}), (y_{min}, y_{max}), (z_{min}, z_{max}) \}$ represents the AABB for a point cloud $\mathcal{X}$, then:
\begin{align}
     \mathbf{S} &= \{ \frac{|x_{max} - x_{min}|_{\mathcal{S}}}{|x_{max} - x_{min}|_{\mathcal{O}}}, \frac{|y_{max} - y_{min}|_{\mathcal{S}}}{|y_{max} - y_{min}|_{\mathcal{O}}} , \frac{|z_{max} - z_{min}|_{\mathcal{S}}}{|z_{max} - z_{min}|_{\mathcal{O}}}    \}
     \label{eq:scale}
 \end{align}
Using the estimated scale and using $\tilde{\mathbf{o}}_{ji} = \mathbf{S}\mathbf{o}_{ji}$ for convenience we are left with a pure rotation to estimate:  
\begin{align}
    \tilde{\mathbf{s}}_{ji} &= \mathbf{R}\tilde{\mathbf{o}}_{ji} \quad .
    \label{eq:trans_scale_invariance}
\end{align}
 We cast the rotation estimation problem into a recursive Bayesian estimation framework and derive a linear state and measurement model. Reformulating Eq.\eqref{eq:trans_scale_invariance} using quaternions we get: 
 \begin{equation}
    \overline{\mathbf{s}}_{ji} = \mathbf{x} \odot \overline{\mathbf{o}}_{ji} \odot \mathbf{x}^{*}, 
    \label{eq:quat_objective}
\end{equation}
where $\mathbf{x}$ is the quaternion form of $\mathbf{R}$, $\odot$ is the quaternion product, ${\mathbf{x}}^{*}$ is the conjugate of $\mathbf{x}$, and $\overline{\mathbf{s}}_{ji}=\{0,\tilde{\mathbf{s}}_{ji}\}$ and $\overline{\mathbf{o}}_{ji}=\{0,\tilde{\mathbf{o}}_{ji}\}$.
Using the matrix form of quaternion product, we can rewrite Eq.\eqref{eq:quat_objective} as:
\begin{align}
    \begin{bmatrix}
        0 & -\tilde{\mathbf{s}}_{ji}^T \\
        \tilde{\mathbf{s}}_{ji} & \tilde{\mathbf{s}}_{ji}^{\times}
    \end{bmatrix}\mathbf{x} -  \begin{bmatrix}
        0 & -\tilde{\mathbf{o}}_{ji}^T \\
        \tilde{\mathbf{o}}_{ji} & -\tilde{\mathbf{o}}_{ji}^{\times}
    \end{bmatrix} \mathbf{x} = \mathbf{0} \\
    \underbrace{\begin{bmatrix}
        0 & -(\tilde{\mathbf{s}}_{ji} - \tilde{\mathbf{o}}_{ij})^T \\
        (\tilde{\mathbf{s}}_{ji} - \tilde{\mathbf{o}}_{ji}) & (\tilde{\mathbf{s}}_j + \tilde{\mathbf{s}}_i + \tilde{\mathbf{o}}_j + \tilde{\mathbf{o}}_i)^{\times}
        \end{bmatrix}_{4 \times 4}}_{\mathbf{H}_t} \mathbf{x} &= \mathbf{0} \quad ,
        \label{eq:expected_measurement}
\end{align}
where $(\ )^\times$ denotes the skew-symmetric matrix formulation. Equation~\eqref{eq:expected_measurement} is of the form $\mathbf{H}_t\mathbf{x} = 0$ where $\mathbf{H}_t$ is the pseudo-measurement matrix~\cite{choukroun2006novel}. We note that Eq.~\eqref{eq:expected_measurement} represents a noise-free state estimation where $\mathbf{H}_t$ depends only on sparse and dense point correspondences which are $\tilde{\mathbf{s}}_{ji}$ and $\tilde{\mathbf{o}}_{ji}$. We design a pseudo-measurement model as $ \mathbf{H}_t \mathbf{x} = \mathbf{z}^h$
% \begin{align}
%     \mathbf{H}_t \mathbf{x} &= \mathbf{z}^h,
%     \label{eq:measurement_model}
% \end{align}
and set $\mathbf{z}^h = 0$. Since we have a static process model, the object does not move and $\mathbf{x}$ and $\mathbf{z}_t$ are Gaussian distributed, 
the state $\mathbf{x}_t$ and covariance matrix $\Sigma^{\mathbf{x}}_{t}$ at each timestep $t$ are computed through a linear Kalman filter. The Kalman filter equations are skipped for brevity and a in-depth derivation is provided in our prior work~\cite{murali2022active}.
As the Kalman filter does not implicitly ensure the constraints on the quaternion as $||\mathbf{x}|| = 1$, we normalise the state and uncertainty after each update step as $\bar{\mathbf{x}}_{t} = \frac{\mathbf{x}_{t}}{||\mathbf{x}_{t}||_2} \quad, \bar{\Sigma}^{\mathbf{x}}_{t} = \frac{\Sigma^{\mathbf{x}}_{t}}{||\mathbf{x}_{t}||_2^2}$. We convert the estimated rotation $\Bar{\mathbf{x}}_t$ to its equivalent rotation matrix $\mathbf{R}$. It used to estimate the translation using the following relation: $\mathbf{t} = \frac{1}{N} \sum_{i=0}^{N} (\Bar{\mathbf{s}}_i - \mathbf{R} \Bar{\mathbf{o}}_i).$
% \begin{equation}
%     \mathbf{t} = \frac{1}{N} \sum_{i=0}^{N} (\Bar{\mathbf{s}}_i - \mathbf{R} \Bar{\mathbf{o}}_i).
%     \label{eq:translation_solution}
% \end{equation}
% \setlength{\columnsep}{1pt}
% \begin{wrapfigure}[18]{r}{0.6\linewidth}
%   \centering
%     \vspace{-0.5cm}
%     % Figure removed
%   \caption{Translation-invariant measurements}
%     % \vspace{-0.5cm}
%   \label{fig:TIMS}
% \end{wrapfigure}
At each iteration, a rotation and translation estimate is found which is used to transform the object point cloud and the process is repeated by re-estimating the correspondence points. The convergence criteria are set by (a) maximum number of iterations or (b) the relative change in estimated pose parameters is less than a predefined threshold ($0.1mm$ and $0.1^o$). 

% the linear Kalman filter equations are given as:
% \begin{align}
%     \mathbf{x}_{t} &= \bar{\mathbf{x}}_{t-1} - \mathbf{K}_t \left( \mathbf{H}_t \bar{\mathbf{x}}_{t-1} \right) \\
%     \Sigma^{\mathbf{x}}_{t} &= \left( \mathbf{I} - \mathbf{K}_t \mathbf{H}_t \right) \bar{\Sigma}^{\mathbf{x}}_{t-1} \\
%     \mathbf{K}_t &= \bar{\Sigma}^\mathbf{x}_{t-1} \mathbf{H}_t^T \left( \mathbf{H}_t\bar{\Sigma}^\mathbf{x}_{t-1} \mathbf{H}_t^T + \Sigma_t^{\mathbf{h}}\right)^{-1}, 
%     \label{eq:kalman_equations}
% \end{align}
% where $\bar{\mathbf{x}}_{t-1}$ refers to the normalized mean of the state at $t-1$, Kalman gain $\mathbf{K}_t$ and $\bar{\Sigma}^{\mathbf{x}}_{t-1}$ is the covariance matrix of the state at $t-1$. 
% The parameter $\Sigma_t^{\mathbf{h}}$ is referred as the measurement uncertainty during time $t$. It is dependent on the state and is provided by~\cite{choukroun2006novel}:
% \begin{align}
%     \Sigma_t^{\mathbf{h}} = \frac{1}{4}\rho\left[ tr(\bar{\mathbf{x}}_{t-1}\bar{\mathbf{x}}_{t-1}^T + \bar{\Sigma}^{x}_{t-1})\mathbb{I}_4 - (\bar{\mathbf{x}}_{t-1}\bar{\mathbf{x}}_{t-1}^T + \bar{\Sigma}^{x}_{t-1} )\right], 
%     \label{eq:choukron}
% \end{align}
% wherein the constant $\rho$ corresponds to the uncertainty of the correspondence measurements and $tr$ refers to trace.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Transparent Object Manipulation}
% \label{ssec:tactile_manipulation}
% With the computed 6D pose and estimated CAD model, we design a simple grasping technique in order to grasp and lift the transparent objects. For each \textit{category} of objects, we generated several grasp plans using GraspIt~\cite{miller2004graspit}. Each grasp plan includes the grasp position, orientation and approach vector relative to the model of the object and a grasp quality score. With the pose of the object, the grasp plans are filtered based on kinematic constraints of the robot, workspace limitations and possible collisions with other objects in the scene. Among the remaining grasp plans, the plan with the highest score is chosen and executed. The robot lifts the transparent object and places it in a pre-defined position.
% An online grasp planning and collision avoidance framework is out of the scope of this current work but can be readily integrated into the current framework.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsubsection{Tactile-based Transparent Object Recognition}
% \label{ssec:classification}
% % Figure environment removed
% We use the pretrained encoder model with fixed weights for category-level classification. We employ three fully-connected layers with parameters 512, 256 and $n$ respectively where $n$ represents the number of categories of the objects. Transfer learning is employed to fine-tune the classification network shown in Figure~\ref{fig:framework}(a) on the sparse pointclouds from ShapeNet database.
% During inference, the real sparse tactile pointclouds are used as input to the network for recognition network described in Sec.~\ref{ssec:recog_net}. While the task is challenging, the real-world tactile data are not used during fine-tuning intentionally as collection of large-scale datasets is prohibitively time consuming. The input pointcloud is pre-processed prior to inference by normalising and scaling to fit in $[0,1]^3$ cube to be uniform with the training dataset.