\section{Experimental Framework}
\label{sec:exp}

\subsection{Dataset}
\label{ssec:datasets}

For our experiments, we construct synthetic mixtures based on single-speaker utterances from Spatial LibriSpeech, as introduced in \cite{tzinis22_interspeech}, which is a synthetically reverberant dataset based on speech utterances from LibriSpeech \cite{panayotov2015librispeech}.

In order to evaluate our method in a range of settings we create an ``Easy" and a ``Hard" dataset as described below. The mixing process consists of first sampling a female and a male speech utterance as well as a near-field and a far-field Room Impulse Response (RIR). By convolving each speech utterance with a randomly matched RIR we get the two source waveforms. Those sources are then mixed with an overlap sampled from $\mathcal{U}[60, 100]\%$ and Signal-to-Noise Ratio (SNR) sampled from $\mathcal{U}([-5, -0.5] \cup [0.5, 5]) \, \text{dB}$ in the case of the Easy dataset and $\mathcal{U}[80, 100]\%$ and $\mathcal{U}([-2.5, -0.5] \cup [0.5, 2.5])\, \text{dB}$ in the case of the Hard dataset. As a result, our generated mixtures consist of two sources $\mathbf{s}_1$ and $\mathbf{s}_2$ each with complementary source attributes (i.e., $\mathcal{G}$: Gender, $\mathcal{E}$ Energy, $\mathcal{O}$: Source order, $\mathcal{S}$: Spatial position) that can be represented in an 8-dimensional multi-one vector. Finally, we sample one condition $\mathcal{C}$ and a condition value $v \in \mathcal{C}$, that describes the target source $\mathbf{s}_T$, which we represent it as an 8-dimensional one-hot vector $\mathbf{c}$.

In total we generate 20,000, 3,000 and 3,000 training, validation and test samples, respectively, sampled at 8 kHz and with a 5-second duration. We note that during each training epoch generation is dynamic and performed on-the-fly. 


\subsection{Completion Model}

We construct the completion model based on the ECAPA-TDNN \cite{desplanques20_interspeech} architecture which has shown impressive performance in the domain of speaker verification. In order to be able to inject the conditional information $\mathbf{c}$ to the model, we make a series of modifications to the architecture. First, we add a FiLM \cite{perez2018film} layer right after the residual connection inside every SE-Res2Block. We also add a FiLM layer before the final fully-connected layer of the network, as well as a sigmoid activation after it. 

To reduce the model size of the modified ECAPA-TDNN network, we reduce the number of channels in the convolutional frame layers to 128 (C = 128). The final fully-connected layer consists of four nodes, one for each estimated source attribute, with the output then converted into the 8-dimensional vector $\widehat{\mathbf{c}}_{\text{full}}$, containing the probabilities assigned by the model to each attribute value. The number of model parameters is equal to 0.63M. The network is fed the 64-dimensional log Mel spectrogram of the mixture, which is extracted from its STFT computed with a Hann window of size 256 samples and 50\% overlap. We set the lower cutoff frequency to 50 Hz and we apply 2-dimensional batch normalization \cite{ioffe2015batch} after the log Mel spectrogram extraction as in \cite{kong2020panns}.   

\subsection{Separation Model}

For the conditional separation model we use the conditional Sudo rm -rf  \cite{tzinis2022compute} described in \cite{tzinis22_interspeech, tzinis2022optimal}. Specifically, inside each U-ConvBlock, a FiLM \cite{perez2018film} layer is added right after the residual connection. In an effort to reduce model size, we use $B = 8$ U-ConvBlocks and the number of intermediate channels is set to 512. The number of encoder and decoder trainable bases is set to 512, with each filter having a length of 41 taps and a hop size of 20. The total number of trainable parameters is 5.38M.


\subsection{Training and Evaluation Details}

% \subsubsection{Completion}

The completion model is trained using the binary cross entropy loss with a batch size of 6. We use the Adam \cite{adam} optimizer with an initial learning rate equal to $10^{-3}$ which decays to half of its previous value every 40 epochs, along with a weight decay of $2 \cdot 10^{-5}$. Gradient clipping is also performed when the $L_2$-norm is greater than $5.0$. In the experiments involving the ``Easy" (inv. ``Hard") dataset we train the completion model for 50 (inv. 200) epochs. Even though the model has not fully converged at 50 epochs, we show that we can save computation while its performance remains adequate for the downstream task of source separation. 
% In contrast, in the ``hard" dataset we train the model for 200 epochs. 

% \subsubsection{Separation}

All of the separation models are trained with the objective to minimize the negative scale-invariant signal to distortion ratio (SI-SDR) \cite{le2019sdr} defined as $\text{SI-SDR}(\widehat{\mathbf{s}}, \mathbf{s}) = 10 \log_{10} \left({\| \alpha \mathbf{s} \|^2}/{\| \alpha \mathbf{s} - \widehat{\mathbf{s}} \|^2} \right)$
% \begin{equation}
% \label{eq:het}
%     \begin{gathered}
%     \text{SI-SDR}(\widehat{\mathbf{s}}, \mathbf{s}) = 10 \log_{10} \left({\| \alpha \mathbf{s} \|^2}/{\| \alpha \mathbf{s} - \widehat{\mathbf{s}} \|^2} \right),
%     \end{gathered}
% \end{equation}
where $\alpha$ is a scalar equal to $\alpha = \widehat{\mathbf{s}}^{\top} \mathbf{s} / \| \mathbf{s} \|^2$. A batch size of 6 is used, along with the Adam \cite{adam} optimizer with an initial learning rate equal to $10^{-3}$ decaying to half of its previous value every 20 epochs without any weight decay. We also perform gradient clipping when the $L_2$-norm exceeds $5.0$. Models are evaluated after completing 150 training epochs where we empirically observe model convergence. We evaluate the estimated target source $\widehat{\mathbf{s}}_T$ against the ground truth target source ${\mathbf{s}}_T$ using the SI-SDR.