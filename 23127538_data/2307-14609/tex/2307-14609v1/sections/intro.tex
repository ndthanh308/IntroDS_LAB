\section{Introduction}
\label{sec:intro}
Undoubtedly, sound source separation is one of the most fundamental problems in audio scene analysis \cite{wang2006computational}. Neural network training recipes like deep clustering and permutation invariant training (PIT) \cite{hershey2016deepclustering,Isik2016Interspeech09,Yu2017PIT} have achieved remarkable performance in speech, music and universal sound separation tasks by separating all independent source waveforms from an input mixture recording \cite{ZQwang2022tf}. However, slicing an acoustic scene according to the user's intent might also require models which are aware of other discriminative attributes of the mixture's constituent sources in order to specify a target waveform using conditional information \cite{schulze2019weakly}. Earlier works have analyzed the importance of conditionally-informed separation models by estimating class-labels of the mixture's constituent sources \cite{tzinis2020improving,chen2014feature} and using event-based queries for speech enhancement \cite{xin2023improving}. Further, some have explored two-stage approaches, where the stage preceding separation extracts semantic information about the sources \cite{zeghidour2021wavesplit}. Conversely, others have shown that sound event detection systems can benefit from sound separation front-ends \cite{turpault2020improvingSoundEventDetectionusingSeparation}. 

One straightforward way to explicitly condition separation models \cite{meseguer2019conditioned} in order to target a source of interest is to use features related to a speaker's identity \cite{wang2019voicefilter} or to describe sounds with class-based semantics \cite{ochiai20_interspeech}. Other types of condition vectors or queries might also contain text-based descriptions \cite{liu22w_interspeech,kilgour22_interspeech,dong2023clipsep} or visual-cues for on-screen audio-visual separation \cite{tzinis2022audioscopev2}. Although the aforementioned methods have successfully proposed single-condition models, more recent works have also proposed to train conditional separation models by combining different and probably non-mutually exclusive discriminative concepts \cite{tzinis22_interspeech,tzinis2022optimal}. The proposed heterogeneous condition training (HCT) \cite{tzinis22_interspeech} recipe is able to produce a more general multi-condition separation model while sometimes surpassing unconditional separation PIT models by leveraging the complimentarity of information originating from the different condition vectors to describe one target source. On the other hand, the performance of HCT is inferior compared to specialist single-conditioned models when the number and/or the difficulty of the conditions that need to be learned increases. To that end, optimal condition training (OCT) \cite{tzinis2022optimal} has been proposed in order to bridge this separation performance gap by performing gradient steps towards the maximal performing condition vector. As shown in \cite{tzinis2022optimal}, OCT paired with the refinement of the input-user query to a condition vector which is more amenable to a specified single-conditioned task can fine-tune the conditional model towards the target separation task. Nevertheless, OCT \cite{tzinis2022optimal} lacks the generality of HCT \cite{tzinis22_interspeech} while also performing much worse compared to the optimal condition model. 

 % Figure environment removed 

In our work, we focus on how to train multi-conditioned separation systems which are able to perform on-par with dedicated single-conditioned models and how to bridge the performance gap with input-queries that contain the maximal possible description of a target sound source (e.g. for the task of target speech extraction a complete input-query could contain information about the gender, the loudness, the time-order, and the spatial position of the target speaker). Specifically, we explore the scenario where partial utterance-level source attributes are available and we investigate whether estimating missing attributes is beneficial to target speech extraction. We propose a simple yet novel and effective metadata completion mechanism (see Fig.~\ref{fig:twostage}) to estimate the missing parts from an incomplete input-query that describes a target source from the input mixture. At a second step, the estimated completed condition vector is used to train the conditional separation model which is able to access potentially more information associated with the source of interest. Our experiments show that our completion module achieves high accuracy in estimating the missing data for easier and harder datasets. Consequently, our method delivers a much more robust multi-condition separation model scoring on-par with specialist single-conditioned models and close to oracle PIT unconditional methods as well as oracle fully-informed queried models that assume that the user has specified every nuance characteristic of the source of interest.

% Tzinis' seminal work:
% HCT \cite{tzinis22_interspeech} OCT \cite{tzinis2022optimal}

% % Conditional separation:
% Listen to what you want  \cite{ochiai20_interspeech}

% Weakly informed audio source separation \cite{schulze2019weakly}

% CONDITIONED-U-NET: INTRODUCING A CONTROL MECHANISM IN THE U-NET FOR MULTIPLE SOURCE SEPARATIONS \cite{meseguer2019conditioned}

% % Text conditional separation:
% Separate What You Describe: Language-Queried Audio Source Separation \cite{liu22w_interspeech}

% Text-Driven Separation of Arbitrary Sounds \cite{kilgour22_interspeech}

% CLIPSep \cite{dong2023clipsep}

% % Audio visual
% Audioscope v2 \cite{tzinis2022audioscopev2}


% % Sep + Classification

% Improving sound event detection using sound separation \cite{turpault2020improvingSoundEventDetectionusingSeparation}

% Improving universal sound separation using sound classification \cite{tzinis2020improving}

% % Sep + Sound event detection:
% Improving Speech Enhancement via Event-based Query (2023 arxiv) \cite{xin2023improving}

% Zero-shot audio source separation through query-based learning from weakly-labeled data \cite{chen2022zero}

% Source separation with weakly labelled data: An approach to computational auditory scene analysis \cite{kong2020source}





