{
  "title": "Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences",
  "authors": [
    "Scott Sanner",
    "Krisztian Balog",
    "Filip Radlinski",
    "Ben Wedin",
    "Lucas Dixon"
  ],
  "submission_date": "2023-07-26T14:47:15+00:00",
  "revised_dates": [],
  "abstract": "Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.",
  "categories": [
    "cs.IR",
    "cs.LG"
  ],
  "primary_category": "cs.IR",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14225",
  "pdf_url": null,
  "comment": "To appear at RecSys'23",
  "num_versions": null,
  "size_before_bytes": 634340,
  "size_after_bytes": 634786
}