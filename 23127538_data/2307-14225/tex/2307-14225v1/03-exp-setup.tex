\section{Experimental Setup}
\label{sec:expsetup}

To study the relationship between item-based and language-based preferences, and their utility for recommendation, we require a parallel corpus from \emph{the same raters} providing both types of information that is \emph{maximally consistent}. There is a lack of existing parallel corpora of this nature, therefore a key contribution of our work is an experiment design that allows such consistent information to be collected. %
Specifically, we designed a two-phase user study where raters were (1) asked to rate items, \emph{and} to describe their preferences in natural language, then (2) recommendations generated based on both types of preferences were uniformly rated by the raters. Hence we perform our experiments in the movie domain, being frequently used for research as movie recommendation is familiar to numerous user study participants.

A key concern in any parallel corpus of this nature is that people may \emph{say} they like items with particular characteristics, but then consume and positively react to quite different items. For instance, this has been observed where people indicate aspirations (e.g.,~subscribe to particular podcasts) yet actually consume quite different items (e.g.,~listen to others)~\citep{Nazari:2022:WWW}.
In general, it has been observed that intentions (such as intending to choose healthy food) often do not lead to actual behaviors \cite{Verplanken:1999:Good}.
Such disparity between corpora could lead to inaccurate prediction about the utility of particular information for recommendation tasks. As such, one of our key considerations was to maximize consistency.



\subsection{Phase 1: Preference Elicitation}
\label{sec:phase1}
\vspace*{-0.25\baselineskip}

Our preference elicitation design collected natural language descriptions of rater interests both at the start and at the end of a questionnaire.
Specifically, raters were first asked to write short paragraphs describing the sorts of movies they liked, as well as the sorts of movies they disliked (free-form text, minimum 150 characters).  These initial liked (+) and disliked (-) self-descriptions for rater $r$ are respectively denoted as $\mathit{desc}^r_{+}$ and $\mathit{desc}^r_{-}$.

Next, raters were asked to name five example items (here, movies) that they like. This was enabled using an online query auto-completion system (similar to a modern search engine) where the rater could start typing the name of a movie and this was completed to specific (fully illustrated) movies. The auto-completion included the top 10,000 movies ranked according to the number of ratings in the MovieLens 25M dataset~\cite{MovieLens} %
to ensure coverage of even uncommon movies. As raters made choices, these were placed into a list which could then be modified.
Each rater was then asked to repeat this process to select five examples of movies they do not like.  These liked (+) and disliked (-) item selections for rater $r$ and item selection index $j \in \{1,
\ldots,5\}$ are respectively denoted as $\mathit{item}^{r,j}_{+}$ and $\mathit{item}^{r,j}_{\mathit{-}}$.

Finally, raters were shown the five liked movies and asked again to write the short paragraph describing the sorts of movies they liked (which we refer to as the \emph{final description}). The was repeated for the five disliked movies. %





\subsection{Phase 2: Recommendation Feedback Collection}
\label{sec:phase2}
\vspace*{-0.25\baselineskip}

To enable a fair comparison of item-based and language-based recommendation algorithms, a second phase of our user study requested raters to assess the quality of recommendations made by a number of recommender algorithms based on the information collected in Phase 1. In particular, past work has observed that completeness of labels is important to ensure fundamentally different algorithms can be compared reliably \cite{Balog:2019:SIGIR, Kaminskas:2016:Diversity}.

\emph{\textbf{Desiderata for recommender selection:}} We aimed for a mix of item-based, language-based, and unbiased recommendations.  
Hence, we collected user feedback (had they seen it or would they see it, and a 1--5 rating in either case) on a shuffled set of 40 movies (displaying both a thumbnail and a short plot synopsis) drawn from four sample pools:
\begin{itemize}
    \item {\bf SP-RandPop}, an unbiased sample of popular items: 10 randomly selected top popular items (ranked 1-1000 in terms of number of MovieLens ratings);
    \item {\bf SP-RandMidPop}, an unbiased sample of less popular items: 10 randomly selected less popular items (ranked 1001-5000 in terms of number of MovieLens ratings);
    \item {\bf SP-EASE}, personalized item-based recommendations: Top-10 from the strong baseline EASE~\cite{ease} collaborative filtering recommender using hyperparameter $\lambda=5000.0$ tuned on a set of held-out pilot data from 15 users; 
    \item {\bf SP-BM25-Fusion}, personalized language-based recommendations: Top-10 from Sparse Review-based Late Fusion Retrieval that, like \cite{Balog:2021:SIGIR}, computes BM25 match between all item reviews in the Amazon Movie Review corpus (v2)~\cite{zemlyanskiy-etal-2021-docent} and rater's natural language preferences ($\mathit{desc}_+$), ranking items by maximal BM25-scoring review.
     
\end{itemize}
Note that SP-RandPop and SP-RandMidPop have 10 different movies for each rater, and that these are a completely unbiased (as they do not leverage any user information, there can be no preference towards rating items that are more obvious recommendations, or other potential sources of bias).  On the other hand, SP-EASE consists of EASE recommendations (based on the user item preferences), which we also evaluate as a recommender---so there is some bias when using this set. We thus refer to the merged set of SP-RandPop and SP-RandMidPop as an {\bf Unbiased Set} in the analysis, with performance on this set being key to our conclusions. 

\subsection{Design Consequences}
Importantly, to ensure a maximally fair comparison of language-based and item-based approaches, consistency of the two types of preferences was key in our data collection approach. As such, we directly crowd-sourced both types of preferences from raters in sequence, with textual descriptions collected twice---before and after self-selected item ratings. This required control means the amount of data per rater must be small. It is also a realistic amount of preference information that may be required of a recommendation recipient in a near-cold-start conversational setting. As a consequence of the manual effort required, the number of raters recruited also took into consideration the required power of the algorithmic comparison, with a key contributions being to the protocol developed rather than data scale. 

Our approach thus contrasts with alternatives of extracting reviews or preference descriptions in bulk from online content similarly to \cite{Bogers:2017:narrative-driven,mysore2023large} (where preferences do not necessarily capture a person's interests fully) and/or relying on item preferences expressed either explicitly or implicitly over time (during which time preferences may change). %
