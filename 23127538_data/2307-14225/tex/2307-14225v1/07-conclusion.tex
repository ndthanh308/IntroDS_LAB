\section{Conclusion}

In this paper, we collected a dataset containing both item-based and language-based preferences for raters along with their ratings of an independent set of item recommendations.  Leveraging a variety of prompting strategies in large language models (LLMs), this dataset allowed us to fairly and quantitatively compare the efficacy of recommendation from pure item- or language-based preferences as well as their combination.
In our experimental results, we find that zero-shot and few-shot strategies 
in LLMs provide remarkably competitive in recommendation performance for \emph{pure language-based preferences} (no item preferences) in the near cold-start case in comparison to item-based collaborative filtering methods.
In particular, despite being general-purpose, LLMs perform competitively with fully supervised item-based CF methods when leveraging either item-based or language-based preferences. %
Finally, we observe that this LLM-based recommendation approach provides a competitive near cold-start recommender system based on an explainable and scrutable language-based preference representation, thus providing a path forward for effective and novel LLM-based recommenders using language-based preferences.
