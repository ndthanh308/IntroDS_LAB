\section{Related Work}




\emph{\textbf{Item-Based Recommendation.}}
Traditional recommender systems rely on item ratings. For a new user, these can be provided over time as the user interacts with the recommender, although this means initial performance is poor. Thus, preferences are often solicited with a questionnaire for new users \cite{Hu:2013:Interview, Rokach:2012:Initial,Sepliarskaia:2018:Optimized}. There has also been work looking at other forms of item-based preferences such as relative preferences between items \cite{Rokach:2012:Initial,Christakopoulou:2016:Towards}, although approaches that rely on individual item ratings dominate the literature.

Given a corpus of user-item ratings, very many recommendation algorithms exist. These range from methods such as item-based k-Nearest Neighbors \cite{Sarwar:2001:ICF}, where simple similarity to existing users is exploited, to matrix factorization approaches that learn a vector representation for the user \cite{Hu:2008:CFI,Ning:2011:SSL}, through to deep learning and autoencoder approaches that jointly learn user and item vector embeddings \cite{He:2017:Neural,Liang:2018:Variational,Huiyuan:2022:Denoising}. Interestingly, the EASE algorithm \cite{ease} is an autoencoder approach that has been found to perform on par with much more complex state-of-the-art approaches.


\emph{\textbf{Natural Language in Recommendation.}}
Following the proposals in \cite{Balog:2019:SIGIR,Radlinski:2022:SIGIR} to model preferences solely in scrutable natural language, recent work has explored the use of tags as surrogates for NL descriptions with promising results~\cite{mysore:2023:editable}.
This contrasts with, for instance \citet{hou2022towards}, who input a (sequence) of 
natural language item descriptions into an LLM to produce an (inscrutable) user representation for recommendation.
Other recent work has sought to use rich, descriptive natural language as the basis for recommendations.  At one extreme, we have narrative-driven recommendations~\citep{Bogers:2017:narrative-driven} that assume very verbose descriptions of specific contextual needs.  In a similar vein, user-studies of NL use in recommendation~\citep{Kang:2017:RecSys} identify a rich taxonomy of recommendation intents and also note that speech-based elicitation is generally more verbose and descriptive than text-based elicitation.
In this work, however, we return to the proposal in \cite{Radlinski:2022:SIGIR} and assume the user provides a more general-purpose language-based description of their preferences and dispreferences for the purpose of recommendation.

Recently, researchers have begun exploring use of language models (LMs) for recommendation tasks \cite{friedman2023leveraging}. %
\citet{Radlinski:2022:SIGIR} present a theoretical motivation for why LLMs may be useful for recommendations and provide an example prompt, but do not conduct any quantitative evaluation.
\citet{mysore2023large} generate preference narratives from ratings and reviews, using the narratives to recommend from held-out items.  \citet{penha2020does} show that off-the-shelf pretrained BERT~\cite{BERT} contains both collaborative- and content-based knowledge about items to recommend.  
They also demonstrate that BERT outperforms information retrieval (IR) baselines for recommendation from language-based descriptions.  However, they do not assess the relative performance of language- vs. item-based recommendation from LMs (for which we curate a dataset specifically for this purpose), nor does BERT's encoder-only LM easily permit doing this in a unified prompting framework that we explore here.
RecoBERT~\cite{malkiel2020recobert} leverages a custom-trained LM for deriving the similarity between text-based item and description pairs, with the authors finding that this outperforms traditional IR methods.   
\citet{hou2023large} focus on item-based recommendation, with an in-context learning (ICL) approach similar in spirit to our item-only few-shot approach. 
Similarly, \citet{kang2023llms} use an LLM to predict ratings of target items.
Finally, ReXPlug~\cite{hada2021rexplug} exploits pretrained LMs to produce explainable recommendations by generating synthetic reviews on behalf of the user.  None of these works, however, explore \emph{prompting strategies} in large LMs to \emph{translate actual natural language preferences into new recommendations} compared directly to item-based approaches. 

Further, we are unaware of any datasets that capture a user's detailed preferences in natural language, and attempt to rate recommendations on unseen items.
Existing datasets such as \cite{cpcd,Balog:2019:SIGIR} tend to rely on much simpler characterizations.

\emph{\textbf{Prompting in Large Language Models.}}
Large language models (LLMs) are an expanding area of research with numerous exciting applications. Beyond traditional natural language understanding tasks like summarization, relation mapping, or question answering, LLMs have also proved adept at many tasks such as generating code, generating synthetic data, and multi-lingual tasks \citep{austin2021program, borisov2023language, chowdhery2022palm}. How to prompt these models to generate the best results is a continuing topic of research. Early prompting approaches relied on few-shot prompting, where a small set of training input-output pairs are prepended to the actual input \citep{brown2020language}. Through additional tuning of pre-trained models on tasks described via instructions, LLMs also achieve impressive performance in the zero-shot setting (i.e., models are given a task and inputs, without any previous training examples)~\citep{wei2022finetuned}. %
\citet{p5} test a variety of prompting techniques with a relatively small (less than one billion parameter) LLM trained on a collection of recommendation tasks, finding promising results across multiple tasks and domains, primarily by using item ratings as input. 


