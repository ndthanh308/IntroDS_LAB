\documentclass[manuscript]{acmart}

\copyrightyear{2023}
\acmYear{2023}
\setcopyright{rightsretained}
\acmConference[RecSys '23]{Seventeenth ACM Conference on Recommender Systems}{September 18--22, 2023}{Singapore, Singapore}
\acmBooktitle{Seventeenth ACM Conference on Recommender Systems (RecSys '23), September 18--22, 2023, Singapore, Singapore}\acmDOI{10.1145/3604915.3608845}
\acmISBN{979-8-4007-0241-9/23/09}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003350</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}

\keywords{recommendation; transparency; scrutability; natural language}

\usepackage{booktabs} 
\usepackage{color}
\usepackage{arydshln} %

\newcommand{\centeredcell}[1]{\begin{tabular}{l} #1 \end{tabular}}


\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}


\acmSubmissionID{582}

\begin{document}

\title[LLMs are Competitive Near Cold-start Recommenders]{Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences}

\author{Scott Sanner}
\email{ssanner@mie.utoronto.ca}
\affiliation{%
  \institution{University of Toronto}
  \city{Toronto}
  \country{Canada}
}
\authornote{Work done while on sabbatical at Google.}
\author{Krisztian Balog}
\email{krisztianb@google.com}
\affiliation{%
  \institution{Google}
  \city{Stavanger}
  \country{Norway}
}
\author{Filip Radlinski}
\email{filiprad@google.com}
\affiliation{%
  \institution{Google}
  \city{London}
  \country{United Kingdom}
}
\author{Ben Wedin}
\email{wedin@google.com}
\affiliation{%
  \institution{Google}
  \city{Cambridge, MA}
  \country{United States}
}
\author{Lucas Dixon}
\email{ldixon@google.com}
\affiliation{%
  \institution{Google}
  \city{Paris}
  \country{France}
}




\begin{abstract}
Traditional recommender systems leverage users' item preference history to recommend novel content that users may like.  However, modern 
dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input.  Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods.
To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items.  
Among numerous experimental results, we find that %
LLMs provide competitive recommendation performance for \emph{pure language-based preferences} (no item preferences) in the near cold-start case 
in comparison to item-based CF methods,
despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). %
This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.




\end{abstract}


\maketitle

\input{01-introduction}
\input{02-related-work}
\input{03-exp-setup} %
\input{04-methods}
\input{05-results}
\input{06-ethics}
\input{07-conclusion}


\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
