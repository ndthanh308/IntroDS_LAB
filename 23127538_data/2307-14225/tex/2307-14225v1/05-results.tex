\section{Results}
\label{sec:results}

\subsection{Data Analysis}

\label{sec:times}


We now briefly analyze the data collected from 153 raters as part of the preference elicitation and rating process.\footnote{We recruited 160 raters, but discard those (5) that did not complete both phases of the data collection and those (2) who provided uniform ratings on all item recommendations in Phase 2.}
The raters took a median of 67 seconds to write their initial descriptions summarizing what they like, and 38 seconds for their dislikes (median lengths: 241 and 223 characters, respectively). Providing five liked and disliked items took a median of 174 and 175 seconds, respectively. 
Following this, writing final descriptions of likes and dislikes took a median of 152 and 161 seconds, respectively (median lengths: 205 and 207 characters, respectively). 
We observe that the initial descriptions were produced 3 to 4 times faster than providing 5 example items, in around one minute. 
As we will see below, this difference in effort is particularly pertinent as item-based and description-based recommendation are comparable in performance. A sample of initial descriptions are shown in Table \ref{tab:examples}.



\begin{table}[t]
    \centering
    \caption{Example initial self-descriptions provided by three raters.}
    \vspace*{-0.5\baselineskip}
    \footnotesize
    \label{tab:examples}
    \begin{tabular}{p{0.4cm}|p{7.8cm}|p{6cm}|}%
    \!\!Rater\!\! & \multicolumn{1}{c|}{Liked Movies} & \multicolumn{1}{c|}{Disliked Movies} 
\\
    \hline
    \#1
      & I like comedy movies because i feel happy whenever i watch them. We can watch those movies with a group of people. I like to watch comedy movies because there will be a lot of fun and entertainment. Its very exciting to watch with friends and family.so,I always watch comedy movies whenever I get time. 
      & I am not at all interested in watching horror movies because whenever I feel alone it will always disturb me with the characters in the movie. It will be affected by dreams and mood always. SO, mostly i ignore watching them when i stay alone in the home.
      \\
    \hline
    \#2
      & Fantasy films often have an element of magic, myth, wonder,and the extraordinary. They may appeal to both children and adults, depending upon the particular film. In fantasy films, the hero often undergoes some kind of mystical experience.
      & Horror is scary. I don't like the feeling of being terrified. Some are either sensitive to suspense, gore or frightful images, or they may have had an experience in their life that makes horror seem real.
      \\
    \hline
    \#3
      & I like comedy genre movies, while watching comedy movies I will feel very happy and relaxed. Comedy films are designed to make the audience laugh. It has different kinds of categories in comedy genres such as horror comedy, romantic comedy, comedy thriller,musical-comedy. 
      & I dislike action genre movies because watching fights gives me a headache and bored me. These kinds of movies mainly concentrate on violence and physical feats.
      \\
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \caption{Baseline rating statistics for items in the fully labeled pools of items across all raters.}
    \label{tab:pool_ratings}
    \vspace*{-0.5\baselineskip}
    \begin{tabular}{l|c|c|c:c|}
                 & Movies & Fraction & \multicolumn{2}{c|}{Average Rating}\\
Sample Pool      & Per Rater   &  Seen    & Seen Movies & Unseen Movies \\
\hline
SP-RandPop        & 10 & 22\% & 4.21 & 2.93 \\
SP-RandMidPop     & 10 & 16\% & 4.00 & 2.85 \\
SP-EASE             & 10 & 46\% & 4.51 & 3.16 \\
SP-BM25-Fusion      & 10 & 24\% & 4.38 & 3.11 \\
\hline
SP-Full             & 40 &  27\% & 4.29 & 3.00 \\ %
    \end{tabular}
\end{table}

Next, we analyze the ratings collected for the movies from the four pools described in Section~\ref{sec:expsetup}.  From Table~\ref{tab:pool_ratings}, we observe: (1) The EASE recommender nearly doubles the rate of recommendations that have already been seen by the rater, which reflects the supervised data on which it is trained where raters only rate what they have seen; (2) There is an inherent positive bias to provide a high ratings for movies the rater has already seen as evidenced by the average 4.29 rating in this case; (3) In contrast, the average rating drops to a neutral 3.00 for unseen items.

\subsection{Recommended Items}


\begin{table}[t]
    \centering
    \caption{Main experimental results comparing mean NDCG@10 ($\pm$ 95\% standard error) over raters for all recommendation methods. In each case, the fully judged rater-specific evaluation set is ranked by the given recommendation algorithms. Mean evaluation set sizes are in the first row. Note that performance on the \emph{Unseen} item set is most important in a practical recommendation setting.}
    \label{tab:main_results}
    \vspace*{-0.5\baselineskip}
    \begin{tabular}{l|c|c|c:c|}
               & Full Set & Unbiased Set & \multicolumn{2}{c|}{Items that are} \\
\multicolumn{1}{r|}{Evaluation Set} & SP-Full  & SP-Rand\{Pop,MidPop\} %
& Seen & {\bf Unseen} \\
\hline
\emph{Mean evaluation set size} & \emph{40} & \emph{20} %
& \emph{10.8} & \emph{29.2} \\
\hline
\hline
\multicolumn{5}{l}{Recommendation Algorithm} \\
\hline

Random Baseline	&	0.504 $\pm$ 0.032	&	0.532 $\pm$ 0.034	&	0.876 $\pm$ 0.023	&	0.511 $\pm$ 0.038	\\
Popularity Baseline	&	0.595 $\pm$ 0.032	&	0.624 $\pm$ 0.029	&	0.894 $\pm$ 0.020	&	0.534 $\pm$ 0.036	\\ \hdashline
(Item) EASE	&	{\bf 0.673 $\pm$ 0.038}	&	0.592 $\pm$ 0.030	&	0.899 $\pm$ 0.023	&	0.559 $\pm$ 0.039	\\
(Item) WRMF	&	0.644 $\pm$ 0.036	&	0.644 $\pm$ 0.029	&	0.897 $\pm$ 0.021	&	0.573 $\pm$ 0.037	\\
(Item) BPR-SLIM	&	0.672 $\pm$ 0.037	&	0.617 $\pm$ 0.029	&	{\bf 0.902 $\pm$ 0.021}	&	0.577 $\pm$ 0.037	\\
(Item) KNN Item	&	0.646 $\pm$ 0.038	&	0.610 $\pm$ 0.028	&	0.889 $\pm$ 0.024	&	0.565 $\pm$ 0.037	\\ 
(Language) BM25-Fusion	&	0.519 $\pm$ 0.032	&	0.623 $\pm$ 0.027	&	0.868 $\pm$ 0.023	&	0.542 $\pm$ 0.036	\\ \hdashline
LLM Item Completion	&	0.649 $\pm$ 0.037	&	0.610 $\pm$ 0.027	&	0.889 $\pm$ 0.022	&	0.563 $\pm$ 0.037	\\
LLM Item Zero-shot	&	0.659 $\pm$ 0.037	&	0.631 $\pm$ 0.028	&	0.895 $\pm$ 0.023	&	0.571 $\pm$ 0.037	\\
LLM Item Few-shot (3)	&	0.664 $\pm$ 0.038	&	0.636 $\pm$ 0.027	&	0.897 $\pm$ 0.022	&	0.572 $\pm$ 0.037	\\ \hdashline
LLM Language Completion	&	0.617 $\pm$ 0.032	&	0.617 $\pm$ 0.029	&	0.889 $\pm$ 0.023	&	0.559 $\pm$ 0.035	\\
LLM Language Zero-shot	&	0.612 $\pm$ 0.034	&	0.626 $\pm$ 0.027	&	0.885 $\pm$ 0.024	&	0.563 $\pm$ 0.034	\\
LLM Language Few-shot (3)	&	0.640 $\pm$ 0.036	&	{\bf 0.650 $\pm$ 0.026}	&	0.891 $\pm$ 0.022	&	0.571 $\pm$ 0.038	\\ \hdashline
LLM Item+Language Completion	&	0.654 $\pm$ 0.037	&	0.639 $\pm$ 0.027	&	0.893 $\pm$ 0.022	&	0.568 $\pm$ 0.037	\\
LLM Item+Language Zero-shot	&	0.660 $\pm$ 0.038	&	0.634 $\pm$ 0.028	&	0.897 $\pm$ 0.023	&	{\bf 0.582 $\pm$ 0.037}	\\
LLM Item+Language Few-shot (3)	&	0.663 $\pm$ 0.038	&	0.640 $\pm$ 0.028	&	0.899 $\pm$ 0.022	&	0.570 $\pm$ 0.037	\\ \hdashline
LLM Item Zero-shot Pos+Neg	&	0.647 $\pm$ 0.037	&	0.629 $\pm$ 0.027	&	0.892 $\pm$ 0.023	&	0.569 $\pm$ 0.038	\\
LLM Language Zero-shot Pos+Neg	&	0.612 $\pm$ 0.034	&	0.626 $\pm$ 0.027	&	0.885 $\pm$ 0.024	&	0.563 $\pm$ 0.034	\\
LLM Item+Language Zero-shot Pos+Neg	&	0.662 $\pm$ 0.037	&	0.626 $\pm$ 0.028	&	0.897 $\pm$ 0.023	&	0.577 $\pm$ 0.037	\\
\hline

\hline
    \end{tabular}
\end{table}

Our main experimental results are shown in Table~\ref{tab:main_results}, using NDCG@10 with exponential gain (a gain of 0 for ratings $s<3$ and a gain of $2^{s-3}$ otherwise). We compare the mean performance of various methods using item- and/or language-based preferences (as described in Section~\ref{sec:phase1}) ranking four different pool-based subsets of the 40 fully judged test recommendation items (as described in Section~\ref{sec:phase2}), recalling that the pool for each rater is personalized to that rater. The language-based results use only the initial natural language descriptions, which raters produced much faster than either liked and disliked item choices or final descriptions, yet they yield equal performance to final descriptions.

We begin with general observations.  First, we note the range of NDCG@10 scores within each subset of items is substantially different, due to both the NDCG normalizer that generally increases with a larger evaluation set size, as well as the average rating of each pool.  On the latter note, we previously observed that the subset of {\bf Seen} recommendations in Table~\ref{tab:pool_ratings} has the smallest pool of items and a high positive rating bias that makes it hard to differentiate recommenders on this subset.  However, and as also recently argued in \cite{Pellegrini:2022:Dont}, in a recommendation setting where an item is typically only consumed once (such as movies), we are much more concerned about recommendation performance on the {\bf Unseen} subset vs. the {\bf Seen} subset.  Similarly, we are also concerned with performance on the {\bf Unbiased} set since this subset explores a wide range of popularity and is not biased towards item-based collaborative filtering (CF) methods.


To address our original research questions from Section~\ref{sec:intro}:

{\bf RQ1: Can language-based preferences replace or improve on item-based preferences?}  An initial affirmative answer comes from observing that the LLM Language Few-shot (3) method is competitive with most of the traditional item-based CF methods in this near cold-start setting, which is important since as observed in Section~\ref{sec:times}, language-based preferences took less time to elicit than item-based preferences; furthermore, language-based preferences are transparent and scrutable~\cite{Radlinski:2022:SIGIR}.
    However, there seems to be little benefit to combining language- and item-based preferences as the Item+Language LLM methods do not appear to provide a boost in performance.
    
{\bf RQ2: LLM-based methods vs. CF?} RQ1 has already established that LLM-based methods are generally competitive with item-based CF methods for the Language variants of the LLMs.  However, it should also be noted that in many cases the LLM-based methods can even perform comparatively well to CF methods with only Item-based preferences (i.e., the names of the preferred movies).  A critical and surprising result here is that a pretrained LLM makes a competitive recommender without the large amounts of supervised data used to train CF methods.

{\bf RQ3: Best prompting methodology?} The Few-shot (3) prompting method generally outperforms Zero-shot and Completion prompting methods. 
The difference between Zero-shot and Completion prompting is less pronounced. 
While not shown due to space constraints, increasing the number of Few-shot examples did not improve performance.

{\bf RQ4: Does inclusion of dispreferences help?} In the bottom three rows of Table~\ref{tab:main_results}, we show the impact of including negative item or language preferences for LLM-based recommenders.  There are no meaningful improvements from including both positive and negative preferences (Pos+Neg) over only positive preferences in these LLM configurations.  While not shown due to space constraints, omitting positive preferences and using only negative preferences yields performance at or below the popularity baseline.%



