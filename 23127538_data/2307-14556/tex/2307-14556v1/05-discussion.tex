\section{Discussion}
\label{disc}
The results have shown that it is possible to improve the results of an existing
fuzzer by applying a generative deep learning model and a reinforcement learning
model to the fuzzer.

% Figure environment removed

The distribution of chosen actions by the DDQN agents gives interesting insights
into the performance of the model and is highlighted in \Cref{disc:distribution}.
For instance, for the $\mathcal{C}2$ agents, it seems like the low performing
models got stuck with a policy that predicts simple render HTML tags over and
over again whereas the well performing models all have the "input" tag in their
top ten of actions taken. In contrast to that the $\mathcal{C}4$ agents achieve
a similar performance with a more balanced tag distribution. Nonetheless, the low
performing models also show an unbalanced policy with a tendency to 'render only'
tags, like 'a'. Furthermore, computing the 'Kullback Leibler Divergence'\cite{kullback1951information} between the
distributions shows that the well performing action policies have a smaller distance
to each other than to the bad performing ones. For example, the distance between
the training runs six and seven of the $\mathcal{C}2$ models have a distance of
$\approx 2.3211$, whereas the distance between training run six and eight is 
$\approx 9.9995$. The training runs six, seven and eight had a total code coverage
performance of $55,594$, $54,874$ and $36,678$ basic blocks respectively.
The difference in policies is also highlighted by \Cref{disc:distribution} where
the policies of runs two to seven build a similarity block and policy eight is
clearly separated from that block. 

The rewards returned by the VMs indicate a high instability in the training process.
They potentially vary significantly in a few training steps. This effect explains 
why a smaller learning rate worked better during all training runs,
especially with growing model complexity. The initial data collection for the DDQN
agent also indicated that it is beneficial to reuse the TCN embedding weights in
the DDQN agent and disable training of the embedding.



% \begin{enumerate}
%     \item improved results
%     \item check distribution of tags
%     \item high fluctuation in vm rewards might be linked to why a small learning rate works better
% \end{enumerate}