\section{Evaluation}
\label{res}

% In this section we present the results of the experiments. We provide
% answers to the following research questions:
% \begin{enumerate}
%     \item Can a TCN augment the code coverage performance of an existing grammar based fuzzer?
%     \item Does our combined approach consisting of a DDQN and a TCN improve the code coverage compared to the TCN alone?
%     \item Can our method improve the code coverage compared to the underlying grammar based fuzzer?
% \end{enumerate}

\subsection{TCN generator}



The training phase showed a stable validation loss over all configurations
which is supported by a low standard deviation, as highlighted in \Cref{res:fig_tcn_training}. 
The results suggest that a larger kernel size and less layers lead to a lower validation loss.
The TCN configuration number seven (see \Cref{exp:tcn_param}) achieved the lowest validation loss
and standard deviation of $0.385$ and $0.014$ respectively.

% Figure environment removed

The TCN models' code coverage performance was very stable across different configurations, as shown in \Cref{res:fig_tcn_perf}. Furthermore, all trained models were able to achieve a performance similar to the dataset except for 
a few outliers. \Cref{res:fig_tcn_perf} also highlights that the
model configurations that utilize more units in the dense layers were able to perform
better on average. The single best performing TCN model used configuration
six and achieved $53,580$ unique basic blocks, which is only $242$
basic blocks short of the best baseline result. This is a very positive result
and on the same level as the baseline. Furthermore, there are differences in
the actual basic blocks as \Cref{res:fig_tcn_perf}. The difference emphasizes
that the model was actually able to trigger basic blocks that were not triggered
by the best performing baseline set. We chose this model as a generator model for the DDQN agent.



\subsection{DDQN agent}

The training phase of the DDQN agent already showed that it is important to take frequent
snapshots of the model's state and test the performance regularly to ensure 
a good performing configuration is recognized. The average reward for the test
cases fluctuated strongly in between training steps. Furthermore, this also
provides a reason why the lower learning rates perform better on average than
higher learning rates, because the higher learning rates diverge quicker from
well performing states.

% Figure environment removed

We conducted a hyper-parameter search in order to find 
parameters that provide stable and promising results.
The hyper-parameter search provided two promising candidates for
further testing, namely configuration two and four of 
\Cref{exp:tbl_parameters}. The overall results of the hyper-parameter
search can be seen in \Cref{res:fig_hyp_results}. The configuration two 
and four models have the highest on average code coverage performance,
and both also performed best with the learning rate set to $0.000645$.

% Figure environment removed

\Cref{res:fig_perf} (left) highlights the overall results of the DDQN agents compared
to the baseline results. In total, three configuration two ($\mathcal{C}2$) models were able to outperform the best baseline set
in terms of basic blocks. The best performing DDQN agent achieved $57,993$ 
basic blocks in total that is a 7.7\% performance increase over the best baseline set.
Furthermore, the figure also shows that the majority of $\mathcal{C}2$
DDQN agents either performed close to the maximum data set performance or 
above it. Only three of the fifteen $\mathcal{C}2$ DDQN models performed below the
minimum data set code coverage. The configuration four ($\mathcal{C}4$) models were not able
to outperform the best baseline and achieved a maximum of $52,614$ basic blocks
that is $2.2\%$ below the maximum baseline performance. In total, six out
of the fifteen $\mathcal{C}4$ models were not able to perform above 
the minimum data set performance.

% % Figure environment removed

In comparison to the baseline the $\mathcal{C}2$ DDQN agents were able to
discover between $3,349$ and $10,206$ unique basic blocks as highlighted by
\Cref{res:fig_perf} (right). Furthermore, the $\mathcal{C}4$ DDQN agents
discovered between $3,859$ and $6,435$ unique basic blocks. In both 
configurations the DDQN agents were able to improve the overall code coverage by
at least 6\% and in the best case by 18.9\%. 

% Figure environment removed

The number of unique basic blocks compared to the underlying TCN generator
unsurprisingly is lower than compared to the baseline as seen in \Cref{res:fig_dif_tcn}.
The $\mathcal{C}2$ DDQN agents achieved $1,573$ to $6,197$ unique basic blocks
and the $\mathcal{C}4$ DDQN agents ranged between $1,886$ and $4,092$ 
This results in an improvement rate of 2.9\% and 11.5\% for worst and best case
respectively.