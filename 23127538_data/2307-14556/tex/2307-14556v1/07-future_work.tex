\section{Future Work}
\label{fut}
We have demonstrated the advantage of an reinforcement learning-based fuzzer in a specific context, namely HTML and a specific rendering engine. Limiting the test cases to only contain HTML places an upper bound on code coverage of the complete rendering engine. Furthermore, the code coverage guidance by the DDQN is based upon the average achieved code coverage over the evaluation test cases. This does not take into account repeated execution of the same code
areas.

An obvious way to improve the DDQN-based fuzzer would be to base the reward on a more
complex metric. This metric could potentially take into account the repeated execution
of code areas and reduce the reward accordingly over time. In addition,
it would lead to an online training setting where the DDQN is able to adjust
the policy in the real fuzzing environment. So, the DDQN agent would not
be trained upfront but placed into the environment and trained based on
the immediate feedback over time with an adapting reward. This would
punish the DDQN agent with bad rewards when it gets stuck in certain
code areas.

Furthermore, we have opted to break down the system in two parts. An obvious extension would be to combine the TCN generator and DDQN agent into one system that is trainable as a single entity. At the moment the DDQN agent's action set is restricted to deciding which HTML-tags should inserted. This reduces the room the agent has to explore considerably. A combined system could be pre-trained on a specific file format and then
used a code-coverage guided grammar based generator. The combined system
could learn decisions for every position in the generated input and be an online
agent to counter the limitations of stopping the training described earlier.

% perhaps a broader discussion about metrics/rewards, e.g. using other metrics than due code coverage such as executing tree, complexity of probed paths etc
% perhaps a discussion about measuring saved time... we have argued this but not measured it

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The most important future development is to improve the feedback loop and
% combine the generator model and DDQN agent into a single system that can
% be trained continuously. This would change the fuzzing workflow to train
% a generator model on publicly available data and then continue the
% training in the reinforcement learning setting. The DDQN agent could then
% decide at every time step which byte value needs to be inserted based on
% the feedback the program provides. Furthermore, a reinforcement learning
% based fuzzer should be able to continuously receive program feedback to
% be trained in an online setting. For example, rediscovering the same code
% paths should be penalized with a negative reward, and new code paths should
% provide a higher one.

Another approach that needs to be explored is the potential of transferring
the provided results to other file formats and programming languages. It
might be valuable to combine recent developments in program synthesis, 
Austin et al.\cite{austin2021program} with fuzzing to improve JavaScript
fuzzing, for example.


% \begin{enumerate}
%     \item DDQN improvements (make the DDQN the generator) Perhaps even use the TCN a DDQN agent?
%     \item Extend to other file formats
%     \item Create an online continuous learning environment
%     \item Combine with results from program synthesis research to get to the next level with JS
%     \item 
% \end{enumerate}