\section{Background}
\label{back}
In this section, we start with an introduction to the basic concepts of fuzzing.
This is followed by a review of the concepts behind Temporal Convolutional Networks (TCN) as well
as Double Deep Q-Networks (DDQN) which were used for the experiments.

\subsection{Fuzzing}
Fuzz testing is a software testing approach where the program or function under test
receives synthetic input data that might be malformed. The intention is to trigger code paths
that lead to unintended behavior, like memory corruptions or abrupt program termination.
As mentioned in the introduction, there are two main approaches to fuzz testing one is
to mutate data from an input corpus, and the other is to generate input data programmatically
from scratch. The American Fuzzy Lop (AFL)\cite{afl} is a prominent example of a general-purpose
mutation-based fuzzer because of its performance in uncovering bugs. For generation-based
fuzzing, there is not one particularly good example since these tools are purpose-built
to cover a given input data structure. However, they have the advantage that they have
the input specification hard-wired and can focus on the variable parts of the input
structure. No matter which approach is used, fuzzers need to be configured and fine-tuned
to avoid them getting stuck in a specific code area.

\subsection{Temporal Convolutional Networks}
The concept of a Temporal Convolutional Network (TCN) for sequence modeling was
introduced by Bai et al.\ \cite{bai2018empirical}. The idea is to modify a default
conventional layer as defined by LeCun et al.\cite{lecun1989backpropagation}
with four additional concepts into one layer. First, Bai et al. utilized
one dimensional fully connected convolution from Long et al.\cite{long2015fully}.
This adds padding to the layer's input and makes sure that the input and the output
have the same length. Secondly, causal convolution was added. It ensures that
the model cannot connect at time step $t$ with time steps $t+1$. So, the model
cannot take information from the future into account while computing the present
time step. Thirdly, dilated convolution provides control about the past time steps
that are taken into account while at time step $t$. The effect can be further
amplified by stacking multiple layers with different dilatation rates.
Finally, the use of residual blocks that are basically a filtered shortcut
connection between a block's input and output enables the input data to influence
the output directly.
The advantage of TCNs compared to Recurrent Neural Networks (RNNs) is TCNs can
compute the input in parallel because there is no need to compute time step $t-1$
before computing time step $t$. This allows faster computation and quicker training.
However, the maximum input length is restricted by the available memory, which is
not the case with RNNs. The input length restricts the past time steps the model
has available to compute the next step in the sequence.

The advantages of TCNs over RNNs in this setting are the faster training times
and the on-par performance with RNNs as Bai et al.\cite{bai2018empirical}
have shown. Furthermore, the limitation of the maximum input length does not
tamper with the test case generation in the HTML setting since it is possible
to adjust the sequence length accordingly to at least 12000 characters.

\subsection{Double Deep Q-Networks}
In general, reinforcement learning describes a group of learning algorithms
that receive the state of environment as input and provides an output that
is intended to take the action that leads to the maximum future reward. Deep
reinforcement learning is the subgroup that uses neural networks to make the
prediction. Watkins et al.\cite{watkins1992q} introduced the iterative Q-learning
algorithm. It provides a reinforcement learning agent with the capability
to learn how to maximize the future reward based on the actual state and
an action the agent takes. Mnih et al.\cite{mnih2013playing} introduced
a method using deep neural networks to estimate the Q-function called 
Deep-Q-Network (DQN). They demonstrated
the performance of their new method by playing ATARI games. They also applied
experience replay introduced by Lin et al.\cite{lin1993reinforcement}. 

Hasselt et al.\cite{van2016deep} introduced Double Deep Q-Networks (DDQN) as
an extension to DQNs. The basic idea is to compute a so-called
Q-value for each (state, action)-pair. The Q-value estimates the future average reward of
taking action $a$ in state $s$. The output of the DDQN provides a Q-value
for each action available to the network as a vector. The next action is then
determined by selecting the element with the highest Q-value. Similar to DQNs, 
DDQNs utilize two networks: the target and the online network. The target network periodically
receives the weights of the online network and is used to evaluate the value
of a chosen action in order to compute the loss. This leads to a reduction
of the value overestimation happening in DQNs, and Double Q Networks as highlighted by Haselt et al.\cite{vanhasselt2015deep}.  

To improve the training, an experience replay memory is used. It contains
the actual state $s_t$, the next state $s_{t+1}$, the reward $r_t$ and the action
$a_t$ taken. The elements from the replay memory are chosen based on their priority,
where new memories get the highest priority. After an experience was used for training,
the new priority is based on the computation error as described by Schaul et al.\cite{schaul2015prioritized}.

The main motivation to employ a DDQN is the demonstrated performance with large
input spaces in the past. For example, Mnih et al.\cite{mnih2015human}
achieved superhuman performance with the less optimized DQN agent in
playing ATARI games. The model used the four most recent frames with 84x84 pixels
as input.
