\section{Proofs in Section \ref{sec: rsgda}}
\label{app: rsgda}

\subsection{Proofs in Section \ref{sec: 4.2}}

\begin{proof}[\textbf{Proof of Theorem \ref{thm: rgda is contractive}}]

For ease of notation, we define the operator $H$ as follows: 
\begin{equation}
    H (x, y) := 
    \begin{dcases}
        (x - \alpha \nabla_x F(x, y), y), & \quad \text{w.p.} \quad p, \\ 
        (x, y + \alpha \nabla_y F(x, y)), & \quad \text{w.p.} \quad 1 - p.
    \end{dcases}
\end{equation}
We also define 
\begin{equation}
    u = \begin{pmatrix}
    x \\ 
    y
    \end{pmatrix}, \quad 
    G(u) = \begin{pmatrix}
    \alpha \nabla_x F(x, y) \\
    - \alpha \nabla_y F(x, y)
    \end{pmatrix}.
\end{equation}
For simplicity, we denote $G(u_k)$ by $G_k$. 
Since $F$ is $\mu$-SCSC, we have 
\begin{equation}
    \frac{1}{\alpha} \langle G(u) - G(\tilde{u}), u - \tilde{u} \rangle \geq \mu \|  u - \tilde{u} \|^2, \quad \forall u, \tilde{u}.
\end{equation}
Particularly, if $u^*$ is the saddle point of $F$, we have
\begin{equation} \label{equ: a6}
    \langle G(u) , u - u^* \rangle \geq \alpha \mu \|  u - u^* \|^2, \quad \forall u.
\end{equation}

Recall that $\e_k[\cdot]$ is the conditional expectation with respect to the filter $\mathcal{F}_k = \{ x_0, x_1, \dots, x_k \}$, thus we have 
\begin{equation} \label{equ: a1}
\begin{aligned}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] &= p \| (x_k - \alpha \nabla_x F(x_k, y_k), y_k) - (x^*, y^*) \|^2 \\
    & \quad + (1 - p) \| (x_k, y_k + \alpha \nabla_y F(x_k, y_k)) - (x^*, y^*) \|^2.
\end{aligned}
\end{equation}
Now, we estimate the two terms separately. 
For the first term, we have
\begin{equation} \label{equ: a2}
\begin{aligned}
    \| (x_k - \alpha \nabla_x F(x_k, y_k), y_k) - (x^*, y^*) \|^2 & = \| x_k - \alpha \nabla_x F(x_k, y_k) - x^* \|^2 + \| y_k - y^* \|^2 \\ 
    & = \| x_k - x^* \|^2 + \| y_k - y^* \|^2 \\ & \quad +  \alpha^2 \| \nabla_x F(x_k, y_k) \|^2 - 2 \alpha \langle x_k - x^*, \nabla_x F(x_k, y_k) \rangle.
\end{aligned}
\end{equation}
Similarly, for the second term, we can obtain that 
\begin{equation} \label{equ: a3}
\begin{aligned}
     \| (x_k, y_k + \alpha \nabla_y F(x_k, y_k)) - (x^*, y^*) \|^2 &= \| x_k - x^* \|^2 + \| y_k - y^* \|^2 \\
     & \quad + \alpha^2 \| \nabla_y F(x_k, y_k) \|^2 + 2 \alpha \langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle.
\end{aligned}
\end{equation}
Putting the above three equations, \eqref{equ: a1}, \eqref{equ: a2} and \eqref{equ: a3} together, we have 
\begin{equation} \label{equ: a4}
\begin{aligned}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] = & \| (x_k, y_k) - (x^*, y^*) \|^2 + \alpha^2 p \| \nabla_x F(x_k, y_k) \|^2 + \alpha^2 (1-p) \| \nabla_y F(x_k, y_k) \|^2 \\
    & - 2 \alpha \left( p \langle x_k - x^*, \nabla_x F(x_k, y_k) \rangle - (1-p) \langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle \right).
\end{aligned}
\end{equation}
Without loss of generality, we assume that $p \leq 1 / 2$ \footnote{Note that $x$ and $y$ are symmetric in this algorithm, which means that if $p \geq 1/2$, one can exchange the notations $x$ and $y$ and define a new probability parameter $\tilde{p}:= 1 - p$ to complete the proof.} Now we analyze the equality~\eqref{equ: a4} under two cases. 
\begin{itemize}
    \item \underline{Case 1: $\langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle \geq 0$.} By rearranging (\ref{equ: a4}), we have 
\begin{equation} 
\begin{aligned}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] = & \| u_k - u^* \|^2 + \alpha^2 (p \| \nabla_x F(x_k, y_k) \|^2 + (1-p) \| \nabla_y F(x_k, y_k) \|^2) \\
    & - 2 \alpha \left( p \langle x_k - x^*, \nabla_x F(x_k, y_k) \rangle - (1-p) \langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle \right) \\ 
\end{aligned}
\end{equation}
Since $p \leq 1/2$, we can obtain that
\begin{equation} \label{eqn:ap}
\begin{aligned}
& \alpha (p \langle x_k - x^*, \nabla_x F(x_k, y_k) \rangle - (1-p) \langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle) \\
& \qquad \qquad \ge p (\langle x_k - x^*, \alpha \nabla_x F(x_k, y_k) \rangle - \langle y_k - y^*, \alpha \nabla_y F(x_k, y_k) \rangle) \\
    & \qquad \qquad = p \langle u_k - u^*, G_k \rangle \ge p \alpha\mu \| u_k - u^* \|^2,
\end{aligned}
\end{equation}
where the last inequality is derived by~\eqref{equ: a6},
and 
\begin{equation}
\begin{aligned}
    & \alpha^2(p \| \nabla_x F(x_k, y_k) \|^2 + (1-p) \| \nabla_y F(x_k, y_k) \|^2) \\
    & \qquad \qquad \leq (1-p) \left(\| \alpha \nabla_x F(x_k, y_k) \|^2 + \| \alpha \nabla_y F(x_k, y_k)\| \right) \\
    & \qquad \qquad  =  (1-p) \|G_k\|^2 \leq (1-p) \alpha^2 L_1 \|u_k - u^*\|^2, 
\end{aligned}
\end{equation}
where the last inequality is derived by the $L_1$ smoothness of $F$. 
Therefore, we can obtain that
\begin{equation}
% \begin{aligned}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2 \right] 
    % & \| u_k - u^* \|^2  - 2 p \mu \alpha \| u_k - u^* \|^2 
    % & + \alpha^2 (1-p) \| \nabla_x F(x_k, y_k) \|^2 + \alpha^2 (1-p) \| \nabla_y F(x_k, y_k) \|^2 \\
    \leq (1 + \alpha^2 (1 - p) L_1^2 - 2p \mu \alpha) \| u_k - u^* \|^2.
% \end{aligned}
\end{equation}
Then, for $\alpha$ sufficiently small (i.e., $0 < \alpha  < \frac{2p\mu}{(1-p)L_1^2}$), we can define $\rho := 1 - 2p \mu \alpha + \alpha^2 (1 - p) L_1^2 < 1$, and thus 
\begin{equation}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] \leq \rho \| (x_k, y_k) - (x^*, y^*) \|^2.
\end{equation}

\item \underline{Case 2: $\langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle \leq 0$.} 
In this case, the inequality~\eqref{eqn:ap} does not hold any more. 
To handle this issue, we decompose $(1-p)$ as $p+(1-2p)$, and obtain that
\begin{equation}
\begin{aligned}
& \alpha (p \langle x_k - x^*, \nabla_x F(x_k, y_k) \rangle - (1-p) \langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle) \\
& \qquad \qquad \ge p (\langle x_k - x^*, \alpha \nabla_x F(x_k, y_k) \rangle - \langle y_k - y^*, \alpha \nabla_y F(x_k, y_k) \rangle) - (1-2p)\| \langle y_k - y^*, \alpha \nabla_y F(x_k, y_k) \rangle \| \\ 
& \qquad \qquad \ge p \alpha\mu \| u_k - u^* \|^2 - (1-2p) \langle y_k - y^*, \alpha \nabla_y F(x_k, y_k) \rangle \ge p \alpha\mu \| u_k - u^* \|^2, 
\end{aligned}
\end{equation}
where the last inequality is due to that $F$ is strongly concave of $y$. 
% According to~\eqref{equ: a6}, we have 
% \begin{equation} \label{equ: a9}
    % \langle x_k - x^*, \nabla_x F(x_k, y_k) \rangle \geq \mu \| u_k - u^* \|^2 + \langle y_k - y^*, \nabla_y F(x_k, y_k) \rangle.
% \end{equation}
Therefore, the inequality 
\begin{equation}
% \begin{aligned}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] 
    % = & \| u_k - u^* \|^2 + \alpha^2 p \| \nabla_x F(x_k, y_k) \|^2 + \alpha^2 (1-p) \| \nabla_y F(x_k, y_k) \|^2 
    % \leq & \| u_k - u^* \|^2 -2 \alpha p \mu \| u_k - u^* \|^2 \\ 
    % & + \alpha^2 p \| \nabla_x F(x_k, y_k) \|^2 + \alpha^2 (1-p) \| \nabla_y F(x_k, y_k) \|^2 \\ 
    \leq (1 - 2 \alpha p \mu + \alpha^2 (1-p) L_1^2) \| u_k - u^* \|^2.
% \end{aligned}
\end{equation}
still holds for this case. 
Hence, for $\alpha$ sufficiently small, we also have
\begin{equation}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] \leq \rho \| (x_k, y_k) - (x^*, y^*) \|^2.
\end{equation}
\end{itemize} 

Combining the Case 1 and 2, we can conclude that,
for sufficiently small $\alpha$ ($0 < \alpha  < \frac{2p\mu}{(1-p)L_1^2}$), and $\rho := (1 - 2 \alpha \mu + \alpha^2 (1 - p) L_1^2 < 1$, 
we have 
\begin{equation}
    \e_k \left[\| H (x_k, y_k) - (x^*, y^*) \|^2\right] \leq \rho \| (x_k, y_k) - (x^*, y^*) \|^2.
\end{equation}

\end{proof}

A direct observation of Theorem \ref{thm: rgda is contractive} indicates the Corollary \ref{coro: 1}. Precisely, we provide the following proof. 

\begin{proof}[\textbf{Proof of Corollary \ref{coro: 1}}]

Let $\{ (x_k, y_k) \}$ be the sequence generated by Algorithm \ref{algo: rsgda}. By Theorem \ref{thm: rgda is contractive}, for $\alpha$ sufficiently small, there exists constant $\rho < 1$, such that  
\begin{equation} \label{equ: a12}
    \e_k [\| H (x_k, y_k) - (x^*, y^*) \|^2] \leq \rho \| (x_k, y_k) - (x^*, y^*) \|^2, \quad \forall k .
\end{equation}
Taking expectation in both sides of (\ref{equ: a12}), we have 
\begin{equation} 
    \e \left[\| H (x_k, y_k) - (x^*, y^*) \|^2 \right] \leq \rho \e \| (x_k, y_k) - (x^*, y^*) \|^2, \quad \forall k \geq 0.
\end{equation}
Hence that 
\begin{equation} 
    \e \left[\| H (x_k, y_k) - (x^*, y^*) \|^2 \right] \leq \rho^k \| (x_0, y_0) - (x^*, y^*) \|^2, \quad \forall k \geq 0,
\end{equation}
which indicates that $\{ (x_k, y_k) \}_k$ linearly converges to the saddle point $(x^*, y^*)$ in expectation.

\end{proof}

\subsection{Proofs in Section \ref{sec: 4.3}}

To prove the results in Section \ref{sec: 4.3}, we introduce three technical lemmas first.

\begin{lemma}[\citet{nouiehed_solving_2019}] \label{lemma: 1}
Under  Assumption \ref{assum: smoothness} and \ref{assum: ncpl condtion}, $\phi$ is $L_2$ smooth with $L_2 = L_1 + \frac{L_1 \kappa}{2}$ and $\kappa = L_1 / \mu$.
\end{lemma}

\begin{lemma}[\citet{karimi2016linear}] \label{lemma: 2}
    
If $g(\cdot)$ is $l$-smooth and it satisfies P{\L} condition with constant $\mu$, i.e., 
\begin{equation*}
    \| \nabla g(x) \|^2 \geq 2 \mu [g(x) - \min_x g(x)], \quad \forall x, 
\end{equation*}
then it also satisfies error bound condition with $\mu$, i.e., 
\begin{equation*}
    \| \nabla g(x) \|^2 \geq \mu \| x_p - x \|, \quad \forall x,
\end{equation*}
where $x_p$ stands for the projection of $x$ onto the optimal set.
\end{lemma}

\begin{lemma}[Robbins-Siegmund theorem \citep{robbins1971convergence}] \label{lemma: 3}
    
Consider a filtration $\{ \mathcal{F}_k \}_k$, the nonnegative sequences of $\{ \mathcal{F}_k \}_k$ adapted processes $\{ V_k \}_k$, $\{ U_k \}_k$ and $\{ Z_k \}_k$ such that $\sum_k Z_k < +\infty$ almost surly, and 
\begin{equation}
    \e_k [V_{k+1} |\mathcal{F}_k ] + U_{k+1} \leq V_k + Z_k, \quad \forall k \geq 0.
\end{equation}
Then $\{ V_k \}_k$ converges and $\sum_k U_k < +\infty$ almost surly.
    
\end{lemma}

The basic idea to prove Theorem \ref{thm: convergence of rsgda} is to introduce a suitable decreasing Lyapunov function $V$, such that $\{ V (x_k, y_k) \}_k$ is decreasing. In this paper, we introduce the following function. 
\begin{equation} \label{equ: a15}
    V_k := V(x_k, y_k) = \phi (x_k) + C[\phi (x_k) - F(x_k, y_k)] = (1+C) \phi (x_k) - C F(x_k, y_k),
\end{equation}
where $C > 0$ is a constant to be determined later. 

\begin{proof}[\textbf{Proof of Theorem \ref{thm: convergence of rsgda}}]

Let $V_k$ be the function given by (\ref{equ: a15}). Without loss of generality, we assume that $\phi (x_k) \geq 0$, $\forall k \geq 0$. Then $\{ V_k \}_k$ is a nonnegative sequence. Next we need to estimate the value 
\begin{equation}
    V_k - \e_k \left[V_{k+1}\right].
\end{equation}
Here we denote $\e_{z_k}[\cdot]$ by the expectation conditioned on the random variable $z_k$, and $\e_k[\cdot]$ by the expectation conditioned on all past random variables.

\begin{itemize}
    \item \underline{Step 1: Estimate $\e_k \left[\phi (x_{k+1})\right] - \phi (x_k)$.} Recall the definition of $x_{k+1}$, we have 
\begin{equation} \label{equ: a17}
    \e_k \left[\phi (x_{k+1})\right] = p \e_{z_k} \left[\phi (x_k^+)\right] + (1-p) \e_{z_k}\left[\phi (x_k)\right] = p \e_{z_k} \left[\phi (x_k^+)\right] + (1-p) \phi (x_k).
\end{equation}
Based on the Assumption \ref{assum: smoothness} and \ref{assum: ncpl condtion} hold, then $\phi$ is $L_2$-smooth by Lemma \ref{lemma: 1}. Hence, 
\begin{equation}
\begin{aligned}
    \phi (x_k^+) & \leq \phi (x_k) + \langle x_k^+ - x_k, \nabla \phi (x_k) \rangle + \frac{L_2}{2} \| x_k^+ - x_k \|^2 \\ 
    & = \phi (x_k) - \alpha_k \langle \nabla_x f(x_k, y_k; z_k), \nabla \phi (x_k) \rangle + \frac{\alpha_k^2 L_2}{2} \| \nabla_x f(x_k, y_k; z_k) \|^2.
\end{aligned}
\end{equation}
Taking conditional expectation in both sides of the above equation, we obtain
\begin{equation} \label{equ: a18}
\begin{aligned}
    \e_{z_k}\left[\phi (x_k^+)\right] & \leq \phi (x_k) - \alpha_k \langle \nabla_x F(x_k, y_k), \nabla \phi (x_k) \rangle + \frac{\alpha_k^2 L_2}{2} \e_{z_k} \left[\| \nabla_x f(x_k, y_k; z_k) \|^2\right] \\
    & = \phi (x_k) - \alpha_k \langle \nabla_x F(x_k, y_k), \nabla \phi (x_k) \rangle + \frac{\alpha_k^2 L_2}{2} \e_{z_k} \left[\| \nabla_x F(x_k, y_k) - \nabla f(x_k, y_k; z_k) \|^2 \right] \\ 
    & \quad + \frac{\alpha_k^2 L_2}{2} \e_{z_k} \left[\| \nabla_x F(x_k, y_k) \|^2\right] + \alpha_k^2 L_2 \e_{z_k} \left[\langle \nabla_x F(x_k, y_k), \nabla_x F(x_k, y_k) - \nabla f(x_k, y_k; z_k) \right]. 
\end{aligned}
\end{equation}
By using Assumption 2, we can bound and eliminate the last two terms in the equation, respectively. Thus, 
\begin{equation}
\e_{z_k}\left[\phi (x_k^+)\right] \leq  \phi (x_k) - \alpha_k \langle \nabla_x F(x_k, y_k), \nabla \phi (x_k) \rangle + \frac{\alpha_k^2 L_2 \sigma^2}{2} + \frac{\alpha_k^2 L_2}{2} \| \nabla_x F(x_k, y_k) \|^2. 
\end{equation}
Furthermore, by rewritting the term $\langle \nabla_x F(x_k, y_k), \nabla \phi (x_k) \rangle$, we have
\begin{equation} \label{equ:res}
\begin{aligned}
\e_{z_k}\left[\phi (x_k^+)\right]
     & \leq \phi (x_k)  + \frac{\alpha_k^2 L_2 \sigma^2}{2} + \frac{\alpha_k}{4} \| \nabla_x F(x_k, y_k) \|^2 \\ 
     & \quad + \frac{\alpha_k}{2} \| \nabla_x F(x_k, y_k) - \nabla \phi (x_k) \|^2 - \frac{\alpha_k}{2} \| \nabla \phi (x_k) \|^2 - \frac{\alpha_k}{2} \| \nabla_x F(x_k, y_k) \|^2 \\
    & =  \phi (x_k) - \frac{\alpha_k}{2} \| \nabla \phi (x_k) \|^2 - \frac{\alpha_k}{4} \| \nabla_x F(x_k, y_k) \|^2 + \frac{\alpha_k}{2} \| \nabla_x F(x_k, y_k) - \nabla \phi (x_k) \|^2 + \frac{\alpha_k^2 L_2 \sigma^2}{2}.
\end{aligned}
\end{equation}
Combining equation~\eqref{equ: a17} and  ~\eqref{equ:res}, we have 
\begin{equation}
    \e_k \left[\phi (x_{k+1})\right] \leq \phi (x_k) - \frac{\alpha_k p}{2} \| \nabla \phi (x_k) \|^2 - \frac{\alpha_k p}{4} \| \nabla_x F(x_k, y_k) \|^2 + \frac{\alpha_k p}{2} \| \nabla_x F(x_k, y_k) - \nabla \phi (x_k) \|^2 + \frac{\alpha_k^2 L_2 \sigma^2 p}{2}.
\end{equation}

\item \underline{Step 2: Estimate the term $\e_k [F(x_{k+1}, y_{k+1})] - F(x_k, y_k)$.} Note that 
\begin{equation} \label{equ: a20}
    \e_k [F(x_{k+1}, y_{k+1})] - F(x_k, y_k) = p \left(\e_{z_k} [F(x_k^+, y_k)] - F(x_k, y_k) \right) + p \left(\e_{z_k} [F(x_k, y_k^+)] - F(x_k, y_k) \right).
\end{equation}
Now we estimate the two terms separately. First, because $F$ is $L_1$-smooth, we obtain 
\begin{equation*}
\begin{aligned}
    F(x_k^+, y_k) & \geq F(x_k, y_k) + \langle \nabla_x F(x_k, y_k), x_k^+ - x_k \rangle - \frac{L_1}{2} \|  x_k^+ - x_k \|^2 \\ 
    & = F(x_k, y_k) - \alpha_k \langle \nabla_x F(x_k, y_k), \nabla_x f(x_k, y_k; z_k) \rangle - \frac{\alpha_k^2 L_1}{2} \|   \nabla_x f(x_k, y_k; z_k) \|^2.
\end{aligned}
\end{equation*}
Hence, based on Assumption 2, we can obtain that
\begin{equation} \label{equ: a21}
\begin{aligned}
    \e_{z_k} \left[ F(x_k^+, y_k) \right] - F(x_k, y_k) 
    & \geq - \alpha_k \| \nabla_x F(x_k, y_k) \|^2 - \frac{\alpha_k^2 L_1}{2} \| \nabla_x F(x_k, y_k) \|^2 \\
    & \quad - \frac{\alpha_k^2 L_1}{2} \e_{z_k} \left[ \| \nabla_x f(x_k, y_k; z_k) - \nabla_x F(x_k, y_k) \|^2 \right] .
\end{aligned}
\end{equation}
Furthermore, since $\alpha_k \leq 1/L_2 < 1/L_1$, we have
\begin{equation}
\e_{z_k} \left[ F(x_k^+, y_k) \right] - F(x_k, y_k) \ge - \frac{3 \alpha_k}{2} \| \nabla_x F(x_k, y_k) \|^2 - \frac{\alpha_k^2 L_1 \sigma^2}{2}.
\end{equation}
Similarly, we have
\begin{equation*}
\begin{aligned}
    F(x_k, y_k^+) & \geq F(x_k, y_k) + \langle \nabla_y F(x_k, y_k), y_k^+ - y_k \rangle - \frac{L_1}{2} \|  y_k^+ - y_k \|^2 \\ 
    & = F(x_k, y_k) + \eta_k \langle \nabla_y F(x_k, y_k), \nabla_y f(x_k, y_k; z_k) \rangle - \frac{\eta_k^2 L_1}{2} \|   \nabla_y f(x_k, y_k; z_k) \|^2.
\end{aligned}
\end{equation*}
Hence, 
\begin{equation} \label{equ: a22}
\begin{aligned}
    \e_{z_k} \left[ F(x_k, y_k^+) \right] - F(x_k, y_k) & \geq \eta_k \| \nabla_y F(x_k, y_k) \|^2 - \frac{\eta_k^2 L_1}{2} \| \nabla_y F(x_k, y_k) \|^2 - \frac{\eta_k^2 L_1 \sigma^2}{2} \\ 
    & \geq \frac{\eta_k}{2} \| \nabla_y F(x_k, y_k) \|^2 - \frac{\eta_k^2 L_1 \sigma^2}{2}.
\end{aligned}
\end{equation}
Combining~\eqref{equ: a20}, \eqref{equ: a21} and \eqref{equ: a22}, we obtain 
{\small
\begin{equation}
\begin{aligned}
    \e_k [F(x_{k+1}, y_{k+1})] - F(x_k, y_k) & \geq p \left( - \frac{3\alpha_k}{2} \| \nabla_x F(x_k, y_k) \|^2 - \frac{\alpha_k^2 L_1 \sigma^2}{2} \right)  + (1-p) \left( \frac{\eta_k}{2} \| \nabla_y F(x_k, y_k) \|^2 - \frac{\eta_k^2 L_1 \sigma^2}{2} \right) \\ 
    & = \frac{(1-p)\eta_k}{2} \| \nabla_y F(x_k, y_k) \|^2 -\frac{3\alpha_k p}{2} \| \nabla_x F(x_k, y_k) \|^2 - \frac{\sigma^2L_1 (p \alpha_k^2 + (1-p) \eta_k^2)}{2}
\end{aligned}
\end{equation}
}
\item \underline{Step 3: Upper bound of $\| \nabla_x F(x_k, y_k) - \phi (x) \|^2$.}
Recall $y^* (x) := \arg\max_y F(x, y)$, thus $\nabla_y F(x, y^* (x)) = 0$ can be derived by Fermat's rule. Now, the gradient $\nabla \phi(x)$ can be computed as
\begin{equation*}
    \nabla \phi (x) = \nabla_x F(x, y^*(x)) = \nabla_x F(x, y^*(x)) + \nabla_y F(x, y^*(x)) \nabla_x y^*(x) = \nabla_x F(x, y^*(x)).
\end{equation*}
Furthermore, based on the smoothness of $F$ and the P{\L} condition, we have 
\begin{equation} \label{equ: a23}
\begin{aligned}
    \| \nabla_x F(x_k, y_k) - \nabla \phi (x_k) \|^2 & = \| \nabla_x F(x_k, y_k) - \nabla_x F(x_k, y^*(x_k)) \|^2 \\
    & \leq L_1^2 \| y_k - y^*(x_k) \|^2 
    \leq \left( \frac{L_1}{\mu} \right)^2 \| \nabla_y F(x_k, y_k) \|^2,
\end{aligned}
\end{equation}
where the last inequality holds due to the Lemma \ref{lemma: 2}.

\item \underline{Step 4: Estimate the Lyapunov function $V_k$.}
First, note that 
\begin{equation}
    \| a \|^2 + \| a - b \|^2 \geq \frac{1}{2} \| b \|^2, \quad \forall a, b
\end{equation}
according to the Young's inequality. Hence, 
\begin{equation*}
    -\frac{3p \alpha_k}{2} \| \nabla_x F(x_k, y_k) \|^2 \geq -3 p \alpha_k \| \nabla \phi (x_k) \|^2 - 3 p \alpha_k \| \nabla \phi (x_k) - \nabla_x F(x_k, y_k) \|^2.
\end{equation*}
Combining Step 1 and 2, we have 
\begin{equation} \label{equ: a24}
\begin{aligned}
V_{k}-\mathbb{E}_{k}\left[V_{k+1}\right]
& =(1+C)\left(\phi\left(x_{k}\right)-\mathbb{E}_{k}\left[\phi\left(x_{k+1}\right)\right]\right)+C\left(\mathbb{E}_{k}\left[F\left(x_{k+1}, y_{k+1}\right)\right]-F\left(x_{k}, y_{k}\right)\right) \\
& \geq (1+C)\left(\frac{p \alpha_{k}}{2}\left\|\nabla \phi\left(x_{k}\right)\right\|^{2}-\frac{p \alpha_{k}}{2}\left\|\nabla \phi\left(x_{k}\right)-\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{p \alpha_{k}}{4}\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}\right) \\
& \qquad + C\left(\frac{(1-p) \eta_{k}}{2}\left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2}-\frac{3 p \alpha_{k}}{2}\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}\right) + U_k (C) \\
& \geq  {\left(\frac{p \alpha_{k}}{2}(1+C)-3 p \alpha_{k} C\right) \cdot\left\|\nabla \phi\left(x_{k}\right)\right\|^{2} -\left(\frac{p \alpha_{k}}{2}(1+C)+3 p \alpha_{k} \cdot C\right) \cdot\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)-\nabla \phi \left(x_{k}\right)\right\|^{2} } \\ 
& \qquad +\frac{p \alpha_{k}}{4}(1+C)\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{(1-p) \eta_{k}}{2} C \cdot\left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2} + U_k (C).
% - \left[\frac{L_{2} \alpha_{k}^{2} p(1+C)}{2}+\frac{p}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2} L_{1} \eta_{k}^{2}\right] \sigma^{2} . 
\end{aligned}
\end{equation}
where 
\begin{equation}
    U_k(C) = -\left(\frac{L_{2} \alpha_{k}^{2} p(1+C)}{2}+\frac{p}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2} L_{1} \eta_{k}^{2}\right) \sigma^{2}.
\end{equation}
Plugging~\eqref{equ: a23} into~\eqref{equ: a24}, we get
\begin{equation}
\begin{aligned}
V_{k}-\mathbb{E}_{k}\left[V_{k+1}\right] 
& \ge {\left(\frac{p \alpha_{k}}{2}(1+C)-3 p \alpha_{k} C\right) \cdot\left\|\nabla \phi\left(x_{k}\right)\right\|^{2}-\left(\frac{p \alpha_{k}}{2}(1+C)+3 p \alpha_{k} \cdot C\right)\left(\frac{L_{1}}{\mu}\right)^{2} \cdot\left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2} } \\
& \qquad + \frac{p \alpha_{k}}{4}(1+C)\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{(1-p) \eta_{k}}{2} C \cdot\left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2} + U_k (C) \\
% &-\left[\frac{L_{2} \alpha_{k}^{2} p(1+C)}{2}+\frac{p}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2} L_{1} \eta_{k}^{2}\right] \sigma^{2} . \\
& = \left(\frac{(1-p) \eta_{k}}{2} C-\left(\frac{p \alpha_{k}}{2}(1+C)+3 p \alpha_{k} C\right) \cdot\left(\frac{L_{1}}{\mu}\right)^{2}\right) \cdot\left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2} \\
& \qquad + {\left(\frac{p \alpha_{k}}{2}(1+C)-3 p \alpha_{k} C\right) \cdot \| \nabla \phi (x_{k}) \|^{2} } +\frac{p \alpha_{k}}{4}(1+C)\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2} + U_k (C).
\end{aligned}
\end{equation}
Now, we try to determine the concrete value of $C$. 
Here, we choose $C = 1/ 10$, then 
\begin{equation*}
\begin{aligned}
& \frac{p \alpha_{k}}{2}(1+C)-3 p \alpha_{k} C = p \alpha_k \frac{1-5C}{2} = \frac{1}{4} p \alpha_k, \\ 
& \frac{(1-p) \eta_{k}}{2} C-\left(\frac{p \alpha_{k}}{2}(1+C)+3 p \alpha_{k} C\right) \cdot\left(\frac{L_{1}}{\mu}\right)^{2} \geq \frac{p}{20} \left(\frac{L_{1}}{\mu}\right)^{2} \alpha_k.
\end{aligned}
\end{equation*}
Hence, 
\begin{equation} \label{equ: a26}
\begin{aligned}
V_{k}-\mathbb{E}_{k}\left[V_{k+1}\right] \ge \frac{1}{4} p \alpha_{k} \cdot\left\|\nabla \phi\left(x_{k}\right)\right\|^{2}+\frac{p}{20}\left(\frac{L_{1}}{\mu}\right)^{2} \alpha_{k} \left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{11}{40} p \alpha_{k} \left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2} + U_k (\frac{1}{10})
% & -\left[\frac{11}{20} L_{2} p \alpha_{k}^{2}+\frac{p}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2} L_{1} \eta_{k}^{2}\right] \cdot \sigma^{2} .
\end{aligned}
\end{equation}
Rearranging equation~\eqref{equ: a26}, we have 
\begin{equation} \label{equ: a27}
\begin{aligned}
\frac{1}{4} p \alpha_{k} \left\|\nabla \phi\left(x_{k}\right)\right\|^{2}+\frac{p}{20}\left(\frac{L_{1}}{\mu}\right)^{2} \alpha_{k} \left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{11}{40} p\alpha_{k} \left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}+\mathbb{E}_{k}\left[V_{k+1}\right] 
\le & V_{k} + U_k (\frac{1}{10})
% +\left[\frac{11}{20} L_{2} p \alpha_{k}^{2}+\frac{p}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2} L_{1} \eta_{k}^{2}\right] \cdot \sigma^{2}
\end{aligned}
\end{equation}
\item \underline{Step 5: Using Robbins-Siegmumd Theorem.}
In this step, we use a similar proof technique to \citet{sebbouh2021randomized}. 
We first define that, for all $k \geq 0$,
\begin{equation}
\begin{aligned}
& w_{k}:= \frac{2 \alpha_{k}}{\sum_{j=0}^{k} \alpha_{j}}, \quad 
g_{0}=\frac{1}{4}\left\|\nabla \phi\left(x_{0}\right)\right\|^{2}+\frac{1}{20}\left(\frac{L_{1}}{\mu}\right)^{2} \cdot\left\|\nabla_{y} F\left(x_{0}, y_{0}\right)\right\|^{2}+\frac{11}{40}\left\|\nabla_{x} F\left(x_{0}, y_{0}\right)\right\|^{2} , \\
& g_{k+1}:=\left(1-w_{k}\right) g_{k}+w_{k} \cdot\left(\frac{1}{4}\left\|\nabla \phi\left(x_{k}\right)\right\|^{2}+\frac{1}{20}\left(\frac{L_{1}}{\mu}\right)^{2}\left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{11}{40}\left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}\right) .
\end{aligned}
\end{equation}
Since $\{ \alpha_k \}$ is non-increasing, we have $w_k \in [0, 1]$ for all $k \ge 0$. 
Hence, $g_k$ can be viewed as a convex combination of $\{ h_0, h_1, \dots, h_{k-1} \}$.

Next, using Lemma \ref{lemma: 3} for~\eqref{equ: a27}, and the fact that $\sum_k \alpha_k^2 < \infty, \sum_k \eta_k^2 < \infty$, we can conclude that $\{ V_k \}_k$ converges almost surly. 
On the other side, Rearranging~\eqref{equ: a27}, we have 
\begin{equation}
\frac{\sum_{j=0}^{k} \alpha_{j}}{2} g_{k+1}+\frac{1}{p} \mathbb{E}_{k}\left[V_{k+1}\right]+\frac{\alpha_{k}}{2} g_{k} \leqslant \frac{\sum_{j=0}^{k-1} \alpha_{j}}{2} g_{k}+\frac{1}{p} V_{k} +4 \cdot  U_k (\frac{1}{10}).
% \left[\frac{11}{20} L_{2} \alpha_{k}^{2}+\frac{1}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2 p} L_{1} \eta_{k}^{2}\right] \sigma^{2} .
\end{equation}
Using Lemma \ref{lemma: 3} again and the fact that $\{ V_k \}_k$ converges almost surly, it can be obtained that $\{ \sum_{j=0}^{k} \alpha_{j} g_{k+1} \}_k$ converges almost surly and $\sum_k \alpha_k g_k < + \infty$. 
Particularly, this implies that $\lim_k \alpha_k g_k = 0$. 
Notice that $\alpha_k g_k = \frac{\alpha_k}{\sum_{j=0}^{k-1} \alpha_j} \sum_{j=0}^{k-1} \alpha_j g_k$, together with this $\{ \sum_{j=0}^{k} \alpha_{j} g_{k+1} \}_k$ converges almost surly and $\sum_k \frac{\alpha_k}{\sum_{j=0}^{k-1} \alpha_j} = \infty$, we have $\lim_k \sum_{j=0}^{k-1} g_k = 0$, i.e., 
\begin{equation*}
    g_k = o\left( \frac{1}{\sum_{j=0}^{k-1} \alpha_j} \right).
\end{equation*}
Finally, since $g_k$ is a convex combination of $\{ h_0, h_1, \dots, h_{k-1} \}$, we have that 
\begin{equation}
    \min_{t=0, 1, \dots, k-1} h_t \leq g_k = o\left( \frac{1}{\sum_{j=0}^{k-1} \alpha_j} \right).
\end{equation}
\end{itemize}
\end{proof}

Theorem \ref{thm: convergence rate} is a direct consequence of Theorem \ref{thm: convergence of rsgda}. The proof is Theorem \ref{thm: convergence rate} relies on the convergence rate of $1 / \sum_{j=0}^k \alpha_j$.

\begin{proof}[\textbf{Proof of Theorem \ref{thm: convergence rate}}]

According to Theorem \ref{thm: convergence of rsgda}, we have 
\begin{equation}
    \min_{t=0, \dots, k} h_t = o \left( \frac{1}{\sum_{j=0}^k \alpha_j} \right),
\end{equation}
almost surly. 
On the other side, for any $\epsilon > 0$ given, we define $\alpha_0 = 1$ and $\alpha_j = j^{-\frac{1}{2} - \epsilon}$ for $j = 1, 2, \dots$. Hence, $\sum_{j=0}^k \alpha_j = O(k^{\frac{1}{2} - \epsilon})$. Thus, we get that 
\begin{equation}
    \min_{t=0, \dots, k} h_t = o \left( \frac{1}{\sum_{j=0}^k \alpha_j} \right) = o (k^{- \frac{1}{2} + \epsilon}).
\end{equation}

\end{proof}

The proof of Corollary \ref{coro: 2} can be derived directly. 

\begin{proof}[\textbf{Proof of Corollary \ref{coro: 2}}]

Without loss of generality, we assume that $V_k \geq 0$ for all $k \geq 0$. The convergence in expectation proof is based on the telescopic cancellation in \eqref{equ: a27}.  Taking the expectation to \eqref{equ: a27} and summing for $k=0,1, \dots, n$, we obtain that 
\begin{equation}
    (n+1) \alpha  \cdot \min_{k=0,\dots, n} \e [h_k] \leq \alpha \sum_{k=0}^n \e [h_k] \leq \e [V_0] - \e [V_{n+1}] \leq \e [V_0], \quad \forall n \geq 0.
\end{equation}
Hence, 
\begin{equation}
    \min_{k=0,\dots, n} \e [h_k] \leq \frac{\e[V_0]}{(n+1) \alpha} = O \left(\frac{1}{n+1} \right).
\end{equation}

\end{proof}

Finally, we prove Corollary \ref{coro: 3} in the following.

\begin{proof}[\textbf{Proof of Corollary \ref{coro: 3}}]

The proof of Corollary \ref{coro: 3} is similar to the proof of Theorem \ref{thm: convergence of rsgda}. Without loss of generality, we assume that $V_k \geq 0$ for all $k \geq 0$. Following the same steps as the proof of Theorem \ref{thm: convergence of rsgda}, we can obtain that \eqref{equ: a27} holds for any $k \geq 0$, i.e., 
\begin{equation} \label{equ: a42}
\begin{aligned}
\frac{1}{4} p \alpha_{k} \left\|\nabla \phi\left(x_{k}\right)\right\|^{2}+\frac{p}{20}\left(\frac{L_{1}}{\mu}\right)^{2} \alpha_{k} \left\|\nabla_{y} F\left(x_{k}, y_{k}\right)\right\|^{2}+\frac{11}{40} p\alpha_{k} \left\|\nabla_{x} F\left(x_{k}, y_{k}\right)\right\|^{2}+\mathbb{E}_{k}\left[V_{k+1}\right] 
\le V_{k} + U_k (\frac{1}{10}).
% +\left[\frac{11}{20} L_{2} p \alpha_{k}^{2}+\frac{p}{2} L_{1} \alpha_{k}^{2}+\frac{1-p}{2} L_{1} \eta_{k}^{2}\right] \cdot \sigma^{2}, \quad \forall k \geq 0.
\end{aligned}
\end{equation}
Taking expectation to equation\eqref{equ: a42} and summing for $k=0,1,2, \dots, n$, we get that 
\begin{equation} \label{equ: a43}
    \alpha \sum_{k=0}^n \e [h_k] \leq \e [V_0] - \e [V_{n+1}] + (n+1) \left[\frac{11}{20} L_{2} p \alpha^{2}+\frac{p}{2} L_{1} \alpha^{2}+\frac{1-p}{2} L_{1} \frac{18^2 p^2}{(1-p)^2} \kappa^4 \alpha^2 \right] \cdot \sigma^{2}.
\end{equation}
For ease of notation, we define $M:= \frac{11}{20}L_2 p + \frac{p}{2}L_1 + \frac{18^2(1-p)p^2}{2(1-p)^2}L_1 \kappa^4$.
Hence, 
\begin{equation}
\begin{aligned}
\min_{k=0,\dots, n} \e [h_k] \leq \frac{1}{n+1} \sum_{k=0}^n \e [h_k]  \leq \frac{\e [V_0]}{(n+1) \alpha} + M \sigma^2 \alpha  = \frac{\sqrt{M \sigma^2 \e [V_0]}}{\sqrt{n+1}},
\end{aligned}
\end{equation}
where $\alpha = \sqrt{\frac{\e [V_0]}{(n+1)M}} \sigma$. Thus, for any $\epsilon > 0$, if $n \geq \epsilon^{-2}$, we have that 
\begin{equation}
    \min_{k=0,\dots, n} \e [h_k] \leq \sqrt{M \sigma^2 \e [V_0]} \cdot \epsilon,
\end{equation}
which completes the proof.

\end{proof}

\subsection{Proofs in Section \ref{sec: 4.4}}

\begin{proof}

Note that $\alpha = \frac{1}{L_2}$ and $\eta = \frac{18 p}{1 - p} \kappa^2 \alpha$.
Without loss of generality, we assume that $\inf_x V(x) = \inf V > - \infty$.
Recall the explicit of equation \eqref{equ: a26}, for any $k \geq 0$, we have
\begin{equation} \label{equ: a63}
   h_k \leq \frac{1}{\alpha p} (V_k - \e_k [V_{k+1}]) + \frac{18^2 p}{2 (1-p)} \kappa^4 L_1 \alpha \sigma^2 + \mathrm{const}, 
\end{equation}
where $\mathrm{const} = \frac{11}{20} L_2 \alpha^2 + \frac{1}{2} L_1 \alpha^2$.
Taking expectation to \eqref{equ: a63} and summing for $k = 1, 2, \dots, n$, we have 
\begin{equation} \label{equ: 65}
    \sum_{k=1}^n h_k \leq \frac{1}{\alpha p} (V_1 - \inf V) + \frac{18^2 p n}{2 (1-p)} \kappa^4 L_1 \alpha \sigma^2 + n \cdot \mathrm{const}.
\end{equation}
Hence, to minimize $\sum_{k=1}^n h_k$, it is sufficient to minimize $\frac{1}{\alpha p} (V_1 - \inf V) + \frac{18^2 p n}{2 (1-p)} \kappa^4 L_1 \alpha \sigma^2$.
Let $\delta = V_1 - \inf V$, and the RHS of \eqref{equ: 65} is denoted by $\Theta (p)$.

First, note that the assumption $\eta \leq \frac{1}{L_1}$ is equivalent to
\begin{equation}
    p \leq \frac{L_2}{9 L_1 \kappa^2 + L_2}.
\end{equation}

\underline{\textbf{Case 1}}. $\sigma = 0$.

The minimization problem is trivial in this case.
We have $\arg\min_p \Theta (p) = \frac{L_2}{9 L_1 \kappa^2 + L_2}$.

\underline{\textbf{Case 2}}. $\sigma > 0$.

On the one side, the minimization problem $\arg\min_{p > 0} \left\{ \frac{1}{\alpha p} \delta + \frac{18^2 p n}{2 (1-p)} \kappa^4 L_1 \alpha \sigma^2 \right\}$ is equal to
\begin{equation}
    p = \frac{\sqrt{\delta} \sqrt{\delta + 648 \alpha^2 \kappa^4 L_1 \sigma^2 n} - \delta}{324 \alpha^2 \kappa^4 L_1 \sigma^2 n}.
\end{equation}

On the other side, note that the assumption $\eta \leq \frac{1}{L_1}$ is equivalent to
\begin{equation}
    p \leq \frac{L_2}{9 L_1 \kappa^2 + L_2}.
\end{equation}

Let $p_1 = \frac{\sqrt{\delta} \sqrt{\delta + 648 \alpha^2 \kappa^4 L_1 \sigma^2 n} - \delta}{324 \alpha^2 \kappa^4 L_1 \sigma^2 n}$ and $p_2 = \frac{L_2}{9 L_1 \kappa^2 + L_2}$.
According to the basic theory of optimization, we have the following result.
If $p_1 \leq p_2$, we have $ \mathop{\arg\min}_{p \in (0, 1)} \Theta (p)$.
Else, we have $\mathop{\arg\min}_{p \in (0, 1)} \Theta (p) = p_2$.
Hence, combining the results above together, we have 
\begin{equation}
    \mathop{\arg\min}_{p \in (0, 1)} \Theta (p) = \min \left\{ p_1, p_2 \right\}.
\end{equation}

\end{proof}