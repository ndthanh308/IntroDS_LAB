\section{Related work} \label{sec:related}

\textbf{P{\L} condition in minimax optimization}. P{\L} condition, named after Polyak and {\L}ojasiewicz, was initially introduced by Polyak in \citet{polyak1963gradient} to obtain the global convergence of gradient descent at a linear rate. The P{\L} condition, roughly speaking, describes the sharpness of a function up to a representation. A generalized form of this condition, which nowadays is called the Kurdyka-{\L}ojasiewicz condition, was introduced by Kurdyak and {\L}ojasiewicz in \citet{kurdyka1998gradients} and \citet{lojasiewicz1963propriete}. There is tremendous work related to the K{\L} condition, making it impossible to list all of them. Curious readers can refer to \citet{bolte2007clarke, bolte2007lojasiewicz} for a systematical discussion. 
In the deterministic case, \citet{nouiehed_solving_2019} showed that GDA and its multi-step variant can achieve an approximate critical point in $O(\epsilon^{-2})$ steps. 
Recently, \citet{fiez2021global} proved that GDA converges to an approximate differential Stackelberg equilibrium with complexity $O(\epsilon^{-2})$; \citet{yang2021faster} proved that a single loop GDA converges to an approximate Stackelberg equilibrium, and an approximate local Nash equilibrium can be constructed from GDA. 

\textbf{Other minimax optimization}. Minimax optimization problems have received wide attention since the work of von Neumann \cite{neumann1928theorie}. Since then, minimax problems have been well studied in the convex-concave setting. However, results beyond the convex-concave setting are much more recent. A large body of existing work~\cite{fiez2021global, lin2020gradient, boct2020alternating, lin2020near} considered GDA in the nonconvex strongly concave setting, obtaining $O(\epsilon^{-2})$ computation complexity in the deterministic case and $O(\epsilon^{-4})$ complexity in the stochastic case. \citet{daskalakis2018limit} considered GDA in the nonconvex-nonconcave setting and provided theoretical analysis of the limit points of GDA. Furthermore, they proposed Optimistic-GDA to robustify the performance of GDA. Due to the difficulty of finding an approximate Nash equilibrium in the general nonconvex-nonconcave setting, different notions of local optimal solutions as well as their properties have been investigated in \citet{jin2020local, fiez2021global} and so on.
\citet{xian_2021_faster} considered the minimax problems in the nonconvex-strongly concave setting and proposed a decentralized algorithm to solve this problem, which achieves a faster convergence rate than SGDA.
\citet{sharma_2022_federated} systemically discussed the local performance of SGDA in both the convex-concave setting and the nonconvex-nonconcave setting.
In \citet{luo_2020_advances}, the authors proposed a variant of SGDA called SREDA, and proved that it achieved the best known stochastic gradient complexity in the nonconvex-strongly concave setting.
Some other work including \citet{diakonikolas_2021_efficient, li_2022_convergence, lee2021fast} considered the applications of extra-gradient method to the minimiax problems.

% \yy{these two parts are still not very clear to me. consider to: 1) summarize a story line of existing work especially for long paragraphs (otherwise it is hard to follow), 2) explain how your work differs from them (this might relate to your story).}
% \subsection{Other related work}

% Several different kinds of momentum techniques are considered in the minimization problems, including the Nesterov's momentum technique \cite{nesterov1983method}, heavy ball method \cite{polyak1964some}, PID Control-based method \cite{an2018pid}, accelerated SGD \cite{kidambi2018insufficiency}, and Quasi-Hyperbolic momentum \cite{ma2018quasihyperbolic}. In this paper, we only focus on the heavy-ball momentum, which is arguably the most popular form of momentum in the current deep learning area. To the best of our knowledge, there are few existing works about the momentum technique in the minimax problems due to the difficulty of analyzing the minimax problems.

% In our opinion, the intrinsic challenge of analyzing the minimax problems is that the targets of two variables, $x$ and $y$, are opposite. For $x$, it hopes to minimize the value of $F$, while the purpose of $y$ is to maximize the value of $F$. The opposite targets make it hard to link the minimax points to other properties of $F$. For example, gradient descent method is contractive if the target function is smooth and the step size is small enough. However, it can be shown that, the gradient descent ascent method might be expansive for some restricted smooth functions for any positive step sizes \cite{farnia2021train}. Thus, the two variables $x$ and $y$ can bring intrinsic difficulties for analyses. ({\color{red}strange! how to rewrite it?})

% \yy{I just simply googled the key words, and some references may be missing: Efficient Algorithms for Smooth Minimax Optimization- NIPS19, Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems - NIPS20, The complexity of constrained min-max optimization - STOC21, Efficient methods for structured nonconvex-nonconcave min-max optimization - AISTATS21.}