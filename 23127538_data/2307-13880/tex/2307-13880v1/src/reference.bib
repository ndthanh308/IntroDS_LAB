%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for new at 2022-07-16 09:42:54 +0800 


%% Saved with string encoding Unicode (UTF-8) 

@inproceedings{loizou2020stochastic,
  title={Stochastic hamiltonian gradient methods for smooth games},
  author={Loizou, Nicolas and Berard, Hugo and Jolicoeur-Martineau, Alexia and Vincent, Pascal and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={International Conference on Machine Learning},
  pages={6370--6381},
  year={2020},
  organization={PMLR}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@article{goodfellow2014explaining,
	author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
	date-added = {2022-07-16 09:42:51 +0800},
	date-modified = {2022-07-16 09:42:51 +0800},
	journal = {arXiv preprint arXiv:1412.6572},
	title = {Explaining and harnessing adversarial examples},
	year = {2014}}

@inproceedings{liu_improved_2020,
	author = {Liu, Yanli and Gao, Yuan and Yin, Wotao},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {18261--18271},
	publisher = {Curran Associates, Inc.},
	title = {An Improved Analysis of Stochastic Gradient Descent with Momentum},
	url = {https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf}}

@inproceedings{nouiehed_solving_2019,
	author = {Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch{\'e}-Buc, F. d' and Fox, E. and Garnett, R.},
	publisher = {Curran Associates, Inc.},
	title = {Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods},
	url = {https://proceedings.neurips.cc/paper/2019/file/25048eb6a33209cb5a815bff0cf6887c-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/file/25048eb6a33209cb5a815bff0cf6887c-Paper.pdf}}

@article{goodfellow2014generative,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	journal = {Advances in neural information processing systems},
	title = {Generative adversarial nets},
	volume = {27},
	year = {2014}}

@article{zhang2021multi,
	author = {Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
	journal = {Handbook of Reinforcement Learning and Control},
	pages = {321--384},
	publisher = {Springer},
	title = {Multi-agent reinforcement learning: A selective overview of theories and algorithms},
	year = {2021}}

@inproceedings{dai2018sbeed,
	author = {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {1125--1134},
	title = {Sbeed: Convergent reinforcement learning with nonlinear function approximation},
	year = {2018}}

@book{cesa2006prediction,
	author = {Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
	publisher = {Cambridge university press},
	title = {Prediction, learning, and games},
	year = {2006}}

@book{von2007theory,
	author = {Von Neumann, John and Morgenstern, Oskar},
	publisher = {Princeton university press},
	title = {Theory of games and economic behavior},
	year = {2007}}

@book{bacsar1998dynamic,
	author = {Ba{\c{s}}ar, Tamer and Olsder, Geert Jan},
	publisher = {SIAM},
	title = {Dynamic noncooperative game theory},
	year = {1998}}

@article{sion1958general,
	author = {Sion, Maurice},
	journal = {Pacific Journal of mathematics},
	number = {1},
	pages = {171--176},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {On general minimax theorems.},
	volume = {8},
	year = {1958}}

@article{korpelevich1976extragradient,
	author = {Korpelevich, Galina M},
	journal = {Matecon},
	pages = {747--756},
	title = {The extragradient method for finding saddle points and other problems},
	volume = {12},
	year = {1976}}

@article{nemirovski2004prox,
	author = {Nemirovski, Arkadi},
	journal = {SIAM Journal on Optimization},
	number = {1},
	pages = {229--251},
	publisher = {SIAM},
	title = {Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
	volume = {15},
	year = {2004}}

@article{bubeck2014convex,
	author = {Bubeck, S{\'e}bastien},
	journal = {arXiv preprint arXiv:1405.4980},
	title = {Convex optimization: Algorithms and complexity},
	year = {2014}}

@article{hazan2019introduction,
	author = {Hazan, Elad},
	journal = {arXiv preprint arXiv:1909.05207},
	title = {Introduction to online convex optimization},
	year = {2019}}

@inproceedings{jin2020local,
	author = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {4880--4889},
	title = {What is local optimality in nonconvex-nonconcave minimax optimization?},
	year = {2020}}

@article{neumann1928theorie,
	author = {Neumann, John Von},
	journal = {Mathematische annalen},
	number = {1},
	pages = {295--320},
	publisher = {Springer},
	title = {Zur theorie der gesellschaftsspiele},
	volume = {100},
	year = {1928}}

@article{nouiehed2019solving,
	author = {Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
	journal = {arXiv preprint arXiv:1902.08297},
	title = {Solving a class of non-convex min-max games using iterative first order methods},
	year = {2019}}

@article{sinha2017certifying,
	author = {Sinha, Aman and Namkoong, Hongseok and Volpi, Riccardo and Duchi, John},
	journal = {arXiv preprint arXiv:1710.10571},
	title = {Certifying some distributional robustness with principled adversarial training},
	year = {2017}}

@article{lin2018solving,
	author = {Lin, Qihang and Liu, Mingrui and Rafique, Hassan and Yang, Tianbao},
	journal = {arXiv preprint arXiv:1810.10207},
	title = {Solving weakly-convex-weakly-concave saddle-point problems as weakly-monotone variational inequality},
	volume = {5},
	year = {2018}}

@inproceedings{sebbouh2021randomized,
	author = {Sebbouh, Othmane and Cuturi, Marco and Peyr{\'e}, Gabriel},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	organization = {PMLR},
	pages = {2941--2969},
	title = {Randomized Stochastic Gradient Descent Ascent},
	year = {2022}}

@article{yang2021faster,
	author = {Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
	journal = {arXiv preprint arXiv:2112.05604},
	title = {Faster Single-loop Algorithms for Minimax Optimization without Strong Concavity},
	year = {2021}}

@inproceedings{lin2020gradient,
	author = {Lin, Tianyi and Jin, Chi and Jordan, Michael},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {6083--6093},
	title = {On gradient descent ascent for nonconvex-concave minimax problems},
	year = {2020}}

@article{zhang2021complexity,
	author = {Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
	journal = {arXiv preprint arXiv:2103.15888},
	title = {The complexity of nonconvex-strongly-concave minimax optimization},
	year = {2021}}

@article{polyak1963gradient,
	author = {Polyak, Boris Teodorovich},
	journal = {Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
	number = {4},
	pages = {643--653},
	publisher = {Russian Academy of Sciences, Branch of Mathematical Sciences},
	title = {Gradient methods for minimizing functionals},
	volume = {3},
	year = {1963}}

@article{chen2021proximal,
	author = {Chen, Ziyi and Zhou, Yi and Xu, Tengyu and Liang, Yingbin},
	journal = {arXiv preprint arXiv:2102.04653},
	title = {Proximal Gradient Descent-Ascent: Variable Convergence under K {\L} Geometry},
	year = {2021}}

@inproceedings{lei2020sgd,
	author = {Lei, Qi and Lee, Jason and Dimakis, Alex and Daskalakis, Constantinos},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {5799--5808},
	title = {Sgd learns one-layer networks in wgans},
	year = {2020}}

@article{nagarajan2017gradient,
	author = {Nagarajan, Vaishnavh and Kolter, J Zico},
	journal = {arXiv preprint arXiv:1706.04156},
	title = {Gradient descent GAN optimization is locally stable},
	year = {2017}}

@article{heusel2017gans,
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	journal = {Advances in neural information processing systems},
	title = {Gans trained by a two time-scale update rule converge to a local nash equilibrium},
	volume = {30},
	year = {2017}}

@article{mescheder2017numerics,
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	journal = {arXiv preprint arXiv:1705.10461},
	title = {The numerics of gans},
	year = {2017}}

@article{sanjabi2018convergence,
	author = {Sanjabi, Maziar and Ba, Jimmy and Razaviyayn, Meisam and Lee, Jason D},
	journal = {arXiv preprint arXiv:1802.08249},
	title = {On the convergence and robustness of training gans with regularized optimal transport},
	year = {2018}}

@article{yan2020optimal,
	author = {Yan, Yan and Xu, Yi and Lin, Qihang and Liu, Wei and Yang, Tianbao},
	journal = {arXiv preprint arXiv:2002.05309},
	title = {Optimal epoch stochastic gradient descent ascent methods for min-max optimization},
	year = {2020}}

@incollection{lojasiewicz1963propriete,
	author = {{\L}ojasiewicz, S.},
	booktitle = {Les {\'E}quations aux {D}{\'e}riv{\'e}es {P}artielles ({P}aris, 1962)},
	mrclass = {26.55 (32.25)},
	mrnumber = {0160856},
	mrreviewer = {H. A. Antosiewicz},
	pages = {87--89},
	publisher = {{\'E}ditions du Centre National de la Recherche Scientifique (CNRS)},
	title = {Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques r{\'e}els},
	year = {1963}}

@article{kurdyka1998gradients,
	author = {Kurdyka, Krzysztof},
	fjournal = {Universit\'{e} de Grenoble. Annales de l'Institut Fourier},
	issn = {0373-0956},
	journal = {Ann. Inst. Fourier (Grenoble)},
	mrclass = {03C65 (14P15 26D10 26E05)},
	mrnumber = {1644089},
	mrreviewer = {A. J. Wilkie},
	number = {3},
	pages = {769--783},
	title = {On gradients of functions definable in o-minimal structures},
	url = {http://www.numdam.org/item?id=AIF_1998__48_3_769_0},
	volume = {48},
	year = {1998},
	bdsk-url-1 = {http://www.numdam.org/item?id=AIF_1998__48_3_769_0}}

@article{bolte2007clarke,
	author = {Bolte, J{\'e}r{\^o}me and Daniilidis, Aris and Lewis, Adrian and Shiota, Masahiro},
	doi = {10.1137/060670080},
	fjournal = {SIAM Journal on Optimization},
	issn = {1052-6234},
	journal = {SIAM J. Optim.},
	mrclass = {49J52 (26D10 32B20)},
	mrnumber = {2338451},
	mrreviewer = {Marian Mure{\c}{s}an},
	number = {2},
	pages = {556--572},
	title = {Clarke subgradients of stratifiable functions},
	url = {https://doi.org/10.1137/060670080},
	volume = {18},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1137/060670080}}

@article{bolte2007lojasiewicz,
	author = {Bolte, J{\'e}r{\^o}me and Daniilidis, Aris and Lewis, Adrian},
	doi = {10.1137/050644641},
	fjournal = {SIAM Journal on Optimization},
	issn = {1052-6234},
	journal = {SIAM J. Optim.},
	mrclass = {49J52 (26D10 32B20 34A60 49J53)},
	mrnumber = {2274510},
	mrreviewer = {Giovanni Colombo},
	number = {4},
	pages = {1205--1223},
	title = {The {{\L}}ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems},
	url = {https://doi.org/10.1137/050644641},
	volume = {17},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1137/050644641}}

@article{absil2005convergence,
	author = {Absil, P.-A. and Mahony, R. and Andrews, B.},
	doi = {10.1137/040605266},
	fjournal = {SIAM Journal on Optimization},
	issn = {1052-6234},
	journal = {SIAM J. Optim.},
	mrclass = {90C26 (37N40 65K10 90C52)},
	mrnumber = {2197994},
	mrreviewer = {Oliver Stein},
	number = {2},
	pages = {531--547},
	title = {Convergence of the iterates of descent methods for analytic cost functions},
	url = {https://doi.org/10.1137/040605266},
	volume = {16},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1137/040605266}}

@article{attouch2009convergence,
	author = {Attouch, Hedy and Bolte, J{\'e}r{\^o}me},
	doi = {10.1007/s10107-007-0133-5},
	fjournal = {Mathematical Programming. A Publication of the Mathematical Programming Society},
	issn = {0025-5610},
	journal = {Math. Program.},
	mrclass = {90C30 (65K05)},
	mrnumber = {2421270},
	mrreviewer = {W. W. Breckner},
	number = {1-2, Ser. B},
	pages = {5--16},
	title = {On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
	url = {https://doi.org/10.1007/s10107-007-0133-5},
	volume = {116},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1007/s10107-007-0133-5}}

@article{attouch2010proximal,
	author = {Attouch, H{\'e}dy and Bolte, J{\'e}r{\^o}me and Redont, Patrick and Soubeyran, Antoine},
	doi = {10.1287/moor.1100.0449},
	fjournal = {Mathematics of Operations Research},
	issn = {0364-765X},
	journal = {Math. Oper. Res.},
	mrclass = {90C26 (49J52 65K10)},
	mrnumber = {2674728},
	number = {2},
	pages = {438--457},
	title = {Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the {K}urdyka-{{\L}}ojasiewicz inequality},
	url = {https://doi.org/10.1287/moor.1100.0449},
	volume = {35},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1287/moor.1100.0449}}

@article{attouch2013convergence,
	abstract = {In view of the minimization of a nonsmooth nonconvex function f, we prove an abstract convergence result for descent methods satisfying a sufficient-decrease assumption, and allowing a relative error tolerance. Our result guarantees the convergence of bounded sequences, under the assumption that the function f satisfies the Kurdyka--{\L}ojasiewicz inequality. This assumption allows to cover a wide range of problems, including nonsmooth semi-algebraic (or more generally tame) minimization. The specialization of our result to different kinds of structured problems provides several new convergence results for inexact versions of the gradient method, the proximal method, the forward--backward splitting algorithm, the gradient projection and some proximal regularization of the Gauss--Seidel method in a nonconvex setting. Our results are illustrated through feasibility problems, or iterative thresholding procedures for compressive sensing.},
	author = {Attouch, Hedy and Bolte, J{\'e}r{\^o}me and Svaiter, Benar Fux},
	day = {01},
	doi = {10.1007/s10107-011-0484-9},
	issn = {1436-4646},
	journal = {Mathematical Programming},
	month = {Feb},
	number = {1},
	pages = {91-129},
	title = {Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward--backward splitting, and regularized Gauss--Seidel methods},
	url = {https://doi.org/10.1007/s10107-011-0484-9},
	volume = {137},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s10107-011-0484-9}}

@article{haraux1998convergence,
	author = {Haraux, A. and Jendoubi, M. A.},
	doi = {10.1006/jdeq.1997.3393},
	fjournal = {Journal of Differential Equations},
	issn = {0022-0396},
	journal = {J. Differential Equations},
	mrclass = {35L70 (35G20)},
	mrnumber = {1616968},
	number = {2},
	pages = {313--320},
	title = {Convergence of solutions of second-order gradient-like systems with analytic nonlinearities},
	url = {https://doi.org/10.1006/jdeq.1997.3393},
	volume = {144},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1006/jdeq.1997.3393}}

@article{huang2001convergence,
	author = {Huang, Sen-Zhong and Tak\'{a}\v{c}, Peter},
	doi = {10.1016/S0362-546X(00)00145-0},
	fjournal = {Nonlinear Analysis. Theory, Methods \& Applications. An International Multidisciplinary Journal},
	issn = {0362-546X},
	journal = {Nonlinear Anal.},
	mrclass = {35K55 (35B40 35K40)},
	mrnumber = {1857152},
	mrreviewer = {Song Mu Zheng},
	number = {5, Ser. A: Theory Methods},
	pages = {675--698},
	title = {Convergence in gradient-like systems which are asymptotically autonomous and analytic},
	url = {https://doi.org/10.1016/S0362-546X(00)00145-0},
	volume = {46},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1016/S0362-546X(00)00145-0}}

@article{chill2003convergence,
	author = {Chill, R. and Jendoubi, M. A.},
	doi = {10.1016/S0362-546X(03)00037-3},
	fjournal = {Nonlinear Analysis. Theory, Methods \& Applications. An International Multidisciplinary Journal},
	issn = {0362-546X},
	journal = {Nonlinear Anal.},
	mrclass = {34D05 (34G10 35K55 35L70 47D06)},
	mrnumber = {1978032},
	mrreviewer = {Min He},
	number = {7-8},
	pages = {1017--1039},
	title = {Convergence to steady states in asymptotically autonomous semilinear evolution equations},
	url = {https://doi.org/10.1016/S0362-546X(03)00037-3},
	volume = {53},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1016/S0362-546X(03)00037-3}}

@article{jendoubi1998simple,
	author = {Jendoubi, Mohamed Ali},
	doi = {10.1006/jfan.1997.3174},
	fjournal = {Journal of Functional Analysis},
	issn = {0022-1236},
	journal = {J. Funct. Anal.},
	mrclass = {35K55 (35B40 35J60 35L70)},
	mrnumber = {1609269},
	mrreviewer = {Qing Fang},
	number = {1},
	pages = {187--202},
	title = {A simple unified approach to some convergence theorems of {L}. {S}imon},
	url = {https://doi.org/10.1006/jfan.1997.3174},
	volume = {153},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1006/jfan.1997.3174}}

@article{simon1983asymptotics,
	author = {Simon, Leon},
	doi = {10.2307/2006981},
	fjournal = {Annals of Mathematics. Second Series},
	issn = {0003-486X},
	journal = {Ann. of Math. (2)},
	mrclass = {58G11 (35B40 49F99 58E20)},
	mrnumber = {727703},
	mrreviewer = {Helmut Kaul},
	number = {3},
	pages = {525--571},
	title = {Asymptotics for a class of nonlinear evolution equations, with applications to geometric problems},
	url = {https://doi.org/10.2307/2006981},
	volume = {118},
	year = {1983},
	bdsk-url-1 = {https://doi.org/10.2307/2006981}}

@inproceedings{zhang2021suboptimality,
	author = {Zhang, Guodong and Wang, Yuanhao},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	organization = {PMLR},
	pages = {2098--2106},
	title = {On the suboptimality of negative momentum for minimax optimization},
	year = {2021}}

@article{huang2021efficient,
	author = {Huang, Feihu and Wu, Xidong and Huang, Heng},
	journal = {Advances in Neural Information Processing Systems},
	title = {Efficient mirror descent ascent methods for nonsmooth minimax problems},
	volume = {34},
	year = {2021}}

@article{beck2009fast,
	author = {Beck, Amir and Teboulle, Marc},
	journal = {SIAM journal on imaging sciences},
	number = {1},
	pages = {183--202},
	publisher = {SIAM},
	title = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	volume = {2},
	year = {2009}}

@article{ghadimi2016accelerated,
	author = {Ghadimi, Saeed and Lan, Guanghui},
	journal = {Mathematical Programming},
	number = {1-2},
	pages = {59--99},
	publisher = {Springer},
	title = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
	volume = {156},
	year = {2016}}

@inproceedings{nesterov1983method,
	author = {Nesterov, Yurii E},
	booktitle = {Dokl. akad. nauk Sssr},
	pages = {543--547},
	title = {A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
	volume = {269},
	year = {1983}}

@article{polyak1964some,
	author = {Polyak, Boris T},
	journal = {Ussr computational mathematics and mathematical physics},
	number = {5},
	pages = {1--17},
	publisher = {Elsevier},
	title = {Some methods of speeding up the convergence of iteration methods},
	volume = {4},
	year = {1964}}

@article{goh2017momentum,
	author = {Goh, Gabriel},
	journal = {Distill},
	number = {4},
	pages = {e6},
	title = {Why momentum really works},
	volume = {2},
	year = {2017}}

@inproceedings{sutskever2013importance,
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	booktitle = {International conference on machine learning},
	organization = {PMLR},
	pages = {1139--1147},
	title = {On the importance of initialization and momentum in deep learning},
	year = {2013}}

@article{nguyen2020momentumrnn,
	author = {Nguyen, Tan M and Baraniuk, Richard G and Bertozzi, Andrea L and Osher, Stanley J and Wang, Bao},
	journal = {arXiv preprint arXiv:2006.06919},
	title = {Momentumrnn: Integrating momentum into recurrent neural networks},
	year = {2020}}

@misc{chen2021accelerated,
	archiveprefix = {arXiv},
	author = {Ziyi Chen and Shaocong Ma and Yi Zhou},
	eprint = {2112.11663},
	primaryclass = {cs.LG},
	title = {Accelerated Proximal Alternating Gradient-Descent-Ascent for Nonconvex Minimax Machine Learning},
	year = {2021}}

@inproceedings{agarwal2012distributed,
	author = {Agarwal, Alekh and Duchi, John C},
	booktitle = {2012 IEEE 51st IEEE Conference on Decision and Control (CDC)},
	organization = {IEEE},
	pages = {5451--5452},
	title = {Distributed delayed stochastic optimization},
	year = {2012}}

@article{feyzmahdavian2016asynchronous,
	author = {Feyzmahdavian, Hamid Reza and Aytekin, Arda and Johansson, Mikael},
	journal = {IEEE Transactions on Automatic Control},
	number = {12},
	pages = {3740--3754},
	publisher = {IEEE},
	title = {An asynchronous mini-batch algorithm for regularized stochastic optimization},
	volume = {61},
	year = {2016}}

@inproceedings{liu2014asynchronous,
	author = {Liu, Ji and Wright, Steve and R{\'e}, Christopher and Bittorf, Victor and Sridhar, Srikrishna},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {469--477},
	title = {An asynchronous parallel stochastic coordinate descent algorithm},
	year = {2014}}

@article{peng2016arock,
	author = {Peng, Zhimin and Xu, Yangyang and Yan, Ming and Yin, Wotao},
	journal = {SIAM Journal on Scientific Computing},
	number = {5},
	pages = {A2851--A2879},
	publisher = {SIAM},
	title = {Arock: an algorithmic framework for asynchronous parallel coordinate updates},
	volume = {38},
	year = {2016}}

@article{glowinski1975approximation,
	author = {Glowinski, Roland and Marroco, Americo},
	journal = {ESAIM: Mathematical Modelling and Numerical Analysis-Mod{\'e}lisation Math{\'e}matique et Analyse Num{\'e}rique},
	number = {R2},
	pages = {41--76},
	title = {Sur l'approximation, par {\'e}l{\'e}ments finis d'ordre un, et la r{\'e}solution, par p{\'e}nalisation-dualit{\'e} d'une classe de probl{\`e}mes de Dirichlet non lin{\'e}aires},
	volume = {9},
	year = {1975}}

@article{gabay1976dual,
	author = {Gabay, Daniel and Mercier, Bertrand},
	journal = {Computers \& mathematics with applications},
	number = {1},
	pages = {17--40},
	publisher = {Elsevier},
	title = {A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
	volume = {2},
	year = {1976}}

@article{zhu2008efficient,
	author = {Zhu, Mingqiang and Chan, Tony},
	journal = {UCLA Cam Report},
	pages = {8--34},
	title = {An efficient primal-dual hybrid gradient algorithm for total variation image restoration},
	volume = {34},
	year = {2008}}

@article{chambolle2011first,
	author = {Chambolle, Antonin and Pock, Thomas},
	journal = {Journal of mathematical imaging and vision},
	number = {1},
	pages = {120--145},
	publisher = {Springer},
	title = {A first-order primal-dual algorithm for convex problems with applications to imaging},
	volume = {40},
	year = {2011}}

@article{he2014convergence,
	author = {He, Bingsheng and You, Yanfei and Yuan, Xiaoming},
	journal = {SIAM Journal on Imaging Sciences},
	number = {4},
	pages = {2526--2537},
	publisher = {SIAM},
	title = {On the convergence of primal-dual hybrid gradient algorithm},
	volume = {7},
	year = {2014}}

@article{hinton2012deep,
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
	doi = {10.1109/MSP.2012.2205597},
	journal = {IEEE Signal Processing Magazine},
	number = {6},
	pages = {82-97},
	title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
	volume = {29},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/MSP.2012.2205597}}

@inproceedings{krizhevsky2012imagenet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	volume = {25},
	year = {2012},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}}

@inproceedings{an2018pid,
	author = {An, Wangpeng and Wang, Haoqian and Sun, Qingyun and Xu, Jun and Dai, Qionghai and Zhang, Lei},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages = {8522--8531},
	title = {A PID controller approach for stochastic optimization of deep networks},
	year = {2018}}

@inproceedings{kidambi2018insufficiency,
	author = {Kidambi, Rahul and Netrapalli, Praneeth and Jain, Prateek and Kakade, Sham},
	booktitle = {2018 Information Theory and Applications Workshop (ITA)},
	organization = {IEEE},
	pages = {1--9},
	title = {On the insufficiency of existing momentum schemes for stochastic optimization},
	year = {2018}}

@inproceedings{ma2018quasihyperbolic,
	author = {Jerry Ma and Denis Yarats},
	booktitle = {International Conference on Learning Representations},
	title = {Quasi-hyperbolic momentum and Adam for deep learning},
	url = {https://openreview.net/forum?id=S1fUpoR5FQ},
	year = {2019},
	bdsk-url-1 = {https://openreview.net/forum?id=S1fUpoR5FQ}}

@inproceedings{farnia2021train,
	author = {Farnia, Farzan and Ozdaglar, Asuman},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {3174--3185},
	title = {Train simultaneously, generalize better: Stability of gradient-based minimax learners},
	year = {2021}}

@incollection{robbins1971convergence,
	author = {Robbins, Herbert and Siegmund, David},
	booktitle = {Optimizing methods in statistics},
	pages = {233--257},
	publisher = {Elsevier},
	title = {A convergence theorem for non negative almost supermartingales and some applications},
	year = {1971}}

@inproceedings{karimi2016linear,
	author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
	booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	organization = {Springer},
	pages = {795--811},
	title = {Linear convergence of gradient and proximal-gradient methods under the polyak-{{\L}}ojasiewicz condition},
	year = {2016}}

@article{fiez2021global,
	author = {Fiez, Tanner and Ratliff, Lillian and Mazumdar, Eric and Faulkner, Evan and Narang, Adhyyan},
	journal = {Advances in Neural Information Processing Systems},
	pages = {29049--29063},
	title = {Global convergence to local minmax equilibrium in classes of nonconvex zero-sum games},
	volume = {34},
	year = {2021}}

@article{boct2020alternating,
	author = {Bo{\c{t}}, Radu Ioan and B{\"o}hm, Axel},
	journal = {arXiv preprint arXiv:2007.13605},
	title = {Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems},
	year = {2020}}

@inproceedings{lin2020near,
	author = {Lin, Tianyi and Jin, Chi and Jordan, Michael I},
	booktitle = {Conference on Learning Theory},
	organization = {PMLR},
	pages = {2738--2779},
	title = {Near-optimal algorithms for minimax optimization},
	year = {2020}}

@article{daskalakis2018limit,
	author = {Daskalakis, Constantinos and Panageas, Ioannis},
	journal = {Advances in neural information processing systems},
	title = {The limit points of (optimistic) gradient descent in min-max optimization},
	volume = {31},
	year = {2018}}

@article{mazumdar2020gradient,
	author = {Mazumdar, Eric and Ratliff, Lillian J and Sastry, S Shankar},
	journal = {SIAM Journal on Mathematics of Data Science},
	number = {1},
	pages = {103--131},
	publisher = {SIAM},
	title = {On gradient-based learning in continuous games},
	volume = {2},
	year = {2020}}

@book {rockafellar_convex_1970,
    AUTHOR = {Rockafellar, R. Tyrrell},
     TITLE = {Convex analysis},
    SERIES = {Princeton Mathematical Series, No. 28},
 PUBLISHER = {Princeton University Press, Princeton, N.J.},
      YEAR = {1970},
     PAGES = {xviii+451},
   MRCLASS = {26.52 (46.00)},
  MRNUMBER = {0274683},
MRREVIEWER = {Ky Fan},
}

@inproceedings{xian_2021_faster,
 author = {Xian, Wenhan and Huang, Feihu and Zhang, Yanfu and Huang, Heng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25865--25877},
 publisher = {Curran Associates, Inc.},
 title = {A Faster Decentralized Algorithm for Nonconvex Minimax Problems},
 url = {https://proceedings.neurips.cc/paper/2021/file/d994e3728ba5e28defb88a3289cd7ee8-Paper.pdf},
 volume = {34},
 year = {2021}
}


@InProceedings{sharma_2022_federated,
  title = 	 {Federated Minimax Optimization: Improved Convergence Analyses and Algorithms},
  author =       {Sharma, Pranay and Panda, Rohan and Joshi, Gauri and Varshney, Pramod},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19683--19730},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/sharma22c/sharma22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/sharma22c.html},
  abstract = 	 {In this paper, we consider nonconvex minimax optimization, which is gaining prominence in many modern machine learning applications, such as GANs. Large-scale edge-based collection of training data in these applications calls for communication-efficient distributed optimization algorithms, such as those used in federated learning, to process the data. In this paper, we analyze local stochastic gradient descent ascent (SGDA), the local-update version of the SGDA algorithm. SGDA is the core algorithm used in minimax optimization, but it is not well-understood in a distributed setting. We prove that Local SGDA has <em>order-optimal</em> sample complexity for several classes of nonconvex-concave and nonconvex-nonconcave minimax problems, and also enjoys <em>linear speedup</em> with respect to the number of clients. We provide a novel and tighter analysis, which improves the convergence and communication guarantees in the existing literature. For nonconvex-PL and nonconvex-one-point-concave functions, we improve the existing complexity results for centralized minimax problems. Furthermore, we propose a momentum-based local-update algorithm, which has the same convergence guarantees, but outperforms Local SGDA as demonstrated in our experiments.}
}

@inproceedings{luo_2020_advances,
 author = {Luo, Luo and Ye, Haishan and Huang, Zhichao and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20566--20577},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems},
 url = {https://proceedings.neurips.cc/paper/2020/file/ecb47fbb07a752413640f82a945530f8-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{
li_2022_convergence,
  title = 	 {On Convergence of Gradient Descent Ascent: A Tight Local Analysis},
  author =       {Li, Haochuan and Farnia, Farzan and Das, Subhro and Jadbabaie, Ali},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12717--12740},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/li22e/li22e.pdf},
  url = 	 {https://proceedings.mlr.press/v162/li22e.html},
  abstract = 	 {Gradient Descent Ascent (GDA) methods are the mainstream algorithms for minimax optimization in generative adversarial networks (GANs). Convergence properties of GDA have drawn significant interest in the recent literature. Specifically, for $\min_{x} \max_{y} f(x;y)$ where $f$ is strongly-concave in $y$ and possibly nonconvex in $x$, (Lin et al., 2020) proved the convergence of GDA with a stepsize ratio $\eta_y/\eta_x=\Theta(\kappa^2)$ where $\eta_x$ and $\eta_y$ are the stepsizes for $x$ and $y$ and $\kappa$ is the condition number for $y$. While this stepsize ratio suggests a slow training of the min player, practical GAN algorithms typically adopt similar stepsizes for both variables, indicating a wide gap between theoretical and empirical results. In this paper, we aim to bridge this gap by analyzing the <em>local convergence</em> of general <em>nonconvex-nonconcave</em> minimax problems. We demonstrate that a stepsize ratio of $\Theta(\kappa)$ is necessary and sufficient for local convergence of GDA to a Stackelberg Equilibrium, where $\kappa$ is the local condition number for $y$. We prove a nearly tight convergence rate with a matching lower bound. We further extend the convergence guarantees to stochastic GDA and extra-gradient methods (EG). Finally, we conduct several numerical experiments to support our theoretical findings.}
}

@inproceedings{
lee2021fast,
title={Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems},
author={Sucheol Lee and Donghwan Kim},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=AYAgKFl78z}
}


@InProceedings{diakonikolas_2021_efficient,
  title = 	 { Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization },
  author =       {Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2746--2754},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/diakonikolas21a/diakonikolas21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/diakonikolas21a.html},
  abstract = 	 { The use of min-max optimization in the adversarial training of deep neural network classifiers, and the training of generative adversarial networks has motivated the study of nonconvex-nonconcave optimization objectives, which frequently arise in these applications. Unfortunately, recent results have established that even approximate first-order stationary points of such objectives are intractable, even under smoothness conditions, motivating the study of min-max objectives with additional structure. We introduce a new class of structured nonconvex-nonconcave min-max optimization problems, proposing a generalization of the extragradient algorithm which provably converges to a stationary point. The algorithm applies not only to Euclidean spaces, but also to general $\ell_p$-normed finite-dimensional real vector spaces. We also discuss its stability under stochastic oracles and provide bounds on its sample complexity. Our iteration complexity and sample complexity bounds either match or improve the best known bounds for the same or less general nonconvex-nonconcave settings, such as those that satisfy variational coherence or in which a weak solution to the associated variational inequality problem is assumed to exist. }
}
