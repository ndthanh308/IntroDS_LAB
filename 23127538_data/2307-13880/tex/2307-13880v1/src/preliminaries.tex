\section{Preliminaries} \label{sec: preliminaries} 

% In this section, we summarize some preliminaries, define some notations for simplifying the presentation of further analysis, and give some basic results.

\subsection{Notations}

Throughout this paper, we let $\| \cdot \| := \sqrt{\langle \cdot, \cdot \rangle}$ denote the $\ell_2$ norm, and $\langle \cdot, \cdot \rangle$ denote the inner product in the Euclidean space. 
% For any operator $A :\r^m \to \r^n$, $JA$ is denoted by the Jacobian of $A$.
We are interested in the minimax problem of this form: 
\begin{equation} \label{problem: minimax problem}
    \min_{x \in \r^m} \max_{y \in \r^n} F(x, y) := \e_z [f(x, y; z)],
\end{equation}
where $z$ is a random variable obeying a distribution $\d$ and $F$ is nonconvex in $x$ for any fixed $y$ and possibly nonconcave in $y$.
Following \citep{jin2020local}, we define $\phi (x) := \arg\max_y F(x, y)$.
Function $\phi$ plays a bridge between the inner problem in $y$ and the outer problem in $x$.

% For ease of notation, we introduce some notations. Define 
% \begin{equation*}
%     u = \begin{pmatrix}
%     x \\ 
%     y
%     \end{pmatrix} \quad 
%     G(u; \alpha, \eta) = \begin{pmatrix}
%     \alpha \nabla_x F(x, y) \\
%     - \eta \nabla_y F(x, y)
%     \end{pmatrix}
% \end{equation*}

\begin{definition}[Smooth function] \label{def: smooth}
We say a function $g$ is $L$-smooth with $L \geq 0$, if it is differentiable and its gradient $\nabla f$ is $L$-Lipschitz continuous.
\end{definition}

\begin{definition}[$\mu$-strongly convexity]
A differentiable function $g : \r^d \to \r$ is called $\mu$-strongly convex if  
\begin{equation}
    g (y) \geq g(x) + \langle \nabla g(x), y - x \rangle + \frac{\mu}{2} \| y - x \|^2.
\end{equation}
\end{definition}
One can easily extend this concept to the minimax problem, where $F(x, y)$ is called $\mu$-strongly-convex-strongly-concave (SCSC), if $F(\cdot, y)$ is $\mu$-strongly convex for any fixed $y$ and  $-F(x, \cdot)$ is $\mu$-strongly convex for any fixed $x$. 

% \lizn{I think here we need max-oracle definition and some illustrations for it.}

% \begin{definition}[Max-oracle]
% We define function $\phi (x) := \max_y F(x, y)$. Then problem (\ref{problem: minimax problem}) is equivalent to find the minimization of function $\phi$.
% \end{definition}
% \yy{this definition is kind of incomplete to me. Also, if the definitions are from existing work, you may need to insert the references.}
% \begin{definition}
    
% For a differentiable function $F$, 
% \begin{itemize}
%     \item A point $(x^*, y^*)$ is a critical point of $F$ if $\nabla F(x^*, y^*) = 0$.
%     \item A critical point $(x^*, y^*)$ is a local minimax point if there exists a neighborhood $U$ around $(x^*, y^*)$ so that for all $(x, y) \in U$ we have that $F(x^*, y) \leq F(x^*, y^*) \leq (x, y^*)$.
% \end{itemize}

% \end{definition}

\subsection{Optimality}
Generally, there are two reasonable solutions to problem~\eqref{problem: stochastic minimax problem} worthy to be noted. 
In simultaneous games, 
one often seeks a Nash equilibrium $(x^*, y^*)$, in which $x^*$ is a global minimum of $F(\cdot, y^*)$ and $y^*$ is a global maximum of $F(x^*, \cdot)$. 
On the other hand, many recent machine learning tasks focus on sequential games, e.g., adversarial training and generative adversarial network, and aim to achieve a Stackelberg equilibrium $(x^*, y^*)$, in which $x^*$ is a global minimum of $\phi(\cdot)$ and $y^*$ is a global maximum of $F(x^*, \cdot)$. 
The Stackelberg equilibrium is also called the global minimax point in some literature~\citep{jin2020local}. 

However, most of the minimax problems arising in machine learning applications are nonconvex in $x$ and nonconcave in $y$, making finding either a Nash equilibrium or a Stackelberg equilibrium impractical. 
This motivates the quest to propose two different notions of local optimality, i.e., local Nash  equilibrium~\citep{daskalakis2018limit, mazumdar2020gradient} and local Stackelberg  equilibrium~\citep{jin2020local}. 
In a nutshell, 
a point $(x^*, y^*)$ is called a local Nash equilibrium if $x^*$ is a local minimum of $F(\cdot, y^*)$ and $y^*$ is a local maximum of $F(x^*, \cdot)$. 
Similarly,  a point $(x^*, y^*)$ is called a local Stackelberg equilibrium if $x^*$ is a local minimum of $\phi (\cdot)$ and $y^* \in \arg\max_y F(x^*, y)$.

However, 
in nonconvex and nonconcave setting, %(or large-scale setting?)
verifying such local equilibrium is still extremely hard since it often requires second-order optimality condition. 
Hence, we instead find a solution that satisfies first-order necessary condition: 
a Stackelberg-type stationary point $(x^*, y^*)$ if $\nabla \phi (x^*) = 0$ and $y^* \in \arg\min_y F(x^*, y)$, and a Nash-type stationary point $(x^*, y^*)$ if $\nabla F(x^*, y^*) = 0$. 
It should be noted that these two kinds of stationary points may not coincide. For example, considering $F(x,y)=xy+x^3$ with $y \in [-2,2]$, one can observe that $(0,0)$ is only the Nash-type stationary point but not the Stackelberg-type one.

% A straightforward observation indicates that a local Stackelberg equilibrium is a Stackelberg type stationary point and a local Nash equilibrium is a Nash type stationary point. 

\subsection{Assumptions}

The following two assumptions are effective throughout, which are standard in stochastic optimization.

\begin{assumption}[Smoothness] \label{assum: smoothness}

The objective function $F$ is $L_1$-smooth.

\end{assumption}

\begin{assumption}[Sampling rule] \label{assum: bounded variance}
For any $x, y$, the gradient estimation of $F$ is unbiased: 
\begin{equation}
    \e_z [\nabla f(x,y;z)] = \nabla F(x, y).
\end{equation}
and its variance is bounded, i.e., there exists a positive constant $\sigma$, such that for all $(x, y)$, 
\begin{equation}
    \e_z \left[ \| \nabla F(x, y) - \nabla f(x, y; z) \|^2 \right] \leq \sigma^2.
\end{equation}
\end{assumption}

Besides the assumptions above, we further assume the following property of the variable $y$.

\begin{assumption}[P{\L} condition in $y$] \label{assum: ncpl condtion}
For any fixed $x$, the maximization $\max_{y} F(x, y)$ has a nonempty solution set and a finite optimal value. Furthermore, there exists $\mu > 0$ such that 
\begin{equation}
    \| \nabla_y F(x, y) \|^2 \geq 2 \mu [\max_y F(x, y) - F(x, y)]
\end{equation}
holds for all $y$.
\end{assumption}

% The P{\L} condition was originally introduced in \citet{polyak1963gradient} for the purpose to guarantee global convergence of gradient descent at a linear rate. 
% Roughly speaking, P{\L} condition describes the sharpness of a function $g$ up to a reparametrization~\cite{lojasiewicz1963propriete}.\yy{the above sentence has appeared in related work}
It should be noted P{\L} condition is an independent property of convexity. 
In other words, there exists nonconvex function $g$ satisfies the P{\L} condition. 
However, the P{\L} condition holds for any strongly convex function $g$, 
and thus P{\L} condition can be viewed as a non-trivial generalization of strong convexity.
 
% On the other hand, the P{\L} condition is a special case of the famous {\L}ojasiewicz inequality. 
% The {\L}ojasiewicz inequality, 
% % which is named after S.{\L}ojasiewicz (see \cite{lojasiewicz1963propriete}), 
% roughly speaking, describes the sharpness of a function $f$ up to a reparametrization~\cite{lojasiewicz1963propriete}. 
% A generalized form of this inequality, which nowadays is called Kurdyka-{\L}ojasiewicz property, has been introduced by Kurdyka in \cite{kurdyka1998gradients}. \lizn{Then?} 
% The reader can refer to \cite{bolte2007lojasiewicz, bolte2007clarke} for non-smooth cases. 
% K\L-property has been successfully used in various fields of mathematics: minimization and algorithms \cite{absil2005convergence, attouch2009convergence, attouch2010proximal, attouch2013convergence, haraux1998convergence, huang2001convergence}; asymptotic theory of partial differential equations \cite{chill2003convergence, jendoubi1998simple, simon1983asymptotics}; among others.

A direct result derived by P{\L} condition is the smoothness of $\phi$.  
Generally, although $F$ is a smooth function, $\phi$ is not so, and even may not be differentiable. 
In this case, it is nearly impossible to define the stationary point of $\phi$. 
However, by combining Assumption~\ref{assum: smoothness} and Assumption~\ref{assum: ncpl condtion}, we can obtain that $\phi$ is $L_2$-smooth with $L_2 = L_1 + \frac{L_1 \kappa}{2}$ and $\kappa = L_1 / \mu$. 
Here $\kappa$ is also referred to as the conditional number of problem~\eqref{problem: minimax problem} \citep{nouiehed_solving_2019}.
% Precisely, we have the following result.

% \begin{lemma}[\citet{nouiehed_solving_2019}] \label{lemma: 1}
% Under  Assumption \ref{assum: smoothness} and \ref{assum: ncpl condtion}, $\phi$ is $L_2$ smooth with $L_2 = L_1 + \frac{L_1 \kappa}{2}$ and $\kappa = L_1 / \mu$.
% \end{lemma}

\subsection{SGDA and its variants}

In this subsection, we provide the formal formulations of SGDA, SGDmax, ESGDA, and RSGDA in Algorithm \ref{algo: rsgda}.
% First, we recall the recursive iterations of well-known SGDA, SGDmax, and RSGDA.
For details, 
at the \textit{k}-th iteration, $z_k$ is first sampled from $\mathcal{D}$. 
Next, 
SGDA takes one descent step in $x$ along $\nabla_x f(x_k, y_k; z_k)$ and one ascent step in $y$ along $\nabla_x f(x_k, y_k; z_k)$; 
SGDmax takes the descent step in $x$ along $\nabla_x f(x_k, y_k; z_k)$ and calculates the maximum of $y$ for $f(x_k, y; z_k)$; 
ESGDA takes $m$ gradient ascent steps in $y$ and one gradient descent step in $x$.
For RSGDA, it 
% We now introduce RSGDA in detail. 
% At each iteration $k$, we first sample $z_k \sim \d$. 
% Then, we 
takes the descent step in $x$ along the stochastic gradient $\nabla_x f(x_k, y_k; z_k)$ with probability $p$ or the gradient ascent step in $y$ along the stochastic gradient $\nabla_x f(x_k, y_k; z_k)$ with probability $1 - p$. 
% The details is summarized in Algorithm~\ref{algo: rsgda}.

\begin{breakablealgorithm}
\caption{SGDmax/ SGDA/ ESGDA/ RSGDA}  \label{algo: rsgda}

\textbf{Inputs:} initial points $x_0, y_0$, step sizes $\{ (\alpha_k, \eta_k) \}_{k=1}^\infty$, loop size $m$, max-oracle accuracy $\delta$, constant parameter $p$.

\begin{algorithmic}
\FOR{$k = 0, 1, 2, \dots$}
\STATE \underline{\textbf{SGDmax}}:

\quad Find $y_{k+1}$, s.t. $F(x_k, y_{k+1}) \geq \phi (x_k) + \delta$;

\quad Sample $z_k \sim \mathcal{D}$;

\quad $x_{k+1} = x_k - \alpha_k \nabla_x f(x_k, y_{k+1}; z_k)$;
\STATE \underline{\textbf{SGDA}}:

\quad Sample $z_k \sim \mathcal{D}$;

\quad $y_{k+1} = y_k + \eta_k \nabla_y f(x_k, y_k; z_k)$;

\quad $x_{k+1} = x_k - \alpha_k \nabla_x f(x_k, y_{k+1}; z_k)$;
\STATE \underline{\textbf{ESGDA}}:
\FOR{$t=0, \dots, m$}
\STATE Sample $z_k^t \sim \mathcal{D}$;
\STATE $y_k^{t+1} = y_k + \eta_k \nabla_y f(x_k, y_k^t; z_k^t)$;
\ENDFOR

\quad $y_{k+1} = y_k^{m+1}$

\quad Sample $z_k \sim \mathcal{D}$;

\quad $x_{k+1} = x_k - \alpha_k \nabla_x f(x_k, y_{k+1}; z_k)$;

\STATE \underline{\textbf{RSGDA}}:

\quad Sample $z_k \sim \mathcal{D}$;

\quad With probability $p$: 
\begin{equation*}
    x_{k+1} = x_k - \alpha_k \nabla_x f(x_k, y_k; z_k); 
\end{equation*}

\quad With probability $1-p$:
\begin{equation*}
y_{k+1} = y_k + \eta_k \nabla_y f (x_k, y_k; z_k);
\end{equation*}
% \begin{equation*}
% (x_{k+1}, y_{k+1}) = 
% \begin{dcases}
%     (x_k^+, y_k), & \quad \text{w.p.} \quad p, \\ 
%     (x_k, y_k^+), & \quad \text{w.p.} \quad 1 - p.
% \end{dcases}
% \end{equation*}

\ENDFOR
\end{algorithmic}

\end{breakablealgorithm}

% \begin{algorithm}
% \caption{Randomized stochastic gradient descent ascent (RSGDA)}  \label{algo: rsgda}

% Inputs: constant parameter $p \in (0, 1)$, initial points $x_0, y_0$, step sizes $\{ \alpha_k \}_{k=1}^\infty$ and $\{ \eta_k \}_{k=1}^\infty$.

% \begin{algorithmic}
% \FOR{$k = 0, 1, 2, \dots$}
% \STATE Sample $z_k \sim \mathcal{D}$;
% \STATE Compute
% \begin{equation*}
% \begin{aligned}
%     & x_k^+ = x_k - \alpha_k \nabla_x f(x_k, y_k; z_k); \\
%     & y_k^+ = y_k + \eta_k \nabla_y f (x_k, y_k; z_k);
% \end{aligned}
% \end{equation*}
% \STATE Update 
% \begin{equation*}
% (x_{k+1}, y_{k+1}) = 
%     \begin{dcases}
%         (x_k^+, y_k), & \quad \text{w.p.} \quad p, \\ 
%         (x_k, y_k^+), & \quad \text{w.p.} \quad 1 - p.
%     \end{dcases}
%     \end{equation*}
% \ENDFOR
% \end{algorithmic}

% \end{algorithm}

% In this subsection, we provide two technical results used in the proofs. 

% Generally, $\phi$ needs not to be smooth even when $F$ is a smooth function, which makes it impossible to define the stationary point of $\phi$. However, by combining Assumption~\ref{assum: smoothness} and Assumption~\ref{assum: ncpl condtion}, we can obtain the smoothness of $\phi$. Precisely, we have the following result.
% \lizn{Maybe using proposition instead of lemma?}

% \begin{lemma}[\citet{nouiehed_solving_2019}] \label{lemma: 1}
% Under  Assumption \ref{assum: smoothness} and \ref{assum: ncpl condtion}, $\phi$ is $L_2$ smooth with $L_2 = L_1 + \frac{L_1 \kappa}{2}$ and $\kappa = L_1 / \mu$.
% \end{lemma}
% Here $\kappa$ is also called the conditional number of problem~\eqref{problem: minimax problem}.

% Moreover, for a function $g$ with the P{\L} condition and any point $x$, the distance between $x$ and the optimal set of $g$ is bounded by $\| \nabla g(x) \|^2$.

% To prove the convergence of RSGDA, we still need to introduce a technical result called Robbins-Siegmumd theorem as follows.
% \begin{lemma}[\citet{robbins1971convergence}]
% Consider a filtration $\{ \mathcal{F}_k \}_k$, three non-negative sequences of $\{\mathcal{F}_k \}_k$-adapted processes $\{ V_k \}_k$, $\{ U_k \}_k$ and $\{ Z_k \}_k$, such that $\sum_k Z_k < + \infty$ almost surly, and 
% \begin{equation}
%     \e_k [V_{k+1} \mid \mathcal{F}_k] + U_{k+1} \leq V_k + Z_k, \quad \forall k \geq 0.
% \end{equation}
% Then $\{ V_k \}_k$ converges and $\sum_k U_k < + \infty$ almost surly.
% \end{lemma}

