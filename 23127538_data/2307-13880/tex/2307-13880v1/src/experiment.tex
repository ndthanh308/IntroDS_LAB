\section{Numerical experiments} \label{sec: experiment}
We conduct experiments on both synthetic data and real data.\footnote{{The code, together with the corresponding dataset, are uploaded to \url{https://figshare.com/s/207ab3663e2acd28d8dd}.}} The experiments are mainly designed to answer the following questions:
\begin{itemize}
    \item Does RSGDA perform consistently with ESGDA?
    %\item Is RSGDA as fast as SGDA as indicated by the theoretical result?
    \item Can RSGDA outperform ESGDA under certain circumstances?
    \item How efficient is the proposed selection strategy for probability $p$ in RSGDA?
\end{itemize}


\subsection{Experiments on synthetic data}

\subsubsection{MLP Wasserstein GANs.} 
Following the problem setting of~\citet{loizou2020stochastic}, we first use the WGAN to approximate a given multivariate Gaussian distribution.
We consider the WGAN task in which the discriminator and generator are both modeled with MLPs~\citep{lei2020sgd}. 
%Following the problem setting of~\citet{loizou2020stochastic}, 
%we use WGAN to approximate a given multivariate Gaussian distribution. 
Elaborately, the real data is drawn from a normal distribution $\mathcal{N}(\mu^*, \sigma^*)$ where $\mu^*=(0.5; -1.5)$ and $\sigma^*=(0.1; 0.3)$, and the fake data is generated by a generator denoted by $g_{\theta}(z)$, where $z$ is drawn from the standard Gaussian distribution.
The discriminator is defined as $f_{w}(x)$, where input $x$ can be either a real example or a fake example. 
The minimax problem of this task can be formulated as
\begin{equation*}
\min_{\theta} \max_{w} F(w, \theta) := ~\e_{x, z} [f_{w}(x) - f_{w}(g_{\theta}(z))]. 
\end{equation*}
In this experiment, we fix the batch size to $100$, learning rates $\alpha=0.01$ and $\eta=0.01$ for all approaches. 
Each reported result in the following is the average of $5$ repeated experiments.

To confirm the consistency between ESGDA and RSGDA, we choose different sizes of the inner loop in ESGDA, and set the probability $p$ in RSGDA accordingly.
Specifically, we set pairs $(m,p)$ by $\{(1, \frac{1}{2}), (3, \frac{1}{4}), (5, \frac{1}{6}), (7, \frac{1}{8}) \}$. 
The training curves of ESGDA and RSGDA are shown in Figure~\ref{fig: 0}, where the y-axis measures the distance to the optimal solution $(\mu^*, \sigma^*)$.
We can observe that, except for a few outliers caused by randomness, the behavior of RSGDA is consistent to that of ESGDA. 

We also evaluate the efficiency of our method AdaRSGDA.
Figure~\ref{fig: 1} provides a comparison of AdaRSGDA to ESGDA and SGDA. %by measuring the distance to the optimal solution $(\mu^*, \sigma^*)$.
For ESGDA, we set $m=5$, 
and for AdaRSGDA, we simply define $N_1 = N_2 = 300$. 
We can first observe that AdaRSGDA generally performs better than SGDA. 
Also, AdaRSGDA is more stable and converges faster than ESGDA at the beginning of the iterations. 
Moreover, as $n$ grows larger, AdaRSGDA has a refined estimation of the optimal solution which is as good as ESGDA.

% Figure environment removed

% Figure environment removed


\subsubsection{Robust non-linear regression.}

We next consider the robust non-linear regression problem proposed by~\citet{yang2021faster}. %to further evaluate the effect of hyperparameter $p$ on the convergence rate.
We generate a dataset consisting of $1000$ data points in $500$ dimensions, sampled from the normal distribution $\mathcal{N}(0, 1)$. %where 
% \lizn{sampled from the Gaussian distribution with mean $0$ and variance $1$.}
The target value $y$ is sampled by a random linear model with an additional noise. 
We define $f_w (z)$ as an MLP model with the parameter $w$.
The goal of robust non-linear regression model is to solve the following minimax problem:
\begin{equation*}
    \min_{w} \max_{y} ~\frac{1}{n} \sum_{j=1}^n \frac{1}{2} \| f_w (x_i) - y \|^2 - \frac{1}{2} \| y - y_i \|^2.
\end{equation*}

In this experiment, we mainly focus on the effect of hyperparameter $p$ on the convergence rate. 
%We confirm the effect of hyperparameter $p$ on the convergence rate of RSGDA and compare the efficiency of RSGDA with ESGDA and SGDA. 
We set the batch size to $1000$, learning rate $\alpha=5e-4$ and $\eta=5$. 
We set the probability $p$ of RSGDA by $p=0.2$ and $p = 0.8$, and compare the convergence rate of RSGDA with ESGDA and SGDA. The size of the inner loop is set as $m=4$ for ESGDA. 
Note that the choice $p=0.8$ leads to more update steps in variable $x$ than $y$.
Meanwhile, ESGDA always takes more update steps in $y$ than $x$.
% Hence, RSGDA with parameter $p=0.8$ goes beyond the ESGDA framework.

The loss curves are shown in Figure~\ref{fig: regression}.
There are some remarkable observations.
First, the curve of RSGDA with $p=0.2$ coincides with that of ESGDA, further indicating the consistency between RSGDA and ESGDA ($p=0.2$ is consistent with $m=4$). % which shows the consistency of ESGDA and RSGDA.
Second, we observe that RSGDA could converge even faster than SGDA, which is consistent with our analysis that RSGDA converges as fast as SGDA.
Third, with larger $p$, RSGDA also converges faster than ESGDA. For example, RSGDA with $p=0.8$ converges fastest in the four curves, which indicates that it is better to take more outer steps in $x$ rather than inner steps in $y$ in this model.
In other words, ESGDA is not always the best choice, and RSGDA can broaden the applicable range by choosing a large parameter $p$ (e.g., $p > 0.5$). % it is meaningful to discuss the performance of RSGDA with parameter $p > 0.5$.


% Figure environment removed


\subsection{Experiments on real data}
\subsubsection{Adversarial training.}
Finally, we study the adversarial training task with real data. Adversarial training aims to ensure the model to be robust against adversarial perturbations. 
Given the training data distribution $\mathcal{D}$, and letting $f_{\bm{w}}(\cdot)$ denote the classifier parameterized by $w$, and $L(\cdot, \cdot)$ denote the cross-entropy loss, the minimax problem of adversarial training can be formulated as~\citep{sinha2017certifying}
\begin{equation*}
    \min_{\bm{w}} \max_{\bm{\delta} \in S_p} ~\e_{(\bm{x}, \bm{y}) \sim \mathcal{D}} L(f_{\bm{w}}(\bm{x}+\bm{\delta}), \bm{y}),
\end{equation*}
where $S_p$ is the $\ell_p$ norm ball introduced to make the perturbation $\bm{\delta}$ small enough in the sense of $\ell_p$ norm. 

We conduct the experiment on the MNIST dataset~\citep{lecun2010mnist}. 
In this task, $f_{\bm{w}}$ is specific to the LeNet-5 model~\citep{lecun1998gradient}, and $S_p$ is defined as $S_p := \{\bm{\delta} \mid \|\bm{\delta}\|_{\infty} \leq 0.3\}$~\citep{madry2017towards}. 
The step sizes $\alpha$ and $\eta$ of the gradient descent and ascent are both fixed to $0.1$. 
%We evaluate the efficiency of AdaRSGDA with the following setting. 
For RSGDA, we set $p = 0.5$.
For AdaRSGDA, we set  $p_0=0.5$ and $N_1=N_2=60$.  %inner loop size $m=5$ for ESGDA,
The training loss curve is plotted in Figure \ref{fig: adv loss} and the accuracy is given in Table \ref{tab: accuracy}. 
% \yy{is it RSGDA or SGDA. the figure and the table are not consistent}

% To confirm the consistency between ESGDA and RSGDA, we choose different sizes of the inner loop in ESGDA, and set the probability $p$ in RSGDA accordingly.
% For details, we set pairs $(k,p)$ by 
% \begin{equation*}
% (k, p) \in \left\{(1, \frac{1}{2}), (3, \frac{1}{4}), (5, \frac{1}{6}), (7, \frac{1}{8}) \right\}.
% \end{equation*}
% Figure environment removed

\begin{table}[tb]
\begin{center}
\caption{The accuracy ($\%$) on MNIST dataset. AdaRSGDA performs better than the plain SGDA and RSGDA on adversarial data.}\label{tab: accuracy}
\begin{threeparttable}
\begin{tabular}{c|c|c}
\toprule 
{\textbf{Methods}} & {\textbf{Benign data}} & {\textbf{Adversarial data}}  \\
\midrule
%ESGDA & $98.2$ & $88.7$ \\
RSGDA & $99.1$ & $57.3$ \\
SGDA & $98.9$ & $75.4$ \\
AdaRSGDA & $98.9$ & $83.7$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\end{table}

Figure \ref{fig: adv loss} shows that the algorithms perform well on the training data, and the losses are close.
The accuracy in Table \ref{tab: accuracy} also supports this conclusion.
Table \ref{tab: accuracy} shows that RSGDA, SGDA, and AdaRSGDA are very close and all achieve high accuracy on the benign data.
However, their performance deviates on the adversarial data, and RSGDA and SGDA perform much worse than AdaRSGDA. 
%That is, SGDA performs much worse than ESGDA and AdaRSGDA on the adversarial (adv) data. 
RSGDA and SGDA only achieve $57.3\%$ and $75.4\%$ accuracies on the adversarial data, while AdaRSGDA has $83.7\%$ accuracy. %ESGDA has $88.7\%$ accuracy and 
Hence, training the same objective function with AdaRSGDA is more robust to adversarial attacks than SGDA and RSGDA in this real data application. The reasons behind this difference need further investigation and we leave them as future work.
A possible reason is that the lack of delay of $p$ slows down the convergence of SGDA and RSGDA.
%\yy{do we have reasons for this?}

%\yy{somewhere we may need to re-mention things  we have said in the intro, eg. RSGDA is as fast as SGDA.}