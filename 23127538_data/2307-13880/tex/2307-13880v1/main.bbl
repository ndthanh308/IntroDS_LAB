\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Bolte, Daniilidis, and Lewis(2006)}]{bolte2007lojasiewicz}
Bolte, J.; Daniilidis, A.; and Lewis, A. 2006.
\newblock The {{\L}}ojasiewicz inequality for nonsmooth subanalytic functions
  with applications to subgradient dynamical systems.
\newblock \emph{SIAM J. Optim.}, 17(4): 1205--1223.

\bibitem[{Bolte et~al.(2007)Bolte, Daniilidis, Lewis, and
  Shiota}]{bolte2007clarke}
Bolte, J.; Daniilidis, A.; Lewis, A.; and Shiota, M. 2007.
\newblock Clarke subgradients of stratifiable functions.
\newblock \emph{SIAM J. Optim.}, 18(2): 556--572.

\bibitem[{Bo{\c{t}} and B{\"o}hm(2020)}]{boct2020alternating}
Bo{\c{t}}, R.~I.; and B{\"o}hm, A. 2020.
\newblock Alternating proximal-gradient steps for (stochastic)
  nonconvex-concave minimax problems.
\newblock \emph{arXiv preprint arXiv:2007.13605}.

\bibitem[{Cesa-Bianchi and Lugosi(2006)}]{cesa2006prediction}
Cesa-Bianchi, N.; and Lugosi, G. 2006.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press.

\bibitem[{Chen et~al.(2021)Chen, Zhou, Xu, and Liang}]{chen2021proximal}
Chen, Z.; Zhou, Y.; Xu, T.; and Liang, Y. 2021.
\newblock Proximal Gradient Descent-Ascent: Variable Convergence under K {\L}
  Geometry.
\newblock \emph{arXiv preprint arXiv:2102.04653}.

\bibitem[{Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and
  Song}]{dai2018sbeed}
Dai, B.; Shaw, A.; Li, L.; Xiao, L.; He, N.; Liu, Z.; Chen, J.; and Song, L.
  2018.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, 1125--1134.
  PMLR.

\bibitem[{Daskalakis and Panageas(2018)}]{daskalakis2018limit}
Daskalakis, C.; and Panageas, I. 2018.
\newblock The limit points of (optimistic) gradient descent in min-max
  optimization.
\newblock \emph{Advances in neural information processing systems}, 31.

\bibitem[{Diakonikolas, Daskalakis, and
  Jordan(2021)}]{diakonikolas_2021_efficient}
Diakonikolas, J.; Daskalakis, C.; and Jordan, M. 2021.
\newblock Efficient Methods for Structured Nonconvex-Nonconcave Min-Max
  Optimization.
\newblock In Banerjee, A.; and Fukumizu, K., eds., \emph{Proceedings of The
  24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, 2746--2754.
  PMLR.

\bibitem[{Farnia and Ozdaglar(2021)}]{farnia2021train}
Farnia, F.; and Ozdaglar, A. 2021.
\newblock Train simultaneously, generalize better: Stability of gradient-based
  minimax learners.
\newblock In \emph{International Conference on Machine Learning}, 3174--3185.
  PMLR.

\bibitem[{Fiez et~al.(2021)Fiez, Ratliff, Mazumdar, Faulkner, and
  Narang}]{fiez2021global}
Fiez, T.; Ratliff, L.; Mazumdar, E.; Faulkner, E.; and Narang, A. 2021.
\newblock Global convergence to local minmax equilibrium in classes of
  nonconvex zero-sum games.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:
  29049--29063.

\bibitem[{Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio}]{goodfellow2014generative}
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair,
  S.; Courville, A.; and Bengio, Y. 2014.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27.

\bibitem[{Goodfellow, Shlens, and Szegedy(2014)}]{goodfellow2014explaining}
Goodfellow, I.~J.; Shlens, J.; and Szegedy, C. 2014.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}.

\bibitem[{Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter}]{heusel2017gans}
Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S.
  2017.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Jin, Netrapalli, and Jordan(2020)}]{jin2020local}
Jin, C.; Netrapalli, P.; and Jordan, M. 2020.
\newblock What is local optimality in nonconvex-nonconcave minimax
  optimization?
\newblock In \emph{International Conference on Machine Learning}, 4880--4889.
  PMLR.

\bibitem[{Karimi, Nutini, and Schmidt(2016)}]{karimi2016linear}
Karimi, H.; Nutini, J.; and Schmidt, M. 2016.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{{\L}}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 795--811. Springer.

\bibitem[{Kurdyka(1998)}]{kurdyka1998gradients}
Kurdyka, K. 1998.
\newblock On gradients of functions definable in o-minimal structures.
\newblock \emph{Ann. Inst. Fourier (Grenoble)}, 48(3): 769--783.

\bibitem[{LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner}]{lecun1998gradient}
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86(11): 2278--2324.

\bibitem[{LeCun, Cortes, and Burges(2010)}]{lecun2010mnist}
LeCun, Y.; Cortes, C.; and Burges, C. 2010.
\newblock MNIST handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2.

\bibitem[{Lee and Kim(2021)}]{lee2021fast}
Lee, S.; and Kim, D. 2021.
\newblock Fast Extra Gradient Methods for Smooth Structured
  Nonconvex-Nonconcave Minimax Problems.
\newblock In Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J.~W., eds.,
  \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Lei et~al.(2020)Lei, Lee, Dimakis, and Daskalakis}]{lei2020sgd}
Lei, Q.; Lee, J.; Dimakis, A.; and Daskalakis, C. 2020.
\newblock Sgd learns one-layer networks in wgans.
\newblock In \emph{International Conference on Machine Learning}, 5799--5808.
  PMLR.

\bibitem[{Li et~al.(2022)Li, Farnia, Das, and Jadbabaie}]{li_2022_convergence}
Li, H.; Farnia, F.; Das, S.; and Jadbabaie, A. 2022.
\newblock On Convergence of Gradient Descent Ascent: A Tight Local Analysis.
\newblock In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and
  Sabato, S., eds., \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, 12717--12740. PMLR.

\bibitem[{Lin, Jin, and Jordan(2020{\natexlab{a}})}]{lin2020gradient}
Lin, T.; Jin, C.; and Jordan, M. 2020{\natexlab{a}}.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning}, 6083--6093.
  PMLR.

\bibitem[{Lin, Jin, and Jordan(2020{\natexlab{b}})}]{lin2020near}
Lin, T.; Jin, C.; and Jordan, M.~I. 2020{\natexlab{b}}.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In \emph{Conference on Learning Theory}, 2738--2779. PMLR.

\bibitem[{Loizou et~al.(2020)Loizou, Berard, Jolicoeur-Martineau, Vincent,
  Lacoste-Julien, and Mitliagkas}]{loizou2020stochastic}
Loizou, N.; Berard, H.; Jolicoeur-Martineau, A.; Vincent, P.; Lacoste-Julien,
  S.; and Mitliagkas, I. 2020.
\newblock Stochastic hamiltonian gradient methods for smooth games.
\newblock In \emph{International Conference on Machine Learning}, 6370--6381.
  PMLR.

\bibitem[{{\L}ojasiewicz(1963)}]{lojasiewicz1963propriete}
{\L}ojasiewicz, S. 1963.
\newblock Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques
  r{\'e}els.
\newblock In \emph{Les {\'E}quations aux {D}{\'e}riv{\'e}es {P}artielles
  ({P}aris, 1962)}, 87--89. {\'E}ditions du Centre National de la Recherche
  Scientifique (CNRS).

\bibitem[{Luo et~al.(2020)Luo, Ye, Huang, and Zhang}]{luo_2020_advances}
Luo, L.; Ye, H.; Huang, Z.; and Zhang, T. 2020.
\newblock Stochastic Recursive Gradient Descent Ascent for Stochastic
  Nonconvex-Strongly-Concave Minimax Problems.
\newblock In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H.,
  eds., \emph{Advances in Neural Information Processing Systems}, volume~33,
  20566--20577. Curran Associates, Inc.

\bibitem[{Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu}]{madry2017towards}
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu, A. 2017.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}.

\bibitem[{Mazumdar, Ratliff, and Sastry(2020)}]{mazumdar2020gradient}
Mazumdar, E.; Ratliff, L.~J.; and Sastry, S.~S. 2020.
\newblock On gradient-based learning in continuous games.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2(1): 103--131.

\bibitem[{Mescheder, Nowozin, and Geiger(2017)}]{mescheder2017numerics}
Mescheder, L.; Nowozin, S.; and Geiger, A. 2017.
\newblock The numerics of gans.
\newblock \emph{arXiv preprint arXiv:1705.10461}.

\bibitem[{Nagarajan and Kolter(2017)}]{nagarajan2017gradient}
Nagarajan, V.; and Kolter, J.~Z. 2017.
\newblock Gradient descent GAN optimization is locally stable.
\newblock \emph{arXiv preprint arXiv:1706.04156}.

\bibitem[{Neumann(1928)}]{neumann1928theorie}
Neumann, J.~V. 1928.
\newblock Zur theorie der gesellschaftsspiele.
\newblock \emph{Mathematische annalen}, 100(1): 295--320.

\bibitem[{Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn}]{nouiehed_solving_2019}
Nouiehed, M.; Sanjabi, M.; Huang, T.; Lee, J.~D.; and Razaviyayn, M. 2019.
\newblock Solving a Class of Non-Convex Min-Max Games Using Iterative First
  Order Methods.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; Alch{\'e}-Buc,
  F.~d.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural Information
  Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[{Polyak(1963)}]{polyak1963gradient}
Polyak, B.~T. 1963.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
  3(4): 643--653.

\bibitem[{Robbins and Siegmund(1971)}]{robbins1971convergence}
Robbins, H.; and Siegmund, D. 1971.
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In \emph{Optimizing methods in statistics}, 233--257. Elsevier.

\bibitem[{Rockafellar(1970)}]{rockafellar_convex_1970}
Rockafellar, R.~T. 1970.
\newblock \emph{Convex analysis}.
\newblock Princeton Mathematical Series, No. 28. Princeton University Press,
  Princeton, N.J.

\bibitem[{Sanjabi et~al.(2018)Sanjabi, Ba, Razaviyayn, and
  Lee}]{sanjabi2018convergence}
Sanjabi, M.; Ba, J.; Razaviyayn, M.; and Lee, J.~D. 2018.
\newblock On the convergence and robustness of training gans with regularized
  optimal transport.
\newblock \emph{arXiv preprint arXiv:1802.08249}.

\bibitem[{Sebbouh, Cuturi, and Peyr{\'e}(2022)}]{sebbouh2021randomized}
Sebbouh, O.; Cuturi, M.; and Peyr{\'e}, G. 2022.
\newblock Randomized Stochastic Gradient Descent Ascent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2941--2969. PMLR.

\bibitem[{Sharma et~al.(2022)Sharma, Panda, Joshi, and
  Varshney}]{sharma_2022_federated}
Sharma, P.; Panda, R.; Joshi, G.; and Varshney, P. 2022.
\newblock Federated Minimax Optimization: Improved Convergence Analyses and
  Algorithms.
\newblock In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and
  Sabato, S., eds., \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, 19683--19730. PMLR.

\bibitem[{Sinha et~al.(2017)Sinha, Namkoong, Volpi, and
  Duchi}]{sinha2017certifying}
Sinha, A.; Namkoong, H.; Volpi, R.; and Duchi, J. 2017.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock \emph{arXiv preprint arXiv:1710.10571}.

\bibitem[{Xian et~al.(2021)Xian, Huang, Zhang, and Huang}]{xian_2021_faster}
Xian, W.; Huang, F.; Zhang, Y.; and Huang, H. 2021.
\newblock A Faster Decentralized Algorithm for Nonconvex Minimax Problems.
\newblock In Ranzato, M.; Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan,
  J.~W., eds., \emph{Advances in Neural Information Processing Systems},
  volume~34, 25865--25877. Curran Associates, Inc.

\bibitem[{Yan et~al.(2020)Yan, Xu, Lin, Liu, and Yang}]{yan2020optimal}
Yan, Y.; Xu, Y.; Lin, Q.; Liu, W.; and Yang, T. 2020.
\newblock Optimal epoch stochastic gradient descent ascent methods for min-max
  optimization.
\newblock \emph{arXiv preprint arXiv:2002.05309}.

\bibitem[{Yang et~al.(2021)Yang, Orvieto, Lucchi, and He}]{yang2021faster}
Yang, J.; Orvieto, A.; Lucchi, A.; and He, N. 2021.
\newblock Faster Single-loop Algorithms for Minimax Optimization without Strong
  Concavity.
\newblock \emph{arXiv preprint arXiv:2112.05604}.

\bibitem[{Zhang, Yang, and Ba{\c{s}}ar(2021)}]{zhang2021multi}
Zhang, K.; Yang, Z.; and Ba{\c{s}}ar, T. 2021.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, 321--384.

\end{thebibliography}
