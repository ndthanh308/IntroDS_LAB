
\UseRawInputEncoding
\documentclass[journal]{IEEEtran}
% \usepackage[style=ieee,dashed=false]{biblatex}
\usepackage[inline]{enumitem}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{booktabs}
\usepackage[mathscr]{euscript}
\usepackage[hyphens]{url}
\usepackage{makecell}
\usepackage{tablefootnote}
\usepackage{diagbox}
\usepackage{hhline}
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
% \title{SpatialNet: Approaching the Upper-bound of Spatial Information for Joint Speech Separation, Enhancement and Dereverberation}
\title{SpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation}

\author{Changsheng~Quan, %~\IEEEmembership{Member,~IEEE,}        
        ~Xiaofei~Li%,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\thanks{Changsheng Quan is with Zhejiang University and Westlake University \& Westlake Institute for Advanced Study, Hangzhou, China. e-mail: quanchangsheng@westlake.edu.cn}% <-this % stops a space
\thanks{Xiaofei Li is with Westlake University \& Westlake Institute for Advanced Study, Hangzhou, China. Corresponding author: lixiaofei@westlake.edu.cn. }% <-this % stops a space
}


% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{}
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\setlength{\doublerulesep}{0pt}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This work proposes a neural network to extensively exploit spatial information for multichannel joint speech separation, denoising and dereverberation, named SpatialNet.  
In the short-time Fourier transform (STFT) domain, the proposed network performs end-to-end speech enhancement. It is mainly composed of interleaved narrow-band and cross-band blocks to respectively exploit narrow-band and cross-band spatial information. 
The narrow-band blocks process frequencies independently, and use self-attention mechanism and temporal convolutional layers to respectively perform spatial-feature-based speaker clustering and temporal smoothing/filtering. 
The cross-band blocks processes frames independently, and use full-band linear layer and frequency convolutional layers to respectively learn the correlation between all frequencies and adjacent frequencies.
Experiments are conducted on various simulated and real datasets, and the results show that 1) the proposed network achieves the state-of-the-art performance on almost all tasks; 2) the proposed network suffers little from the spectral generalization problem; and 3) the proposed network is indeed performing speaker clustering (demonstrated by attention maps). 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Joint speech separation, denoising and dereverberation, end-to-end speech enhancement, learning spatial information.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

% \IEEEPARstart{S}PEECH separation is to separate the speech signals of different speakers from mixture signals.
% The separated speech signals can then be used for better human hearing in scenarios like smart remote conferences, or used in other back-end tasks, such as automatic speech recognition and speaker identification, for better human-machine interactions.

Microphone array signal processing is widely used in many applications, like hearing aid, robot and smart home equipment (e.g. smart speaker).
In real-life applications, the speech recordings are inevitably impaired by ambient noise, room reverberation, and interfering speech signals.
Suppressing these interferences, namely removing noises and reverberations, separating speakers, from the recorded signals can improve the speech quality and promote the accuracy of automatic speech recognition (ASR).
%Over the past decades, especially by applying deep learning techniques in the past decade, promising performance have been attained when the three tasks, i.e. speech denoising, dereverberation and separation, have been individually conducted.
%However, under complex acoustic conditions, the three types of interference may simultaneously exist, and the performance of jointly solving the three tasks is still far from satisfactory. 
This work aims to design a neural network for jointly performing multichannel speech separation, denoising and dereverberation. In the following, for presentation simplicity, sometimes we use the term `speech enhancement' indiscriminately to refer to either one of the three tasks or the joint task.  

% In the multichannel recordings, there are many information can be leveraged for the three tasks.
In multichannel recordings, there are two kinds of information can be leveraged for speech enhancement, namely spectral information and spatial information \cite{vincent_BlindGuidedAudio_2014, gannot_consolidated_2017}, and each of them has their respective temporal dynamics and dependencies. Spectral information mainly refers to the spectral components (spectral pattern) of signals, which carries the signal content. Spatial information mainly refers to the information of signal propagation and sound field. Traditional speech enhancement methods mainly leverage the spatial information, and thus normally agnostic to the signal's spectral content. Speech processing is normally performed in time-frequency (T-F) domain, by applying short-time Fourier transform (STFT), and speech enhancement methods are formulated in narrow-band. 
Beamforming (spatial filtering), e.g. the minimum variance/power distortionless response (MVDR/MPDR) beamformer, is one fundamental speech enhancement method \cite{Markovich-Golan2018b,gannot2001signal,gannot_consolidated_2017}, which suppresses noise and undesired speakers by applying linear spatial filtering onto the microphone signals. Weighted prediction error (WPE) \cite{nakatani2010speech} is one popular speech dereverberation technique. Based on the narrow-band convolutive transfer function (CTF) model \cite{talmon2009convolutive,li2019multichannel}, WPE applies linear prediction to perform inverse filtering onto the microphone signals. 
%the target steering vector and spatial noise covariance matrix are first estimated, then the beamforming weights are obtained by minimizing some criteria.
% beamforming的好处：对ASR友好，被广泛使用
In recent years, beamforming and WPE are combined for simultaneous denoising/separation and dereverberation, such as the Weighted Power minimization Distortionless response (WPD) technique   \cite{nakatani_UnifiedConvolutionalBeamformer_2019} combines MPDR with WPE. 
Spatial clustering is one popular blind source separation method, where T-F bins are clustered according to the spatial vectors (or spatial cues like inter-channel phase/level difference, IPD/ILD) of different speakers \cite{winter_map-based_2006,boeddecker_front-end_2018}. According to  the W-disjoint orthogonality assumption \cite{yilmaz_blind_2004}, each T-F bin is considered to be dominated by a single speaker. The T-F bins belong to the same speaker would have identical/correlated spatial vectors (when the speaker is static), and thus can be clustered together.

There are several ways to leverage deep neural networks for multichannel speech enhancement. 
%Taking inter-channel cues, e.g. IPD in \cite{wang_multi-channel_2018, chen_MultibandPITModel_2019}, as the input of neural networks is the most straight forward way.
%Except for the handcrafted features, inter-channel features can also be derived within the  networks, such as the inter-channel convolution difference (ICD) \cite{gu_enhancing_2020}, attention mechanism \cite{wang_NeuralSpeechSeparation_2020} and the transform-average-concatenate technique \cite{luo_end--end_2020}.
One way is to perform end-to-end multichannel speech enhancement, such as FasNet with transform-average-concatenate (TAC) \cite{luo_end--end_2020}, MC-TasNet \cite{gu_EndtoEndMultiChannelSpeech_2019}, Channel-Attention (CA) Dense U-net \cite{tolooshams_ChannelAttentionDenseUNet_2020}, etc.
Another important technique is the so-called neural beamformer, which estimates the spatial filter using the neural enhanced (multichannel) signals/masks \cite{heymann_blstm_2015, ochiai_beam-tasnet_2020,wang_MultimicrophoneComplexSpectral_2021, chen_BeamGuidedTasNetIterative_2022,  gu_UnifiedAllNeuralBeamforming_2023}. In the two-stage methods, such as \cite{wang_MultimicrophoneComplexSpectral_2021,chen_BeamGuidedTasNetIterative_2022}, the neural beamformer is followed by a neural post-processing. Neural beamformer is currently the main stream research direction, mainly due to that the linear spatial filtering of beamforming is friendly to the ASR backend.
%Moreover, spatial information can be jointly modelled with spectral information in latent STFT domain by applying strong networks like LSTM \cite{tesch2022insights, wang_TFGridNetIntegratingFull_2022} and Transformer \cite{wang_DasformerDeepAlternating_2023} along time and frequency axis alternatively.

% In our previous works \cite{li2019waspaa,li_narrow-band_2019,quan_MultiChannelNarrowBandDeep_2022,quan_MultichannelSpeechSeparation_2022}, narrow-band networks are used to leverage the available information especially spatial information in a single frequency.
As discussed above, spatial information are well-formulated in narrow-band, in the form of steering vector, covariance matrix, IPD/ILD, CTF, etc.
Accordingly, traditional methods like beamforming, WPE and spatial clustering are all performed in narrow-band.
Besides, many other important signal properties are also formulated in narrow-band, for example the signal stationarity \cite{gannot2001signal} and spatial coherence \cite{gannot_consolidated_2017,habets2008generating,mohan2008localization}, which are important for discriminating between speech and noise. 
Inspired by these facts, our previous works proposed to leverage the narrow-band information especially the narrow-band spatial information using a neural network for multichannel speech denoising \cite{li2019waspaa,li_narrow-band_2019} and separation \cite{quan_MultiChannelNarrowBandDeep_2022,quan_MultichannelSpeechSeparation_2022}. The same narrow-band network is used to process each STFT frequency independently. \cite{li2019waspaa,li_narrow-band_2019,quan_MultiChannelNarrowBandDeep_2022} all use a simple two-layer long short-term memory (LSTM) network, and achieve promising speech denoising and separation performance, which demonstrate that the narrow-band network is indeed suitable for denoising and separation. In addition, \cite{quan_MultiChannelNarrowBandDeep_2022} shows that the narrow-band network works well for reverberant speech, which means it also suitable for modelling/processing reverberation. 
%The rationale for distinguishing speech and noise in narrow-band lies in that speech is spatially coherent and non-stationary while noise is weakly correlated across channels and relatively stationary. While the rationale for separating speech sources in narrow-band is that TF bins can be considered as dominated by one speaker according to the sparsity of speech in STFT domain and consequently sources can be separated by clustering the spatial vectors of speakers in each TF bin. 

\textbf{Proposed Method}. As a continuation work of our previous narrow-band networks \cite{li2019waspaa,li_narrow-band_2019,quan_MultiChannelNarrowBandDeep_2022,quan_MultichannelSpeechSeparation_2022}, this work proposes a more powerful network architecture to extensively exploit spatial information, for jointly performing multichannel speech separation, denoising and dereverberation, and it is named SpatialNet. 
The proposed network is composed of a narrow-band block and a cross-band block. 
The narrow-band block is a revision of our previously proposed narrow-band conformer (NBC) network \cite{quan_MultichannelSpeechSeparation_2022}. 
The narrow-band block processes each frequency independently, and is shared by all  frequencies. It consists of a multi-head self-attention (MHSA) module and a time-convolutional module.
One important functional of the MHSA module is to cluster the spatial vector/feature of different frames dominated by different speakers, as is done in \cite{winter_map-based_2006, boeddecker_front-end_2018}. Clustering spatial vectors shares a similar principle with the self-attention mechanism \cite{vaswani2017attention} in the sense of computing the similarity of vectors and then aggregating similar vectors. 
The time-convolutional module is designed for performing local smoothing on the signals, and for modelling the convolutional reverberation. 
Speech and noise signals are random processes, and important information can be estimated by computing the signal statistics, such as the covariance matrix, which requires to smooth the raw signals. Based on the CTF model \cite{talmon2009convolutive,li2019multichannel}, in narrow-band, the microphone signal is still a convolution between the source signal and the room filter, thus convolutional network seems a natural choice for modelling and processing reverberation. 

The cross-band block is composed of two frequency-convolutional modules and one full-band linear module, designed for learning cross-band spatial information. It processes each frame independently, and is shared by all frames. 
According to the bandwidth of STFT window, the adjacent frequencies highly correlate to each other \cite{avargel_SystemIdentificationShortTime_2007}, and the frequency-convolutional layers are used to learn such correlation. 
For one signal propagation path, e.g. the direct-path, the spatial features (such as IPD) for all frequencies correlate to the time difference of arrival (TDOA). Specifically, IPD is a linear function of frequency, and the slope is the TDOA. The full-band linear module applies a linear mapping onto the frequency axis to learn such linear correlations. 
%With reverberation, though the linear relationship is broken, IPDs still distributes around the linear line \cite{mandel_ModelBasedExpectation_2010} with an error following one distribution.
%Besides, according to the inter-frame and inter-frequency convolution model \cite{gilloire_AdaptiveFilteringSubbands_1992, gannot_consolidated_2017}, the STFT of microphone observations can be precisely expressed as the convolution between a set of filters and the STFT of the source signals along time and frequency axis (see Eq. \eqref{eq_inter_frame_inter_frequency}).
%The coupling along frequency axis can be reduced to a number of neighbouring frequencies \cite{avargel_SystemIdentificationShortTime_2007}.
The cross-band block helps to better modelling the spatial information extracted by the narrow-band block, especially for the target direct-path signal.


% In our previous work, we propose to use networks to perform speech enhancement \cite{li_narrow-band_2019} and separation \cite{quan_MultiChannelNarrowBandDeep_2022, quan_MultichannelSpeechSeparation_2022} in narrow-band.
% In \cite{li_narrow-band_2019}, a BLSTM network is proposed to exploit the temporal differences and spatial differences between speech and noise, as speech is non-stationary and spatially coherent while noise is relatively stationary and weakly correlated across channels.
% In \cite{quan_MultiChannelNarrowBandDeep_2022}, the same BLSTM network is used to leverage the spatial differences of speakers in a single frequency to discriminate speakers, and the frequency permutation problem is solved by full-band permutation invariant training.
% Later in \cite{quan_MultichannelSpeechSeparation_2022}, for speech separation, a neural network with multi-head self-attention and convolution is proposed to conduct spatial clustering and local smoothing for the computation of speech statistics in narrow-band.
% The networks \cite{li_narrow-band_2019,quan_MultiChannelNarrowBandDeep_2022,quan_MultichannelSpeechSeparation_2022} are performed in narrow-band and shared by different frequencies, which is motivated by the fact that many traditional methods like beamforming for speech enhancement or separation \cite{Markovich-Golan2018b,gannot2001signal,gannot_consolidated_2017}, linear prediction for dereverberation \cite{yoshioka_GeneralizationMultiChannelLinear_2012,jukic_MultiChannelLinearPredictionBased_2015}, and independent component analysis (ICA) \cite{makino2007blind} or spatial clustering for speech separation \cite{winter_map-based_2006} are all performed in narrow-band.
% Most of these techniques are based on the exploiting of the spatial correlations of speakers.
% The spatial correlations are intrinsically formulated in narrow-band, in the form of steering vector, covariance matrix, IPD/ILD, etc.
% Correspondingly, these techniques are normally performed in narrow-band.
% Moreover, many other important properties are also formulated in narrow-band, for example the signal stationarity \cite{gannot2001signal} is important for discriminating between speech and noise, and the convolutive transfer function \cite{talmon2009convolutive,li2019multichannel} is widely used for modelling reverberation. 
% Thus, spatial information is extensively used in our previous work by using shared narrow-band networks \cite{li_narrow-band_2019,quan_MultiChannelNarrowBandDeep_2022,quan_MultichannelSpeechSeparation_2022}.
% However, the cross-band spatial correlations between spatial cues, e.g. IPD and ILD, are not considered in these works.

% In the STFT domain, TF bins at one frame are closely related.
% In the inter-frame and inter-frequency convolution model \cite{gilloire_AdaptiveFilteringSubbands_1992, gannot_consolidated_2017}, the STFT of microphone observations can be precisely expressed as the convolution between a set of filters and the STFT of the source signals along time and frequency axis (see Eq. \eqref{eq_inter_frame_inter_frequency}).
% The coupling along frequency axis can be reduced to a number of neighbouring frequencies \cite{avargel_SystemIdentificationShortTime_2007}.
% Another cue across frequencies is the spatial correlations between interchannel spatial features, like IPD or ILD.
% For single source, the interchannel spatial cues are conditioned on its direction of arrival (DOA) information or to say its arriving time difference between channels i.e. ITD.
% Due to that, IPDs in different frequencies normally have a linear relationship under anechoic environment.
% With reverberation, though the linear relationship is broken, IPDs still distributes around the linear line \cite{mandel_ModelBasedExpectation_2010} with an error following one distribution.
% The two cross-band relationships are considered in this work to help our previously proposed narrow-band network for better spatial information modelling.


% In this work, we propose a neural network for jointly separation, enhancement and dereverberation. 
% The network is composed of two blocks, a main block to exploit the rich information present in narrow-band, and a sub-block working in full-band and sub-band to exploit the cross-band information in all frequencies and neighbouring frequencies.
% As the network process multichannel signals only and is designed to extensively leverage spatial information, we call the network SpatialNet.
% The main block is revised from our previously proposed narrow-band conformer network \cite{quan_MultichannelSpeechSeparation_2022} and consists of a MHSA module and a time-convolutional module.
% One important functional of the MHSA module is to cluster the spatial vector/feature of frames dominated by different speakers, as is done in \cite{winter_map-based_2006, mandel_ModelBasedExpectation_2010, tzinisUnsupervisedDeepClustering2019,boeddecker_front-end_2018}, under the W-disjoint orthogonality assumption \cite{yilmaz_blind_2004}.
% Clustering spatial vectors share a similar principle with the self-attention mechanism \cite{vaswani2017attention} in the sense of computing the similarity of vectors and then aggregating similar ones.
% Speech signal is somehow a random process, and the estimation of its statistics, e.g. covariance matrix of multichannel speech signals, can be conducted by local smoothing/averaging operations with convolutional layers.
% Based on the convolutive transfer function model \cite{talmon2009convolutive,li2019multichannel}, the narrow-band microphone signal of each speaker is still a convolution between the narrow-band source signal and the convolutive transfer function, and dereverberation can be conducted by linear prediction, i.e. predict and subtract the reverberant components at one frame with previous frames, in that sense convolutional layers seems a natural choice to conduct reverberation by mimicing the linear prediction method.





% \newpage


% \IEEEPARstart{R}ECENTLY, deep learning based methods have made great progress in the field of front-end speech processing.
% In this paper, we considers to jointly perform speech separation, enhancement, and de-reverberation which are to separate speech from speech signals of other speakers or noises or reverberant components respectively.
% For scenarios with a single microphone, 
% deep learning methods conduct speech processing by learning from the differences between spectral patterns \cite{hershey_DeepClusteringDiscriminative_2016, yu_permutation_2017} for separating two different speakers, or distinguishing speech signals from noises by their differences in stationarity \cite{hao_FullsubnetFullBandSubBand_2021,} .



% W-disjoint orthogonality assumption \cite{yilmaz_blind_2004} says each time-frequency (TF) bin of the mixture in short-time Fourier transform (STFT) domain could be roughly considered as being dominated by a single speaker, due to the TF sparsity of speech spectra.
% Based on this assumption, speech signals can be separated by predicting a binary mask for each TF bin. The binary mask can be either directly predicted using a network, or obtained by clustering the TF embeddings as is done in the well-known deep clustering method \cite{hershey_DeepClusteringDiscriminative_2016}.
% If we relax this assumption, let the network predict ratio masks \cite{yu_permutation_2017, kolbaek_MultitalkerSpeechSeparation_2017, williamson_ComplexRatioMasking_2016}, STFT coefficients \cite{lee_FullyComplexDeep_2017}, time domain waveform \cite{luo_TaSNetTimeDomainAudio_2018}, or other targets \cite{wang_SupervisedSpeechSeparation_2018}, better results can usually be obtained.
% % More training targets can be found in \cite{wang_SupervisedSpeechSeparation_2018}.



% For the multiple-microphone (multichannel) case, besides spectral information, neural networks can also leverage the spatial information of speakers, such as the widely used inter-channel phrase difference (IPD) \cite{wang_multi-channel_2018, chen_MultibandPITModel_2019}.
% Besides the handcrafted spatial features (e.g. IPD), spatial information can also be explored automatically by using neural networks \cite{gu_EndtoEndMultiChannelSpeech_2019, gu_enhancing_2020} from the multichannel waveforms \cite{luo_fasnet_2019, luo_end--end_2020} or STFT coefficients \cite{wang_NeuralSpeechSeparation_2020, wang_MultimicrophoneComplexSpectral_2021}.
% Besides the aforementioned targets for the single-microphone case, neural networks can also predict spatial filters directly \cite{luo_fasnet_2019, luo_end--end_2020} or predict the separated multichannel signals to estimate the spatial filters \cite{ochiai_beam-tasnet_2020, chen_BeamGuidedTasNetIterative_2022, wang_NeuralSpeechSeparation_2020, heymann_blstm_2015, wang_MultimicrophoneComplexSpectral_2021}.
% % The last approach can somehow alleviate the non-linear distortion caused by the networks.


% Beyond the deep-learning-based methods, multichannel speech separation has been intensively studied in the past several decades. One popular technique is to cluster the TF bins based on their spatial vectors (or spatial cues), as at one frequency the frames belonging to the same speaker would have identical spatial vectors (when the speaker is static). This technique is also based on the W-disjoint orthogonality assumption.
% %In the case of stationary sound sources (which is the assumption of most arts in the literature), traditional methods mainly utilize the point that the features of single speaker on narrow-band, like steering vectors and IPD, along time follow the same distribution to separate. % different sources.
% % With this point, clustering techniques can then be used to cluster TF bins by their underlying dominated speakers.
% \cite{winter_map-based_2006} proposed to cluster the normalized TF samples (somehow equivalent to the steering vector) for each frequency independently, using a hierarchical clustering algorithm. 
% % Leveraging this point and further assuming the W-disjoint orthogonality assumption \cite{yilmaz_blind_2004} hold for simultaneous speech signals, paper \cite{winter_map-based_2006} proposes to use hierarchical clustering to cluster the normalized T-F samples on a narrow-band.
% %The obtained centroids are the estimated mixing vectors for that narrow-band, and then their inverse can be used to unmix speech sources.
% % It should be noted that even if the W-disjoint orthogonality assumption doesn't perfectly hold, i.e. each T-F sample also has components stemmed from other sources, the normalized T-F samples can also form clusters around true mixing vectors \cite{winter_map-based_2006}.
% % Similarly, besides using clustering techniques, we can estimate masks with the aforementioned point.
% The model-based expectation-maximization source separation and localization (MESSL) method \cite{mandel_ModelBasedExpectation_2010} and the Guided Source Separation (GSS) method \cite{boeddecker_front-end_2018} use a probabilistic Gaussian mixture model (GMM) to model the spatial vectors (or spatial cues) of TF bins, where one Gaussian component is assigned for each speaker. The inter-channel phase/level differences (IPD/ILD) and the normalized TF samples are used in MESSL and GSS, respectively. The expectation-maximization algorithm is used to estimate the model parameters and the posterior probabilities of assigning TF bins to speakers, while the latter can be used directly for speech separation (as is done in MESSL) or for estimating the parameters of beamforming (as is done in GSS). Other multichannel speech separation techniques include beamforming \cite{gannot2001signal,gannot_consolidated_2017}, independent component analysis (ICA) \cite{makino2007blind}, etc. The foundation of these techniques is to exploit the difference of spatial correlations between different speakers. The spatial correlations are intrinsically formulated in narrow-band, in the form of steering vector, covariance matrix, IPD/ILD, etc. Correspondingly, these techniques are normally performed in narrow-band. Moreover, many other important properties are also formulated in narrow-band, for example the signal stationarity \cite{gannot2001signal} is important for discriminating between speech and noise, and the convolutive transfer function \cite{talmon2009convolutive,li2019multichannel} is widely used for modelling reverberation. 


% In this work, we propose a narrow-band conformer network to focus on exploiting the rich information present in narrow-band, as a follow-up of our previous narrow-band LSTM (long short-term memory) networks \cite{li2019waspaa,li_narrow-band_2019,quan_MultiChannelNarrowBandDeep_2022}. 
% In the STFT domain, the proposed network processes each frequency independently, and is shared by all frequencies. 
% For each frequency, the network takes as input the STFT coefficients of multichannel mixture signals, and predicts the STFT coefficients of speech sources.
% %It is known that one frequency has rich information for separating speech sources, such as the spectral sparsity of speech and the inter-channel differences of multiple sources.
% The proposed narrow-band network is trained to learn a function/rule to automatically exploit the narrow-band information, and to perform end-to-end multichannel narrow-band speech separation. Similar to other narrow-band speech separation methods \cite{winter_map-based_2006,makino2007blind}, the proposed narrow-band method also suffers from the frequency permutation problem, namely the correspondences of separated signals at different frequencies are unclear. To solve this problem,  inspired by utterance-level permutation invariant training (uPIT) [16], we propose a full-band
% PIT (fPIT) scheme that forces the separated signals of all frequencies belonging to the same speaker to locate at the same network output position. 

% % 此次还需要再改进下说法。
% One important functional for narrow-band speech separation is to cluster the spatial vector/feature of frames dominated by different speakers, as is done in \cite{winter_map-based_2006, mandel_ModelBasedExpectation_2010, tzinisUnsupervisedDeepClustering2019,boeddecker_front-end_2018}, under the assumption of W-disjoint orthogonality \cite{yilmaz_blind_2004}.
% Clustering spatial vectors share a similar principle with the self-attention mechanism \cite{vaswani2017attention} in the sense of computing the similarity of vectors and then aggregating similar ones.
% Speech signal is somehow a random process, and the estimation of its statistics, e.g. covariance matrix of multichannel speech signals, can be conducted by local smoothing/averaging operations with convolutional layers. Based on the convolutive transfer function model \cite{talmon2009convolutive,li2019multichannel}, the narrow-band microphone signal of each speaker is still a convolution between the narrow-band source signal and the convolutive transfer function, thence convolutional layers seems a natural choice to model reverberation.
% Overall, the proposed narrow-band network integrates self-attention blocks and convolutional layers, and obtains outstanding speech separation performance. The integration of self-attention blocks and convolutional layers shares a similar spirit with the Conformer network \cite{gulati2020conformer}.

%Overall, each module of the proposed SpatialNet has been carefully designed according to the characteristics of the spatial information to be learned. 
SpatialNet is designed for learning sophisticated spatial information, and meanwhile it is made as concise as possible, in terms of the network architecture, model size and computational complexity.
Experiments have been conducted on multiple simulated and real datasets, performing speech separation, denoising and dereverberation either individually or jointly.  
In almost all the experiments, the proposed SpatialNet achieves the state-of-the-art performance in terms of both speech quality and ASR performance. Code and audio examples for the proposed method are available at \footnote{https://github.com/Audio-WestlakeU/NBSS}.

This work is an extension of our previously published conference paper \cite{quan_MultichannelSpeechSeparation_2022}, in which we proposed the NBC network for multichannel speech separation. 
The main contributions of this work over \cite{quan_MultichannelSpeechSeparation_2022} include:
% (i) we revise the NBC network, mainly by proposing a novel normalization method for hidden units, called group batch normalization (GBN). It normalizes the hidden units of a group of training samples in one mini-batch, i.e. all frequencies belonging to the same utterance, to maintain the distribution of hidden units over frequencies. GBN can be applied in the same way for training and inference, as the frequencies of one utterance always present although they are processed independently. Our experiments show that the proposed GBN achieves a signal-to-distortion (SDR) improvement over 3 dB compared to other normalization methods, such as batch normalization \cite{ioffe_BatchNormalizationAccelerating_2015}, layer normalization \cite{ba_LayerNormalization_2016} and group normalization \cite{wu_group_2018}; 
(i) we propose the new cross-band block; (ii) the network is extended for jointly performing speech separation, denoising and dereverberation, and is evaluated with much more experiments. 



% \begin{itemize}[leftmargin=*]
%     \item We revise the narrow-band Conformer network. The revised NBC network is called NBC2 and constitutes the main block of the proposed SpatialNet. Specifically, two major revisions are made. First, we propose a novel normalization method for hidden units, called group batch normalization (GBN). It normalizes the hidden units of a group of training samples in one mini-batch, i.e. all frequencies belonging to the same utterance, to maintain the distribution of hidden units over frequencies. GBN can be applied in the same way for training and inference, as the frequencies of one utterance always present although they are processed independently. Our experiments show that the proposed GBN achieves a signal-to-distortion (SDR) improvement over 3 dB compared to batch normalization \cite{ioffe_BatchNormalizationAccelerating_2015}, layer normalization \cite{ba_LayerNormalization_2016} and group normalization \cite{wu_group_2018}. Second, we remove the relative positional encoding (RPE) \cite{dai_transformer-xl_2019}. In our preliminary experiments, the network with RPE trained on one speech overlap way cannot well generalize to other speech overlap ways (see Fig.~\ref{fig:ovlps} for various overlap ways), since a specific self-attention mode is formed by RPE for each overlap way. In addition, RPE needs a large amount of memory and computation resource.
%     \item A cross-band block is proposed to model the full-band and sub-band correlations in one frame. This block is light-weighted and computational efficient by using linear transformation and group convolution, but huge performance gain has been attained. We has conducted experiments to find out the importance of full-band module and sub-band module in the three tasks. The results show that the sub-band module is helpful for all the three tasks and significant improvement has been observed, while the full-band module is especially useful for the dereverberation task, which is coordinated with the fact that dereverberation requires more direct-path cues compared with other tasks, consequently the full-band module which has the ability to align direct-path information is only helpful for the dereverberation task.
%     \item The proposed method is extensively evaluated with more experiments in terms of various datasets, speech overlap ways, microphone array settings and ablation experiments. The experimental results show that the proposed method works well under various conditions, and outperforms other state-of-the-art methods by a large margin.
%     % In addition, the speaker-generalization ability is also tested, and the proposed method still works well when training with only one hour of four speakers' data.
% \end{itemize}
\textbf{Related works about full-band and narrow-band combination}. Recently, several works have been proposed also for exploiting full-band/cross-band and sub-band/narrow-band information separately and then combining them. Our previous work of FullSubNet \cite{hao2021fullsubnet} was first proposed for single-channel speech enhancement by combining full-band and sub-band spectral information. 
Based on our multichannel narrow-band LSTM network \cite{li2019waspaa}, FT-JNF \cite{tesch_InsightsDeepNonLinear_2023} flips the first LSTM layer to the frequency axis to learn cross-band information, used for multichannel speech enhancement. The proposed SpatialNet shares a similar spirit with FT-JNF, but it replaces the LSTM networks with a more powerful Conformer narrow-band block and a convolutional-linear cross-band block. 
TFGridNet \cite{wang_TFGridNetIntegratingFull_2022} also uses cross-band and narrow-band LSTM networks, plus a cross-frame self-attention module. TFGridNet is a two-stage neural beamformer, using the same cross-band and narrow-band combination network for the two stages. Compared to TFGridNet, the proposed SpatialNet has a much simpler pipeline, performing end-to-end multichannel speech enhancement. 
%Moreover, SpatialNet does not use LSTMs, and thus it is more suitable for parallel computation. 
DasFormer \cite{wang_DasformerDeepAlternating_2023} uses self-attention for both cross-band and narrow-band processing. The proposed SpatialNet uses a convolutional-linear cross-band block, which is more functionally and computationally efficient. Overall, the proposed SpatialNet is quite different from these existing networks. Experiments show that, SpatialNet achieves either comparable or better performance than these existing networks on all tasks. 
%It's worth noting that the proposed GBN normalization method contributes much to the performance. 

%Overall, though the proposed SpatialNet doesn't have powerful network to leverage spectral information, by extensively leveraging spatial information, its performance has surpasses the performance of networks jointly leveraging spectral and spatial information, including TFGridNet \cite{wang_TFGridNetIntegratingFull_2022} and DasFormer \cite{wang_DasformerDeepAlternating_2023}. 

%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section{Related Works}

% \subsection{Deep Learning based Multichannel Speech Separation}
% Currently, for multichannel speech separation, a large portion of advanced methods combine deep learning and beamforming techniques.
% In \cite{ochiai_beam-tasnet_2020}, Beam-TasNet first estimates the multichannel speech signals for each speaker by using MC-TasNet \cite{gu_EndtoEndMultiChannelSpeech_2019}, then the minimum variance distortionless response (MVDR) beamformer is estimated for each speaker using the separated multichannel signals.
% Later, Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022} added a refinement stage on Beam-TasNet to iteratively perform multichannel speech separation and MVDR beamforming.
% The performance of these neural beamformers are limited by the beamforming techniques, more specifically by the beam-pattern of specific beamformers. By contrast, the proposed method performs end-to-end narrow-band speech separation, thus has an unlimited performance potential, especially for the high reverberation case that beamforming techniques cannot well tackle.   


% \subsection{Frequency Permutation Problem}
% The frequency permutation problem can be solved by leveraging the time delay of arrival (TDOA) \cite{sawada_robust_2004,mandel_ModelBasedExpectation_2010}, as the IPDs for all frequencies of the same speaker relate to one fixed TDOA. 
% The inter-frequency correlation, e.g. the spectral correlation of neighbouring frequencies  \cite{mazur_approach_2009, hoffmann_UsingInformationTheoretic_2012}, can locally solve the frequency permutation problem. 
% In \cite{ito_PermutationfreeClusteringRelative_2015}, Ito et al. proposed a permutation-free clustering method for blind source separation, using the common amplitude modulation property of speech, i.e. the frequency components of a speech signal tend to be synchronously activated, to bind the source dominance priors of different frequencies of the same speaker.
% The proposed full-band PIT is similar to this permutation-free method \cite{ito_PermutationfreeClusteringRelative_2015}, in the sense that the predictions of one speaker is also bound across frequencies. To resolve the permutation, the network may partially leverage the common amplitude modulation property, and possibly the TDOA as well.

% \subsection{Network Normalization}
% Batch normalization (BN) \cite{ioffe_BatchNormalizationAccelerating_2015}, layer normalization (LN) \cite{ba_LayerNormalization_2016} and group normalization (GN) \cite{wu_group_2018} are three main widely used normalization methods.
% BN is proposed to solve the internal covariate shift problem, i.e. the parameter changes in low layers may cause large variations to the input distribution of high layers.
% During training, BN uses the samples in one mini-batch to calculate the statistics used for  normalization, which are regarded as approximations of the statistics of the whole population.
% At test time, the population statistics are also used, but are approximated by the moving average of the statistics obtained in the training process. BN usually requires a large training batch size to have a good statistical approximation \cite{wu_group_2018}. In addition, BN is not suitable for recurrent neural networks as it  requires different statistics for different time steps, but the the number of time steps varies from sequence to sequence \cite{ba_LayerNormalization_2016}. 
% LN and GN are proposed to mitigate the problems of BN.
% The statistics in LN are calculated over all the hidden units of one layer of one training sample.
% While in GN, the hidden units of one layer are divided into groups, and the statistics are calculated over the hidden units for each group. The hidden units in one group are more correlated to each other than to the hidden units of other groups, and the group-wise normalization is benefit for maintaining their correlations. Different from BN, LN and GN are irrelevant to batch size, and can use a small batch size.

% The proposed group batch normalization (GBN) targets a situation that is not considered in BN, LN and GN, namely a group of highly correlated data, e.g. the different frequencies of one utterance in this work, are independently processed by the network and treated as independent training samples. This group of data always present together no matter for training or inference. GBN normalizes over this group of data to maintain their correlations. The situation that a group of highly correlated data are independently processed happens in many other frameworks as well, such as the intra-layers in the dual-path framework \cite{luo_DualPathRNNEfficient_2020, dang_DPTFSNetDualPathTransformer_2022, subakan_attention_2021}, or the subband layers in FullSubNet \cite{hao_FullsubnetFullBandSubBand_2021}, for which the proposed GBN strategy may also be applicable. 

%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\section{Problem Formulation}
\label{sec_problem_formulation}

\subsection{Time Domain Formulation}
For $P$ speech sources in a noisy reverberant environment, the $M$-channel microphone signals can be formulated in the time domain as:
\begin{equation}
    {x}_m(n)=\sum_{p=1}^P {y}_{pm}(n) + {e}_m(n), \ m\in\{1,...,M\}
    \label{eq_time_domain}
\end{equation}
where $m$, $p$ and $n$ denote the indices of microphone channel, speech source and discrete time, respectively. ${y}_{pm}(n)$ is the reverberant spatial image of the $p$-th source at the $m$-th microphone, ${e}_m(n)$ is ambient noise.

The spatial image ${y}_{pm}(n)$ is the convolution of the source signal $s_p(n)$ and the room impulse responses (RIR) $a_{pm}(n)$:
\begin{equation}
    {y}_{pm}(n) = a_{pm}(n) * s_p(n),
    \label{eq_time_domain_convolution}
\end{equation}
where $*$ denotes convolution. In this work, we only consider static speakers, and thus the RIRs are time-invariant. RIR is composed of direct-path, early reflections and late reverberation, and can be divided into two parts: the desired part $a_{pm}^d(n)$ and the undesired part $a_{pm}(n)-a_{pm}^d(n)$. In this work, we conduct joint speech separation, denoising and dereverberation, namely estimating the $P$ desired speech signals 
\begin{equation}
y^d_{pm}(n)= a_{pm}^d(n) * s_p(n), \ p\in\{1,...,P\}
\label{desired_time_domain}
\end{equation}
from the multichannel recordings. In practice, we only estimate the desired speech for one reference microphone, say the $r$-th microphone. Correspondingly, the desired speech signal $y^d_{pr}(n)$ will be taken as the training target of the proposed neural network. Normally, the desired RIR part is the direct-path component. 
%In experiments, for certain reasons, sometimes we set the desired RIR part to be the original RIR or the reverberation time shortening (RTS) target  \cite{zhou_SpeechDereverberationReverberation_2022}.  

%RTS reduces the reverberation time (T60) by  modulating the original RIR with an exponentially decaying window.

%For speech separation and denoising tasks, $y_{pr}(n)$ is usually taken as the training target, where $r$ indicates the reference channel.
%For dereverberation tasks, the RIR of reference channel $a_{pr}(\tau)$ is divided to two parts: the desired part $a_{pr}^d(\tau)$ and the undesired part $a_{pr}^u(\tau)=a_{pr}(\tau)-a_{pr}^d(\tau)$.
%The dereverberation target is to recover the desired target signal $d_p(n)=\sum_{\tau} a_{pr}^d(\tau) s_p(n-\tau)$.
%Depending on level of dereverberation desired, the desired part can be the direct path component with/without some early reflections or a reverberation time shortened (RTS) \cite{zhou_SpeechDereverberationReverberation_2022} version of the original RIR, i.e. modulating the T60 of original RIR by multiplying an exponentially decaying window.
%In this work, we conduct speech separation, denoising and dereverberation jointly and the direct-path component in RIRs are taken as the desired response, thus the direct-path signals are taken as the training targets.

\subsection{STFT Domain Formulation}
\label{sec_stft_domain}
% Speech is more often processed in the STFT domain. The advantages are three folds: i) speech is sparse in the T-F domain, namely one speech only occupy a small portion of T-F bins. This allows to more easily estimate the spatial/spectral parameters of each speech with respective dominant T-F bins, and of noise with non-speech T-F bins; ii) the signal propagation model, i.e. RIR convolution, can be decomposed into either narrow-band multiplication \cite{avargel_MultiplicativeTransferFunction_2007,gannot_consolidated_2017} or narrow-band convolution \cite{talmon2009convolutive,li2019multichannel}, which largely reduces the model complexity; iii) STFT automatically obtains a proper temporal and frequency resolution for speech processing. Specifically, the temporal resolution of tens of milliseconds (one STFT frame) is proper for modelling the (long-term) spectral/spatial variation, and meanwhile the speech frequency components, e.g. pitch and formants, can be well presented within one STFT frame.   

In the STFT domain, \eqref{eq_time_domain} can be written as:
\begin{equation}
    {X}_m(f,t)=\sum_{p=1}^P {Y}_{pm}(f,t) + {E}_m(f,t), \ m\in\{1,...,M\}
\end{equation}
where ${X}_m(f,t)$, ${Y}_{pm}(f,t)$ and ${E}_m(f,t)$ are the STFT coefficients of corresponding signals, $t\in \{1,...,T\}$ and $f\in\{0,...,F-1\}$ denote the time frame and frequency indices, respectively. Based on the W-disjoint orthogonality assumption \cite{yilmaz_blind_2004}, i.e. each T-F bin is dominated by one speaker, one effective way \cite{winter_map-based_2006,boeddecker_front-end_2018} to perform speech separation is to cluster the frames using the spatial vector (will be presented latter) estimated at each frame, as different speakers have different spatial vectors.

Applying STFT to \eqref{eq_time_domain_convolution}, the time-domain convolution model is represented with the inter-frame and inter-frequency convolution model \cite{avargel_SystemIdentificationShortTime_2007, gannot_consolidated_2017}:
\begin{equation}
    {Y}_{pm}(f,t) = \sum_{f'=0}^{F-1} {A}_{pm}(f,f',t)  * S_p(f',t)
    \label{eq_inter_frame_inter_frequency}
\end{equation}
where $S_p(f,t)$ is the STFT coefficient of $s_p(n)$, and ${A}_{pm}(f,f',t)$ is a set of band-to-band ($f'=f$) and cross-band ($f'\neq f$) filters derived from ${a}_{pm}$, convolution is applied to the time axis.
% The cross-band filters are used for canceling the aliasing effects caused by the subsampling \cite{avargel_SystemIdentificationShortTime_2007}.
Theoretically, \eqref{eq_inter_frame_inter_frequency} is fully valid when the summation over $f'$ takes all frequencies. 
However, it's shown in \cite{avargel_SystemIdentificationShortTime_2007} that a sufficiently valid approximation of ${Y}_{pm}(f,t)$ can be obtained when $f'$ only takes neighbouring frequencies in the range of $[f-l,f+l]$ (normally $l$ = 4, determined by the bandwidth of the mainlobe of
STFT window).

Although the inter-frame and inter-frequency convolution model is accurate, it is rarely used in practice due to the large complexity of cross-band filtering. Instead, the convolutive transfer function (CTF) approximation \cite{talmon2009convolutive,li2019multichannel} can be used, by discarding the cross-band filters:
\begin{equation}
    {Y}_{pm}(f,t) \approx {A}_{pm}(f,t) * S_p(f,t).
    \label{eq_CTF}
\end{equation}
Using this model, the narrowband inverse filtering techniques have been developed for speech dereverberation in \cite{li_MultichannelOnlineDereverberation_2019} based on channel equalization, and in \cite{yoshioka_GeneralizationMultiChannelLinear_2012, nakatani_UnifiedConvolutionalBeamformer_2019} based on linear prediction. 
%Without inter-frequency relation, the reverberation can be regarded as the convolution of previous frames on the same band. 
%Thus, the dereverberation task (not considering interference sources and noises) can be conducted by first estimating a filter $\textbf{g}_p(f,l)$ to predict the reverberation component in current frame using previous frames, then subtracting the reverberant component from current frame:
%\begin{equation}
%    D_p(f,t) = Y_{pr}(f,t) - \sum_{l} \textbf{g}_p^T(f,l) \textbf{Y}_p(f,t-k-l) \label{eq_linear_prediction}
%\end{equation}
%where $D_p(f,t)$ is the STFT representation of the desired signal $d_p(n)$, and $k$ is a prediction delay.
%This method is widely used and called linear prediction \cite{jukic_MultiChannelLinearPredictionBased_2015, yoshioka_GeneralizationMultiChannelLinear_2012}.

The filtering model can be further simplified with the multiplicative transfer function (MTF) approximation (also called narrow-band approximation) \cite{avargel_MultiplicativeTransferFunction_2007}:
\begin{equation}
    {Y}_{pm}(f,t) \approx {A}_{pm}(f) S_p(f,t)
    \label{eq_narrow_band_approxi}
\end{equation}
where ${A}_{pm}(f)$ is the time-invariant acoustic transfer function (ATF) of RIR, namely it is the same for all frames of one speaker. The ATF of multiple channels can form the steering/spatial vector. 
%In this approximation, different time-frequency bins in one frequency of one speaker share common MTFs, i.e. $\textbf{A}_p(f)$, without the inter-frame and inter-frequency relation in the IFIF model.
%, but it still widely used for the reason that the MTFs, i.e. $\textbf{A}_p(f)$ for $p=1,...,P$, provide a well-formulated way to encode/distinguish the spatial locations of speakers.
The MTF approximation is the most widely used model for its simplicity, based on which many classic speech denoising and separation techniques have been developed, such as beamforming \cite{gannot_consolidated_2017}, spatial vector clustering \cite{winter_map-based_2006, boeddecker_front-end_2018}. However, the MTF approximation is only valid when the STFT window is sufficiently larger than RIR, so it is not suitable for high reverberation scenarios. 

% % Spatial information of one source is formulated in the interchannel clues.
The desired speech signal, i.e. Eq. (\ref{desired_time_domain}), can be well modelled with the MTF model, as the desired part of RIR (the direct-path component) is shorter than the STFT window: 
\begin{equation}
    {Y}_{pm}^d(f,t) \approx {A}_{pm}^d(f) S_p(f,t),
    \label{eq_narrow_band_desired}
\end{equation}
where ${Y}_{pm}^d(f,t)$ is the STFT coefficient of $y_{pm}^d(n)$, ${A}_{pm}^d(f)$ is the Fourier transformation of ${a}_{pm}^d(n)$. In practice, for estimating the desired speech of one reference channel, instead of using the ATF itself, we actually use the relative transfer function (RTF) between other channels and the reference channel: $\tilde{{A}}_{pm}^d(f)={A}_{pm}^d(f)/A_{pr}^d(f)$.
 % \begin{align}
 %     \tilde{{A}}_{pm}^d(f)={A}_{pm}^d(f)/A_{pr}^d(f).
 %     \label{eq_desired_rtf} 
 % \end{align}
 In the free field, for the direct-path wave propagation, the RTF is $\tilde{{A}}_{pm}^d(f)=B_{pm}e^{-j 2 \pi v_f \tau_{pm}}$ in theory, where $v_f$ is the frequency in Hz, $B_{pm}$ and $\tau_{pm}$ are the frequency-independent inter-channel level and time differences, respectively, caused by the different propagation distances from the $p$-th source to the $m$-th and the $r$-th microphones. This means the RTFs of desired speech are highly correlated across frequencies. More specifically, the magnitude of RTF is the same for all frequencies, and the phase of RTF is linearly proportional to frequency. Therefore, modelling the RTFs (of desired speech) across frequencies would be helpful for increasing the modelling accuracy. This across-frequency relation of RTFs is also widely used for resolving the frequency ambiguity problem of narrow-band methods, such as in \cite{mandel_ModelBasedExpectation_2010, boeddecker_front-end_2018}. Note that, for non-ideal scenarios, e.g. in the non-free field or the desired speech involves some reflections, the across-frequency relation of RTFs is still valid to a large extent. From Eq. (\ref{eq_narrow_band_desired}), it is obvious that the desired speech is spatially coherent. 

The undesired signals to be removed mainly consists of the late reverberation and ambient noise. Ambient noise is random signal with either time-invariant (stationary) or time-variant (non-stationary) power spectrum. 
%Late reverberation can also be considered as a random Gaussian noise with an exponentially decaying amplitude. 
%For these random signals, in the frequency domain, the cross-correlation between two frequencies quickly decay with the distance of the two frequencies \cite{schroeder1962frequency, ingle2005statisica}. 
Spatially, the sound field for both late reverberation and ambient noise are (partially) diffuse \cite{gannot_consolidated_2017,habets2008generating}, with the spatial correlation between two microphones of  
\begin{equation}
    {r}(f,d) = \frac{sin(2\pi v_f d/c)}{2\pi v_f d/c},
    \label{eq_diffuse_correlation}
\end{equation}
where $d$ is the distance of the two microphones, $c$ denotes the propagation speed of sound.  The spatial correlation is a \emph{sinc} function of $f$ and $d$, and it gradually decreases with the increase of $f$ and $d$. In \cite{mohan2008localization}, a coherence test method is proposed for detecting direct-path dominated frames based on the difference of spatial correlation for the desired (direct-path) signal and the undesired reverberation and noise.

Overall, the above mentioned methods, i.e. narrowband inverse filtering, beamforming, T-F bin clustering and coherence test, all largely leverage the narrow-band spatial information to conduct speech enhancement. Narrowband indeed involves rich information for discriminating one desired speech from other signals, such as from other desired speech with their different RTFs (steering vectors), from reverberation either by identifying the CTFs (narrowband room filters) or with their different spatial correlations, from noise with their different spatial correlations. Besides the narrow-band information, leveraging the cross-frequency information would also be helpful for increasing the modelling accuracy of desired speech. Based on these analysis, we will propose our SpatialNet in the next section. 
 
 %The interchannel level difference (ILD) in decibels, interchannel phase difference (IPD) in radians can be deduced from the RTF or the theoretical interchannel time difference (ITD):
 % \begin{align}
 %     \text{ILD}_{pm}(f)&=20 \text{log}_{10} |\tilde{A}_{pm}(f)| \\
 %     \text{IPD}_{pm}(f)&=\angle \tilde{A}_{pm}(f) \label{eq_IPD_from_rtf} \\
 %                       &=\angle e^{-j 2 \pi v_f \text{ITD}_{pm}} \label{eq_IPD_theory_wrapped}
 %     %     \text{ITD}_{pm}(f)&=\angle \tilde{A}_{pm}(f)/ 2 \pi v_f
 % \end{align}
 % where $m$ is one microphone index, $j$ is the imaginary unit, $\angle x \in (-\pi,\pi]$ denotes the phase in radians of complex number $x$, $v_f$ is the frequency in Hz for the $f$-th frequency. 
% IPD and ILD respectively measure the time difference and intensity difference between two channels.
% In anechoic and clean environment, they are fully determined by the relative position of sound source and microphones. 
% In noisy reverberant environment, the observed IPD and ILD distribute around their theoretical values (i.e. the value calculated using the relative distance between sound source and microphones), which can be describe by different probabilistic models \cite{mandel_ModelBasedExpectation_2010}.
% Figure \ref{fig_ipd} shows an example of IPD in one noisy reverberant environment, where the observed IPDs are wrapped and distributed around the theoretical IPDs.

% Overall, these approximations though are not absolutely accurate, they respectively reveal/strength different aspects of the physical rules that is useful and widely adapted in different signal applications. 
% Using these approximations, various modules are proposed in this work to conduct speech separation, enhancement and de-reverberation.


% % Figure environment removed


\section{SpatialNet}
This section presents the proposed SpatialNet. It is designed for learning the sophisticated spatial information mentioned in the previous section, and a lot of efforts have been made to maintain the network architecture as simple as possible.
Fig. \ref{fig_arch_spatial_net} (a) shows the systematic overview of SpatailNet.
Before/after the network processing, the time domain waveforms are transformed to/from STFT coefficients.
The input of the network is formed by concatenating the real and imaginary parts of multichannel microphone signals for each T-F bin:
\begin{equation}
\begin{aligned}
    {\mathbf{x}}[f,t,:] =  [ \mathcal{R}&(X_1(f,t)), \mathcal{I}(X_1(f,t)), ...,\\ & \mathcal{R}(X_M(f,t)), \mathcal{I}(X_M(f,t)) ] \in \mathbb{R}^{2M} \notag
\end{aligned}
\end{equation}
where $[:]$ is an operator to take all values of one dimension of a tensor, and $\mathcal{R}(\cdot)$ and $\mathcal{I}(\cdot)$ denote the real and imaginary parts of complex number, respectively. 
The network output is the prediction of concatenated STFT coefficients of all desired speech signals for each T-F bin:
\begin{equation}
\begin{aligned}
    {\mathbf{y}}[f,t,:] = [ \mathcal{R}&(Y_{1r}^d(f,t)), \mathcal{I}(Y_{1r}^d(f,t))), ...,\\ &\mathcal{R}(Y_{Pr}^d(f,t))), \mathcal{I}(Y_{Pr}^d(f,t))) ] \in \mathbb{R}^{2P} \notag
\end{aligned}
\end{equation}

As shown in Fig. \ref{fig_arch_spatial_net} (a), SpatialNet is composed of one convolutional input layer (T-Conv1d), $L$ interleaved narrow-band blocks (see Section \ref{sec_narrow_band}) and cross-band blocks (see Section \ref{sec_cross_band}), and one linear output layer.
The convolutional input layer conducts convolution on $\mathbf{x}$ with a kernel size of 1 and 5 on the frequency and time axes, respectively, and with $C$ channels, obtaining a hidden representation $\mathbf{h}_0\in \mathbb{R}^{F\times T\times C}$. Then, it will be processed by the interleaved cross-band and narrow-band blocks.
The Linear output layer maps the output of the last block to the concatenated target STFT coefficients ${\mathbf{y}}$. 
% The STFT coefficients for source $p$ at a reference channel can be obtained:
% \begin{equation}
%     \widehat{D}_p(f,t) = \tilde{\mathbf{h}}_{\text{out}}[f,t,2p-1] + \tilde{\mathbf{h}}_{\text{out}}[f,t,2p]*i \notag
% \end{equation}
% where $\widehat{x}$ indicates the estimation of $x$, $i$ is the imaginary unit.
Finally, the time-domain speech signals can be obtained by applying inverse STFT. 
For network training, the loss function is set as the negative of the scale-invariant signal-to-distortion ratio (SI-SDR) \cite{roux_sdr_2019} of the time-domain enhanced speech signals. 
%Otherwise, if the number of speakers is flexible and unknown, the negative of source-aggregated SDR (SA-SDR) \cite{vonneumann_SASDRNovelLoss_2022} is used to account for the possible empty output stream. 
% In the time domain, the desired speech ${y}^d_{pr}(n)$ and its network prediction $\widehat{y}_{p}(n)$ are written in vector form (along the time samples) as $\textbf{y}^d_{pr}$ and $\widehat{\textbf{y}}_{p}$, respectively. SI-SDR is computed as:
% \begin{equation}
%    \mathcal{L}(\textbf{y}^d_{pr},\widehat{\textbf{y}}_{p}) 
%    = 10 \log_{10}\frac{\left \| \alpha \textbf{y}^d_{pr} \right \|^2}{\left \| \alpha \textbf{y}^d_{pr} - \widehat{\textbf{y}}_{p} \right \|^2}
%   \label{eq8}
% \end{equation}
% where $\alpha=(\widehat{\textbf{y}}_{p})^T\textbf{y}^d_{pr}/\left \| \textbf{y}^d_{pr} \right \|^2$, and $\left \| \cdot \right \|$ denotes the L$_2$ norm of vector. 
Permutation invariant training is adopted for solving the label permutation problem  \cite{yu_permutation_2017}.
% :
% \begin{equation}
%   \text{PIT}(\textbf{y}^d_{1r},\ldots, \textbf{y}^d_{Pr}, \widehat{\textbf{y}}_{1},\ldots,\widehat{\textbf{y}}_{P}) \\
%   = \mathop{min}_{\pi \in \Pi}
%   \frac{1}{N} \sum_n \mathcal{L}(\textbf{y}^d_{pr}, \widehat{\textbf{y}}_{\pi(p)}) 
%   \label{eq7}
% \end{equation} 
% where $\Pi$ denotes the set of all possible permutations, and $\pi$ denotes a permutation in $\Pi$ which maps the ground truth labels to the prediction labels.

Along the entire network, the input and output of one layer/block will always be a hidden tensor with the dimension of $F\times T\times C$. For notational simplicity, hereafter, we omit the layer/block index for the hidden tensors, and denote them as $\textbf{h}\in \mathbb{R}^{F\times T\times C}$ whenever there is no ambiguity.

% Figure environment removed



\subsection{Narrow-band Block}
\label{sec_narrow_band}
% 空间信息可以用通道间的差异来表示，通道间的差异都是formulate在单个频带上的（窄带）。混响也是一样的。
% 为了model空间信息和混响，我们提出了窄带模型，其在单个频带上使用MHSA去建模空间信息和混响。

% Consider the speech signal of one speaker in the STFT domain, using the narrow-band approximation \cite{gannot_consolidated_2017}, we have $\textbf{Y}^n_{f,t} \approx S^n_{f,t}\textbf{A}^n_f$, where $S^n_{f,t}$ and $\textbf{Y}^n_{f,t}\in \mathbb{C}^{C\times 1}$ respectively denote the speech signal of the $n$-th speaker and its multichannel spatial images at frequency $f$ and frame $t$, and $\textbf{A}^n_f \in \mathbb{C}^{C\times 1}$ is the acoustic transfer function from the $n$-th speaker to microphones at frequency $f$. 
% Note that, this work only considers the static speaker case, for which $\textbf{A}^n_f$ is time independent. 

As presented in Section \ref{sec_stft_domain}, narrow-band involves rich information for speech enhancement, and the narrow-band block is designed for learning such information. The narrow-band block processes each frequency independently, and all frequencies share the same network parameters. 
%Based on the W-disjoint orthogonality assumption \cite{yilmaz_blind_2004}, each T-F bin is dominated by one speaker, one effective way \cite{winter_map-based_2006,boeddecker_front-end_2018} to perform speech separation is to cluster the frames using the spatial vector (RTF or steering vector) estimated at each frame, as different speakers have different spatial vectors. This way can separate the speech frames and non-speech frames as well.
At one frequency, the frames dominated by different speakers (and non-speech signals) can be clustered based on their spatial vectors \cite{winter_map-based_2006,boeddecker_front-end_2018}.
From the perspective of computing the similarity of vectors, spatial vector clustering shares a similar principle with the self-attention mechanism, which motivates us to employ self-attention in our narrow-band block. 

Speech and noise signals are random processes, and the estimation of spatial features relies on the computation of signal statistics, such as the covariance matrix. This motivates us to use convolution layers to perform local smoothing/averaging operations for the computation of signal statistics. In addition, based on the CTF model shown in Eq.~\eqref{eq_CTF}, in narrow-band, the microphone signal is still a convolution between the source signal and CTF. Thus, it seems a natural choice to use convolutional layers along the time axis for modelling and processing reverberation. For example, the convolutional layers may imitate the inverse filtering process in the way of linear prediction \cite{yoshioka_GeneralizationMultiChannelLinear_2012, nakatani_UnifiedConvolutionalBeamformer_2019}. 
%As stated before, dereverberation task can be conducted by first estimating the reverberation component in current frame using previous frames, then subtracting the reverberant component from current frame (see \eqref{eq_linear_prediction}).
%Functionally, this can be imitated by convolutional layers.


As shown in Fig. \ref{fig_arch_spatial_net} (b), the narrow-band block is composed of one multi-head self-attention (MHSA) module and one time-convolutional feed forward network (T-ConvFFN). This block works on a single STFT frequency, and the frequency axis is taken within the batch dimension. 
%The sequence length for each module is always $T$. 
% As the frequencies are processed independently, we omit the frequency index hereafter.

% Conv1d performs 1-D convolution along the time dimension. It takes ${\rm \textbf{X}}$ as its input sequence, and outputs the sequence of input embedding. The input embedding for one time step is denoted as $\textbf{x}_{0} \in R^{H_1 \times 1}$, where $H_1$ is the number of hidden units.
% % 第i个NBC block接收$x_{f,i-1} \in R^{T,H_1}$作为输入，输出$x_{f,i}\in R^{T,H_1}$.
% Each NBC block is composed of two parts: the multi-head self-attention module and the convolutional feed forward network (ConvFFN) module, which will be introduced in subsection \ref{sec:MHSA} and \ref{sec:ConvFFN}, respectively.
% The output hidden vector of the $l$-th NBC block is denoted as $\textbf{x}_{l}\in R^{H_1 \times 1}$.
% % 最后的Linear会接收第L个NBC block的输出然后将其降维得到最后的结果$\widehat{\rm Y}_f$.
% The Linear output layer maps the output of the last NBC block to the separated STFT coefficients of different speakers, i.e. $\boldsymbol{\widehat{\rm Y}}$. 
% % 因为网络是共享的，因此我们可以认为f属于batch dimension，故而可以使用不含f的表示，即$x_0[:,:]$

\subsubsection{Multi-head Self-attention Module}% with Sigmoid Weights}
\label{sec:MHSA}
This module is designed to collect/separate the components of the same/different speakers using the self-attention technique \cite{vaswani_attention_2017} by computing the similarity of spatial vectors in one STFT frequency.
It consists of a Layer Norm (LN) \cite{ba_LayerNormalization_2016}, a standard Multi-Head Self-Attention (MHSA) \cite{vaswani_attention_2017}, a dropout, and a residual connection from the module input to the module output.
This module is formulated as:
\begin{equation}
    \textbf{h}[f,:,:] \leftarrow \textbf{h}[f,:,:] +  \text{Dropout(MHSA(LN}(\textbf{h}[f,:,:]))) \notag
\end{equation}
for all $f$'s.

\subsubsection{T-ConvFFN Module}
\label{sec:ConvFFN}
This module modifies the feed forward network used in Transformer \cite{vaswani_attention_2017} by inserting time-convolutional layers (denoted as T-Convs) in between the two linear layers.
%for local smoothing/averaging of speech statistic, and for modeling/reducing reverberation by mimicking linear prediction.
% Another major difference from Transformer is that we propose a new group batch normalization (GBN) method especially for the proposed narrow-band block, which will be introduced later.
The whole module is formulated as:
\begin{equation}
\begin{aligned}    
    \textbf{h}[f,:,:] \leftarrow & \textbf{h}[f,:,:] + \text{Dropout(Linear(T-Convs(}\\ &\text{SiLU(Linear(LN}(\textbf{h}[f,:,:])))))). \notag
\end{aligned}
\end{equation}
for all $f$'s, where the first linear layer transforms the hidden vector from $C$-dim to a higher dimension, say $C'$, while the last linear layer transforms the hidden vector from $C'$-dim back to $C$-dim.
The Sigmoid Linear Unit (SiLU) activation function  \cite{hendrycks_gaussian_2020, ramachandran_searching_2017} is applied after the first linear layer and after the convolutional layers in T-Convs.

In T-Convs, three group convolutional layers are used and group normalization (GN) \cite{wu_group_2018} is applied after the second convolutional layer.
The group convolutional layers perform 1-D convolution along the time dimension.
The number of channels for the convolutional layers is $C'$, and the channels are split into $G$ groups. 

Compared to the convolutional network used in Conformer proposed in \cite{gulati2020conformer}, the major difference is that the proposed T-ConvFFN puts three convolutional layers in between the two linear layers, thus time convolution is conducted on a larger number of channels, i.e. $C'$, accounting for the high requirement of local smoothing/averaging and reverberation processing for narrowband signal modelling.

% \subsubsection{Group Batch Normalization}

% Group batch normalization (GBN) is especially designed for the proposed narrowband block, and it brings a large performance improvement.
% For the narrow-band block, the frequencies of one utterance are processed by the narrowband block separately, i.e. in a batch-manner. 
% These separately processed frequencies are highly correlated according to the common amplitude modulation property \cite{ito_PermutationfreeClusteringRelative_2015}, i.e. the frequency components of one utterance tend to be activated synchronously.
% Thus, the frequencies of one utterance can be regarded as a closely correlated group.
% Normalizing (the hidden units of) the group members together can better maintain their intrinsic correlation, which can somehow promote the representation capacity of hidden units for different groups.

% The GBN is defined as:
% \begin{align}
%     \text{GBN}(\textbf{h})[:,t,c] = & \frac{\textbf{h}[:,t,c]-\mu_{t}}{\sqrt{\sigma_{t}^2+\epsilon}}\gamma_c+\beta_c,
% \end{align}
% where
% \begin{align}
%     \mu_{t} = & \frac{1}{FC} \sum_{f,c} \textbf{h}[f,t,c] \label{eq_mu} \\
% % \end{align}
% % and
% % \begin{align}
%     \sigma_{t}^2 = & \frac{1}{FC} \sum_{f,c} (\textbf{h}[f,t,c]-\mu_{t})^2 \label{eq_delta}
% \end{align}
% respectively denote the mean and variance of $\textbf{h}[:,t,:]$ and $\epsilon$ is a small value for computational stability, $\{\gamma_c, \beta_c\}_{c=1}^C$ are learnable parameters. 

% We can see that, all frequencies of one utterance share the same normalization mean and variance.
% This keeps the distribution of hidden units over all frequencies unchanged after the normalization, which we think is the major advantage of the proposed GBN.
% GBN is similar with BN, as different frequencies are processed independently and considered as independent samples during training for the narrow-band block.
% However, different from BN, the proposed network can apply the same GBN for both training and inference, as all frequencies of one utterance also present at inference. 
% By contrast, BN uses the statistics of one mini-batch during training, and the moving average of the statistics calculated at training for inference, which may harm the performance.

\subsection{Cross-band Block}
\label{sec_cross_band}
As introduced in Section \ref{sec_stft_domain}, due to the STFT window effect, the microphone signal of ${Y}_{pm}(f,t)$ for frequency $f$ contains the information of the source signal $S_p(f,t)$ for $l$ (about 4) neighbouring frequencies \cite{avargel_SystemIdentificationShortTime_2007}. 
Reversely, the neighbouring frequencies of microphone signals would also be helpful for the estimation of source signal.
Besides, the spatial feature of desired speech, i.e. the RTF of direct-path speech, is highly correlated across all frequencies. 
Accordingly, we propose the cross-band block for learning these cross-band spatial information, including two frequency-convolutional layers and one full-band linear module, which are shown in Figure \ref{fig_arch_spatial_net} (c). This cross-band block processes each time frame independently, and all time frames use the same network. In other words, this block works on a single STFT frame, and the time frame axis is taken within the batch dimension.

\subsubsection{Frequency-convolutional Module}
The frequency-convolutional module is proposed to model the correlation between neighbouring frequencies.
This module consists of one LN, one group convolution along frequency axis (F-GConv1d) and one parametric ReLU (PReLU) activation unit \cite{he_DelvingDeepRectifiers_2015}. It can be formulated as:
\begin{equation}
    \textbf{h}[:,t,:] \leftarrow \textbf{h}[:,t,:] + \text{ PReLU(F\mbox{-}GConv1d(LN}(\textbf{h}[:,t,:]))) \notag
\end{equation}
for all $t$'s.

\subsubsection{Full-band Linear Module}
Due to the existence of interference signals, e.g. interfering speakers, reverberation and noise,
%, interference sound sources 
it is difficult to accurately model the spatial feature of desired speech in narrow-band. Leveraging the spatial feature correlation across frequencies would be helpful, which motivates us to propose the full-band module.
In the full-band linear module, we first use a Linear layer with SiLU activation (shared by all T-F bins) to reduce the number of hidden channels to $C''$:
\begin{equation}
    \textbf{h}'[f,t,:] = \text{SiLU(Linear(}\textbf{h}[f,t,:])) \in \mathbb{R}^{C''} \notag
\end{equation}
for all $f$'s and $t$'s.
Then, full-band mapping is conducted onto the frequency axis by a group of linear networks. Different hidden channels use different linear networks, denoted as $\text{F-Linear}_c$ for $c=1,...,C''$. It is formulated as
\begin{equation}
    \textbf{h}'[:,t,c] \leftarrow \text{F-Linear}_c(\textbf{h}'[:,t,c]) \notag
\end{equation}
for all $t$'s and $c$'s.
%Note that $\text{F-Linear}_c$'s for different $c$'s don't share their parameters and perform linear transformation on different channels.
Finally, the output of the module is obtained by increasing the number of channels to $C$  using a Linear layer with SiLU activation, then add the original input of this module:
\begin{equation}
    \textbf{h}[f,t,:] \leftarrow \textbf{h}[f,t,:] +  \text{SiLU(Linear(}\textbf{h}'[f,t,:]))
\end{equation}
for all $f$'s and $t$'s.

%The full-band mapping networks may learn some spectral information, however we believe that they does not learn much, as they processes all time frames independently. 
In this module, $C''$ is several times smaller than $C$, and $F$ is comparable to or larger than $C$. Thus, the parameters and computations of this module mainly lie in the F-Linear networks.
To be parameter efficient, the same F-Linear networks are shared for all the repeated cross-band blocks. It is interesting to find from our preliminary experiments that sharing the F-Linear networks for all cross-band blocks does not degrade the performance.

\subsection{Discussions}
\label{sec:discussions}
The proposed SpatialNet is designed for learning narrow-band and cross-band spatial information. However, it is difficult to fully disentangle spatial information from spectral information. The narrow-band block processes frequencies independently, but one frequency still involves some spectral information. For example, the narrow-band spectral evolution/envelope of speech is quite different from the one of noise, as speech is more temporally correlated and non-stationary, while noise is less temporally correlated and stationary. The narrow-band block may also learn this spectral difference for denoising. The full-band mapping networks see all frequencies, thus may learn the full-band spectral pattern of signals, but we believe that it does not learn much, as the cross-band block processes frames independently, and one frame does not include any spectral dynamics. In addition, the representation capabilities of the full-band mapping networks are limited, as they are shared for all corss-band blocks. This issue will be verified in Section~\ref{sec_Spectra_Gen}.
%To fully learn spectral information, some extra networks need to be utilized and fused with the SpatialNet, as is done in our LSTM-based McNet \cite{yang2023mcnet}. 

Regarding the order of narrow-band and cross-band blocks, our designing order is first narrow-band then cross-band. As demonstrated by many narrow-band techniques, e.g. beamforming, narrow-band inverse filtering and frame clustering, narrow-band provides fundamental information, while cross-band provides some auxiliary information. Our experiments (Section~\ref{sec:sub-network}) also testify that, compared to the cross-band block, the narrow-band block contributes much more to the performance, and also consumes much more computations. In the proposed SpatialNet, the cross-band block is put before the narrow-band blocks, since this order achieves somewhat better performance than the other way around. However, the two blocks will be repeated many times, so the order of them does not matter much.  

 
\section{Experiments}

\subsection{Experimental Setups}

\subsubsection{Network Configuration} For the proposed network, we set the kernel size of the convolutional input layer (T-Conv1d), time-dimension group convolution (T-GConv1d), and frequency-dimension group convolution (F-GConv1d) to 5, 5 and 3, respectively.
The group numbers of T-GConv1d, F-GConv1d and GN, i.e. $G$, are all set to 8.
The number of self-attention heads is set to 4.
% The number of the F-Linears in the full-band module, i.e. $C'$, is set to 8.
A small version and a large version of the proposed network are proposed/suggested. 
The small network, referred to as \textbf{SpatialNet-small}, uses $L=8$ blocks; and the numbers of hidden units are set to $C=96$, $C'=192$, and $C''=8$.
The large network, referred to as \textbf{SpatialNet-large}, uses $L=12$ blocks; and the numbers of hidden units are set to $C=192$, $C'=384$ and $C''=16$.
%Their model sizes are 0.9 M and 5.6 M, respectively. 

STFT is applied using Hanning window with a length of 512/256 samples (32ms) and a hop size of 256/128 samples for the 16/8 kHz data.
% The direct component, i.e. the $0$-th frequency bin, is not processed, and is directly set to 0 for the network output. % 0频网络也处理了的
The number of frequency bins processed by the network is 257/129 for the 16/8 kHz data. The model size is dependent on the number of frequencies due to the full-band mapping networks. Specifically, for the 16/8 kHz data, the model size of SpatialNet-small is 1.6/1.2 M, and the model size of SpatialNet-large is 7.3/6.5 M. 

As for network training, the batch size is set to 2 utterances. The length of utterances are always 4 seconds. The Adam \cite{kingma2015adam} optimizer is used with a learning rate initialized to 0.001 and exponentially decayed as $lr \xleftarrow{} 0.001 * 0.99^{epoch}$. Gradient clipping is applied with a gradient norm threshold of 5.

\subsubsection{Datasets}
The proposed network is developed for joint speech separation, denoising and dereverberation.
However, there are few public datasets designed for the joint task.
We evaluate the proposed network with six widely used public datasets conducting either one or two of the three tasks, or the joint three tasks, including SMS-WSJ \cite{drude_SMSWSJDatabasePerformance_2019}, WHAMR!  \cite{maciejewski_WHAMRNoisyReverberant_2020}, Spatialized WSJ0-2mix \cite{wang_multi-channel_2018}, LibriCSS \cite{chen_ContinuousSpeechSeparation_2020}, Reverb Challenge \cite{kinoshita_ReverbChallengeCommon_2013} and CHiME 3/4 Challege \cite{barker_ThirdCHiMESpeech_2015}. The former three are simulated two-speaker mixture datasets, while the latter three are real-recorded datasets.  
%When dereverberation is required, we take the direct-path signal as the target signal.
%Otherwise, the reverberant spatial image of each source is used.

The proposed SpatialNet performs end-to-end multichannel speech enhancement in the STFT domain, thus it is microphone-array-dependent. For different datasets, the networks are independently trained using the training data recorded/simulated with their specific microphone arrays. 
% For all experiments, the proposed network is trained for 100 epochs.

\subsubsection{Evaluation Metrics} 
The speech enhancement performance is evaluated in terms of both speech quality and ASR. As for speech quality, we use the widely used metrics, including signal-to-distortion ratio (SDR) \cite{vincent_performance_2006}, scale-invariant SDR (SI-SDR) \cite{le2019sdr},
narrow-band or wide-band perceptual evaluation of speech quality (NB- or WB-PESQ) \cite{rix_perceptual_2001}, short-time objective
intelligibility (STOI) \cite{taal2010short} and extended STOI (eSTOI) \cite{jensen2016algorithm}. For all these metrics, the larger the better. As for ASR,  Word Error Rate (WER) is used as the evaluation metric, for which the smaller the better.
One exception is that, for the Reverb Challenge dataset, we use the official evaluation metrics of the challenge. 

\subsubsection{Comparison Methods}
As experiments are conducted on multiple datasets and tasks, it is not easy to reproduce other methods with proper configurations. Therefore, for each dataset, we compare with the methods that have reported their results on the dataset, and if not otherwise stated their results are directly quoted from their original paper. We have carefully searched the literature to involve as much as possible recently proposed SOTA methods for comparison.  

\subsection{Ablation Studies}
\label{sec_ablation}

To analyze the characteristics of the proposed network, we conduct ablation experiments on an extended SMS-WSJ dataset. SMS-WSJ \cite{drude_SMSWSJDatabasePerformance_2019} is a simulated two-speaker mixture dataset.
%proposed for evaluating joint speech separation and dereverberation in terms of both speech quality and ASR performance.
%It includes 33561 ($\sim$ 87.4 h), 982 ($\sim$ 2.5 h), and 1332 ($\sim$ 3.4 h) training, validation, and test (two-speaker) mixtures, the clean speech signals of which are respectively sampled from the \textit{si284}, \textit{dev93}, and \textit{eval92} sets of the WSJ dataset.
Clean speech signals are sampled from the Wall Street Journal (WSJ) corpus.
A six-microphone circular array with a radius of 10 cm is simulated.
RIRs are generated using the image method \cite{allen_image_1979}.
The reverberation time (T60) is uniformly sampled from 0.2 s to 0.5 s.
The source positions are randomly sampled being [1, 2] m away from the microphone array center.
Artificially generated white sensor noise is added to speech mixtures with a signal-to-noise ratio (SNR) uniformly sampled in the range of [20, 30] dB.
The sampling rate is 8 kHz.
A baseline ASR model is provided based on Kaldi \cite{Povey_Kaldi_ASRU2011}.
%A default WSJ tri-gram Kaldi language model is used.

The original SMS-WSJ dataset is extended by introducing larger reverberation and noise to evaluate the proposed network in more adverse environments. The extended dataset is named SMS-WSJ-Plus. All the six microphones are used. 
T60 is extended to [0.1, 1.0] s.
The speaker-to-microphone distance is extended to [1, 4] m.
Two speech signals are mixed together with a signal-to-interference ratio (SIR) randomly sampled in [-5, 5] dB.
Multichannel diffuse babble or white noise generated using \cite{habets2008generating} is added to the speech mixture with a SNR sampled in [0, 20] dB. 
% Compared with SMS-WSJ, the major differences between SMS-WSJ and SMS-WSJ-Plus are that SMS-WSJ-Plus has larger reverberation and noise.

\subsubsection{Contribution of Sub-networks}
\label{sec:sub-network}

Table~\ref{table_sms_wsj_plus} shows the enhancement performance, the number of parameters and the number of floating point operations (FLOPs) \footnote{FLOPs in Giga per second (G/s) is measured with four-second long utterance and then divided by four, as we normally process four-second long signals in this work. We use the tool provided in https://github.com/pytorch/torcheval for FLOPs computation.} of SpatialNet-small and its variants by removing or replacing each sub-network of it. It can be seen that every sub-networks noticeably contribute to the enhancement performance. The narrow-band modules, i.e. MHSA and T-ConvFFN, have 0.3 M and 0.6 M parameters, and 5.5 G/s and 5.0 G/s FLOPs, respectively. By contrast, the two cross-band modules, i.e. frequency-convolutional modules and full-band linear module, have 0.1 M and 0.15 M parameters, and 0.7 G/s and 0.1 G/s FLOPs, respectively. Correspondingly, the narrow-band block plays a more fundamental role, and contributes more than the cross-band block.
%Specifically, four variants are evaluated: 
%\begin{itemize}[leftmargin=*]
%     \item Replace the time-convolutional feed forward network (T-ConvFFN) with the standard feed forward network (FFN) of Transformer \cite{vaswani2017attention}. It can be seen that removing the narrow-band convolutional layers largely degrades the performance 
%     The major difference between T-ConvFFN and FFN is that an additional convolution sub-layer is employed in T-ConvFFN for estimating the covariance matrix and modelling/processing reverberation based on CTF. The experimental results show that reverting T-ConvFFN to FFN will severely lower the performance of SpatialNet.
%     % \item The proposed T-ConvFFN introduces convolutions for modelling the reverberation. Removing these convolutions will severely influence the performance.
%     \item Remove the MHSA module. As mentioned before, this module is used for spatial clustering purpose, i.e. discriminating one source from other sources like speakers and noises by utilizing the similarity of the spatial vectors of this source in narrow-band. Removing the MHSA module diminishes the ability of SpatialNet to globally collect spatial information, thus as shown in the results, SpatialNet without MHSA module performs worse.
%     \item Remove the sub-band module or full-band module. The the sub-band module is proposed to learn the correlation between neighbouring frequencies, while full-band module is proposed for modelling the spatial correlation of sources. The results show that removing any of them will inevitably degrade the performance of the proposed network, which demonstrates that the correlation between frequencies is essential for the joint task, i.e. speech separation, enhancement and de-reverberation. It should be mentioned that, compared with other methods using LSTM \cite{wang_TFGridNetIntegratingFull_2022} or MHSA \cite{wang_DasformerDeepAlternating_2023} to learn cross-band information, the proposed full-band and sub-band modules are extremely light-weighted in terms of computational cost, especially the full-band module which only has a cost of $\sim$ 0.1 G FLOPs.
% \end{itemize}
% Overall, every part in the proposed SpatialNet is effective and contributes a lot to the achieved performance.

\subsubsection{Spectral Generalization Experiment}
\label{sec_Spectra_Gen}

%Spatial information is extensively leveraged in , which makes the proposed network achieves outstanding speech enhancement performance. 
The proposed SpatialNet is proposed to extensively leverage the spatial information.
However, as mentioned in Section~\ref{sec:discussions}, it may also learn some spectral information.
Consequently, the trained networks would suffer from the spectral generalization problem. 
In this experiment, the spectral generalization ability of the proposed SpatialNet is evaluated by performing cross-language speech enhancement, as the spectral pattern of different languages are quite different. 
SMS-WSJ-Plus is used here as an English dataset.
A Chinese dataset is constructed by simply replacing the clean speech signals of SMS-WSJ-Plus with (our private) clean Chinese speech signals. 
%The trained networks are evaluated on both datasets using 4-second long utterances.
An advanced two-stage neural beamformer, i.e. Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022}, is compared. It interleaves an end-to-end speech enhancement neural network (i.e. MC-TasNet \cite{gu_EndtoEndMultiChannelSpeech_2019}) and the MVDR beamformer. The beamformer is spectral-agnostic, while the end-to-end network learns both spectral and spatial information. 
%In the first stage, a neural network is used to estimate the multichannel speech signal of each speaker; while in the second stage, another neural network is applied to iteratively fine-grain the separated multichannel speech signals and apply MVDR beamforming.


Table \ref{table_lang_gen} reports the results.
We can see that both the proposed SpatialNet and Beam-Guided TasNet have certain cross-language generalization problem, as the performance measures degrade when training with different language as the test data. It is obvious that the performance degradation of the proposed SpatialNet is smaller than the one of Beam-Guided TasNet.
%It is difficult to accurately compare the two methods with the performance degradation, as the two methods work at difference performance level.
As an anchor, we can analyze the `English' training case, for which SpatialNet achieves similar ESTOI and SDR scores on `English' and `Chinese' test data, while Beam-Guided TasNet achieves much lower scores for `Chinese' than `English' test data. 
%This means the proposed SpatialNet suffers much less from the spectral generalization problem than Beam-Guided TasNet. 
%is minor compared to Beam-Guided TasNet. 
%specifically, on the English test set, Beam-Guided TasNet trained on the Chinese dataset has 3.5 dB SDR loss and 0.22 NB-PESQ loss compared with the one trained on English dataset; on the Chinese test set, Beam-Guided TasNet trained on the English dataset has 2.4 dB SDR loss and 0.17 NB-PESQ loss.
%While, SpatialNet has 1.0 dB SDR loss and 0.23 NB-PESQ loss on the English test set, and has 0.9 dB SDR loss and 0.17 NB-PESQ loss.
Overall, the performance degradation of the proposed SpatialNet is mild for cross-language speech enhancement, which indicates that the network dominantly leverages spatial information over spectral information.   
%The more spectral information utilized by the network, the more severe spectral generalization problem the network will suffer. 
%From the result, we can know that the spectral information utilized by SpatialNet is less than Beam-Guided TasNet, which satisfies our previous assertion that the proposed cross-band block does not learn too much spectral information as it works in a single frame.
%Consequently, it has a relatively good spectral generalization ability.

% This result shows that the proposed SpatialNet can well generalize to unseen spectral pattern with small performance degradation.
% A good spectral generalization capability is important for real applications, as the proposed network can be easily trained with a very limited amount of training data, e.g. one hour of clean utterances in this experiment. 


\begin{table}[tpb]
\setlength\tabcolsep{2pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Contribution of sub-networks, experiments on SMS-WSJ-Plus.}
\label{table_sms_wsj_plus}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline\hline
Network & \#Param & FLOPs & NB-PESQ & ESTOI & SI-SDR   & WER  \\
Architecture &  (M) & (G/s) &       &       &  (dB)    &  (\%)  \\
\hline
% unproc.-old & - & - & 1.35 & 0.263 & -11.3 & 91.44 \\ % 91.51 91.42 91.38
% \hline
% SpatialNet-small-old & 1.2 M & 11.5 G/s & 3.19 & 0.879 & 13.8 & 16.41 \\ % 16.25 16.41 16.58 旧babble
% \makecell[l]{\ $-$ T-ConvFFN + \\\ \ \ Transformer FFN}-old & 0.9 M & 8.9 G & 2.78 & 0.811 & 10.8 & 22.95 \\ % 22.6 23.2 23.04 旧babble
% \ $-$ MHSA Module-old & 0.9 M & 6.0 G & 2.59 & 0.771 & 9.12 & 30.10 \\ % 30.03 30.18 30.09
% \ $-$ Sub Module-old & 1.1 M & 10.8 G & 3.01 & 0.856 & 12.5 & 18.37 \\ % 18.12 18.58 18.41
% \ $-$ Full Module-old & 1.0 M & 11.4 G & 2.83 & 0.831 & 11.7 & 21.94 \\ % 21.60 22.10 22.12
% \hline
unproc. & - & - & 1.35 & 0.261 & -11.4 & 91.13 \\ % 
\hline
SpatialNet-small & 1.2  & 11.5  & 3.11 & 0.878 & 13.8 & 15.89 \\ % 
\ remove MHSA  & 0.9  & 6.0  & 2.55 & 0.777 & 9.5 & 31.62 \\ % 
\makecell[l]{\ replace T-ConvFFN with \\\ \ \ Transformer FFN} & 0.9  & 8.9  & 2.67 & 0.796 & 10.5 & 26.91 \\ % 
\ remove Frequency-convolutional  & 1.1  & 10.8  & 3.03 & 0.863 & 12.8 & 17.01 \\ % 
\ remove Full-band Linear Module & 1.0  & 11.4  & 2.80 & 0.838 & 12.1 & 20.99 \\ % 
\hline\hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}

\begin{table}[tbp]
\setlength\tabcolsep{3pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Cross-language speech enhancement results. NB-PESQ, ESTOI and SDR (dB) are reported in the form of "NB-PESQ/ESTOI/SDR".}
\label{table_lang_gen}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{ccccc}
\hline\hline
Test Language & \multicolumn{2}{c}{English} & \multicolumn{2}{c}{Chinese} \\
Training Language & English & Chinese & English & Chinese \\
\hline
unproc. & \multicolumn{2}{c}{1.37/0.261/-2.7} & \multicolumn{2}{c}{1.22/0.305/-2.6} \\
\hline
% Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022} (iter=2) & 7.3/1.88 & 4.5/1.52 & 4.5/1.52 & 6.9/1.69 \\
% Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022} (iter=2)
% \makecell[l]{Beam-Guided TasNet\ (iter=2)}
%  \cite{chen_BeamGuidedTasNetIterative_2022}-old & 7.3/1.88 & 3.8/1.66 & 4.5/1.52 & 6.9/1.69 \\
% SpatialNet-small (prop.)-old & 15.2/3.12 & 14.2/2.89 & 15.3/2.84 & 16.2/3.01 \\
% \hline
\makecell[l]{Beam-Guided TasNet\\\ (iter=2) \cite{chen_BeamGuidedTasNetIterative_2022}}
  & 1.99/0.612/8.1 & 1.69/0.451/4.5 & 1.52/0.496/4.2 & 1.71/0.602/7.3 \\
SpatialNet-small (prop.) & 3.05/0.861/15.2 & 2.84/0.837/14.4 & 2.74/0.869/15.2 & 2.95/0.886/16.1 \\
\hline\hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}



\subsubsection{Attention Map Analysis}

As shown in Table~\ref{table_sms_wsj_plus}, the narrow-band MHSA module contributes the most to the speech enhancement performance. It is expected to perform spatial clustering of different speakers and of non-speech frames. To verify this, in Fig. \ref{fig:attn}, we draw the attention maps of one example utterance. The first three rows and the first three columns of Fig. \ref{fig:attn} are the spectrogram of clean speech signal of the first speaker (`spk1'), the second speaker (`spk2') and their noisy reverberant mixture (`mix'), respectively. 
Let's denote the attention score of one head for all frequencies as $\text{Attention}_{f,q,k} \in [0,1]$, where $f\in\{0,...,F-1\}$, $q\in\{1,...,T\}$ and $k\in\{1,...,T\}$ denote the indices of frequency, query and key, respectively, with $\sum_k \text{Attention}_{f,q,k}=1$ according to the softmax function along $k$. 
In the fourth row, we draw the Q-K (Query-Key) attention maps of two representative heads in the fourth and fifth columns, respectively. The Q-K attention maps draw the attention scores averaged over frequencies, i.e. $\frac{1}{F}\sum_f \text{Attention}_{f,q,k}$, and reflect the attentions between frames. 
In the fifth row, we draw the F-K (Frequency-Key) attention maps of the same two heads. The F-K attention maps draw the attention scores averaged over queries, i.e. $ \frac{1}{T}\sum_q \text{Attention}_{f,q,k}$, and reflect the contribution of each T-F bin (to other T-F bins at the same frequency).
% Note that we clipped the Q-K attention scores to have a maximum value of 0.03 to make the attention patterns more clearly visible. 
We can see that the two heads model the non-speech signals and (direct-path) speech signals, respectively. In the first head (the fourth column), the non-speech T-F bins are attended (shown by the F-K map) and the attentions are temporally global (shown as the vertical bars in the Q-K map). In the second head (the fifth column), only the speech T-F bins are attended, as shown by the F-K map. Moreover, two speakers are well clustered as shown in the Q-K map that the speech frames of one speaker only attend to the speech frames of the same speaker. This verifies that the narrow-band MHSA module is indeed performing speaker (and non-speech) clustering based on narrow-band spatial features. 

% From Fig. \ref{fig:attn}, we observe two interesting points about how the proposed NBC2 network works: 
% \begin{itemize}[leftmargin=*]
% \item There are different heads modelling the undesired components (diffused noise and reverberation) and desired component (directional speech signal) respectively.
% For example, the presented first Q-K and F-K (for the third MHSA layer) attention maps model the relationship between the undesired noise components,
% and the second Q-K and F-K (for the fifth MHSA layer) attention maps model the relationship between desired speech components, which is especially obvious when comparing the areas having large attention scores in the first and second F-K attention maps, i.e. we can see that the areas of the two F-K attention maps are complementary to each other, which is aligned with our common sense that the estimation of noise is more accurate in T-F bins where there are few speech component, while vice versa.
% \item Speaker clustering is one main functional of the attention mechanism in the proposed SpatialNet.
% The second Q-K attention map is one representative example, where the frames are clustered according to the underlying speakers.
% This type of attention head commonly presents at the higher layers of the network, which demonstrates that the proposed network is indeed performing speaker clustering based on the frame-wise spatial vectors.
% \end{itemize}

% Figure environment removed


\subsection{Results on SMS-WSJ}

% SMS-WSJ \cite{drude_SMSWSJDatabasePerformance_2019} is a simulated dataset proposed for evaluating the performance of speech separation and dereverberation algorithms in terms of both speech quality and ASR performance.
% It includes 33561 ($\sim$ 87.4 h), 982 ($\sim$ 2.5 h), and 1332 ($\sim$ 3.4 h) training, validation, and test (two-speaker) mixtures, the clean speech signals of which are respectively sampled from the \textit{si284}, \textit{dev93}, and \textit{eval92} sets of the WSJ dataset.
% A six-microphone circular array with a radius of 10 cm is simulated.
% RIRs are generated using the image method \cite{allen_image_1979}.
% The reverberation time (T60) is uniformly sampled from 0.2 s to 0.5 s.
% The source positions are randomly sampled being [1, 2] m away from the microphone array center.
% Artificially generated white sensor noise is added to speech mixtures with a SNR uniformly sampled in the range of [20, 30] dB.
% The sampling rate of this dataset is 8 kHz.
% A default ASR baseline model is provided based on Kaldi \cite{Povey_Kaldi_ASRU2011}.
% The baseline ASR model is trained with noisy reverberant single-speaker signals.
% A default WSJ tri-gram Kaldi language model is used.

% For the training of the proposed network, the dynamic-mixing script in its official repo\footnote{\url{https://github.com/fgnt/sms_wsj}} is used.
We evaluate the proposed networks on the original SMS-WSJ dataset, and compare with other methods. %For this dataset, joint speech denoising, dereverberation and separation is performed, and the direct-path signal of each speaker is taken as the target signal.
Two-channel and six-channel results are reported in Table \ref{table_smswsj}. For the comparison methods, if the results of several variants have been reported in their papers, we quote the results of the best variant.  
From the table, we can see that both speech quality and ASR performance can be largely improved by the speech enhancement methods. The time-domain end-to-end networks, i.e. FaSNet$+$TAC \cite{luo_end--end_2020} and Multi-TasNet \cite{zhang_mc_convtasnet_2020}, don't perform as well as other methods. Other comparison methods, i.e. MISO$_1$-BF-MISO$_3$ \cite{wang_MultimicrophoneComplexSpectral_2021}, Convolutional Prediction \cite{wang_ConvolutivePredictionReverberant_2021}, MC-CSM with LBT \cite{taherian_LBT_2022} and TFGridNet \cite{wang_TFGridNetIntegratingFull_2022}, all  perform neural beamforming plus neural post-processing, and achieve much better ASR performance than the time-domain end-to-end networks. This demonstrates the advantage of combining beamforming and deep learning techniques. It is also consistent to the widely agreed view in the field that beamforming (linear spatial filtering) is more friendly to ASR, compared to end-to-end neural speech enhancement. Among the comparison methods, TFGridNet performs the best, by adopting an advanced full-band and sub-band combination network. 
%compared with the unprocessed mixture, which proves the effectiveness of separation models for the ASR performance on mixtures.
%Location Based Training (LBT) \cite{taherian_LBT_2022} and two stage neural beamforming methods, i.e. $\text{DNN}_1+(\text{msFCP}_{\text{MVDR}}+$\\ \ $\text{msFCP}+\text{DNN}_2)\times 2$ \cite{wang_ConvolutivePredictionReverberant_2021}, MISO$_1$-BF-MISO$_3$ \cite{wang_MultimicrophoneComplexSpectral_2021} and TFGridNet \cite{wang_TFGridNetIntegratingFull_2022}, further improve the separation performance and ASR performance.

Compared to TFGridNet, the proposed SpatialNet-large achieves better speech enhancement performance and comparable ASR performance. This demonstrates that, by extensively exploit the narrow-band and cross-band spatial information, target direct-path signals can be well recovered from very noisy microphone recordings. In addition, our (STFT-domain) end-to-end speech enhancement network is efficient for improving both speech quality and ASR performance. SpatialNet-small also achieves very good performance with much less parameters and computations. 

%than TFGridNet and   obtains comparable performance with these methods except TFGridNet.
%However, the large version of SpatialNet (SpatialNet-large) obtains a separation performance largely surpassing other methods. 
%In terms of ASR performance, SpatialNet-large outperforms TFGridNet in two-channel case but obtains a slightly worse WER on six-channel case.

\begin{table}[tpb]
\setlength\tabcolsep{3pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Results on 2-channel and 6-channel SMS-WSJ Dataset. 
$^{\star}$ %, $^{\ast}$ and $^{\dagger}$
denote that the scores are quoted from \cite{wang_MultimicrophoneComplexSpectral_2021}.
}
\label{table_smswsj}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\hline\hline
\textbf{Method} & SISDR (dB) & SDR (dB) & NB-PESQ & eSTOI & WER (\%)\\
\hline
unproc. & -5.45 & -0.38 & 1.50 & 0.441 & 78.7 \\ 
\hline
oracle direct-path  & $\infty$ & $\infty$ & 4.5 & 1.0 & 6.31 \\
\hline\hline
\multicolumn{6}{c}{\textbf{Results for 2-channel SMS-WSJ}}\\
\hline
FasNet+TAC \cite{luo_end--end_2020} & 6.9$^{\star}$ & - & 2.27$^{\star}$ & 0.731$^{\star}$ & 34.84$^{\star}$ \\
Multi-TasNet \cite{zhang_mc_convtasnet_2020} & 5.8$^{\star}$ & - & 2.16$^{\star}$ & 0.720$^{\star}$ & 45.72$^{\star}$ \\
% LBT \cite{taherian_LBT_2022} & 13.2 & 14.8 & 3.33 & 0.91 &  \\
MISO$_1$-BF-MISO$_3$ \cite{wang_MultimicrophoneComplexSpectral_2021} & 12.7 & - & 3.43 & 0.907 & 10.67 \\
Convolutional Prediction \cite{wang_ConvolutivePredictionReverberant_2021} & 15.8 & - & 3.71 & - & 8.60 \\
TFGridNet \cite{wang_TFGridNetIntegratingFull_2022} & 20.3 & 22.0 & 3.81 & 0.967 & 7.41 \\
\hline
%SpatialNet-small (prop.)   & 19.6 & 21.1 & 3.80 & 0.958 & 7.88 \\ 16
%SpatialNet-large (8)  & 22.6 & 24.2 & 4.00 & 0.974 & 7.31 \\
% SpatialNet-small (GBN)  & 19.0 & 20.7 & 3.75 & 0.955 & 8.12 \\
% SpatialNet-large (GBN)  & \textbf{22.8} & \textbf{24.3} & \textbf{4.01} & \textbf{0.975} & \textbf{7.22} \\
SpatialNet-small (prop.)  & 19.4 & 21.0 & 3.80 & 0.957 & 8.10 \\
SpatialNet-large (prop.)  & \textbf{23.3} & \textbf{24.6} & \textbf{4.03} & \textbf{0.975} & \textbf{7.20} \\
\hline\hline
\multicolumn{6}{c}{\textbf{Results for 6-channel SMS-WSJ}}\\
\hline
FasNet+TAC \cite{luo_end--end_2020} & 8.60$^{\star}$ & - & 2.37$^{\star}$ & 0.771$^{\star}$ & 29.8$^{\star}$ \\
Multi-TasNet \cite{zhang_mc_convtasnet_2020} & 10.8$^{\star}$ & - & 2.78$^{\star}$ & 0.844$^{\star}$ & 23.1$^{\star}$ \\
MISO$_1$-BF-MISO$_3$ \cite{wang_MultimicrophoneComplexSpectral_2021} & 15.6 & - & 3.76 & 0.942 & 8.28 \\
MC-CSM with LBT \cite{taherian_LBT_2022} & 13.2 & 14.8 & 3.33 & 0.910 & 9.62 \\
TFGridNet \cite{wang_TFGridNetIntegratingFull_2022} & 22.8 & 24.9 & 4.08 & 0.980 & 6.76 \\
\hline
% SpatialNet-small (GBN)  & 20.5 & 23.4 & 3.96 & 0.974 & 6.97 \\ % 8 GBN
% SpatialNet-large (GBN)  & \textbf{25.1} & \textbf{27.2} & \textbf{4.19} & \textbf{0.986} & 6.80 \\
SpatialNet-small (prop.)  & 21.3 & 23.2 & 3.99 & 0.974 & 7.05 \\ %  % LN+GN 7.00 7.02 7.14
SpatialNet-large (prop.)  & \textbf{25.1} & \textbf{27.1} & \textbf{4.17} & \textbf{0.986} & \textbf{6.70} \\ % LN+GN; 6.59 6.78 6.74
\hline\hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}

% \begin{table}[htpb]
% \setlength\tabcolsep{4pt}
% % increase table row spacing, adjust to taste
% \renewcommand{\arraystretch}{1.3}
% \caption{Results on 6-channel SMS-WSJ Dataset. 
% $^{\star}$ %, $^{\ast}$ and $^{\dagger}$
% denote that the scores are quoted from \cite{wang_MultimicrophoneComplexSpectral_2021}.
% }
% \label{table_6chn_smswsj}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|ccccc}
% \hline\hline
% \textbf{Method} & SISDR (dB) & SDR (dB) & NB-PESQ & eSTOI & WER (\%)\\
% \hline
% unproc. & -5.45 & -0.38 & 1.50 & 0.441 & 78.7 \\ \hline
% FasNet+TAC \cite{luo_end--end_2020} & 8.60$^{\star}$ & - & 2.37$^{\star}$ & 0.771$^{\star}$ & 29.8$^{\star}$ \\
% Multi-TasNet \cite{zhang_mc_convtasnet_2020} & 10.8$^{\star}$ & - & 2.78$^{\star}$ & 0.844$^{\star}$ & 23.1$^{\star}$ \\
% MISO$_1$-BF-MISO$_3$ \cite{wang_MultimicrophoneComplexSpectral_2021} & 15.6 & - & 3.76 & 0.942 & 8.28 \\
% MC-CSM with LBT \cite{taherian_LBT_2022} & 13.2 & 14.8 & 3.33 & 0.910 & 9.62 \\
% TFGridNet \cite{wang_TFGridNetIntegratingFull_2022} & 22.8 & 24.9 & 4.08 & 0.980 & 6.76 \\
% \hline
% % SpatialNet-small (GBN)  & 20.5 & 23.4 & 3.96 & 0.974 & 6.97 \\ % 8 GBN
% % SpatialNet-large (GBN)  & \textbf{25.1} & \textbf{27.2} & \textbf{4.19} & \textbf{0.986} & 6.80 \\
% SpatialNet-small (prop.)  & 21.3 & 23.2 & 3.99 & 0.974 & 7.05 \\ %  % LN+GN 7.00 7.02 7.14
% SpatialNet-large (prop.)  & \textbf{25.1} & \textbf{27.1} & \textbf{4.17} & \textbf{0.986} & \textbf{6.70} \\ % LN+GN; 6.59 6.78 6.74
% \hline
% oracle direct-path  & $\infty$ & $\infty$ & 4.5 & 1.0 & 6.31 \\
% \hline\hline
% \end{tabular}
% }
% \end{table}

\subsection{Results on WHAMR!}
WHAMR! \cite{maciejewski_WHAMRNoisyReverberant_2020} extends the WSJ0-2mix dataset \cite{hershey_DeepClusteringDiscriminative_2016} by adding noise recorded with binaural microphones in urban environments and introducing reverberation to the speech sources.
The SNR is randomly sampled from -6 to +3 dB.
%Two-channel room impulse responses were simulated with randomly sampled room configurations. 
%This dataset provides both single-channel and two-channel mixtures, and we use the two-channel data.
%Recently, the two-channel version mixture is used in TF-GridNet \cite{wang_TFGridNetIntegratingFull_2022} for jointly speech separation, de-reverberation and de-noising.
We test on the same version of the dataset as in \cite{wang_TFGridNetIntegratingFull_2022}, namely the 8 kHz and 'min' version.
The first channel is taken as the reference channel. Table \ref{table_whamr} shows the results. The proposed SpatialNet-large slightly outperforms TFGridNet. 
%Compared to TFGridNet, the advantage of SpatialNet-large  on this dataset is not as large as the one on SMS-WSJ. 
This WHAMR! dataset is more difficult to process than SMS-WSJ, as it involves severe environmental noise, and all the performance scores of unprocessed signals are lower. Thence, it is more challenging for the proposed network to process by mainly exploiting spatial information, especially when only two microphones are provided. By contrast, TFGridNet adopts a strong network to fully exploit spectral information.  

\begin{table}[tbp]
\setlength\tabcolsep{4pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Results on 2-channel WHAMR! dataset. 
$^{\star}$ %, $^{\ast}$ and $^{\dagger}$
denote that the scores are quoted from \cite{wang_TFGridNetIntegratingFull_2022}.
}
\label{table_whamr}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\hline\hline
\textbf{Method} & SISDR (dB) & SDR (dB) & NB-PESQ & eSTOI \\
\hline
unproc. & -6.1 & -3.5 & 1.41 & 0.317 \\
\hline
Multi-TasNet \cite{zhang_mc_convtasnet_2020} & 6.0$^{\star}$ &  &  &  \\
\makecell[l]{Multi-TasNet with\\ \ \ speaker extraction} \cite{zhang_TimeDomainSpeechExtraction_2021} & 7.3$^{\star}$ &  &  &  \\
TFGridNet \cite{wang_TFGridNetIntegratingFull_2022} & 13.7 & 14.8 & \textbf{3.16} & 0.868 \\
\hline
%SpatialNet-small (prop.)  & 12.2 & 13.3 & 2.94 & 0.828 \\ GBN Full(16)
% SpatialNet-small (GBN)  & 12.0 & 13.2 & 2.94 & 0.827 \\ GBN Full(8)
% SpatialNet-large (GBN)  & \textbf{14.1} & \textbf{15.1} & \textbf{3.16} & \textbf{0.870} \\ % GBN
SpatialNet-small (prop.)  & 11.8 & 13.1 & 2.93 & 0.826 \\ %(LN,GBN) Full(8)
SpatialNet-large (prop.)  & \textbf{14.1} & \textbf{15.0} & \textbf{3.16} & \textbf{0.870} \\ % LN+GN
%SpatialNet-large (8)  & 13.8 & 14.8 & 3.15 & 0.867 \\
\hline\hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}

\subsection{Results on Spatialized WSJ0-2mix}
The Spatialized WSJ0-2mix dataset \cite{wang_multi-channel_2018} is a spatialized extension of the WSJ0-2mix dataset \cite{hershey_DeepClusteringDiscriminative_2016}.
The clean speech signals in WSJ0-2mix are convolved with 8-channel simulated RIRs.
The microphone array geometry is randomly sampled with an aperture size drawn from 15 cm to 25 cm.
T60 is randomly drawn from 0.2 s to 0.6 s.
The speech pairs are overlapped in ``max" or ``min" type  \cite{hershey_DeepClusteringDiscriminative_2016, wang_multi-channel_2018}.
The relative energy ratio between speakers uniformly distributes in $[-5,\ 5]$ dB.
The sampling rate could be either 8 kHz or 16 kHz.
For fair comparison with other works, we use the first four channels and take the first channel reverberant image as the target signal, namely only speech separation is conducted.


%We compare the proposed networks with MC (multi-channel) Deep Clustering \cite{wang_multi-channel_2018}, the \emph{Parallel Encoder} in \cite{gu_EndtoEndMultiChannelSpeech_2019} (refered to as MC-TasNet following \cite{ochiai_beam-tasnet_2020}), Beam-TasNet \cite{ochiai_beam-tasnet_2020}, Beam-Guided TasNet  \cite{chen_BeamGuidedTasNetIterative_2022} and oracle MVDR. The results are directly quoted from the related papers, as explained in Table \ref{table_random_array}.
%Except that, for full comparison, we retrained Beam-Guided TasNet (iter=2) \cite{chen_BeamGuidedTasNetIterative_2022} for both the 8 kHz and 16 kHz cases. Note that, the SDR  score of our retrained model for the 8 kHz case is slightly better than the one reported in its original paper.
%Both Beam-Guided TasNet and our proposed networks are trained with the ``min"-type data (the \emph{full} overlap shown in Fig. \ref{fig:ovlps}).

\begin{table}[t]
\setlength\tabcolsep{2pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.2}
\caption{Results on the 4-channel Spatialized WSJ0-2mix dataset. $^{\star}$, $^{\ast}$ and $^{\dagger}$ denote that the scores are quoted from \cite{wang_multi-channel_2018}, \cite{ochiai_beam-tasnet_2020} and \cite{chen_BeamGuidedTasNetIterative_2022}, respectively. 
}
\label{table_random_array}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{8 kHz} & \multicolumn{2}{c}{16 kHz}\\
~ & NB-PESQ & SDR (dB) & WB-PESQ & SDR (dB) \\
\midrule
\multicolumn{5}{c}{\textbf{Speech signals are overlapped in the ``max" type}} \\
%\hline
unproc. & 2.00 & 0.1 & 1.45 & 0.1 \\
\hline
% MC Deep Clustering & 2 & 8.9 & - \\
%  & 4 & 9.4 & - \\
MC Deep Clustering \cite{wang_multi-channel_2018} & - & 9.4$^{\star}$ & - & - \\
% \hline
% TasNet & 1 & - & - & - & 11.3 \\
%\hline
% MC-TasNet & 2 & - & 12.7 \\
%  & 4 & - & 12.1 \\
MC-TasNet \cite{gu_EndtoEndMultiChannelSpeech_2019}  & - & - & - & 12.1$^{\ast}$ \\
%\hline
% BeamTasNet & 2 & - & 13.8 \\
%  & 4 & 17.4 & 16.8 \\
Beam-TasNet \cite{ochiai_beam-tasnet_2020} & - & 17.4$^{\dagger}$ & - & 16.8$^{\ast}$ \\
%Oracle signal MVDR & - & - & - & 21.7$^{\ast}$ \\
%\hline
%Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022} (iter=2) & 3.90 & 20.8 & 3.48 & 19.0 \\
\makecell[l]{Beam-Guided TasNet\ (iter=4)}
 \cite{chen_BeamGuidedTasNetIterative_2022} & - & 21.5$^{\dagger}$ & - & - \\
Oracle signal MVDR & - & 23.5$^{\dagger}$ & - & 21.7$^{\ast}$ \\
\hline
% SepFormer & 4 & 3.6 / 15.6 & - \\
% \hline
%NBC2-small (prop.) & 4.14 & 22.3 & 4.03 & 22.2\\ 
% NBC2-base & & & \\
%NBC2-large (prop.) & 4.31 & 26.1 & 4.28 & 26.2 \\
SpatialNet-Small (prop.) & 4.35 & 27.6 & 4.33 & 28.0 \\
%SpatialNet-Small (prop.) & \textbf{4.36} & \textbf{27.8} & \textbf{4.33} & \textbf{27.9} \\16
% SpatialNet-Small-2 (prop.) &  &  & 4.32 & 27.6 \\
SpatialNet-Large (prop.) & \textbf{4.43} & \textbf{32.7} & \textbf{4.46} & \textbf{32.4} \\
\hline \hline 
\multicolumn{5}{c}{\textbf{Speech signals are overlapped in the ``min" type}} \\
%\hline
unproc. & 1.81 & 0.2 & 1.29 & 0.0 \\
\hline
DasFormer \cite{wang_DasformerDeepAlternating_2023} & 4.33 & 26.1 & - & - \\
SpatialNet-Small (prop.) & 4.35 & 26.8 & 4.34 & 26.6 \\
%SpatialNet-Small (prop.) & \textbf{4.35} & \textbf{27.1} & \textbf{4.30} & \textbf{26.9} \\ 16
SpatialNet-Large (prop.) & \textbf{4.43} & \textbf{31.7} & \textbf{4.45} & \textbf{31.2} \\
% SpatialNet-Small-2 (prop.) &  &  & 4.30 & 26.7 \\
\bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\end{table}

Table \ref{table_random_array} shows the speech separation results.
We can see that the neural beamformers, i.e. Beam-TasNet and Beam-Guided TasNet, show the superiority of beamforming over the binary-mask-based method (i.e. MC Deep Clustering), and the time-domain end-to-end method (i.e. MC-TasNet).
%Beam-Guided TasNet further improves the performance of Beam-TasNet by iteratively refining the separated results, at the cost of larger training and inference time.
%By jointly leveraging spatial and spectral information, 
DasFormer obtains very good separation results, by alternating frame-wise (full-band) and band-wise (narrow-band) self-attention networks. 
The proposed SpatialNet-small outperforms all the comparison methods. Different from DasFormer that uses the same self-attention scheme for both full-band and narrow-band learning, the proposed SpatialNet uses a heavy narrow-band conformer block and a light convolutional-linear cross-band block to more efficiently exploit the spatial information. Moreover, SpatialNet-large achieves almost perfect speech quality. 

In this dataset, the geometry of microphone array is varying, which is more challenging for the end-to-end networks compared to the beamforming-based methods as beamforming is array-agnostic, while the end-to-end networks needs to generalize across different arrays. Even though, DasFormer and the proposed SpatialNet still achieve good performance, which shows their capability of array-generalization.

\subsection{Results on LibriCSS}
LibriCSS \cite{chen_ContinuousSpeechSeparation_2020} is a meeting-like dataset recorded in a regular meeting room by playing utterances sampled from LibriSpeech \cite{panayotov_LibrispeechASRCorpus_2015} with loudspeakers.
There are 10 sessions in LibriCSS, among which \textit{session0} can be used for hyper-parameter tuning.
%Each of these sessions is approximately 1 hour long and consists of six ten-minute-long mini-sessions.
Different speech overlap ratios are set, including 0\% with short inter-utterance silence (0S) or long inter-utterance silence (0L), 10\%, 20\%, 30\% and 40\%.
A 7-channel circular array is used.
% , where the first microphone is placed at the center of the array.
% The radius of the array is 4.25 cm.
% The distances between loudspeakers and microphones range from 33 cm to 409 cm.
% \textit{Session0} is used for the hyper-parameter tuning in our experiments.
An ASR system is provided along with this dataset to measure the speech enhancement performance.
%A three layer bidirectional long short term memory (BLSTM) acoustic model is trained on 960 hours of LibriSpeech training data.
%A standard 4-gram language model is used in decoding.
%Two evaluation configurations are provided: continuous input evaluation and utterance-wise evaluation.
We report the performance of our networks for utterance-wise evaluation, where utterances are split with oracle boundary information of each utterance. 
% Some of the utterances contain more than two speakers, but at most two speakers speaking simultaneously.
% Among the separated speech signals, only the ASR performance of one target utterance is counted. 

%Two outputs are required for separation algorithms, however, in utterance evaluation, the one with lower WER is considered as the prediction of the utterance providing the oracle boundary information, thus the reported WER is calculated using this prediction.

\begin{table}[tpb]
\setlength\tabcolsep{3pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.2}
\caption{WER (\%) for LibriCSS (7-channel, utterance evaluation). 
% $^{\star}$ and $^{\ast}$ denote that the scores are quoted from  and , respectively. 
}
\label{table_libricss_results}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c}{Overlap ratio (\%)}\\
~ & 0S & 0L & 10 & 20 & 30 & 40 \\
\hline
unproc. & 11.8 & 11.7 & 18.8 & 27.2 & 35.6 & 43.3 \\
\hline
LSTM $+$ MVDR \cite{chen_ContinuousSpeechSeparation_2020} & 8.4 & 8.3 & 11.6 & 15.8 & 18.7 & 21.7 \\
Conformer-base $+$ MVDR \cite{chen_continuous_2021} & 7.3 & 7.3 & 9.6 & 11.9 & 13.9 & 15.9 \\
Conformer-large $+$ MVDR \cite{chen_continuous_2021} & 7.2 & 7.5 & 9.6 & 11.3 & 13.7 & 15.1 \\
MISO$_1$-BF-MISO$_3$ \cite{wang_MultimicrophoneComplexSpectral_2021} & 5.8 & 5.8 & 5.9 & 6.5 & 7.7 & 8.3 \\
\hline
% SpatialNet-small (8,4s,e29) & 6.1 & 5.6 & 6.6 & 7.8 & 9.0 & 9.9 \\ % 新RIR
SpatialNet-small (prop.) & \textbf{5.5} & 5.5 & 5.8 & 6.7 & 7.8 & 8.6 \\ % 新RIR; 8,123spk,4s,e29,ensemble 10 epochs
% SpatialNet-small (8,uttr,e97) & 5.8 & 5.5 & 6.0 & 7.8 & 10.0 & 12.5 \\
% SpatialNet-small (16,uttr,e95?) & 5.6 & 5.8 & 6.4 & 6.8 & 8.0 & 9.4 \\
% SpatialNet-small (prop.) & 5.4 & 5.5 & 5.7 & 6.6 & 7.8 & 9.2 \\
% SpatialNet-small (sisdr) & 5.8 & 5.9 & 6.4 & 7.7 & 9.8 & 12.0 \\
% SpatialNet-large (8,uttr,e55) & 6.1 & 5.7 & 6.0 & 6.9 & 8.2 & 10.1 \\ 
% SpatialNet-large (16,uttr,e30) & 5.4 & 5.7 & 5.7 & 6.5 & 8.3 & 10.3 \\ % epoch 30
% SpatialNet-large (16,uttr,e60) & 5.7 & 5.9 & 6.2 & 7.0 & 8.8 & 10.6 \\ % epoch 60
% SpatialNet-large (16,4s,e60) & 5.6 & 5.5 & 6.4 & 6.8 & 8.1 & 8.7 \\ % epoch 60; 旧RIR
SpatialNet-large (prop.) & 5.6 & \textbf{5.4} & \textbf{5.6} & \textbf{5.9} & \textbf{6.5} & \textbf{6.9} \\ % 新RIR；e64; ensemble 10 epochs
\hline\hline
\end{tabular}
% }
\vspace{-0.2cm}
\end{table}


As this dataset only provides the evaluation data, we use simulated signals to train our networks.
The simulated array has the same geometry with the LibriCSS array.
According to the overlap mode of evaluation data, two speech streams are simulated and mixed. One stream is the speech signal of one (target) speaker. The other stream could include the speech signal of zero, one or two (non-overlapped) speakers, with the proportion of about 33\%, 53\% and 14\%, respectively.    
%About 33\%/53\%/14\% utterances in the simulated dataset contain one/two/three speakers.
%Same to the evaluation data, at most two speakers talk simultaneously.
When the other stream includes one speaker, the speech overlap ratio is randomly sampled in [10\%, 100\%].
When the other stream includes two speakers, the two speakers talk respectively in the beginning and ending parts of the stream, with a 0.1 $\sim$ 1.0 s silence in between. 
The clean speech signals are sampled from \textit{train-clean-100} and \textit{train-clean-360} sets of LibriSpeech.
Multichannel diffuse noises are generated using the toolkit \cite{habets2008generating} with the single-channel ambient noise signals of Reverb Challenge dataset \cite{kinoshita_ReverbChallengeCommon_2013}.
SNR between reverberant mixture and noise is randomly sampled in [5, 20] dB.
T60 is randomly sampled in [0.2, 1.0] s.
The gpuRIR toolkit \cite{diaz-guerra_gpurir_2021} is used for RIR generation.
%The direct-path signal is taken as the training target.
To train the network with one possible empty output stream, the negative of source-aggregated SDR (SA-SDR) \cite{vonneumann_SASDRNovelLoss_2022} is used as the loss function. 
%as it provides a stabler gradient for training neural networks with one-speaker utterances compared with SI-SDR.
%The SA-SDR is defined as \cite{vonneumann_SASDRNovelLoss_2022}:
%\begin{equation}
%    \text{SA-SDR} = 10\ \text{log}_{10} \frac{\sum_{p=1}^P \left \| \textbf{y}_{pr}^d \right \|^2 }{ \sum_{p=1}^P \left \| \textbf{y}_{pr}^d - \widehat{\textbf{y}}_{p}^d \right \|^2}.
%\end{equation}
% To make it scale-invariant, we scale $\textbf{y}_{pr}^d$ as $\alpha \textbf{y}_{pr}^d$ for $p=1,...,P$ with the common factor $\alpha=(\widehat{\textbf{y}})^T\textbf{y}/\left \| \textbf{y} \right \|^2$, where $\widehat{\textbf{y}}=[(\widehat{\textbf{y}}_{1}^d)^T, ..., (\widehat{\textbf{y}}_{P}^d)^T]^T$ and $\textbf{y}=[(\textbf{y}_{1r}^d)^T, ..., (\textbf{y}_{Pr}^d)^T]^T$.
% The training utterances are all 4-second long.
The WER of \textit{session0} is taken as the validation metric of network training. To produce a stable ASR performance, the network weights of the last ten checkpoints/epochs (relative to the best epoch) are averaged. 
%in the best checkpoint and the weights in the checkpoints of the nearest nine epochs before the best epoch are ensembled, i.e. averaged, for inference, where the best checkpoint/epoch is picked based on the WER on \textit{session0}.
At inference, the evaluation utterances are first chunked to 4-second segments and processed by the network, with 2-second overlapping between consecutive segments.
% After obtaining the estimations of all segments of utterance, the 
%Then, these segments are .
The final output is formed by stitching the segment-level outputs according to the similarity of overlapped parts.


The proposed networks are compared with three neural beamformers \cite{chen_ContinuousSpeechSeparation_2020, chen_continuous_2021, wang_MultimicrophoneComplexSpectral_2021}. 
%In \cite{wang_MultimicrophoneComplexSpectral_2021}, in addition to the neural beamformer, an extra post-processing network is applied. 
%In \cite{chen_ContinuousSpeechSeparation_2020, chen_continuous_2021}, masks for each source are first estimated by using a LSTM or Conformer network respectively, then MVDR is applied based on masked spectrum using the estimated masks to obtain the speech signal of each source. Slightly different from \cite{chen_ContinuousSpeechSeparation_2020, chen_continuous_2021}, \cite{wang_MultimicrophoneComplexSpectral_2021} not only applies the masking and beamforming scheme, but also uses an additional network to conduct post-processing on the masked speech signals and beamformed results.
% All the mentioned methods including the proposed method in this Table are trained with simulated dataset.
Table \ref{table_libricss_results} shows the ASR performance.
WERs can be greatly reduced by the neural beamformers no matter for single-speaker or multi-speaker situations. The proposed SpatialNet-large largely outperforms other methods, which demonstrates the effectiveness of proposed network as an ASR front-end technique on real data.

%the WERs can be greatly reduced by the neural beamformers no matter for single-speaker or multi-speaker situations, which is as expected since the linear spatial filtering of beamforming is friendly to ASR backends.
%Compared with the neural beamforming techniques, the proposed network enhances speech signals in an end-to-end fashion.
%Though beamforming techniques are not used, the proposed SpatialNet-large largely outperforms other methods in terms of WER, which demonstrates the effectiveness of proposed network as a front-end processing technique.
% Note that the presented methods including the proposed one are all trained on simulated data. 


\subsection{Results on Reverb Challenge Dataset}
The Reverb Challenge dataset \cite{kinoshita_ReverbChallengeCommon_2013} includes simulated data (SimData) and real-recorded data (RealData), for evaluating speech dereverberation performance. A 8-channel circular array with a diameter of 20 cm is used. The sampling rate is 16 kHz. Two different speaker-to-array distances (near=50 cm and far=200 cm) are evaluated. We use the official evaluation metrics of Reverb Challenge.
Readers can refer to \cite{kinoshita_ReverbChallengeCommon_2013} for more details about the dataset and evaluation metrics.

%In SimData, 24, 6 and 6 real-recorded multichannel RIRs are used to generate the training, validation, and test set, respectively, by convoluting with the clean speech signal from the WSJCAM0 corpus \cite{robinson_WSJCAMOBritishEnglish_1995}.
% These RIRs are measured by an 8-channel circular array with a diameter of 20 cm in 3 rooms with different volumes (small, medium, large), with two different speaker-array distances (near=50 cm and far=200 cm).
% T60 of the three rooms are 0.25 s, 0.5 s and 0.75 s, respectively.
% Multichannel stationary background noises, which are mainly caused by the air conditioning systems in the rooms, are recorded by the same array, and mixed with the convolved signal with a SNR of 20 dB.
% RealData includes two sub-sets, i.e. Dev and Eval, for validation and test, respectively.
% Utterances in Dev and Eval are multichannel noisy reverberant real recordings sampled from the MC-WSJ-AV corpus \cite{lincoln_MultichannelWallStreet_2005}.
% The T60 of MC-WSJ-AV is approximately 0.7 s, and the microphone array geometry is the same with SimData.
% The sampling rate for both SimData and RealData is 16 kHz. 

The Reverb Challenge training set only consists of 24 real-recorded RIRs, which are insufficient for training the multichannel speech enhancement networks. % due to the shortage of RIR.
For that reason, we use 40,000 simulated RIRs with the same array geometry to train our networks.
% The room size and the speaker-to-microphone distance are not used in the simulation process which meets the requirements of the Reverb Challenge\footnote{The array geometry is allowed to be used in the Reverb Challenge, but the room size and speaker-to-microphone distance are not allowed, see \url{https://reverb2014.dereverberation.com/instructions.html}}.
% The length, width and height of rooms are randomly sampled in [3 m, 8 m], [3 m, 8 m] and [3 m, 4 m], respectively. T60 is uniformly sampled in [0.1, 1.0] s.
% The array positions are randomly sampled in a 1 m x 1 m square area in the center of the room.
% The speaker positions are randomly sampled in the room. 
The direct-path signal is taken as the training and prediction target.
%The official evaluation metrics of Reverb Challenge are used, including the cepstral distance (CD), log likelihood ratio (LLR), frequency-weighted segmental SNR (FWSSNR), PESQ, speech-to-reverberation modulation energy ratio (SRMR).
%Readers can refer to \cite{kinoshita_ReverbChallengeCommon_2013} for more details about these metrics.
%As mentioned above, the direct-path signal is the prediction target of the proposed network,
Accordingly, the direct-path signal is taken as the reference signal for intrusive evaluation metrics. However, the Reverb Challenge traditionally takes the dry (source) speech as the reference signal. For fair comparison, both dry speech and direct-path signal will be tested.  
To align with \cite{wang_DeepLearningBased_2020}, 2.5 ms around the peak value of measured RIRs are considered as the direct-path component.
%For metric computation, either dry source and direct-path signals are taken as the reference, where the former is the default setup of the Reverb Challenge, while the later follows the work of Wang et al. \cite{wang_DeepLearningBased_2020}.
ASR performance is evaluated using the best pretrained ASR checkpoint \footnote{https://github.com/espnet/espnet/tree/master/egs2/reverb/asr1\#transformer-asr--transformer--lm---speedperturbation--specaug--applying-rir-and-noise-data-on-the-fly} for this dataset in ESPnet \cite{li2020espnet}. 
%This ASR system uses a Transformer-based acoustic model and language model. Speed perturbation, spectral augmentation, on-the-fly RIRs and noise mixing are used in the training of the ASR model.
%Meanwhile, RIRs and noise signals are applied on the fly.

%Experiments are conducted using all the eight microphones. 
We compare with the WPE- and beamforming-based methods, including WPE, WPE$+$BeamformIt and WPD, and the deep learning based methods proposed in \cite{wang_DeepLearningBased_2020}. The ESPnet implementation of WPE and WPE$+$BeamformIt are used. For WPD, the implementation \footnote{\url{https://github.com/Emrys365/espnet/blob/wsj1_mix_spatialized/espnet/nets/pytorch_backend/frontends/WPD_beamfomer_v6.py}} is used, where the predication delay and the number of taps are set to 3 and 5, respectively.

Table \ref{table_reverb} shows the enhancement results. 
%, from which we can know that the traditional methods perform comparably good as the single channel deep learning based method DCCRN$+$cTFA \cite{kothapally22b_interspeech}.
The proposed SpatialNet outperforms the comparison methods in terms of CD, FWSSNR, PESQ by a large margin, which demonstrates that the proposed network is able to largely suppress reverberation and meanwhile maintain the speech quality. Table \ref{table_reverb_asr} shows the ASR results. On SimData, all methods achieve good performance, as their WERs are close to the ones of clean speech. On RealData, the proposed method performs much better than WPE- and beamforming-based methods, especially for the even difficult ``far'' data. These results show that the proposed SpatialNet is very efficient for dereverberation in terms of both speech enhancement and ASR. Note that, the proposed networks are trained using simulated data, and thus suffer from the simulation-to-real generalization problem more or less. If the networks can be trained with sufficient real-recorded data, the performance can be further improved. 

% Compared with \cite{wang_DeepLearningBased_2020}, SpatialNet-small performs slightly better, but a much better results have been obtained by SpatialNet-large.
% Note that, both \cite{wang_DeepLearningBased_2020} and the proposed networks are trained with simulated RIRs and take the simulated direct-path signals as the training target, but the RIRs are simulated under different conditions, e.g. T60 and room size.
% % So the comparison might not be fair enough.
% Table \ref{table_reverb_asr} shows the WER results.
% On SimData, our models perform slightly better than traditional methods and obtained WER approach the WER of clean speech signals.
% On RealData, the proposed networks outperform the traditional networks in a large margin.

\begin{table}[t]
\setlength\tabcolsep{2pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Enhancement Results on 8-channel Reverb dataset. 
% DP means the reference signals for metric computation are generated by convolving the dry source with the direct-path part of the measured RIRs, i.e. from 2.5 ms before to 2.5 ms after the peak value of RIRs following \cite{wang_DeepLearningBased_2020}. 
PESQ with $^*$ stands for "NB PESQ MOS", otherwise for "WB MOS LQO".}
\label{table_reverb}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{5}{c}{SimData} & \multicolumn{1}{c}{RealData}\\
~ & CD$\downarrow$ & LLR$\downarrow$ & FWSSNR$\uparrow$ & PESQ$\uparrow$  & SRMR$\uparrow$ & SRMR$\uparrow$ \\
\hline
\multicolumn{7}{c}{Dry source signal is taken as the reference signal} \\
unproc. & 3.97 & 0.58 & 3.62 & 1.50 & 3.68 & 3.18 \\
\hline
WPE \cite{nakatani_SpeechDereverberationBased_2010,drude_NARAWPEPythonPackage_2018} & 3.48 & 0.45 & 6.05 & 1.96 & 4.85 & 5.04 \\
WPE$+$BeamformIt \cite{li2020espnet} & 2.66 & \textbf{0.34} & 8.72 & 2.22 & 5.17 & 5.62\\
WPD \cite{nakatani_UnifiedConvolutionalBeamformer_2019,zhang_EndtoEndFarFieldSpeech_2020} & 2.89 & 0.54 & 5.94 & 1.87 & \textbf{5.98} & 5.72 \\
%DCCRN$+$cTFA \cite{kothapally22b_interspeech} & dry & 2.74 & 0.36 & 8.76 & 2.83 & 5.28 & 5.90 \\
% SkipConvNet & & & & & & & & \\
% STSubNet & & & & & & & & \\
%SpatialNet-small (16) & dry & 1.43 & 0.41 & \textbf{13.30} & 3.60 & 4.58 & \textbf{6.98} \\
\hline
% SpatialNet-small (GBN) & 1.52 & 0.44 & 12.39 & 3.67 & 4.51 & \textbf{7.29} \\
% SpatialNet-large (GBN) & \textbf{1.32} & 0.38 & \textbf{13.03} & \textbf{3.77} & 4.37 & 6.97 \\
%SpatialNet-large (8) & dry & 1.45 & 0.38 & 13.12 & 3.78 & 4.42 & 6.79 \\
SpatialNet-small (prop.) & 1.52 & 0.44 & 12.51 & 3.66 & 4.65 & \textbf{7.18} \\
SpatialNet-large (prop.) & \textbf{1.36} & 0.39 & \textbf{13.04} & \textbf{3.83} & 4.43 & 6.88 \\
\hline\hline
\multicolumn{7}{c}{Direct-path signal is taken as the reference signal} \\
unproc. & 5.04 & 0.67 & 8.38 & 2.37$^*$ &  &  \\\hline
WPE \cite{nakatani_SpeechDereverberationBased_2010,drude_NARAWPEPythonPackage_2018} & 4.75 & 0.53 & 11.42 & 2.83$^*$ &  & \\
WPE$+$BeamformIt \cite{li2020espnet} & 3.94 & 0.49 & 12.52 & 3.12$^*$ &  &  \\
WPD \cite{nakatani_UnifiedConvolutionalBeamformer_2019,zhang_EndtoEndFarFieldSpeech_2020} & 4.15 & 0.49 & 10.50 & 2.80$^*$ &  &  \\
Wang et al. \cite{wang_DeepLearningBased_2020} & 2.78 & 0.39 & 18.75 & 3.71$^*$ &  &  \\
%SpatialNet-small (16) & dp & 2.41 & 0.30 & 20.39 & 3.95$^*$ & 4.58 & \textbf{6.98} \\
\hline
% SpatialNet-small (GBN) & 2.38 & 0.32 & 20.79 & 3.95$^*$ & 4.51   & \textbf{7.29} \\
% SpatialNet-large (GBN) & \textbf{2.25} & \textbf{0.28} & \textbf{22.15} & \textbf{4.02}$^*$ & 4.37 & 6.97 \\
SpatialNet-small (prop.) & 2.32 & 0.32 & 19.41 & 3.93$^*$ &  &  \\
SpatialNet-large (prop.) & \textbf{2.26} & \textbf{0.29} & \textbf{21.80} & \textbf{4.05}$^*$ &  &  \\
%SpatialNet-large (8) & dp & 2.21 & 0.29 & 20.87 & 4.04$^*$ & 4.42 & 6.79 \\
\hline\hline
\end{tabular}
}
\vspace{-0.2cm}
\end{table}


\begin{table}[t]
\setlength\tabcolsep{8pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.2}
\caption{WER (\%) results on 8-channel Reverb dataset.}
\label{table_reverb_asr}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\hline\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{SimData} & \multicolumn{2}{c}{RealData}\\
~ & far & near & far & near \\
\hline
unproc. & 4.9 & 3.7 & 6.5 & 5.9 \\
clean & 3.4 & 3.4 & - & - \\
\hline
WPE \cite{nakatani_SpeechDereverberationBased_2010,drude_NARAWPEPythonPackage_2018} & 3.8 & \textbf{3.5} & 4.6 & 4.9 \\
WPE$+$BeamformIt \cite{li2020espnet} & 3.7 & \textbf{3.5} & 4.4 & 3.6 \\
WPD \cite{nakatani_UnifiedConvolutionalBeamformer_2019,zhang_EndtoEndFarFieldSpeech_2020} & 3.8 & \textbf{3.5} & 4.5 & 4.1 \\
%SpatialNet-small (prop.)  & \textbf{3.6} & \textbf{3.5} & 2.9 & 3.2 \\
\hline
% SpatialNet-small (GBN)  & 3.7 & 3.6 & 2.8 & 3.3 \\
% SpatialNet-large (GBN)  & \textbf{3.6} & \textbf{3.5} & \textbf{2.7} & \textbf{3.0} \\
SpatialNet-small (prop.)  & \textbf{3.6} & 3.7 & \textbf{2.8} & 3.4 \\
SpatialNet-large (prop.)  & \textbf{3.6} & 3.6 & 3.1 & \textbf{3.2} \\
%SpatialNet-large (8)  & \textbf{3.6} & \textbf{3.5} & \textbf{3.2} & \textbf{3.1} \\
\hline\hline
\end{tabular}
% }
\vspace{-0.2cm}
\end{table}

% \subsection{Results on Circular Array Dataset}
% On the circular array dataset, we compare the proposed method with the following methods, for which publicly released code is available and the network is trained from scratch using the circular array dataset.

% \begin{itemize}[leftmargin=*]
%     \item FaSNet-TAC \cite{luo_end--end_2020}: A filter-and-sum network with transform-and-concatenate mechanism. % 说下训练的策略？
%     \item SepFormer \cite{subakan_attention_2021}: A transformer-based single channel speech separation model. SepFormer is modified in our experiments to account for the multichannel case, by simply changing the input channel of its first convolution layer from 1 to the number of microphone channels. Note that this may not be the optimal way to extend SepFormer to the multichannel case, but it still improves the performance when increasing the number of channels. 
%     \item Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022}: A two-stage neural beamformer. In the first stage, a neural network is used to estimate the multichannel speech signal of each speaker; while in the second stage, another neural network is applied to iteratively fine-grain the separated multichannel speech signals and apply MVDR beamforming. 
%     Beam-Guided TasNet was trained with 8 kHz signals in its original paper, we doubled the kernel size and stride of its filterbank layer for this 16 kHz dataset, as advised by its own authors.
%     % \item Oracle MVDR\footnote{https://github.com/Enny1991/beamformers}: Estimates the steering vector of target speaker by using the covariance matrices of ground truth speech signals.
%     \item NB-BLSTM \cite{quan_MultiChannelNarrowBandDeep_2022}: Our previously proposed narrow-band speech separation method. Instead of using the Conformer network proposed in this work, it uses two layers of bidirectional LSTM network.
% \end{itemize}

% In Table \ref{table_circular_dataset}, the results are reported for the four overlap ways. The results of using a sub-array with two or four microphones are also reported, where the sub-arrays are uniformly selected from the 8-channel array. The 2-, 4- and 8-channel arrays take the same  reference channel. 
% %The 2-channel (0-th and 4-th channels), 4-channel (0-th, 2-th, 4-th and 6-th channels), and 8-channel results of each algorithm are reported, except Beam Guided TasNet.
% For Beam-Guided TasNet, the 8-channel results are not reported, as it couldn't obtain reasonable performance in our experiments.
% The single-channel performance of SepFormer is also reported, as it's originally proposed for single-channel speech separation.
% %For the proposed method, two networks with different configurations are reported, i.e. NBC2-small and NBC2-large. The configurations of the two networks can be found in Table \ref{table_parameters}.

% From Table \ref{table_circular_dataset}, we can see that the performance of FasNet-TAC and our previously proposed NB-BLSTM are comparable, which indicates the effectiveness of narrow-band speech separation even with a simple BLSTM network. Beam-Guided TasNet and SepFormer notably outperform FasNet-TAC and NB-BLSTM. The success of SepFormer verifies that the self-attention mechanism is especially fit for speech separation, as clustering the frames of different speakers is one foundation for separating speakers. For speaker clustering, single-channel SepFormer may rely on some speaker features, while multichannel SepFormer may also rely on the spatial features as employed by the proposed narrow-band network. The proposed NBC2-small outperforms comparison methods by a large margin, by using only 0.9 M parameters. NBC2-large further largely improves the performance. This demonstrates that the narrow-band spatial information is highly discriminative for speech separation. The proposed method is effective to fully leverage this information by 1) setting a dedicated narrow-band network to focus on this information, and 2) leveraging a powerful improved conformer network. The computational complexity and thus RTF of the proposed network are actually relative high, since each frequency needs to run the network one time. NBC2-small basically reaches the real time requirement. 

% Among the four overlap ways, the \emph{full} overlap case achieves the worst performance, as it is the most difficult case due to the 100\% overlap ratio. The performance of \emph{head-tail} is the best. The performance of \emph{middle} and \emph{start-or-end} are comparable and worse than the one of \emph{head-tail}. The reason is: non-overlapped segments are important for extracting  separation features of individual speakers. \emph{head-tail} has non-overlapped segments for both the two speakers, while \emph{middle} and \emph{start-or-end} only for one of the two speakers. 

% For the two-speaker separation problem, using more microphones can generally promote the separation performance.
% As shown in Table \ref{table_circular_dataset}, the performance promotion is especially obvious for all the methods when increasing the number of microphones from two to four. 
% However, it becomes less significant when increasing the number of microphones further to eight, which indicates that the spatial information provided by four microphones is sufficiently discriminative for separating two speakers. 


% \begin{table*}[htb]
% \setlength\tabcolsep{4pt}
% % increase table row spacing, adjust to taste
% \renewcommand{\arraystretch}{1.3}
% \caption{Separation performance on circular array dataset. WB-PESQ and SDR (dB) are reported in the format of "WB-PESQ/SDR".}
% \label{table_circular_dataset}
% \centering
% \begin{tabular}{cccc|ccccc}
% \hline\hline
% \multirow{2}{*}{Method} & \multirow{2}{*}{\#CHN} & \multirow{2}{*}{\#Param} & \multirow{2}{*}{RTF} & \multicolumn{5}{c}{\textbf{Performance} (WB-PESQ / SDR (dB))}\\
%  & & & & Head-tail & Middle & Start-or-end & Full & \textbf{Average} \\
% \hline
% \multirow{1}{*}{unproc.} & 1/2/4/8 & - & - & 1.60 / \ 0.2 & 1.64 / \ 0.5 & 1.68 / \ 0.4 & 1.31 / \ 0.2 & 1.56 / \ 0.3 \\
% \hline
% \multirow{3}{*}{FaSNet-TAC \cite{luo_end--end_2020}} & 2 & 2.8 M & 0.27 & 2.32 / 11.9 & 2.18 / 10.6 & 2.24 / 11.0 & 1.86 / \ 9.0 & 2.15 / 10.6 \\
%  & 4 & 2.8 M & 0.45 & 2.49 / 13.0 & 2.33 / 11.7 & 2.39 / 12.1 & 2.04 / 10.1 & 2.31 / 11.7 \\
%  & 8 & 2.8 M & 0.73 & 2.54 / 13.2 & 2.38 / 11.9 & 2.45 / 12.4 & 2.09 / 10.3 & 2.37 / 12.0 \\
% \hline
% \multirow{2}{*}{Beam-Guided TasNet \cite{chen_BeamGuidedTasNetIterative_2022} (iter=2)} & 2 & 5.3 M & 0.50 & 2.88 / 15.4 & 2.66 / 13.9 & 2.70 / 14.1 & 2.36 / 12.1 & 2.65 / 13.9 \\
%  & 4 & 5.7 M & 0.61 & 3.12 / 17.4 & 2.83 / 15.4 & 2.84 / 15.5 & 2.50 / 13.4 & 2.82 / 15.4 \\
% \hline
% \multirow{4}{*}{SepFormer \cite{subakan_attention_2021}} & 1 & 25.7 M & 1.57 & 2.71 / 13.5 & 2.54 / 12.5 & 2.62 / 12.9 & 2.26 / 10.6 & 2.53 / 12.4 \\
%  & 2 & 25.7 M & 1.51 & 3.06 / 15.4 & 2.85 / 14.2 & 2.93 / 14.7 & 2.66 / 12.5 & 2.88 / 14.2 \\
%  & 4 & 25.7 M & 1.51 & 3.22 / 16.2 & 3.00 / 15.1 & 3.07 / 15.5 & 2.85 / 13.3 & 3.03 / 15.0 \\
%  & 8 & 25.7 M & 1.51 & 3.20 / 16.3 & 2.99 / 15.2 & 3.07 / 15.6 & 2.84 / 13.4 & 3.03 / 15.1 \\
% \hline
% \multirow{3}{*}{NB-BLSTM \cite{quan_MultiChannelNarrowBandDeep_2022}} & 2 & 1.2 M & 0.39 & 2.43 / 11.2 & 2.16 / \ 9.7 & 2.20 / \ 9.8 & 1.83 / \ 8.1 & 2.16 / \ 9.7 \\
%  & 4 & 1.2 M & 0.40 & 2.45 / 11.4 & 2.29 / 10.4 & 2.31 / 10.6 & 2.02 / \ 9.3 & 2.27 / 10.4 \\
%  & 8 & 1.2 M & 0.40 & 2.78 / 13.0 & 2.55 / 11.6 & 2.58 / 11.9 & 2.35 / 10.5 & 2.56 / 11.8 \\
% \hline
% \multirow{3}{*}{NBC2-small (prop.)} & 2 & 0.9 M & 0.96 & 3.71 / 19.1 & 3.43 / 17.4 & 3.48 / 17.8 & 3.35 / 15.9 & 3.49 / 17.5 \\
%  & 4 & 0.9 M & 0.97 & 3.89 / 21.2 & 3.62 / 19.2 & 3.66 / 19.6 & 3.54 / 17.7 & 3.68 / 19.4 \\
%  & 8 & 0.9 M & 0.97 & 3.87 / 21.0 & 3.59 / 19.0 & 3.64 / 19.4 & 3.57 / 17.7 & 3.67 / 19.3 \\
% \hline
% \multirow{3}{*}{NBC2-large (prop.)} & 2 & 5.6 M & 2.78 & 4.01 / 21.4 & 3.76 / 19.5 & 3.80 / 19.9 & 3.74 / 18.1 & 3.83 / 19.7 \\
%  & 4 & 5.6 M & 2.77 & 4.18 / 24.3 & 3.95 / 21.9 & 3.97 / 22.4 & 3.97 / 20.8 & 4.02 / 22.3 \\
%  & 8 & 5.6 M & 2.77 & 4.22 / 25.2 & 3.99 / 22.6 & 4.01 / 23.1 & 4.01 / 21.4 & 4.06 / 23.1 \\
% \hline\hline
% \end{tabular}
% \end{table*}

\subsection{Results on CHiME 3/4 Challenge Dataset}
The CHiME 3/4 challenge \cite{barker_ThirdCHiMESpeech_2015} provides both simulated and real-recorded datasets. Speech utterances from WSJ0 corpus are used. 
Multichannel speech are recorded with a speaker holding a tablet device equipped with 6 microphones. The second microphone is normally not used due to its low SNR recording.  
Multichannel background noise are recorded in four real environments, including a bus, cafe, pedestrian area, and street junction. The sampling rate is 16 kHz.
%Our preliminary experiments show that it is difficult to train the networks for processing the real-recorded evaluation data. (1) the quantity of real-recorded training data is limited; (2) the official simulated data are generated by delaying single-channel speech utterances, thence cannot handle reverberation; (3) Self-simulated data cannot well match the recording configuration, namely the speaker holds a tablet device. 
In the literature, the ASR performance on this dataset have been extensively developed, and reached a very high performance level. Therefore, we only report the speech enhancement performance on the simulated dataset. 
%We use the official script of the challenge to generate the simulated dataset, excepted that the training data are generated on the fly to facilitate the network training. 
Reverberation in this dataset is very small, as the multichannel speech is either simulated by delaying a single-channel speech, or recorded in a booth. Thence, this dataset only performs speech denoising, with real-recorded multichannel noise. 
%In the generation of the simulated dataset, time varying filters for direct-path signals are first estimated from the real-recorded signals, then convolved with clean speech signals.
%The clean audios of the simulated training set are taken from the WSJ0 corpus, while the clean audios of the simulated development and test set are recorded in a booth. 
%Noise signals are separately recorded or estimated by subtracting the estimated speech component from the real-recorded dataset, and are respectively used in the train or development/test dataset.
%The SNR is approximately 5 dB.

Table \ref{table_chime4_sim} shows the enhancement performance.
% The proposed SpatialNet outperforms all the compared methods by a large margin.
We can see that, narrow-band deep filtering (NBDF) \cite{li_narrow-band_2019} with a simple two-layer BLSTM network obtains better performance than MNMF beamforming \cite{shimada_UnsupervisedSpeechEnhancement_2019} and time-domain end-to-end FasNet$+$TAC \cite{luo_end--end_2020}, which proves the efficiency of learning narrow-band information. 
By flipping the first LSTM layer of NBDF from time axis to the frequency axis, FT-JNF \cite{tesch_InsightsDeepNonLinear_2023} jointly learns narrow-band and cross-band information, and  further improves the enhancement performance. This demonstrates that the cross-band information is complementary to narrow-band information.
Based on NBDF and FT-JNF, McNet \cite{yang2022mcnet} uses additional LSTM layers to learn the spectral difference between speech and noise, and further largely improve the performance.

The proposed SpatialNet shares a similar spirit with FT-JNF but is equipped with more powerful neural networks for respectively learning the narrow-band and cross-band spatial information. 
As shown in the table, SpatialNet largely outperforms other comparison methods, which shows the effectiveness of the proposed network architecture compared to LSTMs.
It is worth noting that SpatialNet-small slightly outperforms SpatialNet-large on this dataset, which is different from the results on other datasets. 
This is possibly because of the low-reverberation of this dataset, for which less information are needed to learn than other datasets. 
%i.e. low reverberation dataset is much simpler to enhance, thus the small network is enough to obtain a good performance.


\begin{table}[tpb]
\setlength\tabcolsep{2pt}
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.2}
\caption{Speech enhancement results on 5-channel Simulated CHiME 3/4 dataset. $^{\star}$ and $^{\ast}$ denote that the scores are quoted from \cite{shimada_UnsupervisedSpeechEnhancement_2019} and \cite{tolooshams_ChannelAttentionDenseUNet_2020}, respectively. }
\label{table_chime4_sim}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\hline\hline
\textbf{Method} & NB-PESQ & WB-PESQ & STOI & SI-SDR (dB) & SDR (dB) \\
\hline
unproc. & 1.82 & 1.27 & 0.870 & 7.5 & 7.5 \\
\hline
MNMF Beamforming \cite{shimada_UnsupervisedSpeechEnhancement_2019} & - & - & 0.940$^{\star}$ & - & 16.2$^{\star}$ \\
FasNet$+$TAC \cite{luo_end--end_2020} & 2.53 & 1.91 & 0.944 & 16.0 & 16.6 \\
CA Dense U-net \cite{tolooshams_ChannelAttentionDenseUNet_2020} & - & 2.44$^{\ast}$ & - & - & 18.6$^{\ast}$ \\
NBDF \cite{li_narrow-band_2019} & 2.74 & 2.13 & 0.950 & 15.7 & 16.6 \\
FT-JNF \cite{tesch_InsightsDeepNonLinear_2023} & 3.20 & 2.61 & 0.967 & 17.6 & 18.3 \\
McNet \cite{yang2022mcnet} & 3.38 & 2.73 & 0.976 & 19.2 & 19.6 \\
Oracle MVDR & 2.49 & 1.94 & 0.970 & 17.3 & 17.7 \\
\hline
SpatialNet-small (prop.) & \textbf{3.49} & 2.88 & \textbf{0.983} & \textbf{22.1} & \textbf{22.3} \\ % 8
% SpatialNet-small (16) & 3.37 & 2.72 & 0.980 & 21.4 & 21.6 \\
SpatialNet-large (prop.) & 3.45 & \textbf{2.89} & 0.982 & 21.8 & 22.1 \\
% SpatialNet-large (8) & 3.47 & 2.85 & 0.983 & 22.1 & 22.4 \\
\hline
\hline\hline
\end{tabular}
}
\vspace{-0.0cm}
\end{table}

% From Fig. \ref{fig:attn}, we observe two interesting points about how the proposed NBC2 network works:
% \begin{itemize}[leftmargin=*]
% \item \emph{Speaker Clustering.} 
% At the lower layers, the first Q-K map (for the second NBC block) shows that all the frames of two speakers attend to each other, and the two speakers are not well separated. While, at the higher layers, the second and fourth Q-K maps (for the fifth and eighth NBC blocks, respectively) show very strong speaker clustering patterns, as each head attends to only one of the two speakers. This can also be verified by the second and fourth F-K maps. This type of single-speaker head commonly presents at the higher layers of the network. %The second and the third Q-K and F-K images show very strong clustering patterns focusing on the second and the first speakers respectively, which is common in other examples in high layers of the network.
% Besides, at the higher layers, as shown in the third Q-K attention map (for the seventh NBC block), there also exists a few heads that both the two speakers are present in one head, but the frames of one speaker mainly attend to the frames of the same speaker.
% Overall, the proposed network is indeed performing speaker clustering based on the frame-wise spatial vectors, and the speaker clustering is gradually completed from the lower to the higher layers. 
% \item \emph{Reverberations Effect.} 
% For this example utterance, RT60 is 0.8 s and thus reverberation is heavy. 
% At the lower layers, as shown in the first Q-K map, the vertical attention lines mainly locate around the onsets of speech components. We can also observe from the first F-K map that only the onsets of speech components are notably attended. According to the precedence effect \cite{litovsky1999precedence}, the onsets of speech components are mainly composed of the direct-path propagation of sound as  reflections have not arrived yet. In other words, the network starts to learn knowledge from the TF bins that are less contaminated by reverberation. At the higher layers, as shown in the second Q-K map, the vertical attention lines become (visibly) wider relative to the ones in the first Q-K map. The second F-K  map also shows that more reverberant TF bins are attended. When the layer get further higher, 
% in the third and fourth Q-K maps, the attentions appear to be some slashes corresponding to the reverberation effect. The third and fourth F-K maps also show that even more reverberant TF bins are attended relative to the second F-K map. 
% The slashes are well temporally structured even though the proposed network does not use any type of positional embedding. We would like to note that these kind of slashes generally become longer when RT60 becomes larger, and are less prominent in low reverberation examples, thus the slashes can be considered as a proper modelling of reverberation.
% Overall, we can conclude that reverberation is modelled gradually from the lower to the higher layers.
% \end{itemize}




% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%% Figure environment removed

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%% Figure environment removed
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusions}
In this paper, we propose the SpatialNet to extensively leverage spatial information for multichannel joint speech separation, denoising and dereverberation.
SpatialNet is mainly composed of interleaved narrow-band and cross-band blocks, to respectively exploit narrow-band and cross-band spatial information. 
%The narrow-band blocks use self-attention mechanism and reinforced convolution layers to exploit the rich information in a single frequency.
%Specifically, the self-attention mechanism clusters the frame-wise spatial vectors by measuring their similarities; the enhanced convolutional layers are expected to compute speech statistics and model reverberation.
%The cross-band blocks use full-band and sub-band modules to respectively learn the spatial correlation between all frequencies and neighbouring frequencies in a single frame.
Experiments show that the proposed SpatialNet outperforms other state-of-the-art methods on various simulated and real-world tasks. The excellent performance of the proposed SpatialNet verifies that spatial information is highly discriminative for speech separation, denoising and dereverberation, and SpatialNet is effective to fully leverage these information. In addition, the proposed SpatialNet suffers little from the spectral generalization problem, and thus preforms well on a cross-language speech enhancement task. Currently, the proposed SpatialNet is only designed for offline (non-causal) processing, and the online version will be developed in the future.  

% Although the proposed network achieves satisfying speech separation performance on the 
% simulated two-speaker datasets in this work, the performance may degrade for more challenging scenarios, such as with more speakers, moving speakers and/or background noise, or on real-recorded data. The proposed network only exploits the narrow-band spatial information, which can be integrated with  full-band spectral/spatial information to further improve the performance, as future work. The fusion of full-band and sub-band/narrow-band have already been studied in \cite{hao_FullsubnetFullBandSubBand_2021,tesch2022insights,yang2022mcnet}, but for LSTM networks. In addition, this work only considers short utterances, and extending the proposed network for continuous speech separation will be done in the future.  



%We compared the separation performance of our method with other advanced methods on two datasets.
%The first is a spatialized version of the WSJ0-2mix dataset \cite{hershey_DeepClusteringDiscriminative_2016} with circular microphone arrays. 
%On this dataset, we conducted experiments and compared performance on 4 different speech overlap types and 3 different numbers of microphone settings (2-channel, 4-channel and 8-channel).
%Our results show that the proposed method surpasses other methods by a very large margin in all the considered speech overlap types and all the considered microphone number settings.
%Another dataset is a spatialized version of the WSJ0-2mix dataset with random microphone arrays \cite{wang_MultiChannelDeepClustering_2018}.
%On this dataset, we compared our methods with other public results.
%Our methods also surpasses the performance of other methods including MVDR beamforming based Beam-TasNet and Beam-Guided TasNet.
%Moreover, it's surprising that our proposed large version network also surpasses the oracle signal MVDR which can be regarded as the upper bound of Beam-TasNet and Beam-Guided TasNet.

% 由于网络只接触到了单个频带的信息。因此自然地具有谱无关性。
%As the narrow-band network can only know the spatial information in a single frequency, thus it‘s naturally spectra insensitive, which has been confirmed in our extended experiments.
%Spectra insensitivity are expected to cause less spectra distortions.
%We are going to verify that in automatic speech recognition tasks in our future work.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
%\section*{Acknowledgment}


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
%\ifCLASSOPTIONcaptionsoff
%  \newpage
%\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

%%%%%%%%%%%%%  References  %%%%%%%%%%%%%
%\footnotesize
\bibliographystyle{IEEEtran}

\bibliography{mybib}


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


