% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{vincent_BlindGuidedAudio_2014}
E.~Vincent, N.~Bertin, R.~Gribonval, and F.~Bimbot, ``From {{Blind}} to
  {{Guided Audio Source Separation}}: {{How}} models and side information can
  improve the separation of sound,'' \emph{IEEE Signal Processing Magazine},
  vol.~31, no.~3, pp. 107--115, May 2014.

\bibitem{gannot_consolidated_2017}
S.~Gannot, E.~Vincent, S.~Markovich-Golan, and A.~Ozerov, ``A {Consolidated}
  {Perspective} on {Multimicrophone} {Speech} {Enhancement} and {Source}
  {Separation},'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~25, no.~4, pp. 692--730, Apr. 2017.

\bibitem{Markovich-Golan2018b}
S.~Markovich-Golan, W.~Kellermann, and S.~Gannot, \emph{Audio Source Separation
  and Speech Enhancement}.\hskip 1em plus 0.5em minus 0.4em\relax Wiley, Sep.
  2018, ch. Spatial filtering.

\bibitem{gannot2001signal}
S.~Gannot, D.~Burshtein, and E.~Weinstein, ``Signal enhancement using
  beamforming and nonstationarity with applications to speech,'' \emph{IEEE
  Transactions on Signal Processing}, vol.~49, no.~8, pp. 1614--1626, 2001.

\bibitem{nakatani2010speech}
T.~Nakatani, T.~Yoshioka, K.~Kinoshita, M.~Miyoshi, and B.-H. Juang, ``Speech
  dereverberation based on variance-normalized delayed linear prediction,''
  \emph{IEEE Transactions on Audio, Speech, and Language Processing}, vol.~18,
  no.~7, pp. 1717--1731, 2010.

\bibitem{talmon2009convolutive}
R.~Talmon, I.~Cohen, and S.~Gannot, ``Convolutive transfer function generalized
  sidelobe canceler,'' \emph{IEEE transactions on audio, speech, and language
  processing}, vol.~17, no.~7, pp. 1420--1434, 2009.

\bibitem{li2019multichannel}
X.~Li, L.~Girin, S.~Gannot, and R.~Horaud, ``Multichannel speech separation and
  enhancement using the convolutive transfer function,'' \emph{IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, vol.~27, no.~3, pp.
  645--659, 2019.

\bibitem{nakatani_UnifiedConvolutionalBeamformer_2019}
T.~Nakatani and K.~Kinoshita, ``A {{Unified Convolutional Beamformer}} for
  {{Simultaneous Denoising}} and {{Dereverberation}},'' \emph{IEEE Signal
  Processing Letters}, vol.~26, no.~6, pp. 903--907, Jun. 2019.

\bibitem{winter_map-based_2006}
S.~Winter, W.~Kellermann, H.~Sawada, and S.~Makino, ``{MAP}-{Based}
  {Underdetermined} {Blind} {Source} {Separation} of {Convolutive} {Mixtures}
  by {Hierarchical} {Clustering} and $l_1$-{Norm} {Minimization},''
  \emph{EURASIP Journal on Advances in Signal Processing}, vol. 2007, no.~1,
  pp. 1--12, Dec. 2006.

\bibitem{boeddecker_front-end_2018}
C.~Boeddecker, J.~Heitkaemper, J.~Schmalenstroeer, L.~Drude, J.~Heymann, and
  R.~Haeb-Umbach, ``Front-end processing for the {CHiME}-5 dinner party
  scenario,'' in \emph{CHiME}, Sep. 2018, pp. 35--40.

\bibitem{yilmaz_blind_2004}
O.~Yilmaz and S.~Rickard, ``Blind {Separation} of {Speech} {Mixtures} via
  {Time}-{Frequency} {Masking},'' \emph{IEEE Transactions on Signal
  Processing}, vol.~52, no.~7, pp. 1830--1847, Jul. 2004.

\bibitem{luo_end--end_2020}
Y.~Luo, Z.~Chen, N.~Mesgarani, and T.~Yoshioka, ``End-to-end {Microphone}
  {Permutation} and {Number} {Invariant} {Multi}-channel {Speech}
  {Separation},'' in \emph{ICASSP}, May 2020, pp. 6394--6398.

\bibitem{gu_EndtoEndMultiChannelSpeech_2019}
R.~Gu, J.~Wu, S.-X. Zhang, L.~Chen, Y.~Xu, M.~Yu, D.~Su, Y.~Zou, and D.~Yu,
  ``End-to-{{End Multi-Channel Speech Separation}},'' \emph{arXiv preprint
  arXiv:1905.06286}, 2019.

\bibitem{tolooshams_ChannelAttentionDenseUNet_2020}
B.~Tolooshams, R.~Giri, A.~H. Song, U.~Isik, and A.~Krishnaswamy,
  ``Channel-{{Attention Dense U-Net}} for {{Multichannel Speech
  Enhancement}},'' in \emph{ICASSP}, May 2020, pp. 836--840.

\bibitem{heymann_blstm_2015}
J.~Heymann, L.~Drude, A.~Chinaev, and R.~Haeb-Umbach, ``{BLSTM} supported {GEV}
  beamformer front-end for the {3RD} {CHiME} challenge,'' in \emph{ASRU}, 2015,
  pp. 444--451.

\bibitem{ochiai_beam-tasnet_2020}
T.~Ochiai, M.~Delcroix, R.~Ikeshita, K.~Kinoshita, T.~Nakatani, and S.~Araki,
  ``Beam-{TasNet}: {Time}-domain {Audio} {Separation} {Network} {Meets}
  {Frequency}-domain {Beamformer},'' in \emph{ICASSP}, May 2020, pp.
  6384--6388.

\bibitem{wang_MultimicrophoneComplexSpectral_2021}
Z.-Q. Wang, P.~Wang, and D.~Wang, ``Multi-microphone {{Complex Spectral
  Mapping}} for {{Utterance-wise}} and {{Continuous Speech Separation}},''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~29, pp. 2001--2014, 2021.

\bibitem{chen_BeamGuidedTasNetIterative_2022}
H.~Chen, Y.~Yang, F.~Dang, and P.~Zhang, ``Beam-{{Guided TasNet}}: {{An
  Iterative Speech Separation Framework}} with {{Multi-Channel Output}},'' in
  \emph{Interspeech}, 2022, pp. 866--870.

\bibitem{gu_UnifiedAllNeuralBeamforming_2023}
R.~Gu, S.-X. Zhang, Y.~Zou, and D.~Yu, ``Towards {{Unified All-Neural
  Beamforming}} for {{Time}} and {{Frequency Domain Speech Separation}},''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~31, pp. 849--862, 2023.

\bibitem{habets2008generating}
E.~A. Habets, I.~Cohen, and S.~Gannot, ``Generating nonstationary multisensor
  signals under a spatial coherence constraint,'' \emph{The Journal of the
  Acoustical Society of America}, vol. 124, no.~5, pp. 2911--2917, 2008.

\bibitem{mohan2008localization}
S.~Mohan, M.~E. Lockwood, M.~L. Kramer, and D.~L. Jones, ``Localization of
  multiple acoustic sources with small arrays using a coherence test,''
  \emph{The Journal of the Acoustical Society of America}, vol. 123, no.~4, pp.
  2136--2147, 2008.

\bibitem{li2019waspaa}
X.~{Li} and R.~Horaud, ``Multichannel speech enhancement based on
  time-frequency masking using subband long short-term memory,'' in \emph{2019
  IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
  (WASPAA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 298--302.

\bibitem{li_narrow-band_2019}
X.~{Li} and R.~{Horaud}, ``Narrow-band {Deep} {Filtering} for {Multichannel}
  {Speech} {Enhancement},'' \emph{arXiv preprint arXiv:1911.10791}, 2019.

\bibitem{quan_MultiChannelNarrowBandDeep_2022}
C.~{Quan} and X.~Li, ``Multi-{{Channel Narrow-Band Deep Speech Separation}}
  with {{Full-Band Permutation Invariant Training}},'' in \emph{ICASSP}, 2022,
  pp. 541--545.

\bibitem{quan_MultichannelSpeechSeparation_2022}
C.~Quan and X.~Li, ``Multichannel {{Speech Separation}} with {{Narrow-band
  Conformer}},'' in \emph{Interspeech}, 2022, pp. 5378--5382.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{avargel_SystemIdentificationShortTime_2007}
Y.~Avargel and I.~Cohen, ``System {{Identification}} in the {{Short-Time
  Fourier Transform Domain With Crossband Filtering}},'' \emph{IEEE
  Transactions on Audio, Speech, and Language Processing}, vol.~15, no.~4, pp.
  1305--1319, May 2007.

\bibitem{hao2021fullsubnet}
X.~Hao, X.~Su, R.~Horaud, and X.~Li, ``Fullsubnet: A full-band and sub-band
  fusion model for real-time single-channel speech enhancement,'' in
  \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  6633--6637.

\bibitem{tesch_InsightsDeepNonLinear_2023}
K.~Tesch and T.~Gerkmann, ``Insights {{Into Deep Non-Linear Filters}} for
  {{Improved Multi-Channel Speech Enhancement}},'' \emph{IEEE/ACM Transactions
  on Audio, Speech, and Language Processing}, vol.~31, pp. 563--575, 2023.

\bibitem{wang_TFGridNetIntegratingFull_2022}
Z.-Q. Wang, S.~Cornell, S.~Choi, Y.~Lee, B.-Y. Kim, and S.~Watanabe,
  ``{{TF-GridNet}}: {{Integrating Full-}} and {{Sub-Band Modeling}} for
  {{Speech Separation}},'' Nov. 2022.

\bibitem{wang_DasformerDeepAlternating_2023}
S.~Wang, X.~Kong, X.~Peng, H.~Movassagh, V.~Prakash, and Y.~Lu, ``Dasformer:
  {{Deep Alternating Spectrogram Transformer For Multi}}/{{Single-Channel
  Speech Separation}},'' in \emph{ICASSP}, Jun. 2023, pp. 1--5.

\bibitem{li_MultichannelOnlineDereverberation_2019}
X.~Li, L.~Girin, S.~Gannot, and R.~Horaud, ``Multichannel {{Online
  Dereverberation}} based on {{Spectral Magnitude Inverse Filtering}},''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~27, no.~9, pp. 1365--1377, Sep. 2019.

\bibitem{yoshioka_GeneralizationMultiChannelLinear_2012}
T.~Yoshioka and T.~Nakatani, ``Generalization of {{Multi-Channel Linear
  Prediction Methods}} for {{Blind MIMO Impulse Response Shortening}},''
  \emph{IEEE Transactions on Audio, Speech, and Language Processing}, vol.~20,
  no.~10, pp. 2707--2720, Dec. 2012.

\bibitem{avargel_MultiplicativeTransferFunction_2007}
Y.~Avargel and I.~Cohen, ``On {{Multiplicative Transfer Function
  Approximation}} in the {{Short-Time Fourier Transform Domain}},'' \emph{IEEE
  Signal Processing Letters}, vol.~14, no.~5, pp. 337--340, May 2007.

\bibitem{mandel_ModelBasedExpectation_2010}
M.~Mandel, R.~Weiss, and D.~Ellis, ``Model-{{Based Expectation-Maximization
  Source Separation}} and {{Localization}},'' \emph{IEEE Transactions on Audio,
  Speech, and Language Processing}, vol.~18, no.~2, pp. 382--394, 2010.

\bibitem{roux_sdr_2019}
J.~L. Roux, S.~Wisdom, H.~Erdogan, and J.~R. Hershey, ``{SDR} ‚Äì {Half}-baked
  or {Well} {Done}?'' in \emph{ICASSP}, May 2019, pp. 626--630.

\bibitem{yu_permutation_2017}
D.~Yu, M.~Kolbaek, Z.-H. Tan, and J.~Jensen, ``Permutation invariant training
  of deep models for speaker-independent multi-talker speech separation,'' in
  \emph{ICASSP}, Mar. 2017, pp. 241--245.

\bibitem{vaswani_attention_2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  ≈.~Kaiser, and I.~Polosukhin, ``Attention is {All} you {Need},'' in
  \emph{Advances in {Neural} {Information} {Processing} {Systems}}, vol.~30,
  2017, pp. 5998--6008.

\bibitem{ba_LayerNormalization_2016}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton, ``Layer {{Normalization}},''
  \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{hendrycks_gaussian_2020}
D.~Hendrycks and K.~Gimpel, ``Gaussian {Error} {Linear} {Units} ({GELUs}),''
  \emph{arXiv preprint arXiv:1606.08415}, Jul. 2020.

\bibitem{ramachandran_searching_2017}
P.~Ramachandran, B.~Zoph, and Q.~V. Le, ``Searching for {Activation}
  {Functions},'' \emph{arXiv preprint arXiv:1710.05941}, Oct. 2017.

\bibitem{wu_group_2018}
Y.~Wu and K.~He, ``Group {Normalization},'' \emph{arXiv preprint
  arXiv:1803.08494}, Jun. 2018.

\bibitem{gulati2020conformer}
A.~Gulati, J.~Qin, C.-C. Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang,
  Z.~Zhang, Y.~Wu \emph{et~al.}, ``Conformer: Convolution-augmented transformer
  for speech recognition,'' \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{he_DelvingDeepRectifiers_2015}
K.~He, X.~Zhang, S.~Ren, and J.~Sun. Delving {{Deep}} into {{Rectifiers}}:
  {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}.

\bibitem{kingma2015adam}
D.~P. {Kingma} and J.~L. {Ba}, ``Adam: A method for stochastic optimization,''
  in \emph{International Conference on Learning Representations}, 2015.

\bibitem{drude_SMSWSJDatabasePerformance_2019}
L.~Drude, J.~Heitkaemper, C.~Boeddeker, and R.~{Haeb-Umbach}, ``{{SMS-WSJ}}:
  {{Database}}, performance measures, and baseline recipe for multi-channel
  source separation and recognition,'' Oct. 2019.

\bibitem{maciejewski_WHAMRNoisyReverberant_2020}
M.~Maciejewski, G.~Wichern, E.~McQuinn, and J.~L. Roux, ``{{WHAMR}}!: {{Noisy}}
  and {{Reverberant Single-Channel Speech Separation}},'' in
  \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax {Barcelona, Spain}:
  {IEEE}, May 2020, pp. 696--700.

\bibitem{wang_multi-channel_2018}
Z.-Q. Wang, J.~Le~Roux, and J.~R. Hershey, ``Multi-{Channel} {Deep}
  {Clustering}: {Discriminative} {Spectral} and {Spatial} {Embeddings} for
  {Speaker}-{Independent} {Speech} {Separation},'' in \emph{ICASSP}, Apr. 2018,
  pp. 1--5.

\bibitem{chen_ContinuousSpeechSeparation_2020}
Z.~Chen, T.~Yoshioka, L.~Lu, T.~Zhou, Z.~Meng, Y.~Luo, J.~Wu, X.~Xiao, and
  J.~Li, ``Continuous {{Speech Separation}}: {{Dataset}} and {{Analysis}},'' in
  \emph{ICASSP}, 2020, pp. 7284--7288.

\bibitem{kinoshita_ReverbChallengeCommon_2013}
K.~Kinoshita, M.~Delcroix, T.~Yoshioka, T.~Nakatani, E.~Habets,
  R.~{Haeb-Umbach}, V.~Leutnant, A.~Sehr, W.~Kellermann, R.~Maas, S.~Gannot,
  and B.~Raj, ``The reverb challenge: {{A}} common evaluation framework for
  dereverberation and recognition of reverberant speech,'' in \emph{2013 {{IEEE
  Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and
  {{Acoustics}}}.\hskip 1em plus 0.5em minus 0.4em\relax {New Paltz, NY}:
  {IEEE}, Oct. 2013, pp. 1--4.

\bibitem{barker_ThirdCHiMESpeech_2015}
J.~Barker, R.~Marxer, E.~Vincent, and S.~Watanabe, ``The third `{{CHiME}}'
  speech separation and recognition challenge: {{Dataset}}, task and
  baselines,'' in \emph{2015 {{IEEE Workshop}} on {{Automatic Speech
  Recognition}} and {{Understanding}} ({{ASRU}})}, Dec. 2015, pp. 504--511.

\bibitem{vincent_performance_2006}
E.~Vincent, R.~Gribonval, and C.~Fevotte, ``Performance measurement in blind
  audio source separation,'' \emph{IEEE Transactions on Audio, Speech and
  Language Processing}, vol.~14, no.~4, pp. 1462--1469, Jul. 2006.

\bibitem{le2019sdr}
J.~Le~Roux, S.~Wisdom, H.~Erdogan, and J.~R. Hershey, ``Sdr--half-baked or well
  done?'' in \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019,
  pp. 626--630.

\bibitem{rix_perceptual_2001}
A.~Rix, J.~Beerends, M.~Hollier, and A.~Hekstra, ``Perceptual evaluation of
  speech quality ({PESQ})-a new method for speech quality assessment of
  telephone networks and codecs,'' in \emph{ICASSP}, 2001, pp. 749--752.

\bibitem{taal2010short}
C.~H. Taal, R.~C. Hendriks, R.~Heusdens, and J.~Jensen, ``A short-time
  objective intelligibility measure for time-frequency weighted noisy speech,''
  in \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010, pp.
  4214--4217.

\bibitem{jensen2016algorithm}
J.~Jensen and C.~H. Taal, ``An algorithm for predicting the intelligibility of
  speech masked by modulated noise maskers,'' \emph{IEEE/ACM Transactions on
  Audio, Speech, and Language Processing}, vol.~24, no.~11, pp. 2009--2022,
  2016.

\bibitem{allen_image_1979}
J.~B. Allen and D.~A. Berkley, ``Image method for efficiently simulating
  small‚Äêroom acoustics,'' \emph{The Journal of the Acoustical Society of
  America}, vol.~65, no.~4, pp. 943--950, Apr. 1979.

\bibitem{Povey_Kaldi_ASRU2011}
D.~Povey, A.~Ghoshal, G.~Boulianne, L.~Burget, O.~Glembek, N.~Goel,
  M.~Hannemann, P.~Motlicek, Y.~Qian, P.~Schwarz, J.~Silovsky, G.~Stemmer, and
  K.~Vesely, ``The kaldi speech recognition toolkit,'' in \emph{IEEE 2011
  Workshop on Automatic Speech Recognition and Understanding}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE Signal Processing Society, Dec. 2011, iEEE
  Catalog No.: CFP11SRW-USB.

\bibitem{zhang_mc_convtasnet_2020}
J.~Zhang, C.~ZorilƒÉ, R.~Doddipatla, and J.~Barker, ``On end-to-end
  multi-channel time domain speech separation in reverberant environments,'' in
  \emph{ICASSP}, 2020, pp. 6389--6393.

\bibitem{wang_ConvolutivePredictionReverberant_2021}
Z.-Q. Wang, G.~Wichern, and J.~Le~Roux, ``Convolutive {{Prediction}} for
  {{Reverberant Speech Separation}},'' in \emph{WASPAA}, Oct. 2021, pp. 56--60.

\bibitem{taherian_LBT_2022}
H.~Taherian, K.~Tan, and D.~Wang, ``Multi-{{Channel Talker-Independent Speaker
  Separation Through Location-Based Training}},'' \emph{IEEE/ACM Transactions
  on Audio, Speech, and Language Processing}, vol.~30, pp. 2791--2800, 2022.

\bibitem{hershey_DeepClusteringDiscriminative_2016}
J.~R. Hershey, Z.~Chen, J.~Le~Roux, and S.~Watanabe, ``Deep clustering:
  {{Discriminative}} embeddings for segmentation and separation,'' in
  \emph{ICASSP}, 2016, pp. 31--35.

\bibitem{zhang_TimeDomainSpeechExtraction_2021}
J.~Zhang, C.~Zoril{\u a}, R.~Doddipatla, and J.~Barker, ``Time-{{Domain Speech
  Extraction}} with {{Spatial Information}} and {{Multi Speaker Conditioning
  Mechanism}},'' in \emph{ICASSP}, Jun. 2021, pp. 6084--6088.

\bibitem{panayotov_LibrispeechASRCorpus_2015}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``Librispeech: {{An ASR}}
  corpus based on public domain audio books,'' in \emph{ICASSP}, 2015, pp.
  5206--5210.

\bibitem{chen_continuous_2021}
S.~Chen, Y.~Wu, Z.~Chen, J.~Wu, J.~Li, T.~Yoshioka, C.~Wang, S.~Liu, and
  M.~Zhou, ``Continuous {Speech} {Separation} with {Conformer},'' in
  \emph{ICASSP}, Jun. 2021, pp. 5749--5753.

\bibitem{diaz-guerra_gpurir_2021}
D.~Diaz-Guerra, A.~Miguel, and J.~R. Beltran, ``{gpuRIR}: {A} python library
  for room impulse response simulation with {GPU} acceleration,''
  \emph{Multimedia Tools and Applications}, vol.~80, no.~4, pp. 5653--5671,
  Feb. 2021.

\bibitem{vonneumann_SASDRNovelLoss_2022}
T.~{von Neumann}, K.~Kinoshita, C.~Boeddeker, M.~Delcroix, and
  R.~{Haeb-Umbach}, ``{{SA-SDR}}: {{A Novel Loss Function}} for {{Separation}}
  of {{Meeting Style Data}},'' in \emph{ICASSP}, 2022, pp. 6022--6026.

\bibitem{wang_DeepLearningBased_2020}
Z.-Q. Wang and D.~Wang, ``Deep {{Learning Based Target Cancellation}} for
  {{Speech Dereverberation}},'' \emph{IEEE/ACM Transactions on Audio, Speech,
  and Language Processing}, vol.~28, pp. 941--950, 2020.

\bibitem{li2020espnet}
C.~Li, J.~Shi, W.~Zhang, A.~S. Subramanian, X.~Chang, N.~Kamo, M.~Hira,
  T.~Hayashi, C.~Boeddeker, Z.~Chen, and S.~Watanabe, ``{ESPnet-SE}: End-to-end
  speech enhancement and separation toolkit designed for {ASR} integration,''
  in \emph{SLT}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  785--792.

\bibitem{nakatani_SpeechDereverberationBased_2010}
T.~Nakatani, T.~Yoshioka, K.~Kinoshita, M.~Miyoshi, and B.-H. Juang, ``Speech
  {{Dereverberation Based}} on {{Variance-Normalized Delayed Linear
  Prediction}},'' \emph{IEEE Transactions on Audio, Speech, and Language
  Processing}, vol.~18, no.~7, pp. 1717--1731, Sep. 2010.

\bibitem{drude_NARAWPEPythonPackage_2018}
L.~Drude, J.~Heymann, C.~Boeddeker, and R.~{Haeb-Umbach}, ``{{NARA-WPE}}: {{A
  Python}} package for weighted prediction error dereverberation in {{Numpy}}
  and {{Tensorflow}} for online and offline processing,'' in \emph{Speech
  {{Communication}}; 13th {{ITG-Symposium}}}, Oct. 2018, pp. 1--5.

\bibitem{zhang_EndtoEndFarFieldSpeech_2020}
W.~Zhang, A.~S. Subramanian, X.~Chang, S.~Watanabe, and Y.~Qian, ``End-to-{{End
  Far-Field Speech Recognition}} with {{Unified Dereverberation}} and
  {{Beamforming}},'' in \emph{Interspeech 2020}.\hskip 1em plus 0.5em minus
  0.4em\relax {ISCA}, Oct. 2020, pp. 324--328.

\bibitem{shimada_UnsupervisedSpeechEnhancement_2019}
K.~Shimada, Y.~Bando, M.~Mimura, K.~Itoyama, K.~Yoshii, and T.~Kawahara,
  ``Unsupervised {{Speech Enhancement Based}} on {{Multichannel NMF-Informed
  Beamforming}} for {{Noise-Robust Automatic Speech Recognition}},''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~27, no.~5, pp. 960--971, May 2019.

\bibitem{yang2022mcnet}
Y.~Yang, C.~Quan, and X.~Li, ``{{MCNET}}: {{Fuse Multiple Cues}} for
  {{Multichannel Speech Enhancement}},'' in \emph{ICASSP}, Jun. 2023, pp. 1--5.

\end{thebibliography}
