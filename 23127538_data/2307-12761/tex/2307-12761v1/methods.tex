\section{Methodology}
    \subsection{LiDAR Inputs}
        The main differentiating characteristics between different LiDAR sensors are \emph{firstly} the number of channels they have \eg how many laser beams are used to scan the environment. \emph{Secondly}, the vertical field-of-view\,(FOV) that the respective sensors have. Both influence how big the vertical angular difference between individual measurements is and, thus, how dense the resulting depth measurements are. For instance, KITTI \cite{geiger_vision_2013} uses a LiDAR with 64 scanning channels while nuScenes\,\cite{holger_caesar_nuscenes_2019} only uses hardware with 32 channels. In other applications, one may prefer sensors with very few channels \cite{single-line:lidar,Voedisch_2022_RAL}. These differences cause significant distribution shifts in the datapoints sampled from the same scenes, posing challenges to state-of-the-art depth completion networks that require dedicated handling. Processing-wise, the concept of LiDAR channels allows fors emulation of lower channel sensors by dropping channels in the data of higher resolution LiDARs allowing the creation of multiple depth images from the same data. \
        
         % Figure environment removed
	
         \subsection{Depth Input Filtering}
    \label{sec:depth_input_filtering}
        Our experiments require the availability of different input styles for the sparse depth data. However, most of the datasets only feature one LiDAR sensor. To train and evaluate our method, we follow \cite{Voedisch_2022_RAL} to synthesise data of different LiDARs by filtering out a certain amount of input channels of a high-end LiDAR. We use the 64-channel LiDAR of KITTI as the high-end LiDAR and have designed two approaches to generate the depth data: \emph{First}, the number of channels is reduced by only considering every $n$-th channel of the original input such that it is sparsified. 
       This resembles a system with the same vertical FOV but a higher angle between adjacent LiDAR channels.
        \emph{Second}, the number of channels is reduced by removing all channels that are outside a specified interval of the vertical field of view $ [ c_{begin},  c_{end}] $. 
        This preserves the distance between adjacent channels but reduces the vertical FOV. The different effects of both filters are visualised in Fig.\,\ref{fig:filters}.
        KITTI does not directly provide the channels of the depth measurements. Hence, they were extracted by dividing the elevation angle of the points evenly, corresponding to the elevation resolution of the Velodyne\,HDL-64E LiDAR sensor used, allowing to assign a channel to each datapoint.


    \subsection{Base Architecture}\label{sec:base-architecture}
        % Figure environment removed
	    
        Our work utilizes the concept of Sparse-Auxiliary-Networks which were introduced by Guizilini\,\etal\,\cite{guizilini_sparse_nodate} being the first architecture that could perform both monocular depth prediction and full depth input depth completion with the same model. It consists of an encoder-decoder structure for the RGB-data with skip connections and a separate depth encoder for sparse depth input data using a mid-level fusion approach. More precisely, the features from the image and the LiDAR are fused at the skip connections as depicted in Fig.\,\ref{fig:ResSAN_base}. To alleviate the issues encountered with conventional convolutions on sparse data\,\cite{uhrig_sparsity_2017}, the depth encoder is implemented using sparse convolutions\,\cite{choy_4d_2019, yan_revisiting_2020} allowing to neglect spurious information from uninformative areas and also speeding up computations.
        
        From this structure, it follows that the fusion at the skip connections is a promising starting point for a generalization of the SAN architecture for LiDAR-adaptive depth completion. In the following section, we show how the fusion of the features at the skip connection of each abstraction level $i$
        \begin{align}
            \tilde{\mathbf{f}_i} = w_i \cdot \mathbf{f}_i^{\text{rgb}} + \mathbf{f}_i^{\text{lidar}} + b_i
            \label{eq:fusion}
        \end{align}
        can be used to achieve our goal. $\mathbf{f}_i^{\text{rgb}}$ represents the feature map from the RGB inputs while $\mathbf{f}_i^{\text{lidar}}$ is the densified depth feature map. Unlike in the original SAN paper where the fusion weights $w, b$ are trained but fixed at inference time, we use a weight adaption network that calculates fusion weights depending on the input type.
        
        For our implementation we chose to use a ResNet34\,\cite{he_deep_2016} based encoder-decoder structure mainly for reasons of computational cost as opposed to the very big PackNet\,\cite{guizilini_3d_2020} base structure in the reference work. It is from now on referred to as \emph{ResSAN}. Additionally, we include a Non-local Spatial Propagation Network\,(NLSPN)\,\cite{park_non-local_2020} in this architecture to improve the depth predictions. Therefore, the decoder is extended such that it predicts an initial depth estimate, affinity matrices and a depth confidence matrix which is then used by the NLSPN to refine the depth prediction. This module allows us to obtain state-of-the-art predictions with comparably small encoder-decoder structures.

        % Figure environment removed

    \subsection{LiDAR Adaptive Network}
        Our approach to extending the SAN concept is to view the task of feeding different kinds of LiDAR depth maps to the network as a multitask-like job. Specifically, each type of depth input that is to be put into the network is considered one task. Therefore, the network needs to have a third input besides RGB-image and depth image, which can be leveraged to infer the depth type of the current batch via a sensor encoder. This setting allows applying the ideas presented in \cite{sun_task_nodate}, namely that we do not have to have one specialized network for every single task but instead use one network with adaptive weights. In the case of the SAN architecture, the weights of the fusion module can be made adaptive as they control the fusion of LiDAR and RGB-related feature maps. The original work on SAN\,\cite{guizilini_sparse_nodate} already demonstrated that the decoder can be trained to work on two input settings properly. Therefore, it is expected that the decoder in this architecture can be trained to work with inputs that are in between the two extremes, no depth input and full LiDAR input, as well.
        
        One possible approach for implementation could be to use a task vector that explicitly specifies the LiDAR scanning pattern for the network. However, such a strategy would inherently fix the possible input style for depth at training time and therefore limit the general usability of the model. Consequently, a generalised encoding of the depth input type is favourable, seeing the space of possible input types more as a continuous space rather than discrete. Therefore, we derive the input for the sensor encoder directly from the input depth map. Applying this approach alleviates the problem of explicitly designing a suitable input space instead of allowing the sensor encoder network to extract the necessary information. Given that the fusion weights need to be adapted to differences in the density of depth values and/or the spatial distribution of the data points, the depth input image is preprocessed such that characteristics can still be extracted without providing too much additional information, which could adversely affect the training. Therefore, we decided to classify the pixels of the depth input 
    	\begin{align}
    		m(p) = \left\{
    		\begin{array}{ll}
    			1  & \mbox{if } p > 0, \\
    			0 & \mbox{otherwise } 
    		\end{array}
    		\right.
    	\end{align}
    	depending on whether they contain depth information or not with $p$ being the depth value of the pixel and $m(p)$ the value of the same pixel in the resulting depth mask. The result is a binary mask that does not contain any depth information but retains the characteristics of the density and spatial distribution of the pixels that contain depth information.
    	
    	This representation is subsequently passed through the sensor encoder, which consists of convolution blocks followed by a dense block as depicted in Fig.\,\ref{fig:ResSAN_implicit}. As we are operating on a binary input map and are interested in density information, the Average Pooling operation was chosen over the more commonplace MaxPooling. The output of the last convolutional layer is flattened, and passed to two dense layers, with the last one split into two parallel layers, one for each output vector\,($w, b$) that constitute the fusion weights. As introduced in (\ref{sec:base-architecture}) the last layer has a linear activation to allow for a sufficiently large value range of the fusion weights. The fusion weights predicted by the task encoder are then used like in the baseline architecture. The resulting overall architecture is from now on referred to as the \emph{Residual Lidar Adaptive Network\,(ResLAN)}.

