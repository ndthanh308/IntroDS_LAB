\section{Related Work} \label{sec:related}
    \paragraph{LiDAR Processing}
        Depth data obtained from LiDAR sensors distinguishes itself by its irregular sampling and sparsity by measurement principle. It generates a set of points in 3D space. In contrast, a camera records dense 2D data in a cartesian coordinate system but does not possess directly accessible depth information. For depth completion, the point cloud is usually projected onto a 2D plane allowing it to be processed akin to regular image data when using LiDAR data for creating depth maps\,\cite{chen_estimating_2018, guizilini_sparse_nodate, van_gansbeke_sparse_2019, lin_dynamic_2022}. Alternatively, the 3D nature of the data can be retained and specialised processing methods\,\cite{charles_pointnet_2017} be applied, which is more common for segmentation or object detection tasks\,\cite{shi_pointrcnn_2019}.

     \paragraph{LiDAR Depth Completion}
    	Depth completion, like depth prediction, aims to provide a model that generates a depth map from its inputs. In the latter case, only image data is available, making the setup simple and low-cost. Models learn to predict depth from an input image for every pixel inside it. This may involve fully supervised training \,\cite{laina_deeper_2016, eigen_depth_2014} or self-supervised methods \,\cite{ kok_review_2020, zhang_consistent_2021, patil_dont_2020}. In any case, the predictions of these models can only be correct up to a scale factor since depth prediction with one camera is an ill-posed problem. Thus, additional input data \eg LiDAR is required for accurate predictions leading to the adoption of depth completion networks. These models are usually designed as encoder-decoder networks where both inputs are firstly processed separately and then projected onto a common latent space from which they are processed in one stream. Ma and Karaman\,\cite{ma_sparse--dense_2018} note that, generally, the model performance for completion tasks is contingent on how well the models can treat cross-modality information as some of the early works \cite{kuga_multi-task_2017} did not manage to notably outperform depth prediction models when compared against each other. 
    	Besides the main combination of RGB images and LiDAR, some strategies further utilise semantic relations\,\cite{nazir_semattnet_2022}, or surface-normals\,\cite{xu_depth_2019}.

    
    \paragraph{LiDAR Adaption}
    Adapting depth completion networks to different LiDARs relates to the field of domain adaptation, with a change in LiDAR point sampling considered a domain change. A wide range of approaches for (unsupervised) domain adaptation has been developed during the past years\,\cite{yi_complete_2021, zhao_epointda_2021}. So far, concepts for depth estimation/completion have used style-transfer approaches on 2D data\,\cite{atapour-abarghouei_real-time_2018, zheng_t2net_2018}, and sparse-to-dense methods using synthetic data\,\cite{qiu_deeplidar_2019-1, atapour-abarghouei_complete_2019}. A different approach with a similar goal is pursued by Lopez-Rodriguez \etal\cite{lopez-rodriguez_project_2020}, who generate depth measurements from a scene simulation and use real LiDAR inputs from the target domain as a mask to sample depth measurements adapted to the target domain. RGB images are generated using a CycleGAN\,\cite{zhu_unpaired_2020}. Their findings give testament to the importance of the LiDAR sampling pattern when doing depth completion.
    While domain adaptation methods allow adjusting depth completion networks to different LiDARs, they still require training individual networks for each sensor. In contrast, the proposed meta depth completion can handle different sensors with a single network and even generalize to unseen beam patterns.
    A work addressing the challenge of LiDAR-agnostic sparse depth information is Tsai et al.\,\cite{tsai_see_2021}, which, however, focuses on object detection. Their solution is based on applying a ball pivoting algorithm on the point cloud to perform surface completion, yielding a dense surface model. This dense representation serves as the intermediate LiDAR agnostic representation. Lastly, depth information is always sampled in the same manner from the completed shape and passed to the depth completion model. While their concept works for object detection, it is practically infeasible for depth estimation as it would require computing one contiguous surface from all the image pixels. This would be computationally expensive and fail in regions with a low density of depth points.    
