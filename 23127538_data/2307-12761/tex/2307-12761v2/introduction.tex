\section{Introduction}
    % Figure environment removed
	Reliable and accurate depth prediction constitutes an essential computer vision capability needed to successfully execute a wide range of robotic tasks. Its use is especially prevalent for autonomously operating vehicles such as self-driving cars. Related tasks such as environment perception, navigation, and collision avoidance all require depth information. While RGB cameras are cheap, depth estimation based on single RGB images is very challenging \cite{wang_learning_2017, zhou_unsupervised_2017}. Therefore, there is strong research interest 
	 in how to supplement these methods with depth data from active sensors such as LiDARs\,\cite{uhrig_sparsity_2017, van_gansbeke_sparse_2019, ma_self-supervised_2019, guizilini_multi-frame_2022, liu_graphcspn_2022} or Radar\,\cite{long_radar-camera_2021, radar:depth:20,lo_depth_2021}. These active sensors generate direct but sparse depth measurements while cameras record dense RGB values. Hence, combining both can make the best of the two worlds. 
	 Depth completion models combine both sensor modalities and achieve remarkable performance for dense depth prediction. Using LiDARs, however, comes with a significant caveat that has not yet received due attention. Different LiDARs produce varying scanning patterns, which can cause significant performance degradation when a trained depth completion model is applied to another LiDAR. Thus, the LiDARs contribute notably to the domain gap between different datasets in a depth completion setting requiring a model to be retrained if changes to the sparse depth distribution are made. This issue poses a problem if, for instance, a depth completion model is created for a specific type of LiDAR, but multiple types of LiDARs may be used depending on the market cost or the user's applications during deployment. Additionally, recent LiDAR development has given rise to sensors with flexible scan patterns that can be adapted in real-time, like the Bajara Spectrum-Scan\,\cite{noauthor_bajara_nodate}, which also requires adaptive depth completion methods.
		
	To tackle this problem, we conceive a depth completion model that is structured to operate well on multiple depth input distributions without the need to retrain new models. To achieve that, we propose a meta depth completion network based on the Sparse Auxiliary Network Structure\,(SAN)\,\cite{guizilini_sparse_nodate}. We propose a novel extension to this network that enables LiDAR adaptive depth completion for the first time. The meta depth completion network contains a sensor encoder network, which learns to alter the weights of the main depth completion network to perform depth completion conditioned on the used sensor. It derives its input directly from the LiDAR sampling pattern, which enables it to generalize even to unseen data distributions e.g.\,LiDAR types.
	
	To train and evaluate our method, we simulate data of different types of LiDARs by filtering the data of a high-end LiDAR sensor such as a 64-beam LiDAR. This approach allows us to use the same dataset and avoid introducing any other types of domain gaps.
		
	Our main contributions summarize as follows:
	\begin{itemize}
	    \item To the best of our knowledge, we are the first to study LiDAR-adaptive meta depth completion. While previous methods require training a separate model for each used LiDAR, our novel approach dynamically adapts the model to the used scanning pattern of different LiDARs. This allows flexible deployment of a single depth completion model on different sensors.
        \item Specifically, we propose a novel LiDAR adaptive depth completion method that can perform depth prediction as well as completion for multiple types of LiDAR scanning patterns. This is achieved by introducing weight adaption networks that dynamically control the weighting of depth and image features when fusing both together.
        \item The proposed architecture significantly outperforms a non-adaptive baseline trained on different LiDAR patterns. Further, it even outperforms a specialized depth completion baseline, which is trained for a specific scanning pattern in the case of very sparse depth input. Our method is further capable of generalizing to previously unseen depth sample distributions.
    \end{itemize}
