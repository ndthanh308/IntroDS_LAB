\section{Supplement}
\subsection{Overview}
    The supplementary material elucidates further on the filtering of the LiDAR depth input in Sec.\,\ref{app:depth} and a more detailed explanation of the experimental procedure\,\ref{app:proc}.
\subsection{Depth Data Filtering}\label{app:depth}
	For our experiments, we have designed two classes of augmented LiDAR data. \emph{Firstly}, the number of channels is reduced by only considering every $n$-th channel of the original input. This resembles a system with the same vertical FOV but a higher angle between adjacent LiDAR channels. From now on, this filter will be referred to as sparse channel filter for briefness' sake. \emph{Secondly}, the number of channels is reduced by removing all channels that are outside a specific interval, therefore, keeping the angle between adjacent channels constant but reducing the vertical FOV.\\

	\paragraph{Channel identification}
	The datapoints in the depth input image do not contain any explicit information on the channel (i. e. the laser beam that captured them), making it necessary to process the depth map in a way that a LiDAR channel can be assigned to each data point. For this, we use the property of the Velodyne that the vertical angle\,$\theta$ of each laser can be assumed to be nearly constant. Additionally, the angular difference between two adjacent lasers is constant for the used device. Based on this, it is possible to map each channel to a specific vertical angle interval. Using the characteristics specified for the Velodyne 
	{\small
	\begin{align}
		\theta_{\min} & =-24.9^\circ \\
		\theta_{\max} &= +2.0^\circ \\
		\Delta \theta &= \theta_i - \theta_{i-1} = \text{const.}, \hspace{1.5cm} i \in 0,...,63
	\end{align}%
	}%
	the channel 
	{\small
	\begin{align}
		c(\theta) = \text{round}\left( (\theta - \theta_{\min}) \cdot \frac{63.99}{\theta_{\max}-\theta_{\min}} - 0.5 \right)
	\end{align}%
	}%
	is estimated using the aforementioned considerations utilizing the rounding operation as the classifier that returns an integer class label in the range of $0,1,..,63$. $\theta$ represents the vertical angle with respect to a horizontal line, $\theta_{\min}$ is the minimum vertical angle and $\theta_{\max}$ is the maximum one.\\
	
	\paragraph{Re-projection into 3D space}
	The aforementioned estimation algorithm to generate a channel label for each datapoint requires that $\theta$ is known, which is not initially the case. The positional data of each depth point is compressed to a Cartesian pixel coordinate in the image. Extracting the vertical angle requires projecting each datapoint back into 3D space. Such a projection from 2D to 3D space would normally be an ill-posed problem. However, as we have the depth information on each valid pixel, the problem has a unique well-posed solution in our case. From the calibration matrix of the camera where skew coefficient $s=0$, one can obtain the parameters necessary to perform the derivations
	{\small
	\begin{align}
		\frac{X}{Z} &= \frac{x - u_0}{fk_x}\\
		\frac{Y}{Z} &= \frac{y - v_0}{fk_y}\\
		d &= \sqrt{X^2 + Y^2 + Z^2}
	\end{align}%
	}%
	taken from the geometrical image formation formulas and the Euclidean distance in 3D. $(x,y)$ are the pixel coordinates of the depth point, $(X,Y,Z)$ the corresponding coordinates in 3D space, $fk_x, fk_y$ the focal lengths in terms of pixels and ($u_0$, $v_0$) the principal point.
	Having the value of $Y$ allows for the calculation of the vertical angle
	{\small
	\begin{align}
		\theta = \arcsin \left(\frac{Y}{d + \epsilon} \right)
	\end{align}%
	}%
	with $\epsilon = 10^{-5}$ being a modification for computational reasons to avoid NaN in case of depth values of zero allowing to process the whole image and filter out the invalid pixels later.
	
	\paragraph{Filtering}
	% Figure environment removed
	
	After assigning a LiDAR channel to each datapoint in the depth image, it is now possible to perform the filtering. For the first filter method which keeps every $n$-th channel, a datapoint is kept if its channel satisfies
	{\small
	\begin{align}
		0 = c(\theta(p)) \mod n \hspace{0.4cm},
	\end{align}%
	}%
	with $c(\theta(p))$ being the channel of the individual pixel $p$ that has the vertical angle $\theta(p)$. Datapoints which do not satisfy this condition are discarded and replaced with zero values.
	The second method keeps all datapoints that have a channel within
	{\small
	\begin{align}
		c(\theta(p)) \in [ c_{begin},  c_{end}]
	\end{align}%
	}%
	with $ c_{begin}$ being the lowest channel and $c_{end}$ being the highest channel that is kept. The precise settings used in the experiments are illustrated in Fig.\,\ref{fig:FOV-vis} and provided in detail in Tab.\,\ref{tab:FOV-vis}.

    \begin{table}[tbh]
    \centering
        \caption{Starting and End channels for the different filter settings used for the Field-of-View filter.}
        \begin{tabular}{@{}ccc@{}}
            \toprule
            \multicolumn{3}{c}{\textbf{Field-of-View Filter}} \\ \midrule
            Nr. channels    & $ch_{begin}$    & $ch_{end}$\    \\ \midrule
            64              & 0               & 63            \\
            56              & 7               & 63            \\
            48              & 7               & 55            \\
            32              & 17              & 48            \\
            24              & 25              & 48            \\
            16              & 33              & 48            \\
            8               & 33              & 40            \\ \bottomrule
        \end{tabular}
        \label{tab:FOV-vis}
    \end{table}

\newpage
\subsection{Experimental Procedure}\label{app:proc}

This section illustrates further details of the training procedure used to generate the results in Sec.\,\ref{sec:results} and also further analysis of the latter.\\

\paragraph{Sparse Channel Filter}
For the sparse channel filter, all models were evaluated for filter settings that retain 64, 32, 21, 16 and 8 scan lines of the LiDAR sensor. Two models were trained for channels ranging from 64 to 16 channels. One was trained for 64, 32, and 16 channels while the other was also trained for 21 channels. This allows us to compare the effect of the number of filter settings on the performance. As shown in Fig.\,\ref{fig:explicit-comp} this has minuscule consequences. Moreover, a third model was trained on a channel range from 64 to 8 channels with training for 64, 32, 16, 12 and 8 channels focusing more on cases of very sparse LiDAR data. It can be noted that extending the range of possible sparsity levels is not beneficial to overall performance since this method suffers from slight performance degradation for all filter settings except for the 8-channel case where the other models needed to extrapolate.\\

\paragraph{Field-of-View Filter}
For the FOV filter, we likewise trained two models on the same range of retained scan lines. Both were evaluated for 16, 32, 40, 48, 56 and 64 channels. One of the models was trained for 64, 48, and 32 channels. The other one was moreover also trained for data that retained 40 and 48 channels reducing the difference in channels between the filter settings. Especially for the MAE metric we can observe that the model that was trained for more FOV filter settings performs slightly better as can be seen from Fig.\,\ref{fig:implicit-comp}. Noteworthy is that it also exhibits a better generalisation capability when presented with 16-channel data, which it was not trained for. The third model was trained for 5 different channel filters but this time on the range from 64 channels to 8 channels. As previously seen with the sparse channel filter, extending the range of filter settings for the FOV entails small overall performance losses.
