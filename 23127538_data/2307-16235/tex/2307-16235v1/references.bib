%%%%%%%%%%
% books
%%%%%%%%%%

@book{haykin,
    author={Haykin, Simon},
    title={Neural networks and learning machines},
    year={2009},
    publisher={Pearson},
    edition={3}
}

@book{goodfellow,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@book{gerstner,
    title={Spiking neuron models: Single neurons, populations, plasticity},
    author={Gerstner, Wulfram and Kistler, Werner M},
    publisher={Cambridge university press},
    year={2002}
}

@book{gerstner_neuronal,
  title={Neuronal dynamics: From single neurons to networks and models of cognition},
  author={Gerstner, Wulfram and Kistler, Werner M and Naud, Richard and Paninski, Liam},
  year={2014},
  publisher={Cambridge University Press}
}

@book{hoffman,
  title={Numerical methods for engineers and scientists},
  author={Hoffman, Joe D and Frankel, Steven},
  year={2018},
  publisher={CRC press}
}

@book{nocedal,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{cover,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@techreport{anderson,
  title={An introduction to multivariate statistical analysis},
  author={Anderson, Theodore W},
  year={1958}
}

@book{magnus,
  title={Matrix differential calculus with applications in statistics and econometrics},
  author={Magnus, Jan R and Neudecker, Heinz},
  year={2019},
  publisher={John Wiley \& Sons}
}

@article{hyvarinen,
  title={Independent component analysis},
  author={Hyvarinen, Aapo and Karhunen, Juha and Oja, Erkki},
  journal={Studies in informatics and control},
  volume={11},
  number={2},
  pages={205--207},
  year={2002},
  publisher={INFORMATICS AND CONTROL PUBLICATIONS}
}

@book{cichocki,
  title={Adaptive blind signal and image processing: learning algorithms and applications},
  author={Cichocki, Andrzej and Amari, Shun-ichi},
  year={2002},
  publisher={John Wiley \& Sons}
}

@book{oreilly,
    title={Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain},
    author={O'Reilly, Randall C and Munakata, Yuko},
    year={2000},
    publisher={MIT press}
}

@book{lytton,
    title={From Computer to Brain: Foundations of Computational Neuroscience},
    author={Lytton, William W},
    year={2002},
    publisher={Springer Science \& Business Media}
}

@book{dayan,
  title={Theoretical neuroscience: computational and mathematical modeling of neural systems},
  author={Dayan, Peter and Abbott, Laurence F},
  year={2001},
  publisher={Computational Neuroscience Series}
}

@book{white,
    title={Cortical circuits: synaptic organization of the cerebral cortex: structure, function, and theory},
    author={White, Edward L and Keller, Asaf},
    year={1989},
    publisher={Springer}
}

@book{sheperd,
  title={The synaptic organization of the brain},
  author={Sheperd, Gordon M},
  publisher={Oxford University Press},
  year={1990}
}

@book{bear,
  title={Neuroscience: Exploring the brain},
  author={Bear, Mark and Connors, Barry and Paradiso, Michael A},
  year={2020},
  publisher={Jones \& Bartlett Learning, LLC}
}

@book{purves,
  title={Neuroscience},
  author={Purves, Dale},
  year={2001},
  publisher={Sinauer Associates Publishers}
}

@book{kandel,
  title={Principles of neural science},
  author={Kandel, Eric R and Schwartz, James H and Jessell, Thomas M and Department of Biochemistry and Molecular Biophysics Thomas Jessell and Siegelbaum, Steven and Hudspeth, AJ},
  volume={4},
  year={2000},
  publisher={McGraw-hill New York}
}


%%%%%%%%%%
% our work
%%%%%%%%%%

@misc{neurolab,
  author       = {Lagani, Gabriele}, 
  title        = {Neurolab framework for bio-inspired deep learning experiments},
  year         = {2022},
  url={https://github.com/GabrieleLagani/HebbianLearning}
}

@mastersthesis{mthesis,
  author       = {Lagani, Gabriele}, 
  title        = {Hebbian learning algorithms for training convolutional neural networks},
  school       = {School of Engineering, University of Pisa, Italy},
  year         = {2019},
  url = {https://etd.adm.unipi.it/theses/available/etd-03292019-220853/}
}

@phdthesis{phdthesis,
  title={Bio-Inspired Approaches for Deep Learning: From Spiking Neural Networks to Hebbian Plasticity},
  author={Lagani, Gabriele},
  year={2023},
  school={School of Engineering, University of Pisa, Italy},
  url={https://etd.adm.unipi.it/t/etd-05022023-121539}
}

@article{survey_plasticity,
  title={Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey},
  author={Amato, Giuseppe and Carrara, Fabio and Falchi, Fabrizio and Gennaro, Claudio and Lagani, Gabriele},
  year={2023}
}

@article{survey_snn,
  title={Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey},
  author={Amato, Giuseppe and Carrara, Fabio and Falchi, Fabrizio and Gennaro, Claudio and Lagani, Gabriele},
  year={2023}
}

@inproceedings{lagani2019,
  title={Hebbian Learning Meets Deep Convolutional Neural Networks},
  author={Amato, Giuseppe and Carrara, Fabio and Falchi, Fabrizio and Gennaro, Claudio and Lagani, Gabriele},
  booktitle={International Conference on Image Analysis and Processing},
  pages={324--334},
  year={2019},
  organization={Springer}
}

@inproceedings{lagani2021a,
  title={Assessing Pattern Recognition Performance of Neuronal Cultures through Accurate Simulation},
  author={Lagani, Gabriele and Mazziotti, Raffaele and Falchi, Fabrizio and Gennaro, Claudio and Cicchini, Guido Marco and Pizzorusso, Tommaso and Cremisi, Federico and Amato, Giuseppe},
  booktitle={2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)},
  pages={726--729},
  year={2021},
  organization={IEEE}
}

@article{lagani2021b,
  title={Hebbian semi-supervised learning in a sample efficiency setting},
  author={Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
  journal={Neural Networks},
  volume={143},
  pages={719--731},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{lagani2021c,
  title={Evaluating Hebbian Learning in a Semi-supervised Setting},
  author={Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
  booktitle={International Conference on Machine Learning, Optimization, and Data Science},
  pages={365--379},
  year={2021},
  organization={Springer}
}

@inproceedings{lagani2021d,
  title={Training Convolutional Neural Networks with Competitive Hebbian Learning Approaches},
  author={Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
  booktitle={International Conference on Machine Learning, Optimization, and Data Science},
  pages={25--40},
  year={2021},
  organization={Springer}
}

@article{lagani2022a,
  title={Comparing the performance of Hebbian against backpropagation learning using convolutional neural networks},
  author={Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
  journal={Neural Computing and Applications},
  volume={34},
  number={8},
  pages={6503--6519},
  year={2022},
  publisher={Springer}
}

@article{lagani2022b,
  title={Deep Features for CBIR with Scarce Data using Hebbian Learning},
  author={Lagani, Gabriele and Bacciu, Davide and Gallicchio, Claudio and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
  journal={arXiv preprint arXiv:2205.08935},
  year={2022}
}

@inproceedings{lagani2022c,
  title={FastHebb: Scaling Hebbian Training of Deep Neural Networks to ImageNet Level},
  author={Lagani, Gabriele and Gennaro, Claudio and Fassold, Hannes and Amato, Giuseppe},
  booktitle={Similarity Search and Applications: 15th International Conference, SISAP 2022, Bologna, Italy, October 5--7, 2022, Proceedings},
  pages={251--264},
  year={2022},
  organization={Springer}
}


%%%%%%%%%%
% hebbian clustering
%%%%%%%%%%

% first work on competitive learning
@article{malsburg1973, 
    title={Self-organization of orientation sensitive cells in the striate cortex},
    author={Von der Malsburg, Chr},
    journal={Kybernetik},
    volume={14},
    number={2},
    pages={85--100},
    year={1973},
    publisher={Springer}
}

% pioneering work on competitive learning
@article{grossberg1976a, 
    title={Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors},
    author={Grossberg, Stephen},
    journal={Biological cybernetics},
    volume={23},
    number={3},
    pages={121--134},
    year={1976},
    publisher={Springer}
}

% adaptive resonance theory (ART)
@article{grossberg1976b,
  title={Adaptive pattern classification and universal recoding: II. Feedback, expectation, olfaction, illusions},
  author={Grossberg, Stephen},
  journal={Biological cybernetics},
  volume={23},
  number={4},
  pages={187--202},
  year={1976},
  publisher={Springer}
}

% adaptive resonance theory (ART)
@article{grossberg2013,
  title={Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world},
  author={Grossberg, Stephen},
  journal={Neural Networks},
  volume={37},
  pages={1--47},
  year={2013},
  publisher={Elsevier}
}

% pioneering work on competitive learning 
@article{rumelhart1985, 
    title={Feature discovery by competitive learning},
    author={Rumelhart, David E and Zipser, David},
    journal={Cognitive science},
    volume={9},
    number={1},
    pages={75--112},
    year={1985},
    publisher={Elsevier}
}

% Vector quantization
@article{gray1984,
  title={Vector quantization},
  author={Gray, Robert},
  journal={IEEE Assp Magazine},
  volume={1},
  number={2},
  pages={4--29},
  year={1984},
  publisher={IEEE}
}

% WTA with "coscience" term
@inproceedings{desieno1988,
  title={Adding a conscience to competitive learning.},
  author={DeSieno, Duane},
  booktitle={ICNN},
  volume={1},
  year={1988}
}

% k-WTA
@inproceedings{majani1989, 
  title={On the K-winners-take-all network},
  author={Majani, E and Erlanson, Ruth and Abu-Mostafa, Yaser S},
  booktitle={Advances in neural information processing systems},
  pages={634--642},
  year={1989}
}

% soft WTA
@inproceedings{nowlan1990, 
  title={Maximum likelihood competitive learning},
  author={Nowlan, Steven J},
  booktitle={Advances in neural information processing systems},
  pages={574--582},
  year={1990}
}

% Split-and-merge strategy for dynamic clustering: merge centroids of similar small clusters into a unique cluster and split centroids of large clusters into multiple clusters, in order to adaptively find the best cluster subdivision
@article{kaukoranta1998,
  title={Iterative split-and-merge algorithm for vector quantization codebook generation},
  author={Kaukoranta, Timo and Franti, Pasi and Nevalainen, Olli},
  journal={Optical Engineering},
  volume={37},
  number={10},
  pages={2726--2732},
  year={1998},
  publisher={SPIE}
}

% soft cluster assignments improve performance in scene categorization
@inproceedings{vangemert2008,
  title={Kernel codebooks for scene categorization},
  author={Van Gemert, Jan C and Geusebroek, Jan-Mark and Veenman, Cor J and Smeulders, Arnold WM},
  booktitle={European conference on computer vision},
  pages={696--709},
  year={2008},
  organization={Springer}
}

% Lobe component analysis, equivalent to angular clustering with in-place learning rules (i.e. local)
@inproceedings{weng2006,
  title={Optimal in-place learning and the lobe component analysis},
  author={Weng, Juyang and Zhang, Nan},
  booktitle={The 2006 IEEE International Joint Conference on Neural Network Proceedings},
  pages={3887--3894},
  year={2006},
  organization={IEEE}
}

% Multi-Layer In-place Network (MILN) based on lobe component analysis for stacked in-place learning. 6-layers like visual cortex. Also, combined with top-down interaction in a predictive coding fashion for supervised learning.
@article{weng2007,
  title={A multilayer in-place learning network for development of general invariances},
  author={Weng, Juyang and Luwang, Tianyu and Lu, Hong and Xue, Xiangyang},
  journal={International Journal of Humanoid Robotics},
  volume={4},
  number={02},
  pages={281--320},
  year={2007},
  publisher={World Scientific}
}

% Product quantization: decompose vector space into product of linear subspaces, and perform VQ on each subspace separately
@article{jegou2010,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

% Network of WTA units and sgd training applied to computer vision - mnist 1.28% error with single fc hidden WTA layer, 0.57% with 2 hidden conv layers relu + 2 hidden fc WTA layers
@inproceedings{srivastava2013,
  title={Compete to Compute.},
  author={Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino J and Schmidhuber, J{\"u}rgen},
  booktitle={NIPS},
  pages={2310--2318},
  year={2013},
  organization={Citeseer}
}

% additive quantization: after quantization, requantize the residual error iteratively to improve quantization precision.
@inproceedings{babenko2014,
  title={Additive quantization for extreme vector compression},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={931--938},
  year={2014}
}

% som
@article{kohonen1982, 
    title={Self-organized formation of topologically correct feature maps},
    author={Kohonen, Teuvo},
    journal={Biological cybernetics},
    volume={43},
    number={1},
    pages={59--69},
    year={1982},
    publisher={Springer}
}

% More about SOM
@article{kohonen1993, 
    title={Things you haven't heard about the Self-Organizing Map},
    author={Kohonen, Teuvo},
    journal={IEEE International Conference on Neural Networks},
    pages={1147--1156},
    year={1993},
    organization={IEEE}
}

% som theoretical aspects
@article{lo1991a,
    title={Analysis of neighborhood interaction in Kohonen neural networks},
    author={Lo, Zhen-Ping and Fujita, Masahiro and Bavarian, Behnam},
    journal={[1991] Proceedings. The Fifth International Parallel Processing Symposium},
    pages={246--249},
    year={1991},
    organization={IEEE}
}

% som theoretical aspects
@article{lo1993,
    title={Analysis of the convergence properties of topology preserving neural networks},
    author={Lo, Z-P and Yu, Yaoqi and Bavarian, Behnam},
    journal={IEEE Transactions on Neural Networks},
    volume={4},
    number={2},
    pages={207--220},
    year={1993},
    publisher={IEEE}
}

% som theoretical aspects
@misc{erwin1991,
    author={Erwin, Ed and Obermayer, Klaus and Schulten, Klaus},
    title={Convergence properties of self-organizing maps},
    year={1991}
}

% som theoretical aspects
@article{erwin1992a,
    title={Self-organizing maps: ordering, convergence properties and energy functions},
    author={Erwin, Ed and Obermayer, Klaus and Schulten, Klaus},
    journal={Biological cybernetics},
    volume={67},
    number={1},
    pages={47--55},
    year={1992},
    publisher={Springer}
}

% som theoretical aspects
@article{erwin1992b,
    title={Self-organizing maps: Stationary states, metastability and convergence rate},
    author={Erwin, Ed and Obermayer, Klaus and Schulten, Klaus},
    journal={Biological Cybernetics},
    volume={67},
    number={1},
    pages={35--45},
    year={1992},
    publisher={Springer}
}

% som theoretical aspects
@article{cottrell2018,
    title={Self-OrganizingMaps, theory and applications},
    author={Cottrell, Marie and Olteanu, Madalina and Rossi, Fabrice and Villa-Vialaneix, Nathalie},
    journal={Revista de Investigacion Operacional},
    volume={39},
    number={1},
    pages={1--22},
    year={2018}
}

% som can do manifold learning
@inproceedings{martinetz1993,
  title={Competitive Hebbian learning rule forms perfectly topology preserving maps},
  author={Martinetz, Thomas},
  booktitle={International conference on artificial neural networks},
  pages={427--434},
  year={1993},
  organization={Springer}
}

% som can do manifold learning
@article{yin2008,
  title={On multidimensional scaling and the embedding of self-organising maps},
  author={Yin, Hujun},
  journal={Neural Networks},
  volume={21},
  number={2-3},
  pages={160--169},
  year={2008},
  publisher={Elsevier}
}

% som can do manifold learning
@inproceedings{gan2015,
  title={Improved Manifold Learning with competitive Hebbian rule},
  author={Gan, Qiang and Shen, Furao and Zhao, Jinxi},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2015},
  organization={IEEE}
}

% nonlinear ica with som
@inproceedings{pajunen1996,
  title={Nonlinear independent component analysis by self-organizing maps},
  author={Pajunen, Petteri},
  booktitle={International Conference on Artificial Neural Networks},
  pages={815--820},
  year={1996},
  organization={Springer}
}

% nonlinear ica with modified som (gtm)
@inproceedings{pajunen1997,
  title={A maximum likelihood approach to nonlinear blind source separation},
  author={Pajunen, Petteri and Karhunen, Juha},
  booktitle={International Conference on Artificial Neural Networks},
  pages={541--546},
  year={1997},
  organization={Springer}
}

% nonlinear ica with som
@article{van_hulle2002,
  title={Joint entropy maximization in kernel-based topographic maps},
  author={Hulle, Marc M Van},
  journal={Neural Computation},
  volume={14},
  number={8},
  pages={1887--1906},
  year={2002},
  publisher={MIT Press}
}

% subspace clustering review
@article{vidal2011,
  title={Subspace clustering},
  author={Vidal, Ren{\'e}},
  journal={IEEE Signal Processing Magazine},
  volume={28},
  number={2},
  pages={52--68},
  year={2011},
  publisher={IEEE}
}

% Factorization-based subspace clustering
@inproceedings{boult1991,
  title={Factorization-based segmentation of motions},
  author={Boult, Terrance E and Brown, L Gottesfeld},
  booktitle={Proceedings of the IEEE workshop on visual motion},
  pages={179--180},
  year={1991},
  organization={IEEE Computer Society}
}

% Factorization-based subspace clustering
@article{costeira1998,
  title={A multibody factorization method for independently moving objects},
  author={Costeira, Joao Paulo and Kanade, Takeo},
  journal={International Journal of Computer Vision},
  volume={29},
  pages={159--179},
  year={1998},
  publisher={Springer}
}

% Subspace clustering with mixture of PCA
@article{tipping1999,
  title={Mixtures of probabilistic principal component analyzers},
  author={Tipping, Michael E and Bishop, Christopher M},
  journal={Neural computation},
  volume={11},
  number={2},
  pages={443--482},
  year={1999},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA}
}

% subspace clustering - find subspaces along which data tend to align
@article{bradley2000,
  title={K-plane clustering},
  author={Bradley, Paul S and Mangasarian, Olvi L},
  journal={Journal of Global Optimization},
  volume={16},
  number={1},
  pages={23--32},
  year={2000},
  publisher={Springer}
}

% Factorization-based subspace clustering
@inproceedings{gruber2004,
  title={Multibody factorization with uncertainty and missing data using the EM algorithm},
  author={Gruber, Amit and Weiss, Yair},
  booktitle={Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
  volume={1},
  pages={I--I},
  year={2004},
  organization={IEEE}
}

% Subspace clustering with Local Subspce Affinity (LSA) self-expressiveness
@inproceedings{yan2006,
  title={A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate},
  author={Yan, Jingyu and Pollefeys, Marc},
  booktitle={Computer Vision--ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part IV 9},
  pages={94--106},
  year={2006},
  organization={Springer}
}

% Subspace clustering with Locally Linear Manifold Clustering (LLMC) self-expressiveness
@inproceedings{goh2007,
  title={Segmenting motions of different types by unsupervised manifold clustering},
  author={Goh, Alvina and Vidal, Ren{\'e}},
  booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--6},
  year={2007},
  organization={IEEE}
}

% Subspace clustering with Spectral Curvature Clustering (SCC) self-expressiveness
@article{chen2009,
  title={Spectral curvature clustering (SCC)},
  author={Chen, Guangliang and Lerman, Gilad},
  journal={International Journal of Computer Vision},
  volume={81},
  pages={317--330},
  year={2009},
  publisher={Springer}
}

% k-subspace clustering - find clusters that are not necessarily spherically shaped, but they can also be shaped like lower-dimensional subspaces
@inproceedings{wang2009a,
  title={K-subspace clustering},
  author={Wang, Dingding and Ding, Chris and Li, Tao},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20},
  pages={506--521},
  year={2009},
  organization={Springer}
}

% k-flats, similar to k-subspace
@article{canas2012,
  title={Learning manifolds with k-means and k-flats},
  author={Canas, Guillermo and Poggio, Tomaso and Rosasco, Lorenzo},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

% Subspace clustering with Local Best-Fit Flats (LBFF) self-expressiveness
@article{zhang2012,
  title={Hybrid linear modeling via local best-fit flats},
  author={Zhang, Teng and Szlam, Arthur and Wang, Yi and Lerman, Gilad},
  journal={International journal of computer vision},
  volume={100},
  pages={217--240},
  year={2012},
  publisher={Springer}
}

% Sparse Subspace Clustering (SSC) self-expressiveness
@article{elhamifar2013,
  title={Sparse subspace clustering: Algorithm, theory, and applications},
  author={Elhamifar, Ehsan and Vidal, Ren{\'e}},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={11},
  pages={2765--2781},
  year={2013},
  publisher={IEEE}
}

% Subspace clustering with low-rank representation (LRR) self-expressiveness
@article{liu2012,
  title={Robust recovery of subspace structures by low-rank representation},
  author={Liu, Guangcan and Lin, Zhouchen and Yan, Shuicheng and Sun, Ju and Yu, Yong and Ma, Yi},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={1},
  pages={171--184},
  year={2012},
  publisher={IEEE}
}

% deep k-subspace: k-subspace in deep pre-trained AE latent space
@inproceedings{zhang2019,
  title={Scalable deep k-subspace clustering},
  author={Zhang, Tong and Ji, Pan and Harandi, Mehrtash and Hartley, Richard and Reid, Ian},
  booktitle={Computer Vision--ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected Papers, Part V 14},
  pages={466--481},
  year={2019},
  organization={Springer}
}

% deep k-subspace clustering via k-subspace in deep pre-trained AE latent space, weighted by weights determined by another network branch, trained end-to-end to minimize weighted reconstruction error.
@article{huang2019,
  title={Deep Clustering via Weighted $ k $-Subspace Network},
  author={Huang, Weitian and Yin, Ming and Li, Jianzhong and Xie, Shengli},
  journal={IEEE Signal Processing Letters},
  volume={26},
  number={11},
  pages={1628--1632},
  year={2019},
  publisher={IEEE}
}

% discriminative clustering by learning vector quantization
@incollection{kohonen1995,
  title={Learning vector quantization},
  author={Kohonen, Teuvo},
  booktitle={Self-organizing maps},
  pages={175--189},
  year={1995},
  publisher={Springer}
}

% discriminative clustering
@article{kaski2005,
  title={Discriminative clustering},
  author={Kaski, Samuel and Sinkkonen, Janne and Klami, Arto},
  journal={Neurocomputing},
  volume={69},
  number={1-3},
  pages={18--41},
  year={2005},
  publisher={Elsevier}
}

% kernelized infomax clustering
@inproceedings{barber2006,
  title={Kernelized infomax clustering},
  author={Barber, David and Agakov, Felix V},
  booktitle={Advances in neural information processing systems},
  pages={17--24},
  year={2006}
}

% mutual-information and contrastive based clustering: takes two augmented versions of the image and imposes consistency in the cluster assignments of a deep network by maximizing mutual information between the outputs of positive pairs (vs minimizing for negative pairs)
@inproceedings{ji2019,
  title={Invariant information clustering for unsupervised image classification and segmentation},
  author={Ji, Xu and Henriques, Joao F and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9865--9874},
  year={2019}
}

% infomax discriminative clustering
@inproceedings{krause2010,
  title={Discriminative clustering by regularized information maximization},
  author={Krause, Andreas and Perona, Pietro and Gomes, Ryan G},
  booktitle={Advances in neural information processing systems},
  pages={775--783},
  year={2010}
}

% sparse coding clustering
@inproceedings{sprechmann2010,
  title={Dictionary learning and sparse coding for unsupervised clustering},
  author={Sprechmann, Pablo and Sapiro, Guillermo},
  booktitle={2010 IEEE international conference on acoustics, speech and signal processing},
  pages={2042--2045},
  year={2010},
  organization={IEEE}
}

% manifold clustering with k-manifold approach, finds k possibly intersecting manifolds on which the data lie.
@inproceedings{souvenir2005,
  title={Manifold clustering},
  author={Souvenir, Richard and Pless, Robert},
  booktitle={Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
  volume={1},
  pages={648--653},
  year={2005},
  organization={IEEE}
}

% multi-manifold clustering: extends manifold clustering with adaptively determined number of manifolds, and adapts spectral methods to manifold clustering
@inproceedings{wang2010,
  title={Multi-manifold clustering},
  author={Wang, Yong and Jiang, Yuan and Wu, Yi and Zhou, Zhi-Hua},
  booktitle={Pacific Rim International Conference on Artificial Intelligence},
  pages={280--291},
  year={2010},
  organization={Springer}
}

% spectral multi-manifold clustering (smmc): further extends multi-manifold clustering to deal with adaptive determination of manifold dimensionality
@article{wang2011a,
  title={Spectral clustering on multiple manifolds},
  author={Wang, Yong and Jiang, Yuan and Wu, Yi and Zhou, Zhi-Hua},
  journal={IEEE Transactions on Neural Networks},
  volume={22},
  number={7},
  pages={1149--1161},
  year={2011},
  publisher={IEEE}
}

% spectral clustering
@article{luxburg2007,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Statistics and computing},
  volume={17},
  number={4},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

% spectral clustering and kernel k means
@inproceedings{dhillon2004,
  title={Kernel k-means: spectral clustering and normalized cuts},
  author={Dhillon, Inderjit S and Guan, Yuqiang and Kulis, Brian},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={551--556},
  year={2004}
}

% spectral clustering and nmf
@inproceedings{ding2005,
  title={On the equivalence of nonnegative matrix factorization and spectral clustering},
  author={Ding, Chris and He, Xiaofeng and Simon, Horst D},
  booktitle={Proceedings of the 2005 SIAM international conference on data mining},
  pages={606--610},
  year={2005},
  organization={SIAM}
}

% spectral clustering and symmetric nmf/similarity matching
@inproceedings{kuang2012,
  title={Symmetric nonnegative matrix factorization for graph clustering},
  author={Kuang, Da and Ding, Chris and Park, Haesun},
  booktitle={Proceedings of the 2012 SIAM international conference on data mining},
  pages={106--117},
  year={2012},
  organization={SIAM}
}

% spectral clustering k means
@article{tu2014,
  title={A novel graph-based k-means for nonlinear manifold clustering and representative selection},
  author={Tu, Enmei and Cao, Longbing and Yang, Jie and Kasabov, Nicola},
  journal={Neurocomputing},
  volume={143},
  pages={109--122},
  year={2014},
  publisher={Elsevier}
}

% deep clustering
@inproceedings{xie2016,
  title={Unsupervised deep embedding for clustering analysis},
  author={Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
  booktitle={International conference on machine learning},
  pages={478--487},
  year={2016}
}

% deep clustering
@article{jiang2016,
  title={Variational deep embedding: A generative approach to clustering},
  author={Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
  journal={CoRR},
  year={2016}
}

% deep clustering
@inproceedings{yang2017,
  title={Towards k-means-friendly spaces: Simultaneous deep learning and clustering},
  author={Yang, Bo and Fu, Xiao and Sidiropoulos, Nicholas D and Hong, Mingyi},
  booktitle={international conference on machine learning},
  pages={3861--3870},
  year={2017},
  organization={PMLR}
}

% deep clustering
@article{aljalbout2018,
  title={Clustering with deep learning: Taxonomy and new methods},
  author={Aljalbout, Elie and Golkov, Vladimir and Siddiqui, Yawar and Strobel, Maximilian and Cremers, Daniel},
  journal={arXiv preprint arXiv:1801.07648},
  year={2018}
}

% Collaborative clustering: given a latent representation, uses a softmax classifier to assign data to clusters, and computes a classification-based affinity A_c. Then, it finds a subspace affinity matrix A_s by optimizing a self-expressive data representation, and imposes consistency between A_c and A_s.
@inproceedings{zhang2019,
  title={Neural collaborative subspace clustering},
  author={Zhang, Tong and Ji, Pan and Harandi, Mehrtash and Huang, Wenbing and Li, Hongdong},
  booktitle={International Conference on Machine Learning},
  pages={7384--7393},
  year={2019},
  organization={PMLR}
}

% Uses a loss that induces clustered latent representation, then trains cluster specific classifiers
@article{srivastava2022,
  title={ExpertNet: A Symbiosis of Classification and Clustering},
  author={Srivastava, Shivin and Kawaguchi, Kenji and Rajan, Vaibhav},
  journal={arXiv preprint arXiv:2201.06344},
  year={2022}
}

% density-based clustering: dbscan
@inproceedings{ester1996,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise.},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={kdd},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}


%%%%%%%%%%
% hebbian adversarial
%%%%%%%%%%

% Analysis of rbf network robustness to adversarial examples
@inproceedings{vidnerova2018,
  title={Deep networks with rbf layers to prevent adversarial examples},
  author={Vidnerov{\'a}, Petra and Neruda, Roman},
  booktitle={Artificial Intelligence and Soft Computing: 17th International Conference, ICAISC 2018, Zakopane, Poland, June 3-7, 2018, Proceedings, Part I 17},
  pages={257--266},
  year={2018},
  organization={Springer}
}

% Analysis of rbf network robustness to adversarial examples
@article{zadeh2018,
  title={Deep-rbf networks revisited: Robust classification with rejection},
  author={Zadeh, Pourya Habib and Hosseini, Reshad and Sra, Suvrit},
  journal={arXiv preprint arXiv:1812.03190},
  year={2018}
}

% adversarial robustness with a sparse coding network with lateral connectivity on neuromorphic hardware
@inproceedings{kim2019a,
  title={A neuromorphic sparse coding defense to adversarial images},
  author={Kim, Edward and Yarnall, Jessica and Shah, Priya and Kenyon, Garrett T},
  booktitle={Proceedings of the International Conference on Neuromorphic Systems},
  pages={1--8},
  year={2019}
}

% Adversarial robustness through k-WTA nonlinearities
@article{xiao2019,
  title={Enhancing adversarial defense by k-winners-take-all},
  author={Xiao, Chang and Zhong, Peilin and Zheng, Changxi},
  journal={arXiv preprint arXiv:1905.10510},
  year={2019}
}

% Adversarial robustness through a stochastic WTA nonlinearity
@inproceedings{panousis2021a,
  title={Local competition and stochasticity for adversarial robustness in deep learning},
  author={Panousis, Konstantinos and Chatzis, Sotirios and Alexos, Antonios and Theodoridis, Sergios},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3862--3870},
  year={2021},
  organization={PMLR}
}

% Adversarial robustness through a stochastic WTA nonlinearity
@article{panousis2021b,
  title={Stochastic local winner-takes-all networks enable profound adversarial robustness},
  author={Panousis, Konstantinos P and Chatzis, Sotirios and Theodoridis, Sergios},
  journal={arXiv preprint arXiv:2112.02671},
  year={2021}
}

% Vector quantization for adversarial robustness
@article{dong2023,
  title={Adversarial Defenses via Vector Quantization},
  author={Dong, Zhiyi and Mao, Yongyi},
  journal={arXiv preprint arXiv:2305.13651},
  year={2023}
}


%%%%%%%%%%
% hebbian pca
%%%%%%%%%%

% oja's rule: delta w = y (x - yw) extracts first principal component
@article{oja1982, 
  title={Simplified neuron model as a principal component analyzer},
  author={Oja, Erkki},
  journal={Journal of mathematical biology},
  volume={15},
  number={3},
  pages={267--273},
  year={1982},
  publisher={Springer}
}

% bcm rule: delta w = x phi(y - theta) uses threshold on postsynaptic activity to tune learning. theta is adapted to follow the mean activation.
@article{bienenstock1982, 
  title={Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex},
  author={Bienenstock, Elie L and Cooper, Leon N and Munro, Paul W},
  journal={Journal of Neuroscience},
  volume={2},
  number={1},
  pages={32--48},
  year={1982},
  publisher={Soc Neuroscience}
}

% extension of bcm rule, theta is adapted to follow (a multiple of) the variance/mean square value
@article{intrator1992,
  title={Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions},
  author={Intrator, Nathan and Cooper, Leon N},
  journal={Neural Networks},
  volume={5},
  number={1},
  pages={3--17},
  year={1992},
  publisher={Elsevier}
}

% extension of bcm rule, theta is adapted to follow (a multiple of) the variance/mean square value
@article{law1994,
  title={Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory.},
  author={Law, C Charles and Cooper, Leon N},
  journal={Proceedings of the National Academy of Sciences},
  volume={91},
  number={16},
  pages={7797--7801},
  year={1994},
  publisher={National Acad Sciences}
}

% nonlinear hebbian rule generalization of bcm and sparse coding
@article{brito2016,
  title={Nonlinear Hebbian learning as a unifying principle in receptive field formation},
  author={Brito, Carlos SN and Gerstner, Wulfram},
  journal={PLoS computational biology},
  volume={12},
  number={9},
  pages={e1005070},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

% sejnowski's rule: delta w = (x - x_)(y - y_) uses thresholds on input and output given by avg activity
@article{sejnowski1989, 
  title={Building network learning algorithms from hebbian synapses},
  author={Sejnowski, Terrence J and Tesauro, Gerald},
  journal={Brain organization and memory: cells, systems, and circuits. Oxford University Press, New York},
  pages={338--355},
  year={1989}
}

% sanger's rule: delta w_k = y_k(x - sum_i=1..k y_i w_i) extracts principal components
@article{sanger1989, 
  title={Optimal unsupervised learning in a single-layer linear feedforward neural network},
  author={Sanger, Terence D},
  journal={Neural networks},
  volume={2},
  number={6},
  pages={459--473},
  year={1989},
  publisher={Elsevier}
}

% subspace rule: delta w = y(c - sum yw) extracts principal subspace
@article{oja1989, 
  title={Neural networks, principal components, and subspaces},
  author={Oja, Erkki},
  journal={International journal of neural systems},
  volume={1},
  number={01},
  pages={61--68},
  year={1989},
  publisher={World Scientific}
}

% review of PCA algorithms, anti-Hebbian PCA for minor component extraction
@article{oja1992, 
  title={Principal components, minor components, and linear neural networks},
  author={Oja, Erkki},
  journal={Neural networks},
  volume={5},
  number={6},
  pages={927--935},
  year={1992},
  publisher={Elsevier}
}

% review of local plasticity models
@inproceedings{gorchetchnikov2011,
  title={Review and unification of learning framework in cog ex machina platform for memristive neuromorphic hardware},
  author={Gorchetchnikov, Anatoli and Versace, Massimiliano and Ames, Heather and Chandler, Ben and L{\'e}veill{\'e}, Jasmin and Livitz, Gennady and Mingolla, Ennio and Snider, Greg and Amerson, Rick and Carter, Dick and others},
  booktitle={The 2011 International Joint Conference on Neural Networks},
  pages={2601--2608},
  year={2011},
  organization={IEEE}
}

% review of local plasticity models
@inproceedings{vasilkoski2011,
  title={Review of stability properties of neural plasticity rules for implementation on memristive neuromorphic hardware},
  author={Vasilkoski, Zlatko and Ames, Heather and Chandler, Ben and Gorchetchnikov, Anatoli and L{\'e}veill{\'e}, Jasmin and Livitz, Gennady and Mingolla, Ennio and Versace, Massimiliano},
  booktitle={The 2011 International Joint Conference on Neural Networks},
  pages={2563--2569},
  year={2011},
  organization={IEEE}
}

% Foldiak's rule: Hebbian-anti-Hebbian updates on forward-lateral connections whitens data
@inproceedings{foldiak1989,
  title={Adaptive network for optimal linear feature extraction},
  author={F{\"o}ldiak, Peter},
  booktitle={Proceedings of IEEE/INNS Int. Joint. Conf. Neural Networks},
  volume={1},
  pages={401--405},
  year={1989}
}

% hebbian anti hebbian useful not only from information theoretic point of view but also for power saving
@article{plumbley1993a,
  title={Efficient information transfer and anti-Hebbian neural networks},
  author={Plumbley, Mark D},
  journal={Neural Networks},
  volume={6},
  number={6},
  pages={823--833},
  year={1993},
  publisher={Elsevier}
}

% Plumbey's rule: hebbian anti-hebbian updates whiten data
@inproceedings{plumbley1993b,
  title={A Hebbian/anti-Hebbian network which optimizes information capacity by orthonormalizing the principal subspace},
  author={Plumbley, Mark D},
  booktitle={1993 Third International Conference on Artificial Neural Networks},
  pages={86--90},
  year={1993},
  organization={IET}
}

% Lyapunov function for hebbian pca
@article{plumbley1995,
  title={Lyapunov functions for convergence of principal component algorithms},
  author={Plumbley, Mark D},
  journal={Neural Networks},
  volume={8},
  number={1},
  pages={11--23},
  year={1995},
  publisher={Elsevier}
}

% Rubner's rule: Hebbian-anti-Hebbian updates on forward-lateral connections asymmetric with explicit normalization extracts principal components
@article{rubner1989, 
  title={A self-organizing network for principal-component analysis},
  author={Rubner, Jeanne and Tavan, Paul},
  journal={EPL (Europhysics Letters)},
  volume={10},
  number={7},
  pages={693},
  year={1989},
  publisher={IOP Publishing}
}

% APEX rule: Hebbian-anti-Hebbian updates on forward-lateral connections asymmetric extracts principal components
@inproceedings{kung1990, 
  title={A neural network learning algorithm for adaptive principal component extraction (APEX)},
  author={Kung, Sun-Yuan and Diamantaras, KI},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  pages={861--864},
  year={1990},
  organization={IEEE}
}

% Leen's rule: Hebbian-anti-Hebbian updates on forward-lateral connections symmetric/asymmetric extracts principal subspace/components, uses full/weak coupling
@inproceedings{leen1991, 
  title={Dynamics of learning in recurrent feature-discovery networks},
  author={Leen, Todd K},
  booktitle={Advances in neural information processing systems},
  pages={70--76},
  year={1991}
}

% Asymmetric PCA rule: heteroassociative network extract eigenvectors of cross-correlation matrix between input and output
@article{diamantaras1994,
  title={Cross-correlation neural network models},
  author={Diamantaras, Konstantinos I and Kung, Sun-Yuan},
  journal={IEEE Transactions on Signal Processing},
  volume={42},
  number={11},
  pages={3218--3223},
  year={1994},
  publisher={IEEE}
}

% Probabilistic PCA (PPCA): representation of principal subspace in terms of gaussian model, and maximum likelihood procedure for fitting the gaussian parameters
@article{tipping1999,
  title={Probabilistic principal component analysis},
  author={Tipping, Michael E and Bishop, Christopher M},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={61},
  number={3},
  pages={611--622},
  year={1999},
  publisher={Wiley Online Library}
}

% overview of hebbian WTA, PCA, ICA networks
@article{becker1996a, 
  title={Unsupervised neural network learning procedures for feature extraction and classification},
  author={Becker, Suzanna and Plumbley, Mark},
  journal={Applied Intelligence},
  volume={6},
  number={3},
  pages={185--203},
  year={1996},
  publisher={Springer}
}

% Nonlinear PCA rule: delta w = y y'(x - sum yw)
@article{karhunen1995, 
  title={Generalizations of principal component analysis, optimization problems, and neural networks},
  author={Karhunen, Juha and Joutsensalo, Jyrki},
  journal={Neural Networks},
  volume={8},
  number={4},
  pages={549--562},
  year={1995},
  publisher={Pergamon}
}

% nonlinear pca via autoencoder
@article{kramer1991,
  title={Nonlinear principal component analysis using autoassociative neural networks},
  author={Kramer, Mark A},
  journal={AIChE journal},
  volume={37},
  number={2},
  pages={233--243},
  year={1991},
  publisher={Wiley Online Library}
}

% Luo's rule for extracting minor components
@inproceedings{luo1997, 
  title={A generalized learning algorithm of minor component},
  author={Luo, Fa-Long and Unbehauen, Rolf},
  booktitle={1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={4},
  pages={3229--3232},
  year={1997},
  organization={IEEE}
}

% kernel pca (kpca)
@article{scholkopf1998,
  title={Nonlinear component analysis as a kernel eigenvalue problem},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  journal={Neural computation},
  volume={10},
  number={5},
  pages={1299--1319},
  year={1998},
  publisher={MIT Press}
}

% hebbian kpca
@article{kim2003,
  title={Kernel Hebbian algorithm for iterative kernel principal component analysis},
  author={Kim, Kwang In and Franz, Matthias O and Sch{\"o}lkopf, Bernhard},
  year={2003},
  publisher={Max Planck Institute for Biological Cybernetics}
}

% hebbian kpca
@article{kim2005a,
  title={Iterative kernel principal component analysis for image modeling},
  author={Kim, Kwang In and Franz, Matthias O and Scholkopf, Bernhard},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={9},
  pages={1351--1366},
  year={2005},
  publisher={IEEE}
}

% Generalized PCA (GPCA): fitting data distributed along union of subspaces with polynomial approximations
@article{vidal2005,
  title={Generalized principal component analysis (GPCA)},
  author={Vidal, Rene and Ma, Yi and Sastry, Shankar},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={12},
  pages={1945--1959},
  year={2005},
  publisher={IEEE}
}

% discriminative pca
@article{bair2006,
  title={Prediction by supervised principal components},
  author={Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani, Robert},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  pages={119--137},
  year={2006},
  publisher={Taylor \& Francis}
}

% discriminative pca
@article{barshan2011,
  title={Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds},
  author={Barshan, Elnaz and Ghodsi, Ali and Azimifar, Zohreh and Jahromi, Mansoor Zolghadri},
  journal={Pattern Recognition},
  volume={44},
  number={7},
  pages={1357--1371},
  year={2011},
  publisher={Elsevier}
}

% discriminative subspace clustering with a low-rank constraint: a supervised objective is designed to incorporate label information, while seeking for a low-rank representation of the data
@article{li2015,
  title={Learning robust and discriminative subspace with low-rank constraints},
  author={Li, Sheng and Fu, Yun},
  journal={IEEE transactions on neural networks and learning systems},
  volume={27},
  number={11},
  pages={2160--2173},
  year={2015},
  publisher={IEEE}
}

% Discriminative pca - objective for simultaneous principal component estraction and target prediction 
@inproceedings{ritchie2019,
  title={Supervised principal component analysis via manifold optimization},
  author={Ritchie, Alexander and Scott, Clayton and Balzano, Laura and Kessler, Daniel and Sripada, Chandra S},
  booktitle={2019 IEEE Data Science Workshop (DSW)},
  pages={6--10},
  year={2019},
  organization={IEEE}
}


%%%%%%%%%%
% hebbian cca and da
%%%%%%%%%%

% neural cca
@article{lai2001,
  title={A family of Canonical Correlation networks},
  author={Lai, Pei Ling and Fyfe, Colin},
  journal={Neural processing letters},
  volume={14},
  number={2},
  pages={93--105},
  year={2001},
  publisher={Springer}
}

% least squares cca
@inproceedings{sun2008,
  title={A least squares formulation for canonical correlation analysis},
  author={Sun, Liang and Ji, Shuiwang and Ye, Jieping},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1024--1031},
  year={2008}
}

% deep cca
@inproceedings{andrew2013,
  title={Deep canonical correlation analysis},
  author={Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle={International conference on machine learning},
  pages={1247--1255},
  year={2013},
  organization={PMLR}
}

% Deep Canonically Correlated Autoencoders (DCCAE)
@inproceedings{wang2015a,
  title={On deep multi-view representation learning},
  author={Wang, Weiran and Arora, Raman and Livescu, Karen and Bilmes, Jeff},
  booktitle={International conference on machine learning},
  pages={1083--1092},
  year={2015},
  organization={PMLR}
}

% deep variational cca
@article{wang2016a,
  title={Deep variational canonical correlation analysis},
  author={Wang, Weiran and Yan, Xinchen and Lee, Honglak and Livescu, Karen},
  journal={arXiv preprint arXiv:1610.03454},
  year={2016}
}

% Deep cca with LSTMs
@article{mallinar2018,
  title={Deep canonically correlated LSTMs},
  author={Mallinar, Neil and Rosset, Corbin},
  journal={arXiv preprint arXiv:1801.05407},
  year={2018}
}

% kernel cca and application to ica
@inproceedings{fyfe2000,
  title={ICA using kernel canonical correlation analysis},
  author={Fyfe, Colin and Lai, Pei Ling},
  booktitle={In Proc. Int. Workshop on Independent Component Analysis and Blind Signal Separation (ICA2000},
  year={2000},
}

% kernel cca and application to ica
@article{lai2000,
  title={Kernel and nonlinear canonical correlation analysis},
  author={Lai, Pei Ling and Fyfe, Colin},
  journal={International Journal of Neural Systems},
  volume={10},
  number={05},
  pages={365--377},
  year={2000},
  publisher={World Scientific}
}

% discriminative cca
@article{kim2007,
  title={Discriminative learning and recognition of image set classes using canonical correlations},
  author={Kim, Tae-Kyun and Kittler, Josef and Cipolla, Roberto},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={29},
  number={6},
  pages={1005--1018},
  year={2007},
  publisher={IEEE}
}

% kernel discriminative cca
@inproceedings{sun2007,
  title={Kernelized discriminative canonical correlation analysis},
  author={Sun, Ting-Kai and Chen, Song-Can and Jin, Zhong and Yang, Jing-Yu},
  booktitle={2007 International Conference on Wavelet Analysis and Pattern Recognition},
  volume={3},
  pages={1283--1287},
  year={2007},
  organization={IEEE}
}

% relationship discriminative cca - lda
@article{shin2011,
  title={Analysis of correlation based dimension reduction methods},
  author={Shin, Yong and Park, Cheong},
  journal={International Journal of Applied Mathematics and Computer Science},
  volume={21},
  number={3},
  pages={549--558},
  year={2011},
  publisher={Sciendo}
}

% deep discriminative cca
@inproceedings{elmadany2016,
  title={Multiview learning via deep discriminative canonical correlation analysis},
  author={Elmadany, Nour El Din and He, Yifeng and Guan, Ling},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2409--2413},
  year={2016},
  organization={IEEE}
}

% deep discriminative cca
@inproceedings{dorfer2016,
  title={Towards Deep and Discriminative Canonical Correlation Analysis},
  author={Dorfer, Matthias and Widmer, Gerhard and At, Gerhard Widmerajku},
  booktitle={Proc. ICML Workshop on Multi-view Representaiton Learning},
  year={2016}
}

% neural discriminative cca
@inproceedings{gatto2017,
  title={Discriminative canonical correlation analysis network for image classification},
  author={Gatto, Bernardo B and dos Santos, Eulanda M},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
  pages={4487--4491},
  year={2017},
  organization={IEEE}
}

% neural lda
@inproceedings{mao1993,
  title={Discriminant analysis neural networks},
  author={Mao, Jianchang and Jain, Anil K},
  booktitle={IEEE International Conference on Neural Networks},
  pages={300--305},
  year={1993},
  organization={IEEE}
}

% incremental lda
@article{pang2005,
  title={Incremental linear discriminant analysis for classification of data streams},
  author={Pang, Shaoning and Ozawa, Seiichi and Kasabov, Nikola},
  journal={IEEE transactions on Systems, Man, and Cybernetics, part B (Cybernetics)},
  volume={35},
  number={5},
  pages={905--914},
  year={2005},
  publisher={IEEE}
}

% online local lda
@article{demir2005,
  title={Online local learning algorithms for linear discriminant analysis},
  author={Demir, G{\"u}leser Kalayci and Ozmehmet, Kemal},
  journal={Pattern Recognition Letters},
  volume={26},
  number={4},
  pages={421--431},
  year={2005},
  publisher={Elsevier}
}

% nonlinear lda
@article{santacruz1998,
  title={A nonlinear discriminant algorithm for feature extraction and data classification},
  author={Santa Cruz, Carlos and Dorronsoro, Jose R},
  journal={IEEE transactions on neural networks},
  volume={9},
  number={6},
  pages={1370--1376},
  year={1998},
  publisher={IEEE}
}

% kernel lda
@inproceedings{mika1999a,
  title={Fisher discriminant analysis with kernels},
  author={Mika, Sebastian and Ratsch, Gunnar and Weston, Jason and Scholkopf, Bernhard and Mullers, Klaus-Robert},
  booktitle={Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal processing society workshop (cat. no. 98th8468)},
  pages={41--48},
  year={1999},
  organization={Ieee}
}

% locally linear lda
@article{kim2005b,
  title={Locally linear discriminant analysis for multimodally distributed classes for face recognition with a single model image},
  author={Kim, Tae-Kyun and Kittler, Josef},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={3},
  pages={318--327},
  year={2005},
  publisher={IEEE}
}

% locally linear lda
@inproceedings{sugiyama2006,
  title={Local fisher discriminant analysis for supervised dimensionality reduction},
  author={Sugiyama, Masashi},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={905--912},
  year={2006}
}

% nonlinear da - mlp representation equivalence
@article{webb1990,
  title={The optimised internal representation of multilayer classifier networks performs nonlinear discriminant analysis},
  author={Webb, Andrew R and Lowe, David},
  journal={Neural Networks},
  volume={3},
  number={4},
  pages={367--375},
  year={1990},
  publisher={Elsevier}
}

% lda - least squares equivalence
@inproceedings{ye2007,
  title={Least squares linear discriminant analysis},
  author={Ye, Jieping},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={1087--1093},
  year={2007}
}

% lda - small sample size
@article{sharma2015,
  title={Linear discriminant analysis for the small sample size problem: an overview},
  author={Sharma, Alok and Paliwal, Kuldip K},
  journal={International Journal of Machine Learning and Cybernetics},
  volume={6},
  number={3},
  pages={443--454},
  year={2015},
  publisher={Springer}
}

% optimal scoring criterion for da
@article{hastie1994,
  title={Flexible discriminant analysis by optimal scoring},
  author={Hastie, Trevor and Tibshirani, Robert and Buja, Andreas},
  journal={Journal of the American statistical association},
  volume={89},
  number={428},
  pages={1255--1270},
  year={1994},
  publisher={Taylor \& Francis}
}

% maximum margin criterion for da
@inproceedings{li2004,
  title={Efficient and robust feature extraction by maximum margin criterion},
  author={Li, Haifeng and Jiang, Tao and Zhang, Keshu},
  booktitle={Advances in neural information processing systems},
  pages={97--104},
  year={2004}
}

% lasso sparsity measure
@article{tibshirani1996,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}

% elasticnet sparsity measure
@article{zou2005,
  title={Regularization and variable selection via the elastic net},
  author={Zou, Hui and Hastie, Trevor},
  journal={Journal of the royal statistical society: series B (statistical methodology)},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Wiley Online Library}
}

% deep discriminant analysis
@article{dorfer2015,
  title={Deep linear discriminant analysis},
  author={Dorfer, Matthias and Kelz, Rainer and Widmer, Gerhard},
  journal={arXiv preprint arXiv:1511.04707},
  year={2015}
}


%%%%%%%%%%
% sparse coding
%%%%%%%%%%

% neural sparse coding
@article{foldiak1990,
    title={Forming sparse representations by local anti-Hebbian learning},
    author={F{\"o}ldiak, Peter},
    journal={Biological cybernetics},
    volume={64},
    number={2},
    pages={165--170},
    year={1990},
    publisher={Springer}
}

% neural sparse coding
@article{olshausen1996a,
  title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  author={Olshausen, Bruno A and Field, David J},
  journal={Nature},
  volume={381},
  number={6583},
  pages={607},
  year={1996},
  publisher={Nature Publishing Group}
}

% neural sparse coding and relationship with maximum-likelihood bss
@article{olshausen1996b,
  title={Learning linear, sparse, factorial codes},
  author={Olshausen, Bruno A},
  journal={Massachusetts Institute of Technology, AIM-1580},
  year={1996}
}

% neural sparse coding to capture natural image statistics/nonlinear correlations
@article{olshausen1996c,
  title={Natural image statistics and efficient coding},
  author={Olshausen, Bruno A and Field, David J},
  journal={Network: computation in neural systems},
  volume={7},
  number={2},
  pages={333--339},
  year={1996},
  publisher={Taylor \& Francis}
}

% spatio-temporal neural sparse coding leads to spike-like responses
@inproceedings{olshausen2003,
  title={Learning sparse, overcomplete representations of time-varying natural images},
  author={Olshausen, Bruno A},
  booktitle={Proceedings 2003 International Conference on Image Processing (Cat. No. 03CH37429)},
  volume={1},
  pages={I--41},
  year={2003},
  organization={IEEE}
}

% neural sparse coding
@article{rozell2008,
  title={Sparse coding via thresholding and local competition in neural circuits},
  author={Rozell, Christopher J and Johnson, Don H and Baraniuk, Richard G and Olshausen, Bruno A},
  journal={Neural computation},
  volume={20},
  number={10},
  pages={2526--2563},
  year={2008},
  publisher={MIT Press}
}

% neural sparse coding and ica - sparse ica - laplacian sparsity
@inproceedings{lewicki1998,
  title={Learning nonlinear overcomplete representations for efficient coding},
  author={Lewicki, Michael S and Sejnowski, Terrence J},
  booktitle={Advances in neural information processing systems},
  pages={556--562},
  year={1998}
}

% neural sparse coding and ica - soft thresholding
@inproceedings{hyvarinen1998a,
  title={Image feature extraction by sparse coding and independent component analysis},
  author={Hyvarinen, Aapo and Oja, Erkki and Hoyer, Patrik and Hurri, Jarmo},
  booktitle={Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No. 98EX170)},
  volume={2},
  pages={1268--1273},
  year={1998},
  organization={IEEE}
}

% neural sparse coding and ica - denoising - soft thresholding
@inproceedings{hyvarinen1999a,
  title={Sparse code shrinkage: Denoising by nonlinear maximum likelihood estimation},
  author={Hyv{\"a}rinen, Aapo and Hoyer, Patrik O and Oja, Erkki},
  booktitle={Advances in Neural Information Processing Systems},
  pages={473--479},
  year={1999}
}

% Projection pursuits
@article{huber1985,
  title={Projection pursuit},
  author={Huber, Peter J},
  journal={The annals of Statistics},
  pages={435--475},
  year={1985},
  publisher={JSTOR}
}

% exploratory projection pursuit (EPP)
@article{friedman1987,
  title={Exploratory projection pursuit},
  author={Friedman, Jerome H},
  journal={Journal of the American statistical association},
  volume={82},
  number={397},
  pages={249--266},
  year={1987},
  publisher={Taylor \& Francis}
}

% sparse coding with matching pursuits
@article{mallat1993,
  title={Matching pursuits with time-frequency dictionaries},
  author={Mallat, St{\'e}phane G and Zhang, Zhifeng},
  journal={IEEE Transactions on signal processing},
  volume={41},
  number={12},
  pages={3397--3415},
  year={1993},
  publisher={IEEE}
}

% sparse coding with omp
@inproceedings{pati1993,
  title={Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition},
  author={Pati, Yagyensh Chandra and Rezaiifar, Ramin and Krishnaprasad, Perinkulam Sambamurthy},
  booktitle={Proceedings of 27th Asilomar conference on signals, systems and computers},
  pages={40--44},
  year={1993},
  organization={IEEE}
}

% Factorial sparse codes with predictability minimization as an alternative to sparse coding 
@article{schmidhuber1992,
  title={Learning factorial codes by predictability minimization},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={4},
  number={6},
  pages={863--879},
  year={1992},
  publisher={MIT Press}
}

% LOCOCODE: Low complexity coding for sparse code extraction
@article{hochreiter1997a,
  title={Low-complexity coding and decoding},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Theoretical aspects of neural computation (TANC 97), Hong Kong},
  pages={297--306},
  year={1997}
}

% sparse coding with basis pursuit: matching pursuit optimization for the case of sparsity objectives
@article{chen2001,
  title={Atomic decomposition by basis pursuit},
  author={Chen, Scott Shaobing and Donoho, David L and Saunders, Michael A},
  journal={SIAM review},
  volume={43},
  number={1},
  pages={129--159},
  year={2001},
  publisher={SIAM}
}

% sparse component analysis (SCA) - sparse dictionary learning with k-means
@inproceedings{li2003,
  title={Sparse component analysis for blind source separation with less sensors than sources},
  author={Li, Yuanqing and Cichocki, Andrzej and Amari, Shun-Ichi},
  booktitle={Ica},
  volume={2003},
  pages={89--94},
  year={2003},
}

% type of iterative shrinkage-thresholding algorithm (ISTA), precursor of FISTA
@article{daubechies2004,
  title={An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
  author={Daubechies, Ingrid and Defrise, Michel and De Mol, Christine},
  journal={Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
  volume={57},
  number={11},
  pages={1413--1457},
  year={2004},
  publisher={Wiley Online Library}
}

% sparse component analysis (SCA) - identifiability condition with sparsity constraints in underdetermined case, dictionary learning by clustering
@article{georgiev2005,
  title={Sparse component analysis and blind source separation of underdetermined mixtures},
  author={Georgiev, Pando and Theis, Fabian and Cichocki, Andrzej},
  journal={IEEE transactions on neural networks},
  volume={16},
  number={4},
  pages={992--996},
  year={2005},
  publisher={IEEE}
}

% sparse dictionary learning with cluster-wise pca
@article{babaie2006,
  title={Sparse ICA via cluster-wise PCA},
  author={Babaie-Zadeh, Massoud and Jutten, Christian and Mansour, Ali},
  journal={Neurocomputing},
  volume={69},
  number={13-15},
  pages={1458--1466},
  year={2006},
  publisher={Elsevier}
}

% sparse dictionary learning with k-subspace for sparse component analysis (SCA)
@inproceedings{he2006a,
  title={K-subspace clustering and its application in sparse component analysis},
  author={He, Zhaoshui and Cichocki, Andrzej},
  booktitle={Proc. ESANN 2006},
  year={2006},
  organization={Citeseer}
}

% sparse dictionary learning with k-evd for sparse component analysis (SCA)
@inproceedings{he2006b,
  title={K-EVD clustering and its applications to sparse component analysis},
  author={He, Zhaoshui and Cichocki, Andrzej},
  booktitle={International Conference on Independent Component Analysis and Signal Separation},
  pages={90--97},
  year={2006},
  organization={Springer}
}

% sparse dictionary learning with k-svd
@article{aharon2006,
  title={K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation},
  author={Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
  journal={IEEE Transactions on signal processing},
  volume={54},
  number={11},
  pages={4311--4322},
  year={2006},
  publisher={IEEE}
}

% sparse dictionary learning with k-plane clustering for sparse component analysis (SCA)
@inproceedings{washizawa2006,
  title={On-line k-plane clustering learning algorithm for sparse component analysis},
  author={Washizawa, Yoshikazu and Cichocki, Andrzej},
  booktitle={2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings},
  volume={5},
  pages={V--V},
  year={2006},
  organization={IEEE}
}

% online sparse dictionary learning
@inproceedings{mairal2009a,
  title={Online dictionary learning for sparse coding},
  author={Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={689--696},
  year={2009}
}

% sparse coding - fista (gradient based + momentum)
@article{beck2009,
  title={A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  author={Beck, Amir and Teboulle, Marc},
  journal={SIAM journal on imaging sciences},
  volume={2},
  number={1},
  pages={183--202},
  year={2009},
  publisher={SIAM}
}

% sparse coding - fast encoding with inverse mapping w.r.t. deocding, instead of alternating minimization
@article{kavukcuoglu2010a,
  title={Fast inference in sparse coding algorithms with applications to object recognition},
  author={Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
  journal={arXiv preprint arXiv:1010.3467},
  year={2010}
}

% LISTA (Learned ISTA): gradient-based training of a feedforward deep network to compute the sparse codes
@inproceedings{gregor2010,
  title={Learning fast approximations of sparse coding},
  author={Gregor, Karol and LeCun, Yann},
  booktitle={Proceedings of the 27th international conference on international conference on machine learning},
  pages={399--406},
  year={2010}
}

% kl sparsity measure
@inproceedings{bagnell2009,
  title={Differentiable sparse coding},
  author={Bagnell, J Andrew and Bradley, David M},
  booktitle={Advances in neural information processing systems},
  pages={113--120},
  year={2009}
}

% sparsity measures
@article{hurley2009,
  title={Comparing measures of sparsity},
  author={Hurley, Niall and Rickard, Scott},
  journal={IEEE Transactions on Information Theory},
  volume={55},
  number={10},
  pages={4723--4741},
  year={2009},
  publisher={IEEE}
}

% determinant sparsity measure
@article{yang2012,
  title={Nonnegative blind source separation by sparse component analysis based on determinant measure},
  author={Yang, Zuyuan and Xiang, Yong and Xie, Shengli and Ding, Shuxue and Rong, Yue},
  journal={IEEE transactions on neural networks and learning systems},
  volume={23},
  number={10},
  pages={1601--1610},
  year={2012},
  publisher={IEEE}
}

% convolutional sparse coding (proposed for the first time in alternative to patch based) - optimized in batch mode with 2-stage method: 1) solving a linear system of equations, 2) applying shrinkage
@inproceedings{zeiler2010,
  title={Deconvolutional networks},
  author={Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Rob},
  booktitle={2010 IEEE Computer Society Conference on computer vision and pattern recognition},
  pages={2528--2535},
  year={2010},
  organization={IEEE}
}

% convolutional sparse coding optimized with gradient descent
@article{kavukcuoglu2010b,
  title={Learning convolutional feature hierarchies for visual recognition},
  author={Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-Lan and Gregor, Karol and Mathieu, Micha{\"e}l and Cun, Yann and others},
  journal={Advances in neural information processing systems},
  volume={23},
  pages={1090--1098},
  year={2010}
}

% convolutional sparse coding optimized in batch mode with proximal gradient (i.e. gradient projected on constraint) and fista
@inproceedings{chalasani2013,
  title={A fast proximal method for convolutional sparse coding},
  author={Chalasani, Rakesh and Principe, Jose C and Ramakrishnan, Naveen},
  booktitle={The 2013 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--5},
  year={2013},
  organization={IEEE}
}

% convolutional sparse coding in the fourier domain - optimized in batch mode with augmented lagrangian and admm method
@inproceedings{bristow2013,
  title={Fast convolutional sparse coding},
  author={Bristow, Hilton and Eriksson, Anders and Lucey, Simon},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={391--398},
  year={2013}
}

% convolutional sparse coding optimized with admm
@inproceedings{heide2015,
  title={Fast and flexible convolutional sparse coding},
  author={Heide, Felix and Heidrich, Wolfgang and Wetzstein, Gordon},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5135--5143},
  year={2015}
}

% hierarchical convolutional sparse coding neural networks
@article{papyan2017,
  title={Convolutional neural networks analyzed via convolutional sparse coding},
  author={Papyan, Vardan and Romano, Yaniv and Elad, Michael},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={2887--2938},
  year={2017},
  publisher={JMLR. org}
}

% online convolutional sparse coding - gradient methods both in spatial and frequency domains
@article{liu2018,
  title={First-and second-order methods for online convolutional dictionary learning},
  author={Liu, Jialin and Garcia-Cardona, Cristina and Wohlberg, Brendt and Yin, Wotao},
  journal={SIAM Journal on Imaging Sciences},
  volume={11},
  number={2},
  pages={1589--1628},
  year={2018},
  publisher={SIAM}
}

% online convolutional sparse coding in frequency domain with admm
@article{wang2018a,
  title={Scalable online convolutional sparse coding},
  author={Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
  journal={IEEE Transactions on Image Processing},
  volume={27},
  number={10},
  pages={4850--4859},
  year={2018},
  publisher={IEEE}
}


% convolutional sparse coding - gradient-based learning
@inproceedings{sreter2018,
  title={Learned convolutional sparse coding},
  author={Sreter, Hillel and Giryes, Raja},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2191--2195},
  year={2018},
  organization={IEEE}
}

% Sample-dependent dictionary: hierarchical dictionary in which words for a specific sample are formed by combining simpler words. Applied in an online convolutional sparse coding context.
@inproceedings{wang2018b,
  title={Online convolutional sparse coding with sample-dependent dictionary},
  author={Wang, Yaqing and Yao, Quanming and Kwok, James Tin-Yau and others},
  booktitle={International Conference on Machine Learning},
  pages={5209--5218},
  year={2018},
  organization={PMLR}
}

% discriminative sparse coding using class specific dictionaries
@inproceedings{mairal2008,
  title={Discriminative learned dictionaries for local image analysis},
  author={Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--8},
  year={2008},
  organization={IEEE}
}

% discriminative sparse coding using joint optimization of classification and representation loss
@inproceedings{mairal2009b,
  title={Supervised dictionary learning},
  author={Mairal, Julien and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew and Bach, Francis R},
  booktitle={Advances in neural information processing systems},
  pages={1033--1040},
  year={2009}
}

% discriminative sparse coding with max margin loss
@inproceedings{yang2011,
  title={Fisher discrimination dictionary learning for sparse representation},
  author={Yang, Meng and Zhang, Lei and Feng, Xiangchu and Zhang, David},
  booktitle={2011 International Conference on Computer Vision},
  pages={543--550},
  year={2011},
  organization={IEEE}
}

% mixture nonlinear sparse coding
@inproceedings{yang2010,
  title={Efficient highly over-complete sparse coding using a mixture model},
  author={Yang, Jianchao and Yu, Kai and Huang, Thomas},
  booktitle={European Conference on Computer Vision},
  pages={113--126},
  year={2010},
  organization={Springer}
}

% Sparse coding based on local manifold structure
@article{zheng2010,
  title={Graph regularized sparse coding for image representation},
  author={Zheng, Miao and Bu, Jiajun and Chen, Chun and Wang, Can and Zhang, Lijun and Qiu, Guang and Cai, Deng},
  journal={IEEE transactions on image processing},
  volume={20},
  number={5},
  pages={1327--1336},
  year={2010},
  publisher={IEEE}
}

% Sparse coding based on local manifold structure
@inproceedings{gao2010,
  title={Local features are not lonely--Laplacian sparse coding for image classification},
  author={Gao, Shenghua and Tsang, Ivor Wai-Hung and Chia, Liang-Tien and Zhao, Peilin},
  booktitle={2010 IEEE computer society conference on computer vision and pattern recognition},
  pages={3555--3561},
  year={2010},
  organization={IEEE}
}

% locally linear sparse coding
@inproceedings{thiagarajan2011,
  title={Learning dictionaries for local sparse coding in image classification},
  author={Thiagarajan, Jayaraman J and Spanias, Andreas},
  booktitle={2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)},
  pages={2014--2018},
  year={2011},
  organization={IEEE}
}

% nonlinear sparse coding by locality constraint: penalizing large codes for atoms far from input
@article{zhou2013,
  title={Locality constrained dictionary learning for nonlinear dimensionality reduction},
  author={Zhou, Yin and Barner, Kenneth E},
  journal={IEEE Signal Processing Letters},
  volume={20},
  number={4},
  pages={335--338},
  year={2013},
  publisher={IEEE}
}

% nonlinear sparse coding
@inproceedings{xie2013,
  title={On a nonlinear generalization of sparse coding and dictionary learning},
  author={Ho, Jeffrey and Xie, Yuchen and Vemuri, Baba},
  booktitle={International conference on machine learning},
  pages={1480--1488},
  year={2013},
  organization={PMLR}
}

% Sparse coding based on local manifold structure
@inproceedings{sha2017,
  title={Locally linear embedded sparse coding for image representation},
  author={Sha, Lingdao and Schonfeld, Dan and Wang, Jing},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2527--2531},
  year={2017},
  organization={IEEE}
}

% kernel sparse coding
@inproceedings{gao2010,
  title={Kernel sparse representation for image classification and face recognition},
  author={Gao, Shenghua and Tsang, Ivor Wai-Hung and Chia, Liang-Tien},
  booktitle={European conference on computer vision},
  pages={1--14},
  year={2010},
  organization={Springer}
}

% kernel sparse coding
@inproceedings{nguyen2012,
  title={Kernel dictionary learning},
  author={Van Nguyen, Hien and Patel, Vishal M and Nasrabadi, Nasser M and Chellappa, Rama},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2021--2024},
  year={2012},
  organization={IEEE}
}

% nonnegative matrix factorization - nmf
@inproceedings{lee2001,
  title={Algorithms for non-negative matrix factorization},
  author={Lee, Daniel D and Seung, H Sebastian},
  booktitle={Advances in neural information processing systems},
  pages={556--562},
  year={2001}
}

% nmf with projected gradient instead of standard multiplicative approach
@article{lin2007,
  title={Projected gradient methods for nonnegative matrix factorization},
  author={Lin, Chih-Jen},
  journal={Neural computation},
  volume={19},
  number={10},
  pages={2756--2779},
  year={2007},
  publisher={MIT Press}
}

% non negative sparse coding
@inproceedings{hoyer2002,
  title={Non-negative sparse coding},
  author={Hoyer, Patrik O},
  booktitle={Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing},
  pages={557--565},
  year={2002},
  organization={IEEE}
}

% non negative sparse coding
@article{hoyer2003,
  title={Modeling receptive fields with non-negative sparse coding},
  author={Hoyer, Patrik O},
  journal={Neurocomputing},
  volume={52},
  pages={547--552},
  year={2003},
  publisher={Elsevier}
}

% sparse nmf
@inproceedings{liu2003,
  title={Non-negative matrix factorization for visual coding},
  author={Liu, Weixiang and Zheng, Nanning and Lu, Xiaofeng},
  booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03).},
  volume={3},
  pages={III--293},
  year={2003},
  organization={IEEE}
}

% sparse nmf
@article{hoyer2004,
  title={Non-negative matrix factorization with sparseness constraints},
  author={Hoyer, Patrik O},
  journal={Journal of machine learning research},
  volume={5},
  number={Nov},
  pages={1457--1469},
  year={2004}
}

% non negative sparse coding
@inproceedings{eggert2004,
  title={Sparse coding and NMF},
  author={Eggert, Julian and Korner, Edgar},
  booktitle={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541)},
  volume={4},
  pages={2529--2533},
  year={2004},
  organization={IEEE}
}

% kernel nmf
@inproceedings{zhang2006a,
  title={Non-negative matrix factorization on kernels},
  author={Zhang, Daoqiang and Zhou, Zhi-Hua and Chen, Songcan},
  booktitle={Pacific Rim International Conference on Artificial Intelligence},
  pages={404--412},
  year={2006},
  organization={Springer}
}

% deep non negative sparse coding for part-based representations
@article{hosseini2015,
  title={Deep learning of part-based representation of data using sparse autoencoders with nonnegativity constraints},
  author={Hosseini-Asl, Ehsan and Zurada, Jacek M and Nasraoui, Olfa},
  journal={IEEE transactions on neural networks and learning systems},
  volume={27},
  number={12},
  pages={2486--2498},
  year={2015},
  publisher={IEEE}
}

% deep sparse coding
@article{tariyal2016,
  title={Deep dictionary learning},
  author={Tariyal, Snigdha and Majumdar, Angshul and Singh, Richa and Vatsa, Mayank},
  journal={IEEE Access},
  volume={4},
  pages={10096--10109},
  year={2016},
  publisher={Ieee}
}

% Multi-Layer Convolutional Sparse Coding (ML-CSC): Connection between forward pass in encoding architectures and sparse code search algorithms.
@article{papyan2017,
  title={Convolutional neural networks analyzed via convolutional sparse coding},
  author={Papyan, Vardan and Romano, Yaniv and Elad, Michael},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={2887--2938},
  year={2017},
  publisher={JMLR. org}
}

% deep sparse coding
@article{mahdizadehaghdam2019,
  title={Deep dictionary learning: A parametric network approach},
  author={Mahdizadehaghdam, Shahin and Panahi, Ashkan and Krim, Hamid and Dai, Liyi},
  journal={IEEE Transactions on Image Processing},
  volume={28},
  number={10},
  pages={4790--4802},
  year={2019},
  publisher={IEEE}
}

% convolutional dictionary learning with autoencoder structure .Deep convolutional residual architecture computes the sparse code and a linear decoder reconstructs the input. Connection between  gradient-based sparse code search and deep residual encoder. 
@article{tolooshams2020,
  title={Deep residual autoencoders for expectation maximization-inspired dictionary learning},
  author={Tolooshams, Bahareh and Dey, Sourav and Ba, Demba},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  number={6},
  pages={2415--2429},
  year={2020},
  publisher={IEEE}
}

% theoretical analysis of deep sparse coding models
@article{ba2020,
  title={Deeply-Sparse Signal rePresentations (D $$\backslash$text $\{$S$\}$\^{} 2$ P)},
  author={Ba, Demba},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={4727--4742},
  year={2020},
  publisher={IEEE}
}

% locally linear approach for deep sparse coding
@article{tankala2020,
  title={K-deep simplex: Deep manifold learning via local dictionaries},
  author={Tankala, Pranay and Tasissa, Abiy and Murphy, James M and Ba, Demba},
  journal={arXiv preprint arXiv:2012.02134},
  year={2020}
}

% spike and slab sparse coding
@article{goodfellow2012,
  title={Large-scale feature learning with spike-and-slab sparse coding},
  author={Goodfellow, Ian and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1206.6407},
  year={2012}
}


%%%%%%%%%%
% hebbian ica
%%%%%%%%%%

% ica general - pioneering work
@article{jutten1991,
  title={Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture},
  author={Jutten, Christian and Herault, Jeanny},
  journal={Signal processing},
  volume={24},
  number={1},
  pages={1--10},
  year={1991},
  publisher={Elsevier}
}

% ica general - pioneering work
@article{comon1991,
  title={Blind separation of sources, Part II: Problems statement},
  author={Comon, Pierre and Jutten, Christian and Herault, Jeanny},
  journal={Signal processing},
  volume={24},
  number={1},
  pages={11--20},
  year={1991},
  publisher={Elsevier}
}

% ica general - pioneering work
@article{comon1994,
  title={Independent component analysis, a new concept?},
  author={Comon, Pierre},
  journal={Signal processing},
  volume={36},
  number={3},
  pages={287--314},
  year={1994},
  publisher={Elsevier}
}

% ica general - theory: equivariance property
@article{cardoso1996,
  title={Equivariant adaptive source separation},
  author={Cardoso, J-F and Laheld, Beate H},
  journal={IEEE Transactions on signal processing},
  volume={44},
  number={12},
  pages={3017--3030},
  year={1996},
  publisher={IEEE}
}

% ica general - theory: equivalence infomax-maximum likelihood
@article{cardoso1997,
  title={Infomax and maximum likelihood for blind source separation},
  author={Cardoso, J-F},
  journal={IEEE Signal processing letters},
  volume={4},
  number={4},
  pages={112--114},
  year={1997},
  publisher={IEEE}
}

% ica general - independent components are edge filters
@article{bell1997,
  title={The independent components of natural scenes are edge filters},
  author={Bell, Anthony J and Sejnowski, Terrence J},
  journal={Vision research},
  volume={37},
  number={23},
  pages={3327--3338},
  year={1997},
  publisher={Elsevier}
}

% ica general - ica and minimum mi, consistency theorem for ica
@misc{hyvarinen1997a,
  title={Independent component analysis by minimization of mutual information},
  author={Hyv{\"a}rinen, Aapo},
  year={1997},
}

% unified review of maximum likelihood, independence, and mi approaches for ica
@article{lee2000,
  title={A unifying information-theoretic framework for independent component analysis},
  author={Lee, Te-Won and Girolami, Mark and Bell, Anthony J and Sejnowski, Terrence J},
  journal={Computers \& Mathematics with Applications},
  volume={39},
  number={11},
  pages={1--21},
  year={2000},
  publisher={Elsevier}
}

% ica general - three approaches: non gaussianity, non stationarity, non whiteness
@inproceedings{cardoso2001,
  title={The three easy routes to independent component analysis; contrasts and geometry},
  author={Cardoso, Jean-Fran{\c{c}}ois},
  booktitle={Proc. ICA},
  volume={2001},
  pages={1--6},
  year={2001}
}

% ica general - theory: independence as decorrelation + non gaussianity measures
@article{cardoso2003,
  title={Dependence, correlation and gaussianity in independent component analysis},
  author={Cardoso, Jean-Fran{\c{c}}ois},
  journal={Journal of Machine Learning Research},
  volume={4},
  number={Dec},
  pages={1177--1203},
  year={2003}
}

% ica general - rules: ica by infomax
@article{bell1995,
  title={An information-maximization approach to blind separation and blind deconvolution},
  author={Bell, Anthony J and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={7},
  number={6},
  pages={1129--1159},
  year={1995},
  publisher={MIT Press}
}

% ica general - rules: ica by min mi
@inproceedings{amari1996,
  title={A new learning algorithm for blind signal separation},
  author={Amari, Shun-ichi and Cichocki, Andrzej and Yang, Howard Hua},
  booktitle={Advances in neural information processing systems},
  pages={757--763},
  year={1996}
}

% ica general - rules: ica by maximum likelihood - equivalence with min mi
@article{pham1997,
  title={Blind separation of mixture of independent sources through a quasi-maximum likelihood approach},
  author={Pham, Dinh Tuan and Garat, Philippe},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={7},
  pages={1712--1725},
  year={1997},
  publisher={IEEE}
}

% fast ica
@article{hyvarinen1997b,
  title={A fast fixed-point algorithm for independent component analysis},
  author={Hyv{\"a}rinen, Aapo and Oja, Erkki},
  journal={Neural computation},
  volume={9},
  number={7},
  pages={1483--1492},
  year={1997},
  publisher={MIT Press}
}

% fast ica
@inproceedings{hyvarinen1997c,
  title={A family of fixed-point algorithms for independent component analysis},
  author={Hyvarinen, Aapo},
  booktitle={1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={5},
  pages={3917--3920},
  year={1997},
  organization={IEEE}
}

% fast ica
@article{hyvarinen1999b,
  title={Fast and robust fixed-point algorithms for independent component analysis},
  author={Hyvarinen, Aapo},
  journal={IEEE transactions on Neural Networks},
  volume={10},
  number={3},
  pages={626--634},
  year={1999},
  publisher={IEEE}
}

% fast ica (symmetric, local)
@article{tichavsky2006,
  title={Performance analysis of the FastICA algorithm and Crame/spl acute/r-rao bounds for linear independent component analysis},
  author={Tichavsky, Petr and Koldovsky, Zbynek and Oja, Erkki},
  journal={IEEE transactions on Signal Processing},
  volume={54},
  number={4},
  pages={1189--1203},
  year={2006},
  publisher={IEEE}
}

% neural ica - connection between nonlinear pca, ica and projection pursuit (finding directions of maximum nongaussianity)
@article{fyfe1995,
  title={Non-linear data structure extraction using simple Hebbian networks},
  author={Fyfe, Colin and Baddeley, Roland},
  journal={Biological cybernetics},
  volume={72},
  number={6},
  pages={533--541},
  year={1995},
  publisher={Springer}
}

% neural ica
@article{hyvarinen1996,
  title={Simple neuron models for independent component analysis},
  author={Hyv{\"a}rinen, Aapo and Oja, Erkki},
  journal={International Journal of Neural Systems},
  volume={7},
  number={06},
  pages={671--687},
  year={1996},
  publisher={World Scientific}
}

% neural ica
@inproceedings{oja1996,
  title={Principal and independent components in neural networks-recent developments},
  author={Oja, E and Karhunen, J and Wang, L and Vigario, R},
  booktitle={Proc. VII Italian Workshop Neural Networks WIRN},
  volume={95},
  pages={16--35},
  year={1996}
}

% neural ica
@inproceedings{karhunen1996,
  title={Neural approaches to independent component analysis and source separation.},
  author={Karhunen, Juha},
  booktitle={ESANN},
  volume={96},
  pages={249--266},
  year={1996}
}

% neural ica
@article{amari1998,
  title={Adaptive blind signal processing-neural network approaches},
  author={Amari, Shun-ichi and Cichocki, Andrzej},
  journal={Proceedings of the IEEE},
  volume={86},
  number={10},
  pages={2026--2048},
  year={1998},
  publisher={IEEE}
}

% neural ica
@article{hyvarinen1998b,
  title={Independent component analysis by general nonlinear Hebbian-like learning rules},
  author={Hyv{\"a}rinen, Aapo and Oja, Erkki},
  journal={signal processing},
  volume={64},
  number={3},
  pages={301--313},
  year={1998},
  publisher={Elsevier}
}

% noisy ica, ica robust when only super or sub-gaussian distribution families are considered
@article{hyvarinen1998c,
  title={Independent component analysis in the presence of gaussian noise by maximizing joint likelihood},
  author={Hyv{\"a}rinen, Aapo},
  journal={Neurocomputing},
  volume={22},
  number={1-3},
  pages={49--67},
  year={1998},
  publisher={Elsevier}
}

% robust ica
@article{cichocki1996c,
  title={Robust neural networks with on-line learning for blind identification and blind separation of sources},
  author={Cichocki, Andrzej and Unbehauen, Rolf},
  journal={IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume={43},
  number={11},
  pages={894--906},
  year={1996},
  publisher={IEEE}
}

% robust and noisy ica
@article{cichocki1998,
  title={Robust techniques for independent component analysis (ICA) with noisy data},
  author={Cichocki, Andrzej and Douglas, Scott C and Amari, Shun-ichi},
  journal={Neurocomputing},
  volume={22},
  number={1-3},
  pages={113--129},
  year={1998},
  publisher={Elsevier}
}

% robust ica
@inproceedings{baloch2005,
  title={Robust independent component analysis},
  author={Baloch, Sajjad H and Krim, Hamid and Genton, Marc G},
  booktitle={IEEE/SP 13th Workshop on Statistical Signal Processing, 2005},
  pages={61--64},
  year={2005},
  organization={IEEE}
}

% ica arbitrary source number
@inproceedings{cichocki1997,
  title={Self adaptive independent component analysis for sub-Gaussian and super-Gaussian mixtures with unknown number of sources and additive noise},
  author={Cichocki, Andrzej and Sabala, Ireneusz and Choi, Seungjin and Orsier, Bruno and Szupiluk, Ryszard},
  booktitle={Proc. NOLTA},
  volume={97},
  pages={731--734},
  year={1997},
}

% ica arbitrary source number
@article{cichocki1999,
  title={Neural networks for blind separation with unknown number of sources},
  author={Cichocki, Andrzej and Karhunen, Juha and Kasprzak, Wlodzimierz and Vigario, Ricardo},
  journal={Neurocomputing},
  volume={24},
  number={1-3},
  pages={55--93},
  year={1999},
  publisher={Elsevier}
}

% ica arbitrary source number
@article{amari1999,
  title={Natural gradient learning for over-and under-complete bases in ICA},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={11},
  number={8},
  pages={1875--1883},
  year={1999},
  publisher={MIT Press}
}

% ica arbitrary source number
@article{amari2000,
  title={Nonholonomic orthogonal learning algorithms for blind source separation},
  author={Amari, Shun-Ichi and Chen, Tian-Ping and Cichocki, Andrzej},
  journal={Neural computation},
  volume={12},
  number={6},
  pages={1463--1484},
  year={2000},
  publisher={MIT Press}
}

% ica arbitrary source distribution
@inproceedings{douglas1997,
  title={Multichannel blind separation and deconvolution of sources with arbitrary distributions},
  author={Douglas, Scott C and Cichocki, Andrzej and Amari, S-I},
  booktitle={Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop},
  pages={436--445},
  year={1997},
  organization={IEEE}
}

% ica arbitrary source distribution
@article{lee1999,
  title={Independent component analysis using an extended infomax algorithm for mixed subgaussian and supergaussian sources},
  author={Lee, Te-Won and Girolami, Mark and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={11},
  number={2},
  pages={417--441},
  year={1999},
  publisher={MIT Press}
}

% ica arbitrary source distribution
@inproceedings{almeida2000,
  title={Linear and nonlinear ICA based on mutual information},
  author={Almeida, Luis B},
  booktitle={Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No. 00EX373)},
  pages={117--122},
  year={2000},
  organization={IEEE}
}

% ica arbitrary source distribution
@inproceedings{ihm2000,
  title={Blind separation for mixtures of sub-Gaussian and super-Gaussian sources},
  author={Ihm, B-C and Park, D-J and Kwon, Y-H},
  booktitle={2000 IEEE International Symposium on Circuits and Systems. Emerging Technologies for the 21st Century. Proceedings (IEEE Cat No. 00CH36353)},
  volume={3},
  pages={738--741},
  year={2000},
  organization={IEEE}
}

% temporal ica based on decorrelation - first work considering two sources
@article{molgedey1994,
  title={Separation of a mixture of independent signals using time delayed correlations},
  author={Molgedey, Lutz and Schuster, Heinz Georg},
  journal={Physical review letters},
  volume={72},
  number={23},
  pages={3634},
  year={1994},
  publisher={APS}
}

% temporal ica based on nonstationarity
@article{matsuoka1995,
  title={A neural net for blind separation of nonstationary signals},
  author={Matsuoka, Kiyotoshi and Ohoya, Masahiro and Kawamoto, Mitsuru},
  journal={Neural networks},
  volume={8},
  number={3},
  pages={411--419},
  year={1995},
  publisher={Elsevier}
}

% temporal ica based on decorrelation
@article{belouchrani1997,
  title={A blind source separation technique using second-order statistics},
  author={Belouchrani, Adel and Abed-Meraim, Karim and Cardoso, J-F and Moulines, Eric},
  journal={IEEE Transactions on signal processing},
  volume={45},
  number={2},
  pages={434--444},
  year={1997},
  publisher={IEEE}
}

% temporal ica based on decorrelation using Asymmetric PCA network, where the input signal is the ordinary input and the output is the delayed version of the input
@inproceedings{diamantaras1998,
  title={Asymmetric PCA neural networks for adaptive blind source separation},
  author={Diamantaras, Konstantinos I},
  booktitle={Neural Networks for Signal Processing VIII. Proceedings of the 1998 IEEE Signal Processing Society Workshop (Cat. No. 98TH8378)},
  pages={103--112},
  year={1998},
  organization={IEEE}
}

% temporal ica similar to matsuoka, but uses hebbian/anti-hebbian network
@inproceedings{meyer2001,
  title={Hebbian and anti-Hebbian learning for independent component analysis},
  author={Meyer-Base, Anke and Chen, Yunmei and McCullough, Scott},
  booktitle={IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222)},
  volume={2},
  pages={920--925},
  year={2001},
  organization={IEEE}
}

% temporal ica based on nonstationarity with feedforward and recurrent networks
@article{choi2002a,
  title={Equivariant nonstationary source separation},
  author={Choi, Seungjin and Cichocki, Andrzej and Amari, Shunichi},
  journal={Neural networks},
  volume={15},
  number={1},
  pages={121--130},
  year={2002},
  publisher={Elsevier}
}

% temporal ica based on decorrelation and nonstationarity with feedforward and recurrent networks
@article{choi2002b,
  title={Second order nonstationary source separation},
  author={Choi, Seungjin and Cichocki, Andrzej and Beloucharni, Adel},
  journal={Journal of VLSI signal processing systems for signal, image and video technology},
  volume={32},
  number={1-2},
  pages={93--104},
  year={2002},
  publisher={Springer}
}

% temporal ica based on decorrelation and nonstationarity with gradient-based methods for simultaneous diagonalization of correlation matrices
@inproceedings{joho2002,
  title={Joint diagonalization of correlation matrices by using gradient methods with application to blind signal separation},
  author={Joho, Marcel and Mathis, Heinz},
  booktitle={Sensor Array and Multichannel Signal Processing Workshop Proceedings, 2002},
  pages={273--277},
  year={2002},
  organization={IEEE}
}

% temporal ica similar to matsuoka, but uses hebbian/anti-hebbian network
@article{meyer2006,
  title={Blind source separation based on self-organizing neural network},
  author={Meyer-B{\"a}se, Anke and Gruber, Peter and Theis, Fabian and Foo, Simon},
  journal={Engineering Applications of Artificial Intelligence},
  volume={19},
  number={3},
  pages={305--311},
  year={2006},
  publisher={Elsevier}
}

% temporal ica by decorrelation and online eigenvector decomposition of correlation matrix at different delays
@inproceedings{clopath2008,
  title={An online hebbian learning rule that performs independent component analysis},
  author={Clopath, Claudia and Longtin, Andre and Gerstner, Wulfram},
  booktitle={Advances in Neural Information Processing Systems},
  pages={321--328},
  year={2008}
}

% mlp based nonlinear ica
@article{burel1992,
  title={Blind separation of sources: A nonlinear neural algorithm},
  author={Burel, Gilles},
  journal={Neural networks},
  volume={5},
  number={6},
  pages={937--947},
  year={1992},
  publisher={Elsevier}
}

% nonlinear ica with replicator network (autoencoder)
@article{hecht1995,
  title={Replicator neural networks for universal optimal source coding},
  author={Hecht-Nielsen, Robert},
  journal={Science},
  volume={269},
  number={5232},
  pages={1860--1863},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

% nonlinear ica with mlp and backprop
@inproceedings{fisher1997,
  title={Entropy manipulation of arbitrary nonlinear mappings},
  author={Fisher, John W and Principe, Jos{\'e} C},
  booktitle={Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop},
  pages={14--23},
  year={1997},
  organization={IEEE}
}

% nonlinear ica with mlp, natural gradient and backprop
@article{yang1998,
  title={Information--theoretic approach to blind separation of sources in non-linear mixture},
  author={Yang, Howard Hua and Amari, Shun-Ichi and Cichocki, Andrzej},
  journal={Signal processing},
  volume={64},
  number={3},
  pages={291--300},
  year={1998},
  publisher={Elsevier}
}

% nonlinear ica: post nonlinear mixture
@article{taleb1999,
  title={Source separation in post-nonlinear mixtures},
  author={Taleb, Anisse and Jutten, Christian},
  journal={IEEE Transactions on signal Processing},
  volume={47},
  number={10},
  pages={2807--2820},
  year={1999},
  publisher={IEEE}
}

% locally linear ica
@inproceedings{karhunen1999,
  title={Locally linear independent component analysis},
  author={Karhunen, Juha and Malaroiu, Simona},
  booktitle={IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No. 99CH36339)},
  volume={2},
  pages={882--887},
  year={1999},
  organization={IEEE}
}

% locally linear ica mixture model
@article{lee2000,
  title={ICA mixture models for unsupervised classification of non-Gaussian classes and automatic context switching in blind signal separation},
  author={Lee, Te-Won and Lewicki, Michael S and Sejnowski, Terrence J},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={22},
  number={10},
  pages={1078--1089},
  year={2000},
  publisher={IEEE}
}

% mlp based nonlinear ica
@incollection{lappalainen2000,
  title={Bayesian non-linear independent component analysis by multi-layer perceptrons},
  author={Lappalainen, Harri and Honkela, Antti},
  booktitle={Advances in independent component analysis},
  pages={93--121},
  year={2000},
  publisher={Springer}
}

% nonlinear ica with rbf
@article{tan2001,
  title={Nonlinear blind source separation using a radial basis function network},
  author={Tan, Ying and Wang, Jun and Zurada, Jacek M},
  journal={IEEE transactions on neural networks},
  volume={12},
  number={1},
  pages={124--134},
  year={2001},
  publisher={IEEE}
}

% nonlinear ica method inspired by electric field
@inproceedings{hochreiter2001a,
  title={Beyond maximum likelihood and density estimation: A sample-based criterion for unsupervised learning of complex models},
  author={Hochreiter, Sepp and Mozer, Michael C},
  booktitle={Advances in neural information processing systems},
  pages={535--541},
  year={2001}
}

% nonlinear ica
@article{karhunen2001,
  title={Nonlinear independent component analysis},
  author={Karhunen, J},
  journal={ICA: Principles and Practice},
  pages={113--134},
  year={2001},
  publisher={Cambridge University Press Cambridge}
}

% nonlinear bss
@inproceedings{jutten2003,
  title={Advances in nonlinear blind source separation},
  author={Jutten, Christian and Karhunen, Juha},
  booktitle={Proc. of the 4th Int. Symp. on Independent Component Analysis and Blind Signal Separation (ICA2003)},
  pages={245--256},
  year={2003}
}

% Nonlinear ICA with encoder-decoder model with stack of ica layers to enforce independence of latent variables
@article{dinh2014,
  title={Nice: Non-linear independent components estimation},
  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1410.8516},
  year={2014}
}

% nonlinear temporal ica
@article{harmeling2003,
  title={Kernel-based nonlinear blind source separation},
  author={Harmeling, Stefan and Ziehe, Andreas and Kawanabe, Motoaki and M{\"u}ller, Klaus-Robert},
  journal={Neural Computation},
  volume={15},
  number={5},
  pages={1089--1124},
  year={2003},
  publisher={MIT Press}
}

% nonlinear temporal ica
@article{martinez2003,
  title={Nonlinear blind source separation using kernels},
  author={Martinez, Dominique and Bray, Alistair},
  journal={IEEE Transactions on Neural networks},
  volume={14},
  number={1},
  pages={228--235},
  year={2003},
  publisher={IEEE}
}

% nonlinear temporal ica
@inproceedings{hyvarinen2016,
  title={Unsupervised feature extraction by time-contrastive learning and nonlinear ICA},
  author={Hyvarinen, Aapo and Morioka, Hiroshi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3765--3773},
  year={2016}
}

% nonlinear temporal ica
@inproceedings{hyvarinen2017,
  title={Nonlinear ICA of temporally dependent stationary sources},
  author={Hyvarinen, AJ and Morioka, Hiroshi},
  year={2017},
  booktitle={Proceedings of Machine Learning Research}
}

% nonlinear temporal ica
@inproceedings{hyvarinen2019,
  title={Nonlinear ICA using auxiliary variables and generalized contrastive learning},
  author={Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={859--868},
  year={2019}
}

% kernel ica
@article{bach2002,
  title={Kernel independent component analysis},
  author={Bach, Francis R and Jordan, Michael I},
  journal={Journal of machine learning research},
  volume={3},
  number={Jul},
  pages={1--48},
  year={2002}
}

% discriminative ica
@article{akaho2002,
  title={Conditionally independent component analysis for supervised feature extraction},
  author={Akaho, Shotaro},
  journal={Neurocomputing},
  volume={49},
  number={1-4},
  pages={139--150},
  year={2002},
  publisher={Elsevier}
}

% discriminative ica
@inproceedings{bressan2002,
  title={Improving naive Bayes using class-conditional ICA},
  author={Bressan, Marco and Vitri{\`a}, Jordi},
  booktitle={Ibero-American Conference on Artificial Intelligence},
  pages={1--10},
  year={2002},
  organization={Springer}
}

% discriminative ica
@article{dhir2011,
  title={Discriminant independent component analysis},
  author={Dhir, Chandra Shekhar and Lee, Soo-Young},
  journal={IEEE transactions on neural networks},
  volume={22},
  number={6},
  pages={845--857},
  year={2011},
  publisher={IEEE}
}

% nonnegative ica
@article{plumbley2004,
  title={A" nonnegative PCA" algorithm for independent component analysis},
  author={Plumbley, Mark D and Oja, Erkki},
  journal={IEEE Transactions on Neural Networks},
  volume={15},
  number={1},
  pages={66--76},
  year={2004},
  publisher={IEEE}
}

% nmf for bss
@inproceedings{cichocki2006,
  title={New algorithms for non-negative matrix factorization in applications to blind source separation},
  author={Cichocki, Andrzej and Zdunek, Rafal and Amari, Shun-ichi},
  booktitle={2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings},
  volume={5},
  pages={V--V},
  year={2006},
  organization={IEEE}
}

% subspace ica - mica: assume variables are group-wise independent, but dependent within groups. Shown that ica algorithms still work in this case.
@inproceedings{cardoso1998,
  title={Multidimensional independent component analysis},
  author={Cardoso, J-F},
  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)},
  volume={4},
  pages={1941--1944},
  year={1998},
  organization={IEEE}
}

% subspace ica - independent feature subspaces: project onto subspaces which maximize the norm of the projection while maximizing independence between subspaces
@article{hyvarinen2000,
  title={Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces},
  author={Hyv{\"a}rinen, Aapo and Hoyer, Patrik},
  journal={Neural computation},
  volume={12},
  number={7},
  pages={1705--1720},
  year={2000},
  publisher={MIT Press}
}

% competitive ica: use k-means to learn filters for sparse ica with overcomplete super-gaussian sources for better robustness to noise. Vice-versa, non-competitive (called anti-competitive) behavior used for sub-gaussian.
@inproceedings{hyvarinen1998d,
  title={Noisy independent component analysis, maximum likelihood estimation, and competitive learning},
  author={Hyvarinen, A},
  booktitle={1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No. 98CH36227)},
  volume={3},
  pages={2282--2287},
  year={1998},
  organization={IEEE}
}

% Maximal Cause Analysis: a variation of ICA in which the sources are not combined by sums, but rather by max. Shows that the sources can be identified with a reconstruction process that uses a competitive softmax nonlinearity on the demixer.
% What happens instead if the competition process is before the linear mixing? This creates variables with dependent distributions, from which an appropriate disentangling nonlinearity can be derived for the demixer. In this way, the demixer is able to find variables with the desired joint distribution. Then, an inverse competitive layer reconstructs the original variables. The combination of disentangling and competitive nonlinearities gives the overall nonlinearity.
@article{lucke2008,
  title={Maximal causes for non-linear component extraction.},
  author={L{\"u}cke, J{\"o}rg and Sahani, Maneesh},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={6},
  year={2008}
}

% topographic ica
@article{hyvarinen2001,
  title={Topographic independent component analysis},
  author={Hyv{\"a}rinen, Aapo and Hoyer, Patrik O and Inki, Mika},
  journal={Neural computation},
  volume={13},
  number={7},
  pages={1527--1558},
  year={2001},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

% smoca
@inproceedings{cai2007,
  title={Learning a spatially smooth subspace for face recognition},
  author={Cai, Deng and He, Xiaofei and Hu, Yuxiao and Han, Jiawei and Huang, Thomas},
  booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--7},
  year={2007},
  organization={IEEE}
}

% sfa
@article{foldiak1991,
  title={Learning invariance from transformation sequences},
  author={F{\"o}ldi{\'a}k, Peter},
  journal={Neural Computation},
  volume={3},
  number={2},
  pages={194--200},
  year={1991},
  publisher={MIT Press}
}

% sfa
@article{wiskott2002,
  title={Slow feature analysis: Unsupervised learning of invariances},
  author={Wiskott, Laurenz and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={14},
  number={4},
  pages={715--770},
  year={2002},
  publisher={MIT Press}
}

% sfa
@article{sprekeler2014,
  title={An extension of slow feature analysis for nonlinear blind source separation},
  author={Sprekeler, Henning and Zito, Tiziano and Wiskott, Laurenz},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={921--947},
  year={2014},
  publisher={JMLR. org}
}


%%%%%%%%%%
% compressed sensing
%%%%%%%%%%


% image reconstruction from few samples with total variation minimization
@article{needell2013,
  title={Stable image reconstruction using total variation minimization},
  author={Needell, Deanna and Ward, Rachel},
  journal={SIAM Journal on Imaging Sciences},
  volume={6},
  number={2},
  pages={1035--1058},
  year={2013},
  publisher={SIAM}
}



%%%%%%%%%%
% similarity matching
%%%%%%%%%%

% similarity matching
@inproceedings{pehlevan2014,
  title={A hebbian/anti-hebbian network derived from online non-negative matrix factorization can cluster and discover sparse features},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri B},
  booktitle={2014 48th Asilomar Conference on Signals, Systems and Computers},
  pages={769--775},
  year={2014},
  organization={IEEE}
}

% similarity matching
@inproceedings{hu2014,
  title={A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization},
  author={Hu, Tao and Pehlevan, Cengiz and Chklovskii, Dmitri B},
  booktitle={2014 48th Asilomar Conference on Signals, Systems and Computers},
  pages={613--619},
  year={2014},
  organization={IEEE}
}

% similarity matching with pehlevan rule: Hebbian-anti-Hebbian updates on forward-lateral connections derived from strain loss with penalty for cross-correlation extract principal subspace
@article{pehlevan2015a, 
    title={A hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data},
    author={Pehlevan, Cengiz and Hu, Tao and Chklovskii, Dmitri B},
    journal={Neural computation},
    volume={27},
    number={7},
    pages={1461--1495},
    year={2015},
    publisher={MIT Press}
}

% similarity matching
@inproceedings{pehlevan2015b,
  title={A normative theory of adaptive dimensionality reduction in neural networks},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri},
  booktitle={Advances in neural information processing systems},
  pages={2269--2277},
  year={2015}
}

% similarity matching 
@inproceedings{pehlevan2015c, 
  title={Optimization theory of hebbian/anti-hebbian networks for pca and whitening},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri B},
  booktitle={2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={1458--1465},
  year={2015},
  organization={IEEE}
}

% correlation game hebbian/anti-hebbian
@article{seung2017,
  title={A correlation game for unsupervised learning yields computational interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse elimination},
  author={Seung, H Sebastian and Zung, Jonathan},
  journal={arXiv preprint arXiv:1704.00646},
  year={2017}
}

% similarity matching for nonnegative ica
@article{pehlevan2017,
  title={Blind nonnegative source separation using biological neural networks},
  author={Pehlevan, Cengiz and Mohan, Sreyas and Chklovskii, Dmitri B},
  journal={Neural computation},
  volume={29},
  number={11},
  pages={2925--2954},
  year={2017},
  publisher={MIT Press}
}

% similarity matching
@inproceedings{sengupta2018,
  title={Manifold-tiling localized receptive fields are optimal in similarity-preserving neural networks},
  author={Sengupta, Anirvan and Pehlevan, Cengiz and Tepper, Mariano and Genkin, Alexander and Chklovskii, Dmitri},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7080--7090},
  year={2018}
}

% similarity matching
@article{pehlevan2018,
  title={Why do similarity matching objectives lead to hebbian/anti-hebbian networks?},
  author={Pehlevan, Cengiz and Sengupta, Anirvan M and Chklovskii, Dmitri B},
  journal={Neural computation},
  volume={30},
  number={1},
  pages={84--124},
  year={2018},
  publisher={MIT Press}
}

% log similarity matching criterion
@article{miao1998,
  title={Fast subspace tracking and neural network learning by a novel information criterion},
  author={Miao, Yongfeng and Hua, Yingbo},
  journal={IEEE Transactions on Signal Processing},
  volume={46},
  number={7},
  pages={1967--1979},
  year={1998},
  publisher={IEEE}
}

% similarity matching
@article{pehlevan2019,
  title={Neuroscience-inspired online unsupervised learning algorithms: Artificial neural networks},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri B},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={88--96},
  year={2019},
  publisher={IEEE}
}

% deep learning without backprop with similarity matching
@inproceedings{obeid2019,
  title={Structured and deep similarity matching via structured and deep hebbian networks},
  author={Obeid, Dina and Ramambason, Hugo and Pehlevan, Cengiz},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15403--15412},
  year={2019}
}

% deep learning without backprop with similarity matching
@article{qin2020a,
  title={Supervised Deep Similarity Matching},
  author={Qin, Shanshan and Mudur, Nayantara and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2002.10378},
  year={2020}
}

% contrastive similarity matching
@article{qin2021,
  title={Contrastive similarity matching for supervised learning},
  author={Qin, Shanshan and Mudur, Nayantara and Pehlevan, Cengiz},
  journal={Neural Computation},
  volume={33},
  number={5},
  pages={1300--1328},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

% Similarity matching for minor component analysis
@article{bahroun2021a,
  title={A Neural Network with Local Learning Rules for Minor Subspace Analysis},
  author={Bahroun, Yanis and Chklovskii, Dmitri B},
  journal={arXiv preprint arXiv:2102.05501},
  year={2021}
}

% similarity matching for ica
@article{bahroun2021b,
  title={A Normative and Biologically Plausible Algorithm for Independent Component Analysis},
  author={Bahroun, Yanis and Chklovskii, Dmitri and Sengupta, Anirvan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

% kernel similarity matching
@article{luther2022,
  title={Kernel similarity matching with Hebbian neural networks},
  author={Luther, Kyle and Seung, H Sebastian},
  journal={arXiv preprint arXiv:2204.07475},
  year={2022}
}


%%%%%%%%%%
% manifold learning
%%%%%%%%%%

% manifold learning review
@article{cayton2005,
  title={Algorithms for manifold learning},
  author={Cayton, Lawrence},
  journal={Univ. of California at San Diego Tech. Rep},
  volume={12},
  number={1-17},
  pages={1},
  year={2005}
}

% nonlinear dimensionality reduction review
@article{maaten2009,
  title={Dimensionality reduction: a comparative},
  author={Van Der Maaten, Laurens and Postma, Eric and Van den Herik, Jaap},
  journal={J Mach Learn Res},
  volume={10},
  number={66-71},
  pages={13},
  year={2009}
}

% connection metric multidimensional scaling - kernel pca
@inproceedings{williams2001a,
  title={On a connection between kernel PCA and metric multidimensional scaling},
  author={Williams, Christopher KI},
  booktitle={Advances in neural information processing systems},
  pages={675--681},
  year={2001}
}

% connection between spectral embeddings - kernel pca
@article{bengio2004a,
  title={Learning eigenfunctions links spectral embedding and kernel PCA},
  author={Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le and Paiement, Jean-Fran{\c{c}}ois and Vincent, Pascal and Ouimet, Marie},
  journal={Neural computation},
  volume={16},
  number={10},
  pages={2197--2219},
  year={2004},
  publisher={MIT Press}
}

% sammon mapping
@article{sammon1969,
  title={A nonlinear mapping for data structure analysis},
  author={Sammon, John W},
  journal={IEEE Transactions on computers},
  volume={100},
  number={5},
  pages={401--409},
  year={1969},
  publisher={Ieee}
}

% neural sammon's mapping mds (SAMANN)
@article{mao1995,
  title={Artificial neural networks for feature extraction and multivariate data projection},
  author={Mao, Jianchang and Jain, Anil K},
  journal={IEEE transactions on neural networks},
  volume={6},
  number={2},
  pages={296--317},
  year={1995},
  publisher={IEEE}
}

% mds scaling
@incollection{cox2008,
  title={Multidimensional scaling},
  author={Cox, Michael AA and Cox, Trevor F},
  booktitle={Handbook of data visualization},
  pages={315--347},
  year={2008},
  publisher={Springer}
}

% mds with bregman divergences
@article{sun2011,
  title={Extending metric multidimensional scaling with Bregman divergences},
  author={Sun, Jigang and Crowe, Malcolm and Fyfe, Colin},
  journal={Pattern recognition},
  volume={44},
  number={5},
  pages={1137--1154},
  year={2011},
  publisher={Elsevier}
}

% principal curves
@article{hastie1989,
  title={Principal curves},
  author={Hastie, Trevor and Stuetzle, Werner},
  journal={Journal of the American Statistical Association},
  volume={84},
  number={406},
  pages={502--516},
  year={1989},
  publisher={Taylor \& Francis}
}

% connection principal curves - som
@article{mulier1995,
  title={Self-organization as an iterative kernel smoothing process},
  author={Mulier, Filip and Cherkassky, Vladimir},
  journal={Neural computation},
  volume={7},
  number={6},
  pages={1165--1177},
  year={1995},
  publisher={MIT Press}
}

% principal curves
@article{kegl2000,
  title={Learning and design of principal curves},
  author={K{\'e}gl, Bal{\'a}zs and Krzyzak, Adam and Linder, Tam{\'a}s and Zeger, Kenneth},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={22},
  number={3},
  pages={281--297},
  year={2000},
  publisher={IEEE}
}

% principal curves with k-segments
@article{verbeek2002,
  title={A k-segments algorithm for finding principal curves},
  author={Verbeek, Jakob J and Vlassis, Nikos and Kr{\"o}se, B},
  journal={Pattern Recognition Letters},
  volume={23},
  number={8},
  pages={1009--1017},
  year={2002},
  publisher={Elsevier}
}

% curvilinear component analysis
@article{demartines1997,
  title={Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets},
  author={Demartines, Pierre and H{\'e}rault, Jeanny},
  journal={IEEE Transactions on neural networks},
  volume={8},
  number={1},
  pages={148--154},
  year={1997},
  publisher={IEEE}
}

% curvilinear component analysis with bregman divergences
@inproceedings{sun2010,
  title={Curvilinear component analysis and Bregman divergences.},
  author={Sun, Jigang and Fyfe, Colin and Crowe, Malcolm K},
  booktitle={ESANN},
  year={2010}
}

% isomap
@article{tenenbaum2000,
  title={A global geometric framework for nonlinear dimensionality reduction},
  author={Tenenbaum, Joshua B and De Silva, Vin and Langford, John C},
  journal={science},
  volume={290},
  number={5500},
  pages={2319--2323},
  year={2000},
  publisher={American Association for the Advancement of Science}
}

% lle
@article{roweis2000,
  title={Nonlinear dimensionality reduction by locally linear embedding},
  author={Roweis, Sam T and Saul, Lawrence K},
  journal={science},
  volume={290},
  number={5500},
  pages={2323--2326},
  year={2000},
  publisher={American Association for the Advancement of Science}
}

% lle based on correntropy
@article{daza2012,
  title={Locally linear embedding based on correntropy measure for visualization and classification},
  author={Daza-Santacoloma, Genaro and Castellanos-Dominguez, German and Principe, Jose C},
  journal={Neurocomputing},
  volume={80},
  pages={19--30},
  year={2012},
  publisher={Elsevier}
}

% laplacian eigenmaps
@inproceedings{belkin2002,
  title={Laplacian eigenmaps and spectral techniques for embedding and clustering},
  author={Belkin, Mikhail and Niyogi, Partha},
  booktitle={Advances in neural information processing systems},
  pages={585--591},
  year={2002}
}

% laplacian eigenmaps
@article{belkin2003,
  title={Laplacian eigenmaps for dimensionality reduction and data representation},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1373--1396},
  year={2003},
  publisher={MIT Press}
}

% lpp
@inproceedings{he2004,
  title={Locality preserving projections},
  author={He, Xiaofei and Niyogi, Partha},
  booktitle={Advances in neural information processing systems},
  pages={153--160},
  year={2004}
}

% lpp via dnn
@inproceedings{long2019,
  title={Locality Preserving Projection via Deep Neural Network},
  author={Long, Tianhang and Gao, Junbin and Yang, Mingyan and Hu, Yongli and Yin, Baocai},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

% npe
@inproceedings{he2005,
  title={Neighborhood preserving embedding},
  author={He, Xiaofei and Cai, Deng and Yan, Shuicheng and Zhang, Hong-Jiang},
  booktitle={Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
  volume={2},
  pages={1208--1213},
  year={2005},
  organization={IEEE}
}

% mvu
@inproceedings{weinberger2004,
  title={Learning a kernel matrix for nonlinear dimensionality reduction},
  author={Weinberger, Kilian Q and Sha, Fei and Saul, Lawrence K},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={106},
  year={2004}
}

% mvu
@inproceedings{song2008,
  title={Colored maximum variance unfolding},
  author={Song, Le and Gretton, Arthur and Borgwardt, Karsten and Smola, Alex J},
  booktitle={Advances in neural information processing systems},
  pages={1385--1392},
  year={2008}
}

% ltsa
@article{zhang2004,
  title={Principal manifolds and nonlinear dimensionality reduction via tangent space alignment},
  author={Zhang, Zhenyue and Zha, Hongyuan},
  journal={SIAM journal on scientific computing},
  volume={26},
  number={1},
  pages={313--338},
  year={2004},
  publisher={SIAM}
}

% lcc
@inproceedings{yu2009,
  title={Nonlinear learning using local coordinate coding},
  author={Yu, Kai and Zhang, Tong and Gong, Yihong},
  booktitle={Advances in neural information processing systems},
  pages={2223--2231},
  year={2009}
}

% spp
@article{qiao2010,
  title={Sparsity preserving projections with applications to face recognition},
  author={Qiao, Lishan and Chen, Songcan and Tan, Xiaoyang},
  journal={Pattern Recognition},
  volume={43},
  number={1},
  pages={331--341},
  year={2010},
  publisher={Elsevier}
}

% elastic embedding
@inproceedings{carreira2010,
  title={The Elastic Embedding Algorithm for Dimensionality Reduction.},
  author={Carreira-Perpin{\'a}n, Miguel A},
  booktitle={ICML},
  volume={10},
  pages={167--174},
  year={2010}
}

% sne
@inproceedings{hinton2003,
  title={Stochastic neighbor embedding},
  author={Hinton, Geoffrey E and Roweis, Sam T},
  booktitle={Advances in neural information processing systems},
  pages={857--864},
  year={2003}
}

% sne
@article{maaten2008,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

% sne with arbitrary divergences
@article{bunte2012,
  title={Stochastic neighbor embedding (SNE) for dimension reduction and visualization using arbitrary divergences},
  author={Bunte, Kerstin and Haase, Sven and Biehl, Michael and Villmann, Thomas},
  journal={Neurocomputing},
  volume={90},
  pages={23--45},
  year={2012},
  publisher={Elsevier}
}

% UMAP
@article{mcinnes2018,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

% Sparse Manifold Transform (SMT) combines a sparse coding layer with a manifold learning layer, the former providing a high-dimensional projection, and the second for compression. 
@article{chen2018,
  title={The sparse manifold transform},
  author={Chen, Yubei and Paiton, Dylan and Olshausen, Bruno},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% out of sample extensions
@inproceedings{bengio2004b,
  title={Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering},
  author={Bengio, Yoshua and Paiement, Jean-fran{\c{c}}cois and Vincent, Pascal and Delalleau, Olivier and Roux, Nicolas L and Ouimet, Marie},
  booktitle={Advances in neural information processing systems},
  pages={177--184},
  year={2004}
}

% discriminative manifold learning with marginal fisher analysis (mfa)
@inproceedings{yan2005,
  title={Graph embedding: A general framework for dimensionality reduction},
  author={Yan, Shuicheng and Xu, Dong and Zhang, Benyu and Zhang, Hong-Jiang},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={2},
  pages={830--837},
  year={2005},
  organization={IEEE}
}

% discriminative manifold learning with marginal fisher analysis (mfa)
@article{yan2006,
  title={Graph embedding and extensions: A general framework for dimensionality reduction},
  author={Yan, Shuicheng and Xu, Dong and Zhang, Benyu and Zhang, Hong-Jiang and Yang, Qiang and Lin, Stephen},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={29},
  number={1},
  pages={40--51},
  year={2006},
  publisher={IEEE}
}

% discriminative principal curves
@inproceedings{chang1998,
  title={Principal curve classifier-a nonlinear approach to pattern classification},
  author={Chang, Kui-yu and Ghosh, Joydeep},
  booktitle={1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No. 98CH36227)},
  volume={1},
  pages={695--700},
  year={1998},
  organization={IEEE}
}

% discriminative lle
@incollection{deridder2003,
  title={Supervised locally linear embedding},
  author={De Ridder, Dick and Kouropteva, Olga and Okun, Oleg and Pietik{\"a}inen, Matti and Duin, Robert PW},
  booktitle={Artificial Neural Networks and Neural Information ProcessingICANN/ICONIP 2003},
  pages={333--341},
  year={2003},
  publisher={Springer}
}

% discriminative isomap
@article{geng2005,
  title={Supervised nonlinear dimensionality reduction for visualization and classification},
  author={Geng, Xin and Zhan, De-Chuan and Zhou, Zhi-Hua},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={35},
  number={6},
  pages={1098--1107},
  year={2005},
  publisher={IEEE}
}

% discriminative (colored) mvu
@inproceedings{weinberger2006,
  title={An introduction to nonlinear dimensionality reduction by maximum variance unfolding},
  author={Weinberger, Kilian Q and Saul, Lawrence K},
  booktitle={AAAI},
  volume={6},
  pages={1683--1686},
  year={2006}
}

% discriminative neighborhood embedding
@article{zhang2006b,
  title={Discriminant neighborhood embedding for classification},
  author={Zhang, Wei and Xue, Xiangyang and Lu, Hong and Guo, Yue-Fei},
  journal={Pattern Recognition},
  volume={39},
  number={11},
  pages={2240--2243},
  year={2006},
  publisher={Elsevier}
}

% discriminative neighborhood embedding
@inproceedings{zeng2007,
  title={A supervised subspace learning algorithm: supervised neighborhood preserving embedding},
  author={Zeng, Xianhua and Luo, Siwei},
  booktitle={Advanced Data Mining and Applications: Third International Conference, ADMA 2007 Harbin, China, August 6-8, 2007. Proceedings 3},
  pages={81--88},
  year={2007},
  organization={Springer}
}

% discriminative neighborhood embedding
@article{zhang2008,
  title={Metric learning by discriminant neighborhood embedding},
  author={Zhang, Wei and Xue, Xiangyang and Sun, Zichen and Lu, Hong and Guo, Yue-Fei},
  journal={Pattern recognition},
  volume={41},
  number={6},
  pages={2086--2096},
  year={2008},
  publisher={Elsevier}
}

% discriminative manifold learning
@inproceedings{wang2009b,
  title={Manifold discriminant analysis},
  author={Wang, Ruiping and Chen, Xilin},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={429--436},
  year={2009},
  organization={IEEE}
}

% discriminative le
@article{raducanu2012,
  title={A supervised non-linear dimensionality reduction approach for manifold learning},
  author={Raducanu, Bogdan and Dornaika, Fadi},
  journal={Pattern Recognition},
  volume={45},
  number={6},
  pages={2432--2444},
  year={2012},
  publisher={Elsevier}
}

% discriminative sne
@inproceedings{chien2016,
  title={Deep discriminative manifold learning},
  author={Chien, Jen-Tzung and Chen, Ching-Huai},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2672--2676},
  year={2016},
  organization={IEEE}
}

% discriminative spp
@article{liu2019a,
  title={Discriminative low-rank preserving projection for dimensionality reduction},
  author={Liu, Zhonghua and Wang, Jingjing and Liu, Gang and Zhang, Lin},
  journal={Applied Soft Computing},
  volume={85},
  pages={105768},
  year={2019},
  publisher={Elsevier}
}


%%%%%%%%%%
% information theoretic learning
%%%%%%%%%%

% Infomax
@inproceedings{linsker1988,
  title={Towards an organizing principle for a layered perceptual network},
  author={Linsker, Ralph},
  booktitle={Neural information processing systems},
  pages={485--494},
  year={1988}
}

% Predictability maximization for multi-view feature learning similar to IMax
@article{schmidhuber1993,
  title={Discovering predictable classifications},
  author={Schmidhuber, J{\"u}rgen and Prelinger, Daniel},
  journal={Neural Computation},
  volume={5},
  number={4},
  pages={625--635},
  year={1993},
  publisher={MIT Press}
}

% Imax
@article{becker1996b, 
  title={Mutual information maximization: models of cortical self-organization},
  author={Becker, Suzanna},
  journal={Network: Computation in neural systems},
  volume={7},
  number={1},
  pages={7--31},
  year={1996},
  publisher={Taylor \& Francis}
}

% coherent ica
@inproceedings{haykin2007,
  title={Coherent ICA: Implications for Auditory Signal Processing},
  author={Haykin, Simon and Kan, Kevin},
  booktitle={2007 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
  pages={1--5},
  year={2007},
  organization={IEEE}
}

% Information bottleneck
@article{tishby2000,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

% optimal manifold
@inproceedings{chigirev2004,
  title={Optimal manifold representation of data: an information theoretic approach},
  author={Chigirev, Denis V and Bialek, William},
  booktitle={Advances in Neural Information Processing Systems},
  pages={161--168},
  year={2004}
}

% variational lb for infomax
@inproceedings{barber2003,
  title={The im algorithm: a variational approach to information maximization},
  author={Barber, David and Agakov, Felix V},
  booktitle={Advances in neural information processing systems},
  pages={None},
  year={2003}
}

% deep learning with information bottleneck
@inproceedings{alemi2017,
title={Deep Variational Information Bottleneck},
author={Alexander A. Alemi and Ian Fischer and Joshua V. Dillon and Kevin Murphy},
booktitle={International Conference on Learning Representations},
year={2017},
}

% nonlinear information bottleneck
@article{kolchinsky2019,
  title={Nonlinear information bottleneck},
  author={Kolchinsky, Artemy and Tracey, Brendan D and Wolpert, David H},
  journal={Entropy},
  volume={21},
  number={12},
  pages={1181},
  year={2019},
  publisher={MDPI}
}

% deep learning with information bottleneck
@inproceedings{chalk2016,
  title={Relevant sparse codes with variational information bottleneck},
  author={Chalk, Matthew and Marre, Olivier and Tkacik, Gasper},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1957--1965},
  year={2016}
}

% information dropout
@article{achille2018,
  title={Information dropout: Learning optimal representations through noisy computation},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2897--2905},
  year={2018},
  publisher={IEEE}
}

% deep learning without backprop with information bottleneck
@inproceedings{ma2020,
  title={The HSIC Bottleneck: Deep Learning without Back-Propagation.},
  author={Ma, Kurt Wan-Duo and Lewis, JP and Kleijn, W Bastiaan},
  booktitle={AAAI},
  pages={5085--5092},
  year={2020}
}

% deep learning without backprop with information bottleneck
@article{pogodin2020,
  title={Kernelized information bottleneck leads to biologically plausible 3-factor Hebbian learning in deep networks},
  author={Pogodin, Roman and Latham, Peter E},
  journal={arXiv preprint arXiv:2006.07123},
  year={2020}
}

% deep learning without backprop with information bottleneck and working memory
@article{daruwalla2021,
  title={Information Bottleneck-Based Hebbian Learning Rule Naturally Ties Working Memory and Synaptic Updates},
  author={Daruwalla, Kyle and Lipasti, Mikko},
  journal={arXiv preprint arXiv:2111.13187},
  year={2021}
}

% constraining discriminator with information bottleneck in gans
@article{peng2018,
  title={Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow},
  author={Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.00821},
  year={2018}
}

% information bottleneck for exploration in rl
@article{goyal2019,
  title={Infobot: Transfer and exploration via the information bottleneck},
  author={Goyal, Anirudh and Islam, Riashat and Strouse, Daniel and Ahmed, Zafarali and Botvinick, Matthew and Larochelle, Hugo and Bengio, Yoshua and Levine, Sergey},
  journal={arXiv preprint arXiv:1901.10902},
  year={2019}
}

% information bottleneck for regularization in rl
@article{pacelli2020,
  title={Learning task-driven control policies via information bottlenecks},
  author={Pacelli, Vincent and Majumdar, Anirudha},
  journal={arXiv preprint arXiv:2002.01428},
  year={2020}
}

% information bottleneck for regularization in rl
@article{lu2020,
  title={Dynamics generalization via information bottleneck in deep reinforcement learning},
  author={Lu, Xingyu and Lee, Kimin and Abbeel, Pieter and Tiomkin, Stas},
  journal={arXiv preprint arXiv:2008.00614},
  year={2020}
}

% InfoMax-GAN: uses mutual information (infoNCE lower bound) to regularize discriminator in gans
@inproceedings{lee2021,
  title={Infomax-gan: Improved adversarial image generation via information maximization and contrastive learning},
  author={Lee, Kwot Sin and Tran, Ngoc-Trung and Cheung, Ngai-Man},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={3942--3952},
  year={2021}
}

% InfoAT: Information bottleneck for adversarial training. The author notice that samples that are less robust to adversarial attacks are those that have high mutual information between latent representation and input. InfoAT uses information bottleneck to rank adversarially weaker samples and prioritize them for adversarial training in order to increase robustness.
@article{xu2022,
  title={InfoAT: Improving Adversarial Training Using the Information Bottleneck Principle},
  author={Xu, Mengting and Zhang, Tao and Li, Zhongnian and Zhang, Daoqiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

% information bottleneck regularization for domain adaptation
@article{song2019,
  title={Improving unsupervised domain adaptation with variational information bottleneck},
  author={Song, Yuxuan and Yu, Lantao and Cao, Zhangjie and Zhou, Zhiming and Shen, Jian and Shao, Shuo and Zhang, Weinan and Yu, Yong},
  journal={arXiv preprint arXiv:1911.09310},
  year={2019}
}

% information bottleneck regularization for domain adaptation
@article{chen2021,
  title={Beyond mutual information: Generative adversarial network for domain adaptation using information bottleneck constraint},
  author={Chen, Jiawei and Zhang, Ziqi and Xie, Xinpeng and Li, Yuexiang and Xu, Tao and Ma, Kai and Zheng, Yefeng},
  journal={IEEE Transactions on Medical Imaging},
  volume={41},
  number={3},
  pages={595--607},
  year={2021},
  publisher={IEEE}
}

% information bottleneck for self-supervised learning
@article{liu2022,
  title={Masked Reconstruction Contrastive Learning with Information Bottleneck Principle},
  author={Liu, Ziwen and Li, Bonan and Han, Congying and Guo, Tiande and Nie, Xuecheng},
  journal={arXiv preprint arXiv:2211.09013},
  year={2022}
}

% information bottleneck for graphs
@article{wu2020,
  title={Graph information bottleneck},
  author={Wu, Tailin and Ren, Hongyu and Li, Pan and Leskovec, Jure},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20437--20448},
  year={2020}
}

% information bottleneck for graphs
@inproceedings{sun2022,
  title={Graph structure learning with variational information bottleneck},
  author={Sun, Qingyun and Li, Jianxin and Peng, Hao and Wu, Jia and Fu, Xingcheng and Ji, Cheng and Philip, S Yu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={4},
  pages={4165--4174},
  year={2022}
}

% information bottleneck for self-supervised graph representation learning
@article{gu2022,
  title={Self-Supervised Graph Representation Learning via Information Bottleneck},
  author={Gu, Junhua and Zheng, Zichen and Zhou, Wenmiao and Zhang, Yajuan and Lu, Zhengjun and Yang, Liang},
  journal={Symmetry},
  volume={14},
  number={4},
  pages={657},
  year={2022},
  publisher={MDPI}
}

% BottleSum: information bottleneck for sentence summarization
@article{west2019,
  title={Bottlesum: Unsupervised and self-supervised sentence summarization using the information bottleneck principle},
  author={West, Peter and Holtzman, Ari and Buys, Jan and Choi, Yejin},
  journal={arXiv preprint arXiv:1909.07405},
  year={2019}
}

% Renyi's entropy criterion, information potential
@inproceedings{principe1999,
  title={Information-theoretic learning using Renyis quadratic entropy},
  author={Principe, Jose C and Xu, Dongxin},
  booktitle={Proceedings of the first international workshop on independent component analysis and signal separation},
  volume={1},
  year={1999}
}

% Renyi's mutual information criterion
@inproceedings{torkkola2000,
  title={Mutual information in learning feature transformations},
  author={Torkkola, Kari and Campbell, William M},
  booktitle={ICML},
  pages={1015--1022},
  year={2000}
}

% Renyi's mutual information criterion - efficient implementation by stochastic or gmm Parzen window approx
@inproceedings{torkkola2002,
  title={On feature extraction by mutual information maximization},
  author={Torkkola, Kari},
  booktitle={2002 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={1},
  pages={I--821},
  year={2002},
  organization={IEEE}
}

% Reny mi estimation
@article{pal2010,
  title={Estimation of R{\'e}nyi entropy and mutual information based on generalized nearest-neighbor graphs},
  author={P{\'a}l, D{\'a}vid and P{\'o}czos, Barnab{\'a}s and Szepesv{\'a}ri, Csaba},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}


% renyi gan
@article{sarraf2021,
  title={RGAN: R{\'e}nyi generative adversarial network},
  author={Sarraf, Aydin and Nie, Yimin},
  journal={SN Computer Science},
  volume={2},
  pages={1--8},
  year={2021},
  publisher={Springer}
}

% cumulant GAN, equivalent to renyi gan. generalizes other types of GAN
@article{pantazis2022,
  title={Cumulant gan},
  author={Pantazis, Yannis and Paul, Dipjyoti and Fasoulakis, Michail and Stylianou, Yannis and Katsoulakis, Markos A},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

% stochastic information gradient
@article{erdogmus2003,
  title={Online entropy manipulation: Stochastic information gradient},
  author={Erdogmus, Deniz and Hild, Kenneth E and Principe, Jose C},
  journal={IEEE Signal Processing Letters},
  volume={10},
  number={8},
  pages={242--245},
  year={2003},
  publisher={IEEE}
}

% Renyi's entropy criterion with stochastic gradient approx and stochastic Parzen window approx leads to Hebbian-like rule
@inproceedings{erdogmus2002,
  title={Do Hebbian synapses estimate entropy?},
  author={Erdogmus, Deniz and Principe, Jose C and Hild, Kenneth E},
  booktitle={Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing},
  pages={199--208},
  year={2002},
  organization={IEEE}
}

% smi
@article{sugiyama2013,
  title={Machine learning with squared-loss mutual information},
  author={Sugiyama, Masashi},
  journal={Entropy},
  volume={15},
  number={1},
  pages={80--112},
  year={2013},
  publisher={Multidisciplinary Digital Publishing Institute}
}

% entropy estimation with knn criterion
@article{goria2005,
  title={A new class of random vector entropy estimators and its applications in testing statistical hypotheses},
  author={Goria, Mohammed Nawaz and Leonenko, Nikolai N and Mergel, Victor V and Novi Inverardi, Pier Luigi},
  journal={Journal of Nonparametric Statistics},
  volume={17},
  number={3},
  pages={277--297},
  year={2005},
  publisher={Taylor \& Francis}
}

% mi estimation with knn criterion
@article{kraskov2004,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}

% mi estimation with hilbert-schmit criterion
@inproceedings{gretton2005,
  title={Measuring statistical dependence with Hilbert-Schmidt norms},
  author={Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  booktitle={International conference on algorithmic learning theory},
  pages={63--77},
  year={2005},
  organization={Springer}
}

% mi estimation - success actually due to variational bounds employed for estimation
@article{tschannen2019,
  title={On mutual information maximization for representation learning},
  author={Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K and Gelly, Sylvain and Lucic, Mario},
  journal={arXiv preprint arXiv:1907.13625},
  year={2019}
}

% mi estimation with gan for ica
@article{brakel2017,
  title={Learning independent features with adversarial nets for non-linear ica},
  author={Brakel, Philemon and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.05050},
  year={2017}
}

% mi estimation with gan - mine
@article{belghazi2018,
  title={Mine: mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  journal={arXiv preprint arXiv:1801.04062},
  year={2018}
}

% mi estimation with gan - dim
@article{hjelm2018,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}

% contrastive mi estimation
@inproceedings{cheng2020,
  title={Club: A contrastive log-ratio upper bound of mutual information},
  author={Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
  booktitle={International conference on machine learning},
  pages={1779--1788},
  year={2020},
  organization={PMLR}
}


%%%%%%%%%%
% Differential Hebbian learning
%%%%%%%%%%

% differential hebbian learning
@inproceedings{kosko1986,
  title={Differential hebbian learning},
  author={Kosko, Bart},
  booktitle={AIP Conference proceedings},
  volume={151},
  pages={277--282},
  year={1986},
  organization={American Institute of Physics}
}

% differential hebbian learning for ica with non-centered data
@article{choi1998,
  title={Differential Hebbian-type learning algorithms for decorrelation and independent component analysis},
  author={Choi, Seungjin},
  journal={Electronics Letters},
  volume={34},
  number={9},
  pages={900--900},
  year={1998},
  publisher={IET}
}

% temporally asymmetric STDP = differential Hebbian learning 
@article{roberts1999,
  title={Computational consequences of temporally asymmetric learning rules: I. Differential Hebbian learning},
  author={Roberts, Patrick D},
  journal={Journal of computational neuroscience},
  volume={7},
  number={3},
  pages={235--246},
  year={1999},
  publisher={Springer}
}

% continuation of the previous work
@article{roberts2000,
  title={Computational consequences of temporally asymmetric learning rules: II. Sensory image cancellation},
  author={Roberts, Patrick D and Bell, Curtis C},
  journal={Journal of computational neuroscience},
  volume={9},
  pages={67--83},
  year={2000},
  publisher={Springer}
}

% STDP = TD learning
@article{rao2001,
  title={Spike-timing-dependent Hebbian plasticity as temporal difference learning},
  author={Rao, Rajesh PN and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={13},
  number={10},
  pages={2221--2237},
  year={2001},
  publisher={MIT Press}
}

% differential Hebbian learning = td learning
@article{kolodziejski2009a,
  title={On the asymptotic equivalence between differential Hebbian and temporal difference learning},
  author={Kolodziejski, Christoph and Porr, Bernd and W{\"o}rg{\"o}tter, Florentin},
  journal={Neural computation},
  volume={21},
  number={4},
  pages={1173--1202},
  year={2009},
  publisher={MIT Press}
}

% differential Hebbian learning = td learning
@inproceedings{kolodziejski2009b,
  title={On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor},
  author={Kolodziejski, Christoph and Porr, Bernd and Tamosiunaite, Minija and W{\"o}rg{\"o}tter, Florentin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={857--864},
  year={2009}
}

% differential hebbian learning rule: delta w = x*V'
@article{bengio2015a, 
  title={STDP as presynaptic activity times rate of change of postsynaptic activity},
  author={Bengio, Yoshua and Mesnard, Thomas and Fischer, Asja and Zhang, Saizheng and Wu, Yuhuai},
  journal={arXiv preprint arXiv:1509.05936},
  year={2015}
}


%%%%%%%%%%
% Homeostatic plasticity and stp
%%%%%%%%%%

% overview of homeostatic mechanisms
@article{watt2010,
  title={Homeostatic plasticity and STDP: keeping a neuron's cool in a fluctuating world},
  author={Watt, Alanna J and Desai, Niraj S},
  journal={Frontiers in synaptic neuroscience},
  volume={2},
  pages={5},
  year={2010},
  publisher={Frontiers}
}

% overview of homeostatic mechanisms
@article{turrigiano2004,
  title={Homeostatic plasticity in the developing nervous system},
  author={Turrigiano, Gina G and Nelson, Sacha B},
  journal={Nature reviews neuroscience},
  volume={5},
  number={2},
  pages={97--107},
  year={2004},
  publisher={Nature Publishing Group}
}

% overview of homeostatic mechanisms
@article{turrigiano2012,
  title={Homeostatic synaptic plasticity: local and global mechanisms for stabilizing neuronal function},
  author={Turrigiano, Gina},
  journal={Cold Spring Harbor perspectives in biology},
  volume={4},
  number={1},
  pages={a005736},
  year={2012},
  publisher={Cold Spring Harbor Lab}
}

% presynaptic vs postsynaptic redistribution/heterosynaptic competition
@article{choe2000,
  title={Effects of presynaptic and postsynaptic resource redistribution in Hebbian weight adaptation},
  author={Choe, Yoonsuck and Miikkulainen, Risto and Cormack, Lawrence K},
  journal={Neurocomputing},
  volume={32},
  pages={77--82},
  year={2000},
  publisher={Elsevier}
}

% constraints in Hebbian learning --> synaptic redistribution/heterosynaptic competition
@article{miller1994,
  title={The role of constraints in Hebbian learning},
  author={Miller, Kenneth D and MacKay, David JC},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={100--126},
  year={1994},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

% synaptic economics: heterosynaptic competition mechanisms produce a synaptic redistribution of resources in order to maintain stable overall synaptic efficacy or post-synaptic activity
@article{miller1996,
  title={Synaptic economics: competition and cooperation in synaptic plasticity},
  author={Miller, Kenneth D},
  journal={Neuron},
  volume={17},
  number={3},
  pages={371--374},
  year={1996},
  publisher={Elsevier}
}

% A recurrent network with stdp and homeostatic plasticity for reservoir computing
@article{lazar2009,
  title={SORN: a self-organizing recurrent neural network},
  author={Lazar, Andreea and Pipa, Gordon and Triesch, Jochen},
  journal={Frontiers in computational neuroscience},
  pages={23},
  year={2009},
  publisher={Frontiers}
}

% Homeostatic plasaticity models in snns with stdp
@inproceedings{carlson2013,
  title={Biologically plausible models of homeostasis and STDP: stability and learning in spiking neural networks},
  author={Carlson, Kristofor D and Richert, Micah and Dutt, Nikil and Krichmar, Jeffrey L},
  booktitle={The 2013 international joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2013},
  organization={IEEE}
}

% review stp models
@article{hennig2013,
  title={Theoretical models of synaptic short term plasticity},
  author={Hennig, Matthias H},
  journal={Frontiers in computational neuroscience},
  volume={7},
  pages={45},
  year={2013},
  publisher={Frontiers}
}

% stp/dynamic synapses - Tsodyks-Markram model
@article{tsodyks1998,
  title={Neural networks with dynamic synapses},
  author={Tsodyks, Misha and Pawelzik, Klaus and Markram, Henry},
  journal={Neural computation},
  volume={10},
  number={4},
  pages={821--835},
  year={1998},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

% stp gives temporal selectivity
@article{buonomano2000,
  title={Decoding temporal information: a model based on short-term synaptic plasticity},
  author={Buonomano, Dean V},
  journal={Journal of Neuroscience},
  volume={20},
  number={3},
  pages={1129--1141},
  year={2000},
  publisher={Soc Neuroscience}
}

% stp as a mechanism for temporal/frequency filtering
@article{fortune2001,
  title={Short-term synaptic plasticity as a temporal filter},
  author={Fortune, Eric S and Rose, Gary J},
  journal={Trends in neurosciences},
  volume={24},
  number={7},
  pages={381--385},
  year={2001},
  publisher={Elsevier}
}


%%%%%%%%%%
% hybrid hebbian backprop
%%%%%%%%%%

@inproceedings{wood1992,
  title={A neural network that uses a Hebbian/backpropagation hybrid learning rule},
  author={Wood, Richard J and Gennert, Michael A},
  booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
  volume={3},
  pages={863--868},
  year={1992},
  organization={IEEE}
}

@article{dong1996,
  title={Merging Back-propagation and Hebbian Learning Rules for Robust Classifications},
  author={Dong-Gyu, Jeong and Soo-Young, Lee},
  journal={Neural Networks},
  volume={9},
  number={7},
  pages={1213--1222},
  year={1996},
  publisher={Elsevier}
}

@article{jeong2000,
  title={Adaptive learning algorithms to incorporate additional functional constraints into neural networks},
  author={Jeong, So-Young and Lee, Soo-Young},
  journal={Neurocomputing},
  volume={35},
  number={1-4},
  pages={73--90},
  year={2000},
  publisher={Elsevier}
}

% k-wta Hebbian/anti-Hebbian (HaH) approach on 6 early VGG-16 layers, followed by ordinary VGG-16 subsequent layers --> 87.3% accuracy on CIFAR10. Good noise and adversarial robustness.
@inproceedings{cekic2022,
  title={Neuro-inspired deep neural networks with sparse, strong activations},
  author={Cekic, Metehan and Bakiskan, Can and Madhow, Upamanyu},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},
  pages={3843--3847},
  year={2022},
  organization={IEEE}
}


%%%%%%%%%%
% hebbian transferability
%%%%%%%%%%

% transfer learning with hebbian plasticity
@inproceedings{magotra2019,
  title={Transfer Learning for Image Classification Using Hebbian Plasticity Principles},
  author={Magotra, Arjun and kim, Juntae},
  booktitle={Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
  pages={233--238},
  year={2019}
}

% transfer learning with hebbian plasticity
@article{magotra2020,
  title={Improvement of Heterogeneous Transfer Learning Efficiency by Using Hebbian Learning Principle},
  author={Magotra, Arjun and Kim, Juntae},
  journal={Applied Sciences},
  volume={10},
  number={16},
  pages={5631},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

% transfer learning with hebbian plasticity
@inproceedings{canto2020,
  title={Convolutional Neural Networks with Hebbian-Based Rules in Online Transfer Learning},
  author={Canto, Fernando Javier Aguilar},
  booktitle={Mexican International Conference on Artificial Intelligence},
  pages={35--49},
  year={2020},
  organization={Springer}
}

% transfer learning with neuromodulated hebbian plasticity
@article{magotra2021,
  title={Neuromodulated Dopamine Plastic Networks for Heterogeneous Transfer Learning with Hebbian Principle},
  author={Magotra, Arjun and Kim, Juntae},
  journal={Symmetry},
  volume={13},
  number={8},
  pages={1344},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}


%%%%%%%%%%
% multimodal
%%%%%%%%%%

% survey on multimodal machine learning
@article{ramachandram2017,
  title={Deep multimodal learning: A survey on recent advances and trends},
  author={Ramachandram, Dhanesh and Taylor, Graham W},
  journal={IEEE signal processing magazine},
  volume={34},
  number={6},
  pages={96--108},
  year={2017},
  publisher={IEEE}
}

% survey on multi-modal machine learning
@article{baltruvsaitis2018,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={2},
  pages={423--443},
  year={2018},
  publisher={IEEE}
}

% survey on image captioning and visual-text alignment approaches
@article{hossain2019,
  title={A comprehensive survey of deep learning for image captioning},
  author={Hossain, MD Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
  journal={ACM Computing Surveys (CsUR)},
  volume={51},
  number={6},
  pages={1--36},
  year={2019},
  publisher={ACM New York, NY, USA}
}

% survey on image captioning and visual-text alignment approaches
@article{stefanini2022,
  title={From show to tell: A survey on deep learning-based image captioning},
  author={Stefanini, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Cascianelli, Silvia and Fiameni, Giuseppe and Cucchiara, Rita},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={1},
  pages={539--559},
  year={2022},
  publisher={IEEE}
}

% survey on cross-modal retrieval
@article{wang2016b,
  title={A comprehensive survey on cross-modal retrieval},
  author={Wang, Kaiye and Yin, Qiyue and Wang, Wei and Wu, Shu and Wang, Liang},
  journal={arXiv preprint arXiv:1607.06215},
  year={2016}
}

% text to image synthesis with gans
@inproceedings{reed2016,
  title={Generative adversarial text to image synthesis},
  author={Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  booktitle={International conference on machine learning},
  pages={1060--1069},
  year={2016},
  organization={PMLR}
}

% Text2Vis: network for text-to-image synthesis
@article{carrara2018a,
  title={Picture it in your mind: Generating high level visual representations from textual descriptions},
  author={Carrara, Fabio and Esuli, Andrea and Fagni, Tiziano and Falchi, Fabrizio and Moreo Fern{\'a}ndez, Alejandro},
  journal={Information Retrieval Journal},
  volume={21},
  pages={208--229},
  year={2018},
  publisher={Springer}
}

% cycle-consistency criterion for multimodal learning
@inproceedings{cornia2018,
  title={Towards cycle-consistent models for text and image retrieval},
  author={Cornia, Marcella and Baraldi, Lorenzo and Tavakoli, Hamed R and Cucchiara, Rita},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}

% Multi-view learning with information bottleneck
@inproceedings{wang2019,
  title={Deep multi-view information bottleneck},
  author={Wang, Qi and Boudreau, Claire and Luo, Qixing and Tan, Pang-Ning and Zhou, Jiayu},
  booktitle={Proceedings of the 2019 SIAM International Conference on Data Mining},
  pages={37--45},
  year={2019},
  organization={SIAM}
}

% Hebbian cross modal retrieval/multimodal hebbian learning
@article{kaur2021,
  title={Hybrid SOM based cross-modal retrieval exploiting Hebbian learning},
  author={Kaur, Parminder and Malhi, Avleen Kaur and Pannu, Husanbir Singh},
  journal={Knowledge-Based Systems},
  pages={108014},
  year={2021},
  publisher={Elsevier}
}

% TERN transformer-based architecture for image-text matching
@inproceedings{messina2021a,
  title={Transformer reasoning network for image-text matching and retrieval},
  author={Messina, Nicola and Falchi, Fabrizio and Esuli, Andrea and Amato, Giuseppe},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={5222--5229},
  year={2021},
  organization={IEEE}
}

% TERAN - extension of TERN for fine-grained visual-textual alignment
@article{messina2021b,
  title={Fine-grained visual textual alignment for cross-modal retrieval using transformer encoders},
  author={Messina, Nicola and Amato, Giuseppe and Esuli, Andrea and Falchi, Fabrizio and Gennaro, Claudio and Marchand-Maillet, St{\'e}phane},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={17},
  number={4},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

% ALADIN: align and distill network for cross-modal retrieval
@inproceedings{messina2022,
  title={ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval},
  author={Messina, Nicola and Stefanini, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Falchi, Fabrizio and Amato, Giuseppe and Cucchiara, Rita},
  booktitle={Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
  pages={64--70},
  year={2022}
}

% CLIP features for captioning
@inproceedings{barraco2022,
  title={The unreasonable effectiveness of CLIP features for image captioning: an experimental analysis},
  author={Barraco, Manuele and Cornia, Marcella and Cascianelli, Silvia and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4662--4670},
  year={2022}
}

% image captioning using additional textual information retrieved from search engine
@inproceedings{sarto2022,
  title={Retrieval-augmented transformer for image captioning},
  author={Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
  pages={1--7},
  year={2022}
}

% image captioning with image-text and text-image consistency criterion
@inproceedings{sarto2023,
  title={Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation},
  author={Sarto, Sara and Barraco, Manuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6914--6924},
  year={2023}
}


%%%%%%%%%%
% explainability and bias
%%%%%%%%%%

% Information bottleneck for classification: maximize mi with target variable while masking information about an unwanted bias
@inproceedings{gronowski2022,
  title={R{\'e}nyi fair information bottleneck for image classification},
  author={Gronowski, Adam and Paul, William and Alajaji, Fady and Gharesifard, Bahman and Burlina, Philippe},
  booktitle={2022 17th Canadian Workshop on Information Theory (CWIT)},
  pages={11--15},
  year={2022},
  organization={IEEE}
}


%%%%%%%%%%
% hebbian rnns
%%%%%%%%%%

% survey on associative memories
@article{wang2011b,
  title={Recurrent neural networks: Associative memory and optimization},
  author={Wang, Hui and Wu, Yue and Zhang, Biaobiao and Du, KL},
  journal={J Inform Tech Soft Engg},
  volume={1},
  number={104},
  pages={2},
  year={2011}
}

% seminal paper hopfield model
@article{hopfield1982,
  title={Neural networks and physical systems with emergent collective computational abilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

% dynamic hebbian rnn models with multiplicative inhibitory lateral feedback (and related lyapunov function) resulting in competitive behavior and formation of absolutely stable global patterns
@article{cohen1983,
  title={Absolute stability of global pattern formation and parallel memory storage by competitive neural networks},
  author={Cohen, Michael A and Grossberg, Stephen},
  journal={IEEE transactions on systems, man, and cybernetics},
  number={5},
  pages={815--826},
  year={1983},
  publisher={IEEE}
}

% capacity of hopfield model
@article{abu1985,
  title={Information capacity of the Hopfield model},
  author={Abu-Mostafa, Yaser and Jacques, J St},
  journal={IEEE Transactions on Information Theory},
  volume={31},
  number={4},
  pages={461--464},
  year={1985},
  publisher={IEEE}
}

% capacity of hopfield model
@article{mceliece1987,
  title={The capacity of the Hopfield associative memory},
  author={McEliece, ROBERTJ and Posner, Edwardc and Rodemich, EUGENER and Venkatesh, SANTOSHS},
  journal={IEEE transactions on Information Theory},
  volume={33},
  number={4},
  pages={461--482},
  year={1987},
  publisher={IEEE}
}

% almeida-pineda recurrent backprop
@article{pineda1987a,
  title={Generalization of back-propagation to recurrent neural networks},
  author={Pineda, Fernando J},
  journal={Physical review letters},
  volume={59},
  number={19},
  pages={2229},
  year={1987},
  publisher={APS}
}

% almeida-pineda recurrent backprop, also applied to hopfield memory training
@inproceedings{pineda1987b,
  title={Generalization of back propagation to recurrent and higher order neural networks},
  author={Pineda, Fernando},
  booktitle={Neural information processing systems},
  pages={602--611},
  year={1987}
}

% almeida-pineda recurrent backprop, also applied to hopfield memory training
@article{pineda1988,
  title={Dynamics and architecture for neural computation},
  author={Pineda, Fernando J},
  journal={Journal of Complexity},
  volume={4},
  number={3},
  pages={216--245},
  year={1988},
  publisher={Elsevier}
}

% almeida-pineda recurrent backprop
@incollection{almeida1989,
  title={Backpropagation in perceptrons with feedback},
  author={Almeida, Luis B},
  booktitle={Neural computers},
  pages={199--208},
  year={1989},
  publisher={Springer}
}

% real time recurrent learning
@article{williams1989,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural computation},
  volume={1},
  number={2},
  pages={270--280},
  year={1989},
  publisher={MIT Press}
}

% capacity of hopfield model, learning rule for increasing capacity
@inproceedings{storkey1997,
  title={Increasing the capacity of a Hopfield network without sacrificing functionality},
  author={Storkey, Amos},
  booktitle={International Conference on Artificial Neural Networks},
  pages={451--456},
  year={1997},
  organization={Springer}
}

% increasing the capacity of hopfield networks with dynamic synapses/stp
@article{chechik2001a,
  title={Effective neuronal learning with ineffective Hebbian learning rules},
  author={Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},
  journal={Neural Computation},
  volume={13},
  number={4},
  pages={817--840},
  year={2001},
  publisher={MIT Press}
}

% Studies how hopfield networks with dynamic synapses/stp affect stability of fixed points, as well as storage capacity. Depression decreases storage capacity.
@article{bibitchkov2002,
  title={Pattern storage and processing in attractor networks with short-time synaptic dynamics},
  author={Bibitchkov, Dmitri and Herrmann, J Michael and Geisel, Theo},
  journal={Network: Computation in neural systems},
  volume={13},
  number={1},
  pages={115},
  year={2002},
  publisher={IOP Publishing}
}

% hopfield networks with dynamic synapses/stp create attractor states corresponding to stored patterns, but also switches over different stored patterns (persistent dynamic attractors)
@article{pantic2002,
  title={Associative memory with dynamic synapses},
  author={Pantic, Lovorka and Torres, Joaqu{\'\i}n J and Kappen, Hilbert J and Gielen, Stan CAM},
  journal={Neural Computation},
  volume={14},
  number={12},
  pages={2903--2923},
  year={2002},
  publisher={MIT Press}
}

% hopfield networks with dynamic synapses/stp create attractor states corresponding to stored patterns, but also switch over different stored patterns (persistent dynamic attractors). Also studies the effect of stp on attractor stability and storage capacity. Synaptic facilitation decreases attractor stability, allowing faster retrieval with less error, increasing capacity.
@article{torres2007,
  title={Competition between synaptic depression and facilitation in attractor neural networks},
  author={Torres, Joaqu{\'\i}n J and Cortes, Jes{\'u}s M and Marro, Joaqu{\'\i}n and Kappen, Hilbert J},
  journal={Neural Computation},
  volume={19},
  number={10},
  pages={2739--2755},
  year={2007},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

% hopfield networks with dynamic synapses/stp exhibit persistent attractor states, suggesting they can be used for storing time-varying patterns
@article{barak2007,
  title={Persistent activity in neural networks with dynamic synapses},
  author={Barak, Omri and Tsodyks, Misha},
  journal={PLoS computational biology},
  volume={3},
  number={2},
  pages={e35},
  year={2007},
  publisher={Public Library of Science San Francisco, USA}
}

% saliency hopfield memory
@article{tang2010,
  title={Memory dynamics in attractor networks with saliency weights},
  author={Tang, Huajin and Li, Haizhou and Yan, Rui},
  journal={Neural Computation},
  volume={22},
  number={7},
  pages={1899--1926},
  year={2010},
  publisher={MIT Press}
}

% hopfield networks with dynamic synapses/stp affect stability of fixed points and create oscillatory states over stored patterns (persistent dynamic attractors)
@article{katori2013,
  title={Stability analysis of associative memory network composed of stochastic neurons and dynamic synapses},
  author={Katori, Yuichi and Otsubo, Yosuke and Okada, Masato and Aihara, Kazuyuki},
  journal={Frontiers in computational neuroscience},
  volume={7},
  pages={6},
  year={2013},
  publisher={Frontiers}
}

% training rnns by infomax
@article{hayakawa2014,
  title={A biologically plausible learning rule for the Infomax on recurrent neural networks},
  author={Hayakawa, Takashi and Kaneko, Takeshi and Aoyagi, Toshio},
  journal={Frontiers in computational neuroscience},
  volume={8},
  pages={143},
  year={2014},
  publisher={Frontiers}
}

% training rnns by reward modulated hebbian learning
@article{miconi2015,
  title={Training recurrent neural networks with sparse, delayed rewards for flexible decision tasks},
  author={Miconi, Thomas},
  journal={arXiv preprint arXiv:1507.08973},
  year={2015}
}

% training rnns by reward modulated hebbian learning
@article{miconi2017,
  title={Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks},
  author={Miconi, Thomas},
  journal={Elife},
  volume={6},
  pages={e20899},
  year={2017},
  publisher={eLife Sciences Publications Limited}
}

% restricted hopfield network is a hopfield network composed of two sub-modules, visible and hidden. Only the visible nodes are clamped/read, while hidden nodes are just for computation.
@article{yeap2021,
  title={Implementation of an Associative Memory Using a Restricted Hopfield Network},
  author={Yeap, Tet},
  journal={Global Journal of Research In Engineering},
  year={2021}
}

% Hopfield model for supervised tasks: 95% accuracy on MNIST
@article{alemanno2022,
  title={Supervised hebbian learning},
  author={Alemanno, Francesco and Aquaro, Miriam and Kanter, Ido and Barra, Adriano and Agliari, Elena},
  journal={Europhysics Letters},
  year={2022}
}

% Mixed model with a feedforward module to project inputs to decorrelated representations trained with hebbian plasticity, and a recurrent hopfield model for memory storage
@inproceedings{ravichandran2023,
  title={Brain-like combination of feedforward and recurrent network components achieves prototype extraction and robust pattern recognition},
  author={Ravichandran, Naresh Balaji and Lansner, Anders and Herman, Pawel},
  booktitle={Machine Learning, Optimization, and Data Science: 8th International Conference, LOD 2022, Certosa di Pontignano, Italy, September 18--22, 2022, Revised Selected Papers, Part II},
  pages={488--501},
  year={2023},
  organization={Springer}
}


%%%%%%%%%%
% bidirectional networks
%%%%%%%%%%

@article{ardizzone2018,
  title={Analyzing inverse problems with invertible neural networks},
  author={Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W and Klessen, Ralf S and Maier-Hein, Lena and Rother, Carsten and K{\"o}the, Ullrich},
  journal={arXiv preprint arXiv:1808.04730},
  year={2018}
}

@inproceedings{pontes2019,
  title={Bidirectional learning for robust neural networks},
  author={Pontes-Filho, Sidney and Liwicki, Marcus},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@article{adigun2019,
  title={Bidirectional backpropagation},
  author={Adigun, Olaoluwa and Kosko, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={50},
  number={5},
  pages={1982--1994},
  year={2019},
  publisher={IEEE}
}

@article{kosko2021,
  title={Bidirectional associative memories: unsupervised hebbian learning to bidirectional backpropagation},
  author={Kosko, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={51},
  number={1},
  pages={103--115},
  year={2021},
  publisher={IEEE}
}

% invertible attention
@article{zha2021,
  title={Invertible Attention},
  author={Zha, Jiajun and Zhong, Yiran and Zhang, Jing and Zheng, Liang and Hartley, Richard},
  journal={arXiv preprint arXiv:2106.09003},
  year={2021}
}


%%%%%%%%%%
% hebbian language
%%%%%%%%%%

@inproceedings{gorrell2005,
  title={Generalized hebbian algorithm for incremental latent semantic analysis},
  author={Gorrell, Genevieve and Webb, Brandyn},
  booktitle={Ninth European Conference on Speech Communication and Technology},
  year={2005}
}

@inproceedings{gorrell2006,
  title={Generalized Hebbian algorithm for incremental singular value decomposition in natural language processing},
  author={Gorrell, Genevieve},
  booktitle={11th conference of the European chapter of the association for computational linguistics},
  year={2006}
}

@article{golosio2015,
  title={A cognitive neural model of executive functions in natural language processing},
  author={Golosio, Bruno and Cangelosi, Angelo and Gamotina, Olesya and Masala, Giovanni Luca},
  journal={Procedia Computer Science},
  volume={71},
  pages={196--201},
  year={2015},
  publisher={Elsevier}
}

@article{vodrahalli2015,
  title={Comparing Hebbian Semantic Vectors Across Language},
  author={Vodrahalli, Kiran},
  year={2015}
}


%%%%%%%%%%
% hebbian metalearning
%%%%%%%%%%

% meta-learning of a local unsupervised synaptic update rule
@inproceedings{bengio1992,
  title={On the optimization of a synaptic learning rule},
  author={Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  booktitle={Preprints Conf. Optimality in Artificial and Biological Neural Networks},
  volume={2},
  year={1992},
  organization={Univ. of Texas}
}

% meta-learning unsupervised local optimization rules with top-down feedback and lateral decorrelation
@article{metz2018,
  title={Meta-learning update rules for unsupervised representation learning},
  author={Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1804.00222},
  year={2018}
}

% meta-learning unsupervised local plasticity rules by automatic task construction using deep unsupervised clustering methods.
@article{hsu2018,
  title={Unsupervised learning via meta-learning},
  author={Hsu, Kyle and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1810.02334},
  year={2018}
}

% meta-learning with fast and slow weights. Fast hebbian weights in the inner loop and slow meta-learned weights in the outer loop.
@article{munkhdalai2018,
  title={Metalearning with hebbian fast weights},
  author={Munkhdalai, Tsendsuren and Trischler, Adam},
  journal={arXiv preprint arXiv:1807.05076},
  year={2018}
}

% differentiable plasticity: synapses with hebbian weights tuned in the inner loop with naive hebbian rule, and backprop weights meta-trained in the outer loop to leverage local plasticity.
@article{miconi2018,
  title={Differentiable plasticity: training plastic neural networks with backpropagation},
  author={Miconi, Thomas and Clune, Jeff and Stanley, Kenneth O},
  journal={arXiv preprint arXiv:1804.02464},
  year={2018}
}

% meta learning a reward-modulated differentiable plasticity model.
@article{miconi2020,
  title={Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity},
  author={Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
  journal={arXiv preprint arXiv:2002.10585},
  year={2020}
}

% discoverying local synaptic plasticity rules such as oja or pca by meta-learning plasticity rules to obtain the desired function
@article{confavreux2020,
  title={A meta-learning approach to (re) discover plasticity rules that carve a desired function into a neural network},
  author={Confavreux, Basile and Zenke, Friedemann and Agnes, Everton J and Lillicrap, Timothy and Vogels, Tim P},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

% meta-learning local plasticity rules with top-down feedback and supervision
@article{lindsey2020,
  title={Learning to learn with feedback and local plasticity},
  author={Lindsey, Jack and Litwin-Kumar, Ashok},
  journal={arXiv preprint arXiv:2006.09549},
  year={2020}
}

% meta-learning hebbian plasticity rules in random networks
@article{najarro2020,
  title={Meta-learning through hebbian plasticity in random networks},
  author={Najarro, Elias and Risi, Sebastian},
  journal={arXiv preprint arXiv:2007.02686},
  year={2020}
}

% learning to use associative memories
@article{oreilly2006,
  title={Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia},
  author={O'Reilly, Randall C and Frank, Michael J},
  journal={Neural computation},
  volume={18},
  number={2},
  pages={283--328},
  year={2006},
  publisher={MIT Press}
}

% learning to use associative memories
@inproceedings{vertechi2014,
  title={Unsupervised learning of an efficient short-term memory network},
  author={Vertechi, Pietro and Brendel, Wieland and Machens, Christian K},
  booktitle={Advances in neural information processing systems},
  pages={3653--3661},
  year={2014}
}

% learning to use associative memories
@inproceedings{ba2016,
  title={Using fast weights to attend to the recent past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4331--4339},
  year={2016}
}

% meta-learning updates for memory modules
@article{zhang2017a,
  title={Learning to update auto-associative memory in recurrent neural networks for improving sequence memorization},
  author={Zhang, Wei and Zhou, Bowen},
  journal={arXiv preprint arXiv:1709.06493},
  year={2017}
}

% learning to use associative memories
@article{rae2018,
  title={Fast parametric learning with activation memorization},
  author={Rae, Jack W and Dyer, Chris and Dayan, Peter and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1803.10049},
  year={2018}
}

% learning to use associative memories
@article{keller2018,
  title={Fast Weight Long Short-Term Memory},
  author={Keller, T Anderson and Sridhar, Sharath Nittur and Wang, Xin},
  journal={arXiv preprint arXiv:1804.06511},
  year={2018}
}

% learning to use associative memories with rewarded Hebbian learning
@article{pogodin2019,
  title={Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks},
  author={Pogodin, Roman and Corneil, Dane and Seeholzer, Alexander and Heng, Joseph and Gerstner, Wulfram},
  journal={arXiv preprint arXiv:1910.10559},
  year={2019}
}

% meta-learning synaptic plasticity for memory modules
@article{limbacher2020,
  title={H-Mem: Harnessing synaptic plasticity with Hebbian memory networks},
  author={Limbacher, Thomas and Legenstein, Robert},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

% meta-learning local update rules in bidirectional networks
@inproceedings{sandler2021,
  title={Meta-Learning Bidirectional Update Rules},
  author={Sandler, Mark and Vladymyrov, Max and Zhmoginov, Andrey and Miller, Nolan and Madams, Tom and Jackson, Andrew and Arcas, Blaise Ag{\"u}era Y},
  booktitle={International Conference on Machine Learning},
  pages={9288--9300},
  year={2021},
  organization={PMLR}
}


%%%%%%%%%%
% hebbian cv applications
%%%%%%%%%%

% vector quantization and sparse coding for unsupervised learning of convolutional filters. CIFAR10 78% with 1600 filters, 81.5% with OMP1 and 6000 filters. NORB 94%, caltech101 72%. Also uses polarity splitting. Actually, as shown in \cite{stl10}, performance grows from 65% for 100 filters up to this value. There, it is also shown that cifar10 79,6%, norb 97%, stl10 51% with 4000 features.
@inproceedings{coates2011a,
  title={The importance of encoding versus training with sparse coding and vector quantization},
  author={Coates, Adam and Ng, Andrew Y},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={921--928},
  year={2011}
}

% Clustering in large spaces does not work. In order to add further layers of features, proposes to group input vectors based on high-order correlations similarity. Kind of patch extraction based close features not in physical space, but in the similarity space of correlations. Performs clustering on patch groups separately. CIFAR10 81% with 2 layers, 82% accuracy with 3 layers, 1600, 3200, 3200 features. With 400 samples, 65% with 1 layer, 69% with 2 layers, 70% with 3 layers. 60% on STL10.
@article{coates2011b,
  title={Selecting receptive fields in deep networks},
  author={Coates, Adam and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

% Summarizes the results of the previous two papers
@incollection{coates2012,
  title={Learning feature representations with k-means},
  author={Coates, Adam and Ng, Andrew Y},
  booktitle={Neural Networks: Tricks of the Trade: Second Edition},
  pages={561--580},
  year={2012},
  publisher={Springer}
}

% Convolutional clustering 0.5% error on MNIST with 3 layer cnn, 74.1% accuracy on STL-10 with 4 layer cnn
@article{dundar2015, 
  title={Convolutional clustering for unsupervised learning},
  author={Dundar, Aysegul and Jin, Jonghoon and Culurciello, Eugenio},
  journal={arXiv preprint arXiv:1511.06241},
  year={2015}
}

% Hebbian k-WTA application
@article{wadhwa2016a, 
    title={Learning Sparse, Distributed Representations using the Hebbian Principle},
    author={Wadhwa, Aseem and Madhow, Upamanyu},
    journal={arXiv preprint arXiv:1611.04228},
    year={2016}
}

 % Hebbian k-WTA + DHL application 0.65% error on MNIST 75.87% accuracy on CIFAR-10, 3.48% error on NORB with 3 conv layers + svm
@misc{wadhwa2016b,
    title={Bottom-up Deep Learning using the Hebbian Principle},
    author={Wadhwa, Aseem and Madhow, Upamanyu},
    year={2016}
}

% Pehleval rule application 80% accuracy on CIFAR-10 with 2 layer cnn
@inproceedings{bahroun2017, 
  title={Online representation learning with single and multi-layer Hebbian networks for image classification},
  author={Bahroun, Yanis and Soltoggio, Andrea},
  booktitle={International Conference on Artificial Neural Networks},
  pages={354--363},
  year={2017},
  organization={Springer}
}

% CORNet: simplifies CNN architectures to make them more similar to biological visual system. 75% accuracy on ImageNet (top-1) with 4 layers, but still uses full backprop training. Slightly lower than competing artificial architectures, but CORNet achieves higher BrainScore results.
@article{kubilius2018,
  title={Cornet: Modeling the neural mechanisms of core object recognition},
  author={Kubilius, Jonas and Schrimpf, Martin and Nayebi, Aran and Bear, Daniel and Yamins, Daniel LK and DiCarlo, James J},
  journal={BioRxiv},
  pages={408385},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

% competitive learning on single hidden layer net 98% mnist, 50% cifar with 2fc net
@article{krotov2019,
  title={Unsupervised learning by competing hidden units},
  author={Krotov, Dmitry and Hopfield, John J},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={16},
  pages={7723--7731},
  year={2019},
  publisher={National Acad Sciences}
}

% single fc layer + final classifier, pca, sparse coding, ica - best is ica with 53.9 cifar10, 98.8 mnist, pca 50.8% cifar10, 98.2% mnist, sc 50.2% cifar, 98.4% mnist
@article{illing2019,
  title={Biologically plausible deep learningBut how far can we go with shallow networks?},
  author={Illing, Bernd and Gerstner, Wulfram and Brea, Johanni},
  journal={Neural Networks},
  volume={118},
  pages={90--101},
  year={2019},
  publisher={Elsevier}
}

% competitive learning with end-to-end loss, pruning connections, dense activations (i.e. no competition during forward pass) - cifar10 64% with 3 layer cnn
@article{miconi2021,
  title={Hebbian learning with gradients: Hebbian convolutional neural networks with modern deep learning frameworks},
  author={Miconi, Thomas},
  journal={arXiv preprint arXiv:2107.01729},
  year={2021}
}

% soft-WTA on MNIST - 96.94\% acc, 3fc net. Exhibits single epoch advantage. Also seems quite robust to adversarial examples.
@article{moraitis2021,
  title={SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks},
  author={Moraitis, Timoleon and Toichkin, Dmitry and Chua, Yansong and Guo, Qinghai},
  journal={arXiv preprint arXiv:2107.05747},
  year={2021}
}

% Hebbian WTA with coscience term applied to cnns. 98.2% accuracy on mnist (3 layers), 61% accuracy on cifar10 (3 layers), 12% top1 and 26% top5 accuracy on imagenet (4 layers).
@article{shinozaki2021,
  title={Biologically motivated learning method for deep neural networks using hierarchical competitive learning},
  author={Shinozaki, Takashi},
  journal={Neural Networks},
  year={2021},
  publisher={Elsevier}
}

% Based on soft-hebb, authors show that very wide networks, with width factor increasing by 4 at each layer, can learn meaningful representations without supervision. 99.35 on MNIST, 80.31 on CIFAR10 67.2 on STL-10, 27.3 on ImageNet, 80.38 on ImageNette, with 3 conv layers.
@article{journe2022,
  title={Hebbian deep learning without feedback},
  author={Journ{\'e}, Adrien and Rodriguez, Hector Garcia and Guo, Qinghai and Moraitis, Timoleon},
  journal={arXiv preprint arXiv:2209.11883},
  year={2022}
}

% experiments with bio-inspired algorithms show better performance compared to backprop with 20% labels regime and faster convergence (5 vs 100 epochs) with 100% labels. also, authors stress that backprop comes from decades of research and improvement, while Hebbian learning has started to gain attention only more recently.
@article{gupta2022,
  title={Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop},
  author={Gupta, Manas and Modi, Sarthak Ketanbhai and Zhang, Hang and Lee, Joon Hei and Lim, Joo Hwee},
  journal={arXiv preprint arXiv:2212.04614},
  year={2022}
}

% Sparse manifold transform connected to contrastive learning. Experiments with a single layer semi-supervised SMT trained single layer network. 99.3 % on MNIST, 81.1% CIFAR10, 53.2% CIFAR100. 65535 dictionary elements of size 32 and 384.
@article{chen2022,
  title={Minimalistic Unsupervised Learning with the Sparse Manifold Transform},
  author={Chen, Yubei and Yun, Zeyu and Ma, Yi and Olshausen, Bruno and LeCun, Yann},
  journal={arXiv preprint arXiv:2209.15261},
  year={2022}
}

% Stacked Unsupervised Learning (SUL): multiple layers of k-subspaces, hyperparams optimized by meta-learning. 2.1% clustering error on MNIST with 3 layers.
@article{luther2022,
  title={Stacked unsupervised learning with a network architecture found by supervised meta-learning},
  author={Luther, Kyle and Seung, H Sebastian},
  journal={arXiv preprint arXiv:2206.02716},
  year={2022}
}


%%%%%%%%%%
% continual learning
%%%%%%%%%%

@article{pfulb2019,
  title={A comprehensive, application-oriented study of catastrophic forgetting in dnns},
  author={Pf{\"u}lb, Benedikt and Gepperth, Alexander},
  journal={arXiv preprint arXiv:1905.08101},
  year={2019}
}

@article{parisi2019,
  title={Continual lifelong learning with neural networks: A review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={Neural networks},
  volume={113},
  pages={54--71},
  year={2019},
  publisher={Elsevier}
}

@article{hadsell2020,
  title={Embracing change: Continual learning in deep neural networks},
  author={Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A and Pascanu, Razvan},
  journal={Trends in cognitive sciences},
  volume={24},
  number={12},
  pages={1028--1040},
  year={2020},
  publisher={Elsevier}
}

@article{delange2021a,
  title={A continual learning survey: Defying forgetting in classification tasks},
  author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={7},
  pages={3366--3385},
  year={2021},
  publisher={IEEE}
}

@article{french1999,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}

@article{grossberg1987,
  title={Competitive learning: From interactive activation to adaptive resonance},
  author={Grossberg, Stephen},
  journal={Cognitive science},
  volume={11},
  number={1},
  pages={23--63},
  year={1987},
  publisher={Elsevier}
}

@article{robins1995,
  title={Catastrophic forgetting, rehearsal and pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995},
  publisher={Taylor \& Francis}
}

@article{robins1996,
  title={Consolidation in neural networks and in the sleeping brain},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={8},
  number={2},
  pages={259--276},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{french1997,
  title={Pseudo-recurrent connectionist networks: An approach to the'sensitivity-stability'dilemma},
  author={French, Robert M},
  journal={Connection Science},
  volume={9},
  number={4},
  pages={353--380},
  year={1997},
  publisher={Taylor \& Francis}
}

@article{ans1997,
  title={Avoiding catastrophic forgetting by coupling two reverberating neural networks},
  author={Ans, Bernard and Rousset, St{\'e}phane},
  journal={Comptes Rendus de l'Acad{\'e}mie des Sciences-Series III-Sciences de la Vie},
  volume={320},
  number={12},
  pages={989--997},
  year={1997},
  publisher={Elsevier}
}

@article{ans2000,
  title={Neural networks with a self-refreshing memory: knowledge transfer in sequential learning tasks without catastrophic forgetting},
  author={Ans, Bernard and Rousset, St{\'e}phane},
  journal={Connection science},
  volume={12},
  number={1},
  pages={1--19},
  year={2000},
  publisher={Taylor \& Francis}
}

@article{ans2004,
  title={Self-refreshing memory in artificial neural networks: Learning temporal sequences without catastrophic forgetting},
  author={Ans, Bernard and Rousset, St{\'e}phane and French, Robert M and Musca, Serban},
  journal={Connection Science},
  volume={16},
  number={2},
  pages={71--99},
  year={2004},
  publisher={Taylor \& Francis}
}

@article{musca2009,
  title={Artificial neural networks whispering to the brain: nonlinear system attractors induce familiarity with never seen items},
  author={Musca, Serban C and Rousset, St{\'e}phane and Ans, Bernard},
  journal={Connection Science},
  volume={21},
  number={4},
  pages={359--377},
  year={2009},
  publisher={Taylor \& Francis}
}

% Theoretical result about convergence of lifelong learning agents in multi-task settings. The agents are proven to converge to a configuration such that they minimize error bounds for future tasks sampled from the same distribution.
@inproceedings{pentina2014,
  title={A PAC-Bayesian bound for lifelong learning},
  author={Pentina, Anastasia and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={991--999},
  year={2014},
  organization={PMLR}
}

% Extension of pentina2014 theoretical results with non-iid tasks
@article{pentina2015,
  title={Lifelong learning with non-iid tasks},
  author={Pentina, Anastasia and Lampert, Christoph H},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{rusu2016,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@inproceedings{rebuffi2017,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}

@article{kemker2017,
  title={Fearnet: Brain-inspired model for incremental learning},
  author={Kemker, Ronald and Kanan, Christopher},
  journal={arXiv preprint arXiv:1711.10563},
  year={2017}
}

@inproceedings{aljundi2017,
  title={Expert gate: Lifelong learning with a network of experts},
  author={Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3366--3375},
  year={2017}
}

@inproceedings{rannen2017,
  title={Encoder based lifelong learning},
  author={Rannen, Amal and Aljundi, Rahaf and Blaschko, Matthew B and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1320--1328},
  year={2017}
}

@article{shin2017,
  title={Continual learning with deep generative replay},
  author={Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  journal={arXiv preprint arXiv:1705.08690},
  year={2017}
}

@article{nguyen2017,
  title={Variational continual learning},
  author={Nguyen, Cuong V and Li, Yingzhen and Bui, Thang D and Turner, Richard E},
  journal={arXiv preprint arXiv:1710.10628},
  year={2017}
}

@article{li2017,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}

@article{kirkpatrick2017,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@inproceedings{zenke2017,
  title={Continual learning through synaptic intelligence},
  author={Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={3987--3995},
  year={2017},
  organization={PMLR}
}

@article{lee2017a,
  title={Overcoming catastrophic forgetting by incremental moment matching},
  author={Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:1703.08475},
  year={2017}
}

@article{lopez2017,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={6467--6476},
  year={2017}
}

@article{chaudhry2018,
  title={Efficient lifelong learning with a-gem},
  author={Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:1812.00420},
  year={2018}
}

@inproceedings{aljundi2018a,
  title={Memory aware synapses: Learning what (not) to forget},
  author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={139--154},
  year={2018}
}

@article{aljundi2018b,
  title={Selfless sequential learning},
  author={Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
  journal={arXiv preprint arXiv:1806.05421},
  year={2018}
}

@article{ritter2018,
  title={Online structured laplace approximations for overcoming catastrophic forgetting},
  author={Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  journal={arXiv preprint arXiv:1805.07810},
  year={2018}
}

@inproceedings{kim2018a,
  title={Keep and learn: Continual learning by constraining the latent space for knowledge preservation in neural networks},
  author={Kim, Hyo-Eun and Kim, Seungwook and Lee, Jaehwan},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={520--528},
  year={2018},
  organization={Springer}
}

@inproceedings{he2018,
  title={Overcoming catastrophic interference using conceptor-aided backpropagation},
  author={He, Xu and Jaeger, Herbert},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{serra2018,
  title={Overcoming catastrophic forgetting with hard attention to the task},
  author={Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros},
  booktitle={International Conference on Machine Learning},
  pages={4548--4557},
  year={2018},
  organization={PMLR}
}

@article{zeno2018,
  title={Task agnostic continual learning using online variational bayes},
  author={Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.10123},
  year={2018}
}

@inproceedings{isele2018,
  title={Selective experience replay for lifelong learning},
  author={Isele, David and Cosgun, Akansel},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{riemer2018,
  title={Learning to learn without forgetting by maximizing transfer and minimizing interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  journal={arXiv preprint arXiv:1810.11910},
  year={2018}
}

@article{javed2019,
  title={Meta-learning representations for continual learning},
  author={Javed, Khurram and White, Martha},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{rolnick2019,
  title={Experience replay for continual learning},
  author={Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{chaudhry2019,
  title={Continual learning with tiny episodic memories},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, M},
  year={2019}
}

@inproceedings{aljundi2019a,
  title={Task-free continual learning},
  author={Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11254--11263},
  year={2019}
}

@article{aljundi2019b,
  title={Gradient based sample selection for online continual learning},
  author={Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{aljundi2019c,
  title={Online Continual Learning with Maximal Interfered Retrieval},
  author={Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page{-}Caccia, Lucas},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  pages={11849--11860},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/15825aee15eb335cc13f9b559f166ee8-Abstract.html}
}

@inproceedings{li2019,
  title={Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting},
  author={Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},
  booktitle={International Conference on Machine Learning},
  pages={3925--3934},
  year={2019},
  organization={PMLR}
}

@inproceedings{lee2019,
  title={Overcoming catastrophic forgetting with unlabeled data in the wild},
  author={Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={312--321},
  year={2019}
}

@inproceedings{paik2020,
  title={Overcoming catastrophic forgetting by neuron-level plasticity control},
  author={Paik, Inyoung and Oh, Sangjun and Kwak, Taeyeong and Kim, Injung},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5339--5346},
  year={2020}
}

@inproceedings{ebrahimi2020,
  title={Adversarial continual learning},
  author={Ebrahimi, Sayna and Meier, Franziska and Calandra, Roberto and Darrell, Trevor and Rohrbach, Marcus},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16},
  pages={386--402},
  year={2020},
  organization={Springer}
}

@inproceedings{delange2021b,
  title={Continual prototype evolution: Learning online from non-stationary data streams},
  author={De Lange, Matthias and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8250--8259},
  year={2021}
}

@inproceedings{shim2021,
  title={Online class-incremental continual learning with adversarial shapley value},
  author={Shim, Dongsub and Mai, Zheda and Jeong, Jihwan and Sanner, Scott and Kim, Hyunwoo and Jang, Jongseong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9630--9638},
  year={2021}
}

@inproceedings{mai2021,
  title={Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning},
  author={Mai, Zheda and Li, Ruiwen and Kim, Hyunwoo and Sanner, Scott},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3589--3599},
  year={2021}
}

@inproceedings{cha2021,
  title={Co2l: Contrastive continual learning},
  author={Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
  booktitle={Proceedings of the IEEE/CVF International conference on computer vision},
  pages={9516--9525},
  year={2021}
}

@article{kumari2022,
  title={Retrospective adversarial replay for continual learning},
  author={Kumari, Lilly and Wang, Shengjie and Zhou, Tianyi and Bilmes, Jeff A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28530--28544},
  year={2022}
}


%%%%%%%%%%
% learning with dependent data
%%%%%%%%%%

@article{steinwart2009,
  title={Learning from dependent observations},
  author={Steinwart, Ingo and Hush, Don and Scovel, Clint},
  journal={Journal of Multivariate Analysis},
  volume={100},
  number={1},
  pages={175--194},
  year={2009},
  publisher={Elsevier}
}

@article{agarwal2012,
  title={The generalization ability of online algorithms for dependent data},
  author={Agarwal, Alekh and Duchi, John C},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={1},
  pages={573--587},
  year={2012},
  publisher={IEEE}
}

@inproceedings{kuznetsov2016,
  title={Time series prediction and online learning},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle={Conference on Learning Theory},
  pages={1190--1213},
  year={2016},
  organization={PMLR}
}

@inproceedings{zimin2017,
  title={Learning theory for conditional risk minimization},
  author={Zimin, Alexander and Lampert, Christoph},
  booktitle={Artificial Intelligence and Statistics},
  pages={213--222},
  year={2017},
  organization={PMLR}
}

@article{zimin2018,
  title={MACRO: A Meta-Algorithm for Conditional Risk Minimization},
  author={Zimin, Alexander and Lampert, Christoph},
  journal={arXiv preprint arXiv:1801.00507},
  year={2018}
}


%%%%%%%%%%
% backprop bio support and surveys
%%%%%%%%%%

% deep learning and neuroscience
@article{marblestone2016,
  title={Toward an integration of deep learning and neuroscience},
  author={Marblestone, Adam H and Wayne, Greg and Kording, Konrad P},
  journal={Frontiers in computational neuroscience},
  volume={10},
  pages={94},
  year={2016},
  publisher={Frontiers}
}

% deep learning and neuroscience
@article{hassabis2017,
  title={Neuroscience-inspired artificial intelligence},
  author={Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  journal={Neuron},
  volume={95},
  number={2},
  pages={245--258},
  year={2017},
  publisher={Elsevier}
}

% deep learning future directions, also in relation to neuroscience
@article{lake2017,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  year={2017},
  publisher={Cambridge University Press}
}

% bio plausible backprop and neuroscience
@article{richards2019, 
  title={A deep learning framework for neuroscience},
  author={Richards, Blake A and Lillicrap, Timothy P and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and Ganguli, Surya and others},
  journal={Nature neuroscience},
  volume={22},
  number={11},
  pages={1761--1770},
  year={2019},
  publisher={Nature Publishing Group}
}

% review bio plausible learning methods
@article{bengio2015b,
  title={Towards biologically plausible deep learning},
  author={Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  journal={arXiv preprint arXiv:1502.04156},
  year={2015}
}

% rfa
@article{lillicrap2014,
    title={Random feedback weights support learning in deep neural networks},
    author={Lillicrap, Timothy P and Cownden, Daniel and Tweed, Douglas B and Akerman, Colin J},
    journal={arXiv preprint arXiv:1411.0247},
    year={2014}
}

% rfa with backward connections with same sign as feedforward approximates backprop
@article{liao2015,
  title={How important is weight symmetry in backpropagation?},
  author={Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
  journal={arXiv preprint arXiv:1510.05067},
  year={2015}
}

% targetprop
@inproceedings{lee2015a,
  title={Difference target propagation},
  author={Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
  booktitle={Joint european conference on machine learning and knowledge discovery in databases},
  pages={498--515},
  year={2015},
  organization={Springer}
}

% dfa
@inproceedings{nokland2016,
  title={Direct feedback alignment provides learning in deep neural networks},
  author={N{\o}kland, Arild},
  booktitle={Advances in neural information processing systems},
  pages={1037--1045},
  year={2016}
}

% compartment neuron models models could enable backprop in real neurons as predictive coding
@article{urbanczik2014,
  title={Learning by the dendritic prediction of somatic spiking},
  author={Urbanczik, Robert and Senn, Walter},
  journal={Neuron},
  volume={81},
  number={3},
  pages={521--528},
  year={2014},
  publisher={Elsevier}
}

% bap
@article{schiess2016,
  title={Somato-dendritic synaptic plasticity and error-backpropagation in active dendrites},
  author={Schiess, Mathieu and Urbanczik, Robert and Senn, Walter},
  journal={PLoS computational biology},
  volume={12},
  number={2},
  pages={e1004638},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

% segregated dendrites/compartment models could enable backprop in real neurons as predictive coding
@article{guerguiev2017,
  title={Towards deep learning with segregated dendrites},
  author={Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  journal={Elife},
  volume={6},
  pages={e22901},
  year={2017},
  publisher={eLife Sciences Publications Limited}
}

% segregated dendrites/compartment models could enable backprop in real neurons as predictive coding
@article{sacramento2018,
  title={Dendritic cortical microcircuits approximate the backpropagation algorithm},
  author={Sacramento, Jo{\~a}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
  journal={arXiv preprint arXiv:1810.11393},
  year={2018}
}

% targetprop in which backpropagated target is computed by iterative gradient-based inverison rather than by explicitly trained inversion layer
@article{farias2018,
  title={Gradient target propagation.},
  author={de Souza Farias, Tiago and Maziero, Jonas},
  journal={arXiv preprint arXiv:1810.09284},
  year={2018}
}

% rfa and targetprop do not scale to imagenet
@inproceedings{bartunov2018,
  title={Assessing the scalability of biologically-motivated deep learning algorithms and architectures},
  author={Bartunov, Sergey and Santoro, Adam and Richards, Blake and Marris, Luke and Hinton, Geoffrey E and Lillicrap, Timothy},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9368--9378},
  year={2018}
}

% rfa with backward connections with same sign as feedforward can scale to imagenet
@article{xiao2018,
  title={Biologically-plausible learning algorithms can scale to large datasets},
  author={Xiao, Will and Chen, Honglin and Liao, Qianli and Poggio, Tomaso},
  journal={arXiv preprint arXiv:1811.03567},
  year={2018}
}

% rfa improvement with symmetric update for backward weights
@article{amit2019, 
  title={Deep learning with asymmetric connections and Hebbian updates},
  author={Amit, Yali},
  journal={Frontiers in computational neuroscience},
  volume={13},
  pages={18},
  year={2019},
  publisher={Frontiers}
}

% gaitprop: targetprop with small perturbation of output is equivalent to backprop
@article{ahmad2020gait,
  title={GAIT-prop: A biologically plausible learning rule derived from backpropagation of error},
  author={Ahmad, Nasir and van Gerven, Marcel AJ and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2006.06438},
  year={2020}
}


%%%%%%%%%%
% contrastive hebbian learning
%%%%%%%%%%

% top-down feedback for context disambiguation through interactive activation and competition
@article{mcclelland1981,
  title={An interactive activation model of context effects in letter perception: I. An account of basic findings.},
  author={McClelland, James L and Rumelhart, David E},
  journal={Psychological review},
  volume={88},
  number={5},
  pages={375},
  year={1981},
  publisher={American Psychological Association}
}

% CHL: delta w = y+x+ - y-x-
@incollection{movellan1991, 
  title={Contrastive Hebbian learning in the continuous Hopfield model},
  author={Movellan, Javier R},
  booktitle={Connectionist models},
  pages={10--17},
  year={1991},
  publisher={Elsevier}
}

% Develops considerations on CHL. Action generalization for learning time responses.
@article{baldi1991, 
  title={Contrastive learning and neural oscillations},
  author={Baldi, Pierre and Pineda, Fernando},
  journal={Neural Computation},
  volume={3},
  number={4},
  pages={526--545},
  year={1991},
  publisher={MITP}
}

% Recirculation algorithm for autoencoders
@inproceedings{hinton1988, 
  title={Learning representations by recirculation},
  author={Hinton, Geoffrey E and McClelland, James L},
  booktitle={Neural information processing systems},
  pages={358--366},
  year={1988}
}

% Generalized recirculation equivalent to Almeida Pineda (temporal) backprop reduces to CHL
@article{oreilly1996, 
  title={Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm},
  author={O'Reilly, Randall C},
  journal={Neural computation},
  volume={8},
  number={5},
  pages={895--938},
  year={1996},
  publisher={MIT Press}
}

% Principles for biological plausibility and LEABRA
@article{oreilly1998,
  title={Six principles for biologically based computational models of cortical cognition},
  author={O'Reilly, Randall C},
  journal={Trends in cognitive sciences},
  volume={2},
  number={11},
  pages={455--462},
  year={1998},
  publisher={Elsevier}
}

% LEABRA: GenRec + unsupervised update to improve generalization
@article{oreilly2001, 
  title={Generalization in interactive networks: The benefits of inhibitory competition and Hebbian learning},
  author={O'reilly, Randall C},
  journal={Neural computation},
  volume={13},
  number={6},
  pages={1199--1241},
  year={2001},
  publisher={MIT Press}
}

% RFA version of CHL
@article{detorakis2019, 
  title={Contrastive hebbian learning with random feedback weights},
  author={Detorakis, Georgios and Bartley, Travis and Neftci, Emre},
  journal={Neural Networks},
  volume={114},
  pages={1--14},
  year={2019},
  publisher={Elsevier}
}

% CHL with weak feedback
@article{xie2003, 
  title={Equivalence of backpropagation and contrastive Hebbian learning in a layered network},
  author={Xie, Xiaohui and Seung, H Sebastian},
  journal={Neural computation},
  volume={15},
  number={2},
  pages={441--454},
  year={2003},
  publisher={MIT Press}
}

% CHL with weak clamping
@article{scellier2017, 
  title={Equilibrium propagation: Bridging the gap between energy-based models and backpropagation},
  author={Scellier, Benjamin and Bengio, Yoshua},
  journal={Frontiers in computational neuroscience},
  volume={11},
  pages={24},
  year={2017},
  publisher={Frontiers}
}

% spiking contrastive learning, STDP + anti-STDP updates, probabilistic spike time (exponential with rate=membrane potential)
@article{neftci2014, 
  title={Event-driven contrastive divergence for spiking neuromorphic systems},
  author={Neftci, Emre and Das, Srinjoy and Pedroni, Bruno and Kreutz-Delgado, Kenneth and Cauwenberghs, Gert},
  journal={Frontiers in neuroscience},
  volume={7},
  pages={272},
  year={2014},
  publisher={Frontiers}
}

% spiking equilibrium propagation
@inproceedings{oconnor2019,
  title={Training a spiking neural network with equilibrium propagation},
  author={OConnor, Peter and Gavves, Efstratios and Welling, Max},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1516--1523},
  year={2019}
}


%%%%%%%%%%
% backprop with auxiliary variables
%%%%%%%%%%

% method of auxiliary coordinates (mac) - gradient descent optimization method analogue to predictive coding, uses alternating optimization
@inproceedings{carreira2014,
  title={Distributed optimization of deeply nested systems},
  author={Carreira-Perpinan, Miguel and Wang, Weiran},
  booktitle={Artificial Intelligence and Statistics},
  pages={10--19},
  year={2014}
}

% auxiliary variables with admm and bregman iteration
@inproceedings{taylor2016,
  title={Training neural networks without gradients: A scalable admm approach},
  author={Taylor, Gavin and Burmeister, Ryan and Xu, Zheng and Singh, Bharat and Patel, Ankit and Goldstein, Tom},
  booktitle={International conference on machine learning},
  pages={2722--2731},
  year={2016}
}

% auxiliary variables and block coord descent on blocks of neural net. Inside each block ordinary backprop is used.
@article{gotmare2018,
  title={Decoupling backpropagation using constrained optimization methods},
  author={Gotmare, Akhilesh and Thomas, Valentin and Brea, Johanni and Jaggi, Martin},
  year={2018}
}

% lifted neural networks: auxiliary variables + block coord descent
@article{askari2018,
  title={Lifted neural networks},
  author={Askari, Armin and Negiar, Geoffrey and Sambharya, Rajiv and Ghaoui, Laurent El},
  journal={arXiv preprint arXiv:1805.01532},
  year={2018}
}

% auxiliary variables + online alternating optimization
@inproceedings{choromanska2019,
  title={Beyond backprop: Online alternating minimization with auxiliary variables},
  author={Choromanska, Anna and Cowen, Benjamin and Kumaravel, Sadhana and Luss, Ronny and Rigotti, Mattia and Rish, Irina and Diachille, Paolo and Gurev, Viatcheslav and Kingsbury, Brian and Tejwani, Ravi and others},
  booktitle={International Conference on Machine Learning},
  pages={1193--1202},
  year={2019},
  organization={PMLR}
}

% lifted networks exploiting also fenchel duality of convex optimization problem to derive a lagrange multiplier optimizatio that provably upper bounds the original optimization problem
@inproceedings{gu2020,
  title={Fenchel lifted networks: A lagrange relaxation of neural network training},
  author={Gu, Fangda and Askari, Armin and El Ghaoui, Laurent},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3362--3371},
  year={2020},
  organization={PMLR}
}


%%%%%%%%%%
% predictive coding
%%%%%%%%%%

% inhibitory feedback implements presynaptic competition - precursor of hierarchical predictive coding
@article{harpur1996,
  title={Development of low entropy coding in a recurrent network},
  author={Harpur, George F and Prager, Richard W},
  journal={Network: computation in neural systems},
  volume={7},
  number={2},
  pages={277--284},
  year={1996},
  publisher={Taylor \& Francis}
}

% hierarchical predictive coding
@article{rao1999,
  title={Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
  author={Rao, Rajesh PN and Ballard, Dana H},
  journal={Nature neuroscience},
  volume={2},
  number={1},
  pages={79--87},
  year={1999},
  publisher={Nature Publishing Group}
}

% hierarchical predictive coding by free energy principle
@article{friston2005,
  title={A theory of cortical responses},
  author={Friston, Karl},
  journal={Philosophical transactions of the Royal Society B: Biological sciences},
  volume={360},
  number={1456},
  pages={815--836},
  year={2005},
  publisher={The Royal Society London}
}

% modulatory feedback connections
@article{grant2017,
  title={Biologically plausible learning in neural networks with modulatory feedback},
  author={Grant, W Shane and Tanner, James and Itti, Laurent},
  journal={Neural Networks},
  volume={88},
  pages={32--48},
  year={2017},
  publisher={Elsevier}
}

% hierarchical predictive coding
@article{wen2018,
  title={Deep predictive coding network for object recognition},
  author={Wen, Haiguang and Han, Kuan and Shi, Junxing and Zhang, Yizhen and Culurciello, Eugenio and Liu, Zhongming},
  journal={arXiv preprint arXiv:1802.04762},
  year={2018}
}

% hierarchical predictive coding
@inproceedings{han2018,
  title={Deep predictive coding network with local recurrent processing for object recognition},
  author={Han, Kuan and Wen, Haiguang and Zhang, Yizhen and Fu, Di and Culurciello, Eugenio and Liu, Zhongming},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9201--9213},
  year={2018}
}

% hierarchical sparse predictive coding
@article{boutin2019,
  title={Meaningful representations emerge from sparse deep predictive coding},
  author={Boutin, Victor and Franciosini, Angelo and Ruffier, Franck and Perrinet, Laurent},
  journal={arXiv preprint arXiv:1902.07651},
  year={2019}
}

% hierarchical predictive coding approximates backprop
@article{whittington2017,
  title={An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity},
  author={Whittington, James CR and Bogacz, Rafal},
  journal={Neural computation},
  volume={29},
  number={5},
  pages={1229--1262},
  year={2017},
  publisher={MIT Press}
}

% hierarchical predictive coding on arbitrary computational graphs approximates backprop
@article{millidge2020,
  title={Predictive Coding Approximates Backprop along Arbitrary Computation Graphs},
  author={Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L},
  journal={arXiv preprint arXiv:2006.04182},
  year={2020}
}

% unifying view of predictive coding, equilibrium propagation and contrastive hebbian learning
@article{millidge2022,
  title={Backpropagation at the Infinitesimal Inference Limit of Energy-Based Models: Unifying Predictive Coding, Equilibrium Propagation, and Contrastive Hebbian Learning},
  author={Millidge, Beren and Song, Yuhang and Salvatori, Tommaso and Lukasiewicz, Thomas and Bogacz, Rafal},
  journal={arXiv preprint arXiv:2206.02629},
  year={2022}
}


%%%%%%%%%%
% contrastive learning (i.e. context learning)
%%%%%%%%%%

% spatio-temporal predictive coding (i.e. contrastive learning) by ib
@inproceedings{bialek2006,
  title={Efficient representation as a design principle for neural coding and computation},
  author={Bialek, William and Van Steveninck, Rob R De Ruyter and Tishby, Naftali},
  booktitle={2006 IEEE international symposium on information theory},
  pages={659--663},
  year={2006},
  organization={IEEE}
}

% temporal predictive coding (i.e. contrastive learning) for video prediction
@article{lotter2016,
  title={Deep predictive coding networks for video prediction and unsupervised learning},
  author={Lotter, William and Kreiman, Gabriel and Cox, David},
  journal={arXiv preprint arXiv:1605.08104},
  year={2016}
}

% spatio-temporal/contrastive predictive coding (i.e. contrastive learning)
@article{oord2018,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

% spatio-temporal/contrastive predictive coding (i.e. contrastive learning) - a theoretical analysis and similarity with word2vec word embedding
@inproceedings{arora2019,
  title={A theoretical analysis of contrastive unsupervised representation learning},
  author={Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
  booktitle={International Conference on Machine Learning},
  pages={5628--5637},
  year={2019},
  organization={PMLR}
}

% spatio-temporal/contrastive predictive coding (i.e. contrastive learning)
@article{henaff2019,
  title={Data-efficient image recognition with contrastive predictive coding},
  author={H{\'e}naff, Olivier J and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, SM and Oord, Aaron van den},
  journal={arXiv preprint arXiv:1905.09272},
  year={2019}
}

% deep learning with spatio-temporal/contrastive predictive coding (i.e. contrastive learning)
@inproceedings{lowe2019,
  title={Putting an end to end-to-end: Gradient-isolated learning of representations},
  author={L{\"o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3039--3051},
  year={2019}
}

% SimCLR: contrastive predictive coding on augmented data --> given an image, produce other two images by data agumentation, and impose the corresponding hiden representations to concide. 
@inproceedings{chen2020a,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

% VICReg: regularizer for contrastive learning that imposes penalty for low-variance (in order to avoid embedding collapse) and decorrelation regularizer on embedding covariance matrix
@inproceedings{bardes2021,
  title={VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  author={Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

%%%%%%%%%%
% global reward modulation
%%%%%%%%%%

% A survey on reward-modulated Hebbian learning
@article{triche2022,
  title={Exploration in neo-Hebbian reinforcement learning: Computational approaches to the exploration-exploitation balance with bio-inspired neural networks},
  author={Triche, Anthony and Maida, Anthony S and Kumar, Ashok},
  journal={Neural Networks},
  year={2022},
  publisher={Elsevier}
}

% REINFORCE: delta w = (r - b) d ln(g)/dw --> equivalent to gradient ascent on r
@article{williams1992, 
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

% Reward modulated learning in SNNs with edonistic synapses
@article{seung2003,
  title={Learning in spiking neural networks by reinforcement of stochastic synaptic transmission},
  author={Seung, H Sebastian},
  journal={Neuron},
  volume={40},
  number={6},
  pages={1063--1073},
  year={2003},
  publisher={Elsevier}
}

% Reward-modulated learning in SNNs --> reward-modulated STDP-like rule
@inproceedings{florian2005, 
  title={A reinforcement learning algorithm for spiking neural networks},
  author={Florian, Razvan V},
  booktitle={Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)},
  pages={8--pp},
  year={2005},
  organization={IEEE}
}

% AGREL: backward attention-gating feedback
@article{roelfsema2005, 
  title={Attention-gated reinforcement learning of internal representations for classification},
  author={Roelfsema, Pieter R and Ooyen, Arjen van},
  journal={Neural computation},
  volume={17},
  number={10},
  pages={2176--2214},
  year={2005},
  publisher={MIT Press}
}

% kickback
@inproceedings{balduzzi2015,
  title={Kickback cuts backprop's red-tape: Biologically plausible credit assignment in neural networks},
  author={Balduzzi, David and Vanchinathan, Hastagiri and Buhmann, Joachim},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  year={2015}
}

% Q-AGREL: MNIST 99.17% accuracy, CIFAR-10 73.54% accuracy, CIFAR-100 34.93% accuracy
@article{pozzi2018, 
  title={A biologically plausible learning rule for deep learning in the brain},
  author={Pozzi, Isabella and Boht{\'e}, Sander and Roelfsema, Pieter},
  journal={arXiv preprint arXiv:1811.01768},
  year={2018}
}

% Experimental support for neo hebbian three factor learning rules 
@article{gerstner2018,
  title={Eligibility traces and plasticity on behavioral time scales: experimental support of neohebbian three-factor learning rules},
  author={Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
  journal={Frontiers in neural circuits},
  volume={12},
  pages={53},
  year={2018},
  publisher={Frontiers Media SA}
}

% Reward modulated STDP for computer vision: ETH-80 89.5% accuracy, NORB: 88.4% accuracy
@article{mozafari2018, 
  title={First-spike-based visual categorization using reward-modulated STDP},
  author={Mozafari, Milad and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Nowzari-Dalini, Abbas and Ganjtabesh, Mohammad},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={12},
  pages={6178--6190},
  year={2018},
  publisher={IEEE}
}


%%%%%%%%%%
% layerwise training
%%%%%%%%%%

% layerwise training, also stacked autoencoders
@inproceedings{bengio2007,
  title={Greedy layer-wise training of deep networks},
  author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle={Advances in neural information processing systems},
  pages={153--160},
  year={2007}
}

% layerwise training
@article{wang2015b,
  title={Training deeper convolutional networks with deep supervision},
  author={Wang, Liwei and Lee, Chen-Yu and Tu, Zhuowen and Lazebnik, Svetlana},
  journal={arXiv preprint arXiv:1505.02496},
  year={2015}
}

% layerwise training
@inproceedings{lee2015b,
  title={Deeply-supervised nets},
  author={Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  booktitle={Artificial intelligence and statistics},
  pages={562--570},
  year={2015}
}

% layerwise training
@article{wang2017,
  title={Idk cascades: Fast deep learning by learning not to overthink},
  author={Wang, Xin and Luo, Yujia and Crankshaw, Daniel and Tumanov, Alexey and Yu, Fisher and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:1706.00885},
  year={2017}
}

% layerwise training
@article{marquez2018,
  title={Deep cascade learning},
  author={Marquez, Enrique S and Hare, Jonathon S and Niranjan, Mahesan},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={11},
  pages={5475--5485},
  year={2018},
  publisher={IEEE}
}

% layerwise training
@inproceedings{kaya2019,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International Conference on Machine Learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}

% layerwise training
@article{belilovsky2019a,
  title={Decoupled greedy learning of cnns},
  author={Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  journal={arXiv preprint arXiv:1901.08164},
  year={2019}
}


% layerwise training
@inproceedings{belilovsky2019b,
  title={Greedy layerwise learning can scale to imagenet},
  author={Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  booktitle={International conference on machine learning},
  pages={583--593},
  year={2019},
  organization={PMLR}
}

% layerwise training survey
@article{scardapane2020,
  title={Why should we add early exits to neural networks?},
  author={Scardapane, Simone and Scarpiniti, Michele and Baccarelli, Enzo and Uncini, Aurelio},
  journal={arXiv preprint arXiv:2004.12814},
  year={2020}
}

% layerwise supervised kernel similarity matching
@article{kulkarni2017,
  title={Layer-wise training of deep networks using kernel similarity},
  author={Kulkarni, Mandar and Karande, Shirish},
  journal={arXiv preprint arXiv:1703.07115},
  year={2017}
}

% layerwise supervised similarity matching
@article{nokland2019,
  title={Training neural networks with local error signals},
  author={N{\o}kland, Arild and Eidnes, Lars Hiller},
  journal={arXiv preprint arXiv:1901.06656},
  year={2019}
}

% layerwise supervised kernel max margin
@article{duan2021,
  title={Modularizing deep learning via pairwise learning with kernels},
  author={Duan, Shiyu and Yu, Shujian and Pr{\'\i}ncipe, Jos{\'e} C},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={33},
  number={4},
  pages={1441--1451},
  year={2021},
  publisher={IEEE}
}


%%%%%%%%%%
% spiking
%%%%%%%%%%

% introduction to snns and quick guide to implementation for computer vision
@article{vaila2019,
  title={Deep convolutional spiking neural networks for image classification},
  author={Vaila, Ruthvik and Chiasson, John and Saxena, Vishal},
  journal={arXiv preprint arXiv:1903.12272},
  year={2019}
}

% seep learning for snn survey
@article{pfeiffer2018,
  title={Deep learning with spiking neurons: Opportunities and challenges},
  author={Pfeiffer, Michael and Pfeil, Thomas},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={774},
  year={2018},
  publisher={Frontiers Media SA}
}

% snn survey
@article{tavanaei2019a,
  title={Deep learning in spiking neural networks},
  author={Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  journal={Neural Networks},
  volume={111},
  pages={47--63},
  year={2019},
  publisher={Elsevier}
}

% survey of learning approaches in snns
@article{taherkhani2020,
  title={A review of learning in biologically plausible spiking neural networks},
  author={Taherkhani, Aboozar and Belatreche, Ammar and Li, Yuhua and Cosma, Georgina and Maguire, Liam P and McGinnity, T Martin},
  journal={Neural Networks},
  volume={122},
  pages={253--272},
  year={2020},
  publisher={Elsevier}
}

% snn survey
@article{nunes2022,
  title={Spiking neural networks: A survey},
  author={Nunes, Joao D and Carvalho, Marcelo and Carneiro, Diogo and Cardoso, Jaime S},
  journal={IEEE Access},
  volume={10},
  pages={60738--60764},
  year={2022},
  publisher={IEEE}
}

% survey on snn learning approaches
@article{yi2023,
  title={Learning rules in spiking neural networks: A survey},
  author={Yi, Zexiang and Lian, Jing and Liu, Qidong and Zhu, Hegui and Liang, Dong and Liu, Jizhao},
  journal={Neurocomputing},
  volume={531},
  pages={163--179},
  year={2023},
  publisher={Elsevier}
}

% Spiking neurons computationally more powerful than non spiking ones
@article{maass1997, 
  title={Networks of spiking neurons: the third generation of neural network models},
  author={Maass, Wolfgang},
  journal={Neural networks},
  volume={10},
  number={9},
  pages={1659--1671},
  year={1997},
  publisher={Elsevier}
}

 % STDP: potentiation = e^-(t_out - t_in)
@article{song2000,
    title={Competitive Hebbian learning through spike-timing-dependent synaptic plasticity},
    author={Song, Sen and Miller, Kenneth D and Abbott, Larry F},
    journal={Nature neuroscience},
    volume={3},
    number={9},
    pages={919},
    year={2000},
    publisher={Nature Publishing Group}
}

% Mathematical formulation of STDP and Hebbian learning - article version of Gerstner chapter
@article{gerstner2002,
  title={Mathematical formulations of Hebbian learning},
  author={Gerstner, Wulfram and Kistler, Werner M},
  journal={Biological cybernetics},
  volume={87},
  number={5-6},
  pages={404--415},
  year={2002},
  publisher={Springer}
}

% Triplets of spikes mechanistic model. Contribution to plasticity not only by time between pairs of spikes, but also by rate of change of activity determined by triplets of spikes.
@article{pfister2006,
  title={Triplets of spikes in a model of spike timing-dependent plasticity},
  author={Pfister, Jean-Pascal and Gerstner, Wulfram},
  journal={Journal of Neuroscience},
  volume={26},
  number={38},
  pages={9673--9682},
  year={2006},
  publisher={Soc Neuroscience}
}

% 1. Spike Driven Synaptic Plasticity: potentiation = input spike * membrane potential
% 2. Bistable STDP rule with threshold to solve stability-plasticity dilemma
@article{fusi2000,
  title={Spike-driven synaptic plasticity: theory, simulation, VLSI implementation},
  author={Fusi, Stefano and Annunziato, Mario and Badoni, Davide and Salamon, Andrea and Amit, Daniel J},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2227--2258},
  year={2000},
  publisher={MITP}
}

% Extension of stdp model including additional contributions determined by pre and post firing times, but also membrane potentials (voltage driven), current weights (homeostatic plasticity/synaptic scaling) or temporal averages of these quantities (traces).
@article{morrison2008,
  title={Phenomenological models of synaptic plasticity based on spike timing},
  author={Morrison, Abigail and Diesmann, Markus and Gerstner, Wulfram},
  journal={Biological cybernetics},
  volume={98},
  pages={459--478},
  year={2008},
  publisher={Springer}
}

% multiple contributions to plasticity in addition to stdp: membrane voltage and rate of change of membrane voltage
@article{clopath2010,
  title={Voltage and spike timing interact in STDP--a unified model},
  author={Clopath, Claudia and Gerstner, Wulfram},
  journal={Frontiers in synaptic neuroscience},
  volume={2},
  pages={25},
  year={2010},
  publisher={Frontiers Research Foundation}
}

% voltage driven synaptic plasticity: stdp variant using voltage of pre-synaptic neuron instead of presynaptic spike time to modulate the update, thus avoiding storing additional information.
@article{garg2022,
  title={Voltage-dependent synaptic plasticity: Unsupervised probabilistic Hebbian plasticity rule based on neurons membrane potential},
  author={Garg, Nikhil and Balafrej, Ismael and Stewart, Terrence C and Portal, Jean-Michel and Bocquet, Marc and Querlioz, Damien and Drouin, Dominique and Rouat, Jean and Beilliard, Yann and Alibart, Fabien},
  journal={Frontiers in Neuroscience},
  volume={16},
  pages={983950},
  year={2022},
  publisher={Frontiers}
}

% rank order coding
@incollection{thorpe1998, 
  title={Rank order coding},
  author={Thorpe, Simon and Gautrais, Jacques},
  booktitle={Computational neuroscience},
  pages={113--118},
  year={1998},
  publisher={Springer}
}

% learning rule for rank order coding: potentiation = a * mod^order(input spike)
@article{delorme2001, 
  title={Face identification using one spike per neuron: resistance to image degradations},
  author={Delorme, Arnaud and Thorpe, Simon J},
  journal={Neural Networks},
  volume={14},
  number={6-7},
  pages={795--803},
  year={2001},
  publisher={Elsevier}
}

% efficiency of rank order coding
@article{thorpe2004, 
    title={SpikeNet: Real-time visual processing with one spike per neuron},
    author={Thorpe, Simon J and Guyonneau, Rudy and Guilbaud, Nicolas and Allegraud, Jong-Mo and VanRullen, Rufin},
    journal={Neurocomputing},
    volume={58},
    pages={857--864},
    year={2004},
    publisher={Elsevier}
}

% stdp rule: potentiation = w * (1-w)
@article{masquelier2007, 
  title={Unsupervised learning of visual features through spike timing dependent plasticity},
  author={Masquelier, Timoth{\'e}e and Thorpe, Simon J},
  journal={PLoS computational biology},
  volume={3},
  number={2},
  year={2007},
  publisher={Public Library of Science}
}

% stdp does rank coding
@article{masquelier2008, 
  title={Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains},
  author={Masquelier, Timoth{\'e}e and Guyonneau, Rudy and Thorpe, Simon J},
  journal={PloS one},
  volume={3},
  number={1},
  year={2008},
  publisher={Public Library of Science}
}

% stdp rule: potentiation = e^-w - 1
@inproceedings{nessler2009,
  title={STDP enables spiking neurons to detect hidden causes of their inputs},
  author={Nessler, Bernhard and Pfeiffer, Michael and Maass, Wolfgang},
  booktitle={Advances in neural information processing systems},
  pages={1357--1365},
  year={2009}
}

% SWTA: combination of stdp with bcm
@article{wade2010, 
  title={SWAT: a spiking neural network training algorithm for classification problems},
  author={Wade, John J and McDaid, Liam J and Santos, Jose A and Sayers, Heather M},
  journal={IEEE Transactions on Neural Networks},
  volume={21},
  number={11},
  pages={1817--1830},
  year={2010},
  publisher={IEEE}
}

% stdp rule: potentiation = e^-w
@inproceedings{tavanaei2016, 
  title={Acquisition of visual features through probabilistic spike-timing-dependent plasticity},
  author={Tavanaei, Amirhossein and Masquelier, Timoth{\'e}e and Maida, Anthony S},
  booktitle={2016 International Joint Conference on Neural Networks (IJCNN)},
  pages={307--314},
  year={2016},
  organization={IEEE}
}

% Frequency-dependent synaptic plasticity (FDSP). Synapse are considered as band-pass filters, filtering inputs at a given frequency. In this way, an RBF-like neuron model can be obtained.
@article{khan2020,
  title={Frequency-dependent synaptic plasticity model for neurocomputing applications},
  author={Khan, Saad Qasim and Ghani, Arfan and Khurram, Muhammad},
  journal={International Journal of Bio-Inspired Computation},
  volume={16},
  number={1},
  pages={56--66},
  year={2020},
  publisher={Inderscience Publishers (IEL)}
}


%%%%%%%%%%
% spike coding
%%%%%%%%%%

% Survey on spike encoding techniques
@article{auge2021,
  title={A survey of encoding techniques for signal processing in spiking neural networks},
  author={Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
  journal={Neural Processing Letters},
  volume={53},
  number={6},
  pages={4693--4710},
  year={2021},
  publisher={Springer}
}

% binned spike train similarity metric: measures spike train similarity as correlation between binned versions/histograms of spike trains (equivalent to van rossum distance with rectangular kernels)
@article{palm1988,
  title={On the significance of correlations among neuronal spike trains},
  author={Palm, G and Aertsen, AMHJ and Gerstein, GL},
  journal={Biological cybernetics},
  volume={59},
  number={1},
  pages={1--11},
  year={1988},
  publisher={Springer}
}

% Victor-Purpura (VP) metric measures distance between spike trains as minimum cost for transforming a spike sequence into another by moving, adding or deleting spikes.
@article{victor1997,
  title={Metric-space analysis of spike trains: theory, algorithms and application},
  author={Victor, Jonathan D and Purpura, Keith P},
  journal={Network: computation in neural systems},
  volume={8},
  number={2},
  pages={127--164},
  year={1997},
  publisher={Taylor \& Francis}
}

% spike train distance measure in the fourier domain by comparison of the spectra of spike signals
@article{jarvis2001,
  title={Sampling properties of the spectrum and coherency of sequences of action potentials},
  author={Jarvis, MR and Mitra, Partha P},
  journal={Neural computation},
  volume={13},
  number={4},
  pages={717--749},
  year={2001},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

% van rossum metric: euclidean distance between filtered spike trains (using exponential kernels)
@article{vanrossum2001,
  title={A novel spike distance},
  author={van Rossum, Mark CW},
  journal={Neural computation},
  volume={13},
  number={4},
  pages={751--763},
  year={2001},
  publisher={MIT Press}
}

% event synchronization metric: given two spike trains to compare, two spikes (one from each train) are coincident if they are closer than a maximum time tau --> count number of coincidences and normalize by total number of spikes in trains
@article{quiroga2002,
  title={Event synchronization: a simple and fast method to measure synchronicity and time delay patterns},
  author={Quiroga, R Quian and Kreuz, Thomas and Grassberger, Peter},
  journal={Physical review E},
  volume={66},
  number={4},
  pages={041904},
  year={2002},
  publisher={APS}
}

% schreiber metric: correlation between filtered spike trains (using gaussian kernels) - note that an important difference from van rossum distance is that gaussian kernels are non causal, while exponentialk kernels are.
@article{schreiber2003,
  title={A new correlation-based measure of spike timing reliability},
  author={Schreiber, Susanne and Fellous, Jean-Marc and Whitmer, D and Tiesinga, Paul and Sejnowski, Terrence J},
  journal={Neurocomputing},
  volume={52},
  pages={925--931},
  year={2003},
  publisher={Elsevier}
}

% isi (inter spike interval) spike distance measure
@article{kreuz2007,
  title={Measuring spike train synchrony},
  author={Kreuz, Thomas and Haas, Julie S and Morelli, Alice and Abarbanel, Henry DI and Politi, Antonio},
  journal={Journal of neuroscience methods},
  volume={165},
  number={1},
  pages={151--161},
  year={2007},
  publisher={Elsevier}
}

% spike motif identification: neural response typically manifests as a burst of spikes. a spike clustering algorithm is employed to find bursts and then repeating patterns of bursts are observed.
@article{raichman2008,
  title={Identifying repeating motifs in the activation of synchronized bursts in cultured neuronal networks},
  author={Raichman, Nadav and Ben-Jacob, Eshel},
  journal={Journal of neuroscience methods},
  volume={170},
  number={1},
  pages={96--110},
  year={2008},
  publisher={Elsevier}
}

% review of spike train distance measures, also provides generalization of van rossum and schreiber distances to arbitrary kernels and provides efficient estimators.
@article{paiva2010,
  title={A comparison of binless spike train measures},
  author={Paiva, Ant{\'o}nio RC and Park, Il and Pr{\'\i}ncipe, Jos{\'e} C},
  journal={Neural Computing and Applications},
  volume={19},
  number={3},
  pages={405--419},
  year={2010},
  publisher={Springer}
}

% neural spike coding as stochastic point process
@article{perkel1967a,
  title={Neuronal spike trains and stochastic point processes: I. The single spike train},
  author={Perkel, Donald H and Gerstein, George L and Moore, George P},
  journal={Biophysical journal},
  volume={7},
  number={4},
  pages={391--418},
  year={1967},
  publisher={Elsevier}
}

% neural spike coding as stochastic point process
@article{perkel1967b,
  title={Neuronal spike trains and stochastic point processes: II. Simultaneous spike trains},
  author={Perkel, Donald H and Gerstein, George L and Moore, George P},
  journal={Biophysical journal},
  volume={7},
  number={4},
  pages={419--440},
  year={1967},
  publisher={Elsevier}
}

% integrator + trigger + feedback loop circuit (equivalent to a neuron) implements a time encoding machine, i.e. a stochastic point process
@inproceedings{lazar2003,
  title={Time encoding and perfect recovery of bandlimited signals},
  author={Lazar, Aurel A and T{\'o}th, L{\'a}szl{\'o} T},
  booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03).},
  volume={6},
  pages={VI--709},
  year={2003},
  organization={IEEE}
}

% Spike coding as representation of membrane potential trajectories in IF models. Gibbs distribution for spike statistics is derived.
@article{cessac2009,
  title={How Gibbs distributions may naturally arise from synaptic adaptation mechanisms. A model-based argumentation},
  author={Cessac, Bruno and Rostro, Horacio and Vasquez, Juan-Carlos and Vi{\'e}ville, Thierry},
  journal={Journal of Statistical Physics},
  volume={136},
  number={3},
  pages={565--602},
  year={2009},
  publisher={Springer}
}

% Overview of neural spike coding, links between spike codes and continuous signals by filtering (for slow signals w.r.t. spiking rate --> time constants and other parameters are derived)
@article{cessac2010,
  title={Overview of facts and issues about neural coding by spikes},
  author={Cessac, Bruno and Paugam-Moisy, H{\'e}l{\`e}ne and Vi{\'e}ville, Thierry},
  journal={Journal of Physiology-Paris},
  volume={104},
  number={1-2},
  pages={5--18},
  year={2010},
  publisher={Elsevier}
}

% spike coding as predictive encoder, i.e. stochastic point process
@article{boerlin2011,
  title={Spike-based population coding and working memory},
  author={Boerlin, Martin and Den{\`e}ve, Sophie},
  journal={PLoS computational biology},
  volume={7},
  number={2},
  pages={e1001080},
  year={2011},
  publisher={Public Library of Science San Francisco, USA}
}

% neuronal encoding as A-D converter, i.e. stochastic point process
@article{chklovskii2012,
  title={Neuronal spike generation mechanism as an oversampling, noise-shaping a-to-d converter},
  author={Chklovskii, Dmitri and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  pages={503--511},
  year={2012}
}

% a survey on snn and spike coding schemes
@inproceedings{gruning2014,
  title={Spiking neural networks: Principles and challenges.},
  author={Gr{\"u}ning, Andr{\'e} and Bohte, Sander M},
  booktitle={ESANN},
  year={2014},
  organization={Citeseer}
}

% proposes a hybrid snn-ann network model. The output of the snn is a spike code which is hard to decode. Thus, the authors propose to train an ann to learn an optimal decoding of the target signal, through the information bottleneck.
@article{skatchkovsky2021,
  title={Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck},
  author={Skatchkovsky, Nicolas and Simeone, Osvaldo and Jang, Hyeryung},
  journal={arXiv preprint arXiv:2106.01177},
  year={2021}
}


%%%%%%%%%%
% ANN-SNN conversion
%%%%%%%%%%

% ANN-SNN conversion early work. Trains a rate-coded DBN, using contrastive divergence for training, before conversion to SNN. In order to make rate-coded network compatible with converted SNN, uses a formal smoothed rate-coded expression of the LIF nonlinearity for poisson input spike trains known as Siegert neuron.
@article{oconnor2013,
  title={Real-time classification and sensor fusion with a spiking deep belief network},
  author={O'Connor, Peter and Neil, Daniel and Liu, Shih-Chii and Delbruck, Tobi and Pfeiffer, Michael},
  journal={Frontiers in neuroscience},
  volume={7},
  pages={178},
  year={2013},
  publisher={Frontiers Media SA}
}

% ANN-SNN conversion early approach targeting issues of activity balancement and normalization - 99.12% acc on mnist with a 3 layer cnn
@inproceedings{diehl2015a,
  title={Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing},
  author={Diehl, Peter U and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  booktitle={2015 International joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2015},
  organization={ieee}
}

% ANN-SNN conversion where, during training, neuron activity is treated as firing probability and weights are treated as connection probabilities, in order to guarantee compatibility during conversion to snn. A smooth expression for the firing nonlinearity is found for training. the ann is converted to an snn with binary connections for efficient neuromorphic implementation, where the weights define the connection probabilities. Multiple converted networks are generated and ensembled. 99.42% accuracy on MNIST, 108 uJ per classification, 1000 classifications per second implemented on TrueNorth with 4-layer CNN (3840 neurons). Termed "constrain-then-train" approach, i.e. define ann characteristics beforehand so that they will be compatible with snn structure after conversion, rather than train-then-constrain, i.e. take the necessary adjustment after conversion has been performed.
@article{esser2015,
  title={Backpropagation for energy-efficient neuromorphic computing},
  author={Esser, Steve K and Appuswamy, Rathinakumar and Merolla, Paul and Arthur, John V and Modha, Dharmendra S},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

% Spiking network with soft-LIF. Rate-coded soft-LIF nonlinearity used during ANN training, for better consistency with successive conversion to SNN. Spiking AlexNet 84% accuracy on CIFAR10, 99.12% on MNIST, 79.20% top5 on ImageNet.
@article{hunsberger2016,
  title={Training spiking deep networks for neuromorphic hardware},
  author={Hunsberger, Eric and Eliasmith, Chris},
  journal={arXiv preprint arXiv:1611.05141},
  year={2016}
}

% ANN-SNN conversion 0.56% error on MNIST with LeNet8 8-layer cnn, 11.18% error on CIFAR-10 with 6 layer cnn, 50.39& top1 and 18.37% top5 error on ImageNet with VGG-16,  25.4% top1 and 7.96% top5 error on Imagenet with Inception-V3
@article{rueckauer2017, 
  title={Conversion of continuous-valued deep networks to efficient event-driven networks for image classification},
  author={Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={682},
  year={2017},
  publisher={Frontiers}
}

% ANN-SNN conversion deep residual networks - MNIST 0.41% error resnet8, CIFAR10 7.63% error resnet44, cifar100 31.44% error resnet44, imagenet 29.39% top1 error (9.78% top5) resnet34, imagenet 27.25% top1 error (9.03% top5) resnet50
@article{hu2018,
  title={Spiking deep residual network},
  author={Hu, Yangfan and Tang, Huajin and Pan, Gang},
  journal={arXiv preprint arXiv:1805.01352},
  year={2018}
}

% ANN-SNN conversion on deeper networks - VGG16 (8.5% error on CIFAR-10, 30% on ImageNet) and ResNet (12.5% error on CIFAR-10 with 20 layers, 34.53 on ImageNet with 34 layers)
@article{sengupta2019,
  title={Going deeper in spiking neural networks: VGG and residual architectures},
  author={Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
  journal={Frontiers in neuroscience},
  volume={13},
  pages={95},
  year={2019},
  publisher={Frontiers}
}

% Hybrid ANN-SNN conversion + stbp fine tuning. VGG-11 (CIFAR100 67.87% acc), VGG-16 (CIFAR-10 92.02% acc, ImageNet top1 65.19%), resnet-8 (CIFAR-10 91.35% acc), resnet-34 (ImageNet top1 61.48%)
@article{rathi2020,
  title={Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation},
  author={Rathi, Nitin and Srinivasan, Gopalakrishnan and Panda, Priyadarshini and Roy, Kaushik},
  journal={arXiv preprint arXiv:2005.01807},
  year={2020}
}

% Trains ANN with regularization which minimizes the ANN-SNN conversion error layerwise. CIFAR10 acc 90% with 8-layer cnn, 92% with VGG16, 93% with resnet20. CIFAR100 acc 70% vgg16, 69% resnet20. ImageNet acc 72% vgg16.
@article{deng2021,
  title={Optimal conversion of conventional artificial neural networks to spiking neural networks},
  author={Deng, Shikuang and Gu, Shi},
  journal={arXiv preprint arXiv:2103.00476},
  year={2021}
}

% ANN-SNN conversion scheme based on Clamped and Quantized training (CQ training), a training method based on the awareness of successive convergence. Trains VGG-9 and VGG-19 on CIFAR10 achieving 94.16% and 93.44% accuracy respectively
@inproceedings{yan2021,
  title={Near Lossless Transfer Learning for Spiking Neural Networks},
  author={Yan, Zhanglu and Zhou, Jun and Wong, Weng-Fai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={10577--10584},
  year={2021}
}


%%%%%%%%%%
% spiking backprop
%%%%%%%%%%

% a survey on surrogate gradient descent methods for snn training
@article{neftci2019,
  title={Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks},
  author={Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={51--63},
  year={2019},
  publisher={IEEE}
}

% survey on backprop methods for snns
@article{dampfhoffer2023,
  title={Backpropagation-Based Learning Techniques for Deep Spiking Neural Networks: A Survey},
  author={Dampfhoffer, Manon and Mesquida, Thomas and Valentian, Alexandre and Anghel, Lorena},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}

% first spiking backprop approach, smooth spikes on input
@inproceedings{bohte2000, 
  title={SpikeProp: backpropagation for networks of spiking neurons.},
  author={Bohte, Sander M and Kok, Joost N and La Poutr{\'e}, Johannes A},
  booktitle={ESANN},
  pages={419--424},
  year={2000}
}

% teacher neuron, smooth spikes on input + LTP on desired spike, LTD on output spike
@article{ponulak2005, 
    title={ReSuMe-new supervised learning method for spiking neural networks. Technical report},
    author={Ponulak, Filip},
    journal={Institute of Control and Information Engineering, Poznan University of Technology},
    year={2005}
}

% multi-spikeprop
@article{booij2005, 
  title={A gradient descent rule for spiking neurons emitting multiple spikes},
  author={Booij, Olaf and tat Nguyen, Hieu},
  journal={Information Processing Letters},
  volume={95},
  number={6},
  pages={552--558},
  year={2005},
  publisher={Elsevier}
}

% matching output spike distribution
@inproceedings{brea2011, 
  title={Sequence learning with hidden units in spiking neural networks},
  author={Brea, Johanni and Senn, Walter and Pfister, Jean-Pascal},
  booktitle={Advances in neural information processing systems},
  pages={1422--1430},
  year={2011}
}

% matching output spike distribution
@inproceedings{rezende2011, 
  title={Variational learning for recurrent spiking networks},
  author={Rezende, Danilo J and Wierstra, Daan and Gerstner, Wulfram},
  booktitle={Advances in neural information processing systems},
  pages={136--144},
  year={2011}
}

% Chronotron - Two learning rules: e-learning and i-learning. E-learning is based on minimization of victor-purpura (VP) metric to measure distance between spike trains in order to extend backprop to multi-spike settings. VP metric measures distance between spike trains as minimum cost for transforming a spike sequence into another by moving, adding or deleting spikes (see victor1997). E-learning allows for high capacity, but causes synapses to change sign. I-learning is similar to resume and is based on smoothed spike currents. It maintains distinct excitatory and inhibitory synapses.
@article{florian2012, 
  title={The chronotron: A neuron that learns to fire temporally precise spike patterns},
  author={Florian, R{\u{a}}zvan V},
  journal={PloS one},
  volume={7},
  number={8},
  year={2012},
  publisher={Public Library of Science}
}

% SPAN: smooth spikes on input and output + LTP on desired spike, LTD on output spike
@article{mohemmed2012, 
  title={Span: Spike pattern association neuron for learning spatio-temporal spike patterns},
  author={Mohemmed, Ammar and Schliebs, Stefan and Matsuda, Satoshi and Kasabov, Nikola},
  journal={International journal of neural systems},
  volume={22},
  number={04},
  pages={1250012},
  year={2012},
  publisher={World Scientific}
}

% Similar to ReSuMe, generalized to multiple splike training in multilayer snns. Also introduces trainable delay elements and synaptic scaling to maintain stability.
@article{sporea2013,
  title={Supervised learning in multilayer spiking neural networks},
  author={Sporea, Ioana and Gr{\"u}ning, Andr{\'e}},
  journal={Neural computation},
  volume={25},
  number={2},
  pages={473--509},
  year={2013},
  publisher={MIT Press}
}

% similar to ReSuMe
@article{yu2013, 
  title={Precise-spike-driven synaptic plasticity: Learning hetero-association of spatiotemporal spike patterns},
  author={Yu, Qiang and Tang, Huajin and Tan, Kay Chen and Li, Haizhou},
  journal={Plos one},
  volume={8},
  number={11},
  year={2013},
  publisher={Public Library of Science}
}

% DL-ReSuMe: extends resume by including additional trainable synaptic delay parameters
@article{taherkhani2015,
  title={DL-ReSuMe: A delay learning-based remote supervised method for spiking neurons},
  author={Taherkhani, Aboozar and Belatreche, Ammar and Li, Yuhua and Maguire, Liam P},
  journal={IEEE transactions on neural networks and learning systems},
  volume={26},
  number={12},
  pages={3137--3149},
  year={2015},
  publisher={IEEE}
}

% Normalized Adaptive Descent (NormAD) backprop approx for snns based on membrane potential (i.e. basically exponential smoothing of spiking nonlinearity, much like spiking backprop) and also a normalization stage of the resulting gradients.
@inproceedings{anwani2015,
  title={Normad-normalized approximate descent based supervised learning rule for spiking neurons},
  author={Anwani, Navin and Rajendran, Bipin},
  booktitle={2015 international joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2015},
  organization={IEEE}
}

% an early work on backprop in spiking networks, update proportional to input spike count times error spike count, also fractional sgd applies updates already from early time steps before steady state is reached - 96.4% acc with 3fc, 97.93% with fractional sgd on mnist
@article{oconnor2016,
  title={Deep spiking networks},
  author={O'Connor, Peter and Welling, Max},
  journal={arXiv preprint arXiv:1602.08323},
  year={2016}
}

% spiking backprop: replaces temporal error with spike count. approximates non differentiability with constant, uses a WTA circuit - 99.31% acc on mnist with 3 layer cnn
@article{lee2016, 
  title={Training deep spiking neural networks using backpropagation},
  author={Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  journal={Frontiers in neuroscience},
  volume={10},
  pages={508},
  year={2016},
  publisher={Frontiers}
}

% backpropagation in spiking networks for precise temporal coding. FILT rule uses smoothing by exponential kernel to cope with the nonlinearity (similar to spiking backprop), INST rule basically uses rectangular kernel (much like BP-STDP)
@article{gardner2016,
  title={Supervised learning in spiking neural networks for precise temporal encoding},
  author={Gardner, Brian and Gr{\"u}ning, Andr{\'e}},
  journal={PloS one},
  volume={11},
  number={8},
  pages={e0161335},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

% temporal spiking backprop approach, using causal set for determining input spikes that influenced output, and piecewise linear smoothing kernel approx - 97.2% acc on mnist with 2fc net, 96.92% with 3fc
@article{mostafa2017,
  title={Supervised learning based on temporal coding in spiking neural networks},
  author={Mostafa, Hesham},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={7},
  pages={3227--3235},
  year={2017},
  publisher={IEEE}
}

% backward pass interpreted as adjoint optimization. uses synaptic currents, thus basically smoothing spikes.
@article{huh2017,
  title={Gradient descent for spiking neural networks},
  author={Huh, Dongsung and Sejnowski, Terrence J},
  journal={arXiv preprint arXiv:1706.04698},
  year={2017}
}

% weights as delay elements, 99.1% accuracy on MNIST with a 2fc net
@inproceedings{liu2017a,
  title={Mt-spike: A multilayer time-based spiking neuromorphic architecture with temporal error backpropagation},
  author={Liu, Tao and Liu, Zihao and Lin, Fuhong and Jin, Yier and Quan, Gang and Wen, Wujie},
  booktitle={2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={450--457},
  year={2017},
  organization={IEEE}
}

% Uses both delay elements and ordinary weights - 99.36% acc on mnist with 3 layer cnn
@article{shrestha2018,
  title={Slayer: Spike layer error reassignment in time},
  author={Shrestha, Sumit Bam and Orchard, Garrick},
  journal={arXiv preprint arXiv:1810.08646},
  year={2018}
}

% rate coded output as in lee, but smoothed spikes - stbp (spatio temporal backprop) - 98.89% acc on mnist with 2fc net, 99.42% with 3 layer convnet
@article{wu2018, 
  title={Spatio-temporal backpropagation for training high-performance spiking neural networks},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={331},
  year={2018},
  publisher={Frontiers}
}

% as wu 2018 - 98.84% acc on mnist with 2fc net, 99.42% with 3 layer conv
@inproceedings{jin2018, 
  title={Hybrid macro/micro level backpropagation for training deep spiking neural networks},
  author={Jin, Yingyezhe and Zhang, Wenrui and Li, Peng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7005--7015},
  year={2018}
}

% Random Feedback Alignment not appropriate for complex tasks. Spiking backprop rule similar to SPAN
@article{zenke2018,
  title={Superspike: Supervised learning in multilayer spiking neural networks},
  author={Zenke, Friedemann and Ganguli, Surya},
  journal={Neural computation},
  volume={30},
  number={6},
  pages={1514--1541},
  year={2018},
  publisher={MIT Press}
}

% as wu 2018 - 85.24% acc on cifar10 with alexnet-like net, 90.53% with wider 7 layer net called cifar-net
@inproceedings{wu2019, 
  title={Direct training for spiking neural networks: Faster, larger, better},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Xie, Yuan and Shi, Luping},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1311--1318},
  year={2019}
}

% BP-STDP, Time divided in short slots. At every slot, delta w = (d - y) * input spike count. Analogue to ReSuMe with short rectangular smoothing kernel. 97.2% accuracy on MNIST with 3fc net
@article{tavanaei2019b, 
  title={BP-STDP: Approximating backpropagation using spike timing dependent plasticity},
  author={Tavanaei, Amirhossein and Maida, Anthony},
  journal={Neurocomputing},
  volume={330},
  pages={39--47},
  year={2019},
  publisher={Elsevier}
}

% error modulated stdp - 97.3% acc on mnist with 3fc net, 
@inproceedings{shrestha2019,
  title={Approximating back-propagation for a biologically plausible local learning rule in spiking neural networks},
  author={Shrestha, Amar and Fang, Haowen and Wu, Qing and Qiu, Qinru},
  booktitle={Proceedings of the International Conference on Neuromorphic Systems},
  pages={1--8},
  year={2019}
}

% SpikeGrad: equivalence between spike frequency coding and ann rate coding leads to ann-equivalent of snn model. Spike error accumulation in the backward pass can be implemented efficiently as in traditional ann. 99.52% acc on MNIST, 89.99% on CIFAR10 with 4-layer and 8-layer cnn. uses both positive and negative spikes (bipolar spikes) to encode error.
@article{thiele2019,
  title={Spikegrad: An ann-equivalent computation model for implementing backpropagation with spikes},
  author={Thiele, Johannes Christian and Bichler, Olivier and Dupret, Antoine},
  journal={arXiv preprint arXiv:1906.00851},
  year={2019}
}

% recurrent spiking backprop - 99.57% acc on mnist with 3 layer cnn
@article{zhang2019a,
  title={Spike-train level backpropagation for training deep recurrent spiking neural networks},
  author={Zhang, Wenrui and Li, Peng},
  journal={arXiv preprint arXiv:1908.06378},
  year={2019}
}

% interprets backward phase as adjoint optimization. uses synaptic currents, thus basically smoothing input spikes
@article{wunderlich2020,
  title={Eventprop: Backpropagation for exact gradients in spiking neural networks},
  author={Wunderlich, Timo C and Pehle, Christian},
  journal={arXiv preprint arXiv:2009.08378},
  year={2020}
}

% s4nn: a stbp-like algorithms for rank order coded spiking networks aimed at first spike classification based on latencies - 97.4% accuracy on mnist with 2fc net
@article{kheradpisheh2020,
  title={Temporal backpropagation for spiking neural networks with one spike per neuron},
  author={Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e},
  journal={International Journal of Neural Systems},
  volume={30},
  number={06},
  pages={2050027},
  year={2020},
  publisher={World Scientific}
}

% as wu2018 - cifar10 91.41% acc with 7 layer cnn, mnist 99.5% acc with 3 layer cnn
@article{zhang2020,
  title={Temporal spike sequence learning via backpropagation for deep spiking neural networks},
  author={Zhang, Wenrui and Li, Peng},
  journal={arXiv preprint arXiv:2002.10085},
  year={2020}
}

% as wu2018 + batch norm - 99.40% acc on mnist with 4 layers cnn, 90.20% on cifar10 with resnet11, 58.5% on cifar100 with resnet50, 81.2% on imagenette with resnet50
@article{ledinauskas2020,
  title={Training deep spiking neural networks},
  author={Ledinauskas, Eimantas and Ruseckas, Julius and Jur{\v{s}}{\.e}nas, Alfonsas and Bura{\v{c}}as, Giedrius},
  journal={arXiv preprint arXiv:2006.04436},
  year={2020}
}

% spiking backprop: replaces temporal error with spike count. approximates non differentiability with constant, improves lee2016 99.59% accuracy on MNIST with lenet5, 90.95% accuracy on CIFAR-10 with resnet11, 96.21% accuracy on SVHN with resnet7
@article{lee2020,
  title={Enabling spike-based backpropagation for training deep neural network architectures},
  author={Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  journal={Frontiers in neuroscience},
  volume={14},
  pages={119},
  year={2020},
  publisher={Frontiers}
}

% improves over stbp (wu2018) by adding threshold dependent normalization (tdSTBP) and provides experimental results on deeper networks and complex datasets - CIFAR10 resnet19 93.10% acc, ImageNet resnet34 (large) 67.34% top1 acc, resnet34 63.72%, resnet50 64.88%
@article{zheng2020,
  title={Going deeper with directly-trained larger spiking neural networks},
  author={Zheng, Hanle and Wu, Yujie and Deng, Lei and Hu, Yifan and Li, Guoqi},
  journal={arXiv preprint arXiv:2011.05280},
  year={2020}
}

% decolle: basically a spiking network with early exits trained by random feedback alignment
@article{kaiser2020,
  title={Synaptic plasticity dynamics for deep continuous local learning (DECOLLE)},
  author={Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},
  journal={Frontiers in Neuroscience},
  volume={14},
  pages={424},
  year={2020},
  publisher={Frontiers}
}

% T2FSNN: TTFS coding, temporal backpropagation for precise firing time tuning in deep snn. Reduction to 22% of inference time and <1% num spikes without significant performance drop. 91.43% accuracy on CIFAR10 and 68.79% on CIFAR100 with VGG16.
@inproceedings{park2020,
  title={T2FSNN: deep spiking neural networks with time-to-first-spike coding},
  author={Park, Seongsik and Kim, Seijoon and Na, Byunggook and Yoon, Sungroh},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

% GLSNN: multilayer snn trained with supervision with global feedback alignment and differential stdp. 98.62% acc MNIST, 89.05% fashion MNIST with 4 layer fcn with 800 neurons per layer.
@article{zhao2020,
  title={GLSNN: A multi-layer spiking neural network based on global feedback alignment and local STDP plasticity},
  author={Zhao, Dongcheng and Zeng, Yi and Zhang, Tielin and Shi, Mengting and Zhao, Feifei},
  journal={Frontiers in Computational Neuroscience},
  volume={14},
  pages={576841},
  year={2020},
  publisher={Frontiers Media SA}
}

% as in lee2016 using spike count-based backprop with if neurons instead of lif - 99.65% acc on mnist with lenet5 and if neurons (99.60% with lif), 91.35% acc on cifar10 with vgg7 and if neurons (90.11% with lif)
@article{wu2021a,
  title={Training Spiking Neural Networks with Accumulated Spiking Flow},
  author={Wu, Hao and Zhang, Yueyi and Weng, Wenming and Zhang, Yongting and Xiong, Zhiwei and Zha, Zheng-Jun and Sun, Xiaoyan and Wu, Feng},
  journal={ijo},
  volume={1},
  number={1},
  year={2021}
}

% Trains an snn using an auxiliary coupled ann with a nonlinearity that resembles the rate-coded spiking nonlinearity for gradient backprop. 91.77% accuracy on CIFAR10, 57.55% on ImageNet, 99.31% on n-mnist, 65.59% on dvs-cifar10 with alexnet-like 7-layer cnn.
@article{wu2021b,
  title={A tandem learning rule for effective training and rapid inference of deep spiking neural networks},
  author={Wu, Jibin and Chua, Yansong and Zhang, Malu and Li, Guoqi and Li, Haizhou and Tan, Kay Chen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

% as wu2018 + layerwise freezout and backward phase skipping - 99.58% acc on mnist with lenet5, 95.97% acc on svhn with 6 layers alexnet-like cnn
@article{zhu2021,
  title={An Efficient Learning Algorithm for Direct Training Deep Spiking Neural Networks},
  author={Zhu, Xiaolei and Zhao, Baixin and Ma, De and Tang, Huajin},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  year={2021},
  publisher={IEEE}
}

% STiDi-BP (spike time displacement backprop) - backpropagates target firing time instead of error, and then computes error afterwards, and learns in a stbp-like fashion. Also approximates smoothing kernels with piecewise linear functions - 99.2% acc on mnist with 3 layer cnn, 97.4% with 3fc net
@article{mirsadeghi2021,
  title={Spike time displacement based error backpropagation in convolutional spiking neural networks},
  author={Mirsadeghi, Maryam and Shalchian, Majid and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e},
  journal={arXiv preprint arXiv:2108.13621},
  year={2021}
}

% temporal dependent local learning (tdll) - basically layerwise early exits for training deep snns (as decolle but with ordinary spiking backprop instead of random feedback alignment) - 8 layer cnn 88.01% acc on cifar10, 7 layer cnn 95.49% acc on SVHN, 99.40% acc on MNIST with 3 layer cnn and 98.59% with 2fc - also decolle results: 97.51% acc on mnist with 3 layer cnn and 98.10% with 2fc, 82.48% acc on svhn with 7 layers cnn, 74.70% acc on cifar10 with 8 layers cnn
@inproceedings{ma2021,
  title={Temporal Dependent Local Learning for Deep Spiking Neural Networks},
  author={Ma, Chenxiang and Xu, Junhai and Yu, Qiang},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}

% Spiking backprop with TTFS coding using current-based (CuBA) synapses which lead to a differentiable spike time model. Also implementation on BrainScaleS-2
@article{goltz2021,
  title={Fast and energy-efficient neuromorphic deep learning with first-spike times},
  author={G{\"o}ltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and others},
  journal={Nature machine intelligence},
  volume={3},
  number={9},
  pages={823--835},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

% Incorporates learnable time constants in snn training. 99.72% MNIST with 1 128kernel conv layer, 93.50% CIFAR10 with 3 256kernel conv layers
@inproceedings{fang2021,
  title={Incorporating learnable membrane time constant to enhance learning of spiking neural networks},
  author={Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timoth{\'e}e and Huang, Tiejun and Tian, Yonghong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2661--2671},
  year={2021}
}

% In TTFS spiking neurons, expresses firing time in closed form w.r.t. input spike times and weights. Differentiate through smoothed spikes and enables efficient training thanks to the closed form expression. 99.33% accuracy on mnist with 2 conv layers with 32 and 16 filters, 92.68% on CIFAR10 with spiking VGG16, 68.8% on ImageNet with spiking GoogleNet.
@inproceedings{zhou2021,
  title={Temporal-coded deep spiking neural network with easy training and robust performance},
  author={Zhou, Shibo and Li, Xiaohua and Chen, Ying and Chandrasekaran, Sanjeev T and Sanyal, Arindam},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={11143--11151},
  year={2021}
}

% spiking backprop using alpha synaptic function which essentially makes spike nonlinearity smooth. 97.96% on mnist with 1 fc layer of 340 neurons
@article{comcsa2021,
  title={Temporal coding in spiking neural networks with alpha synaptic function: learning with backpropagation},
  author={Com{\c{s}}a, Iulia-Maria and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={10},
  pages={5939--5952},
  year={2021},
  publisher={IEEE}
}

% DIET-SNN: direct training snn with leakage and threshold optimization. CIFAR10 90% accuracy with VGG6, 93.44% VGG16, 92.54% ResNet20, CIFAR100 69.67% VGG16, 64.07% ResNet20, ImageNet 69% VGG16
@article{rathi2021,
  title={Diet-snn: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization},
  author={Rathi, Nitin and Roy, Kaushik},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

% New form of spiking residual blocks, where the addition is replaced by and or inverted-and over the spike trains --> SEW (Spike Element-Wise) ResNet. ImageNet top1 accuracy 63% ResNet18 up to 69% ResNet152, while in previous work accuracy decreased after many layers.top5 accuracy 84% to 88%
@article{fang2021,
  title={Deep residual learning in spiking neural networks},
  author={Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Huang, Tiejun and Masquelier, Timoth{\'e}e and Tian, Yonghong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21056--21069},
  year={2021}
}

% a CHL-like learning framework were learning is supported by frequency dependent synaptic plasticity which is driven by higher layers with top-down feedback
@article{payeur2021,
  title={Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits},
  author={Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A and Naud, Richard},
  journal={Nature neuroscience},
  volume={24},
  number={7},
  pages={1010--1019},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

% As wu2018. Introduces self-feedback synapses (autapses) and excitatory-inhibitory balancement. 4 layer cnn --> 99.5% acc MNIST, 90.93% CIFAR10. Uses both positive and negative spikes (bipolar spikes) in autapses.
@article{zhao2022,
  title={BackEISNN: A deep spiking neural network with adaptive self-feedback and balanced excitatory--inhibitory neurons},
  author={Zhao, Dongcheng and Zeng, Yi and Li, Yang},
  journal={Neural Networks},
  volume={154},
  pages={68--77},
  year={2022},
  publisher={Elsevier}
}

% multiplication free snn with delays/additions in place of multiplications for efficient implementation. Learning strategy translates similar input patterns into correlated output spike trains. experiments with 3 layers fc snn (256, 256, 256 neurons) + classifier yields 89% accuracy on MNIST, 95% top5 accuracy on CIFAR10 and 50% top5 accuracy CIFAR100. Optimization via ADAM with smoothed spikes for gradient approximation.
@article{stanojevic2023,
  title={Time-encoded multiplication-free spiking neural networks: application to data classification tasks},
  author={Stanojevic, Ana and Cherubini, Giovanni and Wo{\'z}niak, Stanis{\l}aw and Eleftheriou, Evangelos},
  journal={Neural Computing and Applications},
  volume={35},
  number={9},
  pages={7017--7033},
  year={2023},
  publisher={Springer}
}


%%%%%%%%%%
% Spiking adversarial
%%%%%%%%%%

% First work on spiking adversarial examples, studies effects of white box attacks and adversarial training is proposed
@inproceedings{bagheri2018,
  title={Adversarial training for probabilistic spiking neural networks},
  author={Bagheri, Alireza and Simeone, Osvaldo and Rajendran, Bipin},
  booktitle={2018 IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)},
  pages={1--5},
  year={2018},
  organization={IEEE}
}

% studies snns robustness against adversarial attacks and proposes a black box greedy attack methodology 
@article{marchisio2019,
  title={Snn under attack: are spiking deep belief networks vulnerable to adversarial examples},
  author={Marchisio, Alberto and Nanfa, Giorgio and Khalid, Faiq and Hanif, Muhammad Abdullah and Martina, Maurizio and Shafique, Muhammad},
  journal={arXiv preprint arXiv:1902.01147},
  year={2019}
}

% A study of adversarial robustness of snns w.r.t. various attacks and various types of training. It is shown that stbp training is more robust than ann-snn conversion
@inproceedings{sharmin2019,
  title={A comprehensive analysis on adversarial robustness of spiking neural networks},
  author={Sharmin, Saima and Panda, Priyadarshini and Sarwar, Syed Shakib and Lee, Chankyu and Ponghiran, Wachirawit and Roy, Kaushik},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

% studies robustness of snns due to input encoding and leaky behavior
@inproceedings{sharmin2020,
  title={Inherent adversarial robustness of deep spiking neural networks: Effects of discrete input encoding and non-linear activations},
  author={Sharmin, Saima and Rathi, Nitin and Panda, Priyadarshini and Roy, Kaushik},
  booktitle={European Conference on Computer Vision},
  pages={399--414},
  year={2020},
  organization={Springer}
}

% Studies adversarial attacks in real world on neuromorphic hardware through Dynamic Vision Sensors (DVS). Proposes noise filters as a countermeasure and then develops new attacks that overcome the noise filters (mask filter aware dash attacks).
@inproceedings{marchisio2021,
  title={Dvs-attacks: Adversarial attacks on dynamic vision sensors for spiking neural networks},
  author={Marchisio, Alberto and Pira, Giacomo and Martina, Maurizio and Masera, Guido and Shafique, Muhammad},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--9},
  year={2021},
  organization={IEEE}
}

% adapts gradient based adversarial attacks to spiking cnns and performs real experiments on neuromorphic hardware
@article{buchel2021,
  title={Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision},
  author={B{\"u}chel, Julian and Lenz, Gregor and Hu, Yalun and Sheik, Sadique and Sorbaro, Martino},
  journal={arXiv preprint arXiv:2110.02929},
  year={2021}
}

% Optimizes snn adversarial robustness by tuning firing threshold and leakage constant
@inproceedings{elallami2021,
  title={Securing deep spiking neural networks against adversarial attacks through inherent structural parameters},
  author={El-Allami, Rida and Marchisio, Alberto and Shafique, Muhammad and Alouani, Ihsen},
  booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={774--779},
  year={2021},
  organization={IEEE}
}

% design a gradient-to-spike converter to generate adversarial attacks against snns
@article{liang2021,
  title={Exploring adversarial attack in spiking neural networks with spike-compatible gradient},
  author={Liang, Ling and Hu, Xing and Deng, Lei and Wu, Yujie and Li, Guoqi and Ding, Yufei and Li, Peng and Xie, Yuan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

% implement a kind of adversarial training for snns
@inproceedings{kundu2021,
  title={HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training With Crafted Input Noise},
  author={Kundu, Souvik and Pedram, Massoud and Beerel, Peter A},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5209--5218},
  year={2021}
}

%%%%%%%%%%
% Spiking metalearning
%%%%%%%%%%

% network with evolvable compartments (evolvable neural units - ENU) can adapt to mimic LIF, STDP, and reinforcement learning
@article{bertens2020,
  title={Network of evolvable neural units can learn synaptic learning rules and spiking dynamics},
  author={Bertens, Paul and Lee, Seong-Whan},
  journal={Nature Machine Intelligence},
  volume={2},
  number={12},
  pages={791--799},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

% Spiking and predictive coding-based backprop-free online-within-online metalearning 
@inproceedings{rosenfeld2021,
  title={Fast on-device adaptation for spiking neural networks via online-within-online meta-learning},
  author={Rosenfeld, Bleema and Rajendran, Bipin and Simeone, Osvaldo},
  booktitle={2021 IEEE Data Science and Learning Workshop (DSLW)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}

 % MAML but for spiking networks
@article{stewart2022,
  title={Meta-learning spiking neural networks with surrogate gradient descent},
  author={Stewart, Kenneth M and Neftci, Emre O},
  journal={Neuromorphic Computing and Engineering},
  volume={2},
  number={4},
  pages={044002},
  year={2022},
  publisher={IOP Publishing}
}

% learning neuromodulated spiking learning rules through metalearning
@article{schmidgall2022,
  title={Learning to learn online with neuromodulated synaptic plasticity in spiking neural networks},
  author={Schmidgall, Samuel and Hays, Joe},
  journal={bioRxiv},
  pages={2022--06},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

% Temporal kernel for spiking backprop optimized through meta-heuristic algorithms (not really meta-learned, but more like hyperparameter search over kernels and cross-validated)
@article{javanshir2023,
  title={Training Spiking Neural Networks with Metaheuristic Algorithms},
  author={Javanshir, Amirhossein and Nguyen, Thanh Thi and Mahmud, MA Parvez and Kouzani, Abbas Z},
  journal={Applied Sciences},
  volume={13},
  number={8},
  pages={4809},
  year={2023},
  publisher={MDPI}
}


%%%%%%%%%%
% Spiking information maximization
%%%%%%%%%%

% information efficacy
@article{london2002,
  title={The information efficacy of a synapse},
  author={London, Michael and Schreibman, Adi and H{\"a}usser, Michael and Larkum, Matthew E and Segev, Idan},
  journal={Nature neuroscience},
  volume={5},
  number={4},
  pages={332--340},
  year={2002},
  publisher={Nature Publishing Group}
}

% spiking infomax
@inproceedings{chechik2001b,
  title={Temporally dependent plasticity: An information theoretic account},
  author={Chechik, Gal and Tishby, Naftali},
  booktitle={Advances in Neural Information Processing Systems},
  pages={110--116},
  year={2001}
}

% spiking infomax
@article{chechik2003,
  title={Spike-timing-dependent plasticity and relevant mutual information maximization},
  author={Chechik, Gal},
  journal={Neural computation},
  volume={15},
  number={7},
  pages={1481--1510},
  year={2003},
  publisher={MIT Press}
}

% spiking ica
@article{perrinet2004,
  title={Finding independent components using spikes: a natural result of hebbian learning in a sparse spike coding scheme},
  author={Perrinet, Laurent},
  journal={Natural Computing},
  volume={3},
  number={2},
  pages={159},
  year={2004},
  publisher={Springer}
}

% Spiking infomax
@article{toyoizumi2005,
  title={Generalized Bienenstock--Cooper--Munro rule for spiking neurons that maximizes information transmission},
  author={Toyoizumi, Taro and Pfister, Jean-Pascal and Aihara, Kazuyuki and Gerstner, Wulfram},
  journal={Proceedings of the National Academy of Sciences},
  volume={102},
  number={14},
  pages={5239--5244},
  year={2005},
  publisher={National Acad Sciences}
}

% Spiking infomax with more than one neuron
@inproceedings{bell2005,
  title={Maximising sensitivity in a spiking network},
  author={Bell, Anthony J and Parra, Lucas C},
  booktitle={Advances in neural information processing systems},
  pages={121--128},
  year={2005}
}

% Spiking infomax with more than one neuron
@article{parra2009,
  title={On the maximization of information flow between spiking neurons},
  author={Parra, Lucas C and Beck, Jeffrey M and Bell, Anthony J},
  journal={Neural Computation},
  volume={21},
  number={11},
  pages={2991--3009},
  year={2009},
  publisher={MIT Press}
}

% Spiking information bottleneck and ica
@inproceedings{klampfl2007,
  title={Information bottleneck optimization and independent component extraction with spiking neurons},
  author={Klampfl, Stefan and Maass, Wolfgang and Legenstein, Robert A},
  booktitle={Advances in neural information processing systems},
  pages={713--720},
  year={2007}
}

% Spiking information bottleneck and ica
@inproceedings{buesing2008,
  title={Simplified rules and theoretical analysis for information bottleneck optimization and PCA with spiking neurons},
  author={Buesing, Lars and Maass, Wolfgang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={193--200},
  year={2008}
}

% Spiking information bottleneck and ica
@article{klampfl2009,
  title={Spiking neurons can learn to solve information bottleneck problems and extract independent components},
  author={Klampfl, Stefan and Legenstein, Robert and Maass, Wolfgang},
  journal={Neural Computation},
  volume={21},
  number={4},
  pages={911--959},
  year={2009},
  publisher={MIT Press}
}

% Spiking information bottleneck and ica
@article{buesing2010,
  title={A spiking neuron as information bottleneck},
  author={Buesing, Lars and Maass, Wolfgang},
  journal={Neural computation},
  volume={22},
  number={8},
  pages={1961--1992},
  year={2010},
  publisher={MIT Press}
}

% Spiking ica with intrinsic plasticity for adaptive regulation of output nonlinearity
@article{savin2010,
  title={Independent component analysis in spiking neurons},
  author={Savin, Cristina and Joshi, Prashant and Triesch, Jochen},
  journal={PLoS Comput Biol},
  volume={6},
  number={4},
  pages={e1000757},
  year={2010},
  publisher={Public Library of Science}
}


%%%%%%%%%%
% Spiking rnn
%%%%%%%%%%

@inproceedings{liaw1998,
  title={Robust speech recognition with dynamic synapses},
  author={Liaw, Jim-Shih and Berger, Theodore W},
  booktitle={1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No. 98CH36227)},
  volume={3},
  pages={2175--2179},
  year={1998},
  organization={IEEE}
}

@article{nager2002,
  title={Speech recognition with spiking neurons and dynamic synapses: a model motivated by the human auditory pathway},
  author={N{\"a}ger, Christian and Storck, Jan and Deco, Gustavo},
  journal={Neurocomputing},
  volume={44},
  pages={937--942},
  year={2002},
  publisher={Elsevier}
}

@article{graves2003,
  title={Comparing LSTM Recurrent Networks with Spiking Recurrent Networks on the Recognition of Spoken Digits},
  author={Graves, Alex and Eck, Douglas and Schmidhuber, J{\"u}rgen},
  year={2003}
}

@article{koopman2003a,
  title={Dynamic neural networks, comparing spiking circuits and LSTM},
  author={Koopman, Arne and Van Leeuwen, Matthijs and Vreeken, Jilles},
  year={2003},
  publisher={Utrecht University: Information and Computing Sciences}
}

@inproceedings{koopman2003b,
  title={Exploring Temporal Memory of LSTM and Spiking Circuits},
  author={Koopman, Arne and van Leeuwen, Matthijs and Vreeken, Jilles},
  year={2003},
  organization={IPCAT: Future of Neural Networks workshop}
}

@article{kroger2009,
  title={Towards a neurocomputational model of speech production and perception},
  author={Kr{\"o}ger, Bernd J and Kannampuzha, Jim and Neuschaefer-Rube, Christiane},
  journal={Speech Communication},
  volume={51},
  number={9},
  pages={793--809},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{costa2017,
  title={Cortical microcircuits as gated-recurrent neural networks},
  author={Costa, Rui and Assael, Ioannis Alexandros and Shillingford, Brendan and de Freitas, Nando and Vogels, TIm},
  booktitle={Advances in neural information processing systems},
  pages={272--283},
  year={2017}
}

@inproceedings{shrestha2017a,
  title={A spike-based long short-term memory on a neurosynaptic processor},
  author={Shrestha, Amar and Ahmed, Khadeer and Wang, Yanzhi and Widemann, David P and Moody, Adam T and Van Essen, Brian C and Qiu, Qinru},
  booktitle={2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={631--637},
  year={2017},
  organization={IEEE}
}

@article{tavanaei2017a,
  title={A spiking network that learns to extract spike signatures from speech signals},
  author={Tavanaei, Amirhossein and Maida, Anthony S},
  journal={Neurocomputing},
  volume={240},
  pages={191--199},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{tavanaei2017b,
  title={Bio-inspired multi-layer spiking neural network extracts discriminative features from speech signals},
  author={Tavanaei, Amirhossein and Maida, Anthony},
  booktitle={International conference on neural information processing},
  pages={899--908},
  year={2017},
  organization={Springer}
}

@article{demin2018,
  title={Recurrent spiking neural network learning based on a competitive maximization of neuronal activity},
  author={Demin, Vyacheslav and Nekhaev, Dmitry},
  journal={Frontiers in neuroinformatics},
  volume={12},
  pages={79},
  year={2018},
  publisher={Frontiers}
}

@inproceedings{bellec2018,
  title={Long short-term memory and learning-to-learn in networks of spiking neurons},
  author={Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={787--797},
  year={2018}
}

% e-prop: eligibility traces in spiking rnn training instead of bptt
@article{bellec2019,
  title={Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets},
  author={Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  journal={arXiv preprint arXiv:1901.09049},
  year={2019}
}

@article{kim2019b,
  title={Simple framework for constructing functional spiking recurrent neural networks},
  author={Kim, Robert and Li, Yinghao and Sejnowski, Terrence J},
  journal={Proceedings of the national academy of sciences},
  volume={116},
  number={45},
  pages={22811--22820},
  year={2019},
  publisher={National Acad Sciences}
}

@article{wang2019a,
  title={Temporal spiking recurrent neural network for action recognition},
  author={Wang, Wei and Hao, Siyuan and Wei, Yunchao and Xiao, Shengtao and Feng, Jiashi and Sebe, Nicu},
  journal={IEEE Access},
  volume={7},
  pages={117165--117175},
  year={2019},
  publisher={IEEE}
}

@inproceedings{bialas2020,
  title={Biologically Plausible Learning of Text Representation with Spiking Neural Networks},
  author={Bia{\l}as, Marcin and Miro{\'n}czuk, Marcin Micha{\l} and Ma{\'n}dziuk, Jacek},
  booktitle={International Conference on Parallel Problem Solving from Nature},
  pages={433--447},
  year={2020},
  organization={Springer}
}

@inproceedings{yin2020,
  title={Effective and efficient computation with multiple-timescale spiking recurrent neural networks},
  author={Yin, Bojian and Corradi, Federico and Boht{\'e}, Sander M},
  booktitle={International Conference on Neuromorphic Systems 2020},
  pages={1--8},
  year={2020}
}

@inproceedings{lotfi2020,
  title={Long Short-Term Memory Spiking Networks and Their Applications},
  author={Lotfi Rezaabad, Ali and Vishwanath, Sriram},
  booktitle={International Conference on Neuromorphic Systems 2020},
  pages={1--9},
  year={2020}
}

% Memory enriched snns with Hebbian 
@article{limbacher2022,
  title={Memory-enriched computation and learning in spiking neural networks through Hebbian plasticity},
  author={Limbacher, Thomas and {\"O}zdenizci, Ozan and Legenstein, Robert},
  journal={arXiv preprint arXiv:2205.11276},
  year={2022}
}


%%%%%%%%%%
% reservoir
%%%%%%%%%%

% Reveiw of reservoir computing approaches, liquid state machines, echo state networks. These models use a large reservoir of recurrently connected units forming a highly nonlinear dynamical system. System dynamics provide a nonlinear mapping of input stimuli to a high dimensional representation, where data are more likely to be linearly separable.
@article{lukovsevivcius2009,
  title={Reservoir computing approaches to recurrent neural network training},
  author={Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
  journal={Computer science review},
  volume={3},
  number={3},
  pages={127--149},
  year={2009},
  publisher={Elsevier}
}

% Survey on the Deep Echo State Networks (DeepESN).
@article{gallicchio2017,
  title={Deep echo state network (deepesn): A brief survey},
  author={Gallicchio, Claudio and Micheli, Alessio},
  journal={arXiv preprint arXiv:1712.04323},
  year={2017}
}

% Echo state network
@article{jaeger2001,
  title={The echo state approach to analysing and training recurrent neural networks-with an erratum note},
  author={Jaeger, Herbert},
  journal={Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  volume={148},
  number={34},
  pages={13},
  year={2001},
  publisher={Bonn}
}

% Liquid state machine
@article{maass2002,
  title={Real-time computing without stable states: A new framework for neural computation based on perturbations},
  author={Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  journal={Neural computation},
  volume={14},
  number={11},
  pages={2531--2560},
  year={2002},
  publisher={MIT Press}
}

% Extreme learning machines - feedforward only very wide untrained architecture providing a nonlinear mapping to a high dimensional space for successive linear separability, analogous to reservoir computing but without recurrent processing.
@article{huang2006, 
  title={Extreme learning machine: theory and applications},
  author={Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  journal={Neurocomputing},
  volume={70},
  number={1-3},
  pages={489--501},
  year={2006},
  publisher={Elsevier}
}

% Bio-plausible implementation of liquid state machine with bio-inspired learning
@article{zhang2015a,
  title={A digital liquid state machine with biologically inspired learning and its application to speech recognition},
  author={Zhang, Yong and Li, Peng and Jin, Yingyezhe and Choe, Yoonsuck},
  journal={IEEE transactions on neural networks and learning systems},
  volume={26},
  number={11},
  pages={2635--2649},
  year={2015},
  publisher={IEEE}
}

% Deep liquid state machine with stdp adaptation
@inproceedings{wang2016c,
  title={D-lsm: Deep liquid state machine with unsupervised recurrent reservoir tuning},
  author={Wang, Qian and Li, Peng},
  booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
  pages={2652--2657},
  year={2016},
  organization={IEEE}
}

% Gated ESN: gating mechanisms in esn to improve recall of long term dependencies.
@inproceedings{disarli2020,
  title={Gated echo state networks: a preliminary study},
  author={Di Sarli, Daniele and Gallicchio, Claudio and Micheli, Alessio},
  booktitle={2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},
  pages={1--5},
  year={2020},
  organization={IEEE}
}

% ESN for continual learning, not having a learning mechanism in the recurrent processing part of the network helps reducing catastrophic forgetting
@article{cossu2021,
  title={Continual learning with echo state networks},
  author={Cossu, Andrea and Bacciu, Davide and Carta, Antonio and Gallicchio, Claudio and Lomonaco, Vincenzo},
  journal={arXiv preprint arXiv:2105.07674},
  year={2021}
}


%%%%%%%%%%
% spiking autoencoders
%%%%%%%%%%

% recurrent spiking autoencoders with mirrored stdp for feedback connections
@article{burbank2015,
  title={Mirrored STDP implements autoencoder learning in a network of spiking neurons},
  author={Burbank, Kendra S},
  journal={PLoS computational biology},
  volume={11},
  number={12},
  pages={e1004566},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

% spiking stacked autoencoders - 99.05% acc on mnist with 3 layers cnn, 75.42% acc on cifar10 with 4 layers cnn
@inproceedings{panda2016,
  title={Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition},
  author={Panda, Priyadarshini and Roy, Kaushik},
  booktitle={2016 International Joint Conference on Neural Networks (IJCNN)},
  pages={299--306},
  year={2016},
  organization={IEEE}
}

% A 2 layer model. The first layer is a convolution learned by encoding-decoding task. The second layer is a feature discovery layer learned by local stdp + WTA. Finally, there is an SVM classifier. Achieves 97.15% accuracy on MNIST.
@inproceedings{tavanaei2017,
  title={Multi-layer unsupervised learning in a spiking convolutional neural network},
  author={Tavanaei, Amirhossein and Maida, Anthony S},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2023--2030},
  year={2017},
  organization={IEEE}
}

% spiking autoencoders directly trained end-to-end using spiking backprop. Also used for audio-to-image generation.
@article{roy2019a,
  title={Synthesizing images from spatio-temporal representations using spike-based backpropagation},
  author={Roy, Deboleena and Panda, Priyadarshini and Roy, Kaushik},
  journal={Frontiers in neuroscience},
  volume={13},
  pages={621},
  year={2019},
  publisher={Frontiers}
}

% spiking autoencoders directly trained end-to-end with spiking backprop
@article{coma2021,
  title={Spiking Autoencoders With Temporal Coding},
  author={Coma, Iulia Maria and Versari, Luca and Fischbacher, Thomas and Alakuijala, Jyrki},
  journal={Frontiers in Neuroscience},
  volume={15},
  pages={936},
  year={2021},
  publisher={Frontiers}
}

% spiking vae directly trained end-to-end with spiking backprop
@article{kamata2021,
  title={Fully Spiking Variational Autoencoder},
  author={Kamata, Hiromichi and Mukuta, Yusuke and Harada, Tatsuya},
  journal={arXiv preprint arXiv:2110.00375},
  year={2021}
}


%%%%%%%%%%
% spiking generative
%%%%%%%%%%

% energy-based generative model (DBM) with spiking neurons enhanced with stp
@article{leng2018,
  title={Spiking neurons with short-term synaptic plasticity form superior generative networks},
  author={Leng, Luziwei and Martel, Roman and Breitwieser, Oliver and Bytschok, Ilja and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A},
  journal={Scientific reports},
  volume={8},
  number={1},
  pages={1--11},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{kotariya2021,
  title={Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding},
  author={Kotariya, Vineet and Ganguly, Udayan},
  journal={arXiv preprint arXiv:2106.15420},
  year={2021}
}


%%%%%%%%%%
% spiking transfer learning
%%%%%%%%%%

@article{zhan2021,
  title={Effective Transfer Learning Algorithm in Spiking Neural Networks},
  author={Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Sun, Guolin and Tang, Huajin},
  journal={IEEE Transactions on Cybernetics},
  year={2021},
  publisher={IEEE}
}


%%%%%%%%%%
% spiking detection and segmentation
%%%%%%%%%%

% conversion of detseg models to snns
@inproceedings{kirkland2020,
  title={SpikeSEG: Spiking segmentation via STDP saliency mapping},
  author={Kirkland, Paul and Di Caterina, Gaetano and Soraghan, John and Matich, George},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

% conversion of yolo to snn
@inproceedings{kim2020,
  title={Spiking-YOLO: Spiking neural network for energy-efficient object detection},
  author={Kim, Seijoon and Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={11270--11277},
  year={2020}
}

% Object localization with snn trained with DECOLLE
@inproceedings{barchid2021,
  title={Deep spiking convolutional neural network for single object localization based on deep continuous local learning},
  author={Barchid, Sami and Mennesson, Jos{\'e} and Dj{\'e}raba, Chaabane},
  booktitle={2021 International Conference on Content-Based Multimedia Indexing (CBMI)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

% direct training of snn detseg models
@article{kim2021,
  title={Beyond Classification: Directly Training Spiking Neural Networks for Semantic Segmentation},
  author={Kim, Youngeun and Chough, Joshua and Panda, Priyadarshini},
  journal={arXiv preprint arXiv:2110.07742},
  year={2021}
}


%%%%%%%%%%
% spiking cv applications
%%%%%%%%%%

% STDP rule 95% accuracy on MNIST with 1 conv layer followed by svm
@article{diehl2015b, 
    title={Unsupervised learning of digit recognition using spike-timing-dependent plasticity},
    author={Diehl, Peter U and Cook, Matthew},
    journal={Frontiers in computational neuroscience},
    volume={9},
    pages={99},
    year={2015},
    publisher={Frontiers}
}

% Q2PS approximation of STDP in nessler delta w = 2^-w, soft-WTA, teacher neuron, 89.7% accuracy on MNIST with 2fc net
@inproceedings{shrestha2017b,
  title={Stable spike-timing dependent plasticity rule for multilayer unsupervised and supervised learning},
  author={Shrestha, Amar and Ahmed, Khadeer and Wang, Yanzhi and Qiu, Qinru},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={1999--2006},
  year={2017},
  organization={IEEE}
}

% STDP variant delta w = w(1-w) (masquelier2007) 82.8% accuracy on ETH-80 98.4% accuracy on MNIST with 3 layers cnn + svm
@article{kheradpisheh2018, 
  title={STDP-based spiking deep convolutional neural networks for object recognition},
  author={Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J and Masquelier, Timoth{\'e}e},
  journal={Neural Networks},
  volume={99},
  pages={56--67},
  year={2018},
  publisher={Elsevier}
}

% STDP + WTA 98.48% accuracy on MNIST, 75.2% accuracy on ETH-80, 71.2% accuracy on CIFAR-10, 60.1% accuracy on STL-10 - also uses hybrid architectures with early layers stdp and higher layers backprop ann - architectures: 2 stdp conv + 2 backprop fc for mnist, 3 stdp conv + 1 backprop fc for eth-80, 1 stdp conv + 3 backprop fc for cifar-10
@article{ferre2018, 
    title={Unsupervised feature learning with winner-takes-all based STDP},
    author={Ferr{\'e}, Paul and Mamalet, Franck and Thorpe, Simon J},
    journal={Frontiers in computational neuroscience},
    volume={12},
    pages={24},
    year={2018},
    publisher={Frontiers}
}

% hybrid network 2 layers stdp cnn + 3 layers spiking backprop fc 98.60% acc on mnist
@inproceedings{tavanaei2018,
  title={Training spiking convnets by stdp and gradient descent},
  author={Tavanaei, Amirhossein and Kirby, Zachary and Maida, Anthony S},
  booktitle={2018 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}

% semisupervised strategy: stdp pre-training followed by spiking backprop fine tuning - 99.28% acc on mnist with 4 layer cnn
@article{lee2018,
  title={Training deep spiking convolutional neural networks with stdp-based unsupervised pre-training followed by supervised fine-tuning},
  author={Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={435},
  year={2018},
  publisher={Frontiers}
}

% Convolution-over-time stdp. a filter is convolved over the image sliding over different patches as time progresses. If at some point a spike is triggered stdp occurs and the kernel is updated. Sliding progresses with the updated kernel for a number of iterations. Also, spatial lateral inhibitory neurons. 80.98% accuracy on mnist with just 800 training samples (81.5% with 6000) with a 50 neurons convolutional snn. 80% on caltech101 with 1730 training images from 10 classes and 100 neurons.
@article{srinivasan2018,
  title={Stdp-based unsupervised feature learning using convolution-over-time in spiking neural networks for energy-efficient neuromorphic computing},
  author={Srinivasan, Gopalakrishnan and Panda, Priyadarshini and Roy, Kaushik},
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume={14},
  number={4},
  pages={1--12},
  year={2018},
  publisher={ACM New York, NY, USA}
}

% 3-layer spiking cnn with 32, 128, 4096 kernels on mnist 98.6% with stdp and a threshold adaptation rule to adapt spike latency towards a target average latency.
@inproceedings{falez2019,
  title={Multi-layered spiking neural network with target timestamp threshold adaptation and stdp},
  author={Falez, Pierre and Tirilly, Pierre and Bilasco, Ioan Marius and Devienne, Philippe and Boulet, Pierre},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

% ensemble of 9 4-layer conv snns with rank order coding, lateral inhibition, stdp training. 99.7% accuracy on mnist, 93% on cifar10.
@article{fu2021,
  title={An ensemble unsupervised spiking neural network for objective recognition},
  author={Fu, Qiang and Dong, Hongbin},
  journal={Neurocomputing},
  volume={419},
  pages={47--58},
  year={2021},
  publisher={Elsevier}
}

% compressed and quantized snn architecture for constrained devices. trained with stdp variant with soft thresholds (delta w = w(1-w) masquelier2007) and synaptic traces, and a strengthening factor proportional to the maximum number of spikes recorded in the last time interval to boost confident updates. 92% accuracy on MNIST with 200MB model, 70% with 1MB, with an excitatory + an inhibitory fc layer.
@article{putra2022,
  title={tinySNN: Towards memory-and energy-efficient spiking neural networks},
  author={Putra, Rachmad Vidya Wicaksana and Shafique, Muhammad},
  journal={arXiv preprint arXiv:2206.08656},
  year={2022}
}


%%%%%%%%%%
% tricks
%%%%%%%%%%

% exploding/vanishing gradients - solution by gradient free methods, 2nd order regularization methods, discrete gradient propagation methods
@article{bengio1994,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

% exploding/vanishing gradients - solution by gradient clipping/gradient scaling methods, also mentions L2 regularization methods, 2nd order regularization methods
@inproceedings{pascanu2013,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}

% cross-entropy vs mse minimization
@article{kline2005,
  title={Revisiting squared-error and cross-entropy functions for training neural network classifiers},
  author={Kline, Douglas M and Berardi, Victor L},
  journal={Neural Computing \& Applications},
  volume={14},
  number={4},
  pages={310--318},
  year={2005},
  publisher={Springer}
}

% Backprop
@article{rumelhart1986,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

% BPTT
@article{werbos1990,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}

% unbiased truncated bptt with adaptive stochastic truncation
@article{tallec2017,
  title={Unbiasing truncated backpropagation through time},
  author={Tallec, Corentin and Ollivier, Yann},
  journal={arXiv preprint arXiv:1705.08209},
  year={2017}
}

% sgd
@article{bottou1998,
  title={Online learning and stochastic approximations},
  author={Bottou, L{\'e}on and others},
  journal={On-line learning in neural networks},
  volume={17},
  number={9},
  pages={142},
  year={1998}
}

% lr schedule
@inproceedings{darken1992,
  title={Learning rate schedules for faster stochastic gradient search},
  author={Darken, Christian and Chang, Joseph and Moody, John and others},
  booktitle={Neural networks for signal processing},
  volume={2},
  year={1992},
  organization={Citeseer}
}

% sgd with momentum
@article{qian1999,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

% sgd with nesterov momentum
@article{assran2020,
  title={On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings},
  author={Assran, Mahmoud and Rabbat, Michael},
  journal={arXiv preprint arXiv:2002.12414},
  year={2020}
}

% Adam
@article{kingma2014a,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% overview gradient descent optimization
@article{ruder2016,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

% bag of tricks for dnn training
@inproceedings{he2019,
  title={Bag of tricks for image classification with convolutional neural networks},
  author={He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={558--567},
  year={2019}
}

% std init
@incollection{lecun2012,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

% xavier init
@inproceedings{glorot2010,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

% he init (works better with rectifier activations). Also introduces prelu activation.
@inproceedings{he2015a,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

% init by orthonormal matrices followed by normalization of activations to unit variance. Comparable to other init schemes and also to complex pre-training init scheme applied on very deep nets. It is related to the fact that one could achieve good training (preventing exploding/vanishing gradients) by unsupervised pre-training methods based on variance normalization/reconstruction/information preservation.
@article{mishkin2015,
  title={All you need is a good init},
  author={Mishkin, Dmytro and Matas, Jiri},
  journal={arXiv preprint arXiv:1511.06422},
  year={2015}
}

% weight init strategies - weight init for general nonlinearities
@article{kumar2017,
  title={On weight initialization in deep neural networks},
  author={Kumar, Siddharth Krishna},
  journal={arXiv preprint arXiv:1704.08863},
  year={2017}
}

% ReLU activation
@inproceedings{nair2010,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Icml},
  year={2010}
}

% ReLU and SoftPlus activation
@inproceedings{glorot2011,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

% Leaky ReLU activation
@inproceedings{maas2013,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  booktitle={Proc. icml},
  volume={30},
  number={1},
  pages={3},
  year={2013},
  organization={Citeseer}
}

% ELU activation
@article{clevert2015,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}

% GELU activation, also mentions SiLU activation
@article{hendrycks2016,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

% SELU activation
@inproceedings{klambauer2017,
  title={Self-normalizing neural networks},
  author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  booktitle={Proceedings of the 31st international conference on neural information processing systems},
  pages={972--981},
  year={2017}
}

% Swish activation, aka SiLU
@article{ramachandran2017,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

% SiLU activation
@article{elfwing2018,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

% Learning activation functions as picewise linear functions by maxout
@inproceedings{goodfellow2013,
  title={Maxout networks},
  author={Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1319--1327},
  year={2013},
  organization={PMLR}
}

% Learning activation functions as picewise linear functions by learning bias and slope parameters of each piece.
@article{agostinelli2014,
  title={Learning activation functions to improve deep neural networks},
  author={Agostinelli, Forest and Hoffman, Matthew and Sadowski, Peter and Baldi, Pierre},
  journal={arXiv preprint arXiv:1412.6830},
  year={2014}
}

% Learning activation functions as linear combination of basis functions
@article{goyal2019,
  title={Learning Activation Functions: A new paradigm for understanding Neural Networks},
  author={Goyal, Mohit and Goyal, Rajan and Lall, Brejesh},
  journal={arXiv preprint arXiv:1906.09529},
  year={2019}
}

% softmax activation
@article{gao2017,
  title={On the properties of the softmax function with application in game theory and reinforcement learning},
  author={Gao, Bolin and Pavel, Lacra},
  journal={arXiv preprint arXiv:1704.00805},
  year={2017}
}

% Batch Norm
@article{ioffe2015, 
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

% stochastic pooling
@article{zeiler2013,
  title={Stochastic pooling for regularization of deep convolutional neural networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  journal={arXiv preprint arXiv:1301.3557},
  year={2013}
}

% softmax pooling
@inproceedings{abdel2013,
  title={Exploring convolutional neural network structures and optimization techniques for speech recognition.},
  author={Abdel-Hamid, Ossama and Deng, Li and Yu, Dong},
  booktitle={Interspeech},
  volume={11},
  pages={73--5},
  year={2013},
  organization={Citeseer}
}

% spatial pyramid pooling
@article{he2015b,
  title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={9},
  pages={1904--1916},
  year={2015},
  publisher={IEEE}
}

% deep generalized max pooling
@inproceedings{christlein2019,
  title={Deep generalized max pooling},
  author={Christlein, Vincent and Spranger, Lukas and Seuret, Mathias and Nicolaou, Anguelos and Kr{\'a}l, Pavel and Maier, Andreas},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1090--1096},
  year={2019},
  organization={IEEE}
}

% review on pooling - max pooling, avg pooling and other methods
@article{gholamalinezhad2020,
  title={Pooling Methods in Deep Neural Networks, a Review},
  author={Gholamalinezhad, Hossein and Khosravi, Hossein},
  journal={arXiv preprint arXiv:2009.07485},
  year={2020}
}

% l2 regularization weight decay
@inproceedings{krogh1992,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

% double backprop smoothing regularization
@article{drucker1992,
  title={Improving generalization performance using double backpropagation},
  author={Drucker, Harris and Le Cun, Yann},
  journal={IEEE Transactions on Neural Networks},
  volume={3},
  number={6},
  pages={991--997},
  year={1992}
}

% tangent propagation regularization: impose order 2 smoothing condition but only along desired directions (i.e. directions of invariance). Also tangent distance classifier: similar to knn, but instead of using euclidean distance, one uses the distance between the tangent planes of data points. the tangent plane is given by the local directions of invariant transformations. Hence, two points are close if there is at least a transformation that makes them close.
@incollection{simard1998,
  title={Transformation invariance in pattern recognitiontangent distance and tangent propagation},
  author={Simard, Patrice Y and LeCun, Yann A and Denker, John S and Victorri, Bernard},
  booktitle={Neural networks: tricks of the trade},
  pages={239--274},
  year={1998},
  publisher={Springer}
}

% regularization by dropout
@article{hinton2012, 
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

% regularization by dropout
@article{srivastava2014,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

% Inception-v3. Also introduces label smoothing regularization.
@inproceedings{szegedy2016,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

% low entropy penalty regularization
@article{pereyra2017,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}

% image quilting: synthetize new images by stitching together patches from other images
@inproceedings{efros2001,
  title={Image quilting for texture synthesis and transfer},
  author={Efros, Alexei A and Freeman, William T},
  booktitle={Proceedings of the 28th annual conference on Computer graphics and interactive techniques},
  pages={341--346},
  year={2001}
}

% data augmentation by cutout
@article{devries2017,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

% data augmentation by mixup
@article{zhang2017b,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

% data augmentation by manifold mixup
@inproceedings{verma2019a,
  title={Manifold mixup: Better representations by interpolating hidden states},
  author={Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={6438--6447},
  year={2019},
  organization={PMLR}
}

% cutmix: similar to mixup, but mixes patches from pairs of images instead of linear interpolation
@inproceedings{yun2019,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

% RICAP: random image cropping and patching. Data augmentation that takes four crops from distinct images and creates a new image by patching them together. Also, mixes the labels from each image.
@article{takahashi2019,
  title={Data augmentation using random image cropping and patching for deep CNNs},
  author={Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={30},
  number={9},
  pages={2917--2931},
  year={2019},
  publisher={IEEE}
}

% data augmentation by autoaugment
@inproceedings{cubuk2019,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={113--123},
  year={2019}
}

% data augmentation by randaugment
@inproceedings{cubuk2020,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={702--703},
  year={2020}
}


%%%%%%%%%%
% self-adaptive training
%%%%%%%%%%

% self-adaptive training: refine the training process adaptively by looking at validation results
@article{huang2020,
  title={Self-adaptive training: beyond empirical risk minimization},
  author={Huang, Lang and Zhang, Chao and Zhang, Hongyang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19365--19376},
  year={2020}
}


%%%%%%%%%%
% explainability and fairness
%%%%%%%%%%

% DNN visualization
@inproceedings{zeiler2014,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

% DNN visualization
@article{simonyan2013,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}

% DNN visualization
@article{yosinski2015,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}
}


%%%%%%%%%%
% architectures
%%%%%%%%%%

% AlexNet
@article{krizhevsky2012, 
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

% AlexNet single column
@article{krizhevsky2014,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

% VGG
@article{simonyan2014,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

% GoogleNet/Inception architecture
@inproceedings{szegedy2015, 
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

% ResNet
@inproceedings{he2016, 
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

% FractalNet
@article{larsson2016, 
  title={Fractalnet: Ultra-deep neural networks without residuals},
  author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  journal={arXiv preprint arXiv:1605.07648},
  year={2016}
}

% DenseNet
@inproceedings{huang2017, 
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

% Capsules
@article{sabour2017, 
    title={Dynamic Routing Between Capsules},
    author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
    journal={arXiv preprint arXiv:1710.09829},
    year={2017}
}

% odenet
@inproceedings{chen2018,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in neural information processing systems},
  pages={6571--6583},
  year={2018}
}

% PCANet 78.67% accuracy on CIFAR-10
@article{chan2015, 
  title={PCANet: A simple deep learning baseline for image classification?},
  author={Chan, Tsung-Han and Jia, Kui and Gao, Shenghua and Lu, Jiwen and Zeng, Zinan and Ma, Yi},
  journal={IEEE transactions on image processing},
  volume={24},
  number={12},
  pages={5017--5032},
  year={2015},
  publisher={IEEE}
}

% Decoupled neural interfaces (dni) and synthetic gradients
@inproceedings{jaderberg2017,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1627--1635},
  year={2017},
  organization={PMLR}
}


%%%%%%%%%%
% metric learning
%%%%%%%%%%

% siamese learning
@inproceedings{koch2015,
  title={Siamese neural networks for one-shot image recognition},
  author={Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
  booktitle={ICML deep learning workshop},
  volume={2},
  year={2015},
  organization={Lille}
}

% Triplet learning
@inproceedings{hoffer2015,
  title={Deep metric learning using triplet network},
  author={Hoffer, Elad and Ailon, Nir},
  booktitle={International Workshop on Similarity-Based Pattern Recognition},
  pages={84--92},
  year={2015},
  organization={Springer}
}


%%%%%%%%%%
% rnns
%%%%%%%%%%

% lstm
@article{hochreiter1997b,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

% gru
@article{cho2014,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

% seq2seq
@article{graves2013,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

% seq2seq
@inproceedings{sutskever2014,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}


%%%%%%%%%%
% attention
%%%%%%%%%%

% attention
@article{bahdanau2014,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

% visual attention
@inproceedings{mnih2014,
  title={Recurrent models of visual attention},
  author={Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and others},
  booktitle={Advances in neural information processing systems},
  pages={2204--2212},
  year={2014}
}

% visual attention
@article{stollenga2014,
  title={Deep networks with internal selective attention through feedback connections},
  author={Stollenga, Marijn F and Masci, Jonathan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={3545--3553},
  year={2014}
}

% visual attention for image caption generation
@inproceedings{xu2015,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015}
}

% transformers
@inproceedings{vaswani2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

% bert
@inproceedings{devlin2019,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

% ViT visual transformer
@article{dosovitskiy2020,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

% transformers for visual attention
@article{wu2020,
  title={Visual transformers: Token-based image representation and processing for computer vision},
  author={Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
  journal={arXiv preprint arXiv:2006.03677},
  year={2020}
}

% lambda attention: simplified transformer for visual attention which uses a linear attention model which achieves comparable performance with other approaches but with fewer parameters
@article{bello2021,
  title={Lambdanetworks: Modeling long-range interactions without attention},
  author={Bello, Irwan},
  journal={arXiv preprint arXiv:2102.08602},
  year={2021}
}


%%%%%%%%%%
% ensemble
%%%%%%%%%%

% adaptive mixture ensemble: different models each one specialized on a specific portion of the task space.
@article{jacobs1991,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}


%%%%%%%%%%
% adversarial
%%%%%%%%%%

% adversarial examples review
@article{akhtar2018,
  title={Threat of adversarial attacks on deep learning in computer vision: A survey},
  author={Akhtar, Naveed and Mian, Ajmal},
  journal={IEEE Access},
  volume={6},
  pages={14410--14430},
  year={2018},
  publisher={IEEE}
}

% adversarial examples review
@article{yuan2019,
  title={Adversarial examples: Attacks and defenses for deep learning},
  author={Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
  journal={IEEE transactions on neural networks and learning systems},
  volume={30},
  number={9},
  pages={2805--2824},
  year={2019},
  publisher={IEEE}
}

% adversarial examples
@article{szegedy2013,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

% adversarial examples, defending from adversarial examples by adversarial training. also, rbf networks robust against adversarial examples.
@article{goodfellow2014b, 
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

% defending from adversarial examples by training a denoising autoencoder to remove the perturbation. Also, train the autoencoder to be itself robust, thus obtaining a contractive autoencoder
@article{gu2014,
  title={Towards deep neural network architectures robust to adversarial examples},
  author={Gu, Shixiang and Rigazio, Luca},
  journal={arXiv preprint arXiv:1412.5068},
  year={2014}
}

% defending from adversarial examples by adversarial training
@article{kurakin2016a,
  title={Adversarial machine learning at scale},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.01236},
  year={2016}
}

% generating adversarial exapmles by iterative perturbation methods - adversarial examples in the physical world
@article{kurakin2016b,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1607.02533},
  year={2016}
}

% jbsm method for generating adversarial examples with minimum l0 norm (i.e. perturbing minimum number of pixels)
@inproceedings{papernot2016,
  title={The limitations of deep learning in adversarial settings},
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkay and Swami, Ananthram},
  booktitle={2016 IEEE European symposium on security and privacy (EuroS\&P)},
  pages={372--387},
  year={2016},
  organization={IEEE}
}

% deepfool method for generating adversarial examples
@inproceedings{moosavi2016,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}

% CW method for generating adversarial examples
@inproceedings{carlini2017,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 ieee symposium on security and privacy (sp)},
  pages={39--57},
  year={2017},
  organization={IEEE}
}

% Goodfellow's lecture on adversarial examples. RBF networks resist to adversarial examples but deep RBF nets suffer from gradient vanishing
@misc{goodfellow_cs231n_lecture, 
    organization = {Stanford School of Engineering},
    author = {I. Goodfellow},
    title = {CS231n Lecture 16 - Adversarial Examples and Adversarial Training},
    year = {2017},
    url = {https://youtu.be/CIfsB_EYsVI?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&t=2765}
}

% defending from adversarial examples by feature squeezing, i.e. preprocessing by quantization + smoothing. then one compares the neural activation without and with squeezing and, if the two significantly differ, we consider to have detected an adversarial examples
@article{xu2017,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv preprint arXiv:1704.01155},
  year={2017}
}

% Adversarial training with gans: train generator to generate adversarial examples, train discriminator to correctly classify samples, thus making it robust.
@article{lee2017b,
  title={Generative adversarial trainer: Defense to adversarial perturbations with gan},
  author={Lee, Hyeungill and Han, Sungyeob and Lee, Jungwoo},
  journal={arXiv preprint arXiv:1705.03387},
  year={2017}
}

% Randomized input perturbations are helpful as adversarial defenses (specifically, the authors used randomized resizing of the inputs in this case).
@article{xie2017,
  title={Mitigating adversarial effects through randomization},
  author={Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan},
  journal={arXiv preprint arXiv:1711.01991},
  year={2017}
}

% Developing robustness to adversarial examples by constraining the lipschitz constant of the network
@inproceedings{cisse2017,
  title={Parseval networks: Improving robustness to adversarial examples},
  author={Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={854--863},
  year={2017},
  organization={PMLR}
}

% training detectors to recognize adversarial examples. second round of training to make detectors themselves robust
@article{metzen2017,
  title={On detecting adversarial perturbations},
  author={Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
  journal={arXiv preprint arXiv:1702.04267},
  year={2017}
}

% training network to recognize adversarial examples by adding an extra "adversarial" class
@article{grosse2017,
  title={On the (statistical) detection of adversarial examples},
  author={Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1702.06280},
  year={2017}
}

% Adversarial defense by means of input transformations: total variation minimization for denoising and image quilting (efros2001)
@article{guo2017,
  title={Countering adversarial images using input transformations},
  author={Guo, Chuan and Rana, Mayank and Cisse, Moustapha and Van Der Maaten, Laurens},
  journal={arXiv preprint arXiv:1711.00117},
  year={2017}
}

% generating natural adversarial examples with gan
@article{zhao2017,
  title={Generating natural adversarial examples},
  author={Zhao, Zhengli and Dua, Dheeru and Singh, Sameer},
  journal={arXiv preprint arXiv:1710.11342},
  year={2017}
}

% Quantized activation functions help adversarial robustness
@article{rakin2018,
  title={Defend deep neural networks against adversarial examples via fixed and dynamic quantized activation functions},
  author={Rakin, Adnan Siraj and Yi, Jinfeng and Gong, Boqing and Fan, Deliang},
  journal={arXiv preprint arXiv:1807.06714},
  year={2018}
}

% adversarial esample detection by looking at network activations
@inproceedings{carrara2018b,
  title={Adversarial examples detection in features distance spaces},
  author={Carrara, Fabio and Becarelli, Rudy and Caldelli, Roberto and Falchi, Fabrizio and Amato, Giuseppe},
  booktitle={Proceedings of the European conference on computer vision (ECCV) workshops},
  pages={0--0},
  year={2018}
}

% adversarial example detection by looking at network activations
@article{carrara2019,
  title={Adversarial image detection in deep neural networks},
  author={Carrara, Fabio and Falchi, Fabrizio and Caldelli, Roberto and Amato, Giuseppe and Becarelli, Rudy},
  journal={Multimedia Tools and Applications},
  volume={78},
  number={3},
  pages={2815--2835},
  year={2019},
  publisher={Springer}
}

% Discretization of inputs helps adversarial robustness
@article{panda2019,
  title={Discretization based solutions for secure machine learning against adversarial attacks},
  author={Panda, Priyadarshini and Chakraborty, Indranil and Roy, Kaushik},
  journal={IEEE Access},
  volume={7},
  pages={70157--70168},
  year={2019},
  publisher={IEEE}
}

% Randomized discretization of inputs helps adversarial robustness
@inproceedings{zhang2019,
  title={Defending against whitebox adversarial attacks via randomized discretization},
  author={Zhang, Yuchen and Liang, Percy},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={684--693},
  year={2019},
  organization={PMLR}
}

% Adversarial robustness with denoising filters
@inproceedings{xie2019a,
  title={Feature denoising for improving adversarial robustness},
  author={Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan L and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={501--509},
  year={2019}
}

% Randomized smoothing: randomized perturbations of the input and ensabling of the results can provide certifiable adversarial robustness to the classifier.
@inproceedings{cohen2019,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={international conference on machine learning},
  pages={1310--1320},
  year={2019},
  organization={PMLR}
}

% adversarial example deflection - generated adversarial example is perceptually similar to true example. Uses capsule nets to develop adversarial detectors.
@article{qin2020b,
  title={Deflecting adversarial attacks},
  author={Qin, Yao and Frosst, Nicholas and Raffel, Colin and Cottrell, Garrison and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.07405},
  year={2020}
}


%%%%%%%%%%
% gan
%%%%%%%%%%

% gan review
@article{creswell2018,
  title={Generative adversarial networks: An overview},
  author={Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
  journal={IEEE signal processing magazine},
  volume={35},
  number={1},
  pages={53--65},
  year={2018},
  publisher={IEEE}
}

% gan review
@article{gui2021,
  title={A review on generative adversarial networks: Algorithms, theory, and applications},
  author={Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
  journal={IEEE transactions on knowledge and data engineering},
  year={2021},
  publisher={IEEE}
}

% gan
@inproceedings{goodfellow2014a,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

% DCGAN: Deep Convolutional GAN
@article{radford2015,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}

% gan evaluation methods
@article{theis2015,
  title={A note on the evaluation of generative models},
  author={Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
  journal={arXiv preprint arXiv:1511.01844},
  year={2015}
}

% multi-scale structural similarity (MS-SSIM) for image quality evaluation
@inproceedings{wang2003,
  title={Multiscale structural similarity for image quality assessment},
  author={Wang, Zhou and Simoncelli, Eero P and Bovik, Alan C},
  booktitle={The Thrity-Seventh Asilomar Conference on Signals, Systems \& Computers, 2003},
  volume={2},
  pages={1398--1402},
  year={2003},
  organization={Ieee}
}

% gan evaluation with inception distance 
@article{salimans2016,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

% gan evaluation with deep feature based similarity metrics
@article{dosovitskiy2016,
  title={Generating images with perceptual similarity metrics based on deep networks},
  author={Dosovitskiy, Alexey and Brox, Thomas},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

% gan evaluation with frechet inception distance
@article{heusel2017,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% problems with inception score
@article{barratt2018,
  title={A note on the inception score},
  author={Barratt, Shane and Sharma, Rishi},
  journal={arXiv preprint arXiv:1801.01973},
  year={2018}
}

% effectiveness of neural feature based image quality scores and correlation with human judgement
@inproceedings{zhang2018a,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

% Unrolled gans: stabilizes training by correcting gradients for the generator by differentiating through the unrolled discriminator
@article{metz2016,
  title={Unrolled generative adversarial networks},
  author={Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1611.02163},
  year={2016}
}

% Energy-based gan
@article{zhao2016,
  title={Energy-based generative adversarial network},
  author={Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
  journal={arXiv preprint arXiv:1609.03126},
  year={2016}
}

% vgan: gan implemented as energy based model. A relationship between generator-discriminator pairs and variational bounds allows to train energy based models without markov chain monte carlo sampling.
@article{zhai2016,
  title={Generative adversarial networks as variational training of energy based models},
  author={Zhai, Shuangfei and Cheng, Yu and Feris, Rogerio and Zhang, Zhongfei},
  journal={arXiv preprint arXiv:1611.01799},
  year={2016}
}

% f-gan
@inproceedings{nowozin2016,
  title={f-gan: Training generative neural samplers using variational divergence minimization},
  author={Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  booktitle={Advances in neural information processing systems},
  pages={271--279},
  year={2016}
}

% wgan
@article{arjovsky2017,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

% SN-GAN: GAN training regularized with spectral normalization
@article{miyato2018,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}

% Stackelberg gan
@article{zhang2018b,
  title={Stackelberg gan: Towards provable minimax equilibrium via multi-generator architectures},
  author={Zhang, Hongyang and Xu, Susu and Jiao, Jiantao and Xie, Pengtao and Salakhutdinov, Ruslan and Xing, Eric P},
  journal={arXiv preprint arXiv:1811.08010},
  year={2018}
}

% convergence of stackelberg games
@article{fiez2019,
  title={Convergence of learning dynamics in stackelberg games},
  author={Fiez, Tanner and Chasnov, Benjamin and Ratliff, Lillian J},
  journal={arXiv preprint arXiv:1906.01217},
  year={2019}
}

% VQGAN: Vector Quantization GAN (VQ-GAN), GAN model where the latenr codes are encoded through VQ as in VQ-VAE (oord2017)
@inproceedings{esser2021,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

% Problems in gans: mode collapse and training instability. Practical solutions and regularizers to address these problems.
@article{arjovsky2017,
  title={Towards principled methods for training generative adversarial networks},
  author={Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.04862},
  year={2017}
}

% cycle-consistency gan
@inproceedings{zhu2017a,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}

% bicycle gan: cycle consistency gan with encoder that inverts the generation process to address the mode collapse problem
@article{zhu2017b,
  title={Toward multimodal image-to-image translation},
  author={Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% BiGAN: Bidirectional gans to address the mode collapse problem with additional "encoder" model that inverts the generation process
@article{donahue2016,
  title={Adversarial feature learning},
  author={Donahue, Jeff and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
  journal={arXiv preprint arXiv:1605.09782},
  year={2016}
}

% Trains a generator together with an inference network that is required to reconstruct the latent variables from the generated sample, which allows to address the mode collapse problem.
@article{dumoulin2016,
  title={Adversarially learned inference},
  author={Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  journal={arXiv preprint arXiv:1606.00704},
  year={2016}
}

% InfoGAN: Address mode collapse problem by mutual information maximization between latent variables and generated samples
@article{chen2016,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

% Regularizers to address the mode collapse problem in GANs
@article{che2016,
  title={Mode regularized generative adversarial networks},
  author={Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
  journal={arXiv preprint arXiv:1612.02136},
  year={2016}
}

% VEEGAN: addressing mode collapse problem with additional "encoder" model that inverts the generation process on randomly generated samples in data space
@article{srivastava2017,
  title={Veegan: Reducing mode collapse in gans using implicit variational learning},
  author={Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U and Sutton, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Regularizer to enhance diversity in the generation process to address the mode collapse problem
@article{yang2019,
  title={Diversity-sensitive conditional generative adversarial networks},
  author={Yang, Dingdong and Hong, Seunghoon and Jang, Yunseok and Zhao, Tianchen and Lee, Honglak},
  journal={arXiv preprint arXiv:1901.09024},
  year={2019}
}

% Mode seeking regularization in gan to address the mode collapse problem
@inproceedings{mao2019,
  title={Mode seeking generative adversarial networks for diverse image synthesis},
  author={Mao, Qi and Lee, Hsin-Ying and Tseng, Hung-Yu and Ma, Siwei and Yang, Ming-Hsuan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1429--1437},
  year={2019}
}

% GMAN: gan with multiple discriminators
@article{durugkar2016,
  title={Generative multi-adversarial networks},
  author={Durugkar, Ishan and Gemp, Ian and Mahadevan, Sridhar},
  journal={arXiv preprint arXiv:1611.01673},
  year={2016}
}

% D2GAN: dual discriminator gan to address the mode collapse problem
@article{nguyen2017,
  title={Dual discriminator generative adversarial nets},
  author={Nguyen, Tu and Le, Trung and Vu, Hung and Phung, Dinh},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Triple GAN: generator, discriminator and classifier. Conditional image synthesis where the classifier recognizes the image class, while the discriminator simply distinguishes real vs fake image-label pairs.
@article{li2017,
  title={Triple generative adversarial nets},
  author={Li, Chongxuan and Xu, Taufik and Zhu, Jun and Zhang, Bo},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% multi-generator gan with mutual information
@article{li2021,
  title={Multi-generator GAN learning disconnected manifolds with mutual information},
  author={Li, Wei and Liang, Zhixuan and Neuman, Julian and Chen, Jinlin and Cui, Xiaohui},
  journal={Knowledge-Based Systems},
  volume={212},
  pages={106513},
  year={2021},
  publisher={Elsevier}
}

% MCL-GAN: gan with multiple discrminators based on multiple choice learning
@article{choi2022,
  title={Mcl-gan: Generative adversarial networks with multiple specialized discriminators},
  author={Choi, Jinyoung and Han, Bohyung},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29597--29609},
  year={2022}
}

% Mixture GAN multi-generator gan model
@inproceedings{hoang2018,
  title={MGAN: Training generative adversarial nets with multiple generators},
  author={Hoang, Quan and Nguyen, Tu Dinh and Le, Trung and Phung, Dinh},
  booktitle={Proceedings of the International Conference on Learning Representations, Vancouver, BC, Canada},
  volume={30},
  year={2018}
}

% MAD-GAN multi generator gan model
@inproceedings{ghosh2018,
  title={Multi-agent diverse generative adversarial networks},
  author={Ghosh, Arnab and Kulharia, Viveka and Namboodiri, Vinay P and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8513--8521},
  year={2018}
}

% Disconnected manifold learning with multi-generator gan. Also introduces a mechanism for automatically tuning the priors over generators and introducing new generators.
@article{khayatkhoei2018,
  title={Disconnected manifold learning for generative adversarial networks},
  author={Khayatkhoei, Mahyar and Singh, Maneesh K and Elgammal, Ahmed},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

% gan-tree multi-generator model
@inproceedings{kundu2019,
  title={Gan-tree: An incrementally learned hierarchical generative framework for multi-modal data distributions},
  author={Kundu, Jogendra Nath and Gor, Maharshi and Agrawal, Dakshit and Babu, R Venkatesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8191--8200},
  year={2019}
}

% MIX+GAN: multiple generators and discriminators
@inproceedings{arora2017,
  title={Generalization and equilibrium in generative adversarial nets (gans)},
  author={Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={224--232},
  year={2017},
  organization={PMLR}
}

% class-splitting gan: an intresting idea to incorporate clustering with gan training. during gan training, clustering automatically identifies regions in the data space, which can then be used as a conditional variable in the generation process, even in absence of a supervision variable.
@article{grinblat2017,
  title={Class-splitting generative adversarial networks},
  author={Grinblat, Guillermo L and Uzal, Lucas C and Granitto, Pablo M},
  journal={arXiv preprint arXiv:1709.07359},
  year={2017}
}

% Cluster gan: an inverse generator mapping maps samples back to latent space. Discovers clusters in latent space.
@inproceedings{mukherjee2019,
  title={Clustergan: Latent space clustering in generative adversarial networks},
  author={Mukherjee, Sudipto and Asnani, Himanshu and Lin, Eugene and Kannan, Sreeram},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4610--4617},
  year={2019}
}

% DOT: refine samples generated by gans thrugh gradient flow from discriminator
@article{tanaka2019,
  title={Discriminator optimal transport},
  author={Tanaka, Akinori},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% DDLS: refine samples generated by gans through gradient flow from discriminator
@article{che2020,
  title={Your gan is secretly an energy-based model and you should use discriminator driven latent sampling},
  author={Che, Tong and Zhang, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12275--12287},
  year={2020}
}

% DGF: refine samples generated by gans through gradient flow from discriminator
@article{ansari2020,
  title={Refining deep generative models via discriminator gradient flow},
  author={Ansari, Abdul Fatir and Ang, Ming Liang and Soh, Harold},
  journal={arXiv preprint arXiv:2012.00780},
  year={2020}
}

% high resolution image generation using a laplacian pyramid of gans
@article{denton2015,
  title={Deep generative image models using a laplacian pyramid of adversarial networks},
  author={Denton, Emily L and Chintala, Soumith and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

% Progressive growing gans for high resolution image synthesis: start with few layers during training that generate low resolution images and progressively add layer that increase resolution
@article{karras2017,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

% StackGAN: high resolution image synthesis through multiple stages of generators, each generating an image at a progressively higher resolution
@inproceedings{zhang2017,
  title={Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks},
  author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5907--5915},
  year={2017}
}

% StackGAN++: extension of StackGAN that also uses a tree-like multi-generator structure
@article{zhang2018c,
  title={Stackgan++: Realistic image synthesis with stacked generative adversarial networks},
  author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1947--1962},
  year={2018},
  publisher={IEEE}
}

% BigGAN: Large scale GAN for high resolution image synthesis
@article{brock2018,
  title={Large scale GAN training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}

% conditional gan
@article{mirza2014,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

% Conditional image generation with gan
@inproceedings{nguyen2017,
  title={Plug \& play generative networks: Conditional iterative generation of images in latent space},
  author={Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4467--4477},
  year={2017}
}

% conditional gan for natural image synthesis
@inproceedings{odena2017,
  title={Conditional image synthesis with auxiliary classifier gans},
  author={Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  booktitle={International conference on machine learning},
  pages={2642--2651},
  year={2017},
  organization={PMLR}
}

% Conditional gan conditioned on segmentation maps for image generation
@inproceedings{wang2018,
  title={High-resolution image synthesis and semantic manipulation with conditional gans},
  author={Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8798--8807},
  year={2018}
}

% CG-GAN: Contrastive Disentanglement GAN, disentangling factors of variations in gans with a contrastive objective: maximizing intra-class similarity of latent representations minimizing inter-class similarity
@article{pan2021,
  title={Contrastive disentanglement in generative adversarial networks},
  author={Pan, Lili and Tang, Peijun and Chen, Zhiyong and Xu, Zenglin},
  journal={arXiv preprint arXiv:2103.03636},
  year={2021}
}


%%%%%%%%%%
% autoencoders
%%%%%%%%%%

% autoencoder
@article{hinton2006,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

% autoencoder with sparsifying nonlinearity on hidden representations
@article{ranzato2007,
  title={Efficient learning of sparse representations with an energy-based model},
  author={Ranzato, Marc and Poultney, Christopher and Chopra, Sumit and LeCun, Yann and others},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={1137},
  year={2007},
  publisher={MIT; 1998}
}

% denoising autoencoder
@inproceedings{vincent2008,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008}
}

% stacked denoising autoencoders
@article{vincent2010,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}

% SAE - sparse autoencoder (sparsity penalty on hidden representations)
@article{ng2011,
  title={Sparse autoencoder},
  author={Ng, Andrew and others},
  journal={CS294A Lecture notes},
  volume={72},
  number={2011},
  pages={1--19},
  year={2011}
}

% Contractive autoencoders: autoencoders regularized by order 2 smoothing condition
@inproceedings{rifai2011a,
  title={Contractive auto-encoders: Explicit invariance during feature extraction},
  author={Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  booktitle={Icml},
  year={2011}
}

% Contractive autoencoders + tangent propagation. In this case, the directions selected for tangent propagation regularization are the tangent directions obtained from the data manifold, given by the jacobian of the contractive autoencoder.
@article{rifai2011b,
  title={The manifold tangent classifier},
  author={Rifai, Salah and Dauphin, Yann N and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={2294--2302},
  year={2011}
}

% VAE - variational autoencoder
@article{kingma2013,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

% k-sparse autoencoder - sparse autoencoder with sparsity achieved by taking only top-k activations in hidden representation
@article{makhzani2013,
  title={K-sparse autoencoders},
  author={Makhzani, Alireza and Frey, Brendan},
  journal={arXiv preprint arXiv:1312.5663},
  year={2013}
}

% WTA autoencoder - Layerwise trained autoencoder with WTA nonlinearities
@article{makhzani2015a,
  title={Winner-take-all autoencoders},
  author={Makhzani, Alireza and Frey, Brendan J},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

% adversarial autoencoders (AAE) - disentangling with gan
@article{makhzani2015b,
  title={Adversarial autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv preprint arXiv:1511.05644},
  year={2015}
}

% ladder network
@incollection{valpola2015,
  title={From neural PCA to deep unsupervised learning},
  author={Valpola, Harri},
  booktitle={Advances in independent component analysis and learning machines},
  pages={143--171},
  year={2015},
  publisher={Elsevier}
}

% beta-VAE
@article{higgins2016,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2016}
}

% VAE-GAN: Combination of VAE encoding-decoding and GAN adversarial learning of similarity metric between samples
@inproceedings{larsen2016,
  title={Autoencoding beyond pixels using a learned similarity metric},
  author={Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
  booktitle={International conference on machine learning},
  pages={1558--1566},
  year={2016},
  organization={PMLR}
}

% formula to express elbo in terms of mutual information and disentanglement
@inproceedings{hoffman2016,
  title={Elbo surgery: yet another way to carve up the variational evidence lower bound},
  author={Hoffman, Matthew D and Johnson, Matthew J},
  booktitle={Workshop in Advances in Approximate Bayesian Inference, NIPS},
  volume={1},
  pages={2},
  year={2016}
}

% VQ-VAE: VAE with vector quantization of the latent representations
@article{oord2017,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% AVB: vae generalization to arbitrary latent distribution through adversarial loss 
@inproceedings{mescheder2017,
  title={Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks},
  author={Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  booktitle={International conference on machine learning},
  pages={2391--2400},
  year={2017},
  organization={PMLR}
}

% IntroVAE: like vae-gan, but the encoder also acts as discriminator
@article{huang2018,
  title={Introvae: Introspective variational autoencoders for photographic image synthesis},
  author={Huang, Huaibo and He, Ran and Sun, Zhenan and Tan, Tieniu and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% Multi-stage VAE for high quality image generation
@inproceedings{cai2019,
  title={Multi-stage variational auto-encoders for coarse-to-fine image generation},
  author={Cai, Lei and Gao, Hongyang and Ji, Shuiwang},
  booktitle={Proceedings of the 2019 SIAM International Conference on Data Mining},
  pages={630--638},
  year={2019},
  organization={SIAM}
}

% RQ-VAE: Residual Quantization VAE. Like VQ-VAE but with residual/additive quantization: latent code is quantized and then the residual is again quantized iteratively for improved precision. Also, RQ-Transformer is introduced to predict the residual quantization in one shot rather than iteratively for improved performance.
@inproceedings{lee2022,
  title={Autoregressive image generation using residual quantization},
  author={Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11523--11532},
  year={2022}
}

% Discovering and disentangling factors of variation in VAE with cross covaraince (xCOV) penalty.
@article{cheung2014,
  title={Discovering hidden factors of variation in deep networks},
  author={Cheung, Brian and Livezey, Jesse A and Bansal, Arjun K and Olshausen, Bruno A},
  journal={arXiv preprint arXiv:1412.6583},
  year={2014}
}

% factor-vae - vae + disentanglement penalty estimated through gan
@article{kim2018b,
  title={Disentangling by factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  journal={arXiv preprint arXiv:1802.05983},
  year={2018}
}

% sVAE: Symmetric VAE using symmetric divergence. Connections with generative models and mode collapse problem.
@inproceedings{chen2018,
  title={Symmetric variational autoencoder and connections to adversarial learning},
  author={Chen, Liqun and Dai, Shuyang and Pu, Yunchen and Zhou, Erjin and Li, Chunyuan and Su, Qinliang and Chen, Changyou and Carin, Lawrence},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={661--669},
  year={2018},
  organization={PMLR}
}

% InfoVAE
@inproceedings{zhao2019,
  title={Infovae: Balancing learning and inference in variational autoencoders},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5885--5892},
  year={2019}
}

% Disentanglement without inductive biases is impossible
@inproceedings{locatello2019,
  title={Challenging common assumptions in the unsupervised learning of disentangled representations},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle={international conference on machine learning},
  pages={4114--4124},
  year={2019},
  organization={PMLR}
}

% Disentanglement with limited supervision
@article{locatello2019,
  title={Disentangling factors of variation using few labels},
  author={Locatello, Francesco and Tschannen, Michael and Bauer, Stefan and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  journal={arXiv preprint arXiv:1905.01258},
  year={2019}
}

% Disentanglement with based on pairs of images and weak supervision
@inproceedings{locatello2020,
  title={Weakly-supervised disentanglement without compromises},
  author={Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6348--6359},
  year={2020},
  organization={PMLR}
}

% Contrastive VAE: learning a latent distribution that better fits the modeled data through contrastive InfoNCE learning. Most of the latent space region in VAEs is actually mapped to insignificant images. Significant images only cover a subset of the latent space. The idea is to adapt the prior distribution in latent space, through contrastive learning, to match the actual distribution of samples latent representations.
@article{aneja2021,
  title={A contrastive learning approach for training variational autoencoder priors},
  author={Aneja, Jyoti and Schwing, Alex and Kautz, Jan and Vahdat, Arash},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={480--493},
  year={2021}
}


%%%%%%%%%%
% detseg
%%%%%%%%%%

% semi-supervised segmentation with perturbation consistency and mutual information regularization
@article{wu2023,
  title={Perturbation consistency and mutual information regularization for semi-supervised semantic segmentation},
  author={Wu, Yulin and Liu, Chang and Chen, Lei and Zhao, Dong and Zheng, Qinghe and Zhou, Hongchao},
  journal={Multimedia Systems},
  volume={29},
  number={2},
  pages={511--523},
  year={2023},
  publisher={Springer}
}


%%%%%%%%%%
% im2im
%%%%%%%%%%

% conditional vae-gan based architecture for neural photo editing. also introduces orthogonal regularization
@article{brock2016,
  title={Neural photo editing with introspective adversarial networks},
  author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  journal={arXiv preprint arXiv:1609.07093},
  year={2016}
}

% image super resolution with total variation regularization
@inproceedings{babacan2008,
  title={Total variation super resolution using a variational approach},
  author={Babacan, S Derin and Molina, Rafael and Katsaggelos, Aggelos K},
  booktitle={2008 15th IEEE International Conference on Image Processing},
  pages={641--644},
  year={2008},
  organization={IEEE}
}

% image superresolution with gan
@inproceedings{ledig2017,
  title={Photo-realistic single image super-resolution using a generative adversarial network},
  author={Ledig, Christian and Theis, Lucas and Husz{\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4681--4690},
  year={2017}
}

% pix2pix: conditional gan for general purpose image to image translation
@inproceedings{isola2017,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

% image restoration with deep image priors (DIP): untrained generator is tuned on a single given image. You don't need a supervised dataset of corrputed and restored image pairs. the tuning process minimizes the error |H(y) - x|^2 + R(y) where H(y) is a corruption of y, x is the input image, and y is the generated restored image. R(y) is a regularizer. In DIP, the regularizer is implicitly given by the underlying architecture.
@inproceedings{ulyanov2018,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9446--9454},
  year={2018}
}

% image restoration with deep image priors plus total variation regularization
@inproceedings{liu2019b,
  title={Image restoration using total variation regularized deep image prior},
  author={Liu, Jiaming and Sun, Yu and Xu, Xiaojian and Kamilov, Ulugbek S},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7715--7719},
  year={2019},
  organization={Ieee}
}


%%%%%%%%%%
% multi-view learning, context learning, contrastive learning
%%%%%%%%%%

% context learning in nlp - word2vec
@article{mikolov2013a,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

% context learning in nlp - word2vec
@inproceedings{mikolov2013b,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

% context learning in speech (temporal coherence)
@inproceedings{carlin2011,
  title={Exploiting temporal coherence in speech for data-driven feature extraction},
  author={Carlin, Michael A and Elhilali, Mounya},
  booktitle={2011 45th Annual Conference on Information Sciences and Systems},
  pages={1--5},
  year={2011},
  organization={IEEE}
}

% context learning in speech (temporal coherence)
@article{synnaeve2016,
  title={A temporal coherence loss function for learning unsupervised acoustic embeddings},
  author={Synnaeve, Gabriel and Dupoux, Emmanuel},
  journal={Procedia Computer Science},
  volume={81},
  pages={95--100},
  year={2016},
  publisher={Elsevier}
}

% context learning in speech (temporal coherence) - wav2vec
@article{schneider2019,
  title={wav2vec: Unsupervised pre-training for speech recognition},
  author={Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  journal={arXiv preprint arXiv:1904.05862},
  year={2019}
}

% context learning in video (temporal coherence) with imax
@inproceedings{becker1993,
  title={Learning to categorize objects using temporal coherence},
  author={Becker, Suzanna},
  booktitle={Advances in neural information processing systems},
  pages={361--368},
  year={1993}
}

% context learning in video (temporal coherence)
@inproceedings{mobahi2009,
  title={Deep learning from temporal coherence in video},
  author={Mobahi, Hossein and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={737--744},
  year={2009}
}

% context learning in video (temporal coherence)
@inproceedings{zou2011,
  title={Unsupervised learning of visual invariance with temporal coherence},
  author={Zou, Will Y and Ng, Andrew Y and Yu, Kai},
  booktitle={NIPS 2011 workshop on deep learning and unsupervised feature learning},
  volume={3},
  year={2011}
}

% context learning in video (temporal coherence)
@inproceedings{springenberg2012,
  title={Learning temporal coherent features through life-time sparsity},
  author={Springenberg, Jost Tobias and Riedmiller, Martin},
  booktitle={International Conference on Neural Information Processing},
  pages={347--356},
  year={2012},
  organization={Springer}
}

% context learning in video (temporal coherence) - connection context learning, metric learning. sfa
@inproceedings{goroshin2015,
  title={Unsupervised learning of spatiotemporally coherent metrics},
  author={Goroshin, Ross and Bruna, Joan and Tompson, Jonathan and Eigen, David and LeCun, Yann},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4086--4093},
  year={2015}
}


%%%%%%%%%%
% semisupervised learning and scarce data
%%%%%%%%%%

% Cost of collecting large amount of training data
@article{roh2019,
  title={A survey on data collection for machine learning: a big data-ai integration perspective},
  author={Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={4},
  pages={1328--1347},
  year={2019},
  publisher={IEEE}
}

% semisupervised learning by minimum entropy
@article{grandvalet2005,
  title={Semi-supervised learning by entropy minimization.},
  author={Grandvalet, Yves and Bengio, Yoshua and others},
  journal={CAP},
  volume={367},
  pages={281--296},
  year={2005}
}

% manifold regularization semi-supervised learning
@article{belkin2006,
  title={Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.},
  author={Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  journal={Journal of machine learning research},
  volume={7},
  number={11},
  year={2006}
}

% semisupervised learning - unsupervised pre-training - also, stacked autoencoders
@article{larochelle2009,
  title={Exploring strategies for training deep neural networks.},
  author={Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Lamblin, Pascal},
  journal={Journal of machine learning research},
  volume={10},
  number={1},
  year={2009}
}

% semisupervised learning with manifold embedding
@incollection{weston2012,
  title={Deep learning via semi-supervised embedding},
  author={Weston, Jason and Ratle, Fr{\'e}d{\'e}ric and Mobahi, Hossein and Collobert, Ronan},
  booktitle={Neural networks: Tricks of the trade},
  pages={639--655},
  year={2012},
  publisher={Springer}
}

% semisupervised learning with generative models
@article{kingma2014b,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={3581--3589},
  year={2014}
}

% semisupervised learning with ladder network
@inproceedings{rasmus2015,
  title={Semi-supervised learning with ladder networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  booktitle={Advances in neural information processing systems},
  pages={3546--3554},
  year={2015}
}

% semisupervised learning with autoencoders
@inproceedings{zhang2016,
  title={Augmenting supervised neural networks with unsupervised objectives for large-scale image classification},
  author={Zhang, Yuting and Lee, Kibok and Lee, Honglak},
  booktitle={International conference on machine learning},
  pages={612--621},
  year={2016}
}

% semisupervised learning with gans
@article{odena2016,
  title={Semi-supervised learning with generative adversarial networks},
  author={Odena, Augustus},
  journal={arXiv preprint arXiv:1606.01583},
  year={2016}
}

% semisupervised learning with simclr contrastive pre-training
@article{chen2020b,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22243--22255},
  year={2020}
}

% semisupervised learning with pseudo-labels - use model prediction as pseudo label, equivalent to entropy minimization
@inproceedings{lee2013,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013}
}

% semisupervised learning with temporal ensambling - impose consistency between network output and exponential moving average of network outputs for the same input over previous epochs
@article{laine2016,
  title={Temporal ensembling for semi-supervised learning},
  author={Laine, Samuli and Aila, Timo},
  journal={arXiv preprint arXiv:1610.02242},
  year={2016}
}

% semisupervised learning with mean teacher - impose consistency between network output and teacher output, where teacher is exponential moving average of models over previous epochs
@article{tarvainen2017,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={arXiv preprint arXiv:1703.01780},
  year={2017}
}

% semisupervised learning with dual student - impose consistency between outputs of two networks
@inproceedings{ke2019,
  title={Dual student: Breaking the limits of the teacher in semi-supervised learning},
  author={Ke, Zhanghan and Wang, Daoye and Yan, Qiong and Ren, Jimmy and Lau, Rynson WH},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6728--6736},
  year={2019}
}

% semisupervised learning with virtual adversarial training - impose consistency between responses to normal and perturbed sample, where perturbation is computed adversarially
@article{miyato2018,
  title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1979--1993},
  year={2018},
  publisher={IEEE}
}

% semisupervised learning with label propagation - use graph knn to compute pseudo labels
@inproceedings{iscen2019,
  title={Label propagation for deep semi-supervised learning},
  author={Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ondrej},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5070--5079},
  year={2019}
}

% semisupervised learning with ict - combine data with mixup and impose consistency between prediction of interpolation and interpolation of predictions
@article{verma2019b,
  title={Interpolation consistency training for semi-supervised learning},
  author={Verma, Vikas and Kawaguchi, Kenji and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1903.03825},
  year={2019}
}

% semisupervised learning with advanced data augmentation strategies and impose consistency between responses to differently augmented inputs
@article{xie2019b,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.12848},
  year={2019}
}

% semisupervised learning with mixmatch - augment data with mixup and regular data augmentation
@article{berthelot2019a,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  journal={arXiv preprint arXiv:1905.02249},
  year={2019}
}

% semisupervised learning with remixmatch - add distribution alignment and augmentation anchoring
@article{berthelot2019b,
  title={Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring},
  author={Berthelot, David and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Sohn, Kihyuk and Zhang, Han and Raffel, Colin},
  journal={arXiv preprint arXiv:1911.09785},
  year={2019}
}

% semisupervised learning with fixmatch - impose consistency between responses to differently augmented versions of same input (i.e. augmentation anchoring), and also use only high confidence pseudo-labels
@article{sohn2020,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  journal={arXiv preprint arXiv:2001.07685},
  year={2020}
}

% confirmation bias in semi-supervised learning with pseudo-labels - use soft pseudo-labels and minimum number of labeled samples per batch
@inproceedings{arazo2020,
  title={Pseudo-labeling and confirmation bias in deep semi-supervised learning},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and OConnor, Noel E and McGuinness, Kevin},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

% semisupervised learning with confidence threshold on pseudo-labels
@inproceedings{xu2021,
  title={Dash: Semi-supervised learning with dynamic thresholding},
  author={Xu, Yi and Shang, Lei and Ye, Jinxing and Qian, Qi and Li, Yu-Feng and Sun, Baigui and Li, Hao and Jin, Rong},
  booktitle={International Conference on Machine Learning},
  pages={11525--11536},
  year={2021},
  organization={PMLR}
}

% semisupervised learning with differentiable teacher and student as a stackelberg game
@article{zuo2021,
  title={Self-training with differentiable teacher},
  author={Zuo, Simiao and Yu, Yue and Liang, Chen and Jiang, Haoming and Er, Siawpeng and Zhang, Chao and Zhao, Tuo and Zha, Hongyuan},
  journal={arXiv preprint arXiv:2109.07049},
  year={2021}
}

% semisupervised learning with simple - mixup between similar samples and impose consistency
@inproceedings{hu2021,
  title={SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification},
  author={Hu, Zijian and Yang, Zhengyu and Hu, Xuefeng and Nevatia, Ram},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15099--15108},
  year={2021}
}

% semisupervised learning with comatch: fixmatch + contrastive regularization
@inproceedings{li2021,
  title={Comatch: Semi-supervised learning with contrastive graph regularization},
  author={Li, Junnan and Xiong, Caiming and Hoi, Steven CH},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9475--9484},
  year={2021}
}

% semisupervised learning with laplacenet - use graph-based pseudo labels, mixup and advanced data augmentation
@article{sellars2021,
  title={LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification},
  author={Sellars, Philip and Aviles-Rivero, Angelica I and Sch{\"o}nlieb, Carola-Bibiane},
  journal={arXiv preprint arXiv:2106.04527},
  year={2021}
}

% semisupervised learning with class-ambiguous data, i.e. a sort of mixup where mixed data are obtained by combining regions from different images, and an attention regularization mechanism.
@article{huo2022,
  title={Attention regularized semi-supervised learning with class-ambiguous data for image classification},
  author={Huo, Xiaoyang and Zeng, Xiangping and Wu, Si and Wong, Hau-San},
  journal={Pattern Recognition},
  volume={129},
  pages={108727},
  year={2022},
  publisher={Elsevier}
}


%%%%%%%%%%
% low shot learning
%%%%%%%%%%

% low shot learning
@inproceedings{socher2013,
  title={Zero-shot learning through cross-modal transfer},
  author={Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew},
  booktitle={Advances in neural information processing systems},
  pages={935--943},
  year={2013}
}

% low shot learning
@inproceedings{vinyals2016,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  booktitle={Advances in neural information processing systems},
  pages={3630--3638},
  year={2016}
}

% low shot learning
@article{santoro2016,
  title={One-shot learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1605.06065},
  year={2016}
}

% low shot learning
@article{ravi2016,
  title={Optimization as a model for few-shot learning},
  author={Ravi, Sachin and Larochelle, Hugo},
  year={2016}
}

% low shot learning
@inproceedings{snell2017,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  booktitle={Advances in neural information processing systems},
  pages={4077--4087},
  year={2017}
}

% low shot learning
@inproceedings{sung2018,
  title={Learning to compare: Relation network for few-shot learning},
  author={Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip HS and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1199--1208},
  year={2018}
}

% multi-phase sampling for low shot learning: train on meta-generated tasks which sill resemble test-time tasks
@article{zhou2022,
  title={Few-shot class-incremental learning by sampling multi-phase tasks},
  author={Zhou, Da-Wei and Ye, Han-Jia and Ma, Liang and Xie, Di and Pu, Shiliang and Zhan, De-Chuan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

% forward compatible class incremental low shot learning
@inproceedings{zhou2022,
  title={Forward compatible few-shot class-incremental learning},
  author={Zhou, Da-Wei and Wang, Fu-Yun and Ye, Han-Jia and Ma, Liang and Pu, Shiliang and Zhan, De-Chuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9046--9056},
  year={2022}
}


%%%%%%%%%%
% transfer learning and domain adaptation
%%%%%%%%%%

% Transfer Learning
@article{yosinski2014,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={arXiv preprint arXiv:1411.1792},
  year={2014}
}

% Maximum Mean Discrepancy (MMD) to measure alignment between distributions
@article{gretton2006,
  title={A kernel method for the two-sample-problem},
  author={Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte and Sch{\"o}lkopf, Bernhard and Smola, Alex},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}

% Domain adaptation by finding subspace that minimizes MMD between source and target distributions
@article{pan2010,
  title={Domain adaptation via transfer component analysis},
  author={Pan, Sinno Jialin and Tsang, Ivor W and Kwok, James T and Yang, Qiang},
  journal={IEEE transactions on neural networks},
  volume={22},
  number={2},
  pages={199--210},
  year={2010},
  publisher={IEEE}
}

% Deep Adaptation Networks (DAN): domain adaptation by minimizing MMD in deep representation space
@inproceedings{long2015,
  title={Learning transferable features with deep adaptation networks},
  author={Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={97--105},
  year={2015},
  organization={PMLR}
}

% Deep Transfer Network (DTN): domain adaptation like DAN but also aligns the conditional distribution of classes given the features, in addition to the unconditional distributions of the features.
@article{zhang2015b,
  title={Deep transfer network: Unsupervised domain adaptation},
  author={Zhang, Xu and Yu, Felix Xinnan and Chang, Shih-Fu and Wang, Shengjin},
  journal={arXiv preprint arXiv:1503.00591},
  year={2015}
}

% Domain adaptation with Subspace Alignment (SA): find projection to common principal component subspace between source and target domains
@inproceedings{fernando2013,
  title={Unsupervised visual domain adaptation using subspace alignment},
  author={Fernando, Basura and Habrard, Amaury and Sebban, Marc and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2960--2967},
  year={2013}
}

% Decaf: transferring deep features for domain adaptation. Also, use deep subspace alignment to align source and target feature distributions.
@inproceedings{donahue2014,
  title={Decaf: A deep convolutional activation feature for generic visual recognition},
  author={Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={647--655},
  year={2014},
  organization={PMLR}
}

% Coral: Domain adaptation by correlation alignment, i.e. whiten source data and recolor according to target statistics
@article{sun2017,
  title={Correlation alignment for unsupervised domain adaptation},
  author={Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  journal={Domain adaptation in computer vision applications},
  pages={153--171},
  year={2017},
  publisher={Springer}
}

% Deep Coral: Coral extended to deep features for domain adaptation
@inproceedings{sun2016,
  title={Deep coral: Correlation alignment for deep domain adaptation},
  author={Sun, Baochen and Saenko, Kate},
  booktitle={Computer Vision--ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14},
  pages={443--450},
  year={2016},
  organization={Springer}
}

% mSDA: marginalized stacked denoising autoencoders for domain adaptation by learning a common representation of source and target domains
@article{chen2012,
  title={Marginalized denoising autoencoders for domain adaptation},
  author={Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Sha, Fei},
  journal={arXiv preprint arXiv:1206.4683},
  year={2012}
}

% Deep domain adaptation by adding adaptation layers in pre-trained network
@article{tzeng2014,
  title={Deep domain confusion: Maximizing for domain invariance},
  author={Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
  journal={arXiv preprint arXiv:1412.3474},
  year={2014}
}

% Domain Adversarial Neural Networks (DANN): gan training to make a generator map source domain data to target domain data, while a discriminator tries to identify the correct domain. 
@article{ajakan2014,
  title={Domain-adversarial neural networks},
  author={Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario},
  journal={arXiv preprint arXiv:1412.4446},
  year={2014}
}

% Deep domain adaptation by adding a network branch that is trained to discriminate the domain from the deep representation. A gradient reversal layer right before the extra branch makes earlier feature extraction layers train to confuse the domain discriminator, in a sort of adversarial architecture, while the original network branch, together with the same feature extraction layers, simultaneously optimize the desired objective.
@inproceedings{ganin2015,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}

% DANN extension by merging the original DANN and the gradient reversal layer and further experiments
@article{ganin2016,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

% domain adaptation/transfer with gans regularized by a constancy constraint of a target function
@article{taigman2016,
  title={Unsupervised cross-domain image generation},
  author={Taigman, Yaniv and Polyak, Adam and Wolf, Lior},
  journal={arXiv preprint arXiv:1611.02200},
  year={2016}
}

% coupled gan for cross-domain transfer and domain adaptation. Two generators generate images from two modes. Two discriminators discriminate generated images from real images from each mode. Early layers of the two generators have shared weights, while higher layers are specific for each generator. This induces the two learn a joint distribution of data from the two domains, rather than the product of the observed marginal distributions.
@article{liu2016,
  title={Coupled generative adversarial networks},
  author={Liu, Ming-Yu and Tuzel, Oncel},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

% Domain adaptation from simulated to real images with a pixel-level gan adaptation
@inproceedings{bousmalis2017,
  title={Unsupervised pixel-level domain adaptation with generative adversarial networks},
  author={Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3722--3731},
  year={2017}
}

% Domain adaptation from simulated to real images with a pixel-level and feature-level gan adaptation
@inproceedings{shrivastava2017,
  title={Learning from simulated and unsupervised images through adversarial training},
  author={Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Joshua and Wang, Wenda and Webb, Russell},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2107--2116},
  year={2017}
}

% ADDA: domain adaptation by mapping source data to target data while training a discriminator to distinguish the original domain directly in pixel space
@inproceedings{tzeng2017,
  title={Adversarial discriminative domain adaptation},
  author={Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7167--7176},
  year={2017}
}

% CyCADA: Domain adaptation based on gans both in feature space and pixel space by imposing cycle consistency
@inproceedings{hoffman2018,
  title={Cycada: Cycle-consistent adversarial domain adaptation},
  author={Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={1989--1998},
  year={2018},
  organization={Pmlr}
}

% Contrastive Adaptation Network: Domain adaptation by contrastive learning, by minimizing inter-domain distance, and maximizing intra-domain distance in representation space
@inproceedings{kang2019,
  title={Contrastive adaptation network for unsupervised domain adaptation},
  author={Kang, Guoliang and Jiang, Lu and Yang, Yi and Hauptmann, Alexander G},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4893--4902},
  year={2019}
}

% cVAE: Contrastive VAE approach captures "salient" features that are common between a source and a target domain in latent space. Source and target samples are mapped to latent space, and a contrastive procedure extract a common feature subspace
@article{aneja2021,
  title={A contrastive learning approach for training variational autoencoder priors},
  author={Aneja, Jyoti and Schwing, Alex and Kautz, Jan and Vahdat, Arash},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={480--493},
  year={2021}
}

% Sim-to-real domain adaptation in reinforcement learning with robust adversarial reinforcement learning. An adversary is trained to play an optimal "destabilizing" policy on the agent, so that the agent learns a more robust behavior under unexpected conditions.
@inproceedings{pinto2017,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={2817--2826},
  year={2017},
  organization={PMLR}
}

% Meta learning for sim-to-real domain adaptation in reinforcement learning
@inproceedings{arndt2020,
  title={Meta reinforcement learning for sim-to-real domain adaptation},
  author={Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
  booktitle={2020 IEEE international conference on robotics and automation (ICRA)},
  pages={2725--2731},
  year={2020},
  organization={IEEE}
}

% MetaAlign: Domain adaptation based on metalearning to align the optimization process from source domain to target domain.
@inproceedings{wei2021,
  title={Metaalign: Coordinating domain alignment and classification for unsupervised domain adaptation},
  author={Wei, Guoqiang and Lan, Cuiling and Zeng, Wenjun and Chen, Zhibo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16643--16653},
  year={2021}
}

% Domain adaptation review
@article{csurka2017,
  title={Domain adaptation for visual applications: A comprehensive survey},
  author={Csurka, Gabriela},
  journal={arXiv preprint arXiv:1702.05374},
  year={2017}
}

% Domain adaptation review
@inproceedings{zhao2020,
  title={Sim-to-real transfer in deep reinforcement learning for robotics: a survey},
  author={Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi},
  booktitle={2020 IEEE symposium series on computational intelligence (SSCI)},
  pages={737--744},
  year={2020},
  organization={IEEE}
}

% Domain adaptation review
@article{farahani2021,
  title={A brief review of domain adaptation},
  author={Farahani, Abolfazl and Voghoei, Sahar and Rasheed, Khaled and Arabnia, Hamid R},
  journal={Advances in Data Science and Information Engineering: Proceedings from ICDATA 2020 and IKE 2020},
  pages={877--894},
  year={2021},
  publisher={Springer}
}


%%%%%%%%%%
% metalearning and multi-task learning
%%%%%%%%%%

% metalearning optimizer
@inproceedings{hochreiter2001b,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={International Conference on Artificial Neural Networks},
  pages={87--94},
  year={2001},
  organization={Springer}
}

% metalearning optimizer
@inproceedings{andrychowicz2016,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  booktitle={Advances in neural information processing systems},
  pages={3981--3989},
  year={2016}
}

% hypernetworks meta-trained to generate nn weights
@article{ha2016,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

% multi-task learning with multi-head network
@article{ruder2017,
  title={An overview of multi-task learning in deep neural networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  year={2017}
}

% multi-task learning with task embedding, which can be fed as input to the conditioned network additively, multiplicatively or both
@article{perez2017,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  journal={arXiv preprint arXiv:1709.07871},
  year={2017}
}

% learning multiple tasks in a single network by learning to mask weights in a differentiable ent-to-end fashio for each tasks. Weight masks also allow to protect non-task-related weights from being updates, thus reducing catastrophic forgetting
@inproceedings{mallya2018,
  title={Piggyback: Adapting a single network to multiple tasks by learning to mask weights},
  author={Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={67--82},
  year={2018}
}

% metalearning - maml
@article{finn2017,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1703.03400},
  year={2017}
}

% Modular meta-learning, different network modules are trained on different tasks and generalization to new tasks is achieved by mixing learned modules appropriately.
@inproceedings{alet2018,
  title={Modular meta-learning},
  author={Alet, Ferran and Lozano-P{\'e}rez, Tom{\'a}s and Kaelbling, Leslie P},
  booktitle={Conference on robot learning},
  pages={856--868},
  year={2018},
  organization={PMLR}
}

% multi-task meta reinforcement learning with task embedding and shared policy (TESP).
@article{lan2019,
  title={Meta reinforcement learning with task embedding and shared policy},
  author={Lan, Lin and Li, Zhenguo and Guan, Xiaohong and Wang, Pinghui},
  journal={arXiv preprint arXiv:1905.06527},
  year={2019}
}

% Online metaearning, Follow The Meta Leader (FTML): extends MAML to online task learning settings
@inproceedings{finn2019,
  title={Online meta-learning},
  author={Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1920--1930},
  year={2019},
  organization={PMLR}
}

% Online-within-online metalearning
@article{denevi2019,
  title={Online-within-online meta-learning},
  author={Denevi, Giulia and Stamos, Dimitris and Ciliberto, Carlo and Pontil, Massimiliano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% Online Structured Meta-Learning (OSML): modularize meta-learning agent with structured trainable modules. When a new task arrive, either train a new module or fine-tune a previously trained module.
@article{yao2020,
  title={Online structured meta-learning},
  author={Yao, Huaxiu and Zhou, Yingbo and Mahdavi, Mehrdad and Li, Zhenhui Jessie and Socher, Richard and Xiong, Caiming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6779--6790},
  year={2020}
}

% metalearning survey
@article{hospedales2020,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2004.05439},
  year={2020}
}


%%%%%%%%%%
% memory nets
%%%%%%%%%%

% neural turing machines
@article{graves2014,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

% memory networks
@article{weston2014,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal={arXiv preprint arXiv:1410.3916},
  year={2014}
}

% memory networks
@inproceedings{sukhbaatar2015,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle={Advances in neural information processing systems},
  pages={2440--2448},
  year={2015}
}

% neural turing machine
@article{graves2016,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}

% recurrent entity networks - memory augmented neural nets where memories are a stack of rnns which can be accessed in parallel
@article{henaff2016,
  title={Tracking the world state with recurrent entity networks},
  author={Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
  journal={arXiv preprint arXiv:1612.03969},
  year={2016}
}

% memory nets with gating shortcurts for better backprop
@inproceedings{liu2017b,
  title={Gated end-to-end memory networks},
  author={Liu, Fei and Perez, Julien},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages={1--10},
  year={2017}
}

% metalearning to use external memories for storing learned programs (i.e. weight configurations)
@inproceedings{schlag2017,
  title={Gated fast weights for on-the-fly neural program generation},
  author={Schlag, Imanol and Schmidhuber, J{\"u}rgen},
  booktitle={NIPS Metalearning Workshop},
  year={2017}
}

% metalearning to use external memories for storing learned programs (i.e. weight configurations)
@article{le2019,
  title={Neural stored-program memory},
  author={Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  journal={arXiv preprint arXiv:1906.08862},
  year={2019}
}

% metalearning to use external memories for storing learned programs (i.e. weight configurations)
@article{le2020a,
  title={Neurocoder: Learning General-Purpose Computation Using Stored Neural Programs},
  author={Le, Hung and Venkatesh, Svetha},
  journal={arXiv preprint arXiv:2009.11443},
  year={2020}
}

% metalearning to use neural memories
@article{munkhdalai2019,
  title={Metalearned neural memory},
  author={Munkhdalai, Tsendsuren and Sordoni, Alessandro and Wang, Tong and Trischler, Adam},
  journal={arXiv preprint arXiv:1907.09720},
  year={2019}
}


%%%%%%%%%%
% reasoning
%%%%%%%%%%

% relational reasoning by compositional concept learning
@article{lake2015,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

% relational reasoning and visual analogy question answering
@article{sadeghi2015,
  title={Visalogy: Answering visual analogy questions},
  author={Sadeghi, Fereshteh and Zitnick, C Lawrence and Farhadi, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  pages={1882--1890},
  year={2015}
}

% relational network
@article{santoro2017,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David GT and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1706.01427},
  year={2017}
}

% recurrent relational reasoning on graphs
@article{palm2017,
  title={Recurrent relational networks},
  author={Palm, Rasmus Berg and Paquet, Ulrich and Winther, Ole},
  journal={arXiv preprint arXiv:1711.08028},
  year={2017}
}

% recurrent relational network
@article{santoro2018,
  title={Relational recurrent neural networks},
  author={Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1806.01822},
  year={2018}
}

% relational reasoning with third order tensor product
@article{schlag2018,
  title={Learning to reason with third-order tensor products},
  author={Schlag, Imanol and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1811.12143},
  year={2018}
}

% analogy making, abstract reasoning, relational reasoning, raven matrices, wild relational network
@inproceedings{barrett2018,
  title={Measuring abstract reasoning in neural networks},
  author={Barrett, David and Hill, Felix and Santoro, Adam and Morcos, Ari and Lillicrap, Timothy},
  booktitle={International conference on machine learning},
  pages={511--520},
  year={2018},
  organization={PMLR}
}

% use disentangled vae representations for abstract reasoning and raven matrices
@article{steenbrugge2018,
  title={Improving generalization for abstract reasoning tasks using disentangled feature representations},
  author={Steenbrugge, Xander and Leroux, Sam and Verbelen, Tim and Dhoedt, Bart},
  journal={arXiv preprint arXiv:1811.04784},
  year={2018}
}

% relational reasoning with analogy triplets
@inproceedings{peyre2019,
  title={Detecting unseen visual relations using analogies},
  author={Peyre, Julia and Laptev, Ivan and Schmid, Cordelia and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1981--1990},
  year={2019}
}

% Humans generalize from little experience thanks to relational reasoning and causal concept representation 
@article{lake2020,
  title={People infer recursive visual concepts from just a few examples},
  author={Lake, Brenden M and Piantadosi, Steven T},
  journal={Computational Brain \& Behavior},
  volume={3},
  pages={54--65},
  year={2020},
  publisher={Springer}
}

% abstract reasoning with attention, raven matrices
@article{hahne2019,
  title={Attention on abstract visual reasoning},
  author={Hahne, Lukas and L{\"u}ddecke, Timo and W{\"o}rg{\"o}tter, Florentin and Kappel, David},
  journal={arXiv preprint arXiv:1911.05990},
  year={2019}
}

% abstract reasoning, analogy making, relational reasoning on raven matrices with (spatio-temporal) contrastive learning (context learning)
@article{zhang2019b,
  title={Learning perceptual inference by contrasting},
  author={Zhang, Chi and Jia, Baoxiong and Gao, Feng and Zhu, Yixin and Lu, Hongjing and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:1912.00086},
  year={2019}
}

% analogy making, relational reasoning, abstract reasoning with multi-layer relation networks for raven matrices
@inproceedings{jahrens2020,
  title={Solving ravens progressive matrices with multi-layer relation networks},
  author={Jahrens, Marius and Martinetz, Thomas},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

% analogy making, relational reasoning, abstract reasoning with neural networks for raven matrices
@article{zhuo2020,
  title={Solving Raven's Progressive Matrices with Neural Networks},
  author={Zhuo, Tao and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2002.01646},
  year={2020}
}

% analogy making, relational reasoning, abstract reasoning with external memory for raven matrices
@article{webb2020,
  title={Emergent Symbols through Binding in External Memory},
  author={Webb, Taylor W and Sinha, Ishan and Cohen, Jonathan D},
  journal={arXiv preprint arXiv:2012.14601},
  year={2020}
}

% self-attentiave memory for relational reasoning
@article{le2020b,
  title={Self-Attentive Associative Memory},
  author={Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  journal={arXiv preprint arXiv:2002.03519},
  year={2020}
}

% abstract reasoning with abduction (i.e. deduce relationship from data), and execution (i.e. simulate execution of relationship on imagined data), applied on raven matrices
@inproceedings{zhang2021,
  title={Abstract spatial-temporal reasoning via probabilistic abduction and execution},
  author={Zhang, Chi and Jia, Baoxiong and Zhu, Song-Chun and Zhu, Yixin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9736--9746},
  year={2021}
}

% survey on analogy making, abstract reasoning, relational reasoning (and applications on raven matrices)
@article{mitchell2021,
  title={Abstraction and analogy-making in artificial intelligence},
  author={Mitchell, Melanie},
  journal={arXiv preprint arXiv:2102.10717},
  year={2021}
}

%%%%%%%%%%
% creativity, novelty search
%%%%%%%%%%

% survey in creativity in machine learning
@article{franceschelli2021,
  title={Creativity and machine learning: A survey},
  author={Franceschelli, Giorgio and Musolesi, Mirco},
  journal={arXiv preprint arXiv:2104.02726},
  year={2021}
}

% introduction to creativity measure and optimization
@inproceedings{schmidhuber2007,
  title={Simple algorithmic principles of discovery, subjective beauty, selective attention, curiosity \& creativity},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={International conference on discovery science},
  pages={26--38},
  year={2007},
  organization={Springer}
}

% Review of creativity theory contributions by schmidhuber
@inproceedings{schmidhuber2008,
  title={Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Workshop on anticipatory behavior in adaptive learning systems},
  pages={48--76},
  year={2008},
  organization={Springer}
}

% Review of creativity theory contributions by schmidhuber
@article{schmidhuber2010,
  title={Formal theory of creativity, fun, and intrinsic motivation (1990--2010)},
  author={Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on autonomous mental development},
  volume={2},
  number={3},
  pages={230--247},
  year={2010},
  publisher={Ieee}
}

% Art by minimal complexity programs
@article{schmidhuber1997,
  title={Low-complexity art},
  author={Schmidhuber, J{\"u}rgen},
  journal={Leonardo},
  volume={30},
  number={2},
  pages={97--103},
  year={1997},
  publisher={The MIT Press}
}

% intrinsically motivated rl - can be interpreted as a form of novelty search
@article{singh2004,
  title={Intrinsically motivated reinforcement learning},
  author={Singh, Satinder and Barto, Andrew and Chentanez, Nuttapong},
  journal={Advances in neural information processing systems},
  volume={17},
  pages={1281--1288},
  year={2004}
}

% intrinsic motivation to exploration - can be interpreted as a form of novelty search
@article{auer2002,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

% novelty search
@inproceedings{lehman2008,
  title={Exploiting open-endedness to solve problems through the search for novelty.},
  author={Lehman, Joel and Stanley, Kenneth O},
  booktitle={ALIFE},
  pages={329--336},
  year={2008}
}

% novelty search for evolving/metalearning neural networks
@article{risi2010,
  title={Evolving plastic neural networks with novelty search},
  author={Risi, Sebastian and Hughes, Charles E and Stanley, Kenneth O},
  journal={Adaptive Behavior},
  volume={18},
  number={6},
  pages={470--491},
  year={2010},
  publisher={Sage Publications Sage UK: London, England}
}

% comprehensive study of novelty search algorithms
@inproceedings{gomes2015,
  title={Devising effective novelty search algorithms: A comprehensive empirical study},
  author={Gomes, Jorge and Mariano, Pedro and Christensen, Anders Lyhne},
  booktitle={Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  pages={943--950},
  year={2015}
}

% innovation engines - automated creativity by novelty search
@inproceedings{nguyen2015,
  title={Innovation engines: Automated creativity and improved stochastic optimization via deep learning},
  author={Nguyen, Anh Mai and Yosinski, Jason and Clune, Jeff},
  booktitle={Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  pages={959--966},
  year={2015}
}

% conceptualization and highligght of deep learning-based techniques for creativity 
@article{schneider2020,
  title={Creativity of deep learning: Conceptualization and assessment},
  author={Schneider, Johannes and Basalla, Marcus},
  journal={arXiv preprint arXiv:2012.02282},
  year={2020}
}

% Deep learning based measures of creativity
@article{franceschelli2022,
  title={DeepCreativity: measuring creativity with deep learning techniques},
  author={Franceschelli, Giorgio and Musolesi, Mirco},
  journal={Intelligenza Artificiale},
  volume={16},
  number={2},
  pages={151--163},
  year={2022},
  publisher={IOS Press}
}

%%%%%%%%%%
% reinforcement
%%%%%%%%%%

% rl with rnns for partially observable decision processes
@inproceedings{schafer2005,
  title={Solving partially observable reinforcement learning problems with recurrent neural networks},
  author={Sch{\"a}fer, Anton Maximilian and Udluft, Steffen and others},
  booktitle={Workshop Proc. of the European Conf. on Machine Learning},
  pages={71--81},
  year={2005}
}

% AI outperforms humans on some Atari games
@article{mnih2015,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

% AI outperforms humans on Go (alphago)
@article{silver2016, 
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

% intrinsic motivation to exploration - can be interpreted as a form of novelty search
@article{haarnoja2017,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1702.08165},
  year={2017}
}

% unsupervised rl by exploring space of skills - can be considered as a form of novelty search
@article{gregor2016,
  title={Variational intrinsic control},
  author={Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint arXiv:1611.07507},
  year={2016}
}

% unsupervised rl by exploring space of goals - can be considered as a form of novelty search
@article{pong2019,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}

% unsupervised rl with POET - generate novel environments for rl by novelty search
@article{wang2019b,
  title={Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions},
  author={Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O},
  journal={arXiv preprint arXiv:1901.01753},
  year={2019}
}

%%%%%%%%%%
% causal learning
%%%%%%%%%%

% Overview of causal inference methods in statistics
@article{pearl2009,
  title={Causal inference in statistics: An overview},
  author={Pearl, Judea},
  year={2009}
}

% causal learning with model based rl. dyna system: model based learning + policy iteration. model based system provides simulated experience to train the policy iteration system.
@incollection{sutton1990,
  title={Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={Machine learning proceedings 1990},
  pages={216--224},
  year={1990},
  publisher={Elsevier}
}

% causal learning with model based rl
@inproceedings{oh2015,
  title={Action-conditional video prediction using deep networks in atari games},
  author={Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  booktitle={Advances in neural information processing systems},
  pages={2863--2871},
  year={2015}
}

% causal learning with model based rl
@article{leibfried2016,
  title={A deep learning approach for joint video frame and reward prediction in atari games},
  author={Leibfried, Felix and Kushman, Nate and Hofmann, Katja},
  journal={arXiv preprint arXiv:1611.07078},
  year={2016}
}

% causal learning with model based rl
@article{chiappa2017,
  title={Recurrent environment simulators},
  author={Chiappa, Silvia and Racaniere, S{\'e}bastien and Wierstra, Daan and Mohamed, Shakir},
  journal={arXiv preprint arXiv:1704.02254},
  year={2017}
}

% causal learning in indoor visual navigation
@inproceedings{zhu2017,
  title={Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  author={Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3357--3364},
  year={2017},
  organization={IEEE}
}

% uses a generative model to generate latent embeddings for the world state, kind of a model-based system. The embeddings are then use to pilot a policy network. It could also be possible to generate world embeddings to describe a particular world model, and then feed this world embeddings to a model-based system, together with current state and action, to predict future state conditioned on the particular world model, which can be used to do meta- causal learning.
@article{ha2018,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

% causal learning with model based rl
@article{kaiser2019,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}


%%%%%%%%%%
% Functional embeddings
%%%%%%%%%%

% latent embedding optimization for metalearning - it is kind of a functional embedding
@article{rusu2018,
  title={Meta-learning with latent embedding optimization},
  author={Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  journal={arXiv preprint arXiv:1807.05960},
  year={2018}
}


%%%%%%%%%%
% constrained
%%%%%%%%%%

% constrained learning
@article{dhar2019,
  title={On-device machine learning: An algorithms and learning theory perspective},
  author={Dhar, Sauptik and Guo, Junyao and Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
  journal={arXiv preprint arXiv:1911.00623},
  year={2019}
}

%%%%%%%%%%
% distillation/quantization/compression
%%%%%%%%%%

% distillation
@inproceedings{bucilua2006,
  title={Model compression},
  author={Bucilu, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}

% distillation
@inproceedings{ba2014,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  booktitle={Advances in neural information processing systems},
  pages={2654--2662},
  year={2014}
}

% distillation
@article{romero2014,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

% distillation
@article{hinton2015,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

% distillation
@article{sau2016,
  title={Deep model compression: Distilling knowledge from noisy teachers},
  author={Sau, Bharat Bhusan and Balasubramanian, Vineeth N},
  journal={arXiv preprint arXiv:1610.09650},
  year={2016}
}

% distillation with information bottleneck
@inproceedings{dai2018,
  title={Compressing neural networks using the variational information bottleneck},
  author={Dai, Bin and Zhu, Chen and Guo, Baining and Wipf, David},
  booktitle={International Conference on Machine Learning},
  pages={1135--1144},
  year={2018},
  organization={PMLR}
}


%%%%%%%%%%
% gradient free learning
%%%%%%%%%%

% gradient free learning - evolutionary algorithms
@inproceedings{vikhar2016,
  title={Evolutionary algorithms: A critical review and its future prospects},
  author={Vikhar, Pradnya A},
  booktitle={2016 International conference on global trends in signal processing, information computing and communication (ICGTSPICC)},
  pages={261--265},
  year={2016},
  organization={IEEE}
}

% gradient free learning - cma evolutionary strategy
@incollection{hansen2006,
  title={The CMA evolution strategy: a comparing review},
  author={Hansen, Nikolaus},
  booktitle={Towards a new evolutionary computation},
  pages={75--102},
  year={2006},
  publisher={Springer}
}

% gradient free learning - particle swarm optimization
@inproceedings{kennedy1995,
  title={Particle swarm optimization},
  author={Kennedy, James and Eberhart, Russell},
  booktitle={Proceedings of ICNN'95-International Conference on Neural Networks},
  volume={4},
  pages={1942--1948},
  year={1995},
  organization={IEEE}
}

% gradient free learning - direct search
@article{kolda2003,
  title={Optimization by direct search: New perspectives on some classical and modern methods},
  author={Kolda, Tamara G and Lewis, Robert Michael and Torczon, Virginia},
  journal={SIAM review},
  volume={45},
  number={3},
  pages={385--482},
  year={2003},
  publisher={SIAM}
}

% gradient free learning - finite difference gradient estimation
@article{lam2020,
  title={Minimax efficient finite-difference stochastic gradient estimators using black-box function evaluations},
  author={Lam, Henry and Li, Haidong and Zhang, Xuhui},
  journal={Operations Research Letters},
  year={2020},
  publisher={Elsevier}
}

% gradient free learning - pattern search
@article{torczon1997,
  title={On the convergence of pattern search algorithms},
  author={Torczon, Virginia},
  journal={SIAM Journal on optimization},
  volume={7},
  number={1},
  pages={1--25},
  year={1997},
  publisher={SIAM}
}

% gradient free learning - nelder mead downhill simplex method
@article{nelder1965,
  title={A simplex method for function minimization},
  author={Nelder, John A and Mead, Roger},
  journal={The computer journal},
  volume={7},
  number={4},
  pages={308--313},
  year={1965},
  publisher={Oxford University Press}
}

% gradient free learning - random search
@article{solis1981,
  title={Minimization by random search techniques},
  author={Solis, Francisco J and Wets, Roger J-B},
  journal={Mathematics of operations research},
  volume={6},
  number={1},
  pages={19--30},
  year={1981},
  publisher={INFORMS}
}

% gradient free learning - weight perturbation
@article{jabri1992,
  title={Weight perturbation: An optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks},
  author={Jabri, Marwan and Flower, Barry},
  journal={IEEE Transactions on Neural Networks},
  volume={3},
  number={1},
  pages={154--157},
  year={1992}
}

% gradient free learning - node perturbation
@inproceedings{flower1993,
  title={Summed weight neuron perturbation: An O (n) improvement over weight perturbation},
  author={Flower, Barry and Jabri, Marwan},
  booktitle={Advances in neural information processing systems},
  pages={212--219},
  year={1993}
}

% gradient free learning - simulated annealing
@article{kirkpatrick1983,
  title={Optimization by simulated annealing},
  author={Kirkpatrick, Scott and Gelatt, C Daniel and Vecchi, Mario P},
  journal={science},
  volume={220},
  number={4598},
  pages={671--680},
  year={1983},
  publisher={American association for the advancement of science}
}

% gradient free learning - multilevel coordinate search
@article{huyer1999,
  title={Global optimization by multilevel coordinate search},
  author={Huyer, Waltraud and Neumaier, Arnold},
  journal={Journal of Global Optimization},
  volume={14},
  number={4},
  pages={331--355},
  year={1999},
  publisher={Springer}
}


%%%%%%%%%%
% forward learning
%%%%%%%%%%

@article{hinton2022,
  title={The forward-forward algorithm: Some preliminary investigations},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2212.13345},
  year={2022}
}

@article{kohan2023,
  title={Signal propagation: The framework for learning and inference in a forward pass},
  author={Kohan, Adam and Rietman, Edward A and Siegelmann, Hava T},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}


%%%%%%%%%%
% kernel methods
%%%%%%%%%%

% kernel methods
@article{muller2001,
  title={An introduction to kernel-based learning algorithms},
  author={Muller, K-R and Mika, Sebastian and Ratsch, Gunnar and Tsuda, Koji and Scholkopf, Bernhard},
  journal={IEEE transactions on neural networks},
  volume={12},
  number={2},
  pages={181--201},
  year={2001},
  publisher={IEEE}
}

% preimage kernel methods
@inproceedings{mika1999b,
  title={Kernel PCA and de-noising in feature spaces},
  author={Mika, Sebastian and Sch{\"o}lkopf, Bernhard and Smola, Alex J and M{\"u}ller, Klaus-Robert and Scholz, Matthias and R{\"a}tsch, Gunnar},
  booktitle={Advances in neural information processing systems},
  pages={536--542},
  year={1999}
}

% preimage kernel methods
@article{kwok2004,
  title={The pre-image problem in kernel methods},
  author={Kwok, JT-Y and Tsang, IW-H},
  journal={IEEE transactions on neural networks},
  volume={15},
  number={6},
  pages={1517--1525},
  year={2004},
  publisher={IEEE}
}

% preimage kernel methods
@inproceedings{honeine2009,
  title={Solving the pre-image problem in kernel machines: A direct method},
  author={Honeine, Paul and Richard, C{\'e}dric},
  booktitle={2009 IEEE International Workshop on Machine Learning for Signal Processing},
  pages={1--6},
  year={2009},
  organization={IEEE}
}

% connection out of sample (nystrom) - preimage problems
@inproceedings{arias2007,
  title={Connecting the out-of-sample and pre-image problems in kernel methods},
  author={Arias, Pablo and Randall, Gregory and Sapiro, Guillermo},
  booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--8},
  year={2007},
  organization={IEEE}
}

% nystrom method for kernel machines
@inproceedings{williams2001b,
  title={Using the Nystr{\"o}m method to speed up kernel machines},
  author={Williams, Christopher KI and Seeger, Matthias},
  booktitle={Advances in neural information processing systems},
  pages={682--688},
  year={2001}
}

% clustered nystrom method for kernel machines
@article{zhang2010,
  title={Clustered Nystr{\"o}m method for large scale manifold learning and dimension reduction},
  author={Zhang, Kai and Kwok, James T},
  journal={IEEE Transactions on Neural Networks},
  volume={21},
  number={10},
  pages={1576--1587},
  year={2010},
  publisher={IEEE}
}


%%%%%%%%%%
% retrieval
%%%%%%%%%%

% CNN retrieval - Results on cv datasets
@inproceedings{wan2014,
  title={Deep learning for content-based image retrieval: A comprehensive study},
  author={Wan, Ji and Wang, Dayong and Hoi, Steven Chu Hong and Wu, Pengcheng and Zhu, Jianke and Zhang, Yongdong and Li, Jintao},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={157--166},
  year={2014}
}

% CNN retrieval
@inproceedings{babenko2014,
  title={Neural codes for image retrieval},
  author={Babenko, Artem and Slesarev, Anton and Chigorin, Alexandr and Lempitsky, Victor},
  booktitle={European conference on computer vision},
  pages={584--599},
  year={2014},
  organization={Springer}
}

% CNN retrieval
@article{babenko2015,
  title={Aggregating deep convolutional features for image retrieval},
  author={Babenko, Artem and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1510.07493},
  year={2015}
}

% CNN retrieval
@inproceedings{yue2015,
  title={Exploiting local features from deep networks for image retrieval},
  author={Yue-Hei Ng, Joe and Yang, Fan and Davis, Larry S},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={53--61},
  year={2015}
}

% CNN retrieval
@inproceedings{gordo2016,
  title={Deep image retrieval: Learning global representations for image search},
  author={Gordo, Albert and Almaz{\'a}n, Jon and Revaud, Jerome and Larlus, Diane},
  booktitle={European conference on computer vision},
  pages={241--257},
  year={2016},
  organization={Springer}
}

% CNN retrieval
@article{gordo2017,
  title={End-to-end learning of deep visual representations for image retrieval},
  author={Gordo, Albert and Almazan, Jon and Revaud, Jerome and Larlus, Diane},
  journal={International Journal of Computer Vision},
  volume={124},
  number={2},
  pages={237--254},
  year={2017},
  publisher={Springer}
}

% CNN retrieval
@article{alzu2017,
  title={Content-based image retrieval with compact deep convolutional features},
  author={Alzu'bi, Ahmad and Amira, Abbes and Ramzan, Naeem},
  journal={Neurocomputing},
  volume={249},
  pages={95--105},
  year={2017},
  publisher={Elsevier}
}

% CNN retrieval - OANIR method, also comprehensive comparison with other methods on cv datasets
@article{bai2018,
  title={Optimization of deep convolutional neural network for large scale image retrieval},
  author={Bai, Cong and Huang, Ling and Pan, Xiang and Zheng, Jianwei and Chen, Shengyong},
  journal={Neurocomputing},
  volume={303},
  pages={60--67},
  year={2018},
  publisher={Elsevier}
}

% CNN retrieval
@article{tzelepi2018,
  title={Deep convolutional learning for content based image retrieval},
  author={Tzelepi, Maria and Tefas, Anastasios},
  journal={Neurocomputing},
  volume={275},
  pages={2467--2478},
  year={2018},
  publisher={Elsevier}
}

% Deep Features, Similarity searching
@InProceedings{amato2019,
  title={YFCC100M-HNfc6: a large-scale deep features benchmark for similarity search},
  author={Amato, Giuseppe and Falchi, Fabrizio and Gennaro, Claudio and Rabitti, Fausto},
  booktitle={Similarity Search and Applications: 9th International Conference, SISAP 2016, Tokyo, Japan, October 24-26, 2016, Proceedings 9},
  pages={196--209},
  year={2016},
  organization={Springer}
}

% CNN retrieval
@article{gkelios2021,
  title={Deep convolutional features for image retrieval},
  author={Gkelios, Socratis and Sophokleous, Aphrodite and Plakias, Spiros and Boutalis, Yiannis and Chatzichristofis, Savvas A},
  journal={Expert Systems with Applications},
  volume={177},
  pages={114940},
  year={2021},
  publisher={Elsevier}
}

% SPLH
@article{wang2012,
  title={Semi-supervised hashing for large-scale search},
  author={Wang, Jun and Kumar, Sanjiv and Chang, Shih-Fu},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={12},
  pages={2393--2406},
  year={2012},
  publisher={IEEE}
}

% CCA-ITQ
@article{gong2012,
  title={Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval},
  author={Gong, Yunchao and Lazebnik, Svetlana and Gordo, Albert and Perronnin, Florent},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={12},
  pages={2916--2929},
  year={2012},
  publisher={IEEE}
}

% FastH
@inproceedings{lin2014,
  title={Fast supervised hashing with decision trees for high-dimensional data},
  author={Lin, Guosheng and Shen, Chunhua and Shi, Qinfeng and Van den Hengel, Anton and Suter, David},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1963--1970},
  year={2014}
}

% SMLSH
@inproceedings{weng2015,
  title={Supervised multi-scale locality sensitive hashing},
  author={Weng, Li and Jhuo, I-Hong and Shi, Miaojing and Sun, Meng and Cheng, Wen-Huang and Amsaleg, Laurent},
  booktitle={Proceedings of the 5th ACM on International Conference on Multimedia Retrieval},
  pages={259--266},
  year={2015}
}

% SDH
@inproceedings{shen2015,
  title={Supervised discrete hashing},
  author={Shen, Fumin and Shen, Chunhua and Liu, Wei and Tao Shen, Heng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={37--45},
  year={2015}
}

% DeepH
@inproceedings{zhao2015,
  title={Deep semantic ranking based hashing for multi-label image retrieval},
  author={Zhao, Fang and Huang, Yongzhen and Wang, Liang and Tan, Tieniu},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1556--1564},
  year={2015}
}

% NDH
@article{chen2016,
  title={Nonlinear discrete hashing},
  author={Chen, Zhixiang and Lu, Jiwen and Feng, Jianjiang and Zhou, Jie},
  journal={IEEE Transactions on Multimedia},
  volume={19},
  number={1},
  pages={123--135},
  year={2016},
  publisher={IEEE}
}


%%%%%%%%%%
% general aspects and algorithms
%%%%%%%%%%

% stochastic approximation
@article{robbins1951,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

% stochastic approximation
@article{ljung1977,
  title={Analysis of recursive stochastic algorithms},
  author={Ljung, Lennart},
  journal={IEEE transactions on automatic control},
  volume={22},
  number={4},
  pages={551--575},
  year={1977},
  publisher={IEEE}
}

% Expectation-Maximization (EM) algorithm for maximum likelihood estimation
@article{dempster1977,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

% RANSAC - Random Sample Consensus
@article{fischler1981,
  title={Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  author={Fischler, Martin A and Bolles, Robert C},
  journal={Communications of the ACM},
  volume={24},
  number={6},
  pages={381--395},
  year={1981},
  publisher={ACM New York, NY, USA}
}

% Decision trees ID3 (Iterative Dichotomizer) algorithm
@incollection{quinlan1983,
  title={Learning efficient classification procedures and their application to chess end games},
  author={Quinlan, J Ross},
  booktitle={Machine learning},
  pages={463--482},
  year={1983},
  publisher={Elsevier}
}

% Decision trees introduction
@article{myles2004,
  title={An introduction to decision tree modeling},
  author={Myles, Anthony J and Feudale, Robert N and Liu, Yang and Woody, Nathaniel A and Brown, Steven D},
  journal={Journal of Chemometrics: A Journal of the Chemometrics Society},
  volume={18},
  number={6},
  pages={275--285},
  year={2004},
  publisher={Wiley Online Library}
}

% Decision trees gradient boosting
@article{friedman2001,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

% XGBOOST: Decision trees gradient boosting
@inproceedings{chen2016,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

% Dynamic Time Warping (DTW) algorithm in speech recognition
@article{sakoe1978,
  title={Dynamic programming algorithm optimization for spoken word recognition},
  author={Sakoe, Hiroaki and Chiba, Seibi},
  journal={IEEE transactions on acoustics, speech, and signal processing},
  volume={26},
  number={1},
  pages={43--49},
  year={1978},
  publisher={IEEE}
}

% Dynamic Time Warping (DTW) algorithm for pattern discovery in time series
@inproceedings{berndt1994,
  title={Using dynamic time warping to find patterns in time series},
  author={Berndt, Donald J and Clifford, James},
  booktitle={Proceedings of the 3rd international conference on knowledge discovery and data mining},
  pages={359--370},
  year={1994}
}

% optimization with augmented lagrangian method
@article{hestenes1969,
  title={Multiplier and gradient methods},
  author={Hestenes, Magnus R},
  journal={Journal of optimization theory and applications},
  volume={4},
  number={5},
  pages={303--320},
  year={1969},
  publisher={Springer}
}

% optimization with admm - alternating direction method of multipliers
@article{gabay1976,
  title={A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
  author={Gabay, Daniel and Mercier, Bertrand},
  journal={Computers \& mathematics with applications},
  volume={2},
  number={1},
  pages={17--40},
  year={1976},
  publisher={Elsevier}
}

% optimization with projected gradient methods - special case of proximal gradient where the proximity operator is the projection operator
@article{calamai1987,
  title={Projected gradient methods for linearly constrained problems},
  author={Calamai, Paul H and Mor{\'e}, Jorge J},
  journal={Mathematical programming},
  volume={39},
  number={1},
  pages={93--116},
  year={1987},
  publisher={Springer}
}

% optimization with Bregman iteration for L1 regularized problems - split Bregman method
@article{goldstein2009,
  title={The split Bregman method for L1-regularized problems},
  author={Goldstein, Tom and Osher, Stanley},
  journal={SIAM journal on imaging sciences},
  volume={2},
  number={2},
  pages={323--343},
  year={2009},
  publisher={SIAM}
}

% optimization with proximal gradient methods -  generalization of projected gradient with arbitrary proximity operator
@incollection{combettes2011,
  title={Proximal splitting methods in signal processing},
  author={Combettes, Patrick L and Pesquet, Jean-Christophe},
  booktitle={Fixed-point algorithms for inverse problems in science and engineering},
  pages={185--212},
  year={2011},
  publisher={Springer}
}

% optimization with block coordinate descent
@article{beck2013,
  title={On the convergence of block coordinate descent type methods},
  author={Beck, Amir and Tetruashvili, Luba},
  journal={SIAM journal on Optimization},
  volume={23},
  number={4},
  pages={2037--2060},
  year={2013},
  publisher={SIAM}
}

% optimization with stochastic admm
@inproceedings{ouyang2013,
  title={Stochastic alternating direction method of multipliers},
  author={Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={80--88},
  year={2013},
  organization={PMLR}
}

% Image structure descriptor
@article{koenderink1984,
  title={The structure of images},
  author={Koenderink, Jan J},
  journal={Biological cybernetics},
  volume={50},
  number={5},
  pages={363--370},
  year={1984},
  publisher={Springer}
}

% Harris detector: harris edge detector based on derivative filters
@inproceedings{harris1988,
  title={A combined corner and edge detector},
  author={Harris, Chris and Stephens, Mike and others},
  booktitle={Alvey vision conference},
  volume={15},
  number={50},
  pages={10--5244},
  year={1988},
  organization={Citeseer}
}

% Edge detection filter based on nonlinear anisotropic diffusion
@article{perona1990,
  title={Scale-space and edge detection using anisotropic diffusion},
  author={Perona, Pietro and Malik, Jitendra},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={12},
  number={7},
  pages={629--639},
  year={1990},
  publisher={IEEE}
}

% gabor filters
@inproceedings{rosenthaler1992,
  title={Detection of general edges and keypoints},
  author={Rosenthaler, Lukas and Heitger, Friedrich and K{\"u}bler, Olaf and von der Heydt, R{\"u}diger},
  booktitle={Computer VisionECCV'92: Second European Conference on Computer Vision Santa Margherita Ligure, Italy, May 19--22, 1992 Proceedings 2},
  pages={78--86},
  year={1992},
  organization={Springer}
}

% gabor filters
@article{heitger1992,
  title={Simulation of neural contour mechanisms: from simple to end-stopped cells},
  author={Heitger, Friedrich and Rosenthaler, Lukas and Von Der Heydt, R{\"u}diger and Peterhans, Esther and K{\"u}bler, Olaf},
  journal={Vision research},
  volume={32},
  number={5},
  pages={963--981},
  year={1992},
  publisher={Elsevier}
}

% nonlinear noise filter based on total variation
@article{rudin1992,
  title={Nonlinear total variation based noise removal algorithms},
  author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  journal={Physica D: nonlinear phenomena},
  volume={60},
  number={1-4},
  pages={259--268},
  year={1992},
  publisher={Elsevier}
}

% Bilateral filtering nonlinear noise filter
@inproceedings{tomasi1998,
  title={Bilateral filtering for gray and color images},
  author={Tomasi, Carlo and Manduchi, Roberto},
  booktitle={Sixth international conference on computer vision (IEEE Cat. No. 98CH36271)},
  pages={839--846},
  year={1998},
  organization={IEEE}
}

% non-local means nonlinear noise filter
@inproceedings{buades2005,
  title={A non-local algorithm for image denoising},
  author={Buades, Antoni and Coll, Bartomeu and Morel, J-M},
  booktitle={2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)},
  volume={2},
  pages={60--65},
  year={2005},
  organization={Ieee}
}

% kaze detector based on nonlinear diffusion filtering
@inproceedings{alcantarilla2012,
  title={KAZE features},
  author={Alcantarilla, Pablo Fern{\'a}ndez and Bartoli, Adrien and Davison, Andrew J},
  booktitle={Computer Vision--ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12},
  pages={214--227},
  year={2012},
  organization={Springer}
}

% ANN stats
@inproceedings{agrawal2014,
  title={Analyzing the performance of multilayer neural networks for object recognition},
  author={Agrawal, Pulkit and Girshick, Ross and Malik, Jitendra},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13},
  pages={329--344},
  year={2014},
  organization={Springer}
}

% DNN energy cost (and research bias towards more complex models but unfair comparisons)
@inproceedings{badar2021,
  title={Highlighting the importance of reducing research bias and carbon emissions in cnns},
  author={Badar, Ahmed and Varma, Arnav and Staniec, Adrian and Gamal, Mahmoud and Magdy, Omar and Iqbal, Haris and Arani, Elahe and Zonooz, Bahram},
  booktitle={International Conference of the Italian Association for Artificial Intelligence},
  pages={515--531},
  year={2021},
  organization={Springer}
}

% dimensionality of data representations in dnns - dimension expansion in early layers and compression in later layers
@article{recanatesi2019,
  title={Dimensionality compression and expansion in Deep Neural Networks},
  author={Recanatesi, Stefano and Farrell, Matthew and Advani, Madhu and Moore, Timothy and Lajoie, Guillaume and Shea-Brown, Eric},
  journal={arXiv preprint arXiv:1906.00443},
  year={2019}
}

% dimensionality of data representations in dnns - dimension expansion in early layers and compression in later layers
@article{ansuini2019,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  journal={arXiv preprint arXiv:1905.12784},
  year={2019}
}

% union of manifolds structure in image data
@inproceedings{brown2023,
  title={Verifying the union of manifolds hypothesis for image data},
  author={Brown, Bradley CA and Caterini, Anthony L and Ross, Brendan Leigh and Cresswell, Jesse C and Loaiza-Ganem, Gabriel},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

% MLP universal approximator with non polynomial activations
@article{pinkus1999,
  title={Approximation theory of the MLP model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}

% Universal approximation by RBF activation network
@article{park1991,
  title={Universal approximation using radial-basis-function networks},
  author={Park, Jooyoung and Sandberg, Irwin W},
  journal={Neural computation},
  volume={3},
  number={2},
  pages={246--257},
  year={1991},
  publisher={MIT Press}
}

% Network of WTA units is a universal approximator
@article{maass2000,
  title={On the computational power of winner-take-all},
  author={Maass, Wolfgang},
  journal={Neural computation},
  volume={12},
  number={11},
  pages={2519--2535},
  year={2000},
  publisher={MIT Press}
}


%%%%%%%%%%
% neuromorphic hw
%%%%%%%%%%

% hbp
@article{hbp,
    title={The human brain project: A report to the European Commission},
    author={Markram, Henry and Meier, K and Ailamaki, A and Alvandpour, A and Amunts, K and Andreoni, W and others},
    journal={The HBP-PS Consortium, Lausanne},
    year={2012}
}

% Overview
@article{furber2007, 
    title={Neural systems engineering.},
    author={Furber, S and Temple, S},
    journal={Journal of the Royal Society, Interface},
    volume={4},
    number={13},
    pages={193--206},
    year={2007},
    publisher={The Royal Society}
}

% Spinnaker
@article{furber2013, 
  title={Overview of the spinnaker system architecture},
  author={Furber, Steve B and Lester, David R and Plana, Luis A and Garside, Jim D and Painkras, Eustace and Temple, Steve and Brown, Andrew D},
  journal={IEEE Transactions on Computers},
  volume={62},
  number={12},
  pages={2454--2467},
  year={2013},
  publisher={IEEE}
}

% Spinnaker
@article{furber2014, 
  title={The spinnaker project},
  author={Furber, Steve B and Galluppi, Francesco and Temple, Steve and Plana, Luis A},
  journal={Proceedings of the IEEE},
  volume={102},
  number={5},
  pages={652--665},
  year={2014},
  publisher={IEEE}
}

% Spinnaker
@article{galluppi2015, 
    title={A framework for plasticity implementation on the SpiNNaker neural architecture},
    author={Galluppi, Francesco and Lagorce, Xavier and Stromatias, Evangelos and Pfeiffer, Michael and Plana, Luis A and Furber, Steve B and Benosman, Ryad B},
    journal={Frontiers in neuroscience},
    volume={8},
    pages={429},
    year={2015},
    publisher={Frontiers}
}

% Neurogrid
@article{benjamin2014, 
    title={Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations},
    author={Benjamin, Ben Varkey and Gao, Peiran and McQuinn, Emmett and Choudhary, Swadesh and Chandrasekaran, Anand R and Bussat, Jean-Marie and Alvarez-Icaza, Rodrigo and Arthur, John V and Merolla, Paul A and Boahen, Kwabena},
    journal={Proceedings of the IEEE},
    volume={102},
    number={5},
    pages={699--716},
    year={2014},
    publisher={IEEE}
}

% TrueNorth
@article{merolla2014, 
    title={A million spiking-neuron integrated circuit with a scalable communication network and interface},
    author={Merolla, Paul A and Arthur, John V and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and others},
    journal={Science},
    volume={345},
    number={6197},
    pages={668--673},
    year={2014},
    publisher={American Association for the Advancement of Science}
}

% BrainScaleS
@inproceedings{schemmel2010, 
  title={A wafer-scale neuromorphic hardware system for large-scale neural modeling},
  author={Schemmel, Johannes and Briiderle, Daniel and Griibl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
  booktitle={Proceedings of 2010 IEEE International Symposium on Circuits and Systems},
  pages={1947--1950},
  year={2010},
  organization={IEEE}
}

% BrainScaleS-2
@inproceedings{billaudelle2020,
  title={Versatile emulation of spiking neural networks on an accelerated neuromorphic substrate},
  author={Billaudelle, Sebastian and Stradmann, Yannik and Schreiber, Korbinian and Cramer, Benjamin and Baumbach, Andreas and Dold, Dominik and G{\"o}ltz, Julian and Kungl, Akos F and Wunderlich, Timo C and Hartel, Andreas and others},
  booktitle={2020 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--5},
  year={2020},
  organization={IEEE}
}

% Loihi
@article{davies2018,
  title={Loihi: A neuromorphic manycore processor with on-chip learning},
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and others},
  journal={Ieee Micro},
  volume={38},
  number={1},
  pages={82--99},
  year={2018},
  publisher={IEEE}
}

% TripleBrain
@article{wang2022,
  title={TripleBrain: A compact neuromorphic hardware core with fast on-chip self-organizing and reinforcement spike-timing dependent plasticity},
  author={Wang, Haibing and He, Zhen and Wang, Tengxiao and He, Junxian and Zhou, Xichuan and Wang, Ying and Liu, Liyuan and Wu, Nanjian and Tian, Min and Shi, Cong},
  journal={IEEE Transactions on Biomedical Circuits and Systems},
  volume={16},
  number={4},
  pages={636--650},
  year={2022},
  publisher={IEEE}
}

% neuromorphic
@article{gamrat2015, 
    title={Memristive based device arrays combined with Spike based coding can enable efficient implementations of embedded neuromorphic circuits},
    author={Gamrat, C and Bichler, O and Roclin, D},
    journal={2015 IEEE International Electron Devices Meeting (IEDM)},
    volume={2016},
    pages={4--5},
    year={2015},
    organization={Institute of Electrical and Electronics Engineers Inc.}
}

% neuromorphic
@article{azghadi2015, 
    title={Programmable spike-timing-dependent plasticity learning circuits in neuromorphic vlsi architectures},
    author={Azghadi, Mostafa Rahimi and Moradi, Saber and Fasnacht, Daniel B and Ozdas, Mehmet Sirin and Indiveri, Giacomo},
    journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
    volume={12},
    number={2},
    pages={17},
    year={2015},
    publisher={ACM}
}

% neuromorphic
@article{wu2015, 
    title={A CMOS Spiking Neuron for Brain-Inspired Neural Networks With Resistive Synapses andIn SituLearning},
    author={Wu, Xinyu and Saxena, Vishal and Zhu, Kehan and Balagopal, Sakkarapani},
    journal={IEEE Transactions on Circuits and Systems II: Express Briefs},
    volume={62},
    number={11},
    pages={1088--1092},
    year={2015},
    publisher={IEEE}
}

% neuromorphic computing review
@article{chicca2014,
  title={Neuromorphic electronic circuits for building autonomous cognitive systems},
  author={Chicca, Elisabetta and Stefanini, Fabio and Bartolozzi, Chiara and Indiveri, Giacomo},
  journal={Proceedings of the IEEE},
  volume={102},
  number={9},
  pages={1367--1388},
  year={2014},
  publisher={IEEE}
}

% neuromorphic computing review
@article{nawrocki2016,
  title={A mini review of neuromorphic architectures and implementations},
  author={Nawrocki, Robert A and Voyles, Richard M and Shaheen, Sean E},
  journal={IEEE Transactions on Electron Devices},
  volume={63},
  number={10},
  pages={3819--3829},
  year={2016},
  publisher={IEEE}
}

% neuromorphic computing review
@article{roy2019b,
  title={Towards spike-based machine intelligence with neuromorphic computing},
  author={Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
  journal={Nature},
  volume={575},
  number={7784},
  pages={607--617},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

% snn hardware implementations review
@article{bouvier2019,
  title={Spiking neural networks hardware implementations and challenges: A survey},
  author={Bouvier, Maxence and Valentian, Alexandre and Mesquida, Thomas and Rummens, Francois and Reyboz, Marina and Vianello, Elisa and Beigne, Edith},
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume={15},
  number={2},
  pages={1--35},
  year={2019},
  publisher={ACM New York, NY, USA}
}

% neuromorphic computing review
@article{zhu2020,
  title={A comprehensive review on emerging artificial neuromorphic devices},
  author={Zhu, Jiadi and Zhang, Teng and Yang, Yuchao and Huang, Ru},
  journal={Applied Physics Reviews},
  volume={7},
  number={1},
  pages={011312},
  year={2020},
  publisher={AIP Publishing LLC}
}

% overview of snns and hardware implementations
@article{nguyen2021,
  title={A review of algorithms and hardware implementations for spiking neural networks},
  author={Nguyen, Duy-Anh and Tran, Xuan-Tu and Iacopi, Francesca},
  journal={Journal of Low Power Electronics and Applications},
  volume={11},
  number={2},
  pages={23},
  year={2021},
  publisher={MDPI}
}

% overview of snns and hardware implementations
@article{dora2021,
  title={Spiking neural networks for computational intelligence: an overview},
  author={Dora, Shirin and Kasabov, Nikola},
  journal={Big Data and Cognitive Computing},
  volume={5},
  number={4},
  pages={67},
  year={2021},
  publisher={MDPI}
}

% neuromorphic computing review
@article{schuman2022,
  title={Opportunities for neuromorphic computing algorithms and applications},
  author={Schuman, Catherine D and Kulkarni, Shruti R and Parsa, Maryam and Mitchell, J Parker and Date, Prasanna and Kay, Bill},
  journal={Nature Computational Science},
  volume={2},
  number={1},
  pages={10--19},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

% neuromorphic computing review
@article{huynh2022,
  title={Implementing spiking neural networks on neuromorphic architectures: A review},
  author={Huynh, Phu Khanh and Varshika, M Lakshmi and Paul, Ankita and Isik, Murat and Balaji, Adarsha and Das, Anup},
  journal={arXiv preprint arXiv:2202.08897},
  year={2022}
}

% neuromorphic copmuting review
@article{shrestha2022,
  title={A survey on neuromorphic computing: Models and hardware},
  author={Shrestha, Amar and Fang, Haowen and Mei, Zaidao and Rider, Daniel Patrick and Wu, Qing and Qiu, Qinru},
  journal={IEEE Circuits and Systems Magazine},
  volume={22},
  number={2},
  pages={6--35},
  year={2022},
  publisher={IEEE}
}


%%%%%%%%%%
% neuroscience
%%%%%%%%%%

% Parameters for bio simulation
@phdthesis{jug2012, 
  title={On competition and learning in cortical structures},
  author={Jug, Florian},
  year={2012},
  school={ETH Zurich}
}

% HH Hodgkin-Huxley neuron model
@article{hodgkin1952,
  title={A quantitative description of membrane current and its application to conduction and excitation in nerve},
  author={Hodgkin, Alan L and Huxley, Andrew F},
  journal={The Journal of physiology},
  volume={117},
  number={4},
  pages={500},
  year={1952},
  publisher={Wiley-Blackwell}
}

% LIF
@article{abbott1993, 
  title={Asynchronous states in networks of pulse-coupled oscillators},
  author={Abbott, LF and van Vreeswijk, Carl},
  journal={Physical Review E},
  volume={48},
  number={2},
  pages={1483},
  year={1993},
  publisher={APS}
}

% Izhikevich neuron model
@article{izhikevich2003,
  title={Simple model of spiking neurons},
  author={Izhikevich, Eugene M},
  journal={IEEE Transactions on neural networks},
  volume={14},
  number={6},
  pages={1569--1572},
  year={2003},
  publisher={IEEE}
}

% SRM spike response model and stochastic neuron models
@article{gerstner1995,
  title={Time structure of the activity in neural network models},
  author={Gerstner, Wulfram},
  journal={Physical review E},
  volume={51},
  number={1},
  pages={738},
  year={1995},
  publisher={APS}
}

% characteristics of synaptic plasticity: stdp, synaptic scaling/homeostatic plasticity and synaptic redistribution/heterosynaptic competition
@article{abbott2000,
  title={Synaptic plasticity: taming the beast},
  author={Abbott, Larry F and Nelson, Sacha B},
  journal={Nature neuroscience},
  volume={3},
  number={11},
  pages={1178--1183},
  year={2000},
  publisher={Nature Publishing Group}
}

% Organiz. of cortex, simple and complex cells
@article{carandini2006,
  title={What simple and complex cells compute},
  author={Carandini, Matteo},
  journal={The Journal of physiology},
  volume={577},
  number={Pt 2},
  pages={463},
  year={2006},
  publisher={Wiley-Blackwell}
}

% Organiz. of cortex, bidir. conn.
@article{felleman1991, 
    title={Distributed hierarchical processing in the primate cerebral cortex.},
    author={Felleman, Daniel J and Van, DC Essen},
    journal={Cerebral cortex (New York, NY: 1991)},
    volume={1},
    number={1},
    pages={1--47},
    year={1991}
}

% Organiz. of cortex, inhibitory interneurons
@incollection{stefanis2020,
  title={Interneuronal mechanisms in the cortex},
  author={Stefanis, Costas},
  booktitle={The interneuron},
  pages={497--526},
  year={2020},
  publisher={University of California Press}
}

% Organiz. of cortex, types of inhibitory neurons - in particular, sst neurons target dentrites, pv neurons target soma providing strong inhibition
@article{kubota2016,
  title={The diversity of cortical inhibitory synapses},
  author={Kubota, Yoshiyuki and Karube, Fuyuki and Nomura, Masaki and Kawaguchi, Yasuo},
  journal={Frontiers in neural circuits},
  volume={10},
  pages={27},
  year={2016},
  publisher={Frontiers}
}

% Organization of cortex in columns
@article{mountcastle1997,
  title={The columnar organization of the neocortex.},
  author={Mountcastle, Vernon B},
  journal={Brain: a journal of neurology},
  volume={120},
  number={4},
  pages={701--722},
  year={1997}
}

% Organization of cortical microcircuits: forward excitation, backward excitation and inhibition, lateral inhibition, segregated compartments, multiplicative modulation, recurrent loops, and functions associated with microcircuits.
@article{luo2021,
  title={Architectures of neuronal circuits},
  author={Luo, Liqun},
  journal={Science},
  volume={373},
  number={6559},
  pages={eabg7285},
  year={2021},
  publisher={American Association for the Advancement of Science}
}

% Role of cerebellum, basal ganglia, cortex, possibly related to supervised, reinforcement, unsupervised learning, respectively, although it is still not clear how supervised learning might be implemented.
@article{doya1999,
  title={What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?},
  author={Doya, Kenji},
  journal={Neural networks},
  volume={12},
  number={7-8},
  pages={961--974},
  year={1999},
  publisher={Elsevier}
}

% possible support for backpropagation: instructive signals in auditory cortex of owls
@article{ito2000,
  title={Mechanisms of motor learning in the cerebellum},
  author={Ito, Masao},
  journal={Brain research},
  volume={886},
  number={1-2},
  pages={237--245},
  year={2000},
  publisher={Elsevier}
}

% possible support for backpropagation: instructive signals in auditory cortex of owls
@article{knudsen2002,
  title={Instructed learning in the auditory localization pathway of the barn owl},
  author={Knudsen, Eric I},
  journal={Nature},
  volume={417},
  number={6886},
  pages={322--328},
  year={2002},
  publisher={Nature Publishing Group UK London}
}

% possible support for backpropagation: instructive signals in motor learning
@article{carey2005,
  title={Instructive signals for motor learning from visual cortical area MT},
  author={Carey, Megan R and Medina, Javier F and Lisberger, Stephen G},
  journal={Nature neuroscience},
  volume={8},
  number={6},
  pages={813--819},
  year={2005},
  publisher={Nature Publishing Group US New York}
}

% recurrent connections in the brain
@article{douglas1995,
  title={Recurrent excitation in neocortical circuits},
  author={Douglas, Rodney J and Koch, Christof and Mahowald, Misha and Martin, KA and Suarez, Humbert H},
  journal={Science},
  volume={269},
  number={5226},
  pages={981--985},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

% Hebbian LTP/LTD
@article{bear1996, 
    title={A synaptic basis for memory storage in the cerebral cortex},
    author={Bear, Mark F},
    journal={Proceedings of the National Academy of Sciences},
    volume={93},
    number={24},
    pages={13453--13459},
    year={1996},
    publisher={National Acad Sciences}
}

% STDP
@article{bi1998,
    title={Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type},
    author={Bi, Guo-qiang and Poo, Mu-ming},
    journal={Journal of neuroscience},
    volume={18},
    number={24},
    pages={10464--10472},
    year={1998},
    publisher={Soc Neuroscience}
}

% synaptic delay affected during plasticity
@article{lin2002,
  title={Modulation of synaptic delay during synaptic plasticity},
  author={Lin, Jen-Wei and Faber, Donald S},
  journal={Trends in neurosciences},
  volume={25},
  number={9},
  pages={449--455},
  year={2002},
  publisher={Elsevier}
}

% frequency-dependent synapses: synaptic response depends on the frequency of stimulation and synaptic weight acts as a bandpass filters. This could support RBF-like units.
@article{collingridge1988,
  title={Frequency-dependent N-methyl-D-aspartate receptor-mediated synaptic transmission in rat hippocampus.},
  author={Collingridge, GL and Herron, CE and Lester, RA},
  journal={The Journal of physiology},
  volume={399},
  number={1},
  pages={301--312},
  year={1988},
  publisher={Wiley Online Library}
}

% frequency-dependent synapses
@article{markram1998,
  title={Information processing with frequency-dependent synaptic connections},
  author={Markram, Henry and Gupta, Anirudh and Uziel, Asher and Wang, Yun and Tsodyks, Misha},
  journal={Neurobiology of learning and memory},
  volume={70},
  number={1-2},
  pages={101--112},
  year={1998},
  publisher={Elsevier}
}

% frequency-dependent synaptic plasticity
@article{ito2007,
  title={Frequency-dependent gating of synaptic transmission and plasticity by dopamine},
  author={Ito, Hiroshi T and Schuman, Erin M},
  journal={Frontiers in neural circuits},
  pages={1},
  year={2007},
  publisher={Frontiers}
}

% frequency-dependent synaptic plasticity
@article{kumar2011,
  title={Frequency-dependent changes in NMDAR-dependent synaptic plasticity},
  author={Kumar, Arvind and Mehta, Mayank R},
  journal={Frontiers in computational neuroscience},
  volume={5},
  pages={38},
  year={2011},
  publisher={Frontiers Research Foundation}
}

% frequency-dependent synaptic plasticity
@article{kenney2013,
  title={NMDA receptor-dependent synaptic plasticity in dorsal and intermediate hippocampus exhibits distinct frequency-dependent profiles},
  author={Kenney, Jana and Manahan-Vaughan, Denise},
  journal={Neuropharmacology},
  volume={74},
  pages={108--118},
  year={2013},
  publisher={Elsevier}
}

% reward driven learning
@article{schultz1997,
  title={A neural substrate of prediction and reward},
  author={Schultz, Wolfram and Dayan, Peter and Montague, P Read},
  journal={Science},
  volume={275},
  number={5306},
  pages={1593--1599},
  year={1997},
  publisher={American Association for the Advancement of Science}
}

% third factor modulates plasticity
@article{seol2007,
  title={Neuromodulators control the polarity of spike-timing-dependent synaptic plasticity},
  author={Seol, Geun Hee and Ziburkus, Jokubas and Huang, ShiYong and Song, Lihua and Kim, In Tae and Takamiya, Kogo and Huganir, Richard L and Lee, Hey-Kyoung and Kirkwood, Alfredo},
  journal={Neuron},
  volume={55},
  number={6},
  pages={919--929},
  year={2007},
  publisher={Elsevier}
}

% two rl systems in the brain: goal-directed (model-based, accurate but slow, prefrontal striate system) and habitual (model-free, less accurate but faster, dorsolateral striate system). speed-accuracy trade-off between the two systems. also, model-based system provides simulated experience to train the model free system.
@article{gershman2014,
  title={Retrospective revaluation in sequential decision making: A tale of two systems.},
  author={Gershman, Samuel J and Markman, Arthur B and Otto, A Ross},
  journal={Journal of Experimental Psychology: General},
  volume={143},
  number={1},
  pages={182},
  year={2014},
  publisher={American Psychological Association}
}

% intrinsic motivation/curiosity in the brain for exploration and novelty search
@article{berlyne1966,
  title={Curiosity and exploration},
  author={Berlyne, Daniel E},
  journal={Science},
  volume={153},
  number={3731},
  pages={25--33},
  year={1966},
  publisher={JSTOR}
}

% Synaptic traces to solve the distal reward problem
@article{izhikevich2007,
  title={Solving the distal reward problem through linkage of STDP and dopamine signaling},
  author={Izhikevich, Eugene M},
  journal={Cerebral cortex},
  volume={17},
  number={10},
  pages={2443--2452},
  year={2007},
  publisher={Oxford University Press}
}

% Biological support for synaptic traces
@article{yagishita2014,
  title={A critical time window for dopamine actions on the structural plasticity of dendritic spines},
  author={Yagishita, Sho and Hayashi-Takagi, Akiko and Ellis-Davies, Graham CR and Urakubo, Hidetoshi and Ishii, Shin and Kasai, Haruo},
  journal={Science},
  volume={345},
  number={6204},
  pages={1616--1620},
  year={2014},
  publisher={American Association for the Advancement of Science}
}

% Biological evidence of eligibility traces
@article{shindou2019,
  title={A silent eligibility trace enables dopamine-dependent synaptic plasticity for reinforcement learning in the mouse striatum},
  author={Shindou, Tomomi and Shindou, Mayumi and Watanabe, Sakurako and Wickens, Jeffery},
  journal={European Journal of Neuroscience},
  volume={49},
  number={5},
  pages={726--736},
  year={2019},
  publisher={Wiley Online Library}
}

% synaptic redistribution/heterosynaptic competition
@article{lo1991b,
  title={Activity-dependent synaptic competition in vitro: heterosynaptic suppression of developing synapses},
  author={Lo, Yi-Jiuan and Poo, Mu-ming},
  journal={Science},
  volume={254},
  number={5034},
  pages={1019--1022},
  year={1991},
  publisher={American Association for the Advancement of Science}
}

% synaptic redistribution/heterosynaptic competition
@article{markram1996,
  title={Redistribution of synaptic efficacy between neocortical pyramidal neurons},
  author={Markram, Henry and Tsodyks, Misha},
  journal={Nature},
  volume={382},
  number={6594},
  pages={807--810},
  year={1996},
  publisher={Nature Publishing Group}
}

% homeostatic plasticity/synaptic scaling, activity dependent synaptic scaling
@article{turrigiano1998,
  title={Activity-dependent scaling of quantal amplitude in neocortical neurons},
  author={Turrigiano, Gina G and Leslie, Kenneth R and Desai, Niraj S and Rutherford, Lana C and Nelson, Sacha B},
  journal={Nature},
  volume={391},
  number={6670},
  pages={892--896},
  year={1998},
  publisher={Nature Publishing Group}
}

% homeostatic plasticity/synaptic scaling, activity dependent synaptic scaling
@article{vanrossum2000,
  title={Stable Hebbian learning from spike timing-dependent plasticity},
  author={Van Rossum, Mark CW and Bi, Guo Qiang and Turrigiano, Gina G},
  journal={Journal of neuroscience},
  volume={20},
  number={23},
  pages={8812--8821},
  year={2000},
  publisher={Soc Neuroscience}
}

% homeostatic plasticity/synaptic scaling
@article{turrigiano2008,
  title={The self-tuning neuron: synaptic scaling of excitatory synapses},
  author={Turrigiano, Gina G},
  journal={Cell},
  volume={135},
  number={3},
  pages={422--435},
  year={2008},
  publisher={Elsevier}
}

% metaplasticity (change of plasticity based on previous activity)
@article{abraham1996,
  title={Metaplasticity: the plasticity of synaptic plasticity},
  author={Abraham, Wickliffe C and Bear, Mark F},
  journal={Trends in neurosciences},
  volume={19},
  number={4},
  pages={126--130},
  year={1996},
  publisher={Elsevier}
}

% mechanisms of metaplasticity
@article{abraham2008,
  title={Metaplasticity: tuning synapses and networks for plasticity},
  author={Abraham, Wickliffe C},
  journal={Nature Reviews Neuroscience},
  volume={9},
  number={5},
  pages={387--387},
  year={2008},
  publisher={Nature Publishing Group}
}

% multiple forms of plasticity in neurons
@article{burrone2002,
  title={Multiple forms of synaptic plasticity triggered by selective suppression of activity in individual neurons},
  author={Burrone, Juan and O'Byrne, Michael and Murthy, Venkatesh N},
  journal={Nature},
  volume={420},
  number={6914},
  pages={414--418},
  year={2002},
  publisher={Nature Publishing Group UK London}
}

% diverse synaptic plasticity mechanisms orchestrated in the brain
@article{zenke2015,
  title={Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks},
  author={Zenke, Friedemann and Agnes, Everton J and Gerstner, Wulfram},
  journal={Nature communications},
  volume={6},
  number={1},
  pages={1--13},
  year={2015},
  publisher={Nature Publishing Group}
}

% bap 
@article{williams2000,
  title={Backpropagation of physiological spike trains in neocortical pyramidal neurons: implications for temporal coding in dendrites},
  author={Williams, Stephen R and Stuart, Greg J},
  journal={Journal of Neuroscience},
  volume={20},
  number={22},
  pages={8238--8246},
  year={2000},
  publisher={Soc Neuroscience}
}

% Compartment model and segregated dendrites suggest 2 variables are used to represent activity and error in neurons
@article{kording2001,
  title={Supervised and unsupervised learning with two sites of synaptic integration},
  author={K{\"o}rding, Konrad P and K{\"o}nig, Peter},
  journal={Journal of computational neuroscience},
  volume={11},
  number={3},
  pages={207--215},
  year={2001},
  publisher={Springer}
}

% Dendritic computation
@article{london2005,
  title={Dendritic computation},
  author={London, Michael and H{\"a}usser, Michael},
  journal={Annu. Rev. Neurosci.},
  volume={28},
  pages={503--532},
  year={2005},
  publisher={Annual Reviews}
}

% Dendritic computation: biological neuron as two layer neural network
@article{poirazi2003,
  title={Pyramidal neuron as two-layer neural network},
  author={Poirazi, Panayiota and Brannon, Terrence and Mel, Bartlett W},
  journal={Neuron},
  volume={37},
  number={6},
  pages={989--999},
  year={2003},
  publisher={Elsevier}
}

% backward/retrograde chemical signaling
@article{wilson2001,
  title={Endogenous cannabinoids mediate retrograde signalling at hippocampal synapses},
  author={Wilson, Rachel I and Nicoll, Roger A},
  journal={Nature},
  volume={410},
  number={6828},
  pages={588--592},
  year={2001},
  publisher={Nature Publishing Group}
}

% backward/retrograde chemical signaling
@article{arancio1996,
  title={Nitric oxide acts directly in the presynaptic neuron to produce long-term potentiationin cultured hippocampal neurons},
  author={Arancio, Ottavio and Kiebler, Michael and Lee, C Justin and Lev-Ram, Varda and Tsien, Roger Y and Kandel, Eric R and Hawkins, Robert D},
  journal={Cell},
  volume={87},
  number={6},
  pages={1025--1035},
  year={1996},
  publisher={Elsevier}
}

% Lateral inibition
@article{gabbot1986, 
    title={Quantitative distribution of GABA-immunoreactive neurons in the visual cortex (area 17) of the cat},
    author={Gabbott, PLA and Somogyi, P},
    journal={Experimental Brain Research},
    volume={61},
    number={2},
    pages={323--331},
    year={1986},
    publisher={Springer}
}

% non negative sparse coding in the brain
@article{beyeler2019,
  title={Neural correlates of sparse coding and dimensionality reduction},
  author={Beyeler, Michael and Rounds, Emily L and Carlson, Kristofor D and Dutt, Nikil and Krichmar, Jeffrey L},
  journal={PLoS computational biology},
  volume={15},
  number={6},
  pages={e1006908},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

% sparsity of neural activity in the brain: 1%
@article{lennie2003,
  title={The cost of cortical computation},
  author={Lennie, Peter},
  journal={Current biology},
  volume={13},
  number={6},
  pages={493--497},
  year={2003},
  publisher={Elsevier}
}

% temporal codes - neurons have very precise firing times, suggesting temporal coding
@article{mainen1995,
  title={Reliability of spike timing in neocortical neurons},
  author={Mainen, Zachary F and Sejnowski, Terrence J},
  journal={Science},
  volume={268},
  number={5216},
  pages={1503--1506},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

% temporal codes - neurons have very precise firing times, suggesting temporal coding
@article{gerstner1996,
  title={A neuronal learning rule for sub-millisecond temporal coding},
  author={Gerstner, Wulfram and Kempter, Richard and Van Hemmen, J Leo and Wagner, Hermann},
  journal={Nature},
  volume={383},
  number={6595},
  pages={76--78},
  year={1996},
  publisher={Nature Publishing Group}
}

% population codes
@article{averbeck2006,
  title={Neural correlations, population coding and computation},
  author={Averbeck, Bruno B and Latham, Peter E and Pouget, Alexandre},
  journal={Nature reviews neuroscience},
  volume={7},
  number={5},
  pages={358--366},
  year={2006},
  publisher={Nature Publishing Group}
}

% population codes
@article{wohrer2013,
  title={Population-wide distributions of neural activity during perceptual decision-making},
  author={Wohrer, Adrien and Humphries, Mark D and Machens, Christian K},
  journal={Progress in neurobiology},
  volume={103},
  pages={156--193},
  year={2013},
  publisher={Elsevier}
}

% Brain consumes 20W
@article{javed2010,
  title={Brain and high metabolic rate organ mass: contributions to resting energy expenditure beyond fat-free mass},
  author={Javed, Fahad and He, Qing and Davidson, Lance E and Thornton, John C and Albu, Jeanine and Boxt, Lawrence and Krasnow, Norman and Elia, Marinos and Kang, Patrick and Heshka, Stanley and others},
  journal={The American journal of clinical nutrition},
  volume={91},
  number={4},
  pages={907--912},
  year={2010},
  publisher={Oxford University Press}
}

% multiplicative responses in the brain
@article{salinas1996,
  title={A model of multiplicative neural responses in parietal cortex},
  author={Salinas, Emilio and Abbott, Laurence F},
  journal={Proceedings of the national academy of sciences},
  volume={93},
  number={21},
  pages={11956--11961},
  year={1996},
  publisher={National Acad Sciences}
}

% attentional feedback in the brain
@article{treue2001,
  title={Neural correlates of attention in primate visual cortex},
  author={Treue, Stefan},
  journal={Trends in neurosciences},
  volume={24},
  number={5},
  pages={295--300},
  year={2001},
  publisher={Elsevier}
}

% saliency in the brain: hippocampus does experience replay based on interestingness
@article{singer2009,
  title={Rewarded outcomes enhance reactivation of experience in the hippocampus},
  author={Singer, Annabelle C and Frank, Loren M},
  journal={Neuron},
  volume={64},
  number={6},
  pages={910--921},
  year={2009},
  publisher={Elsevier}
}

% divisive normalization
@article{carandini2012,
  title={Normalization as a canonical neural computation},
  author={Carandini, Matteo and Heeger, David J},
  journal={Nature Reviews Neuroscience},
  volume={13},
  number={1},
  pages={51--62},
  year={2012},
  publisher={Nature Publishing Group}
}

% persistent dynamic attractors/persistent activity in the brain
@article{brody2003,
  title={Basic mechanisms for graded persistent activity: discrete attractors, continuous attractors, and dynamic representations},
  author={Brody, Carlos D and Romo, Ranulfo and Kepecs, Adam},
  journal={Current opinion in neurobiology},
  volume={13},
  number={2},
  pages={204--211},
  year={2003},
  publisher={Elsevier}
}

% transient dynamics in the brain carry a relevant portion of information about the input, suggesting reservoir computing
@article{mazor2005,
  title={Transient dynamics versus fixed points in odor representations by locust antennal lobe projection neurons},
  author={Mazor, Ofer and Laurent, Gilles},
  journal={Neuron},
  volume={48},
  number={4},
  pages={661--673},
  year={2005},
  publisher={Elsevier}
}

% transient dynamics in the brain carry a relevant portion of information about the input, suggesting reservoir computing
@article{durstewitz2008,
  title={Computational significance of transient dynamics in cortical networks},
  author={Durstewitz, Daniel and Deco, Gustavo},
  journal={European Journal of Neuroscience},
  volume={27},
  number={1},
  pages={217--227},
  year={2008},
  publisher={Wiley Online Library}
}

% self-organizing synaptic and hoemostatic plasticity mechanisms reproduce synaptic distribution observed in cortex
@article{zheng2013,
  title={Network self-organization explains the statistics and dynamics of synaptic connection strengths in cortex},
  author={Zheng, Pengsheng and Dimitrakakis, Christos and Triesch, Jochen},
  journal={PLoS computational biology},
  volume={9},
  number={1},
  pages={e1002848},
  year={2013},
  publisher={Public Library of Science San Francisco, USA}
}

% Motion illusion reproduced in DNN with temporal predictive coding.
@article{watanabe2018,
  title={Illusory motion reproduced by deep neural networks trained for prediction},
  author={Watanabe, Eiji and Kitaoka, Akiyoshi and Sakamoto, Kiwako and Yasugi, Masaki and Tanaka, Kenta},
  journal={Frontiers in psychology},
  volume={9},
  pages={345},
  year={2018},
  publisher={Frontiers}
}

% Unsupervised learning with spatio-temporal predictive coding reproduce biological behavior of visual cortex
@article{zhuang2021,
  title={Unsupervised neural network models of the ventral visual stream},
  author={Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C and DiCarlo, James J and Yamins, Daniel LK},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={3},
  year={2021},
  publisher={National Acad Sciences}
}


%%%%%%%%%%
% neuronal cultures
%%%%%%%%%%

% Recognition of L and ups-L patterns
@article{ruaro2005,
  title={Toward the neurocomputer: image processing and pattern recognition with neuronal cultures},
  author={Ruaro, Maria Elisabetta and Bonifazi, Paolo and Torre, Vincent},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={52},
  number={3},
  pages={371--383},
  year={2005},
  publisher={IEEE}
}

% Neural cultures performing BSS
@article{isomura2015,
  title={Cultured cortical neurons can perform blind source separation according to the free-energy principle},
  author={Isomura, Takuya and Kotani, Kiyoshi and Jimbo, Yasuhiko},
  journal={PLoS computational biology},
  volume={11},
  number={12},
  year={2015},
  publisher={Public Library of Science}
}

% Temporally repeated pattern
@article{shahaf2001,
  title={Learning in networks of cortical neurons},
  author={Shahaf, Goded and Marom, Shimon},
  journal={Journal of Neuroscience},
  volume={21},
  number={22},
  pages={8782--8788},
  year={2001},
  publisher={Soc Neuroscience}
}

% Temporal interval learning
@article{goel2016,
  title={Temporal interval learning in cortical cultures is encoded in intrinsic network dynamics},
  author={Goel, Anubhuti and Buonomano, Dean V},
  journal={Neuron},
  volume={91},
  number={2},
  pages={320--327},
  year={2016},
  publisher={Elsevier}
}

% persistent and synchronous stimulation of adjacent electrodes can create desired connectivity patterns
@article{ferrandez2013,
  title={Training biological neural cultures: Towards hebbian learning},
  author={Ferr{\'a}ndez, Jos{\'e} Manuel and Lorente, Victor and de la Paz, F{\'e}lix and Fern{\'a}ndez, Eduardo},
  journal={Neurocomputing},
  volume={114},
  pages={3--8},
  year={2013},
  publisher={Elsevier}
}

% identification of exc-inh links - neural activity patterns
@article{pastore2018,
  title={Identification of excitatory-inhibitory links and network topology in large-scale neuronal assemblies from multi-electrode recordings},
  author={Pastore, Vito Paolo and Massobrio, Paolo and Godjoski, Aleksandar and Martinoia, Sergio},
  journal={PLoS computational biology},
  volume={14},
  number={8},
  pages={e1006381},
  year={2018},
  publisher={Public Library of Science}
}

% neural cultures interfaced through MEA learn to play pong
@article{kagan2022,
  title={In vitro neurons learn and exhibit sentience when embodied in a simulated game-world},
  author={Kagan, Brett J and Kitchen, Andy C and Tran, Nhi T and Habibollahi, Forough and Khajehnejad, Moein and Parker, Bradyn J and Bhat, Anjali and Rollo, Ben and Razi, Adeel and Friston, Karl J},
  journal={Neuron},
  volume={110},
  number={23},
  pages={3952--3969},
  year={2022},
  publisher={Elsevier}
}

% neural activity patterns
@article{li2007,
  title={Dynamics of learning in cultured neuronal networks with antagonists of glutamate receptors},
  author={Li, Yanling and Zhou, Wei and Li, Xiangning and Zeng, Shaoqun and Luo, Qingming},
  journal={Biophysical journal},
  volume={93},
  number={12},
  pages={4151--4158},
  year={2007},
  publisher={Elsevier}
}

% neural activity patterns
@article{hammond2013,
  title={Endogenous cholinergic tone modulates spontaneous network level neuronal activity in primary cortical cultures grown on multi-electrode arrays},
  author={Hammond, Mark W and Xydas, Dimitris and Downes, Julia H and Bucci, Giovanna and Becerra, Victor and Warwick, Kevin and Constanti, Andrew and Nasuto, Slawomir J and Whalley, Benjamin J},
  journal={BMC neuroscience},
  volume={14},
  number={1},
  pages={38},
  year={2013},
  publisher={Springer}
}

% neural activity patterns
@article{bettencourt2007,
  title={Functional structure of cortical neuronal networks grown in vitro},
  author={Bettencourt, Luis MA and Stephens, Greg J and Ham, Michael I and Gross, Guenter W},
  journal={Physical Review E},
  volume={75},
  number={2},
  pages={021915},
  year={2007},
  publisher={APS}
}

% neural activity patterns
@article{wagenaar2006,
  title={Persistent dynamic attractors in activity patterns of cultured neuronal networks},
  author={Wagenaar, Daniel A and Nadasdy, Zoltan and Potter, Steve M},
  journal={Physical Review E},
  volume={73},
  number={5},
  pages={051907},
  year={2006},
  publisher={APS}
}


%%%%%%%%%%
% MEAs, neuroelectronic and light-based interfaces, hybrots
%%%%%%%%%%

% mea first presentation
@article{gross1977,
  title={A new fixed-array multi-microelectrode system designed for long-term monitoring of extracellular single unit neuronal activity in vitro},
  author={Gross, Guenter W and Rieske, E and Kreutzberg, GW and Meyer, A},
  journal={Neuroscience letters},
  volume={6},
  number={2-3},
  pages={101--105},
  year={1977},
  publisher={Elsevier}
}

% mea first recording study
@article{pine1980,
  title={Recording action potentials from cultured neurons with extracellular microcircuit electrodes},
  author={Pine, Jerome},
  journal={Journal of neuroscience methods},
  volume={2},
  number={1},
  pages={19--31},
  year={1980},
  publisher={Elsevier}
}

% mea
@article{novak1986,
  title={Recording from the Aplysia abdominal ganglion with a planar microelectrode array},
  author={Novak, James L and Wheeler, Bruce C},
  journal={IEEE transactions on biomedical engineering},
  pages={196--202},
  year={1986},
  publisher={IEEE}
}

% mea
@article{jimbo1992,
  title={Electrical stimulation and recording from cultured neurons using a planar electrode array},
  author={Jimbo, Yasuhiko and Kawana, Akio},
  journal={Bioelectrochemistry and Bioenergetics},
  volume={29},
  number={2},
  pages={193--204},
  year={1992},
  publisher={Elsevier}
}

% mea
@article{martinoia1993,
  title={A general-purpose system for long-term recording from a microelectrode array coupled to excitable cells.},
  author={Martinoia, S and Bove, M and Carlini, G and Ciccarelli, C and Grattarola, M and Storment, C and Kovacs, G},
  journal={Journal of neuroscience methods},
  volume={48},
  number={1-2},
  pages={115--121},
  year={1993}
}

% mea
@article{zeck2001,
  title={Noninvasive neuroelectronic interfacing with synaptically connected snail neurons immobilized on a semiconductor chip},
  author={Zeck, G{\"u}nther and Fromherz, Peter},
  journal={Proceedings of the National Academy of Sciences},
  volume={98},
  number={18},
  pages={10457--10462},
  year={2001},
  publisher={National Acad Sciences}
}

% mea
@article{bonifazi2002,
  title={Silicon Chip for Electronic Communication Between Nerve Cells by Non-invasive Interfacing and Analog--Digital Processing},
  author={Bonifazi, Paolo and Fromherz, Peter},
  journal={Advanced materials},
  volume={14},
  number={17},
  pages={1190--1193},
  year={2002},
  publisher={Wiley Online Library}
}

% neuroelectronic interface
@article{grahn2014,
  title={Restoration of motor function following spinal cord injury via optimal control of intraspinal microstimulation: toward a next generation closed-loop neural prosthesis},
  author={Grahn, Peter J and Mallory, Grant W and Berry, B Michael and Hachmann, Jan T and Lobel, Darlene A and Lujan, J Luis},
  journal={Frontiers in neuroscience},
  volume={8},
  pages={296},
  year={2014},
  publisher={Frontiers}
}

% neuroelectronic interface
@article{bonifazi2013,
  title={In vitro large-scale experimental and theoretical studies for the realization of bi-directional brain-prostheses},
  author={Bonifazi, Paolo and Difato, Francesco and Massobrio, Paolo and Breschi, Gian Luca and Pasquale, Valentina and Levi, Timoth{\'e}e and Goldin, Miri and Bornat, Yannick and Tedesco, Mariateresa and Bisio, Marta and others},
  journal={Frontiers in neural circuits},
  volume={7},
  pages={40},
  year={2013},
  publisher={Frontiers}
}

% neuroelectronic interface
@article{berco2019,
  title={Recent Progress in Synaptic Devices Paving the Way toward an Artificial Cogni-Retina for Bionic and Machine Vision},
  author={Berco, Dan and Shenp Ang, Diing},
  journal={Advanced Intelligent Systems},
  volume={1},
  number={1},
  pages={1900003},
  year={2019},
  publisher={Wiley Online Library}
}

% neuroelectronic interface
@article{keene2020,
  title={A biohybrid synapse with neurotransmitter-mediated plasticity},
  author={Keene T, Scott and Lubrano, Claudia and Kazemzadeh, Setareh and  Melianas, Armantas and Tuchman, Yaakov and Polino, Giuseppina and Scognamiglio, Paola and Cin, Lucio and Salleo, Alberto and van de Burgt, Yoeri and Santoro, Francesca },
  journal={Nature Materials},
  year={2020},
  publisher={Nature}
}

% neuroelectronic interfaces review
@article{lebedev2017,
  title={Brain-machine interfaces: From basic science to neuroprostheses and neurorehabilitation},
  author={Lebedev, Mikhail A and Nicolelis, Miguel AL},
  journal={Physiological reviews},
  volume={97},
  number={2},
  pages={767--837},
  year={2017},
  publisher={American Physiological Society Bethesda, MD}
}

% visual neuroprosthesis review
@article{yu2020,
  title={Toward the next generation of retinal neuroprosthesis: Visual computation with spikes},
  author={Yu, Zhaofei and Liu, Jian K and Jia, Shanshan and Zhang, Yichen and Zheng, Yajing and Tian, Yonghong and Huang, Tiejun},
  journal={Engineering},
  volume={6},
  number={4},
  pages={449--461},
  year={2020},
  publisher={Elsevier}
}

% optogenetic light based neural interface
@article{meloni2020,
  title={Controlling the behaviour of Drosophila melanogaster via smartphone optogenetics},
  author={Meloni, Ilenia and Sachidanandan, Divya and Thum, Andreas S and Kittel, Robert J and Murawski, Caroline},
  journal={Scientific Reports},
  volume={10},
  number={1},
  pages={1--11},
  year={2020},
  publisher={Springer}
}

% hybrots
@inproceedings{potter2003,
  title={Long-term bidirectional neuron interfaces for robotic control, and in vitro learning studies},
  author={Potter, Steve M and Wagenaar, Daniel A and Madhavan, Radhika and DeMarse, Thomas B},
  booktitle={Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No. 03CH37439)},
  volume={4},
  pages={3690--3693},
  year={2003},
  organization={IEEE}
}

% hybrots
@article{bakkum2007a,
  title={MEART: the semi-living artist},
  author={Bakkum, Douglas J and Gamblen, Philip M and Ben-Ary, Guy and Chao, Zenas C and Potter, Steve M},
  journal={Frontiers in neurorobotics},
  volume={1},
  pages={5},
  year={2007},
  publisher={Frontiers}
}

% hybrots
@inproceedings{bakkum2007b,
  title={Embodying cultured networks with a robotic drawing arm},
  author={Bakkum, Douglas J and Chao, Zenas C and Gamblen, Phil and Ben-Ary, Guy and Shkolnik, Alec G and DeMarse, Thomas B and Potter, Steve M},
  booktitle={2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  pages={2996--2999},
  year={2007},
  organization={IEEE}
}

% hybrots
@article{demarse2001,
  title={The neurally controlled animat: biological brains acting with simulated bodies},
  author={DeMarse, Thomas B and Wagenaar, Daniel A and Blau, Axel W and Potter, Steve M},
  journal={Autonomous robots},
  volume={11},
  number={3},
  pages={305--310},
  year={2001},
  publisher={Springer}
}

% hybrots
@article{chao2005,
  title={Effects of random external background stimulation on network synaptic stability after tetanization},
  author={Chao, Zenas C and Bakkum, Douglas J and Wagenaar, Daniel A and Potter, Steve M},
  journal={Neuroinformatics},
  volume={3},
  number={3},
  pages={263--280},
  year={2005},
  publisher={Springer}
}


%%%%%%%%%%
% neuron cultivation
%%%%%%%%%%

@article{eiraku2008,
  title={Self-organized formation of polarized cortical tissues from ESCs and its active manipulation by extrinsic signals},
  author={Eiraku, Mototsugu and Watanabe, Kiichi and Matsuo-Takasaki, Mami and Kawada, Masako and Yonemura, Shigenobu and Matsumura, Michiru and Wataya, Takafumi and Nishiyama, Ayaka and Muguruma, Keiko and Sasai, Yoshiki},
  journal={Cell stem cell},
  volume={3},
  number={5},
  pages={519--532},
  year={2008},
  publisher={Elsevier}
}

@article{gaspard2008,
  title={An intrinsic mechanism of corticogenesis from embryonic stem cells},
  author={Gaspard, Nicolas and Bouschet, Tristan and Hourez, Raphael and Dimidschstein, Jordane and Naeije, Gilles and Van den Ameele, Jelle and Espuny-Camacho, Ira and Herpoel, Ad{\`e}le and Passante, Lara and Schiffmann, Serge N and others},
  journal={Nature},
  volume={455},
  number={7211},
  pages={351--357},
  year={2008},
  publisher={Nature Publishing Group}
}

@article{chambers2009,
  title={Highly efficient neural conversion of human ES and iPS cells by dual inhibition of SMAD signaling},
  author={Chambers, Stuart M and Fasano, Christopher A and Papapetrou, Eirini P and Tomishima, Mark and Sadelain, Michel and Studer, Lorenz},
  journal={Nature biotechnology},
  volume={27},
  number={3},
  pages={275--280},
  year={2009},
  publisher={Nature Publishing Group}
}

@article{terrigno2018,
  title={Neurons generated by mouse ESCs with hippocampal or cortical identity display distinct projection patterns when co-transplanted in the adult brain},
  author={Terrigno, Marco and Busti, Irene and Alia, Claudia and Pietrasanta, Marta and Arisi, Ivan and D'Onofrio, Mara and Caleo, Matteo and Cremisi, Federico},
  journal={Stem cell reports},
  volume={10},
  number={3},
  pages={1016--1029},
  year={2018},
  publisher={Elsevier}
}

@article{gonccalves2016,
  title={Adult neurogenesis in the hippocampus: from stem cells to behavior},
  author={Gon{\c{c}}alves, J Tiago and Schafer, Simon T and Gage, Fred H},
  journal={Cell},
  volume={167},
  number={4},
  pages={897--914},
  year={2016},
  publisher={Elsevier}
}

@article{bertacchi2015,
  title={The double inhibition of endogenously produced BMP and W nt factors synergistically triggers dorsal telencephalic differentiation of mouse ES cells},
  author={Bertacchi, Michele and Pandolfini, Luca and D'Onofrio, Mara and Brandi, Rossella and Cremisi, Federico},
  journal={Developmental neurobiology},
  volume={75},
  number={1},
  pages={66--79},
  year={2015},
  publisher={Wiley Online Library}
}

@article{skorheim2014,
  title={A spiking network model of decision making employing rewarded STDP},
  author={Skorheim, Steven and Lonjers, Peter and Bazhenov, Maxim},
  journal={PloS one},
  volume={9},
  number={3},
  pages={e90821},
  year={2014},
  publisher={Public Library of Science}
}

@article{je2013,
  title={ProBDNF and mature BDNF as punishment and reward signals for synapse elimination at mouse neuromuscular junctions},
  author={Je, H Shawn and Yang, Feng and Ji, Yuanyuan and Potluri, Srilatha and Fu, Xiu-Qing and Luo, Zhen-Ge and Nagappan, Guhan and Chan, Jia Pei and Hempstead, Barbara and Son, Young-Jin and others},
  journal={Journal of Neuroscience},
  volume={33},
  number={24},
  pages={9957--9962},
  year={2013},
  publisher={Soc Neuroscience}
}

@incollection{lu2014,
  title={BDNF and synaptic plasticity, cognitive function, and dysfunction},
  author={Lu, B and Nagappan, G and Lu, Y},
  booktitle={Neurotrophic factors},
  pages={223--250},
  year={2014},
  publisher={Springer}
}


%%%%%%%%%%
% datasets, frameworks and tools
%%%%%%%%%%

@article{brainscore,
  title={Brain-score: Which artificial neural network for object recognition is most brain-like?},
  author={Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J and Rajalingham, Rishi and Issa, Elias B and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and others},
  journal={BioRxiv},
  pages={407007},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@article{nmnist,
  title={Converting static image datasets to spiking neuromorphic datasets using saccades},
  author={Orchard, Garrick and Jayawant, Ajinkya and Cohen, Gregory K and Thakor, Nitish},
  journal={Frontiers in neuroscience},
  volume={9},
  pages={437},
  year={2015},
  publisher={Frontiers},
}

@article{svhn,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@misc{cifar,
    title={Learning multiple layers of features from tiny images},
    author={Krizhevsky, Alex and Hinton, Geoffrey},
    year={2009},
}

@article{cifar10dvs,
  title={Cifar10-dvs: an event-stream dataset for object classification},
  author={Li, Hongmin and Liu, Hanchao and Ji, Xiangyang and Li, Guoqi and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={11},
  pages={309},
  year={2017},
  publisher={Frontiers}
}

@inproceedings{norb,
  title={Learning methods for generic object recognition with invariance to pose and lighting},
  author={LeCun, Yann and Huang, Fu Jie and Bottou, Leon},
  booktitle={Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
  volume={2},
  pages={II--104},
  year={2004},
  organization={IEEE},
}

@inproceedings{eth80,
  title={Analyzing appearance and contour based methods for object categorization},
  author={Leibe, Bastian and Schiele, Bernt},
  booktitle={2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},
  volume={2},
  pages={II--409},
  year={2003},
  organization={IEEE},
}

@inproceedings{stl10,
  title={An analysis of single-layer networks in unsupervised feature learning},
  author={Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={215--223},
  year={2011},
}

@inproceedings{caltech101,
  title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle={2004 conference on computer vision and pattern recognition workshop},
  pages={178--178},
  year={2004},
  organization={IEEE},
}

@article{caltech256,
  title={Caltech-256 object category dataset},
  author={Griffin, Gregory and Holub, Alex and Perona, Pietro},
  year={2007},
  publisher={California Institute of Technology}
}

@article{ncaltech,
  title={Converting static image datasets to spiking neuromorphic datasets using saccades},
  author={Orchard, Garrick and Jayawant, Ajinkya and Cohen, Gregory K and Thakor, Nitish},
  journal={Frontiers in neuroscience},
  volume={9},
  pages={437},
  year={2015},
  publisher={Frontiers},
}

@techreport{tinyimagenet,
  title={Tiny ImageNet Challenge},
  author={Wu, Jiayu and Zhang, Qixiang and Xu, Guoxi},
  year={2017},
  institution={Stanford University}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{pascalvoc,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  number={2},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{omniglot,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science},
}

@online{python,
    organization = {Python Software Foundation},
    title = {The Python Programming Language},
    url = {https://www.python.org/}
}

@online{pytorch,
    title = {PyTorch Website},
    url = {https://pytorch.org/}
}

@article{brian,
    title={Brian: a simulator for spiking neural networks in python},
    author={Goodman, Dan FM and Brette, Romain},
    journal={Frontiers in neuroinformatics},
    volume={2},
    pages={5},
    year={2008},
    publisher={Frontiers},
    url = {http://briansimulator.org/}
}

@article{nest,
  author  = {Marc-Oliver Gewaltig and Markus Diesmann},
  title   = {NEST (NEural Simulation Tool)},
  journal = {Scholarpedia},
  year    = {2007},
  volume  = {2},
  pages   = {1430},
  number  = {4},
  url = {nest-simulator.org}
}

@article{pynn,
  title={PyNN: a common interface for neuronal network simulators},
  author={Davison, Andrew P and Br{\"u}derle, Daniel and Eppler, Jochen M and Kremkow, Jens and Muller, Eilif and Pecevski, Dejan and Perrinet, Laurent and Yger, Pierre},
  journal={Frontiers in neuroinformatics},
  volume={2},
  pages={11},
  year={2009},
  publisher={Frontiers},
  url = {https://neuralensemble.org/PyNN/}
}

@online{hbp_neuromorphic_platform,
  organization = {Human Brain Project},
  title = {Human Brain Project Neuromorphic Computing Platform},
  url = {https://www.humanbrainproject.eu/en/silicon-brains/neuromorphic-computing-platform/}
}

@article{bindsnet,
  title={BindsNET: A machine learning-oriented spiking neural networks library in Python},
  author={Hazan, Hananel and Saunders, Daniel J and Khan, Hassaan and Sanghavi, Darpan T and Siegelmann, Hava T and Kozma, Robert},
  journal={Frontiers in neuroinformatics},
  volume={12},
  pages={89},
  year={2018},
  publisher={Frontiers},
  url={https://github.com/BindsNET/bindsnet}
}

% snntorch library for gradient based snn training. also a survey on snn training
@article{snntorch,
  title={Training spiking neural networks using lessons from deep learning},
  author={Eshraghian, Jason K and Ward, Max and Neftci, Emre and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D},
  journal={arXiv preprint arXiv:2109.12894},
  year={2021},
  url={https://snntorch.readthedocs.io/en/latest/readme.html}
}

