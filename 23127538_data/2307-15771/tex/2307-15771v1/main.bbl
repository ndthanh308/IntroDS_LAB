\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain and Bengio(2016)]{alain2016understanding}
G.~Alain and Y.~Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock \emph{arXiv preprint arXiv:1610.01644}, 2016.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
D.~Bahdanau, K.~Cho, and Y.~Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bau et~al.(2018)Bau, Zhu, Strobelt, Zhou, Tenenbaum, Freeman, and
  Torralba]{bau2018gan}
D.~Bau, J.-Y. Zhu, H.~Strobelt, B.~Zhou, J.~B. Tenenbaum, W.~T. Freeman, and
  A.~Torralba.
\newblock Gan dissection: Visualizing and understanding generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:1811.10597}, 2018.

\bibitem[Belrose et~al.(2023)Belrose, Furman, Smith, Halawi, Ostrovsky,
  McKinney, Biderman, and Steinhardt]{belrose2023eliciting}
N.~Belrose, Z.~Furman, L.~Smith, D.~Halawi, I.~Ostrovsky, L.~McKinney,
  S.~Biderman, and J.~Steinhardt.
\newblock Eliciting latent predictions from transformers with the tuned lens.
\newblock \emph{arXiv preprint arXiv:2303.08112}, 2023.

\bibitem[Cammarata et~al.(2020)Cammarata, Goh, Carter, Schubert, Petrov, and
  Olah]{cammarata2020curve}
N.~Cammarata, G.~Goh, S.~Carter, L.~Schubert, M.~Petrov, and C.~Olah.
\newblock Curve detectors.
\newblock \emph{Distill}, 5\penalty0 (6):\penalty0 e00024--003, 2020.

\bibitem[Carter et~al.(2019)Carter, Armstrong, Schubert, Johnson, and
  Olah]{carter2019activation}
S.~Carter, Z.~Armstrong, L.~Schubert, I.~Johnson, and C.~Olah.
\newblock Activation atlas.
\newblock \emph{Distill}, 2019.
\newblock \doi{10.23915/distill.00015}.
\newblock https://distill.pub/2019/activation-atlas.

\bibitem[Chan et~al.(2022)Chan, Garriga-Alonso, Goldwosky-Dill, Greenblatt,
  Nitishinskaya, Radhakrishnan, Shlegeris, and Thomas]{chan2022causal}
L.~Chan, A.~Garriga-Alonso, N.~Goldwosky-Dill, R.~Greenblatt, J.~Nitishinskaya,
  A.~Radhakrishnan, B.~Shlegeris, and N.~Thomas.
\newblock Causal scrubbing, a method for rigorously testing interpretability
  hypotheses.
\newblock \emph{AI Alignment Forum}, 2022.
\newblock
  \url{https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing}.

\bibitem[Chughtai et~al.(2023)Chughtai, Chan, and Nanda]{chughtai2023toy}
B.~Chughtai, L.~Chan, and N.~Nanda.
\newblock A toy model of universality: Reverse engineering how networks learn
  group operations.
\newblock \emph{arXiv preprint arXiv:2302.03025}, 2023.

\bibitem[Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and
  Garriga-Alonso]{conmy2023towards}
A.~Conmy, A.~N. Mavor-Parker, A.~Lynch, S.~Heimersheim, and A.~Garriga-Alonso.
\newblock Towards automated circuit discovery for mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2304.14997}, 2023.

\bibitem[Dar et~al.(2022)Dar, Geva, Gupta, and Berant]{dar2022analyzing}
G.~Dar, M.~Geva, A.~Gupta, and J.~Berant.
\newblock Analyzing transformers in embedding space.
\newblock \emph{arXiv preprint arXiv:2209.02535}, 2022.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds,
  Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{elhage2021mathematical}
N.~Elhage, N.~Nanda, C.~Olsson, T.~Henighan, N.~Joseph, B.~Mann, A.~Askell,
  Y.~Bai, A.~Chen, T.~Conerly, N.~DasSarma, D.~Drain, D.~Ganguli,
  Z.~Hatfield-Dodds, D.~Hernandez, A.~Jones, J.~Kernion, L.~Lovitt, K.~Ndousse,
  D.~Amodei, T.~Brown, J.~Clark, J.~Kaplan, S.~McCandlish, and C.~Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, Grosse, McCandlish, Kaplan, Amodei,
  Wattenberg, and Olah]{elhage2022superposition}
N.~Elhage, T.~Hume, C.~Olsson, N.~Schiefer, T.~Henighan, S.~Kravec,
  Z.~Hatfield-Dodds, R.~Lasenby, D.~Drain, C.~Chen, R.~Grosse, S.~McCandlish,
  J.~Kaplan, D.~Amodei, M.~Wattenberg, and C.~Olah.
\newblock Toy models of superposition.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock https://transformer-circuits.pub/2022/toy\_model/index.html.

\bibitem[Finlayson et~al.(2021)Finlayson, Mueller, Gehrmann, Shieber, Linzen,
  and Belinkov]{finlayson2021causal}
M.~Finlayson, A.~Mueller, S.~Gehrmann, S.~Shieber, T.~Linzen, and Y.~Belinkov.
\newblock Causal analysis of syntactic agreement mechanisms in neural language
  models.
\newblock \emph{arXiv preprint arXiv:2106.06087}, 2021.

\bibitem[Forde et~al.(2022)Forde, Lovering, Konidaris, Pavlick, and
  Littman]{forde2022concepts}
J.~Z. Forde, C.~Lovering, G.~Konidaris, E.~Pavlick, and M.~L. Littman.
\newblock Where, when \& which concepts does alphazero learn? lessons from the
  game of hex.
\newblock In \emph{AAAI Workshop on Reinforcement Learning in Games}, volume~2,
  2022.

\bibitem[Geiger et~al.(2021)Geiger, Lu, Icard, and Potts]{geiger2021causal}
A.~Geiger, H.~Lu, T.~Icard, and C.~Potts.
\newblock Causal abstractions of neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 9574--9586, 2021.

\bibitem[Geiger et~al.(2022)Geiger, Wu, Lu, Rozner, Kreiss, Icard, Goodman, and
  Potts]{geiger2022inducing}
A.~Geiger, Z.~Wu, H.~Lu, J.~Rozner, E.~Kreiss, T.~Icard, N.~Goodman, and
  C.~Potts.
\newblock Inducing causal structure for interpretable neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  7324--7338. PMLR, 2022.

\bibitem[Geiger et~al.(2023{\natexlab{a}})Geiger, Potts, and
  Icard]{geiger2023causal}
A.~Geiger, C.~Potts, and T.~Icard.
\newblock Causal abstraction for faithful model interpretation.
\newblock \emph{arXiv preprint arXiv:2301.04709}, 2023{\natexlab{a}}.

\bibitem[Geiger et~al.(2023{\natexlab{b}})Geiger, Wu, Potts, Icard, and
  Goodman]{geiger2023finding}
A.~Geiger, Z.~Wu, C.~Potts, T.~Icard, and N.~D. Goodman.
\newblock Finding alignments between interpretable causal variables and
  distributed neural representations.
\newblock \emph{arXiv preprint arXiv:2303.02536}, 2023{\natexlab{b}}.

\bibitem[Geva et~al.(2022{\natexlab{a}})Geva, Caciularu, Dar, Roit, Sadde,
  Shlain, Tamir, and Goldberg]{geva2022lm}
M.~Geva, A.~Caciularu, G.~Dar, P.~Roit, S.~Sadde, M.~Shlain, B.~Tamir, and
  Y.~Goldberg.
\newblock Lm-debugger: An interactive tool for inspection and intervention in
  transformer-based language models.
\newblock \emph{arXiv preprint arXiv:2204.12130}, 2022{\natexlab{a}}.

\bibitem[Geva et~al.(2022{\natexlab{b}})Geva, Caciularu, Wang, and
  Goldberg]{geva2022transformer}
M.~Geva, A.~Caciularu, K.~R. Wang, and Y.~Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting
  concepts in the vocabulary space.
\newblock \emph{arXiv preprint arXiv:2203.14680}, 2022{\natexlab{b}}.

\bibitem[Glymour et~al.(2016)Glymour, Pearl, and Jewell]{glymour2016causal}
M.~Glymour, J.~Pearl, and N.~P. Jewell.
\newblock \emph{Causal inference in statistics: A primer}.
\newblock John Wiley \& Sons, 2016.

\bibitem[Goh et~al.(2021)Goh, †, †, Carter, Petrov, Schubert, Radford, and
  Olah]{goh2021multimodal}
G.~Goh, N.~C. †, C.~V. †, S.~Carter, M.~Petrov, L.~Schubert, A.~Radford,
  and C.~Olah.
\newblock Multimodal neurons in artificial neural networks.
\newblock \emph{Distill}, 2021.
\newblock \doi{10.23915/distill.00030}.
\newblock https://distill.pub/2021/multimodal-neurons.

\bibitem[Gurnee et~al.(2023)Gurnee, Nanda, Pauly, Harvey, Troitskii, and
  Bertsimas]{gurnee2023finding}
W.~Gurnee, N.~Nanda, M.~Pauly, K.~Harvey, D.~Troitskii, and D.~Bertsimas.
\newblock Finding neurons in a haystack: Case studies with sparse probing.
\newblock \emph{arXiv preprint arXiv:2305.01610}, 2023.

\bibitem[Halpern(2016)]{halpern2016actual}
J.~Y. Halpern.
\newblock \emph{Actual causality}.
\newblock MiT Press, 2016.

\bibitem[Hilton et~al.(2020)Hilton, Cammarata, Carter, Goh, and
  Olah]{hilton2020understanding}
J.~Hilton, N.~Cammarata, S.~Carter, G.~Goh, and C.~Olah.
\newblock Understanding rl vision.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00029}.
\newblock https://distill.pub/2020/understanding-rl-vision.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Leavitt and Morcos(2020)]{leavitt2020towards}
M.~L. Leavitt and A.~Morcos.
\newblock Towards falsifiable interpretability research.
\newblock \emph{arXiv preprint arXiv:2010.12016}, 2020.

\bibitem[Massidda et~al.(2022)Massidda, Geiger, Icard, and
  Bacciu]{massidda2022causal}
R.~Massidda, A.~Geiger, T.~Icard, and D.~Bacciu.
\newblock Causal abstraction with soft interventions.
\newblock \emph{arXiv preprint arXiv:2211.12270}, 2022.

\bibitem[McGrath et~al.(2022)McGrath, Kapishnikov, Toma{\v{s}}ev, Pearce,
  Wattenberg, Hassabis, Kim, Paquet, and Kramnik]{mcgrath2022acquisition}
T.~McGrath, A.~Kapishnikov, N.~Toma{\v{s}}ev, A.~Pearce, M.~Wattenberg,
  D.~Hassabis, B.~Kim, U.~Paquet, and V.~Kramnik.
\newblock Acquisition of chess knowledge in alphazero.
\newblock \emph{Proceedings of the National Academy of Sciences}, 119\penalty0
  (47):\penalty0 e2206625119, 2022.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
K.~Meng, D.~Bau, A.~Andonian, and Y.~Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17359--17372, 2022.

\bibitem[Morcos et~al.(2018)Morcos, Barrett, Rabinowitz, and
  Botvinick]{morcos2018importance}
A.~S. Morcos, D.~G. Barrett, N.~C. Rabinowitz, and M.~Botvinick.
\newblock On the importance of single directions for generalization.
\newblock \emph{arXiv preprint arXiv:1803.06959}, 2018.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Liberum, Smith, and
  Steinhardt]{nanda2023progress}
N.~Nanda, L.~Chan, T.~Liberum, J.~Smith, and J.~Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2301.05217}, 2023.

\bibitem[nostalgebraist(2020)]{nostalgebraist2020logit}
nostalgebraist.
\newblock interpreting gpt: the logit lens, 2020.
\newblock URL
  \url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}.

\bibitem[Olah et~al.(2018)Olah, Satyanarayan, Johnson, Carter, Schubert, Ye,
  and Mordvintsev]{olah2018building}
C.~Olah, A.~Satyanarayan, I.~Johnson, S.~Carter, L.~Schubert, K.~Ye, and
  A.~Mordvintsev.
\newblock The building blocks of interpretability.
\newblock \emph{Distill}, 3\penalty0 (3):\penalty0 e10, 2018.

\bibitem[Olah et~al.(2020)Olah, Cammarata, Schubert, Goh, Petrov, and
  Carter]{olah2020zoom}
C.~Olah, N.~Cammarata, L.~Schubert, G.~Goh, M.~Petrov, and S.~Carter.
\newblock Zoom in: An introduction to circuits.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00024.001}.
\newblock https://distill.pub/2020/circuits/zoom-in.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
C.~Olsson, N.~Elhage, N.~Nanda, N.~Joseph, N.~DasSarma, T.~Henighan, B.~Mann,
  A.~Askell, Y.~Bai, A.~Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Pearl(2009)]{pearl2009causality}
J.~Pearl.
\newblock \emph{Causality}.
\newblock Cambridge university press, 2009.

\bibitem[Phuong and Hutter(2022)]{phuong2022formal}
M.~Phuong and M.~Hutter.
\newblock Formal algorithms for transformers.
\newblock \emph{arXiv preprint arXiv:2207.09238}, 2022.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
A.~Power, Y.~Burda, H.~Edwards, I.~Babuschkin, and V.~Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Tinbergen(1963)]{tinbergen1963aims}
N.~Tinbergen.
\newblock On aims and methods of ethology.
\newblock \emph{Zeitschrift f{\"u}r tierpsychologie}, 20\penalty0 (4):\penalty0
  410--433, 1963.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Veit et~al.(2016)Veit, Wilber, and Belongie]{veit2016residual}
A.~Veit, M.~J. Wilber, and S.~Belongie.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Vig et~al.(2020{\natexlab{a}})Vig, Gehrmann, Belinkov, Qian, Nevo,
  Sakenis, Huang, Singer, and Shieber]{vig2020causal}
J.~Vig, S.~Gehrmann, Y.~Belinkov, S.~Qian, D.~Nevo, S.~Sakenis, J.~Huang,
  Y.~Singer, and S.~Shieber.
\newblock Causal mediation analysis for interpreting neural nlp: The case of
  gender bias.
\newblock \emph{arXiv preprint arXiv:2004.12265}, 2020{\natexlab{a}}.

\bibitem[Vig et~al.(2020{\natexlab{b}})Vig, Gehrmann, Belinkov, Qian, Nevo,
  Singer, and Shieber]{vig2020investigating}
J.~Vig, S.~Gehrmann, Y.~Belinkov, S.~Qian, D.~Nevo, Y.~Singer, and S.~Shieber.
\newblock Investigating gender bias in language models using causal mediation
  analysis.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 12388--12401, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and
  Steinhardt]{wang2022interpretability}
K.~Wang, A.~Variengien, A.~Conmy, B.~Shlegeris, and J.~Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object
  identification in gpt-2 small.
\newblock \emph{arXiv preprint arXiv:2211.00593}, 2022.

\bibitem[Zhang and Sennrich(2019)]{zhang2019root}
B.~Zhang and R.~Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
