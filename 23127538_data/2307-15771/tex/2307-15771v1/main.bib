@article{phuong2022formal,
  title={Formal algorithms for transformers},
  author={Phuong, Mary and Hutter, Marcus},
  journal={arXiv preprint arXiv:2207.09238},
  year={2022}
}

@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge university press}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy\_model/index.html}
}

@article{goh2021multimodal,
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  title = {Multimodal Neurons in Artificial Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030}
}

@article{hilton2020understanding,
  author = {Hilton, Jacob and Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris},
  title = {Understanding RL Vision},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/understanding-rl-vision},
  doi = {10.23915/distill.00029}
}

@article{bau2018gan,
  title={Gan dissection: Visualizing and understanding generative adversarial networks},
  author={Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B and Freeman, William T and Torralba, Antonio},
  journal={arXiv preprint arXiv:1811.10597},
  year={2018}
}

@article{carter2019activation,
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  title = {Activation Atlas},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/activation-atlas},
  doi = {10.23915/distill.00015}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{morcos2018importance,
  title={On the importance of single directions for generalization},
  author={Morcos, Ari S and Barrett, David GT and Rabinowitz, Neil C and Botvinick, Matthew},
  journal={arXiv preprint arXiv:1803.06959},
  year={2018}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@book{glymour2016causal,
  title={Causal inference in statistics: A primer},
  author={Glymour, Madelyn and Pearl, Judea and Jewell, Nicholas P},
  year={2016},
  publisher={John Wiley \& Sons}
}

@article{wang2022interpretability,
  title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine N and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={arXiv preprint arXiv:2304.14997},
  year={2023}
}

@book{halpern2016actual,
  title={Actual causality},
  author={Halpern, Joseph Y},
  year={2016},
  publisher={MiT Press}
}

@article{geva2022transformer,
  title={Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space},
  author={Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2203.14680},
  year={2022}
}

@article{tinbergen1963aims,
  title={On aims and methods of ethology},
  author={Tinbergen, Niko},
  journal={Zeitschrift f{\"u}r tierpsychologie},
  volume={20},
  number={4},
  pages={410--433},
  year={1963},
  publisher={Wiley Online Library}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{avin2005identifiability,
  title={Identifiability of path-specific effects},
  author={Avin, Chen and Shpitser, Ilya and Pearl, Judea},
  year={2005}
}

@inproceedings{beckers2020approximate,
  title={Approximate causal abstractions},
  author={Beckers, Sander and Eberhardt, Frederick and Halpern, Joseph Y},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={606--615},
  year={2020},
  organization={PMLR}
}

@article{geiger2021causal,
  title={Causal abstractions of neural networks},
  author={Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9574--9586},
  year={2021}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{belrose2023eliciting,
  title={Eliciting latent predictions from transformers with the tuned lens},
  author={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2303.08112},
  year={2023}
}

@misc{nostalgebraist2020logit,
    title={interpreting GPT: the logit lens},
    author={nostalgebraist},
    url={https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
    year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{vig2020causal,
  title={Causal mediation analysis for interpreting neural nlp: The case of gender bias},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Sakenis, Simas and Huang, Jason and Singer, Yaron and Shieber, Stuart},
  journal={arXiv preprint arXiv:2004.12265},
  year={2020}
}

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@article{finlayson2021causal,
  title={Causal analysis of syntactic agreement mechanisms in neural language models},
  author={Finlayson, Matthew and Mueller, Aaron and Gehrmann, Sebastian and Shieber, Stuart and Linzen, Tal and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2106.06087},
  year={2021}
}

@article{geiger2023finding,
  title={Finding alignments between interpretable causal variables and distributed neural representations},
  author={Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah D},
  journal={arXiv preprint arXiv:2303.02536},
  year={2023}
}

@article{geiger2023causal,
  title={Causal abstraction for faithful model interpretation},
  author={Geiger, Atticus and Potts, Chris and Icard, Thomas},
  journal={arXiv preprint arXiv:2301.04709},
  year={2023}
}

@article{massidda2022causal,
  title={Causal Abstraction with Soft Interventions},
  author={Massidda, Riccardo and Geiger, Atticus and Icard, Thomas and Bacciu, Davide},
  journal={arXiv preprint arXiv:2211.12270},
  year={2022}
}

@inproceedings{geiger2022inducing,
  title={Inducing causal structure for interpretable neural networks},
  author={Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah and Potts, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={7324--7338},
  year={2022},
  organization={PMLR}
}

@article{veit2016residual,
  title={Residual networks behave like ensembles of relatively shallow networks},
  author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{leavitt2020towards,
  title={Towards falsifiable interpretability research},
  author={Leavitt, Matthew L and Morcos, Ari},
  journal={arXiv preprint arXiv:2010.12016},
  year={2020}
}

@article{chan2022causal, 
	title={Causal scrubbing, a method for rigorously testing interpretability hypotheses},
	author={Chan, Lawrence and Garriga-Alonso, Adrià and Goldwosky-Dill, Nicholas and Greenblatt, Ryan and Nitishinskaya, Jenny and Radhakrishnan, Ansh and Shlegeris, Buck and Thomas, Nate},
	year={2022},
	journal={AI Alignment Forum},
	note={\url{https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing}}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@article{cammarata2020curve,
  title={Curve detectors},
  author={Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  journal={Distill},
  volume={5},
  number={6},
  pages={e00024--003},
  year={2020}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Liberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{chughtai2023toy,
  title={A toy model of universality: Reverse engineering how networks learn group operations},
  author={Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  journal={arXiv preprint arXiv:2302.03025},
  year={2023}
}

@article{geva2022lm,
  title={Lm-debugger: An interactive tool for inspection and intervention in transformer-based language models},
  author={Geva, Mor and Caciularu, Avi and Dar, Guy and Roit, Paul and Sadde, Shoval and Shlain, Micah and Tamir, Bar and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2204.12130},
  year={2022}
}

@article{dar2022analyzing,
  title={Analyzing transformers in embedding space},
  author={Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  journal={arXiv preprint arXiv:2209.02535},
  year={2022}
}

@article{gurnee2023finding,
  title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}

@article{mcgrath2022acquisition,
  title={Acquisition of chess knowledge in alphazero},
  author={McGrath, Thomas and Kapishnikov, Andrei and Toma{\v{s}}ev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={47},
  pages={e2206625119},
  year={2022},
  publisher={National Acad Sciences}
}

@inproceedings{forde2022concepts,
  title={Where, when \& which concepts does AlphaZero learn? Lessons from the game of Hex},
  author={Forde, Jessica Zosa and Lovering, Charles and Konidaris, George and Pavlick, Ellie and Littman, Michael L},
  booktitle={AAAI Workshop on Reinforcement Learning in Games},
  volume={2},
  year={2022}
}

@inproceedings{kim2018interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}