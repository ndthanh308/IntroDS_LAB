\documentclass[twoside,11pt, preprint]{article} % for arxiv
\usepackage{jmlr2e_preprint}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% \usepackage{jmlr2e}
\usepackage{lastpage}
\usepackage{subcaption, caption, adjustbox}
\usepackage{tabularx}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage[htt]{hyphenat}
\usepackage{wrapfig}
\usepackage[subpreambles=true]{standalone}

%%%%%%%%%%%%
% PACKAGE AND SETTINGS FOR CODE CHUNK AND IMAGE OVERLAY
\usepackage{listings,wrapfig}
\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tlrb,
  language=Python,
  aboveskip=2mm,
  belowskip=2mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\scriptsize\ttfamily},
  morekeywords={as},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%%%%%%%%%%%%

% Definitions of handy macros can go here

%  \usepackage{draftwatermark}
% \SetWatermarkText{DRAFT}
% \SetWatermarkScale{5}



% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{X}{2023}{1-X}{XX}{XX/23}{00-000}{}

% Short headings should be running head and authors last names

\ShortHeadings{ODTlearn: A Package for Learning Optimal Decision Trees}{Vossler et al}
\firstpageno{1}

\begin{document}


\title{ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription}

\author{\name Patrick Vossler\thanks{Corresponding Author} \email pvossler@usc.edu \\
       \name Sina Aghaei \email saghaei@usc.edu\\
       \name Nathan Justin \email njustin@usc.edu\\
       \name Nathanael Jo \email nathanael.jo@gmail.com\\
       \name Andrés Gómez \email gomezand@usc.edu\\
       \name Phebe Vayanos \email phebe.vayanos@usc.edu\\
       \addr University of Southern California, Center for AI in Society, Los Angeles, CA 90089
       }


\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file

\texttt{ODTLearn} is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in \citep{aghaei2019learning} and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at \url{https://d3m-research-group.github.io/odtlearn/}. Additionally, users can view the package source code and submit feature requests and bug reports by visiting \url{https://github.com/D3M-Research-Group/odtlearn}.
\end{abstract}

\begin{keywords}
Mixed-integer optimization, prescriptive trees, classification trees, distribution shifts, fair classification trees, robust classification trees, open source software.
\end{keywords}



\section{Introduction}

Automated data-driven predictive and prescriptive methods are increasingly being used in high-stakes domains to inform and support decision-making.
In such settings, these machine learning (ML) and artificial intelligence (AI) tools should be:
(a) \textit{accurate} (to minimize erroneous predictions/prescriptions that may negatively affect the populations on which they are deployed), (b) \textit{interpretable} (so that predictions and decisions are transparent, accountable, and easy to audit), (c) \textit{flexible} (i.e., possible to easily augment with domain specific constraints such as capacity and/or fairness constraints), and (d) \textit{robust} (to ensure high-quality solutions even under adversarial shifts between training and deployment data).

Despite their popularity, decision trees~\citep{breiman1984classification} are not necessarily well suited for data-driven decision-making in high-stakes domains. While the structure of decision trees makes them easy to interpret, they are typically constructed using heuristics and may yield suboptimal solutions. Furthermore, with heuristic-based decision trees, it is not immediately apparent how to incorporate relevant side information into the tree construction process or how to make a tree robust to adversarial shifts between training and deployment data. Optimal decision trees retain the interpretability of heuristic decision trees while still being flexible enough to model the types of problems decision-makers face and provide optimal solutions that decision-makers can trust.

Our \texttt{ODTlearn} Python package provides methods for fitting provably optimal decision trees using mixed-integer optimization (MIO) for various problem types and settings commonly encountered by practitioners in high-stakes settings. The modeling methods provided in \texttt{ODTlearn} build upon the modeling and solution paradigm proposed by \citet{aghaei2021strong}, which is significantly faster and provides better out-of-sample performance than previous MIO-based algorithms (see Section~\ref{sec:comparison} for additional discussion). This approach generalizes beyond standard classification problems to problems involving imbalanced datasets (e.g., by optimizing weighted accuracy or worst-case accuracy, by constraining recall or precision, or by balancing sensitivity and specificity). In addition to the core algorithm, we implement its generalization to learn optimal fair decision trees that optimize accuracy while satisfying arbitrary domain specific fairness constraints such as statistical parity, conditional statistical parity, or equalized odds as proposed in~\citep{jo2022learning}. These were show to outperform some of the most popular (heuristic-based) algorithms for learning fair trees. For users deploying decision trees in settings with potential distribution shifts between training and testing, we implement the method proposed in~\citep{justin2021optimal} for learning optimal robust classification trees. Finally, we implement the  method in~\citep{jo2022learning} for learning optimal prescriptive trees from observational data. This framework can be used to design treatment assignment policies in the form of decision trees, being highly interpretable while offering a tunable degree of personalization. Importantly, the learned trees can also be constrained to satisfy domain specific requirements such as budget constraints (e.g., limited amount of treatments) or fairness constraints (e.g., conditional statistical parity in allocation or in outcomes).
Our \texttt{ODTlearn} Python package provides methods for fitting provably optimal decision trees using mixed-integer optimization (MIO) for various problem types and settings commonly encountered by practitioners in high-stakes settings. The modeling methods provided in \texttt{ODTlearn} build upon the modeling and solution paradigm proposed by \citet{aghaei2021strong}, which is significantly faster and provides better out-of-sample performance than previous MIO-based algorithms (see Section~\ref{sec:comparison} for additional discussion). This approach generalizes beyond standard classification problems to problems involving imbalanced datasets (e.g., by optimizing weighted accuracy or worst-case accuracy, by constraining recall or precision, or by balancing sensitivity and specificity). In addition to the core algorithm, we implement its generalization to learn optimal fair decision trees that optimize accuracy while satisfying arbitrary domain specific fairness constraints such as statistical parity, conditional statistical parity, or equalized odds as proposed in~\citep{jo2022learning}. These were show to outperform some of the most popular (heuristic-based) algorithms for learning fair trees. For users deploying decision trees in settings with potential distribution shifts between training and testing, we implement the method proposed in~\citep{justin2021optimal} for learning optimal robust classification trees. Finally, we implement the  method in~\citep{jo2022learning} for learning optimal prescriptive trees from observational data. This framework can be used to design treatment assignment policies in the form of decision trees, being highly interpretable while offering a tunable degree of personalization. Importantly, the learned trees can also be constrained to satisfy domain specific requirements such as budget constraints (e.g., limited amount of treatments) or fairness constraints (e.g., conditional statistical parity in allocation).

The remainder of this paper is organized as follows. Section~\ref{sec:comparison} compares \texttt{ODTlearn} to related packages. Its architecture and a usage example are introduced in Section~\ref{sec:arch_usage}. Finally, the quality practices under which the package is developed are described in Section~\ref{sec:dev}.


\section{Comparison to Related Software}
\label{sec:comparison}
The most related software package is provided by Interpretable AI~\citep{InterpretableAI}. This is a proprietary software package written in Julia~\citep{Julia-2017} that provides methods for learning optimal classification trees and prescriptive trees using MIO based on papers by \citet{bertsimas2017optimal} and \citet{amram2022optimal}, respectively. It features a Python wrapper that allows users to call these methods from Python. Unlike \texttt{ODTlearn}, the algorithms in the Interpretable AI package do not provide functionality for incorporating side constraints such as fairness constraints nor to handle settings involving a distribution shift in the data. Additionally, the MIO formulation in the \texttt{ODTlearn} package has been shown to be 50 times faster than the MIO formulation used in Interpretable AI~\citep{aghaei2021strong}. Furthermore, for classification problems and prescriptive problems, the methods in \texttt{ODTlearn} have been shown in experiments to be 13\% and 13.6\% more accurate on out-of-sample observations, respectively~\citep{aghaei2021strong,jo2021learning}.

\texttt{PyDL8.5}~\citep{aglin2021pydl8} is an open source Python package that implements the DL8.5~\citep{aglin2020learning} algorithm for learning optimal decision trees through the use of itemset mining techniques. The package provides methods for learning optimal classification trees, however their method is unable to incorporate side constraints.

Finally, while packages such as \texttt{scikit-learn}~\citep{pedregosa2011scikit} provide high-quality implementations of heuristic-based methods for learning decision trees, these methods lack the modeling flexibility and optimality guarantees of MIO-based methods.



\section{Software Architecture and Usage Example}
\label{sec:arch_usage}

The software architecture of \texttt{ODTlearn} is motivated by the optimal decision tree literature in which researchers have recently proposed numerous new problem classes, reformulation strategies, and solution approaches. With this in mind, we have created a class structure for our package that follows the SOLID principles of object-oriented programming for developing software~\citep{martin2003agile}. These principles emphasize structuring classes that are easy to maintain and extend. Figure~\ref{fig:simple_diagram} shows the inheritance diagram in \texttt{ODTlearn}.
The classes in \texttt{ODTlearn} are derived from our abstract base class \texttt{OptimalDecisionTree}. This class provides a standardized interface for the two types of trees currently supported (classification and prescription) while keeping their implementation details separate from one another. \texttt{OptimalClassificationTree} and \texttt{OptimalPrescriptiveTree} extend \texttt{OptimalDecisionTree} by specifying problem-specific methods for traversing and visualizing the decision tree. The structure prevents unnecessary code duplication within each child class. Next, the children of \texttt{OptimalClassificationTree} and \texttt{OptimalPrescriptiveTree} implement methods for creating the decision variables, the constraints, and the objective function necessary for constructing the optimization problem of interest. The separate classes for each of the variations of the MIO formulations ensure that any changes to the structure of one problem formulation do not affect any of the other problem formulations. Finally, the classes in the third and fourth levels of Figure~\ref{fig:simple_diagram} implement user-facing methods such as \texttt{fit} and \texttt{predict}. Thus, our adherence to the SOLID principles ensures that researchers and practitioners building upon \texttt{ODTlearn} can easily augment it with more features (e.g., different objectives or additional constraints) or even build new types of trees altogether.


% Figure environment removed



Figure~\ref{fig:fair_example} provides a code snippet demonstrating how to use the \texttt{ODTlearn} API to fit a fair optimal classification tree. 
This is achieved through the API using the familiar fit-predict structure. Once an optimal decision tree has been learned, users can employ the build in \texttt{plot\_tree} function to visualize the tree. Finally, we note that while the fair optimal classification tree in the example uses the Gurobi solver~\citep{gurobi}, users can also use the open source COIN-OR branch and cut solver~\citep{cbc} because \texttt{ODTlearn} builds upon the \texttt{python-mip}~\citep{santos2020mixed} solver interface.


% Figure environment removed

\section{Development}\label{sec:dev}
Releases of the ODTlearn package are available via PyPI at \url{https://pypi.org/project/odtlearn/1.0.0/}. The package source code and documentation are hosted on GitHub (\url{https://github.com/D3M-Research-Group/odtlearn}). Collaboration in the form of discussions, feature requests, or bug reports is made possible through the GitHub issue and pull request workflow. We have implemented continuous integration through GitHub Actions to ensure backward compatibility and quickly identify any code regressions. Our documentation includes installation instructions, a user guide, an API reference, and downloadable example notebooks demonstrating each of the classification methods implemented in the package. The documentation is hosted via GitHub pages at \url{https://d3m-research-group.github.io/odtlearn}. The package is distributed under the GPL-3.0 license and makes use of several core libraries within Python’s scientific computing ecosystem: \texttt{scikit-learn}~\citep{pedregosa2011scikit}, \texttt{numpy}~\citep{harris2020array}, and \texttt{pandas}~\citep{mckinney2010data}.



\acks{P.\ Vayanos and N.\ Justin are funded in part by the National Science Foundation under CAREER award number
2046230. She is grateful for this support. P.\ Vayanos and P.\ Vossler acknowledge support from the USC Zumberge Special Solicitation – Epidemic \& Virus Related Research and Development award.
N.\ Jo acknowledges support from the Epstein Institute at the University of Southern California. A.\ G\'omez is funded in part by the National Science Foundation under grant 2152777.
N.\ Justin is funded in part by the National Science Foundation under the Graduate Research Fellowship Program.
% add N Justin's GRFP number
}

\newpage
\bibliography{odtlearn}

\end{document}