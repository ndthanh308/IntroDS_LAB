\documentclass[twoside,11pt, preprint]{article} % for arxiv
\usepackage{jmlr2e_preprint}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% \usepackage{jmlr2e}
\usepackage{lastpage}
\usepackage{subcaption, caption, adjustbox}
\usepackage{tabularx}
\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage[htt]{hyphenat}
\usepackage{wrapfig}


%%%%%%%%%%%%
% PACKAGE AND SETTINGS FOR CODE CHUNK AND IMAGE OVERLAY
\usepackage{listings, color, wrapfig}
\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tlrb,
  language=Python,
  aboveskip=2mm,
  belowskip=2mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\scriptsize\ttfamily},
  linewidth=\linewidth,
  xleftmargin=\dimexpr(\linewidth-\textwidth)/4\relax,
  morekeywords={as},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}
%%%%%%%%%%%%

% Definitions of handy macros can go here

%  \usepackage{draftwatermark}
% \SetWatermarkText{DRAFT}
% \SetWatermarkScale{5}


%% editing comments
\newcommand{\delpv}[1]{{\color{gray}{}}}% [{\sffamily\bfseries DELETED PV:} {\em #1}]}}
\newcommand{\notepv}[1]{{\color{blue} [{\sffamily\bfseries NOTE PV:} {\em #1}]}}
\newcommand{\newpv}[1]{{\color{blue} #1}}

\newcommand{\delag}[1]{{\color{red} [{\sffamily\bfseries DELETED AG:} #1]}}
\newcommand{\noteag}[1]{{\color{red} [{\sffamily\bfseries NOTE AG:} {\em #1}]}}
\newcommand{\newag}[1]{{\color{red} #1}}

\newcommand{\delsa}[1]{{\color{orange} [{\sffamily\bfseries DELETED SA:} #1]}}
\newcommand{\notesa}[1]{{\color{orange} [{\sffamily\bfseries NOTE SA:} {\em #1}]}}
\newcommand{\newsa}[1]{{\color{orange} #1}}


\newcommand{\delpatv}[1]{{\color{gray} [{\sffamily\bfseries DELETED PATV:} #1]}}
\newcommand{\notepatv}[1]{{\color{magenta} [{\sffamily\bfseries NOTE PATV:} {\em #1}]}}
\newcommand{\newpatv}[1]{{\color{blue} #1}}

\newcommand{\delnj}[1]{{\color{dkgreen} [{\sffamily\bfseries DELETED NJ:} #1]}}
\newcommand{\notenj}[1]{{\color{dkgreen} [{\sffamily\bfseries NOTE NJ:} {\em #1}]}}
\newcommand{\newnj}[1]{{\color{dkgreen} #1}}


% \newcommand{\delpv}[1]{}
% \newcommand{\notepv}[1]{}
% \newcommand{\newpv}[1]{{\color{black} #1}}

% \newcommand{\delag}[1]{}
% \newcommand{\noteag}[1]{}
% \newcommand{\newag}[1]{{\color{black} #1}}

% \newcommand{\delsa}[1]{}
% \newcommand{\notesa}[1]{}
% \newcommand{\newsa}[1]{{\color{black} #1}}


% \newcommand{\delpatv}[1]{}
% \newcommand{\notepatv}[1]{}
% \newcommand{\newspatv}[1]{{\color{black} #1}}

% \newcommand{\delnj}[1]{}
% \newcommand{\notenj}[1]{}
% \newcommand{\newnj}[1]{{\color{black} #1}}



% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{X}{2023}{1-X}{XX}{XX/23}{00-000}{}

% Short headings should be running head and authors last names

\ShortHeadings{ODTlearn: A Package for Learning Optimal Decision Trees}{Vossler et al}
\firstpageno{1}

% \usepackage[subpreambles=true]{standalone}
\usepackage{standalone}

\begin{document}


\title{ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription}

\author{\name Patrick Vossler\thanks{Corresponding Author} \email pvossler@usc.edu \\
       \name Sina Aghaei \email saghaei@usc.edu\\
       \name Nathan Justin \email njustin@usc.edu\\
       \name Nathanael Jo \email nathanael.jo@gmail.com\\
       \name Andrés Gómez \email gomezand@usc.edu\\
       \name Phebe Vayanos \email phebe.vayanos@usc.edu\\
       \addr University of Southern California, Center for AI in Society, Los Angeles, CA 90089
       }


\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file

\texttt{ODTLearn} is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in \citep{aghaei2021strong} and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at \url{https://d3m-research-group.github.io/odtlearn/}. Additionally, users can view the package source code and submit feature requests and bug reports by visiting \url{https://github.com/D3M-Research-Group/odtlearn}.
\end{abstract}

\begin{keywords}
Mixed-integer optimization, prescriptive trees, classification trees, distribution shifts, fair classification trees, robust classification trees, open source software.
\end{keywords}



\section{Introduction}~\label{sec:intro}

Automated data-driven predictive and prescriptive methods are increasingly being used in high-stakes domains to inform and support decision-making.
In such settings, these tools should be:
(a) \textit{accurate} (to minimize erroneous predictions/prescriptions that may negatively affect the populations on which they are deployed), (b) \textit{interpretable} (so that predictions and decisions are transparent, accountable, and easy to audit), (c) \textit{flexible} (i.e., possible to easily augment with domain specific constraints such as capacity and/or fairness constraints), and (d) \textit{robust} (to ensure high-quality solutions even under adversarial shifts between training and deployment data).

Despite their popularity, decision trees~\citep{breiman1984classification} are not necessarily well suited for data-driven decision-making in high-stakes domains. While the structure of decision trees makes them easy to interpret, they are typically constructed using heuristics and may yield suboptimal solutions. Furthermore, with heuristic-based decision trees, it is not immediately apparent how to incorporate relevant side information into the tree construction process or how to make a tree robust to adversarial shifts between training and deployment data. Optimal decision trees retain the interpretability of heuristic decision trees while providing the flexibility to model the types of problems decision-makers face and deliver optimal, trustworthy solutions.

Our \texttt{ODTlearn} Python package provides methods for fitting provably optimal decision trees using mixed-integer optimization (MIO) for various problem types and settings commonly encountered by practitioners in high-stakes settings. \texttt{ODTlearn} implements the FlowOCT modeling and solution paradigm proposed by \citet{aghaei2021strong}, which is significantly faster and provides better out-of-sample performance than previous MIO-based algorithms (see Section~\ref{sec:comparison} for additional discussion). This approach generalizes beyond standard classification problems to problems involving imbalanced datasets (e.g., by optimizing weighted accuracy or worst-case accuracy, by constraining recall or precision, or by balancing sensitivity and specificity). In addition to the core algorithm, we implement its generalization to learn optimal fair decision trees (FairOCT) that optimize accuracy while satisfying arbitrary domain specific fairness constraints such as statistical parity, conditional statistical parity, or equalized odds as proposed in~\citep{jo2022learning}. Experiments demonstrated that these methods outperform popular heuristic-based algorithms for learning fair trees. For users deploying decision trees in settings with potential distribution shifts between training and testing, we implement the MIP formulation proposed in~\citep{justin2021optimal} (RobustOCT) for learning optimal robust classification trees. Finally, we implement the MIP formulation proposed in~\cite{jo2022learning} (FlowOPT) for learning optimal prescriptive trees from observational data. This framework can be used to design treatment assignment policies in the form of decision trees, being highly interpretable while offering a tunable degree of personalization. Importantly, the learned trees can also be constrained to satisfy domain specific requirements such as budget constraints (e.g., limited amount of treatments) or fairness constraints (e.g., conditional statistical parity in allocation).

The remainder of this paper is organized as follows. Section~\ref{sec:comparison} compares \texttt{ODTlearn} to related packages. We discuss the structure of the package in Section~\ref{sec:arch}. We explain how our package interacts with MIP solvers and provide a usage example in Section~\ref{sec:impl_details}. Finally, the quality practices under which the package is developed are described in Section~\ref{sec:dev}.


\section{Comparison to Related Software}
\label{sec:comparison}

There are numerous high-quality implementations of heuristic-based methods for learning decision trees in R and Python, including \texttt{scikit-learn}~\citep{pedregosa2011scikit}, \texttt{Rpart}~\citep{therneau1997introduction}, and \texttt{Caret}~\citep{kuhn2008caret} among many others. As discussed in Section~\ref{sec:intro}, these methods are fast, but lack the modeling flexibility and optimality guarantees of MIO-based methods.

Several different R and Python packages provide implementations of methods for constructing optimal decision trees using approaches other than mixed-integer optimization.
The \texttt{evtree}~\citep{Grubinger2014-zl} Python package implements an evolutionary algorithm that searches the space of possible decision trees to find globally optimal decision trees. \texttt{PyDL8.5}~\citep{aglin2021pydl8} is an open-source Python package that implements the DL8.5~\citep{aglin2020learning} algorithm in C++ for learning optimal decision trees through the use of itemset mining techniques. 
The \texttt{gosdt} Python package\footnote{\url{https://pypi.org/project/gosdt/}} provides a C++ implementation of Generalized Optimal Sparse Decision Trees (GOSDT)~\citep{Lin2020-hv, mctavish2022fast}, which employs a branch-and-bound method with dynamic programming, similar to DL8.5. GOSDT can directly model continuous features and construct decision trees for various objectives, including accuracy, AUC, and partial area under ROC convex hull. While the MIO formulations used in \texttt{ODTlearn} provide the flexibility to incorporate side constraints, they are generally slower than the methods used in \texttt{evtree}, DL8.5, and GOSDT, especially for larger datasets or deeper trees. These methods employ specialized algorithms, such as evolutionary algorithms or branch-and-bound with dynamic programming, which can often find optimal trees more quickly than general-purpose MIO solvers. However, it is not immediately obvious how to incorporate side constraints into the tree construction process for these methods, which is a strength of the MIO approach.

The \texttt{policytree} package~\citep{Sverdrup2020-jk} provides an R implementation of Policy Trees (PT)~\citep{zhou2022offline}, which learn personalized treatment assignment policies from observational data using decision trees and doubly robust estimators to handle confounding. PT uses a recursive partitioning approach, allowing it to learn trees that handle continuous and discrete covariates without pre-discretization and in less time than the FlowOPT MIO formulation. However, PT's recursive partitioning approach may not converge to the global optimum and cannot easily incorporate complex constraints.


There are also open-source implementations of other MIO formulations for learning optimal classification trees.
The authors of the BinOCT MIO formulation~\citep{verwer2019learning} provide an open-source implementation\footnote{\url{https://github.com/SiccoVerwer/binoct}} of their method. Additionally, \cite{BoTangOCT} provides open-source implementations of the BinOCT, OCT~\citep{bertsimas2017optimal}, and FlowOCT~\citep{aghaei2021strong} MIO formulations. While both projects provide valuable implementations of these methods for technical users and researchers, these projects are less accessible to a general audience with limited knowledge of MIO. Neither project distributes its code as a Python package, and they are not indexed on PyPi. Furthermore, their implementations rely exclusively upon proprietary MIP solvers such as CPLEX or Gurobi, which require paid or academic licenses to use with large-scale problems.


In comparisons on simulated data, the FlowOCT and FlowOPT formulations solve problem instances faster and are more accurate out-of-sample compared to existing MIO formulations. In \cite{aghaei2021strong}, the authors implement FlowOCT, BinOCT, and OCT in Gurobi to directly compare the in-sample and out-of-sample performance of the methods. When comparing the speed at which each method solves a fixed number of problem instances, they find that FlowOCT is 29 times faster than the other two formulations at solving problems with balanced decision trees.  This significant speedup is attributed to the stronger linear optimization relaxation of the FlowOCT formulation, which leads to more effective pruning of the branch-and-
bound tree during the solving process.  Regarding out-of-sample accuracy, FlowOCT improves accuracy compared to BinOCT and OCT by up to 7\% and 21\%, respectively. In \cite{jo2021learning}, the authors compare FlowOPT to Gurobi implementations of the MIO formulations proposed in \cite{kallus2017recursive} (K-PT) and \cite{bertsimas2019optimal} (B-PT). FlowOPT outperforms K-PT and B-PT in terms of the out-of-sample probability of correct treatment assignment across various experiment designs.

Interpretable AI~\citep{InterpretableAI} offers a proprietary Julia~\citep{Julia-2017} package with a Python wrapper that provides methods for learning optimal classification, prescriptive, and regression trees. These implementations use the heuristic algorithm based on local search proposed by \cite{bertsimas2019machine} (LST). On the one hand, LST is much faster than MIO formulations like FlowOCT, requiring only seconds to find a local optimum. On the other hand, MIO formulations are more flexible and able to solve instances to provable optimality.

In summary, \texttt{ODTlearn} distinguishes itself from existing implementations of optimal decision tree methods in several key ways. First, it provides open-source, user-friendly implementations of state-of-the-art MIO formulations for learning optimal decision trees, including FlowOCT and FlowOPT, which have been shown to outperform other MIO formulations in terms of solution speed and out-of-sample performance. Second, \texttt{ODTlearn} is distributed as a Python package indexed on PyPi, making it easily accessible to a broad audience. Third, \texttt{ODTlearn} supports open-source and proprietary MIP solvers, allowing users without access to commercial solvers to still benefit from the package. Finally, the MIO formulations in \texttt{ODTlearn} provide the flexibility to incorporate custom constraints and objectives, enabling users to tailor the learned decision trees to specific requirements of the problem they wish to solve.

\section{Software Architecture}
\label{sec:arch}

The software architecture of \texttt{ODTlearn} is motivated by the optimal decision tree literature in which researchers have recently proposed numerous new problem classes, reformulation strategies, and solution approaches. With this in mind, we have created a class structure for our package, summarized in Figure~\ref{fig:simple_diagram}, that follows the SOLID principles of object-oriented programming for developing software~\citep{martin2003agile}. These principles emphasize structuring classes that are easy to maintain and extend. The Single Responsibility Principle (SRP) states that each class should have a single responsibility or purpose. The Open-Closed Principle (OCP) requires that users should be able to extend a class's behavior without modifying its existing code. According to the Liskov Substitution Principle (LSP), derived classes should be able to replace their base classes without affecting correctness. The Interface Segregation Principle (ISP) asserts that users should not be forced to depend on methods they do not use. Finally, the Dependency Inversion Principle (DIP) suggests that software design should rely upon abstract interfaces rather than concrete implementations. These principles establish practices for developing software with considerations for maintaining and extending it as the project grows.

Following the dependency inversion principle, all classes in \texttt{ODTlearn} are derived from our abstract base class \texttt{OptimalDecisionTree}. This class provides a standardized interface for the two types of trees currently supported (classification and prescription) while keeping their implementation details separate to follow the interface segregation principle.
The \texttt{OptimalDecisionTree} abstract class allows our class structure to follow the open-closed principle as we can create the derived classes \texttt{OptimalClassificationTree} and \texttt{OptimalPrescriptiveTree} with problem-specific methods for traversing and visualizing the decision tree.
Next, the children of \texttt{OptimalClassificationTree} and \texttt{OptimalPrescriptiveTree} implement methods for creating the decision variables, the constraints, and the objective function necessary for constructing the optimization problem of interest.
The separate classes for each of the variations of the MIO formulations ensure that each of the derived classes has a single job, obeying the single responsibility principle. Finally, the classes in the third and fourth levels of Figure~\ref{fig:simple_diagram} implement user-facing methods such as \texttt{fit} and \texttt{predict} required by their parent classes. This ensures that our low-level classes respect the Liskov substitution principle by adhering to the basic requirements of their parent class. Thus, our adherence to the SOLID principles ensures that researchers and practitioners building upon \texttt{ODTlearn} can easily augment it with more features (e.g., different objectives or additional constraints) or even build new types of trees.

% Figure environment removed

\section{Implementation Details}
\label{sec:impl_details}

\texttt{ODTlearn} is built upon the Python MIP package~\citep{santos2020mixed}, which provides a unified interface for implementing MIO formulations with callbacks across both open-source and commercial solvers. By leveraging Python MIP's consistent interface, our package is able to support both the open-source CBC solver and the commercial Gurobi solver without the need for solver-specific code. This design choice allows us to focus on implementing the core functionality of \texttt{ODTlearn} while still supporting a wide range of users with different solver preferences and access to commercial licenses. 
The use of Python MIP is particularly important for our \texttt{BendersOCT} formulation, which relies on callbacks to implement the Benders' decomposition algorithm. Callbacks are essential for the efficient solution of large-scale problems using \texttt{BendersOCT}, and Python MIP's interface allows us to implement these callbacks in a solver-independent manner. Furthermore Python MIP interacts directly with the solver's native dynamic loadable library (DLL) interface, ensuring that any computationally intensive steps are performed by the solver and not in Python.

For fitting optimal classification and prescriptive trees, we provide users with an interface that follows a common fit-predict structure.
Figure~\ref{fig:fair_example} provides a code snippet demonstrating how to use the \texttt{ODTlearn} API to fit a fair optimal classification tree. Once an optimal decision tree has been learned, users can employ the build in \texttt{plot\_tree} function to visualize the tree.



% Figure environment removed

\section{Development}\label{sec:dev}
Releases of the ODTlearn package are available via PyPI at \url{https://pypi.org/project/odtlearn}. The package source code and documentation are hosted on GitHub (\url{https://github.com/D3M-Research-Group/odtlearn}). Collaboration in the form of discussions, feature requests, or bug reports is made possible through the GitHub issue and pull request workflow. We have implemented continuous integration through GitHub Actions to ensure backward compatibility and quickly identify any code regressions. Our documentation includes installation instructions, a user guide, an API reference, and downloadable example notebooks demonstrating each of the classification methods implemented in the package. The documentation is hosted via GitHub pages at \url{https://d3m-research-group.github.io/odtlearn}. The package is distributed under the GPL-3.0 license and makes use of several core libraries within Python’s scientific computing ecosystem: \texttt{scikit-learn}~\citep{pedregosa2011scikit}, \texttt{numpy}~\citep{harris2020array}, and \texttt{pandas}~\citep{mckinney2010data}.



\acks{P.\ Vayanos and N.\ Justin are funded in part by the National Science Foundation under CAREER award number
2046230. P.\ Vayanos and P.\ Vossler acknowledge support from the USC Zumberge Special Solicitation – Epidemic \& Virus Related Research and Development award.
N.\ Jo acknowledges support from the Epstein Institute at the University of Southern California. A.\ G\'omez is funded in part by the National Science Foundation under grant 2152777.
N.\ Justin is funded in part by the National Science Foundation under the Graduate Research Fellowship Program.
P. Vayanos and A. G\'omez are funded in part by the National Science Foundation under NRT grant 2346058. They are grateful for the support.
}

\newpage
\bibliography{odtlearn}

\end{document}