\section{Background}\label{sec:back}
We first show the notations used in this paper as  \S~\ref{sec:notations}. Then, we present the key building blocks of Transformer models in \S~\ref{sec:transformer}. Finally, we give a brief introduction to 2-out-of-3 replicated secret sharing in \S~\ref{sec:3partyss}.

\subsection{Notations}\label{sec:notations}
The main used notations are as follows: $P_i$ represents the $i$-th computing party in 3PC, $i \in \{0,1,2\}$. The uppercase bold letter $\mathbf{X}$ is used for matrices, and the lowercase bold letter $\x$ denotes vectors. $\x[i]$ denotes the $i$-th element of vector $\x$, while lowercase letter $x$ is used for scalar values. $\ZL$ denotes the discrete ring modulo $2^\ell$, $\mathbb{R}$ denotes real numbers.
$\share{\cdot}$ is used for 2-out-of-3 replicated secret sharing~\citep{araki2016high,aby3}.

\subsection{Transformer Model}\label{sec:transformer}
A Transformer model~\citep{transformer} mainly consists of an \textbf{Embedding} layer and multiple \textbf{Transformer} layers.  
Given a token (\eg, a word) id, the Embedding layer maps it to a hidden vector representation.
And one Transformer layer includes \textbf{Attention}, \textbf{Feed-Forward Network}, and \textbf{LayerNorm} sub-layers:

\textbf{Attention.} Given inputs $(\Q, \K, \V)$, the $\attention$ function is computed as 
$\attention(\Q,\K,\V)=
\softmax(\Q \cdot \K^\mathsf{T} + \M_{\{0,-\infty\}}) \cdot \V$,
where $\M_{\{0,-\infty\}}$, which is composed of $\{0, -\infty\}$, is used to perform \textit{masking} $\attention$ in Decoder, and it can be viewed as a bias matrix. Besides, \citep{transformer} proposed Multi-Head $\attention$ to jointly attend to information from different representation subspaces at different positions.

\textbf{Feed-Forward Network ($\ffn$).} 
$\ffn$ is applied to each position separately and identically. This consists of two linear transformations with a activation in between, and the most common used activation function is $\gelu$. Given input $\x$ and parameters $\{\mathbf{W}_1, \mathbf{b}_1, \mathbf{W}_2, \mathbf{b}_2\}$, $\ffn$ can be formalized as $\ffn(\x) = \mathbf{W}_2\gelu(\mathbf{W}_1\x+\mathbf{b}_1)+\mathbf{b}_2$.
Note that the parameters of linear transformations are different from layer to layer.


\textbf{LayerNorm.} Given vector $\x\in \mathbb{R}^n$, $\layernorm$ is defined as: $\layernorm(\x)[i] =  \gamma \cdot \frac{\x[i]-\mu}{\sqrt{\sigma}} + \beta$, where $(\gamma, \beta)$ are trained parameters, $\mu = \frac{\sum_{i=1}^n \x[i]}{n}$, and $\sigma = \sum_{i=1}^n (\x[i] - \mu)^2$.

Indeed, the Transformer model has evolved into various variants, each designed for specific tasks and domains. Two popular variants are Bert (Bidirectional Encoder Representations from Transformers)~\citep{bert} and GPT (Generative Pre-Trained models)~\citep{gpt}.
In these variants, the last layer of the Transformer model is often followed by a \textbf{Prediction} layer, also known as the task-specific layer. This additional layer is appended to the Transformer to generate the desired output for the specific task at hand.



\subsection{2-out-of-3 Replicated Secret Sharing}\label{sec:3partyss}
Secret value $x\in \ZL$ is shared by three random values $x_0, x_1, x_2 \in \ZL$ with $x=x_0+x_1+x_2 \pmod{2^\ell}$. In 2-out-of-3 replicated secret sharing (denoted as $\share{\cdot}$-sharing), party $P_i$ gets $\share{x}_i = (x_i, x_{i+1})$. Without special declaration, we compute in $\ZL$ and omit $\pmod{2^\ell}$ for brevity.
In the case of $\ell>1$ (\eg, $\ell=64$) which support arithmetic operations (\eg, $+$, $-$, and $\cdot$), we refer to this type as \textit{Arithmetic Sharing} and use notation $\share{\cdot}$. \textit{Boolean Sharing} ($\shareb{\cdot}$) refers to $\ell=1$ where $(+,-)$ and $\cdot$ are respectively replaced by bit-wise $\oplus$ and $\land$.

\textbf{Addition.}
Let $(c_1$, $c_2$, $c_3)$ be public constants, and $(\share{x}, \share{y})$ be two secret-shared values. Then, $\share{c_1 x+c_2 y+c_3}$ can be computed as $(c_1 x_0+c_2 y_0+c_3, c_1 x_1+c_2 y_1, c_1 x_2+c_2 y_2)$ where $P_i$ can compute its share locally. When $(c_1 =1,c_2 =1,c_3 =0)$, we get $\share{x+y}$.  

\textbf{Multiplication.}
In secure multiplication protocol $\Pi_{\mathsf{Mul}}$, given two shared values $\share{x}$ and $\share{y}$, parties follows steps: \romannumeral1) First, $P_i$ computes $z_i = x_iy_i + x_{i+1}y_i + x_iy_{i+1}$ locally, \romannumeral2) Parties then perform \textit{re-sharing} by letting $P_i$ sends $z_i'=\alpha_i+z_i$ to $P_{i-1}$, where $\alpha_0+\alpha_1+\alpha_2=0$ ($P_i$ can generate $\alpha_i$ in the setup phase as \cite{aby3}). \romannumeral3) Finally, $\{(z_0', z_1'), (z_1', z_2'), (z_2',z_0')\}$ form $\share{x\cdot y}$. 

\textbf{Underlying Protocols.}
In addition to addition and multiplication, \puma\ relies on several other underlying protocols: boolean-arithmetic multiplication ($\Pi_{\mathsf{Mul_{BA}}}$), square $\Pi_{\mathsf{Square}}$, equality test ($\Pi_{\mathsf{Eq}}$), less than ($\Pi_{\mathsf{LT}}$), reciprocal ($\Pi_{\mathsf{Recip}}$), maximum ($\Pi_{\mathsf{Max}}$), and reciprocal of square root ($\Pi_{\mathsf{rSqrt}}$), from the state-of-the-art works. We employ them in a black-box manner, thus we only enumerate the inputs and outputs of these protocols as follows: 
\begin{itemize}
\begin{multicols}{2}
    \item $\share{z}=\Pi_{\mathsf{Mul_{BA}}}(\shareb{b}, \share{x})$, s.t. $z=b\cdot x$
    \item $\share{z}=\Pi_{\mathsf{Square}}(\share{x})$, s.t. $z=x^2$
    \item $\shareb{z}=\Pi_{\mathsf{Eq}}(\share{x},\share{y})$, s.t. $z=1\{x=y\}$
    \item $\shareb{z}=\Pi_{\mathsf{LT}}(\share{x}, \share{y})$, s.t. $z=1\{x<y\}$
    \item $\share{z}=\Pi_{\mathsf{Recip}}(\share{x})$, s.t. $z=1/x$
    \item $\share{z}=\Pi_{\mathsf{rSqrt}}(\share{x})$, s.t. $z=1/\sqrt{x}$
    \item $\share{z}=\Pi_{\mathsf{Max}}(\share{\x})$, s.t. $z=\mathsf{maximum}(\x)$
\end{multicols}
\end{itemize}
$1\{e\}$ returns $1$ that when condition $e$ is \textsf{true}, and $0$ otherwise. 
For detailed protocol constructions, please refer to~\citep{aby3,rSqrt,keller2020mp}.

\textbf{Fixed-Point Representation \& Truncation.}
Real tasks (\eg, Transformer models) usually use floating-point values, we need to encode the floating-point value as integers in finite rings/fields to support secret sharing-based MPC protocols~\citep{aby3}. To avoid overflow in several sequential secure multiplications, \citep{aby3} proposed protocol $\Pi_{\mathsf{Trunc}}^{f}$ to truncate the least $f$ bits securely. For ease of use, we include $\Pi_{\mathsf{Trunc}}^{f}$ in $\Pi_{\mathsf{Mul}}$ and $\Pi_{\mathsf{Square}}$ by default and and do not explicitly mention it in our protocol designs.


The above operations can be easily extended to vectors and matrices, and we use the same notation for vector and matrix operations for simplicity. For more details, please refer to~\citep{aby3,wagh2020falcon}.

\textbf{Threat Model.}
Following previous works~\citep{aby3,li2023mpcformer},
\puma\ resists a semi-honest (a.k.a., honest-but-curious) adversary in honest-majority~\citep{lindell2009proof}, where the adversary passively corrupts no more than one computing party. Such an adversary follows the protocol specification exactly, but may try to learn more information than permitted. Please note that \puma\ cannot protect against the extraction of information from the inference results, and the examination of mitigating solutions (\eg, differential privacy~\citep{abadi2016deep}) falls outside the scope of this study.


