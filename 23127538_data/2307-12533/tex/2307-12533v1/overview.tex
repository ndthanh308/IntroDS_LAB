\section{Secure Design of \puma}\label{sec:design}
In this section, we first present an overview of \puma~ in \S~\ref{sec:overview}. Then, we present the protocol for secure embedding in \S~\ref{sec:embed}. Next, we give our accurate approximation protocols for function $\gelu$ and $\softmax$ in \S~\ref{sec:gelu} and \S~\ref{sec:secureatten}, respectively. Finally, we show the secure design of $\layernorm$ in \S~\ref{sec:seclayernorm} to reach framework \puma. 

\subsection{Overview of \puma}\label{sec:overview}
In \puma, we aim to enable secure computation of Transformer-based models. To achieve this, the system defines three entities: model owner, client, and computing parties. The model owner provides the trained Transformer models, client is responsible for providing data to the system and receiving the inference results, while the computing parties (i.e., $P_0$, $P_1$, and $P_2$) execute the secure computation protocols. Note that the model owner and client can also be the computing parties, we describe them separately for ease of illustration.

During the secure inference process, a key invariant is maintained: the computing parties always start with 2-out-of-3 replicated secret shares of the clients' input and model owner's weights of the layer, and end with 2-out-of-3 replicated secret shares of layer's output. As the shares do not leak any information to each party, this ensures that the protocol modules can be sequentially combined for arbitrary depths to obtain a secure computation scheme for any Transformer-based model.
The main focus of \puma\ is to reduce the runtime and communication costs between the computing parties while maintaining the desired level of security. By leveraging replicated secret sharing and our 3PC protocols, \puma\ enables fast secure inference of Transformer-based models in 3-party setting.

\iffalse
\textbf{Threat Model.}
Following previous works~\citep{aby3,li2023mpcformer},
\puma\ resists a semi-honest (a.k.a., honest-but-curious) adversary in honest-majority~\citep{lindell2009proof}, where the adversary passively corrupts no more than one computing party. Such an adversary follows the protocol specification exactly, but may try to learn more information than permitted. Please note that \puma\ cannot protect against the extraction of information from the inference results, and the examination of mitigating solutions (\eg, differential privacy~\citep{abadi2016deep}) falls outside the scope of this study.
\fi 