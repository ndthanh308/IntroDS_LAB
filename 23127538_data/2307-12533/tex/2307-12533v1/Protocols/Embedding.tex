\subsection{Protocol for Secure Embedding}\label{sec:embed}

The current secure embedding procedure~\citep{li2023mpcformer} requires the client to create a one-hot vector using the token $\tokenid$, which deviates from the plaintext workflow and undermines the Transformer structure. Therefore, this method is not easy to deploy in real Transformer models services applications.

To address this issue, we propose a secure embedding design as follows. Assuming that the token $\tokenid\in [n]$ and all embedding vectors are denoted by $\E= (\e_1^T, \e_2^T, \dots, \e_n^T)$, the embedding can be formulated as $\e_{\tokenid} = \mathbf{E}[\tokenid]$. Given $(\tokenid, \E)$ are in secret-shared fashion, our secure embedding protocol $\Pi_{\mathsf{Embed}}$ works as follows:
\begin{itemize}
    \item The computing parties securely compute the one-hot vector $\shareb{\mathbf{o}}$ after receiving $\share{\tokenid}$ from the client. Specifically, $\shareb{\mathbf{o}[i]}=\Pi_{\mathsf{Eq}}(i,\share{\tokenid})$ for $i\in [n]$.
    \item The parties can compute the embedded vector via $\share{\e_{\tokenid}} = \Pi_{\mathsf{Mul_{BA}}}(\share{\E}, \shareb{\mathbf{o}})$, where $\Pi_{\mathsf{Mul_{BA}}}$ does not require secure truncation.
\end{itemize}
In this way, our $\Pi_{\mathsf{Embed}}$ does not require explicit modification of the workflow of Transformer models.


