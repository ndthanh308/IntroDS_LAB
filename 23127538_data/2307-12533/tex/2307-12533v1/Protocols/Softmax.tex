\subsection{Protocol for Secure Softmax}\label{sec:secureatten}

In the function $\attention(\Q,\K,\V)=
\softmax(\Q \cdot \K^\mathsf{T} + \M) \cdot \V$, where $\M$ can be viewed as a bias matrix, the key challenge is computing function $\softmax$. For the sake of numerical stability, the $\softmax$ function is computed as
\begin{equation}\label{eq:softmax}
    \softmax(\x)[i]=\frac{\exp(\x[i] - \bar{x} - \epsilon)}{\sum_i \exp(\x[i] - \bar{x} - \epsilon)},
\end{equation}
where $\bar{x}$ is the maximum element of the input vector $\x$. 
For the normal plaintext softmax, $\epsilon=0$. For a two-dimension matrix, we apply equation~(\ref{eq:softmax}) to each of its row vector.

Formally, our detailed secure protocol  $\Pi_{\softmax}$ is illustrated in algorithm~\ref{protocol:softmax}, where we propose two optimizations:
\begin{itemize}
\item 
For the first optimization, we set $\epsilon$ in equation~\ref{eq:softmax} to a tiny and positive
value, e.g., $\epsilon =
10^{-6}$, so that the inputs to exponentiation
in equation~\ref{eq:softmax} are all negative. We exploit the negative operands
for acceleration. Particularly, we compute the exponentiation using the Taylor series~\citep{tan2021cryptgpu} with a simple clipping
\begin{equation}\label{eq:negexp}
\mathsf{negExp}(x) = \begin{cases}
    0, &x < T_{\exp} \\
    (1+\frac{x}{2^t})^{2^t}, &x\in [T_{\exp},0].
\end{cases}
\end{equation}
Indeed, we apply the less-than for the branch $x < T_{\exp}$
The division by $2^t$ can be achieved using
$\Pi_{\mathsf{Trunc}}^t$ since the input is already negative. Also, we can
compute the power-of-$2^t$ using $t$-step sequences of square function $\Pi_{\mathsf{square}}$ and $\Pi_{\mathsf{Trunc}}^f$. Suppose our MPC program uses
$18$-bit fixed-point precision. Then we set $T_{\exp}=-14$ given $\exp(-14) < 2^{-18}$, and empirically set $t = 5$.


\item 
Our second optimization is to reduce the number of divisions, which ultimately saves computation and communication costs.
To achieve this, for a vector $\x$ of size $n$, we have replaced the operation $\mathsf{Div}(\x, \mathsf{Broadcast}(y))$ with $\x \cdot  \mathsf{Broadcast}(\frac{1}{y})$, where $y=\sum_{i=1}^n\x[i]$. By making this replacement, we effectively reduce $n$ divisions to just one reciprocal operation and $n$ multiplications.
This optimization is particularly beneficial in the case of the $\softmax$ operation. The $\frac{1}{y}$ in the $\softmax$ operation is still large enough to maintain sufficient accuracy under fixed-point values. As a result, this optimization can significantly reduce the computational and communication costs while still providing accurate results.
\end{itemize}

\input{Protocols/softmaxprotocol}
