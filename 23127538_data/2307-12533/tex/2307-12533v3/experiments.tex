\section{Experimental Evaluations}\label{sec:experiment}

\textbf{Implementation.}
We implement \puma\ on top of SecretFlow-SPU~\citep{spu} in \textrm{C++} and Python. %SecretFlow compiles a high-level Flax code to secure computation protocols, which are then executed by our designed cryptographic backends, and 
We encode the data in a fixed-point form under ring $\mathbb{Z}_{2^{64}}$ with $18$-bit fractional part. 
Our experiments are run on 3 Alibaba Cloud ecs.g7.8xlarge servers with 32 vCPU and 128GB RAM each. The CPU model is Intel Xeon(Ice Lake) Platinum 8369B CPU @ 2.70GHz. We evaluate \puma\ on Ubuntu 20.04.6 LTS with Linux kernel 5.4.0-144-generic. Our bandwidth is about 5Gbps and round trip time is about 1ms. %\cheng{Describe fixed point parameters: scale, share bits.}

\textbf{Models \& Datasets.}
We evaluate \puma\ on seven NLP models: Bert-Base, Roberta-Base, and Bert-Large~\citep{bert}; GPT2-Base, GPT2-Medium, and GPT2-Large~\citep{gpt}; and LLaMA-7B~\citep{touvron2023llama}. We measure the Bert performance for three NLP tasks over the datasets of Corpus of Linguistic Acceptability (CoLA), Recognizing Textual Entailment (RTE), Stanford Question Answering Dataset (QNLI) from GLUE benchmarks~\citep{wang2018glue}, and GPT2 performance on Wikitext-103 V1~\citep{merity2016pointer}.

\textbf{Baseline.}
We compare \puma\ to the most similar prior work \mpcformer~\citep{li2023mpcformer}. But for fair comparison, we have the following considerations:
\romannumeral1) As \mpcformer\ neither supports loading pretrained transformer models nor implements LayerNorm faithfully\footnote{ As \mpcformer~does not support loading pre-trained Transformer models, we did an experiment in plaintext Bert-Base that replaced LayerNorm with BatchNorm  as \mpcformer~did. This resulted in a significant drop in the MCC score for CoLA task from $0.616$ to $-0.020$. On the contrary, \puma~achieves an MCC score of $0.613$. }, we cannot achieve meaningful secure inference results using their framework.
Therefore, we compare our performance to that of plaintext (floating-point) to show our precision guarantee.
\romannumeral2) \mpcformer\ with \textit{Quad} approximations requires retraining the  modified models. As \puma\ does not require retraining, we compare our cost to that of \mpcformer\ without \textit{Quad} approximations. Also, we re-run \mpcformer~in our environment.



\subsection{Precision}\label{sec:accuracy}
\iffalse
% Figure environment removed
\fi 


\input{Table/accppl}

We compare our secure model 
inference performance to that of plaintext (floating-point) in Table~\ref{table:bertacc} and~\ref{tab:gpot2ppl} to show our precision guarantee.

In Table~\ref{table:bertacc}, we show the Matthews correlation/accuracy of plaintext and \puma\ on the Bert-Base, Roberta-base, and Bert-Large. We observe that the accuracy achieved by \puma~ matches the accuracy of the plaintext Flax code. Specifically, the accuracy difference does
not exceed $0.011$ over all datasets. 
Moreover, in Table~\ref{tab:gpot2ppl}, we also compare our perplexity on dataset Wikitext-103 V1 with the plaintext baseline on GPT2 models. The results are similar and the perplexity differences do not exceed $0.02$ over all models.

The above accuracy and perplexity advantages experimentally validate that our protocols are numerically precise. 


\input{Table/CostsofOneInput}

\subsection{Inference Costs}\label{sec:efficiency}


We compare \puma's inference cost to that of \mpcformer. 
%We evaluate three Bert models (Bert-Base, Roberta-Base, and Bert-Large) and three GPT2 models (GPT2-Base, GPT2-Medium, and GPT2-Large).
The costs are for processing one input sentence: \romannumeral1) For Bert models the input sentence is of length $128$. \romannumeral2) For GPT2 models the input  length is 32 and generate $1$ new word. 

On the 3 Bert models in Table~\ref{tab:costbert}, \puma\ is  $1.375\sim 1.916\times$ faster than  \mpcformer, and is $1.079\sim 1.195\times$ more communication-efficient. For the GPT2 models in Table~\ref{tab:costgpt2}, \puma\ is $2.250\sim 2.414\times$ faster than \mpcformer, and is $1.325\sim 1.884\times$ more communication-efficient. 
    
We observe that \puma's improvements increase as the model size grows, particularly for the GPT2 models. This trend is because our specialized optimizations are more effective when processing large-scale evaluations.



\subsection{Scalability}\label{sec:scala}

In this subsection, we measure the costs of evaluating \puma\ on Bert-Base and GPT2-Base models for batched inputs, varying-length inputs, and varying-length outputs (only for GPT2-Base). We also compare our costs to those of \mpcformer~to demonstrate our improvements.

\iffalse
\input{Table/CostBatch}
\textbf{Batch Inputs Evaluation.}
Table~\ref{tab:costbertbatch} presents our costs on batched inputs. For Bert-Base, \puma\ is $1.570\sim 1.602\times$ faster. For GPT2-Base, our improvements in runtime are in the range of $1.051\sim 1.730\times$, respectively. 
And our communication costs are comparable to \mpcformer.
Unlike the observations in Section~\ref{sec:efficiency}, our efficiency gains decrease with increasing batch sizes, and \puma~requires more communication when processing large batch of inputs. This phenomenon is attributed to the interesting fact: To directly support pre-trained plaintext models, \puma\ strictly follows the plaintext model format that only accept token ids as input, so \puma\ has to compute the one-hot vectors from token ids in an MPC way. On the other hand, \mpcformer\ uses modified models that accept one-hot vectors as input, so the one-hot function could be computed at the client side in plaintext. Nevertheless, \puma\ remains faster than \mpcformer.
\fi 

\input{Table/CostInputLength}

\textbf{Input Length Evaluation.}
Table~\ref{tab:costbertinput} shows our costs on varying-length inputs, we evaluate Bert-Base on inputs of length $\{64, 128, 256\}$, and GPT2-Base on inputs of length $\{16, 32, 64\}$.
For Bert-Base, \puma\ is $1.631\sim 1.837\times$ faster, and for GPT2-Base, \puma\ is $1.744\sim 2.686\times$ faster. 

%\input{Table/CostGPT2token}



\textbf{Output Length Evaluation.}
Fig~\ref{fig:gptwoutcosts} presents our costs on varying-length outputs for GPT2-Base. Our improvements against \mpcformer\  range from $1.279\sim 2.700\times$.

We observe in Table~\ref{tab:costbertinput} and Fig~\ref{fig:gptwoutcosts} that for GPT-2, our efficiency gains decrease  with more input/output tokens. This is because \puma\ introduces extra one-hot embedding costs (as described in~\ref{sec:embed}). We should emphasize
again that  \puma\ is compatible with plaintext models, and could achieve a similar accuracy as plaintext models while \mpcformer\ could not.

 % Figure environment removed



\subsection{Evaluating LLaMA-7B in Five Minutes.}\label{sec:llama}
\input{Table/llama7b}

Our protocols are already complete for evaluating any Transformer-based models including LLaMA-7B. Unfortunately, existing serialization libraries such as Protobuf~\citep{protobuf} and FlatBuffers~\citep{van2014flatbuffers} only support data trunks with size up to 2GB, which is not sufficient for large MPC tasks. To address this problem, we propose an optimization to SecretFlow-SPU. Concretely, the system could automatically divide and serialize overly large secret-shared structures into smaller chunks when communicating or performing I/O operations.

We evaluated the large language model LLaMA-7B using \puma\ under 3 Alibaba Cloud
ecs.r7.32xlarge servers, each has 128 threads and 1TB RAM, with 20GB bandwidth, 0.1ms round-trip-time. 
As shown in Table~\ref{tab:llama7b}, \puma\ can support secure inference of LLaMA-7B with reasonable costs. For example, given an input sentence of 8 tokens, \puma\ can output one token in around $200$ seconds with communication costs of $1.794$ GB. To our knowledge, this is the first time that LLaMA-7B has been evaluated using MPC. Moreover, \puma\ can generate the same tokens exactly as plaintext LLaMA-7B, see Appendix~for an example.


%Llama-7B, LAN=(20GB, 0.06ms), 128 threads, input length=8, output=1 token, costs: 346.126s, 2002213760 bytes