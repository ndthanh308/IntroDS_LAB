\section{Introduction}\label{sec:intro}
Pre-trained Transformer models~\citep{transformer} have attracted much attentions for their high performance in practical tasks~\citep{gpt,zhuge2021kaleido} and been widely in Deep Learning as a Service (DLaaS) paradigm~\citep{soifer2019deep}. However, these services can raise privacy concerns, such as in the case of ChatGPT~\citep{chatgpt}, which requires either users to reveal their private prompts to the service provider or the service provider to release their proprietary trained weights to users.

One solution to address the privacy concerns of Transformer models service is Secure Multi-Party Computation (MPC)~\citep{shamir1979share,yao1986generate,gmw}, which can keep data and model weights private during inference. %However, the vanilla Transformer inference in MPC is too time- and communication-expensive to be practical for real-world applications.
\citep{hao2022iron,li2023mpcformer,privformer,liang2023merge,liu2023llms} have proposed various ways to support  secure Transformer models inference, but these approaches still have one or several of the following drawbacks: 
%\romannumeral1) \textit{

 \textbf{High inference cost.} Non-linear functions like $\gelu$ and $\softmax$ are challenge to design in MPC. \citep{hao2022iron} computes these non-linear functions in a faithful way. \eg, they design $\gelu$ using $\tanh$ based on general MPC exponentiation method proposed by~\citep{rathee2021sirnn}. But these general methods are quite expensive in terms of computation and communication, and only tested under small bitwidth (e.g. below 32). 

\textbf{Retraining required.} 
To reduce the cost of non-linear functions, several works \citep{li2023mpcformer,privformer,liu2023llms} suggested to approximate $\gelu$ and $\softmax$ using simpler functions like ReLU and quadratics. These functions are  up to an order of magnitude cheaper in MPC, but would introduce utility loss to the Transformer model. As a result, they require an extra step of model retraining (fine-tuning).  However, retraining is unfriendly for data-limited participants, and  might not achieve satisfactory performance~\citep{kumar2022fine}. 
%Furthermore, it is worth noting that \citep{li2023mpcformer} only evaluated model performance in plaintext, rather than in the context of MPC. 

 \textbf{Incompatible architectures.} 
\citep{li2023mpcformer,liang2023merge} proposed to modify the architecture of Transformer models to further accelerate secure inference, \eg, decompose the embedding procedure or reorganize the linear layers. %LayerNorm requires $\mathsf{rSqrt}$ function in MPC so it's costly, 
Worsely, \citep{li2023mpcformer} does not support secure LayerNorm and  simulated the costs using BatchNorm, resulting in incorrect secure inference results.
These modifications are in conflicts with existing plaintext Transformer systems, and would lead to deployment obstacles. 

%, and not easy to be deployed in real applications.   
%Existing works are either not open-sourced~\cite{hao2022iron} or only used for simulating time and communication costs (and cannot generate correct outputs)~\citep{li2023mpcformer}. To our best knowledge, there is no open-source library that can support accurate secure inference of Transformer models.

To summarize, in the field of MPC Transformer inference, achieving both model performance and efficiency is challenging, and people may ask the following question:

\textit{Could pre-trained large transformer models be securely and efficiently evaluated with similar accuracy as in plaintext, without further retraining ?}

 To address this challenge, we propose the \puma\ framework, which is a fast and accurate end-to-end secure Transformer inference framework. Our contributions can be summarized as follows:
\begin{itemize}
    \item \textbf{New  Approximations for Non-linear Functions.} We propose more accurate and faster approximations for the expensive non-linear functions (\eg, $\gelu$ and $\softmax$) in Transformer models. Different from existing works, we design the approximations based on the specialized properties of these non-linear functions to achieve both accuracy and efficiency. 

    \item \textbf{Faster and More Accurate  Secure Inference.}   We make extensive experiments on 6 transformer models and 4 datasets,  the results show that \puma's precision is similar to plaintext ones'  and is about $2\times$ faster than \mpcformer\  (note that \mpcformer\ does not achieve similar precision as \puma). \puma\ can even evaluate LLaMA-7B in around 5 minutes to generate one word. To our best knowledge, this is the first time that such a large language model is able to be evaluated under MPC.
        
    \item \textbf{End-to-End Framework compatible with plaintext.}
    We design and implement all the layers required by Transformer (including the Embedding and LayerNorm layers that are missing in other works)  in MPC.  This allows us to load and securely evaluate the pre-trained plaintext Transfomer models (\eg\ downloaded from Hugging face) easily. To our best knowledge,  \puma\ is the first open-sourced MPC solution  that supports accurate inference of pre-trained Transformer models without further modifications such as re-training.  

    

\end{itemize}

\textbf{Organization.} We summarize the related work in \S~\ref{sec:relatedwork} and present the background in \S~\ref{sec:back}. We give \puma's high-level view and concrete design in \S~\ref{sec:design}. We analyze the experimental results in \S~\ref{sec:experiment} and conclude this work in \S~\ref{sec:conclusion}.