\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{color}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{amsmath,amsfonts,amssymb,stmaryrd,xspace,enumitem}
\usepackage{multirow}
\usepackage{algorithmic,algorithm}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{wrapfig}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, positioning}


\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
\input{math_commands}

\title{\puma: Secure Inference of LLaMA-7B in Five Minutes}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\input{authors}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
With ChatGPT as a representative, tons of companies have began to provide services based on large Transformers models. However, using such a service inevitably leak users' prompts to the model provider. Previous studies have studied secure inference for Transformer models using secure multiparty computation (MPC), where model parameters and clients' prompts are kept secret. Despite this, these frameworks are still limited in terms of model performance, efficiency, and deployment. To address these limitations, we propose framework \puma\ to enable fast and secure Transformer model inference. 
Our framework designs high quality approximations for expensive functions such as $\gelu$ and $\softmax$, and significantly reduce the cost of secure inference while preserving the model performance. Additionally, we design secure Embedding and LayerNorm procedures that faithfully implement the desired functionality without undermining the Transformer architecture. \puma\ is about $2\times$ faster than the state-of-the-art  framework \mpcformer (ICLR 2023) and has similar accuracy as plaintext models without fine-tuning (which the previous works failed to achieve).  
\puma\ can even evaluate LLaMA-7B in around 5 minutes to generate $1$ token. To our best knowledge, this is the first time that a model with such a parameter size is able to be evaluated under MPC. \puma\ has been open-sourced in the Github repository of SecretFlow-SPU\footnote{\url{https://github.com/secretflow/spu/tree/main/examples/python/ml/flax_llama7b}}.

\end{abstract}

\input{Introduction}
\input{relatedwork}
\input{prem}
\input{overview}
\input{experiments}
\input{conclusion}


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\input{appendix}

\end{document}
