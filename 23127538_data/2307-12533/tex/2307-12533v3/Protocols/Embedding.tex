\subsection{Protocol for Secure Embedding}\label{sec:embed}


The current secure embedding procedure described in~\citep{li2023mpcformer} necessitates the client to  generate a one-hot vector using the token $\tokenid$ locally. This deviates from a plaintext Transformer workflow where the one-hot vector is generated inside the model. As a result, they have to carefully strip off the one-hot step from the pre-trained models, and add the step to the client side, which could be an obstacle for deployment. 



To address this issue, we propose a secure embedding design as follows. Assuming that the token $\tokenid\in [n]$ and all embedding vectors are denoted by $\E= (\e_1^T, \e_2^T, \dots, \e_n^T)$, the embedding can be formulated as $\e_{\tokenid} = \mathbf{E}[\tokenid]$. Given $(\tokenid, \E)$ are in secret-shared fashion, our secure embedding protocol $\Pi_{\mathsf{Embed}}$ works as follows:
\begin{itemize}
    \item The computing parties securely compute the one-hot vector $\shareb{\mathbf{o}}$ after receiving $\share{\tokenid}$ from the client. Specifically, $\shareb{\mathbf{o}[i]}=\Pi_{\mathsf{Eq}}(i,\share{\tokenid})$ for $i\in [n]$.
    \item The parties can compute the embedded vector via $\share{\e_{\tokenid}} = \Pi_{\mathsf{Mul_{BA}}}(\share{\E}, \shareb{\mathbf{o}})$, where  does not require secure truncation.
\end{itemize}
In this way, our $\Pi_{\mathsf{Embed}}$ does not require explicit modification of the workflow of plaintext Transformer models, at the cost of more $\Pi_{\mathsf{Eq}}$ and $\Pi_{\mathsf{Mul_{BA}}}$ operations. 


