\section{Introduction}\label{sec:intro}
Pre-trained Transformer models~\citep{transformer} have attracted much attentions for their high performance in practical tasks~\citep{gpt,zhuge2021kaleido} and been widely in Deep Learning as a Service (DLaaS) paradigm~\citep{soifer2019deep}. However, these services can raise privacy concerns, such as in the case of ChatGPT~\citep{chatgpt}, which requires either users to reveal their private prompts to the service provider or the service provider to release their proprietary trained weights to users.

One solution to address the privacy concerns of Transformer models service is Secure Multi-Party Computation (MPC)~\citep{shamir1979share,yao1986generate,gmw}, which can keep data and model weights securely during inference. However, the vanilla Transformer inference in MPC is too time- and communication-expensive to be practical for real-world applications.
To achieve better efficiency, existing work~\citep{hao2022iron,li2023mpcformer,privformer,liang2023merge,liu2023llms} proposed various ways to speed up the secure inference of Transformer models, but these approaches still have one or several of the following drawbacks: 
%\romannumeral1) \textit{
\begin{itemize}
    \item \textbf{Rough Replacements.} 
Recently, several works \citep{li2023mpcformer,privformer,liu2023llms} have proposed using fast approximations such as quadratic and ReLU functions to replace expensive functions like $\gelu$ and $\softmax$ to reduce costs. However, simply replacing these functions can result in a significant decrease in Transformer model performance, which may require extra model retraining (fine-tuning) and lead to deployment obstacles. 
%Furthermore, it is worth noting that \citep{li2023mpcformer} only evaluated model performance in plaintext, rather than in the context of MPC. 
\item  \textbf{High inference cost.} \citep{hao2022iron} adopted to approximate the expensive non-linear functions by using more accurate polynomials, but their approximation methods do not take the special properties of $\gelu$ and $\softmax$ into account. Therefore, their cost is still high  after using approximations. 
\item  \textbf{Not-Easy Deployment.} 
\citep{li2023mpcformer,liang2023merge} proposed to modify the architecture of Transformer models to accelerate secure inference, \eg, decompose the embedding procedure and reorganize the linear layers. %LayerNorm requires $\mathsf{rSqrt}$ function in MPC so it's costly, 
Worsely, as framework Crypten~\citep{crypten2020} does not support secure LayerNorm, \citep{li2023mpcformer} only simulated the costs using BatchNorm, resulting in incorrect secure inference results.
These modifications are in conflicts with existing plaintext Transformer systems. 
\end{itemize}
%, and not easy to be deployed in real applications.   
%Existing works are either not open-sourced~\cite{hao2022iron} or only used for simulating time and communication costs (and cannot generate correct outputs)~\citep{li2023mpcformer}. To our best knowledge, there is no open-source library that can support accurate secure inference of Transformer models.

To summarize, in the field of MPC Transformer inference, achieving both model performance and efficiency is challenging, and people may ask the following question:

\textit{Could pre-trained large transformer models be securely and efficiently evaluated with similar accuracy as in plaintext, without further retraining ?}

 To address this challenge, we propose the \puma\ framework, which is a fast and accurate end-to-end secure Transformer inference framework. Our contributions can be summarized as follows:
\begin{itemize}
    \item \textbf{New  Approximations for Non-linear Functions.} We propose more accurate and faster approximations for the expensive non-linear functions (\eg, $\gelu$ and $\softmax$) in Transformer models. Different from existing works, we design the approximations based on the specialized properties of these non-linear functions to achieve both accuracy and efficiency. 

    \item \textbf{Faster and More Accurate  Secure Inference.}   We make extensive experiments on 6 transformer models and 4 datasets,  the results show that \puma's precision is similar to plaintext ones'  and  is about $2\times$ faster than \mpcformer\  (note that \mpcformer\ does not achieve similar precision as \puma). \puma\ can even evaluate LLaMA-7B in around 5 minutes to generate one word. To our best knowledge, this is the first time that such a large language model is able to be evaluated under MPC.
        
    \item \textbf{Open-sourced End-to-End  Framework.}
    We design and implement the secure Embedding and LayerNorm procedures (which are lacked in other related works) faithfully in MPC. As a result, \puma\ follows the workflow of plaintext Transformer models and does not change any model architecture, allowing loading and evaluating the pre-trained plaintext Transfomer models (\eg\ downloaded from Hugging face) easily. To our best knowledge,  \puma\ is the first open-sourced MPC solution that supports accurate inference of pre-trained Transformer models without further modification efforts such as re-training. 
    

\end{itemize}

\textbf{Organization.} We summarize the related work in \S~\ref{sec:relatedwork} and present the background in \S~\ref{sec:back}. We give \puma's high-level view and concrete design in \S~\ref{sec:design}. We analyze the experimental results in \S~\ref{sec:experiment} and conclude this work in \S~\ref{sec:conclusion}.