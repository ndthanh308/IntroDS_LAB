\appendix
\section{Details of Experimental Models}
In this section, we present the architecture of the experimental models in brief. For more details, please refer to HuggingFace Transformers library~\citep{huggingfacetransformers}.
\begin{itemize}
    \item Bert-Base: Bert-Base is the base version of the Bert model and consists of $12$ Transformer encoder layers, $768$ hidden size, and $12$ heads. It has $110$ million parameters and is trained on a large corpus of unlabeled text data.
    
    \item Roberta-Base: Similar to Bert-base, Roberta-base is a base version of the Roberta model. It comprises $12$ Transformer layers, $768$ hidden size, and $12$ heads. It has around 125 million parameters.

    \item Bert-Large: Bert-Large is an extended version of Bert-base with $24$ Transformer encoder layers, $1024$ hidden size, and $16$ heads. It has approximately $340$ million parameters, making it more powerful and capable of capturing complex language patterns.

    \item GPT2-Base: GPT2-Base is the base version of the Gpt2 model and consists of $12$ Transformer decoder layers, $768$ hidden size, and $12$ heads. It has $117$ million parameters and is trained on a large corpus of text data. GPT2-Base is mainly used for tasks involving text generation and language understanding.

    \item GPT2-Medium: GPT2-Medium comprises $24$ Transformer decoder layers, $1024$ hidden size, and $16$ heads. And it has approximately $345$ million parameters. 

    \item GPT2-Large: GPT2-Large is the largest variant of the GPT2 model, featuring $36$ Transformer decoder layers, $1280$ hidden size, and $16$ heads.
    It approximately $774$ million parameters.
\end{itemize}

%\section{Time in WAN}
%\input{Table/time_wan}