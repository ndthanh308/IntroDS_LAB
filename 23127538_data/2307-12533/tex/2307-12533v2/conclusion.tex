\section{Conclusion}\label{sec:conclusion}
We propose an efficient MPC framework \puma\ for secure inference on Transformer models based on replicated secret sharing. To reduce the costs of secure inference, we approximate expensive functions with accurate polynomials and propose secure Embedding and LayerNorm protocols to support end-to-end secure inference. Although the inference cost is still quite high, we successfully make it one step closer to solving users' privacy concerns in Transformer-based DLaaS. We believe that by combining \puma\ with quantization methods and hardware accelerations in the future, secure inference of large Transformer models in seconds is no longer impossible.