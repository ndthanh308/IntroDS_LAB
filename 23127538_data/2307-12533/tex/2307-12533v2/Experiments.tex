\section{Experimental Evaluations}\label{sec:experiment}

\textbf{Implementation.}
We implement \puma\ on top of SecretFlow~\citep{spu} in \textrm{C++} and Python. SecretFlow compiles a high-level Flax code to secure computation protocols, which are then executed by our designed cryptographic backends, and we encode the floating-ponit values as $64$-bit integers in ring $\mathbb{Z}_{2^{64}}$ with $18$-bit fractional part. 
Our experiments are run on 3 Alibaba Cloud ecs.g7.8xlarge servers with 32 vCPU and 128GB RAM each. The CPU model is Intel Xeon(Ice Lake) Platinum 8369B CPU @ 2.70GHz. We evaluate \puma\ on Ubuntu 20.04.6 LTS with Linux kernel 5.4.0-144-generic. Our bandwidth is about 5Gbps and round trip time is about 1ms. %\cheng{Describe fixed point parameters: scale, share bits.}

\textbf{Models \& Datasets.}
We evaluate \puma\ on seven NLP models: Bert-Base, Roberta-Base, and Bert-Large~\citep{bert}; GPT2-Base, GPT2-Medium, and GPT2-Large~\citep{gpt}; and LLaMA-7B~\citep{touvron2023llama}. We measure the Bert performance for three NLP tasks over the datasets of Corpus of Linguistic Acceptability (CoLA), Recognizing Textual Entailment (RTE), Stanford Question Answering Dataset (QNLI) from GLUE benchmarks~\citep{wang2018glue}, and GPT2 performance on Wikitext-103 V1~\citep{merity2016pointer}.

\textbf{Baseline.}
We compare \puma\ to the most similar prior work \mpcformer~\citep{li2023mpcformer}. But for fair comparison, we have the following considerations:
\romannumeral1) As \mpcformer\ neither supports loading pretrained transformer models nor implements LayerNorm faithfully\footnote{ As \mpcformer~does not support loading pre-trained Transformer models, we did an experiment in plaintext Bert-Base that replaced LayerNorm with BatchNorm  as \mpcformer~did. This  resulted in a significant drop in the MCC score for CoLA task from $0.616$ to $-0.020$. On the contrary, \puma~achieves an MCC score of $0.613$. }, we cannot achieve meaningful secure inference results using their framework.
Therefore, we compare our secure Transformer models inference performance to that of plaintext (floating-point) to show our precision guarantee.
\romannumeral2) \mpcformer\ with \textit{Quad} approximations (for both $\gelu$ and $\softmax$) requires retraining the  modified models. As \puma\ does not require retraining, we compare our cost to that of \mpcformer\ without \textit{Quad} approximations. Also, we re-run \mpcformer~in our environment.



\subsection{Precision}\label{sec:accuracy}

% Figure environment removed

%\input{Table/accppl}

We compare our secure model 
inference performance to that of plaintext (floating-point) in Figure~\ref{fig:performance} to show our precision guarantee.

In Figure~\ref{fig:bert-base}-\ref{fig:bert-large}, we show the Matthews correlation/accuracy of plaintext and \puma\ on the Bert-Base, Roberta-base, and Bert-Large. We observe that the accuracy achieved by \puma~ matches the accuracy of the plaintext Flax code. Specifically, the accuracy difference does
not exceed $0.011$ over all datasets. 

Moreover, in Figure~\ref{fig:gpt2}, we also compare our perplexity on dataset Wikitext-103 V1 with the plaintext baseline on models GPT2-Base, GPT2-Medium, and GPT2-Large. The results are similar and the perplexity differences do not exceed $0.02$ over all models.

The above accuracy and perplexity advantages experimentally validate that our protocols are numerically precise. 

\subsection{Inference cost}\label{sec:efficiency}
\input{Table/CostsofOneInput}

In this subsection, we compare \puma's inference cost to that of \mpcformer. 
We evaluate  three Bert models (Bert-Base, Roberta-Base, and Bert-Large) and three GPT2 models (GPT2-Base, GPT2-Medium, and GPT2-Large).
The costs are for processing one input sentence: \romannumeral1) For Bert models the input sentence is of length $128$. \romannumeral2) GPT2 models input one length-32 sentence and generate $1$ new word. 

On the 3 Bert models in Table~\ref{tab:costbert}, \puma\ is  $1.375\sim 1.916\times$ faster than  \mpcformer, and is $1.079\sim 1.195\times$ more communication-efficient. For the GPT2 models in Table~\ref{tab:costgpt2}, \puma\ is $2.250\sim 2.414\times$ faster than \mpcformer, and is $1.325\sim 1.884\times$ more communication-efficient. 
    
We observe that \puma's improvements increase as the model size grows, particularly for the GPT2 models. This trend is because our specialized optimizations are more effective when processing large-scale evaluations.



\subsection{Scalability}\label{sec:scala}

In this subsection, we measure the costs of evaluating \puma\ on Bert-Base and GPT2-Base models for varying-length inputs, and varying-length outputs (only for GPT2-Base). We also compare our costs to those of \mpcformer~to demonstrate our improvements.




\input{Table/CostInputLength}
\textbf{Input Length Evaluation.}
Table~\ref{tab:costbertinput} shows our costs on varying-length inputs, we evaluate Bert-Base on the inputs of length $\{64, 128, 256, 512\}$, and GPT2-Base on the inputs of length $\{16, 32, 64, 128\}$.
For Bert-Base, \puma\ is $1.720\sim 2.282\times$ faster, and for GPT2-Base, \puma\ is $1.550\sim 2.686\times$ faster. Unlike the observations in Section~\ref{sec:efficiency}, our efficiency gains decrease with increasing input sizes in GPT2, and \puma\ requires more communication when the input length is greater than 64. This phenomenon is attributed to the interesting fact: To directly support pre-trained plaintext models, \puma\ strictly follows the plaintext model format that only accept token ids as input, so \puma\ has to compute the one-hot vectors from token ids in an MPC way. On the other hand, \mpcformer\ uses modified models that accept one-hot vectors as input, so the one-hot function could be computed at the client side in plaintext. Nevertheless, \puma\ remains faster than \mpcformer.

%\input{Table/CostGPT2token}

\begin{wrapfigure}{r}{0.4\textwidth}
    % Figure removed
    \caption{Runtime of GPT2-Base for generating different number of output tokens, the input length is of length $32$.} 
    \label{fig:gptwoutcosts}
\end{wrapfigure}

\textbf{Output Length Evaluation.}
Fig~\ref{fig:gptwoutcosts} presents our costs on varying-length outputs for GPT2-Base, and compares our costs to those of \mpcformer. Our improvements in runtime range from $1.279\sim 2.700\times$ respectively.
As more output tokens are generated, both costs increase in a linear way, this is because each output token must be input back into the model to generate the next token, increasing the required one-hot embedding costs. We should emphasize
again that although the time costs might be close for long outputs, \puma\ could achieve a similar accuracy as plaintext models while \mpcformer\  could not. 


\input{Table/llama7b}

\textbf{Scale to LLaMA-7B in Five Minutes.}
We evaluated the large language model LLaMA-7B using \puma\ under 3 Alibaba Cloud
ecs.r7.32xlarge servers, each has 128 threads and 1TB RAM, with 20GB bandwidth, 0.06ms round-trip-time. 
As shown in Table~\ref{tab:llama7b}, \puma\ can support the secure inference of large language model LLaMA-7B with reasonable costs. For example, given an input sentence of 8 tokens, \puma\ can output one token in around $346.126$ seconds with communication costs of $1.865$ GB. To our knowledge, this is the first time that LLaMA-7B has been evaluated using MPC.


%Llama-7B, LAN=(20GB, 0.06ms), 128 threads, input length=8, output=1 token, costs: 346.126s, 2002213760 bytes