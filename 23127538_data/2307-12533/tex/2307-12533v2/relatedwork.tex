\section{Related Work}\label{sec:relatedwork}
Transformer models have achieved remarkable success in language understanding~\citep{gpt,bert,xlnet,touvron2023llama}, vision understanding~\citep{zhuge2021kaleido,dong2022bootstrapped,chen2021pre}, and etc. Typically, Transformer models employ a two-stage training strategy: \romannumeral1) Transformer models are first pre-trained on a large dataset for general understanding, \romannumeral2) and then fine-tuned~\citep{sun2020finetune} on a small downstream dataset to learn task-specific features, resulting in improved model performance. This training strategy has been proven to be effective in various settings and has become the dominant paradigm~\citep{gpt,liu2019roberta,bert}. In this work, we assume that model providers use pre-trained and fine-tuned Transformer models for online services.

Secure Multiparty Computation (MPC)~\citep{shamir1979share,yao1986generate,gmw} enables distrusted parties to jointly compute a function with keeping their private inputs securely, and secure NN inference using MPC has gained much attention due its high privacy protection.
These works operate in a variety of different models and architectures, including two-party setting~\citep{mohassel2017secureml,liu2017oblivious,mishra2020delphi,cheetah,patra2021aby2,cryptflow2}, three-party setting~\citep{wagh2019securenn,aby3,wagh2020falcon,kumar2019cryptflow,patra2020blaze,tan2021cryptgpu,meteor}, four-party setting~\citep{byali2020flash,dalskov2021fantastic}, and etc~\citep{braun2022motion}. 
Among these works, the three-party based approaches resisting semi-honest adversaries in honest majority has the highest concrete efficiency, and have gained much attention.
However, most of these approaches only consider secure inference of convolutional/deep neural networks, and cannot be directly extended to support fast secure Transformer models inference. Recently, several research works \citep{hao2022iron,li2023mpcformer,privformer,liang2023merge,liu2023llms} have proposed MPC-based secure inference solutions for Transformer models. However, these approaches still have limitations in terms of model performance, efficiency, and deployment. Among these works, \mpcformer~\citep{li2023mpcformer} is the only one that have been open-sourced, it is based on CrypTen~\citep{crypten2020} which is a three-party framework that uses a non-colluding third party to produce correlated randomness for the client and server. In this work, we mainly compare our proposed framework \puma\ with \mpcformer\ under the same three-party threat model.
