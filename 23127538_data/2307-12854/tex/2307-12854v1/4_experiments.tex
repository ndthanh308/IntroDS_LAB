\section{Experiments}
%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|c|ccc|ccc|ccc|}
%\hline
%Pretraining & Multiple  &  & Ego4D \uparrow &  &  & EK55 \uparrow & &  & EK100 \uparrow & \\
%approach & clips used & Verb & Noun & Action & Verb & Noun & Action & Verb & Noun & Action\\
%\hline
%\rowcolor{lightgray} Action recognition & No & 20.70 & 14.41 & 17.56 & 18.11 & 11.48 & 14.80 & 18.82 & 12.46 & 15.64\\
%CPC \cite{oord2018representation} & Yes & 27.26 & 26.57 & 26.91 & 23.00 & 17.24 & 20.13 & 23.16 & 17.06 & 20.11\\
%DPC \cite{han2019video} & Yes & 28.18 & 29.03 & 28.61 & 24.02 & 19.03 & 21.52 & 25.25 & 18.18 & 21.72\\
%CVRL (single) \cite{qian2021spatiotemporal} & Yes & 25.90 & 25.85 & 25.88 & 22.17 & 17.07 & 19.62 & 22.92 & 16.60 & 19.76\\
%CVRL (sequence) \cite{qian2021spatiotemporal} & No & 28.27 & 29.74 & 29.00 & 23.91 & 18.32 & 21.12 & 24.94 & 19.24 & 22.09\\
%LSTCL \cite{wang2022long} & Yes & 26.82 & 27.76 & 27.29 & 23.59 & 18.52 & 21.05 & 23.47 & 17.15 & 20.31\\
%CONSTCL \cite{yuan2022contextualized} & Yes & 27.49 & 29.13 & 28.31 & 24.47 & 19.52 & 22.00 & 25.41 & 19.35 & 22.38\\
%\modelabb (Ours) & Yes & \textbf{30.18} & \textbf{32.33} & \textbf{31.25} & \textbf{25.83} & \textbf{20.78} & \textbf{23.31} & \textbf{26.69} & \textbf{20.18} & \textbf{23.44}\\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Order-unaware long-term forecasting evaluation on Ego4D and Epic-Kitchens 55/100 datasets.} We report the mean average precision over all verb and noun classes.}
%\label{tab:order_unaware_forecasting_results}
%\end{table*}

\subsection{Downstream tasks}
We compare our \modelname objective to state-of-the-art self-supervised video pretraining methods on the tasks of \emph{order-agnostic} and \emph{specific} long-term action forecasting as well as video summary forecasting. We pretrain the video models on Ego4D \cite{grauman2022ego4d} and finetune them on both Ego4D and EpicsKitchen-55/100 \cite{Damen2018EPICKITCHENS,damen2020rescaling} for the downstream tasks. Additionally, we use a transformer encoder \cite{vaswani2017attention} and the meanpooling operation as our temporal context aggregators $h_\phi$ and $h_\mu$ (Section~\ref{sec:video_model}), respectively. We refer readers to the supplemental for more details of these datasets, implementation and baseline models.
%We refer interested readers to the supplemental for more details of these datasets and implementation.

%% Figure environment removed

\noindent\textbf{Order-agnostic action forecasting.} In order-agnostic long-term forecasting, we observe K\% of a video of duration T and predict if an action will occur within the remaining video. Given a vocabulary of $N_{\text{verb}}$ and $N_{\text{noun}}$ classes, we predict a $N_{\text{verb}}$-dimensional and $N_{\text{noun}}$-dimensional binary vectors, where each dimension indicate the probability of the class occurring in the future. We formulate this as a multi-label prediction task and finetune all pretrained models by optimizing the binary cross-entropy loss computed over all verb and noun classes. We compute the mean average precision (mAP) over all verb and noun classes.

\noindent\textbf{Order-specific action forecasting.} The order-specific task is a much more challenging setting, where the model is penalized even if it predicts the correct verb or noun but in the wrong order. Since the accuracy of the predicted actions depends on their temporal ordering, this can be formulated as a sequence prediction task. We finetune the pretrained models by optimizing the total cross-entropy losses for both verbs and nouns computed over all time steps. We adopt the edit distance metric \cite{grauman2022ego4d} to quantify how dissimilar the predicted and ground-truth action  sequences are to each other.

\noindent\textbf{Video summary forecasting.} In this multimodal task, for a video $V$ of $T$ temporal clips and an observed subsequence of length $T^O$, the goal is to retrieve its corresponding summary from a set of distractor summaries. Given the video $V$ and its summary $L$ containing $N_L$ words, we first extract the contextualized representation for the observed clip sequence: $c_{T^O} = h_\theta^{\text{agg}} (g_\theta^V (V_{0:T^O}))$. We extract a natural language representation $f_L \in \mathbb{R}^{L \text{x} D_L}$ for the summary using the pretrained BERT-Base \cite{devlin2018bert} model: $f_L = k_\phi(L)$, where $D_L$ is the output dimension of the BERT model and $k_\phi$ denotes the BERT model that is parameterized by $\phi$. We use linear layers $W_V$ and $W_L$ to project the video and language representations into the joint visual-semantic embedding space and finetune the models by optimizing the following contrastive loss formulation:
\begin{equation}
    L = \sum^B_{b=1} -\log \frac{\exp(c_{b,T^O} \cdot f_{b,L} / \tau)}{\splitfrac{\textstyle
            \exp(c_{b,T^O} \cdot f_{b,L} / \tau) +}
          {\textstyle 
           \sum\limits_{m \neq b}\exp(c_{b,T^O} \cdot f_{m,L} / \tau)}}.
\end{equation}
Intuitively, this objective encourages the model to learn an alignment between the video and language representations by maximizing the similarity between corresponding pairs of videos and text summaries. Consistent with prior work in text-to-video retrieval \cite{zhou2018towards}, we adopt the Recall@$K$ metric which computes the percentage of times the ground-truth summary is ranked in the top $K$ retrievals.

%\subsection{Datasets}
%\noindent\textbf{Ego4D. } The Ego4D dataset is the largest dataset of egocentric videos spanning over 3600 hours of daily life activities ranging from household to outdoor leisure scenarios. These videos are collected by 931 camera-wearers from 9 different countries, who record their unscripted interactions as they engage in daily activities under a large variety of settings. In contrast to existing video recognition datasets, videos in Ego4D are generally much longer in duration since they span from 1 to 10 hours as compared to 10 seconds video clips in Kinetics 400/600 \cite{carreira2018short,carreira2019short}. Additionally, it is much larger in scale and diversity of activities than existing egocentric video datasets such as Epic-Kitchens 55/100 \cite{Damen2018EPICKITCHENS, damen2020rescaling}. Each video is also densely annotated by humans, who provide annotations describing notable interactions in the videos as well as high-level summaries. This dataset facilitates the exploration and further research in a variety of downstream tasks such as audio-visual diarization and forecasting. We use the provided annotations to evaluate our proposed MTPL approach on long-term forecasting as well as video summary predictions. We adopt the same splits for training and evaluation on the target tasks as Grauman \etal \cite{grauman2022ego4d}. In this dataset, we conduct our evaluations on the training and validation splits since the test evaluation is conducted on a held-out set via a submission to their challenge portal. We also note that the number of verb and noun classes present in all 3 provided splits are not consistent since each split contains some verb and noun classes that are not present in other splits. Please refer to the supplementary material for more details. 

%\noindent\textbf{EpicsKitchen-55/100. } EpicKitchens-100 (EK100) \cite{damen2020rescaling} is another large dataset of egocentric videos. Similar to Ego4D, it also provides 700 long unscripted egocentric videos that span approximately 100 hours. It is less diverse than Ego4D since the participants only engage in daily activities in the kitchen. EpicKitchens-55 (EK55) \cite{Damen2018EPICKITCHENS} is an earlier and smaller version of EK100 but it provides the same types of videos and annotations. We use EK55 and EK100 to evaluate on the tasks of order-agnostic and order-specific long-term forecasting. 

%\subsection{Implementation details}
%We implement all models and experiments using the Pytorch deep learning library. We use the Multiscale Vision Transformer (MVIT) \cite{fan2021multiscale} as our base video encoder and 1 transformer encoder layers with 1 attention heads as our temporal context aggregator. The MVIT encoder typically accepts a video clip of 16 frames as input and outputs a global clip representation, which is the contextualized output of the classification token. However, in our case, we reduce the number of frames per clip to 8 due to memory constraints. Additionally, we discard the classification token during pretraining and perform our future feature predictions at the spatiotemporal region granularity. During the second stage of finetuning, we compute a global clip representation by performing meanpooling over the spatiotemporal region representations. 

%Since we sample the video frames at 10 frames per second (FPS), the temporal duration of each clip is approximately 0.8 seconds long. Each input video clip is preprocessed by randomly scaling the height of the frames between 248 and 280 pixels and taking crops of 224 x 224 pixels. During the first stage of pretraining on the Ego4D dataset, we also perform random augmentations to the video clips including random horizontal flipping and color jittering. The future feature prediction function is represented as a two-layer multilayer perceptron (MLP) with a non-linear ReLU operation and hidden dimension of 768.

%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|c|ccc|ccc|}
%\hline
%Pretraining approach & Multiple clips used & Verb (4) & Noun (4) & Mean (4) & Verb (8) & Noun (8) & Mean (8) \\
%\hline
%\rowcolor{lightgray} Ego4D action recognition & No & 17.69 & 14.05 & 15.87 & 20.70 & 14.41 & 17.56 \\
%CVRL (single clip) \cite{qian2021spatiotemporal} & No & 22.64 & 24.85 & 23.74 & 25.90 & 25.85 & 25.88 \\
%CPC \cite{oord2018representation} & Yes & 23.74 & 25.53 & 24.64 & 27.26 & 26.57 & 26.91 \\
%LSTCL \cite{wang2022long} & Yes & 23.57 & 27.20 & 25.38 & 26.82 & 27.76 & 27.29 \\
%CONSTCL \cite{yuan2022contextualized} & Yes & 23.81 & 27.81 & 25.81 & 27.49 & 29.13 & 28.31 \\
%DPC \cite{han2019video} & Yes & 25.76 & 28.22 & 26.99 & 28.18 & 29.03 & 28.61 \\
%CVRL (clip sequence) \cite{qian2021spatiotemporal} & Yes & 24.01 & 28.05 & 26.03 & 28.27 & 29.74 & 29.00 \\
%Ours & Yes & \textbf{26.17} & \textbf{31.83} & \textbf{29.01} & \textbf{30.18} & \textbf{32.33} & \textbf{31.25} \\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Order-unaware long-term forecasting evaluation on Ego4D.} We report the mean average precision (mAP) over all verb and noun classes.}
%\label{ego4d_order_unaware_results}
%\end{table*}

%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|c|ccc|ccc|}
%\hline
%Pretraining approach & Multiple clips used & Verb (4) & Noun (4) & Mean (4) & Verb (8) & Noun (8) & Mean (8) \\
%\hline
%CVRL (single clip) \cite{qian2021spatiotemporal} & No & 21.38 & 15.52 & 18.45 & 22.17 & 17.07 & 19.62 \\
%CPC \cite{oord2018representation} & Yes & 21.81 & 16.08 & 18.95 & 23.00 & 17.24 & 20.13 \\
%LSTCL \cite{wang2022long} & Yes & 21.63 & 16.15 & 18.89 & 23.59 & 18.52 & 21.05 \\
%CVRL (clip sequence) \cite{qian2021spatiotemporal} & Yes & 22.80 & 17.08 & 19.94 & 23.91 & 18.32 & 21.12 \\
%DPC \cite{han2019video} & Yes & 22.87 & \textbf{18.72} & 20.79 & 24.02 & 19.03 & 21.52 \\
%CONSTCL \cite{yuan2022contextualized} & Yes & 23.26 & 17.19 & 20.23 & 24.47 & 19.52 & 22.00\\
%Ours & Yes & \textbf{24.92} & 18.08 & \textbf{21.50} & \textbf{25.83} & \textbf{20.78} & \textbf{23.31} \\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Order-unaware long-term forecasting evaluation on EpicKitchen-55.} We report the mean average precision (mAP) over all verb and noun classes.}
%\label{ek55_order_unaware_results}
%\end{table*}

%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|c|ccc|ccc|}
%\hline
%Pretraining approach & Multiple clips used & Verb (4) & Noun (4) & Mean (4) & Verb (8) & Noun (8) & Mean (8) \\
%\hline
%CVRL (single clip) \cite{qian2021spatiotemporal} & No & 20.26 & 14.06 & 17.16 & 22.92 & 16.60 & 19.76 \\
%CPC \cite{oord2018representation} & Yes & 22.44 & 15.82 & 19.13 & 23.16 & 17.06 & 20.11 \\
%LSTCL \cite{wang2022long} & Yes & 21.91 & 15.25 & 18.58 & 23.47 & 17.15 & 20.31 \\
%DPC \cite{han2019video} & Yes & 22.72 & 15.67 & 19.20 & 25.25 & 18.18 & 21.72 \\
%CVRL (clip sequence) \cite{qian2021spatiotemporal} & Yes & 22.69 & 16.55 & 19.62 & 24.94 & 19.24 & 22.09 \\
%CONSTCL \cite{yuan2022contextualized} & Yes & 23.13 & 16.99 & 20.06 & 25.41 & 19.35 & 22.38\\
%Ours & Yes & \textbf{24.67} & \textbf{18.04} & \textbf{21.36} & \textbf{26.69} & \textbf{20.18} & \textbf{23.44} \\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Order-unaware long-term forecasting evaluation on EpicKitchen-100.} We report the mean average precision (mAP) over all verb and noun classes.}
%\label{ek100_order_unaware_results}
%\end{table*}

\begin{table*}[t]
{
\setlength{\tabcolsep}{3pt}
\begin{center}
\begin{tabular}{|c|c|c|ccc|ccc|ccc|}
\hline
Pretraining & Multiple & Pretraining &  & Ego4D $\downarrow$ &  &  & EK55 $\downarrow$ & &  & EK100 $\downarrow$ & \\
\cline{4-12}
approach & clips used & approach & Verb & Noun & Action & Verb & Noun & Action & Verb & Noun & Action\\
\hline
\rowcolor{lightgray} Action recognition & No & Strong & 0.754 & 0.901 & 0.977 & 0.741 & 0.947 & 0.962 & 0.758 & 0.952 & 0.969\\
CVRL \cite{qian2021spatiotemporal} & No & Self & 0.746 & 0.845 & 0.960 & 0.719 & 0.926 & 0.948 & 0.753 & 0.948 & 0.954\\
CPC \cite{oord2018representation} & Yes & Self & 0.735 & 0.838 & 0.956 & 0.719 & 0.936 & 0.951 & 0.746 & 0.944 & 0.954\\
LSTCL \cite{wang2022long} & Yes & Self & 0.752 & 0.846 & 0.963 & 0.721 & 0.935 & 0.950 & 0.739 & 0.939 & 0.950\\
DPC \cite{han2019video} & Yes & Self & 0.734 & 0.821 & 0.950 & 0.708 & 0.927 & 0.946 & 0.738 & 0.932 & 0.951\\
CVRL \cite{qian2021spatiotemporal} & Yes & Self & 0.735 & 0.822 & 0.952 & 0.719 & 0.926 & 0.948  & 0.735 & 0.930 & 0.948\\
CONSTCL \cite{yuan2022contextualized} & Yes & Self & 0.735 & 0.818 & 0.951 & 0.704 & 0.922 & 0.946 & 0.732 & 0.930 & 0.948\\
\modelabb (Ours) & Yes & Self & \textbf{0.724} & \textbf{0.809} & \textbf{0.943} & \textbf{0.690} & \textbf{0.908} & \textbf{0.941} & \textbf{0.721} & \textbf{0.918} & \textbf{0.942}\\
\hline 
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Order-specific long-term forecasting evaluation.} We use edit distance as the metric and report performance on verb, noun and action classes. An action class is a combination of its verb and noun classes. The results suggest that learning to understand the multiscale nature of videos is crucial for making accurate fine-grained predictions.}
\label{ego4d_lta_results}
\vspace{-10pt}
}
\end{table*}

\subsection{Quantitative results}
\subsubsection{Order-agnostic long-term forecasting}
\label{subsec:order-unaware}
We aim to evaluate the effectiveness of our proposed \modelabb pretraining approach at learning video representations that encode future context over different temporal horizons. As such, we predict the future actions over the next 8 time steps and report the results on Ego4D, EK55 and EK100 in Table~\ref{tab:order_unaware_forecasting_results}. We observe that self-supervised video pretraining is generally more beneficial to tasks requiring the key capability of long-term forecasting as compared to the strongly supervised variant of action recognition (first row of Table~\ref{tab:order_unaware_forecasting_results}). Despite not requiring human-annotated labels during pretraining, our proposed \modelabb approach leads to approximately 14\% improvement in future verb and noun predictions over its strongly-supervised counterpart when finetuned on the Ego4D task annotations. We hypothesize that the learning objective of predicting future clip representations is crucial for action anticipation.

We also observe across all datasets that the state-of-the-art pretraining objective of learning clip-invariant video representations \cite{qian2021spatiotemporal,feichtenhofer2021large} does not generalize well to downstream tasks that require effective reasoning over clip sequences. In fact, simply extending the aforementioned pretraining objective to maximize the similarity between representations of two clip sequences sampled from the same video leads to significant improvements in future action predictions, especially over the longer temporal horizon of 8 clips. Our \modelabb approach also outperforms LSTCL \cite{wang2022long} by a significant margin (\eg, we obtain a 3-5\% improvement on Ego4D). Since LSTCL aims to encode long-term temporal cues in video representations of shorter clip sequences, our gains suggest that learning to predict contextual information of future clip sequences serves as an effective pretraining objective for long-term video understanding.

\subsubsection{Order-specific long-term forecasting} 
Table~\ref{ego4d_lta_results} reports the results across all three datasets  on the more challenging task of predicting actions at specific time steps. Similar to our results for the order-unaware task in Section~\ref{subsec:order-unaware}, we also observe that our proposed \modelabb approach generalizes better to a task that requires accurate fine-grained predictions. We note that pretraining approaches that learn to predict future clip representations at the fine-grained region-level such as DPC, CONSTCL and ours generally perform better under this challenging setting as compared to variants that predict global representations of future video clips including CPC and CVRL. One possible reason is that predicting fine-grained spatiotemporal region representations in the future is a much more challenging objective that necessitates the video model to understand the structure of different atomic actions in untrimmed videos. In particular, our gains across all three datasets suggest that learning to predict future region-level representations is especially crucial for verb predictions. This is evidenced by the much larger margins of improvement achieved by such approaches in predicting verbs in future clips as compared to nouns. For example, \modelabb reduces the edit distances by 0.029 and 0.018 on verb and noun predictions, respectively. In contrast to the order-agnostic task, we see that the improvements achieved by our \modelabb objective are smaller, which further emphasizes the difficulty of predicting actions precisely at specific timesteps. 

Additionally, we aim to understand the effectiveness of learning to predict future contextual information that is aggregated from video clips over different temporal horizons. In particular, we compare against CONSTCL \cite{yuan2022contextualized}, which also aims to reconstruct fine-grained spatiotemporal region representations of a future video clip sequence given the context of an observed clip sequence. Despite not relying on pretrained object detectors to identify location priors, our proposed \modelabb approach outperforms CONSTCL on both verb and noun predictions (\eg reducing edit distance by 0.008 on Ego4D) while only using dense spatiotemporal feature maps. We hypothesize that our pretraining objective of predicting aggregated future spatiotemporal region representations helps a video model to better reason about the correlations between different atomic actions and how they contribute to the overarching goal in videos.

%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|c|c|cccc|}
%\hline
%Pretraining approach & Multiple clips used & Pretraining supervision & R@1 \uparrow & R@5 \uparrow & R@10 \uparrow & Median Rank \downarrow\\
%\hline
%\rowcolor{lightgray} Action recognition & No & Strong & 0.90 & 5.00 & 8.80 & 60.88\\
%CPC \cite{oord2018representation} & Yes & Self & 9.70 & 28.60 & 41.80 & 16.00\\
%DPC \cite{han2019video} & Yes & Self & 10.10 & 29.70 & 43.20 & 14.25\\
%CVRL (single) \cite{qian2021spatiotemporal} & Yes & Self & 11.00 & 34.80 & 49.50 & 10.63 \\
%CVRL (sequence) \cite{qian2021spatiotemporal} & Yes & Self & 15.90 & 40.70 & 56.50 & 8.25\\
%LSTCL \cite{wang2022long} & Yes & Self & 12.70 & 38.90 & 53.10 & 9.25\\
%CONSTCL \cite{yuan2022contextualized} & Yes & Self & 11.40 & 41.80 & 53.90 & 9.63\\
%\modelabb (Ours) & Yes & Self & \textbf{19.30} & \textbf{50.70} & \textbf{65.00} & \textbf{5.38}\\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Video summary prediction on the Ego4D dataset. } MVP helps the video model to learn more robust representations that generalize better than prior work to the multimodal task of text summary retrieval.}
%\label{ego4d_summary_prediction_results}
%\end{table*}

\subsubsection{Video summary forecasting}
Finally, Table~\ref{ego4d_summary_prediction_results} reports our results on the multimodal video summary forecasting task. Besides video-only tasks, we note that the self-supervised pretraining approaches also generalize much better to a downstream task that involves the language modality than the strongly-supervised task of action recognition. Unlike the results on the previous tasks of order-unaware and specific long-term forecasting, we observe that the pretraining objective of learning clip-invariant video representations such as CVRL (single and multiple clips) and LSTCL outperforms DPC by a substantial margin of $1 - 5\%$ in R@1 accuracy. 

We hypothesize that this may be due to the DPC pretraining approach training the video model to predict the representations of consecutive video clips in the future. In contrast, the aforementioned approaches sample the observed and predicted video clip sequences from the same video but at randomly determined times. This may encourage the video model to learn to extrapolate the contextual information further into the future instead of always predicting the \emph{immediate} future as in the case of the DPC method. Interestingly, we also observe that learning to predict fine-grained spatiotemporal region representations during pretraining may be not be as critical for understanding the overarching context of a video as the previous evaluation tasks. This is evidenced by the fact that CVRL pretrained with multiple video clips actually outperforms CONSTCL by $4\sim\%$ in R@1 accuracy. Lastly, the performance gains of approximately $3 - 8\%$ in R@1 accuracy achieved by our proposed \modelabb approach over CVRL clip sequence, LSTCL and CONSTCL suggest that learning to reason about aggregated future contextual information over multiple time scales is especially beneficial to helping a model to extrapolate the semantics of the entire video.

\begin{table}[t!]
{
\setlength{\tabcolsep}{2pt}
\begin{center}
\begin{tabular}{|c|c|c|ccc|}
\hline
\small{Pretraining} & \small{Multiple} & \small{Pretraining} &  &  & \\
\small{approach} & \small{clips} & \small{supervision} & \small{R@1}$\uparrow$ & \small{R@5}$\uparrow$ & \small{R@10}$\uparrow$ \\
\hline
\rowcolor{lightgray} \small{Action recognition} & \small{No} & \small{Strong} & 0.90 & 5.00 & 8.80\\
\small{CPC \cite{oord2018representation}} & \small{Yes} & \small{Self} & 9.70 & 28.60 & 41.80 \\
\small{DPC \cite{han2019video}} & \small{Yes} & \small{Self} & 10.10 & 29.70 & 43.20 \\
\small{CVRL \cite{qian2021spatiotemporal}} & \small{No} & \small{Self} & 11.00 & 34.80 & 49.50 \\
\small{LSTCL \cite{wang2022long}} & \small{Yes} & \small{Self} & 12.70 & 38.90 & 53.10\\
\small{CONSTCL \cite{yuan2022contextualized}} & \small{Yes} & \small{Self} & 11.40 & 41.80 & 53.90\\
\small{CVRL \cite{qian2021spatiotemporal}} & \small{Yes} & \small{Self} & 15.90 & 40.70 & 56.50\\
\small{\modelabb (Ours)} & \small{Yes} & \small{Self} & \textbf{19.30} & \textbf{50.70} & \textbf{65.00}\\
\hline 
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Video summary forecasting on the Ego4D dataset. } MVP helps the video model to learn more robust representations that generalize better than prior work to the multimodal task of text summary retrieval.}
\vspace{-10pt}
\label{ego4d_summary_prediction_results}
}
\end{table}

%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|ccc|ccc|}
%\hline
%Temporal offset K & Verb (4) & Noun (4) & Mean (4) & Verb (8) & Noun (8) & Mean (8) \\
%\hline
%1 & 20.68 & 20.33 & 20.51 & 23.47 & 21.10 & 22.28 \\
%4 & 23.87 & 25.39 & 24.63 & 27.15 & 26.09 & 26.62 \\
%8 & 24.74 & 26.34 & 25.54 & 27.95 & 26.78 & 27.37 \\
%12 & 22.96 & 25.01 & 23.99 & 26.39 & 25.98 & 26.18 \\
%16 & 24.37 & 25.34 & 24.85 & 27.88 & 26.09 & 26.99 \\
%Geometric & 23.78 & 25.49 & 24.63 & 26.80 & 25.99 & 26.39 \\
%Random (ours) & \textbf{26.17} & \textbf{31.83} & \textbf{29.01} & \textbf{30.18} & \textbf{32.33} & \textbf{31.25} \\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Temporal offset ablation study on Ego4D.} We ablate the effect of the temporal offset during pretraining on the downstream task of order-unaware long-term forecasting.}
%\label{ego4d_order_unaware_temporal_offset_ablation}
%\end{table*}

%% Figure environment removed

\subsubsection{Ablation studies}
We ablate different aspects of MVP approach to determine their relative contributions to the robustness of the learnt representations. Specifically, we compare the effectiveness of the representations of different model variants on the downstream task of order-unaware forecasting on Ego4D. 

% Figure environment removed

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|ccc|ccc|}
\hline
Temporal offset $K$ & Verb $\uparrow$ & Noun $\uparrow$ & Mean $\uparrow$\\
\hline
1 & 23.47 & 21.10 & 22.28 \\
4 & 27.15 & 26.09 & 26.62 \\
8 & 27.95 & 26.78 & 27.37 \\
12 & 26.39 & 25.98 & 26.18 \\
16 & 27.88 & 26.09 & 26.99 \\
Geometric & 26.80 & 25.99 & 26.39 \\
Random (ours) & \textbf{30.18} & \textbf{32.33} & \textbf{31.25} \\
\hline 
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Temporal offset ablation on Ego4D.} We ablate the effect of the temporal offset during pretraining on the downstream task of order-unaware long-term forecasting.}
\label{ego4d_order_unaware_temporal_offset_ablation}
\vspace{-15pt}
\end{table}

% Figure environment removed

\noindent\textbf{Effectiveness of \modelabb.} We evaluate the benefit of our \modelname approach in Figure~\ref{fig:pretraining_acc_plot} by studying the correlation between the prediction accuracy of the video model during pretraining and the downstream performance by using checkpoints at various stages of pretraining. While \modelabb uses a contrastive formulation, we compute the prediction accuracy as the percentage of predicted regions that have the highest similarity with their ground-truth counterparts. We observe a direct correlation between the prediction accuracy during pretraining and the mean mAP score over all verb and noun classes, which suggests that learning to encode the multiscale nature of videos in the base representations is beneficial for long-term forecasting tasks.

\noindent\textbf{Temporal offset $\mathrm{\mathbf{K}}$.} In Table~\ref{ego4d_order_unaware_temporal_offset_ablation}, we observe that verb and noun prediction accuracy increases as we increase $K$ during pretraining. This is unsurprising since the video model should be able to better predict future actions by learning to reason about the contextual information further into the future during pretraining. However, we also see that using a temporal offset of 12 clips actually leads to a drop in performance. One possible reason is that the future is non-deterministic and predicting information too far into the future introduces a high degree of noise during pretraining. We also hypothesize that sampling random temporal offset values works the best because learning to predict future contextual information over varying temporal horizons acts as a form of regularization and prevents the model from overfitting to predictions over a constant temporal period.

\noindent\textbf{Multiscale benefits.} We investigate the importance of multiscale aggregation during pretraining on downstream performance (Fig~\ref{fig:ablations_plot}(a)). Specifically, we train the video model with a variant of \modelabb where we only predict the uncontextualized representations of future clips (no aggregation) and another where the aggregation of context is computed over a single scale. To begin, we observe the importance of predicting contextualized representations, where predicting uncontextualized clip representations results in a drop of $2\sim$ \% in mean mAP. More importantly, we also see that learning to predict future clip representations that are aggregated over multiple timescales results in a significant improvement over predicting those that are only contextualized over a single timescale. These results may support our hypothesis that learning to understand the multiscale nature of actions helps the video model to better infer the underlying goals and thus, anticipate future actions.

%\begin{table*}[t]
%\begin{center}
%\begin{tabular}{|c|c|c|ccc|}
%\hline
%# input clips & # predicted clips & Temporal stride $s$ & Verb (8) & Noun (8) & Mean (8) \\
%\hline
%2 & 4 & 1 &  27.42 & 27.10 & 27.26 \\
%4 & 2 & 1 & 27.08 & 27.00 & 27.04\\
%4 & 4 & 3 & 27.07 & 26.34 & 26.71 \\
%4 & 6 & 1 & 27.69 & 27.19 & 27.44 \\
%6 & 4 & 1 & 27.17 & 26.98 & 27.07 \\
%4 & 4 & 1 & 28.51 & 30.28 & 29.40 \\
%4 & 4 & 2 & \textbf{30.18} & \textbf{32.33} & \textbf{31.25} \\
%\hline 
%\end{tabular}
%\end{center}
%\caption{\textbf{Ablation over the number of input and output clips as well as temporal stride used during pretraining on Ego4D.} }
%\label{tab:stride_ablation}
%\end{table*}

\noindent\textbf{Number of input clips $\mathrm{\mathbf{N_O}}$.} In Figure~\ref{fig:ablations_plot}(b), we observe that increasing the number of clips in the observed sequence $V^O$ during pretraining generally leads to better downstream performance. However, we see that the forecasting results drop when we use 8 input clips. One possible reason is that using more input clips results in more observed context which may ease the difficulty of the pretraining objective and consequently, reducing the robustness of the learnt representations to downstream forecasting tasks. 

\noindent\textbf{Number of predicted clips $\mathrm{\mathbf{N_P}}$.} We also aim to understand the importance of varying the number of predicted clips during pretraining on downstream forecasting performance in Figure~\ref{fig:ablations_plot}(c). Intuitively, setting a higher number of predicted future clips increases the difficulty of our \modelabb objective since the video has to learn to predict contextual information that is further out into the future. While increasing the number of predicted clips is generally beneficial for downstream performance, we also see that predicting 8 future clips results in a drop in performance. We theorize that it may be too hard to predict the contextualized information too far out into the future since it is non-deterministic. This may introduce some noise during pretraining which adversely affects the learnt video representations. 

\noindent\textbf{Temporal stride $\mathrm{\mathbf{S}}$ for aggregation.} Last but not least, we ablate the effect of the temporal stride $S$ during pretraining in Figure~\ref{fig:ablations_plot}(d). We obtain the best downstream performance when we increase the temporal stride from $1$ to $2$, which may suggest that a higher temporal stride encourages the video model to learn to encode longer-term future contextual information. We hypothesize that larger strides actually results in a significant drop in performance because it may be too challenging for the video model to learn to understand the structure and relationships between different atomic actions if they are very distant in time.

\subsection{Limitations} 
The target representations in \modelabb are computed by aggregating information over future clips using a fixed temporal stride for different timescales. However, this may not always be realistic since different complex actions can consist of varying numbers of atomic actions. %One potential avenue for future work is to learn to segment sequences of atomic action clips into composite actions as part of the pretraining approach.