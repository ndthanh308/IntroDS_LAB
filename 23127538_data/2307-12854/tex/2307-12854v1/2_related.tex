\section{Related work}
\noindent\textbf{Self-supervised video pretraining.} Self-supervised video pretraining \cite{feichtenhofer2021large,han2019video,wang2022long} has been demonstrated to be beneficial for improving performance on downstream tasks such as activity recognition \cite{fan2021multiscale,feichtenhofer2019slowfast,feichtenhofer2021large,han2019video,han2020memory,recasens2021broaden,wang2022long}, video object segmentation \cite{jabri2020space}, early action prediction \cite{shao2020finegym} and unintentional action detection \cite{han2019video,han2020memory} on target datasets including Kinetics-400/600 \cite{carreira2018short,carreira2019short,kay2017kinetics}, HMDB-51 \cite{kuehne2011hmdb} and UCF101 \cite{soomro2012ucf101}. Inspired by image-based self-supervised pretraining objectives \cite{chen2020simple, chen2020improved, chen2021exploring}, state-of-the-art video approaches \cite{feichtenhofer2021large,qian2021spatiotemporal,wang2022long,yuan2022contextualized} often use a similar learning objective of maximizing the similarity between representations of two clips sampled from the same video. The Contrastive Video Representation Learning (CVRL) \cite{qian2021spatiotemporal} approach also demonstrates that the applied transformations have to be consistent across all frames for optimal performance. 

Feichtenhofer \etal \cite{feichtenhofer2021large} also demonstrate that this objective of learning video clip-invariant representions can be extended beyond pairs of clips, which further improves the robustness of the learnt representations to the downstream task of action recognition. Additionally, the Contrastive Predictive Coding (CPC) \cite{oord2018representation} and Dense Predictive Coding (DPC) \cite{han2019video} approaches are also similar in spirit, where their learning objectives are to predict coarse clip-level and fine-grained spatiotemporal region representations of future clips given an observed sequence of clips for context, respectively. Han \etal \cite{han2020memory} further build on this by introducing a memory bank of learnable vectors to account for the non-deterministic nature of predicting the future. However, in contrast to our MVP approach, the aforementioned approaches learn to predict the information in the future clips that directly follow after the observed sequence. More importantly, they only predict the base representations of future video clips instead of their \emph{contextualized} representations, where their information has been aggregated over all preceding future clips in a causal manner. 

Additionally, BraVe \cite{recasens2021broaden} and LSTCL \cite{wang2022long} embody a similar idea of learning to encode long-term temporal cues in clip-level representations by maximizing the similarity between a pair of short and long clips from the same video. The \emph{multiscale} aspect of \modelabb distinguishes it from BraVe and LSTCL. While these methods help the video model to extrapolate the contextual information contained in the longer clip from the short clip, their learning objective does not explicitly encourage it to understand how the contextual information may change over different durations. This may limit the video model's ability to understand the relations between short actions that occur within a few frames and long-term actions that may span several seconds or more. In contrast, by learning to predict future contextual information over varying temporal spans, \modelabb may enable the trained video model to gain a deeper understanding of actions at different levels of abstraction and recognize complex actions by identifying their sub-actions.

%\noindent\textbf{Multimodal video pretraining.} Given the demonstrated benefits of multimodal learning, a significant number of recent work \cite{miech2020end, akbari2021vatt} has focused on leveraging the complementary supervision from multiple modalities such as video, language and audio to learn more robust representations that are transferable to downstream multimodal tasks including text-to-video retrieval \cite{miech2020end, fu2021violet, bain2021frozen}, spatiotemporal localization based on natural language queries \cite{tan2021look} and action recognition \cite{miech2020end, akbari2021vatt}.

\noindent\textbf{Action forecasting.} State-of-the-art approaches \cite{damen2020rescaling,girdhar2021anticipative} are often aimed at addressing the short-term problem formulation where the goal is to anticipate the action that will occur in the next $\tau_a$ seconds using the context of an observed video sequence of $\tau_o$ seconds. Prior approaches have proposed to address this task by leveraging free additional information in the query videos either by aggregating past temporal context \cite{furnari2020rolling, sener2020temporal} or predicting representations of future frames and video clips \cite{vondrick2016anticipating, wu2020learning}. Gong \etal \cite{gong2022future} also leverage fully-attentional models to compute a more effective understanding of long-term temporal dynamics in the partially observed videos to generate more accurate predictions in the more challenging task of long-term forecasting \cite{damen2020rescaling,farha2020long,girdhar2021anticipative,grauman2022ego4d,sener2020temporal}. However, these strongly-supervised approaches often leverage pretrained visual representations that do not encode the multiscale nature of actions in videos, which limits their effectiveness. As such, \modelabb is orthogonal to these methods since we aim to learn more efficient base representations for downstream long-term forecasting tasks. We leave it to future work to integrate multiscale representations into state-of-the-art forecasting approaches.