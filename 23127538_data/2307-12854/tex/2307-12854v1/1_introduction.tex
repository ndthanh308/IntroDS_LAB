\section{Introduction}
% Figure environment removed

Long-term forecasting of human activities (illustrated in Figure~\ref{fig:forecasting_motiv}) is a key capability that is crucial for developing intelligent and collaborative machines. Machines that reason about future actions given some observations are better able to plan their own behavior accordingly and interact more effectively with other agents in dynamic environments. However, forecasting future actions is inherently challenging. To begin, the model has to understand the current state of the environment under partial observability. More importantly, the non-deterministic nature of the future compounds the difficulty of having to infer the relationships between actions and objects observed over time and also predict how these relationships will evolve in the future. State-of-the-art long-term forecasting methods (\eg, \cite{gong2022future,grauman2022ego4d}) have focused on learning more effective functions for modeling long-term temporal dynamics in videos by leveraging fully attentional models \cite{vaswani2017attention}, but still rely on pretrained visual representations that are learnt using the standard objective developed for action recognition. However, this objective often encourages a video model to only understand short-term dependencies in a short clip instead of capturing long-term interactions and dynamics of the video. This may limit the generalizability of the pretrained visual representations to long-term forecasting tasks. Despite relying on strong training supervision from human-annotated action labels, the above-mentioned approaches still generalize poorly to unseen data \cite{gong2022future}, which lends support to our theory.

%While state-of-the-art short \cite{girdhar2021anticipative, damen2020rescaling, zatsarynna2021multi} and long-term forecasting \cite{grauman2022ego4d,gong2022future} approaches have achieved notable improvements by leveraging fully attentional models \cite{vaswani2017attention}, their effectiveness is still limited by their dependence on pretrained visual representations that are learnt using image object \cite{girdhar2021anticipative} or video action classification  \cite{gong2022future} pretraining objectives.  However, the latter objective often focuses on modeling short-term dependencies in a short clip instead of capturing long-term interactions and dynamics of the video. Consequently, this may curtail the robustness of the learnt visual representations for long-term forecasting tasks. Despite relying on strong training supervision from human-annotated action labels, the achieved performance of the above-mentioned approaches still leaves a lot of room for improvement, which lends support to our hypothesis.

% Figure environment removed

To improve pretraining for long-term forecasting, we first make the observation that videos generally have a \emph{multiscale} nature, where actions can be decomposed into sub-actions that occur at different levels of granularity. Consider Figure~\ref{fig:motiv} that depicts a video of someone preparing a meal. At the highest level of abstraction, the complex action of making an omelette comprises multiple actions, which generally occur at shorter timescales, such as cracking eggs and adding oil. We hypothesize that learning to understand this structure may be crucial for inferring the underlying goals and intentions of the agents involved, thus facilitating more accurate predictions of their subsequent actions. We endeavor to encode the multiscale nature of actions into the learnt video representations in a self-supervised manner during pretraining, which will generalize more effectively to downstream long-term forecasting tasks.

To this end, we introduce a novel \modelname (\modelabb) approach (illustrated in Figure~\ref{fig:motiv}), which encourages a video model to learn to predict contextualized representations of future video clips that have been aggregated over different timescales using information from a partially observed video sequence. MVP draws inspiration from the required capability in long-term forecasting tasks, which necessitates being able to reason about the spatial and temporal structures of observed actions and predict future events that occur over multiple scales and temporal resolutions. During pretraining, \modelabb learns to infer the knowledge from an observed clip sequence that is required to predict the contextual information contained in future clips. %For example, the objective of making an omelette comprises the action of preparing ingredients, which can be further decomposed into the sub-actions of cutting ham and onions.

Given the lack of ground-truth labels in our self-supervised formulation, we generate prediction targets by computing contextualized representations of future video clips. This key aspect of MVP distinguishes it from the state-of-the-art video pretraining objective of maximizing the similarity between representations of different clips sampled from the same video \cite{qian2021spatiotemporal,feichtenhofer2021large} (Figure~\ref{fig:motiv} top). Feichtenhofer \etal \cite{feichtenhofer2021large} demonstrate that the latter objective encourages different clips of the same video to have similar representations over the spatiotemporal dimensions. While learning clip-invariant video representations may be beneficial to the task of short-term action recognition, they do not encode the high-level semantic structure of the observed video. In contrast, the MVP learning objective trains the video model to extrapolate future information at multiple scales from an observed video sequence. By recognizing the relations between different actions in long videos at different levels of granularity, the video model can better understand the underlying structure of videos and make more accurate predictions about what will happen next.

 %However, while learning video representations that are persistent to varying degrees of visual invariance (\eg actions and objects) is beneficial to the task of short-term action recognition, they do not encode the high-level semantic structure of the observed video, such as the identities of objects and their relationships to each other, which are crucial for long-term forecasting. In contrast, the MVP learning objective trains the video model to extrapolate future information at multiple scales from an observed video sequence, which necessitates being able to reason about the temporal and spatial structure of actions and events that occur at multiple scales and temporal resolutions. By recognizing the relations between different atomic actions in long videos at different levels of granularity, the video model can better understand the underlying structure of videos and make more accurate predictions about what will happen next.

We evaluate the effectiveness of MVP by transferring its pretrained representations to downstream long-term forecasting tasks including order-agnostic and specific action forecasting (Figure~\ref{fig:forecasting_motiv}). Furthermore, we also introduce the novel multimodal task of video summary forecasting, where the goal is to retrieve the corresponding textual summary of the observed and future activities from a set of distractors. MVP significantly outperforms state-of-the-art video pretraining approaches across the Ego4D and Epic-Kitchens-55/100 datasets. More importantly, we extract key insights on the contributions of different aspects of \modelabb through an extensive ablation study that we hope will be beneficial to future work on learning multiscale video representations.

%We summarize our contributions in this paper as follows:
%\begin{enumerate}
%   \item We introduce the self-supervised \modelname pretraining approach, which facilitates encoding future contextual information over multiple timescales in video representations.
%   \item We propose the novel evaluation task of video summary forecasting to evaluate the effectiveness of the learnt video representations to generalize to corresponding elements of the language modality.
%   \item We conduct extensive evaluations on different downstream long-term forecasting tasks, where we demonstrate the effectiveness of MVP across the Ego4D and EpicKitchens-55/100 datasets over state-of-the-art video pretraining approaches. More importantly, we conduct an in-depth analysis of MVP to extract key insights for learning \emph{multiscale} video representations.
   %\item We also demonstrate the benefits of using a curriculum learning strategy during pretraining on downstream tasks, where we gradually increase the number of prediction steps as well as temporal scales.
%\end{enumerate}

%\section{Introduction}
%% Figure environment removed

%Given a sequence of observations, the goal of long-term action forecasting is to predict the subsequent sequence of actions that the human agent is likely to perform over a specified duration. This is an inherently challenging task due to complex world dynamics (including human behavior), partial observability, and irreducible uncertainty. While state-of-the-art short \cite{girdhar2021anticipative, damen2020rescaling, zatsarynna2021multi} and long-term forecasting \cite{grauman2022ego4d,gong2022future} approaches have made significant gains recently by leveraging fully attentional models \cite{vaswani2017attention}, their current performance still leaves a lot of room for improvement despite relying on strong training supervision from human-annotated action labels. Inspired by recent self-supervised learning approaches which have demonstrated the benefits of pretraining without labels on downstream tasks such as action classification \cite{feichtenhofer2019slowfast,feichtenhofer2021large} and early action anticipation \cite{ryoo2011human,hoai2014max}, we investigate the effectiveness of these state-of-the-art approaches at learning robust video representations that generalize well to tasks that require long-term forecasting. However, as we will demonstrate empirically later, simply using the aforementioned approaches naively does not help a trained video model to transfer well to long-term reasoning tasks.

%Consider Figure~\ref{fig:motiv} (top) where the learning objective of state-of-the-art video pretraining approaches is to maximize the similarity between representations of clips that are randomly sampled from the same video. This is largely based on the popular self-supervised image representation learning paradigm of maximizing the similarity between representations of two randomly transformed versions of an image. Feichtenhofer \etal \cite{feichtenhofer2021large} demonstrate that the learning objective of maximizing the semantic similarity between different temporal clips from the same video results in feature persistency over the spatiotemporal dimensions. However, while learning a high-level representation that is persistent to varying degrees of visual invariance (\eg actions and objects) is beneficial to the task of short-term action recognition, it reduces the ability of the representation to predict fine-grained categories over a longer temporal horizon. To alleviate this limitation, we introduce a novel \modelname (\modelabb) pretraining objective (Figure~\ref{fig:motiv} bottom), which exploits future clips in unlabeled videos and their temporal structure for free training supervision. A key difference that distinguishes \modelabb and prior self-supervised approaches is its \emph{multi-scale} aspect. 

%Instead of enforcing representation persistency across space and time, \modelabb encourages a video model to learn to predict future spatiotemporal region representations that have been \emph{contextualized} over multiple temporal horizons given an observed video sequence. This is inspired by our observation that a composite goal often consists of multiple atomic actions. For example, the goal of preparing a meal shown in Figure~\ref{fig:motiv} can be decomposed into smaller actions of washing cooking utensils and preparing ingredients. To succeed in predicting future contextual information at different time steps, a video model has to learn to understand the structure of atomic actions and how they relate to each other. We demonstrate that our proposed \modelabb approach helps a video model to learn robust representations that generalize well to long-term reasoning tasks.

%We conduct extensive evaluations on video-only tasks including order-specific and order-unaware action forecasting as well as the multimodal task of video summary predictions to validate the effectiveness of our proposed \modelname approach.
%To evaluate the capability of the learnt video representations to generalize to corresponding elements of another modality, we introduce a novel evaluation task of video summary prediction, where the goal is to partially observe a sequence of clips in a video and retrieve its corresponding textual summary from a set of distractors. We demonstrate that our completely self-supervised pretraining approach facilitates our fully-attentional video model to perform better at long-term reasoning tasks as compared to state-of-the-art self-supervised video pretraining approaches. 

%We summarize our contributions in this paper as follows:
%\begin{enumerate}
%   \item We introduce the fully self-supervised \modelname (\modelabb) pretraining method, which facilitates learning to encode future contextual information at varying temporal scales into video clip representations.
%  \item We propose a simple yet effective sequential video model architecture that leverages state-of-the-art fully attentional video encoders to enable long-term reasoning beyond a single video clip.
%   \item We conduct extensive evaluations of our proposed \modelabb approach on downstream tasks which require the capability of long-term reasoning. We demonstrate the effectiveness of our pretraining approach on video-only tasks including order-specific and unaware long-term action anticipation over different temporal horizons as well as the multimodal tasks of video summary prediction, where it outperforms state-of-the-art approaches on all tasks by a significant margin across the Ego4D and EpicKitchens-55/100 datasets.
%   \item We demonstrate that using a curriculum learning strategy is also beneficial for our proposed \modelabb pretraining approach, where gradually increasing the number of prediction steps as well as the temporal scale helps our proposed video model to learn more robust video representations that generalize well to downstream tasks and target datasets.
%\end{enumerate}
