\section{\modelname}
%% Figure environment removed

Our goal is to learn robust video representations that generalize well to downstream long-term forecasting tasks from a set of unlabeled videos. To this end, we introduce a self-supervised \modelname (\modelabb) objective, that aims to enable a video model to generate more accurate fine-grained action predictions of the forthcoming video clips given context from a partially observed clip sequence. Our approach is motivated by the reasoning that long-term forecasting requires the key capability of predicting the occurrences of future events at multiple timescales (\eg near and distant future). Similarly, \modelabb requires a video model to infer the initial context of the video from an observed clip sequence and leverage the context to condition its predictions of information that is contained in future clips. Due to a lack of explicit annotations during pretraining, we propose to exploit the \emph{multiscale} nature of complex actions in long videos for pseudo supervision. For example, the complex action of making an omelette can be decomposed into shorter atomic actions including cutting the onions and cracking the eggs. More specifically, \modelabb trains the video model to predict fine-grained spatiotemporal representations of the future that have been contextualized by aggregating information over varying numbers of future clips. We hypothesize that this objective encourages a video model to learn representations that encode future contextual information over multiple temporal spans. 

Unlike state-of-the-art video pretraining approaches \cite{feichtenhofer2021large,oord2018representation,qian2021spatiotemporal, wang2022long} which generally encourage different clips of the same video to have similar representations, MVP trains a video model to effectively represent the spatial and temporal structure of the observed video to extrapolate long-term information about future short and long actions. Intuitively, understanding the hierarchy of actions enables the video model to better reason about and recognize complex actions by identifying their sub-actions. Such an understanding may help the model to compute a more accurate prior distribution to condition its predictions on.

\subsection{Temporal aggregation of video clip sequences} \label{sec:video_model}
 While state-of-the-art video pretraining methods \cite{feichtenhofer2021large,qian2021spatiotemporal} often utilize pairs of video clips from the same video, our \modelabb objective trains a video model with pairs of video clip sequences $V^O$ and $V^F$ instead. \modelabb requires the video model to observe $V^O$ and infer the knowledge required to predict future contextual information that have been aggregated over the clips in $V^F$ at multiple timescales. To begin, we partition an input video into non-overlapping clips of 8 frames each (about 0.8s) and randomly sample the observed as well as future clip sequences $V^O = \{V_1^O, \cdot \cdot \cdot, V_{N_O}^O\}$ and $V^F = \{V_{N_O + K}^O, \cdot \cdot \cdot, V_{N_O + K + N_F}^F\}$, where $N_{O}$, $N_F$, and $K$ denote the number of observed, future, and temporal offset clips, respectively. We also define the temporal stride $S$ as the difference in number of clips between two timescales. Thus, \modelabb makes $N_P$ predictions, where $N_P = \frac{N_F}{S}$.

 Our video model (Figure~\ref{fig:model_fig}) is comprised of a video clip encoding function $g_\theta$ as well as temporal context aggregation functions $h_{\phi}$ and $h_{\mu}$. $g_\theta$ is used to encode an input clip into a set of spatiotemporal region representations while $h_{\phi}$ and $h_{\mu}$ are used to aggregate the temporal context of the observed and future clip sequences, respectively, by combining information over their constituent clip representations.

%During the pretraining stage, we partition an input video into non-overlapping clips of 8 frames each. We randomly sample a sequence of observed video clips $V^O = \{V_1^O, \cdot \cdot \cdot, V_{N_O}^O\}$ and a future sequence $V^F = \{V_{N_O + K}^O, \cdot \cdot \cdot, V_{N_O + K + N_F}^F\}$, where $N_{O}$, $N_F$, and $K$ denote the number of observed, future, and temporal offset clips, respectively. While state-of-the-art video pretraining methods \cite{feichtenhofer2021large,qian2021spatiotemporal} often utilize pairs of video clips from the same video, we make a simple modification by extending short input clips to the longer video clip sequences $V^O$ and $V^F$.  Our video model (Figure~\ref{fig:model_fig}) comprises a base video clip encoding function $g_\theta$, that is used to encode an input clip into a set of spatiotemporal region representations, as well as temporal context aggregation functions $h_{\phi}$ and $h_{\mu}$, which are used to aggregate the temporal context of the observed and future clip sequences, respectively, by combining information over their constituent clip representations.

Due to the computationally demanding nature of our \modelabb objective, we adopt the lightweight yet powerful Multiscale Vision Transformers (MViT) \cite{fan2021multiscale} as our base encoder $g_\theta$ without modifications, which has been shown to outperform prior video encoders in action recognition despite containing significantly fewer parameters. We encode the $i$-th video clip as: $f_i = g_\theta(V_i), f_i \in \mathbb{R}^{L \times H \times W \times D}$ where $L$, $H$, $W$ and $D$ denote the temporal, height, width and channel dimensions, respectively. Then, we compute contextualized representations for both input sequences by aggregating information over the clips:
\begin{equation}
 z^O = z^O_{N_O} = h_{\phi}(g_{\theta}(V^O)), \enspace z^F = z^F_{N_F} = h_{\mu}(g_{\theta}(V^F)), 
\end{equation}
where $z^O$ and $z^F$ are the contextualized representations for the observed and future sequences, respectively.
%\begin{equation}
%\begin{aligned}
%    z^O &= z^O_{N_O} = h_{\phi}(g_{\theta}(V^O)), \\
%    z^F &= z^F_{N_F} = h_{\mu}(g_{\theta}(V^F)),
%\end{aligned}
%\end{equation}

% Figure environment removed 

\subsection{Spatiotemporal multi-head self-attention}
We argue that learning to predict fine-grained region representations over the spatial and temporal dimensions may be beneficial to understanding interactions between objects and actions in videos, unlike prior work focused on predicting global clip representations \cite{oord2018representation, qian2021spatiotemporal, wang2022long}. To this end, we train our model to predict spatiotemporal region representations of future clip sequences that have been contextualized over multiple timescales. This requires our temporal aggregation function to be able to compute contextual information between different spatial regions across multiple time steps. Intuitively, this objective can only be achieved with a strong understanding of the movement of objects over time and their relevance to different actions. 

A widely adopted convention for learning this function is to use multi-head self-attention (MHA) \cite{bertasius2021space} over the entire set of spatiotemporal regions in the video clip sequence. However, since self-attention has quadratic complexity, the computational requirements increase rapidly even for short sequences of video clips. To address this, we only aggregate temporal context information between video clips by computing self-attention over all regions at the same spatial locations in the video clip sequence. This is motivated by our observation that the output region representations of MViT for each time step have already been contextualized by aggregating information over other spatial regions, since the self-attention operation is an implicit function composited in the final video clip encoding function learnt by the MViT model. We refer interested readers to \cite{fan2021multiscale} for more details on the MViT architecture.

To begin, given an input spatiotemporal block $S \in \mathbb{R}^{L \times H \times W \times D}$, we project the set of temporal region features for the $j$-th spatial location $S_j \in \mathbb{R}^{L \times D}$, where $j \in hw$,  into its queries, keys and values:
\vspace{-5pt}
\begin{equation}
 S_{j,q} = S_j W_q, 
   \quad S_{j,k} = S_j W_k, 
\quad S_{j,v} = S_j W_v,
\vspace{-10pt}
\end{equation}
where $W_q$, $W_k$ and $W_v$ are the query, key and value projection weights of dimensions $D \text{x} D$. Then, we compute contextualized representations for the sequence using the MHA operation as follows:
\vspace{-10pt}
\begin{equation}
\hspace{-5pt}
    \text{MHA}(S_{j,q}, S_{j,k}, S_{j,v}w) = S_{j,v} \text{  Softmax}\left(\frac{S_{j,q} ^T S_{j,k}}{\sqrt{D}}\right)
    \vspace{-10pt}
\end{equation}
For a given spatiotemporal region representation $z_{i,t,h,w}$ from the i-th video clip, we compute its contextualized representations as: $z_{i,t,h,w}^{'} = \text{MHA}(z_{i,t,h,w})$. Finally, we predict the $j$-th future region representation at the $k$-th time step with a temporal stride of $S$ by passing the contextualized spatiotemporal region representations through a two-layer multilayer perceptron (MLP), \ie, $\hat{z}_{i,t,h,w} = \text{MLP}_k(z_{i,t,h,w}^{'})$. The entire set of predicted region representations $\hat{z}$ is used in Section~\ref{sec:multiscale_loss} to compute the training loss. Note that we use a different prediction head for each predicted timestep.
%\begin{equation}
%   z_{i,t,h,w}^{'} = \text{MHA}(z_{i,t,h,w}).
%\end{equation} 

%\begin{equation}
%    \hat{z}_{i,t,h,w}^{'} = \text{MLP}(z_{i,t,h,w}^{'}).
%\end{equation}

\begin{table*}[t]
{
\setlength{\tabcolsep}{3pt}
\begin{center}
\begin{tabular}{|c|c|c|ccc|ccc|ccc|}
\hline
Pretraining & Multiple & Pretraining  &  & Ego4D $\uparrow$ &  &  & EK55 $\uparrow$ & &  & EK100 $\uparrow$ & \\
\cline{4-12}
approach & clips used & supervision & Verb & Noun & Mean & Verb & Noun & Mean & Verb & Noun & Mean\\
\hline
\rowcolor{lightgray} Action recognition & No & Strong & 20.70 & 14.41 & 17.56 & 18.11 & 11.48 & 14.80 & 18.82 & 12.46 & 15.64\\
CVRL \cite{qian2021spatiotemporal} & No & Self & 25.90 & 25.85 & 25.88 & 22.17 & 17.07 & 19.62 & 22.92 & 16.60 & 19.76\\
CPC \cite{oord2018representation} & Yes & Self & 27.26 & 26.57 & 26.91 & 23.00 & 17.24 & 20.13 & 23.16 & 17.06 & 20.11\\
LSTCL \cite{wang2022long} & Yes & Self & 26.82 & 27.76 & 27.29 & 23.59 & 18.52 & 21.05 & 23.47 & 17.15 & 20.31\\
DPC \cite{han2019video} & Yes & Self & 28.18 & 29.03 & 28.61 & 24.02 & 19.03 & 21.52 & 25.25 & 18.18 & 21.72\\
CVRL \cite{qian2021spatiotemporal} & Yes & Self & 28.27 & 29.74 & 29.00 & 23.91 & 18.32 & 21.12 & 24.94 & 19.24 & 22.09\\
CONSTCL \cite{yuan2022contextualized} & Yes & Self & 27.49 & 29.13 & 28.31 & 24.47 & 19.52 & 22.00 & 25.41 & 19.35 & 22.38\\
\modelabb (Ours) & Yes & Self & \textbf{30.18} & \textbf{32.33} & \textbf{31.25} & \textbf{25.83} & \textbf{20.78} & \textbf{23.31} & \textbf{26.69} & \textbf{20.18} & \textbf{23.44}\\
\hline 
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Order-agnostic long-term forecasting.} We report the mean average precision over all verb and noun classes. We see that self-supervised pretraining is generally more beneficial for long-term forecasting tasks than action recognition.}
\vspace{-10pt}
\label{tab:order_unaware_forecasting_results}
}
\end{table*}

\subsection{Multiscale targets and loss formulation}\label{sec:multiscale_loss}
To compute the prediction targets for self-supervision, we apply the aggregation function $h_\mu$ to $V_F$ in a causal manner, \ie the set of contextualized spatial region representations $S_{t,j}$ for the $j$-th spatial region at the $l$-th time step is computed by attending only to the regions that precede it temporally. For the $b$-th sequence of future video clips in a sampled batch, we extract a set of target representations $Z_b = \{z_{b,k}\}$, where $k \enspace \% \enspace S = 0$ and $Z_b \in \mathbb{R}^{N_P \times LHW \times D}$. Given a batch of unlabeled videos, we train the video model end-to-end using a contrastive loss \cite{oord2018representation} formulation as:
\begin{equation} 
A = \sum^B_{b=1} \sum^{N_P}_{j=1} \sum^{LHW}_{n=1} -\log\frac{\exp(\hat{z}_{b,j,n} \cdot z_{b,j,n} / \tau)}{%
\splitfrac{\textstyle
            \exp(\hat{z}_{b,j,n} \cdot z_{b,j,n} / \tau) +} 
          {\hspace*{-25pt}\textstyle 
           \sum\limits_{(b',j', n') != (b,j,n)}\exp(\hat{z}_{b,j,n} \cdot z_{b', j', n'} / \tau)}},
\end{equation}
where $\tau$ denotes the temperature value.