%%%%%%%%% TITLE
%\section{Appendix}

In this supplemental, we provide the following additional material to the main submission:
\begin{enumerate}
    \item[A.] Training and evaluation datasets details
    \item[B.] Implementation details
    \item[C.] Spatiotemporal constrastive loss formulation
    \item[D.] Baseline models for comparisons
\end{enumerate}

\section{Datasets}

\noindent\textbf{Ego4D}~\cite{grauman2022ego4d} is the largest dataset of egocentric videos spanning over 3600 hours of daily life activities ranging from household to outdoor leisure scenarios. These videos are collected by 931 camera-wearers from 9 different countries, who record their unscripted interactions as they engage in daily activities under a large variety of settings. In contrast to existing video recognition datasets, videos in Ego4D are generally much longer in duration since they span from 1 to 10 hours as compared to 10 seconds video clips in Kinetics 400/600 \cite{carreira2018short,carreira2019short}. Additionally, it is much larger in scale and diversity of activities than existing egocentric video datasets such as Epic-Kitchens 55/100 \cite{Damen2018EPICKITCHENS, damen2020rescaling}. Each video is also densely annotated by humans, who provide annotations describing notable interactions in the videos as well as high-level summaries. This dataset facilitates the exploration and further research in a variety of downstream tasks such as audio-visual diarization and forecasting. We use the provided annotations to evaluate our proposed MTPL approach on long-term forecasting as well as video summary predictions. We adopt the same splits for training and evaluation on the target tasks as Grauman \etal \cite{grauman2022ego4d}. In this dataset, we conduct our evaluations on the training and validation splits since the test evaluation is conducted on a held-out set via a submission to their challenge portal. We also note that the number of verb and noun classes present in all 3 provided splits are not consistent since each split contains some verb and noun classes that are not present in other splits. Please refer to the supplementary material for more details.
\smallskip

\noindent\textbf{EpicsKitchen-55/100.} EpicKitchens-100 (EK100) \cite{damen2020rescaling} is another large dataset of egocentric videos. Similar to Ego4D, it also provides 700 long unscripted egocentric videos that span approximately 100 hours. It is less diverse than Ego4D since the participants only engage in daily activities in the kitchen. EpicKitchens-55 (EK55) \cite{Damen2018EPICKITCHENS} is an earlier and smaller version of EK100 but it provides the same types of videos and annotations. We use EK55 and EK100 to evaluate on the tasks of order-agnostic and order-specific long-term forecasting. 

% Figure environment removed

\section{Implementation details}

\subsection{\modelname}
We implement all models and experiments using the Pytorch deep learning library. We use the Multiscale Vision Transformer (MViT) \cite{fan2021multiscale} as our base video encoder and 1 transformer encoder layers with 1 attention heads as our temporal context aggregator. The MVIT encoder typically accepts a video clip of 16 frames as input and outputs a global clip representation, which is the contextualized output of the classification token. However, in our case, we reduce the number of frames per clip to 8 due to memory constraints. Additionally, we discard the classification token during pretraining and perform our future feature predictions at the spatiotemporal region granularity. During the second stage of finetuning, we compute a global clip representation by performing meanpooling over the spatiotemporal region representations. 

Since we sample the video frames at 10 frames per second (FPS), the temporal duration of each clip is approximately 0.8 seconds long. Each input video clip is preprocessed by randomly scaling the height of the frames between 248 and 280 pixels and taking crops of 224 x 224 pixels. During the first stage of pretraining on the Ego4D dataset, we also perform random augmentations to the video clips including random horizontal flipping and color jittering. The future feature prediction function is represented as a two-layer multilayer perceptron (MLP) with a non-linear ReLU operation and hidden dimension of 768.

\subsection{Downstream long-term forecasting tasks}
Figure~\ref{fig:downstream_tasks} illustrates how our pretrained video model and its learnt representations are transferred to the order-agnostic and order-specific action forecasting as well as video summary forecasting. To begin, given the sequence of $N_V$ observed video clips in each task $V = \{V_1, \cdot \cdot \cdot V_{N_V}\}$, we extract the contextualized representation of the last timestep as follows:
\begin{equation}
    z_{N_V} = h_\phi(g_\theta(Vz)), \quad z_{N_V} \in \mathbb{R}^D
\end{equation}
where $D$ is the output channel dimension. For all downstream tasks, we finetune linear probes on top of the pretrained video model, which is kept frozen.

\noindent\textbf{Order-agnostic action forecasting.} Given a vocabulary of $N_{\text{verb}}$ and $N_{\text{noun}}$ classes, we predict a $N_{\text{verb}}$-dimensional and $N_{\text{noun}}$-dimensional binary vectors as:
%\begin{equation}
%    p_{\text{verb}} = f_{\text{verb}}(z_{N_V})
%\end{equation}

\begin{equation}
\begin{aligned}
    p_{\text{verb}} = f_{\text{verb}}(z_{N_V}), \\
    p_{\text{noun}} = f_{\text{noun}}(z_{N_V}),
\end{aligned}
\end{equation}
where each dimension in the predicted vectors indicates the probability of the verb or noun class occurring in the future. We formulate this as a multi-label prediction task and finetune all pretrained models by optimizing the binary cross-entropy loss computed over all verb and noun classes as:
\begin{equation}
    L = -\sum^B_{b=1}(\sum^{N_{\text{verb}}}_{i=1} y_{\text{verb},b,i} \log(p_{\text{verb},b,i}) + \sum^{N_{\text{noun}}}_{i=1} y_{\text{noun},b,i} \log(p_{\text{noun},b,i})),
\end{equation} where $y_{\text{verb},b,i}$ and $y_{\text{noun},b,i}$ are the ground-truth verb and noun binary labels, respectively.

\noindent\textbf{Order-specific action forecasting.} In this more challenging setting, the goal is to make fine-grained action predictions at specific timesteps. For simplicity, we adopt the same training and evaluation setup as in \cite{grauman2022ego4d} and use separate prediction heads for different timesteps. For each timestep, we formulate the subtask as a multiclass prediction problem for both verbs and nouns. Consequently, we finetune the pretrained video models using the following loss formulation:
\begin{equation}
    L = -\sum^B_{b=1} \sum^{N_P}_{t=1} (y_{\text{verb},b,t} \log(p_{\text{verb},b,t}) + 
    (y_{\text{noun},b,t} \log(p_{\text{noun},b,t})
    ).
\end{equation}

\noindent\textbf{Video summary forecasting.} As shown in Figure~\ref{fig:downstream_tasks} (right), we adopt the dual encoder architecture to address this multimodal task. Similar to prior work on learning joint visual-language representations including CLIP and ALIGN, we also use the late fusion mechanism where the semantic similarity between the final video and language representations are computed using a final dot product operation. 

%% Figure environment removed

% Figure environment removed

\section{Spatiotemporal constrastive loss formulation}
We provide an illustration of how our proposed \modelabb objective trains a video model to predict fine-grained spatiotemporal region representations using the contrastive loss formulation in Figure~\ref{fig:spatiotemporal_region_preds}. Given the predicted representation of the $j$-th spatial region at the $t$-th timestep $\hat{z}_{t,j}$, we aim to maximize its semantic similarity with its ground-truth aggregated representation $z_{t,j}$ and the negative samples in the entire set of distractors consist of both hard negatives such as other spatial regions at the same timestep and easy negatives including representations that belong to clips from other videos in the sampled batch.
 
\section{Baseline models} We briefly describe the self-supervised video pretraining baselines that we compare our proposed \modelabb objective against in our evaluations.

\noindent\textbf{Contrastive predictive coding (CPC).} The Contrastive Predictive Coding (CPC) \cite{oord2018representation} approach aims to learn video representations that encode global information that is shared between different clips of a video. CPC uses the context from an observed clip sequence to predict the future \emph{uncontextualized} information in the future clips that directly follow after the observed sequence. It also uses multiple prediction heads for representations of different timesteps that it tries to predict for.

\noindent\textbf{Dense predictive coding (DPC).} The Dense Predictive Coding (DPC) \cite{han2019video} approach builds on top of CPC to learn video representations of predicting \emph{uncontextualized} information but conditions its predictions for a given timestep with the context of the predicted information at the preceding timestep. Additionally, unlike CPC, the DPC objective aims to compute spatiotemporal representations instead of global clip representations.

\noindent\textbf{Contrastive video representation learning (CVRL).} We also compare \modelabb to the Contrastive Video Representation Learning (CVRL)  \cite{qian2021spatiotemporal} approach, which is largely inspired by popular image-based self-supervised pretraining objectives \cite{chen2020simple,chen2020improved,chen2021exploring}. CVRL trains a video model to maximize the similarity between representations of different clips that are randomly sampled from the same videos. While we compare to CVRL in its vanilla setting which uses pairs of video clips, we also train and evaluate a variant of CVRL which maximizes the similarity between representations of pairs of clip sequences.

\noindent\textbf{Long-Short Term Contrastive Learning (LSTCL).} Similar to the CVRL approach, the Long-Short Term Contrastive Learning (LSTCL) \cite{wang2022long} is initially proposed to learn video representations by maximizing the similarity between representations of video clip pairs. During pretraining, it accepts as input a short clip and another long clip which contains temporal information that is not present in the former. LSTCL trains a video model to extrapolate past and future information from a small observed temporal window. We also extend LSTCL to train on pairs of video clip sequences with the same total number of video clips per sample during pretraining to facilitate fair comparisons.

\noindent\textbf{Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision (CONSTCL).} Last but not least, we also compare to the Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision (CONSTCL) \cite{yuan2022contextualized} approach. CONSTCL aims to address the limitation of spatiotemporal invariance \cite{feichtenhofer2021large} enforced by the CVRL objective. The CONSTCL objective leverages a region-based preraining task which trains the video model to transform video representations from one clip sequence to another, given the context from the first sequence.