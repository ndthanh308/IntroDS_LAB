\section{Conclusion}
In summary, we introduce \modelname, a self-supervised approach that aims to learn robust video representations for downstream long-term forecasting tasks. Given an observed video clip sequence, we train a video model to predict aggregated representations of future clips over multiple timescales. We demonstrate empirically that learning to encode future contextual information helps the video model to generalize better to long-term forecasting tasks than prior work, which highlights the importance of multiscale pretraining to long-term video understanding. Last but not least, we extract key insights on different aspects of \modelabb, through an extensive ablation study, that we hope will be beneficial to further research on learning multiscale video representations. Some interesting avenues for future work may include further exploring the capabilities of these representations for other video and multimodal tasks such as action recognition and text-to-video retrieval.

\noindent 
\textbf{Acknowledgements}: This material is based upon work supported, in part, by DARPA under agreement number HR00112020054. We would like to thank Gideon Stocek and Nishanth Alapati for their assistance with setting up the compute infrastructure for the experiments.