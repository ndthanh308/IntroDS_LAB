
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{caption}

\usepackage{multicol}
\usepackage{array}
\usepackage{booktabs}

% \usepackage[dvipsnames]{xcolor}
% \usepackage{xcolor}         % colors
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\usepackage{colortbl}
\definecolor{cyan}{cmyk}{.3,0,0,0}

\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

    
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

% \newcommand{\R}{\mathbb{R}}
\newcommand{\algo}{MC-JEPA}
\newcommand{\algospace}{MC-JEPA }

\newcommand{\parspace}{\vspace{0mm}}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).

% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=RoyalBlue]{hyperref}
% \hypersetup{
%     colorlinks=true,
%     citecolor=RoyalBlue,
%     linkcolor=red,
%     filecolor=magenta,      
%     urlcolor=cyan,
%     pdftitle={Overleaf Example},
%     pdfpagemode=FullScreen,
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\title{\algo: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
    Adrien Bardes$^{1,2}$ \And Jean Ponce$^{2,3,4}$ \And Yann LeCun$^{1,3,4}$ \AND
    \begin{tabular}{l}
        $^1$\normalfont Meta AI, FAIR\\
        $^2$\normalfont Inria, École normale supérieure, CNRS, PSL Research University \\
        $^3$\normalfont Courant Institute, New York University\\
        $^4$\normalfont Center for Data Science, New York University\\
    \end{tabular}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% !!! See sty L89 !!!
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
% !!! See sty L89 !!!

\begin{document}


\maketitle

\begin{abstract}
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce \algo, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}

Self-supervised learning in vision has been dominated lately by approaches that aim at learning content features; i.e. features containing information that allows to identify and differentiate objects; in images~\citep{chen2020simclr, grill2020byol, chen2020simsiam, zbontar2021barlow, bardes2022vicreg, caron2020swav, caron2021dino, zhou2022ibot, assran2022msn, assran2023ijepa}, or videos~\citep{qian2021cvrl, recasens2021brave, feichtenhofer2021rbyol, tong2022videomae}. Most methods focus on learning global features that achieve strong results in tasks such as object classification or action recognition in videos. A more recent trend aims at learning localized features, that perform well on local tasks such as detection and segmentation~\citep{xiao2021resim, wang2021densecl, henaff2021detcon, henaff2022odin, bardes2022vicregl}. However, these methods focus on understanding the content of images and videos and are not able to learn information at the pixel level, such as motion in videos or details in textures. In this paper, we focus on jointly learning motion features by using self-supervised optical flow estimation~\citep{horn1981flow} from videos as a pretext task, and content features with general self-supervised learning.

The Optical flow captures the motion, or dense-pixel correspondence, that occurs between two images, for instance consecutive frames in a video, or images from a stereo pair. Estimating it is a fundamental problem in computer vision, whose solution is key to tasks such as visual odometry, depth estimation, or object tracking. Classical approaches cast optical flow estimation as an optimization problem~\citep{horn1981flow, brox2004warping}, where the objective is to match pixels with a smoothness constraint. Approaches based on neural networks and supervised learning~\citep{yu2016flownet, ilg2017flownet2, hui2018liteflownet, sun2018pwcnet, yang2019vcn, zhao2020mashflownet, teed2020raft, jiang2021gma, bai2022dqflow}, are limited by the difficulty of labelling data in the real world, compared to using synthetic data. Self-supervised methods allow learning from large collections of real-world video data~\citep{ren2017dstflow, liu2019selflow, lui2019ddflow, zhong2019epiflow, im2020unsupsimflow, liu2020arflow, luo2021upflow, jonschkowski2020uflow, stone2021smurf} and offer an alternative that is now competitive with supervised approaches. However, most current methods only focus on motion without relying on the (semantic) content of the video, a problem that we solve by learning motion and content features in images at the same time with a multi-task approach.

Recent techniques learn spatial correspondences between video frames~\citep{jabri2020crw, bian2022mcrw, xu2021vfs, tokmakov2022ram}. The goal is to track the location of objects and therefore capture content information that optical flow estimation does not. These approaches can be seen as object-level motion estimation. They learn features that are very specific to the tracking task, with very poor generalization to other visual downstream tasks. Very often, they are trained on small video datasets that are not as diverse as large image datasets such as ImageNet~\citep{deng2009imagenet}, which reinforces the poor quality of the visual features learned. A more reliable way to build visual representations is to learn multiple tasks at the same time~\citep{zhang2021cover, girdhar2022omnimae}. We thus propose \algospace (Motion-Content Joint-Embedding Predictive Architecture), a method that learns optical flow estimation and content features, in a multi-task setting with a shared encoder, with a joint-embedding-predictive architecture~\citep{lecun2022jepa}. Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a method for learning self-supervised optical flow from synthetic and real video data, based on PWC-Net~\citep{sun2018pwcnet}, and improved with several additional components such as a backward consistency loss and a variance-covariance regularization term. We call this first method M-JEPA.
    \item We combine M-JEPA in a multi-task setup with VICReg~\citep{bardes2022vicreg}, a self-supervised learning method trained on ImageNet, in order to improve our estimated flow, and produce content features that transfer well on many downstream tasks. Our final method is called \algo.
    \item We evaluated \algospace on a range of optical flow benchmarks such as KITTI 2015~\citep{menze2015kitti15} and Sintel~\citep{butler2012sintel}, image and video segmentation tasks on Cityscapes~\citep{cordts2016cityscapes} or DAVIS~\citep{ponttuset2017davis}, and demonstrate strong performance on all these tasks with a single encoder.
\end{itemize}

We hope that \algospace will be a first step towards self-supervised learning approaches that are based on multi-task learning and joint-embedding architectures, and that can be trained on any visual data, images or video, and that generalizes well on a wide range of tasks, from motion prediction tasks to content understanding tasks.

% Figure environment removed

% Figure environment removed

\section{Related Work}

\noindent \textbf{Self-supervised learning.} The recent advances in self-supervised learning have been mainly driven by the general approach of learning invariances to hand-crafted data augmentations, using a joint-embedding architecture~\citep{lecun2022jepa}. Among self-supervised learning methods learning from images, contrastive methods push together concepts that are visually close and push away concepts that are different in the embedding space~\citep{hjelm2019mutual, chen2020simclr, he2020moco, chen2020mocov2, mitrovic2021relic, dwibedi2021nnclr, chen2021mocov3, tomasev2022relicv2, li2022esvit}, clustering methods categorized embeddings into a balanced set of clusters~\citep{caron2018clustering, caron2020swav, caron2021dino}, non-contrastive methods either prevent collapsing solutions with architectural tricks~\citep{grill2020byol, lee2021cbyol, chen2020simsiam}, or with covariance-based regularization~\citep{ermolov2021whitening, zbontar2021barlow, bardes2022vicreg, garrido2023sie}, which is equivalent under some assumptions to contrastive methods~\citep{garrido2022duality}. Finally, some methods are based on masking and patch-reconstruction~\citep{bao2022beit, he2022mae, zhou2022ibot, assran2022msn, assran2023ijepa}. These methods focus on learning a global representation of the input, which is best suited for classification tasks. Dense self-supervised learning rather focuses on learning local features~\citep{xie2021pixpro, wang2021densecl, xiao2021resim, yang2021insloc, wang2022cp2, yang2022inscon, henaff2021detcon, henaff2022odin, chen2022intrainstance, caron2023loca}, which is best suited for detection and segmentation downstream tasks. The loss functions and methods developed with images have led to the application of similar approaches to videos~\citep{qian2021cvrl, recasens2021brave, feichtenhofer2021rbyol, tong2022videomae, parthasarathy2022vito}, with the objective of learning a representation that transfers well on action recognition benchmarks.
\parspace

\noindent \textbf{Optical flow estimation.} Classical techniques for optical flow estimation are based on the optimization of a matching term and a smoothness term for a given pair of images, without any kind of learning~\citep{horn1981flow, brox2004warping, sun2010secrets}. Later, methods based on supervised learning and convolutional neural networks came, first without any prior knowledge in architecture~\citep{yu2016flownet, ilg2017flownet2}, then specifically designed to tackle flow estimation~\citep{ranjan2017spynet, sun2018pwcnet, yang2019vcn, teed2020raft}. Supervised flow estimation is limited to learning with synthetic data, and unsupervised flow estimation is a promising direction towards learning on any video data. Photometric consistency was introduced by~\citep{ren2017dstflow} and is at the basis of every unsupervised optical flow estimation method. Additional self-supervision signals can be found with distillation of reliable matches~\citep{liu2019selflow, lui2019ddflow}, global geometric constraint~\citep{zhong2019epiflow}, or data augmentation consistency~\citep{liu2020arflow, stone2021smurf}. Fusing multi-layer similarities~\citep{im2020unsupsimflow} and carefully designing the interpolation for upsampling~\citep{luo2021upflow} further improve the estimated flow quality. Finally, a comprehensive set of additional tricks that help unsupervised optical flow is presented in~\citep{jonschkowski2020uflow}.
\parspace

\noindent \textbf{Learning correspondences.} Learning from videos has been focusing on learning a global representation for a video, but another interesting task is learning spatial correspondences between consecutive frames. A promising direction for learning these correspondences is contrastive random walks~\citep{jabri2020crw}, which can also be done at the pixel level~\citep{bian2022mcrw}. Correspondences can also be learned at the object level~\citep{xu2021vfs, mandala2021mformer}, or combined with a memory~\citep{tokmakov2022ram}, in order to deal with occluded objects. Learning optical flow can be seen as learning correspondences at the pixel-level, which is not captured by popular self-supervised learning methods.
\parspace

\noindent \textbf{Multi-task Learning.} Multi-task learning is commonly used to train an encoder on multiple tasks, when the different tasks benefit from each other. Several works use it to learn a shared representation between images and videos~\citep{zhang2021cover, girdhar2022omnimae}. However, very few works use multi-task learning for self-supervised learning, the idea was introduced in~\citep{doersch2017multitaskssl} and used for anomaly detection tasks in~\citep{georgescu2021anomaly}, without many follow-up work. We simply use multi-task learning for learning self-supervised content features and optical flow at the same time with a single shared encoder.

\section{Proposed approach}

In this section we describe our architecture and improvements for self-supervised optical flow estimation with a hierarchical coarse-to-fine approach, the loss functions of our method, our self-supervised general objective and multi-task setup, our data sampling strategy, and a set of tricks for stabilizing training. Section~\ref{sec:optical_flow} introduces our M-JEPA method for optical flow estimation, and Section~\ref{sec:multi_task} presents how we combine M-JEPA with multi-task learning into our final \algospace method.

\subsection{Optical flow} \label{sec:optical_flow}

Given a pair of RGB images, $I_{t}$, $I_{t+1} \in \mathbb{R}^{3,H,W}$, the corresponding optical flow is defined by the correspondence map $f \in \mathbb{R}^{2,H,W}$, that for a given position in $I_{t}$, denotes the position of the corresponding pixel in $I_{t+1}$. The goal is to learn a flow estimator function $F_{\theta}$ with parameters $\theta$, which outputs the flow for a pair of images $f = F_{\theta}(I_{t}, I_{t+1})$, by training it on a set of image sequences $D = \{\{I_{t}\}_{t=1}^{T}\}_{i=1}^{N}$. Unsupervised flow estimation usually works with a regression loss, or photometric consistency loss, which ensures that the image $I_{t}$ warped by the predicted flow $f$ is consistent with $I_{t+1}$, and a regularizer that encourages $f$ to be smooth. Most methods differ in the way these terms are implemented, in the details of the encoder and flow estimator architecture, and in additional self-supervisory signals.
\parspace

\noindent\textbf{Regression and smoothness.} We use the coarse-to-fine hierarchical flow estimator PWC-Net~\citep{sun2018pwcnet}, which we adapt to work with our custom encoder architecture described in Appendix~\ref{app:arch}. Given a set of features $X_{t}^{(l)}, X_{t+1}^{(l)} \in \mathbb{R}^{d^{(l)} \times h^{(l)} \times w^{(l)}}$, corresponding to level $l$ of pyramids for images $I_{t}$ and $I_{t+1}$ with $l \in \{1, ..., L\}$, we first estimate a flow $f_{t,t+1}^{(2)} = F_{\theta}(X_{t}^{(1)}, X_{t+1}^{(1)}, 0)$, then recursively refine this flow at higher and higher resolutions by predicting the residual flow at every layer:
\begin{equation}
    f_{t,t+1}^{(l+1)} = F_{\theta}(X_{t}^{(l)}, X_{t+1}^{(l)}, f_{t,t+1}^{(l)}).
\end{equation}
Our estimator $F_{\theta}(X_{t}, X_{t+1}, f)$ works as follows. First the feature $X_{t}$ is warped as $\hat{X}_{t+1} = f(X_{t})$, then a 4D correlation volume $V = \hat{X}_{t+1}X_{t+1}^T$ is calculated and is fed to a small convolutional network $g_{\phi}(V, X_{t}, \hat{X}_{t+1}, f)$ which predicts the residual flow. We then use a multi-scale loss on the intermediate feature layers of the encoder, defined as follows:
\begin{equation} \label{eq:reg_loss}
    \mathcal{L}_{\mathrm{reg}} = \sum_{l=1}^{L} \| X^{(l)}_{t+1} - \hat{X}_{t+1}^{(l)} \|^{2}_{2},
\end{equation}
and a reconstruction loss on the last layer that is at the image level:
\begin{equation} \label{eq:reg_loss}
    \mathcal{L}_\mathrm{{rec}} = d(I_{t+1}, \hat{I}_{t+1}),
\end{equation}
where $d$ is a loss function that is a linear combination of an $l2$, $l1$, and $\mathrm{SSIM}$ losses. In addition, we use the smoothness regularizer of \citep{jonschkowski2020uflow} that constrains the produced flow to be smooth, and allows us to deal with repetitive or textureless paterns:
\begin{equation} \label{eq:smooth_loss}
    \mathcal{L}_{\mathrm{smooth}} = \sum_{d \in \{x,y\}} \sum_{p} \exp({-\lambda \nabla_d I}) \| \nabla_{d} f_{t,t+1} \|_1,
\end{equation}
where x and y are directions in which the predicted flow is constrained to remain stable if the image gradient does not significantly change.
\parspace

\begin{table*}[t!]
\caption{\textbf{Quantitative results.} Comparison of the performance of our model on: (1) Sintel~\citep{butler2012sintel} clean and final, and KITTI 2015~\citep{menze2015kitti15} optical flow estimation benchmarks; (2) Pascal VOC~\citep{everingham2010voc}, Cityscapes~\citep{cordts2016cityscapes} and ADE20k~\citep{zhou2019ade20k}, both frozen and fine-tune linear segmentation benchmarks; (3) DAVIS-2017~\citep{ponttuset2017davis} and video object segmentation benchmark, against several self-supervised methods optimized for a single task specifically. EPE is the average end-point-error ($\downarrow$ Lower is better). F1 is the average-f1 error in (\%) ($\uparrow$ Lower is better). mIoU is the mean intersection-over-union ($\uparrow$ Higher is better). $(\mathcal{J}\&\mathcal{F})_m$ is the average between mean region similarity and mean contour-based accuracy ($\uparrow$ Higher is better). \algospace is our full model trained in multi-task way on ImageNet and flow estimation. M-JEPA is our model without content learning, trained only on flow estimation. The best and second best result for each benchmark are \textbf{bold} and \underline{underlined}.}
\vspace{-2mm}
\centering
\small
\setlength{\tabcolsep}{1.5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{} l @{\hspace{-3pt}} c c cc c cc c cc c cc c cc c cc c c@{}}
\toprule
& &~~~& \multicolumn{8}{c}{Optical Flow Estimation} &~~~~& \multicolumn{8}{c}{Image Segmentation} &~~~~& Video Seg. \\
\cmidrule{4-11} \cmidrule{13-20} \cmidrule{22-22}
& &~~~& \multicolumn{2}{c}{Sintel Clean} &~~~~& \multicolumn{2}{c}{Sintel Final} &~~~~& \multicolumn{2}{c}{KITTI 2015} &~~~~& \multicolumn{2}{c}{Pascal VOC} &~~~~& \multicolumn{2}{c}{CityScapes} &~~~~& \multicolumn{2}{c}{ADE20k} &~~~~& Davis 2017 \\
\cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-14} \cmidrule{16-17} \cmidrule{19-20} \cmidrule{22-22}
Method & Backbone &~~~& train & test && train & test && train & test && Frozen & FT && Frozen & FT  && Frozen & FT &&  \\
& &~~~& EPE & EPE && EPE & EPE && EPE & F1 && mIoU & mIoU && mIoU & mIoU && mIoU & mIoU && $(\mathcal{J}\&\mathcal{F})_m$\\
\midrule
Rand. weights & CNX-T && 23.71 & - && 24.02 & - && 24.88 & - && 0.5 & -  && - & - && - & - && - \\
\midrule
\textit{flow methods} \\
UFlow~\citep{jonschkowski2020uflow} & PWC && 2.50 & 5.21 && 3.39 & 6.50 && 2.71 & 11.13 && 7.8 & -  && - & - && - & - && 42.0 \\
ARFLow~\citep{liu2020arflow} & PWC && 2.79 & 4.78 && 3.73 & 5.89 && 2.85 & 11.80 && 7.9 & -  && - & - && - & - && - \\
UPFlow~\citep{luo2021upflow} & PWC && \underline{2.33} & \underline{4.68} && \underline{2.67} & \underline{5.32} && \underline{2.45} & \underline{9.38} && 8.8 & -  && - & - && - & - && - \\
SMURF~\citep{stone2021smurf} & RAFT && \textbf{1.71} & \textbf{3.15} && \textbf{2.58} & \textbf{4.18} && \textbf{2.00} & \textbf{6.83} && 10.4 & -  && - & - && - & - && - \\
\midrule
\textit{correspondence methods} \\
VFS~\citep{xu2021vfs} & R50 && - & - && - & - && - & - && 51.2 & -  && - & - && - & - && 68.9 \\
MCRW~\citep{bian2022mcrw} & PWC && 2.84 & 5.68 && 3.82 & 6.72 && 2.81 & 11.67 && 39.8 & -  && - & - && - & - && 57.9 \\
\midrule
\textit{content methods} \\
VICReg~\citep{bardes2022vicreg} & CNX-T && - & - && - & - && 13.5 & - && 60.1 & 77.8  && 59.8 & 76.3 && 28.6 & 41.1 && 58.1 \\
VICRegL~\citep{bardes2022vicregl} & CNX-T && - & - && - & - && 11.4 & - && \underline{66.8} & \underline{79.7} && \underline{64.9} & \underline{78.3} && \underline{30.6} & \underline{44.1} && 66.7 \\
MoCo v3~\citep{chen2021mocov3} & ViT-S && - & - && - & - && 12.9 & - && 57.1 & 75.9 && 56.5 & 74.0 && 23.7 & 39.8 && - \\
DINO~\citep{caron2021dino} & ViT-S && - & - && - & - && 11.8 & - && 65.2 & 79.5 && 64.8 & 78.1 && 30.5 & 43.5 && \underline{69.9} \\
\midrule
\textit{ours}  \\
% \algo \ (flow) 
\rowcolor{cyan!50}
M-JEPA & CNX-T && 2.98 & - && 3.82 & - && 3.01 & - && 9.4 & - && - & - && - & - && - \\
\rowcolor{cyan!50}
\algo & CNX-T && 2.81 & 5.01 && 3.51 & 6.12 && 2.67 & 11.33 && \textbf{67.1} & \textbf{79.9} && \textbf{65.5} & \textbf{78.4} && \textbf{30.8} & \textbf{44.2} && \textbf{70.5} \\
\bottomrule
\end{tabular}}
\label{tab:results}
\end{table*}

\noindent\textbf{Cycle consistency.} Flow estimation is a non-symmetric operation, as not all pixels of $I_{t}$ have a correspondence in $I_{t+1}$ and vice versa. For a given pair of images, we estimate both the forward and backward flows. We introduce a cycle-consistency loss that constraint the features $X_{t}$ warped by $f_{t,t+1}$ then by $f_{t+1,t}$ to match with $X_{t}$, the loss is defined as follows:
\begin{equation} \label{eq:cycle_loss}
    \mathcal{L}_{\mathrm{cycle}} = \sum_{l=1}^{L} \| X_{t}^{(l)} - f_{t+1,t}(f_{t,t+1}(X_{t}^{(l)})) \|_2^2,
\end{equation}
 where $f(X)$ is the warping operation of $X$ by flow $f$. We symmetrize the loss and do the same for $X_{t+1}$. In order to deal with occlusion, we follow \citep{lui2019ddflow} and use forward-backward compatibility, only applying $\mathcal{L}_{\mathrm{reg}}$ on the pixels that have a correspondence in both the forward and the backward flows.
 \parspace
 
 \noindent\textbf{Variance-covariance regularization.} Finally, in order to regularize the features produced by our encoder, we introduce a variance-covariance regularization loss function \citep{bardes2022vicreg}, defined as follows:
\begin{equation} \label{eq:vc_loss}
\begin{split}
\mathcal{L}_{\mathrm{vc}} & = \sum_{l=1}^{L} \frac{1}{d} \sum_{j=1}^{d} \max(0, \gamma - \sqrt{\mathrm{Var}(X_{t,j}^{(l)}) + \epsilon}) \\
& + \frac{1}{d} \sum_{i \ne j} [C(X_{t}^{(l)})]_{i,j}^2.
\end{split}
\end{equation}
where $\mathrm{Var}$ is the empirical variance and $C$ is the empirical covariance matrix after centering the features. This loss helps stabilizing the training with the multi-task setup described in Section~\ref{sec:multi_task}, and also improves the performance of the method as shown by Table~\ref{tab:vc}.

\subsection{Multi-task self-supervised learning} \label{sec:multi_task}

This section describes how we combine M-JEPA with content learning into our final \algospace method. \parspace

\noindent\textbf{Learning content features.} We follow the literature~\citep{chen2020simclr, grill2020byol, caron2020swav, bardes2022vicreg} and learn content features by simply pre-training our encoder to jointly-embed two views of an image. We generate the views using image transformation such as random cropping and color jittering. In particular, we use the VICReg objective~\citep{bardes2022vicreg} and follow its protocol. From a seed image sampled in an unlabelled training dataset $\mathcal{D}$, two views are generated using common data augmentation such as random croppring and color jittering, the views are then rescaled to a fixed size and fed to an encoder, then mapped to an expander network on which the VICReg loss is applied. The VICReg loss $\mathcal{L}_{\mathrm{ssl}}$ is similar to Eq.~(\ref{eq:vc_loss}), with in addition an invariance term ($l_2$ loss) that makes the embedding of the two views closer to each other and is minimized over $\mathcal{D}$.
\parspace

\noindent\textbf{Multi-task learning.} At a given iteration of training, we sample a batch of sequences from our video dataset and compute the flow loss, then sample a batch of images from ImageNet and compute our self-supervised learning loss, and then add the two losses and back-propagate the gradients into our encoder, expander, and flow estimator network. The encoder architecture and weights are shared between the two tasks. We illustrate our approach in Figure~\ref{fig:teaser} for the general idea and Figure~\ref{fig:arch} for the detailed architecture. The final loss function that \algospace optimizes is as defined follows:
\begin{equation} \label{eq:final_loss}
% \begin{split}
\sum_{\mathcal{D}_1} \mathcal{L}_\mathrm{{rec}} + \mathcal{L}_{\mathrm{reg}} + \mathcal{L}_{\mathrm{smooth}} + \mathcal{L}_{\mathrm{cycle}} + \mathcal{L}_{\mathrm{vc}} + \sum_{\mathcal{D}_2} \mathcal{L}_{\mathrm{ssl}},
% \end{split}
\end{equation}
where $\mathcal{D}_1$ is our video sequences dataset and $\mathcal{D}_2$ is our image dataset. The losses are balanced with additional coefficients that we tune carefully. Additional details are given in Appendix~\ref{app:hyper}, including the values we use for these coefficients.

% Figure environment removed

% Figure environment removed


\section{Experiments}

\subsection{Datasets}

Our model is pretrained in a single phase on a set of datasets commonly used for optical flow estimation, as well as on ImageNet-1k~\citep{deng2009imagenet}. Our video and flow datasets are KITTI (raw~\citep{geiger2013kitti}, 2012 multiview~\citep{geiger2012kitti12} and 2015 multiview~\citep{menze2015kitti15}), MPI Sintel~\citep{butler2012sintel} (clean, final and raw movie), FlyingChairs~\citep{yu2016flownet}, FlyingThings~\citep{mayer2016flyingthings}, and HD1K~\citep{kondermann2016hd1k}. We evaluate the quality of our estimated flow on Sintel clean and final and KITTI 2015 and compare our model with state-of-the-art methods in self-supervised flow estimation. We evaluate the quality of our features on instance segmentation on Pascal VOC~\citep{everingham2010voc}, CityScapes~\citep{cordts2016cityscapes} and ADE20k~\citep{zhou2019ade20k}, both in linear frozen and fine-tuning evaluation. Finally, we evaluate our model on the DAVIS 2017~\citep{ponttuset2017davis} video segmentation and instance tracking benchmark popularized by~\citep{caron2021dino}.

\subsection{Main results}

\noindent\textbf{Optical flow.} We compare the flow estimated by our model with several state-of-the-art methods optimized for flow estimation, as well as with MCRW, which discovers the flow by learning contrastive random walks between pixels. Table~\ref{tab:results} presents our results, which are on par with UFLow~\citep{jonschkowski2020uflow}, ARFlow~\citep{liu2020arflow} and UPFLow~\citep{luo2021upflow}, which are all optimized for flow estimation. SMURF~\citep{stone2021smurf} is better on all the benchmarks, but our goal is not to learn the best flow possible but rather to use it as a pretext task to learning general features and motion. However, we outperform MCRW which shares the same goal. Figure~\ref{fig:flow_viz} presents our optical flow qualitative results.
\parspace

\noindent\textbf{Instance Segmentation.} Table~\ref{tab:results} presents the performance of \algospace in various frozen and fine-tuned linear segmentation tasks, which are commonly used to evaluate the quality of the features learned by self-supervised learning models~\citep{zhou2022ibot, bardes2022vicregl}. We outperform MoCo v3~\citep{chen2021mocov3} and VICReg~\citep{bardes2022vicreg}, which is the method we use for our content features learning, by a large margin, which indicates that our flow estimation pretext task significantly helps the localization. Our results are on-par with VICRegL~\citep{bardes2022vicregl} which is specialized for segmentation and DINO~\citep{caron2021dino} which has among the best self-supervised features available. 
% We outperform MAE~\citep{he2022mae}, on frozen tasks but not in fine-tuning where it excels. Note that MAE is not available with the ViT-S backbone and we compare our results to a ViT-B backbone that has significantly more parameters (86M against 23M for our model).
\parspace

\noindent\textbf{Video Segmentation.} Finally, we compare the performance of \algospace on a video segmentation instance tracking task on the DAVIS 2017 dataset, against VFS~\citep{xu2021vfs} and MCRW~\citep{bian2022mcrw} which are correspondence learning methods and DINO. We outperform all these methods, which shows that learning motion through flow estimation is a good way of improving the learning of content features for tasks that requires motion information. Figure~\ref{fig:seg_viz} shows qualitative results on DAVIS 2017. Overall, our method allows us to train a single model that performs very well on all the above-mentioned tasks, whereas all the concurrent works are specialized for either content feature learning or motion and optical flow estimation learning.

\subsection{Ablations}

We perform many ablations on the components and training procedure of \algospace, and evaluate our models on KITTI 2015 train (K15 in tables, metric is EPE), Sintel clean and final (clean and final in tables, metric is EPE), Pascal VOC linear frozen evaluation (ISeg in tables, metric is mIoU), and DAVIS 2017 video segmentation (VSeg in tables, metric is $(\mathcal{J}\&\mathcal{F})_m$, which are all relatively fast to perform.
\parspace

\noindent\textbf{Flow datasets.} We start by evaluating the effect of varying the set of data used flow estimation. Table~\ref{tab:datasets} presents our results when incorporating or not various datasets. As expected, training on only KITTI or Sintel offers great performance in their respective evaluation set. Progressively adding FlyingChairs and Things, and HD1k, improves the flow results, but has very little influence on the segmentation tasks. The benefit on segmentation from doing flow estimation is independent from the domain on which the flow estimator is trained.
\parspace

\noindent\textbf{Flow estimator architecture.} When pretraining in our multi-task setup with ImageNet we observed many instabilities related to the gradient and the exploding norm of the estimator, and that we describe in Section~\ref{sec:imp_details}. We tried several changes to the flow estimator architecture to overcome these issues, namely using LayerNorm and l2-normalization. Table~\ref{tab:flowest_arch} presents our results when incorporating these elements, as well as when increasing the size of the estimator. Not regularizing the estimator led to crashing runs. $l2$-normalization is very inefficient, as it constrains the last layer to directly produce flows in the correct range of values. Using LayerNorm is the best solution and effectively prevents the estimator from exploding norms and gradients. Increasing the size of the estimator marginally improves the results.
\parspace

\noindent\textbf{Backbone.} Our backbone is a ConvNeXt-T~\citep{liu2022convnext}, we study the impact of pretraining models with other backbones, in particular ResNet-50, and the backbone of PWC-Net~\citep{sun2018pwcnet} commonly used by concurrent flow estimation methods. Table~\ref{tab:backbone} presents our results. The original PWC backbone is not adapted to learn good content features, and Resnet-50 results are not as good as ConvNeXt-T results.
\parspace

\noindent\textbf{Data sampling.} We experiment with different strategies for sampling the data. For a simple baseline, we use a pretrained self-supervised model in ImageNet and train the flow estimator on top of the frozen features, or by fine-tuning the model. We demonstrate the usefulness of multi-task learning by playing with various other strategies; either we alternate between one epoch of ImageNet learning and one epoch of flow estimation, or we alternate between one batch of each, or we finally sample a batch from each, and back-propagate through the addition of the losses. Table~\ref{tab:data_sampling} presents our results for each strategy. Training the flow estimator on top of frozen features is too hard of a constraint, but even when fine-tuning is done, optimizing the flow estimation task degrades the performance on segmentation too much. Alternating between epochs is not optimal, and the best solution is to alternate between batches and even combine the losses for optimal flow estimation results.
\parspace



\begin{table}[t!]
    \parbox{.48\linewidth}{
        \centering
        \caption{\textbf{Ablation: flow datasets.} Impact on performance when varying the set of pretraining datasets. KITTI means pretraining on KITTI raw, 2012 and 2015. Sintel means pretraining Sintel raw, clean and final. FT/FC are FlyingThings and FlyingChairs. The metric for K15 (KITTI 2015), clean and final is the EPE. ISeg is the linear frozen evaluation on Pascal VOC, in mIoU, VSeg is the evaluation on DAVIS 2017, in $(\mathcal{J}\&\mathcal{F})_m$.}
        \vspace{-2.5mm}
        \label{tab:datasets}
        \setlength{\tabcolsep}{2pt}
        \resizebox{0.48\textwidth}{!}{%
        \begin{tabular}{@{} cccc c ccccc @{}}
        \toprule
        KITTI & Sintel & FT/FC & HD1k && K15 & clean & final & ISeg & VSeg \\
        \midrule
        \checkmark & & & && 2.93 & 3.23 & 3.96 & 66.8 & 70.0 \\
         & \checkmark & & && 3.78 & 2.95 & 3.61 & 66.4 & 69.9 \\
         \checkmark & \checkmark & & && 2.91 & 2.99 & 3.70 & 67.2 & 70.4 \\
         \checkmark & \checkmark & \checkmark & && 2.88 & 2.93 & 3.66 & 67.1 & 70.3 \\
         \rowcolor{cyan!50}
         \checkmark & \checkmark & \checkmark & \checkmark && 2.67 & 2.81 & 3.51 & 67.1  & 70.5 \\
        \bottomrule
        \end{tabular}
        }
    }
    \hfill
    \parbox{.48\linewidth}{
        \centering
        \caption{\textbf{Ablation: estimator architecture.} Comparison between different flow estimator size form of normalization. The factor size influences the number of filters in each convolution of the estimator. LN means layer norm means usage of layer norm after every layer of the estimator, except the last one. l2 means l2-normalization before the last layer of the estimator.}
        \label{tab:flowest_arch}
        \setlength{\tabcolsep}{2pt}
        \resizebox{0.48\textwidth}{!}{%
        \begin{tabular}{@{} c cccc c ccccc @{}}
        \toprule
        Factor size & \#Params &  LN & l2 && K15 & clean & final & ISeg & VSeg \\
        \midrule
        1 & 2M & & && \multicolumn{5}{c}{crashed} \\
        1 & 2M & \checkmark & && 2.68 & 2.88 & 3.57 & 67.0  & 70.2 \\
        1 & 2M & & \checkmark && 6.21 & 6.04 & 6.99 & 53.2  & 47.9 \\
        1 & 2M & \checkmark & \checkmark && 4.55 & 4.47 & 5.66 & 62.3  & 63.6 \\
        \rowcolor{cyan!50}
        2 & 8M & \checkmark & && 2.67 & 2.81 & 3.51 & 67.1  & 70.5 \\
        \bottomrule
        \end{tabular}
        }
    }
\end{table}



\begin{table}[t!]
    \parbox{.48\linewidth}{
        \centering
        \caption{\textbf{Ablation: backbone.} Comparison of the performance of \algospace when using different backbones.}
        \vspace{-2mm}
        \label{tab:backbone}
        \setlength{\tabcolsep}{2pt}
        \resizebox{0.48\textwidth}{!}{%
        \begin{tabular}{@{} l c c ccccc @{}}
        \toprule
        Backbone & \#Params && K15 & clean & final & ISeg & VSeg \\
        \midrule
        PWC-Net & 8M && 2.66 & 2.80 & 3.47 & 14.8  & 10.1 \\
        ResNet-50 & 21M && 2.71 & 2.85 & 3.59 & 55.8  & 60.1 \\
        \rowcolor{cyan!50}
        ConvNeXt-T & 23M && 2.67 & 2.81 & 3.51 & 67.1  & 70.5 \\
        \bottomrule
        \end{tabular}
        }
    }
    \hfill
    \parbox{.48\linewidth}{
        \centering
        \caption{\textbf{Ablation: data sampling.} Comparison between different training order and data sampling strategies.}
        \vspace{-3mm}
        \label{tab:data_sampling}
        \setlength{\tabcolsep}{2pt}
        \resizebox{0.48\textwidth}{!}{%
        \begin{tabular}{@{} l c ccccc @{}}
        \toprule
        Strategy && K15 & clean & final & ISeg & VSeg \\
        \midrule
        Flow estimator training && 13.52 & 13.82 & 14.81 & 60.1  & 65.2 \\
        Flow estimator fine-tuning && 2.71 & 2.82 & 3.77 & 61.3  & 62.3 \\
        Epoch alternation && 4.54 & 4.91 & 5.57 & 63.5  & 66.9 \\
        Batch alternation && 2.78 & 2.95 & 3.62 & 67.1  & 70.5 \\
        \rowcolor{cyan!50}
        Combined loss && 2.67 & 2.81 & 3.51 & 67.1  & 70.5 \\
        \bottomrule
        \end{tabular}
        }
    }
\end{table}


% Figure environment removed

\noindent\textbf{Flow start epoch.} We found that starting multi-task learning of flow and content features at the beginning of training was not necessary, as the features are changing very fast, and we only start with ImageNet pretraining and introduce flow estimation after a given number of epochs. Figure~\ref{fig:ablations} (1) shows that starting after 10 epochs of ImageNet pretraining is the best among several values, when the total number of epochs is fixed to 100. Starting later and doing fewer flow estimation epochs saves a lot of computation time while giving similar results.
\parspace

\noindent\textbf{Cycle consistency.} Figure~\ref{fig:ablations} (2) shows an ablation on the cycle consistency coefficient that controls the importance of the cycle consistency loss of Eq~(\ref{eq:cycle_loss}). Introducing the loss significantly improves the flow estimation, which is explained by the fact that it adds an additional constraint on the embeddings to be predictable from each other. The coefficient needs to be carefully tuned, as the performance is very sensitive to it.
\parspace

\noindent\textbf{Multi-task balancing coefficient.} Figure~\ref{fig:ablations} (3) shows an ablation on the multi-task coefficient that balances our flow estimation loss and our content features loss. We already observe a significant improvement when introducing flow estimation, even with a very small coefficient. As we increase the coefficient, both the flow estimation and segmentation improve until we reach a threshold (0.1), after which the segmentation results degrade a lot. This shows that even if flow estimation improves the segmentation performance, there is a trade-off between learning motion and content features, and tuning the multi-task coefficient is crucial to maintain a strong level of performance for both.

\section{Conclusion}

We have introduced \algo, a multi-task approach to learning of motion and content features with self-supervised learning and optical flow estimation. \algospace performs well in a wide variety of tasks, ranging from optical flow estimation to segmentation of images and videos. We hope that our approach will foster the use of multi-task learning in self-supervised learning, which might be a path towards learning features that generalize to any downstream task. Future work will learn motion and content from larger collections of natural videos and train the two objectives in a shared data domain, capturing short- and long-range interactions in a hierarchical way.


\newpage

\textbf{Acknowledgement.} Jean Ponce was supported in part by the French government under management of Agence Nationale de la Recherche as part of the ”Investissements d’avenir” program,
reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton/ENS Chair in Artificial
Intelligence, and a Global Distinguished Professorship at the Courant Institute of Mathematical Sciences and the Center for Data Science at New York University. Adrien Bardes was supported in part by a FAIR/Prairie
CIFRE PhD Fellowship.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix
\clearpage

\input{appendix.tex}

\end{document}
