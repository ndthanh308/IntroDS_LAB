\documentclass[11pt]{article}

\usepackage{bm} 
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}

\usepackage{typearea}
\typearea{12}

\begin{document}
\title{Information Geometry of Wasserstein Statistics \\on Shapes and Affine Deformations}
\author{Shun-ichi Amari \\
	Teikyo University, Advanced Comprehensive  Research Organization, \\
	RIKEN Center for Brain Science \and
	Takeru Matsuda \\
	The University of Tokyo, \\
	RIKEN Center for Brain Science}
\date{}
\maketitle

\begin{abstract}
Information geometry and Wasserstein geometry are two main structures introduced in a manifold of probability distributions, and they capture its different characteristics. 
We study characteristics of Wasserstein geometry in the framework of \cite{LZ2019} for the affine deformation statistical model, which is a multi-dimensional generalization of the location-scale model.
We compare merits and demerits of estimators based on information geometry and Wasserstein geometry. 
The shape of a probability distribution and its affine deformation are separated in the Wasserstein geometry, showing its robustness against the waveform perturbation in exchange for the loss in Fisher efficiency. 
We show that the Wasserstein estimator is the moment estimator in the case of the elliptically symmetric affine deformation model.  
It coincides with the information-geometrical estimator (maximum-likelihood estimator) when and only when the waveform is Gaussian.  
The role of the Wasserstein efficiency is elucidated in terms of robustness against waveform change.  
\end{abstract}


\section{Introduction}
\label{sec:1}

We study statistics based on probability distribution patterns $p({\bm{x}})$ over ${\bm{x}} \in X={\bm{R}}^d$, by using both information geometry \citep[see][etc]{Amari2016,Ay2017} and Wasserstein geometry \citep[see][among many others]{Villani2003,PC2019,Santambrogio2015}.  Here, $p({\bm{x}})$ is a probability distribution on $X={\bm{R}}^d$.  When $d=2$, $p({\bm{x}})$ is regarded as a visual pattern on ${\bm{R}}^2$.  

There are lots of applications of Wasserstein geometry to statistics \citep[see, e.g.,][and others]{AM2022,BJGR2019,yatracos2022,BBR2006,Matsuda2021, IOH2022, LM2020, Chen}, machine learning \citep[see, e.g.,][among many others]{ACB2917,FZMAP2015,WL2019,PC2019,MMC2015} and statistical physics \citep{ito2023}.  
We have a good review paper and a book, \cite{PZ2019, PZ2022}, which include lots of references.  
However, applications to statistical inference look still premature.  
For example, the statistical efficiency of the Wasserstein estimator is studied only in the one-dimensional location-scale model \citep{AM2022}. 
We give characterization of the Wasserstein estimator from the point of view of the robustness for changes of the shape of probability distributions.  We further focus on the efficiency of the Wasserstein estimator for the affine deformation statistical model (deformation model in short), where deformation parameters and the waveform of a probability distribution are separated. 

The affine deformation statistical model $p({\bm{x}}, {\bm{\theta}})$ is generated from a standard shape distribution $f({\bm{z}})$ satisfying
\begin{eqnarray}
	\label{eq:am2}
	\int f ({\bm{z}})d{\bm{z}} &=& 1, \\
	\label{eq:am3}
	\int {\bm{z}} f({\bm{z}})d{\bm{z}} &=& 0, \\
	\label{eq:am4}
	\int {\bm{z}}{\bm{z}}^{\top} f({\bm{z}}) d{\bm{z}} &=& I,
\end{eqnarray}              
where $I$ is the identity matrix.  
The deformation parameter consists of ${\bm{\theta}}=\left( {\bm{\mu}}, \Lambda \right) \in \Theta$ such that ${\bm{\mu}}$ is a vector specifying translation of the location and $\Lambda$ is a non-singular matrix representing scale changes and rotations of ${\bm{x}}$.
Then, $p({\bm{x}}, {\bm{\theta}})$ is written as
\begin{equation}
	\label{eq:am220230131}
	p({\bm{x}}, {\bm{\theta}}) = |\Lambda|
	f \left( \Lambda ({\bm{x}}-{\bm{\mu}}) \right).
\end{equation}
Given a standard $f$, we have a statistical model parameterized by ${\bm{\theta}}$,
\begin{equation}
	M_f = \left\{
	p ({\bm{x}}, {\bm{\theta}})
	\right\}.
\end{equation}
Geometrically, it forms a finite-dimensional statistical manifold, where ${\bm{\theta}}$ plays the role of a coordinate system.  
The deformation model is a generalization of the location-scale model. 
Note that this model is often called the location-scatter model in several fields such as statistics and signal processing \citep{tyler1987distribution,ollila2014regularized}.

Let $T_{\bm{\theta}}$ denote the affine deformation from ${\bm{x}}$ to ${\bm{z}}$ given by
\begin{equation}
	{\bm{z}} = T_{\bm{\theta}}{\bm{x}} = \Lambda ({\bm{x}}-{\bm{\mu}}).
\end{equation}
This may be regarded as deformation of shape $f$ to $\tilde{T}_{\bm{\theta}} f$,
\begin{equation}
 \label{eq:am620230410}
	(\tilde{T}_{\bm{\theta}} f) ({\bm{x}}) =
	f \left( T_{\bm{\theta}} {\bm{x}} \right),
\end{equation}
where $\tilde{T}_{\bm{\theta}}$ is an operator to change a standard waveform $f$ to another waveform $\tilde{f}=\tilde{T}_{\bm{\theta}} f$ defined by (\ref{eq:am620230410}).   

Let ${\cal{F}}=\{q({\bm{x}})\}$ be the space of all smooth positive probability density functions that have mean and covariance.
Let ${\cal{F}}_S=\{f({\bm{z}})\}$ be its subspace consisting of all the standard distributions $f({\bm{z}})$ satisfying \eqref{eq:am2}, \eqref{eq:am3} and \eqref{eq:am4}.  
Then, any $q({\bm{x}}) \in {\cal{F}}$ is uniquely written in the form
\begin{equation}
    q({\bm{x}}) = |\Lambda| f \left( \Lambda ({\bm{x}}-{\bm{\mu}}) \right)
\end{equation}
for $f \in {\cal{F}}_S$ and ${\bm{\theta}}=( {\bm{\mu}}, \Lambda) \in \Theta$.
Hence, ${\cal{F}}$ is the direct product of ${\cal{F}}_S$ and $\Theta$: ${\cal{F}}={\cal{F}}_S \times \Theta$.  See Figure~\ref{fig1}. 

% Figure environment removed

It is interesting to see that ${\cal{F}}_S$ is not a manifold.  It includes lots of singularities.  Let $O$ be an orthogonal transformation.  Then, $f({\bm{x}})$ and $(Of)({\bm{x}})=f(O{\bm{x}})$ give different shape functions in general.  However, when $f$ is rotationally invariant, the shapes of $f$ and $Of$ are identical, $f=Of$.  The isotropic Gaussian shape is such an example, and all $Of$ reduces to one point in such a case.  It is also the case when $f$ is invariant under a subgroup of the orthogonal group.  See Figure~\ref{fig2} for the shape of ${\cal{F}}_S$.  It is a covex set, because, for $f, g \in {\cal{F}}_S$, $\alpha f + (1-\alpha) g$ also belongs to ${\cal{F}}_S$ for $0 \le \alpha \le 1$. 

% Figure environment removed

Geometry of a manifold of probability distributions has so far been studied by information geometry and Wasserstein geometry.  The two geometries capture different aspects of a manifold of probability distributions.  We use a divergence measure to explain this.  Let $D_F [p({\bm{x}}),q({\bm{x}})]$ and $D_W [p({\bm{x}}),q({\bm{x}})]$ be two divergence measures between distributions $p({\bm{x}})$ and $q({\bm{x}})$, where subscripts $F$ and $W$ represent Fisher-based information geometry and Wasserstein geometry, respectively.  Information geometry uses an invariant divergence $D_F$, typically the Kullback--Leibler divergence.  Wasserstein divergence $D_W$ is defined by the cost of transporting masses distributed in form $p({\bm{x}})$ to another $q({\bm{x}})$.  Roughly speaking, $D_F$ measures the vertical differences of $p({\bm{x}})$ and $q({\bm{x}})$, for example, represented by their log-ratio $\log (p({\bm{x}})/q({\bm{x}}))$, at ${\bm{x}}$, whereas $D_W$ measures the horizontal differences of $p({\bm{x}})$ and $q({\bm{x}})$ which corresponds to the transportation cost from $p({\bm{x}})$ to $q({\bm{x}})$. See Figure~\ref{fig3}.

% Figure environment removed

Information geometry is constructed based on the invariance principle of Chentsov \citep{Chentsov} such that $D_F [p({\bm{x}}),q({\bm{x}})]$ is invariant under invertible transformations of the coordinates ${\bm{x}}$ of the sample space $X$.  This implies that the divergence does not depend on the coordinate system of $X$.  We then have a unique Riemannian metric, which is Fisher--Rao metric, and also a dual pair of affine connections \citep{AmariNagaoka2007}.  This is useful not only for analyzing the performances of statistical inference but also for vision analysis, machine learning, statistical physics, and many others \citep[see][]{Amari2016}.

Wasserstein geometry has an old origin, proposed by G. Monge in 1781 as a problem of transporting mass distributed in the form $p({\bm{x}})$ to another $q({\bm{x}})$ such that the total transportation cost is minimized.  It depends on the transportation cost $c({\bm{x}}, {\bm{y}})$ between two locations ${\bm{x}}, {\bm{y}} \in X$.  The cost is usually a function of the Euclidean distance between ${\bm{x}}$ and ${\bm{y}}$.  We use the square of the distance as a cost function, which gives $L^2$-Wasserstein geometry.  This Wasserstein geometry directly depends on the Euclidean distance of $X={\bm{R}}^d$.  Therefore, it is responsible for the metric structure of $X$ and is useful for problems that intrinsically depend on the structure of $X$, such as the transportation problem, non-equilibrium statistical physics, pattern analysis, machine learning and many others.

It is natural to search for the relation between the two geometries. There are a number of such trials, including \citet{AKO2018,AKOC2019,KZ2022,RW2023, ito2023} and others.
Among them, \cite{LZ2019} gave a unified framework for the two geometries. The present article is based on their framework and focuses on the affine deformation model, for which the standard waveform $f$ and the deformation parameter ${\bm{\theta}}$ are separated. 

\cite{LZ2019} introduced the $W$-score function in parallel to the Fisher score function, defining two estimators $\hat{\bm{\theta}}_F$  and $\hat{\bm{\theta}}_W$ thereby.  The former is the maximum likelihood estimator that maximizes the log likelihood.  This is the one that minimizes an invariant divergence from the empirical distribution $\hat{p}({\bm{x}})$ to parametrized model $M_f$, where the empirical distribution is given based on $n$ independent and identical observations ${\bm{x}}_1, \cdots, {\bm{x}}_n$ as
\begin{equation}
	\hat{p}({\bm{x}}) = \frac 1n \sum_i \delta
	\left( {\bm{x}}-{\bm{x}}_i \right),
\end{equation}
and $\delta({\bm{x}})$ is the delta function.  
The latter $W$-estimator $\hat{\bm{\theta}}_W$ is asymptotically equivalent to the minimizer of the $W$-divergence between the empirical distribution and model $M_f$ (see Section 4).
\cite{LZ2019} further defined the $F$-efficiency and $W$-efficiency of an estimator $\hat{\bm{\theta}}$ given a statistical model $M=\left\{p({\bm{x}}, {\bm{\theta}}) \right\}$, proving the Cram\'{e}r-Rao type inequalities. 
We apply their theory to analyze the effects of the shape $f$ on the efficiencies of the two types of estimators.  

The present paper is organized as follows. 
In section \ref{sec:2}, we introduce two divergences between distributions, one based on the invariance principle and the other based on the transportation cost.  
The divergences give two Riemannian structures in the space ${\cal{F}}$ of probability distributions $p({\bm{x}})$ over $X={\bm{R}}^d$.  
A regular statistical model $M= \left\{ p({\bm{x}}, {\bm{\theta}})\right\}$ parameterized by ${\bm{\theta}}$ is a finite-dimensional submanifold embedded in ${\cal{F}}$.  
In section \ref{sec:3}, we define the $F$- and $W$-score functions following \cite{LZ2019}.  
The Riemannian structure of the tangent space of probability distributions is pulled-back to the model submanifold, giving both the Riemannian metrics and score functions to $M$.  
We define the $F$- and $W$-estimators $\hat{\bm{\theta}}_F$ and $\hat{\bm{\theta}}_W$ by using the $F$- and $W$-score functions, respectively.
Section \ref{sec:4} defines the affine deformation statistical model.  
Section \ref{sec:ellip} studies the elliptically symmetric affine deformation model $M_f$, where $f$ is a spehrically symmetric standard form.
For this model, we show that the $W$-score functions are quadratic functions of ${\bm{x}}$.  
Hence, it is proved that $\hat{\bm{\theta}}_W$ is a moment estimator.   
We also show that $M_f$ and ${\cal{F}}_S$ are orthogonal in the $W$-geometry, implying the separation of the waveform and deformation.
In Section \ref{sec:robust}, we elucidate the role of $W$-efficiency from the point of view of robustness to a change in the waveform $f$ due to observation noise. In Section \ref{sec:waveform}, we prove that the Gaussian shape is a unique model in which $F$-estimator and $W$-estimator coincide, with $\hat{\bm{\theta}}_W$ satisfying the $F$-efficiency and $\hat{\bm{\theta}}_F$ satisfying $W$-efficiency.  
Section \ref{sec:conc} briefly summarizes the paper and mentions future work.


\section{Riemannian structures in the space of probability densities on ${\bm{R}}^d$}
\label{sec:2}

We consider the space ${\cal{F}}= \left\{p({\bm{x}}) \right\}$ of all smooth positive probability density functions on ${\bm{R}}^d$, having the mean and covariance.  Later, we may relax the conditions of positivity and smoothness, when we discuss a parametric model, in particular the deformation model.  We define a divergence function $D[p({\bm{x}}), q({\bm{x}})]$, which represents the degree of difference between $p({\bm{x}})$ and $q({\bm{x}})$.  The square of the distance between $p({\bm{x}})$ and $q({\bm{x}})$ plays this role, but a divergence does not necessarily need to be symmetric with respect to $p({\bm{x}})$ and $q({\bm{x}})$.  A divergence function satisfies the following conditions:
\begin{enumerate}
	\item $D[p({\bm{x}}),q({\bm{x}})] \ge 0$ and the equality holds if and only if $p({\bm{x}}) = q({\bm{x}})$.
	\item  Let $\delta p({\bm{x}})$ be an infinitesimally small deviation of $p({\bm{x}})$.  Then, $D[p({\bm{x}}),p({\bm{x}}) +  \delta p({\bm{x}})]$ is approximated by a positive quadratic functional of $\delta p({\bm{x}})$.
\end{enumerate}

A divergence is said to be invariant if
\begin{equation}
	D \left[ p({\bm{x}}),q({\bm{x}}) \right] = 
	D \left[ \tilde{p}({\bm{y}}), \tilde{q}({\bm{y}}) \right]
\end{equation}
holds for every smooth reversible transformation ${\bm{k}}$ of the coordinates from ${\bm{x}} \in {\bm{R}}^d$ to ${\bm{y}}={\bm{k}}({\bm{x}})$, where 
\begin{equation}
	\tilde{p} ({\bm{y}}) =
	\left|
	\frac{\partial {\bm{x}}}{\partial {\bm{y}}}
	\right| p({\bm{x}}).
\end{equation}
A typical invariant divergence is the $\alpha$-divergence $(\alpha \ne \pm 1)$ defined by
\begin{eqnarray}
	D_{\alpha}[p,q] &=& \frac 4{1-\alpha^2} \left( 1-\int p({\bm{x}})^{(1+\alpha)/2} q({\bm{x}})^{(1-\alpha)/2}  d{\bm{x}} \right)
\end{eqnarray} 
for $\alpha \ne \pm 1$.
For $\alpha= 1$, we define $D_1[p,q]$ by the Kullback--Leibler divergence
\begin{equation}
	D_1 [p, q] = \int p({\bm{x}}) \log \frac{p({\bm{x}})}{q({\bm{x}})} d{\bm{x}}.
\end{equation}
For $\alpha=-1$, we define $D_{-1}[p:q]=D_1[q:p]$.
The case $\alpha=0$ is equivalent to the Hellinger divergence
\begin{equation}
	H^2[p,q] = \frac{1}{2} \int \left(
	\sqrt{p({\bm{x}})} - \sqrt{q({\bm{x}})} \right)^2 d{\bm{x}}.
\end{equation}
A characterization of the $\alpha$-divergence is given in \cite{Amari2016}.  
The $\alpha$-divergence gives information-geometric structure to $\cal{F}$.

Another divergence is the Wasserstein divergence.  
Let us transport masses piled in the form $p({\bm{x}})$ to another $q({\bm{x}})$.  
To this end, we need to move some mass at ${\bm{x}}$ to another position ${\bm{y}}$.  
Let $\pi ({\bm{x}}, {\bm{y}})$ be a stochastic matrix, showing the probability of mass at ${\bm{x}}$ to be transported to ${\bm{y}}$.  
We call $\pi$ a transportation plan when it satisfies the following terminal conditions
\begin{eqnarray}
	\label{eq:am1420230116}
	\int \pi ({\bm{x}}, {\bm{y}}) d {\bm{y}}
	&=& p({\bm{x}}), \\
	\label{eq:am1520230116}
	\int \pi ({\bm{x}}, {\bm{y}}) d {\bm{x}}
	&=& q({\bm{y}}).
\end{eqnarray}
Let $c({\bm{x}}, {\bm{y}})$ be the cost of transporting a unit of mass from ${\bm{x}}$ to ${\bm{y}}$.
Then, the Wasserstein divergence $D_W [p({\bm{x}}),q({\bm{x}})]$ is the minimum transporting cost from $p({\bm{x}})$ to $q({\bm{x}})$.  By using stochastic plan $\pi ({\bm{x}}, {\bm{y}})$,  the Wasserstein divergence between $p({\bm{x}})$ and $q({\bm{x}})$ is given by
\begin{equation}
	\label{eq:am16}
	D_W \left[p({\bm{x}}),q({\bm{x}}) \right] =
	{\mathop{\inf}_{\pi}} \int c({\bm{x}}, {\bm{y}}) \pi
	({\bm{x}}, {\bm{y}}) d{\bm{x}} d{\bm{y}},
\end{equation}
where infimum is taken over all stochastic plans $\pi$ satisfying (\ref{eq:am1420230116}) and (\ref{eq:am1520230116}).  
When the cost is the square of the Euclidean distance
\begin{equation}
	\label{eq:am13}
	c({\bm{x}}, {\bm{y}}) = \| {\bm{x}}-{\bm{y}} \|^2,
\end{equation}
we call $D_W$ the $L^2$-Wasserstein divergence. 
We focus on this divergence in the following.
Note that the $L^2$-Wasserstein divergence is the square of the $L^2$-Wasserstein distance.

The dynamic formulation of the optimal transport problem proposed by \cite{Brenier} and developed further by \cite{BB2000} is useful. Let $\rho ({\bm{x}}, t)$ be a family of probability distributions parameterized by $t$.  It represents the time course $\rho({\bm{x}}, t)$ of transporting $p({\bm{x}})$ to $q({\bm{x}})$, satisfying 
\begin{equation}
	\rho({\bm{x}}, 0) = p({\bm{x}}), \quad
	\rho({\bm{x}}, 1) = q({\bm{x}}).
\end{equation}
We introduce potential $\Phi ({\bm{x}}, t)$ such that its gradient $\nabla_{\bm{x}} \Phi ({\bm{x}}, t)$ represents the velocity 
\begin{equation}
	{\bm{v}}({\bm{x}}, t) =
	\nabla_{\bm{x}} \Phi({\bm{x}}, t)
\end{equation}
of mass flow at ${\bm{x}}$ and $t$ in the dynamic plan. 
Then, $\Phi({\bm{x}}, t)$ satisfies the following equation of continuity
\begin{equation}
	\label{eq:am1920230118}
	\partial_t \rho ({\bm{x}}, t) + \nabla_{\bm{x}} \cdot
	\left\{ \rho ({\bm{x}}, t) \nabla_{\bm{x}} \Phi ({\bm{x}}, t) \right\}
	= 0.
\end{equation}
The Wasserstein divergence is written in the dynamic formulation as
\begin{equation}
	D_W \left[ p({\bm{x}}),q({\bm{x}}) \right] =
	{\mathop{\inf}_{\Phi}} \int^1_0 \int 
	\| \nabla_{\bm{x}} \Phi ({\bm{x}}, t) \|^2 \rho({\bm{x}}, t) d {\bm{x}} dt.
\end{equation}

We introduce a Riemannian structure to $\cal{F}$ by the Taylor expansion of $D[p,p + \delta p]$.  The Riemannian metric $g$ is an operator to give the squared magnitude $ds^2$ of an infinitensimal deviation $\delta p ({\bm{x}})$ in the tangent space of $\cal{F}$, for example, by
\begin{equation}
	ds^2 = \int \delta p({\bm{x}}) g ({\bm{x}}, {\bm{y}})
	\delta p({\bm{y}}) d{\bm{x}} d{\bm{y}}.
\end{equation}
In the case of the invariant divergence, we have
\begin{equation}
	g_F ({\bm{x}}, {\bm{y}}) = \frac{\delta{({\bm{x}}-{\bm{y}})}}{p({\bm{x}})}, 
\end{equation}
where $\delta({\bm{x}})$ is the delta function and $g_F$ is a positive integral operator.
In the case of the $L^2$-Wasserstein divergence, for infinitesimally small $\delta t$, the change of distribution from $\rho ({\bm{x}}, 0)=p({\bm{x}})$ at $t=0$ to $\rho ({\bm{x}}, \delta t) = p({\bm{x}})+ \delta p({\bm{x}})$ at $t=\delta t$  satisfies
\begin{equation}
	\delta p({\bm{x}}) = -\Delta_p \Phi({\bm{x}}) dt,
\end{equation}
where $\Delta_p$ is the $p$-Laplacian defined by
\begin{equation}
	\Delta_p \Phi = \nabla_{\bm{x}} \cdot
	\left( p({\bm{x}}) \nabla_{\bm{x}} \Phi ({\bm{x}}) \right).
\end{equation}
The $L^2$-Wasserstein divergence is given in the quadratic form of $\delta p$ as
\begin{equation}
	D_W [p,p+ \delta p] = -\int
	\delta p({\bm{x}}) \Delta^{-1}_{\rho}
	\delta p({\bm{x}}) d{\bm{x}},
\end{equation}
giving the Riemannian metric $g_W$ which is an operator represented by
\begin{equation}
	g_W (p) = -\Delta^{-1}_p.
\end{equation}
This is Otto's Riemannian metric \citep{Otto}.
See \cite{LZ2019} for details.

\section{Score functions and estimators in parametric model}
\label{sec:3}

We consider a regular statistical model $M=\left\{ p({\bm{x}}, {\bm{\theta}}) \right\}$ parameterized by $m$-dimensional vector ${\bm{\theta}}$.  The tangent space of $M$ at ${\bm{\theta}}$ is spanned by $\partial_i p ({\bm{x}}, {\bm{\theta}})$ for $i=1, \cdots, m$, such that
\begin{equation}
	\label{eq:am2920230131}
	\delta p({\bm{x}}) = \partial_i p ({\bm{x}}, {\bm{\theta}})
	d \theta^i,
\end{equation}
where
\begin{equation}
	\partial_i p ({\bm{x}}, {\bm{\theta}}) =
	\frac{\partial}{\partial \theta^i}
	p ({\bm{x}}, {\bm{\theta}}).
\end{equation}
Hereafter, the summation convention is used, that is, all indices appearing twice, once as upper and the other as lower indices, e.g. $i$'s in (\ref{eq:am2920230131}), are summed up.

Let us define $S_i ({\bm{x}}, {\bm{\theta}})$ from the basis functions $\partial_i p ({\bm{x}}, {\bm{\theta}})$ of the tangent space of $M$ for $i=1, \cdots, m$ by applying the Riemannian metric operator $g$:
\begin{equation}
	\label{eq:am32}
	S_i ({\bm{x}}, {\bm{\theta}})  =
	g \circ \partial_i p ({\bm{x}}, {\bm{\theta}}).
\end{equation}
We call them score functions following the tradition of statistics. In the case of invariant Fisher geometry, the score functions are
\begin{eqnarray}
	S^F_i ({\bm{x}}, {\bm{\theta}}) &=&
	\frac{\partial_i p}{p({\bm{x}}, {\bm{\theta}})}
	= \partial_i l ({\bm{x}}, {\bm{\theta}}), \\
	l ({\bm{x}}, {\bm{\theta}}) &=& \log p ({\bm{x}}, {\bm{\theta}}),
\end{eqnarray}
which is the derivative of log-likelihood. 
In Wasserstein geometry, we have
\begin{equation}
	S^W_i ({\bm{x}}, {\bm{\theta}}) = -\Delta_p^{-1}
	\partial_i p ({\bm{x}}, {\bm{\theta}}).
\end{equation} 
By using the identity
\begin{equation}
	\label{eq:am3520210118}
	\int a({\bm{x}}) \Delta_p b({\bm{x}}) d {\bm{x}} = -\int \left( \nabla_{\bm{x}} a \cdot \nabla_{\bm{x}} b \right) p({\bm{x}}) d{\bm{x}},
\end{equation}
we see that $S^W_i ({\bm{x}}, {\bm{\theta}})$ are solutions of the following Poisson equations,
\begin{equation}
	\label{eq:poisson}
	\nabla_{\bm{x}} \log p ({\bm{x}}, {\bm{\theta}}) \cdot \nabla_{\bm{x}} S^W_i ({\bm{x}}, {\bm{\theta}}) +
	\Delta_{\bm{x}} S^W_i
	({\bm{x}}, {\bm{\theta}}) +
	\frac{\partial}{\partial \theta_i} \log
	p({\bm{x}}, {\bm{\theta}}) = 0.
\end{equation}
For infinitesimal $\delta$, the map $\bm{x} \mapsto \bm{x}+\delta \nabla_{\bm{x}} S_i^{{W}}(\bm{x}, \bm{\theta})$ is the optimal transport map from $p(\bm{x}, \bm{\theta})$ to $p(\bm{x}, \bm{\theta} + \delta \bm{e}_i)$ with transportation cost
\begin{equation}
	D_W(p(\bm{x}, \bm{\theta}), p(\bm{x}, \bm{\theta} + \delta \bm{e}_i)) = \int \| \delta \nabla_{\bm{x}} S_i^{{W}}(\bm{x}, \bm{\theta}) \|^2 p(\bm{x}, \bm{\theta}) {\rm d} x,
\end{equation}
where $\bm{e}_i$ is the $i$-th standard unit vector.
In order to eliminate the indefiniteness due to the integral constant, we pose an additional condition
\begin{equation}
	\label{eq:am36}
	{\rm{E}} \left[ S_i ({\bm{x}}, {\bm{\theta}}) \right]
	= 0.
\end{equation}     
This is automatically satisfied in the Fisherian case. 

The Riemannian metric tensor $g_{ij}({\bm{\theta}})$ is pulled-back from $g$ in ${\cal{F}}$ to $M$, and is derived in terms of the score functions as
\begin{equation}
	g_{ij} ({\bm{\theta}}) =
	\langle S_i, g^{-1} S_j \rangle,
\end{equation}
where
\begin{equation}
	\langle
	a({\bm{x}}), b({\bm{x}})
	\rangle = \int a({\bm{x}}) b({\bm{x}}) d{\bm{x}}.
\end{equation}
In the Fisherian case, 
\begin{equation}
	g^F_{ij} ({\bm{\theta}}) = {\rm{E}}
	\left[ \partial_i l \partial_j l \right] =
	\int p (x, {\bm{\theta}}) \partial_i l 
	({\bm{x}}, {\bm{\theta}}) \partial_j l
	({\bm{x}}, {\bm{\theta}}) d{\bm{x}}.
\end{equation}
In the Wasserstein case, 
\begin{equation}
	g^W_{ij}({\bm{\theta}}) = 
	\int p ({\bm{x}}, {\bm{\theta}})
	\nabla_{\bm{x}} S^W_i ({\bm{x}}, {\bm{\theta}})^{\top}
	\nabla_{\bm{x}} S^W_j ({\bm{x}}, {\bm{\theta}}) d {\bm{x}} = {\rm E} [\nabla_{\bm{x}} S^W_i ({\bm{x}}, {\bm{\theta}})^{\top}
	\nabla_{\bm{x}} S^W_j ({\bm{x}}, {\bm{\theta}})], \label{WIM}
\end{equation}
where identity (\ref{eq:am3520210118}) is used.

The score functions $S_i ({\bm{x}}, {\bm{\theta}})$  give a set of estimating functions from (\ref{eq:am36}), which are used to obtain an estimator $\hat{\bm{\theta}}$.  Let $\hat{p}_{\rm{emp}}({\bm{x}})$ be the empirical distribution given by
\begin{equation}
	\hat{p}_{\rm{emp}}({\bm{x}}) = \frac 1n
	\sum^n_{j=1} \delta
	\left({\bm{x}}-{\bm{x}}_j \right),
\end{equation}
where ${\bm{x}}_1, \cdots, {\bm{x}}_n$ are $n$ independent observations.  Then, replacing expectation ${\rm{E}}$ in (\ref{eq:am36}) by the expectation with respect to the empirical distribution, we have estimating equations, 
\begin{equation}
 \label{eq:am4420230516}
	{\rm{E}}_{\rm{emp}} 
	\left[ S_i ({\bm{x}}, {\bm{\theta}}) \right] =
	\frac 1n \sum^n_{j=1} S_i \left( {\bm{x}}_j, {\bm{\theta}} \right) = 0, \quad i=1, \cdots, m.
\end{equation}
It is known that the solution $\hat{\bm{\theta}}$ gives a consistent estimator for large $n$.  Roughly speaking, $\hat{\bm{\theta}}$ is the projection of $\hat{p}_{\rm{emp}}$ to the model $M$ with respect to the metric $g$ (see Figure~\ref{fig4}).  It is the solution of
\begin{equation}
	\langle \hat{p}_{\rm{emp}} ({\bm{x}}), S_i ({\bm{x}}, {\bm{\theta}}) \rangle
	= 0,
\end{equation}
giving a consistent estimator $\hat{\bm{\theta}}$. 
A consistent estimator is Fisher efficient when the projection is orthogonal with respect to Fisher-Rao metric \citep{AmariNagaoka2007}.

% Figure environment removed

Estimator $\hat{\bm{\theta}}_F$ (the invariant Fisherian case) is the maximum likelihood estimator, that maximizes the likelihood.  Cram\'{e}r-Rao theorem gives a matrix inequality for any unbiased estimator $\hat{\bm{\theta}}$,
\begin{equation}
	{\rm{Cov}} \left[ \hat{\bm{\theta}}-{\bm{\theta}} \right] \succeq
	\frac 1n g^{-1}_F ({\bm{\theta}}),
\end{equation}
where ${\rm{Cov}} [\cdot]$ is the covariance matrix and $\succeq$ denotes the matrix order defined by the positive definiteness.  
The maximum likelihood estimator $\hat{\bm{\theta}}_F$ satisfies 
\begin{equation}
	{\rm{Cov}} 
	\left[ \hat{\bm{\theta}}_F -{\bm{\theta}} \right] \approx
	\frac 1n g^{-1}_F ({\bm{\theta}})
\end{equation}
asymptotically.  Hence, it minimizes the error covariance matrix and the minimized error covariance is given asymptotically by the inverse of the Fisher metric tensor $g_F$ divided by $n$.
Such a property is called the Fisher efficiency.

In parallel to the Fisherian case, we need to study the characteristics of the Wasserstein estimator $\hat{\bm{\theta}}_W$ in the following.  
In the case of the one-dimensional location-scale model, the Wasserstein estimator is asymptotically equivalent to the estimator obtained by minimizing the Wasserstein divergence (transportation cost) from the empirical distribution $\hat{p}_{\rm{emp}}$ to model $M$,
\begin{equation}\label{am_est}
	\hat{\bm{\theta}}_W = {\mathop{\arg\min}_{{\bm{\theta}}}} \; D_W
	\left[ \hat{p}_{\rm{emp}}, p ({\bm{x}}, {\bm{\theta}}) \right].
\end{equation}
See the end of Section~\ref{sec:ellip}.
The properties of $\hat{\bm{\theta}}_W$ were studied in detail by \cite{AM2022} in the case of the one-dimensional location-scale model. 



\section{Affine deformation model}
\label{sec:4}
Now, we focus on the affine deformation model.
Let $f({\bm{z}})$ be a standard probability density function satisfying (\ref{eq:am2}), (\ref{eq:am3}), and (\ref{eq:am4}).  
To define $M_f$, we use affine deformation of ${\bm{x}}$ to ${\bm{z}}$ by
\begin{equation}
	{\bm{z}} = \Lambda ({\bm{x}}-{\bm{\mu}}),
\end{equation}
where ${\bm{\mu}}$ is a vector representing shift of location and $\Lambda$ is a non-singular matrix.  
Hence, ${\bm{\theta}}= ({\bm{\mu}}, \Lambda)$ is $m=d^2+d$ dimensional. 
The model $M_f$ is defined from
\begin{equation}
	p ({\bm{x}}, {\bm{\theta}}) d{\bm{x}} =
	f({\bm{z}}) d{\bm{z}},
\end{equation}
that is
\begin{equation}
	p ({\bm{x}}, {\bm{\theta}}) = |\Lambda| f (\Lambda ({\bm{x}}-{\bm{\mu}})),
\end{equation}
satisfying
\begin{eqnarray}
	\int {\bm{x}} p ({\bm{x}}, {\bm{\theta}})
	d{\bm{x}} &=& {\bm{\mu}}, \\
	\int {\bm{x}} {\bm{x}}^{\top} p ({\bm{x}}, {\bm{\theta}})
	d{\bm{x}} &=& \Lambda^{-2} + {\bm{\mu}}{\bm{\mu}}^{\top}. 
\end{eqnarray}
This is a generalization of the location-scale model, which is simply obtained by putting $\Lambda = (1/\sigma) I$, with $\sigma$ being the scale factor.  It should be noted that $\Lambda$ is decomposed as $\Lambda = U DO$, where $U$ and $O$ are orthogonal matrices and $D$ is a positive diagonal matrix.  
In the following, we denote the log probability of standard shape $f$ by 
\begin{equation}
	l ({\bm{z}}) = \log f ({\bm{z}}).  
\end{equation} 





As we discussed in Introduction, the set of all standard shape functions  ${\cal{F}}_S = \{ f \}$ does not form a manifold but has an interesting topological structure due to the rotational invariance for some $f$.  
For each standard shape function $f \in {\cal{F}}_S$, an affine deformation model $M_f$  parameterized by ${\bm{\theta}} = ({\bm{\mu}}, \Lambda)$ is attached.  Thus, ${\cal{F}}$ is decomposed into the direct product of ${\cal{F}}_S$ and $M_f$,
\begin{equation}
	{\cal{F}} = {\cal{F}}_S \times M_f. 
\end{equation}
For any $f$, $M_f$ has cone structure parameterized by $({\bm{\mu}}, D, U, O)$, where $\Lambda = U D O$ and $D$ is a diagonal matrix with diagonal elements $d_i>0$.
Thus, $D$ can be identified with a vector in the open positive quadrant $\bm{R}_+^d$ of ${\bm{R}}^d$, which has the cone structure. 
Since ${\bm{\mu}} \in {\bm{R}}^d$, and $U,O \in {\cal{O}}(d)$, we have the decomposition
\begin{equation}
    M_f = {\bm{R}}^d \times \bm{R}_+^{d} \times {\cal{O}}(d) \times {\cal{O}}(d).
\end{equation}
See \cite{Takatsu2012} for the cone structure of ${\cal{F}}$.  When $f$ is Gaussian, its structure is studied in detail by \cite{Takatsu2011}.

When $p({\bm{x}})$ belongs to $M_f$, the waveform of $p({\bm{x}})$ is said to be equivalent to that of $f$.  $M_f$ consists of the distributions of all equivalent waveforms.
All ellipsoidal shapes are equivalent to a spherical shape.  A family of special parallel-piped shapes are equivalent to a cubic form (see Fig. 5).  Therefore, our model is useful for separating the effect of the shape from location and affine deformation.  

% Figure environment removed















We may consider subclasses of the transformation model.
One simple example is the location model, in which $\Lambda$ is fixed to the identity matrix $I$. A stronger theorem is known in such a simple model \citep{Givens}.
In our context, it can be expressed as follows.

\begin{proposition}\upshape
	Wasserstein geometry gives an orthogonal decomposition of the shape and locations,
	\begin{equation}
		D^W \left[ 
		f_1 \left({\bm{x}}-{\bm{\mu}}_1 \right),  f_2 \left({\bm{x}}-{\bm{\mu}}_2 \right)
		\right] = D^W \left[f_1({\bm{x}}), f_2 ({\bm{x}}) \right] + \|{\bm{\mu}}_1-{\bm{\mu}}_2 \|^2.
	\end{equation}
\end{proposition}









\section{Elliptically symmetric deformation model}\label{sec:ellip}
Here, we focus on deformation models that are elliptically symmetric:
\begin{equation}
	\label{eq:am61}
	p ({\bm{x}}, {\bm{\theta}}) =
	|\Lambda| g(\| \Lambda ({\bm{x}}-{\bm{\mu}}) \|),
\end{equation}
where $f(\bm{z})=g(\| \bm{z} \|)$ satisfies the standard density conditions (\ref{eq:am2}), (\ref{eq:am3}), and (\ref{eq:am4}).
Note that $f(\bm{z})=f(UDO \bm{x})=g(\| DO \bm{x} \|)$ does not depend on $U$ and thus the parameter is reduced to $(\bm{\mu},D,O)$ in this case.

First, we consider the $F$-estimator $\hat{\bm{\theta}}_F$ (maximum likelihood estimator).
The log-likelihood is given by
\begin{equation}
	\log p ({\bm{x}}, {\bm{\theta}})  
	= \log |\Lambda| + \log g (\| \Lambda ({\bm{x}}- {\bm{\mu}}) \|).
\end{equation}
When there are $n$ observations ${\bm{x}}_1, \cdots, {\bm{x}}_n$, summation is taken over them so that we have the likelihood equations
\begin{equation}
	\sum^n_{j=1} \partial_{\bm{\theta}} \log p \left({\bm{x}}_j, {\bm{\theta}} \right) = 0.
\end{equation}
The solution $\hat{\bm{\theta}}_F$ strongly depends of the shape $g$.

Contrary to this, the $W$-estimator $\hat{\bm{\theta}}_W$ does not depend on the shape $g$ as follows.

\begin{lemma}\label{lem:quad}
\begin{align*}
	\nabla_x \left( \frac{1}{2} x^{\top} A x + b^{\top} x + c \right) = \frac{A+A^{\top}}{2} x + b, \quad \Delta_x \left( \frac{1}{2} x^{\top} A x + b^{\top} x + c \right) = {\rm tr} (A).
\end{align*}
\end{lemma}
\begin{proof}
Straightforward calculation.
Note that $(A+A^{\top})/2 \neq A$ when $A$ is not symmetric.
\end{proof}

\begin{lemma}\label{lem:sylvester}
Let $A,B \in \mathbb{R}^{d \times d}$ be symmetric matrices.
If $A$ is positive semidefinite, then the Sylvester equation $AX + XA = B$ has the unique solution $X$, which satisfies $X^{\top}=X$ and ${\rm tr} (X) = {\rm tr} (A^{-1} B)/2$.
\end{lemma}
\begin{proof}
    From the positive semidefiniteness of $A$, the spectra of $A$ and $-A$ are disjoint. Thus, from Theorem~VII.2.1 of \cite{Bhatia}, the Sylvester equation $AX + XA = B$ has a unique solution.
    
    Let $X$ be the solution of the Sylvester equation.
    From $A^{\top} = A$, we have $A X^{\top} + X^{\top} A = (AX+XA)^{\top} = B^{\top} = B$, which means that $X^{\top}$ is also a solution of the Sylvester equation.
    Since the solution is unique, it implies $X^\top = X$.
    Also, from the positive semidefiniteness of $A$ and $AX + XA = B$, we have $X + A^{-1}XA = A^{-1}B$.
    Taking the trace and using ${\rm tr} (A^{-1}XA)={\rm tr} (X)$, we obtain ${\rm tr} X = {\rm tr} (A^{-1} B)/2$.
\end{proof}

\begin{theorem}\upshape
	\label{theorem_moment}
    For the elliptically symmetric deformation model \eqref{eq:am61}, the Wasserstein score functions are quadratic. 
    Specifically, the Wasserstein score function for $\mu_i$ is
\begin{align*}
	S_{\mu_i}^{{W}} (\bm{x},\bm{\theta}) = x_i-\mu_i,
\end{align*}
     and the Wasserstein score function for $\Lambda_{ij}$ is
\begin{align*}
    S_{\Lambda_{ij}}^W(\bm{x},\bm{\theta}) = \frac{1}{2} \bm{x}^{\top} A \bm{x} + b^{\top} \bm{x} - {\rm E}_{\bm{\theta}} \left[ \frac{1}{2} \bm{x}^{\top} A \bm{x} + b^{\top} \bm{x} \right],
\end{align*}
where $A$ is the unique solution of the Sylvester equation $\Lambda^2 A + A \Lambda^2 = -L$ and $b=-A \mu$.
\end{theorem}
\begin{proof}
We show that the above $S^W$'s satisfy the Poisson equation \eqref{eq:poisson} directly.

First, we consider the mean parameter $\mu_i$.
Let $\bm{e}_i$ be the $i$-th standard unit vector.
From \eqref{eq:am61},
\begin{align*}
	\frac{\partial}{\partial \mu_i} \log p(\bm{x},\bm{\theta}) = -\frac{\partial}{\partial x_i} \log p(\bm{x},\bm{\theta}) = -\nabla_{\bm{x}} \log p(\bm{x},\bm{\theta})^{\top} \bm{e}_i.
\end{align*}
Also, from Lemma~\ref{lem:quad},
\begin{align*}
	\nabla_{\bm{x}} (x_i-\mu_i) = \bm{e}_i, \quad \Delta_{\bm{x}} (x_i-\mu_i) = 0.
\end{align*}
Therefore,
\begin{align*}
	\nabla_{\bm{x}} \log p ({\bm{x}}, {\bm{\theta}})^{\top} \nabla_{\bm{x}} (x_i-\mu_i) + \Delta_{\bm{x}} (x_i-\mu_i) +
	\frac{\partial}{\partial \mu_i} \log
	p({\bm{x}}, {\bm{\theta}}) = 0.
\end{align*}
Thus, the Wasserstein score function for the mean parameter $\mu_i$ is
\begin{align*}
	S_{\mu_i}^{{W}} (\bm{x},\bm{\theta}) = x_i-\mu_i.
\end{align*}

Next, we consider the deformation parameter $\Lambda_{ij}$.
Since
\begin{align*}
	\frac{\partial}{\partial \Lambda_{ij}} \| \Lambda (\bm{x}-\mu) \| &= \frac{1}{2} \| \Lambda (\bm{x}-\mu) \|^{-1} \frac{\partial}{\partial \Lambda_{ij}} (\bm{x}-\mu)^{\top} \Lambda^2 (\bm{x}-\mu) \\
    &= \frac{1}{2} \| \Lambda (\bm{x}-\mu) \|^{-1} (\bm{x}-\mu)^{\top} \frac{\partial \Lambda^2}{\partial \Lambda_{ij}} (\bm{x}-\mu) \\
    &= \frac{1}{2} \| \Lambda (\bm{x}-\mu) \|^{-1} (\bm{x}-\mu)^{\top} (\Lambda \bm{e}_i \bm{e}_j^{\top} + \bm{e}_i \bm{e}_j^{\top} \Lambda) (\bm{x}-\mu),
\end{align*}
we have
\begin{align*}
	\frac{\partial}{\partial \Lambda_{ij}} \log p(\bm{x}, \theta) &= \frac{\partial}{\partial \Lambda_{ij}} \log \det \Lambda + \frac{\partial}{\partial \Lambda_{ij}} \log g (\| \Lambda (\bm{x}-\mu) \|) \\
    &= (\Lambda^{-1})_{ij} + \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{g (\| \Lambda (\bm{x}-\mu) \|)} \frac{\partial}{\partial \Lambda_{ij}} \| \Lambda (\bm{x}-\mu) \| \\
    &= (\Lambda^{-1})_{ij} + \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{2 \| \Lambda (\bm{x}-\mu) \| g (\| \Lambda (\bm{x}-\mu) \|)} (\bm{x}-\mu)^{\top} (\Lambda \bm{e}_i \bm{e}_j^{\top} + \bm{e}_i \bm{e}_j^{\top} \Lambda) (\bm{x}-\mu) \\
	&= - \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{\| \Lambda (\bm{x}-\mu) \| g (\| \Lambda (\bm{x}-\mu) \|)} \left( -\frac{1}{2} \bm{x}^{\top} L \bm{x} + \bm{x}^{\top} L \mu - \frac{1}{2} \mu^{\top} L \mu \right) + (\Lambda^{-1})_{ij},
\end{align*}
where $L=\Lambda \bm{e}_i \bm{e}_j^{\top} + \bm{e}_i \bm{e}_j^{\top} \Lambda$.
Let 
\begin{align*}
    S(\bm{x}) = \frac{1}{2} \bm{x}^{\top} A \bm{x} + b^{\top} \bm{x} - {\rm E}_{\theta} \left[ \frac{1}{2} \bm{x}^{\top} A \bm{x} + b^{\top} \bm{x} \right],
\end{align*}
where $A$ is the unique solution of the Sylvester equation $\Lambda^2 A + A \Lambda^2 = -L$ and $b=-A \mu$.
From Lemma~\ref{lem:quad} and Lemma~\ref{lem:sylvester},
\begin{align*}
	\Delta_{\bm{x}} S(\bm{x}) = {\rm tr} A = -\frac{1}{2} {\rm tr} (\Lambda^{-2} L) = -\frac{1}{2} {\rm tr} (\Lambda^{-2} (\Lambda \bm{e}_i \bm{e}_j^{\top} + \bm{e}_i \bm{e}_i^{\top} \Lambda)) = -(\Lambda^{-1})_{ij}.
\end{align*}
Also,
\begin{align*}
    &\nabla_{\bm{x}} \log p ({\bm{x}}, {\bm{\theta}})^{\top} \nabla_{\bm{x}} S(\bm{x}) \\ 
    =& \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{g (\| \Lambda (\bm{x}-\mu) \|)} \nabla_{\bm{x}} (\| \Lambda (\bm{x}-\mu) \|)^{\top} (A\bm{x}+b) \\
	=& \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{\| \Lambda (\bm{x}-\mu) \| g (\| \Lambda (\bm{x}-\mu) \|)} (\Lambda^2 (\bm{x}-\mu))^{\top} (A\bm{x}+b) \\
	=& \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{\| \Lambda (\bm{x}-\mu) \| g (\| \Lambda (\bm{x}-\mu) \|)} \left( \frac{1}{2} \bm{x}^{\top} (\Lambda^2 A + A \Lambda^2) \bm{x} + \bm{x}^{\top} (\Lambda^2 b-A \Lambda^2 \mu) -\mu^{\top} \Lambda^2 b \right) \\
	=& \frac{g' (\| \Lambda (\bm{x}-\mu) \|)}{\| \Lambda (\bm{x}-\mu) \| g (\| \Lambda (\bm{x}-\mu) \|)} \left( -\frac{1}{2} \bm{x}^{\top} L \bm{x} + \bm{x}^{\top} L \mu - \frac{1}{2} \mu^{\top} L \mu \right),
\end{align*}
where we used
\begin{align*}
    \nabla_{\bm{x}} (\| \Lambda (\bm{x}-\mu) \|) &= \frac{1}{2 \| \Lambda (\bm{x}-\mu) \|} \nabla_{\bm{x}} (\| \Lambda (\bm{x}-\mu) \|^2) \\
    &= \frac{1}{2 \| \Lambda (\bm{x}-\mu) \|} \nabla_{\bm{x}} ((\bm{x}-\mu)^{\top} \Lambda^2 (\bm{x}-\mu)) \\
    &= \frac{1}{\| \Lambda (\bm{x}-\mu) \|} \Lambda^2 (\bm{x}-\mu).
\end{align*}
Therefore,
\begin{align*}
	\nabla_{\bm{x}} \log p ({\bm{x}}, {\bm{\theta}})^{\top} \nabla_{\bm{x}} S(\bm{x}) + \Delta_{\bm{x}} S(\bm{x}) +
	\frac{\partial}{\partial \Lambda_{ij}} \log
	p({\bm{x}}, {\bm{\theta}}) = 0,
\end{align*}
which means that $S(\bm{x})$ is the Wasserstein score function for $\Lambda_{ij}$.

\end{proof}

In Theorem~\ref{theorem_moment}, we considered $d^2$ elements of $\Lambda$ as independent parameters.
If we impose the symmetric constraint on $\Lambda$, then the Wasserstein score function for the off-diagonal elements of $\Lambda$ becomes the twice of the one in Theorem~\ref{theorem_moment}.

\begin{corollary}\upshape \label{cor_ellip}
    For the elliptically symmetric deformation model \eqref{eq:am61}, the $W$-estimator $\hat{\bm{\theta}}_W$ is the second-order moment estimator irrespective of the waveform $f(\bm{z})=g(\| \bm{z} \|)$.  
\end{corollary}
\begin{proof}
    From Theorem~\ref{theorem_moment}, the Wasserstein functions are quadratic functions of $\bm{x}$.
    Thus, the estimating equation for the $W$-estimator is linear (non-singular) with respect to the first and second-order empirical moments of $\bm{x}$.
    Also, the number of estimated parameters is equal to the number of first and second-order moments.
    Therefore, the $W$-estimator coincides with the second-order moment estimator.
\end{proof}

Note that \cite{Gelbrich} showed that the $L^2$-Wasserstein divergence for the elliptically symmetric deformation model \eqref{eq:am61} does not depend on the waveform and is given by
\[
	D_W(p(\bm{x},\bm{\theta_1}),p(\bm{x},\bm{\theta_2})) = \| \mu_1 - \mu_2 \|^2 + {\rm tr} (\Lambda_1^{-2} + \Lambda_2^{-2} -  2 (\Lambda_1^{-1} \Lambda_2^{-2} \Lambda_1^{-1})^{1/2}).
\]
It is an interesting future problem to derive the Wasserstein score function and $W$-estimator for general affine deformation models.

Regarding the geometric structure of the elliptically symmetric deformation model \eqref{eq:am61}, we obtain the following.
See Figure~\ref{fig1}.

\begin{theorem}\upshape
	When $f$ is spherically symmetric, the model $M_f$ is orthogonal to ${\cal{F}}_S$ at the origin ${\bm{\mu}}=0, \Lambda=I$ of $M_f$ with respect to the Wasserstein metric.
\end{theorem}
\begin{proof}\upshape
	Let $\delta p({\bm{x}})$ be a tangent vector of ${\cal{F}}_S$ at the origin.
	Since all $p({\bm{x}})$ in  ${\cal{F}}_S$ satisfy the standard conditions (\ref{eq:am2}), (\ref{eq:am3}), and (\ref{eq:am4}), $\delta p({\bm{x}})$ is orthogonal to any quadratic function of ${\bm{x}}$.  
    Since the $W$-score functions of $M_S$ are quadratic functions from Theorem~\ref{theorem_moment}, it implies that $\delta p({\bm{x}})$ is orthogonal to the Waserstein functions of ${\bm{x}}$, which form the basis of the tangent space of $M_f$, with respect to the Wasserstein metric.
\end{proof}

Note that $M_f$ is orthogonal to ${\cal{F}}_S$ only when $f$ is Gaussian in the case of the Fisher-Rao metric.











Here, we discuss the relation between the current $W$-estimator and the estimator \eqref{am_est}.
\cite{AM2022} studied the estimator $\hat{{\bm \theta}}_W$ in \eqref{am_est} for the one-dimensional location-scale model by using the order statistics $x_{(i)}$.  
This estimator minimizes the Wasserstein distance between the empirical distribution and the model.
Here, we show that the estimator $\hat{\bm{\theta}}_W=(\hat{{\bm \mu}}_W,\hat{\sigma}_W)$ given by \eqref{am_est} is asymptotically equivalent to the $W$-estimator, which coincides with the second-order moment estimator from Theorem~\ref{theorem_moment}.  
We assume $\mu=0$ without loss of generality.  
The estimator \eqref{am_est} of the location is 
\begin{equation}
    \hat{\mu}_W = \frac 1n \sum_{i=1}^n x_{(i)} =
    \frac 1n \sum_{i=1}^n x_i,
\end{equation}
which is the empirical mean and coincides with the moment estimator.  
Also, the estimator \eqref{am_est} of the scale is
\begin{equation}
    \hat{\sigma}_W = \sum_{i=1}^n k_i x_{(i)},
\end{equation}
where
\begin{equation}
    k_i = \int^{z_i}_{z_{i-1}} zf(z) dz.
\end{equation}
Here, $z_i$ is the $i$-th equipartition point of $f(z)$ defined by
\begin{equation}
    z_i = F^{-1} \left( \frac{i}{n} \right),
\end{equation}
where $F$ is the cumulative distribution function of $f(z)$.  
From $\mu=0$, we have $x_{(i)} \approx \sigma z_i$ asymptotically.
Hence, 
\begin{equation}
    k_i \approx \frac 1n z_i \approx \frac 1n \frac{x_{(i)}}{\sigma},
\end{equation}
which leads to
\begin{equation}
    \hat{\sigma}_W = \sum_{i=1}^n k_i x_{(i)} \approx \frac{1}{n \sigma} \sum_{i=1}^n x^2_{(i)} = \frac{1}{n \sigma} \sum_{i=1}^n x^2_{i}.
\end{equation}
Since $\hat{\sigma}_W \approx \sigma$ asymptotically,
\begin{equation}
    \hat{\sigma}_W^2 \approx \frac{1}{n} \sum_{i=1}^n x^2_{i}.
\end{equation}
This shows that $\hat{\bm{\theta}}_W=(\hat{{\bm \mu}}_W,\hat{\sigma}_W)$ asymptotically coincides with the second-order moment estimator.

\section{W-efficiency implies robustness to waveform change}\label{sec:robust}
Following \cite{LZ2019}, we define the Wasserstein covariance ($W$-covariance) matrix ${\rm Var}_{\theta}^{\mathrm{W}} [\hat{\bm{\theta}}]$ of an estimator $\hat{\bm{\theta}}$ by the positive semidefinite matrix given by
\begin{align}
	{\rm Var}_{\theta}^{\mathrm{W}} [\hat{\bm{\theta}}] = ({\rm E}_{\theta} [ (\nabla_{\bm{x}} \hat{\bm{\theta}}_a)^{\top} (\nabla_{\bm{x}} \hat{\bm{\theta}}_b) ])_{ab}. \label{Wcov}
\end{align}
\cite{LZ2019} showed the Wasserstein--Cramer--Rao inequality
\begin{equation}
	\label{eq:WCR}
{\rm Var}_{\theta}^{\mathrm{W}} (\hat{\bm{\theta}}) \succeq \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\bm{\theta}}] \right)^{\top} G_{\mathrm{W}} (\theta)^{-1} \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\bm{\theta}}] \right),
\end{equation}  
where
\begin{align*}
	\frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\bm{\theta}}] := \left( \frac{\partial}{\partial \theta_j} {\rm E}_{\theta} [ \hat{\bm{\theta}}_i ] \right)_{ij}.
\end{align*}
A consistent estimator $\hat{\bm{\theta}}$ is said to be Wasserstein efficient ($W$-efficient) if its Wasserstein covariance asymptotically satisfies \eqref{eq:WCR} with equality.   
We give a proof of the Wasserstein--Cramer--Rao inequality based on the Cauchy--Schwarz inequality in the Appendix.



We show that the $W$-covariance can be viewed as a measure of robustness of an estimator to noise.  
Suppose that $X \sim p ({\bm{x}}, {\bm{\theta}})$ and we estimate $\bm{\theta}$ from noisy observation $X+Z$ where $X$ and $Z$ are independent, ${\rm E} [Z] = 0$ and ${\rm Var} [Z] = \sigma^2 I$ with $\sigma^2$ sufficiently small.


\begin{theorem}
The Wasserstein covariance satisfies
\begin{align}
{\rm Var}^{\mathrm{W}}_{\theta} [\hat{\bm{\theta}}]_{ab} =& \lim_{\sigma^2 \to 0} \frac{{\rm Var}_{\theta} [\hat{\bm{\theta}} (X+Z)]_{ab} - {\rm Var}_{\theta} [\hat{\bm{\theta}}(X)]_{ab}}{\sigma^2} \\
& \quad - \frac{1}{2} \left( {\rm Cov}_{\theta} [ \hat{\bm{\theta}}_a (X), \Delta \hat{\bm{\theta}}_b (X) ] + {\rm Cov}_{\theta} [ \hat{\bm{\theta}}_b (X), \Delta \hat{\bm{\theta}}_a (X) ] \right),
\end{align}
where $\Delta$ is the Laplacian.
In particular, when $\hat{\bm{\theta}}$ is quadratic in $\bm{x}$,
\begin{equation}
{\rm Var}^{\mathrm{W}}_{\theta} [\hat{\bm{\theta}}] = \lim_{\sigma^2 \to 0} \frac{{\rm Var}_{\theta} [\hat{\bm{\theta}} (X+Z)] - {\rm Var}_{\theta} [\hat{\bm{\theta}}(X)]}{\sigma^2}.
\end{equation}
\end{theorem}
\begin{proof}
By Taylor expansion, for sufficiently small $z$,
\begin{align*}
    \hat{\bm{\theta}}_a (x+z) \approx \hat{\bm{\theta}}_a(x) + \sum_i \frac{\partial \hat{\bm{\theta}}_a}{\partial x_i}(x) z_i + \frac{1}{2} \sum_{i,j} \frac{\partial^2 \hat{\bm{\theta}}_a}{\partial x_i \partial x_j}(x) z_i z_j.
\end{align*}
From ${\rm E} [Z] = 0$, ${\rm Var} [Z] = \sigma^2 I$ and the independence of $X$ and $Z$,
\begin{align*}
    {\rm E}_{\theta} [\hat{\bm{\theta}}_a (X+Z)] &= {\rm E}_{\theta} [\hat{\bm{\theta}}_a(X)] + \sum_i {\rm E}_{\theta} \left[ \frac{\partial \hat{\bm{\theta}}_a}{\partial x_i} (X) \right] {\rm E}[z_i] + \frac{1}{2} \sum_{i,j} {\rm E}_{\theta} \left[ \frac{\partial^2 \hat{\bm{\theta}}_a}{\partial x_i \partial x_j} (X) \right] {\rm E} [z_i z_j] \\
	&= {\rm E}_{\theta} [\hat{\bm{\theta}}_a(X)] + \frac{1}{2} {\rm E}_{\theta} [ \Delta\hat{\bm{\theta}}_a (X) ] \sigma^2.
\end{align*}
Also,
\begin{align*}
    {\rm E}_{\theta} [\hat{\bm{\theta}}_a (X+Z) \hat{\bm{\theta}}_b (X+Z)] &= {\rm E}_{\theta} [\hat{\bm{\theta}}_a(X) \hat{\bm{\theta}}_b(X)] + \frac{1}{2} {\rm E}_{\theta} [ \hat{\bm{\theta}}_a(X) \Delta \hat{\bm{\theta}}_b (X) + \hat{\bm{\theta}}_b(X) \Delta \hat{\bm{\theta}}_a (X) ] \sigma^2 \\
    & \quad + {\rm E}_{\theta} [ (\nabla \hat{\bm{\theta}}_a (X))^{\top} (\nabla \hat{\bm{\theta}}_b(X)) ] \sigma^2 + o(\sigma^2) \\
    &= {\rm E}_{\theta} [\hat{\bm{\theta}}_a(X) \hat{\bm{\theta}}_b(X)] + \frac{1}{2} {\rm E}_{\theta} [ \hat{\bm{\theta}}_a(X) \Delta \hat{\bm{\theta}}_b (X) + \hat{\bm{\theta}}_b(X) \Delta \hat{\bm{\theta}}_a (X) ] \sigma^2 \\
    & \quad + {\rm Var}_{\theta} [\hat{\bm{\theta}}]_{ab} \sigma^2 + o(\sigma^2).
    \end{align*}
Then,
\begin{align*}
	&{\rm Var}_{\theta} [\hat{\bm{\theta}} (X+Z)]_{ab} \\
    =& {\rm E}_{\theta} [\hat{\bm{\theta}}_a (X+Z) \hat{\bm{\theta}}_b (X+Z)] - {\rm E}_{\theta} [\hat{\bm{\theta}}_a (X+Z)] {\rm E}_{\theta} [\hat{\bm{\theta}}_b (X+Z)] \\
	=& {\rm Var}_{\theta} [\hat{\bm{\theta}}(X)]_{ab} + {\rm Var}^{\mathrm{W}}_{\theta} [\hat{\bm{\theta}}]_{ab} \sigma^2 + \frac{1}{2} \left( {\rm Cov}_{\theta} [ \hat{\bm{\theta}}_a (X), \Delta \hat{\bm{\theta}}_b (X) ] + {\rm Cov}_{\theta} [ \hat{\bm{\theta}}_b (X), \Delta \hat{\bm{\theta}}_a (X) ] \right) \sigma^2 + o(\sigma^2),
\end{align*}
where the covariance term vanishes when $\hat{\bm{\theta}}$ is quadratic in $\bm{x}$.
\end{proof}

Thus, the Wasserstein covariance quantifies the robustness of estimators against the waveform change due to noise.
In particular, from Corollary~\ref{cor_ellip}, the Wasserstein covariance quantifies the robustness of the Wasserstein estimator for elliptically symmetric deformation models.
The Wasserstein--Cramer--Rao inequality gives the limit of robustness.
It is an interesting future problem to investigate when the Wasserstein estimator attains the Wasserstein efficiency.
Note that the Fisher efficiency (in finite samples), which is defined by the conventional Cramer--Rao inequality, is attained by the maximum likelihood estimator if and only if the estimand is the expectation parameter of an exponential family.






\section{Contribution of waveform $f$ to $F$- and $W$-efficiencies}
\label{sec:waveform}

We study how the waveform $f$ contributes to the $F$-efficiency and $W$-efficiency of estimators.  We first show the following theorem.

\begin{theorem}\upshape
	The Gaussian waveform $f$ has the following properties: When and only when $f$ is Gaussian, $F$-estimator $\hat{\bm{\theta}}_F$ and $W$-estimator $\hat{\bm{\theta}}_W$ are identical, and they are both $F$- and $W$-efficient.
\end{theorem}

\begin{proof}\upshape
	For the standard Gaussian $f$,
	\begin{equation}
		f({\bm{z}}) = \frac 1{(\sqrt{2 x})^d} \exp 
		\left\{
		-\frac{|{\bm{z}}|^2}2
		\right\},
	\end{equation}        
	the $F$-score functions are
	\begin{equation}
		S_F ({\bm{x}}, {\bm{\theta}}) =
		\partial_{\bm{\theta}}
		\log p({\bm{x}}, {\bm{\theta}}) =
		-{\bm{z}} \cdot \frac{\partial {\bm{z}}}{\partial {\bm{\theta}}}.
	\end{equation}
	Hence, the $F$-score functions are quadratic with respect to ${\bm{x}}$.  So they are equivalent to the $W$-score functions.  On the contrary, when the score functions are quadratic with respect to ${\bm{x}}$, the waveform $f$ is Gaussian. 
\end{proof}

When $f$ is not Gaussian, the $F$-efficiency of $\hat{\bm{\theta}}_W$ and $W$-efficiency of $\hat{\bm{\theta}}_F$ degrade.  When $f$ is close to Gaussian, their cumulants of order larger than two are small.  We use the Gram-Charlie expansion of $f$ to represent how cumulants of the waveform $f$ contribute to the $F$-efficiency of $\hat{\bm{\theta}}_W$.

We study how waveform $f$ contributes to the amounts of Fisher information $g_F$ and Wasserstein information $g_W$, when $f$ is close to the Gaussian distribution.  We use the Gram-Charlie expansion \citep{McCullagh}
\begin{equation}
	f({\bm{x}}) = S(\bm{x}) \left\{
	1+\frac{\kappa_3}{3!} \circ h_3 ({\bm{x}}) +
	\frac{\kappa_4}{4!} \circ h_4 ({\bm{x}})
	\right\},
\end{equation}
where $\kappa_i$ are the $i$th order cumulant tensors and $h_i({\bm{x}})$ are the $i$th order tensorial Hermite polynomials, and $\circ$ denotes the tensorial inner product such as
\begin{equation}
	\kappa_3 \circ h_3 = \sum \kappa_{3, ijk}
	h_3^{ijk}.
\end{equation}
Note that $\kappa_1=0, \kappa_2=1$ when $f$ is the standard Gaussian distribution $\phi$,
\begin{equation}
	\phi({\bm{x}}) = \frac 1{(\sqrt{2 \pi)}} \exp \left\{
	-\frac{{\bm{x}} \cdot {\bm{x}}}2
	\right\}.
\end{equation}
The logarithm of $p({\bm{x}}, {\bm{\theta}})$ is expanded as
\begin{equation}
	l({\bm{x}}, {\bm{\theta}}) = -\log
	|\Lambda| + \left\{-\frac 12 |{\bm{z}}|^2
	+ \frac{\kappa_3}{3!} \circ h_3 ({\bm{z}})
	+ \frac{\kappa_4}{4!} \circ h_4 ({\bm{z}})
	\right\},
\end{equation}
in terms of ${\bm{z}}= \Lambda ({\bm{x}}-{\bm{\mu}})$, where higher order terms of $\kappa_3$ and $\kappa_4$ and terms of $\kappa_i$, $i \ge 5$ are neglected.

Fisher information $g_F$ is given by
\begin{equation}
	g_F = -{\rm{E}} \left[ 
	\partial_{\bm{\theta}} \partial_{\bm{\theta}} l
	({\bm{x}}, {\bm{\theta}})
	\right].
\end{equation}
We have
\begin{eqnarray}
	\partial_{\bm{\theta}} l &=&
	\frac{\partial l}{\partial {\bm{z}}} \cdot
	\frac{\partial {\bm{z}}}{\partial_{\bm{\theta}}} \\
	\partial_{\bm{\theta}}\partial_{\bm{\theta}}l
	&=&
	\frac{\partial^2 l}{\partial {\bm{z}}\partial {\bm{z}}} \cdot
	\left(\frac{\partial {\bm{z}}}{\partial_{\bm{\theta}}} 
	\frac{\partial {\bm{z}}}{\partial_{\bm{\theta}}} 
	\right) + \frac{\partial l}{\partial {\bm{z}}} \cdot 
	\frac{\partial^2 {\bm{z}}}{\partial_{\bm{\theta}}\partial_{\bm{\theta}}},
\end{eqnarray}
where $\cdot$ denotes the inner product with respect to the indices of ${\bm{z}}$.  From the derivative of Hermite polynomials, we have
\begin{eqnarray}
	\partial_{\bm{z}} l &=& \Lambda^{-1}
	\left\{
	-{\bm{z}} + \frac{\kappa_3}2 \circ h_2({\bm{z}}) + \frac{\kappa_4}6 \circ h_3
	({\bm{z}})
	\right\} \\
	\partial_{\bm{z}}\partial_{\bm{z}} l &=& 
	\Lambda^{-1} 
	\left\{
	-I + \kappa_3 \circ h_1 ({\bm{z}}) +
	\frac{\kappa_4}2 \circ h_2({\bm{z}})
	\right\},
\end{eqnarray}
where $\kappa_i \circ h_j ({\bm{z}})$ are tensorial polynomials of ${\bm{z}}$.  On the other hand, $\partial {\bm{z}}/\partial {\bm{\theta}}$ are given by
\begin{eqnarray}
	\frac{\partial {\bm{z}}}{\partial {\bm{\mu}}} &=& \Lambda, \\
	\frac{\partial {\bm{z}}}{\partial {\bm{\Lambda}}}
	&=& {\bm{x}}-{\bm{\mu}}, 
\end{eqnarray}

In order to avoid complicated tensorial calculations, we study only the case of $d=1$, that is the location-scale model.  We show the results after simple calculations.
\begin{eqnarray}
	\frac{\partial^2 l}{\partial \mu \partial \mu} 
	&=& \lambda^2 \left\{
	-1+ \kappa_3 z + \frac{\kappa_4}2
	\left( z^2-1 \right)
	\right\} \\
	\frac{\partial^2 l}{\partial \lambda \partial \mu} 
	&=& \lambda^2 \left\{
	-2z + \frac{\kappa_3}2 \left( 3z^2-1 \right)
	+ \frac{\kappa_4}3 \left( 2z^3-3z \right)
	\right\} \\
	\frac{\partial^2 l}{\partial \lambda \partial \lambda} &=& \lambda^2 \left\{
	\left( -3z^2+1 \right) + \frac{\kappa_3}2
	\left( 4z^3-3z \right) + \frac{\kappa_4}6
	\left( 4z^4-6z^2 \right)
	\right\}.
\end{eqnarray}
Here, $z$ is subject to $f(z)$, so we have
\begin{equation}
	g_F (\mu, \lambda) = \lambda^2
	\left[ 
	\begin{array}{cc}
		1 & -\kappa_3  \\
		-\kappa_3 &  2-\kappa_4
	\end{array}
	\right].
\end{equation}
This shows how $g_F (\mu, \lambda)$ deviates from the Gaussian case depending on $\kappa_3$ and $\kappa_4$.

It is also interesting to consider the case when $f$ has high-frequency wavy structure. Since $F$-score functions are derivatives of the $\log$ probability, high-frequency components are sensitive to them, contributing to the $F$-metric.  However, by adding small noises to ${\bm{x}}$, those components are smoothed out. Hence the $W$-metric is insensitive to the high-frequency components.

We observe that, when $f$ includes a high frequency component such as
\begin{equation}
	f(x) = \phi({\bm{x}}) \left\{
	1+ \varepsilon \sin \tau x
	\right\},
\end{equation}
where $\varepsilon$ is small and $\tau$ is the frequency of small deviation, $\partial^2 l/\partial z^2$ has a component proportional to $\varepsilon \tau^2$.  Hence, the increment due to the high-frequency component is proportional to $\tau^2$, implying that the increment of Fisher information is proportional to $\tau^2$.  Hence, high frequency ripples of waveform $f$ increases $g_F$.

On the other hand, the $W$-estimator $\hat{\bm{\theta}}_W$ is not $F$-efficient except for the Gaussian case.  The loss of $F$-efficiency depends on the waveform $f$.  We again use the Gram-Charlie expansion and see the effect of $\kappa_3$ and $\kappa_4$, assuming they are small.  We show this only in the location-scale model with $d=1$.  Let us define the empirical moments of order $s$ by
\begin{equation}
	m_r = \frac 1n \sum^n_{i=1} x^r_i, \quad r=1, 2, \cdots
\end{equation}
$\hat{\bm{\theta}}_W$ uses only $m_1$ and $m_2$, discarding higher-order moments.  We calculate Fisher information $g_{F, W} ({\bm{\theta}})$ included in $\hat{\bm{\theta}}_W$.
The Fisher information is covariance matrix of $F$-score $\nabla l({\bm{x}}, {\bm{\theta}})$.  We classify ${\bm{x}}= \left(x_1, \cdots, x_n \right)$ into classes specified by $M= \left( m_1, m_2 \right)$, such that class $C_M$ consists of ${\bm{x}}$ having moments $s_1, s_2$
\begin{equation}
	C_M = \left\{
	{\bm{x}} \left|    
	m_1({\bm{x}}) = s_1, m_2 ({\bm{x}})
	=s_2
	\right.
	\right\}.
\end{equation}
Then, covariance of $\nabla l$ is decomposed into the sum of within-class covariance and between class covariance,
\begin{equation}
	{\rm{Cov}} [\nabla l] = {\rm{E}}_M
	[ {\rm{Cov}} [\nabla l | M]] +
	{\rm{Cov}} [{\rm{E}} [\nabla l | M]],
\end{equation}
where ${\rm{E}} [ \cdot | M ]$ and ${\rm{Cov}}[ \cdot | M ]$ denotes the conditional expectation and conditional covariance conditioned on $M$.  Since $\hat{\bm{\theta}}_W$ does not use higher-order moment information not included in $M$, the Fisher information included in $\hat{\bm{\theta}}_W$ is only the between-class covariance.  The loss of information in $\hat{\bm{\theta}}_W$ is
\begin{equation}
	\Delta g_{F, W} = {\rm{E}}_M
	\left[ 
	{\rm{Cov}}[\nabla l | M]
	\right].
\end{equation}
The conditional expectation of $\nabla l$ is
\begin{eqnarray}
	E \left[ 
	\partial_{\mu} l \left|  
	s_1, s_2
	\right.
	\right] &=& -s_1 + \frac{\kappa_3}2
	\left( s_2-1 \right) +
	\frac{\kappa_4}6 \left(
	{\rm{E}} \left[ s_3 \left| M \right. \right]
	-3s_1
	\right) \\
	E \left[ 
	\partial_{\lambda} l \left|  
	s_1, s_2
	\right.
	\right] &=& -s_2 + \frac{\kappa_3}2
	\left\{
	\left( {\rm{E}} \left[ s_3 \left| M \right. \right] \right) -s_1
	\right\} + \frac{\kappa_4}6
	\left\{ {\rm{E}} \left[ s_4 \left| M \right. \right]
	-3s_2
	\right\}.
\end{eqnarray}
Hence, the conditional covariances are
\begin{eqnarray}
	{\rm{Cov}} \left[ 
	\partial_{\mu} l \left| M \right.
	\right] &=&
	\frac{\kappa^2_4}{36} {\rm{Cov}}
	\left[ s_3 \left| M \right. \right], \\
	{\rm{Cov}} \left[ 
	\partial_{\lambda} l \left| M \right.
	\right] &=&
	\frac{\kappa^2_3}4 {\rm{Cov}}
	\left[ s_3 \left| M \right. \right]
	+\frac{\kappa^2_4}{36} {\rm{Cov}}
	\left[ s_4 \left| M \right. \right].
\end{eqnarray}

\begin{equation}
	{\rm{Cov}} \left[ 
	\partial_{\mu} l, \partial_{\lambda} l \left| M \right.
	\right] = \frac{\kappa_3 \kappa_4}{12}
	{\rm{E}} \left[ s_3 | M \right]
	{\rm{E}} \left[ s_4 | M \right].
\end{equation}
It should be noted that $s_3$ and $s_4$ are asymptotically independent of $M$, because $\left( s_1, s_2, s_3, s_4 \right)$ are jointly Gaussian and asymtptically independent.

We thus have asymptotically
\begin{eqnarray}
	{\rm{E}} \left[ s_3 \left| M \right. \right] &=&
	{\rm{E}} \left[ x^3 \right] = \frac{\kappa_3}{\lambda^3}, \\
	{\rm{E}} \left[ s_4 \left| M \right. \right] &=&
	{\rm{E}} \left[ x^4 \right] =  \frac{3 \left(1+\kappa_4 \right)}{\lambda^4} +
	\frac 6{\lambda^2} \mu^2 + \mu^4,
\end{eqnarray}
and so on.  Summing up all these results, we have $\Delta g_{F, W}$ in terms of $\kappa_3$ and $\kappa_4$.  


\section{Conclusion}
\label{sec:conc}

Statistical inference has so far been studied mostly based on information geometry from the Fisherian point of view, with remarkable success.  It is based on the likelihood principle, and the invariant divergence has played a fundamental role.  However, Wasserstein divergence gives another viewpoint, which is based on the geometric structure of the sample space $X$.  There are many applications of the Wasserstein geometry not only to the transportation problem but to vision analysis, signal analysis and AI in which the geometry of $X$ is sensible.

We studied the Wasserstein statistics  using the framework of \cite{LZ2019}, proving that the Wasserstein covariance  quantifies robustness against the convolutional waveform deformation due to observation noise.  
We further studied $W$-statistics of the affine deformation model.
We showed $F$-efficiency and $W$-efficiency of estimators $\hat{\bm{\theta}}_F$ and $\hat{\bm{\theta}}_W$.   We elucidated how the waveform $f$ contributes to the efficiencies.  The Gaussian distribution gives the only waveform in which the $F$-estimator and $W$-estimator coincide, satisfying both efficiencies.

The present paper is only a first step to construct general Wasserstein statistics.  In future work, we need to use more general statistical models. 
We also need to extend our approach to statistical theories of hypothesis testing, pattern classification, clustering and many other statistical problems based on the Wasserstein geometry.

\section*{Acknowledgements}
We thank Asuka Takatsu and Tomonari Sei for helpful comments.
We thank Emi Namioka for drawing the figures.
Takeru Matsuda was supported by JSPS KAKENHI Grant Numbers 19K20220, 21H05205, 22K17865 and JST Moonshot Grant Number JPMJMS2024.

\begin{thebibliography}{}
\bibitem[Amari(2016)]{Amari2016} 
	Amari, S. (2016). 
	\textit{Information Geometry and Its Applications}. 
	Springer.
\bibitem[Amari et al.(2018)]{AKO2018} 
	Amari, S., Karakida, R. \& Oizumi, M. (2018).
	Information geometry connecting Wasserstein distance and Kullback--Leibler divergence via the entropy-relaxed transportation problem. 
	\textit{Information Geometry}, \textbf{1}, 13--37.
\bibitem[Amari et al.(2019)]{AKOC2019} 
	Amari, S., Karakida, R., Oizumi, M. \& Cuturi, M. (2019).
	Information geometry for regularized optimal transport and barycenters of patterns. 
	\textit{Neural Computation}, \textbf{31}, 827--848.
\bibitem[Amari and Matsuda(2022)]{AM2022} 
	Amari, S. \& Matsuda, T. (2022).
	Wasserstein statistics in one-dimensional location scale models
	\textit{Annals of the Institute of Statistical Mathematics}, \textbf{74}, 33--47.
\bibitem[Amari and Nagaoka(2007)]{AmariNagaoka2007} 
	Amari, S. \& Nagaoka, H. (2016). 
	\textit{Methods of Information Geometry}. 
	{American Mathematical Society}.
\bibitem[Arjovsky et al.(2017)]{ACB2917} 
	Arjovsky, M., Chintala, S. \& Bottou, L. (2017). 
	Wasserstein GAN. 
	arXiv:1701.07875.
\bibitem[Ay et al.(2017)]{Ay2017} 
	Ay, N. and Jost, J. and V{\^a}n L{\^e}, H. \& Schwachh{\"o}fer, L. (2017).
	\textit{Information Geometry}. 
	Springer.
\bibitem[Bassetti et al.(2006)]{BBR2006} 
	Bassetti, F., Bodini, A. \& Regazzini, E. (2006).
	On minimum Kantorovich distance estimators. 
	\textit{Statistics \& Probability Letters}, \textbf{76}, 1298--1302.
\bibitem[Benamou and Brenier(2000)]{BB2000} Benamou, J. D. \& Brenier, Y. (2000). A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. \textit{Numerische Mathematik}, \textbf{84}, 375--393.  
\bibitem[Bernton et al.(2019)]{BJGR2019} 
	Bernton, E., Jacob, P. E., Gerber, M. \& Robert, C. P. (2019).
	On parameter estimation with the Wasserstein distance. 
	\textit{Information and Inference: A Journal of the IMA}, \textbf{8}, 657--676.
\bibitem[Bhatia(1997)]{Bhatia} 
	Bhatia, R. (1997).
	\textit{Matrix Analysis}. 
	Springer.
\bibitem[Brenier(1999)]{Brenier} 
	Brenier, Y. (1999).
	Minimal geodesics on groups of volume-preserving maps and generalized solutions of the Euler equations.
	\textit{Comm. Pure Appl. Math.}, \textbf{52}, 411--452.
\bibitem[Chentsov(1982)]{Chentsov} 
	Chentsov, N. (1982).
	\textit{Statistical decision rules and optimal inference}.
	American Mathematical Society.
\bibitem[Chen et al.(2021)]{Chen} 
	Chen, Y., Lin, Z. \& Mller, H. G. (2021).
	Wasserstein regression.
	\textit{Journal of the American Statistical Association}, accepted.
\bibitem[Fronger et al.(2015)]{FZMAP2015} 
	Fronger, C., Zhang, C., Mobahi, H., Araya-Polo, M. \& Poggio, T. (2015).
	Learning with a Wasserstein loss. 
	Advances in Neural Information Processing Systems 28 (NIPS 2015).
\bibitem[{Gelbrich(1990)}]{Gelbrich}
{Gelbrich, M.} (1990).
{On a formula for the L2 Wasserstein metric between measures on Euclidean and Hilbert spaces}.
\textit{Mathematics Nachrichten} \textbf{147}, 185--203.
\bibitem[{Givens and Shortt(1984)}]{Givens}
{Givens, C. R.} \& {Shortt, R. M.} (1984).
{A class of Wasserstein metrics for probability distributions}.
\textit{Michigan Mathematics Journal} \textbf{31}, 231--240.
\bibitem[Imaizumi et al.(2022)]{IOH2022} Imaizumi, M., Ota, H. \&  Hamaguchi, T. (2022).
   Hypothesis test and confidence analysis with Wasserstein distance on general dimension. 
   \textit{Neural Computation} \textbf{34}. 1448--1487.
\bibitem[Ito(2023)]{ito2023} Ito, S. (2023).
Geometric thermodynamics for the FokkerPlanck equation: stochastic thermodynamic links between information geometry and optimal transport. \textit{Information Geometry}.
\bibitem[Khan et al.(2022)]{KZ2022}
        Khan, G. \& Zhang, J. (2022). 
        When optimal transport meats information geometry. \textit{Infromation Geometry}, 1--32.
\bibitem[Kurose et al.(2019)]{KYA2019} 
	Kurose, T., Yoshizawa, S. \& Amari, S. (2019). 
	Optimal transportation plan with generalized entropy regularization. 
	submitted.
\bibitem[Li and Montufar(2020)]{LM2020} 
	Li, W. \& Mont\'{u}far, G. (2020).
	Ricci curvature for parametric statistics via optimal transport. 	
	\textit{Information Geometry}, \textbf{3}, 89-117.
\bibitem[Li and Zhao(2023)]{LZ2019} 
	Li, W. \& Zhao, J. (2023).
	Wasserstein information matrix. 
	\textit{Information Geometry}, accepted.
\bibitem[Matsuda and Strawderman(2021)]{Matsuda2021} 
	Matsuda, T. \& Strawderman, W. E. (2021).
	Predictive density estimation under the Wasserstein loss. 
	\textit{Journal of Statistical Planning and Inference}, \textbf{210}, 53--63.
\bibitem[McCullagh(2018)]{McCullagh} 
McCullagh, P. (2018).
\textit{Tensor Methods in Statistics}. 
Dover.
\bibitem[Montavon et al.(2015)]{MMC2015} 
	Montavon, G., M\"{u}ller, K. R. \& Cuturi, M. (2015). 
	Wasserstein training for Boltzmann machine. 
	Advances in Neural Information Processing Systems 29 (NIPS 2016).
\bibitem[Ollila and Tyler(2014)]{ollila2014regularized} 
	Ollila, E. \& Tyler, D. (2014).
	{Regularized $M$-estimators of scatter matrix}.
	\textit{IEEE Transactions on Signal Processing}, \textbf{62}, 6059--6070.
\bibitem[Otto(2001)]{Otto} 
	Otto, F. (2001).
	{The geometry of dissipative evolution equations: The porous medium equation}.
	\textit{Commun. Partial Differ. Equations}, \textbf{26}, 101--174.
\bibitem[Panaretos and Zemel(2019)]{PZ2019} Panaretos, V. M. \& Zemel, Y. (2019). Statistical aspects of Wasserstein distances. arXiv:1806.05500v3. 
\bibitem[Panaretos and Zemel(2022)]{PZ2022} Panaretos, V. M. \& Zemel, Y. (2022). \textit{An Invitation to Statistics in Wasserstein Space}. Springer.
\bibitem[Peyr\'{e} and Cuturi(2019)]{PC2019} 
	Peyr\'{e}, G. \& Cuturi, M. (2019).
	Computational optimal transport: With Applications to Data Science.
	\textit{Foundations and Trends{\textregistered} in Machine Learning}, \textbf{11}, 355--607.
\bibitem[Rankin and Wong(2023)]{RW2023} Rankin, C. \& Wong, TK,  L. (2023). Bregman-Wasserstein divergence: geometry and applications. arXiv:2302.05833.
\bibitem[Santambrogio(2015)]{Santambrogio2015} 
	Santambrogio, F. (2015).
	\textit{Optimal Transport for Applied Mathematicians}. 
	Springer.
\bibitem[Takatsu(2011)]{Takatsu2011} 
Takatsu, A. (2011). 
Wasserstein geometry of Gaussian measures. 
\textit{Osaka Journal of Mathematics}, \textbf{48}, 1005--1026.
\bibitem[Takatsu and Yokota(2012)]{Takatsu2012} 
Takatsu, A. \& Yokota, T.(2012). 
Cone structure of $L^2$-Wasserstein spaces.
\textit{Journal of Topology and Analysis}, \textbf{4}, 237--253.
\bibitem[Tyler(1987)]{tyler1987distribution} 
	Tyler, D. (1987). 
	{A distribution-free M-estimator of multivariate scatter}.
	\textit{Annals of Statistics}, \textbf{15}, 234--251.
\bibitem[van der Vaart(1998)]{vV} 
	van der Vaart, A. W. (1998). 
	\textit{Asymptotic Statistics}. 
	Cambridge University Press.
\bibitem[Villani(2003)]{Villani2003} 
	Villani, C. (2003). 
	\textit{Topics in Optimal Transportation}. 
	American Mathematical Society.
\bibitem[Villani(2009)]{Villani2009} 
	Villani, C. (2009). 
	\textit{Optimal Transport: Old and New}. 
	Springer.
\bibitem[Wang and Li(2020)]{WL2019} 
	Wang, Y. \& Li, W. (2020).
	Information Newtons flow: Second-order optimization method in probability space.
	arXiv:2001.04341.
\bibitem[Yatracos(2022)]{yatracos2022} Yatracos, Y. G. (2022).
 Limitations of the Wasserstein MDE for univariate data.
 \textit{Statistics and Computing volume}, \textbf{32}, 32--95.
\end{thebibliography}

\appendix

\section{Proof of Wasserstein--Cramer--Rao inequality}


For random vectors $A$ and $B$, 
\begin{align}
	0 \leq {\rm E} \| tA+B \|^2 = {\rm E} [\| A \|^2] t^2 + 2 {\rm E} [A^{\top} B] t + {\rm E} [\| B \|^2]
\end{align}
for every $t$.
Thus, by considering the discriminant of the quadratic equation,
\begin{align}
	{\rm E} [A^{\top} B]^2 \leq {\rm E} [\| A \|^2] {\rm E} [\| B \|^2]. \label{CS}
\end{align}
Substituting 
\begin{align}
	A = \sum_i a_i \nabla_{\bm{x}} \hat{\theta}_i, \quad B = \sum_j b_j \nabla_{\bm{x}} S_j^W
\end{align}
into \eqref{CS} yields
\begin{align}
	\left( \sum_{i,j} a_i b_j {\rm E}_{\theta} [(\nabla_{\bm{x}} \hat{\theta}_i)^{\top} (\nabla_{\bm{x}} S_j^W)] \right)^2 \leq {\rm E}_{\theta} \left[ \left\| \sum_i a_i \nabla_{\bm{x}} \hat{\theta}_i \right\|^2 \right] {\rm E}_{\theta} \left[ \left\| \sum_j b_j \nabla_{\bm{x}} S_j^W \right\|^2 \right]. \label{CS2}
\end{align}

From the property \eqref{eq:poisson} of the Wasserstein score function,
\begin{align}
	\frac{\partial}{\partial \theta_j} {\rm E}_{\theta} [\hat{\theta}_i] &= \int \hat{\theta}_i(x) \frac{\partial}{\partial \theta_j} p(x, \theta) {\rm d} x \\
	&= -\int \hat{\theta}_i(x) \nabla_{\bm{x}} \cdot (p(x, \theta) \nabla_{\bm{x}} S_j^W(x, \theta)) {\rm d} x \\
	&= -\int (\nabla_{\bm{x}} \cdot (\hat{\theta}_i(x) p(x, \theta) \nabla_{\bm{x}} S_j^W(x, \theta)) - (\nabla_{\bm{x}} \hat{\theta}_i(x))^{\top} (\nabla_{\bm{x}} S_j^W(x, \theta)) p(x, \theta) ) {\rm d} x \\
	&= {\rm E}_{\theta} [(\nabla_{\bm{x}} \hat{\theta}_i)^{\top} (\nabla_{\bm{x}} S_j^W)],
\end{align}
which yields
\begin{align}
	\sum_{i,j} a_i b_j {\rm E}_{\theta} [(\nabla_{\bm{x}} \hat{\theta}_i)^{\top} (\nabla_{\bm{x}} S_j^W)] = \sum_{i,j} a_i b_j \frac{\partial}{\partial \theta_j} {\rm E}_{\theta} [\hat{\theta}_i] = a^{\top} \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\theta}] \right) b. \label{App1}
\end{align}
From the definition of the Wasserstein covariance \eqref{Wcov},
\begin{align}
	{\rm E}_{\theta} \left[ \left\| \sum_i a_i \nabla_{\bm{x}} \hat{\theta}_i \right\|^2 \right] &= {\rm E}_{\theta} \left[ \sum_{i,j} a_i a_j (\nabla_{\bm{x}} \hat{\theta}_i)^{\top} (\nabla_{\bm{x}} \hat{\theta}_j) \right] \\
    &= \sum_{i,j} a_i a_j {\rm Var}_{\theta}^{\mathrm{W}} (\hat{\theta})_{ij} \\
    &= a^{\top} {\rm Var}_{\theta}^{\mathrm{W}} (\hat{\theta}) a. \label{App2}
\end{align}
From the definition of the Wasserstein information matrix \eqref{WIM},
\begin{align}
	{\rm E}_{\theta} \left[ \left\| \sum_j b_j \nabla_{\bm{x}} S_j^W \right\|^2 \right] &= {\rm E}_{\theta} \left[ \sum_{i,j} b_i b_j (\nabla_{\bm{x}} S_i^W)^{\top} (\nabla_{\bm{x}} S_j^W) \right]\\ 
    &= \sum_{i,j} b_i b_j G_{\mathrm{W}} (\theta)_{ij} \\
    &= b^{\top} G_{\mathrm{W}} (\theta) b. \label{App3}
\end{align}

Substituting \eqref{App1}, \eqref{App2} and \eqref{App3} into \eqref{CS2}, we obtain
\begin{align}
	\left( a^{\top} \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\theta}] \right) b \right)^2 \leq a^{\top} {\rm Var}_{\theta}^{\mathrm{W}} (\hat{\theta}) a \cdot b^{\top} G_{\mathrm{W}} (\theta) b.
\end{align}
By putting 
\begin{align}
	b=G_{\mathrm{W}} (\theta)^{-1} \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\theta}] \right) a,
\end{align}
it leads to 
\begin{align}
	a^{\top} \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\theta}] \right) G_{\mathrm{W}} (\theta)^{-1} \left( \frac{\partial}{\partial \theta} {\rm E}_{\theta} [\hat{\theta}] \right) a \leq a^{\top} {\rm Var}_{\theta}^{\mathrm{W}} (\hat{\theta}) a,
\end{align}
which is equal to the Wasserstein Cramer--Rao inequality \eqref{eq:WCR}.

\end{document}
