\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage {amssymb}
\usepackage {pifont}
\usepackage{multirow}
\usepackage{bm}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

%\title{ Collaborative Perception based on the Spatial-Temporal Importance of Semantic Information}
\title{Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception}

\author{Yuntao~Liu,~Qian~Huang,~Rongpeng~Li,~Xianfu~Chen,~Zhifeng~Zhao,~Shuyuan~Zhao,~Yongdong~Zhu and Honggang~Zhang
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}
\thanks{Y. Liu is with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China as well as with Zhejiang Lab, Hangzhou 311121, China (e-mail: liuyuntao@zju.edu.cn).}% <-this % stops a space
\thanks{R. Li is with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: lirongpeng@zju.edu.cn).}% <-this % stops a space
\thanks{X. Chen is with the VTT Technical Research Centre of Finland, Oulu, 90570, Finland (e-mail: xianfu.chen@vtt.fi).}% <-this % stops a space
\thanks{Z. Zhao and H. Zhang are with Zhejiang Lab, Hangzhou 311121, China (e-mail: \{zhaozf, honggangzhang\}@zhejianglab.com) as well as with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China.}
\thanks{Q. Huang, S. Zhao and Y. Zhu are with Zhejiang Lab, Hangzhou 311121, China (e-mail: \{huangq, zhaosy, zhuyd\}@zhejianglab.com).}
% <-this % stops a space
% <-this % stops a space
%\thanks{Manuscript received August 19, 2022; revised October 26, 2022.}
}


% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Collaboration by leveraging the shared semantic information plays a crucial role in overcoming the perception capability limitations of isolated agents. However, existing collaborative perception methods tend to focus solely on the spatial features of semantic information, while neglecting the importance of the temporal dimension. Consequently, the potential benefits of collaboration remain underutilized. In this article, we propose Select2Col, a novel collaborative perception framework that takes into account the \underline{s}patial-t\underline{e}mpora\underline{l} importanc\underline{e} of semanti\underline{c} informa\underline{t}ion. Within the Select2Col, we develop a collaborator selection method that utilizes a lightweight graph neural network (GNN) to estimate the importance of semantic information (IoSI) in  enhancing perception performance, thereby identifying contributive collaborators while excluding those that bring negative impact. Moreover, we present a semantic information fusion algorithm called HPHA (historical prior hybrid attention), which integrates multi-scale attention and short-term attention modules to capture the IoSI in feature representation from the spatial and temporal dimensions respectively, and assigns IoSI-consistent weights for efficient fusion of information from selected collaborators. Extensive experiments on two open datasets demonstrate that our proposed Select2Col significantly improves the perception performance compared to state-of-the-art approaches. The code associated with this research is publicly available at https://github.com/huangqzj/Select2Col/.
\end{abstract}

\begin{IEEEkeywords}
Collaborative Perception, Importance of Semantic Information, Spatial-Temporal Dimensions, Semantic Information Fusion, Hybrid Attention.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{P}{erception} is an essential capability for agents, especially for autonomous vehicles (AVs). However, relying solely on the agent's own perception capabilities often encounters significant challenges in real-world scenarios involving occluded or distant objects\cite{1}, and possibly leads to catastrophic outcomes, as depicted in Fig. 1. Fortunately, the emergence of the Internet of Vehicles (IoV) allows vehicles to communicate with each other, thereby acquiring additional perception information \cite{2}. Consequently, collaborative perception, wherein participating agents (e.g., vehicles and road infrastructures) exchange their perception information to obtain enhanced understanding of surroundings beyond their individual capabilities, has become a promising solution to avoid perception issues encountered by isolated autonomous vehicles\cite{3}. 

In terms of the embedded characteristics of shared information, collaborative perception can be classified as early fusion \cite{4}, late fusion\cite{5,6}, and intermediate fusion\cite{7,8,9,10,11,12,13,14,15}. Specifically, the former two categories, which share the raw data directly or perception results (e.g., object classification and regression results), could either inflict significant communication cost or yield limited performance due to the inadequately shared scene context. Instead, intermediate fusion, which involves to share extracted feature information (i.e., semantic information) from raw perception data, has been consistently demonstrated as a promising strategy to effectively balance perception performance and communication overhead \cite{9,10,11}. \par

% Figure environment removed

However, as semantic information suffers from multi-level heterogeneity (e.g., diverse transmission delays, different sensor configurations, and distinct views of agents), the shared semantic information could be rather asynchronous and misaligned in both temporal and spatial dimensions. For example, the semantic information of different agents may offer inconsistent sizes and outdated locations for the same object. Therefore, the direct fusion of semantic information could be troublesome. In addition, selecting appropriate collaborators is crucial, as unsuitable candidates can inevitably introduce noise to the achieved performance. Nonetheless, identifying beneficial collaborators that can enhance perception performance is not straightforward, due to their diverse spatial-temporal perspectives.\par

In order to realize efficient collaborative perception, several recent studies, including CoFF\cite{8}, V2VNet\cite{9}, V2X-Vit\cite{10}, Where2comm\cite{11}, Who2com\cite{12}, and When2com\cite{13} have been proposed. However, these studies take account of the spatial aspect of semantic information only, with a particular focus on spatial location, but neglect the importance of the temporal dimension in collaborator selection and semantic information fusion. Consequently, the performance improvement could be rather limited and even questionable. For instance, the exchange of outdated semantic information (e.g., an object no longer available in the focus region) might confuse other participating agents and lead to misleading perception results. Thus, the spatial and temporal dimensions of semantic information become indispensable for successful collaborative perception. Besides, it sounds appealing to evaluate the importance of semantic information (IoSI) before its application in collaborative perception.\par

In this article, motivated by the aforementioned discussions and the significant contributions of artificial intelligence studies \cite{16,17,18}, we propose an IoSI-based collaborative perception framework called Select2Col to more comprehensively take account of the \underline{s}patial-t\underline{e}mpora\underline{l} importanc\underline{e} of semanti\underline{c}  informa\underline{t}ion. In addition, within the framework of Select2Col, we develop an IoSI-based collaborator selection method that can efficiently select contributive collaborators for perception jointly. Moreover, we design a historical prior hybrid attention (HPHA) to implement more effective information fusion by assigning IoSI-consistent weights. Compared to existing works\cite{8,9,10,11,12,13}, the main contributions of this article can be summarized as follows. \par
\begin{itemize}

\item[1)]We propose the Select2Col\footnote{The codes for our proposed Select2Col are available at https://github.com/huangqzj/Select2Col/.} framework for collaborative perception, which allows to capture the IoSI from  spatial and temporal dimensional for both effective collaborator selection and meaningful information fusion.
\item[2)]
We design a semantic information fusion algorithm HPHA, which employs a multi-scale attention module and a short-term attention module to efficiently aggregate semantic information from the spatial and temporal dimensions of IoSI. Moreover, we present a collaborator selection method based on IoSI to leverage information from contributive collaborators only.
\item[3)]
We carry out a comprehensive evaluation of our proposed Select2Col on two open datasets OPV2V\cite{14} and V2XSet\cite{10}. The experimental results show that Select2Col is more effective in improving perception performance than state-of-the-art (SOTA) works, such as V2VNet\cite{9}, V2X-Vit\cite{10}, and Where2comm\cite{11}.
\end{itemize}

The remainder of this article is organized as follows. Section II reviews the related works on collaborative perception. Section III presents the system model and formulates the problem. Section IV elaborates on our proposed Select2Col framework. Sections V and VI introduce our innovative collaborator selection method and semantic information fusion algorithm HPHA, respectively. We conduct experiments to verify our proposed Select2Col in Section VII. Finally, we conclude this article with a summary in Section VIII.\par



\section{Related Work}

\subsection{Perception Means in IoV}

Perception plays a crucial role in various tasks of IoV (especially AVs), such as 3D object detection \cite{19}, multiple object tracking\cite{20}, semantic segmentation \cite{21}, and depth estimation \cite{22}. In particular, 3D object detection belongs to one of the most fundamental tasks, and enjoys many mature detection methods (e.g., camera-based, LiDAR-based, and camera-LiDAR). In that regard, benefitting from the inherent 3D information provided by LiDAR point clouds \cite{23}, LiDAR-based detection is widely favored and contingent on voxel-based \cite{24} or pillar-based \cite{25} strategies to extract features from unstructured point clouds. Notably, pillar-based feature extraction requires fewer computational and memory resources than voxel-based feature extraction \cite{26} and becomes the preferred strategy in most SOTA collaborative perception methods, such as V2VNet, V2X-Vit, and Where2comm. \par

Consistent with these works, we primarily consider the pillar-based feature extraction strategy and focus on how to extend to collaborative perception, given the severe limitation of single-agent perception \cite{26}.\par

\begin{table}[!t]
\centering
\caption{A comparison between Select2Col and highly related works.}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|m{2.2cm}<{\centering}|m{2.2cm}<{\centering}|m{2.2cm}<{\centering}|}
\hline
\textbf{Works} & \textbf{Collaborator selection strategy} & \textbf{Information fusion strategy} \\ \hline
CoFF \cite{8}          &All agents                                                                             &Spatial fusion                                                              \\ \hline
V2VNet\cite{9}         & All agents                                                                         &Spatial fusion                                                              \\ \hline
V2X-Vit\cite{10}        &All agents                                                                             &Spatial fusion                                                              \\ \hline
Where2comm\cite{11}     &Spatial selection                                                        &Spatial fusion                                                              \\ \hline
Who2comm\cite{12}     &Spatial selection                                                        &Spatial fusion                                                              \\ \hline
When2com\cite{13}       &Spatial selection                                                        &Spatial fusion                                                              \\ \hline

\textbf{Select2Col (Ours)}        &\textbf{Spatial-temporal selection}            &\textbf{Spatial-temporal fusion}                                                    \\ \hline
\end{tabular}
\end{table}

\subsection{Related Studies in Collaborative Perception}
V2VNet\cite{9} is a pioneering framework that explores the utilization of vehicle-to-vehicle communication to enhance the perception performance of AVs. In particular, it leverages a space-aware graph neural network (GNN) to aggregate information from all neighboring vehicles. V2X-ViT\cite{10} devises a unified vision transformer architecture consisting of heterogeneous multi-agent self-attention, so as to capture the spatial relationship between agents. Meanwhile, CoFF\cite{8} fuses feature information by enhancing the spatial feature information of the original perception data. \par

Though the incorporation of multiple agents for collaborative perception yields performance improvement, studies of Who2com\cite{12} and literature \cite{27} demonstrate that bluntly adding more collaborators does not always guarantee to benefit the performance and even results in counter-intuition negative effects. Therefore, it becomes worthwhile to select effective collaborators and conduct meaningful information fusion. For example, When2com\cite{13} proposes to forge several communication groups from neighboring agents in terms of the spatial feature similarity of the shared information and determines when to collaborate within these groups. On the other hand, Where2comm\cite{11} employs a spatial confidence map to select collaborators and utilizes multi-head attention to fuse the exchanged information. Despite the remarkable progress, the lack of exploring temporal dimension makes these works far from the optimality. Therefore, targeting at filling the performance gap, the proposed Select2Col highlights the utilization of both spatial and temporal dimensions of IoSI in selecting collaborators and fusing semantic information, so as to enhance the perception performance. As summarized in TABLE I, the Select2Col significantly differs from the existing literature.\par

\section{System Model and Problem Formulation}

\subsection{System Model}
Beforehand, Table II presents a list of notations used in this manuscript.\par

Consistent with SOTA methods, such as\cite{9,10,11}, we adopt a collaborative perception scenario wherein at a given time-slot $t$, there exists an ego agent \emph{i} performing collaborative perception with a set  ${\mathcal{N}_{i}^{t}}$ neighboring agents (e.g., capable vehicles and/or road infrastructures with perception and communication functionalities), as depicted in Fig. 1. From the ego's perspective, the objective of collaborative perception is to effectively leverage the received intermediate semantic information to maximize its perception performance.\par


Mathematically, we denote that at the very beginning of $t$, agent $i$ obtains its raw perception data $X_{i}$ from its sensors and receives semantic information transmitted by its neighboring agents. From that time, the collaborative perception undergoes the following procedures.\par

\begin{table}[]
\centering
\caption{Notations and Explanation}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|c|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Notation}} &  \multicolumn{1}{c|}{\textbf{Explanation}}  
 \\ \hline
$X_{i}$       & Raw perception data of agent \emph{i}                      \\
\hline
${\mathcal{N}_{i}^{t}}$       & Set of neighbor agents for agent \emph{i} at time-slot $t$                   \\
\hline
${\mathcal{C}_{i}^{t}}$       & Set of selected collaborators for agent \emph{i} at time-slot $t$                            \\
\hline
${\mathcal{S}}$       & Set of  scales  for mult-scale attention operation                      \\
\hline

${\bm{F}_{i}^{t}}$         & \begin{tabular}[c]{@{}l@{}} Semantic information of agent \emph{i} at time-slot $t$\end{tabular}                                       \\
\hline

${\bm{F}_{j}^{ t-\tau _{ji}}}$  & \begin{tabular}[c]{@{}l@{}}Semantic information of agent \emph{j}  shared to agent \emph{i} \\ at time-slot \emph{t} \end{tabular} \\
\hline 


$\bm{H}_{s}$  & \begin{tabular}[c]{@{}l@{}}Aggregated feature semantic information after   \\spatial attention operation at a specific scale $s$\end{tabular}                                                \\ 
\hline
$\bm{H}_\mathrm{ms}$  & \begin{tabular}[c]{@{}l@{}}Aggregated feature semantic information after  \\ multi-scale attention operation\end{tabular}        \\    
\hline

$\bm{H}_{i}$       & \begin{tabular}[c]{@{}l@{}}Fused semantic information of agent \emph{i} \\  \end{tabular}                                                 \\        
\hline  

$\bm{M}_j$       & \begin{tabular}[c]{@{}l@{}}Sparse map of agent \emph{j} \\\end{tabular}                                                 \\     
\hline     


$\bm{W}_{s,ji}^\mathrm{{sa}}$ & \begin{tabular}[c]{@{}l@{}}Spatial-attention weight of agent \emph{j} to agent \emph{i} at  \\ scale $s$ \end{tabular}                                                \\
\hline

$\bm{W}^\mathrm{{ta}}$ &Temporal-attention weight \\
\hline

$w_{ji}^\mathrm{{en}}$       & \begin{tabular}[c]{@{}l@{}}Enhanced weight of agent \emph{j} to agent \emph{i} \\ \end{tabular}                                                 \\    
\hline


$\widehat{Y}_{i}$       & \begin{tabular}[c]{@{}l@{}}Perception result of agent \emph{i} \end{tabular}                                          \\
\hline
$Y_{i}$       & \begin{tabular}[c]{@{}l@{}}Ground truth of the perception result of  agent \emph{i} \\ \end{tabular}                                                 \\
\hline

$f_\mathrm{{enc}}$   & \begin{tabular}[c]{@{}l@{}}Encoder that extracts  semantic information from raw\\    perception data\end{tabular}                                         \\
\hline
$f_\mathrm{{select}}$   & \begin{tabular}[c]{@{}l@{}}Selector that outputs the selected collaborators\\ \end{tabular}                                         \\
\hline
$f_\mathrm{{fuse}}$   & \begin{tabular}[c]{@{}l@{}}Fuser that outputs the fused semantic information \\ \end{tabular}                                         \\
\hline
$f_\mathrm{{dec}}$   & \begin{tabular}[c]{@{}l@{}}Decoder that outputs perception result  \\  \end{tabular}                                      \\
\hline
$f_\mathrm{{eva}}$   & \begin{tabular}[c]{@{}l@{}}Standard perception performance evaluation function  \\  \end{tabular}                                      \\
\hline

$\mathrm{conv}\left( \cdot \right)$   & \begin{tabular}[c]{@{}l@{}}Convolution block operation  \\  \end{tabular}                                      \\
\hline

$\mathrm{deconv}\left( \cdot \right)$    & \begin{tabular}[c]{@{}l@{}}Deconvolution block operation \\  \end{tabular}                                      \\
\hline

$\mathrm{sa}\left( \cdot \right)$    & \begin{tabular}[c]{@{}l@{}}Spatial attention operation   \\  \end{tabular}                                      \\
\hline

$\mathrm{ta}\left( \cdot \right)$    & \begin{tabular}[c]{@{}l@{}}Temporal attention operation  \\  \end{tabular}                                      \\
\hline

$\tau _{ji}$  &Information availability latency of agent \emph{j} to agent \emph{i} \\

\hline
$t_{ji}^{\mathrm{asyn}}$   & \begin{tabular}[c]{@{}l@{}}Asynchronous overhead between the sensors of   \\ agent \emph{j} and agent \emph{i} \end{tabular}                      \\
\hline
$t_{ji}^\mathrm{{ext}}$       & \begin{tabular}[c]{@{}l@{}}Semantic information extraction time of agent \emph{j}  \end{tabular} \\          
\hline                                         
$t_{ji}^\mathrm{{tx}}$      & \begin{tabular}[c]{@{}l@{}}Transmission time of shared semantic information \\ from  agent \emph{j} to agent \emph{i} \end{tabular}                               \\
\hline
$t_{ji}^\mathrm{{idle}}$      & \begin{tabular}[c]{@{}l@{}}Idle time between  the perception system and  \\   communication system \\   \end{tabular}                               \\
\hline
$b_{ji}$       & Bandwidth of agent \emph{j} to agent \emph{i}                                                                                                                               \\
\hline
$p_{ji}^\mathrm{{tx}}$      & \begin{tabular}[c]{@{}l@{}}Transmission power of agent \emph{j} to agent \emph{i} \\ \end{tabular} \\    
\hline
$p_{ji}^\mathrm{{loss}}$      & Path loss between agent \emph{j} and agent \emph{i}                                                                                                               \\
\hline

$p_{ji}^\mathrm{{noise}}$      & \begin{tabular}[c]{@{}l@{}}Noise power between agent \emph{j} and agent \emph{i} \\  \end{tabular} \\    
\hline
$d_{ji}$     & Distance between agent \emph{j} and \emph{i}                                                                                                                 \\
\hline
$f_\mathrm{c}$       & Center frequency in GHz                                                                                                                                        \\
\hline
$\mathrm{s}_{\mathrm{ms}}$    &  Multi-scale attention output  scale                                                                     \\
\hline
 $\mathrm{T}$    &  Sensor sampling interval                                                                                                                \\
\hline
 $\mathrm{K}$    &  Number of historical prior frames                                                                                                                 \\
\hline

\end{tabular}
\end{table}

\subsubsection{Semantic information extraction}
We adopt the intermediate feature fusion strategy\cite{9}. In other words, et each time-slot \emph{t}, agent \emph{i} employs an encoder $f_{\mathrm{enc}}$ to extract semantic information from its raw perception data $X_{i}$, such as Lidar point cloud data \cite{11}. For any agent $i$, the corresponding semantic information $\bm{F}_{i}^{ t }$ is extracted at time-slot $t$ as follows
\begin{align}
\bm{F}_{i}^{t}=f_{\mathrm{enc}}\left( X_{i} \right). 
\end{align}
In addition, for a simpler and more consistent representation of semantic information, including that of neighboring agents and the ego, we employ the notation $\bm{F}_{j}^{t-\tau _{ji}}$  to denote the semantic information of  agent \emph{j} in  $\mathcal{N}_{i}^{ t }\cup \left\{ i \right\}$, where  $\tau _{ji}$ represents the information availability latency between the source agent \emph{j} and the target agent \emph{i}. Specifically, when \emph{j} equals \emph{i}, $\tau _{ji}$ is set to 0 as there is no latency between agent \emph{i} and itself. Otherwise, $\tau _{ji}$ can be defined as the sum of sensor asynchronous overhead $t_{ji}^{\mathrm{asyn}}$, the semantic information extraction time $t_{ji}^{\mathrm{ext}}$, the semantic information transmission time  $t_{ji}^{\mathrm{tx}}$, and  the idle time $t_{ji}^{\mathrm{idle}}$ caused by the lack of synchronization between the perception system and communication system\cite{10}. In other words,
\begin{align}
\tau _{ji}=t_{ji}^{\mathrm{asyn}}+t_{ji}^{\mathrm{ext}}+t_{ji}^{\mathrm{tx}}+t_{ji}^{\mathrm{idle}}.
\end{align}
According to the specification 3GPP TR 38.901 ``study on channel model for frequencies from 0.5 to 100 GHz"\cite{28}, network transmission time $t_{ji}^{\mathrm{tx}}$ can be approximately derived from a path-loss driven channel as
\begin{equation}
\begin{aligned}
t_{ji}^{\mathrm{tx}}\,\,=\,\,\frac{\mathrm{size}\left(\bm{F}_{j}^{t-\tau _{ji}} \right)}{b_{ji}\log _2\left.( 1+10^{0.1\left(p_{ji}^{\mathrm{tx}}\,\,-p_{ji}^{\mathrm{loss}}-p_{ji}^{\mathrm{noise}} \right)} \right.)},
\end{aligned}
\end{equation}
where $\mathrm{size}\left( \cdot \right) $ denotes a function that calculates the size of the transmitted semantic information, $b_{ji}$, $p_{ji}^{\mathrm{tx}}$, $p_{ji}^{\mathrm{noise}}$ and $p_{ji}^{\mathrm{loss}}=28.0+22\log _{10}\left( d_{ji} \right) +20\log _{10}\left( f_{\mathrm{c}} \right)$ represent agent \emph{j}'s transmission bandwidth, transmission power, transmission noise power, and  transmission path loss to agent \emph{i}, respectively. Furthermore, $d_{ji}$ denotes the distance between agent \emph{j} and  \emph{i} in meters, and  $f_{\mathrm{c}}$ represents  the center frequency in GHz. \par

\subsubsection{Contributive collaborator selection}
The semantic information of neighboring agents originates from a variety of raw perception data with various spatial and temporal dimensions. Notably, semantic information is not always beneficial, as in some cases, the semantic information provided by certain agents may even degrade perception performance due to excessive delay or unreasonable spatial viewpoint\cite{10}.  Therefore,  agent \emph{i} employs  a selector $f_{\mathrm{select}}$  to determine a  contributive collaborator set $\mathcal{C}_{i}^{t }$ from  $\mathcal{N}_{i}^{t }$, namely,
\begin{align}
\mathcal{C}_{i}^{t}=f_{\mathrm{select}}\Big(\big\{ \bm{F}_{j}^{t-\tau _{ji}} \big\} _{j\in \mathcal{N}_{i}^{t} \cup \left\{ i \right\} } \Big).
\end{align}



\subsubsection{Semantic information fusion}
After selecting contributive collaborators, agent \emph{i} utilizes a fuser $f_{\mathrm{fuse}}$  to aggregate its semantic information $\bm{F}_{i}^{t}$  with selected collaborators' semantic information $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{C}_{i}^{t}}$  and correspondingly obtain the fused semantic information $\bm{H}_{i}$ with a more comprehensive representation of features.  The semantic information fusion expression can be written as 
\begin{align}
\bm{H}_{i}=f_{\mathrm{fuse}}\Big( \big\{ \bm{F}_{j}^{t-\tau _{ji}} \big\} _{j\in \mathcal{C}_{i}^{t} \cup \left\{ i \right\} } \Big).
\end{align}


\subsubsection{Semantic information decoding}
Finally, for each agent $i$, the fused semantic information $\bm{H}_{i}$ needs to be converted into appropriate perception result $\widehat{Y}_{i}$ by a decoder $f_{\mathrm{dec}}$. Mathematically, the expression for this process is as follows
\begin{align}
\widehat{Y_{i}}=f_{\mathrm{dec}}(\bm{H}_{i}).
\end{align}


\subsection{Problem Formulation}
Consistent with all collaborative perception works, we use the average precision (AP) metric to evaluate the perception performance, that is
\begin{align}
\mathrm{AP}=\,\,f_{\mathrm{eva}}(\widehat{Y_{i}},Y_{i}),
\end{align}
where $Y_{i}$ represents the ground truth of agent \emph{i}'s perception result, and $f_{\mathrm{eva}}\left( \cdot \right) $ is a standard perception performance evaluation function \cite{29} that measures the accuracy of the perception result relative to the ground truth. 

Thus, in order to maximize the perception performance (i.e., maximize the AP), together with (4) to (7), the  collaborative perception problem can be formulated as 

\begin{equation}
\begin{aligned}
\max \mathrm{AP}=f_{\mathrm{eva}}\Bigg( f_{\mathrm{dec}}\Big( f_{\mathrm{fuse}}\big( \big\{ \bm{F}_{j}^{t-\tau _{ji}} \big\} _{j\in \mathcal{C}_{i}^{t} \cup \left\{ i \right\}      } \big) \Big) , Y_{i} \Bigg) \\
 s.t. \ \mathcal{C}_{i}^{t}=f_{\mathrm{select}}\Big( \big\{ \bm{F}_{j}^{t-\tau _{ji}} \big\} _{j\in \mathcal{N}_{i}^{t} \cup \left\{ i \right\}  } \Big) .\hspace{0.8cm} 
\end{aligned}
\end{equation}

As implied in (8), it turns to optimize the design of the selector function $f_{\mathrm{select}}\left( \cdot \right)$ and fuser function $f_{\mathrm{fuse}}\left( \cdot \right)$, since the $f_{\mathrm{enc}}\left( \cdot \right) $,  $f_{\mathrm{dec}}\left( \cdot \right)$, and  $f_{\mathrm{eva}}\left( \cdot \right)$  are typically common and well-established elements\cite{9,10,11}. Furthermore, the former $f_{\mathrm{select}}\left( \cdot \right)$ involves identifying effective collaborators that can provide high-quality perception semantic information, while the latter considers the correlation of semantic information to improve the feature representations. Nevertheless, it is non-trivial to describe these two functions in compact, mathematical formulations. Hence, vanilla approaches \cite{9,10,11,12,13,14} approximate these functions with the guidance of intuitive observations. For example, collaborators with larger ego interest regions promise to be more contributive in improving the perception performance of ego. Furthermore, assigning higher weights to more important semantic information for fusion can contribute to yielding an enhanced feature representation. However, these approaches ignore the importance of semantic information from the temporal dimension, resulting in limitations in accurately fitting these functions.\par

Therefore, in order to address these challenges, we leverage a lightweight GNN and hybrid attention modules to calibrate the selector function $f_{\mathrm{select}}\left( \cdot \right)$ and fuser function $f_{\mathrm{fuse}}\left( \cdot \right)$ based on the IoSI from spatial-temporal dimensions. The details shall be given in Section V and Section VI. \par


% Figure environment removed

\section{Select2Col: An IoSI-based Collaborative Perception Framework}

Based on the previous analysis, we propose Select2Col, an IoSI-based collaborative perception framework. As depicted in Fig. 2, Select2Col comprises four components (i.e., encoder $f_{\mathrm{enc}}$, selector $f_{\mathrm{select}}$, fuser $f_{\mathrm{fuse}}$, and decoder  $f_{\mathrm{dec}}$) to implement the entire process of collaborative perception. We  briefly  introduce these components of Select2Col in this section, while leaving the design of collaborator selection and information fusion in Section V and Section VI, respectively.\par



\subsection{Overview of Select2Col}

Typically, the Select2Col framework encompasses the following components.\par

\subsubsection{Encoder}The encoder extracts semantic information from raw perception data. In this article, we focus on LiDAR-based 3D object detection as the perception task. Correspondingly, we adopt Pointpillars\cite{22} as our encoder backbone because of its low inference latency and optimized memory usage, consistent with the literature\cite{9,10,11}.\par

\subsubsection{Selector} This selector is invoked to determine contributive  collaborators $\mathcal{C}_{i}^{t}$ using the obtained semantic information $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{N}_{i}^{ t } \cup \left\{ i \right\}}$, which are the outputs of the encoder. As mentioned earlier, we design an IoSI-based collaborator selection method, the details of which are given in Section V.\par

\subsubsection{Fuser}This fuser is responsible for obtaining the fused semantic information $\bm{H}_{i}$ using $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{C}_{i}^{t}\cup \left\{ i \right\}}$. In particular, we propose a semantic information fusion algorithm named HPHA to enhance the fusion feature representation as further discussed in Section VI.\par

\subsubsection{Decoder}This decoder outputs the perception result $\widehat{Y}_{i}$ based on the fused semantic information $\bm{H}_{i}$. Consistent with \cite{11}, we utilize a two-layer convolution block to downsample the fused semantic information and then employ a single-layer convolutional classification head and a single-layer convolutional regression head to obtain the classification and regression information of objects, respectively. \par

\subsection{End-to-End Network Model}
We employ an end-to-end neural network model to effectively integrate the components above, given that the output of each component serves as the input for the next one.\par

This network model takes the raw perception data of the ego agent and its neighboring agents as input. It produces collaborative perception results (i.e., the results of object detection, including  classification and regression outcomes) for the ego agent. The loss function of the model consists of a classification loss and a regression loss. We utilize focal loss \cite{30} for the classification loss  and smooth L1 loss \cite{31} for the regression loss. \par
Furthermore, the network model is trained in an end-to-end manner under the supervision Lidar-based 3D object detection task. During training, a random agent is selected as the ego from the training subset of the open datasets, such as OPV2V\cite{14} and V2XSet\cite{10}. The training method and parameters are consistent with the SOTA works\cite{9,10,11}, and more details can be found in our open-source code.  \par

\section{Collaborator Selection Based on IoSI}

Based on the spatial-temporal variability of an agent's semantic information, this section introduces our proposed IoSI-based collaborator selection method, that is the fuser function $f_{\mathrm{fuse}}\left( \cdot \right)$, to address the challenges of identifying contributive  collaborators. \par

Specifically, we use distinctive enhanced weights for agents to signify their respective IoSI in improving the ego's perception performance. Furthermore, we utilize a lightweight GNN module to learn the enhanced weight of each agent from spatial-temporal dimensions and identify contributive collaborators accordingly. Different from solely relying on spatial relationships to determine contributive collaborators, our method takes into account both spatial and temporal dimensions, resulting in a more precise selection of the contributors. Fig. 3 gives the related diagram.\par


\subsection{Enhanced Weight to Select Contributive Collaborators}

For each agent \emph{j} in $\mathcal{N}_{i}^{ t }\cup \left\{ i \right\}$, we use a convolution module $\mathrm{conv}_{\mathrm{map}}$ \cite{11} to extract its  sparse map $\bm{M}_j$ from the semantic information $\bm{F}_{j}^{ t-\tau _{ji}}$. The sparse map which contains the spatial location information of potential objects can be obtained as 
\begin{align}
\bm{M}_j=\mathrm{conv}_{\mathrm{map}}\big( \bm{F}_{j}^{ t-\tau _{ji}} \big)\label{convmap},
\end{align}
where $\mathrm{conv}_{\mathrm{map}}\left( \cdot \right) $ denotes the convolution map operation. In addition, the element of the sparse map takes either 0 or 1 depending on the existence of potential objects (i.e., $1$ indicates the existence; $0$ implies only the existence of background). Notably, for each agent \emph{j}, we also augment its semantic information latency $\tau _{ji}$ to represent its temporal dimension information. \par


After acquiring the sparse map $\bm{M}_j$ and $\tau _{ji}$ of any agent \emph{j} in $\mathcal{N}_{i}^{ t }\cup \left\{ i \right\}$, we employ a lightweight GNN\cite{16} as the backbone to estimate the importance of agents in enhancing perception performance for ego agent \emph{i} from spatial-temporal dimensions. Within this GNN module, each node corresponds to an agent and comprises its sparse map along with the semantic information latency, which respectively indicates the spatial and temporal characteristics of this agent. For each neighboring agent \emph{j} in $\mathcal{N}_{i}^{ t }$, the trained GNN generates its enhanced weight $w_{ji}^{\mathrm{en}}$, reflecting the importance of this agent's semantic information to the ego.\par

In this article, neighboring agents with an enhanced weight less than or equal to 0 are considered invalid collaborators, which are removed from the neighboring agent set $\mathcal{N}_{i}^{ t }$. After eliminating all invalid collaborators, we then obtain the selected collaborator set $\mathcal{C}_{i}^{ t }$.\par

% Figure environment removed






\subsection{Collaborator Information Enhancement}
As not all selected individuals contribute equally to improving perception performance, it is still essential to demonstrate the contribution of collaborators to the ego. Therefore, we utilize the enhanced weights to enhance the semantic information of the selected collaborators.  \par

Specifically, for the semantic information of each selected collaborator, such as agent \emph{j}, we use its sparse map $\bm{M}_j$ to purposefully acquire the spatial features of detection objects. Then, we employ the enhanced weight $w_{ji}^{\mathrm{en}}$ to magnify the effective features while suppressing background noise. For each agent \emph{j} in  $ \mathcal{C}_{i}^{ t }$, its enhanced semantic information $\bm{F}_{j}^{t-\tau _{ji}}$ is performed as follows
\begin{align}
\bm{F}_{j}^{t-\tau_{ji}}=\bm{F}_{j}^{ t-\tau _{ji}}\otimes \bm{M}_j\otimes w_{ji}^{\mathrm{en}},
\end{align}
where $\otimes$ denotes the elementwise multiplication.\par

Recalling the definition of  sparse map $\bm{M}_j$  in (9), (10) implies  the enhanced weight $w_{ji}^{\mathrm{en}}$ is only applied to potential object regions only, which can effectively  improve the object features while suppressing background noise. \par


In conclusion, by identifying contributive collaborators and enhancing their semantic information based on IoSI, the collaborator selection method provides high-quality perception information for subsequent semantic information fusion, thereby further ensuring the effectiveness of the fused semantic information. We summarize the details of the collaborator selection method in Algorithm 1. \par

\begin{algorithm}[!t]
\caption{Collaborator Selection Method}\label{alg:alg1}
\begin{algorithmic}[1]
\STATE {\textbf{Input}\textsc{:}}  semantic information of ego and its neighboring agents  $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{N}_{i}^{ t } \cup \left\{ i \right\}}$ 
\STATE {\textbf{Output}\textsc{:}}  selected collaborator set $\mathcal{C}_{i}^{t}$, and corresponding enhanced semantic information $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{C}_{i}^{t}\cup \left\{ i \right\}}$
\STATE initialize $\mathcal{C}_{i}^{ t}$ as neighboring agent set $\mathcal{N}_{i}^{ t }$ 
\STATE  \textbf{for}   each agent $j\in \mathcal{N}_{i}^{ t}\cup \left\{ i \right\} $  \textbf{do} 
\STATE \hspace{0.5cm} get the corresponding sparse map  $\bm{M}_j$ using (9) 
\STATE   \hspace{0.5cm} \textbf{if} $\left( j=i \right)$  \textbf{then} 
\STATE \hspace{1cm}  the latency between ego and itself is 0, i.e., $\tau _{ji}=0$
 \STATE \hspace{0.5cm}  \textbf{endif}
\STATE \hspace{0.5cm} using $\left( \bm{M}_j,\tau _{ji} \right)$ as agent's spatial-temporal information
 \STATE  \textbf{endfor}
\STATE agents in $ \mathcal{N}_{i}^{ t}\cup \left\{ i \right\} $ as nodes to build the GNN network
\STATE GNN outputs each agent's enhanced weight $\left\{ w_{ji}^{en} \right\} _{j\in \mathcal{N} _{i}^{t}}$

\STATE  \textbf{for} each agent $j\in \mathcal{N}_{i}^{ t}$  \textbf{do} 
\STATE \hspace{0.5cm}  \textbf{if} $\left( w_{ji}^{en}\leqslant 0 \right) $ \textbf{then} 
\STATE \hspace{1cm}   delete invalid agent $j$ from  $\mathcal{C}_{i}^{t }$
\STATE \hspace{0.5cm}  \textbf{else then} 
\STATE \hspace{1cm}   enhanced semantic information of selected collaborator using (10)


\STATE \hspace{0.5cm}  \textbf{endif} 
\STATE \textbf{endfor} 
\STATE \textbf{Return} $\mathcal{C}_{i}^{ t }$, $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{C}_{i}^{t}\cup \left\{ i \right\}}$
\end{algorithmic}
\label{alg1}
\end{algorithm}




\section{HPHA: A Historical Prior Hybrid Attention Information Fusion algorithm}

In this section, we introduce our proposed semantic information fusion algorithm HPHA to develop the fuser function $f_{\mathrm{fuse}}\left( \cdot \right)$.

As illustrated in Fig. 4, in order to enhance the feature presentation of the aggregated semantic information, HPHA leverages a multi-scale attention module and a short-term attention module to learn the IoSI in feature presentation from spatial and temporal dimensions respectively, and assigns IoSI-consistent weights to different semantic information for optimal feature fusion. The multi-scale attention module serves to effectively capture the importance correlation between the ego and its collaborators under distinct spatial resolutions, while the short-term attention module effectively captures the temporal importance correlation between past and present features. \par


Furthermore, our proposed HPHA algorithm is outlined in Algorithm 2. \par


\subsection{Semantic Information Aggregated from Spatial Dimension}

In order to improve the egoâ€™s object feature representation from the spatial dimension, we first introduce a multi-scale attention module\cite{17} to extract spatial attention weights at various scales, and utilize the spatial-attention weight to indicate the importance of a collaborator's semantic information. Specifically, the spatial-attention weight of agent \emph{j} to agent \emph{i} at a specific scale $s$ (${s\in \mathcal{S}}$)
is represented as $\bm{W}_{s,ji}^{\mathrm{sa}}$ and can be defined as 
\begin{align}
\bm{W}_{\!s,ji}^{\mathrm{sa}}\!=\mathrm{sa}\big( \mathrm{conv\!}\left.( \!\bm{F}_{i}^{ t}, s \right.) , \mathrm{conv}\!\left.( \!{\bm{F}_{j}^{ t-\tau _{ji}}}, s\right.)\big) ,
\end{align}
where $\mathcal{S}$ is the set of scales, $\mathrm{conv}\left( F,s \right)$ denotes a convolution block \cite{32} that reshapes the scale of semantic information $F$ to $s$, and  $\mathrm{sa}\left( \cdot \right) $ represents the spatial attention operation, referring to the dot-product attention\cite{33}. Moreover, when \emph{j}  equals \emph{i}, the spatial-attention weight in Eq. (11) is reduced to the self-attention weight, resulting in a simpler and more consistent equation.\par

% Figure environment removed



Next, we aggregate the semantic information of selected collaborators by assigning spatial-attention weights in Eq. (11) to generate the aggregated semantic information $H_{s}$ with a specific scale $s$ as

\begin{align}
\bm{H}_{s}=\sum \nolimits_{j\in \mathcal{C}_{i}^{ t }\cup \left\{ i \right\}}{\bm{W}_{s,ji}^{\mathrm{sa}}\otimes \mathrm{conv}\!\left.( {\bm{F}_{j}^{ t-\tau _{ji}}},s \right.)}.
\end{align}

Subsequently, in order to ensure the scale consistency of the aggregated semantic information after spatial attention operation, we utilize a deconvolution block \cite{34} to reshape it to a unifying scale $\mathrm{s}_{\mathrm{ms}}$, as follows
\begin{align}
\bm{H}_{s}=\mathrm{deconv}\left.\!(\bm{H}_{s}, {s}_{\mathrm{ms}} \right.),
\end{align}
where  $\mathrm{deconv}\left( H,s \right)$ denotes the deconvolution block that reshapes the scale of semantic information $H$ to $s$. \par

Finally, we combine all the information $\left\{ \bm{H}_s \right\} _{s\in \mathcal{S}}$ acquired at various attention scales to produce the ultimate aggregated semantic information $\bm{H}_{\mathrm{ms}}$, that is,
\begin{align}
\bm{H}_{\mathrm{ms}}=\mathrm{concat}\left( \left\{ \bm{H}_s \right\} _{s\in \mathcal{S}} \right). 
\end{align}


The multi-scale attention module effectively learns the collaborative importance correlation between the ego and its collaborators from spatial dimension at different resolutions, which enhances the aggregated features and presents a comprehensive representation of features. \par


\begin{algorithm}[!t]
\caption{HPHA: Information Fusion Algorithm}\label{alg:alg2}
\begin{algorithmic}[1]
\STATE {\textbf{Input}\textsc{:}} semantic information of ego and its selected collaborators  $\left.\{ \bm{F}_{i}^{t-k\mathrm{T}} \right\} _{k=\left\{0,1\cdots \mathrm{K} \right.\}}$, $\left.\{ \bm{F}_{j}^{t-\tau _{ji}} \right.\} _{j\in \mathcal{C}_{i}^{t}}$

\STATE {\textbf{Output}\textsc{:}}    fused semantic information  $\bm{H}_{i}$
\STATE  \textbf{for}   each scale $s$ in $\mathcal{S}$ \textbf{do} 
\STATE \hspace{0.5cm} \textbf{for} each agent $j\in C_{i}^{ t }\cup \left\{ i \right\} $ \textbf{do} 
\STATE \hspace{1cm} calculate the corresponding spatial-attention weight $\bm{W}_{\!s,ji}^{\mathrm{sa}}$ using  (11)
\STATE \hspace{0.5cm} \textbf{endfor} 
\STATE \hspace{0.5cm}obtain aggregated information $\bm{H}_{s}$ with spatial-attention weight at specific $s$ using  (12) 
\STATE \hspace{0.5cm} convert $\bm{H}_{s}$ into  unifying scale ${s}_{\mathrm{ms}}$ using  (13)
\hspace{0.5cm} 
\STATE \textbf{endfor} 
\STATE combine all information $\left\{ \bm{H}_s \right\} _{s\in \mathcal{S}}$ to produce the ultimate aggregated semantic information $\bm{H}_{\mathrm{ms}}$ using (14)

\STATE create a temporary semantic information $\bm{H}_{\mathrm{h}}$ with historical prior information $\left.\{ \bm{F}_{i}^{t-k\mathrm{T}} \right\} _{k=\left\{1,2\cdots \mathrm{K} \right.\}}$ using  (15)

\STATE get temporal-attention weight $\bm{W}^{\mathrm{ta}}$ using (16)
\STATE refine the final fused semantic information $\bm{H}_{i}$ with temporal-attention weight  $\bm{W}^{\mathrm{ta}}$ using  (17) \\

\STATE \textbf{Return} $\bm{H}_{i}$
\end{algorithmic}
\label{alg2}
\end{algorithm}



\subsection{Semantic Information Refined from Temporal Dimension}
Furthermore, we design a historical prior short-term attention module to refine the aggregated semantic information from the temporal dimension considering that the historical semantic information of the ego contains rich object information from the temporal dimension. \par

First, the historical semantic information of the ego $\left.\{ \bm{F}_{i}^{t-k\mathrm{T}} \right\} _{k=\left.\{1,2\cdots \mathrm{K}\right.\}}$and the last-step aggregated semantic information $\bm{H}_{\mathrm{ms}}$  are concatenated to produce new semantic information $\bm{H}_{\mathrm{h}}$, such as
\begin{align}
\bm{H}_{\mathrm{h}}=\mathrm{concat}\big( \bm{H}_{\mathrm{ms}}, \left.\{ \bm{F}_{i}^{t-k\mathrm{T}} \right\} _{k=\left\{ 1,2\cdots \mathrm{K} \right.\}} \big),
\end{align}
where $\mathrm{T}$ represents the sensor sampling interval, and $\mathrm{K}$ represents the number of historical frames.

Next, this new semantic information  $\bm{H}_{\mathrm{h}}$ undergoes average pooling and max pooling operations to generate average-pooled features and max-pooled features. Subsequently, we devise a short-term attention operation to process the two features to obtain the temporal-attention weight $\bm{W}^{\mathrm{ta}}$, that is,
\begin{align}
\bm{W}^{\mathrm{ta}}=\mathrm{ta}\big( \mathrm{avgpool}\left.( \bm{H}_{\mathrm{h}} \right.) , \mathrm{maxpool}\left.( \bm{H}_{\mathrm{h}} \right.) \big) ,
\end{align}
where $\mathrm{avgpool}\left( \cdot \right)$ and  $\mathrm{maxpool}\left( \cdot \right)$ represent the average pooling operation and the max pooling operation respectively, and $\mathrm{ta}\left( \cdot \right) $ denotes a temporal attention operation, and refers to the channel attention in \cite{18}.

Finally, the temporal-attention weight $\bm{W}^{\mathrm{ta}}$ is further multiplied by the semantic information $\bm{H}_{\mathrm{h}}$, so as to create the final fused semantic information $\bm{H}_{i}$ as
\begin{align}
\bm{H}_{i}= {\bm{W}^\mathrm{{ta}}  \otimes   \bm{H}_{\mathrm{h}} }.
\end{align}


By incorporating historical prior semantic information, the short-term attention module effectively captures the temporal importance correlation between past and present features. This enables it to identify discriminating object features from a temporal perspective, thereby enhancing the quality of object representation.\par


Overall, the proposed HPHA provides an enhanced and comprehensive feature representation for the fused semantic information $\bm{H}_{i}$, derived from both spatial and temporal dimensions. Consequently, the decoder $f_{\mathrm{dec}}$ can more efficiently decode the fused semantic information $\bm{H}_{i}$  to generate  perception result $\widehat{Y}_{i}$, as depicted  in (6). \par




\section{Experimental evaluation}
This section provides a comprehensive performance comparison of the proposed Select2Col with the existing SOTA methods \cite{9,10,11}. Furthermore, to demonstrate the superiority of Select2Col, we conduct various ablation studies to showcase the performance improvements attained using our innovations. \par

\subsection{Experimental Settings}
Rather than evaluating on popular datasets such as KITTI\cite{35} and Nuscenes\cite{36}, which mainly provide single-agent samples, all experiments in this article are conducted on  open collaborative perception datasets OPV2V\cite{14} and V2XSet\cite{10}. In particular, OPV2V is the first large-scale open dataset targeted at vehicle-to-vehicle collaborative perception, generated using CARLA\cite{37} and SUMO\cite{38}. On the other hand, V2XSet is the first open dataset for vehicle-to-vehicle and vehicle-to-infrastructure perception to take into account real-world noise during collaboration.  \par

For the measurement of perception performance, we adopt the average precision (AP) and use Intersection-over-Union (IoU) thresholds at 0.3, 0.5, and 0.7. All experiments are conducted on an X86 PC station equipped with an Intel TM i7-11700 @2.50 GHz, 128 core CPU, 256 GB RAM, and 4 NVIDIA RTX3090 GPUs.\par

In this work, we evaluate the performance of Select2Col and compare it with three SOTA approaches, namely V2VNet\cite{9}, V2X-Vit\cite{10}, and Where2comm\cite{11}. To maintain fairness, we implement all baselines and our proposed Select2Col on OpenCOOD, an open-source code framework for collaborative perception.  \par

As presented in TABLE III, the key parameters employed in our study are in line with V2VNet, V2X-Vit, and Where2comm. In addition, Eq. (3) is further adopted for a more realistic calculation of the transmission time, where the related parameters are consistent with the literature \cite{39}. Notably, we allocate equal bandwidth to each individual agent. Consequently, our setting is more precise compared to V2X-Vit\cite{10} and Where2comm\cite{11}, which employ a fixed transmission rate of 27 Mbps for all agents. Furthermore, the semantic information extraction time $t_{ji}^{\mathrm{ext}}$ is obtained based on our computing device. More experimental parameters can be found in our open-source code. \par


\begin{table}[]
\centering
\caption{Experimental Parameters}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}} &  \multicolumn{1}{c|}{\textbf{Value}}  
 \\ \hline
Carrier frequency $f_{\mathrm{c}}$       & 5.9Ghz                        \\
\hline
Total bandwidth       & 10M                  \\
\hline
Transmit power $p_{ji}^{\mathrm{tx}}$       & 23dbm                          \\
\hline
Power of noise $p_{ji}^{\mathrm{noise}}$             &-95dbm $\sim$  -110dbm                                   \\
\hline
Sensor asynchronous overhead  $t_{ji}^{\mathrm{asyn}}$        &-100ms $\sim$  100ms     \\
\hline
Semantic  extraction time $t_{ji}^\mathrm{{ext}}$       &20ms $\sim$ 40ms   \\
\hline
Multi-scale attention scale set $\mathcal{S}$   & \begin{tabular}[c]{@{}c@{}}{[}64, 96, 352{]}\\ {[}128, 48, 176{]}\\ {[}256, 24, 88{]}\end{tabular} \\ 
\hline
Multi-scale attention output scale $\mathrm{s}_{\mathrm{ms}}$ &{[}128, 96, 352{]} \\ 
\hline
Sensor sampling interval $\mathrm{T}$      & 100ms                              \\ 
\hline
Number of historical prior frames $\mathrm{K}$   & 2\\
\hline
\end{tabular}
\end{table}



\subsection{Perception Performance Comparison}
In this experiment, we evaluate our proposed Select2Col on a total of $664$ test data from the OPV2V dataset and $1,466$ test samples from the V2XSet dataset. To reduce randomness, we use their average inference results as our experimental outcomes.\par

As presented in TABLE IV (``Normal" part), our proposed Select2Col outperforms V2VNet, V2X-Vit, and Where2comm in terms of AP on both  OPV2V and V2XSet datasets. For example, at IoU 0.7, compared with V2VNet, V2V-Vit, and Where2comm, our Select2Col improves the AP performance by 24.69\%/2.09\%/6.30\%, respectively, on the OPV2V dataset. In addition, our proposed Select2Col significantly enhances perception performance  compared to single-agent perception (i.e., no fusion method). For instance, our proposed Select2Col demonstrates an improvement of 9.99\%/10.81\%/15.53\% in perception performance at IoU 0.3/0.5/0.7 on the OPV2V dataset compared to the no-fusion method. \par


\textit{Discussions}: The remarkable results achieved by Select2Col are attributed to its introduction of innovative features. First, selecting contributive collaborators while pruning invalid ones can reduce noise and interference, thereby improving the perception performance. Second, by enhancing the effective features while suppressing noise, the effectiveness of semantic information is boosted. Third, a multi-scale attention module aggregates semantic information from different collaborators with IoSI-consistent weights from the spatial dimension, which provides a comprehensive representation of features. Finally, a short-term attention module is applied with historical prior semantic information, which refines the final fused semantic information from the temporal dimension and further improves the perception performance. Overall, Select2Col comprehensively considers collaborative perception from both spatial and temporal dimensions, unlike other works that only focus on the spatial dimension. As a result, Select2Col achieves unparalleled  performance.\par

\begin{table}
\centering
%\raggedleft
\caption{Overview performance}
\renewcommand{\arraystretch}{1.1}
%\begin{tabular}{|m{0.6cm}<{\centering}|p{1.3cm}<{\centering}|p{0.5cm}<{\centering}  m{0.5cm}<{\centering} m{0.5cm}<{\centering}  |m{0.5cm}<{\centering}  m{0.5cm}<{\centering} m{0.5cm}<{\centering}|}
\begin{tabular}{|c|c|ccc|ccc|}
\hline
\multirow{2}{*}{\makebox[0.6cm]{Dataset}} & \multirow{2}{*}{\makebox[1.3cm]{Methods}} & \multicolumn{3}{c|}{\makebox[1.5cm]{AP (Normal)}}                                                                & \multicolumn{3}{c|}{\makebox[1.5cm]{AP (Noisy)}}         \\ \cline{3-8} 
                         &                          & \multicolumn{1}{c|}{\makebox[0.5cm]{IoU0.3}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{IoU0.5}}          & \makebox[0.5cm]{IoU0.7}          & \multicolumn{1}{c|}{\makebox[0.5cm]{IoU0.3}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{IoU0.5}}          & \makebox[0.5cm]{IoU0.7}          \\ \hline
\multirow{5}{*}{\makebox[0.6cm]{OPV2V}}   
                         & \makebox[1.3cm]{No fusion}           & \multicolumn{1}{c|}{\makebox[0.5cm]{79.81}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{77.70}}          & \makebox[0.5cm]{62.12}          & \multicolumn{1}{c|}{\makebox[0.5cm]{79.81}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{77.70}}          & \makebox[0.5cm]{62.12}          \\ \cline{2-8} 
                         &\makebox[1.3cm]{V2X-Vit}                  & \multicolumn{1}{c|}{\makebox[0.5cm]{87.09}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{85.89}}          & \makebox[0.5cm]{75.56}          & \multicolumn{1}{c|}{\makebox[0.5cm]{86.76}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{85.41}}          & \makebox[0.5cm]{74.51}          \\ \cline{2-8} 
                         &\makebox[1.3cm]{V2VNet}                   & \multicolumn{1}{c|}{\makebox[0.5cm]{82.71}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{80.38}}          & \makebox[0.5cm]{52.96}          & \multicolumn{1}{c|}{\makebox[0.5cm]{82.24}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{78.33}}          & \makebox[0.5cm]{44.72}          \\ \cline{2-8} 
                         &\makebox[1.3cm]{Where2comm}               & \multicolumn{1}{c|}{\makebox[0.5cm]{86.71}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{85.20}}          & \makebox[0.5cm]{71.35}          & \multicolumn{1}{c|}{\makebox[0.5cm]{86.47}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{84.30}}          & \makebox[0.5cm]{66.52}          \\ \cline{2-8} 
                         & \textbf{\makebox[1.3cm]{Select2Col}}         & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{89.80}}} & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{88.51}}} & \textbf{\makebox[0.5cm]{77.65}} & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{89.73}}} & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{88.20}}} & \textbf{\makebox[0.5cm]{74.54}} \\ \hline
\multirow{5}{*}{\makebox[0.6cm]{V2XSet}}  
                         & \makebox[1.3cm]{No fusion}           & \multicolumn{1}{c|}{\makebox[0.5cm]{75.55}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{71.17}}          & \makebox[0.5cm]{47.12}          & \multicolumn{1}{c|}{\makebox[0.5cm]{75.55}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{71.17}}          & \makebox[0.5cm]{47.12}          \\ \cline{2-8} 
                         &\makebox[1.3cm]{V2X-Vit}                  & \multicolumn{1}{c|}{\makebox[0.5cm]{81.54}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{78.76}}          & \makebox[0.5cm]{62.36}          & \multicolumn{1}{c|}{\makebox[0.5cm]{80.77}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{77.02}}          & \makebox[0.5cm]{56.90}          \\ \cline{2-8} 
                         &\makebox[1.3cm]{V2VNet}                   & \multicolumn{1}{c|}{\makebox[0.5cm]{77.85}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{72.27}}          & \makebox[0.5cm]{46.99}          & \multicolumn{1}{c|}{\makebox[0.5cm]{76.12}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{68.54}}          & \makebox[0.5cm]{36.23}          \\ \cline{2-8} 
                         &\makebox[1.3cm]{Where2comm}               & \multicolumn{1}{c|}{\makebox[0.5cm]{82.70}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{79.05}}          & \makebox[0.5cm]{57.16}          & \multicolumn{1}{c|}{\makebox[0.5cm]{80.74}}          & \multicolumn{1}{c|}{\makebox[0.5cm]{74.89}}          & \makebox[0.5cm]{51.15}          \\ \cline{2-8} 
                         &\textbf{\makebox[1.3cm]{Select2Col}}         & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{87.18}}} & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{84.25}}} & \textbf{\makebox[0.5cm]{64.92}} & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{84.90}}} & \multicolumn{1}{c|}{\textbf{\makebox[0.5cm]{78.74}}} & \textbf{\makebox[0.5cm]{58.41}} \\ \hline
\end{tabular}
\end{table}


\subsection{Computational Efficiency Comparison}

Table V and Fig. 5 evaluate the computing efficiency of Select2Col. Specifically, as demonstrated in TABLE V, our Select2Col has fewer network model parameters than V2X-Vit and V2VNet and a similar number of parameters to  Where2comm. In other words, compared to other spatial dimension-only solutions, the incorporation of  both spatial and temporal dimensions of IoSI in selecting collaborators and fusing semantic information does not add  any    computational complexity. Furthermore, as depicted in Fig. 5, the inference time of our proposed Select2Col is significantly lower than that of V2VNet and V2X-Vit, and slightly higher than that of Where2comm due to the introduction of the lightweight GNN and the short-term attention module; however, Select2Col outperforms Where2comm in terms of perception performance. The inference time of Select2Col is much less than 100 ms, which is  acceptable  given  the perception cycle (i.e., $10$ Hz) of the LiDAR sensing device.\par

\textit{Discussions}: Compared to Where2comm, the newly added  lightweight GNN and short-term attention module can enhance the perception performance and are easily handled by mainstream computing devices without an excessive computational burden. Overall, Select2Col generates superior perception performance with a high computational efficiency.\par



\begin{table}[!t]
\centering
\caption{Number of  network model parameters}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|}
\hline
\textbf{Works} & \begin{tabular}[c]{@{}c@{}}\textbf{Number of} \\  \textbf {paramters  (M)}\end{tabular}    \\ \hline
%\textbf{Works}  &\textbf{Number of paramters  (M)} 
V2VNet         &14.6122       \\ \hline
V2X-Vit        &12.4554    \\ \hline
Where2comm     &8.0574     \\ \hline
\textbf{Select2Col}        &\textbf{8.2875}   \\ \hline
\end{tabular}
\end{table}


% Figure environment removed



\subsection{Perception Performance under Distance}

Collaborative perception improves the agent's ability to perceive objects that are located far away. Fig. 6 to Fig. 8 illustrate the perception performance of our proposed Select2Col and other SOTA methods at different object distances on the OPV2V and V2XSet datasets.\par

% Figure environment removed


% Figure environment removed

% Figure environment removed



It is evident that the performance decreases as the distance increases. Nonetheless, our proposed Select2Col is still able to achieve the highest perception accuracy in long-distance perception scenarios. For instance, at IoU of 0.5 and object distance of 60 meters, Select2Col improves the AP performance by 16.97\%/9.18\%/10.07\% in comparison to V2VNet, V2X-Vit and Where2comm, respectively, on the V2XSet dataset.\par

\textit{Discussions}: The efficacy of Select2Col in perceiving distant objects lies in its ability to enhance the fused semantic information of collaborators in both spatial and temporal dimensions with multi-scale attention and short-term attention, thus enhancing the capability to perceive distant objects.  \par




% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Perception Performance under Latency}
This experiment aims to evaluate the influence of information latency on perception performance. To achieve this, we add a neighboring agent with a specific latency based on the previous experiment.\par

Fig. 9 to Fig. 11 demonstrate the results of experiments conducted under different IoU conditions. As depicted in these figures, the collaborator with a small latency has the potential to enhance perception performance, whereas a large latency has a negative impact. Furthermore, our proposed Select2Col exhibits the best performance among all approaches and remains performance-stable even under large latency. For instance, at IoU 0.3, when the latency of the collaborator is 400 ms, Select2Col enhances the AP performance by 11.19\%/7.75\%/5.23\% on the V2XSet dataset compared to V2VNet, V2X-Vit, and Where2comm, respectively.\par

\textit{Discussions}: As anticipated, the results of the experiment support our hypothesis that timely perception information leads to a more significant profit gain, while large delayed information introduces noise. Hence, collaborative perception should take into account the IoSI from the temporal dimension. Benefitting from collaborator selection and semantic information fusion, Select2Col is more robust than other methods.\par

\subsection{Perception Performance under Localization Noise}
Notably, localization noise can be widely observed due to the impact of spatial and temporal misalignment. To evaluate the robustness of our proposed Select2Col, we add localization noise to the test data, which follows a Gaussian distribution with a standard deviation of 0.2 for both position and heading.\par

As presented in TABLE IV (``Noisy" part), localization noise impairs the perception performance of all approaches, and the reduction is more noticeable when the IoU is larger (i.e., the detection difficulty is increased). This indicates that high-precision detection is more vulnerable to noise. However, compared to other methods, our proposed Select2Col shows superior noise suppression ability. For instance, on the V2XSet dataset, Select2Col maintains an AP of 78.74\%/58.41\%  at IoU 0.5 and 0.7, presenting an improvement of 10.20\%/22.18\%, 1.72\%/1.51\% and 3.85\%/7.26\%  compared to V2VNet, V2X-Vit, and Where2comm, respectively. The same conclusion can be observed in the OPV2V dataset.\par

\textit{Discussions}: Select2Col introduces historical prior semantic information and utilizes a short-term attention module to learn the correlation of semantic information from the temporal dimension. This enables the suppression of spatial localization noise for the current semantic information by using historical information. As a result, the localization noise has less impact on Select2Col.\par




\subsection{Ablation Studies}
In this subsection, we evaluate the effectiveness of the innovations incorporated in our proposed Select2Col. As previously mentioned, Select2Col includes two novel innovations: i) the collaborator selection method; ii) the  semantic information fusion algorithm HPHA. \par

\begin{table}
\centering
\caption{Innovative Component Ablation Study Results}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|m{0.8cm}<{\centering}|m{0.7cm}<{\centering}|m{1.4cm}<{\centering}|m{0.8cm}<{\centering}|m{0.8cm}<{\centering}|m{0.8cm}<{\centering}|}
\hline
\textbf{Open dataset} & \textbf{HPHA} &\textbf{Collaborator selection}  &\textbf{AP at IoU0.3} &\textbf{AP at IoU0.5} &\textbf{AP at IoU0.7}\\
\hline
\multirow{3}* {OPV2V} &\ding{56} &\ding{56} &79.81	&77.70	&62.12 \\
\cline{2-6}
                      &\ding{52} &\ding{56} &88.11	&86.13	&73.91 \\
\cline{2-6}
			      &\ding{52} &\ding{52} &\textbf{89.80}	&\textbf{88.51}	&\textbf{77.65} \\
\cline{2-6}
\hline

\multirow{3}* {V2XSet}&\ding{56} &\ding{56} &75.55	&71.17	&47.12 \\
\cline{2-6}
                      &\ding{52} &\ding{56} &85.47	&81.85	&61.52 \\
\cline{2-6}
			      &\ding{52} &\ding{52} &\textbf{87.18}	&\textbf{84.25}	&\textbf{64.92} \\
\cline{2-6}

\hline
\end{tabular}
\end{table}


TABLE VI presents the results of our ablation studies. The outcome clearly illustrates that each innovative component significantly contributes to the advancement of perception performance. Specifically, the collaborator selection method enhances the perception performance of AP by 1.71\%, 2.40\% and 3.40\%, respectively, on the V2XSet dataset. Moreover, HPHA improves the perception performance of AP by 9.92\%, 10.68\%, and 14.4\%  on the V2XSet dataset. In addition, we conclude that the gains from the collaborator selection method and HPHA are more pronounced when the value of IoU is larger (i.e., the detection difficulty is increased). This trend agrees with our previous experimental findings and is more apparent in HPHA. \par



\textit{Discussions}: The collaborator selection method benefits the removal of unsuitable collaborators that cause noise, resulting in improved perception performance. Likewise, the gain from HPHA is obtained due to the efficient utilization of the correlation between temporal and spatial dimensions embedded in the semantic information for information fusion. Thus, both techniques can effectively enhance the perception performance.\par









\section{CONCLUSION}
In this article, we have proposed Select2Col, a novel collaborative perception framework that improves perception performance based on IoSI from both spatial and temporal dimensions. Specifically, we have designed a collaborator selection method that capably selects contributive collaborators efficiently. To further boost the perception performance, we have presented a semantic information fusion algorithm named HPHA by integrating a multi-scale attention module and a short-term attention module to capture the IoSI from both spatial and temporal dimensions and aggregate semantic information by assigning IoSI-consistent weights. Extensive experimental results are conducted on two open datasets, OPV2V and V2XSet, and demonstrate that Select2Col outperforms SOTA methods in terms of perception performance.\par

There still exist many promising means to improve the performance of our Select2Col framework. For example, since one-dimensional weight for selecting collaborators encounters limitations, a more accurate high-dimensional representation method is highly anticipated. Furthermore, it is worthwhile to investigate the impact of collaborative privacy concerns and malicious behaviors to mitigate potential risks. 



\par





\section*{Acknowledgments}
This work was supported in part by the  National Key Research and Development Program of China under Grant 2021YFB2900200, in part by the National Natural Science Foundation of China under Grant 62071425, in part by the Zhejiang Key Research and Development Plan under Grant 2022C01129, and in part by the Zhejiang Provincial Natural Science Foundation of China under Grant LR23F010005.


\bibliographystyle{IEEEtran} 
\bibliography{ref}

%
%

%\begin{thebibliography}{1}
%\bibliographystyle{IEEEtran}

%\bibitem{ref1}
%{\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf
%\bibitem{ref2}
%T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

%\end{thebibliography}


\newpage

\section{Biography Section}
\vspace{11pt}

\begin{IEEEbiographynophoto}{YUNTAO LIU}
is a Ph.D. candidate at Zhejiang University as well as a senior engineer at Zhejiang Lab, Hangzhou, China. His research interests currently focus on collective intelligence and distributed systems.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{QIAN HUANG}
is a senior Engineer at Zhejiang Lab, Hangzhou, China. Her research interests currently focus on computer vision and the Internet of Vehicles.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{RONGPENG LI}
is an assistant professor at Zhejiang University Hangzhou, China. His research interests currently focus on multi-agent reinforcement learning and network slicing.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{XIANFU CHEN}
is a senior scientist at VTT Technical Research Centre of Finland, Oulu, Finland.  His research interests currently focus on human-level and artificial intelligence for resource awareness in next-generation communication networks. 
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{ZHIFENG ZHAO}
is a principal researcher with Zhejiang Lab as well as with Zhejiang University, Hangzhou, China. His research area includes collective intelligence and software-defined networks.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{SUYUAN ZHAO}
is working as a postdoctoral researcherÂ in Zhejiang Lab, Hangzhou, China. HisÂ current research interests include Internet of Vehicles and deep learning.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{YONGDONG ZHU}
is a principal researcher at Zhejiang Lab, Hangzhou, China. His research interests currently focus on Internet of Vehicles and intelligent transportation.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{HONGGANG ZHANG}
is a principal researcher with Zhejiang Lab as well as  a professor with Zhejiang University, Hangzhou, China. He is currently involved in research on cognitive green communications.
\end{IEEEbiographynophoto}

\vfill


\end{document}


