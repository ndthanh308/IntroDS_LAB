\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage {amssymb}
\usepackage {pifont}
\usepackage{multirow}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Rethinking Collaborative Perception from the Spatial-Temporal Importance of Semantic Information}

\author{Yuntao~Liu,~Qian~Huang,~Rongpeng~Li,~Xianfu~Chen,~Zhifeng~Zhao,~Shuyuan~Zhao,~Yongdong~Zhu and Honggang~Zhang
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}
\thanks{Y. Liu is with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China as well as with Zhejiang Lab, Hangzhou 311121, China (e-mail: liuyuntao@zju.edu.cn) .}% <-this % stops a space
\thanks{R. Li is with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: lirongpeng@zju.edu.cn).}% <-this % stops a space
\thanks{X. Chen is with the VTT Technical Research Centre of Finland, Oulu, 90570, Finland (e-mail: xianfu.chen@vtt.fi).}% <-this % stops a space
\thanks{Z. Zhao and H. Zhang are with Zhejiang Lab, Hangzhou 311121, China (e-mail: \{zhaozf, honggangzhang\}@zhejianglab.com) as well as with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China.}
\thanks{Q. Huang, S. Zhao and Y. Zhu are with Zhejiang Lab, Hangzhou 311121, China (e-mail: \{huangq, zhaosy, zhuyd\}@zhejianglab.com).}
\thanks{Y. Liu and Q. Huang are co-first authors, R. Li is corresponding author.}% <-this % stops a space
% <-this % stops a space
%\thanks{Manuscript received August 19, 2022; revised October 26, 2022.}
}


% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Collaboration by the sharing of semantic information is crucial to enable the enhancement of perception capabilities. However, existing collaborative perception methods tend to focus solely on the spatial features of semantic information, while neglecting the importance of the temporal dimension in collaborator selection and semantic information fusion, which instigates performance degradation. In this article, we propose a novel collaborative perception framework, IoSI-CP, which takes into account the importance of semantic information (IoSI) from both temporal and spatial dimensions. Specifically, we develop an IoSI-based collaborator selection method that effectively identifies advantageous collaborators but excludes those that bring negative benefits. Moreover, we present a semantic information fusion algorithm called HPHA (historical prior hybrid attention), which integrates a multi-scale transformer module and a short-term attention module to capture IoSI from spatial and temporal dimensions, and assigns varying weights for efficient aggregation. Extensive experiments on two open datasets demonstrate that our proposed IoSI-CP significantly improves the perception performance compared to state-of-the-art approaches. The code associated with this research is publicly available at https://github.com/huangqzj/IoSI-CP/.
\end{abstract}

\begin{IEEEkeywords}
Collaborative Perception, Semantic Information, Importance of Semantic Information, Spatial-Temporal Dimensions, Hybrid Attention.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{P}{erception} is an essential capability for agents in the Internet of Vehicles (IoV), especially for autonomous vehicles (AVs). Agents rely on their sensors to obtain perception capabilities, known as single-agent perception. Single-agent perception usually encounters significant challenges in real-world scenarios with occluded or distant objects \cite{1} and possibly leads to catastrophic outcomes, as depicted in Fig. 1. It can be seen that, from the perspective of  vehicle \emph{i}, the pedestrian is occluded by vehicle \emph{j}, causing a dangerous collision risk for the pedestrian when vehicle \emph{i} accelerates. Such collisions occur frequently. \par

Fortunately, collaborative perception allows neighboring agents to share perception information, which mutually enhances the understanding of their surroundings beyond individual capabilities. In terms of the detailed content of shared perception information, collaborative perception can be classified as early fusion \cite{2}, late fusion\cite{3,4}, and intermediate fusion\cite{5,6,7,8,9,10}. Specifically, the former two categories, which share the raw data or perception result (e.g., object classification and regression result), could either inflict significant communication cost or yield limited performance due to the inadequately shared scene context. Instead, intermediate fusion, which involves sharing extracted feature information (i.e., semantic information) from raw perception data, has emerged as a promising solution to effectively balance perception performance and communication overhead \cite{6}. \par

% Figure environment removed

However, as semantic information suffers from diverse transmission delays, heterogeneous sensor configurations and the distinct views of agents, the shared semantic information could be rather asynchronous and misaligned in both temporal and spatial dimensions. For example, the semantic information of different collaborators may offer varying sizes and spatial positions for the same object. Therefore, the accurate perception of objects from fused semantic information remains a challenging problem. In addition, selecting appropriate collaborators is crucial, as unsuitable candidates can introduce noise and interference, negatively affecting performance. Nonetheless, identifying beneficial collaborators that can enhance perception performance is also challenging, given their diverse spatial-temporal perspectives.\par

Several recent studies, including CoFF\cite{5}, V2VNet\cite{6}, V2X-Vit\cite{7},  Where2comm\cite{8}, Who2com\cite{9} and When2com\cite{10} propose  efficient approaches for collaborative perception. However, they tend to focus on the spatial aspect of semantic information only and ignore the impact of the temporal dimension in collaborator selection and information fusion, which impedes the improvement of  perception performance. For instance, suppose that a collaborator shares outdated semantic information that includes an object no longer available in the focus region of ego; then, neglecting the temporal dimension  might generate an incorrect perception output that wrongly identifies the position of the corresponding object. Thus, it is obviously crucial to consider the spatial and temporal dimensions of semantic information for successful collaborative perception. Besides, the importance of semantic information (IoSI) provided by diverse collaborators in enhancing the perception performance shows variation. \par

In this article, we propose an importance of semantic information (IoSI)-based collaborative perception framework called IoSI-CP to more comprehensively take into account the IoSI from both temporal and spatial dimensions. In addition, we develop a collaborator selection method that efficiently selects collaborators producing significant perception performance benefits. Moreover, we design a semantic feature fusion algorithm to implement more effective information fusion by assigning IoSI-consistent weights.  \par

Accordingly, the main contributions of this article can be summarized as follows. \par
1)We introduce IoSI into collaborative perception from spatial and temporal dimensions, offering a unique perspective to optimize the perception performance. \par
2) We propose IoSI-CP, an IoSI-based collaborative perception framework, which can significantly improve the perception performance. In addition, we open related source codes to inspire further research in this field.\par
3) We design a semantic information fusion algorithm called HPHA, which employs a multi-scale transformer module and a short-term attention module to efficiently fuse semantic information from the spatial and temporal dimensions of IoSI. Moreover, we present a collaborator selection method based on IoSI to select effective collaborators and disregard those that negatively impact the perception performance.\par
4) We carry out a detailed evaluation of our proposed IoSI-CP on two open datasets OPV2V and V2XSet. The experimental results show that IoSI-CP is more effective in improving perception performance than state-of-the-art (SOTA) works, such as V2VNet\cite{6}, V2X-Vit\cite{7} and Where2comm\cite{8}.\par

The remainder of this article is organized as follows. Section II reviews the related work on the collaborative perception of IoV. Section III explains the collaborative perception problem from a mathematical perspective. Section IV elaborates on our proposed IoSI-CP framework. Section V introduces our innovative HPHA semantic information fusion algorithm. We conduct experiments to verify our proposed IoSI-CP in Section VI. Finally, we conclude this article with a summary in Section VII.\par



\section{Related Work}

\subsection{Means of Implementation for Perception in IoV}
The IoV involves repetitive communications to timely exchange semantic information among various agents \cite{11}. Given that AVs form the foundation of IoV, we take AVs as an example and focus on their perception means. \par

Perception plays a crucial role in various tasks, such as 3D object detection \cite{12}, multiple object tracking\cite{13}, semantic segmentation \cite{14}, and depth estimation \cite{15}. As one of the fundamental tasks, 3D object detection can be categorized into camera-based, LiDAR-based and camera-LiDAR fusion detection methods. Due to the inherent 3D information provided by LiDAR point clouds \cite{16}, LiDAR-based detection is widely utilized in collaborative perception tasks. In such tasks, voxel-based \cite{17} or pillar-based \cite{18} strategies can be employed to extract features from unstructured point clouds. \par

Pillar-based feature extraction requires fewer computational and memory resources than voxel-based feature extraction  \cite{19} . Therefore, most state-of-the-art collaborative perception methods, such as V2VNet \cite{6} , V2X-Vit \cite{7} and Where2comm \cite{8} , adopt the pillar-based feature extraction strategy. \par

Consistent with these works, we primarily consider the pillar-based feature extraction strategy and focus on how to extend to collaborative perception, given the severe limitation of single-agent perception \cite{19}.\par

\begin{table}[!t]
\caption{Comparison of related work}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|m{1.5cm}<{\centering}|m{2.7cm}<{\centering}|m{2.6cm}<{\centering}|}
\hline
\textbf{Works} & \textbf{Strategy of collaborator selection} & \textbf{Strategy of fusion} \\ \hline
V2VNet         & All agents                                                                             &Spatial dimension                                                              \\ \hline
When2com       &Spatial dimension                                                                      &Spatial dimension                                                              \\ \hline
CoFF           &All agents                                                                             &Spatial dimension                                                              \\ \hline
V2X-Vit        &All agents                                                                             &Spatial dimension                                                              \\ \hline
Where2comm     &Spatial dimension                                                                      &Spatial dimension                                                              \\ \hline
IoSI-CP        &Spatial-temporal dimensions                                                            &Spatial-temporal dimensions                                                    \\ \hline
\end{tabular}
\end{table}

\subsection{Related Studies in Collaborative Perception}
V2VNet\cite{6} is a pioneering framework that explores the utilization of vehicle-to-vehicle communication to enhance the perception performance of autonomous vehicles. In particular, it leverages  a spatially aware graph neural network (GNN) to aggregate information from all neighboring vehicles to intelligently fuse various angles and time frames of the same scene. V2X-ViT\cite{7} devises a unified vision transformer architecture consisting of heterogeneous multi-agent self-attention and multi-scale window self-attention, thus effectively capturing the spatial relationship between each agent. Meanwhile, CoFF\cite{5} fuses feature information by enhancing the spatial feature information of the original perception data. \par

Although the utilization of all neighboring agents as collaborators in V2VNet, V2X-ViT and CoFF achieves benefits, studies involving Who2com\cite{9} ,V2X-ViT\cite{7}, and \cite{20} demonstrate that adding more collaborators does not always improve perception; in fact, in some cases, it can even lead to negative returns. Therefore, the current research focuses on selecting effective collaborators in addition to information fusion. For example, when2com \cite{10} creates communication groups from neighboring agents based on the spatial feature similarity of shared information and determines when to collaborate within these groups. On the other hand, Where2comm\cite{8} employs a spatial confidence map to select collaborators and utilizes multi-head transform attention to fuse their information effectively. \par

To date, significant contributions have been made to collaborator selection and information fusion. However, our work differs from previous studies, as highlighted in TABLE I. It is unique due to the emphasis on using both spatial and temporal dimensions of IoSI in selecting collaborators and fusing semantic information, as other studies primarily focus on the spatial dimension alone. Moreover, we introduce a novel framework called IoSI-CP, which aims to enhance the perception performance based on IoSI. \par

\section{System Model and Problem Formulation}
\subsection{System Model }

The collaborative perception system incorporates agents that capably perceive the environment at a fixed interval and can share their perception information with other agents. We adopt the intermediate feature fusion strategy\cite{6}, which aligns with state-of-the-art methods such as V2X-Vit\cite{7} and Where2comm\cite{8}.

\begin{table}[]
\caption{Notations and Explanation}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Notation}} &  \multicolumn{1}{c|}{\textbf{Explanation}}  
 \\ \hline
$N$        & Total number of agents                                                          \\
 \hline
$X_{i}\!^{\!\left(\!t\!\right)}$       & Raw perception data of agent \emph{i} at timestamp $t$                        \\
\hline
$N_{i}\!^{\!\left(\!t\!\right)}$       & Set of neighbor agents for agent \emph{i} at timestamp $t$                   \\
\hline
$C_{i}\!^{\!\left(\!t\!\right)}$       & Set of collaborators for agent \emph{i} at timestamp $t$                            \\
\hline
$F_{i}\!^{\!\left(\!t\!\right)}$         & \begin{tabular}[c]{@{}l@{}}Semantic information extracted from raw perception data \\ of agent \emph{i} at timestamp $t$\end{tabular}                                       \\
\hline
$\widehat{Y}_{i}\!^{\left( \!t \right)}$       & \begin{tabular}[c]{@{}l@{}}Final perception result of agent \emph{i} based on the perception \\   information at timestamp $t$\end{tabular}                                          \\
\hline
$Y_{i}\!^{\!\left(\!t\!\right)}$       & \begin{tabular}[c]{@{}l@{}}Ground truth of the perception result of  agent \emph{i} at  \\ timestamp $t$\end{tabular}                                                 \\
\hline
$f_\mathrm{{enc}}$   & \begin{tabular}[c]{@{}l@{}}Encoder fuction that extracts  semantic information from\\   raw perception data\end{tabular}                                         \\
\hline
$f_\mathrm{{select}}$   & \begin{tabular}[c]{@{}l@{}}Selection fuction that outputs the selected collaborators\\ \end{tabular}                                         \\
\hline
$f_\mathrm{{fuse}}$   & \begin{tabular}[c]{@{}l@{}}Fusion fuction that outputs the fused semantic information \\ \end{tabular}                                         \\
\hline
$f_\mathrm{{dec}}$   & \begin{tabular}[c]{@{}l@{}}Decoder function that outputs final perception result  \\  \end{tabular}                                      \\
\hline
$H_{i}\!^{\!\left(\!t\!\right)}$       & \begin{tabular}[c]{@{}l@{}}Fused semantic information of agent \emph{i} at timestamp $t$\\  \end{tabular}                                                 \\        
\hline   
$F_{j\_\mathrm{en}}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$  & \begin{tabular}[c]{@{}l@{}}Enhanced semantic information of agent \emph{j} at timestamp \emph{i}\\ \end{tabular} \\
\hline 
$\tau _{ji}$  &Latency of agent \emph{j}'s raw perception data for agent \emph{i} \\
\hline
$t_{ji}^\mathrm{{C}}$       & \begin{tabular}[c]{@{}l@{}}Computing time of semantic information extraction \\  from raw perception data\end{tabular} \\          
\hline                                         
$t_{ji}^\mathrm{{N}}$      & \begin{tabular}[c]{@{}l@{}}Transmission time of shared semantic information from\\   agent \emph{j} to agent \emph{i} \end{tabular}                               \\
\hline
$\varDelta t_{ji}$   & \begin{tabular}[c]{@{}l@{}}Asynchronous overhead between the sensors of agent \emph{j}  \\ and agent \emph{i} \end{tabular}                      \\
\hline
$B_j$       & Bandwidth of agent \emph{j}                                                                                                                               \\
\hline
$PL_{ji}$       & Path loss between agent \emph{j} and agent \emph{i}                                                                                                               \\
\hline
$P_{ji}^\mathrm{{TX}}$      & \begin{tabular}[c]{@{}l@{}}Transmission power of agent \emph{j} to agent \emph{i} \\ \end{tabular} \\    
\hline
$P_{ji}^\mathrm{{Noise}}$      & \begin{tabular}[c]{@{}l@{}}Noise power between agent \emph{j} and agent \emph{i} \\  \end{tabular} \\    
\hline
$d_{ji}$     & Distance between agent \emph{j} and \emph{i}                                                                                                                 \\
\hline
$f_\mathrm{c}$       & Center frequency in GHz                                                                                                                                        \\
\hline
T       & Perception cycle  of sensor device                                                                                                                                   \\
\hline
$m_j$       & \begin{tabular}[c]{@{}l@{}}Sparse map of agent \emph{i} \\\end{tabular}                                                 \\     
\hline     
$w_{j}^\mathrm{{en}}$       & \begin{tabular}[c]{@{}l@{}}Enhanced weight of agent \emph{j} to agent \emph{i} \\ \end{tabular}                                                 \\    
\hline
$S$       & Attention scale size                                                                                                                                       \\
\hline
$W_{S,j\rightarrow i}^\mathrm{{att}}$ & Attention weight of agent \emph{j} to agent \emph{i} at scale size $S$ \\
\hline
$W^\mathrm{{ST}}$ &Short-term weight \\
\hline
$H_{S,\mathrm{att}}^{\left( t \right)}$  & \begin{tabular}[c]{@{}l@{}}Aggregated feature semantic information after  attention \\   operation at scale size $S$\end{tabular}                                                \\ 
\hline
$H_\mathrm{{att}}^{\left( t \right)}$  & \begin{tabular}[c]{@{}l@{}}Aggregated feature semantic information after  multi-scale\\   attention operation\end{tabular}        \\    
\hline

\end{tabular}
\end{table}
Table II presents a list of notations and their corresponding explanations to facilitate comprehension. \par

 Assuming a collaborative perception system comprising \emph{N} agents, each agent can engage in a collaborative perception with its neighboring agents. To illustrate the process of collaborative perception, let us consider agent \emph{i} performing collaborative perception at timestamp \emph{t}, as shown in Fig. 1.\par

Prior to  performing collaborative perception, agent \emph{i} possesses  raw perception data $X_{i}^{\left( t \right)}$ , and corresponding semantic information $F_{i}^{\left( t \right)}$ obtained through an encoder function $f_{\mathrm{enc}}\left( \cdot \right) $ to extract from $X_{i}^{\left( t \right)}$, as follows:
\begin{align}
F_{i}^{\left( t \right)}\,\,=f_{\mathrm{enc}}\left( X_{i}^{\left( t \right)} \right) , 
\end{align}
where $f_{\mathrm{enc}}\left( \cdot \right)$ is implemented by the backbone network of Pointpillars\cite{18}.

In addition, we let $N_{i}^{\left( t \right)}$ $\left( \left| N_{i}^{\left( t \right)} \right|\leqslant N \right)$ represent the set of neighboring agents for agent \emph{i}.  Assuming a nonideal environment with uncontrollable communication and computation latency, $\tau _{ji}^{\left( t \right)}$ denotes the latency of the raw perception data of neighboring agent \emph{j} for agent \emph{i}, and $F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$ represents the semantic information of agent \emph{j}, where $j\in N_{i}^{\left( t \right)}$. \par

Since not all neighboring agents contribute to improving the perception performance,  agent \emph{i} employs  a collaborator selection function $f_{\mathrm{select}}\left( \cdot \right)$  to determine  a suitable collaborator set $C_{i}^{\left( t \right)}$, such as:
\begin{align}
C_{i}^{\left( t \right)}=f_{\mathrm{select}}\left( F_{i}^{\left( t \right)}, \sum_{j\in N_{i}^{\left( t \right)}}{F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}} \right) .
\end{align}

If $C_{i}^{\left( t \right)}$ is an empty set, this indicates that agent \emph{i} is not engaging in collaborative perception with any other agent. For simplicity, we omit the superscript $t$ hereafter because  $\tau _{ji}^{\left( t \right)}$ is time-varying.  Furthermore,  $\tau _{ji}^{}$ can be defined as the sum of semantic information extraction calculation time $t_{ji}^{\mathrm{C}}$ , network transmission time  $t_{ji}^{\mathrm{N}}$, and sensor asynchronous  overhead $\varDelta t_{ji}$, as follows,
\begin{align}
\tau _{ji}=t_{ji}^{\mathrm{C}}+t_{ji}^{\mathrm{N}}+\varDelta t_{ji} .
\end{align}

And according to the specification 3GPP TR 38.901, network transmission time $t_{ji}^{\mathrm{N}}$  can be  expressed through the following equations: 
\begin{align}
t_{ji}^{\mathrm{N}}\,\,=\,\,\frac{\mathrm{size}\left( F_{ji}^{\left( t \right)} \right)}{B_j\log _2\left( 1+10^{0.1\left( P_{ji}^{\mathrm{TX}}\,\,-PL_{ji}-P_{ji}^{\mathrm{Noise}} \right)} \right)}, \\
PL_{ji}=28.0+22\log _{10}\left( d_{ji} \right) +20\log _{10}\left( f_{\mathrm{c}} \right)  ,
\end{align}
where $\mathrm{size}\left( \cdot \right) $ denotes a function that calculates the data size of the semantic information, $B_j$ represents the bandwidth of agent \emph{j}, $PL_{ji}$ represents the path loss between agent \emph{j} and agent \emph{i} measured in db,  $P_{j}^{\mathrm{TX}}$ represents the transmission power of agent \emph{j} to agent \emph{i},  $P_{j}^{\mathrm{Noise}}$ represents the transmission noise power between agent \emph{j} and agent  \emph{i},  $d_{ji}$ represents the distance between agent  \emph{j} and  \emph{i} in meters, and  $f_{\mathrm{c}}$ represents  the center frequency in GHz. Note that both $P_{j}^{\mathrm{TX}}$ and  $P_{j}^{\mathrm{Noise}}$ are  represented in dBm. \par

$H_{i}^{\left( t \right)}$ denotes  the fused semantic information of agent \emph{i}, which can be obtained through a fusion function $f_{\mathrm{fuse}}\left( \cdot \right) $  as follows:
\begin{align}
H_{i}^{\left( t \right)}=f_{\mathrm{fuse}}\left( F_{i}^{\left( t \right)},\sum_{j\in C_{i}^{\left( t \right)}}{F_{j}^{\left( t-\tau _{ji} \right)}} \right) .
\end{align}

Finally, $\widehat{Y}_{i}^{\left( t \right)}$ represents the final perception result of agent \emph{i}, which is the output of the decoder function $f_{\mathrm{dec}}\left( \cdot \right)$  with  fused semantic information $H_{i}^{\left( t \right)}$ as the input. This can be expressed as follows:
\begin{align}
\widehat{Y}_{i}^{\left( t \right)}=f_{\mathrm{dec}}\left( f_{\mathrm{fuse}}\left( F_{i}^{\left( t \right)},\sum_{j\in S_{i}^{\left( t \right)}}{F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}} \right) \right) ,
\end{align}
where  $f_{\mathrm{dec}}\left( \cdot \right) $ is implemented by a  convolutional neural network. 

\subsection{Problem Formulation}
The collaborative perception problem is formulated by referring to state-of-the-art methods, such as V2VNet\cite{6}, V2X-ViT\cite{7}, and Where2comm\cite{8}. The objective of collaborative perception is to maximize the perception performance of all agents.

\begin{equation}
\begin{aligned}
\max \! \frac{1}{N}  \!\! \sum_{i\in N} & {G \!\!\left( \!\!  f_{\mathrm{dec}} \! \!\left(f_{\mathrm{fuse}}\!\!\left( F_{i}^{\left( t \right)}\!,\!\!\sum_{j\in C_{i}^{\left( t \right)}}\!\!\!\!{F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}} \!\right) \!\right) ,  Y_{i}^{\left( t \right)} \!\!\right)}
\\
&s.t. \  \    C_{i}^{\left( t \right)}\subseteq N_{i}^{\left( t \right)}, \left| N_{i}^{\left( t \right)} \right|\leqslant N \ ,
\end{aligned}
\end{equation}
where $G\left( \cdot \right) $ represents the perception evaluation function, which indicates the accuracy of the predicted result relative to the ground truth.\par

Given that the encoder function $f_{\mathrm{enc}}\left( \cdot \right) $, decoder function $f_{\mathrm{dec}}\left( \cdot \right) $, and evaluation function $G\left( \cdot ,\cdot \right)$  are typically common and well-established elements, the primary challenge in tackling the collaborative perception problem lies in designing the collaborator selection function $f_{\mathrm{select}}\left( \cdot \right) $  and fusion function  $f_{\mathrm{fuse}}\left( \cdot \right) $. The first one involves identifying appropriate collaborators that can provide high-quality perceptual semantic information, and the second involves exploiting the correlation  of semantic information to improve the feature representations. In our proposed IoSI-CP, we devise an IoSI-based collaborator selection method to optimize the selection of collaborators and a semantic information fusion algorithm HPHA to enhance fusion efficacy.



\section{IoSI-CP: An IoSI-based Collaborative Perception Framework}
Based on the previous analysis, we propose an IoSI-based collaborative perception framework, IoSI-CP. In this framework, we design a new collaborator selection component to select effective collaborators according to the spatial and temporal dimensional importance of semantic information. Furthermore, we present HPHA (historical prior hybrid attention), a hybrid attention semantic information fusion algorithm to efficiently fuse semantic information from the spatial and temporal dimensions.\par

\subsection{Overview of IoSI-CP}

% Figure environment removed

As illustrated in Fig. 2, the proposed IoSI-CP framework has four components (i.e., encoder, collaborator selection, fusion, and decoder).\par

\subsubsection{Encoder component}The function of this component is to extract semantic information from the raw perception data. In this article, we focus on LiDAR-based 3D object detection as the perception task. Correspondingly, we adopt Pointpillars\cite{18} as our encoder backbone because of its low inference latency and optimized memory usage,  consistent with the literature\cite{6,7,8}.\par

\subsubsection{Collaborator Selection component}It is crucial to ensure that the selected collaborators positively enhance the perception performance and do not introduce any adverse effects. Thus, we propose a collaborator selection method to effectively identify beneficial collaborators while excluding those that bring negative benefits based on IoSI. Specifically, we utilize a convolutional network to produce a sparse map of semantic information. Then, we design a GNN network \cite{21} to learn the importance of semantic information provided by the collaborators and eliminate those who cannot contribute positively to improving the perception performance.\par

\subsubsection{Fusion component}This component belongs to the core part of IoSI-CP, which is responsible for fusing semantic information obtained from various  sources to enhance feature representation. In particular, a semantic information fusion algorithm on top of a multi-scale transformer module and a short-term attention module is proposed to capture the IoSI from the spatial and temporal dimensions, and aggregate semantic information by assigning varying weights. Moreover, we enhance the semantic information from the spatial and temporal dimensions before aggregation, making informative features more robust while suppressing the background noise.\par

\subsubsection{Decoder component}This component outputs the result of the perception task. To achieve this, we utilize a two-layer convolutional network to downsample fused semantic information. Then, we employ a single-layer convolutional classification header and a single-layer convolutional regression header\cite{8} to extract the classification and regression information of the object, respectively. Notably, the IoSI-CP framework is also applicable to other perception tasks such as semantic segmentation and object tracking.\par

\subsection{End-to-End Network Model}
In this study, we utilize an end-to-end neural network model to integrate the encoder, feature fusion, and decoder components. \par
In order to describe  this network model, we take the collaborative perception of agent \emph{i}  as an example. The input of the model  comprises the raw perception data of agent \emph{i}  $X_{i}^{\left( t \right)}$, $K$ frames of historical prior semantic information $F_{i}^{\left( t-k\mathrm{T} \right)}\,\,\left( k=\left\{ 1,2\cdots K \right\} \right) $, and semantic information received from neighboring agents $F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$ $\left( j\in N_{i}^{\left( t \right)} \right) $. \par

First, the model uses the encoder function $f_{\mathrm{enc}}$  (corresponding to the encoder component) to obtain the semantic information of agent \emph{i} $F_{i}^{\left( t \right)}$ , where the input is the raw perception data $X_{i}^{\left( t \right)}$. \par

Next, the collaborator selection function $f_{\mathrm{select}}$  (corresponding to the collaborator selection component) is invoked to obtain suitable collaborators  $C_{i}^{\left( t \right)}$ using the semantic information of agent \emph{i}  $F_{i}^{\left( t \right)}$ and its neighboring agents  $F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$ $\left( j\in N_{i}^{\left( t \right)} \right) $.  \par

Subsequently, the model utilizes the fusion function $f_{\mathrm{fuse}}$  (corresponding to the  fusion component) to obtain the fused semantic information $H_{i}^{\left( t \right)}$ using  the semantic information of agent \emph{i}  and its selected collaborators  $C_{i}^{\left( t \right)}$.  \par

Finally, it employs the decoder function $f_{\mathrm{dec}}$  (corresponding to the decoder component) to acquire the perception result  $\widehat{Y}_{i}^{\left( t \right)}$. 

Hence, our network model can be trained in an end-to-end manner under supervision from the 3D object detection task. As our focus is on Lidar-based 3D object detection, the output of the network is the result of object detection, which includes a classification result and regression result. \par



\subsection{Collaborator Selection Based on IoSI}
We design a collaborator selection method, that is, the collaborator selection function $f_{\mathrm{select}}$, to identify effective collaborators and eliminate those with poor performance from the spatial-temporal dimensions of IoSI. For illustration purposes, we depict the selection of effective collaborators by agent \emph{i} and present the complete algorithmic flow in Algorithm 1. 

\begin{algorithm}[!t]
\caption{Collaborator Selection Method}\label{alg:alg1}
\begin{algorithmic}[1]
\STATE {\textbf{Input}\textsc{:}}    agent \emph{i}'s semantic information $F_{i}^{\left( t \right)}$, agent \emph{i}'s neighboring agent set $N_{i}^{\left( t \right)}$,  agent $j$'s semantic information $F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$ $(j\in N_{i}^{\left( t \right)})$
\STATE {\textbf{Output}\textsc{:}}    $C_{i}^{\left( t \right)}$,  $w_{j}^{\mathrm{en}}$, $m_j$ $(j\in N_{i}^{\left( t \right)})$
\STATE  \textbf{for}   $j\in N_{i}^{\left( t \right)}\cup \left\{ i \right\} $  \textbf{do} 
\STATE \hspace{0.5cm} get sparse map $m_j=\mathrm{conv}_{\mathrm{map}}\left( F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)} \right) $
\STATE   \hspace{0.5cm} \textbf{if} $\left( j=i \right)$  \textbf{then} 
\STATE \hspace{1cm}  $\tau _{ji}^{\left( t \right)}=0$
 \STATE \hspace{0.5cm}  \textbf{endif}
\STATE \hspace{0.5cm} build GNN input $M_{\mathrm{input}}=\mathrm{concat}\left( m_j\,\,,\tau _{ji}^{\left( t \right)} \right) $
 \STATE  \textbf{endfor}
 \STATE get enhanced weight of agents $W^{\mathrm{en}}=\mathrm{GNN}\left( M_{\mathrm{input}} \right)$ 
\STATE  $C_{i}^{\left( t \right)}=N_{i}^{\left( t \right)}$
\STATE  \textbf{for}  each element $w_{j}^{\mathrm{en}}$ in $W^{\mathrm{en}}$  \textbf{do} 
\STATE \hspace{0.5cm}  \textbf{if}  $\left( w_{j}^{en}\leqslant 0 \right) $ \textbf{then} 
\STATE \hspace{1cm}   delete agent $j$ from  $C_{i}^{\left( t \right)}$
\STATE \hspace{0.5cm}  \textbf{endif} 
\STATE \textbf{endfor} 
\STATE \textbf{Return} $C_{i}^{\left( t \right)}$, $m_j$, $w_{j}^{\mathrm{en}}$
\end{algorithmic}
\label{alg1}
\end{algorithm}

We use a convolutional network $\mathrm{conv}_{\mathrm{map}}$  to extract the  sparse map of semantic information, which contains the spatial location information of potential objects. The sparse map of agent \emph{j} is obtained as follows:
\begin{align}
m_j=\mathrm{conv}_{\mathrm{map}}\left( F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)} \right)      
&\ \ j\in N_{i}^{\left( t \right)}\cup \left\{ i \right\} ,
\end{align}
where $\mathrm{conv}_{\mathrm{map}}\left( \cdot \right) $ is implemented by a convolutional network. In addition, the element of a sparse map is either 0 or 1. Areas with a value of 1 indicate regions with potential objects, while those with 0 represent the background. \par

After acquiring the sparse map of agent \emph{i} and its neighboring agents, we employ a GNN\cite{21} as the backbone to learn the inherent importance correlation  between agent \emph{i} and its neighboring agents. In this GNN network, each agent serves  as a node, and each node contains the sparse map and latency of the corresponding agent. Specifically, the node of agent \emph{j} contains  $m_j$ and $\tau _{ji}^{\left( t \right)}$ , which indicate the spatial and temporal information of  the raw perception data of agent \emph{j}, respectively. The output of the GNN is the enhanced weight of each agent to the ego, i.e., agent \emph{i}. \par

As the importance of semantic information decreases, its enhanced weight decreases proportionally until it reaches zero or even a negative value. Neighboring agents with an enhanced weight less than or equal to 0 are considered invalid collaborators, which are removed from the collaborator candidate set.


\section{HPHA: A Historical Prior Hybrid Attention Fusion algorithm}
In this section, we introduce our proposed semantic information fusion algorithm denoted as Historical Prior Hybrid Attention (HPHA). This fusion algorithm is designed to capture significant object features from the semantic information from both spatial and temporal dimensions. To achieve this, it leverages a multi-scale transformer module and a short-term attention module to assign varying weights to different semantic information of collaborators for optimal feature fusion. Furthermore, HPHA empowers the semantic information from both spatial and temporal dimensions before the fusion process to enhance the informative features and reduce background noise. Our proposed HPHA algorithm is illustrated in Fig. 3, and its algorithmic flow is outlined in Algorithm 2. \par

% Figure environment removed

\subsection{Semantic Information Enhanced by IoSI}
The semantic information of collaborators originates from a variety of raw perception data, including various spatial and temporal dimensions. From the viewpoint of ego, it is reasonable to assume that such semantic information varies in its importance in enhancing the perception performance. \par
In order to demonstrate the contribution of collaborators to the ego, we utilize the sparse maps and enhanced weights obtained in the collaborator selection component to enhance the semantic information of the selected collaborators. Specifically, for the raw semantic information of each collaborator, we use its sparse map to purposefully choose the spatial features of detection objects. Then, we employ the enhanced weight to magnify the effective features while suppressing background noise. \par
The enhancement of feature semantic information is performed as follows: 
\begin{align}
F_{j\_en}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}=F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}\otimes m_j\otimes w_{j}^{\mathrm{en}} ,
\end{align}
where  $F_{j\_en}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$ represents the enhanced semantic information of agent \emph{j}, $w_{j}^{\mathrm{en}}$ and $m_j$ are obtained from the collaborator selection method, and $\otimes$ denotes the elementwise multiplication.  \par

The sparse map assigns a value of 1 to the areas where objects are likely to be present, and 0 for the background. Therefore, the enhanced weights are only applied to potential object regions, which can effectively  improve the object features while suppressing background noise.

\begin{algorithm}[!t]
\caption{HPHA semantic information fusion algorithm}\label{alg:alg2}
\begin{algorithmic}[1]
\STATE {\textbf{Input}\textsc{:}}   $F_{i}^{\left( t-k\mathrm{T} \right)}$ \ $k=\left\{ 1,2\cdots \mathrm{K} \right\} $, $F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}$, $m_j$, $w_{j}^\mathrm{{en}}$ $\left( j\in C_{i}^{\left( t \right)} \right) $
\STATE {\textbf{Output}\textsc{:}}    agent \emph{i}'s final fused semantic information $H_{i}^{\left( t \right)}$
\STATE  \textbf{for}   $j\in C_{i}^{\left( t \right)}$  \textbf{do} 
\STATE \hspace{0.5cm} enhance semantic information \\ 
\hspace{0.5cm} $F_{j\_\mathrm{en}}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}=F_{j}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}\otimes m_j\otimes w_{j}^{\mathrm{en}}$
\STATE  \textbf{endfor}
\STATE  \textbf{for}   each attention scale size $S$ \textbf{do} 
\STATE \hspace{0.5cm} \textbf{for} $j\in C_{i}^{\left( t \right)}\cup \left\{ i \right\} $ \textbf{do} 
\STATE \hspace{1cm} calculate  attention weight \\
               \hspace{0.5cm}  $W_{S,j\rightarrow i}^{\mathrm{att}}\!\!=\!\mathrm{msa}\!\left(\!\! \mathrm{conv}\!\left(\! F_{i}^{\left( t \right)}\!, S \right)\!, \mathrm{conv}\!\!\left(\! F_{j\_{\mathrm{en}}}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}, S\! \right)\!\!\right) $
\STATE \hspace{0.5cm} \textbf{endfor} 
\STATE \hspace{0.5cm} aggregate semantic information with weights \\ 
\hspace{0.5cm}  $H_{S,\mathrm{att}}^{\left( t \right)}\!=\!\sum_{j\in C_{i}^{\left( t \right)}\cup i}{W_{S,j\rightarrow i}^{\mathrm{att}}\otimes \mathrm{conv}\!\left( \!F_{S,j\_en}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}, S \right)}$
\STATE \hspace{0.5cm} convert semantic information into same size \\
               \hspace{0.5cm}  $H_{S,\mathrm{att}}^{\left( t \right)}=\mathrm{conv}\left( H_{S,\mathrm{att}}^{\left( t \right)}, \mathrm{S}_{\mathrm{out}} \right)  $
\STATE \hspace{0.5cm} build semantic information list \\
             \hspace{0.5cm} $H_{\mathrm{att}}^{\left( t \right)}=\mathrm{concat}\left( H_{\mathrm{att}}^{\left( t \right)}, H_{S,\mathrm{att}}^{\left( t \right)} \right) $
\STATE \textbf{endfor} 

\STATE create new fused semantic information with historical prior information \\
 $H_\mathrm{{h}}^{\left( t \right)}=\mathrm{concat}\left( F_{i}^{\left( t-k\mathrm{T} \right)}, H_\mathrm{{att}}^{\left( t \right)} \right) $ \ \ $k=\left\{ 1,2\cdots \mathrm{K} \right\} $
\STATE get short-term weight   \\
$W^{\mathrm{ST}}=\mathrm{sta}\left( \mathrm{avgpool}\left( H_{\mathrm{h}}^{\left( t \right)} \right) ,\mathrm{maxpool}\left( H_{\mathrm{h}}^{\left( t \right)} \right) \right) $
\STATE refine the fused semantic information with short-term weight \\
$H_{i}^{\left( t \right)}=H_{\mathrm{h}}^{\left( t \right)}\otimes W^\mathrm{{ST}}$
\STATE \textbf{Return} $H_{i}^{\left( t \right)}$
\end{algorithmic}
\label{alg2}
\end{algorithm}



\subsection{Semantic Information Aggregated from the Spatial Dimension}
In order to improve the perception performance by maximizing the effectiveness of aggregated semantic information, we devise a multi-scale transformer module that captures and aggregates semantic information from different collaborators with varying weights from the spatial dimension.  \par

We design this module based on the literature\cite{22}. Specifically, we introduce attention weights as the representation of the spatial collaboration correlation between the collaborators and the ego. We employ a multi-scale transformer module to extract the attention weights at different scales, thus enhancing the precision of the collaborative correlation in distinct spatial resolution dimensions. The attention weights obtained with a specific size $S$ can be expressed as follows: 
\begin{align}
W_{\!S,j\rightarrow i}^{\mathrm{att}}\!=\mathrm{msa}\!\left( \mathrm{\!\!conv}\left( \!F_{i}^{\left( t \right)}, S \right) , \mathrm{conv\!}\left( \!F_{j\_en}^{\left( t-\tau _{ji}^{\left( t \right)} \right)}, S \!\right)\!\! \right) ,
\end{align}
where $W_{S,j\rightarrow i}^{\mathrm{att}}$ represents the attention weight of agent \emph{j} to agent \emph{i} at scale size $S$,  $\mathrm{conv}\left( F,S \right)$ represents a convolutional block that reshapes the semantic information $F$ to size $S$, and  $\mathrm{msa}\left( \cdot \right) $ represents the multi-scale attention operation. For each scale of the attention operation, we use a scaled dot-product attention\cite{23} to create feature maps $Q$, $K$ and $V$ from the semantic information, and then further obtain the weights of each collaborator as Eq. 11. Moreover, when \emph{j}  equals \emph{i}  in Eq. 11, the collaboration-attention weight is transformed into the self-attention weight, resulting in a simpler and more consistent equation.\par

Next, we aggregate the semantic information of collaborators by assigning them attention weights as obtained in the last step. We represent the aggregated feature semantic information with a specific size $S$ using the following equation:

\begin{align}
H_{S,\mathrm{att}}^{\left( t \right)}=\sum_{j\in G_{i}^{\left( t \right)}\cup \left\{ i \right\}}{W_{S,j\rightarrow i}^{\mathrm{att}}\otimes \mathrm{conv}\left( F_{S,j\_\mathrm{en}}^{\left( t-\tau _{ji}^{\left( t \right)} \right)},S \right)}.
\end{align}

Finally, to ensure the dimensional consistency of the aggregated feature semantic information after multi-scale attention processing, we utilize a distinct convolutional block to reshape it to a unifying size $\mathrm{S}_{\mathrm{out}}$. The final feature semantic information after aggregation is expressed as follows:
\begin{align}
H_\mathrm{{att}}^{\left( t \right)}=\sum_S{\mathrm{conv}\left( H_{S,\mathrm{att}}^{\left( t \right)}, \mathrm{S}_{\mathrm{out}} \right)} .
\end{align}

The multi-scale transformer module can effectively capture the collaborative correlation  between the ego and its collaborators from the spatial dimension, which enhances the aggregated features and presents a comprehensive representation of features.

\subsection{Semantic Information Refined from the Temporal Dimension}
Considering that the historical semantic information of the ego contains rich object information from the temporal dimension, we design a historical prior short-term attention module to refine the aggregated semantic information. \par

First, the historical semantic information of the ego and the last-step aggregated semantic information $H_{\mathrm{att}}^{\left( t \right)}$  are concatenated to produce new semantic information $H_{\mathrm{h}}^{\left( t \right)}$. \par

Next, this new semantic information undergoes average pooling and max pooling operations to generate average-pooled features and max-pooled features.\par
 Subsequently, we devise a short-term attention operation to process the two features to obtain the short-term weight $W^{\mathrm{ST}}$, expressed as follows: 
\begin{align}
W^{\mathrm{ST}}=\mathrm{sta}\left( \mathrm{avgpool}\left( H_{\mathrm{h}}^{\left( t \right)} \right) , \mathrm{maxpool}\left( H_{\mathrm{h}}^{\left( t \right)} \right) \right) ,
\end{align}
where $\mathrm{avgpool}\left( \cdot \right) $ represents the average pooling operation, $\mathrm{maxpool}\left( \cdot \right) $ represents the max pooling, and $\mathrm{sta}\left( \cdot \right) $ represents a short-term attention operation, referring to the channel attention in the literature \cite{24}.

Finally, the short-term weight $W^{\mathrm{ST}}$ is multiplied by the semantic information $H_{\mathrm{h}}^{\left( t \right)}$, creating the final fused semantic information $H_{i}^{\left( t \right)}$, which is expressed as follows:
\begin{align}
H_{i}^{\left( t \right)}=H_{\mathrm{h}}^{\left( t \right)}\otimes W^\mathrm{{ST}}.
\end{align}

The short-term attention module is advantageous because it integrates historical prior semantic information; therefore, it can effectively capture the correlation between historical features and current features and identify discriminative features of objects from the temporal dimension. In addition, due to the short-term attention operation, it can focus on meaningful features from the temporal dimension in the semantic information, leading to a better representational quality of object features.\par


\subsection{Analysis of IoSI-CP}

\begin{table}[!t]\centering
\caption{Number of  network model paramters}
\renewcommand{\arraystretch}{1}
\begin{tabular}{|c|c|}
\hline
\textbf{Works} & \begin{tabular}[c]{@{}c@{}}\textbf{Number of} \\  \textbf {paramters  (M)}\end{tabular}    \\ \hline
%\textbf{Works}  &\textbf{Number of paramters  (M)} 
V2VNet         &14.6122       \\ \hline
V2X-Vit        &12.4554    \\ \hline
Where2comm     &8.0574     \\ \hline
IoSI-CP        &8.2875   \\ \hline
\end{tabular}
\end{table}

On the one hand, our proposed IoSI-CP offers numerous benefits for improving the perception performance. First, selecting effective collaborators while pruning invalid ones can reduce noise and interference, thereby improving the perception performance. Second, by enhancing the effective features in the spatial and temporal dimensions while suppressing noise, the effectiveness of semantic information is boosted. Third, a multi-scale transformer module aggregates semantic information from different collaborators with varying weights from the temporal dimension, which provides a comprehensive representation of features. Finally, a short-term attention module is applied with historical prior semantic information, which refines the final fused semantic information and further improves the perception performance.\par

Furthermore, while other studies primarily focus on the spatial dimension alone, our work emphasizes using both spatial and temporal dimensions of IoSI in selecting collaborators and fusing semantic information. However, this strategy does not make our proposed IoSI-CP overly complex. As demonstrated in TABLE III, our IoSI-CP has fewer network model parameters than V2X-Vit and V2VNet, and shows only a slight increase compared to Where2comm. In comparison with Where2comm, our additional computing overhead involves a lightweight GNN, a limited number of elementwise multiplications, and a single attention module operation. These additions enhance the perception performance without significantly increasing the computing requirements. Our experimental results conducted on a real device support the above analysis.\par

Overall, our proposed IoSI-CP provides a unique perspective for collaborative perception performance improvement by considering the IoSI from both temporal and spatial dimensions, providing a significant improvement while introducing an reasonably limited computing requirement.\par

\section{Experimental evaluation}
This section provides a comprehensive performance comparison of the proposed IoSI-CP with the existing state-of-the-art methods. Furthermore, to demonstrate the superiority of IoSI-CP, we conduct various ablation studies to showcase the performance improvements attained using our innovations. The source code for our proposed IoSI-CP is accessible at https://github.com/huangqzj/IoSI-CP/. \par

\begin{table}
\centering
\caption{Experimental Parameters}
\renewcommand{\arraystretch}{1}
\begin{tabular}{|c|c|c|c} 
\hline
\textbf{Parameter}                      &\textbf{Value}                            \\ 
\hline
Carrier frequency $f_{\mathrm{c}}$              & 5.9Ghz                            \\ 
\hline
Total bandwidth                & 10Mhz                             \\ 
\hline
Transmit power $P_{ji}^{\mathrm{TX}}$         & 23dbm                             \\ 
\hline
Power of noise $P_{ji}^{\mathrm{Noise}}$                 & -95dbm $\sim$  -110dbm  \\ 
\hline
Lidar asynchronous time $\varDelta t_{ji}$        & 0-100ms                           \\ 
\hline
Lidar Sampling interval       & 10Hz                              \\ 
\hline
Semantic information extraction time $t_{ji}^{\mathrm{C}}$  & 20ms $\sim$ 40ms                         \\ 
\hline
Number of historical prior frames $\mathrm{K}$    & 2                                 \\
\hline
Number of muti-scale size   & 3                                 \\
\hline

\end{tabular}
\end{table}


\subsection{Experimental Settings}
Rather than evaluating on popular datasets such as KITTI\cite{25} and Nuscenes\cite{26}, which mainly provide single-agent samples, all experiments in this article are conducted on the open collaborative perception datasets OPV2V\cite{27} and V2XSet\cite{7}, in line with V2X-Vit\cite{7}. OPV2V is the first large-scale open dataset targeted at vehicle-to-vehicle collaborative perception. It comprises 11,464 frames of 3D point clouds and 232,913 annotated 3D boxes, generated using CARLA\cite{28} and SUMO\cite{29}. On the other hand, V2XSet is the first open dataset for V2X perception to take into account real-world noise during V2X collaboration.  \par

For the measurement of perception performance, we adopt the average precision (AP) and use Intersection-over-Union (IoU) thresholds of 0.3, 0.5, and 0.7. All experiments are conducted on an X86 PC station equipped with an Intel TM i7-11700 @2.50 GHz, 128 core CPU, 256 GB RAM, and 4 NVIDIA RTX3090 GPUs.\par

In this work, we make comparisons with three state-of-the-art approaches, namely V2VNet\cite{6}, V2X-Vit\cite{7}, and Where2comm\cite{8}. To maintain fairness, we implement all baselines and our proposed IoSI-CP on OpenCOOD, an open-source code framework for collaborative perception.  \par

As presented in TABLE IV, the key parameters employed in our study are consistent with V2VNet, V2X-Vit and Where2comm, except for the transmission rate of the agent. We use Eq. 4 for a more realistic calculation of the transmission rate, where the related parameters are consistent with the literature \cite{30}. For simplicity, we equally allocate an independent bandwidth to each agent. Compared to V2X-Vit and Where2comm that utilize a fixed transmission rate of 27 Mbps for all agents, our setting is more precise. In addition, the semantic information extraction calculation time $t_{ji}^{\mathrm{C}}$ is obtained based on our computing device.

% Figure environment removed

\subsection{Overall Performance Comparison}
In this experiment, we evaluate our proposed IoSI-CP on a total of 664 test data from the OPV2V dataset and 1466 test samples from the V2XSet dataset. To reduce randomness, we use their average inference results as our experimental outcomes.\par
As shown in Fig. 4, our proposed IoSI-CP outperforms V2VNet, V2X-Vit and Where2comm in terms of perception performance on both the OPV2V and V2XSet datasets. Our advantages become especially obvious when the value of IoU is larger (i.e., the detection difficulty is increased). For example, at IoU 0.7, compared with V2VNet, V2V-Vit and Where2comm, our IoSI-CP improves the AP performance by 24.69\%/2.09\%/6.30\% on the OPV2V dataset and 17.93\%/2.56\%/7.76\% on the V2XSet dataset, respectively. Since the test samples in OPV2V contain more challenging object detection scenes than V2VSet, the performance improvement of our approach is more significant on the OPV2V dataset.\par

The remarkable results achieved by IoSI-CP are attributed to its introduction of innovative features. Specifically, it comprehensively considers collaborative perception from both spatial and temporal dimensions, unlike other works that only focus on the spatial dimension.\par

Furthermore, as depicted in Fig. 5, the perception performance using our framework has been enhanced, while the computing efficiency has not increased significantly. The inference time of IoSI-CP is much lower than that of V2VNet and V2X-Vit and slightly higher than that of Where2comm, while its perception performance is better than that of Where2comm. The inference time of IoSI-CP is much less than 100 ms, which is well within an acceptable range given that the perception cycle of the LiDAR sensing device is usually 10 Hz.\par

As previously analysed, compared to Where2comm, the newly added computing requirements for IoSI-CP include a lightweight graph neural network, a limited number of elementwise multiplications, and a single attention module. These additions can enhance the perception performance and are easily handled by mainstream computing devices without an excessive computational burden.\par

Consequently, IoSI-CP generates superior perception performance with a high computing efficiency.\par

% Figure environment removed

\subsection{Perception Performance under Latency}
This experiment aims to evaluate the influence of perception information latency on the perception performance. To achieve this, we add a neighboring agent with a specific latency based on the previous experiment.\par

Figures 6, 7, and 8 demonstrate the results of experiments conducted under different IoU conditions. As depicted in these figures, the collaborator with a shorter latency has the potential to enhance the perception performance, whereas a large latency has a negative impact. Furthermore, our proposed IoSI-CP exhibits the best performance among all approaches and remains performance-stable even under large latency. For instance, at IoU 0.3, when the latency of the collaborator is 400 ms, IoSI-CP enhances the AP performance by 11.19\%/7.75\%/5.23\%, on the V2XSet dataset compared to V2VNet, V2X-Vit, and Where2comm, respectively.\par

As anticipated, the results of the experiment support our hypothesis that fresh perception information leads to a more significant profit gain, while large delayed information introduces additional noise and interference. IoSI-CP is more robust than other methods because it considers the temporal dimension of perception information during collaborator selection and semantic information fusion.\par

To sum up, the experimental results validate our premise that selecting collaborators and fusing semantic information necessitate considering the temporal aspect of perceptual information.\par



% Figure environment removed

% Figure environment removed

% Figure environment removed


\subsection{Perception Performance under Distance}

% Figure environment removed

% Figure environment removed

% Figure environment removed

Figures 9, 10, and 11 illustrate the perception performance of our proposed IoSI-CP and other state-of-the-art methods at different object distances on the OPV2X and V2XSet datasets.\par

Consistent with the expectations, the performance declines with increasing distance. Nonetheless, IoSI-CP is still able to achieve high accuracy in long-distance perception scenarios. For instance, at an IoU of 0.5 and an object distance of 60 meters, IoSI-CP improves the AP performance by 16.97\%/9.18\%/10.07\% in comparison to V2VNet, V2X-Vit and Where2comm, respectively, on the V2XSet dataset.\par

The efficacy of IoSI-CP in perceiving distant objects lies in its ability to enhance the semantic information of collaborators in both spatial and temporal dimensions and perform semantic information fusion with a multiscale transformer and short-term attention. Consequently, the semantic information of the collaborators is more effectively employed in the spatial and temporal dimensions, allowing for better accuracy in perceiving distant objects.\par


\subsection{Perception Performance under Localization Noise}
Localization noise often occurs due to the impact of spatial and temporal misalignment. To evaluate the sensitivity of our proposed IoSI-CP, we add localization noise to the test data, which follows a Gaussian distribution with a standard deviation of 0.2 for both position and heading.\par

As depicted in Fig. 12, localization noise impairs the perception performance of all approaches, and the reduction is the most noticeable when IoU is 0.7. This indicates that high-precision detection is more vulnerable to noise. However, compared to other methods, our proposed IoSI-CP shows superior noise suppression ability. For instance, on the V2XSet dataset, IoSI-CP maintains an AP of 0.78/0.58 at IoU 0.5 and 0.7, presenting an improvement of 17\%/25\%, 3\%/5\% and 2\%/4\% compared to V2VNet, V2X-Vit, and Where2comm, respectively. Similar phenomena can be observed on the OPV2V dataset.\par

IoSI-CP introduces historical prior semantic information and utilizes a short-term attention module to learn the correlation of semantic information from the temporal dimension. This enables the suppression of spatial position noise for the current semantic information by using historical information. As a result, the positional noise has less impact on IoSI-CP.\par





% Figure environment removed


\subsection{Ablation Studies}
In this subsection, we evaluate the effectiveness of the innovations incorporated in our proposed IoSI-CP. As previously mentioned, IoSI-CP includes two novel innovations: i) the collaborator selection method; ii) the HPHA semantic information fusion algorithm. To demonstrate the benefits of our innovations, we utilize V2VNet as the baseline to represent the performance without the enhancements.\par

TABLE V presents the results of our ablation studies. The outcome clearly illustrates that each innovative component significantly contributes to the advancement of perception performance. Specifically, the collaborator selection method enhances the perception performance of AP by 1.69\%, 2.38\% and 3.74\% at IoU 0.3, 0.5, and 0.7, respectively, on the OPV2X dataset and by 1.71\%, 2.40\% and 3.40\%, respectively, on the V2XSet dataset. Moreover, HPHA improves the perception performance of AP by 8.30\%, 8.43\%, and 11.79\% at IoU 0.3, 0.5, and 0.7, respectively, on the OPV2X dataset and by 9.92\%, 10.68\% and 14.4\%  on the V2XSet dataset. In addition, we conclude that the gains from the collaborator selection method and HPHA are more pronounced when the value of IoU is larger (i.e., the detection difficulty is increased). This trend agrees with our previous experimental findings and is more apparent in HPHA. \par

The collaborator selection method benefits from removing unsuitable collaborators that cause noise, resulting in improved perception performance. Likewise, the gain from HPHA is the efficient utilization of the correlation between temporal and spatial dimensions embedded in the semantic information for information fusion. Thus, both techniques can effectively enhance the perception performance.\par




\begin{table}
\centering
\caption{Innovative Component Ablation Study Results}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|m{0.8cm}<{\centering}|m{0.7cm}<{\centering}|m{1.3cm}<{\centering}|m{0.7cm}<{\centering}|m{0.7cm}<{\centering}|m{0.7cm}<{\centering}|}
\hline
\textbf{Open dataset} & \textbf{HPHA} &\textbf{Collborator selection}  &\textbf{IoU 0.3} &\textbf{IoU 0.5} &\textbf{IoU 0.7}\\
\hline
\multirow{3}* {OPV2V} &\ding{56} &\ding{56} &0.7981	&0.7770	&0.6212 \\
\cline{2-6}
                      &\ding{52} &\ding{56} &0.8811	&0.8613	&0.7391 \\
\cline{2-6}
			      &\ding{52} &\ding{52} &0.8980	&0.8851	&0.7765 \\
\cline{2-6}
\hline

\multirow{3}* {V2XSet}&\ding{56} &\ding{56} &0.7555	&0.7117	&0.4712 \\
\cline{2-6}
                      &\ding{52} &\ding{56} &0.8547	&0.8185	&0.6152 \\
\cline{2-6}
			      &\ding{52} &\ding{52} &0.8718	&0.8425	&0.6492 \\
\cline{2-6}

\hline
\end{tabular}
\end{table}




\section{CONCLUSION}
In this article, we propose IoSI-CP, a novel collaborative perception framework that improves the perception performance based on the importance of semantic information (IoSI) from both spatial and temporal dimensions. Specifically, we design a collaborator selection method that selects collaborators efficiently and prunes those that bring negative benefits. To further boost the perception performance, we present a semantic information fusion algorithm named as HPHA, which integrates a multiscale transformer module and a short-term attention module to capture the importance of semantic information from both spatial and temporal dimensions and aggregates semantic information by assigning varying weights. Experimental results are conducted on two open datasets, OPV2V and V2XSet, which demonstrate that IoSI-CP outperforms state-of-the-art methods in terms of perception performance.\par

In our future work, we aim to concentrate on key areas for improvement within the domain of our IoSI-CP framework. First, we shall acknowledge the utility of enhanced weight for collaborator selection and semantic information enhancement. However, the one-dimensional enhanced weight that is similar to the CoFF \cite{5} is flawed and insufficient. Consequently, we shall explore a more accurate high-dimensional representation method in our future research efforts. Furthermore, we shall investigate collaborative privacy concerns and malicious behavior to mitigate potential risks.\par




\section*{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of China under Grants 62071425, in part by the National Key Research and Development Program of China under Grant 2021YFB2900200, in part by the Zhejiang Key Research and Development Plan under Grant 2022C01129, and in part by the Zhejiang Provincial Natural Science Foundation of China under Grant LR23F010005.


\bibliographystyle{IEEEtran} 
\bibliography{ref}

%
%

%\begin{thebibliography}{1}
%\bibliographystyle{IEEEtran}

%\bibitem{ref1}
%{\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf
%\bibitem{ref2}
%T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

%\end{thebibliography}


\newpage

\section{Biography Section}
\vspace{11pt}

\begin{IEEEbiographynophoto}{YUNTAO LIU}
is a Ph.D. candidate with Zhejiang University as well as a senior engineer with Zhejiang Lab, Hangzhou, China. His research interests currently focus on collective intelligence and blockchain.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{QIAN HUANG}
is a senior Engineer at Zhejiang Lab, Hangzhou, China. Her research interests currently focus on computer vision and internet of vehicles.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{RONGPENG LI}
is an assistant professor at Zhejiang University Hangzhou, China. His research interests currently focus on multi-agent reinforcement learning and network slicing.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{XIANFU CHEN}
is a senior scientist at VTT Technical Research Centre of Finland, Oulu, Finland.  His research interests currently focus on human-level and artificial intelligence for resource awareness in next-generation communication networks. 
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{ZHIFENG ZHAO}
is a principal researcher with Zhejiang Lab as well as with Zhejiang University, Hangzhou, China. His research area includes collective intelligence and software-defined networks.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{SUYUAN ZHAO}
is working as a postdoctoral researcherin Zhejiang Lab, Hangzhou, China. Hiscurrent research interests include Internet of Vehicles and deep learning.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{YONGDONG ZHU}
is a principal researcher at Zhejiang Lab, Hangzhou, China. His research interests currently focus on Internet of Vehicles and intelligent transportation.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{HONGGANG ZHANG}
is a principal researcher with Zhejiang Lab as well as  a professor with Zhejiang University, Hangzhou, China. He is currently involved in research on cognitive green communications.
\end{IEEEbiographynophoto}

\vfill

\end{document}


