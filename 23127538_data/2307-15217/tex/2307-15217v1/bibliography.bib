@article{lee2021pebble,
  title={Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training},
  author={Lee, Kimin and Smith, Laura and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2106.05091},
  year={2021}
}

@article{irving2018ai,
  title={AI safety via debate},
  author={Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal={arXiv preprint arXiv:1805.00899},
  year={2018}
}

@misc{christiano2023thoughts, 
    title={Thoughts on the Impact of RLHF Research}, 
    url={https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research#The_case_for_a_positive_impact:~:text=I\%20think\%20it\%20is\%20hard\%20to\%20productively\%20work\%20on\%20more\%20challenging\%20alignment\%20problems\%20without\%20first\%20implementing\%20basic\%20solutions.}, 
    journal={AI Alignment Forum}, 
    author={Christiano, Paul}, 
    year={2023}, 
    month={Jan}
} 

@misc{Hubinger_2022_likely, 
    title={How likely is deceptive alignment?}, 
    url={https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment}, 
    journal={AI Alignment Forum}, 
    author={Hubinger, Evan}, 
    year={2022}, 
    month={Aug}
} 

@phdthesis{biyik2022dissertation,
 title={Learning Preferences For Interactive Autonomy},
 author={Biyik, Erdem},
 school = {EE Department, Stanford University},
 year={2022}
}

@inproceedings{biyik2019asking,
 title={Asking Easy Questions: A User-Friendly Approach to Active Reward Learning},
 author={Biyik, Erdem and Palan, Malayandi and Landolfi, Nicholas C. and Losey, Dylan P. and Sadigh, Dorsa},
 booktitle={Proceedings of the 3rd Conference on Robot Learning (CoRL)},
 year={2019}
}

@article{wei2023jailbroken,
  title={Jailbroken: How Does LLM Safety Training Fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}

@article{hubinger2020overview,
  title={An overview of 11 proposals for building safe advanced ai},
  author={Hubinger, Evan},
  journal={arXiv preprint arXiv:2012.07532},
  year={2020}
}

@inproceedings{zhao2016learning,
  title={Learning mixtures of Plackett-Luce models},
  author={Zhao, Zhibing and Piech, Peter and Xia, Lirong},
  booktitle={International Conference on Machine Learning},
  pages={2906--2914},
  year={2016},
  organization={PMLR}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{bhatia2020preference,
  title={Preference learning along multiple criteria: A game-theoretic perspective},
  author={Bhatia, Kush and Pananjady, Ashwin and Bartlett, Peter and Dragan, Anca and Wainwright, Martin J},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7413--7424},
  year={2020}
}

@inproceedings{hejna2022fewshot,
 title={Few-Shot Preference Learning for Human-in-the-Loop RL},
 author={Hejna, Joey and Sadigh, Dorsa},
 booktitle={Proceedings of the 6th Conference on Robot Learning (CoRL)},
 year={2022}
}

@inproceedings{bennett2007netflix,
  title={The netflix prize},
  author={Bennett, James and Lanning, Stan and others},
  booktitle={Proceedings of KDD cup and workshop},
  volume={2007},
  pages={35},
  year={2007},
  organization={New York}
}

@article{hallinan2016recommended,
  title={Recommended for you: The Netflix Prize and the production of algorithmic culture},
  author={Hallinan, Blake and Striphas, Ted},
  journal={New media \& society},
  volume={18},
  number={1},
  pages={117--137},
  year={2016},
  publisher={Sage Publications Sage UK: London, England}
}

@book{chambers2016revealed,
  title={Revealed preference theory},
  author={Chambers, Christopher P and Echenique, Federico},
  volume={56},
  year={2016},
  publisher={Cambridge University Press}
}


@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes and others},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017},
  publisher={Journal of Machine Learning Research/Massachusetts Institute of Technology~…}
}

@article{feffer2023moral,
  title={Moral Machine or Tyranny of the Majority?},
  author={Feffer, Michael and Heidari, Hoda and Lipton, Zachary C},
  journal={arXiv preprint arXiv:2305.17319},
  year={2023}
}

@article{luketina2019survey,
  title={A survey of reinforcement learning informed by natural language},
  author={Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:1906.03926},
  year={2019}
}

@article{reddy_assisted_2020,
	title = {Assisted {Perception}: {Optimizing} {Observations} to {Communicate} {State}},
	abstract = {We aim to help users estimate the state of the world in tasks like robotic teleoperation and navigation with visual impairments, where users may have systematic biases that lead to suboptimal behavior: they might struggle to process observations from multiple sensors simultaneously, receive delayed observations, or overestimate distances to obstacles. While we cannot directly change the user’s internal beliefs or their internal state estimation process, our insight is that we can still assist them by modifying the user’s observations. Instead of showing the user their true observations, we synthesize new observations that lead to more accurate internal state estimates when processed by the user. We refer to this method as assistive state estimation (ASE): an automated assistant uses the true observations to infer the state of the world, then generates a modiﬁed observation for the user to consume (e.g., through an augmented reality interface), and optimizes the modiﬁcation to induce the user’s new beliefs to match the assistant’s current beliefs. To predict the effect of the modiﬁed observation on the user’s beliefs, ASE learns a model of the user’s state estimation process: after each task completion, it searches for a model that would have led to beliefs that explain the user’s actions. We evaluate ASE in a user study with 12 participants who each perform four tasks: two tasks with known user biases – bandwidth-limited image classiﬁcation and a driving video game with observation delay – and two with unknown biases that our method has to learn – guided 2D navigation and a lunar lander teleoperation video game. ASE’s general-purpose approach to synthesizing informative observations enables a different assistance strategy to emerge in each domain, such as quickly revealing informative pixels to speed up image classiﬁcation, using a dynamics model to undo observation delay in driving, identifying nearby landmarks for navigation, and exaggerating a visual indicator of tilt in the lander game. The results show that ASE substantially improves the task performance of users with bandwidth constraints, observation delay, and other unknown biases.},
	language = {en},
	author = {Reddy, Siddharth and Levine, Sergey and Dragan, Anca D},
	file = {Reddy et al. - Assisted Perception Optimizing Observations to Co.pdf:/Users/micah/Zotero/storage/JEL9ES33/Reddy et al. - Assisted Perception Optimizing Observations to Co.pdf:application/pdf},
 year = {2020},
}

@article{mokander2023auditing,
  title={Auditing large language models: a three-layered approach},
  author={M{\"o}kander, Jakob and Schuett, Jonas and Kirk, Hannah Rose and Floridi, Luciano},
  journal={arXiv preprint arXiv:2302.08500},
  year={2023}
}

@misc{shavit2023does,
      title={What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring}, 
      author={Yonadav Shavit},
      year={2023},
      eprint={2303.11341},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nanda2023progress,
      title={Progress measures for grokking via mechanistic interpretability}, 
      author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
      year={2023},
      eprint={2301.05217},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{casper2023robust,
      title={Robust Feature-Level Adversaries are Interpretability Tools}, 
      author={Stephen Casper and Max Nadeau and Dylan Hadfield-Menell and Gabriel Kreiman},
      year={2023},
      eprint={2110.03605},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cui2022local,
      title={Local Relighting of Real Scenes}, 
      author={Audrey Cui and Ali Jahanian and Agata Lapedriza and Antonio Torralba and Shahin Mahdizadehaghdam and Rohit Kumar and David Bau},
      year={2022},
      eprint={2207.02774},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bai_artificial_2023,
	title = {Artificial {Intelligence} {Can} {Persuade} {Humans}},
	language = {en},
	author = {Bai, Hui},
	year = {2023},
	keywords = {Read},
	file = {Bai - Artificial Intelligence Can Persuade Humans.pdf:/Users/micah/Zotero/storage/F7I9QCXV/Bai - Artificial Intelligence Can Persuade Humans.pdf:application/pdf},
}

@misc{meng2023locating,
      title={Locating and Editing Factual Associations in GPT}, 
      author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
      year={2023},
      eprint={2202.05262},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vincent_microsofts_2023,
	title = {Microsoft’s {Bing} is an emotionally manipulative liar, and people love it},
	url = {https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams},
	abstract = {Bing’s acting unhinged, and lots of people love it.},
	language = {en-US},
	urldate = {2023-03-12},
	journal = {The Verge},
	author = {Vincent, James},
	month = feb,
	year = {2023},
	keywords = {Read},
	file = {Snapshot:/Users/micah/Zotero/storage/GI6L5WIZ/microsoft-ai-bing-personality-conversations-spy-employees-webcams.html:text/html},
}

@misc{griffin_susceptibility_2023,
	title = {Susceptibility to {Influence} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.06074},
	abstract = {Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Griffin, Lewis D. and Kleinberg, Bennett and Mozes, Maximilian and Mai, Kimberly T. and Vau, Maria and Caldwell, Matthew and Marvor-Parker, Augustine},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06074 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7, I.2.m, J.4, To Read (5)},
	file = {Full Text:/Users/micah/Zotero/storage/ZEN6UPPS/Griffin et al. - 2023 - Susceptibility to Influence of Large Language Mode.pdf:application/pdf},
}

@misc{snoswell_galactica_2022,
	title = {The {Galactica} {AI} model was trained on scientific knowledge – but it spat out alarmingly plausible nonsense},
	url = {http://theconversation.com/the-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible-nonsense-195445},
	abstract = {The story of Meta’s latest AI model shows the pitfalls of machine learning – and a disregard for potential risks.},
	language = {en},
	urldate = {2023-03-15},
	journal = {The Conversation},
	author = {Snoswell, Aaron J. and Burgess, Jean},
	month = nov,
	year = {2022},
	keywords = {Read},
	file = {Snapshot:/Users/micah/Zotero/storage/A3MJQFWI/the-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible.html:text/html},
}

@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={2817--2826},
  year={2017},
  organization={PMLR}
}

@inproceedings{bajcsy2018learning,
  title={Learning from physical human corrections, one feature at a time},
  author={Bajcsy, Andrea and Losey, Dylan P and O'Malley, Marcia K and Dragan, Anca D},
  booktitle={Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={141--149},
  year={2018}
}

@misc{bowman_measuring_2022,
	title = {Measuring {Progress} on {Scalable} {Oversight} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2211.03540},
	abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukošiūtė, Kamilė and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2211.03540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/micah/Zotero/storage/MTZJI5JP/2211.html:text/html;Full Text PDF:/Users/micah/Zotero/storage/E43YUI8Q/Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large.pdf:application/pdf},
}

@misc{carroll_characterizing_2023,
	title = {Characterizing {Manipulation} from {AI} {Systems}},
	url = {http://arxiv.org/abs/2303.09387},
	abstract = {Manipulation is a common concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our interactions with the world, it is important to understand the degree to which AI systems might manipulate humans {\textbackslash}textit\{without the intent of the system designers\}. Our work clarifies challenges in defining and measuring manipulation in the context of AI systems. Firstly, we build upon prior literature on manipulation from other fields and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, harm, and covertness. We review proposals on how to operationalize each factor. Second, we propose a definition of manipulation based on our characterization: a system is manipulative {\textbackslash}textit\{if it acts as if it were pursuing an incentive to change a human (or another agent) intentionally and covertly\}. Third, we discuss the connections between manipulation and related concepts, such as deception and coercion. Finally, we contextualize our operationalization of manipulation in some applications. Our overall assessment is that while some progress has been made in defining and measuring manipulation from AI systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. We argue that such manipulation poses a significant threat to human autonomy, suggesting that precautionary actions to mitigate it are warranted.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Carroll, Micah and Chan, Alan and Ashton, Henry and Krueger, David},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09387 [cs]},
	keywords = {Computer Science - Computers and Society, Read},
	file = {Full Text:/Users/micah/Zotero/storage/KNY47THQ/Carroll et al. - 2023 - Characterizing Manipulation from AI Systems.pdf:application/pdf},
}

@misc{steinhardt_emergent_2023,
	title = {Emergent {Deception} and {Emergent} {Optimization}},
	url = {https://bounded-regret.ghost.io/emergent-deception-optimization/},
	abstract = {I’ve previously argued that machine learning systems often exhibit emergent capabilities, and that these capabilities could lead to unintended negative consequences. But how can we reason concretely about these consequences?},
	language = {en},
	urldate = {2023-03-25},
	journal = {Bounded Regret},
	author = {Steinhardt, Jacob},
	month = feb,
	year = {2023},
	keywords = {Read},
	file = {Snapshot:/Users/micah/Zotero/storage/W69BRDBJ/emergent-deception-optimization.html:text/html},
}

@article{lin2022inferring,
  title={Inferring rewards from language in context},
  author={Lin, Jessy and Fried, Daniel and Klein, Dan and Dragan, Anca},
  journal={arXiv preprint arXiv:2204.02515},
  year={2022}
}
@misc{eloundou2023gpts,
      title={GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models}, 
      author={Tyna Eloundou and Sam Manning and Pamela Mishkin and Daniel Rock},
      year={2023},
      eprint={2303.10130},
      archivePrefix={arXiv},
      primaryClass={econ.GN}
}

@article{ziegler2022adversarial,
  title={Adversarial training for high-stakes reliability},
  author={Ziegler, Daniel and Nix, Seraphina and Chan, Lawrence and Bauman, Tim and Schmidt-Nielsen, Peter and Lin, Tao and Scherlis, Adam and Nabeshima, Noa and Weinstein-Raun, Benjamin and de Haas, Daniel and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9274--9286},
  year={2022}
}

@article{casper2022white,
  title={White-Box Adversarial Policies in Deep Reinforcement Learning},
  author={Casper, Stephen and Hadfield-Menell, Dylan and Kreiman, Gabriel},
  journal={arXiv preprint arXiv:2209.02167},
  year={2022}
}

@article{pang2021deep,
  title={Deep learning for anomaly detection: A review},
  author={Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--38},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{zhang2019adversarial,
  title={Adversarial examples: Opportunities and challenges},
  author={Zhang, Jiliang and Li, Chen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={7},
  pages={2578--2593},
  year={2019},
  publisher={IEEE}
}

@article{raukur2022toward,
  title={Toward transparent ai: A survey on interpreting the inner structures of deep neural networks},
  author={R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2207.13243},
  year={2022}
}

@article{falco2021governing,
  title={Governing AI safety through independent audits},
  author={Falco, Gregory and Shneiderman, Ben and Badger, Julia and Carrier, Ryan and Dahbura, Anton and Danks, David and Eling, Martin and Goodloe, Alwyn and Gupta, Jerry and Hart, Christopher and others},
  journal={Nature Machine Intelligence},
  volume={3},
  number={7},
  pages={566--571},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{cihon2019standards,
  title={Standards for AI governance: international standards to enable global coordination in AI research \& development},
  author={Cihon, Peter},
  journal={Future of Humanity Institute. University of Oxford},
  year={2019}
}

@article{hadfield2023regulatory,
  title={Regulatory Markets: The Future of AI Governance},
  author={Hadfield, Gillian K and Clark, Jack},
  journal={arXiv preprint arXiv:2304.04914},
  year={2023}
}

@article{perry2019ai,
  title={AI governance and the policymaking process: key considerations for reducing AI risk},
  author={Perry, Brandon and Uuk, Risto},
  journal={Big data and cognitive computing},
  volume={3},
  number={2},
  pages={26},
  year={2019},
  publisher={MDPI}
}

@misc{perry2022users,
      title={Do Users Write More Insecure Code with AI Assistants?}, 
      author={Neil Perry and Megha Srivastava and Deepak Kumar and Dan Boneh},
      year={2022},
      eprint={2211.03622},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}



@misc{kenton_alignment_2021,
	title = {Alignment of {Language} {Agents}},
	url = {http://arxiv.org/abs/2103.14659},
	abstract = {For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14659 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, To Read (5)},
	file = {arXiv Fulltext PDF:/Users/micah/Zotero/storage/7N63FHIZ/Kenton et al. - 2021 - Alignment of Language Agents.pdf:application/pdf;arXiv.org Snapshot:/Users/micah/Zotero/storage/56TUQ43R/2103.html:text/html},
}

@article{krueger_hidden_2020,
	title = {Hidden {Incentives} for {Auto}-{Induced} {Distributional} {Shift}},
	url = {http://arxiv.org/abs/2009.09153},
	abstract = {Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.},
	urldate = {2020-11-18},
	journal = {arXiv:2009.09153 [cs, stat]},
	author = {Krueger, David and Maharaj, Tegan and Leike, Jan},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.09153},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/micah/Zotero/storage/WJXSSF99/Krueger et al. - 2020 - Hidden Incentives for Auto-Induced Distributional .pdf:application/pdf;arXiv.org Snapshot:/Users/micah/Zotero/storage/LTRY85U4/2009.html:text/html},
}

@article{everitt_reward_2021,
	title = {Reward {Tampering} {Problems} and {Solutions} in {Reinforcement} {Learning}: {A} {Causal} {Influence} {Diagram} {Perspective}},
	shorttitle = {Reward {Tampering} {Problems} and {Solutions} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.04734},
	abstract = {Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.},
	urldate = {2022-05-09},
	journal = {arXiv:1908.04734 [cs]},
	author = {Everitt, Tom and Hutter, Marcus and Kumar, Ramana and Krakovna, Victoria},
	month = mar,
	year = {2021},
	note = {arXiv: 1908.04734},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read},
	file = {arXiv Fulltext PDF:/Users/micah/Zotero/storage/WKDUKX5C/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf:application/pdf;arXiv Fulltext PDF:/Users/micah/Zotero/storage/PCWYEH8A/Everitt et al. - 2021 - Reward Tampering Problems and Solutions in Reinfor.pdf:application/pdf;arXiv.org Snapshot:/Users/micah/Zotero/storage/YFIH8JDJ/1908.html:text/html},
}

@inproceedings{Turner2019OptimalPT,
  title={Optimal Policies Tend To Seek Power},
  author={Alexander Matt Turner and Logan Smith and Rohin Shah and Andrew Critch and Prasad Tadepalli},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@article{bowman2023eight,
  title={Eight things to know about large language models},
  author={Bowman, Samuel R},
  journal={arXiv preprint arXiv:2304.00612},
  year={2023}
}

@article{Ngo2022TheAP,
  title={The alignment problem from a deep learning perspective},
  author={Richard Ngo},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.00626}
}

@article{Chan2023HarmsFI,
  title={Harms from Increasingly Agentic Algorithmic Systems},
  author={Alan Chan and Rebecca Salganik and Alva Markelius and Chris Pang and Nitarshan Rajkumar and Dmitrii Krasheninnikov and Lauro Langosco di Langosco and Zhonghao He and Yawen Duan and Micah Carroll and Michelle Lin and Alex Mayhew and Katherine Collins and Maryam Molamohammadi and John Burden and Wanru Zhao and Shalaleh Rismani and Konstantinos Voudouris and Umang Bhatt and Adrian Weller and David Krueger and Tegan Maharaj},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.10329}
}

@article{Turner2022ParametricallyRD,
  title={Parametrically Retargetable Decision-Makers Tend To Seek Power},
  author={Alexander Matt Turner and Prasad Tadepalli},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.13477}
}

@article{krakovna2023power,
  title={Power-seeking can be probable and predictive for trained agents},
  author={Krakovna, Victoria and Kramar, Janos},
  journal={arXiv preprint arXiv:2304.06528},
  year={2023}
}



@misc{schulman_reinforcement_2023,
	title = {Reinforcement {Learning} from {Human} {Feedback}: {Progress} and {Challenges}},
	shorttitle = {John {Schulman} - {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {https://www.youtube.com/watch?v=hhiLw5Q_UFg},
	abstract = {EECS Colloquium 
Wednesday, April 19, 2023
Banatao Auditorium
5-6p

Caption available upon request},
	urldate = {2023-05-08},
	author = {Schulman, John},
	month = apr,
	year = {2023},
}

@article{cotra_without_2022,
	title = {Without specific countermeasures, the easiest path to transformative {AI} likely leads to {AI} takeover},
	url = {https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to},
	abstract = {I think that in the coming 15-30 years, the world could plausibly develop “transformative AI”: AI powerful enough to bring us into a new, qualitatively different future, via an explosion in science a…},
	language = {en},
	urldate = {2023-05-08},
	author = {Cotra, Ajeya},
	year = {2022},
}

@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2017},
  organization={PMLR}
}

@article{arumugam2019deep,
  title={Deep reinforcement learning from policy-dependent human feedback},
  author={Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L},
  journal={arXiv preprint arXiv:1902.04257},
  year={2019}
}

@article{reddy_where_2019,
	title = {Where {Do} {You} {Think} {You}'re {Going}?: {Inferring} {Beliefs} about {Dynamics} from {Behavior}},
	shorttitle = {Where {Do} {You} {Think} {You}'re {Going}?},
	url = {http://arxiv.org/abs/1805.08010},
	abstract = {Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspeciﬁcation: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules – the dynamics – governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user’s internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.},
	language = {en},
	urldate = {2020-04-03},
	journal = {arXiv:1805.08010 [cs, stat]},
	author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey},
	month = jan,
	year = {2019},
	note = {arXiv: 1805.08010},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Reddy et al. - 2019 - Where Do You Think You're Going Inferring Belief.pdf:/Users/micah/Zotero/storage/2M2GUHQ5/Reddy et al. - 2019 - Where Do You Think You're Going Inferring Belief.pdf:application/pdf},
}

@article{chan_assistive_2019,
	title = {The {Assistive} {Multi}-{Armed} {Bandit}},
	url = {http://arxiv.org/abs/1901.08654},
	abstract = {Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.},
	language = {en},
	urldate = {2020-04-03},
	journal = {arXiv:1901.08654 [cs, stat]},
	author = {Chan, Lawrence and Hadfield-Menell, Dylan and Srinivasa, Siddhartha and Dragan, Anca},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.08654},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read, Statistics - Machine Learning},
	file = {Chan et al. - 2019 - The Assistive Multi-Armed Bandit.pdf:/Users/micah/Zotero/storage/DDPRAZIE/Chan et al. - 2019 - The Assistive Multi-Armed Bandit.pdf:application/pdf},
}

@misc{tian_towards_2023,
	title = {Towards {Modeling} and {Influencing} the {Dynamics} of {Human} {Learning}},
	url = {http://arxiv.org/abs/2301.00901},
	abstract = {Humans have internal models of robots (like their physical capabilities), the world (like what will happen next), and their tasks (like a preferred goal). However, human internal models are not always perfect: for example, it is easy to underestimate a robot's inertia. Nevertheless, these models change and improve over time as humans gather more experience. Interestingly, robot actions influence what this experience is, and therefore influence how people's internal models change. In this work we take a step towards enabling robots to understand the influence they have, leverage it to better assist people, and help human models more quickly align with reality. Our key idea is to model the human's learning as a nonlinear dynamical system which evolves the human's internal model given new observations. We formulate a novel optimization problem to infer the human's learning dynamics from demonstrations that naturally exhibit human learning. We then formalize how robots can influence human learning by embedding the human's learning dynamics model into the robot planning problem. Although our formulations provide concrete problem statements, they are intractable to solve in full generality. We contribute an approximation that sacrifices the complexity of the human internal models we can represent, but enables robots to learn the nonlinear dynamics of these internal models. We evaluate our inference and planning methods in a suite of simulated environments and an in-person user study, where a 7DOF robotic arm teaches participants to be better teleoperators. While influencing human learning remains an open problem, our results demonstrate that this influence is possible and can be helpful in real human-robot interaction.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Tian, Ran and Tomizuka, Masayoshi and Dragan, Anca and Bajcsy, Andrea},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00901 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/micah/Zotero/storage/ZSRLUP9Q/Tian et al. - 2023 - Towards Modeling and Influencing the Dynamics of H.pdf:application/pdf;arXiv.org Snapshot:/Users/micah/Zotero/storage/LPXUE8R9/2301.html:text/html},
}



@inproceedings{krasheninnikov2022assistance,
  title={Assistance with large language models},
  author={Krasheninnikov, Dmitrii and Krasheninnikov, Egor and Krueger, David},
  booktitle={NeurIPS ML Safety Workshop},
  year={2022}
}

@article{dobbe2021hard,
  title={Hard choices in artificial intelligence},
  author={Dobbe, Roel and Gilbert, Thomas Krendl and Mintz, Yonatan},
  journal={Artificial Intelligence},
  volume={300},
  pages={103555},
  year={2021},
  publisher={Elsevier}
}

@article{gilbert2022reward,
  title={Reward reports for reinforcement learning},
  author={Gilbert, Thomas Krendl and Lambert, Nathan and Dean, Sarah and Zick, Tom and Snoswell, Aaron},
  journal={arXiv preprint arXiv:2204.10817},
  year={2022}
}

@article{sartori2022sociotechnical,
  title={A sociotechnical perspective for the future of AI: narratives, inequalities, and human control},
  author={Sartori, Laura and Theodorou, Andreas},
  journal={Ethics and Information Technology},
  volume={24},
  number={1},
  pages={4},
  year={2022},
  publisher={Springer}
}

@article{floridi2022unified,
  title={A unified framework of five principles for AI in society},
  author={Floridi, Luciano and Cowls, Josh},
  journal={Machine learning and the city: Applications in architecture and urban design},
  pages={535--545},
  year={2022},
  publisher={Wiley Online Library}
}

@inproceedings{chao2010transparent,
  title={Transparent active learning for robots},
  author={Chao, Crystal and Cakmak, Maya and Thomaz, Andrea L},
  booktitle={2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={317--324},
  year={2010},
  organization={IEEE}
}

@inproceedings{knox2008tamer,
  title={Tamer: Training an agent manually via evaluative reinforcement},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={2008 7th IEEE international conference on development and learning},
  pages={292--297},
  year={2008},
  organization={IEEE}
}

@inproceedings{brown2020better,
  title={Better-than-demonstrator imitation learning via automatically-ranked demonstrations},
  author={Brown, Daniel S and Goo, Wonjoon and Niekum, Scott},
  booktitle={Conference on robot learning},
  pages={330--359},
  year={2020},
  organization={PMLR}
}

@article{zhang2019leveraging,
  title={Leveraging human guidance for deep reinforcement learning tasks},
  author={Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H and Stone, Peter},
  journal={arXiv preprint arXiv:1909.09906},
  year={2019}
}

@article{bobu2022learning,
  title={Learning perceptual concepts by bootstrapping from human queries},
  author={Bobu, Andreea and Paxton, Chris and Yang, Wei and Sundaralingam, Balakumar and Chao, Yu-Wei and Cakmak, Maya and Fox, Dieter},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={4},
  pages={11260--11267},
  year={2022},
  publisher={IEEE}
}

@article{abel2017agent,
  title={Agent-agnostic human-in-the-loop reinforcement learning},
  author={Abel, David and Salvatier, John and Stuhlm{\"u}ller, Andreas and Evans, Owain},
  journal={arXiv preprint arXiv:1701.04079},
  year={2017}
}

@book{sadigh2017active,
  title={Active preference-based learning of reward functions},
  author={Sadigh, Dorsa and Dragan, Anca D and Sastry, Shankar and Seshia, Sanjit A},
  year={2017}
}

@inproceedings{carey2018incorrigibility,
  title={Incorrigibility in the CIRL Framework},
  author={Carey, Ryan},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={30--35},
  year={2018}
}

@article{shah2020benefits,
  title={Benefits of assistance over reward learning},
  author={Shah, Rohin and Freire, Pedro and Alex, Neel and Freedman, Rachel and Krasheninnikov, Dmitrii and Chan, Lawrence and Dennis, Michael D and Abbeel, Pieter and Dragan, Anca and Russell, Stuart},
  journal={OpenReview preprint},
  year={2020}
}

@article{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{zhang2021confidence,
  title={Confidence-aware imitation learning from demonstrations with varying optimality},
  author={Zhang, Songyuan and Cao, Zhangjie and Sadigh, Dorsa and Sui, Yanan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12340--12350},
  year={2021}
}

@article{prabhakaran2021releasing,
  title={On releasing annotator-level labels and information in datasets},
  author={Prabhakaran, Vinodkumar and Davani, Aida Mostafazadeh and Diaz, Mark},
  journal={arXiv preprint arXiv:2110.05699},
  year={2021}
}

@inproceedings{kumar2021designing,
  title={Designing Toxic Content Classification for a Diversity of Perspectives.},
  author={Kumar, Deepak and Kelley, Patrick Gage and Consolvo, Sunny and Mason, Joshua and Bursztein, Elie and Durumeric, Zakir and Thomas, Kurt and Bailey, Michael},
  booktitle={SOUPS@ USENIX Security Symposium},
  pages={299--318},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{biyik2018batch,
  title={Batch active preference-based learning of reward functions},
  author={Biyik, Erdem and Sadigh, Dorsa},
  booktitle={Conference on robot learning},
  pages={519--528},
  year={2018},
  organization={PMLR}
}

@article{daniels2022expertise,
  title={The Expertise Problem: Learning from Specialized Feedback},
  author={Daniels-Koch, Oliver and Freedman, Rachel},
  journal={arXiv preprint arXiv:2211.06519},
  year={2022}
}

@article{barnett2023active,
  title={Active Reward Learning from Multiple Teachers},
  author={Barnett, Peter and Freedman, Rachel and Svegliato, Justin and Russell, Stuart},
  journal={arXiv preprint arXiv:2303.00894},
  year={2023}
}

@inproceedings{noothigattu2018voting,
  title={A voting-based system for ethical decision making},
  author={Noothigattu, Ritesh and Gaikwad, Snehalkumar and Awad, Edmond and Dsouza, Sohan and Rahwan, Iyad and Ravikumar, Pradeep and Procaccia, Ariel},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@article{ngo2022alignment,
  title={The alignment problem from a deep learning perspective},
  author={Ngo, Richard},
  journal={arXiv preprint arXiv:2209.00626},
  year={2022}
}

@article{zhuang2020consequences,
  title={Consequences of misaligned AI},
  author={Zhuang, Simon and Hadfield-Menell, Dylan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15763--15773},
  year={2020}
}

@inproceedings{peng2022investigations,
  title={Investigations of performance and bias in human-AI teamwork in hiring},
  author={Peng, Andi and Nushi, Besmira and Kiciman, Emre and Inkpen, Kori and Kamar, Ece},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={12089--12097},
  year={2022}
}

@article{hendrycks2021unsolved,
  title={Unsolved problems in ml safety},
  author={Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2109.13916},
  year={2021}
}

@misc{timeExclusiveHour,
	author = {Perrigo, Billy},
	title = {Exclusive: The \$2 Per Hour Workers Who Made ChatGPT Safer},
    url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
	year = {2023},
	note = {[Accessed 07-May-2023]},
}

@article{french2019mandela,
  title={The Mandela effect and new memory},
  author={French, Aaron},
  journal={Correspondences},
  volume={6},
  number={2},
  year={2019}
}

@inproceedings{peng2019you,
  title={What you see is what you get? the impact of representation criteria on human bias in hiring},
  author={Peng, Andi and Nushi, Besmira and K{\i}c{\i}man, Emre and Inkpen, Kori and Suri, Siddharth and Kamar, Ece},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={7},
  pages={125--134},
  year={2019}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@article{khlaaf2023toward,
  title={Toward Comprehensive Risk Assessments and Assurance of AI-Based Systems},
  author={Khlaaf, Heidy},
  journal={Trail of Bits},
  year={2023}
}

@article{manheim2018categorizing,
  title={Categorizing variants of Goodhart's Law},
  author={Manheim, David and Garrabrant, Scott},
  journal={arXiv preprint arXiv:1803.04585},
  year={2018}
}

@article{pan2022effects,
  title={The effects of reward misspecification: Mapping and mitigating misaligned models},
  author={Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2201.03544},
  year={2022}
}

@book{united1978belmont,
  title={The Belmont report: ethical principles and guidelines for the protection of human subjects of research},
  author={National Commission for the Protection of Human Subjects, United States},
  volume={1},
  year={1978},
  publisher={United States Department of Health, Education, and Welfare, National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research}
}

@article{ross2017right,
  title={Right for the right reasons: Training differentiable models by constraining their explanations},
  author={Ross, Andrew Slavin and Hughes, Michael C and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1703.03717},
  year={2017}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{skalse2022defining,
  title={Defining and characterizing reward hacking},
  author={Skalse, Joar and Howe, Nikolaus HR and Krasheninnikov, Dmitrii and Krueger, David},
  journal={arXiv preprint arXiv:2209.13085},
  year={2022}
}

@article{hilton2020understanding,
  title={Understanding rl vision},
  author={Hilton, Jacob and Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris},
  journal={Distill},
  volume={5},
  number={11},
  pages={e29},
  year={2020}
}

@article{davani2022dealing,
  title={Dealing with disagreements: Looking beyond the majority vote in subjective annotations},
  author={Davani, Aida Mostafazadeh and D{\'\i}az, Mark and Prabhakaran, Vinodkumar},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={92--110},
  year={2022},
  publisher={MIT Press}
}

@inproceedings{di2022goal,
  title={Goal misgeneralization in deep reinforcement learning},
  author={Di Langosco, Lauro Langosco and Koch, Jack and Sharkey, Lee D and Pfau, Jacob and Krueger, David},
  booktitle={International Conference on Machine Learning},
  pages={12004--12019},
  year={2022},
  organization={PMLR}
}

@article{shah2022goal,
  title={Goal misgeneralization: Why correct specifications aren't enough for correct goals},
  author={Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
  journal={arXiv preprint arXiv:2210.01790},
  year={2022}
}


@article{kirk2023survey,
  title={A Survey of Zero-shot Generalisation in Deep Reinforcement Learning},
  author={Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={Journal of Artificial Intelligence Research},
  volume={76},
  pages={201--264},
  year={2023}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{wan2023poisoning,
  Author = {Alex Wan and Eric Wallace and Sheng Shen and Dan Klein},
  Booktitle = {International Conference on Machine Learning},                   
  Year = {2023},
  Title = {Poisoning Language Models During Instruction Tuning}
}    

@article{carlini2023poisoning,
  title={Poisoning web-scale training datasets is practical},
  author={Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2302.10149},
  year={2023}
}

@misc{siddique2023fairness,
      title={Fairness in Preference-based Reinforcement Learning}, 
      author={Umer Siddique and Abhinav Sinha and Yongcan Cao},
      year={2023},
      eprint={2306.09995},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{li2023multi,
  title={Multi-step Jailbreaking Privacy Attacks on ChatGPT},
  author={Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu},
  journal={arXiv preprint arXiv:2304.05197},
  year={2023}
}

@article{whittlestone2021societal,
  title={The societal implications of deep reinforcement learning},
  author={Whittlestone, Jess and Arulkumaran, Kai and Crosby, Matthew},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1003--1030},
  year={2021}
}

@article{veselovsky2023artificial,
  title={Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks},
  author={Veselovsky, Veniamin and Ribeiro, Manoel Horta and West, Robert},
  journal={arXiv preprint arXiv:2306.07899},
  year={2023}
}

@article{knox2022models,
  title={Models of human preference for learning reward functions},
  author={Knox, W Bradley and Hatgis-Kessell, Stephane and Booth, Serena and Niekum, Scott and Stone, Peter and Allievi, Alessandro},
  journal={arXiv preprint arXiv:2206.02231},
  year={2022}
}

@article{cabi2019scaling,
  title={Scaling data-driven robotics with reward sketching and batch reinforcement learning},
  author={Cabi, Serkan and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and Konyushkova, Ksenia and Reed, Scott and Jeong, Rae and Zolna, Konrad and Aytar, Yusuf and Budden, David and Vecerik, Mel and others},
  journal={arXiv preprint arXiv:1909.12200},
  year={2019}
}

@article{santurkar2023whose,
  title={Whose Opinions Do Language Models Reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2303.17548},
  year={2023}
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{lehman2018surprising,
  title={The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities},
  author={Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J and Bernard, Samuel and Beslon, Guillaume and Bryson, David M and others},
  journal={arXiv preprint arXiv:1803.03453},
  year={2018}
}

@misc{similarweb,
  title={ChatGPT Tops 25 Million Daily Visits},
  author={Similarweb},
  url={https://www.similarweb.com/blog/insights/ai-news/chatgpt-25-million/},
  year={2023}
}

@article{krakovna2020specification,
  title={Specification gaming: the flip side of AI ingenuity},
  author={Krakovna, Victoria and Uesato, Jonathan and Mikulik, Vladimir and Rahtz, Matthew and Everitt, Tom and Kumar, Ramana and Kenton, Zac and Leike, Jan and Legg, Shane},
  journal={DeepMind Blog},
  year={2020}
}

@article{uc2023survey,
  title={Survey on reinforcement learning for language processing},
  author={Uc-Cetina, Victor and Navarro-Guerrero, Nicolas and Martin-Gonzalez, Anabel and Weber, Cornelius and Wermter, Stefan},
  journal={Artificial Intelligence Review},
  volume={56},
  number={2},
  pages={1543--1575},
  year={2023},
  publisher={Springer}
}

@article{lindner2023learning,
  title={Learning Safety Constraints from Demonstrations with Unknown Rewards},
  author={Lindner, David and Chen, Xin and Tschiatschek, Sebastian and Hofmann, Katja and Krause, Andreas},
  journal={arXiv preprint arXiv:2305.16147},
  year={2023}
}

@inproceedings{malik2021inverse,
  title={Inverse constrained reinforcement learning},
  author={Malik, Shehryar and Anwar, Usman and Aghasi, Alireza and Ahmed, Ali},
  booktitle={International conference on machine learning},
  pages={7390--7399},
  year={2021},
  organization={PMLR}
}


@inproceedings{brown2020safe,
  title={Safe imitation learning via fast bayesian reward inference from preferences},
  author={Brown, Daniel and Coleman, Russell and Srinivasan, Ravi and Niekum, Scott},
  booktitle={International Conference on Machine Learning},
  pages={1165--1177},
  year={2020},
  organization={PMLR}
}


@article{rafailov2023direct,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@misc{asilomar2017,
  title={AI Principles},
  year={2017},
  url={https://futureoflife.org/open-letter/ai-principles/}
}

@misc{google2023,
  author={Google},
  title={Bard},
  year={2023},
  url={https://bard.google.com/}
}

@misc{arcevals,
  author={ARC},
  title={ARC Evals},
  year={2022},
  url={https://evals.alignment.org/}
}

@misc{anthropic2023,
  author={Anthropic},
  title={Introducing Claude},
  year={2023},
  url={https://www.anthropic.com/index/introducing-claude}
}

@misc{microsoft2023,
  author={Microsoft},
  title={Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web},
  year={2023},
  url={https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/}
}

@article{kirk2023personalisation,
  title={Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback},
  author={Kirk, Hannah Rose and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A},
  journal={arXiv preprint arXiv:2303.05453},
  year={2023}
}

@article{casper2020achilles,
  title={Achilles Heels for AGI/ASI via Decision Theoretic Adversaries},
  author={Casper, Stephen},
  journal={arXiv preprint arXiv:2010.05418},
  year={2020}
}

@misc{wang2023aligning,
      title={Aligning Large Language Models with Human: A Survey}, 
      author={Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu},
      year={2023},
      eprint={2307.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hao2023hidden,
  author={Hao, Karen},
  title={The Hidden Workforce That Helped Filter Violence and Abuse Out of ChatGPT},
  date={July 11, 2023},
  series={Podcast audio},
  year={2023},
  url={https://www.wsj.com/podcasts/the-journal/the-hidden-workforce-that-helped-filter-violence-and-abuse-out-of-chatgpt/ffc2427f-bdd8-47b7-9a4b-27e7267cf413}
}

@article{hoskin1996awful,
  title={The ‘awful idea of accountability’: inscribing people into the measurement of objects},
  author={Hoskin, Keith},
  journal={Accountability: Power, ethos and the technologies of managing},
  volume={265},
  year={1996},
  publisher={International Thomson Business Press London}
}

@inproceedings{gordon2022jury,
  title={Jury learning: Integrating dissenting voices into machine learning models},
  author={Gordon, Mitchell L and Lam, Michelle S and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@inproceedings{gordon2021disagreement,
  title={The disagreement deconvolution: Bringing machine learning performance metrics in line with reality},
  author={Gordon, Mitchell L and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}

@inproceedings{baumler2023examples,
  title={Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree},
  author={Baumler, Connor and Sotnikova, Anna and Daum{\'e} III, Hal},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={10352--10371},
  year={2023}
}

@misc{openchatkit2023,
  author={OpenChatKit},
  title={Announcing OpenChatKit},
  year={2023},
  url={https://www.together.xyz/blog/openchatkit}
}

@inproceedings{knox2009interactively,
  title={Interactively shaping agents via human reinforcement: The TAMER framework},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={Proceedings of the fifth international conference on Knowledge capture},
  pages={9--16},
  year={2009}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{meta2023llama2,
      title={Meta and Microsoft Introduce the Next Generation of Llama}, 
      author={Meta},
      year={2023},
      url={https://about.fb.com/news/2023/07/llama-2/}
}

@article{perez2022discovering,
  title={Discovering Language Model Behaviors with Model-Written Evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@misc{cotra2021cold,
  title={Why AI alignment could be hard with modern deep learning},
  author={Cotra, Ajeya},
  howpublished={\url{https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/}},
  year={2021}
}


@article{pandey2022modeling,
  title={Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning},
  author={Pandey, Rahul and Purohit, Hemant and Castillo, Carlos and Shalin, Valerie L},
  journal={International Journal of Human-Computer Studies},
  volume={160},
  pages={102772},
  year={2022},
  publisher={Elsevier}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{chmielewski2020mturk,
  title={An MTurk crisis? Shifts in data quality and the impact on study results},
  author={Chmielewski, Michael and Kucker, Sarah C},
  journal={Social Psychological and Personality Science},
  volume={11},
  number={4},
  pages={464--473},
  year={2020},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{gilardi2023chatgpt,
  title={Chatgpt outperforms crowd-workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={arXiv preprint arXiv:2303.15056},
  year={2023}
}

@article{zhang2023language,
  title={How language model hallucinations can snowball},
  author={Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A},
  journal={arXiv preprint arXiv:2305.13534},
  year={2023}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{abid2021persistent,
  title={Persistent anti-muslim bias in large language models},
  author={Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={298--306},
  year={2021}
}

@article{carlini2023aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Awadalla, Anas and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and others},
  journal={arXiv preprint arXiv:2306.15447},
  year={2023}
}

@article{kos2017delving,
  title={Delving into adversarial attacks on deep policies},
  author={Kos, Jernej and Song, Dawn},
  journal={arXiv preprint arXiv:1705.06452},
  year={2017}
}

@misc{rao2023tricking,
      title={Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks}, 
      author={Abhinav Rao and Sachin Vashistha and Atharva Naik and Somak Aditya and Monojit Choudhury},
      year={2023},
      eprint={2305.14965},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023jailbreaking,
      title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study}, 
      author={Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
      year={2023},
      eprint={2305.13860},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{wolf2023fundamental,
  title={Fundamental Limitations of Alignment in Large Language Models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  volume={6},
  year={2021}
}

@article{inan2021training,
  title={Training data leakage analysis in language models},
  author={Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and R{\"u}hle, Victor and Withers, James and Sim, Robert},
  journal={arXiv preprint arXiv:2101.05405},
  year={2021}
}

@inproceedings{pan2020privacy,
  title={Privacy risks of general-purpose language models},
  author={Pan, Xudong and Zhang, Mi and Ji, Shouling and Yang, Min},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)},
  pages={1314--1331},
  year={2020},
  organization={IEEE}
}

@misc{wu2023finegrained,
      title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training}, 
      author={Zeqiu Wu and Yushi Hu and Weijia Shi and Nouha Dziri and Alane Suhr and Prithviraj Ammanabrolu and Noah A. Smith and Mari Ostendorf and Hannaneh Hajishirzi},
      year={2023},
      eprint={2306.01693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{guerdan2023ground,
  title={Ground (less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making},
  author={Guerdan, Luke and Coston, Amanda and Wu, Zhiwei Steven and Holstein, Kenneth},
  journal={arXiv preprint arXiv:2302.06503},
  year={2023}
}

@article{song2023reward,
  title={Reward Collapse in Aligning Large Language Models},
  author={Song, Ziang and Cai, Tianle and Lee, Jason D and Su, Weijie J},
  journal={arXiv preprint arXiv:2305.17608},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{zhu2023principled,
  title={Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons},
  author={Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2301.11270},
  year={2023}
}

@article{akyurek2023rl4f,
  title={RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs},
  author={Aky{\"u}rek, Afra Feyza and Aky{\"u}rek, Ekin and Madaan, Aman and Kalyan, Ashwin and Clark, Peter and Wijaya, Derry and Tandon, Niket},
  journal={arXiv preprint arXiv:2305.08844},
  year={2023}
}

@article{du2023improving,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}

@article{roger2023large,
  title={Large Language Models Sometimes Generate Purely Negatively-Reinforced Text},
  author={Roger, Fabien},
  journal={arXiv preprint arXiv:2306.07567},
  year={2023}
}

@article{kim2023aligning,
  title={Aligning Large Language Models through Synthetic Feedback},
  author={Kim, Sungdong and Bae, Sanghwan and Shin, Jamin and Kang, Soyoung and Kwak, Donghyun and Yoo, Kang Min and Seo, Minjoon},
  journal={arXiv preprint arXiv:2305.13735},
  year={2023}
}

@inproceedings{o2020windfall,
  title={The windfall clause: Distributing the benefits of AI for the common good},
  author={O'Keefe, Cullen and Cihon, Peter and Garfinkel, Ben and Flynn, Carrick and Leung, Jade and Dafoe, Allan},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={327--331},
  year={2020}
}

@misc{ye2023selfee,
      title={SELFEE: ITERATIVE SELF-REVISING LLM EMPOWERED BY SELF-FEEDBACK GENERATION}, 
      author={Seonghyeon Ye and Yongrae Jo and Doyoung Kim and Sungdong Kim and Hyeonbin Hwang and Minjoon Seo},
      year={2023},
      url={https://kaistai.github.io/SelFee/}
}

@misc{anderljung2023frontier,
      title={Frontier AI Regulation: Managing Emerging Risks to Public Safety}, 
      author={Markus Anderljung and Joslyn Barnhart and Jade Leung and Anton Korinek and Cullen O'Keefe and Jess Whittlestone and Shahar Avin and Miles Brundage and Justin Bullock and Duncan Cass-Beggs and Ben Chang and Tantum Collins and Tim Fist and Gillian Hadfield and Alan Hayes and Lewis Ho and Sara Hooker and Eric Horvitz and Noam Kolt and Jonas Schuett and Yonadav Shavit and Divya Siddarth and Robert Trager and Kevin Wolf},
      year={2023},
      eprint={2307.03718},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{kuziemski2020ai,
  title={AI governance in the public sector: Three tales from the frontiers of automated decision-making in democratic settings},
  author={Kuziemski, Maciej and Misuraca, Gianluca},
  journal={Telecommunications policy},
  volume={44},
  number={6},
  pages={101976},
  year={2020},
  publisher={Elsevier}
}

@article{shevlane2023model,
  title={Model evaluation for extreme risks},
  author={Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and others},
  journal={arXiv preprint arXiv:2305.15324},
  year={2023}
}

@article{el2022sok,
  title={SoK: On the Impossible Security of Very Large Foundation Models},
  author={El-Mhamdi, El-Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Hoang, L{\^e}-Nguy{\^e}n and Pinot, Rafael and Stephan, John},
  journal={arXiv preprint arXiv:2209.15259},
  year={2022}
}

@article{hubinger2019risks,
  title={Risks from learned optimization in advanced machine learning systems},
  author={Hubinger, Evan and van Merwijk, Chris and Mikulik, Vladimir and Skalse, Joar and Garrabrant, Scott},
  journal={arXiv preprint arXiv:1906.01820},
  year={2019}
}
@misc{mode_collapse,
    Author = {Janus},
      howpublished = {\url{https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse}},
    Title = {Mysteries of mode collapse},
Year = {2022}}

@inproceedings{
khalifa2021a,
title={A Distributional Approach to Controlled Text Generation},
author={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=jWkw45-9AbL}
}

@article{casper2023benchmarking,
  title={Benchmarking Interpretability Tools for Deep Neural Networks},
  author={Casper, Stephen and Li, Yuxiao and Li, Jiawei and Bu, Tong and Zhang, Kevin and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2302.10894},
  year={2023}
}

@article{hartmann2023political,
  title={The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation},
  author={Hartmann, Jochen and Schwenzow, Jasper and Witte, Maximilian},
  journal={arXiv preprint arXiv:2301.01768},
  year={2023}
}

@article{bakker2022fine,
  title={Fine-tuning language models to find agreement among humans with diverse preferences},
  author={Bakker, Michiel and Chadwick, Martin and Sheahan, Hannah and Tessler, Michael and Campbell-Gillingham, Lucy and Balaguer, Jan and McAleese, Nat and Glaese, Amelia and Aslanides, John and Botvinick, Matt and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38176--38189},
  year={2022}
}

@misc{oneal2023,
  author = {Oneal, A.J.},
  title = {Chat GPT "DAN" (and other "Jailbreaks")},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516}}}

@article{turner2022parametrically,
  title={Parametrically Retargetable Decision-Makers Tend To Seek Power},
  author={Turner, Alex and Tadepalli, Prasad},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31391--31401},
  year={2022}
}

 @misc{turner2021seeking, title={Seeking Power is Convergently Instrumental in a Broad Class of Environments}, url={https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk/p/hzeLSQ9nwDkPc4KNt}, journal={Alignment Forum}, author={Turner, Alexander M}, year={2021}} 

@inproceedings{korbak-etal-2022-rl,
    title = "{RL} with {KL} penalties is better viewed as {B}ayesian inference",
    author = "Korbak, Tomasz  and
      Perez, Ethan  and
      Buckley, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.77",
    pages = "1083--1091",
    abstract = "Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.",
}
@misc{korbak2023pretraining,
      title={Pretraining Language Models with Human Preferences}, 
      author={Tomasz Korbak and Kejian Shi and Angelica Chen and Rasika Bhalerao and Christopher L. Buckley and Jason Phang and Samuel R. Bowman and Ethan Perez},
      year={2023},
      eprint={2302.08582},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{weidinger2021ethical,
      title={Ethical and social risks of harm from Language Models}, 
      author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
      year={2021},
      eprint={2112.04359},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{glaese2022improving,
      title={Improving alignment of dialogue agents via targeted human judgements}, 
      author={Amelia Glaese and Nat McAleese and Maja Trębacz and John Aslanides and Vlad Firoiu and Timo Ewalds and Maribeth Rauh and Laura Weidinger and Martin Chadwick and Phoebe Thacker and Lucy Campbell-Gillingham and Jonathan Uesato and Po-Sen Huang and Ramona Comanescu and Fan Yang and Abigail See and Sumanth Dathathri and Rory Greig and Charlie Chen and Doug Fritz and Jaume Sanchez Elias and Richard Green and Soňa Mokrá and Nicholas Fernando and Boxi Wu and Rachel Foley and Susannah Young and Iason Gabriel and William Isaac and John Mellor and Demis Hassabis and Koray Kavukcuoglu and Lisa Anne Hendricks and Geoffrey Irving},
      year={2022},
      eprint={2209.14375},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{rlhf_does_not_cause_mode_collapse,
    Author = {Arthur Conmy and Beren Millidge},
      howpublished = {\url{https://www.lesswrong.com/posts/pjesEx526ngE6dnmr/rlhf-does-not-appear-to-differentially-cause-mode-collapse}},
    Title = {RLHF does not appear to differentially cause mode-collapse},
Year = {2023}}
@misc{go2023aligning,
      title={Aligning Language Models with Preferences through f-divergence Minimization}, 
      author={Dongyoung Go and Tomasz Korbak and Germán Kruszewski and Jos Rozen and Nahyeon Ryu and Marc Dymetman},
      year={2023},
      eprint={2302.08215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{korbak_2022,
 author = {Korbak, Tomasz and Elsahar, Hady and Kruszewski, Germ\'{a}n and Dymetman, Marc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16203--16220},
 publisher = {Curran Associates, Inc.},
 title = {On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{krueger2020hidden,
      title={Hidden Incentives for Auto-Induced Distributional Shift}, 
      author={David Krueger and Tegan Maharaj and Jan Leike},
      year={2020},
      eprint={2009.09153},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lightman2023let,
  title={Let's Verify Step by Step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}

@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@article{sen1986social,
  title={Social choice theory},
  author={Sen, Amartya},
  journal={Handbook of mathematical economics},
  volume={3},
  pages={1073--1181},
  year={1986},
  publisher={Elsevier}
}

@inproceedings{rauker2023toward,
  title={Toward transparent ai: A survey on interpreting the inner structures of deep neural networks},
  author={R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  pages={464--483},
  year={2023},
  organization={IEEE}
}

@misc{touvron2023llama,
    title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{scheurer2022training,
  title={Training language models with language feedback},
  author={Scheurer, J{\'e}r{\'e}my and Campos, Jon Ander and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan},
  booktitle={The First Workshop on Learning with Natural Language Supervision at ACL},
  year={2022}
}

@article{hernandez2023measuring,
  title={Measuring and manipulating knowledge representations in language models},
  author={Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
  journal={arXiv preprint arXiv:2304.00740},
  year={2023}
}

@article{scheurer2023training,
  title={Training language models with language feedback at scale},
  author={Scheurer, J{\'e}r{\'e}my and Campos, Jon Ander and Korbak, Tomasz and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan},
  journal={arXiv preprint arXiv:2303.16755},
  year={2023}
}

@article{chen2023improving,
  title={Improving Code Generation by Training with Natural Language Feedback},
  author={Chen, Angelica and Scheurer, J{\'e}r{\'e}my and Korbak, Tomasz and Campos, Jon Ander and Chan, Jun Shern and Bowman, Samuel R and Cho, Kyunghyun and Perez, Ethan},
  journal={arXiv preprint arXiv:2303.16749},
  year={2023}
}



@article{goyal2019using,
  title={Using natural language for reward shaping in reinforcement learning},
  author={Goyal, Prasoon and Niekum, Scott and Mooney, Raymond J},
  journal={arXiv preprint arXiv:1903.02020},
  year={2019}
}

@inproceedings{sumers2021learning,
  title={Learning rewards from linguistic feedback},
  author={Sumers, Theodore R and Ho, Mark K and Hawkins, Robert D and Narasimhan, Karthik and Griffiths, Thomas L},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={6002--6010},
  year={2021}
}

@article{fu2019language,
  title={From language to goals: Inverse reinforcement learning for vision-based instruction following},
  author={Fu, Justin and Korattikara, Anoop and Levine, Sergey and Guadarrama, Sergio},
  journal={arXiv preprint arXiv:1902.07742},
  year={2019}
}

@inproceedings{zhou2021inverse,
  title={Inverse reinforcement learning with natural language goals},
  author={Zhou, Li and Small, Kevin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={11116--11124},
  year={2021}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{xu2023instructions,
  title={Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models},
  author={Xu, Jiashu and Ma, Mingyu Derek and Wang, Fei and Xiao, Chaowei and Chen, Muhao},
  journal={arXiv preprint arXiv:2305.14710},
  year={2023}
}

@misc{ethayarajh2022authenticity,
      title={The Authenticity Gap in Human Evaluation}, 
      author={Kawin Ethayarajh and Dan Jurafsky},
      year={2022},
      eprint={2205.11930},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yannakakis2011ranking,
  title={Ranking vs. preference: a comparative study of self-reporting},
  author={Yannakakis, Georgios N and Hallam, John},
  booktitle={Affective Computing and Intelligent Interaction: 4th International Conference, ACII 2011, Memphis, TN, USA, October 9--12, 2011, Proceedings, Part I 4},
  pages={437--446},
  year={2011},
  organization={Springer}
}

@inproceedings{brown2019extrapolating,
  title={Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations},
  author={Brown, Daniel and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
  booktitle={International conference on machine learning},
  pages={783--792},
  year={2019},
  organization={PMLR}
}

@inproceedings{wilde2022learning,
  title={Learning Reward Functions from Scale Feedback},
  author={Wilde, Nils and Biyik, Erdem and Sadigh, Dorsa and Smith, Stephen L},
  booktitle={Conference on Robot Learning},
  pages={353--362},
  year={2022},
  organization={PMLR}
}

@inproceedings{ng2000algorithms,   title={Algorithms for inverse reinforcement learning.},   author={Ng, Andrew Y and Russell, Stuart and others},   booktitle={Icml},   volume={1},   pages={2},   year={2000} } 

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@inproceedings{ramachandran2007bayesian,
author = {Ramachandran, Deepak and Amir, Eyal},
title = {Bayesian Inverse Reinforcement Learning},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2586–2591},
numpages = {6},
location = {Hyderabad, India},
series = {IJCAI'07}
}

@article{pomerleau1988alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{ibarz2018reward,
  title={Reward learning from human preferences and demonstrations in atari},
  author={Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{critch2020ai,
  title={AI research considerations for human existential safety (ARCHES)},
  author={Critch, Andrew and Krueger, David},
  journal={arXiv preprint arXiv:2006.04948},
  year={2020}
}

@article{biyik2022learning,
  title={Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
  author={B{\i}y{\i}k, Erdem and Losey, Dylan P and Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
  journal={The International Journal of Robotics Research},
  volume={41},
  number={1},
  pages={45--67},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{hubinger2023conditioning,
  title={Conditioning Predictive Models: Risks and Strategies},
  author={Hubinger, Evan and Jermyn, Adam and Treutlein, Johannes and Hudson, Rubi and Woolverton, Kate},
  journal={arXiv preprint arXiv:2302.00805},
  year={2023}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{emmons2021rvs,
  title={RvS: What is Essential for Offline RL via Supervised Learning?},
  author={Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2112.10751},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@inproceedings{codevilla2018end,
  title={End-to-end driving via conditional imitation learning},
  author={Codevilla, Felipe and M{\"u}ller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={4693--4700},
  year={2018},
  organization={IEEE}
}

@article{eysenbach2020rewriting,
  title={Rewriting history with inverse rl: Hindsight inference for policy improvement},
  author={Eysenbach, Ben and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={14783--14795},
  year={2020}
}

@article{shi2022life,
  title={When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels},
  author={Shi, Weiyan and Dinan, Emily and Shuster, Kurt and Weston, Jason and Xu, Jing},
  journal={arXiv preprint arXiv:2210.15893},
  year={2022}
}



@article{hong2022sensitivity,
  title={On the Sensitivity of Reward Inference to Misspecified Human Models},
  author={Hong, Joey and Bhatia, Kush and Dragan, Anca},
  journal={arXiv preprint arXiv:2212.04717},
  year={2022}
}

@article{bobu2020quantifying,
  title={Quantifying hypothesis space misspecification in learning from human--robot demonstrations and physical corrections},
  author={Bobu, Andreea and Bajcsy, Andrea and Fisac, Jaime F and Deglurkar, Sampada and Dragan, Anca D},
  journal={IEEE Transactions on Robotics},
  volume={36},
  number={3},
  pages={835--854},
  year={2020},
  publisher={IEEE}
}

@article{freedman2021choice,
  title={Choice set misspecification in reward inference},
  author={Freedman, Rachel and Shah, Rohin and Dragan, Anca},
  journal={arXiv preprint arXiv:2101.07691},
  year={2021}
}

@inproceedings{milli2020literal,
  title={Literal or pedagogic human? analyzing human model misspecification in objective learning},
  author={Milli, Smitha and Dragan, Anca D},
  booktitle={Uncertainty in artificial intelligence},
  pages={925--934},
  year={2020},
  organization={PMLR}
}

@article{lindner2022humans,
  title={Humans are not Boltzmann Distributions: Challenges and Opportunities for Modelling Human Feedback and Interaction in Reinforcement Learning},
  author={Lindner, David and El-Assady, Mennatallah},
  journal={arXiv preprint arXiv:2206.13316},
  year={2022}
}

@article{milano2021ethical,
  title={Ethical aspects of multi-stakeholder recommendation systems},
  author={Milano, Silvia and Taddeo, Mariarosaria and Floridi, Luciano},
  journal={The information society},
  volume={37},
  number={1},
  pages={35--45},
  year={2021},
  publisher={Taylor \& Francis}
}

@inproceedings{skalsereward,
  title={The Reward Hypothesis is False},
  author={Skalse, Joar Max Viktor and Abate, Alessandro},
  booktitle={NeurIPS ML Safety Workshop},
  year={2022}
}

@article{bobu2023aligning,
  title={Aligning Robot and Human Representations},
  author={Bobu, Andreea and Peng, Andi and Agrawal, Pulkit and Shah, Julie and Dragan, Anca D},
  journal={arXiv preprint arXiv:2302.01928},
  year={2023}
}

@article{silver2021reward,
  title={Reward is enough},
  author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S},
  journal={Artificial Intelligence},
  volume={299},
  pages={103535},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{peng2023diagnosis,
  title={Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation},
  author={Peng, Andi and Netanyahu, Aviv and Ho, Mark K and Shu, Tianmin and Bobu, Andreea and Shah, Julie and Agrawal, Pulkit},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  year={2023}
}

@article{skalse2022misspecification,
  title={Misspecification in Inverse Reinforcement Learning},
  author={Skalse, Joar and Abate, Alessandro},
  journal={arXiv preprint arXiv:2212.03201},
  year={2022}
}

@inproceedings{mindermann2018occam,
author = {Mindermann, Soren and Armstrong, Stuart},
title = {Occam's Razor is Insufficient to Infer the Preferences of Irrational Agents},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5603–5614},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{shah2019feasibility,
  title={On the feasibility of learning, rather than assuming, human biases for reward inference},
  author={Shah, Rohin and Gundotra, Noah and Abbeel, Pieter and Dragan, Anca},
  booktitle={International Conference on Machine Learning},
  pages={5670--5679},
  year={2019},
  organization={PMLR}
}

@article{losey2022physical,
  title={Physical interaction as communication: Learning robot objectives online from human corrections},
  author={Losey, Dylan P and Bajcsy, Andrea and O’Malley, Marcia K and Dragan, Anca D},
  journal={The International Journal of Robotics Research},
  volume={41},
  number={1},
  pages={20--44},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{li2021learning,
  title={Learning human objectives from sequences of physical corrections},
  author={Li, Mengxi and Canberk, Alper and Losey, Dylan P and Sadigh, Dorsa},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2877--2883},
  year={2021},
  organization={IEEE}
}

@article{jeon2020reward,
  title={Reward-rational (implicit) choice: A unifying formalism for reward learning},
  author={Jeon, Hong Jun and Milli, Smitha and Dragan, Anca},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4415--4426},
  year={2020}
}

@article{palan2019learning,
  title={Learning reward functions by integrating human demonstrations and preferences},
  author={Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:1906.08928},
  year={2019}
}

@article{mehta2022unified,
  title={Unified Learning from Demonstrations, Corrections, and Preferences during Physical Human-Robot Interaction},
  author={Mehta, Shaunak A and Losey, Dylan P},
  journal={arXiv preprint arXiv:2207.03395},
  year={2022}
}

@misc{freire2020derail,
      title={DERAIL: Diagnostic Environments for Reward And Imitation Learning}, 
      author={Pedro Freire and Adam Gleave and Sam Toyer and Stuart Russell},
      year={2020},
      eprint={2012.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{carroll22a,
  title = {Estimating and Penalizing Induced Preference Shifts in Recommender Systems},
  author = {Carroll, Micah D and Dragan, Anca and Russell, Stuart and Hadfield-Menell, Dylan},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
}



@article{gao2022scaling,
  title={Scaling Laws for Reward Model Overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  journal={arXiv preprint arXiv:2210.10760},
  year={2022}
}

@misc{chan2021human,
      title={Human irrationality: both bad and good for reward inference}, 
      author={Lawrence Chan and Andrew Critch and Anca Dragan},
      year={2021},
      eprint={2111.06956},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wu2021recursively,
      title={Recursively Summarizing Books with Human Feedback}, 
      author={Jeff Wu and Long Ouyang and Daniel M. Ziegler and Nisan Stiennon and Ryan Lowe and Jan Leike and Paul Christiano},
      year={2021},
      eprint={2109.10862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
dard,
title={Dynamics-Aware Comparison of Learned Reward Functions},
author={Blake Wulfe and Logan Michael Ellis and Jean Mercat and Rowan Thomas McAllister and Adrien Gaidon},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=CALFyKVs87}
}

@article{epic,
  title={Quantifying differences in reward functions},
  author={Gleave, Adam and Dennis, Michael and Legg, Shane and Russell, Stuart and Leike, Jan},
  journal={arXiv preprint arXiv:2006.13900},
  year={2020}
}


@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={49--58},
  year={2016},
  organization={PMLR}
}

@inproceedings{bajcsy2017learning,
  title={Learning robot objectives from physical human interaction},
  author={Bajcsy, Andrea and Losey, Dylan P and O’malley, Marcia K and Dragan, Anca D},
  booktitle={Conference on Robot Learning},
  pages={217--226},
  year={2017},
  organization={PMLR}
}

@article{sharma2022correcting,
  title={Correcting robot plans with natural language feedback},
  author={Sharma, Pratyusha and Sundaralingam, Balakumar and Blukis, Valts and Paxton, Chris and Hermans, Tucker and Torralba, Antonio and Andreas, Jacob and Fox, Dieter},
  journal={arXiv preprint arXiv:2204.05186},
  year={2022}
}

%% RL is difficult to Optimize 

@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29304--29320},
  year={2021}
}

@article{jayawardana2022impact,
  title={The Impact of Task Underspecification in Evaluating Deep Reinforcement Learning},
  author={Jayawardana, Vindula and Tang, Catherine and Li, Sirui and Suo, Dajiang and Wu, Cathy},
  journal={arXiv preprint arXiv:2210.08607},
  year={2022}
} 

@article{amin2021survey,
  title={A survey of exploration methods in reinforcement learning},
  author={Amin, Susan and Gomrokchi, Maziar and Satija, Harsh and van Hoof, Herke and Precup, Doina},
  journal={arXiv preprint arXiv:2109.00157},
  year={2021}
}

@article{yang2021exploration,
  title={Exploration in deep reinforcement learning: a comprehensive survey},
  author={Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Hao, Jianye and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
  journal={arXiv preprint arXiv:2109.06668},
  year={2021}
}

@inproceedings{nikishin2018improving,
  title={Improving stability in deep reinforcement learning with weight averaging},
  author={Nikishin, Evgenii and Izmailov, Pavel and Athiwaratkun, Ben and Podoprikhin, Dmitrii and Garipov, Timur and Shvechikov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={Uncertainty in artificial intelligence workshop on uncertainty in Deep learning},
  year={2018}
}

@misc{rlblogpost,
    title={Deep Reinforcement Learning Doesn't Work Yet},
    author={Irpan, Alex},
    howpublished={\url{https://www.alexirpan.com/2018/02/14/rl-hard.html}},
    year={2018}
}

@misc{christiano2019worst,
    title={Worst-case guarantees},
    author={Christiano, Paul},
    howpublished={\url{https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d}},
    year={2019}
}

@article{ding2020challenges,
  title={Challenges of reinforcement learning},
  author={Ding, Zihan and Dong, Hao},
  journal={Deep Reinforcement Learning: Fundamentals, Research and Applications},
  pages={249--272},
  year={2020},
  publisher={Springer}
}

@inproceedings{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@article{michaud2020understanding,
  title={Understanding learned reward functions},
  author={Michaud, Eric J and Gleave, Adam and Russell, Stuart},
  journal={arXiv preprint arXiv:2012.05862},
  year={2020}
}

@inproceedings{tien2023causal,
  title={Causal Confusion and Reward Misidentification in Preference-Based Reward Learning},
  author={Tien, Jeremy and He, Jerry Zhi-Yang and Erickson, Zackory and Dragan, Anca and Brown, Daniel S},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{mckinney2023fragility,
  title={On The Fragility of Learned Reward Functions},
  author={McKinney, Lev and Duan, Yawen and Krueger, David and Gleave, Adam},
  journal={arXiv preprint arXiv:2301.03652},
  year={2023}
}

@article{fu2017learning,
  title={Learning robust rewards with adversarial inverse reinforcement learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  journal={arXiv preprint arXiv:1710.11248},
  year={2017}
}

@inproceedings{Gleave2020Adversarial,
	title={Adversarial Policies: Attacking Deep Reinforcement Learning},
	author={Adam Gleave and Michael Dennis and Cody Wild and Neel Kant and Sergey Levine and Stuart Russell},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=HJgEMpVFwB}
}

@inproceedings{wu2021adversarial,
  title={Adversarial Policy Training against Deep Reinforcement Learning.},
  author={Wu, Xian and Guo, Wenbo and Wei, Hua and Xing, Xinyu},
  booktitle={USENIX Security Symposium},
  pages={1883--1900},
  year={2021}
}

@article{wang2022adversarial,
  title={Adversarial Policies Beat Professional-Level Go AIs},
  author={Wang, Tony Tong and Gleave, Adam and Belrose, Nora and Tseng, Tom and Miller, Joseph and Dennis, Michael D and Duan, Yawen and Pogrebniak, Viktor and Levine, Sergey and Russell, Stuart},
  journal={arXiv preprint arXiv:2211.00241},
  year={2022}
}

@article{grupen2023policy,
  title={Policy-Value Alignment and Robustness in Search-based Multi-Agent Learning},
  author={Grupen, Niko A and Hanlon, Michael and Hao, Alexis and Lee, Daniel D and Selman, Bart},
  journal={arXiv preprint arXiv:2301.11857},
  year={2023}
}

@article{ilahi2021challenges,
  title={Challenges and countermeasures for adversarial attacks on deep reinforcement learning},
  author={Ilahi, Inaam and Usama, Muhammad and Qadir, Junaid and Janjua, Muhammad Umar and Al-Fuqaha, Ala and Hoang, Dinh Thai and Niyato, Dusit},
  journal={IEEE Transactions on Artificial Intelligence},
  volume={3},
  number={2},
  pages={90--109},
  year={2021},
  publisher={IEEE}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{jailbreakChat2023,
    title={Jailbreak Chat},
    author={Alex Albert},
    url={https://www.jailbreakchat.com/},
    year={2023}
}

@article{promptInjection2023,
    title={Prompt Injection},
    author={Simon Willison},
    url={https://simonwillison.net/series/prompt-injection/},
    year={2023}
}

@article{brown2018unrestricted,
  title={Unrestricted adversarial examples},
  author={Brown, Tom B and Carlini, Nicholas and Zhang, Chiyuan and Olsson, Catherine and Christiano, Paul and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1809.08352},
  year={2018}
}

@article{vinitsky2020robust,
  title={Robust reinforcement learning using adversarial populations},
  author={Vinitsky, Eugene and Du, Yuqing and Parvate, Kanaad and Jang, Kathy and Abbeel, Pieter and Bayen, Alexandre},
  journal={arXiv preprint arXiv:2008.01825},
  year={2020}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{hendrycks2023natural,
  title={Natural Selection Favors AIs over Humans},
  author={Hendrycks, Dan},
  journal={arXiv preprint arXiv:2303.16200},
  year={2023}
}


%%%

@inproceedings{myers2022learning,
  title={Learning multimodal rewards from rankings},
  author={Myers, Vivek and Biyik, Erdem and Anari, Nima and Sadigh, Dorsa},
  booktitle={Conference on Robot Learning},
  pages={342--352},
  year={2021},
  organization={PMLR}
}
@article{yu2023language,
title={Language to Rewards for Robotic Skill Synthesis},
author={Yu, Wenhao and Gileadi, Nimrod and Fu, Chuyuan and Kirmani, Sean and Lee, Kuang-Huei and Gonzalez Arenas, Montse and Lewis Chiang, Hao-Tien and Erez, Tom and Hasenclever, Leonard and Humplik, Jan and Ichter, Brian and Xiao, Ted and Xu, Peng and Zeng, Andy and Zhang, Tingnan and Heess, Nicolas and Sadigh, Dorsa and Tan, Jie and Tassa, Yuval and Xia, Fei},
year={2023},
journal={Arxiv  preprint arXiv:2306.08647},
}


@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@inproceedings{biyik2020active,
 title={Active Preference-Based Gaussian Process Regression for Reward Learning},
 author={Biyik, Erdem and Huynh, Nicolas and Kochenderfer, Mykel J. and Sadigh, Dorsa},
 booktitle={Proceedings of Robotics: Science and Systems (RSS)},
 year={2020},
 month=jul,
 doi={10.15607/rss.2020.xvi.041}
}

@article{vamplew2022scalar,
  title={Scalar reward is not enough: A response to Silver, Singh, Precup and Sutton (2021)},
  author={Vamplew, Peter and Smith, Benjamin J and K{\"a}llstr{\"o}m, Johan and Ramos, Gabriel and R{\u{a}}dulescu, Roxana and Roijers, Diederik M and Hayes, Conor F and Heintz, Fredrik and Mannion, Patrick and Libin, Pieter JK and others},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={36},
  number={2},
  pages={41},
  year={2022},
  publisher={Springer}
}




@article{rendle2012bpr,
  title={BPR: Bayesian personalized ranking from implicit feedback},
  author={Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
  journal={arXiv preprint arXiv:1205.2618},
  year={2012}
}
@inproceedings{weibl2021,
    title = "Challenges in Detoxifying Language Models",
    author = "Welbl, Johannes  and
      Glaese, Amelia  and
      Uesato, Jonathan  and
      Dathathri, Sumanth  and
      Mellor, John  and
      Hendricks, Lisa Anne  and
      Anderson, Kirsty  and
      Kohli, Pushmeet  and
      Coppin, Ben  and
      Huang, Po-Sen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.210",
    doi = "10.18653/v1/2021.findings-emnlp.210",
    pages = "2447--2469",
    abstract = "Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions{---}highlighting further the nuances involved in careful evaluation of LM toxicity.",
}
@inproceedings{palms,
 author = {Solaiman, Irene and Dennison, Christy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {5861--5873},
 publisher = {Curran Associates, Inc.},
 title = {Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets},
 url = {https://proceedings.neurips.cc/paper/2021/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{sanh_t0,
title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=9Vrb9D0WI4}
}
@misc{chung2022_scaling_instruction,
  doi = {10.48550/ARXIV.2210.11416},
  
  url = {https://arxiv.org/abs/2210.11416},
  
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Instruction-Finetuned Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}
@misc{yuan2023rrhf,
      title={RRHF: Rank Responses to Align Language Models with Human Feedback without tears}, 
      author={Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang},
      year={2023},
      eprint={2304.05302},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{junod2008fda,
  title={FDA and clinical drug trials: a short history},
  author={Junod, Suzanne},
  journal={FDLI Update},
  pages={55},
  year={2008},
  publisher={HeinOnline}
}

@misc{snell_ilql,
  doi = {10.48550/ARXIV.2206.11871},
  
  url = {https://arxiv.org/abs/2206.11871},
  
  author = {Snell, Charlie and Kostrikov, Ilya and Su, Yi and Yang, Mengjiao and Levine, Sergey},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Offline RL for Natural Language Generation with Implicit Language Q Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{zheng2023invariant,
  title={An Invariant Learning Characterization of Controlled Text Generation},
  author={Zheng, Carolina and Shi, Claudia and Vafa, Keyon and Feder, Amir and Blei, David M},
  journal={arXiv preprint arXiv:2306.00198},
  year={2023}
}

@inproceedings{jacovi2021formalizing,
  title={Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in AI},
  author={Jacovi, Alon and Marasovi{\'c}, Ana and Miller, Tim and Goldberg, Yoav},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={624--635},
  year={2021},
  url={https://arxiv.org/pdf/2010.07487.pdf}
}

@article{hase2023does,
  title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={arXiv preprint arXiv:2301.04213},
  year={2023},
  url={https://arxiv.org/pdf/2301.04213.pdf}
}

@article{geiger2023causal,
  title={Causal Abstraction for Faithful Model Interpretation},
  author={Geiger, Atticus and Potts, Chris and Icard, Thomas},
  journal={arXiv preprint arXiv:2301.04709},
  year={2023},
  url={https://arxiv.org/pdf/2301.04709.pdf}
}

@article{cohen2020curiosity,
  title={Curiosity killed the cat and the asymptotically optimal agent},
  author={Cohen, Michael K and Hutter, Marcus},
  journal={arXiv preprint arXiv:2006.03357},
  year={2020}
}

@article{srinivasan2020learning,
  title={Learning to be safe: Deep rl with a safety critic},
  author={Srinivasan, Krishnan and Eysenbach, Benjamin and Ha, Sehoon and Tan, Jie and Finn, Chelsea},
  journal={arXiv preprint arXiv:2010.14603},
  year={2020}
}

@article{jansen2018safe,
  title={Safe reinforcement learning via probabilistic shields},
  author={Jansen, Nils and K{\"o}nighofer, Bettina and Junges, Sebastian and Serban, Alexandru C and Bloem, Roderick},
  journal={arXiv preprint arXiv:1807.06096},
  year={2018}
}

@article{hadfield2017inverse,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{rastogi2023supporting,
  title={Supporting Human-AI Collaboration in Auditing LLMs with LLMs},
  author={Rastogi, Charvi and Ribeiro, Marco Tulio and King, Nicholas and Amershi, Saleema},
  journal={arXiv preprint arXiv:2304.09991},
  year={2023},
  url={https://arxiv.org/pdf/2304.09991.pdf}
}

@article{brief2020ai,
  title={Why AI Chips Matter},
  author={Brief, CSET Policy},
  year={2020}
}

@article{chan2023reclaiming,
  title={Reclaiming the Digital Commons: A Public Data Trust for Training Data},
  author={Chan, Alan and Bradley, Herbie and Rajkumar, Nitarshan},
  journal={arXiv preprint arXiv:2303.09001},
  year={2023}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@article{larsson2020transparency,
  title={Transparency in artificial intelligence},
  author={Larsson, Stefan and Heintz, Fredrik},
  journal={Internet Policy Review},
  volume={9},
  number={2},
  year={2020}
}

@article{dafoe2018ai,
  title={AI governance: a research agenda},
  author={Dafoe, Allan},
  journal={Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK},
  volume={1442},
  pages={1443},
  year={2018}
}

@article{casper2023explore,
  title={Explore, Establish, Exploit: Red Teaming Language Models from Scratch},
  author={Casper, Stephen and Lin, Jason and Kwon, Joe and Culp, Gatlen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2306.09442},
  year={2023}
}

@book{shapin2011leviathan,
  title={Leviathan and the air-pump: Hobbes, Boyle, and the experimental life},
  author={Shapin, Steven and Schaffer, Simon},
  year={2011},
  publisher={Princeton University Press}
}

@article{gilbert2021subjectifying,
  title={Subjectifying objectivity: Delineating tastes in theoretical quantum gravity research},
  author={Gilbert, Thomas Krendl and Loveridge, Andrew},
  journal={Social Studies of Science},
  volume={51},
  number={1},
  pages={73--99},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@inproceedings{yue2023clare,
  title={CLARE: Conservative Model-Based Reward Learning for Offline Inverse Reinforcement Learning},
  author={Yue, Sheng and Wang, Guanbo and Shao, Wei and Zhang, Zhaofeng and Lin, Sen and Ren, Ju and Zhang, Junshan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}
@inproceedings{liangreward,
  title={Reward Uncertainty for Exploration in Preference-based Reinforcement Learning},
  author={Liang, Xinran and Shu, Katherine and Lee, Kimin and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
@article{gleave2022uncertainty,
  title={Uncertainty estimation for language reward models},
  author={Gleave, Adam and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2203.07472},
  year={2022}
}
@article{rame2023rewarded,
  title={Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards},
  author={Rame, Alexandre and Couairon, Guillaume and Shukor, Mustafa and Dancette, Corentin and Gaya, Jean-Baptiste and Soulier, Laure and Cord, Matthieu},
  journal={arXiv preprint arXiv:2306.04488},
  year={2023}
}

@article{omar2013machine,
  title={Machine learning techniques for anomaly detection: an overview},
  author={Omar, Salima and Ngadi, Asri and Jebur, Hamid H},
  journal={International Journal of Computer Applications},
  volume={79},
  number={2},
  year={2013},
  publisher={Citeseer}
}

@misc{li2023inferencetime,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2023},
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{skalse2023invariance,
  title={Invariance in policy optimisation and partial identifiability in reward learning},
  author={Skalse, Joar Max Viktor and Farrugia-Roberts, Matthew and Russell, Stuart and Abate, Alessandro and Gleave, Adam},
  booktitle={International Conference on Machine Learning},
  pages={32033--32058},
  year={2023},
  organization={PMLR}
}
