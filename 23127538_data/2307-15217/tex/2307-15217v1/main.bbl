\begin{thebibliography}{259}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Aky{\"u}rek, Madaan, Kalyan,
  Clark, Wijaya, and Tandon]{akyurek2023rl4f}
Afra~Feyza Aky{\"u}rek, Ekin Aky{\"u}rek, Aman Madaan, Ashwin Kalyan, Peter
  Clark, Derry Wijaya, and Niket Tandon.
\newblock Rl4f: Generating natural language feedback with reinforcement
  learning for repairing model outputs.
\newblock \emph{arXiv preprint arXiv:2305.08844}, 2023.

\bibitem[Albert(2023)]{jailbreakChat2023}
Alex Albert.
\newblock Jailbreak chat.
\newblock 2023.
\newblock URL \url{https://www.jailbreakchat.com/}.

\bibitem[Amin et~al.(2021)Amin, Gomrokchi, Satija, van Hoof, and
  Precup]{amin2021survey}
Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup.
\newblock A survey of exploration methods in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.00157}, 2021.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Anderljung et~al.(2023)Anderljung, Barnhart, Leung, Korinek, O'Keefe,
  Whittlestone, Avin, Brundage, Bullock, Cass-Beggs, Chang, Collins, Fist,
  Hadfield, Hayes, Ho, Hooker, Horvitz, Kolt, Schuett, Shavit, Siddarth,
  Trager, and Wolf]{anderljung2023frontier}
Markus Anderljung, Joslyn Barnhart, Jade Leung, Anton Korinek, Cullen O'Keefe,
  Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan
  Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan
  Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav
  Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf.
\newblock Frontier ai regulation: Managing emerging risks to public safety,
  2023.

\bibitem[Anthropic(2023)]{anthropic2023}
Anthropic.
\newblock Introducing claude, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[ARC(2022)]{arcevals}
ARC.
\newblock Arc evals, 2022.
\newblock URL \url{https://evals.alignment.org/}.

\bibitem[Arumugam et~al.(2019)Arumugam, Lee, Saskin, and
  Littman]{arumugam2019deep}
Dilip Arumugam, Jun~Ki Lee, Sophie Saskin, and Michael~L Littman.
\newblock Deep reinforcement learning from policy-dependent human feedback.
\newblock \emph{arXiv preprint arXiv:1902.04257}, 2019.

\bibitem[Bai(2023)]{bai_artificial_2023}
Hui Bai.
\newblock Artificial {Intelligence} {Can} {Persuade} {Humans}.
\newblock 2023.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bajcsy et~al.(2018)Bajcsy, Losey, O'Malley, and
  Dragan]{bajcsy2018learning}
Andrea Bajcsy, Dylan~P Losey, Marcia~K O'Malley, and Anca~D Dragan.
\newblock Learning from physical human corrections, one feature at a time.
\newblock In \emph{Proceedings of the 2018 ACM/IEEE International Conference on
  Human-Robot Interaction}, pages 141--149, 2018.

\bibitem[Bakker et~al.(2022)Bakker, Chadwick, Sheahan, Tessler,
  Campbell-Gillingham, Balaguer, McAleese, Glaese, Aslanides, Botvinick,
  et~al.]{bakker2022fine}
Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy
  Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John
  Aslanides, Matt Botvinick, et~al.
\newblock Fine-tuning language models to find agreement among humans with
  diverse preferences.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 38176--38189, 2022.

\bibitem[Barnett et~al.(2023)Barnett, Freedman, Svegliato, and
  Russell]{barnett2023active}
Peter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell.
\newblock Active reward learning from multiple teachers.
\newblock \emph{arXiv preprint arXiv:2303.00894}, 2023.

\bibitem[Baumler et~al.(2023)Baumler, Sotnikova, and
  Daum{\'e}~III]{baumler2023examples}
Connor Baumler, Anna Sotnikova, and Hal Daum{\'e}~III.
\newblock Which examples should be multiply annotated? active learning when
  annotators may disagree.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 10352--10371, 2023.

\bibitem[Bennett et~al.(2007)Bennett, Lanning, et~al.]{bennett2007netflix}
James Bennett, Stan Lanning, et~al.
\newblock The netflix prize.
\newblock In \emph{Proceedings of KDD cup and workshop}, volume 2007, page~35.
  New York, 2007.

\bibitem[Bhatia et~al.(2020)Bhatia, Pananjady, Bartlett, Dragan, and
  Wainwright]{bhatia2020preference}
Kush Bhatia, Ashwin Pananjady, Peter Bartlett, Anca Dragan, and Martin~J
  Wainwright.
\newblock Preference learning along multiple criteria: A game-theoretic
  perspective.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 7413--7424, 2020.

\bibitem[Biyik(2022)]{biyik2022dissertation}
Erdem Biyik.
\newblock \emph{Learning Preferences For Interactive Autonomy}.
\newblock PhD thesis, EE Department, Stanford University, 2022.

\bibitem[Biyik and Sadigh(2018)]{biyik2018batch}
Erdem Biyik and Dorsa Sadigh.
\newblock Batch active preference-based learning of reward functions.
\newblock In \emph{Conference on robot learning}, pages 519--528. PMLR, 2018.

\bibitem[Biyik et~al.(2019)Biyik, Palan, Landolfi, Losey, and
  Sadigh]{biyik2019asking}
Erdem Biyik, Malayandi Palan, Nicholas~C. Landolfi, Dylan~P. Losey, and Dorsa
  Sadigh.
\newblock Asking easy questions: A user-friendly approach to active reward
  learning.
\newblock In \emph{Proceedings of the 3rd Conference on Robot Learning (CoRL)},
  2019.

\bibitem[Biyik et~al.(2020)Biyik, Huynh, Kochenderfer, and
  Sadigh]{biyik2020active}
Erdem Biyik, Nicolas Huynh, Mykel~J. Kochenderfer, and Dorsa Sadigh.
\newblock Active preference-based gaussian process regression for reward
  learning.
\newblock In \emph{Proceedings of Robotics: Science and Systems (RSS)}, July
  2020.
\newblock \doi{10.15607/rss.2020.xvi.041}.

\bibitem[B{\i}y{\i}k et~al.(2022)B{\i}y{\i}k, Losey, Palan, Landolfi, Shevchuk,
  and Sadigh]{biyik2022learning}
Erdem B{\i}y{\i}k, Dylan~P Losey, Malayandi Palan, Nicholas~C Landolfi, Gleb
  Shevchuk, and Dorsa Sadigh.
\newblock Learning reward functions from diverse sources of human feedback:
  Optimally integrating demonstrations and preferences.
\newblock \emph{The International Journal of Robotics Research}, 41\penalty0
  (1):\penalty0 45--67, 2022.

\bibitem[Bobu et~al.(2020)Bobu, Bajcsy, Fisac, Deglurkar, and
  Dragan]{bobu2020quantifying}
Andreea Bobu, Andrea Bajcsy, Jaime~F Fisac, Sampada Deglurkar, and Anca~D
  Dragan.
\newblock Quantifying hypothesis space misspecification in learning from
  human--robot demonstrations and physical corrections.
\newblock \emph{IEEE Transactions on Robotics}, 36\penalty0 (3):\penalty0
  835--854, 2020.

\bibitem[Bobu et~al.(2023)Bobu, Peng, Agrawal, Shah, and
  Dragan]{bobu2023aligning}
Andreea Bobu, Andi Peng, Pulkit Agrawal, Julie Shah, and Anca~D Dragan.
\newblock Aligning robot and human representations.
\newblock \emph{arXiv preprint arXiv:2302.01928}, 2023.

\bibitem[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner,
  Lukošiūtė, Askell, Jones, Chen, Goldie, Mirhoseini, McKinnon, Olah,
  Amodei, Amodei, Drain, Li, Tran-Johnson, Kernion, Kerr, Mueller, Ladish,
  Landau, Ndousse, Lovitt, Elhage, Schiefer, Joseph, Mercado, DasSarma, Larson,
  McCandlish, Kundu, Johnston, Kravec, Showk, Fort, Telleen-Lawton, Brown,
  Henighan, Hume, Bai, Hatfield-Dodds, Mann, and Kaplan]{bowman_measuring_2022}
Samuel~R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott
  Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, Anna
  Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela
  Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson
  Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal
  Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph,
  Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu,
  Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Timothy
  Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac
  Hatfield-Dodds, Ben Mann, and Jared Kaplan.
\newblock Measuring {Progress} on {Scalable} {Oversight} for {Large} {Language}
  {Models}, November 2022.
\newblock URL \url{http://arxiv.org/abs/2211.03540}.
\newblock arXiv:2211.03540 [cs].

\bibitem[Brief(2020)]{brief2020ai}
CSET~Policy Brief.
\newblock Why ai chips matter.
\newblock 2020.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and
  Niekum]{brown2019extrapolating}
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{International conference on machine learning}, pages
  783--792. PMLR, 2019.

\bibitem[Brown et~al.(2020)Brown, Coleman, Srinivasan, and
  Niekum]{brown2020safe}
Daniel Brown, Russell Coleman, Ravi Srinivasan, and Scott Niekum.
\newblock Safe imitation learning via fast bayesian reward inference from
  preferences.
\newblock In \emph{International Conference on Machine Learning}, pages
  1165--1177. PMLR, 2020.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  Zolna, Aytar, Budden, Vecerik, et~al.]{cabi2019scaling}
Serkan Cabi, Sergio~G{\'o}mez Colmenarejo, Alexander Novikov, Ksenia
  Konyushkova, Scott Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden,
  Mel Vecerik, et~al.
\newblock Scaling data-driven robotics with reward sketching and batch
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1909.12200}, 2019.

\bibitem[Carlini et~al.(2023{\natexlab{a}})Carlini, Jagielski, Choquette-Choo,
  Paleka, Pearce, Anderson, Terzis, Thomas, and
  Tram{\`e}r]{carlini2023poisoning}
Nicholas Carlini, Matthew Jagielski, Christopher~A Choquette-Choo, Daniel
  Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian
  Tram{\`e}r.
\newblock Poisoning web-scale training datasets is practical.
\newblock \emph{arXiv preprint arXiv:2302.10149}, 2023{\natexlab{a}}.

\bibitem[Carlini et~al.(2023{\natexlab{b}})Carlini, Nasr, Choquette-Choo,
  Jagielski, Gao, Awadalla, Koh, Ippolito, Lee, Tramer,
  et~al.]{carlini2023aligned}
Nicholas Carlini, Milad Nasr, Christopher~A Choquette-Choo, Matthew Jagielski,
  Irena Gao, Anas Awadalla, Pang~Wei Koh, Daphne Ippolito, Katherine Lee,
  Florian Tramer, et~al.
\newblock Are aligned neural networks adversarially aligned?
\newblock \emph{arXiv preprint arXiv:2306.15447}, 2023{\natexlab{b}}.

\bibitem[Carroll et~al.(2023)Carroll, Chan, Ashton, and
  Krueger]{carroll_characterizing_2023}
Micah Carroll, Alan Chan, Henry Ashton, and David Krueger.
\newblock Characterizing {Manipulation} from {AI} {Systems}, March 2023.
\newblock URL \url{http://arxiv.org/abs/2303.09387}.
\newblock arXiv:2303.09387 [cs].

\bibitem[Carroll et~al.(2022)Carroll, Dragan, Russell, and
  Hadfield-Menell]{carroll22a}
Micah~D Carroll, Anca Dragan, Stuart Russell, and Dylan Hadfield-Menell.
\newblock Estimating and penalizing induced preference shifts in recommender
  systems.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, 2022.

\bibitem[Casper(2020)]{casper2020achilles}
Stephen Casper.
\newblock Achilles heels for agi/asi via decision theoretic adversaries.
\newblock \emph{arXiv preprint arXiv:2010.05418}, 2020.

\bibitem[Casper et~al.(2022)Casper, Hadfield-Menell, and
  Kreiman]{casper2022white}
Stephen Casper, Dylan Hadfield-Menell, and Gabriel Kreiman.
\newblock White-box adversarial policies in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2209.02167}, 2022.

\bibitem[Casper et~al.(2023{\natexlab{a}})Casper, Li, Li, Bu, Zhang, and
  Hadfield-Menell]{casper2023benchmarking}
Stephen Casper, Yuxiao Li, Jiawei Li, Tong Bu, Kevin Zhang, and Dylan
  Hadfield-Menell.
\newblock Benchmarking interpretability tools for deep neural networks.
\newblock \emph{arXiv preprint arXiv:2302.10894}, 2023{\natexlab{a}}.

\bibitem[Casper et~al.(2023{\natexlab{b}})Casper, Lin, Kwon, Culp, and
  Hadfield-Menell]{casper2023explore}
Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell.
\newblock Explore, establish, exploit: Red teaming language models from
  scratch.
\newblock \emph{arXiv preprint arXiv:2306.09442}, 2023{\natexlab{b}}.

\bibitem[Casper et~al.(2023{\natexlab{c}})Casper, Nadeau, Hadfield-Menell, and
  Kreiman]{casper2023robust}
Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, and Gabriel Kreiman.
\newblock Robust feature-level adversaries are interpretability tools,
  2023{\natexlab{c}}.

\bibitem[Chambers and Echenique(2016)]{chambers2016revealed}
Christopher~P Chambers and Federico Echenique.
\newblock \emph{Revealed preference theory}, volume~56.
\newblock Cambridge University Press, 2016.

\bibitem[Chan et~al.(2023{\natexlab{a}})Chan, Bradley, and
  Rajkumar]{chan2023reclaiming}
Alan Chan, Herbie Bradley, and Nitarshan Rajkumar.
\newblock Reclaiming the digital commons: A public data trust for training
  data.
\newblock \emph{arXiv preprint arXiv:2303.09001}, 2023{\natexlab{a}}.

\bibitem[Chan et~al.(2023{\natexlab{b}})Chan, Salganik, Markelius, Pang,
  Rajkumar, Krasheninnikov, di~Langosco, He, Duan, Carroll, Lin, Mayhew,
  Collins, Molamohammadi, Burden, Zhao, Rismani, Voudouris, Bhatt, Weller,
  Krueger, and Maharaj]{Chan2023HarmsFI}
Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar,
  Dmitrii Krasheninnikov, Lauro~Langosco di~Langosco, Zhonghao He, Yawen Duan,
  Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam
  Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos
  Voudouris, Umang Bhatt, Adrian Weller, David Krueger, and Tegan Maharaj.
\newblock Harms from increasingly agentic algorithmic systems.
\newblock \emph{ArXiv}, abs/2302.10329, 2023{\natexlab{b}}.

\bibitem[Chan et~al.(2019)Chan, Hadfield-Menell, Srinivasa, and
  Dragan]{chan_assistive_2019}
Lawrence Chan, Dylan Hadfield-Menell, Siddhartha Srinivasa, and Anca Dragan.
\newblock The {Assistive} {Multi}-{Armed} {Bandit}.
\newblock \emph{arXiv:1901.08654 [cs, stat]}, January 2019.
\newblock URL \url{http://arxiv.org/abs/1901.08654}.
\newblock arXiv: 1901.08654.

\bibitem[Chen et~al.(2023)Chen, Scheurer, Korbak, Campos, Chan, Bowman, Cho,
  and Perez]{chen2023improving}
Angelica Chen, J{\'e}r{\'e}my Scheurer, Tomasz Korbak, Jon~Ander Campos,
  Jun~Shern Chan, Samuel~R Bowman, Kyunghyun Cho, and Ethan Perez.
\newblock Improving code generation by training with natural language feedback.
\newblock \emph{arXiv preprint arXiv:2303.16749}, 2023.

\bibitem[Chmielewski and Kucker(2020)]{chmielewski2020mturk}
Michael Chmielewski and Sarah~C Kucker.
\newblock An mturk crisis? shifts in data quality and the impact on study
  results.
\newblock \emph{Social Psychological and Personality Science}, 11\penalty0
  (4):\penalty0 464--473, 2020.

\bibitem[Christiano(2019)]{christiano2019worst}
Paul Christiano.
\newblock Worst-case guarantees.
\newblock
  \url{https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d},
  2019.

\bibitem[Christiano(2023)]{christiano2023thoughts}
Paul Christiano.
\newblock Thoughts on the impact of rlhf research, Jan 2023.
\newblock URL
  \url{https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research#The_case_for_a_positive_impact:~:text=I\%20think\%20it\%20is\%20hard\%20to\%20productively\%20work\%20on\%20more\%20challenging\%20alignment\%20problems\%20without\%20first\%20implementing\%20basic\%20solutions.}

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Castro-Ros,
  Pellat, Robinson, Valter, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov,
  Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{chung2022_scaling_instruction}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter,
  Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew
  Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam
  Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.11416}.

\bibitem[Cihon(2019)]{cihon2019standards}
Peter Cihon.
\newblock Standards for ai governance: international standards to enable global
  coordination in ai research \& development.
\newblock \emph{Future of Humanity Institute. University of Oxford}, 2019.

\bibitem[Cohen and Hutter(2020)]{cohen2020curiosity}
Michael~K Cohen and Marcus Hutter.
\newblock Curiosity killed the cat and the asymptotically optimal agent.
\newblock \emph{arXiv preprint arXiv:2006.03357}, 2020.

\bibitem[Cotra(2021)]{cotra2021cold}
Ajeya Cotra.
\newblock Why ai alignment could be hard with modern deep learning.
\newblock
  \url{https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/},
  2021.

\bibitem[Critch and Krueger(2020)]{critch2020ai}
Andrew Critch and David Krueger.
\newblock Ai research considerations for human existential safety (arches).
\newblock \emph{arXiv preprint arXiv:2006.04948}, 2020.

\bibitem[Cui et~al.(2022)Cui, Jahanian, Lapedriza, Torralba, Mahdizadehaghdam,
  Kumar, and Bau]{cui2022local}
Audrey Cui, Ali Jahanian, Agata Lapedriza, Antonio Torralba, Shahin
  Mahdizadehaghdam, Rohit Kumar, and David Bau.
\newblock Local relighting of real scenes, 2022.

\bibitem[Dafoe(2018)]{dafoe2018ai}
Allan Dafoe.
\newblock Ai governance: a research agenda.
\newblock \emph{Governance of AI Program, Future of Humanity Institute,
  University of Oxford: Oxford, UK}, 1442:\penalty0 1443, 2018.

\bibitem[Daniels-Koch and Freedman(2022)]{daniels2022expertise}
Oliver Daniels-Koch and Rachel Freedman.
\newblock The expertise problem: Learning from specialized feedback.
\newblock \emph{arXiv preprint arXiv:2211.06519}, 2022.

\bibitem[Davani et~al.(2022)Davani, D{\'\i}az, and
  Prabhakaran]{davani2022dealing}
Aida~Mostafazadeh Davani, Mark D{\'\i}az, and Vinodkumar Prabhakaran.
\newblock Dealing with disagreements: Looking beyond the majority vote in
  subjective annotations.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 92--110, 2022.

\bibitem[Di~Langosco et~al.(2022)Di~Langosco, Koch, Sharkey, Pfau, and
  Krueger]{di2022goal}
Lauro~Langosco Di~Langosco, Jack Koch, Lee~D Sharkey, Jacob Pfau, and David
  Krueger.
\newblock Goal misgeneralization in deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  12004--12019. PMLR, 2022.

\bibitem[Ding and Dong(2020)]{ding2020challenges}
Zihan Ding and Hao Dong.
\newblock Challenges of reinforcement learning.
\newblock \emph{Deep Reinforcement Learning: Fundamentals, Research and
  Applications}, pages 249--272, 2020.

\bibitem[Dobbe et~al.(2021)Dobbe, Gilbert, and Mintz]{dobbe2021hard}
Roel Dobbe, Thomas~Krendl Gilbert, and Yonatan Mintz.
\newblock Hard choices in artificial intelligence.
\newblock \emph{Artificial Intelligence}, 300:\penalty0 103555, 2021.

\bibitem[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and
  Mordatch]{du2023improving}
Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch.
\newblock Improving factuality and reasoning in language models through
  multiagent debate.
\newblock \emph{arXiv preprint arXiv:2305.14325}, 2023.

\bibitem[El-Mhamdi et~al.(2022)El-Mhamdi, Farhadkhani, Guerraoui, Gupta, Hoang,
  Pinot, and Stephan]{el2022sok}
El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta,
  L{\^e}-Nguy{\^e}n Hoang, Rafael Pinot, and John Stephan.
\newblock Sok: On the impossible security of very large foundation models.
\newblock \emph{arXiv preprint arXiv:2209.15259}, 2022.

\bibitem[Eloundou et~al.(2023)Eloundou, Manning, Mishkin, and
  Rock]{eloundou2023gpts}
Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock.
\newblock Gpts are gpts: An early look at the labor market impact potential of
  large language models, 2023.

\bibitem[Ethayarajh and Jurafsky(2022)]{ethayarajh2022authenticity}
Kawin Ethayarajh and Dan Jurafsky.
\newblock The authenticity gap in human evaluation, 2022.

\bibitem[Everitt et~al.(2021)Everitt, Hutter, Kumar, and
  Krakovna]{everitt_reward_2021}
Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna.
\newblock Reward {Tampering} {Problems} and {Solutions} in {Reinforcement}
  {Learning}: {A} {Causal} {Influence} {Diagram} {Perspective}.
\newblock \emph{arXiv:1908.04734 [cs]}, March 2021.
\newblock URL \url{http://arxiv.org/abs/1908.04734}.
\newblock arXiv: 1908.04734.

\bibitem[Falco et~al.(2021)Falco, Shneiderman, Badger, Carrier, Dahbura, Danks,
  Eling, Goodloe, Gupta, Hart, et~al.]{falco2021governing}
Gregory Falco, Ben Shneiderman, Julia Badger, Ryan Carrier, Anton Dahbura,
  David Danks, Martin Eling, Alwyn Goodloe, Jerry Gupta, Christopher Hart,
  et~al.
\newblock Governing ai safety through independent audits.
\newblock \emph{Nature Machine Intelligence}, 3\penalty0 (7):\penalty0
  566--571, 2021.

\bibitem[Feffer et~al.(2023)Feffer, Heidari, and Lipton]{feffer2023moral}
Michael Feffer, Hoda Heidari, and Zachary~C Lipton.
\newblock Moral machine or tyranny of the majority?
\newblock \emph{arXiv preprint arXiv:2305.17319}, 2023.

\bibitem[Floridi and Cowls(2022)]{floridi2022unified}
Luciano Floridi and Josh Cowls.
\newblock A unified framework of five principles for ai in society.
\newblock \emph{Machine learning and the city: Applications in architecture and
  urban design}, pages 535--545, 2022.

\bibitem[Freedman et~al.(2021)Freedman, Shah, and Dragan]{freedman2021choice}
Rachel Freedman, Rohin Shah, and Anca Dragan.
\newblock Choice set misspecification in reward inference.
\newblock \emph{arXiv preprint arXiv:2101.07691}, 2021.

\bibitem[French(2019)]{french2019mandela}
Aaron French.
\newblock The mandela effect and new memory.
\newblock \emph{Correspondences}, 6\penalty0 (2), 2019.

\bibitem[Fu et~al.(2019)Fu, Korattikara, Levine, and
  Guadarrama]{fu2019language}
Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama.
\newblock From language to goals: Inverse reinforcement learning for
  vision-based instruction following.
\newblock \emph{arXiv preprint arXiv:1902.07742}, 2019.

\bibitem[Gao et~al.(2022)Gao, Schulman, and Hilton]{gao2022scaling}
Leo Gao, John Schulman, and Jacob Hilton.
\newblock Scaling laws for reward model overoptimization.
\newblock \emph{arXiv preprint arXiv:2210.10760}, 2022.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman-etal-2020-realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {R}eal{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3356--3369, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.301}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.301}.

\bibitem[Geiger et~al.(2023)Geiger, Potts, and Icard]{geiger2023causal}
Atticus Geiger, Chris Potts, and Thomas Icard.
\newblock Causal abstraction for faithful model interpretation.
\newblock \emph{arXiv preprint arXiv:2301.04709}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2301.04709.pdf}.

\bibitem[Gilardi et~al.(2023)Gilardi, Alizadeh, and Kubli]{gilardi2023chatgpt}
Fabrizio Gilardi, Meysam Alizadeh, and Ma{\"e}l Kubli.
\newblock Chatgpt outperforms crowd-workers for text-annotation tasks.
\newblock \emph{arXiv preprint arXiv:2303.15056}, 2023.

\bibitem[Gilbert and Loveridge(2021)]{gilbert2021subjectifying}
Thomas~Krendl Gilbert and Andrew Loveridge.
\newblock Subjectifying objectivity: Delineating tastes in theoretical quantum
  gravity research.
\newblock \emph{Social Studies of Science}, 51\penalty0 (1):\penalty0 73--99,
  2021.

\bibitem[Gilbert et~al.(2022)Gilbert, Lambert, Dean, Zick, and
  Snoswell]{gilbert2022reward}
Thomas~Krendl Gilbert, Nathan Lambert, Sarah Dean, Tom Zick, and Aaron
  Snoswell.
\newblock Reward reports for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2204.10817}, 2022.

\bibitem[Glaese et~al.(2022)Glaese, McAleese, Trębacz, Aslanides, Firoiu,
  Ewalds, Rauh, Weidinger, Chadwick, Thacker, Campbell-Gillingham, Uesato,
  Huang, Comanescu, Yang, See, Dathathri, Greig, Chen, Fritz, Elias, Green,
  Mokrá, Fernando, Wu, Foley, Young, Gabriel, Isaac, Mellor, Hassabis,
  Kavukcuoglu, Hendricks, and Irving]{glaese2022improving}
Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo
  Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy
  Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan
  Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz,
  Jaume~Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu,
  Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor,
  Demis Hassabis, Koray Kavukcuoglu, Lisa~Anne Hendricks, and Geoffrey Irving.
\newblock Improving alignment of dialogue agents via targeted human judgements,
  2022.

\bibitem[Gleave and Irving(2022)]{gleave2022uncertainty}
Adam Gleave and Geoffrey Irving.
\newblock Uncertainty estimation for language reward models.
\newblock \emph{arXiv preprint arXiv:2203.07472}, 2022.

\bibitem[Gleave et~al.(2020{\natexlab{a}})Gleave, Dennis, Legg, Russell, and
  Leike]{epic}
Adam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike.
\newblock Quantifying differences in reward functions.
\newblock \emph{arXiv preprint arXiv:2006.13900}, 2020{\natexlab{a}}.

\bibitem[Gleave et~al.(2020{\natexlab{b}})Gleave, Dennis, Wild, Kant, Levine,
  and Russell]{Gleave2020Adversarial}
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart
  Russell.
\newblock Adversarial policies: Attacking deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=HJgEMpVFwB}.

\bibitem[Go et~al.(2023)Go, Korbak, Kruszewski, Rozen, Ryu, and
  Dymetman]{go2023aligning}
Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and
  Marc Dymetman.
\newblock Aligning language models with preferences through f-divergence
  minimization, 2023.

\bibitem[Google(2023)]{google2023}
Google.
\newblock Bard, 2023.
\newblock URL \url{https://bard.google.com/}.

\bibitem[Gordon et~al.(2021)Gordon, Zhou, Patel, Hashimoto, and
  Bernstein]{gordon2021disagreement}
Mitchell~L Gordon, Kaitlyn Zhou, Kayur Patel, Tatsunori Hashimoto, and
  Michael~S Bernstein.
\newblock The disagreement deconvolution: Bringing machine learning performance
  metrics in line with reality.
\newblock In \emph{Proceedings of the 2021 CHI Conference on Human Factors in
  Computing Systems}, pages 1--14, 2021.

\bibitem[Gordon et~al.(2022)Gordon, Lam, Park, Patel, Hancock, Hashimoto, and
  Bernstein]{gordon2022jury}
Mitchell~L Gordon, Michelle~S Lam, Joon~Sung Park, Kayur Patel, Jeff Hancock,
  Tatsunori Hashimoto, and Michael~S Bernstein.
\newblock Jury learning: Integrating dissenting voices into machine learning
  models.
\newblock In \emph{Proceedings of the 2022 CHI Conference on Human Factors in
  Computing Systems}, pages 1--19, 2022.

\bibitem[Goyal et~al.(2019)Goyal, Niekum, and Mooney]{goyal2019using}
Prasoon Goyal, Scott Niekum, and Raymond~J Mooney.
\newblock Using natural language for reward shaping in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.02020}, 2019.

\bibitem[Griffin et~al.(2023)Griffin, Kleinberg, Mozes, Mai, Vau, Caldwell, and
  Marvor-Parker]{griffin_susceptibility_2023}
Lewis~D. Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly~T. Mai, Maria
  Vau, Matthew Caldwell, and Augustine Marvor-Parker.
\newblock Susceptibility to {Influence} of {Large} {Language} {Models}, March
  2023.
\newblock URL \url{http://arxiv.org/abs/2303.06074}.
\newblock arXiv:2303.06074 [cs].

\bibitem[Guerdan et~al.(2023)Guerdan, Coston, Wu, and
  Holstein]{guerdan2023ground}
Luke Guerdan, Amanda Coston, Zhiwei~Steven Wu, and Kenneth Holstein.
\newblock Ground (less) truth: A causal framework for proxy labels in
  human-algorithm decision-making.
\newblock \emph{arXiv preprint arXiv:2302.06503}, 2023.

\bibitem[Hadfield and Clark(2023)]{hadfield2023regulatory}
Gillian~K Hadfield and Jack Clark.
\newblock Regulatory markets: The future of ai governance.
\newblock \emph{arXiv preprint arXiv:2304.04914}, 2023.

\bibitem[Hadfield-Menell et~al.(2016)Hadfield-Menell, Russell, Abbeel, and
  Dragan]{hadfield2016cooperative}
Dylan Hadfield-Menell, Stuart~J Russell, Pieter Abbeel, and Anca Dragan.
\newblock Cooperative inverse reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Hadfield-Menell et~al.(2017)Hadfield-Menell, Milli, Abbeel, Russell,
  and Dragan]{hadfield2017inverse}
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart~J Russell, and Anca
  Dragan.
\newblock Inverse reward design.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hao(2023)]{hao2023hidden}
Karen Hao.
\newblock The hidden workforce that helped filter violence and abuse out of
  chatgpt, 2023.
\newblock URL
  \url{https://www.wsj.com/podcasts/the-journal/the-hidden-workforce-that-helped-filter-violence-and-abuse-out-of-chatgpt/ffc2427f-bdd8-47b7-9a4b-27e7267cf413}.

\bibitem[Hartmann et~al.(2023)Hartmann, Schwenzow, and
  Witte]{hartmann2023political}
Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte.
\newblock The political ideology of conversational ai: Converging evidence on
  chatgpt's pro-environmental, left-libertarian orientation.
\newblock \emph{arXiv preprint arXiv:2301.01768}, 2023.

\bibitem[Hase et~al.(2023)Hase, Bansal, Kim, and Ghandeharioun]{hase2023does}
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.
\newblock Does localization inform editing? surprising differences in
  causality-based localization vs. knowledge editing in language models.
\newblock \emph{arXiv preprint arXiv:2301.04213}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2301.04213.pdf}.

\bibitem[Hejna and Sadigh(2022)]{hejna2022fewshot}
Joey Hejna and Dorsa Sadigh.
\newblock Few-shot preference learning for human-in-the-loop rl.
\newblock In \emph{Proceedings of the 6th Conference on Robot Learning (CoRL)},
  2022.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2018deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
  and David Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Carlini, Schulman, and
  Steinhardt]{hendrycks2021unsolved}
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt.
\newblock Unsolved problems in ml safety.
\newblock \emph{arXiv preprint arXiv:2109.13916}, 2021.

\bibitem[Hernandez et~al.(2023)Hernandez, Li, and
  Andreas]{hernandez2023measuring}
Evan Hernandez, Belinda~Z Li, and Jacob Andreas.
\newblock Measuring and manipulating knowledge representations in language
  models.
\newblock \emph{arXiv preprint arXiv:2304.00740}, 2023.

\bibitem[Hilton et~al.(2020)Hilton, Cammarata, Carter, Goh, and
  Olah]{hilton2020understanding}
Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah.
\newblock Understanding rl vision.
\newblock \emph{Distill}, 5\penalty0 (11):\penalty0 e29, 2020.

\bibitem[Hong et~al.(2022)Hong, Bhatia, and Dragan]{hong2022sensitivity}
Joey Hong, Kush Bhatia, and Anca Dragan.
\newblock On the sensitivity of reward inference to misspecified human models.
\newblock \emph{arXiv preprint arXiv:2212.04717}, 2022.

\bibitem[Hoskin(1996)]{hoskin1996awful}
Keith Hoskin.
\newblock The ‘awful idea of accountability’: inscribing people into the
  measurement of objects.
\newblock \emph{Accountability: Power, ethos and the technologies of managing},
  265, 1996.

\bibitem[Hubinger(2020)]{hubinger2020overview}
Evan Hubinger.
\newblock An overview of 11 proposals for building safe advanced ai.
\newblock \emph{arXiv preprint arXiv:2012.07532}, 2020.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018reward}
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario
  Amodei.
\newblock Reward learning from human preferences and demonstrations in atari.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Irpan(2018)]{rlblogpost}
Alex Irpan.
\newblock Deep reinforcement learning doesn't work yet.
\newblock \url{https://www.alexirpan.com/2018/02/14/rl-hard.html}, 2018.

\bibitem[Irving et~al.(2018)Irving, Christiano, and Amodei]{irving2018ai}
Geoffrey Irving, Paul Christiano, and Dario Amodei.
\newblock Ai safety via debate.
\newblock \emph{arXiv preprint arXiv:1805.00899}, 2018.

\bibitem[Jacovi et~al.(2021)Jacovi, Marasovi{\'c}, Miller, and
  Goldberg]{jacovi2021formalizing}
Alon Jacovi, Ana Marasovi{\'c}, Tim Miller, and Yoav Goldberg.
\newblock Formalizing trust in artificial intelligence: Prerequisites, causes
  and goals of human trust in ai.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pages 624--635, 2021.
\newblock URL \url{https://arxiv.org/pdf/2010.07487.pdf}.

\bibitem[Jansen et~al.(2018)Jansen, K{\"o}nighofer, Junges, Serban, and
  Bloem]{jansen2018safe}
Nils Jansen, Bettina K{\"o}nighofer, Sebastian Junges, Alexandru~C Serban, and
  Roderick Bloem.
\newblock Safe reinforcement learning via probabilistic shields.
\newblock \emph{arXiv preprint arXiv:1807.06096}, 2018.

\bibitem[Jeon et~al.(2020)Jeon, Milli, and Dragan]{jeon2020reward}
Hong~Jun Jeon, Smitha Milli, and Anca Dragan.
\newblock Reward-rational (implicit) choice: A unifying formalism for reward
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4415--4426, 2020.

\bibitem[Ji et~al.(2023)Ji, Lee, Frieske, Yu, Su, Xu, Ishii, Bang, Madotto, and
  Fung]{ji2023survey}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
  Ye~Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (12):\penalty0 1--38, 2023.

\bibitem[Junod(2008)]{junod2008fda}
Suzanne Junod.
\newblock Fda and clinical drug trials: a short history.
\newblock \emph{FDLI Update}, page~55, 2008.

\bibitem[Kenton et~al.(2021)Kenton, Everitt, Weidinger, Gabriel, Mikulik, and
  Irving]{kenton_alignment_2021}
Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik,
  and Geoffrey Irving.
\newblock Alignment of {Language} {Agents}, March 2021.
\newblock URL \url{http://arxiv.org/abs/2103.14659}.
\newblock arXiv:2103.14659 [cs].

\bibitem[Khalifa et~al.(2021)Khalifa, Elsahar, and Dymetman]{khalifa2021a}
Muhammad Khalifa, Hady Elsahar, and Marc Dymetman.
\newblock A distributional approach to controlled text generation.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=jWkw45-9AbL}.

\bibitem[Khlaaf(2023)]{khlaaf2023toward}
Heidy Khlaaf.
\newblock Toward comprehensive risk assessments and assurance of ai-based
  systems.
\newblock \emph{Trail of Bits}, 2023.

\bibitem[Kim et~al.(2023)Kim, Bae, Shin, Kang, Kwak, Yoo, and
  Seo]{kim2023aligning}
Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang~Min
  Yoo, and Minjoon Seo.
\newblock Aligning large language models through synthetic feedback.
\newblock \emph{arXiv preprint arXiv:2305.13735}, 2023.

\bibitem[Kirk et~al.(2023)Kirk, Vidgen, R{\"o}ttger, and
  Hale]{kirk2023personalisation}
Hannah~Rose Kirk, Bertie Vidgen, Paul R{\"o}ttger, and Scott~A Hale.
\newblock Personalisation within bounds: A risk taxonomy and policy framework
  for the alignment of large language models with personalised feedback.
\newblock \emph{arXiv preprint arXiv:2303.05453}, 2023.

\bibitem[Knox and Stone(2008)]{knox2008tamer}
W~Bradley Knox and Peter Stone.
\newblock Tamer: Training an agent manually via evaluative reinforcement.
\newblock In \emph{2008 7th IEEE international conference on development and
  learning}, pages 292--297. IEEE, 2008.

\bibitem[Knox and Stone(2009)]{knox2009interactively}
W~Bradley Knox and Peter Stone.
\newblock Interactively shaping agents via human reinforcement: The tamer
  framework.
\newblock In \emph{Proceedings of the fifth international conference on
  Knowledge capture}, pages 9--16, 2009.

\bibitem[Knox et~al.(2022)Knox, Hatgis-Kessell, Booth, Niekum, Stone, and
  Allievi]{knox2022models}
W~Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter
  Stone, and Alessandro Allievi.
\newblock Models of human preference for learning reward functions.
\newblock \emph{arXiv preprint arXiv:2206.02231}, 2022.

\bibitem[Korbak et~al.(2022{\natexlab{a}})Korbak, Elsahar, Kruszewski, and
  Dymetman]{korbak_2022}
Tomasz Korbak, Hady Elsahar, Germ\'{a}n Kruszewski, and Marc Dymetman.
\newblock On reinforcement learning and distribution matching for fine-tuning
  language models with no catastrophic forgetting.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 16203--16220. Curran Associates, Inc., 2022{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf}.

\bibitem[Korbak et~al.(2022{\natexlab{b}})Korbak, Perez, and
  Buckley]{korbak-etal-2022-rl}
Tomasz Korbak, Ethan Perez, and Christopher Buckley.
\newblock {RL} with {KL} penalties is better viewed as {B}ayesian inference.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 1083--1091, Abu Dhabi, United Arab Emirates, December
  2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.77}.

\bibitem[Korbak et~al.(2023)Korbak, Shi, Chen, Bhalerao, Buckley, Phang,
  Bowman, and Perez]{korbak2023pretraining}
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher~L.
  Buckley, Jason Phang, Samuel~R. Bowman, and Ethan Perez.
\newblock Pretraining language models with human preferences, 2023.

\bibitem[Kos and Song(2017)]{kos2017delving}
Jernej Kos and Dawn Song.
\newblock Delving into adversarial attacks on deep policies.
\newblock \emph{arXiv preprint arXiv:1705.06452}, 2017.

\bibitem[Krakovna and Kramar(2023)]{krakovna2023power}
Victoria Krakovna and Janos Kramar.
\newblock Power-seeking can be probable and predictive for trained agents.
\newblock \emph{arXiv preprint arXiv:2304.06528}, 2023.

\bibitem[Krakovna et~al.(2020)Krakovna, Uesato, Mikulik, Rahtz, Everitt, Kumar,
  Kenton, Leike, and Legg]{krakovna2020specification}
Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom
  Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg.
\newblock Specification gaming: the flip side of ai ingenuity.
\newblock \emph{DeepMind Blog}, 2020.

\bibitem[Krasheninnikov et~al.(2022)Krasheninnikov, Krasheninnikov, and
  Krueger]{krasheninnikov2022assistance}
Dmitrii Krasheninnikov, Egor Krasheninnikov, and David Krueger.
\newblock Assistance with large language models.
\newblock In \emph{NeurIPS ML Safety Workshop}, 2022.

\bibitem[Krueger et~al.(2020)Krueger, Maharaj, and Leike]{krueger2020hidden}
David Krueger, Tegan Maharaj, and Jan Leike.
\newblock Hidden incentives for auto-induced distributional shift, 2020.

\bibitem[Kumar et~al.(2021)Kumar, Kelley, Consolvo, Mason, Bursztein,
  Durumeric, Thomas, and Bailey]{kumar2021designing}
Deepak Kumar, Patrick~Gage Kelley, Sunny Consolvo, Joshua Mason, Elie
  Bursztein, Zakir Durumeric, Kurt Thomas, and Michael Bailey.
\newblock Designing toxic content classification for a diversity of
  perspectives.
\newblock In \emph{SOUPS@ USENIX Security Symposium}, pages 299--318, 2021.

\bibitem[Larsson and Heintz(2020)]{larsson2020transparency}
Stefan Larsson and Fredrik Heintz.
\newblock Transparency in artificial intelligence.
\newblock \emph{Internet Policy Review}, 9\penalty0 (2), 2020.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{lee2021pebble}
Kimin Lee, Laura Smith, and Pieter Abbeel.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock \emph{arXiv preprint arXiv:2106.05091}, 2021.

\bibitem[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and
  Legg]{leike2018scalable}
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane
  Legg.
\newblock Scalable agent alignment via reward modeling: a research direction.
\newblock \emph{arXiv preprint arXiv:1811.07871}, 2018.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Guo, Fan, Xu, and Song]{li2023multi}
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.
\newblock Multi-step jailbreaking privacy attacks on chatgpt.
\newblock \emph{arXiv preprint arXiv:2304.05197}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Patel, Viégas, Pfister, and
  Wattenberg]{li2023inferencetime}
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin
  Wattenberg.
\newblock Inference-time intervention: Eliciting truthful answers from a
  language model, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2021)Li, Canberk, Losey, and Sadigh]{li2021learning}
Mengxi Li, Alper Canberk, Dylan~P Losey, and Dorsa Sadigh.
\newblock Learning human objectives from sequences of physical corrections.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 2877--2883. IEEE, 2021.

\bibitem[Liang et~al.(2022{\natexlab{a}})Liang, Bommasani, Lee, Tsipras, Soylu,
  Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022{\natexlab{a}}.

\bibitem[Liang et~al.(2022{\natexlab{b}})Liang, Shu, Lee, and
  Abbeel]{liangreward}
Xinran Liang, Katherine Shu, Kimin Lee, and Pieter Abbeel.
\newblock Reward uncertainty for exploration in preference-based reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Lin et~al.(2022)Lin, Fried, Klein, and Dragan]{lin2022inferring}
Jessy Lin, Daniel Fried, Dan Klein, and Anca Dragan.
\newblock Inferring rewards from language in context.
\newblock \emph{arXiv preprint arXiv:2204.02515}, 2022.

\bibitem[Lindner and El-Assady(2022)]{lindner2022humans}
David Lindner and Mennatallah El-Assady.
\newblock Humans are not boltzmann distributions: Challenges and opportunities
  for modelling human feedback and interaction in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2206.13316}, 2022.

\bibitem[Lindner et~al.(2023)Lindner, Chen, Tschiatschek, Hofmann, and
  Krause]{lindner2023learning}
David Lindner, Xin Chen, Sebastian Tschiatschek, Katja Hofmann, and Andreas
  Krause.
\newblock Learning safety constraints from demonstrations with unknown rewards.
\newblock \emph{arXiv preprint arXiv:2305.16147}, 2023.

\bibitem[Liu et~al.(2023)Liu, Deng, Xu, Li, Zheng, Zhang, Zhao, Zhang, and
  Liu]{liu2023jailbreaking}
Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida
  Zhao, Tianwei Zhang, and Yang Liu.
\newblock Jailbreaking chatgpt via prompt engineering: An empirical study,
  2023.

\bibitem[Losey et~al.(2022)Losey, Bajcsy, O’Malley, and
  Dragan]{losey2022physical}
Dylan~P Losey, Andrea Bajcsy, Marcia~K O’Malley, and Anca~D Dragan.
\newblock Physical interaction as communication: Learning robot objectives
  online from human corrections.
\newblock \emph{The International Journal of Robotics Research}, 41\penalty0
  (1):\penalty0 20--44, 2022.

\bibitem[Luketina et~al.(2019)Luketina, Nardelli, Farquhar, Foerster, Andreas,
  Grefenstette, Whiteson, and Rockt{\"a}schel]{luketina2019survey}
Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob
  Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rockt{\"a}schel.
\newblock A survey of reinforcement learning informed by natural language.
\newblock \emph{arXiv preprint arXiv:1906.03926}, 2019.

\bibitem[MacGlashan et~al.(2017)MacGlashan, Ho, Loftin, Peng, Wang, Roberts,
  Taylor, and Littman]{macglashan2017interactive}
James MacGlashan, Mark~K Ho, Robert Loftin, Bei Peng, Guan Wang, David~L
  Roberts, Matthew~E Taylor, and Michael~L Littman.
\newblock Interactive learning from policy-dependent human feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  2285--2294. PMLR, 2017.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Malik et~al.(2021)Malik, Anwar, Aghasi, and Ahmed]{malik2021inverse}
Shehryar Malik, Usman Anwar, Alireza Aghasi, and Ali Ahmed.
\newblock Inverse constrained reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  7390--7399. PMLR, 2021.

\bibitem[Manheim and Garrabrant(2018)]{manheim2018categorizing}
David Manheim and Scott Garrabrant.
\newblock Categorizing variants of goodhart's law.
\newblock \emph{arXiv preprint arXiv:1803.04585}, 2018.

\bibitem[McKinney et~al.(2023)McKinney, Duan, Krueger, and
  Gleave]{mckinney2023fragility}
Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave.
\newblock On the fragility of learned reward functions.
\newblock \emph{arXiv preprint arXiv:2301.03652}, 2023.

\bibitem[Meng et~al.(2022)Meng, Sharma, Andonian, Belinkov, and
  Bau]{meng2022mass}
Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer.
\newblock \emph{arXiv preprint arXiv:2210.07229}, 2022.

\bibitem[Meng et~al.(2023)Meng, Bau, Andonian, and Belinkov]{meng2023locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt, 2023.

\bibitem[Michaud et~al.(2020)Michaud, Gleave, and
  Russell]{michaud2020understanding}
Eric~J Michaud, Adam Gleave, and Stuart Russell.
\newblock Understanding learned reward functions.
\newblock \emph{arXiv preprint arXiv:2012.05862}, 2020.

\bibitem[Milano et~al.(2021)Milano, Taddeo, and Floridi]{milano2021ethical}
Silvia Milano, Mariarosaria Taddeo, and Luciano Floridi.
\newblock Ethical aspects of multi-stakeholder recommendation systems.
\newblock \emph{The information society}, 37\penalty0 (1):\penalty0 35--45,
  2021.

\bibitem[Milli and Dragan(2020)]{milli2020literal}
Smitha Milli and Anca~D Dragan.
\newblock Literal or pedagogic human? analyzing human model misspecification in
  objective learning.
\newblock In \emph{Uncertainty in artificial intelligence}, pages 925--934.
  PMLR, 2020.

\bibitem[Mindermann and Armstrong(2018)]{mindermann2018occam}
Soren Mindermann and Stuart Armstrong.
\newblock Occam's razor is insufficient to infer the preferences of irrational
  agents.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, NIPS'18, page 5603–5614, Red Hook, NY,
  USA, 2018. Curran Associates Inc.

\bibitem[M{\"o}kander et~al.(2023)M{\"o}kander, Schuett, Kirk, and
  Floridi]{mokander2023auditing}
Jakob M{\"o}kander, Jonas Schuett, Hannah~Rose Kirk, and Luciano Floridi.
\newblock Auditing large language models: a three-layered approach.
\newblock \emph{arXiv preprint arXiv:2302.08500}, 2023.

\bibitem[Myers et~al.(2021)Myers, Biyik, Anari, and Sadigh]{myers2022learning}
Vivek Myers, Erdem Biyik, Nima Anari, and Dorsa Sadigh.
\newblock Learning multimodal rewards from rankings.
\newblock In \emph{Conference on Robot Learning}, pages 342--352. PMLR, 2021.

\bibitem[National Commission for the Protection~of
  Human~Subjects(1978)]{united1978belmont}
United~States National Commission for the Protection~of Human~Subjects.
\newblock \emph{The Belmont report: ethical principles and guidelines for the
  protection of human subjects of research}, volume~1.
\newblock United States Department of Health, Education, and Welfare, National
  Commission for the Protection of Human Subjects of Biomedical and Behavioral
  Research, 1978.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Andrew~Y Ng, Stuart Russell, et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, page~2, 2000.

\bibitem[Ngo(2022)]{ngo2022alignment}
Richard Ngo.
\newblock The alignment problem from a deep learning perspective.
\newblock \emph{arXiv preprint arXiv:2209.00626}, 2022.

\bibitem[Nikishin et~al.(2018)Nikishin, Izmailov, Athiwaratkun, Podoprikhin,
  Garipov, Shvechikov, Vetrov, and Wilson]{nikishin2018improving}
Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur
  Garipov, Pavel Shvechikov, Dmitry Vetrov, and Andrew~Gordon Wilson.
\newblock Improving stability in deep reinforcement learning with weight
  averaging.
\newblock In \emph{Uncertainty in artificial intelligence workshop on
  uncertainty in Deep learning}, 2018.

\bibitem[Noothigattu et~al.(2018)Noothigattu, Gaikwad, Awad, Dsouza, Rahwan,
  Ravikumar, and Procaccia]{noothigattu2018voting}
Ritesh Noothigattu, Snehalkumar Gaikwad, Edmond Awad, Sohan Dsouza, Iyad
  Rahwan, Pradeep Ravikumar, and Ariel Procaccia.
\newblock A voting-based system for ethical decision making.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[O'Keefe et~al.(2020)O'Keefe, Cihon, Garfinkel, Flynn, Leung, and
  Dafoe]{o2020windfall}
Cullen O'Keefe, Peter Cihon, Ben Garfinkel, Carrick Flynn, Jade Leung, and
  Allan Dafoe.
\newblock The windfall clause: Distributing the benefits of ai for the common
  good.
\newblock In \emph{Proceedings of the AAAI/ACM Conference on AI, Ethics, and
  Society}, pages 327--331, 2020.

\bibitem[Omar et~al.(2013)Omar, Ngadi, and Jebur]{omar2013machine}
Salima Omar, Asri Ngadi, and Hamid~H Jebur.
\newblock Machine learning techniques for anomaly detection: an overview.
\newblock \emph{International Journal of Computer Applications}, 79\penalty0
  (2), 2013.

\bibitem[Oneal(2023)]{oneal2023}
A.J. Oneal.
\newblock Chat gpt "dan" (and other "jailbreaks").
\newblock
  \url{https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516},
  2023.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Pan et~al.(2022)Pan, Bhatia, and Steinhardt]{pan2022effects}
Alexander Pan, Kush Bhatia, and Jacob Steinhardt.
\newblock The effects of reward misspecification: Mapping and mitigating
  misaligned models.
\newblock \emph{arXiv preprint arXiv:2201.03544}, 2022.

\bibitem[Pandey et~al.(2022)Pandey, Purohit, Castillo, and
  Shalin]{pandey2022modeling}
Rahul Pandey, Hemant Purohit, Carlos Castillo, and Valerie~L Shalin.
\newblock Modeling and mitigating human annotation errors to design efficient
  stream processing systems with human-in-the-loop machine learning.
\newblock \emph{International Journal of Human-Computer Studies}, 160:\penalty0
  102772, 2022.

\bibitem[Pang et~al.(2021)Pang, Shen, Cao, and Hengel]{pang2021deep}
Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van~Den Hengel.
\newblock Deep learning for anomaly detection: A review.
\newblock \emph{ACM computing surveys (CSUR)}, 54\penalty0 (2):\penalty0 1--38,
  2021.

\bibitem[Peng et~al.(2019)Peng, Nushi, K{\i}c{\i}man, Inkpen, Suri, and
  Kamar]{peng2019you}
Andi Peng, Besmira Nushi, Emre K{\i}c{\i}man, Kori Inkpen, Siddharth Suri, and
  Ece Kamar.
\newblock What you see is what you get? the impact of representation criteria
  on human bias in hiring.
\newblock In \emph{Proceedings of the AAAI Conference on Human Computation and
  Crowdsourcing}, volume~7, pages 125--134, 2019.

\bibitem[Peng et~al.(2022)Peng, Nushi, Kiciman, Inkpen, and
  Kamar]{peng2022investigations}
Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, and Ece Kamar.
\newblock Investigations of performance and bias in human-ai teamwork in
  hiring.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 12089--12097, 2022.

\bibitem[Peng et~al.(2023)Peng, Netanyahu, Ho, Shu, Bobu, Shah, and
  Agrawal]{peng2023diagnosis}
Andi Peng, Aviv Netanyahu, Mark~K Ho, Tianmin Shu, Andreea Bobu, Julie Shah,
  and Pulkit Agrawal.
\newblock Diagnosis, feedback, adaptation: A human-in-the-loop framework for
  test-time policy adaptation.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, 2023.

\bibitem[Perez et~al.(2022{\natexlab{a}})Perez, Huang, Song, Cai, Ring,
  Aslanides, Glaese, McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock \emph{arXiv preprint arXiv:2202.03286}, 2022{\natexlab{a}}.

\bibitem[Perez et~al.(2022{\natexlab{b}})Perez, Ringer,
  Luko{\v{s}}i{\=u}t{\.e}, Nguyen, Chen, Heiner, Pettit, Olsson, Kundu,
  Kadavath, et~al.]{perez2022discovering}
Ethan Perez, Sam Ringer, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Karina Nguyen,
  Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu,
  Saurav Kadavath, et~al.
\newblock Discovering language model behaviors with model-written evaluations.
\newblock \emph{arXiv preprint arXiv:2212.09251}, 2022{\natexlab{b}}.

\bibitem[Perrigo(2023)]{timeExclusiveHour}
Billy Perrigo.
\newblock Exclusive: The \$2 per hour workers who made chatgpt safer, 2023.
\newblock URL \url{https://time.com/6247678/openai-chatgpt-kenya-workers/}.
\newblock [Accessed 07-May-2023].

\bibitem[Perry and Uuk(2019)]{perry2019ai}
Brandon Perry and Risto Uuk.
\newblock Ai governance and the policymaking process: key considerations for
  reducing ai risk.
\newblock \emph{Big data and cognitive computing}, 3\penalty0 (2):\penalty0 26,
  2019.

\bibitem[Perry et~al.(2022)Perry, Srivastava, Kumar, and Boneh]{perry2022users}
Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh.
\newblock Do users write more insecure code with ai assistants?, 2022.

\bibitem[Prabhakaran et~al.(2021)Prabhakaran, Davani, and
  Diaz]{prabhakaran2021releasing}
Vinodkumar Prabhakaran, Aida~Mostafazadeh Davani, and Mark Diaz.
\newblock On releasing annotator-level labels and information in datasets.
\newblock \emph{arXiv preprint arXiv:2110.05699}, 2021.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and
  Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D
  Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Ramachandran and Amir(2007)]{ramachandran2007bayesian}
Deepak Ramachandran and Eyal Amir.
\newblock Bayesian inverse reinforcement learning.
\newblock In \emph{Proceedings of the 20th International Joint Conference on
  Artifical Intelligence}, IJCAI'07, page 2586–2591, San Francisco, CA, USA,
  2007. Morgan Kaufmann Publishers Inc.

\bibitem[Rame et~al.(2023)Rame, Couairon, Shukor, Dancette, Gaya, Soulier, and
  Cord]{rame2023rewarded}
Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette,
  Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord.
\newblock Rewarded soups: towards pareto-optimal alignment by interpolating
  weights fine-tuned on diverse rewards.
\newblock \emph{arXiv preprint arXiv:2306.04488}, 2023.

\bibitem[Rao et~al.(2023)Rao, Vashistha, Naik, Aditya, and
  Choudhury]{rao2023tricking}
Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit
  Choudhury.
\newblock Tricking llms into disobedience: Understanding, analyzing, and
  preventing jailbreaks, 2023.

\bibitem[Rastogi et~al.(2023)Rastogi, Ribeiro, King, and
  Amershi]{rastogi2023supporting}
Charvi Rastogi, Marco~Tulio Ribeiro, Nicholas King, and Saleema Amershi.
\newblock Supporting human-ai collaboration in auditing llms with llms.
\newblock \emph{arXiv preprint arXiv:2304.09991}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2304.09991.pdf}.

\bibitem[R{\"a}uker et~al.(2023)R{\"a}uker, Ho, Casper, and
  Hadfield-Menell]{rauker2023toward}
Tilman R{\"a}uker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell.
\newblock Toward transparent ai: A survey on interpreting the inner structures
  of deep neural networks.
\newblock In \emph{2023 IEEE Conference on Secure and Trustworthy Machine
  Learning (SaTML)}, pages 464--483. IEEE, 2023.

\bibitem[Reddy et~al.(2019)Reddy, Dragan, and Levine]{reddy_where_2019}
Siddharth Reddy, Anca~D. Dragan, and Sergey Levine.
\newblock Where {Do} {You} {Think} {You}'re {Going}?: {Inferring} {Beliefs}
  about {Dynamics} from {Behavior}.
\newblock \emph{arXiv:1805.08010 [cs, stat]}, January 2019.
\newblock URL \url{http://arxiv.org/abs/1805.08010}.
\newblock arXiv: 1805.08010.

\bibitem[Reddy et~al.(2020)Reddy, Levine, and Dragan]{reddy_assisted_2020}
Siddharth Reddy, Sergey Levine, and Anca~D Dragan.
\newblock Assisted {Perception}: {Optimizing} {Observations} to {Communicate}
  {State}.
\newblock 2020.

\bibitem[Rendle et~al.(2012)Rendle, Freudenthaler, Gantner, and
  Schmidt-Thieme]{rendle2012bpr}
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
\newblock Bpr: Bayesian personalized ranking from implicit feedback.
\newblock \emph{arXiv preprint arXiv:1205.2618}, 2012.

\bibitem[Sadigh et~al.(2017)Sadigh, Dragan, Sastry, and
  Seshia]{sadigh2017active}
Dorsa Sadigh, Anca~D Dragan, Shankar Sastry, and Sanjit~A Seshia.
\newblock \emph{Active preference-based learning of reward functions}.
\newblock 2017.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim,
  Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan, Scao,
  Biderman, Gao, Wolf, and Rush]{sanh_t0}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
  Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful
  Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla,
  Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
  Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong,
  Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
  Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan
  Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M
  Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Santurkar et~al.(2023)Santurkar, Durmus, Ladhak, Lee, Liang, and
  Hashimoto]{santurkar2023whose}
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and
  Tatsunori Hashimoto.
\newblock Whose opinions do language models reflect?
\newblock \emph{arXiv preprint arXiv:2303.17548}, 2023.

\bibitem[Sartori and Theodorou(2022)]{sartori2022sociotechnical}
Laura Sartori and Andreas Theodorou.
\newblock A sociotechnical perspective for the future of ai: narratives,
  inequalities, and human control.
\newblock \emph{Ethics and Information Technology}, 24\penalty0 (1):\penalty0
  4, 2022.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and
  Leike]{saunders2022self}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan
  Ward, and Jan Leike.
\newblock Self-critiquing models for assisting human evaluators.
\newblock \emph{arXiv preprint arXiv:2206.05802}, 2022.

\bibitem[Scheurer et~al.(2022)Scheurer, Campos, Chan, Chen, Cho, and
  Perez]{scheurer2022training}
J{\'e}r{\'e}my Scheurer, Jon~Ander Campos, Jun~Shern Chan, Angelica Chen,
  Kyunghyun Cho, and Ethan Perez.
\newblock Training language models with language feedback.
\newblock In \emph{The First Workshop on Learning with Natural Language
  Supervision at ACL}, 2022.

\bibitem[Scheurer et~al.(2023)Scheurer, Campos, Korbak, Chan, Chen, Cho, and
  Perez]{scheurer2023training}
J{\'e}r{\'e}my Scheurer, Jon~Ander Campos, Tomasz Korbak, Jun~Shern Chan,
  Angelica Chen, Kyunghyun Cho, and Ethan Perez.
\newblock Training language models with language feedback at scale.
\newblock \emph{arXiv preprint arXiv:2303.16755}, 2023.

\bibitem[Sen(1986)]{sen1986social}
Amartya Sen.
\newblock Social choice theory.
\newblock \emph{Handbook of mathematical economics}, 3:\penalty0 1073--1181,
  1986.

\bibitem[Shah et~al.(2019)Shah, Gundotra, Abbeel, and
  Dragan]{shah2019feasibility}
Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan.
\newblock On the feasibility of learning, rather than assuming, human biases
  for reward inference.
\newblock In \emph{International Conference on Machine Learning}, pages
  5670--5679. PMLR, 2019.

\bibitem[Shah et~al.(2022)Shah, Varma, Kumar, Phuong, Krakovna, Uesato, and
  Kenton]{shah2022goal}
Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna,
  Jonathan Uesato, and Zac Kenton.
\newblock Goal misgeneralization: Why correct specifications aren't enough for
  correct goals.
\newblock \emph{arXiv preprint arXiv:2210.01790}, 2022.

\bibitem[Shapin and Schaffer(2011)]{shapin2011leviathan}
Steven Shapin and Simon Schaffer.
\newblock \emph{Leviathan and the air-pump: Hobbes, Boyle, and the experimental
  life}.
\newblock Princeton University Press, 2011.

\bibitem[Sharma et~al.(2022)Sharma, Sundaralingam, Blukis, Paxton, Hermans,
  Torralba, Andreas, and Fox]{sharma2022correcting}
Pratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker
  Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox.
\newblock Correcting robot plans with natural language feedback.
\newblock \emph{arXiv preprint arXiv:2204.05186}, 2022.

\bibitem[Shavit(2023)]{shavit2023does}
Yonadav Shavit.
\newblock What does it take to catch a chinchilla? verifying rules on
  large-scale neural network training via compute monitoring, 2023.

\bibitem[Shevlane et~al.(2023)Shevlane, Farquhar, Garfinkel, Phuong,
  Whittlestone, Leung, Kokotajlo, Marchal, Anderljung, Kolt,
  et~al.]{shevlane2023model}
Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess
  Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus
  Anderljung, Noam Kolt, et~al.
\newblock Model evaluation for extreme risks.
\newblock \emph{arXiv preprint arXiv:2305.15324}, 2023.

\bibitem[Siddique et~al.(2023)Siddique, Sinha, and Cao]{siddique2023fairness}
Umer Siddique, Abhinav Sinha, and Yongcan Cao.
\newblock Fairness in preference-based reinforcement learning, 2023.

\bibitem[Silver et~al.(2021)Silver, Singh, Precup, and
  Sutton]{silver2021reward}
David Silver, Satinder Singh, Doina Precup, and Richard~S Sutton.
\newblock Reward is enough.
\newblock \emph{Artificial Intelligence}, 299:\penalty0 103535, 2021.

\bibitem[Skalse and Abate(2022{\natexlab{a}})]{skalse2022misspecification}
Joar Skalse and Alessandro Abate.
\newblock Misspecification in inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2212.03201}, 2022{\natexlab{a}}.

\bibitem[Skalse et~al.(2022)Skalse, Howe, Krasheninnikov, and
  Krueger]{skalse2022defining}
Joar Skalse, Nikolaus~HR Howe, Dmitrii Krasheninnikov, and David Krueger.
\newblock Defining and characterizing reward hacking.
\newblock \emph{arXiv preprint arXiv:2209.13085}, 2022.

\bibitem[Skalse and Abate(2022{\natexlab{b}})]{skalsereward}
Joar Max~Viktor Skalse and Alessandro Abate.
\newblock The reward hypothesis is false.
\newblock In \emph{NeurIPS ML Safety Workshop}, 2022{\natexlab{b}}.

\bibitem[Skalse et~al.(2023)Skalse, Farrugia-Roberts, Russell, Abate, and
  Gleave]{skalse2023invariance}
Joar Max~Viktor Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro
  Abate, and Adam Gleave.
\newblock Invariance in policy optimisation and partial identifiability in
  reward learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  32033--32058. PMLR, 2023.

\bibitem[Snell et~al.(2022)Snell, Kostrikov, Su, Yang, and Levine]{snell_ilql}
Charlie Snell, Ilya Kostrikov, Yi~Su, Mengjiao Yang, and Sergey Levine.
\newblock Offline rl for natural language generation with implicit language q
  learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.11871}.

\bibitem[Snoswell and Burgess(2022)]{snoswell_galactica_2022}
Aaron~J. Snoswell and Jean Burgess.
\newblock The {Galactica} {AI} model was trained on scientific knowledge –
  but it spat out alarmingly plausible nonsense, November 2022.
\newblock URL
  \url{http://theconversation.com/the-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible-nonsense-195445}.

\bibitem[Solaiman and Dennison(2021)]{palms}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society (palms) with
  values-targeted datasets.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 5861--5873. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf}.

\bibitem[Song et~al.(2023)Song, Cai, Lee, and Su]{song2023reward}
Ziang Song, Tianle Cai, Jason~D Lee, and Weijie~J Su.
\newblock Reward collapse in aligning large language models.
\newblock \emph{arXiv preprint arXiv:2305.17608}, 2023.

\bibitem[Srinivasan et~al.(2020)Srinivasan, Eysenbach, Ha, Tan, and
  Finn]{srinivasan2020learning}
Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn.
\newblock Learning to be safe: Deep rl with a safety critic.
\newblock \emph{arXiv preprint arXiv:2010.14603}, 2020.

\bibitem[Steinhardt(2023)]{steinhardt_emergent_2023}
Jacob Steinhardt.
\newblock Emergent {Deception} and {Emergent} {Optimization}, February 2023.
\newblock URL
  \url{https://bounded-regret.ghost.io/emergent-deception-optimization/}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3008--3021, 2020.

\bibitem[Sumers et~al.(2021)Sumers, Ho, Hawkins, Narasimhan, and
  Griffiths]{sumers2021learning}
Theodore~R Sumers, Mark~K Ho, Robert~D Hawkins, Karthik Narasimhan, and
  Thomas~L Griffiths.
\newblock Learning rewards from linguistic feedback.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 6002--6010, 2021.

\bibitem[Tian et~al.(2023)Tian, Tomizuka, Dragan, and
  Bajcsy]{tian_towards_2023}
Ran Tian, Masayoshi Tomizuka, Anca Dragan, and Andrea Bajcsy.
\newblock Towards {Modeling} and {Influencing} the {Dynamics} of {Human}
  {Learning}, January 2023.
\newblock URL \url{http://arxiv.org/abs/2301.00901}.
\newblock arXiv:2301.00901 [cs].

\bibitem[Tien et~al.(2023)Tien, He, Erickson, Dragan, and
  Brown]{tien2023causal}
Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca Dragan, and Daniel~S
  Brown.
\newblock Causal confusion and reward misidentification in preference-based
  reward learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull,
  Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini,
  Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril,
  Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton,
  Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang,
  Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang,
  Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Turner(2021)]{turner2021seeking}
Alexander~M Turner.
\newblock Seeking power is convergently instrumental in a broad class of
  environments, 2021.
\newblock URL
  \url{https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk/p/hzeLSQ9nwDkPc4KNt}.

\bibitem[Turner and Tadepalli(2022)]{Turner2022ParametricallyRD}
Alexander~Matt Turner and Prasad Tadepalli.
\newblock Parametrically retargetable decision-makers tend to seek power.
\newblock \emph{ArXiv}, abs/2206.13477, 2022.

\bibitem[Turner et~al.(2019)Turner, Smith, Shah, Critch, and
  Tadepalli]{Turner2019OptimalPT}
Alexander~Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad
  Tadepalli.
\newblock Optimal policies tend to seek power.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Uc-Cetina et~al.(2023)Uc-Cetina, Navarro-Guerrero, Martin-Gonzalez,
  Weber, and Wermter]{uc2023survey}
Victor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez, Cornelius
  Weber, and Stefan Wermter.
\newblock Survey on reinforcement learning for language processing.
\newblock \emph{Artificial Intelligence Review}, 56\penalty0 (2):\penalty0
  1543--1575, 2023.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang,
  Creswell, Irving, and Higgins]{uesato2022solving}
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa
  Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.
\newblock Solving math word problems with process-and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Vamplew et~al.(2022)Vamplew, Smith, K{\"a}llstr{\"o}m, Ramos,
  R{\u{a}}dulescu, Roijers, Hayes, Heintz, Mannion, Libin,
  et~al.]{vamplew2022scalar}
Peter Vamplew, Benjamin~J Smith, Johan K{\"a}llstr{\"o}m, Gabriel Ramos, Roxana
  R{\u{a}}dulescu, Diederik~M Roijers, Conor~F Hayes, Fredrik Heintz, Patrick
  Mannion, Pieter~JK Libin, et~al.
\newblock Scalar reward is not enough: A response to silver, singh, precup and
  sutton (2021).
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 36\penalty0
  (2):\penalty0 41, 2022.

\bibitem[Veselovsky et~al.(2023)Veselovsky, Ribeiro, and
  West]{veselovsky2023artificial}
Veniamin Veselovsky, Manoel~Horta Ribeiro, and Robert West.
\newblock Artificial artificial artificial intelligence: Crowd workers widely
  use large language models for text production tasks.
\newblock \emph{arXiv preprint arXiv:2306.07899}, 2023.

\bibitem[Vincent(2023)]{vincent_microsofts_2023}
James Vincent.
\newblock Microsoft’s {Bing} is an emotionally manipulative liar, and people
  love it, February 2023.
\newblock URL
  \url{https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams}.

\bibitem[Wan et~al.(2023)Wan, Wallace, Shen, and Klein]{wan2023poisoning}
Alex Wan, Eric Wallace, Sheng Shen, and Dan Klein.
\newblock Poisoning language models during instruction tuning.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Wang et~al.(2022)Wang, Gleave, Belrose, Tseng, Miller, Dennis, Duan,
  Pogrebniak, Levine, and Russell]{wang2022adversarial}
Tony~Tong Wang, Adam Gleave, Nora Belrose, Tom Tseng, Joseph Miller, Michael~D
  Dennis, Yawen Duan, Viktor Pogrebniak, Sergey Levine, and Stuart Russell.
\newblock Adversarial policies beat professional-level go ais.
\newblock \emph{arXiv preprint arXiv:2211.00241}, 2022.

\bibitem[Wang et~al.(2023)Wang, Zhong, Li, Mi, Zeng, Huang, Shang, Jiang, and
  Liu]{wang2023aligning}
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang,
  Lifeng Shang, Xin Jiang, and Qun Liu.
\newblock Aligning large language models with human: A survey, 2023.

\bibitem[Wei et~al.(2023)Wei, Haghtalab, and Steinhardt]{wei2023jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does llm safety training fail?
\newblock \emph{arXiv preprint arXiv:2307.02483}, 2023.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang,
  Cheng, Glaese, Balle, Kasirzadeh, Kenton, Brown, Hawkins, Stepleton, Biles,
  Birhane, Haas, Rimell, Hendricks, Isaac, Legassick, Irving, and
  Gabriel]{weidinger2021ethical}
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato,
  Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac
  Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba
  Birhane, Julia Haas, Laura Rimell, Lisa~Anne Hendricks, William Isaac, Sean
  Legassick, Geoffrey Irving, and Iason Gabriel.
\newblock Ethical and social risks of harm from language models, 2021.

\bibitem[Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor, Hendricks,
  Anderson, Kohli, Coppin, and Huang]{weibl2021}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang.
\newblock Challenges in detoxifying language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 2447--2469, Punta Cana, Dominican Republic, November 2021.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.210}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.210}.

\bibitem[Whittlestone et~al.(2021)Whittlestone, Arulkumaran, and
  Crosby]{whittlestone2021societal}
Jess Whittlestone, Kai Arulkumaran, and Matthew Crosby.
\newblock The societal implications of deep reinforcement learning.
\newblock \emph{Journal of Artificial Intelligence Research}, 70:\penalty0
  1003--1030, 2021.

\bibitem[Wilde et~al.(2022)Wilde, Biyik, Sadigh, and Smith]{wilde2022learning}
Nils Wilde, Erdem Biyik, Dorsa Sadigh, and Stephen~L Smith.
\newblock Learning reward functions from scale feedback.
\newblock In \emph{Conference on Robot Learning}, pages 353--362. PMLR, 2022.

\bibitem[Willison(2023)]{promptInjection2023}
Simon Willison.
\newblock Prompt injection.
\newblock 2023.
\newblock URL \url{https://simonwillison.net/series/prompt-injection/}.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz,
  et~al.]{wirth2017survey}
Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes F{\"u}rnkranz, et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.

\bibitem[Wolf et~al.(2023)Wolf, Wies, Levine, and Shashua]{wolf2023fundamental}
Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock Fundamental limitations of alignment in large language models.
\newblock \emph{arXiv preprint arXiv:2304.11082}, 2023.

\bibitem[Wu et~al.(2021{\natexlab{a}})Wu, Ouyang, Ziegler, Stiennon, Lowe,
  Leike, and Christiano]{wu2021recursively}
Jeff Wu, Long Ouyang, Daniel~M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike,
  and Paul Christiano.
\newblock Recursively summarizing books with human feedback,
  2021{\natexlab{a}}.

\bibitem[Wu et~al.(2021{\natexlab{b}})Wu, Guo, Wei, and
  Xing]{wu2021adversarial}
Xian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing.
\newblock Adversarial policy training against deep reinforcement learning.
\newblock In \emph{USENIX Security Symposium}, pages 1883--1900,
  2021{\natexlab{b}}.

\bibitem[Wu et~al.(2023)Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith,
  Ostendorf, and Hajishirzi]{wu2023finegrained}
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
  Ammanabrolu, Noah~A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi.
\newblock Fine-grained human feedback gives better rewards for language model
  training, 2023.

\bibitem[Wulfe et~al.(2022)Wulfe, Ellis, Mercat, McAllister, and Gaidon]{dard}
Blake Wulfe, Logan~Michael Ellis, Jean Mercat, Rowan~Thomas McAllister, and
  Adrien Gaidon.
\newblock Dynamics-aware comparison of learned reward functions.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=CALFyKVs87}.

\bibitem[Xu et~al.(2023)Xu, Ma, Wang, Xiao, and Chen]{xu2023instructions}
Jiashu Xu, Mingyu~Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen.
\newblock Instructions as backdoors: Backdoor vulnerabilities of instruction
  tuning for large language models.
\newblock \emph{arXiv preprint arXiv:2305.14710}, 2023.

\bibitem[Yang et~al.(2021)Yang, Tang, Bai, Liu, Hao, Meng, Liu, and
  Wang]{yang2021exploration}
Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng,
  Peng Liu, and Zhen Wang.
\newblock Exploration in deep reinforcement learning: a comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2109.06668}, 2021.

\bibitem[Yannakakis and Hallam(2011)]{yannakakis2011ranking}
Georgios~N Yannakakis and John Hallam.
\newblock Ranking vs. preference: a comparative study of self-reporting.
\newblock In \emph{Affective Computing and Intelligent Interaction: 4th
  International Conference, ACII 2011, Memphis, TN, USA, October 9--12, 2011,
  Proceedings, Part I 4}, pages 437--446. Springer, 2011.

\bibitem[Ye et~al.(2023)Ye, Jo, Kim, Kim, Hwang, and Seo]{ye2023selfee}
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and
  Minjoon Seo.
\newblock Selfee: Iterative self-revising llm empowered by self-feedback
  generation, 2023.
\newblock URL \url{https://kaistai.github.io/SelFee/}.

\bibitem[Yu et~al.(2023)Yu, Gileadi, Fu, Kirmani, Lee, Gonzalez~Arenas,
  Lewis~Chiang, Erez, Hasenclever, Humplik, Ichter, Xiao, Xu, Zeng, Zhang,
  Heess, Sadigh, Tan, Tassa, and Xia]{yu2023language}
Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse
  Gonzalez~Arenas, Hao-Tien Lewis~Chiang, Tom Erez, Leonard Hasenclever, Jan
  Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas
  Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia.
\newblock Language to rewards for robotic skill synthesis.
\newblock \emph{Arxiv preprint arXiv:2306.08647}, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and
  Huang]{yuan2023rrhf}
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.
\newblock Rrhf: Rank responses to align language models with human feedback
  without tears, 2023.

\bibitem[Yue et~al.(2023)Yue, Wang, Shao, Zhang, Lin, Ren, and
  Zhang]{yue2023clare}
Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju~Ren, and Junshan
  Zhang.
\newblock Clare: Conservative model-based reward learning for offline inverse
  reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Zhang and Li(2019)]{zhang2019adversarial}
Jiliang Zhang and Chen Li.
\newblock Adversarial examples: Opportunities and challenges.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  31\penalty0 (7):\penalty0 2578--2593, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Press, Merrill, Liu, and
  Smith]{zhang2023language}
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah~A Smith.
\newblock How language model hallucinations can snowball.
\newblock \emph{arXiv preprint arXiv:2305.13534}, 2023.

\bibitem[Zhang et~al.(2021)Zhang, Cao, Sadigh, and Sui]{zhang2021confidence}
Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui.
\newblock Confidence-aware imitation learning from demonstrations with varying
  optimality.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12340--12350, 2021.

\bibitem[Zhao et~al.(2016)Zhao, Piech, and Xia]{zhao2016learning}
Zhibing Zhao, Peter Piech, and Lirong Xia.
\newblock Learning mixtures of plackett-luce models.
\newblock In \emph{International Conference on Machine Learning}, pages
  2906--2914. PMLR, 2016.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu,
  et~al.]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}, 2023.

\bibitem[Zhou and Small(2021)]{zhou2021inverse}
Li~Zhou and Kevin Small.
\newblock Inverse reinforcement learning with natural language goals.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 11116--11124, 2021.

\bibitem[Zhu et~al.(2023)Zhu, Jiao, and Jordan]{zhu2023principled}
Banghua Zhu, Jiantao Jiao, and Michael~I Jordan.
\newblock Principled reinforcement learning with human feedback from pairwise
  or $ k $-wise comparisons.
\newblock \emph{arXiv preprint arXiv:2301.11270}, 2023.

\bibitem[Zhuang and Hadfield-Menell(2020)]{zhuang2020consequences}
Simon Zhuang and Dylan Hadfield-Menell.
\newblock Consequences of misaligned ai.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15763--15773, 2020.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\bibitem[Ziegler et~al.(2022)Ziegler, Nix, Chan, Bauman, Schmidt-Nielsen, Lin,
  Scherlis, Nabeshima, Weinstein-Raun, de~Haas, et~al.]{ziegler2022adversarial}
Daniel Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter
  Schmidt-Nielsen, Tao Lin, Adam Scherlis, Noa Nabeshima, Benjamin
  Weinstein-Raun, Daniel de~Haas, et~al.
\newblock Adversarial training for high-stakes reliability.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 9274--9286, 2022.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
