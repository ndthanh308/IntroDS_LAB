\section{Approximate heavy hitter under linear aggregation}
\label{sec:ahh}
In this section, we study the approximate heavy hitter problem and show that the problem can be solved with per-user communication complexity $\tilde{O}\Paren{\frac{mn}{\thr} \log d}$, stated in \cref{thm:ahh}.

A natural comparison to make is the heavy hitter recovery algorithm obtained from getting a frequency oracle up to accuracy $\Theta(\thr)$. Since there are $R$ rounds, the naive approach would require an accuracy of $\Theta(\thr/R)$ in each round and classic methods such as Count-min and Count-sketch would require a per-user communication complexity of $\tilde{\Theta}(\nspu\ns R/\thr)$. In the $R$-round case, our result improves upon this by a factor of $R$. In fact, as we show in \cref{thm:ahist_lower}, any frequency oracle-based approach would require per-user communication complexity of at least $\Omega(\nspu\ns \sqrt{R}/\thr)$. Our result improves upon these and show that the dependence on $R$ is at most logarithmic.

\begin{theorem} \label{thm:ahh}
There exists a non-interactive linear sketching protocol with communication cost %
$\tilde{O}\Paren{\frac{mn}{\thr} }$
bits per user, %
which solves the \textbf{$\thr$-approximate heavy hitter}
problem. Moreover, the running time of the algorithm is
$\tilde{O}\Paren{\frac{mn}{\thr}}$. 
\end{theorem}

The next theorem shows that the above communication complexity is minmax optimal up to logarithmic factors. 
\begin{theorem} \label{thm:ahh_lower}
    For any $\thr$ and \new{interactive linear sketching %
    protocol} $\cA$ with per-user communication cost $o\Paren{\frac{mn}{\thr}}$, there exists a dataset $h_i, i \in B_r, r \in [R]$, such that $\cA$ cannot solve $\thr$-heavy hitter (HH) with success probability at least 4/5.
\end{theorem}



Next we will present the protocol that achieves \cref{thm:ahh} in \cref{sec:ahh_upper} and discuss the proof of the lower bound \cref{thm:ahh_lower} in \cref{sec:ahh_lower}.

\label{sec:ahh_upper}

At a high level, the protocol relies on two main components: (i) a probabilistic data structure called Invertible Bloom Lookup Table (IBLT) introduced by \citet{Goodrich2011iblt}, and (ii) local subsampling. We start by introducing IBLTs, starting from the more standard (counting) Bloom filters. 

\paragraph{IBLT: Bloom filters with efficient listing.} Note that each user's local histogram $h_i$ can be viewed as a sequence of key-value pairs $(x, h_i(x))$.
The Bloom filter data structure
is a standard linear data structure to represent a 
set of key-value pairs with keys coming from a large domain. 
IBLT is a version of Bloom filter that %
supports an efficient listing operation -- while preserving the other nice properties of Bloom counting filters, namely linearity (and thus mergeable by summation), and succintness (linear size in number of indices it holds).\footnote{\new{In our algorithm, IBLT could be replaced by other data structures with these properties.}}
These properties are summarized in the following Lemma. 



\begin{lemma}[\cite{Goodrich2011iblt}] \label{lem:iblt}
Consider a collection of local histograms $(h_i)_{i\in [n]}$ over $[d]$ such that $\norm{\sum_{i \in [\ns]} h_i}_0 \le L_0$.

For any $\gamma  > 0$, there exist local linear sketches $\{f_i\}_{i \in [\ns]}$ of length $\ell = \tilde{O}(\gamma L_0)$ and an $O(\ell)$ time decoding procedure $\deciblt(\cdot)$~such that 
\[
    \deciblt\paren{\sum_{i \in [\ns]} f_i(h_i)} = \sum_{i \in [\ns]} h_i
\]
 succeeds except with probability at most $O\Paren{L_0^{2 -\gamma}}$.
\end{lemma}


For the purpose of this paper we can focus on 
the two main operations supported by an IBLT instance $\mathcal{B}$ (see~\cite{Goodrich2011iblt} for details on deletions and look-ups):
\begin{itemize}
    \item $\texttt{Insert}(k, v)$, which inserts the pair $(k, v)$ into 
$\mathcal{B}$.
\item
$\texttt{ListEntries}()$,
which enumerates the set of 
key-value pairs in $\mathcal{B}$.
\end{itemize} Note that
$f_i(h_i)$ in Lemma~\ref{lem:iblt} corresponds to the IBLT $\mathcal{B}_i$ resulting from inserting the set $\{(x,h_i(x)) ~|~ h_i(x) > 0\}$ into an empty IBLT.
Also, $\texttt{ListEntries}()$
corresponds to $\deciblt$
in Lemma~\ref{lem:iblt}.

Finally, 
$\sum_i^n f_i(h_i)$ corresponds to 
the encoding of 
the IBLT resulting 
from inserting the set $\{(x, \sum_i^n h_i(x)) ~|~ \exists i\in[n]: h_i(x) > 0\}$ into an empty IBLT.
In other words, each client $i\in [n]$
computes {\em local}
IBLT $\mathcal{B}_i := f_i(h_i)$,
and the (secure) aggregation of the 
$\mathcal{B}_i$'s results in the
{\em global} IBLT 
$\mathcal{B}:= \sum_i^n f_i(h_i)$. Further details on IBLT are stated in \cref{sec:iblt_app}.

\paragraph{Reducing capacity via threshold sampling.}
The second tool in our main protocol is threshold sampling. Note that
the guarantee in \cref{lem:iblt} relies on the number of unique elements in $\sum_{i \in [\ns]} h_i$, which can be at most $\nspu \ns$ in the worst-case, leading to an $O(mn)$ worst-case communication cost, not matching our lower bound in Lemma~\ref{thm:ahh_lower}. For heavy hitter recovery, we reduce the communication cost by local subsampling. More precisely, we use the threshold sampling algorithm from \cite{duffield05}, detailed in \cref{alg:threshold_sampling}
to achieve the (optimal) dependency $O(mn/\tau)$. 

\new{
\begin{remark}
Threshold sampling can be replaced by any unbiased local subsampling method that offers sparsity, \eg binomial sampling where $p \cdot h'(x) \sim \text{Binomial}(h(x), p)$ for some $p \in (0,1)$, and similar theoretical guarantee will hold. In this work, we choose threshold sampling due to the property that it minimizes the total variance of $h'$ under an expected sparsity constraint (see \citet{duffield05} for details).
\end{remark}
}
\begin{algorithm}[h]
\caption{Threshold sampling.}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $h:$ local histogram. $t \in \RR_{+}:$ threshold.

\FOR{$x \in {\rm supp}(h)$}
    \IF {$h(x) \ge t$, }
        \STATE $h'(x) = h(x)$.
    \ELSE
        \STATE
        \[
            h'(x) = \begin{cases}
                t & \text{ with prob } \frac{h(x)}{t}, \\
                0 & \text{ otherwise.}
            \end{cases}
        \]
    \ENDIF
\ENDFOR
\STATE \textbf{Return:} $h'$.
\end{algorithmic}
\label{alg:threshold_sampling}
\end{algorithm}

The protocol that achieves the desired communication complexity in Theorem~\ref{thm:ahh} is detailed in \cref{alg:subsample_IBLT}. 

\begin{algorithm}[h!]
\caption{Subsampled IBLT with \linagg.}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\{h_i\}_{i \in B_r, r \in [R]}:$ local histograms; $d:$ alphabet size; $R:$ number of rounds; $m:$ per-user contribution bound; $n:$ number of users per round; $\thr:$ threshold for heavy hitter recovery; $\beta:$ failure probability.
\STATE Let $t = \max\{\tau/2, 1\}$, $\rep = \ceil{10\log(\frac{4\nspu\ns R}{\tau\beta})}$ and $L_0 = 20 \frac{mn}{\tau} \log R, \gamma = \log R$.

\FOR{$r \in [R]$}
\FOR{$j \in [b]$}
\STATE Each user $i \in B_r$ applies \cref{alg:threshold_sampling} with threshold $t$ in to their local histogram with fresh randomness to get $h'_{i, j}$. \label{line:iblt_encoding}
\STATE Each user sends message
$
    Y_{i, j} = f_{i,j}(h'_{i, j})
$
where $f_{i, j}$'s are mappings from \cref{lem:iblt} with parameter $L_0, \gamma$ and fresh randomness.
\STATE Server observes $\sum_{i \in B_r} Y_{i,j}$ and computes $$\hat{h}_{r, j} = \deciblt (\sum_{i \in B_r} Y_{i,j}).$$
If the decoding is not successful, we set $\hat{h}_{r, j}$ be the all-zero vector.\label{line:iblt_decoding}
\ENDFOR
\ENDFOR
\FOR{$j \in [b]$}\STATE Server computes $\hat{h}^{[R]}_j = \sum_{r \in [R]} \hat{h}_{r,j},$
and obtain list 
\[
    \heavy_j = \{x \in [d] \mid  \hat{h}^{[R]}_j > 0\}.
\]
\ENDFOR
\STATE \textbf{Return: } 
\[
    \heavy = \{x \mid \sum_{j \in [\rep]} \idc{x \in \heavy_j} \ge \frac{\rep}{2} \}.
\]
\end{algorithmic}
\label{alg:subsample_IBLT}
\end{algorithm}



The algorithm can be viewed as $\rep \eqdef \ceil{20\log(\frac{40\nspu\ns R}{\tau\beta})}$ independent runs of a basic protocol, each of which returns a list $\heavy_i$ of potential heavy hitters. And the repetition is to boost the error probability.

In each basic protocol, users first apply \cref{alg:threshold_sampling} to subsample to the data, which reduces the number of unique elements while maintaining the heavy hitters upon aggregation. Then the user encodes their samples using IBLTs, whose aggregation is then sent to the server to decode. Since the number of unique elements is reduced through subsampling, the decoding of the aggregated IBLT will be successful with high probabiltiy, hence recovering the aggregation of subsampled local histograms. The detailed proof of \cref{thm:ahh} is presented in \cref{sec:proof_ahh}.



