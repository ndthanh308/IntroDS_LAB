\section{Motivation}
\label{sec:motivation}

Collecting and aggregating user data drives improvements in the app and web ecosystems. For instance, learning popular out-of-dictionary words can improve the auto-complete feature in a smart keyboard, and discovering malicious URLs can enhance the security of a browser. However, sharing user data directly with a service provider introduces several privacy risks. 

It is thus desirable to only make aggregate data available to the service provider, rather than directly sharing (unanonymized) user data with them. This is typically achieved via multi-party cryptographic primitives, such as a \textit{secure vector summation} protocol \citep{MelisDC16, bonawitz2017secure, TelemtryPrio, BellBGL020}. For instance, for closed domain histogram applications, the users can first “one-hot” encode their data into a vector of length $d$ (the size of the domain) and then participate in a secure vector summation protocol to make the aggregate histogram (but never the individual user contributions) available to the service provider. 

\paragraph{Federated heavy hitters recovery. } The abovementioned solution requires $\Omega(d)$ communication. However, in many real life applications the domain size is very large or even unknown a priori. For example, the set of new URLs can be represented via 8-bit character strings of length 100, and can thus take $d = 256^{100}$ values, which is clearly impossible to support in practice. In such settings, linear\footnote{Linearity is necessary because non-linear compression/sketching schemes would not work under the secure vector summation primitive which only makes the sum of client-held vectors available to the server.} sketching is often used to reduce the communication load.
For example, ~\citet{MelisDC16} use secure count-min sketch aggregation 
for privacy preserving training of recommender systems, and ~\citet{Corrigan-GibbsB17} rely on count-min sketches
for gathering browser statistics, i.e. aggregate histogram queries.
Similarly, ~\citet{Hu0LGWGLD21} rely on secure aggregation of variants of Flajolet-Martin sketches for distributed cardinality estimation. \citet{bnoeh2021lightweight} uses sketching to reduce the cost for distributed subset-histogram queries. In the work closest to ours, \citet{chen2022secagg} show that
count-sketches can be used to recover the \textit{heavy hitter} items (i.e. frequently appearing items) while reducing the communication overhead. All these protocols operate in the single-round setting.

\paragraph{Sketching in multi-round aggregation schemes.} Even though count-sketches are great step towards solving the heavy hitters problem, this approach has only been analyzed in the single round data aggregation setting. However, most commonly deployed systems for federated analytics employ \textit{multi-round} schemes for data aggregation \citep{fl-sys}. This is primarily because (a) not all users are available around the same time, (b) the population may be very large (in the billions of devices) and therefore the server has to aggregate data over batches for bandwidth/compute reasons, and (c) running the cryptographic secure vector summation protocol has compute and communication costs that are super linear in the number of users we are aggregating over \citep{BellBGL020, bonawitz2017secure}. Further, count sketch based approaches have a decoding runtime that is linear in $d$, which is infeasible in the open domain setting, and improving it to $\log d$ involves blowing up the communication cost by the same factor. 

\paragraph{Our contributions.}
Our paper thus takes a principled approach towards uncovering the fundamental accuracy-communication tradeoffs of the heavy hitters recovery problem under the linearity constraints imposed by secure vector summation protocols. \new{We show that linearity constraints increase the per-user communication complexity. For a fixed total number of users, as the number of rounds increases, the required communication decreases due to less stringent linearity constraint.} 
\new{Moreover, surprisingly, }we show that count-sketches are strictly sub-optimal for this application, and we develop a novel provably optimal approach that combines client-side (local) subsampling with inverse Bloom lookup tables (IBLTs). Roughly speaking, we show (via lower bounds) that in the $R$-round case, any approach that solves an approximate histogram problem (with additive error)  will incur a $\sqrt{R}$ factor penalty in the communication cost, while our optimal approach incurs $\log(R)$.
Hence, even non-trivial modifications of count-sketches \new{and other frequency oracle-based algorithms} are strictly sub-optimal. 

We also empirically evaluate our proposed algorithms and compare it with count-sketch baselines. Significant advantage of our algorithm is observed, especially when $R$ is large. In the setting of \cref{fig:communication_m10000}, to achieve an F1 score of 0.8, we see a 10x improvement in communication compared to the baseline using Count-sketch.


\paragraph{Organization.} We formally define the problem in \cref{sec:prelim} and then discuss our results in \cref{sec:results}. Algorithms for heavy hitter recovery and approximate histogram are presented in \cref{sec:ahh} and \cref{sec:ahist}, respectively. We discuss a practical modification of our algorithm in \cref{sec:adaptive} and present the experimental results in \cref{sec:exp}.

