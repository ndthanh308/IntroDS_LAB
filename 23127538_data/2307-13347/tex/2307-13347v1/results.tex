
\section{Results and technique}\label{sec:results}

\new{We consider both approximate heavy hitter recovery and approximate histogram estimation in the linear aggregation model. 
We establish tight (up to logarithmic factors) communication complexity for both tasks in the single-round and multi-round settings.
The results are summarized in \cref{tab:results}. Our results have the following interesting implications on the communication complexity of these problems.}

\paragraph{Linear aggregation increases the communication cost.} As shown in \cref{tab:results}, under \linagg, for both tasks, the per-user communication would incur a linear dependence on $\ns = N/R$, the number of users in each round. On the other hand, without linear aggregation constraint, there won't be a linear dependence on $\ns$ since each user can simply send their $\nspu$ local samples losslessly using $O(\nspu \log d)$ bits.
The result establishes the fundamental cost of linear aggregation communication protocols for heavy hitter recovery.







\paragraph{\ahist~is harder than \ahh.} As mentioned before, a natural way to obtain heavy hitters is to obtain an approximate histogram and do proper thresholding to select the heavy elements. Although in the single-round case, there is at most a logarithmic gap between the communication complexity for the two problems. In the $R$-round case, our result shows that this is strictly sub-optimal. More precisely, the communication cost for $\thr$-\ahh~increases by a factor of $\sqrt{R}$ while that of \ahist~depends at most logarithmically in $R$.
This implies a gap between the per-user communication cost for $\thr$-\ahist~and $\thr$-\ahh~in the multi-round case. 

\new{\paragraph{The impact of $R$.} With a fixed total number of users $N$, our result shows that the per-user communication complexity decreases as $R$ increases. This is due to the fact that as $R$ increases, the linearity constraints are imposed over a smaller group of users with size $N/R$, and hence less stringent. However, this also comes at the cost that the privacy implication from aggregation becomes weaker.}


\subsection{Our technique - IBLT with local subsampling}
As discussed above, when solving the approximate heavy hitter problem in the multi-round setting, algorithms that rely on obtaining an approximate histogram and thresholding won't give the optimal communication complexity. In the paper, we propose to use invertible bloom lookup tables (IBLTs) \citep{Goodrich2011iblt} and local subsampling. At a high-level, IBLT is a bloom filter-type linear data structure that supports efficient listing of the inserted elements and their exact counts. The size of the table scales linearly with the number of unique keys inserted. To reduce the communication cost, we perform local threshold sampling \citep{Duffield2005threshold} on users' local datasets. This guarantees that the ``light'' elements will be discarded with high probability and hence won't take up the capacity of the IBLT data structure. Compared to frequency-oracle based approach, the variance of the noise introduced in our subsampling-based approach for each item is proportional to its accumulative count, which gives better estimates for elements with frequencies near the threshold. For elements with counts way above the threshold, the frequency estimate will have a larger error but this won't affect heavy hitter recovery since only whether the count is above $\thr$ is crucial to our problem. 
See detail of the algorithm in \cref{sec:ahh_upper}.


\subsection{Related work}
Linear dimensionality reduction techniques for frequency estimation and heavy hitter recovery has been widely studied to reduce storage or communication cost, such as Count-sketch, Count-min sketch ~\citep{charikar2002finding, CORMODE200558, donoho2006compressed, minton2014improved}, and efficient decoding techniques have also been proposed \citep{cormode2006combinatorial, gilbert2010approximate}.


The closest to our work is \new{that} of \cite{chen2022secagg}, which studies approximate histogram estimation under linear sketching constraint. The work also establishes gap between communication complexities with/without Secure Aggregation. However, their result is in a more restricted setting of $m = 1$ and $R = 1$. Moreover, our algorithm also has the advantage of being computationally efficient (runtime only depends logartihmically in $d$), which is important for applications with large support but sparse data. \new{Their work also considers algorithms that guarantee distributed differential privacy guarantees, which we leave as interesting future directions.}




