\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem[Cohen et~al.(2022)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2022gradient}
Jeremy~M. Cohen, Simran Kaur, Yuanzhi Li, J.~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability, 2022.

\bibitem[Foret et~al.(2020)Foret, Research, Research, and Blueshift]{Foret2020}
Pierre Foret, Ariel Kleiner~Google Research, Hossein Mobahi~Google Research,
  and Behnam~Neyshabur Blueshift.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock 10 2020.
\newblock \doi{10.48550/arxiv.2010.01412}.
\newblock URL \url{https://arxiv.org/abs/2010.01412v3}.

\bibitem[Fukushima(1975)]{relu}
Kunihiko Fukushima.
\newblock Cognitron: A self-organizing multilayered neural network.
\newblock \emph{Biol. Cybern.}, 20\penalty0 (3–4):\penalty0 121–136, sep
  1975.
\newblock ISSN 0340-1200.
\newblock \doi{10.1007/BF00342633}.
\newblock URL \url{https://doi.org/10.1007/BF00342633}.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and Xiao]{Ghorbani2019}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via {H}essian
  eigenvalue density.
\newblock \emph{36th International Conference on Machine Learning, ICML 2019},
  2019-June:\penalty0 4039--4052, 1 2019.
\newblock \doi{10.48550/arxiv.1901.10159}.
\newblock URL \url{https://arxiv.org/abs/1901.10159v1}.

\bibitem[Granziol(2020)]{granziol2020flatness}
Diego Granziol.
\newblock Flatness is a false friend, 2020.

\bibitem[Granziol et~al.(2020)Granziol, Zohren, Roberts, and
  Lacoste-Julien]{granziol2020lr}
Diego Granziol, Stefan Zohren, Stephen Roberts, and Simon Lacoste-Julien.
\newblock Learning rates as a function of batch size: A random matrix theory
  approach to neural network training.
\newblock \emph{Journal of Machine Learning Research}, 1:\penalty0 1--48, 2020.

\bibitem[Hochreiter and Schmidhuber(1997)]{Hochreiter1997}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock {Flat Minima}.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 01 1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.1.1}.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.1.1}.

\bibitem[Hoffer et~al.(2018)Hoffer, Hubara, and Soudry]{hoffer2018train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks, 2018.

\bibitem[Hunter(2007)]{Hunter:2007}
J.~D. Hunter.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  90--95, 2007.
\newblock \doi{10.1109/MCSE.2007.55}.

\bibitem[Izmailov et~al.(2019)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2019swa}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization,
  2019.

\bibitem[Kaur et~al.(2022)Kaur, Cohen, and Lipton]{Kaur2022}
Simran Kaur, Jeremy Cohen, and Zachary~C Lipton.
\newblock On the maximum {H}essian eigenvalue and generalization.
\newblock 2022.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima, 2017.

\bibitem[Kingma and Ba(2017)]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS'12, page 1097–1105, Red
  Hook, NY, USA, 2012. Curran Associates Inc.

\bibitem[Lanczos(1950)]{Lanczos:1950zz}
Cornelius Lanczos.
\newblock {An iteration method for the solution of the eigenvalue problem of
  linear differential and integral operators}.
\newblock \emph{J. Res. Natl. Bur. Stand. B}, 45:\penalty0 255--282, 1950.
\newblock \doi{10.6028/jres.045.026}.

\bibitem[LeCun et~al.(1992)LeCun, Simard, and Pearlmutter]{lecun1992}
Yann LeCun, Patrice Simard, and Barak Pearlmutter.
\newblock Automatic learning rate maximization by on-line estimation of the
  {H}essian\textquotesingle s eigenvectors.
\newblock In S.~Hanson, J.~Cowan, and C.~Giles, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~5. Morgan-Kaufmann, 1992.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/1992/file/30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf}.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism, 2020.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets, 2018.

\bibitem[MacKay(1992)]{djcmThesis}
David~J.C. MacKay.
\newblock Bayesian methods for adaptive models, 1992.

\bibitem[Maddox et~al.(2020)Maddox, Benton, and Wilson]{maddox2020rethinking}
Wesley~J. Maddox, Gregory Benton, and Andrew~Gordon Wilson.
\newblock Rethinking parameter counting in deep models: Effective
  dimensionality revisited, 2020.

\bibitem[Nesterov(2014)]{Nesterov2013}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, Incorporated, 1 edition, 2014.
\newblock ISBN 1461346916.

\bibitem[Papyan(2019{\natexlab{a}})]{papyan2019measurements}
Vardan Papyan.
\newblock Measurements of three-level hierarchical structure in the outliers in
  the spectrum of deepnet {H}essians, 2019{\natexlab{a}}.

\bibitem[Papyan(2019{\natexlab{b}})]{papyan2019spectrum}
Vardan Papyan.
\newblock The full spectrum of deepnet {H}essians at scale: Dynamics with
  {S}{G}{D} training and sample size, 2019{\natexlab{b}}.

\bibitem[Papyan(2020)]{papyan2020traces}
Vardan Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (252):\penalty0 1--64, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-933.html}.

\bibitem[Pearlmutter(1994)]{pearlmutter}
Barak~A. Pearlmutter.
\newblock {Fast Exact Multiplication by the {H}essian}.
\newblock \emph{Neural Computation}, 6\penalty0 (1):\penalty0 147--160, 01
  1994.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1994.6.1.147}.
\newblock URL \url{https://doi.org/10.1162/neco.1994.6.1.147}.

\bibitem[Sagun et~al.(2017)Sagun, Bottou, and LeCun]{sagun2017eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the {H}essian in deep learning: Singularity and
  beyond, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2017.

\bibitem[Wang and Roberts(2023)]{sane}
L.~Wang and S.J. Roberts.
\newblock {S}{A}{N}{E}: the phases of gradient descent through sharpness aware
  number of effective pamareters.
\newblock 2023.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashionmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{M}{N}{I}{S}{T}: a novel image dataset for benchmarking
  machine learning algorithms, 2017.

\bibitem[Ye and Lim(2016)]{ye2016schubert}
Ke~Ye and Lek-Heng Lim.
\newblock Schubert varieties and distances between subspaces of different
  dimensions, 2016.

\end{thebibliography}
