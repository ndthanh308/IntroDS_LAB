@article{einstein,
    author =       "Albert Einstein",
    title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
        [{On} the electrodynamics of moving bodies]",
    journal =      "Annalen der Physik",
    volume =       "322",
    number =       "10",
    pages =        "891--921",
    year =         "1905",
    DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
}

@book{latexcompanion,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The \LaTeX\ Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts"
}

@misc{knuthwebsite,
    author    = "Donald Knuth",
    title     = "Knuth: Computers and Typesetting",
    url       = "http://www-cs-faculty.stanford.edu/\~{}uno/abcde.html"
}

@article{Li2022,
   abstract = {Recent findings (e.g., arXiv:2103.00065) demonstrate that modern neural
networks trained by full-batch gradient descent typically enter a regime called
Edge of Stability (EOS). In this regime, the sharpness, i.e., the maximum
{H}essian eigenvalue, first increases to the value 2/(step size) (the progressive
sharpening phase) and then oscillates around this value (the EOS phase). This
paper aims to analyze the GD dynamics and the sharpness along the optimization
trajectory. Our analysis naturally divides the GD trajectory into four phases
depending on the change of the sharpness. We empirically identify the norm of
output layer weight as an interesting indicator of sharpness dynamics. Based on
this empirical observation, we attempt to theoretically and empirically explain
the dynamics of various key quantities that lead to the change of sharpness in
each phase of EOS. Moreover, based on certain assumptions, we provide a
theoretical proof of the sharpness behavior in EOS regime in two-layer
fully-connected linear neural networks. We also discuss some other empirical
findings and the limitation of our theoretical results.},
   author = {Zhouzi Li and Zixuan Wang and Jian Li},
   doi = {10.48550/arxiv.2207.12678},
   month = {7},
   title = {Analyzing Sharpness along GD Trajectory: Progressive Sharpening and Edge of Stability},
   url = {https://arxiv.org/abs/2207.12678v2},
   year = {2022},
}

@misc{papyan2019measurements,
      title={Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet {H}essians}, 
      author={Vardan Papyan},
      year={2019},
      eprint={1901.08244},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{papyan2019spectrum,
      title={The Full Spectrum of Deepnet {H}essians at Scale: Dynamics with {S}{G}{D} Training and Sample Size}, 
      author={Vardan Papyan},
      year={2019},
      eprint={1811.07062},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{papyan2020traces,
  author  = {Vardan Papyan},
  title   = {Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {252},
  pages   = {1--64},
  url     = {http://jmlr.org/papers/v21/20-933.html}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{granziol2020lr,
   abstract = {We study the effect of mini-batching on the loss landscape of deep neural networks using spiked, field-dependent random matrix theory. We demonstrate that the magnitude of the extremal values of the batch {H}essian are larger than those of the empirical {H}essian. We also derive similar results for the Generalised Gauss-Newton matrix approximation of the {H}essian. As a consequence of our theorems we derive an analytical expressions for the maximal learning rates as a function of batch size, informing practical training regimens for both stochastic gradient descent (linear scaling) and adaptive algorithms, such as Adam (square root scaling), for smooth, non-convex deep neural networks. Whilst the linear scaling for stochastic gradient descent has been derived under more restrictive conditions, which we generalise, the square root scaling rule for adaptive optimisers is, to our knowledge, completely novel. We validate our claims on the VGG/WideResNet architectures on the CIFAR-100 and ImageNet datasets. Based on our investigations of the sub-sampled {H}essian we develop a stochastic Lanczos quadrature based on the fly learning rate and momentum learner, which avoids the need for expensive multiple evaluations for these key hyper-parameters and shows good preliminary results on the Pre-Residual Architecure for CIFAR-100.},
   author = {Diego Granziol and Stefan Zohren and Stephen Roberts and Simon Lacoste-Julien},
   journal = {Journal of Machine Learning Research},
   keywords = {Adam,Adaptive Optimization,Deep Learning Theory,Learning Rate Scaling,Loss Surfaces,Neural Network Training,Random Matrix Theory,Square root rule},
   pages = {1-48},
   title = {Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training},
   volume = {1},
   year = {2020},
}

@misc{granziol2020flatness,
      title={Flatness is a False Friend}, 
      author={Diego Granziol},
      year={2020},
      eprint={2006.09091},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Kaur2022,
   abstract = {The mechanisms by which certain training interventions, such as increasing learning rates and applying batch normalization, improve the generalization of deep networks remains a mystery. Prior works have speculated that "flatter" solutions generalize better than "sharper" solutions to unseen data, motivating several metrics for measuring flatness (particularly λmax, the largest eigenvalue of the {H}essian of the loss); and algorithms, such as Sharpness-Aware Minimization (SAM) [1], that directly optimize for flatness. Other works question the link between λmax and generalization. In this paper, we present findings that call λmax's influence on generalization further into question. We show that: (1) while larger learning rates reduce λmax for all batch sizes, generalization benefits sometimes vanish at larger batch sizes; (2) by scaling batch size and learning rate simultaneously, we can change λmax without affecting generalization; (3) while SAM produces smaller λmax for all batch sizes, generalization benefits (also) vanish with larger batch sizes; (4) for dropout, excessively high dropout probabilities can degrade generalization, even as they promote smaller λmax; and (5) while batch-normalization does not consistently produce smaller λmax, it nevertheless confers generalization benefits. While our experiments affirm the generalization benefits of large learning rates and SAM for minibatch SGD, the GD-SGD discrepancy demonstrates limits to λmax's ability to explain generalization in neural networks.},
   author = {Simran Kaur and Jeremy Cohen and Zachary C Lipton},
   title = {On the Maximum {H}essian Eigenvalue and Generalization},
   year = {2022},
}

@misc{cohen2022gradient,
      title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability}, 
      author={Jeremy M. Cohen and Simran Kaur and Yuanzhi Li and J. Zico Kolter and Ameet Talwalkar},
      year={2022},
      eprint={2103.00065},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Ghorbani2019,
   abstract = {To understand the dynamics of optimization in deep neural networks, we
develop a tool to study the evolution of the entire {H}essian spectrum throughout
the optimization process. Using this, we study a number of hypotheses
concerning smoothness, curvature, and sharpness in the deep learning
literature. We then thoroughly analyze a crucial structural feature of the
spectra: in non-batch normalized networks, we observe the rapid appearance of
large isolated eigenvalues in the spectrum, along with a surprising
concentration of the gradient in the corresponding eigenspaces. In batch
normalized networks, these two effects are almost absent. We characterize these
effects, and explain how they affect optimization speed through both theory and
experiments. As part of this work, we adapt advanced tools from numerical
linear algebra that allow scalable and accurate estimation of the entire
{H}essian spectrum of ImageNet-scale neural networks; this technique may be of
independent interest in other applications.},
   author = {Behrooz Ghorbani and Shankar Krishnan and Ying Xiao},
   doi = {10.48550/arxiv.1901.10159},
   isbn = {9781510886988},
   journal = {36th International Conference on Machine Learning, ICML 2019},
   month = {1},
   pages = {4039-4052},
   publisher = {International Machine Learning Society (IMLS)},
   title = {An Investigation into Neural Net Optimization via {H}essian Eigenvalue Density},
   volume = {2019-June},
   url = {https://arxiv.org/abs/1901.10159v1},
   year = {2019},
}

@misc{li2018visualizing,
      title={Visualizing the Loss Landscape of Neural Nets}, 
      author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
      year={2018},
      eprint={1712.09913},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Foret2020,
   abstract = {In today's heavily overparameterized models, the value of the training loss
provides few guarantees on model generalization ability. Indeed, optimizing
only the training loss value, as is commonly done, can easily lead to
suboptimal model quality. Motivated by prior work connecting the geometry of
the loss landscape and generalization, we introduce a novel, effective
procedure for instead simultaneously minimizing loss value and loss sharpness.
In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks
parameters that lie in neighborhoods having uniformly low loss; this
formulation results in a min-max optimization problem on which gradient descent
can be performed efficiently. We present empirical results showing that SAM
improves model generalization across a variety of benchmark datasets (e.g.,
CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel
state-of-the-art performance for several. Additionally, we find that SAM
natively provides robustness to label noise on par with that provided by
state-of-the-art procedures that specifically target learning with noisy
labels. We open source our code at
\url\{https://github.com/google-research/sam\}.},
   author = {Pierre Foret and Ariel Kleiner Google Research and Hossein Mobahi Google Research and Behnam Neyshabur Blueshift},
   doi = {10.48550/arxiv.2010.01412},
   month = {10},
   title = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
   url = {https://arxiv.org/abs/2010.01412v3},
   year = {2020},
}

@article{Strathern1997ImprovingRA,
  title={‘Improving ratings’: audit in the British University system},
  author={Strathern, Marilyn},
  journal={European Review},
  year={1997},
  volume={5},
  pages={305 - 321}
}

@thesis{djcmThesis,
title={Bayesian methods for adaptive models},
author={David J.C. MacKay},
year={1992}}

@misc{maddox2020rethinking,
      title={Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited}, 
      author={Wesley J. Maddox and Gregory Benton and Andrew Gordon Wilson},
      year={2020},
      eprint={2003.02139},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{keskar2017,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      eprint={1609.04836},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hoffer2018train,
      title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks}, 
      author={Elad Hoffer and Itay Hubara and Daniel Soudry},
      year={2018},
      eprint={1705.08741},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{pearlmutter,
    author = {Pearlmutter, Barak A.},
    title = "{Fast Exact Multiplication by the {H}essian}",
    journal = {Neural Computation},
    volume = {6},
    number = {1},
    pages = {147-160},
    year = {1994},
    month = {01},
    abstract = "{Just storing the {H}essian H (the matrix of second derivatives δ2E/δwiδwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv\\{f(w)\\} = (δ/δr)f(w + rv)|r=0, note that Rv\\{▽w\\} = Hv and Rv\\{w\\} = v, and then apply Rv\\{·\\} to the equations used to compute ▽w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full {H}essian.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1994.6.1.147},
    url = {https://doi.org/10.1162/neco.1994.6.1.147},
    eprint = {https://direct.mit.edu/neco/article-pdf/6/1/147/812672/neco.1994.6.1.147.pdf},
}


@Techreport{krizhevsky2009learning,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
 address = {Toronto, Ontario},
 institution = {University of Toronto},
 number = {0},
 publisher = {Technical report, University of Toronto},
 title = {Learning multiple layers of features from tiny images},
 year = {2009},
 title_with_no_special_chars = {Learning multiple layers of features from tiny images}
}

@misc{izmailov2019swa,
      title={Averaging Weights Leads to Wider Optima and Better Generalization}, 
      author={Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
      year={2019},
      eprint={1803.05407},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{
shwartz-ziv2019representation,
title={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},
author={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},
year={2019},
url={https://openreview.net/forum?id=SkeL6sCqK7},
}

@Inbook{Gull1989,
author="Gull, Stephen F.",
editor="Skilling, J.",
title="Developments in Maximum Entropy Data Analysis",
bookTitle="Maximum Entropy and Bayesian Methods: Cambridge, England, 1988",
year="1989",
publisher="Springer Netherlands",
address="Dordrecht",
pages="53--71",
abstract="The Bayesian derivation of ``Classic'' MaxEnt image processing (Skilling 1989a) shows that exp($\alpha$S(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of ``Classic'' MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter $\alpha$, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of structure -2$\alpha$S should be equal to the number of good singular values contained in the data. The performance of this new condition is discussed with reference to image deconvolution, but leads to a reconstruction that is visually disappointing. A deeper hypothesis space is proposed that overcomes these difficulties, by allowing for spatial correlations across the image.",
isbn="978-94-015-7860-8",
doi="10.1007/978-94-015-7860-8_4",
url="https://doi.org/10.1007/978-94-015-7860-8_4"
}

@techreport{spiegelhalter1998bayesian,
  title={Bayesian deviance, the effective number of parameters, and the comparison of arbitrarily complex models},
  author={Spiegelhalter, David J and Best, Nicola G and Carlin, Bradley P and Van der Linde, A},
  year={1998},
  institution={Citeseer}
}

@Article{Hunter:2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

@misc{smith2017cyclical,
      title={Cyclical Learning Rates for Training Neural Networks}, 
      author={Leslie N. Smith},
      year={2017},
      eprint={1506.01186},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lewkowycz2020large,
      title={The large learning rate phase of deep learning: the catapult mechanism}, 
      author={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},
      year={2020},
      eprint={2003.02218},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sagun2017eigenvalues,
      title={Eigenvalues of the {H}essian in Deep Learning: Singularity and Beyond}, 
      author={Levent Sagun and Leon Bottou and Yann LeCun},
      year={2017},
      eprint={1611.07476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Lanczos:1950zz,
    author = "Lanczos, Cornelius",
    title = "{An iteration method for the solution of the eigenvalue problem of linear differential and integral operators}",
    doi = "10.6028/jres.045.026",
    journal = "J. Res. Natl. Bur. Stand. B",
    volume = "45",
    pages = "255--282",
    year = "1950"
}

@misc{xiao2017fashionmnist,
      title={Fashion-{M}{N}{I}{S}{T}: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}, 
      author={Han Xiao and Kashif Rasul and Roland Vollgraf},
      year={2017},
      eprint={1708.07747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{moody1991,
 author = {Moody, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d64a340bcb633f536d56e51874281454-Paper.pdf},
 volume = {4},
 year = {1991}
}

@article{Hochreiter1997,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Flat Minima}",
    journal = {Neural Computation},
    volume = {9},
    number = {1},
    pages = {1-42},
    year = {1997},
    month = {01},
    abstract = "{We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a “flat” minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to “simple” networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a “good” weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and “optimal brain surgeon/optimal brain damage.”}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.1.1},
    url = {https://doi.org/10.1162/neco.1997.9.1.1},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/1/1/813385/neco.1997.9.1.1.pdf},
}




@article{Lim_2021,
	doi = {10.1098/rsta.2020.0209},
  
	url = {https://doi.org/10.1098%2Frsta.2020.0209},
  
	year = 2021,
	month = {feb},
  
	publisher = {The Royal Society},
  
	volume = {379},
  
	number = {2194},
  
	pages = {20200209},
  
	author = {Bryan Lim and Stefan Zohren},
  
	title = {Time-series forecasting with deep learning: a survey},
  
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}
}

@misc{simonyan2015vgg,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fort2020deep,
      title={Deep Ensembles: A Loss Landscape Perspective}, 
      author={Stanislav Fort and Huiyi Hu and Balaji Lakshminarayanan},
      year={2020},
      eprint={1912.02757},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{lecun1992,
 author = {LeCun, Yann and Simard, Patrice and Pearlmutter, Barak},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Automatic Learning Rate Maximization by On-Line Estimation of the {H}essian\textquotesingle s Eigenvectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf},
 volume = {5},
 year = {1992}
}

@misc{mahsereci2017early,
      title={Early Stopping without a Validation Set}, 
      author={Maren Mahsereci and Lukas Balles and Christoph Lassner and Philipp Hennig},
      year={2017},
      eprint={1703.09580},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nakkiran2019deep,
      title={Deep Double Descent: Where Bigger Models and More Data Hurt}, 
      author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
      year={2019},
      eprint={1912.02292},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Bottcher2022,
   abstract = {Analyzing geometric properties of high-dimensional loss functions, such as
local curvature and the existence of other optima around a certain point in
loss space, can help provide a better understanding of the interplay between
neural network structure, implementation attributes, and learning performance.
In this work, we combine concepts from high-dimensional probability and
differential geometry to study how curvature properties in lower-dimensional
loss representations depend on those in the original loss space. We show that
saddle points in the original space are rarely correctly identified as such in
lower-dimensional representations if random projections are used. In such
projections, the expected curvature in a lower-dimensional representation is
proportional to the mean curvature in the original loss space. Hence, the mean
curvature in the original loss space determines if saddle points appear, on
average, as either minima, maxima, or almost flat regions. We use the
connection between expected curvature and mean curvature (i.e., the normalized
{H}essian trace) to estimate the trace of {H}essians without calculating the
{H}essian or {H}essian-vector products as in Hutchinson's method. Because random
projections are not able to correctly identify saddle information, we propose
to study projections along {H}essian directions that are associated with the
largest and smallest principal curvatures. We connect our findings to the
ongoing debate on loss landscape flatness and generalizability. Finally, we
illustrate our method in numerical experiments on different image classifiers
with up to about $7\times 10^6$ parameters.},
   author = {Lucas Böttcher and Gregory Wheeler},
   doi = {10.48550/arxiv.2208.13219},
   keywords = {51M99,53Z50,dimensionality reduction,high-dimensional functions,loss visualization,principal curvature AMS subject classifications 68T07},
   month = {8},
   title = {Visualizing high-dimensional loss landscapes with {H}essian directions},
   url = {https://arxiv.org/abs/2208.13219v1},
   year = {2022},
}

@misc{jastrzębski2019relation,
      title={On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length}, 
      author={Stanisław Jastrzębski and Zachary Kenton and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},
      year={2019},
      eprint={1807.05031},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1097–1105},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@article{lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
} 

@article{relu,
author = {Fukushima, Kunihiko},
title = {Cognitron: A Self-Organizing Multilayered Neural Network},
year = {1975},
issue_date = {September 1975},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3–4},
issn = {0340-1200},
url = {https://doi.org/10.1007/BF00342633},
doi = {10.1007/BF00342633},
abstract = {A new hypothesis for the organization of synapses between neurons is proposed: "The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y". By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named "cognitron", is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a "teacher" which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
journal = {Biol. Cybern.},
month = {sep},
pages = {121–136},
numpages = {16}
}

@article{sane,
author = {Wang, L. and Roberts, S.J.},
title = {{S}{A}{N}{E}: the phases of gradient descent through Sharpness Aware Number of Effective pamareters},
year = {2023},
}


@misc{ye2016schubert,
      title={Schubert varieties and distances between subspaces of different dimensions}, 
      author={Ke Ye and Lek-Heng Lim},
      year={2016},
      eprint={1407.0900},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}


@book{Nesterov2013,
author = {Nesterov, Yurii},
title = {Introductory Lectures on Convex Optimization: A Basic Course},
year = {2014},
isbn = {1461346916},
publisher = {Springer Publishing Company, Incorporated},
edition = {1},
abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].}
}