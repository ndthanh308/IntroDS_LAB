%\documentclass[lettersize,journal]{IEEEtran}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}%algorithmic
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{soul}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
%\usepackage{hyperref}
\usepackage{wrapfig,lipsum}
\usepackage{subfigure}
\usepackage{balance}
\usepackage[english]{babel}
\usepackage{enumitem}
%for tabular
\usepackage{booktabs}
\usepackage{multirow}
% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\allowdisplaybreaks
\newcommand{\target}{\mathcal{T}}
\newcommand{\anom}{\mathcal{A}}
\newcommand{\threshold}{\Theta}
\newcommand{\simil}{\vec{s}}
\newcommand{\budget}{K}
\newcommand{\FA}{\widehat{A}}
\newcommand{\FR}{\widehat{R}}

%Annotation Color Defined-------------
%Usage: \red{xxx}, \blue{xxx}
\newcommand\blue[1]{\textcolor{blue}{#1}}
\newcommand\red[1]{\textcolor{red}{#1}}
%--------------------------------------
%ICDE2024: Submission due: July 28, 2023 (Friday)
%Research papers must not exceed 12 pages for the main text plus up to 4 pages for the bibliography. No appendix is allowed. 

%\title{Graph-guided Attacks against Random-Walk-based Anomaly Detection}
\title{Dual-Space Attacks against Random-Walk-based Anomaly Detection}
%\thanks{Identify applicable funding agency here. If none, delete this.}

% authors
\author{
\IEEEauthorblockN{Yuni Lai\IEEEauthorrefmark{1}, Marcin Waniek\IEEEauthorrefmark{2}, Yulin Zhu\IEEEauthorrefmark{1}, Liying Li\IEEEauthorrefmark{1}, Jingwen Wu\IEEEauthorrefmark{1}}
\IEEEauthorblockN{Tomasz P. Michalak\IEEEauthorrefmark{3}, Talal Rahwan\IEEEauthorrefmark{2}, Kai Zhou\IEEEauthorrefmark{1}}

\IEEEauthorblockA{csylai@comp.polyu.edu.hk, mjwaniek@gmail.com, yulin.zhu@polyu.edu.hk, kushr.li@connect.polyu.hk}
\IEEEauthorblockA{jingwen22.wu@connect.polyu.hk, tpm@mimuw.edu.pl, talal.rahwan@nyu.edu, kaizhou@polyu.edu.hk}

\IEEEauthorblockA{\IEEEauthorrefmark{1}\textit{Department of Computing}, \textit{The Hong Kong Polytechnic University}, HKSAR}
\IEEEauthorblockA{\IEEEauthorrefmark{2}\textit{Department of Computer Science}, \textit{New York University Abu Dhabi}, Abu Dhabi, UAE}
\IEEEauthorblockA{\IEEEauthorrefmark{3}\textit{Department of Computer Science}, \textit{University of Warsaw and IDEAS NCBR}, Poland}
}


\begin{document}
\pagestyle{plain}
\maketitle


\begin{abstract}
%*CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.

%Graph anomaly detection (GAD) plays an important role in safeguarding the system from attacks by detecting abnormal and deviant data. Random-walks-based anomaly detection (RWAD) is a widely-used technique for evaluating the anomaly scores of graph nodes across multiple domains. %Attackers have incentives to evade detection by manipulating the graph structure or the feature which indirectly impacts the graph. 

Random Walks-based Anomaly Detection (RWAD) is commonly used to identify anomalous patterns in various applications. An intriguing characteristic of RWAD is that the input graph can either be pre-existing or constructed from raw features. Consequently, there are two potential attack surfaces against RWAD: graph-space attacks and feature-space attacks. In this paper, we explore this vulnerability by designing practical dual-space attacks, investigating the interplay between graph-space and feature-space attacks. To this end, we conduct a thorough complexity analysis, proving that attacking RWAD is NP-hard. Then, we proceed to formulate the graph-space attack as a bi-level optimization problem and propose two strategies to solve it: alternative iteration (alterI-attack) or utilizing the closed-form solution of the random walk model (cf-attack). Finally, we utilize the results from the graph-space attacks as guidance to design more powerful feature-space attacks (i.e., graph-guided attacks). Comprehensive experiments demonstrate that our proposed attacks are effective in enabling the target nodes from RWAD with a limited attack budget. In addition, we conduct transfer attack experiments in a black-box setting, which show that our feature attack significantly decreases the anomaly scores of target nodes. Our study opens the door to studying the dual-space attack against graph anomaly detection in which the graph space relies on the feature space.



%Random-walks-based anomaly detection (RWAD)
%, a classical unsupervised graph anomaly detection (GAD) technique, 
%is widely used for safeguarding the system from attacks by detecting abnormal data.
%Although non-graph data is even more ubiquitous, proximity graphs can be constructed based on the similarity of feature data, making RWAD still applicable.
%In this scenario, however, the attackers can only manipulate the raw feature in practice, indirectly altering the virtual graph structure. Existing research has not explored adversarial attacks on RWAD, particularly the feature-space attack.
%In this paper, while we prove that the poisoning attacks against RWAD are NP-Hard due to the discrete nature of graphs, we exploit this vulnerability by designing practical \textit{dual}-space attacks, investigating the interplay between \textit{graph-space} and \textit{feature-space} attacks. Specifically, we formulate the graph-space attack as a bi-level optimization problem and propose two strategies to solve it: alternative iteration (alterI-attack) or utilizing the closed-form solution of the random walk model (cf-attack). Furthermore, we utilize the results from the graph-space attacks as guidance to design more powerful feature-space attacks (graph-guided attacks). 
%Comprehensive experiments on four datasets demonstrate that our proposed attacks are effective in shielding the target nodes from detection with a limited attack budget. In addition, we conduct transfer attack experiments in a black-box setting, which show that our feature attack significantly decreases the anomaly scores of target nodes. Our study opens the door to studying the dual-space attack against GAD in which the graph space relies on the feature space. 
\end{abstract}

\begin{IEEEkeywords}
Graph-based anomaly detection; Random walk; Poisoning attack; Adversarial attacks; Security and privacy.
\end{IEEEkeywords}



\section{Introduction}

%{\color{red} 1. Graph-based anomaly detection, in particular, RW-based anomaly detection}

Graph-based Anomaly Detection (GAD) has gained significant research attention in recent years due to the widespread use of graph data across various application domains. GAD algorithms are designed to identify anomalies in a graph, 
where nodes represent entities and edges indicate their relations. Essentially, a GAD algorithm works by initially measuring the similarities among nodes and then identifying nodes that are less similar to the rest as anomalous.
Random Walks (RWs), such as PageRank~\cite{page1999pagerank}, have emerged as a powerful tool for measuring node similarities over graphs and have become a fundamental component of many GAD systems that are extensively employed in diverse applications. For instance, \ul{R}andom-\ul{W}alk-based \ul{A}nomaly \ul{D}etection (RWAD) is used to detect money laundering in financial industry~\cite{oliveira2021guiltywalker}, fraudsters in online shopping~\cite{li2019trust}, and fake accounts in social network~\cite{jia2017random}.


%{\color{red} 2. Attackers have incentives to evade detection, so investigating the robustness of RW is important}


As the accuracy of predictions produced by the RWAD methods is crucial for system security, it is essential to assess their robustness in a real-world adversarial environment. In fact, the individuals that RWAD aims to detect may have both the incentive and capability to evade detection. 
For example, adversaries controlling bank accounts to be used in money laundering schemes may wish to remain undetected to continue their malicious activities. They could carefully manage the everyday transactions on the accounts to make them appear similar to normal ones, causing the system to falsely classify them as benign.
In essence, in an adversarial environment, attackers can intentionally manipulate the input data to RWAD in order to mislead its predictions, leading to what is known as \textit{data poisoning attacks} in the literature.  

%Unfortunately, while previous research has demonstrated the susceptibility of a wide range of machine-learning-based systems to such attacks \cite{goldblum2022dataset,fan2022survey,zhao2018data},  
%TM: I will work on the connection between these two sentences - it is a crucial part of our motivation
%there exists little systematic study on the adversarial robustness of RWAD.
%Therefore, in this paper, we aim to fill this gap by investigating how and to what extent attackers can exploit the \textit{unique} vulnerabilities of RWAD to evade detection.

%thoroughly investigating the adversarial robustness of widely-used RWAD by analyzing how and to what extent attackers can evade detection.


%{\color{red} 3. A unique feature of GAD is that graphs can be constructed. Thus, we have two attacking surfaces: graph-space attack and feature space attack. A limitation of previous work is that they do not consider feature space attack.}

% Figure environment removed


However, studying the adversarial robustness of RWAD imposes new challenges due to an intriguing characteristic of RWAD. Specifically, in an RWAD system,  the graph is often not directly accessible and needs to be \textit{constructed} from raw data. 
As illustrated in Fig~\ref{fig:RW_AnomalyDetection} (top), 
entities in the system are represented as vectors in a feature space, and a graph is then constructed based on the relationships among the entities \textit{as determined by their feature vectors}. For instance, a proximity graph can be constructed based on feature similarity. This graph is then fed into the RWAD system, which produces anomaly scores for each node in the graph. 

Consequently, there are \textit{two potential attack surfaces} against RWAD: \textbf{graph-space} attacks and \textbf{feature-space} attacks.
In graph-space attacks (Fig.~\ref{fig:RW_AnomalyDetection}, bottom), the attacker can directly modify the structure of the graph, which is a common assumption made by previous works \cite{zugner2018adversarial,zugner2019adversarial,zhu2022binarizedattack} that design structural attacks on graphs. In feature-space attacks (Fig.~\ref{fig:RW_AnomalyDetection}, middle), the attacker does not have direct control over the graph but can modify the features, which indirectly affects the graph's structure. It is worth noting that in the latter case, where the graph is not directly accessible,  feature-space attacks are deemed more realistic.

Unfortunately, previous research treats attacks in the graph space and feature space rather separately. On the one hand, many existing works have investigated \textit{structural attacks}~\cite{zugner2018adversarial,zhou2019attacking,zhou2020robust,zhu2022binarizedattack} against a wide range of graph learning models. On the other hand, another line of research has focused on studying feature manipulation attacks~\cite{goodfellow2014explaining,carlini2017towards,moosavi2017universal} primarily in the computer vision domain, where the data objects represented by features are independent of each other.
In contrast, one unique characteristic of RWAD is that it examines data objects that are interdependent (that is, objects are connected in a graph determined by their features), which makes the interplay between the graph-space and feature-space attacks possible. Thus, for the first time, we aim to investigate the adversarial robustness of RWAD under \textit{dual-space} attacks, with a focus on designing more powerful realistic feature-space attacks from the guidance of graph-space attacks.

Towards this end, we begin with a formal analysis of graph-space attacks. Specifically, we define the attacks in the graph space as a decision problem. We ask whether an attacker can reduce the anomaly scores of the target nodes below a certain threshold, thereby classifying them as benign, by modifying a limited number of edges in a given graph. Our in-depth complexity analysis shows that this problem is NP-hard for both \textit{directed} and \textit{undirected} graphs. Furthermore, since feature-space attacks ultimately modify edges, they can be viewed as special cases of this problem, and the hardness results remain applicable. The hardness results serve as the anchor for us to investigate efficient attack algorithms in both the graph space and feature space.

We then proceed to design effective graph-space attacks, which are formulated as an optimization problem with the objective of minimizing the target nodes' anomaly scores output by RWAD. Solving this optimization problem encounters several challenges.
Firstly, random walk (PageRank) is an iterative algorithm that operates on an input graph, thus any changes made to the graph will require the iterations to be re-executed. Consequently, attacks against RWAD will result in a \textit{bi-level} optimization where the inner layer involves complex iteration. Second, the discrete nature of graph structure further complicates the solving of the optimization. 
To address these challenges, we propose two efficient attacks:  \textbf{alterI}-attack and \textbf{cf}-attack. The former is an iterative approach that optimizes the attack objective by projected gradient descent (PGD)~\cite{wang2019attacking} and updates the random walk model alternatively. The latter utilizes the closed form of the random walk model to transform the bi-level optimization into a single-level problem.

Finally, we investigate the more realistic feature-space attacks. Our major innovation is to use the results from the \textit{virtual} graph-space attack as our guidance to design more powerful feature-space attacks. Specifically, we utilize the guidance from two aspects: selecting the attack nodes and formulating an effective attack objective. Through extensive experiments, we demonstrate that by fully exploring the dynamics between attacks in dual spaces, more powerful attacks could be designed, revealing more realistic security threats against RWAD systems.

The main contributions are summarized as follows:

\begin{itemize}
    \item We study the adversarial robustness of RWAD, for the first time, exploring the interplay between attacks in dual spaces.
    \item We present a deep theoretical analysis of the hardness of attacking RWAD, which is proved to be NP-hard on both directed and undirected graphs.
    \item We propose effective attacks in dual spaces. In particular, we innovatively utilize the results from the graph-space attacks as guidance to design more powerful feature-space attacks.
    \item We conduct comprehensive experiments to demonstrate the effectiveness of our proposed attacks. Especially we also transfer our attacks to other anomaly detection methods in the feature space. It is shown that our graph-guided feature-space attack remains effective even without knowing the target models, demonstrating a realistic threat in real-world application scenarios.
    %\item We conduct comprehensive experiments to evaluate our proposed attacks, which are demonstrated to be more effective than baseline attack methods.    
\end{itemize}

In summary, our work uncovers a unique vulnerability of RWAD and unleashes the power of attackers by exploring the interplay between attacks in dual spaces, significantly advancing our knowledge of the adversarial robustness of RWAD in deployment. 



%the alternative iteration attack (\textbf{alterI}-attack) and the closed-form attack (\textbf{cf}-attack). The former is adapted from \cite{wang2019attacking}, which was originally introduced for attacking belief propagation models. Specifically, this method converts the discrete graph adjacency matrix into the continuous domain, optimizing the attack objective by projected gradient descent (PGD) and updating the random walk model alternatively. Finally, it discretizes the optimized adjacency matrix to obtain the attacked graph. \blue{The latter utilizes the closed form of the random walk model to transform the bi-level optimization into a single-level problem.}

%have focused either on the analysis of graph-space attacks~\cite{zugner2018adversarial,zügner2018adversarial,zhu2022binarizedattack} or the attacks where node features and the graph structure can be modified simultaneously \cite{zugner2018adversarial,zügner2018adversarial}. 

%The first approach models an ideal scenario in which the attacker's ability to manipulate the topology of the graph is taken for granted. However, this does not have to be the case as options to modify data in a virtual space are often limited. For instance, Facebook users are allowed to sever existing connections \textit{to} their neighbours, but they cannot do the same with connections \textit{between} their neighbours. The second approach assumes that the attacker is able to manipulate not only the graph structure but also node features. Nevertheless, in those models, the graph structure pre-exists and does not depend on node features.

%{\color{red} 4. In this paper, we present ...}

%In contrast, this paper focuses on feature-space attacks in which the graph \textit{depends} on the features, and this indirectly impacts the graph's structure. We consider more realistic feature-space attacks, where attackers operate within the constraints imposed by the limited control over the graph's structure. While feature-space attacks are more practical, they are not devoid of theoretical insights. By examining the effects of graph-space attacks, which represent an extreme scenario, we gain valuable theoretical understanding of the system's vulnerabilities and attack vectors. This knowledge can guide the design of feature-space attacks.

%Furthermore, studying feature-space attacks enables us to explore the interplay between the graph structure and the node features. In many real-world scenarios, the graph is constructed based on the relationships among entities, as determined by their feature vectors. Therefore, by manipulating the features, an attacker can indirectly impact the resulting graph structure, potentially leading to successful evasion of the anomaly detection system. By investigating feature-space attacks, we uncover the complex dynamics between the feature space and the graph structure, shedding light on the underlying vulnerabilities of graph-based anomaly detection systems.

%In summary, while graph-space attacks have traditionally received more attention in the literature, our work emphasizes the significance of feature-space attacks in practical settings. By exploring attacks that manipulate node features and indirectly influence the graph's structure, we address the realistic adversarial scenarios encountered in graph-based anomaly detection systems. Moreover, the insights gained from studying graph-space attacks provide valuable theoretical foundations and guidance for the development of more practical and effective feature-space attacks. Through our comprehensive analysis, we aim to enhance the robustness and security of graph-based anomaly detection systems, enabling their deployment in real-world applications.


%{\color{red} 5. First, theoretical analysis}

%{\color{red} 6. Second, graph-space attack. Challenges and solutions}

%{\color{red} 7. Third, feature-space attack. Challenges and solutions}

%Similarly, in feature-space attacks, the difficulty lies in several areas. Firstly, the various feature constraints, such as discrete features and limited feature domains, pose a challenge. Additionally, the joint effect between feature space and graph space makes the attacks more complicated in practice. 
%Unlike previous feature attacks designed for attributed graphs, the graph structure remains fixed and does not change with the features. 
%To address the challenge of discrete features, we relax them to the continuous space during optimization and use PGD to keep the features within certain ranges. Lastly, we bridge the gap between graph space and feature space by introducing graph-guided practical feature attacks. Despite the NP-hard problem's difficulty, we demonstrate the effectiveness of our proposed attack framework by providing extensive experiments on four datasets.

\textbf{Road Map}: Related works (\ref{RelatedWork}) $\Rightarrow$ Target RWAD models (\ref{RW_AnomalyDetection}) $\Rightarrow$ Problem statements (\ref{ProblemSate}) $\Rightarrow$ Complexity analysis of attacks (\ref{Complexity}) $\Rightarrow$ Effective graph-space attacks (\ref{structural_attack}) $\Rightarrow$ Graph-guided feature-space attacks (\ref{FeatureAttack}) $\Rightarrow$ Evaluation (\ref{Experiments})  $\Rightarrow$ Conclusion (\ref{conclusion}).

%The paper is organized as follows for the remainder of the sections. In Section~\ref{RelatedWork}, we introduce the related work on GAD and  graph attacks. In Section~\ref{RW_AnomalyDetection}, We briefly describe the RW model for two types of
%graphs: directly accessible and indirectly accessible graphs. Then, we clarify our problem statement in Section~\ref{ProblemSate} and provide theoretical complexity analysis in Section~\ref{Complexity}. Our proposed practical graph-space attack is introduced in Section~\ref{structural_attack}. Further, we illustrate our graph-guided feature attack in Section~\ref{FeatureAttack}. Experimental results are demonstrated in Section~\ref{Experiments}. Finally, we conclude in Section~\ref{conclusion}.

% % Figure environment removed

%--- old version ---
%Anomaly detection plays an important role in safeguarding the system from attacks by detecting abnormal and deviant data. Random walks (RWs), a graph-based algorithm, are widely employed to serve as tools for evaluating the anomaly scores of nodes in the graph among various domains, such as 
%detecting money laundering in financial industry~\cite{oliveira2021guiltywalker}, fraud consumers in online shopping~\cite{li2019trust}, and fake accounts in social network~\cite{jia2017random}. Although non-graph-based data is even more ubiquitous, proximity graphs can be constructed based on the distance of the feature data~\cite{yao2012anomaly,cheng2009detection,ren2017piecewise, pang2016outlier, moonesinghe2006outlier}.
%One of the most classical RW algorithms, PageRank~\cite{page1999pagerank}, which was originally proposed to assess the importance of websites, can be regarded as similarity quantification between nodes. Further, the similarity serves as the anomaly score either directly or indirectly due to the fact that anomalies are usually dissimilar to others. Hence, PageRank is adopted as a general unsupervised anomaly detection tool~\cite{sun2005neighborhood,yao2012anomaly,cheng2009detection,ren2017piecewise,moonesinghe2006outlier,pang2016outlier,wang2018new,wang2019applying}. 

% RW-based anomaly detection systems, however, expose a new attacking vulnerability that the graph it relies on can be easily manipulated. For example, in online shopping, the abnormal user can easily decide which items to buy and hence reduce its anomaly score. That is, it is possible for adversarial attackers to manipulate the graph structure and evade the detection, which is termed \textit{structural attack}. Besides, it is also feasible for attackers to customize the node features to indirectly handle the proximity graph, which is named \textit{feature attack}.


% Thus, we study both structural attacks and feature attacks to reveal the vulnerability of the RW-based anomaly detection system. For attacks on graph data, previous work has studied the vulnerability of the Graph Neural Networks (GNNs)~\cite{wu2020comprehensive} model. For example, nettack~\cite{zugner2018adversarial}, and metattack~\cite{zügner2018adversarial} are two strong attacks proposed for GNNs model, and the latter is able to handle both structural and feature attacks. In addition to GNNs, there are also research efforts on the RW model, but they mainly focus on attacking the node embedding generated by RW~\cite{chang2022adversarial,bojchevski2019adversarial}. However, attack methods that apply to GNNs model or graph embedding cannot be directly applied to attacking the RW-based anomaly detection due to the distinct structure and rationale. Besides, to our knowledge, there is no existing feature attack on RW with proximity graphs. Lacking direct security analysis on RW anomaly detection, we aim to provide effective attacks on it, and comprehensively study the attacker’s ability to evade RW detection. In this paper, we instantiate our study by attacking the Random-Walk-based Anomaly Detection system (RWAD), which is the most representative model among RW anomaly detection. 

% However, attacking the RWAD encounters several challenges. First, 1) RWAD usually conducts in an unsupervised and inductive setting. Attacking RWAD thus falls under the category of poisoning attacks which modify the data before the model construction. This leads to a bi-level optimization of the attack problem which is challenging to solve in common sense. Second, 2) the discrete nature of graph data makes it even harder for optimization. By providing theoretical proof, we show that the problem is NP-hard even for a simple graph. Moreover, in feature attacks, there are various kinds of constraints for the variables (e.g., discrete features, limited ranges of variables). Last but not least, 3) when attacking the proximity graph, the attack problem does not only lie in the graph space but also in the original feature space used to construct the graph. This joint effect makes the attacks more complicated in practice. 

% To address these challenges, we employ several techniques. To solve the bi-level optimization 1), we employ the alternative iteration method proposed by \cite{wang2019attacking} which was originally introduced for attacking belief propagation models. To deal with the non-differentiable problem of the discrete data (e.g., graph adjacency matrix, discrete features) 2), we first relax the discrete variables to continuous space and discrete it at the end of the iteration. Furthermore, we employ projected gradient descent (PGD) to deal with the limited ranges constraints of variables. Most importantly, for proximity graph 3), we bridge the gap between graph space and feature space by introducing graph-guided practical feature attacks. Finally, despite the difficulty of the NP-hard problem, we demonstrate the effectiveness of our proposed attack framework by providing extensive experiments on 4 datasets.
%-------------------------------------------

\section{Related Works}
\label{RelatedWork}
\subsection{Graph-based anomaly detection}

This paper focus on unsupervised and node-level anomaly detection on plain and static graph. 
%Static graph anomaly detection can roughly be categorized into $4$ types. 
\textit{Random-walk-based techniques}~\cite{sun2005neighborhood,moonesinghe2006outlier,cheng2009detection,yao2012anomaly,pang2016outlier,you2017provable,wang2018new} discussed in this paper, exploiting random walk as a similarity or connectivity measurement.
Traditional \textit{feature-based} techniques \cite{akoglu2010oddball,ding2012intrusion,hooi2016fraudar} utilize statistical features, such as in and out node degrees, to extract structural information from graphs and transform the GAD to usual
anomaly detection problem. For example, OddBall ~\cite{akoglu2010oddball} built a regression model based on the density power law to estimate anomalous local patterns. These labor-intensive handcrafted features have limitations on generalizing to unknown anomalies. Beyond handcrafted features, \textit{network-representation-based} techniques, such as DeepWalk~\cite{perozzi2014deepwalk} and Node2Vec~\cite{grover2016node2vec}, are widely exploited to extract a more flexible feature representation which can be used for downstream anomaly detection tasks\cite{bandyopadhyay2020outlier}. Most recent work mainly focuses on investigating \textit{deep learning based} anomaly detection, such as DOMINANT \cite{ding2019deep} and GAL\cite{zhao2020error}. 


%{\color{red} \subsection{Combine attacks on graphs and attacks on random walk, emphasize how our work is different}}
\subsection{Adversarial attacks on graph}
Our work belongs to the category of targeted and poisoning adversarial attacks. Here, we include  the most related existing attacks on graphs. There are some previous research
efforts on the random walk (RW) based models. 
\cite{bojchevski2019adversarial} reformulate the DeepWalk model as a matrix factorization form to reduce the bi-level optimization to single-level, and then optimize the untargeted attack loss by optimizing the graph spectrum. \cite{chang2022adversarial} make further improvement to make the spectrum-based attack works in a black-box system. Different from our attacks on RW-based anomaly detection, they mainly focus on attacking
node embedding generated by RW.

In addition to RW-based model, Nettack~\cite{zugner2018adversarial}, Metattack~\cite{zügner2018adversarial} 
are two strong poisoning attacks for the GCN-based models. Nettack greedily selects the perturbation edges among the candidate sets with the largest gradient obtained by incremental updates. Metattack greedily selects the perturbation edges with the largest gradient obtained by meta-gradient. Note that, both of these methods can be extended to attack node features. However, Nettack does not introduce the attack node selection, and Metattack is only applicable to binary features. Furthermore, the proximity graph is different from other graphs. The proximity graph is changing along with features, while the node feature attack in \cite{zugner2018adversarial},\cite{zügner2018adversarial} have fixed graph structures. %For GCN models, \cite{ma2022adversarial} proposed a black-box attack by formulating it as an influence maximization problem.
For belief propagation models, \cite{wang2019attacking} introduced a poisoning attack for graph data. For another classical graph-based anomaly detection model called OddBall, \cite{zhu2022binarizedattack} proposed BinarizedAttack which is well-designed for the binary property of edges. For graph contrastive learning, \cite{zhang2022unsupervised} attack the graph embedding by greedily choosing the most informative edges. Beyond gradient-based methods, perturbing the intrinsic property of graphs, such as spectral changes \cite{lin2022graph} shows to be more effective, but it is only suitable for untargeted attacks. These works are orthogonal to our study.

%\section{Preliminary}
\section{Random-Walk-based Anomaly Detection}
\label{RW_AnomalyDetection}

% {\color{red}First, give an overview of RW-based detection (may refer to the diagram in Introduction). The central question you need to answer is what's the role of RW in detection. It is not clear from the description of the second target model.
% Then, based on the different types of graphs, introduce two representative target models.}

In this section, we introduce the necessary background on unsupervised random-walk-based anomaly detection (RWAD). We first present an overview of the framework with an emphasis on the role of random walk (RW) in anomaly detection, and then give two concrete exemplar RWAD models, which are also the target models considered in this paper.

%{\color{red} Re-organize as follows. This has two benefits: make it clear the role of random walk in anomaly detection; emphasize that random-walk-based is a framework.}

\subsection{Overview}
\subsubsection{Input data as a graph} In general, RWAD takes a \textit{plain} graph as input and produces anomaly scores for the nodes in the graph as output. In practice, the input graph could be either directly available or constructed from raw data. Depending on the levels of accessibility of the graph, we divide RWAD systems into two types: %{\color{red} the names may need to change. We divide RWAD into two types, but we are saying "Directly accessible graphs" and "Indirectly accessible graphs"}
\begin{itemize}
    \item RWAD over \textit{directly accessible graph} (\textit{Di-RWAD}):
    In this case, the input to RWAD is a graph that represents relational data in a specific application. For instance, %the graph could be a social network that represents the social interactions among a group of individuals. Another example is a \textit{bipartite graph} that models the customer ratings towards products on E-commerce platforms.
    in recommender systems, the rating towards products given be customers on E-commerce platforms can be modeled as a \textit{bipartite graph}.
    \item RWAD over \textit{indirectly accessible graph} (\textit{InDi-RWAD}): In this case, the input to RWAD are raw features of entities, and a graph is constructed as a data preprocessing step in the pipeline of anomaly detection (Fig.~\ref{fig:RW_AnomalyDetection}, top). Typically, given the feature vectors, a \textit{proximity graph} is constructed, where the nodes represent entities and an edge exists between two nodes only if they are similar enough
    in certain similarity metrics~\cite{yao2012anomaly,moonesinghe2006outlier, pang2016outlier, wang2018new}.
\end{itemize}
We note that in both cases, RWAD will operate on graphs; however, the difference lies in whether the graph is directly accessible. Later we will see that such a difference is crucial for determining the attacker's ability when designing attacks.

\subsubsection{RW as a similarity measurement}
The core of unsupervised anomaly detection is to identify data points that are significantly different from the rest of the population. RW has been shown to be an effective method for measuring the similarities of nodes in a graph. Specifically, given a graph $G=(V,E)$ with its adjacency matrix denoted as $W$, we define the transition matrix $P=(p_{ij})_{|V|\times|V|}$ as the column-normalized version of the adjacency matrix $W$, where $p_{ij}=w_{ij}/\sum_{t=1}^{|V|} w_{i,t}$. If vertex $i$ has no outgoing edges (i.e., $\sum_{t=1}^{k+n} w_{i,t}=0$), we set the transition probability to 0. The widely used Page-Rank algorithm with restart can be represented as follows:
\begin{equation}
\label{eqn:RW_general}
    \vec{s}= (1-\alpha) P \vec{s}+ \alpha \vec{r},
\end{equation}

where $\alpha$ is the restart rate, a hyper-parameter that controls the probability of restart; the vector $\vec{r}$ specifies the restart strategy, and $\vec{s}$ characterizes the node similarities. With the similarity, the anomaly score of a node is calculated as the opposite of its average similarity to all other nodes, or the average similarity among its neighbors. Next, we present two representative models to instantiate the \textit{Di-RWAD} and \textit{InDi-RWAD} systems.
%If $\vec{r}=\frac{1}{n}$ means it restarts from any node with equal probability. Let $\vec{s}(v)$ denote the element corresponding to node $v$ in $\vec{s}$, and in this case, $\vec{s}(v)$ quantifies the average similarity between node $v$ to all other nodes. If $\vec{r}=\vec{e}_{v}$, where $\vec{e}_{v}$ is a vector with zeros element except node $v$, which means that it always restarts from node $v$. In this case, $\vec{s}(u)$ quantifies the similarity between node $v$ to node $u$. Based on the node's similarity, the anomaly score of a node is the opposite of its average similarity to all other nodes, or the average similarity among their neighbors.
%Next, we introduce two main representative models: RWAD on bipartite graphs and proximity graphs.

%\red{explain how to use random walk to derive similarity scores}

%Unsupervised anomaly detection, also known as outlier detection, aims to identify data points that are dissimilar from the majority. With the prevalence of graph data, random walk has become a crucial tool in graph mining. It contributes to anomaly detection tasks by measuring the similarity of nodes. Nodes that have lower similarity to others are predicted as anomalies. The advantage of RWAD is its simplicity and effectiveness on plain graphs. There are two main representative models: RWAD on bipartite graphs and proximity graphs. Next, we illustrate these two models. 

\subsection{Representative Target Models}

%\red{Using bi-RWAD and pro-RWAD is not good. It's easy to confuse with Di-RWAD and InDi-RWAD}

\subsubsection{Di-RWAD}
\label{target1} 
% \red{this are redundant information} \st{Bipartite graphs are graphs whose vertices can be divided into two disjoint sets, such that every edge connects a vertex in one set to a vertex in the other set. It is very common in many applications. For example, review and rating data is ubiquitous in e-commerce platforms, in which an edge exit if a user gives a review to an item. Normal users have their own preferences for the items they buy, while anomalous users might not have such a pattern. 
% Another example is research publication platforms, where researchers publish their papers in journals, and these relationships exist between two different kinds of objects: authors and journals~\cite{sun2005neighborhood}. 
% Researchers are likely to publish papers in similar journals within the same or related fields, while anomalous users tend to publish papers in various unrelated fields. %The hypothesis is that a normal author publishes papers in similar journals (in the same/related field). 
% As a result, random walks (RW), such as PageRank, can serve as anomaly detection for this type of data because they can quantify node similarity in graphs. }

We consider bipartite graphs as a representative example of directly accessible graphs. We next describe how to apply the RWAD algorithm to the bipartite graphs of this kind, which we term as \textit{BiGraphRW} model.%In the following, we define the notation and briefly explain the \textit{BiGraphRW} model. 

To begin, we define a bipartite graph $G=(U\cup V,E)$ as a graph with two disjoint sets of vertices $U=\{u_i|1\leq i\leq k\}$ and $V=\{v_i|1\leq i\leq n\}$, and a set of edges $E\subseteq U\times V$ that connect the vertices in $U$ to the vertices in $V$. We represent the edges in $E$ as a binary edge matrix $M=(m_{ij}){k\times n}$, where $m_{ij}=1$ if $\langle i,j\rangle\in E$, and $m_{ij}=0$ otherwise. Then, the adjacency matrix for a bipartite graph can be constructed as $W=(w_{ij})_{(k+n)\times(k+n)}=\begin{pmatrix}0 & M \\M^{T} & 0\end{pmatrix}$.  

For each node $u\in U$, \textit{BiGraphRW} applies Eqn~\ref{eqn:RW_general} with $\vec{r}=\vec{e}_{u}$, where $\vec{e}_{u}$ is a vector with zeros element except node $u$, which means that it always restarts from node $u$. The resulting vector $\vec{s}_{u}=(1-\alpha) P \vec{s}_u+ \alpha \vec{e}_{u}$ represents the connectivity scores of node pairs $\{ \left \langle u,t\right \rangle| t \in U\cup V$\}, which quantifies the similarity between node $u$ and others. By assumption, a node $v$ tends to have a lower mean similarity score among its neighbors if it is anomalous. We denote the average neighbor similarity as $\bar{S}_{v}$:
%If node $v$ has $|\mathcal{N}(v)|$ one-hop neighbors, its mean similarity score, denoted by $\bar{S}_{v}$, is the average of $|\mathcal{N}(v)| \times |\mathcal{N}(v)|$ similarity matrix except the diagonal elements:
\begin{equation}
    \bar{S}_{v}=\frac{ \sum_{i=1}^k M_{iv} \sum_{j=1, i \neq j}^k M_{jv}\vec s_{u_{i}}(u_{j}) }{\sum_{i=1}^k M_{iv} \sum_{j=1,i \neq j}^k M_{jv}},
\end{equation}
where $\vec s_{u_{i}}(u_{j})$ represent the element corresponding to node $u_{j}$ in $\vec{s_{u_{i}}}$, which is the similarity between node $u_{i}$ and $u_{j}$. Anomaly score of node $v$ is in contract to the mean similarity score $\bar{S}_{v}$, so we denoted it by 
\begin{equation}
\label{eqn:AS_bp}
    \mathcal{A}(v)=1-\bar{S}_{v}= 
    \begin{cases} 
        \text{anomaly,}  & \text{if }\mathcal{A}(v)  \geq \theta,\\
        \text{normal node,} & \text{if }\mathcal{A}(v) < \theta,
    \end{cases}
\end{equation}
where the parameter $\theta$ is a given and fixed threshold of the anomaly detection model. 


\subsubsection{InDi-RWAD}
\label{target2}
\par 
A representative way to apply RWAD to non-graph data is by constructing a proximity graph. We call this variant as \textit{ProxGraphRW} model. In this approach, the input feature data is represented as $\mathbf{X}=\{\mathbf{x}_1, \mathbf{x}_2, \cdots,\mathbf{x}_n\}$, $\mathbf{x}_{i} \in \mathbb{R}^{d}$.
% \cite{yao2012anomaly,moonesinghe2006outlier, pang2016outlier, wang2018new} proposed random-walk-based anomaly detection frameworks on proximity graphs in which the graphs are induced from non-graphical data. 
% Similar schemes were employed by \cite{cheng2009detection} and \cite{ren2017piecewise} to characterize anomaly detection on multivariate time series data. 
% These frameworks could be generalized and formulated as bellows.
The first step is to construct a proximity graph according to the similarity or distance measurement between each pair of samples. To construct a proximity graph $G=\left( V,E\right)$, the vertices $V$ represent data samples $\mathbf{x}_1, \mathbf{x}_2, \cdots ,\mathbf{x}_n$, and the edges imply the similarity among vertices. This can be achieved through similarity measures, such as Euclidean distance, cosine similarity, or correlation coefficient. We denote the similarity function between $\mathbf{x}_i$ and $\mathbf{x}_j$ as $sim(\mathbf{x}_i,\mathbf{x}_j)$. Then, proximity graphs can be constructed by different rules. %such as $k$NN-Graph \cite{wang2018new}, $\epsilon$-Graph~\cite{moonesinghe2006outlier,yao2012anomaly}, and Fully-Connected-Graph~\cite{cheng2009detection,pang2016outlier,ren2017piecewise}. 
%To name a few, $k$NN - Graph \cite{wang2018new}: for every data sample $\mathbf{x}_{i}$, it has edges connecting to its $k$ neighbors; % Fully-Connected-Graph with weights \cite{cheng2009detection,pang2016outlier,ren2017piecewise}: when the size of the graph is small, the normalized similarity matrix can be regarded as a fully connected graph with weights of each edge are the similarity between data samples. 
In this paper, we take $\epsilon$-Graph~\cite{moonesinghe2006outlier,yao2012anomaly} as an example, where for every data sample $\mathbf{x}_{i}$, an edge is connected to $\mathbf{x}_{j}$ if $sim\left(\mathbf{x}_{i},\mathbf{x}_{j}\right) > \epsilon$. 
We define the weighted adjacency matrix as $W=(w_{ij})_{n\times n}$, where $w_{ij}=sim(\mathbf{x}_i,\mathbf{x}_j)\cdot\mathbb{I}(sim(\mathbf{x}_i,\mathbf{x}_j)>\epsilon)$, and $\mathbb{I}(\cdot)$ is an indicator function. With the proximity graph constructed, \textit{ProxGraphRW} applies the Eqn~\ref{eqn:RW_general} with $\vec{r}=\frac{1}{n}$, which means that the RW restart from any node with equal probability. The resulting vector $\vec{s}= (1-\alpha) P \vec{s}+\frac{\alpha}{n}$ contains the connectivity scores of all nodes, where each element $\vec{s}(v)$ quantifies the overall similarity of node $v$ to all other nodes. Finally, based on the hypothesis that anomalies have low connectivity to most others, 
%$\vec{s}$ is sorted in ascending order, and the top few points are regarded as anomalies~\cite{ren2017piecewise}. 
the anomaly score of node $v$ is
\begin{equation}
\label{eqn:AS_px}
    \mathcal{A}(v) = 1- \vec{s}(v),
\end{equation}
where $\vec{s}(v)$ is the element corresponding to node $v$ in $\vec{s}$.


%\section{System and Threat Model}
\section{Problem Statements}
\label{ProblemSate}
In this section, we introduce the adversarial environment that random-walk-based anomaly detection (RWAD) operates in, and then formally define the attack problem.

\subsection{System and Threat Model}
We consider a system consisting of two parties: an analyst who runs an RWAD algorithm to detect potential anomalies and an attacker who aims to evade the detection. In practice, the analyst would first collect data from the environment and construct a graph, which is fed into the RWAD system for anomaly detection. However, the attacker could tamper with the data collection process which will result in a poisoned graph, leading to the malfunction of the system. For instance, in online shopping platforms, the attacker may manipulate some users to provide fake ratings for target items. The resulting poisoned data can lead to biased recommendations from the recommender system.

We further introduce the threat model by specifying the attacker’s knowledge, goal, and capability. By Kerckhoffs’s principle, we assume a worst-case scenario where the attacker knows all the data as well as the anomaly detection model, which is a common assumption employed by many previous attacks~\cite{zhu2022binarizedattack,anelli2021adversarial}. 
We assume that the attacker has a set of target nodes in mind. Initially, the target nodes would have been determined as abnormal by the RWAD system if no data was manipulated. The attacker then tries to decrease the anomaly scores of those target nodes in the hope that they would evade the detection. To this end, the attacker can manipulate the data constrained by a certain budget. Specifically, depending on whether the graph is directly accessible or not, we divide the attacks into two types:
\begin{itemize}
    \item \textit{Graph-space} attack: the attacker can directly modify the structure of the graph by adding and deleting the edges under a budget constraint $K$.
    %alters the graph's structure by adding or deleting $K$ edges, with the goal of enabling the target nodes to evade detection.
    \item \textit{Feature-space} attack: the attacker can only modify the features of a set of attack nodes, which will indirectly cause changes in the graph structure. Considering a practical scenario that the targeted anomaly nodes are crafted to have specific malicious functions, we can not modify their features arbitrarily. Therefore, an indirect feature attack, aiming to decrease the anomaly scores of target nodes while keeping their features unchanged, is ideal for such a problem. Hence, we restrict the selection of attack nodes to those other than the target nodes.
    %the attacker manipulates the features of $K'$ nodes, with the aim of indirectly enabling the target nodes to evade detection.
\end{itemize}


%Generally, our goal is to set up a targeted and poisoning attack with structural or feature manipulation. Take online review platforms as an example, the attacker might manufacture some malicious users to interrupt the recommender system. However, to ensure the attack power, these nodes work with a prerequisite that they can evade detection. This can be achieved by graph structural attack by manipulating the graph, such as adding/deleting ratings among users and items. However, in the proximity graph model, manipulating features instead of the graph might be more realistic since the graph is constructed based on the feature. Considering a practical scenario that the anomaly nodes are crafted to have specific malicious functions, they are not flexible to be unnoticeable at the same time. An indirect feature attack, aiming to decrease the anomaly scores of target nodes while keeping the target node features unchanged, is ideal for such a problem. By manipulating the feature on which the graph is constructed, the attacker can indirectly interrupt the graph and the anomaly detection results. We further describe the 
%threat model setting, including the attacker's goal, knowledge, and capacity as follows.

%\subsubsection{Attacker's goal} The attackers aim to make the target nodes (originally anomalous) in the graph, denoted as $\mathcal{T}$, have lower anomaly scores to evade the detection.

%\subsubsection{Attacker's knowledge} According to Kerckhoffs’s principle, we assume a worst-case scenario that the attacker knows all the data (i.e., graph) and the anomaly detection model. 

%\subsubsection{Attacker's ability}

%\paragraph{Structural Attack}
%To constrain the ability of the attacker and also the attack cost, we assume that the attacker can arbitrarily modify up to $K$ edges.

%\paragraph{Feature Attack}
%Similarly, we assume that the attacker can select up to $K'$ attack nodes and modify their features subjected to the feature constraint $\mathcal{X}$. Target nodes are excluded in the selection of attack nodes to keep their well-designed malicious functions. 

\subsection{Problem definition}
To facilitate our theoretical analysis, we formally define the attacks against RWAD as follows.


\begin{definition}[\textbf{PA-RWAD}: poisoning attacks against RWAD]
\label{def:decision-problem}
An instance of the problem is defined by a tuple, $(G,\target,\anom,\threshold,\budget,\FA,\FR)$, where $G=(V,E)$ is a network, $\target \subseteq V$ is the set of targets, $\anom: \mathbb{G} \times V \rightarrow \mathbb{R}$ is the anomaly score function, $\threshold \in \mathbb{N}$ is the safety threshold, $\budget \in \mathbb{N}$ is the budget specifying the maximum number of edges that can be added or removed, $\FA \subseteq (V \times V) \setminus E$ is the set of edges that can be added, and $\FR \subseteq E$ is the set of edges that can be removed. The goal is then to identify two sets, $A^* \subseteq \FA$ and $R^* \subseteq \FR$, such that $|A^*|+|R^*| \leq K$, and for $G^*=(V, (E \cup A^*) \setminus R^*)$ we have:
\[
\left|\left\{ v_i \in V : \forall_{v_j \in \target} \anom(G^*,v_i) > \anom(G^*,v_j)  \right\}\right| \geq \threshold.
\]
\end{definition}
In practice, the top-$\threshold$ nodes ranked by their anomaly scores in descending order are determined as anomalous.
Then, the goal of \textbf{PA-RWAD} is to find a way of modifying the network by adding and removing edges, so that there are at least $\threshold$ nodes with anomaly scores greater than any of the target nodes. In other words, the target nodes are considered as benign.

We note that although \textbf{PA-RWAD} emphasizes modifying the structure of the graph, a feature-space attack is still an instance of \textbf{PA-RWAD}, since the modification of features will ultimately lead to the changes of the graph.

%{\color{red} Both graph structural attack and feature attack are special cases of Problem 1}
%\blue{I think that the feature attack problem is different from graph attack. for graph, the constraint is the limited number of edges we can modify, while feature attack is the number of nodes the attacker can control. The latter one does not include the edge limitation.} {\color{red} we can somehow add a very loose edge budget to feature attack, for the sake of unnoticeable attack.}

\section{Complexity Analysis}
\label{Complexity}
% {\color{red} I am thinking of removing definition 2 and Theorem 3. Because they are not quite relevant, and Theorem 3 only proves the results on directed graphs}
 


%\subsection{Hardness Results}

%We first define the decision version (Definition~\ref{def:decision-problem}) and the optimization version (Definition~\ref{def:optimization-problem}) of the computational problem of attacking anomaly detection based on random walks. 
We now proceed to analyze the computational complexity of the attacks against RWAD. We summarize the hardness results in Table~\ref{table-hard}.
\vspace{-8pt}
\begin{table}[!h]
\caption{Hardness results of \textbf{PA-RWAD}.}
\label{table-hard}
\centering
\begin{tabular}{|c|c|c|}
\hline
                 & Directed graph                                                          & Undirected graph                                             \\ \hline
\textbf{PA-RWAD} & \begin{tabular}[c]{@{}c@{}}NP-hard\\ (\textbf{Lemma 1} \& \textbf{Theom. 1})\end{tabular} & \begin{tabular}[c]{@{}c@{}}NP-hard\\ (\textbf{Theom. 2})\end{tabular} \\ \hline
\end{tabular}
\end{table}
%Under the Definition~\ref{def:decision-problem}, we show that the structural attack is NP-hard.


% \begin{definition}[Random Walk Anomaly Detection Minimal Attack]
% \label{def:optimization-problem}
% An instance of the problem is defined by a tuple, $(G,\target,\anom,\threshold,\FA,\FR)$, where $G=(V,E)$ is a network, $\target \subseteq V$ is the set of targets, $\anom: \mathbb{G} \times V \rightarrow \mathbb{R}$ is the anomaly score function, $\threshold \in \mathbb{N}$ is the safety threshold, $\FA \subseteq (V \times V) \setminus E$ is the set of edges that can be added, and $\FR \subseteq E$ is the set of edges that can be removed. The goal is then to identify two sets, $A^* \subseteq \FA$ and $R^* \subseteq \FR$, such that $|A^*|+|R^*|$ is minimal, and for $G^*=(V, (E \cup A^*) \setminus R^*)$ we have:
% \[
% \left|\left\{ v_i \in V : \forall_{v_j \in \target} \anom(G^*,v_i) > \anom(G^*,v_j)  \right\}\right| \geq \threshold.
% \]
% \end{definition}
% In other words, the goal is to find a way of satisfying the safety threshold $\threshold$ while performing the minimal possible number of network modifications.


\begin{theorem}
\label{thrm:decision-directed}
The \textbf{PA-RWAD}  problem is NP-hard given a directed graph.
\end{theorem}

\begin{proof}
We will prove that the problem is NP-hard by showing a reduction from the NP-complete \textit{3-Set Cover} problem. An instance of this problem is defined by a collection of subsets  $Q=\{Q_1, \ldots, Q_{|Q|}\}$ of the universe $U=\{u_1, \ldots, u_{|U|}\}=\bigcup_{Q_i \in Q}Q_i$ such that $\forall_i |Q_i|=3$, and a number $k \in \mathbb{N}$. The goal is to determine whether there exist at most $k$ elements of $Q$ that cover the entire universe, i.e., $Q^* \subseteq Q$ such that $|Q^*| \leq k$ and $U = \bigcup_{Q_i \in Q^*}Q_i$.

Let $(Q,k)$ be a given instance of the 3-Set Cover problem. We will now construct an instance of the \textbf{PA-RWAD} problem. In what follows, let $Q(u_i)$ be the subsets in $Q$ that contain $u_i$, i.e.,  $Q(u_i)=\{Q_j \in Q : u_i \in Q_j\}$. Let us also assume that $|Q| \geq 4$, as all smaller instances can be easily solved in constant time. First, we construct a directed network $G_Q=(V,E)$, where:
\begin{itemize}
\item $V = U \cup \bigcup_{Q_i \in Q} \{Q_i,q_i,o_i\} \cup \{h_1,h_2,h_3\} \cup \bigcup_{u_i \in U} \bigcup_{j=1}^{|Q|-|Q(u_i)|} \{x_{i,j},y_{i,j},z_{i,j}\},$
\vspace{3pt}
\item $E = \bigcup_{u_i \in Q_j} \{(Q_j,u_i)\} \cup \bigcup_{o_i \in V} \bigcup_{h_j \in V} \{(o_i,h_j)\} \cup \bigcup_{x_{i,j} \in V} \{(x_{i,j},u_i),(x_{i,j},y_{i,j}),(x_{i,j},z_{i,j})\}.$
\end{itemize}
%$|Q|=|q|=|o|$, $|h|=3$, and $|x|=|y|=|z|$.
An example of this construction (e.g., $|U|=5$, $|Q|=3$) is presented in Fig.~\ref{fig:decision-directed}. 
% Figure environment removed
Now, consider the instance $(G_Q,\target,\anom,\threshold,\budget,\FA,\FR)$ of the \textbf{PA-RWAD} problem, where:
\begin{itemize}
\item $G_Q$ is the network we just constructed,
\item $\target = U$ is the target set,
\item $\anom$ is the anomaly score function with the restart rate parameter $\alpha=\frac{1}{|Q|}$,
\item $\threshold=n-|U|$ is the safety threshold,
\item $\budget=k$ is the budget,
\item $\FA = \bigcup_{Q_i \in Q} \{(q_i,Q_i)\}$, i.e., only edges from $q_i$ to corresponding $Q_i$ can be added,
\item $\FR = \emptyset$, i.e., none of the edges can be removed.
\end{itemize}

Since $\FR = \emptyset$, for any solution to the constructed instance of the \textbf{PA-RWAD} problem, we must have $R^* = \emptyset$. Hence, we will omit the mentions of $R^*$ in the remainder of the proof, and we will assume that a solution consists just of $A^*$. We next prove a useful lemma.

% \noindent\fbox{%
% \parbox{0.47\textwidth}{%
\begin{lemma}
\label{lem:directed-construction}
Let $A \subseteq \bigcup_{Q_i \in Q} \{(q_i,Q_i)\}$, and let $G_Q \cup A = (V,E \cup A)$. We have that:
\[
\forall_{u_i \in U} \forall_{v \notin U} \anom(G_Q \cup A,v) > \anom(G_Q \cup A,u_i)
\]
if and only if $\forall_{u_i \in U} \exists_{(q_j,Q_j) \in A} u_i \in Q_j.$
\end{lemma}


\begin{proof}
From the formula of the anomaly score function, we have that $\anom(G_Q \cup A,v_i) = 1 - \simil(G_Q \cup A,v_i)$, where:
\[
\simil(G_Q \cup A,v_i) = \frac{\alpha}{n} + (1-\alpha) \textstyle\sum_{v_j \in V} \simil(G_Q \cup A,v_j) P_{j,i}.
\]
Therefore, we have that $\anom(G_Q \cup A,v_i) > \anom(G_Q \cup A,v_j)$ if and only if $\simil(G_Q \cup A,v_i) < \simil(G_Q \cup A,v_j)$. Let $A(u_i)$ be the set of $Q_j$ containing $u_i$ that got connected to the corresponding node $q_j$ via the edges in $A$, i.e., $A(u_i) = \{Q_j \in Q: u_i \in Q_j \land (q_j,Q_j) \in A \}$. We now compute the values of $\simil(G_Q \cup A,v_i)$ for all nodes in $V$:
\begin{itemize}
\item $\simil(G_Q \cup A,q_i) = \simil(G_Q \cup A,x_{i,j}) = \simil(G_Q \cup A,o_i) = \frac{\alpha}{n} = \frac{1}{|Q|n}$, as nodes $q_i$, $x_{i,j}$, and $o_i$ do not have any predecessors,
\item $\simil(G_Q \cup A,y_{i,j}) = \simil(G_Q \cup A,z_{i,j}) = \frac{\alpha}{n} + (1-\alpha) \simil(G_Q \cup A,x_{i,j}) \frac{1}{3} = \frac{\alpha}{n} + \frac{(1-\alpha)\alpha}{3n} = \frac{(4-\alpha)\alpha}{3n} = \frac{\left(4-\frac{1}{|Q|}\right)}{3|Q|n}$, as the only predecessor of nodes $y_{i,j}$ and $z_{i,j}$ is the node $x_{i,j}$ with out-degree $3$,
\item $\simil(G_Q \cup A,h_i) = \frac{\alpha}{n} + (1-\alpha) \sum_{o_j \in V} \simil(G_Q \cup A,o_j) \frac{1}{3} = \frac{\alpha}{n} + \frac{|Q|(1-\alpha)\alpha}{3n} = \frac{((1-\alpha)|Q|+3)\alpha}{3n} = \frac{|Q|+2}{3|Q|n}$, as the predecessors of $h_i$ are all $|Q|$ nodes $o_j$, each with out-degree $3$,
\item if $(q_i,Q_i) \notin A$ then $\simil(G_Q \cup A,Q_i) = \frac{\alpha}{n} = \frac{1}{|Q|n}$, as such node $Q_i$ has no predecessors,
\item if $(q_i,Q_i) \in A$ then $\simil(G_Q \cup A,Q_i) = \frac{\alpha}{n} + (1-\alpha)\simil(G_Q \cup A,q_i) = \frac{\alpha}{n} + (1-\alpha)\frac{\alpha}{n} = \frac{2|Q|-1}{|Q|^2 n}$, as the only predecessor of such node $Q_i$ is the node $q_i$,
\item $\simil(G_Q \cup A,u_i) = \frac{\alpha}{n} + (1-\alpha) \sum_{Q_j \in Q(u_i)} \simil(G,Q_j) \frac{1}{3} + (1-\alpha) \sum_{x_{i,j} \in V} \simil(G,x_{i,j}) \frac{1}{3} = \frac{\alpha}{n} + \frac{|Q|(1-\alpha)\alpha}{3n} + |A(u_i)|\frac{(1-\alpha)^2\alpha}{3n} = \frac{((1-\alpha)|Q|+3+|A(u_i)|(1-\alpha)^2)\alpha}{3n} = \frac{|Q|+2+|A(u_i)|\left(1-\frac{1}{|Q|}\right)^2}{3|Q|n}$, as the predecessors of $u_i$ are $|Q(u_i)|$ nodes $Q_j$, as well as $|Q|-|Q(u_i)|$ nodes $x_{i,j}$, each with out-degree $3$.
\end{itemize}

We now prove the main equivalence of the lemma. Assume that $\forall_{u_i \in U} \forall_{v \notin U} \anom(G_Q \cup A,v) > \anom(G_Q \cup A,u_i)$. In particular, it implies that: $
\forall_{u_i \in U}\, \simil(G_Q \cup A,u_i) - \simil(G_Q \cup A,h_1) > 0.
$
By substituting the values in the inequality, we get:
\[
\forall_{u_i \in U}\, \frac{|A(u_i)|\left(1-\frac{1}{|Q|}\right)^2}{3|Q|n} > 0,
\]
which in turn implies that $\forall_{u_i \in U} |A(u_i)| > 0$. Hence, we have that for every $u_i \in U$ there exists at least one $Q_j$ such that $u_i \in Q_j$ and $(q_j,Q_j) \in A$.

To prove the implication in the other direction, assume that $\forall_{u_i \in U}\, \exists_{(q_j,Q_j) \in A} u_i \in Q_j$. Hence, we get that $\forall_{u_i \in U} |A(u_i)| > 0$, which implies that: $
\forall_{u_i \in U}\, \simil(G_Q \cup A,u_i) \geq \frac{|Q|+2+\left(1-\frac{1}{|Q|}\right)^2}{3|Q|n}.
$
By comparing this value to the values computed above, we have that $\forall_{u_i \in U}, \forall_{v \notin U}$:
\[
 \simil(G_Q \cup A,v) < \frac{|Q|+2+\left(1-\frac{1}{|Q|}\right)^2}{3|Q|n} \leq \simil(G_Q \cup A,u_i),
\]
which in turn implies that:
\[
\forall_{u_i \in U} \forall_{v \notin U}\, \anom(G_Q \cup A,v) > \anom(G_Q \cup A,u_i).
\]
This concludes the proof of the lemma.
\end{proof}


Let $Q^* \subseteq Q$ be a solution to the given instance of the 3-Set Cover problem, i.e., $|Q^*| \leq k$ and $\forall_{u_i \in U} \exists_{Q_j \in Q^*} u_i \in Q_j$. From Lemma~\ref{lem:directed-construction} we have that $\anom(G_Q \cup A^*,v) > \anom(G_Q \cup A^*,u_i)$ where $A^*=\{(q_i,Q_i) : Q_i \in Q^*\}$. Hence, in network $G_Q \cup A^*$ all $\threshold=n-|U|$ nodes other than the nodes in $U$ have greater anomaly scores than all the nodes in $U$, and $|A^*| \leq k=\budget$. Therefore, $A^*$ is a solution to the constructed instance of the \textbf{PA-RWAD} problem.

To prove the implication in the other direction, assume that $A^*$ is a solution to the constructed instance of the \textbf{PA-RWAD} problem. In particular, it implies that $|A^*| \leq \budget = k$ and $\forall_{u_i \in U} \forall_{v \notin U} \anom(G_Q \cup A,v) > \anom(G_Q \cup A,u_i)$. From Lemma~\ref{lem:directed-construction} we have that $\forall_{u_i \in U} \exists_{(q_j,Q_j) \in A} u_i \in Q_j$. Therefore, $\{Q_i \in Q : (q_i,Q_i) \in A^*\}$ is a solution to the given instance of the 3-Set Cover problem.

We have shown that the constructed instance of the \textbf{PA-RWAD} problem has a solution if and only if the given instance of the 3-Set Cover problem has a solution, which concludes the proof of NP-hardness.
\end{proof}


\begin{theorem}
\label{thrm:decision-undirected}
The \textbf{PA-RWAD}  problem is NP-hard given an undirected graph.
\end{theorem}

\begin{proof}
We will prove that the problem is NP-hard by showing a reduction from the NP-complete \textit{Finding $k$-Clique} problem. An instance of this problem is defined by a network $G'=(V',E')$, and a number $k \in \mathbb{N}$. The goal is to determine whether there exist $k$ nodes that induce a clique in $G'$.

Let $(G',k)$ be a given instance of the Finding $k$-Clique problem. We will now construct an instance of the \textbf{PA-RWAD} problem. Let $n'=|V'|$, and let $d(G,v)$ be the degree of $v$ in network $G$, i.e., $d(G,v)=|\{w \in V : (v,w) \in E\}|$. First, we construct a undirected network $G=(V,E)$, where:
\begin{itemize}
\item $V = V' \cup \{t\} \cup \bigcup_{v'_i \in V'} \bigcup_{j=1}^{n'+k-d(G',v')-3} \{x_{i,j}\},$
\item $E = E' \cup \bigcup_{v'_i \in V'} \{(t,v'_i)\} \cup \bigcup_{x_{i,j} \in V} \{(v'_i,x_{i,j})\}.$
\end{itemize}
An example of this (e.g., $|V'|=4,k=3$) construction is presented in Fig.~\ref{fig:decision-undirected}.
% Figure environment removed
Now, consider the instance $(G,\target,\anom,\threshold,\budget,\FA,\FR)$ of the \textbf{PA-RWAD} problem, where:
\begin{itemize}
\item $G$ is the network we just constructed,
\item $\target = \{t\}$ is the target set,
\item $\anom$ is the anomaly score function with the restart rate parameter $\alpha=0$,
\item $\threshold=n-(n'-k+1)$ is the safety threshold,
\item $\budget=\frac{k(k-1)}{2}$ is the budget,
\item $\FA = \emptyset$, i.e., none of the edges can be added,
\item $\FR = E'$, i.e., only edges existing in $G'$ can be removed from $G$.
\end{itemize}

Since $\FA = \emptyset$, for any solution to the constructed instance of the \textbf{PA-RWAD} problem, we must have $A^* = \emptyset$. Hence, we will omit the mentions of $A^*$ in the remainder of the proof, and we will assume that a solution consists just of $R^*$.

From the formula of the anomaly score function with $\alpha = 0$ we have that $\anom(G,v_i) = 1 - \simil(G, v_i)$, where:
\[
\simil(G,v_i) = \textstyle\sum_{v_j \in V} \simil(G,v_j) P_{j,i}.
\]
Therefore, we have that $\anom(G,v_i) > \anom(G,v_j)$ if and only if $\simil(G,v_i) < \simil(G,v_j)$.

Moreover, from Perra and Fortunato~\cite{perra2008spectral}, we have that for the stationary distribution $\simil$ of this form (i.e., for $\alpha = 0$) in an undirected network $G$ we have that $\simil(G,v_i) \sim d(G,v_i)$, i.e., the value of the entry in $\simil$ for a given node is proportional to its degree. Therefore, we have that $\anom(G,v_i) > \anom(G,v_j)$ if and only if $d(G,v_i) < d(G,v_j)$.
Let us now compute the values of $d(G,v_i)$ for all nodes in $G$:
\begin{itemize}
\item $d(G,t) = n'$, as the node $t$ is connected with all $n'$ nodes $v'_i$,
\item $d(G,x_{i,j}) = 1 < d(G,t)$, as each node $x_{i,j}$ is only connected with the node $v'_i$,
\item $d(G,v'_i) = 1 + d(G',v'_i) + n'+k-d(G',v'_i)-3 = n'+k-2 \geq d(G,t)$, as each node $v'_i$ is connected with the node $t$, $d(G',v'_i)$ nodes from $V'$, as well as $n'+k-d(G',v'_i)-3$ nodes $x_{i,j}$.
\end{itemize}

Since $\threshold=n-(n'-k+1)$, all nodes $x_{i,j}$ have a smaller degree than $t$, and the total number of $x_{i,j}$ is $n-n'-1$, we need at least $k$ out of $n'$ nodes in $V'$ to have a smaller degree than $t$ in order for the safety threshold to be satisfied. However, they all have equal or greater degrees than $t$. Hence, the safety threshold is not satisfied in $G$.

Since the removal of edges from $\FR$ can only change the degrees of nodes in $V'$, we need to decrease the degree of $k$ of these nodes to a value smaller than that of $t$. For each of these $k$ nodes we have to remove at least $\Delta$ edges incident with it, where:
\[
\Delta = d(G',t) - d(G',v'_i) + 1 = n'+k-2 - n' + 1 = k-1.
\]

Let $V^* \subseteq V'$ be a solution to the given instance of the Finding $k$-Clique problem, i.e., a set of $k$ nodes forming a clique in $G'$. Since $\FR=E'$ and the degree of each node in $k$-clique is $k-1$, we have that $V^* \times V^* \subseteq \FR$, and removing $V^* \times V^*$ from $G$ decreases the degree of $k$ nodes from $V'$ by $\Delta=k-1$ each. Therefore, $V^* \times V^*$ is a solution to the constructed instance of the \textbf{PA-RWAD} problem.

To prove the implication in the other direction, assume that $R^*$ is a solution to the constructed instance of the \textbf{PA-RWAD} problem. At least $\frac{k\Delta}{2}=\frac{k(k-1)}{2}$ of the removed edges have to be incident with the $k$ nodes from $V'$ contributing to the safety threshold. However, since the total budget is $\budget=\frac{k(k-1)}{2}$, all of the removed edges have to be incident with the $k$ nodes from $V'$ contributing to the safety threshold, and $\frac{k(k-1)}{2}$ edges incident with $k$ nodes constitute a clique. Since we have that $\FR=E'$, the same edges constitute a $k$-clique in $G'$. Therefore, $\bigcup_{(v'_i,v'_j) \in R^*} \{v'_i, v'_j\}$ is a solution to the given instance of the Finding $k$-Clique problem.

We have shown that the constructed instance of the \textbf{PA-RWAD} problem has a solution if and only if the given instance of the Finding $k$-Clique problem has a solution, which concludes the proof of NP-hardness.
\end{proof}


% \begin{theorem}
% \label{thrm:optimization-directed}
% The Random Walk Anomaly Detection Minimal Attack problem given a directed network cannot be approximated within a ratio of $(1-\epsilon) \ln |\FA \cup \FR|$ for any $\epsilon > 0$, unless $P=NP$.
% \end{theorem}

% \begin{proof}
% We will prove the theorem by using a result for the \textit{Minimum 3-Set Cover} problem. An instance of this problem is defined by a collection of subsets  $Q=\{Q_1, \ldots, Q_{|Q|}\}$ of the universe $U=\{u_1, \ldots, u_{|U|}\}=\bigcup_{Q_i \in Q}Q_i$ such that $\forall_i |Q_i|=3$. The goal is to find the smallest cover of the universe, i.e., $Q^* \subseteq Q$, such that $U = \bigcup_{Q_i \in Q^*}Q_i$ and $|Q^*|$ is minimal. Dinur and Steurer~\cite{dinur2014analytical} showed that this problem could not be approximated better than logarithmically unless $P=NP$.

% In what follows, we will define function $f$ that based on an instance $(Q)$ of the Minimum 3-Set Cover constructs an instance $f(Q)$ of the Random Walk Anomaly Detection Minimal Attack problem, as well as function $g$ that based on a solution to $f(Q)$ constructs a solution to $(Q)$. We will then show that having a sublogarithmic approximation algorithm that can solve $f(Q)$ would contradict the result by Dinur and Steurer~\cite{dinur2014analytical}.

% Let $(Q)$ be a given instance of the Minimum 3-Set Cover problem. We define function $f$ as:
% \[
% f(Q) = (G_Q,\target,\anom,\threshold,\FA,\FR),
% \]
% where:
% \begin{itemize}
% \item $G_Q$ is the network defined in the proof of Theorem~\ref{thrm:decision-directed},
% \item $\target = U$ is the target set,
% \item $\anom$ is the anomaly score function with the restart rate parameter $\alpha=\frac{1}{|Q|}$,
% \item $\threshold=n-|U|$ is the safety threshold,
% \item $\FA = \bigcup_{Q_i \in Q} \{(q_i,Q_i)\}$, i.e., only edges from $q_i$ to corresponding $Q_i$ can be added,
% \item $\FR = \emptyset$, i.e., none of the edges can be removed.
% \end{itemize}

% Since $\FR = \emptyset$, for any solution $(A,R)$ to $f(Q)$ we must have $R = \emptyset$. Hence, we will omit the mentions of $R$ in the remainder of the proof, and we will assume that a solution consists just of $A$. Based on this, we define function $g$ as $g(A)=\{Q_i \in Q : (q_i,Q_i) \in A\}$ where $A$ is a solution to $f(Q)$.

% First, we will show that if $A$ is a solution to $f(Q)$, then $g(A)$ is a solution to the given instance of the Minimum 3-Set Cover problem $(Q)$. Notice that since $\target = U$ and  $\threshold=n-|U|$, if $A$ is a solution to $f(Q)$, then $\forall_{u_i \in U} \forall_{v \notin U} \anom(G_Q \cup A,v) > \anom(G_Q \cup A,u_i)$. From Lemma~\ref{lem:directed-construction} we have that it implies $\forall_{u_i \in U} \exists_{(q_j,Q_j) \in A} u_i \in Q_j$. Therefore $g(A)=\{Q_i \in Q : (q_i,Q_i) \in A\}$ covers the entire universe, i.e., $g(A)$ is a solution to $(Q)$.

% Now, we will show that the optimal solutions to the given instance of the Minimum 3-Set Cover problem $(Q)$ and to $f(Q)$ are of the same size. Let $Q^*$ be an optimal (i.e., the smallest) solution to $f(Q)$, and let $A^*$ be an optimal (i.e., the smallest) solution to $(Q)$. Assume that $|A^*| < |Q^*|$. However, $|g(A^*)|=|A^*|$ and we showed that $g(A^*)$ is a solution to $(Q)$, hence we have contradiction. Now, assume that $|A^*| > |Q^*|$. Since $Q^*$ is a solution to $(Q)$, we have that $\forall_{u_i \in U} \exists_{Q_j \in Q^*} u_i \in Q_j$. However, it implies that for $A=\{(q_j,Q_j) : Q_j \in Q^*\}$ we have $\forall_{u_i \in U} \exists_{(q_j,Q_j) \in A} u_i \in Q_j$. From Lemma~\ref{lem:directed-construction} we then have that $\forall_{u_i \in U} \forall_{v \notin U} \anom(G_Q \cup A,v) > \anom(G_Q \cup A,u_i)$. Therefore, $A$ is a solution to $f(Q)$ and $|A| < |A^*|$, hence we have contradiction. Therefore, we must have that $|A^*| = |Q^*|$.

% Assume that there exists an approximation algorithm solving the Random Walk Anomaly Detection Minimal Attack problem with approximation ratio $(1-\epsilon) \ln |\FA \cup \FR|$ for some $\epsilon > 0$. We can use this algorithm to solve $f(Q)$, obtaining solution $A$ where $|A| \leq |A^*|(1-\epsilon) \ln |\FA \cup \FR|$. We can then use function $g$ to obtain $g(A) \subseteq Q$, which is a solution to the given instance of the Minimum 3-Set Cover problem $(Q)$. Since $|g(A)|=|A|$, $|\FA \cup \FR|=|Q|$ and $|A^*| =|Q^*|$, we have that:
% \[
% |g(A)|=|A| \leq |A^*|(1-\epsilon) \ln |\FA \cup \FR| = |Q^*|(1-\epsilon) \ln |Q|.
% \]
% Therefore, we obtained an approximation algorithm solving the Minimum 3-Set Cover problem  with approximation ratio $(1-\epsilon) \ln |Q|$ for $\epsilon > 0$. However, Dinur and Steurer~\cite{dinur2014analytical} showed that the Minimum 3-Set Cover problem could not be approximated to within $(1-\epsilon) \ln |Q|$ for any $\epsilon > 0$, unless $P=NP$. Therefore, such an approximation algorithm for the Random Walk Anomaly Detection Minimal Attack problem cannot exist unless $P=NP$. This concludes the proof.
% \end{proof}


\section{Practical Graph-Space Attacks}
\label{structural_attack}
In this section, we investigate practical attacks in the graph space. We note that the graph-space attack itself is important in the case where the graph is directly accessible. Moreover, as we will show later, the results of graph-space attacks provide insightful guidance for feature-space attacks.

%Regarding the hardness of the attack problem, this paper aims to explore an approximation solution. We first formulate the structural attack in Section~\ref{ProblemSate} to an optimization problem as follows,
\subsection{Attack Formulation}
We begin by formulating the decision problem \textbf{PA-RWAD} as an optimization problem.
% {\color{red} Re-write. All the notations should be clearly explained. Make sure that the readers can understand.
% Your writing should reflect the process of thinking: how to formulate and solve the problem "step by step". Do not just throw out a mathematical equation and expect the readers to understand by themselves.
% Suggestion: explain "important" notations first -- the explanation at the same time reflect the thinking process, you are using the explanation to guide the readers how to think. Then the formulation of the mathematical optimization problem should be a natural result
% $\threshold$ has a different meaning in Definition 1.}
We use $G =(V,E)$ with its corresponding adjacency matrix $W$ to represent the original clean graph. We assume that the anomaly detection system predicts node $v$ as an anomaly if the anomaly score $\mathcal{A}(v)$ is greater than a threshold $\theta$. The attacker aims to decrease the number of nodes in a given target set $\mathcal{T} \subset V$ that are identified as anomalies by modifying at most $K$ edges in the graph. To represent the edge manipulations, we denote the modification by a binary matrix $B=(b_{uv})_{(|V|\times|V|)}$, where the element $b_{uv} \in \{0,1\}$. If $b_{uv}=0$, the edge $\langle u,v \rangle$ remains unchanged, and $b_{uv}=1$ lead to add/delete of edge $\langle u,v \rangle$. Then the attack graph can be represented by $|W-B|$. In this paper, we consider undirected graphs where the adjacency matrix is always symmetric, and the budget constraint can be represented as $\sum_{u>v} b_{uv} \le K$. Then the graph-space attack problem can be formulated as follows: %, \red{format of equations. Is it $>\theta$ or $<\theta$} 
\begin{equation}
\label{opt:graphAttack}
    \begin{array}{rlclcl}
        \displaystyle \min\limits_{B} & \multicolumn{3}{l}{\sum_{v\in \mathcal{T}} \mathbb{I}(\mathcal{A}(v)>\theta),} \\
        \textrm{s.t.} & b_{uv}  \in  \{0,1\}, \, \sum_{u>v} b_{uv} \le  K,
    \end{array}
\end{equation}
where $\mathbb{I}(\cdot)$ is a indicator function, $\mathbb{I}(\mathcal{A}(v)>\theta)=1$ if the anomaly scores of node $v$ is greater than $\theta$.

\subsection{Attack Method}

\par To address the non-differentiable issue of the binary values in $B$, we adopt a relaxation strategy by representing $b_{uv}$ in a continuous space that ranges from 0 to 1. This is denoted as $\tilde{B}$, which is subsequently converted back to binary form $\bar{B}$ after solving the optimization problem. 
To handle the discrete objective function in Eqn.~\ref{opt:graphAttack}, we replace it with the sum of anomaly scores among target nodes, $\mathcal{L}_a(\tilde{B})= \sum_{v \in \mathcal{T}} \mathcal{A}(v)$, then we can re-formulate the attack problem as:
\begin{equation}
\label{opt:graphAttack_relaxed}
    \begin{array}{rlclcl}
        \displaystyle \min\limits_{\tilde{B}} & \multicolumn{3}{l}{ \mathcal{L}_a(\tilde{B})= \sum_{v \in \mathcal{T}} \mathcal{A}(v),} \\
        \textrm{s.t.} & \tilde{b}_{uv} \in [0,1],\, \sum_{u>v} \bar{b}_{uv} \le K, \\
    \end{array}
\end{equation}
where $\tilde{B}$ is the relaxed and continuous adjacency matrix, $\bar{B}=(\bar{b}_{ij})$ is the discrete version of $\tilde{B}$.  

To solve the challenging bi-level optimization problem, we propose two strategies: alternative iteration attack (\textbf{alterI}-attack) and closed-form attack (\textbf{cf}-attack). 
In brief, the \textbf{alterI}-attack iterates the inner RW model and the attack optimization alternatively to approximate the bi-level optimization, while the \textbf{cf}-attack transforms the bi-level optimization into a single-level problem. We first introduce the \textbf{alterI}-attack and then highlight the difference in the \textbf{cf}-attack.

% \red{Write these methods in a tone that they are very innovative. "exaggerate" your original work.}

\subsubsection{\textbf{alterI}-attack}
The optimization of problem~\ref{opt:graphAttack_relaxed} remains a challenging task due to the need to reverse the continuous variable $\bar{B}$ to binary $\bar{B}$ while satisfying the budget constraint. To overcome this difficulty, we first use projected gradient descent (PGD) to efficiently optimize $\tilde{B}$ without considering the budget constraint $\sum_{u>v} \bar{b}_{uv} \le K$. Then, we obtain the binary matrix $\bar{B}$ by selecting the top-$K$ elements from $\tilde{B}$. This approach allows us to efficiently approximate the constrained optimization problem while ensuring that the attack budget is satisfied. However, optimizing the relaxed optimization problem is still challenging because the anomaly score $\mathcal{A}(v)$ in the loss function $\mathcal{L}_a(\tilde{B})$ depends on the variable $\tilde{B}$ in a complex way. After updating $\tilde{B}$, obtaining $\mathcal{A}(v)$ requires iterating over Eqn~\ref{eqn:RW_general} dozens of times to get the converged node similarity vector $\vec{s}$, and the gradient needs to be traced back to each iteration. To address this issue, we only iterate over Eqn~\ref{eqn:RW_general} once instead of multiple times.
The detailed procedures are summarized in Alg.~\ref{alg:graphAttack}. 
%Specifically, we iterate the inner RW model and optimize the attack graph $\tilde{B}$ alternatively (line:4-16). 
Firstly, we update the adjacency matrix with $\tilde{W}=|W-\tilde{B}|$ (line:5), and then we update the similarity score $\mathcal{A}(v)$ based on $\tilde{W}$ for one step using Eqn~\ref{eqn:RW_general} and then obtain the anomaly score with Eqn \ref{eqn:AS_bp} or \ref{eqn:AS_px} (line:6-9). Next, we update attack loss $\mathcal{L}_a (\tilde{B})$ based on $\mathcal{A}(v)$ (line:10), and calculate the projected gradient to optimize $\tilde{B}$ for one step (line: 11-15). Repeating the alternative iteration leads to the convergence of the inner model $\vec{s}$ and also the continuous attack variable $\tilde{B}$. After the iterations, we keep the top-$K$ elements in $\tilde{B}$ to obtain $\bar{B}$ and the others are set to zeros (line:17-18). Finally, the attacked graph is obtained by $\hat{W}=|W-\bar{B}|$ (line:19). This algorithm is also suitable for weighted graphs in which the weights on edges are in $[0,1]$, and the final solution is to modify $K$ edge weights while the other weights remain unchanged. 
\begin{algorithm}[htb]
\caption{Attack algorithm on graph space}
\label{alg:graphAttack}
\begin{algorithmic}[1]
\State \textbf{Input:} Graph with adjacency matrix $W$, attack budget $K$, attack iteration $T$, learning rate $\eta$.
\State \textbf{Output:} Attacked graph with  adjacency matrix $\hat{W}$.
\Function{AlterI-attack}{$W$, $K$, $T$, $\eta$}
\For {$t=1$ to $T$}
    \State Update adjacency matrix: $\tilde{W}=|W-\tilde{B}|$.
    % \If {bipartite graph}
    %     \State $\tilde{M}=\left|\tilde{B}-M\right|$, $\tilde{W}=\begin{pmatrix}0 & \tilde{M} \\\tilde{M}^{T} & 0 \end{pmatrix}$
    % \Else
    % \If{proximity graph}
    %     \State $\tilde{W}=(\tilde{w}_ij)_{(n\times n)}$
    %     \State{$\tilde{w}_{ij}=|\tilde{b}_{ij}-w_{ij}|\cdot \mathbb{I}(|\tilde{b}_{ij}-w_{ij}|>\epsilon)$}
    %     \EndIf 
    % \EndIf
    \For{each node $v$ in target set $\mathcal{T}$}
    \State Update similarity scores $\vec{s}$ with Eqn~\ref{eqn:RW_general}.
    \State Update anomaly score $\mathcal{A}(v)$ based on $\vec{s}$.
    %by Eqn~\ref{eqn:update_s_v} and Eqn~\ref{eqn:AS_bp} (bipartite) or Eqn~\ref{eqn:update_s} and Eqn~\ref{eqn:AS_px} (proximity). 
    \EndFor
    
    \State Update objective function $\mathcal{L}_a (\tilde{B})$ with $\mathcal{A}(v)$.
    \For {each edge $\tilde{b}_{uv}$ in $\tilde{B}$}
        \State Calculate gradient $g_{uv}=\tilde{b}_{uv}-\eta \frac{\partial \mathcal{L}(\tilde{B}^)}{ \partial \tilde{b}_{uv}}$
        \State Project $g_{uv}$ into $[0,1]$
        \State Update $\tilde{b}_{uv}$ in $\tilde{B}$
    \EndFor 
    \EndFor
\State Choose top-$K$ edges in $\tilde{B}$ to obtain $\bar{B}$:
\State
    \begin{equation}
    \bar b_{uv}= 
    \begin{cases} 
        \tilde{b}_{uv}  & \text{if }\tilde{b}_{uv} \in  top_K(\tilde{B}),\nonumber\\
        0 & \text{otherwise}.
    \end{cases}
    \end{equation}
\State Obtain attacked graph $\hat{W}=|W-\bar{B}|$.
\State \Return $\hat{W}$.
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsubsection{\textbf{cf}-attack}
While the \textbf{alterI}-attack approach is feasible, the one-step update of the inner model is a simple estimation that may not provide accurate attack loss during the iteration. To address this issue and obtain accurate attack loss, we employ the closed-form solution of the inner model to transform the bi-level optimization problem into a single-level problem. 
% Another approach to solving bi-level optimization problems is to employ the closed-form solution of the inner model.
According to \cite{boldi2007deeper,gasteiger2018combining}, the inner model (Eqn~\ref{eqn:RW_general}) has closed-form solution as follows:
\begin{equation}
    \label{eqn:RW_closed-form}
    \vec{s} = \alpha (I- (1-\alpha)P)^{-1}\vec{r},
\end{equation}
where $I$ is an identity matrix. With the closed-form solution, we can directly obtain the accurate anomaly scores after the update of $\tilde{B}$. In contrast to the \textbf{alterI}-attack, which iterates the inner model once after updating $\tilde{B}$, our innovative \textbf{cf}-attack approach substitutes the Eqn~\ref{eqn:RW_general} (line:7) with Eqn~\ref{eqn:RW_closed-form} to  
obtain the accurate connectivity scores $\vec{s}$ for current $\tilde{B}$, and others remain the same. While \textbf{cf}-attack offers a more accurate formulation than \textbf{alterI}-attack, it comes with the cost of potential time consumption when calculating the matrix inverse, particularly for graphs with a large number of nodes or edges. In contrast, \textbf{alterI}-attack does not encounter such a problem, making it a more efficient option for such scenarios. Both \textbf{cf}-attack and \textbf{alterI}-attack have their own unique advantages. 

%\section{Feature Attack on RWAD}
\section{Graph-Guided Feature-Space Attacks}
\label{FeatureAttack}

%{\red{Need to emphasize the importance of feature space attack and our contribution of using graph space attack to guide feature space attack}}
%Given the graph-space attack, we further provide a more practical attack based on feature manipulation. We first formally define the attack problem and then introduce our graph-guided feature attack.

\subsection{Motivation for Feature-space Attacks}
Previously, we presented effective graph-space attacks against Di-RWAD. However, for InDi-RWAD, where the graphs are not directly accessible, the \emph{realizability} of the attacks becomes a serious concern: the attacker cannot directly modify the edges in a virtual graph space. Instead, in many practical application scenarios, what the attacker can modify are the attributes associated with the entities in their control. 
%\red{in this example, you should describe how an attacker can control a few nodes to help target nodes to evade detection} 
%\red{there are two issues with this description: 1)the readers may not know the connections are entities or nodes in the graph, while your goal is to give an example of an attacker controlling nodes. 2) "attacker can control some other TCP connections, such as manipulating the connection duration, ..." this is an error of using "such as" }
%\blue{in the network intrusions task, the attacker can control some other TCP connections, such as manipulating the connection duration, type of the protocol, number of urgent packets, and other attributes. These attribute manipulations will lead to the perturbation of the proximity graph in the \textit{ProxGraphRW} model, protecting the targeted anomaly TCP connection from being detected. }
For example, when it comes to network intrusion detection, each TCP connection represents an entity or node, and attackers can manipulate certain TCP connections by altering attributes such as connection duration, protocol type, and the number of urgent packets. Such manipulations will change the structure of the proximity graph in the \textit{ProxGraphRW} model to become perturbed, which can help shield the targeted anomaly TCP connection from being detected.


Thus, investigating feature-space attacks against InDi-RWAD is of significant practical importance. In particular, we consider the scenario where an attacker can manipulate a set of entities (corresponding to nodes in the constructed proximity graph) and modify their features to assist a group of target nodes in avoiding detection. We explore the connection between graph-space and feature-space attacks and demonstrate how guidance from graph-space attacks can be leveraged to construct effective feature-space attacks.

\subsection{Attack Formulation}
Consider a set of entities with features $\mathbf{X}=\{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\}$, where $\mathbf{x}_i \in \mathcal{X}$ denotes the feature vector associated with entity $i$. As introduced in Section~\ref{target2}, a proximity graph can be constructed from $\mathbf{X}$, where the nodes represent those entities and edges indicate similar node pairs.  An attacker aims to allow a set of target entities (nodes) $\mathcal{T}$ to evade detection. We assume that the attacker has control of a set of \textit{attack nodes} $\mathcal{Z}$ such that the features of the nodes in $\mathcal{Z}$ can be arbitrarily modified in a certain domain $\mathcal{X}$. To limit the attacker's ability, we make the restriction that $\mathcal{Z} \cap \mathcal{T} = \emptyset$ and $|\mathcal{Z}| \leq K'$. For an attack node $i \in \mathcal{Z}$, we denote the modified feature vector as $\hat{\mathbf{x}}_i$. The manipulated feature matrix is $\hat{\mathbf{X}}$.
We note that since the manipulation of the features leads to the change of graph structure, the anomaly score function $\mathcal{A}(v;\hat{\mathbf{X}})$ depends on the features $\hat{\mathbf{X}}$.
%the proximity graph is constructed with each node representing a data point, and edges representing feature proximity among data points. We assume that the attacker can control $K'$ of attack nodes (excluding the targets), denoted as $\mathcal{Z}\subset V\setminus \mathcal{T}$, and the attacker can modify their features in a certain domain $\mathcal{X}$. We denote the manipulating features as $\hat{x}_i\in \mathcal{X}, i \notin \mathcal{T}$. If a node $i$ is not selected as an attack node, then $\hat{x}_{i}= x_{i}$. We use $z_i=\mathbb{I}(\hat{x}_{i} \neq x_{i})$ as the indicator of whether $x_i$ is modified, such that $\sum_{i \notin \mathcal{T}} z_i$ should be less or equal to $K'$. The manipulation of features then leads to the change in the proximity graph.  
Then, we can formulate the feature-space attack as follows: %\red{check notations. use {eqnarray}}

% \begin{equation}
% \label{opt:feature}
%     \begin{array}{rlclcl}
%     %\mathcal{L}(\tilde{X})=
%         \displaystyle \min\limits_{\hat{x}_i, i \notin \mathcal{T}} & \multicolumn{3}{l}{\mathcal{L}(\hat{X})=\sum_{v\in \mathcal{T}} \mathbb{I}(\mathcal{A}(v;\hat{X})>\theta),} \\
%         \textrm{s.t.}& \hat{X} = \{\hat{x}_i, i\notin \mathcal{T}; x_v,v\in \mathcal{T}\},\\
%         & \hat{x}_{i} \in \mathcal{X}, \\
%         & z_i = \mathbb{I}\,(\hat{x}_{i} \neq x_{i}), \\
%         & \sum_{i \notin \mathcal{T}} z_i \le K'.
%     \end{array}
% \end{equation}

\begin{equation}
\label{opt:feature}
    \begin{array}{rlclcl}
    %\mathcal{L}(\tilde{X})=
        \displaystyle \min\limits_{\hat{\mathbf{x}}_i, i \notin \mathcal{T}} & \multicolumn{3}{l}{\mathcal{L}(\hat{\mathbf{X}})=\sum_{v\in \mathcal{T}} \mathbb{I}(\mathcal{A}(v;\hat{\mathbf{X}})>\theta),} \\
        \textrm{s.t.}
        &  \hat{\mathbf{x}}_v = \mathbf{x}_v, \forall v\in \mathcal{T},\, \hat{\mathbf{x}}_{i} \in \mathcal{X}, \\
        & \mathcal{Z}=\{i|\hat{\mathbf{x}}_{i} \neq \mathbf{x}_{i}\},\, |\mathcal{Z}| \leq K'.
    \end{array}
\end{equation}

%Solving problem~\eqref{opt:feature} involves two key technical challenges: how to select the attack nodes and how to optimize the features of the selected attack nodes



%This problem involves optimizing both in the selection of
%attack nodes as well as feature optimization of the attack
%nodes. Commonly used techniques are insufficient to solve this optimization problem, so we introduce a graph-guided feature attack model to address this challenge.

%\subsection{Graph-guided Feature Attack}
\subsection{Two Levels of Guidance from Graph-Space Attacks}
Applying the gradient-descent method to solve problem~\eqref{opt:feature} faces a crucial challenge: while gradient descent can be used to effectively optimize the node features, it is hard to control which nodes are to be manipulated. In other words, it is nontrivial to guarantee the constraint $|\mathcal{Z}| \leq K'$ while preserving optimization performance. We adopt a divide-and-conquer strategy to tackle this problem: we first select up to $K'$ nodes as the attack nodes and then utilize gradient descent to optimize node features.  In particular, we show that the results from graph-space attacks can be innovatively utilized to guide both the selection of attack nodes and feature optimization.

Specifically, given a proximity graph $\mathcal{G}$, we can leverage the attacks in Section~\ref{structural_attack} to produce a poisoned graph $\mathcal{G}'$. Even though $\mathcal{G}'$ cannot be directly realized, it represents an excellent candidate in the graph space with which the target nodes $\mathcal{T}$ could evade detection with high probability. Thus, our intuition is to manipulate features so that the resulting proximity graph would approximate $\mathcal{G}'$. To this end, we utilize the guidance from the following two aspects.

%In this section, we introduce a graph-guided feature attack model to address the difficulty of simultaneously optimising the selection of attack nodes and their features. To achieve this, we leverage the results of the graph-space attack to identify the attack nodes. Once the attack nodes are identified, we formulate a loss function based on the previous graph attack to guide the optimization of the attack node features. This approach simplifies the process of indirect feature attack by providing a straightforward and efficient approximation. Next, we illustrate the strategy in detail. 



%\paragraph{Attack node selection} 
\paragraph{Guidance on attack node selection} In the graph-space attack, we denote the set of edges/non-edges modified by the attacker as $\mathcal{E}_a$. Intuitively, the modification of $\mathcal{E}_a$ will influence the anomaly scores of the targets most. To preserve such an influence, we set the attack nodes as those ones incident to the edges/non-edges in  $\mathcal{E}_a$. Note that we can always easily adjust the budget in the graph-space attack such that the constraint $|\mathcal{Z}| \leq K'$ is satisfied. 
%The remaining task thus amounts to optimizing the features of the nodes in $\mathcal{Z}$.
%the most important edges are selected according to the top-$K$ values of $\tilde{B}$. We think that these edges are probably inherent important nodes. Hence, we choose the nodes involved in the graph-space attack (except the target nodes) as the attack node set $\mathcal{Z}$. Now, the optimization lies in the features in $\mathcal{Z}$.

After fixing the attack nodes $\mathcal{Z}$, we can follow a similar approach in the graph-space attack to optimize the features. Specifically, we replace the indicator function in~\eqref{opt:feature} with the sum of anomaly scores of target nodes. For discrete features, we relaxed their discrete feature domain to the continuous space denoted by $\tilde{\mathcal{X}}$. Then, let $\tilde{\mathbf{x}}_i\in \tilde{\mathcal{X}}$ denote the relaxed feature, and $\tilde{\mathbf{X}}=\{\tilde{\mathbf{x}}_i| i\in V\}$, the feature-space attack can be formulated as the following optimization problem:
%Then, we optimize the features such that the anomaly scores of target nodes are minimized by PGD: 
\begin{equation}
\label{loss:anomaly}
\min\limits_{\hat{\mathbf{x}}_i\in\tilde{\mathcal{X}}, i \in \mathcal{Z}} \mathcal{L}_a(\tilde{\mathbf{X}})=\textstyle\sum_{v \in \mathcal{T}}\mathcal{A}(v;\tilde{\mathbf{X}}). 
\end{equation}

We term this type (with objective function $\mathcal{L}_a$) of feature-space attacks as \textbf{G-Guided}. We can straightforwardly adopt the two algorithms \textbf{alterI-attack} and \textbf{cf-attack} to solve the optimization problem~\eqref{loss:anomaly}, resulting in two variants named \textbf{G-Guided-alterI} and \textbf{G-Guided-cf}.

%\paragraph{Feature optimization} 
\paragraph{Guidance on reformulation of attack objective}
Beyond the selection of attack nodes, the poisoned graph $\mathcal{G}'$ obtained from the graph-space attack can provide vital information for optimizing the features. Specifically, we aim to optimize the features such that the proximity graph constructed from the modified features would approximate $\mathcal{G}'$ as much as possible. 
To this end, we reformulate the attack objective function as follows: 
%We can also optimize the features of attack nodes such that the graph of attack features is close to the attacked graph. This strategy aims to reverse the attack graph to feature space as much as possible. To achieve this, we define the graph-guided attack loss as follows:
\begin{equation}
    \label{loss:attack_graph}
    \mathcal{L}_g(\tilde{\mathbf{X}})=\sum_{\{(i,j)|\bar{b}_{ij}>0\}\atop i/j \in \mathcal{Z}} |sim(\mathbf{x}_i,\mathbf{x}_j)-\hat{w}_{i,j}|,
\end{equation}
where $\hat{w}_{ij}$ is the element in the attacked adjacency matrix $\hat{W}$.
This objective function aims to push the similarity between control nodes $\mathbf{x}_i$ and other nodes $\mathbf{x}_j$ (denoted by $sim(\mathbf{x}_i,\mathbf{x}_j)$) close to the manipulated edges $\hat{w}_{i,j}$ in the poisoned graph $\mathcal{G}'$. Intuitively, minimizing $\mathcal{L}_g$ allows us to approximate an inverse problem: given $\mathcal{G}'$, find the node features from which $\mathcal{G}'$ can be constructed. Since \eqref{loss:attack_graph} is a single-level function, we can directly adopt PGD (similar to the graph-space attack) to solve the optimization problem. We term this type (with objective function $\mathcal{L}_g$) of feature-space attacks as \textbf{G-Guided-plus}. The attack algorithm in the feature space is summarized in Alg.~\ref{alg:featureAttack}.

%Generally, the optimization problem in Eqn~\ref{opt:feature} becomes,
%\begin{equation}
%\label{opt:feature_graph_guided}
 %   \begin{array}{rrclcl}
%        \displaystyle \min\limits_{\tilde{x}_i, i \in \mathcal{Z}} & \multicolumn{3}{l}{\mathcal{L}(\tilde{X})} \\
%        \textrm{s.t.}& \tilde{x}_{i} & \in & \tilde{\mathcal{X}}
%    \end{array}
%\end{equation}
%where the $\mathcal{L}(\tilde{X})$ is $\mathcal{L}_a(\tilde{X})$ or $\mathcal{L}_g(\tilde{X})$. After the optimization in continuous space, we reverse the discrete features by rounding. We further summarize our feature attack in Alg.~\ref{alg:featureAttack}. The attack framework with $\mathcal{L}_a(\tilde{X})$ is similar to the graph-space attack where we can employ alternative iteration (Eqn~\ref{eqn:RW_general}) or closed-form (Eqn~\ref{eqn:RW_closed-form}) in line:6. If we use $\mathcal{L}_g(\tilde{X})$, this approach aims to optimize the feature to be similar to the attack graph and does not require any estimation of anomaly scores. Therefore, no anomaly score updates are needed in line:6-7.
%In summary, graph-guided attack contributes to the selection of attack nodes and the attack loss $\mathcal{L}_g(\tilde{X})$. 

\begin{algorithm}[htb]
\caption{Attack algorithm on feature space}
\label{alg:featureAttack}
\begin{algorithmic}[1]
\State \textbf{Input:} Feature matrix $\tilde{\mathbf{X}}$, attack nodes $\mathcal{Z}$, attack iteration $T$, learning rate $\eta$.
\State \textbf{Output:} Attacked feature matrix $\hat{\mathbf{X}}$.
\Function{FeatureAttack}{$\tilde{\mathbf{X}}$, $\mathcal{Z}$, $T$, $\eta$}
\For {$t=1$ to $T$}
\State Construct graph based on $\tilde{\mathbf{X}}$ (Section~\ref{target2}).
\State Update similarity scores $\vec{s}$ with Eqn~\ref{eqn:RW_general}.
\State Update the anomaly scores based on $\vec{s}$ (Eqn~\ref{eqn:AS_px}). 
\State Update objective function $\mathcal{L} (\tilde{\mathbf{X}})$.
\For {each attack nodes ${\tilde{\mathbf{x}}_i, i\in \mathcal{Z}}$}
\State Calculate gradient $g_{i}=\tilde{\mathbf{x}}_{i}-\eta\frac{\partial \mathcal{L}(\tilde{\mathbf{X}})}{\partial \tilde{\mathbf{x}}_{i}}$.
\State Project $g_{i,j}$ into the feasible set $\tilde{\mathcal{X}}$.
\State Update $\tilde{\mathbf{x}}_{i,j}$ in $\tilde{\mathbf{X}}$.
\EndFor
\EndFor
\State Rounding the attacked feature:
    \begin{equation}
    \nonumber
    \hat{\mathbf{x}}_i= 
    \begin{cases} 
        round(\tilde{\mathbf{x}}_{ij})  & \text{if feature $j$ is discrete},\\
        \tilde{\mathbf{x}}_{ij} & \text{otherwise}.
    \end{cases}
    \end{equation}
\Return Attacked feature matrix $\hat{\mathbf{X}}$.
\EndFunction
\end{algorithmic}
\end{algorithm}


\section{Experiments}
\label{Experiments}


In this section, we evaluate the performances of our proposed attacks
%on $4$ datasets, including both bipartite graphs and proximity graphs.
%{\color{red} We aim to answer the following questions: each question in one subsection}
by answering these four major questions:
1) Are our proposed graph-space attacks effective? 2) What are the preferences of the proposed graph attack?
% \red{attack has no pattern}?
3) How effective are the graph-guided feature-space attacks? 4) How is the transferability of the graph-guided feature-space attacks?
\subsection{Datasets and Experiment Settings}
We consider four datasets that are commonly used for graph-based anomaly/outlier detection: Paper-Author, Magazine, 
KDD-99, and MINIST (outlier). Among them, the first two are bipartite graphs while the latter two datasets are feature data. Below is the detailed description. All datasets, source code for our proposed attacks, and all evaluated baselines are in our \textit{GitHub} page.~\footnote{https://github.com/Yuni-Lai/DualAttackRW.}
\begin{itemize}
    \item Paper-Author~\cite{sun2005neighborhood}: This dataset contains papers crawled from the arXiv preprint database. Nodes $U$ represent papers, while nodes $V$ represent authors. An edge $\langle u, v\rangle$ indicates that the author $v$ is shown in the paper $u$. We randomly sampled $10,000$ records and deleted nodes with degrees lower than $5$, resulting in $|U|,|V|=2311,\,405$. We manually inject $10\%$ of anomaly nodes following~\cite{hooi2016fraudar}.
    \item Magazine: This dataset contains Amazon Aeviews Data~\footnote{https://nijianmo.github.io/amazon/, accessed May 2023.} under the category of Magazine Subscriptions. We randomly sampled $100,000$ records and removed nodes with degrees lower than $3$, resulting in $|U|,|V|=1079,\, 1180$ nodes. We also injected $10\%$ of anomaly nodes manually following~\cite{hooi2016fraudar}.
    \item KDD-99~\cite{moonesinghe2006outlier}: The dataset contains network intrusion data with $41$ features and $4$ types of attacks. We randomly sampled $10,000$ benign data and $100$ anomaly data for the experiment. 
    \item MINIST (outlier): This is a subset of the MINST handwritten digits dataset, created for the outlier detection task in Outlier Detection DataSets~\footnote{http://odds.cs.stonybrook.edu/, accessed May 2023.}. It contains a total of $7603$ images, with $6903$ images of digit-$0$ regarded as normal points and $700$ images of digit-$6$ regarded as outliers. Each sample has $100$ features. 
\end{itemize}

\subsection{Experimental Settings}

We conduct our experiments on Ubuntu $20.04$ system with an NVIDIA GeForce RTX $3090$ GPU, Python $3.7$, and PyTorch $1.10.0$. All the experiments are repeated $10$ times with different random seeds, and different target nodes are sampled. 
\subsubsection{Target nodes and budgets}
For attacking \textit{BiGraphRW} model, we sample $5$ target nodes from the top $100$ anomaly nodes, while in \textit{ProxGraphRW} model, we sample $20$ target nodes from the top $100$ anomaly nodes. We set the attack edge budget proportion to the sum of \textit{target node degrees} (e.g., budget $10\%: K = 0.1 \times \sum_{v\in\mathcal{T}} d(v)$, where $d(v)$ is the degree of node $v$). Setting the budget associated with node degree is commonly adopted in targeted attacks such as Nettack~\cite{zugner2018adversarial,bojchevski2019adversarial}. In feature-space attacks, we set the number of attack nodes as the number of nodes involved in the \textbf{alterI} graph-space attack.

\subsubsection{Evaluation metrics}
Our main focus is to evaluate the effectiveness of our proposed method facilitating target nodes to evade detection under different detection thresholds. Usually, the detection threshold $\theta$ is set to the proportion of data size, and we evaluate the level of detect ratio as the top $5\%$ and $10\%$. We then use the \textit{evasion rate} $\mathsf{ER}$ of target nodes under these detection thresholds as the main metric. Specifically, the evasion rate is computed as $\mathsf{ER} = n_0/|\mathcal{T}|$, where $n_0$ is the number of target nodes not shown in the top $5\%$ or $10\%$ anomaly scores (i.e., evaded successfully). Besides, we also evaluate the average anomaly scores of target nodes.
\subsubsection{Baselines}
We evaluate the effectiveness of our proposed attacks against several baselines for both graph-space attacks and feature-space attacks.
\paragraph{Graph-space attack}
The most relevant prior work is \cite{bojchevski2019adversarial}. Although this work also proposes a targeted attack for the RW model, it is specific to the DeepWalk model and cannot be directly applied to our RWAD systems. Therefore, we transfer its targeted attack to our model. Besides, we also adopt two common baselines RndAdd and DegAdd following \cite{bojchevski2019adversarial}.
\begin{itemize}
    \item \textbf{RndAdd}: This baseline randomly adds candidate edges, where the candidate edges are the edges incident to target nodes. %\red{usually, we don't want to use the word "random"} \blue{-but most of papers refer to it as the "Rnd" method, do you mean it overlaps with the "random" feature attack?}
    \item \textbf{DegAdd}: This baseline adds candidate edges with the top-$K$ highest degrees, where the candidate edges are also the edges incident to target nodes. %\red{The explanation is not clear.}
    \item \textbf{DeepWalk}\cite{bojchevski2019adversarial}: In this baseline, we transfer the attack designed for DeepWalk to RWAD models. 
    \item Our methods: \textbf{alterI} and \textbf{cf} are our proposed attacks with alternative iteration and closed-form solution, respectively.
\end{itemize}
\paragraph{Feature-space attack}
To evaluate the effectiveness of our graph-guided attack in node selection, we include random selection as a baseline for comparison. 
%The candidate attack nodes are all nodes other than the target nodes.
\begin{itemize}
    \item \textbf{VanillaOpt}: This baseline randomly selects attack nodes from candidates and optimizes node features with the objective function $\mathcal{L}_a(\tilde{\mathbf{X}})$ in Eqn~\eqref{loss:anomaly} with optimization strategy \textbf{alterI}. %\red{which optimization method?}
    \item Our methods: We use the graph-space attacks to guide the selection of attack nodes and choose  $\mathcal{L}_a(\tilde{\mathbf{X}})$ as the attack objective function, resulting in two attack methods \textbf{G-guided-alterI} and \textbf{G-guided-cf}, which adopt \textbf{alterI} and \textbf{cf} to optimize node features respectively. In addition, when the objective function $\mathcal{L}_g(\tilde{\mathbf{X}})$~\eqref{loss:attack_graph} is selected, the attack method is \textbf{G-guided-plus}.
    %\item \textbf{G-guided-alterI:} This is the graph-guided attack that selects the nodes involved in the graph attack as attack nodes and optimizes with $\mathcal{L}_a(\tilde{\mathbf{X}})$ using the \textbf{alterI}.
    %\item \textbf{G-guided-cf:} This is the graph-guided attack that selects the nodes involved in the graph attack as attack nodes and optimizes with $\mathcal{L}_a(\tilde{\mathbf{X}})$ using the \textbf{cf}-attack. 
    %\item \textbf{G-guided-plus:} This is the graph-guided attack that selects the attack nodes involved in the graph attack and optimizes with the graph-guided attack loss $\mathcal{L}_g(\tilde{\mathbf{X}})$ in Eqn~\ref{loss:attack_graph}.
\end{itemize}
\subsubsection{Hyper-parameters}
Grid search is employed to find the optimal hyper-parameters in all the attack methods over different datasets. For \textit{BiGraphRW} model, the regularization parameter $\lambda= 1 \times 10^{-6}$, learning rate $lr=1.0$, $60$ epochs with SGD optimizer. 
For \textit{ProxGraphRW} model, we evaluate proximity graphs constructed with cosine similarity and correlation similarity. The similarity threshold $\epsilon$ for constructing the graph is $0.8$ for the KDD-99 dataset and $0.5$ for the MNIST dataset; the regularization parameter $\lambda= 1 \times 10^{-4}$, learning rate $lr=1.0$, $35$ epochs for the KDD-99 dataset and $100$ for the MNIST dataset with Adam optimizer in the graph-space attack. In feature-space attack, learning rate $lr=1.0$, $500$ epochs with Adam optimizer. 



%\subsection{Experimental Results}
\subsection{Performance of Graph-space Attacks}

%and anomaly ranking: the average ranking of target nodes' anomaly scores among all nodes, the higher ranking, the higher possibility to be detected.
% First of all, the anomaly detection itself is effective with AUC (area under reception curve) of at least $0.89$ (Table~\ref{tab:auc}). 
% Under the prerequisite of a working anomaly detection algorithm, Fig.~\ref{tab:G-attack_bi},\ref{tab:G-attack_KDD}, and \ref{tab:G-attack_MNIST} present the evasion rate under different detection levels (top-$5\%/10\%$). At the most common detection level of top-$5\%$, for example, we observe that the evasion rate is over $85\%$ with a budget of $40.0\%$ in the bipartite graphs, indicating the effectiveness of our proposed attack under small budgets. Similar results can be seen in proximity graphs, where with a budget of $60.0\%$, the evasion rate (under detection threshold top-$10\%$) is over $80\%$ on the MNIST dataset. Our proposed graph attack methods, alterI and cf-attack, outperform other baselines significantly in all datasets.
%Fig.~\ref{fig:AD_result_a},\ref{fig:AD_result_c} shows the node degrees in the graphs before the attack, where our injected anomaly nodes are not extremely high compared to other nodes, which further shows that the anomaly scores are not simply relied on the node degrees. This implies that decreasing the anomaly scores might not be achieved by simply decreasing the neighbors. 
%\red{why do we need Fig. 4(a)(e)? What conclusions do we want to convey? Are the conclusions important for us?}\blue{- We inject the anomaly nodes by ourselves, and I want to clarify that the anomaly nodes we injected are not all with high degrees, and the RWAD are not simply predicting nodes with high degrees. Such that, decreasing the anomaly scores can not be achieved by simply decreasing the neighbours}. 

To begin with, we evaluate the performance of the target RWAD models over corresponding datasets. As shown in Table~\ref{tab:auc}, both models achieved an AUC (area under reception curve) of at least $0.89$, demonstrating a strong ability to identify anomalies. 


\subsubsection{Effectiveness of attacks}
%With this as a prerequisite, 
We present the evasion rates $\mathsf{ER}$ of those attack methods under different detection levels (top-$5\%/10\%$) in Table.~\ref{tab:G-attack_bi}, \ref{tab:G-attack_pro}.
%\red{mention two different similarity metrics} 
We can observe that our proposed graph attack methods, \textbf{alterI} and \textbf{cf}, significantly outperform other baselines on all datasets. For instance, at the detection level of top-$5\%$, our results indicate that our proposed attack on \textit{BiGraphRW} model is highly effective, achieving an evasion rate of over $85\%$ with a budget of $40.0\%$. Similarly, for \textit{ProxGraphRW} model, with a budget of $60.0\%$, the evasion rate (under detection threshold top-$10\%$) is over $80\%$ on the MNIST dataset. 
%\red{why top 5\% for table IV and 10\% for table V}
Since the MNIST dataset is relatively easier to attack, we report the attack performance at a higher detection threshold. The reason why the \textbf{DeepWalk} method does not exhibit a strong attack effect could be attributed to its transferability across different types of random walk models.
%Our proposed graph attack methods, \textbf{alterI} and \textbf{cf} attack, significantly outperform other baselines in all datasets.
\vspace{-5pt}
\begin{table}[htb]
\caption{AUC of RWAD.}
\label{tab:auc}
\centering
\begin{tabular}{lllll}
\hline
Models  & \multicolumn{2}{c}{\textit{BiGraphRW}} & \multicolumn{2}{c}{\textit{ProxGraphRW}} \\ \hline
Dataset & Author-Paper    & Magazine    & KDD-99          & MNIST         \\ \hline
AUC     & 1.00            & 0.89        & 0.98            & 0.90          \\ \hline
\end{tabular}
\vspace{-10pt}
\end{table}


\begin{table}[thb]
\centering
\setlength{\tabcolsep}{3.1pt}
\caption{Graph attack results on \textit{BiGraphRW} model.}
\label{tab:G-attack_bi}
\begin{tabular}{ccllllll}
\hline
\multicolumn{1}{l}{Dataset} & \multicolumn{1}{l}{Metrics} & budget & RndAdd & DegAdd & DeepWalk & alterI & cf \\ \hline
\multirow{12}{*}{\begin{tabular}[c]{@{}c@{}}Author-\\ Paper\end{tabular}} & \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}$\mathsf{ER}$ \\ (5\%)\end{tabular}} & 0\% & 0.560 & 0.560 & 0.560 & 0.560 & 0.560 \\
 &  & 20\% & 0.560 & 0.560 & 0.578 & \ul{0.720} & \textbf{0.760} \\
 &  & 40\% & 0.560 & 0.560 & 0.578 & \ul{0.880} & \textbf{0.940} \\
 &  & 60\% & 0.580 & 0.560 & 0.578 & \ul{0.920} & \textbf{0.960} \\
 &  & 80\% & 0.580 & 0.560 & 0.600 & \ul{0.980} & \textbf{1.000} \\
 &  & 100\% & 0.580 & 0.560 & 0.600 & \textbf{1.000} & \textbf{1.000} \\ \cline{2-8} 
 & \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}$\mathsf{ER}$\\ (10\%)\end{tabular}} & 0\% & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
 &  & 20\% & 0.000 & 0.000 & 0.000 & \ul{0.060} & \textbf{0.280} \\
 &  & 40\% & 0.000 & 0.000 & 0.000 & \ul{0.260} & \textbf{0.360} \\
 &  & 60\% & 0.000 & 0.000 & 0.000 & \textbf{0.460} & 0.360 \\
 &  & 80\% & 0.000 & 0.000 & 0.000 & \textbf{0.660} & 0.600 \\
 &  & 100\% & 0.000 & 0.000 & 0.000 & \textbf{0.820} & \ul{0.740} \\ \hline
\multirow{12}{*}{Magzine} & \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}$\mathsf{ER}$\\ (5\%)\end{tabular}} & 0\% & 0.740 & 0.740 & 0.740 & 0.740 & \textbf{0.740} \\
 &  & 20\% & 0.760 & 0.740 & 0.760 & \ul{0.760} & \textbf{0.780} \\
 &  & 40\% & 0.760 & 0.760 & 0.760 & \textbf{0.880} & \ul{\textbf{0.860}} \\
 &  & 60\% & 0.760 & 0.760 & 0.760 & \textbf{0.920} & \ul{\textbf{0.880}} \\
 &  & 80\% & 0.760 & 0.760 & 0.760 & \textbf{0.960} & \ul{\textbf{0.880}} \\
 &  & 100\% & 0.780 & 0.760 & 0.760 & \textbf{0.980} & \ul{\textbf{0.880}} \\ \cline{2-8} 
 & \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}$\mathsf{ER}$\\ (10\%)\end{tabular}} & 0\% & 0.380 & 0.380 & 0.380 & 0.380 & 0.380 \\
 &  & 20\% & 0.380 & 0.380 & 0.380 & \ul{0.500} & \textbf{0.600} \\
 &  & 40\% & 0.400 & 0.380 & 0.380 & \ul{\textbf{0.560}} & \textbf{0.740} \\
 &  & 60\% & 0.400 & 0.380 & 0.400 & \ul{\textbf{0.620}} & \textbf{0.760} \\
 &  & 80\% & 0.400 & 0.380 & 0.400 & \ul{\textbf{0.760}} & \textbf{0.820} \\
 &  & 100\% & 0.400 & 0.400 & 0.400 & \ul{\textbf{0.840}} & \textbf{0.860} \\ \hline
\end{tabular}
\end{table}

\begin{table}[htb]
\centering
\setlength{\tabcolsep}{2.5pt}
\caption{Graph attack results on \textit{ProxGraphRW} model.}
\label{tab:G-attack_pro}
\begin{tabular}{ccllllll}
\hline
\multicolumn{1}{l}{Dataset} & \multicolumn{1}{l}{Similarity} & budget & RndAdd & DegAdd & DeepWalk & alterI & cf \\ \hline
\multirow{14}{*}{\begin{tabular}[c]{@{}c@{}}KDD99\\ $\mathsf{ER}$ \\(5\%)\end{tabular}} & \multirow{7}{*}{cosine} & 0\% & 0.045 & 0.045 & 0.045 & 0.045 & 0.045 \\
 &  & 10\% & 0.045 & 0.045 & 0.045 & \ul{0.050} & \textbf{0.055} \\
 &  & 20\% & 0.045 & 0.045 & 0.045 & \ul{0.155} & \textbf{0.245} \\
 &  & 40\% & 0.045 & 0.045 & 0.045 & \ul{0.605} & \textbf{0.620} \\
 &  & 60\% & 0.045 & 0.045 & 0.045 & \ul{0.745} & \textbf{0.825} \\
 &  & 80\% & 0.055 & 0.045 & 0.050 & \ul{0.775} & \textbf{0.865} \\
 &  & 100\% & 0.085 & 0.045 & 0.060 & \ul{0.775} & \textbf{0.875} \\ \cline{2-8} 
 & \multirow{7}{*}{correlation} & 0\% & 0.045 & 0.045 & 0.045 & 0.045 & 0.045 \\
 &  & 10\% & 0.045 & 0.045 & 0.045 & \ul{0.055} & \textbf{0.060} \\
 &  & 20\% & 0.045 & 0.045 & 0.045 & \ul{0.110} & \textbf{0.150} \\
 &  & 40\% & 0.045 & 0.045 & 0.045 & \ul{0.315} & \textbf{0.405} \\
 &  & 60\% & 0.045 & 0.045 & 0.045 & \ul{0.575} & \textbf{0.690} \\
 &  & 80\% & 0.050 & 0.045 & 0.050 & \ul{0.670} & \textbf{0.735} \\
 &  & 100\% & 0.060 & 0.045 & 0.055 & \ul{0.695} & \textbf{0.845} \\ \hline
\multirow{14}{*}{\begin{tabular}[c]{@{}c@{}}MNIST\\ $\mathsf{ER}$\\ (10\%)\end{tabular}} & \multirow{7}{*}{cosine} & 0\% & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
 &  & 10\% & 0.000 & 0.000 & 0.000 & \textbf{0.060} & \ul{0.045} \\
 &  & 20\% & 0.000 & 0.000 & 0.000 & \textbf{0.210} & \ul{0.135} \\
 &  & 40\% & 0.000 & 0.000 & 0.000 & \textbf{0.585} & \ul{0.515} \\
 &  & 60\% & 0.000 & 0.000 & 0.020 & \ul{0.800} & \textbf{0.860} \\
 &  & 80\% & 0.005 & 0.000 & 0.030 & \ul{0.940} & \textbf{0.975} \\
 &  & 100\% & 0.050 & 0.000 & 0.070 & \ul{0.985} & \textbf{0.995} \\ \cline{2-8} 
 & \multirow{7}{*}{correlation} & 0\% & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
 &  & 10\% & 0.000 & 0.000 & 0.000 & \ul{0.045} & \textbf{0.060} \\
 &  & 20\% & 0.000 & 0.000 & 0.000 & \textbf{0.205} & \ul{0.185} \\
 &  & 40\% & 0.000 & 0.000 & 0.000 & \textbf{0.555} & \ul{0.550} \\
 &  & 60\% & 0.010 & 0.000 & 0.010 & \ul{0.770} & \textbf{0.825} \\
 &  & 80\% & 0.040 & 0.000 & 0.045 & \textbf{0.940} & \textbf{0.940} \\
 &  & 100\% & 0.095 & 0.005 & 0.080 & \textbf{0.995} & \textbf{0.995} \\ \hline
\end{tabular}
\vspace{-5pt}
\end{table}



%\red{what's the purpose of stating the following observations? You need to state some conclusions and use experiment observations to support the conclusions.}
Comparing \textbf{alterI} and \textbf{cf} attack, it was observed that \textbf{cf} attack slightly outperforms \textbf{alterI} in most cases. In our experiments, we observe that \textbf{cf} can achieve significantly lower attack loss in the continuous domain (i.e., $\tilde{B}$). However, when discretizing the optimization results, the attack performance is not guaranteed to be preserved. While \textbf{cf} is generally more effective (also observed for feature-space attacks in Section~\ref{featureAttackResults}), \textbf{alterI} is more efficient on larger graphs such as KDD-99 and MNIST (see Table~\ref{tab:runtime}).


%In Fig~\ref{fig:attack-loss}, we further analyze the difference between \textbf{alterI} and \textbf{cf} during the optimization. Our analysis revealed that \textbf{cf} attack achieved significantly lower attack loss in continuous space ($\tilde{B}$) as the number of iterations increased (Fig.\ref{fig:attack-loss_a}). This might own to the more precise attack loss and gradient obtained by \textbf{cf}. 
%We further analyze the performance in discrete space by selecting the top-$200$ edges from $\tilde{B}$ to construct $\bar{B}$, we observe that \textbf{cf}-attack eventually leads to much lower anomaly scores (Fig.\ref{fig:attack-loss_b}). Nevertheless, the performance of \textbf{cf} fluctuates after $120$ epochs, so we early stop the iteration at epoch $100$, where the performance of \textbf{alterI} and \textbf{cf}-attack are very similar. This may be the reason why sometimes \textbf{alterI} outperforms \textbf{cf}. Notably, the \textbf{cf} optimization strategy also shows its benefit in the feature-space attack (Section~\ref{featureAttackResults}).
%Despite being more effective in most cases, \textbf{cf}-attack takes twice as long as \textbf{alterI} to run in larger graphs such as KDD-99 and MNIST (see Table~\ref{tab:runtime}).



%{\color{red} When presenting experiment results, the most important thing is to clearly state the take-away message for the readers. What conclusions can we get? (the answers to the questions at the beginning) Furthermore, besides these conclusions, are there any other observations/insights? The worst thing is: simply state "the results are shown in Fig..."}

%In both proximity graph constructed with cosine similarity and correlation, the anomaly scores are greatly decreased (Fig.~\ref{fig:Pro_results_a},\ref{fig:Pro_results_b}), and evasion rates are significantly improved (Fig.~\ref{fig:Pro_results_c},\ref{fig:Pro_results_d}). With the budget $20.0\%$ proportion to the target nodes' degree, the evasion rate is over $80\%$ on the MNIST dataset. 



%% Figure environment removed





\subsubsection{Preferences of graph attack}
%{\color{red} Think more about the purposes of presenting these results.}
We further present a more detailed analysis of the graph attack results in Fig.~\ref{fig:graph_attack_analysis}.
Fig.~\ref{fig:analysis_a} and \ref{fig:analysis_b} show the proportion of the attacked nodes (to the total number of nodes) corresponding to different budgets. On average, only about $1\%-6\%$ (KDD-99) and $0.3\%-2.7\%$ (MNIST) of nodes are involved in the edge modification under various budgets (Fig.~\ref{fig:analysis_b}).
%\red{suggestion: remove fig 5 (a) and (b). Reason: changing thousands of edges looks not good. What you can do is: at a global perspective, what's the percentage of edges modified. What's the percentage of attack nodes involved. This result is important for graph-guided feature-space attack, because we do not want to control too many nodes. }
In Fig.\ref{fig:analysis_c}, we present the node degrees of attack nodes and others, and we observe that the attacker prefers nodes with lower degrees as attack nodes.
Fig.~\ref{fig:analysis_d} presents the weights changed in the attack. We observe that when the budget is limited, the attacker tends to delete the edges with the original weights
%\red{what's the meaning of weights?} \blue{In the proximity graph, we use the similarity between vertices as the weighted adjacency matrix.} 
close to $1.0$ or add edges with original weights close to $0$. The attacker mainly adds/deletes edges between target nodes and other nodes (Fig.~\ref{fig:analysis_e}), and the target-other edge modification tends to increase the degree of target nodes (Fig.~\ref{fig:analysis_f}). These actually provide convenience for our graph-guided feature attack with attack loss $\mathcal{L}_g(\tilde{\mathbf{X}})$, where the target node features are fixed (the edges between target-target are fixed) and the attack nodes can be optimized to be close to the desired edge weights (the edges between target nodes and control nodes). We observe similar phenomena in the MNIST dataset. 

% Figure environment removed


% Figure environment removed


% Figure environment removed




\subsection{Performances of Graph-guided Feature-space Attacks}
\label{featureAttackResults}
\subsubsection{Effectiveness of attacks}
We compare the performances of the feature-space attacks in Fig.~\ref{fig:feature_attack}. 
%\red{this is a good example of using experiment results to support your conclusions:}
Our analysis shows that \textbf{G-guided-alterI} outperforms the \textbf{VanillaOpt} method, achieving much lower anomaly scores and higher evasion rates. These two models are only different in the selection of attack nodes, which indicates the effectiveness of using guidance from graph-space attacks in node selection. 
Comparing the performance of the \textbf{alterI} and \textbf{cf} attack strategies under $\mathcal{L}_a$, we observe that \textbf{cf}-attack also improves the performance. However, the side effect is that \textbf{cf}-attack takes about 7 times longer than \textbf{alterI} in our experiments (Table~\ref{tab:runtime}). 
Additionally, \textbf{G-guided-plus} has a higher evasion rate than \textbf{G-guided-alterI} and \textbf{G-guided-cf} in most cases, indicating the advantage of using the attack loss $\mathcal{L}_g$ as further guidance for feature attack. 


\subsubsection{Unnoticeability of attack}
%\red{State unnoticeability as an advantage of proposed attack}
In Fig.\ref{fig:feature_analysis}, we provide an analysis of the feature attack highlighting its advantage of unnoticeability. As mentioned earlier, the graph attack prefers the attack nodes with lower degrees. As a result, our graph-guided attack nodes have lower node degrees compared to \textbf{VanillaOpt} (Fig.~\ref{fig:feature_analysis_a}). This leads to significantly fewer edge modifications in graph-guided attacks compared to \textbf{VanillaOpt} (Fig.\ref{fig:feature_analysis_b}), which enhances the unnoticeability of the attack.

%Compared to the graph-space attack, \textbf{G-guided-alterI} has better performance when the budgets are small, and this might be due to the flexibility of the feature space.
%As shown in Fig.~\ref{fig:feature_analysis_b},\ref{fig:feature_analysis_d} the feature attacks lead to much more edge modification than the budget in the graph attack. 


\begin{table}[htb]
\caption{Runtime comparison of \textbf{alterI} and \textbf{cf}-attack.}
\label{tab:runtime}
\centering
\setlength{\tabcolsep}{4.0pt}
\begin{tabular}{llcccc}
\hline
 & Attacks & Author-Paper & Magazine & KDD-99 & MNIST \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Graph\\  attack\end{tabular}} & alterI & 00:00:07 & 00:00:04 & 00:00:10 & 00:00:16 \\
 & cf & 00:00:02 & 00:00:02 & 00:00:23 & 00:00:35 \\ \hline
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Feature\\ attack\end{tabular}}} & alterI & - & - & 00:00:27 & 00:00:18 \\
\multicolumn{1}{c}{} & cf & - & - & 00:03:26 & 00:01:49 \\ \hline
\end{tabular}
\vspace{-5pt}
\end{table}

\subsection{Transferability of graph-guided attack}
We transfer our feature-space attacks to several unsupervised anomaly detection models, including Beta-VAE~\cite{burgess2018understanding}, IForest~\cite{liu2008isolation}, and ECOD~\cite{li2022ecod}. 
Table~\ref{tab:transferAttack} shows the anomaly scores of target nodes before and after the transfer attack based on our \textbf{G-guided-alterI} and \textbf{G-guided-plus} feature attack on the KDD-99 dataset with feature constraint considered. The results indicate that the graph-guided attack with graph attack loss significantly decreases the anomaly scores of the target nodes across different models. This suggests that the graph-guided attack on RWAD has the potential to be used as a surrogate model for black-box attacks. The graph-guided attack could be a useful tool for attackers to evade detection and deceive anomaly detection systems in real-world scenarios. 



\begin{table}[htb]
\centering
\caption{Transferability: The change in anomaly score (\%) compared to the clean data. Lower is better. }
\label{tab:transferAttack}
\setlength{\tabcolsep}{3.0pt}
\scalebox{0.98}{
    \begin{tabular}{ccccccc}
    \hline
        Detect Methods             &Attack Methods    & 20\%  & 40\%  & 60\%  & 80\%  & 100\% \\ \hline
        \multirow{3}{*}{Beta-VAE} & VanillaOpt           & -11.56 & -13.97 & -14.70 & -15.18 & -15.68 \\
        \multirow{3}{*}{}         & G-guided-alterI & -4.15 &  -5.65 &  -6.13 &  -7.14 & -9.711  \\
        \multirow{3}{*}{}         & G-guided-plus & \textbf{-25.26} & \textbf{-31.79} & \textbf{-33.25} & \textbf{-33.94} & \textbf{-33.99} \\ \hline
        
        \multirow{3}{*}{IForest}  & VanillaOpt                           & -10.63 & -0.06 & -8.41 & -0.82 & -11.93 \\
        \multirow{3}{*}{}         & G-guided-alterI      & 9.94 & 10.44 & -0.18 &  0.39 & -2.31  \\
        \multirow{3}{*}{}         & G-guided-plus & \textbf{-25.03} & \textbf{-44.21} & \textbf{-40.27} & \textbf{-47.58} & \textbf{-47.26} \\ \hline
        
        \multirow{3}{*}{ECOD}     & VanillaOpt                           & -2.20 & -2.72 & -2.90 & -2.988 & -3.099 \\
        \multirow{3}{*}{}         & G-guided-alterI      & -0.29 & -0.64 & -0.66 & -0.90 & -1.28 \\
        \multirow{3}{*}{}         & G-guided-plus & \textbf{-3.70} & \textbf{-5.32} & \textbf{-5.81} & \textbf{-6.01} & \textbf{-6.00} \\ \hline
    \end{tabular}
}
\vspace{-5pt}
\end{table}


\section{Conclusion}
\label{conclusion}
In this paper, we introduce a novel study of adversarial poisoning attacks on RWAD, where the graph is constructed on top of the feature space. We provide a theoretical understanding of these attacks, including a proof of NP-hardness. Our approach involves proposing graph-space attacks and using the graph attack to guide the feature-space attack, which bridges the gap between these two attacks. Our experiments on four datasets, including directly and indirectly accessible graphs, demonstrate the effectiveness of our proposed graph-space attack and its ability to guide the selection of attack nodes and optimization of the attack loss for feature-space attacks. Our study provides valuable insights into the effectiveness of graph-space attacks and feature-space attacks. Future research can extend this work to apply RWAD for black-box attacks on other deep learning-based anomaly detection systems, without relying on labeled data or inner models.

\clearpage
\newpage
\bibliographystyle{IEEEtran} 	
\bibliography{IEEEabrv,references}
\end{document}











%{\color{red} \section{A story of this paper}
%Before writing a paper, you can write a very short story of this paper, summarizing the \textbf{logic chain} in point form. Imagine that you are telling a story to a non-expert. This will help you clear your mind and allow you to see the big picture of your paper.
%}
%\blue{
%\begin{itemize}
%    \item Introduction
%    \begin{itemize}
%    \item RW is a widely used unsupervised anomaly detection tool
%    \item Security analysis is lacking in it. Previous studies have not studied the attack on RW in anomaly detection.
%    \item Structural and feature are two types of attack and are necessary for different applications/scenarios. 
%    \item Attacks on RW anomaly detection face some challenges. We prove that the structural attack is NP-hard, and the feature attack in the proximity graph is even more complex due to the strong relationship with the graph.  
%    \item Regardless of the hardness, we proposed practical attacks on it and showed its effectiveness. 
%    \end{itemize}
%    \item Preliminary: we describe the RW model for two types of graphs: Bipartite and Proximity graph.
%    \item Threat model: According to different application scenarios, we make different settings for the attack. For graph data, on which the attacker can directly manipulate the graph structure, we assume that the attacker can insert/delete at most $K$ edges; For proximity graph, although we can also attack the graph structure, the original data is in feature space. That is, modifying the graph should be mapped back to the feature space. Hence, we assume that the attacker can modify the features of at most $K$ nodes. 
 %   \item STRUCTURAL ATTACK: 1) the attack can be formulated into an optimization problem 2)we prove that the structural attack is NP-hard. 3) we propose a feasible solution to approximate the problem.
%    \item FEATURE ATTACK: In this problem, we face with the difficulty of optimizing both the selection of attack nodes and their features simultaneously. To tackle the challenge, we introduce a graph-guided attack that uses the structural attack results to select the attack nodes. Furthermore, the graph can also provide a direction about how we should modify the features to approach our attack goals. 
%\end{itemize}
%}
% % Figure environment removed



% % Figure environment removed










% % Figure environment removed




% % Figure environment removed

