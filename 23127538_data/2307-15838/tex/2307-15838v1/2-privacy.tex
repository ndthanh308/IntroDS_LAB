
\section{Privacy}\label{Sec: privacy}
\subsection{Preliminaries} 
%The remarkable capabilities of ML methodologies, including supervised, unsupervised, semi-supervised, and RL, have captured the attention of many scholars in recent years. Despite their potential benefits \cite{medeiros2021forecasting}, concerns have been raised regarding the privacy implications of using ML in the era of big data. The use of sensitive personal data to train ML models can lead to the exposure of potentially sensitive information and the identification of individuals, posing a serious privacy threat \cite{song2019privacy}. Thus, it is essential to develop ML algorithms that respect privacy and mitigate such risks. To this end, we present an in-depth analysis of the privacy implications of ML technologies in this section, with the aim of promoting the development of privacy-preserving solutions and the responsible deployment of ML in various domains. 




%\subsubsection{Current Challenges and Motivations}
ML has transformed various industries, including healthcare \cite{nayyar2021machine}, transportation \cite{tizghadam2019machine}, and finance \cite{dixon2020machine}. The capacity of these algorithms to process vast quantities of data, identify patterns, generate predictions, and deliver precise and efficient recommendations is remarkable. However, the use of personal data in ML models has given rise to significant privacy concerns. A primary concern is the potential misuse of sensitive personal information \cite{ngiam2019big}, such as names, addresses, social security numbers, and medical records. If not adequately safeguarded, such data could lead to identity theft, financial fraud, and other adverse consequences. The issue of data privacy was starkly highlighted in $2018$ when Cambridge Analytica illicitly harvested data from millions of Facebook users \cite{hinds2020wouldn}. This data was subsequently used to craft effective political advertisements during the 2016 US presidential election~\cite{berghel2018malice}, sparking widespread concern over the use of personal information in political campaigns.

To address these privacy concerns, researchers are developing privacy-preserving algorithms that secure sensitive data while allowing accurate and efficient model creation. Differential privacy (DP) is one such technique, which adds noise to the data to prevent individual records from being identified~\cite{dwork2008differential}. Another technology that allows data to be processed without being decrypted is homomorphic encryption~\cite{acar2018survey}, which ensures that sensitive information is never divulged. While some research have focused on specific privacy concerns connected to certain types of ML learning methodologies, such as supervised, unsupervised, semi-supervised, and RL, a comprehensive evaluation of all potential privacy hazards and remedies is still lacking. Furthermore, there may be privacy issues that are specific to specific domains or applications that necessitate a more concentrated analysis. Consequently, it is necessary to continue exploring privacy concerns in these ML techniques to guarantee that all potential dangers are properly detected and addressed. This will ensure that ML technologies are developed and deployed in a way that respects individuals' privacy rights while also minimizing the possible damages associated with these technologies. 



\begin{comment}
\subsubsection{ML Approaches}


% Figure environment removed


The classification of ML models is a fundamental task that plays a crucial role in many applications, ranging from computer vision to natural language processing. A key aspect of this classification is the identification of the underlying probability distributions that the models learn to recognize. In the context of supervised learning, where input data is classified into corresponding labels, \bfph{discriminative} and \bfph{generative} models have emerged as two primary methodologies:
\begin{itemize}
    \item \textit{Discriminative models \cite{adel2018discovering}:}  Discriminative models, in contrast, aim to learn the conditional probability distribution $p(y|x)$, to differentiate between various classes such as men and women. They focus on determining the most probable label for a given input, rather than generating new data. Despite their limited scope compared to generative models, discriminative models are highly effective in their specific domain of application, including image classification, natural language processing, and speech recognition. Discriminative models are crucial for accurately classifying input data in various real-world applications.
    \item \textit{Generative models \cite{theis2015note}:} Generative models capture the joint probability distribution $p(x,y)$ of input data and labels. These models offer a comprehensive understanding of data and its labels, useful for tasks like data augmentation. Prominent examples include generative adversarial networks (GANs), known for their ability to generate new data mimicking statistical properties of the training set. GANs are a powerful tool for data generation and synthesis, applied in various fields like image and speech generation.
\end{itemize}
ML learning tasks can also be classified as either \bfph{centralized} or \bfph{distributed}, based on data location, computation location, communication overhead, and scalability:
\begin{itemize}
    \item \textit{Centralized learning:} In centralized learning, all the data and computation are stored and processed in a single central location, such as a server or data center. This paradigm enables the collection of all data in one place and the performance of all computation on a single machine or a cluster of machines in a central location. Centralized ML offers numerous advantages, including ease of management, low communication overhead, and efficient utilization of hardware resources.
    \item \textit{Distributed learning:} Distributed learning, on the other hand, involves multiple participants collaborating to create a joint model by training their own local model on their respective data and exchanging updates, model parameters, or partially constructed models with other participants. Collaborative filtering is an example of a technique used in distributed learning that is utilized in recommender systems to provide personalized recommendations to users.
\end{itemize}
\subsubsection{Privacy Attacks in ML}
The domain of ML is susceptible to privacy attacks, which can be orchestrated by nefarious actors to compromise the privacy of individuals whose data is utilized in training or testing ML models. The adverse outcomes of such attacks can be disastrous, leading to the loss of personal identification, financial fraud, and various other forms of privacy breaches.  Following are four common privacy attacks that have gained notoriety in the sphere of ML (see figure~\ref{fig1}):

    \begin{itemize}
        \item \textit{Membership inference attacks \cite{shokri2017membership}:} Membership inference attacks are executed by exploiting the behavior of the ML model to ascertain if a person's data was used in its training dataset. To accomplish this, the attacker needs access to the model and some knowledge of the original training dataset, enabling them to train a separate model with a similar dataset for launching the attack. This attack involves querying a target model with data and inferring whether that data was part of the model's training dataset. It relies on the prediction probabilities and label of the queried data.
        \item \textit{Model extraction attacks \cite{chandrasekaran2020exploring}:} Model extraction attacks pilfer a copy of a trained ML model without authorization by analyzing its responses to queries and reconstructing a similar copy. In the context of ML as a service (MLaaS), a malicious actor attempts to extract a proprietary model from the server's interface via a model extraction attack, akin to a learner acquiring knowledge through supervised learning. This attack poses a significant threat to data owners and underscores the importance of robust security measures to safeguard intellectual property.
        \item \textit{Model inversion attacks \cite{fredrikson2015model}:} Model inversion attacks involve recreating the initial input data utilized in training an ML model, using the learned attributes of the model. Attackers estimate the original data using the model's parameters, which is not precise, yet useful for targeted attacks. This may involve using statistical techniques, such as optimization or ML algorithms, to estimate the training data or input records used by the model. The attacker can then use this information to compromise user privacy or expose proprietary information.
        \item \textit{Adversarial attacks \cite{apruzzese2019addressing}:} Adversarial attacks manipulate input data deliberately to confuse an ML model. These attacks aim to mislead the model into producing erroneous predictions or classifications by making tiny and often undetectable modifications to the input data. This attack generates adversarial samples at test time by perturbing original input data to create new samples that are designed to mislead ML models. These adversarial samples are then used to evaluate the model's robustness against potential attacks in real-world scenarios.
    \end{itemize}

\end{comment}




\subsection{Privacy Techniques}
\subsubsection{Differential Privacy (DP)}


DP is a privacy protection method commonly used in different stages of the ML pipeline to enhance privacy of individuals. In this section, we will examine the concepts and definitions, common DP mechanisms, and applications of DP in various ML techniques.
\paragraph{Notions and Definitions}
\begin{defn}[$\epsilon$-Differential Privacy\cite{dwork2008differential}] 
A randomized algorithm $\mathcal{M}$ is said to be $(\epsilon, \delta)$-differentially private if, for any two datasets $D_1$ and $D_2$ that differ in only one data point, and any subset of the range of $\mathcal{M}$, the following holds:
    \begin{equation}
        Pr[M(D_1) \in S] \leq e^{\epsilon} Pr[M(D_2) \in S] + \delta,
    \end{equation}
where $\epsilon$ and $\delta$ are privacy parameters, and $S$ is any subset of the range of $\mathcal{M}$. This inequality ensures that the probability of observing a certain output of $\mathcal{M}$ on a dataset $D_1$ is almost the same as the probability of observing the same output on a dataset $D_2$ that differs in only one data point, with the exception of a small amount of random noise controlled by $\epsilon$ and $\delta$. The parameter $\epsilon$ controls the strength of the privacy guarantee, with lower values providing stronger privacy protection, while $\delta$ is a parameter that accounts for the probability that the privacy guarantee is violated due to the randomness introduced by the algorithm.
\end{defn}

\begin{defn}[$L1$-Sensitivity\cite{dwork2006calibrating}] 
L1-sensitivity is a measure of how much the output of a function changes when a single data point is added or removed from a dataset. It is defined as the maximum absolute difference between the output of the function on two adjacent datasets that differ in only one data point. Formally, given a function $f: \mathcal{D} \rightarrow \mathbb{R}^n$ that maps datasets in domain $\mathcal{D}$ to vectors in $\mathbb{R}^n$, the L1-sensitivity of $f$ is defined as:
\begin{equation}
    \Delta f = \max_{d\in D, d' \sim d} \left\vert\vert f(d) - f(d') \right\vert\vert_1,
\end{equation}
where $d'$ is the neighboring dataset that differs from $d$ by a single data point, and $||\cdot||_1$ denotes the L1-norm. Intuitively, L1-sensitivity captures the largest change that can occur in the output of $f$ due to the presence or absence of a single data point. It is a fundamental parameter in DP, as it determines the amount of noise that needs to be added to the output of $f$ to achieve a desired level of privacy protection.
\end{defn}

\paragraph{Common Mechanisms in DP}
DP approaches involve the addition of controlled noise to data to safeguard the privacy of people while preserving the accuracy of analytic results. The \textit{Laplace mechanism} and \textit{Exponential mechanism} are two often used differentially private mechanisms, which are detailed as follows.
\begin{table*}
    \centering
    \caption{Comparison of Laplace and Exponential Mechanisms in DP.}
    \label{Table:ComparisonDPMechanisms}
    \renewcommand{\arraystretch}{1.4}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Mechanism} & \textbf{Description} & \textbf{Pros} & \textbf{Cons} \\
        \hline
        Laplace & \makecell[l]{Adds independent noise drawn from \\a Laplace distribution to the true out-\\put, proportional to the sensitivity of\\ the query and inversely proportional \\to the privacy budget.} & \makecell[l]{Simple implementation, \\provides strong privacy\\ guarantees, and works\\ efficiently for simple qu-\\eries.} & \makecell[l]{Produces noisy results, calib-\\ration of noise parameter can \\be difficult, may not perform\\ well for high-dimensional da-\\ta or complex queries.} \\ 
        \hline
        Exponential & \makecell[l]{Adds independent noise drawn from\\ an exponential distribution to the tr-\\ue output, proportional to the sensiti-\\vity of the query and inversely prop-\\ortional to the privacy budget.} & \makecell[l]{More precise results th-\\an Laplace, can perform\\ well for high-dimensio-\\nal data or complex que-\\ries.} & \makecell[l]{Requires more sophisticated\\ implementation, may be vul-\\nerable to adaptive attacks, \\calibration of noise parame-\\ter can be challenging.} \\ 
        \hline
    \end{tabular}
\end{table*}
\subparagraph{Laplace Mechanism}
The Laplace mechanism \cite{holohan2018bounded} is a method for achieving DP by adding random noise to the output of a query in a way that satisfies DP guarantees. Specifically, given a function $f: D \rightarrow R$ that we want to compute on a dataset $D$, the Laplace mechanism adds random noise to $f(D)$ according to the following formula:
\begin{equation}
    f(D)+Lap\Bigg(\frac{\Delta f}{\epsilon}\Bigg),
\end{equation}
where $Lap(\Delta f/\epsilon)$ is a random variable drawn from the Laplace distribution with mean 0 and scale parameter $\Delta f/\epsilon$, where $\Delta f$ is the sensitivity of the function $f$ and $\epsilon$ is the privacy parameter that controls the amount of noise added. More formally, the Laplace distribution is defined as:
\begin{equation}
    Lap(x \mid \mu, b) = \frac{1}{2b} \exp \left( -\frac{|x-\mu|}{b} \right),
\end{equation}
where $\mu$ is the mean and $b$ is the scale parameter. In the case of the Laplace mechanism, the mean is 0 and the scale parameter is $\Delta f/\epsilon$, so the Laplace distribution becomes:
\begin{equation}
    Lap(x\mid 0,\Delta f/\epsilon)=\frac{1}{2(\Delta f/\epsilon)}\exp\Bigg(-\frac{|x|}{\Delta f /\epsilon}\Bigg).
\end{equation}
Adding Laplace noise to the output of $f(D)$ in this way ensures that the output is differentially private with parameter $\epsilon$. The amount of noise added is proportional to the sensitivity of the function $f$, with higher sensitivities resulting in more noise being added to the output. \newline
Although the Laplace mechanism is commonly used to achieve DP in ML, it has several limitations \cite{koufogiannis2015optimality} \cite{fernandes2021laplace}. The amount of noise added to the data depends on the sensitivity of the function being computed, which can be significant for some functions. This can lead to a considerable loss of output accuracy, making it difficult to obtain meaningful results. Moreover, the Laplace mechanism assumes that the data is continuous and unbounded, which may not always hold for all datasets. Furthermore, the Laplace distribution employed in the mechanism may not be optimal, as it assumes that the noise added to the data is symmetric, which may not be the case in reality. However, this technique is also commonly employed in terms of DP. For example, methods have been devised in \cite{rastogi2010differentially} and \cite{shaham2022differentially} for releasing counts on specific types of data, such as time series. The authors from \cite{xu2013differentially}, \cite{shaham2022htf} and \cite{shaham2021htf} concentrate on releasing histograms, while other authors in \cite{xiao2010differential}, \cite{cormode2012differentially} present ways for reducing the worst-case error of a specified set of count queries.





\subparagraph{Exponential Mechanism}
The exponential mechanism is proposed in \cite{mcsherry2007mechanism}, which is a privacy-preserving approach that selects an item from a dataset based on a specific objective function. It ensures the confidentiality of individuals in the dataset while maximizing the objective function. Formally, let $f : D \to R$ be a function that maps a dataset $D$ to a real number. The exponential mechanism selects an output $d \in D$ with probability proportional to the exponential of the privacy loss incurred by releasing $f(d)$, scaled by a parameter $\epsilon > 0$, which controls the amount of privacy protection:
\begin{equation}
    P(M(D)=d) \propto \exp\left(\frac{\epsilon f(d)}{2\Delta f}\right),
\end{equation}
where $\epsilon$ is the privacy budget and $\propto$ denotes proportionality. The denominator $2\Delta f$ is used to scale the noise so that it is proportional to the sensitivity of the objective function. \newline
In practice, the exponential mechanism is used when we want to select an element from a dataset that satisfies a certain property while minimizing the disclosure of information about the other elements in the dataset. For example, we might want to select a movie from a database that satisfies certain genre preferences while minimizing the disclosure of information about the users who rated the other movies in the database. \newline
Popular way for creating DP in ML, the exponential mechanism has certain limitations\cite{dong2020optimal}. When the dataset is huge, the exponential process can be computationally costly. In addition, as the privacy parameter falls, the precision of the result degrades. In addition, the exponential technique is only appropriate for functions with a low sensitivity value. If the function has a high sensitivity value, the mechanism will add an excessive amount of noise to the output, reducing its precision. In certain instances, such as when the output space is discrete or the objective function is non-convex, the exponential mechanism might be biased. Despite these limitations, this permits DP solutions for a variety of intriguing issues with non-real outputs. As an illustration, the exponential mechanism has been used in the publication of audition results \cite{mcsherry2007mechanism}, coresets \cite{feldman2009private}, support vector machines \cite{rubinstein2009learning}, and frequent patterns \cite{bhaskar2010discovering}.


\paragraph{Spectrum of DP Variations}
\subparagraph{Differential Privacy Stochastic Gradient Descent (DP-SGD)}
DP-SGD emerged from the convergence of two important concepts in the field of ML: Stochastic Gradient Descent (SGD) and DP. SGD is an iterative method for optimizing an objective function and has been extensively used in ML, especially in the training of large-scale deep neural networks. The idea was to provide formal privacy guarantees when disclosing statistical information about a dataset. In 2016, Abadi et al. \cite{abadi2016deep} successfully combined these concepts to develop DP-SGD, a variant of SGD that offers strong privacy guarantees by incorporating differential privacy into the optimization process. 

The DP-SGD algorithm begins by sampling a minibatch from the dataset. For each instance in the minibatch, the gradient $\nabla L(\theta; x)$ of the loss function $L$ with respect to the model parameters $\theta$ is computed. This results in a vector of gradients for the minibatch. The next crucial step in DP-SGD is gradient clipping. This process involves limiting the $L_2$ norm of each individual gradient vector to a predefined threshold $C$. In mathematical terms, this operation can be expressed as:
\begin{equation}
    \nabla L_{\text{{clipped}}}(\theta; x) = \min\left(1, \frac{C}{\|\nabla L(\theta; x)\|}\right) \nabla L(\theta; x).
\end{equation}
This gradient clipping operation ensures that the contribution of each individual instance to the gradient computation is limited, thereby mitigating the impact of outliers and reducing the sensitivity of the output to changes in the input data, a key requirement for DP. After gradient clipping, the algorithm computes the average of the clipped gradients and adds calibrated Gaussian noise to this average. If $G$ represents the average of the clipped gradients, the noisy gradient $G_{\text{{noisy}}}$ is given by:
\begin{equation}
G_{\text{{noisy}}} = G + \mathcal{N}(0, (\sigma C)^2\mathbf{I}),
\end{equation}
where $\mathcal{N}(0, (\sigma C)^2\mathbf{I})$ represents multivariate Gaussian noise with mean 0 and covariance matrix $(\sigma C)^2\mathbf{I}$, and $\mathbf{I}$ is the identity matrix. The model parameters $\theta$ are then updated using this noisy gradient.

DP-SGD is primarily used in scenarios where models need to be trained on sensitive data while preserving privacy. For instance, in healthcare, DP-SGD could be used to build predictive models using patient data without compromising individual privacy \cite{suriyakumar2021challenges}. DP-SGD has also been used in federated learning \cite{geyer2017differentially}, a paradigm where the model is trained across multiple decentralized edge devices, maintaining data on the original device. Several libraries and frameworks have been developed for implementing DP-SGD. Google's TensorFlow Privacy library provides a version of DP-SGD that can be used with TensorFlow models \cite{mcmahan2018general}. Another library is PyTorch-DP (now Opacus) \cite{yousefpour2021opacus}, which provides an implementation for PyTorch models. These libraries provide convenient tools to add privacy-preserving capabilities to ML models with minimal code changes.

\subparagraph{Differential Privacy for Support Vector Data Description (DP-SVDD)}
Support Vector Data Description (SVDD) \cite{tax2004support} is a one-class classification method that is often used for anomaly detection. The main idea behind SVDD is to find a hypersphere in the feature space that encapsulates the majority of the data points. This hypersphere is described by its center and radius, and it is found by solving an optimization problem that aims to minimize the radius while penalizing data points that lie outside the hypersphere. Mathematically, the SVDD problem can be formulated as follows:
\begin{align}
\text{Minimize:} \quad & R^2 + C \sum \xi_i, \\
\text{Subject to:} \quad & ||\phi(x_i) - a||^2 \leq R^2 + \xi_i \quad \text{and} \quad \xi_i \geq 0.
\end{align}
In this formulation, $R$ is the radius of the hypersphere, $a$ is the center, $\phi(x_i)$ is the mapping of data point $x_i$ in the feature space, $\xi_i$ is the slack variable that allows data points to lie outside the hypersphere, and $C$ is a regularization parameter that controls the trade-off between the volume of the hypersphere and the errors.
DP-SVDD, first introduced in \cite{park2023efficient}, is a method that combines the principles of SVDD with those of DP to create a privacy-preserving one-class classification model. The method involves two main phases. In the first phase, the goal is to train a SVDD model while ensuring differential privacy. The center of the hypersphere in the SVDD model is represented as a weighted sum of the mapped data points. To ensure DP, the center of the hypersphere is perturbed by adding noise. This noise is drawn from a Laplace distribution. The perturbed center, $\hat{a}$, is then given by:
\begin{equation}
\hat{a} = a + l = \sum_{i=1}^{n} b_i \phi(x_i) + l.
\end{equation}
In this formulation, $a$ is the center of the hypersphere, $b_i$ are the dual variables, $\phi(x_i)$ is the mapping of data point $x_i$ in the feature space, $l$ is the Laplace noise. The sensitivity is a measure of how much the output of a function can change when a single data point is added or removed from the dataset. In the second phase, the input space is partitioned into separate regions using a dynamical system based on the differentially private support function from the first phase. This dynamical system is defined by the gradient of the support function:
\begin{equation}
\frac{dx}{dt} = \nabla \hat{f}(x),
\end{equation}
where $\hat{f}(x)$ is the differentially private support function. Regions, associated with Equilibrium Points (EPs) of the dynamical system, are labeled using a noisy count of class labels of converging data points. The privacy-preserving predictions are released by publishing private EPs and labels. A new data point's label is predicted based on its region, determined by the EP it converges to. The privacy of predictions is ensured by the differential privacy of the support function and the noisy count.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The preceding discussion regarding the DP has improved our understanding of the definitions and mechanisms of the DP, as well as the limitations of each mechanism, which are summarized in Table \ref{Table:ComparisonDPMechanisms}. Additionally, a comprehensive examination of the real-world applications of DP in supervised, unsupervised, semi-supervised, and reinforcement learning is presented in Appendix B.%~\ref{section: appendix applications of DP}


\subsubsection{Homomorphic Encryption (HE)}
Homomorphic Encryption (HE) represents a cryptographic technique that confers the capacity to perform computations on encrypted data without the need for decryption. Such an approach stands out as a promising means for preserving data privacy while enabling useful computations to be conducted on it. The following section reviews and categorizes the concepts, prevalent HE mechanisms, and a range of applications of HE in the context of ML techniques.
\paragraph{Notions and Definitions}
\begin{defn}[Homomorphic Encryption\cite{yi2014homomorphic}] 
Homomorphic encryption enables computations to be performed on ciphertexts without the need to decrypt them first. Mathematically, let $f$ be an algebraic function and $Enc$ and $Dec$ be encryption and decryption functions respectively. Homomorphic encryption allows for the following equation to hold:
    \begin{equation}
f(\operatorname{Dec}_k(\operatorname{Enc}_k(m_1))\circ \operatorname{Dec}_k(\operatorname{Enc}_k(m_2)))=\operatorname{Enc}_k(f(m_1\circ m_2)),
    \end{equation}
In this equation, $m_1$ and $m_2$ are plaintext messages that are encrypted under the same key $k$ using HE. The function $f$ is a homomorphic function that operates on the plaintext messages, and the operator $\circ$ represents the algebraic operation that $f$ preserves. The equation shows that applying $f$ to the plaintext messages $m_1$ and $m_2$ and then encrypting the result under the key $k$ is equivalent to first encrypting the plaintext messages separately, applying the decryption function $\mathrm{Dec}_k$ to each ciphertext, performing the algebraic operation $\circ$ on the resulting plaintexts, and then encrypting the result again under the key $k$. This property allows computations to be performed on encrypted data without ever revealing the plaintext to the party performing the computation.
\end{defn}

\paragraph{Typical Schemes in HE}
There exist various schemes of homomorphic encryption, each possessing its unique merits and demerits. The \textit{Fully Homomorphic Encryption}, \textit{Partially Homomorphic Encryption}, and \textit{Somewhat Homomorphic Encryption} are three frequently utilized HE schemes, explicated as follows.

%\setlength{\tabcolsep}{40pt}

\begin{table*}
    \centering
    \caption{Comparison of Homomorphic Encryption Schemes.}
    \label{Table:ComparisonHESchemes}
    \renewcommand{\arraystretch}{1.4}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Property} & \textbf{FHE} & \textbf{PHE} & \textbf{SHE} \\
        \hline
        Supports addition & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ 
        \hline
        Supports multiplication & $\checkmark$ & $\times$ & $\checkmark$ \\ 
        \hline
        \makecell{Supports arbitrary \\circuits} & $\checkmark$ & $\times$ & $\times$ \\ 
        \hline
        \makecell{Computational \\complexity} & High & Moderate & Low \\ 
        \hline
        \makecell{Encryption/decryption\\ speed} & Slow & Moderate & Fast \\ 
        \hline
        Application examples & \makecell{Cloud computing, privacy-\\preserving machine learning} & \makecell{Secure multi-party compu-\\tation, secure function eva-\\luation} & \makecell{Privacy-preserving data \\analysis, secure computa-\\tion protocols} \\ \hline
    \end{tabular}
\end{table*}

\subparagraph{Fully Homomorphic Encryption (FHE)}
FHE allows computations on encrypted data without decryption, leading to direct computation on ciphertexts and yielding an encrypted plaintext. It employs lattice-based cryptography \cite{micciancio2009lattice} with ideal lattices to efficiently compute homomorphic operations. This type of cryptography is based on mathematical lattices, regular patterns of points. FHE carries out encryption and decryption on ideal lattices, which are sets of linear combinations of $n$ independent vectors with integer coefficients in $n$-dimensional space. Mathematically, this can be expressed as:
\begin{equation}
    L = {a_1.v_1 + a_2.v_2 + ... + a_n.v_n | a_i \in Z},
\end{equation}
where $L$ is the lattice, $v_1$, $v_2$, ..., $v_n$ are linearly independent vectors in $n$-dimensional space, and $a_1$, $a_2$, ..., $a_n$ are integers. The security of FHE is based on the hardness of certain problems related to lattices, such as the Shortest Vector Problem (SVP) and the Closest Vector Problem (CVP) \cite{hanrot2011algorithms}. These problems are known to be difficult to solve in high dimensions, which provides the basis for the security of FHE.
\newline
To perform homomorphic operations on ciphertexts in FHE, a technique called "bootstrapping" or "gating" is used. This technique involves decrypting the ciphertext using the secret key, performing a homomorphic operation on the resulting plaintext, and then encrypting the result using the public key. Mathematically, this can be represented as:
\begin{equation}
    C' = Enc_{pk}(F(Dec_{sk}(C))),
\end{equation}
where $C$ is the original ciphertext, $sk$ is the secret key, $pk$ is the public key, $F$ is the homomorphic operation, $Dec_{sk}(C)$ is the decrypted ciphertext, and $Enc_{pk}$ is the encryption function using the public key.
\newline
FHE presents a spectrum of advantages and disadvantages \cite{yousuf2020systematic}. On the one hand, FHE confers a paramount level of security as it allows for arbitrary computations on encrypted data without necessitating decryption. This characteristic proves exceptionally advantageous in domains such as ML and cloud computing, where privacy and security concerns are of utmost importance. Conversely, FHE exhibits a high level of computational complexity that can render it infeasible for certain applications \cite{ogburn2013homomorphic}. Moreover, with each homomorphic operation, the size of the ciphertext augments, requiring extensive memory, which can become a significant impediment \cite{kogos2017fully}. Despite these challenges, FHE remains an active area of research and development, with researchers continuously seeking ways to enhance its efficiency and transcend its limitations.

\subparagraph{Partially Homomorphic Encryption (PHE)}
Partially Homomorphic Encryption (PHE) is a type of encryption scheme that enables computation on encrypted data without the need to decrypt it \cite{moore2014practical}. Mathematically, PHE is defined using algebraic structures such as groups, rings, or fields to enable certain types of computation, such as addition or multiplication, on ciphertexts while still maintaining the confidentiality of the underlying plaintext \cite{sen2013homomorphic}. PHE schemes can be partially homomorphic, meaning that they support computations of only one type, such as addition or multiplication. For example, a PHE scheme that is homomorphic with respect to addition is defined by the following property:
\begin{equation}
    Enc(m_1) + Enc(m_2) = Enc(m_1 + m_2),
\end{equation}
where $m_1$ and $m_2$ are plaintext messages, $Enc$ is the encryption function, and $+$ denotes addition in the plaintext space $M$. This property allows ciphertexts to be added together and then decrypted to obtain the sum of the corresponding plaintexts. Similarly, a PHE scheme that is partially homomorphic with respect to multiplication is defined by the following property: 
\begin{equation}
    Enc(k \cdot m) = Enc(m)^k,
\end{equation}
where $k$ is a scalar value and $m$ is a plaintext message. This property allows a ciphertext to be raised to a scalar power $k$ without revealing the plaintext, but it does not allow multiplication of two ciphertexts to obtain a ciphertext that represents the multiplication of the corresponding plaintexts.
\newline
In reality, PHE is a powerful cryptographic technique that offers many benefits \cite{yu2012partial}. PHE allows for computations on encrypted data, enabling secure processing of sensitive data without its disclosure to unauthorized parties. Unlike FHE, PHE does not require heavy computational resources and is, therefore, much easier to implement in real-world applications. PHE can be implemented with relatively straightforward mathematical operations, making it both basic and effective. In addition, PHE can be used to build secure protocols for a range of applications, such as secure auctions, electronic voting, and secure multi-party computation. By keeping the data encrypted, PHE can ensure that sensitive information remains private while allowing authorized parties to perform meaningful computations on it. While there are some limitations to PHE \cite{hellwig2022distributed}, such as its limited computational capacity and susceptibility to attack if not implemented correctly, the benefits of this technique make it a valuable tool in a variety of situations.

\subparagraph{Somewhat Homomorphic Encryption (SHE)}
Somewhat Homomorphic Encryption (SHE) is a form of encryption that permits certain calculations on encrypted data without revealing the original data \cite{fan2012somewhat}. Using a polynomial representation of the plaintext and encrypting it with a public key is a central concept of SHE \cite{boneh2013private}. By manipulating the coefficients of the polynomial, it is possible to perform computations on the ciphertext. A commonly used example of a SHE scheme is the BGV (Bajard, Gentry and Vaikuntanathan) scheme \cite{aguilar2013recent}. Consider the BGV scheme, which operates over the polynomial ring $R_q = \mathbb{Z}[x]/(xn + 1)$, where $q$ is a prime number that determines the security level and $n$ is the degree of the polynomial. $R_2$ denotes the set of polynomials with coefficients in $\{0,1\}$, which is the plaintext space. 
\newline
To encrypt a plaintext polynomial $m(x)$, the BGV scheme first generates a random polynomial $r(x)$ with coefficients in $\{0,1\}$. It then computes the ciphertext polynomial $c(x)$ as:
\begin{equation}
    c(x) = r(x) * pk + m(x) * 2^k \text{ (mod } q \text{)},
\end{equation}
where $pk$ is the public key, $k$ is a positive integer, and * denotes polynomial multiplication. The random polynomial $r(x)$ serves as a noise term that hides the underlying plaintext, while the term $m(x) * 2^k$ ensures that the ciphertext coefficients are sufficiently large to prevent decryption attacks. Conversely, to decrypt a ciphertext $c(x)$, one needs to compute:
\begin{equation}
    m(x) = c(x) * sk \text{ (mod } q \text{)} \text{ mod } 2,
\end{equation}
where $sk$ is the secret key. The term $c(x) * sk$ cancels out the noise term $r(x)$, and yields the original plaintext polynomial $m(x)$. Finally, to perform a computation on two ciphertexts $c_1(x)$ and $c_2(x)$, one can simply add or multiply them as polynomials, and obtain the resulting ciphertext $c_3(x)$ as:
\begin{equation}
    c_3(x) = c_1(x) + c_2(x) \text{  or 
 }c_3(x) = c_1(x) * c_2(x).
\end{equation}
Though SHE can enable computations to be performed on encrypted data without requiring the data to be decrypted, this scheme suffers from certain limitations that affect its practicality in certain scenarios \cite{hamza2022towards}. These limitations stem from its lack of full homomorphic capabilities, which constrain the range of computations that can be performed. Furthermore, SHE is typically associated with higher computational overheads and requires more computational resources, which can affect its overall efficiency and practicality. Nevertheless, SHE offers several benefits \cite{migliore2018practical}, such as preserving the privacy of sensitive data, while allowing computations to be performed in a secure and confidential manner. This makes SHE a useful technique in scenarios where privacy and security are paramount, such as in the healthcare and financial sectors. Additionally, SHE can be used in conjunction with other cryptographic techniques, such as fully homomorphic encryption, to provide a more comprehensive security framework. Hence, despite its limitations, SHE remains a valuable and promising technique in the field of cryptography.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The preceding discussion regarding the HE has improved our understanding of the definitions and mechanisms of the HE, as well as the limitations of each schemes highlighted in Table \ref{Table:ComparisonHESchemes}. Appendix~\ref{section: appendix applications of HE} thorough review of practical uses of HE in supervised, unsupervised, semi-supervised, and RL.


The preceding discourse on HE has enhanced our knowledge of its definitions, mechanisms, and the limitations of each scheme, as detailed in Table \ref{Table:ComparisonHESchemes}. Additionally, a thorough examination of the practical applications of HE in supervised, unsupervised, semi-supervised, and reinforcement learning is provided in Appendix C.\\


%~\ref{section: appendix applications of HE}