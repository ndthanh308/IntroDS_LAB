





%\vspace{-15}
\section{Introduction}



%{\bf Welcome to ML World.} 
The rapid expansion of big data, along with the rise in computational resources, have allowed for remarkable gains in the capabilities of ML algorithms, igniting a competitive landscape in this field. These algorithms, initially devised by humans, now actively participate in decision-making and policy formation for the same people who created them. The advantages of these algorithms are vast, as they enhance efficiency, accuracy, and speed in various domains. They contribute to improved legal outcomes~\cite{surden2014machine}, streamlined lending and hiring processes~\cite{chalfin2016productivity}, and optimized allocation of resources and benefits~\cite{hussain2020machine}. Harnessing the power of ML to develop equitable and efficient systems can catalyze both social and economic progress.


%The emergence of big data and accessible computational resources has revealed the immense potential of algorithms, sparking a race in the field of Machine Learning (ML). These very algorithms, which were once created by humans, now play a critical role in making decisions and shaping policies for the same individuals who developed them. The influence of these algorithms extends to areas such as legal judgments, loan approvals, recruitment processes, and the allocation of resources and benefits, all of which carry significant social and economic consequences.





%- why trustworthy ML, two key components area privacy and fairness
%--ChatGPT and the letter signed by CEO about halt till 


{\bf Trustworthy ML.} The initial belief that more data would result in better decision-making in the world of ML was quickly shattered as it became clear that accurate algorithms alone are not enough to make responsible decisions~\cite{dwork2012fairness,chen2022fairness,mehrabi2021survey}. The significance of trustworthiness in ML can be explored by making an analogy with the stages of human development. Consider a child who inherits characteristics from their parents – this is akin to the initial model selection in ML, taking into account the mathematical limitations inherent in the chosen structure. As the child matures, they absorb crucial knowledge during their formative years - comparable to an ML model being trained with carefully selected datasets. The child's interaction with their socio-economic environment shapes their behavior and choices, just as an ML model's responses are influenced by the dataset it interacts with and the feedback it receives. The child experiences both opportunities and limitations in society, just as an ML model's functionality is affected by the boundaries of its mathematical design and the quality of its datasets. Ultimately, the child matures into an individual whose ethical decisions impact those around them, much like an ML model that must make responsible decisions affecting real-world outcomes. To develop a trustworthy ML pipeline, each element in the learning cycle, like each stage in a child's life, carries shared responsibility. In this research, we focus on understanding and integrating the two main pillars of trustworthy ML: Privacy and Fairness.



{\bf Privacy.} Information privacy refers to an individual's right to maintain a certain level of control over how their personal data is gathered and utilized~\cite{solove2008understanding}. Take, for instance, a photo shared by an individual on social media platforms, intended solely for communication and social interaction. Even basic data mining techniques can extract sensitive information from this image, which could be exploited by malicious attackers. Aspects like ornaments, background, and facial features may inadvertently disclose the individual's religion, geographical location, gender, race, or other sensitive details. This raises the question of how much control users have over their own data. Expanding this concept to the vast quantities of data utilized in modern ML models highlights the importance of privacy for ensuring trustworthy ML. Incidents like the 2020 Facebook scandal~\cite{FinancialTimes2020} and the Edward Snowden revelations~\cite{ForbesTechCouncil2023} underscore the critical nature of user data privacy in the context of ML.




{\bf Fairness.} From a different perspective, while privacy deals with the extent of control over data, fairness aims to ensure that the revealed user information is handled fairly and equitably. The philosophical notions of fairness have existed for centuries~\cite{anderson2004pursuit}; however, with the rapid growth of ML, algorithmic fairness and its application to ML have emerged as some of the most critical challenges of the decade. Unfortunately, models intended to intelligently avoid errors and biases in decision-making have themselves become sources of bias and discrimination within society. Various forms of unfairness in ML have raised concerns, including racial biases in criminal justice systems~\cite{angwin_larson_kirchner_mattu_2016} and disparities in employment~\cite{raghavan2020mitigating} and loan approval processes~\cite{kozodoi2022fairness}. The entire life-cycle of an ML model – encompassing input data, modeling, evaluation, and feedback – is vulnerable to both external and inherent biases, leading to unjust outcomes.  Compounding the issue is the tendency of the pipeline's life-cycle to amplify biases due to oversimplification and assumptions made throughout the process. Moreover, unlike the concept of privacy, for which there are well-defined and accepted metrics, the large number of varied and often conflicting definitions of fairness presents a significant challenge in establishing trustworthy ML systems.





{\bf Privacy vs Fairness.} Investigations into privacy and fairness have often been carried out separately, without a holistic comprehension of how these two goals intertwine. Although the trade-offs between privacy and utility, as well as fairness and utility, are well-established, the complex relationship between these objectives remains less clear. Several studies such as \cite{bagdasaryan2019differential} and \cite{chen2022fairness}, indicate the presence of trade-offs, while others, like \cite{khalili2021improving} and \cite{pannekoek2021investigating}, consider them to be in harmony. Given the lack of studies elucidating their interconnection, there is an urgent need for more research to uncover the link between these two goals, ultimately paving the way for truly responsible ML models.

{\bf Motivation.} Pursuing privacy and fairness as separate objectives may appear intuitive, yet this approach is fraught with significant issues. First, engineers and researchers often find that the attainment of even one of these goals can significantly impact a model's performance, necessitating careful alignment of both objectives. Second, research exploring how the achievement of one goal influences the other remains limited. This knowledge gap introduces uncertainty concerning the model's reliability. As a result, our goal is to bridge this divide between privacy and fairness, traditionally pursued as independent objectives. We aim to establish a foundation for more advanced techniques facilitating their concurrent implementation, honoring these elements as the two primary pillars of trustworthy ML models.

{\bf Contribution.} In this comprehensive survey, we present an in-depth examination of the main concepts in privacy and fairness by analyzing nearly $200$ recent studies in the field. We explore these approaches across four primary aspects of ML, namely, Supervised Learning (SL), Unsupervised Learning (UL), Semi-Supervised Learning (SSL), and Reinforcement Learning (RL), with the aim of consolidating terminology and ideas. For instance, we collate and explain $15$ distinct fairness notions to facilitate a better understanding of the principles. By establishing a solid comprehension of privacy and fairness across various ML techniques, we offer an extensive review of existing research on architectures designed to meet these goals, the interplay between the objectives, their concurrent implementation, and ultimately, their manifestation in several applications. Moreover, we identify several key unresolved questions and challenges in understanding two objective functions, from large language models to the disparate impact of privacy-preserving methods in ML. 



The rest of this survey is organized as follows. A detailed examination of privacy within the realm of ML is presented in Section~\ref{Sec: privacy}. Concepts of fairness and algorithms to ensure it are then discussed in Section~\ref{Sec: fairness}. The intersection of privacy and fairness is the central focus of Section~\ref{Sec: privacy vs fairness}. Open issues and potential directions are explored in Section~\ref{Sec: Vision and Challenges}. Conclusions are drawn in Section~\ref{sec: conclusion}. A comprehensive map, illustrating the sections and subsections, can be found in Appendix A. To the best of our knowledge, this is the first survey that attempts to provide a critical review of privacy and fairness in ML. Some of the most relevant and recent surveys are reviewed in Table~\ref{Table:Related_Works}.%~\ref{section: mind map}









\renewcommand{\arraystretch}{1}

%{\bf Existing Surveys.}

\begin{table}
    \centering
    \caption{Comparison of recent related works and our paper’s key contributions on privacy and fairness in ML.}
    \label{Table:Related_Works}
    \renewcommand{\arraystretch}{1.5}
    % \begin{tabular}{|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{2.45cm}|>{\centering\arraybackslash}p{0.61cm}|>{\centering\arraybackslash}p{0.61cm}|>{\centering\arraybackslash}p{0.61cm}|>{\centering\arraybackslash}p{0.61cm}|>{\centering\arraybackslash}p{1.4cm}|>{\centering\arraybackslash}p{1.4cm}|>{\centering\arraybackslash}p{3.8cm}|}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{\textbf{Paper}} 
    & \multirow{2}{*}{\textbf{Key topic}} 
    & \multicolumn{4}{c|}{\textbf{ML categories}} 
    & \multicolumn{2}{c|}{\textbf{Key contributions}} 
    & \multirow{2}{*}{\makecell[c]{\textbf{Highlights}}} \\
    \cline{3-8} & & \makecell{\textbf{SL}} & \makecell{\textbf{UL}} & \makecell{\textbf{SSL}} & \makecell{\textbf{RL}} & \textbf{Privacy} & \textbf{Fairness} & \\
    \hline
    \cite{soykan2022survey} & \makecell[c]{Privacy-preserv-\\ing collaboration\\ in ML} & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & \makecell[l]{Pioneering study concentrat-\\ing on collaborative ML pri-\\vacy needs and limitations} \\ [0.45cm]
    \hline
    \cite{blanco2022critical} & \makecell[c]{ Limitations of DP \\in ML applications} & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & \makecell[l]{DP assessment: flaws, trade-\\offs, ML implementation} \\ [0.3cm]
    \hline
    \cite{le2022survey} & \makecell[c]{Fairness-aware ML \\in different datasets} & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & \makecell[l]{Fairness in ML through in-de-\\pth real data analysis} \\ [0.3cm]
    \hline
    \cite{pessach2022review} & \makecell[c]{Algorithmic fairness\\ in ML} & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & \makecell[l]{Overview of identifying, mea-\\suring, and improving algorit-\\hmic fairness} \\ [0.45cm]
    \hline
    \cite{choudhary2022survey} & \makecell[c]{Fairness in \\graph mining} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & \makecell[l]{Fairness in graph algorithms: \\measures, benchmarks, and \\research directions} \\ [0.45cm]
    \hline
    \cite{liu2021machine} & \makecell[c]{Privacy-preserving \\ML} & $\checkmark$ & $\checkmark$& $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & \makecell[l]{Identifying gaps and challen-\\ges in privacy preservation \\for ML} \\ [0.45cm]
    \hline
    \cite{de2021critical} & \makecell[c]{Privacy defense\\ trade-offs\\ in ML evaluation} & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & \makecell[l]{Balancing privacy and utility \\in ML defense evaluation} \\
    \hline
    \cite{xu2021privacy} & \makecell[c]{Privacy-preserving \\ML} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & \makecell[l]{Integration of privacy techniq-\\ues in ML for data-driven app-\\lications} \\ [0.45cm]
    \hline
    \cite{wan2021modeling} & \makecell[c]{In-processing \\fairness mitigation} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & \makecell[l]{Categorization of explicit and \\implicit methods in achieving\\ fairness} \\ [0.45cm]
    \hline
    \cite{mehrabi2021survey} & \makecell[c]{Fairness and bias \\in AI systems} & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & \makecell[l]{Taxonomy of fairness defini-\\tions for mitigating biases in \\AI} \\ [0.45cm]
    \hline
    \cite{chhabra2021overview} & Fair clustering & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & \makecell[l]{Organized overview with new \\insights and classifications in\\ fair clustering} \\ [0.45cm]
    \hline
    \cite{tanuwidjaja2020privacy} & \makecell[c]{Privacy-Preserving \\DL in MLaaS} & $\times$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & \makecell[l]{Adversarial models, attacks, \\and solutions in privacy-pre-\\serving DL} \\ [0.45cm]
    \hline
    Our paper & \makecell[c]{Privacy-Fairness \\Interrelation in ML} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \makecell[l]{Thorough review of privacy,\\ fairness in ML, examining im-\\pact, architectures, and re-\\search gaps} \\ [0.65cm]
    \hline
\end{tabular}
\end{table}

%\clearpage
%\onecolumn
%\def\checkmark{\tikz\fill[scale=0.6](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
%\def\checkmark{\tikz\fill[scale=0.6];} 
% \begin{longtable}[!ht]{|l|p{7cm}|l|l|l|l|} 
%     %\centering
%    % \begin{tabular}{|l|l|l|l|l|l|l|}
%     \hline Citation & Title & Year & Privacy & Fairness & Highlights\\ \hline
%         \cite{soykan2022survey} & A Survey and Guideline on Privacy Enhancing Technologies for Collaborative Machine Learning & 2022 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{blanco2022critical} & A Critical Review on the Use (and Misuse) of Differential Privacy in Machine Learning & 2022 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{liu2021machine} & When machine learning meets privacy: A survey and outlook & 2021 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{de2021critical} & A critical overview of privacy in machine learning & 2021 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{xu2021privacy} & Privacy-preserving machine learning: Methods, challenges and directions & 2021 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{tanuwidjaja2020privacy} & Privacy-preserving deep learning on machine learning as a service—a comprehensive survey & 2020 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{tanuwidjaja2019survey} & A survey on deep learning techniques for privacy-preserving & 2019 &  \hfil \checkmark  & ~& ~\\ \hline
%         \cite{le2022survey} & A survey on datasets for fairness-aware machine learning & 2022 & ~ &  \hfil \checkmark & ~ \\ \hline
%         \cite{pessach2022review} & A Review on Fairness in Machine Learning & 2022 & ~ &  \hfil \checkmark & ~\\ \hline
%         \cite{choudhary2022survey} & A Survey on Fairness for Machine Learning on Graphs & 2022 & ~ &  \hfil \checkmark & ~ \\ \hline
%         \cite{mehrabi2021survey} & A survey on bias and fairness in machine learning & 2021 & ~ &  \hfil \checkmark & ~  \\ \hline
%         \cite{chhabra2021overview} & An overview of fairness in clustering & 2021 & ~ &  \hfil \checkmark & ~ \\ \hline
%         \cite{wan2021modeling} & Modeling Techniques for Machine Learning Fairness: A Survey & 2021 & ~ &  \hfil \checkmark & ~\\ \hline
% \end{longtable}\label{table: surveys}

% \usepackage{tabularray}

%\clearpage
%\twocolumn






%\bibliographystyle{abbrv}
%\bibliography{Ref_main}