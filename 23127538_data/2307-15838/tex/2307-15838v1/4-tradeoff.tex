


\section{Privacy \& Fairness}\label{Sec: privacy vs fairness}


Privacy-focused methods such as DP aim to make individuals unidentifiable to outside observers, while fairness mechanisms aim to ensure balanced outputs across different groups. There are two different perspectives on the relationship between these two goals. One perspective sees them as compatible, while the other highlights a potential trade-off between them. In this section, we aim to explore this relationship in order to facilitate further research on more advanced algorithms that can achieve both goals simultaneously. The section is structured as follows:


\vspace{-6pt}
\begin{itemize}
    \item {\bf Architectures.} This subsection provides some common architectures that can be used to implement both privacy and fairness objectives together.
    \item {\bf Impact of Privacy on Fairness.} This section examines the empirical evidence on how achieving privacy can affect fairness.
    \item {\bf Impact of Fairness on Privacy.} This section focuses on the consequences and benefits of achieving fairness on privacy and presents existing evidence.
    \item {\bf Concurrent Implementation of Privacy and Fairness.} Here, we discuss algorithms that aim to achieve both privacy and fairness at the same time.
    \item {\bf Applications.} This subsection provides examples of the interrelation between privacy and fairness in various application domains.
\end{itemize}


\vspace{-6pt}







%\bibliographystyle{abbrv}
%\bibliographystyle{ACM-Reference-Format}
%\bibliography{Ref_main}

\subsection{Architectures}

In this section, we introduce five prominent architectures specifically developed to tackle the dual challenges of privacy and fairness in ML. The visual representations of these architectures are presented in Figure~\ref{Fig: Architecture}.


{\bf Architecture A.} In the first approach, a single entity seeks to ensure both privacy and fairness for their models. Initially, privacy-preserving algorithms are applied to the data, resulting in a privacy-protected dataset. This sanitized data is then used for fair training, with fairness pre-processing, in-processing, and post-processing techniques being applicable. For instance, the model-agnostic privacy-preserving k-means algorithm~\cite{su2016differentially} follows this approach by applying DP to data points before implementing the k-means algorithm. Techniques like fairlet decomposition~\cite{chierichetti2017fair} can be employed after sanitization to enhance fair learning.

{\bf Architecture B.} The second approach aims to achieve privacy and fairness simultaneously during model training. In this case, the ML model receives the data directly and attempts to perform fair and private learning, often using an augmented objective function, adversarial learning, or incentivizing methods in RL. An example of such an approach is seen in~\cite{chen2022fairness} and~\cite{liu2020fair}, where the authors focus on attaining objectives during training.

{\bf Architecture C.} The third approach involves separating sensitive and non-sensitive user attributes in the dataset. Sensitive user attributes undergo privacy-preserving methods for sanitization, after which both protected and unprotected attributes are passed through the ML pipeline. Fairness techniques can be applied at various stages, such as pre-, in-, or post-processing. This approach assumes the presence of a trusted entity with access to sensitive data, sharing them only in a private manner. An example of this method can be found in~\cite{lowy2022stochastic}.






{\bf Architecture D.} In the fourth scenario, an FL framework is highlighted, which leverages distributed learning. FL allows for the decentralized training of large-scale models without requiring direct access to clients' data, effectively maintaining their privacy. In a standard FL setting~\cite{mcmahan2017communication}, client nodes work together with a server to find a parameter vector that minimizes the weighted average of the loss across all clients. Techniques like Secure Aggregation~\cite{bonawitz2017practical} are used to guarantee that the server does not gain any information about the values of the individual updates sent by the clients, other than the aggregated value it aims to compute. Fairness is typically addressed through local debiasing methods and global unfairness mitigation strategies. A notable example of this structure is presented in~\cite{ezzeldin2021fairfed}.



{\bf Architecture E.} In the final common scenario, privacy-preserving fairness auditing is considered. We describe the framework in relation to Secure Multiparty Computation (MPC), a cryptographic technique that enables multiple parties to collaboratively compute a specific output from their confidential information in a distributed manner without actually exposing this private data. In this setup, a company (Alice) with a proprietary model must undergo an audit by an authority (Bob) with access to sensitive audit information. Alice wishes to keep her trained model parameters private, while Bob seeks to protect the sensitive audit data because it contains critical attributes necessary for fairness auditing but may also be subject to anti-discrimination and data protection regulations. A representative example of this privacy-fairness architecture is discussed in~\cite{pentyala2022privfair}.









% Figure environment removed







\subsection{Impact of Privacy on Fairness}



The aim of this subsection is to comprehend the ways in which privacy-preserving methods affect the fair treatment of individuals and groups. The subsequent discussion reviews two perspectives on perspectives on the potential positive and negative impacts of privacy on fairness: one focused on ways in which they are aligned, and another focusing on ways in they contrast with each other.

\subsubsection{Aligned} 



To begin our discussion, we examine publications that argue for the compatibility of privacy and fairness objectives. Pannekoek et al.~\cite{pannekoek2021investigating} build their experiments based on a simple neural network consisting of three fully connected layers. For fairness, they used the "reject option classification" approach as a postprocessing technique to adjust output labels. Meanwhile, for privacy protection, they implemented the DP version of the Adam optimizer~\cite{mcmahan2018general}. By incorporating both fairness and privacy constraints, their model displayed superior fairness compared to the model that only utilized fairness constraints, while maintaining high accuracy. Furthermore, the model's performance did not exhibit a declining trend as privacy protection measures increased. The authors in~\cite{khalili2021improving} demonstrate that the exponential mechanism developed to achieve DP can also help in attaining fairness once used as a post-processing approach at the output of classifiers in selection problems. 


Sarhan et al.~\cite{sarhan2020fairness} examine the effect of DP on fairness in FL. The predominant scenarios in FL is explored including local DP and global DP. In the local DP scenario, clients use DP-SGD on their local nodes and transmit the model hyperparameters to the server for evaluation of fairness metrics such as equalized odds, equalized opportunity, and demographic parity. In contrast, in the global DP scenario, client hyperparameters are aggregated at the server and sanitized to achieve DP. The authors discover that DP reduces discrimination in both scenarios, but strict privacy budgets can result in a trade-off between privacy and fairness. As a result, the need for parameter tuning is emphasized.



The utilization of learned language models' hidden representations has been proven to be capable of extracting confidential information about users. In~\cite{lyu2020differentially}, the authors suggest a technique for safeguarding user privacy and demonstrate that incorporating privacy into the process can generally decrease bias. The proposed approach involves adding a noise layer to the extracted features from text, which achieves DP before sharing the representation for classification. Maheshwari et al. \cite{maheshwari2022fair} explore the integration of DP and adversarial learning, focusing on the Equalized Fairness metric in NLP. Their proposed framework perturbs the output of a text encoder to achieve DP, and employs a classifier branch in conjunction with an adversarial branch to actively foster fairness. The authors provide empirical evidence demonstrating that privacy and fairness are not only compatible in this context, but also mutually supportive.






\subsubsection{Contrasting}


On the contrary, some works argue that there exists a trade-off between privacy and fairness. Sanyal et al.~\cite{sanyal2022unfair} investigated the use of DP for classifiers, with a focus on accuracy discrepancy as a metric for fairness. Through their theoretical analysis, they established that it is not feasible to maintain high accuracy for minority groups and safeguard privacy simultaneously when data follows a long-tailed structure. Nevertheless, they demonstrated that relaxing accuracy requirements can lead to achieving a high degree of strict privacy and fairness. The authors in~\cite{bagdasaryan2019differential} focus on neural networks trained using Differentially Private Stochastic Gradient Descent (DP-SGD) and demonstrate that the accuracy of the private model drops more for the underrepresented classes. In other words, the DP-SGD amplifies the model’s bias towards popular elements in the distribution being learned. The results are empirically shown in gender classification, sentiment analysis of tweets,  species classification, and FL of language models.



Du et al.~\cite{du2019robust} study the role of DP on outlier detection. To detect outliers, an ML model is trained to learn the distribution from which the training data samples were drawn. Using this information, the model can detect samples that significantly differ from the learned distribution. The authors demonstrate that the random noise added during the model training for DP purposes hides the impact of an individual record on the learned model. Essentially, rare training examples are hidden by the added noise, resulting in a model that is less capable of detecting outliers. In other words, the accuracy of the model decreases for minority instances when DP is applied to the training process. In \cite{chen2022fairness}, the author validates the presence of a trade-off between privacy and fairness in semi-private settings, where a small fraction of sensitive data is clean, while the remaining data is safeguarded by privacy measures.



%\newpage
\subsection{Impact of Fairness on Privacy}

%As opposed to understanding the impact of privacy on fairness, there exist very limited discussions on how Algorithmic fairness can impact the privacy of indivdiuals and groups. Most unfairness mitigation algorithms proposed in the literature are on the basis of the existence of sensitive user data. Therefore, the pursuit of fair ML can lead to intentional and untentional overuse of user data and compromise of user privacy. The necessity of further discussion on understanding the privacy risks of algorithms in the machine learning community has been emphasized in several works such as~\cite{andrus2022demographic} and ~\cite{chang2021privacy}. In the following, we review the very limited number of work existing in the literature on this topic. 



%Despite significant implications that fairness can have on privacy, only little attention is given to how algorithmic fairness may affect the privacy of individuals and groups. Most methods proposed to address unfairness rely on sensitive user data, which can lead to unintentional or deliberate overuse of such data, compromising user privacy. There is a need for more discussion within the machine learning community to understand the privacy risks associated with algorithmic fairness. Several studies, including~\cite{andrus2022demographic} and ~\cite{strobel2022data}, have emphasized this necessity. In the following, we will examine the limited number of studies that have been conducted on this topic.


Despite the considerable impact that fairness may have on privacy, there's been relatively scant attention devoted to understanding how algorithmic fairness might affect the privacy of individuals and groups. Many strategies developed to tackle unfairness depend on sensitive user information, which can result in unintended or intentional overexploitation of such data, thereby violating user privacy. More discussion and dialog is needed within the ML community to comprehend the privacy hazards linked to algorithmic fairness. This need has been emphasized in numerous studies, such as~\cite{andrus2022demographic} and ~\cite{strobel2022data}. In the following, we delve into the few studies that have been undertaken on this subject matter.



\subsubsection{Aligned} 

In this subsection, we discuss methods where the pursuit of fairness in ML models has led to a positive influence on user privacy. Starting with one of the most significant results in this regard, the authors in~\cite{dwork2012fairness} demonstrate that the concept of individual fairness is indeed a generalization of the DP notion under specific distance metric definitions. This finding is crucial because it enables the use of algorithms developed for individual fairness to positively impact privacy. More specifically, inspired by \cite{fioretto2022differential}, consider the individual fairness notion \cite{dwork2012fairness} described in Definition \ref{def: Individual location fairness}. In \cite{dwork2012fairness}, the authors reveal that the mapping function $\mathcal{M}: X\rightarrow Y$ satisfies $\epsilon$-DP given that the similarity distance metrics for the input and output are defined as follows for any two users $u$ and $v$:

\begin{align}
&d_{\mathcal{X}}(u,v) = \epsilon |\boldsymbol{u} \Delta \boldsymbol{v}|,\
& d_{\mathcal{Y}}(\mathcal{M}(u),\mathcal{M}(v))= \underset{y\in \mathcal{Y}}{\text{sup}} \left( \dfrac{P(\mathcal{M}(\boldsymbol{u} = y))}{P(\mathcal{M}(\boldsymbol{v} = y))} \right).
\end{align}

Here, $\boldsymbol{u} \Delta \boldsymbol{v}$ denotes the set difference between two inputs $\boldsymbol{u}$ and $\boldsymbol{v}$ of $\mathcal{X}$.


In a different perspective, Aalmoes et al.~\cite{aalmoes2022leveraging} concentrate on attribute inference attacks as a measure of privacy risk, where the objective is to deduce sensitive characteristics such as race and gender from the training data of an ML model. In this scenario, the model is perceived as a black box, with the adversary having access to the model's output predictions for any input query. The authors explore how fairness constraints applied during model training influence the attribute inference attack. They find that fairness algorithms, which enforce equalized odds, serve as an effective safeguard against attribute inference attacks without affecting the model's utility. Consequently, the goals of algorithmic fairness and sensitive attribute privacy are found to be in harmony.





\subsubsection{Contrasting}

Chang et al.~\cite{chang2021privacy} define the risk of privacy as the success of membership inference attack on a trained model. In such attacks an adversary observes the model predictions attempting to distinguish between members and non-members of the training set. The proposed attack is used to compare the information leakage of models trained with and without fairness constraints on different groups in their training data. The authors provide empirical evidence showing that fairness-aware learning has a disproportionate impact on the privacy risks of subgroups, creating a trade-off. This phenomenon is explained by the fact that fair models have a greater tendency to memorize data from unprivileged subgroups, which makes them more vulnerable to membership inference attacks.



In another work, Zhang et al.~\cite{zhang2023interaction} examine the interplay between privacy and fairness within the node classification of GNNs. They provide empirical evidence of the negative influence of individual node fairness on edge privacy. In this study, the individual fairness notion is applied as treating nodes comparably in classification, ensuring they receive identical service quality irrespective of their backgrounds. Privacy, on the other hand, is assessed through link prediction attacks between nodes, which reveal the connections between two nodes in a specific pair. The authors provide empirical evidence highlighting the adverse effect of individual fairness on privacy in this setting.











%\newpage
\subsection{Concurrent Implementation of Privacy and Fairness}


%\subsubsection{Approaches that help}


This section will examine algorithms that have been proposed in existing literature with the aim of achieving both privacy and fairness objectives while minimizing the overall loss of utility. Unfortunately, there is a limited number of research works that have theoretically investigated the interaction between these two objectives, such as those presented in references~\cite{cummings2019compatibility} and~\cite{khalili2021improving}. Cummings et al.~\cite{cummings2019compatibility} prove that it is not possible to simultaneously achieve DP and perfect fairness in terms of equalized odds while maintaining higher accuracy than a constant classifier. On the contrary, the authors in~\cite{khalili2021improving} demonstrate that this assertion is not applicable in selection problems. In non-selection problems, the goal is to minimize the expected loss across the entire population while ensuring fairness constraints. For instance, in a hiring scenario, all applicants who meet the classifier's criteria should be accepted. However, in selection problems, only a limited number of candidates can be chosen. The authors in~\cite{khalili2021improving} suggest that the exponential mechanism developed for DP can be an effective tool for improving fairness under specific circumstances.





Liu et al.~\cite{liu2020fair} develop an algorithm called FairDP, which aims to address disparate impact introduced by DP on underrepresented groups in the private training of classification models. The authors frame the learning process as a bilevel programming problem, which incorporates fairness and DP. FairDP utilizes an adaptive clipping threshold to regulate the impact of instances in each class, enabling the model accuracy to be adjusted for classes based on its privacy cost and fairness considerations. Chen et al.~\cite{chen2022fairness} investigated fair classification in semi-private settings where sensitive attributes, such as gender, are secured by specific privacy mechanisms, and only a few clean attributes are available. Their proposed framework aims to utilize the limited clean attributes to correct the noisy sensitive attributes while ensuring privacy.



Xu et al.~\cite{xu2019achieving} propose a method to combine privacy and fairness in logistic regression while maintaining high model accuracy. Their approach involves adding a decision boundary fairness constraint to the objective function and applying the functional mechanism for DP. The decision boundary fairness constraint is defined as the correlation between users' protected attributes and the distance from their unprotected attribute vectors to the decision boundary. The functional mechanism adds randomness to the polynomial coefficients of the constrained objective function by introducing Laplace noise. The experiments demonstrate a trade-off between the amount of privacy budget in DP and discrimination based on statistical parity. The authors in~\cite{hajian2015discrimination} argue for the benefits of addressing discrimination risk and privacy concerns in data mining, with a focus on the privacy metric $k$-anonymity. The authors consider a scenario where a set of patterns needs to be published in a way that preserves privacy and avoids discrimination, while minimizing pattern distortion. To achieve this goal, the proposed approach first identifies patterns that are more susceptible to vulnerability during the sanitization process, and then applies special measures to sanitize them based on the $k$-anonymity principle.


Jin et al.~\cite{jin2022privacy} examine an inference as service (IAS) scenario to make decisions in the cloud. In this setting, data is transmitted from devices to the cloud, and the cloud provider's server trains the ML model. To enhance user privacy and decrease bias, the author suggests a random mapping of data generated based on a non-convex optimization problem and an iterative algorithm to solve it. Inference accuracy, the mutual information between the transformed variable and the label, and the degree of leaked information are used to evaluate the utility, privacy, and impartiality of the mapping.






There are multiple works that focus on achieving privacy and fairness objectives simultaneously for ML models in the FL setting. Padala et al.~\cite{ padala2021federated} propose a framework for incorporating both privacy and fairness into FL. The authors use statistical parity as fairness metric and local DP as a means of ensuring privacy. The proposed framework is based on separating the training dataset and following a two-step process. In the first phase, each client trains a model on their own private dataset for unbiased prediction. In the second phase, clients train a DP model to mimic the unbiased prediction achieved in the first phase. Once training is complete, client data is transmitted to the server. Zhang et al.~\cite{zhang2020fairfl} propose a framework called FairFL to address the challenges of restricted information and constrained coordination in FL. The authors use statistical parity as a fairness metric and ensure privacy by not sharing raw training and ground truth labels, as well as sensitive demographic information. The FairFL framework consists of two components: a Team Markov Game for Client Selection (TMGCS) and a Secure Aggregation Protocol (SAP). The TMGCS is a Multi-Agent Reinforcement Learning approach that allows clients to collaboratively decide whether to participate in the local update process, while the SAP addresses the issue of each client's myopic view by allowing them to gather information about all clients' statuses without violating their privacy constraints.







\subsection{Applications}
%\newpage


In this section, we will examine domain-specific approaches that researchers have taken to incorporate privacy and fairness as key components of trustworthy ML in their respective applications.


{\bf Healthcare.} Patient electronic health records (EHRs) are regarded as some of the most sensitive information available, subject to stringent legal protection. However, such data is highly valuable to researchers and decision-makers, providing guidance for policy creation and medical advancements, as demonstrated during the recent unfortunate Covid-19 pandemic. This data often includes highly personal and sensitive patient information, such as gender, race, and age, which must be considered to ensure equitable treatment of individuals and groups. A prevalent solution for balancing privacy with the incorporation of fairness is utilizing synthetic datasets that mimic patient records to enhance privacy and train equitable models for future predictions~\cite{seastedt2022global}. Determining the degree of resemblance (privacy), fairness notions in healthcare, and their interplay remains an ongoing challenge and an active research area. For instance, in~\cite{bhanot2021problem}, multiple fairness notions have been suggested for covariate-level insights in synthetically generated healthcare data, underscoring the need for additional research to comprehend the interaction between privacy and fairness.




{\bf Natural Language Processing.} Modern natural language processing (NLP) models heavily rely on the encoded representation of text, which often captures sensitive attributes about individuals (e.g., race or gender). This raises privacy concerns and can cause downstream models to be unfair to certain groups. Sensitive information can be implicitly or explicitly present in the input text. Maheshwari et al.~\cite{maheshwari2022fair} propose a framework to address this problem. The framework aims to perturb the output of a text encoder to achieve DP and then uses a classifier branch along with an adversarial branch to actively promote fairness. The authors show that not only are these two objectives not in conflict with each other, but they also help each other in improving both objectives.  The proposed approach in~\cite{lyu2020differentially} adds a noise layer to the feature extraction process, achieving DP before using the representation for classification tasks, also showing the alignment of two objectives.



{\bf Computer Vision.} Comprehending privacy in Computer Vision involves various facets; however, present approaches for addressing privacy and fairness interplay mainly focus on defining privacy as instances where the model might memorize the training data in some way or use features as proxies for the original data to deduce private attributes about individuals, a concept known as attribute privacy. In this case, the model inadvertently or intentionally transfers information about the private attribute into the features, bearing resemblance to fairness-related concerns. The authors in~\cite{paul2023evaluating} examine the privacy-fairness trade-off, concentrating on the method where a feature extractor transforms images into features using weights. They employ an adversarial technique to integrate privacy and fairness by adding penalty terms for both objectives to the feature extractor's loss function during training. Findings reveal that privacy and fairness are conflicting in this context. In~\cite{tian2022fairness}, facial attribute classification is explored. The authors leverage GANs to maintain privacy through the generation of synthetic images and use contrastive learning-based loss designs to concurrently enforce fairness protections.





{\bf Spatial Data Processing.} User location data contains essential information about individuals, such as socio-economic aspects and indicators for sensitive attributes like race. With the US 2020 census data being published using DP across neighborhoods, comprehending the relationship between privacy and fairness has grown increasingly crucial. In~\cite{pujol2020fair}, the authors explore the impact of DP on supervised decision-making in three applications: (I) assigning voting rights benefits to minority languages, where privacy noise leads to significant disparities in accurately identifying deserving beneficiaries; (II) parliamentary apportionment, where certain privacy budget settings result in a more equitable distribution of seats to Indian states using noisy data compared to deterministic methods; and (III) federal funds allocation, where under strict privacy settings, some districts receive an uneven share of funds. The study underscores DP applied to location data can lead to disparities in the outcome, emphasizing the need for more in-depth research on privacy's influence on fairness. Furthermore, fairness in spatial data is a relatively new concept, with studies like~\cite{shaham2022models} and~\cite{shaham2023fair} addressing issues such as discrepancies in fairness metrics across neighborhoods, abrupt changes in classifier outputs due to neighborhood shifts, and the use of spatial continuity to enhance fairness. The impact of privacy on these aspects has not yet been considered and calls for further investigation.










%{\bf Finance.} Financial decision-making is a crucial and influential domain of fairness that holds significant implications for individuals in society, while also being highly sensitive and necessitating strict privacy measures to safeguard user data~\cite{o2017weapons}. Several applications that involve balancing these dual objectives include (I) Credit scoring, (II) Loan approval, (III) Insurance pricing, (IV) Target Marketing, (5) Financial advisory services, (6) Customer segmentation, and (7) Employment and promotion decisions. Legal considerations and their interplay with fairness in finance are examined and discussed in~\cite{das2021fairness}. The interaction between privacy and fairness in the aforementioned applications is explored in selection problems~\cite{khalili2021improving} and ranking~\cite{sun2023privacy}. 











%When it comes to looking at the two objective functions of Fairness and Privacy next to each other, an important indicator that stands out is that in many cases the concept of fairness is indeed equivalent to equity~\cite{pujol2021equity}. 




%The authors in [1] consider the impact of DP on supervised decision-making for 3 applications, (I) assignment of voting right benefits to minority language, in which the result indicate that noise for privacy can lead to significant disparities in the rates of correct identification of those deserving the benefits, (II) For the parliamentary apportionment problem, surprisingly, there are settings of ϵ where the apportionment of seats to
%Indian states based on the noisy data is more equitable, ex ante, than the standard deterministic apportionment, (III) federal funds allocation use case, under strict privacy settings of ϵ = 10−3, some districts receive over 500× their proportional share of funds while others receive less than half their proportional share. Under weaker privacy settings (ϵ = 10), this disparity is still
%observed but on a much smaller scale.

