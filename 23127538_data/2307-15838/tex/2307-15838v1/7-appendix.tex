



\appendix

\section{Mind Map of Survey}\label{section: mind map}

% Figure environment removed


%\newpage







\section{Applications of DP}\label{section: appendix applications of DP}

This section explores the practical use cases of DP in supervised, unsupervised, semi-supervised, and RL.

\subsection{Supervised Learning}
 Various studies have demonstrated the crucial role of DP in enhancing the privacy of ML tasks such as regression and decision tree classification. For instance, Sheffet et al. \cite{sheffet2019old} proposed DP algorithms for approximating the 2nd-moment matrix, while Milionis et al. \cite{milionis2022differentially} presented efficient DP algorithms for classical regression settings. Furthermore, Kim et al. \cite{kim2019secure} proposed a hybrid approach using DP and homomorphic encryption for privacy-preserving distributed logistic regression. In the context of decision tree classification, Liu et al. \cite{liu2018differentially} and Fletcher et al. \cite{fletcher2015differentially} \cite{fletcher2017differentially} introduced decision tree and forest algorithms that ensure privacy while achieving high accuracy. Their proposed algorithms showcase the importance of DP in the development of ML systems that maintain high levels of privacy without compromising performance. 
 \newline
Random Forest classification algorithms have also been shown to benefit significantly from DP techniques. Specifically, Patil et al. \cite{patil2014differential} and Hou et al. \cite{hou2019dprf} proposed algorithms that integrate DP with the Gini index and demonstrate the effectiveness of DP in preserving classification accuracy and privacy in Random Forest classification. Moreover, the significance of DP in SVM for safeguarding privacy in ML is exemplified by Senekane et al. \cite{senekane2019differentially} and Park et al. \cite{park2023efficient}. Senekane et al. proposed privacy-preserving image classification using SVM and DP with $\epsilon$-DP, while Park et al. proposed an algorithm for multi-class classification utilizing SVDD with DP-SVDD for training and EPs for classification, respectively. Their approaches showcase the efficacy of DP in enhancing the privacy of ML systems while maintaining high levels of accuracy.

\subsection{Unsupervised Learning}
Numerous investigations have explored the implementation of DP in unsupervised learning. For example, Tantipongpipat et al. \cite{tantipongpipat2019differentially} propound a conceptual framework that amalgamates autoencoder and GANs to create top-tier synthetic data in unsupervised settings. They also introduce novel metrics to appraise the excellence of synthetic data and showcase the efficacy of their approach on medical and census datasets. Similarly, Torfi et al. \cite{torfi2022differentially} confront the challenges of synthetic data generation in medical domains, such as safeguarding privacy, handling discrete data, and incorporating temporal and correlated features. They put forth a privacy-preserving framework that utilizes RDP-CGAN and surpasses current methodologies in terms of privacy guarantee and quality of synthetic data. Their approach is designed to provide both privacy and quality in synthetic data generation.
\newline
Other studies have focused on employing DP to particular unsupervised learning tasks. Specifically, Wang et al. \cite{wang2015differentially} scrutinize differentially private subspace clustering algorithms and present a pragmatic Gibbs sampling subspace clustering algorithm using the exponential mechanism. This algorithm is designed to preserve the privacy of individuals' data. In another study, Bun et al. \cite{bun2021differentially} introduce an algorithm for differentially private correlation clustering that accomplishes subquadratic additive error compared to the optimal cost. Their approach improves upon previous methods, which have typically incurred quadratic costs. Similarly, Blocki et al. \cite{blocki2021differentially} devise private sublinear-time clustering algorithms for k-median and k-means clustering in metric spaces, and initiate a sampling algorithm with group privacy analysis. Their approach focuses on reducing computation time while ensuring privacy. In yet another application, the authors in \cite{niinimaki2019representation} address privacy preservation in human genomic datasets and propose differentially private ML using representation learning, evincing enhanced accuracy in drug sensitivity prediction.

\subsection{Semi-supervised Learning}
Both \cite{pham2018differentially} and \cite{long2017differentially} offer novel frameworks for differentially private semi-supervised classification, utilizing both labeled and unlabeled data to train a classifier. Whereas the former prioritizes known class priors and provides proofs on privacy and utility, the latter introduces two distinctively differentially private methods, output perturbation and objective perturbation, and assesses their performance against regular differentially private empirical risk minimization (ERM). In addition, \cite{long2017differentially} conducts a thorough analysis of the global sensitivity of the objective function in SSL, demonstrating that their proposed methods attain superior accuracy while ensuring DP.
\newline
In contrast, \cite{jagannathan2013semi} tackles the issue of enhancing the accuracy of a differentially private classifier using non-private data when only a small amount of private data is accessible. Their approach fabricates a differentially private classifier from private data and subsequently employs non-private data to boost accuracy, extending the random decision tree idea to leverage the availability of unlabeled data for denser partitioning of the instance space and label propagation. This method also boosts classifier accuracy without compromising privacy, as demonstrated on small and moderate-sized datasets. This diverges from the differentially private boosting algorithm that amplifies the accuracy of a class of real-valued queries. All in all, these studies underscore the significance of DP in ML and propose compelling solutions to address privacy concerns while enhancing classification accuracy.

\subsection{Reinforcement Learning}
The studies \cite{ma2020differentially} and \cite{zhou2022differentially} make contributions to the field of privacy-preserving RL. Whereas \cite{ma2020differentially} proffers solutions to attain DP in RL contexts through the utilization of exponential and Laplace mechanisms, \cite{zhou2022differentially} posits a novel approach for devising privacy-preserving RL algorithms with rigorous statistical guarantees. The latter expounds both value-based and policy-based optimistic private RL algorithms under linear mixture MDPs, which revel in sublinear regret in the total number of steps while ensuring joint DP. These results open up new avenues for safeguarding data privacy while ensuring the scalability and efficiency of RL algorithms in large-scale MDPs.
\newline
Similarly, the studies \cite{cheng2022multi} and \cite{li2019differentially} advance approaches for integrating DP in meta-learning and multi-agent RL, correspondingly. \cite{cheng2022multi} ushers in a new technique for translocating knowledge with DP, christened Differential knowledge Transfer with relevance Weight, which boosts the model's resilience to negative transfer and augments the knowledge set. Meanwhile, \cite{li2019differentially} proffers a new framework for incorporating DP into meta-learning, with encouraging results in both theory and practice. Their proposed privacy setting accords better performance than previously studied notions of privacy. These studies make significant contributions to broadening the horizons of privacy-preserving ML algorithms in diverse learning scenarios.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Applications of HE}\label{section: appendix applications of HE}
%\paragraph{Applications of HE}
This section explores the practical use cases of HE in supervised, unsupervised, semi-supervised, and reinforcement learning.

\subsection{Supervised Learning}
HE has been applied to several supervised learning models to ensure privacy and protect sensitive data. One study proposed a secure method for linear regression on split data using Paillier's PHE scheme, which allows for the homomorphic addition of encrypted values \cite{hall2011secure}. Another study introduced novel techniques using FHE to train logistic regression models on encrypted data, with potential applications to other models such as neural networks \cite{chen2018logistic}. In addition, a homomorphically encrypted logistic regression outsourcing model was presented, allowing ML models to be learned without accessing raw data \cite{kim2018secure}. However, there are still limitations to the application of these models, including overheads in computation and storage due to HE, and the need for polynomial approximation to reduce computation cost.
\newline
HE has also been applied to decision tree and random forest models. One study proposed SortingHat, an efficient non-interactive design for private decision tree evaluation using FHE techniques \cite{cong2022sortinghat}. SortingHat addresses cryptographic problems related to FHE and presents a version without transciphering that significantly improves computation cost. Another study proposed a privacy-preserving protocol for multiple model owners to delegate the evaluation of random forests to an untrusted party, incorporating a SHE scheme and optimization techniques \cite{aloufi2019blindfolded}. Both studies present new secure protocols and optimization techniques to enable practical use of heavy-weight HE schemes. Finally, a study presented a secure multi-label tumor classification method using HE, based on a neural network model with the softmax activation function \cite{hong2022secure}. The model achieved high accuracy and successfully computed the tumor classification inference steps on encrypted test data.

\subsection{Unsupervised Learning}
HE has gained attention as a promising tool for privacy-preserving ML, especially in unsupervised learning tasks such as clustering and principal component analysis. Several studies have proposed different techniques to utilize HE for these tasks. For instance, Catak et al. \cite{catak2020practical} proposed a privacy-preserving clustering system that utilized Paillier Cryptography and HE to preserve privacy and minimize computational time. Alabdulatif et al. \cite{alabdulatif2017privacy} introduced a distributed data clustering approach using fully homomorphic encryption, resulting in significant improvements in computational performance efficiency for clustering tasks. Wu et al. \cite{wu2020secure} proposed a secure and efficient outsourced k-means clustering scheme using YASHE homomorphic encryption, achieving privacy preservation in database security, clustering results, and data access pattern hiding.
\newline
In addition, HE has also been applied to principal component analysis. Panda et al. \cite{panda2021principal} proposed a non-interactive technique to perform principal component analysis using the CKKS HE scheme, achieving good performance on higher-dimensional datasets with a sub-ciphertext packing technique that reduces computations. Moreover, HE has also been used for privacy-preserving recurrent neural networks. Bakshi et al. \cite{bakshi2020cryptornn} proposed privacy-preserving recurrent neural networks using HE, evaluating five different methods to deal with increasing noise and linearity of activation functions on various datasets and network architectures, and proposing three original methods to retain non-linear function benefits.

\subsection{Semi-supervised Learning}
HE has shown great potential in supporting SSL tasks by enabling secure computation on encrypted data. The study by Arai et al. \cite{arai2011privacy} proposes a novel privacy-preserving label prediction solution with HE, while the study by Erkin et al. \cite{erkin2012generating} uses secure multiplication and decryption protocols, as well as data packing, to provide a privacy-preserving recommender system. On the other hand, the study by Pejic et al. \cite{pejic2022effect} compares the performance loss of different HE techniques and Multi-Party Computations (MPC) in Federated Learning (FL) to train a Generative Adversarial Network (GAN) on sensitive data.
\newline
Despite their differences in applications and techniques, all three studies highlight the potential of HE in protecting sensitive data while still allowing for useful computations to be performed. Arai et al. \cite{arai2011privacy} and Erkin et al. \cite{erkin2012generating} demonstrate the feasibility and efficiency of privacy-preserving solutions with HE in their respective fields, while Pejic et al. \cite{pejic2022effect} show the trade-off between the complexity of encryption methods and the time taken for computations. These studies suggest that HE can enable SSL tasks with privacy concerns in various contexts.

\subsection{Reinforcement Learning}
HE has become an increasingly popular tool for addressing privacy and security concerns in RL in cloud computing and IoT environments. Park et al. \cite{park2020privacy} proposed the Secure Q-Learning algorithm using FHE, which processes data in a single cloud server and restricts error growth without the bootstrapping algorithm. In addition, Suh et al. \cite{suh2021sarsa} developed the encrypted SARSA(0) algorithm, which offers privacy guarantees and induces minimal precision loss in control synthesis over FHE. Both studies demonstrate the potential of HE in addressing security and privacy challenges in RL tasks. 
\newline
Moreover, Miao et al. \cite{miao2021federated} proposed a FL based Secure data Sharing mechanism (FL2S) for IoT with privacy preservation, which uses an asynchronous multiple FL scheme with sub-task grading and deep RL, as well as HE for privacy protection. Meanwhile, Sun et al. \cite{sun2021privacy} proposed secure computation protocols using FHE for the A3C RL algorithm in health data, and design the first secure A3C RL algorithm for treatment decision-making. These studies demonstrate the efficiency of HE-based protocols and algorithms in secure RL and offer promising solutions to privacy and security concerns in various domains.








































\section{Bias}\label{section: appendix bias}

This section presents our classification of the different forms of bias illustrated in Figure~\ref{Fig: categorization of bias}. The types of bias have been divided into four major categories, namely, \emph{A Biased World}, \emph{Data Collection and Preparation}, \emph{Model Training}, and finally, \emph{Evaluation and Deployment}.



{\bf A Biased World.} Even if all fairness-related considerations are made for ML models, a significant source of bias is introduced in the data cycle due to perceptions and beliefs in society. \emph{Historical bias}~\cite{suresh2019framework} is the primary source in this category that highlights the adverse effects of reflecting historically unfair facts in ML models. For example, in 2018, image search results for female CEOs showed a bias towards male CEOs, reflecting the underrepresentation of women as CEOs in Fortune 500 companies (5 only)~\cite{suresh2019framework}. The question of whether search algorithms should reflect this reality remains uncertain~\cite{mehrabi2021survey}.

 
 {\bf Data Collection \& Preparation.} The role of data and how they are transformed is pivotal in ensuring fairness practices are in place. Well-known sources of bias in this category include {\em Representation bias} occurring when certain parts of input space are not reflected in the collected data, {\em Measurement bias} due to intentional or unintentional use of certain features as proxies for others, particularly for sensitive attribute, and {\em Sampling bias} in which non-random sampling of the dataset is used for the purpose of training~\cite{zadrozny2004learning}. 


{\bf Model Training.} The most subtle but equally detrimental sources of unfairness are introduced during training. {\em Latent bias} is one such factor for which the model will learn existing biases in society. For example, existing stereotypes in society are learned and reflected by the model. Another such bias is called {\em Linking bias} caused by learning fundamentally different patterns from real-world counterparts raised mostly in models applied on social networks~\cite{bakshy2012role}.

 
 {\bf Evaluation \& Deployment.} The most intuitive types of bias incur in the evaluation and somewhat less obvious ones in the deployment of ML models. For example, an unrepresentative test dataset is formulated under {\em Test Dataset bias} incurred during evaluation, and {\em Behavioral bias} formulates systematic distortions in user behavior across platforms or contexts, or across users represented in datasets~\cite{olteanu2019social}. Miller et al.~\cite{miller2016blissfully} demonstrated behavior bias by considering types of emojis existing on different platforms and how they can lead to different user reactions.







% Figure environment removed



\section{Sensitive Attributes}\label{section: appendix sensitive attributes}


Protected attributes are characteristics like gender and race that are determined by either legal obligations or specific values of an organization. Laws like the US Fair Housing Act, the Federal Equal Employment Opportunity, and the equal credit opportunity act specify requirements for algorithmic fairness in areas such as housing, employment, and credit. The following table briefly summarizes protected attributes based on the particular organization.

\begin{table}[]
    \caption{Sensitive attributes in different organizations.}
%\begin{tabular}{|c|c|c|p{4.7cm}|p{4.7cm}|}
%\begin{tabular}{|l|l|l|l|}
\renewcommand{\arraystretch}{1.35}
\begin{tabular}{|c|p{2.9cm}|p{3.5cm}|p{3.5cm}|}
\hline
%\textbf{Attribute}                         & \textbf{US Fair Housing Act} & \textbf{Federal Equal Employment   Opportunity Act} & \textbf{Equal Credit Opportunity Act} \\ \hline
\textbf{Attribute} & \textbf{US Fair Housing Act} & \makecell[c]{\textbf{Employment}\\\textbf{ Opportunity Act}} & \makecell[c]{\textbf{Equal Credit}\\\textbf{ Opportunity Act}} \\ \hline 

Race or Color  & \hfil \checkmark & \hfil \checkmark  & \hfil \checkmark \\\hline

Sex & \hfil \checkmark & \hfil \checkmark  & \hfil \checkmark \\ \hline

Religion  & \hfil \checkmark & \hfil \checkmark & \hfil \checkmark  \\ \hline

National   Origin & \hfil \checkmark  & \hfil \checkmark & \hfil \checkmark \\ \hline

Marital   Status &  \makecell[c]{$\times$} & \makecell[c]{$\times$} & \hfil \checkmark \\ \hline

Familial   Status & \hfil \checkmark & \makecell[c]{$\times$} &   \makecell[c]{$\times$} \\ \hline

Disability & \hfil \checkmark & \hfil \checkmark &  \makecell[c]{$\times$} \\ \hline

Age & \makecell[c]{$\times$} & \hfil \checkmark & \hfil \checkmark \\ \hline

Genetic   Info & \makecell[c]{$\times$} & \hfil \checkmark & \makecell[c]{$\times$} \\ \hline

Pregnancy & \makecell[c]{$\times$} & \hfil \checkmark & \hfil \checkmark \\ \hline

\end{tabular}
\end{table}





\section{Fairness in Semi-Supervised Learning} \label{section: appendix SSL}

SSL is a type of ML that falls in between supervised and unsupervised algorithms. It allows the use of both labeled and unlabeled data, which is crucial due to the tremendous need for data in large models and the high cost of generating labeled data. Moreover, previous research has shown that more data leads to a better balance between fairness and accuracy. Despite the essential role of SSL and a handful of surveys on this topic, including~\cite{prakash2014survey, zhu2005semi, pise2008survey, van2020survey}, there exists limited work on understanding fairness in SSL. To our knowledge, no survey covers the implications of fairness in SSL. The existing SSL methods can be categorized into the following $4$ groups:

\begin{itemize}
    \item {\bf Wrapper Methods.} The objective of this group, with the most widely adopted algorithm being pseudo-labeling, is to train a classifier using the labeled data available and then forecast labels for unlabeled data points.
    \item {\bf Unsupervised Extraction Methods.} This strategy aims to obtain beneficial features from data that has not been labeled by clustering and to utilize the information gained to improve training on labeled data.
    \item {\bf Intrinsic Methods.} In this class, unlabelled data are directly incorporated into the objective function of learning methods commonly used in supervised objectives for labeled data.
    \item {\bf Graph-based Approaches.} In this method, a graph is constructed, where each data point serves as a node, and the similarity matrix reflects the connections and associations among them. This graph is then utilized to make predictions about nodes or labels.
\end{itemize}


The amalgamation of fairness concepts from supervised and unsupervised learning is seen in SSL, due to its interdisciplinary nature. Consequently, our primary focus is on the mitigation algorithms.

\subsection{Unfairness Mitigation Algorithms} 
















\subsubsection{Pre-processing Strategies} The approaches commonly employed as pre-processing techniques in SSL have been classified into two categories: fair embeddings and reweighting.


{\bf Fair Embedding.} Graph embedding aims to transform graph data into a low-dimensional vector space, where nodes are represented as vectors. This process helps to capture the underlying structure and relationships in data such that it can be used for SSL tasks such as node classification and link prediction. Learning fair embeddings in the pre-processing phase allows for a significant reduction of bias in subsequent SSL models. 


Based on the node2vec algorithm~\cite{grover2016node2vec}, the authors in~\cite{rahman2019fairwalk} propose a method called Fairwalk for generating fair embeddings predicated on statistical parity. In Fairwalk, instead of randomly selecting a node to jump to from all of its neighbors, neighbors are divided into clusters based on their sensitive attribute values, with each cluster having an equal probability of being chosen, regardless of its size. Then, a node is randomly selected from the chosen cluster for the jump. Meanwhile, in another recent paper, Fan et al.~\cite{fan2021fair} introduced FairGAE, a method that employs an auto-encoder model to generate unbiased graph embeddings. FairGAE achieves the objective by blocking the message-passing procedure from certain neighbors of nodes based on their sensitive attributes, so that each node has an equal chance of being influenced by other groups. By doing so, FairGAE ensures that the graph convolutional network mechanism depends only on the network structure, rather than on sensitive attributes.


{\bf Reweighting.} The reweighting technique tends to boost the associated weight of certain groups or individuals in order to improve fairness in SSL outcomes. Khajehnejad et al.~\cite{khajehnejad2022crosswalk} proposed a method to reweight graph edges in order to promote fairness in random walks. Specifically, edges that connect different groups or are in close proximity to group boundaries are given greater weight. This approach leads to more transitions across group boundaries during the random walk process, which results in graph embeddings that better capture the structure of the entire network. The authors also demonstrate that this approach improves the performance of SSL algorithms in terms of statistical parity.\\











\subsubsection{In-processing Strategies} We have grouped in-processing techniques into two broad categories of {\em adversarial regularizers} and {\em fairness regularizers}.


{\bf Adversarial Regularizers}. In this group, a discriminator is trained to learn sensitive attributes during training. Such information is then commonly added as an extra regularizer in the loss function. The majority of adversarial regularizers in SSL are explored for node classification in GNNs. As proven in~\cite{li2018deeper, wang2020unifying}, embedding nodes with connected components will be closer even after one aggregation of message-passing. Therefore, it is understandable nodes with similar sensitive attributes, once used together, tend to reach similar embedding. The authors in~\cite{dai2021say} demonstrate this susceptibility of GNNs on different GNN architectures and propose a framework called FairGNN to address it. The framework utilizes an estimator to learn sensitive attributes of nodes and incorporates an adversarial learner in GNN that ensures that node predictions are independent of sensitive attributes. Additionally, fairness regularizers based on statistical parity and equalized odds are used in the objective to achieve group fairness. The authors in~\cite{bose2019compositional} focus on how node embedding can be learned such that they do not correlate with sensitive attributes. The mechanism to achieve fair embeddings is by introducing a set of adversarial filters applied to remove information about sensitive attributes. The filters are learned during the training and could also be used as a postprocessing approach to ensure embeddings are invariant with respect to protected features. 



{\bf Fairness Regularizers} In this group, fairness constraints are implemented directly into the loss function without requiring a discriminator. The method in~\cite{wang2022unbiased} assumes that a bias-free graph can be generated from pre-defined non-sensitive attributes. The non-sensitive attributes are used to construct a graph that is assumed to be free of bias. The authors then propose a regularization term that encourages the learned embeddings to satisfy certain fairness properties that are consistent with the bias-free graph. The regularization term is designed to penalize differences between the learned embeddings and the embeddings that would be obtained from the bias-free graph. The framework in~\cite{agarwal2021towards} considers counterfactual fairness and aims to maximize the similarity between representations of the original nodes in the graph, and their counterparts in the augmented graph. This is done by the introduction of a new learning objective function. Each counterfactual example is generated by modifying sensitive attributes and random masking of sensitive attributes.\\



\subsubsection{Post-processing Strategies} Several techniques outlined for both supervised and unsupervised learning can be employed as post-processing tactics in SSL. However, we have recognized "Fair Boosting" as the principal approach primarily utilized in SSL.


{\bf Fair Boosting.} Iosifidis et al.~\cite{iosifidis2019adafair} propose an algorithm termed AdaFair that builds upon the AdaBoost algorithm to enhance fairness during Boost rounds. As opposed to the AdaBoost, where in each round weak classifier only takes into account the hard classification, AdaFair additionally considers the discriminated group as they are dynamically being identified. Then, follows a reweighting strategy based on the accumulative fairness notion to tackle the problem of class imbalance. Zhu et al.~\cite{zhu2021rich} focus on pseudo labeling and reveal the disparate impact in SSL. The authors demonstrate that subpopulations with higher baseline accuracy levels without SSL benefit more from SSL. The opposite is also true, meaning subgroups who suffer low baseline accuracy tend to experience performance drop after SSL. An evaluation metric called Benefit Ratio is proposed to capture the normalized accuracy improvement on subgroups. The authors in~\cite{zhang2020fairness} propose a post-processing approach on top of pseudo labeling to lower discrimination-level explained in the following three steps. First, pseudo labeling is conducted based on a relatively small portion of data that are labeled. Second, several training datasets are generated by sampling from the pseudo labeled dataset. The sampling is conducted such that groups are fairly represented in terms of statistical parity. Finally, separate models are trained over the fair datasets, and ensemble learning is used to select the labels with the highest vote. 











