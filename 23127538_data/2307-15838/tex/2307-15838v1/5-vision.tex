\section{Vision and Challenges}\label{Sec: Vision and Challenges}
%--

%{\it Subgroup fairness}

%{\it counterfactual fairness}

%{\it Not just dp but others against fairness}

%{\it Problem with notions}

%{\it Studying fairness constraints with regard to multiple protected attributes
%(e.g. gender and occupation) as well as with multi-valued (more than two)
%protected attributes (e.g. race)}

\subsection{Privacy and Fairness in the Current Era of Large Language Models}

As we step into the era of large language models, the interplay between privacy and fairness becomes an ever more critical component. As models tend towards being more conversational, they will exhibit higher risks of running into privacy and fairness violations. There are several avenues of active research that might dictate the future of this area.

%\subsubsection{}

{\bf The Use of APIs.} A significant number LLMs have restricted access \citep{brown2020language, openai2023gpt4, Soltan2022} and some of these models can be accessed via APIs. These APIs can host models along with an arsenal of ex-ante and post hoc qualitative checks that enable API owners to control privacy as well as fairness of the model output. These qualitative checks are implemented via a variety of techniques ranging from simple filters to secondary models that are trained to detect and process model output in accordance with some chosen policies. As LLMs and related APIs become more ubiquitous, the qualitative checks implemented by these APIs are likely going to be key in terms of mitigating bias and privacy issues and improving the overall responsibility of model output. 

%\subsubsection{Logic-aware Models}

{\bf Logic-aware Models.} When it comes to building models that are fair, the de-biasing process can have an effect on privacy preservation \citep{Agarwal2020TradeOffsBF}. One way to approach this problem is explore bias mitigation methods that skip this step. Logic-aware language models \citep{luo2023logic} can be trained to reduce harmful stereotypes. Instead of typical sentence encoding they use a textual entailment which learns if parts of the second sentence text {\it entails}, {\it contradicts} or is neutral with respect to parts of the first one. Models trained in this way were significantly less biased than other baselines, without any extra data or additional training paradigms used. Logic-aware training methods might be paired with privacy preservation techniques in order to build models that are both private and fair. In order to address privacy-preservation, smaller (i.e. 500X smaller than the state-of-the-art models; \citep{luo2023logic}) logical language models that are qualitatively measured as fair, can be deployed locally with no human-annotated training samples for downstream tasks.

%\subsubsection{Privacy and Fairness in the Context of Learning from Human Feedback}

{\bf Privacy and Fairness in the Context of Learning from Human Feedback.} Fine-tuning with human feedback \citep{ouyang2022training}, has provided a promising way to make large language models align more with human intent. This technique can also be utilized to train models that are privacy-preserving and unbiased. Here, modes are expected to learn to return content that is preferred by humans, based on a training loop where feedback is provided via a reward model trained to rank model output based on what humans might prefer. The collection of human preferences that are used to train the reward model can be made in a fair and private way so that the reward model will learn these traits. This will in turn enable the foundational model to learn to generalize its behavior based on the feedback provided by the fair and privacy preserving reward model. It has already shown some promise in reducing harmful content \citep{ouyang2022training}, but more research in this area is needed.


\subsection{Fairness Through Privacy}

The majority of previous approaches aimed at mitigating bias require access to sensitive attributes. However, obtaining a significant amount of data with sensitive attributes is often impractical due to people's growing privacy concerns and legal compliance. Consequently, a crucial area of research inquiry that merits attention is how to ensure fair predictions while preserving privacy. This is a persistent challenge faced by technology companies that seek to balance the goal of ensuring fair ML processing of user data, including sensitive attributes such as Race and Gender, while simultaneously protecting user privacy and restricting the use of sensitive user data.



%This is a persistent challenge faced by technology companies as they are legally bound to perform fair ML processing of user data, including sensitive attributes such as Race and Gender, while simultaneously obligated not to use such data to maintain user privacy.




\subsection{Fair Privacy Protection}

%A crucial idea of fair privacy protection has been implicitly raised in several works in a nutshell and despite its severe consequences has not been studied enough. The motivation behind the approach is explained by the authors in~\cite{ekstrand2018privacy}: Does the system provide comparable privacy protections to different groups of subjects? The key idea is that this question raises is that privacy mechanisms do not provide degrees of privacy for users. Despite existing a lower bound on the amount of privacy achieved in a broader picture, some groups of the population are indeed receiving a greater amount of attention. 


The authors in~\cite{ekstrand2018privacy} pose a crucial question that sparked this subsection: Does a system provide equivalent privacy protections to different groups of individuals? The main idea behind fair privacy protection is to ensure that privacy mechanisms offer equal levels of privacy to all users, meaning that users are being treated fairly in terms of the amount of privacy protection they receive. Although there is a lower limit on the level of privacy achieved, such as in DP, some groups of the population may receive more attention than others in a broader context.



The significance of fair privacy protection is explained in the following example predicated on an observation made in DP publication of the US $2020$ census. In an observation made by the US Census Bureau Researchers, the Laplace mechanism in DP appeared to be disadvantaging low-populated areas like villages compared to highly populated cities such as metropolitan areas~\cite{o2019differential}. To demonstrate this, let us consider two cities, $A$ and $B$, with populations $a$ and $b$, respectively, where $a<<b$. The populations are sanitized using Laplace noise, with two noise values drawn from a Laplace distribution ($Lap(1/\epsilon )$) added to each population, and the private values are published. At first glance, both cities appear to be sanitized using the same Laplace distribution, and both achieve $\epsilon$-DP. However, upon closer inspection, the amount of noise added per individual in each city is examined. With knowledge that the variance of Laplace noise is $2/\epsilon^2$, the amount of noise variance per individual is derived as $2/(a\epsilon^ 2)$ and $2/(b\epsilon^2 )$. If $a<<b$, then it can be seen that $2/(b\epsilon^2 )$ is much less than $2/(a\epsilon^2 )$. In other words, the amount of noise per individual in the low-populated city is much higher than in the highly populated city, which raises questions about the fairness of the privacy guarantees imposed.











%\subsection{Going Beyond Race and Gender: The Role of Sensitive Attributes in ML}

%The existing literature primarily focuses on understanding the relationship between fairness and privacy concerning gender and race, with most studies considering these attributes in binary settings. However, achieving fairness for other sensitive attributes and their interaction with privacy remains a significant challenge. One key difference for other sensitive or non-sensitive attributes that can serve as proxies is the numerous categories they encompass. This issue is sometimes referred to as subgroup fairness~\cite{mehrabi2021survey} and is a complex and underexplored area. For instance, spatial information, which is often a proxy for race~\cite{mhasawade2021machine}, includes numerous groups vulnerable to unfair treatment. This information is crucial in social and political decision-making processes.



\subsection{Incorporating Privacy and Fairness based on Cryptographic Approaches}

%{TODO: Prof. Ghinita}
%Current literature does address the topic of privacy in ML through cryptography, with several reasonable-overhead solutions available for achieving private inference using PHE. The problem of private training requires FHE use, and it incurs a significantly higher overhead. However, 
No existing approach addresses {\em both} privacy and fairness in the cryptographic setting. Such an approach presents great promise, because it may be able to provide privacy and fairness under more relaxed system architecture assumptions. For instance, in the differential privacy case, one assumes the presence of a trusted curator, or that an extensive distributed infrastructure for federated learning exists. With cryptography, no trusted party is required to perform the computation.

The main challenge becomes how can one express fairness constraints so that they become implementable using the rather restrictive set of operations provided by various searchable encryption approaches. Can one achieve fairness directly under the encrypted ciphertext using primitives like PHE? Or are there more expensive primitives required, like FHE? And even with FHE, only polynomial evaluation is supported in the best case, whereas other operations (e.g., logarithm, sigmoid) must be simulated using polynomial approximations. Achieving fairness by using such primitives is an important and challenging research problem.






%\bibliographystyle{abbrv}
%\bibliographystyle{ACM-Reference-Format}
%\bibliography{Ref_main}

