




\section{Fairness}\label{Sec: fairness}
\subsection{Preliminaries}

The concept of fairness in society has been a recurring study subject throughout history~\cite{anderson2004pursuit}. Although early discussions were mainly philosophical, the rise of data and ML in the past decade has attracted tremendous attention to fairness in algorithms. As opposed to the initial perception of models and algorithms being trustworthy, soon it was realized that they could lead to severe unjust decisions, affecting especially individuals from disadvantaged groups. Perhaps, the most significant of such discoveries was revealed in an article published by ProPublica in $2016$, highlighting the significance of algorithmic fairness. The article focuses on a software named COMPAS~\cite{wenger2011compass} designed to determine the risk of a person committing another crime and assist US judges in making release decisions. The investigation found that COMPAS was biased against African Americans as it had a higher rate of false positives for this group compared to Caucasians. This and numerous other examples indicate the necessity to quantify and mitigate unfairness-related issues in ML. In the remaining of this subsection, we discuss bias in ML, what the law says about the issue, and online tools available to address the problem. 




















\subsubsection{Bias}

The term "bias" in ML has a distinct meaning that is different from the typical understanding of the term in social and news contexts~\cite{campolo2017ai}. Bias is seen as the root cause of unfairness and is often tied to a specific term that indicates where in the process, the data is being distorted. Over time, many different types of bias have been introduced in the literature, some of which are subcategories of others, leading to confusion in properly defining each one. For interested readers, we have provided a thorough classification and visualization of bias in ML in Appendix D. In this categorization, we have grouped potential types of bias into four general categories: \emph{A Biased World}, \emph{Data Collection and Preparation}, \emph{Model Training}, and finally, \emph{Evaluation and Deployment}.

%~\ref{section: appendix bias}
\begin{comment}
    


\subsubsection{Fairness in Legal Domain}

The United States labor law outlines employees, employers, and labor unions' rights and duties. Under this law, two legal terms are defined for lawful and unlawful practices regarding discrimination and fairness in the workplace, i.e., {\em Disparate Impact} and {\em Disparate Treatment}.

{\bf Disparate Treatment}~\cite{zimmer1995emerging} refers to unlawful behavior toward someone due to their protected attributes such as race. For example, the skill examination of job applicants due to their belonging to a particular ethnic group is an example of disparate treatment prohibited by law. 

 {\bf Disparate Impact}\cite{rutherglen1987disparate} is referred to practices that adversely affect a protected group even though rules applied by employers are formally neutral. Unlike disparate treatment, where discrimination is direct, disparate impact affects individuals and groups indirectly. In a court of law, a violation of Title VII of the 1964 Civil Rights Act may be proven by demonstrating the adverse impact of a policy on members of a protected class~\cite{selbst2017disparate}.

The definition of sensitive attributes in ML is specified by the legislative bodies. For example, in the US, organizations such as US Fair Housing Act, Federal Equal Employment Opportunity, and Equal Credit Opportunity Act~\cite{agrawal2018prediction} have separate guidelines for sensitive attributes. The most commonly protected features include race, gender, religion, and national origin. Appendix~\ref{section: appendix sensitive attributes} provides a table of sensitive attributes for several organizations throughout the US.
\end{comment}





\subsubsection{Philosophies of Fairness in Context}

In the domain of work and employment, principles of fairness and non-discrimination guide the relationships among employees, employers, and labor unions. Two core fairness principles, often identified as `Disparate Impact' and `Disparate Treatment', are observed in this context. Disparate Treatment~\cite{zimmer1995emerging} acknowledges that unjust behaviors towards individuals due to their protected attributes, such as race, are unacceptable. An instance reflecting this principle in action could be prohibiting the exclusive skill examination of job applicants based on their ethnic group affiliation. Disparate impact~\cite{rutherglen1987disparate} pertains to practices that inadvertently disadvantage a protected group, even though the policies implemented by organizations appear neutral on the surface. This principle recognizes that discrimination is not always direct, and it can affect individuals and groups in indirect ways. A classic example includes policies that, while appearing neutral, disproportionally impact members of a protected group in a negative manner~\cite{selbst2017disparate}.

In ML, fairness principles are implemented in various ways to uphold the aforementioned principles for sensitive attributes. Notably, different organizations provide guidelines on what constitutes sensitive attributes. The most commonly protected features include race, gender, religion, and national origin. For more detailed information, please refer to Appendix E, which provides a table of sensitive attributes identified by several organizations.

%~\ref{section: appendix sensitive attributes}















\subsubsection{Available Online Tools}

The significance of algorithmic fairness has led to the development of several online tools to assist the incorporation of fair practices in ML models. \emph{Fairlearn} \cite{bird2020fairlearn} is an open-source tool developed at Microsoft Research that helps data scientists and developers assess and improve the fairness of their AI models. \emph{AI Fairness 360} (AIF360) \cite{bellamy2019ai} is an open-source toolkit created by IBM that helps developers analyze, document, and eliminate unfairness in ML models throughout their lifecycles. It provides various metrics and mitigation algorithms to assess and address bias in models. \emph{Aequitas} \cite{saleiro2018aequitas} is another open-source tool that helps identify and eliminate bias in ML models. It offers metrics, visualizations, and techniques for auditing models, allowing researchers, analysts, and policymakers to make informed decisions during model development and deployment. Google's \emph{What-If Tool} \cite{wexler2019if} and LinkedIn Fairness Toolkit (LiFT) \cite{vasudevan2020lift} are also some of the latest developments to further enhance algorithmic fairness. 



\textbf{Structure}
In the following subsections, we thoroughly review fairness in supervised, unsupervised, semi-supervised, and RL. In each subsection, we start by defining fairness notions and definitions dedicated to the type of ML learner, followed by explaining the existing unfairness mitigation techniques for fair treatment of individuals and groups. The mitigation algorithms are divided into pre-processing, in-processing, and post-processing strategies. SSL lies at the intersection of supervised and unsupervised learning. To the best of our knowledge, there is no specific fairness notion proposed particularly for SSL, despite the existence of dedicated unfairness mitigation algorithms. Due to limited space, we have moved the discussions related to SSL to Appendix F. %~\ref{section: appendix SSL}






























\subsection{Fairness in Supervised Learning}
\subsubsection{Notions and Definitions}

As opposed to privacy, where at least for statistical databases, there is a consensus on DP, there does not exist such an agreement on a common notion for fairness. One suggested guideline is to select the notion based on the underlying application. This section reviews some of the most widely adopted fairness notions for supervised learning. In this context, we use terms notion and definition interchangeably. Also, deviation from a fairness notion is referred to as {\em discrimination level}. Discrimination is usually manifested as the absolute value of the difference in metrics for different groups. Moreover, we denote the set of sensitive attributes by $A$, all observed attributes by $X$, latent attributes not observed by $U$, true label to be predicted by $Y$, and finally, predictor by $\hat{Y}$.


We debut our discussions on notions with statistical parity, one of the primary group-level fairness notions. 

\begin{defn} \label{def: statistical parity}
(Demographic or Statistical Parity~\cite{dwork2012fairness,calders2010three}).  A predictor $\hat{Y}$ satisfies demographic parity if:
\begin{equation}
    P(\hat{Y}=1| A = 0) = P(\hat{Y}=1|A=1).
\end{equation}
\end{defn}

Statistical parity dictates that regardless of an individual's group, they should have an equal chance of being assigned to a positive class. Figure~\ref{fig: statistical parity example} exemplifies statistical parity. Consider two groups of male and female job applicants and an ML model that decides whether a person should proceed for further evaluation in their application. Here, the likelihood of moving ahead with male and female applicants is $5/10$ and $7/10$, respectively. Hence, discrimination based on statistical parity is $20\%$. The notion of equalized odds, presented next, takes a step further and requires an equal true positive rate across groups. 



\begin{defn} \label{def: Equalized Opportunity}
(Equalized Opportunity~\cite{dwork2012fairness}). A predictor $\hat{Y}$ satisfies equal opportunity with respect to protected attribute $A$ and outcome $Y$, if $\hat{Y}$ and $A$ are independent conditional on $Y$,
\begin{equation}
    P( \hat{Y}=1|A=0,Y =1) = P( \hat{Y}=1|A=1,Y =1).
\end{equation}
\end{defn}

That means the true positive rate should be the same for both groups. Going back to the example in Figure~\ref{fig: statistical parity example}, the true positive rate for males and females is $3/6$ and $5/7$, leading to discrimination of $21.4\%$ based on equalized opportunity. The next notion, equalized odds, dictates an even stricter fairness notion requiring equal true and false positive rates across groups. 





% Figure environment removed
















\begin{defn} \label{def: Equalized Odds}
(Equalized Odds~\cite{dwork2012fairness}). A predictor $\hat{Y}$ satisfies equalized odds with respect to protected attribute $A$ and outcome $Y$~if:
\begin{equation}
    P( \hat{Y}=1|A=0,Y =y) = P( \hat{Y}=1|A=1,Y =y),\;\; y\in \{0,1\}.
\end{equation}
\end{defn}

In the example, discrimination based on equalized odds is $38.1\%$. As can be seen, imposing higher fairness guarantees intuitively results in a higher percentage of discrimination. 

%Changing gear, calibration notion, borrowed as one primary concept in ML, is presented in Definition~\ref{def: Calibration}. Calibration is a group-level fairness metric that ensures confidence scores generated by the ML model can be considered as likelihoods. 


Definition~\ref{def: Calibration} introduces the concept of calibration, which is a crucial idea borrowed from ML. This notion ensures that the confidence scores produced by the model can be interpreted as probabilities and is considered a group-level fairness notion. 


\begin{defn} \label{def: Calibration}
(Calibration~\cite{flores2016false,pleiss2017fairness}).  An ML model is said to be calibrated if it produces calibrated confidence scores. Formally, the outcome score $R$ is said to be calibrated if for all the scores $r$ in the support of $R$ following stands, 
\begin{equation}
    P( y = 1 | R = r  )= r.
\end{equation}
\end{defn}

Calibration ensures that the set of all instances assigned a score value $r$ has an $r$ fraction of positive instances among them. Note that the metric is defined on a group level, and it does not mean that an individual who has a score of $r$ corresponds to $r$ probability of a positive outcome. For example, given $10$ people who are assigned a confidence score of $0.7$, in a well-calibrated model, we expect to have $7$ individuals with positive labels among them. 

So far, the fairness definitions discussed were all focused on group-level fairness. In the following, two of the common notions to achieve fairness at an individual level are presented. 


\begin{defn} \label{def: counterfactual fairness}
(Counterfactual Fairness~\cite{kusner2017counterfactual}). Given a causal model ($U$, $V$, $F$), where $U$, $V$, and $F$ represent the set of latent (unobserved) background variables, the set of observable variables, and a set of functions defining the mapping $U \cup V \rightarrow V$, respectively, a predictor $\hat{Y}$ is considered counterfactually fair if, under any context $X = x$ and $A = a$, the following equation holds:

\begin{equation}
P(\hat{Y}_{A\leftarrow{a}} (U)=y | X=x, A=a) = P(\hat{Y}_{A\leftarrow{a'}} (U)=y | X=x, A=a).
\end{equation}

This holds for all $y$ and for any value $a'$ attainable by $A$. Here, $A$, $X$, and $\hat{Y}$ represent the set of sensitive attributes, remaining attributes, and decision output, respectively. In other words, the model's predictions for a person should not change in a counterfactual world in which the person's sensitive features are different.
\end{defn}



\begin{defn} \label{def: Individual location fairness}
(Individual Fairness by Dwork et al.~\cite{dwork2012fairness}). 
For a mechanism $\mathcal{M}$ mapping $\boldsymbol{u}$ in the input space $\mathcal{X}$ to value $y$ in the output space $\mathcal{Y}$, individual fairness is satisfied when for any $\boldsymbol{u},\boldsymbol{v} \in \mathcal{X}$:
\begin{equation}
     d_{\mathcal{X}}(\boldsymbol{u},\boldsymbol{v})\geq d_{\mathcal{Y}}(\mathcal{M}(\boldsymbol{u}),\mathcal{M}(\boldsymbol{v})),
\end{equation}
where $d_{\mathcal{X}}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}_+$ and $d_{\mathcal{Y}}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}_+$.
\end{defn}

To illustrate, let's consider the scenario of classification, where the classifier's predictor $\hat{Y}$ serves as the mapping mechanism. In the context of individual fairness, the fundamental idea is that two individuals who are alike in relevant ways should receive comparable outcomes. To operationalize this concept, we rely on two crucial distance metrics: (1) a similarity distance metric $d_{\mathcal{X}}$ that gauges how similar two individuals are to each other, and (2) a distance metric $d_{\mathcal{Y}}$ that quantifies the disparity between the distributions of outcomes.














\subsubsection{Unfairness Mitigation Algorithms} 


%\hfill 
\paragraph{Pre-processing Strategies}
%Pre-processing techniques are focused on making ML models fairer prior to training. This can be achieved through various methods, including (I) {\em reweighting;} adjusting the weight of data entries to reduce unfairness; (II) {\em representation learning;} transforming the data so that it does not reveal information about specific groups or promote fair treatment in the trained model; (III) {\em label alteration;} changing some labels in the input data to remove bias; (IV) {\em suppression;} removing sensitive attributes or those highly correlated with them, which is a common practice as a performance evaluation baseline; 



{\bf Reweighting.} This approach focuses on modifying the significance of data points manifested as weights during the training to mitigate bias. The method employed in \cite{kamiran2012data} estimates the probability of an individual from a group receiving a specific result and employs the ratios of these probabilities as reweighting factors during optimization. Nevertheless, in some instances, access to sensitive information may be restricted. To tackle this issue, Lahoti et al.~\cite{lahoti2020fairness} learn the reweighting factor using adversarial learning. Furthermore, Roh et al. \cite{roh2020fairbatch} suggest a two-tier optimization method that chooses specific mini-batch sizes to attain group fairness.




{\bf Representation learning.} More recently, a thread of research has focused on changing data representation to improve fairness in classifiers. Several techniques have been proposed predicated on either philosophy of fairness through unawareness or fairness through awareness. In the former, the logic is to learn a new representation for individuals independent of their protected attribute while preserving other information, and the latter focuses on achieving fairness while considering protected group information. In~\cite{feldman2015certifying}, the authors modified dataset features to have similar distributions for both protected and unprotected groups, making it hard to distinguish between them. Zemel et al.~\cite{zemel2013learning} proposed mapping individuals to a new distribution that protects the protected group information while retaining other information. The authors in~\cite{louizos2015variational} follow up this idea by using a variational auto-encoder to make the sensitive attributes independent of the latent representation and applying further learning to that representation. %Samadi et al.~\cite{samadi2018price} showed that dimensional reduction could inadvertently cause bias in ML and proposed a polynomial time algorithm to maintain group fidelity.





{\bf Label alteration.} Label alteration aims to make classifiers fairer by adjusting the labels of training samples. Some works modify the labels to achieve an equal proportion of positive examples across protected groups \cite{kamiran2012data}, while others flip the labels of instances that have been determined to be discriminatory based on differences in treatment among similar samples \cite{luong2011k}.

%Studies have used label alteration to 











\paragraph{In-processing Strategies}
%In-processing techniques focus on mitigating fairness during the ML training process. We have grouped these techniques into three broad categories: Regularizers, Adversarial-based, and Reweighting-based approaches. 

{\bf Regularizers and constraints.} This strategy aims to add penalty terms to the classifier's objective function to either minimize the impact of sensitive features on prediction~\cite{kamishima2012fairness}, or achieve similar False Positive and False Negative rates across populations~\cite{bechavod2017penalizing, berk2017convex}. In a similar approach, Kamiran et al.~\cite{kamiran2010discrimination} propose modifying the splitting criterion in decision trees to minimize the impact of sensitive features and maximize the information gain between the split feature and class label. The authors in several studies aim to enforce fairness notions constraints in optimization. This approach for statistical parity is discussed in~\cite{zafar2017fairness}, equalized odds and opportunity in~\cite{zafar2017fairness2, woodworth2017learning}. Quadrianto et al.~\cite{quadrianto2017recycling} suggest using privileged learning to ensure fairness where sensitive features are only available during training. %The authors in \cite{agarwal2018reductions} propose reducing fair binary classification into cost-sensitive problems where misclassification costs are not equal, and the goal is to minimize the total cost to balance the accuracy-fairness trade-off.

{\bf Adversarial learning.} The main concept within this group involves utilizing Generative Adversarial Networks (GANs) to optimize the effectiveness of a predictor while reducing its capability to forecast sensitive characteristics~\cite{zhang2018mitigating}. This approach can be implemented across various gradient-based learning models, such as classification and regression assignments. 



{\bf Reweighting.} The reweighting approach proposed part of pre-processing strategies has also been employed during the training. The proposed approach by Krasanakis et al. \cite{krasanakis2018adaptive} trains an unweighted classifier, then learns weights for each sample and retrains the classifier to improve the fairness-accuracy trade-off. The iterative approach of improving reweighting factors helps to derive more accurate reweighting factors.






\paragraph{Post-processing Strategies} %Post-processing approaches focus on achieving fairness once the training of the ML model is completed. We have categorized the post-processing approach into two broad categories: {\bf transformation} and {\bf thresholding}. Transformation techniques modify the output scores to achieve higher fairness levels, while thresholding techniques focus on adjusting the label generation threshold of the classifier for fairness improvement.

{\bf Transformation.} This technique aims to modify the output scores to achieve higher fairness levels. One of the primary techniques in this category is Platt Scaling focused on improving miscalibration in ML models~\cite{platt1999probabilistic}. Calibration is improved by fitting the output scores to a logistic regression model. Histogram Binning and Isotonic Regression~\cite{zadrozny2001obtaining} also enhance calibration by fitting output scores to a monotonic function. To achieve individual fairness, the authors in~\cite{shaham2022models} propose using $c$-fair polynomials, which map classifier scores to a polynomial and restrict scores of each individual by their sensitive feature distance. Petersen et al.~\cite{petersen2021post} improve individual fairness by smoothing output scores using a similarity graph and Laplacian regularization. Kim et al.~\cite{kim2019multiaccuracy} present a Multi-accuracy Boost framework that improves accuracy across all subgroups, using iterations and weights to enhance predictions conducted by the auditor.

{\bf Thresholding.} The proposed techniques in this category aim to adjust the label generation threshold of classifiers to make non-discriminatory decisions \cite{kamiran2012decision}. For instance, different threshold values are selected for protected groups in \cite{menon2017cost} to maximize accuracy while achieving statistical parity. Hardt et al.~\cite{hardt2016equality} optimizes threshold selection for each sensitive group for high utility and improved fairness. Similarly, the authors in~\cite{corbett2017algorithmic} infer group-specific thresholds for a trade-off between accuracy and fairness. Lohia et al.~\cite{lohia2019bias} develop a bias-mitigation technique by targeting samples that inhibit individual bias from improving individual and group-level fairness notions.





















\subsection{Fairness in Unsupervised Learning}


When data labels are unavailable, unsupervised ML algorithms are commonly used as opposed to supervised algorithms. However, evaluating fairness is more challenging in the absence of labels because there is no ground truth available for assessment. To address this issue, we will begin our discussion by examining individual and group-level fairness notions that have been suggested for unsupervised learning, followed by an exploration of mitigation algorithms.


\subsubsection{Notions and Definitions}

The majority of fairness concepts for unsupervised learning are based on the disparate impact doctrine, which seeks to achieve a comparable proportion of protected groups across all clusters. To begin our conversations on fairness concepts, we will start by examining the Balance metric, regarded as one of the fundamental definitions of group-level fairness in unsupervised learning.


\begin{defn}
(Balance\cite{chierichetti2018fair,bera2019fair}). Define the ratio of the protected group $b\in [m]$ in the entire dataset as $r_b$, and let $r_{a,b}$ represent this proportion in the generated cluster $a\in [k]$. The balance metric evaluates the disparity between these two ratios by defining $R_{a,b} = r_b/r_{a,b}$ and introducing the balance fairness concept as follows:

\begin{equation}
\min_{a\in [k], b\in [m]} \min { R_{a,b}, 1/R_{a,b} }.
\end{equation}
\end{defn}

The Balance metric produces values within the range of $0$ to $1$, where higher scores indicate a higher level of fairness. This measure takes into account both the percentage of protected group members in the entire dataset and within individual clusters, with fairness achieved when the ratio remains consistent across all clusters.







\begin{defn}
(Bounded Representation\cite{ahmadian2019clustering}). Let $r_{a,b}$ denote the ratio of protected group $b\in [m]$ in cluster $a\in [k]$. The $(\alpha,\beta)$-bounded representations dictates that:

\begin{equation}
    \beta \leq   r_{a,b}   \leq \alpha.
\end{equation}

\end{defn}

Bounded representation allows some degree of deviation in the proportion of protected groups within clusters. When the bounds are equal, it suggests that the proportion of protected groups in each cluster should be consistent with the overall ratio in the dataset. 





\begin{defn} \label{def: MFC}
(Max Fairness Cost (MFC)~\cite{chhabra2020fair}). Let $I_b$ denote the ideal proportion of protected group $b\in [m]$ in clusters. Once the ideal ratio parameter is passed as input, the MFC notion is defined as 

\begin{equation}
    \max_{a\in [k]} \sum_{b\in[m]} |r_{a,b}-I_b|,
\end{equation}
where $r_{a,b}$ denotes the ratio of protected group $b$ in cluster $a\in [k]$. 
\end{defn}

Intuitively, MFC calculates the summation of all deviations from the ideal ratios for each protected group, and returns the maximum value. A lower MFC value indicates a higher degree of fairness. Setting the value of $I_b$ equal to the ratio of the protected group in the original dataset ($r_b$) ensures that this ratio remains consistent across all clusters.



\begin{defn} \label{def: Social Fairness}
(Social Fairness~\cite{ghadiri2021socially}). Let $C$ denote the cluster centers in $k$-means algorithm and $L(C,D_b)$ denote the $k$-means clustering cost, where $D_b$ is the input error on the samples of the protected group $b\in [m]$. The social fairness notion is then defined as:
%$L(C,D_b) =  \sum_{d\in D_b} \min_{c\in C}$ 
\begin{equation}
    \max_{b\in [m]} \dfrac{L(C,D_b)}{|D_b|},
\end{equation}
\end{defn}
Social fairness focuses on the maximum imposed loss on protected groups. Next, we focus on fairness notions proposed on the individual-level. 


\bigskip

\begin{comment}


\begin{defn} \label{def: MFC}
(Entropy\cite{li2020deep}). Let $I_b$ denote the ideal proportion of protected group $b\in [m]$ in clusters. Once the ideal ratio parameter is passed as input, the MFC notion is defined as 

\begin{equation}
    \max_{a\in [k]} \sum_{b\in[m]} |r_{a,b}-I_b|,
\end{equation}
where $r_{a,b}$ denotes the ratio of protected group $b$ in cluster $a\in [k]$. 
\end{defn}



{\bf 5. Entropy (Group Level)} borrowed from Information Theory is also used quantify fairness in clustering~\cite{li2020deep}. Generally speaking, higher value of Entropy indicates a higher level uncertaintly, referring to a better distribution of individuals in each group among the clusters. As before, consider $r_{a,b}$ to be the ratio of protected group $b\in [m]$ in cluster $a\in [k]$.  Then, the entropy fairness metric can be defined for clustering as 


\begin{equation}
    \sum_{a\in [k]} r_{a,b} \times \log ( r_{a,b})
\end{equation}
\end{comment}

\bigskip

\begin{defn} \label{def: Fuzzy Individual Fairness}
(Fuzzy Individual Fairness~\cite{dwork2012fairness}). For every two points $x$ and $y$, and their respective distributions $X$ and $Y$ over clusters in a given fuzzy clustering algorithm, let $F(x,y)$ measure the similarity between the two datapoints, and let $D_f(X||Y)$ denote the statistical distance between their distributions. Fuzzy individual fairness requires the satisfaction of the following constraint:

\begin{equation}
D_f(X||Y) \leq F(x,y).
\end{equation}
\end{defn} 

This notion aims to apply the individual fairness notion in~\cite{dwork2012fairness} for fuzzy clustering. Common choices for measuring the statistical distance include the variations of $f$-divergence metric, such as KL-divergence, reverse KL-divergence, and the total variation distance.


\bigskip


\begin{defn} \label{def: kleindessner2020notion}
(Individual Fairness~\cite{kleindessner2020notion}). This notion requires the average distance of every sample point to members in its own cluster to be smaller than its average distance to members of any other cluster. Formally, for a disjoint clustering of data denoted by $\mathcal{C} = { C_1,, C_2,, \ldots, C_k }$, and a distance metric $d$, for every sample point $x\in C_i$, the following inequality should hold:

\begin{equation}
\dfrac{1}{|C_i|-1} \sum_{y\in C_i} d(x,y) \leq \dfrac{1}{|C_j|} \sum_{y\in C_j} d(x,y), ,,, i\neq j.
\end{equation}
\end{defn} 


The intuition behind the above notion is to ensure every sample point is associated with a cluster that has the highest average similarity. 










%Several other notions also exist in the literature to quantify fairness based on their distance to the center of clusters. The authors in~\cite{jung2019center}  define "neighborhood radius" for a data point as the minimum radius ball with the data point at the center such that it entails $n/k$ data samples. The objective is to ensure that each individual has a center within at most a small constant factor of its neighborhood radius. Fairness definition in~\cite{chakrabarti2021new} requires the distance of a point from its center to be at most $\alpha$ times the average distance of the in that cluster to the center. The approach in~\cite{chen2019proportionally} provides an alternative, and somehow less noticed fairness notion motivated by the notion of core in economics. The definition is developed for center-based algorithms and tends to select $k$ fair cluster centers among a pool of possible centers for individuals. The core idea is that $n/k$ points are entitled to form their own cluster if there is another center that is closer in the distance for all $n/k$ points. In simple words, for given cluster centers, there must not exist a large enough coalition of data points that have a lower cost to one of the centers in the original pool of cluster centers.















\subsubsection{Unfairness Mitigation Algorithms}  


\paragraph{Pre-processing Strategies} %We have classified the preprocessing strategies into two distinct categories: fairlet decomposition and data augmentation.






{\bf Fairlet Decomposition.} The concept of fairlets, which was introduced in~\cite{chierichetti2017fair}, aims to improve the fairness of various clustering algorithms based on the Balance metric. The strategy involves dividing the data points into small groups, or so-called fairlets, before performing clustering in such a way that the disparate impact doctrine is maintained. Each fairlet is then represented by a single point, and a vanilla clustering algorithm is applied to these representative points. Because the representative points are reasonably fair, the final clustering also tends to be fair. In~\cite{chierichetti2018fair}, near-linear algorithms are proposed for fairlet decomposition. Ahmadian et al.~\cite{ahmadian2020fair} employ the idea of fairlets for hierarchical clustering considering several objective functions such as revenue, value and cost. %Fairlet decomposition for correlation analysis is studied in~\cite{ahmadian2020fair}. As opposed to $k$-median and $k$-means clustering in which the objective function is well-defined, the definition of fair clustering for correlations depends on the properties of metric space. The authors propose a cost function to tackle this problem and show that given a solution for fairlet decomposition problem, the fair correlation clustering instance can be reduced to a regular correlation clustering instance based on graph transformation. 

%In~\cite{schmidt2019fair}, the authors employ the concept of corsets from~\cite{har2004coresets} to achieve fair clustering, building on the idea of disparate impact. Essentially, a corset is a condensed representation of a set of points that can provide a good approximation for any potential solution. By using corsets, it becomes feasible to attain fairness in datasets that are of a large scale. The integration of fair clustering algorithms and corsets offers a means of performing scalable fair clustering.

{\bf Data Augmentation.} Inspired by the approach in~\cite{rastegarpanah2019fighting}, Chhabra et al.~\cite{chhabra2022fair} propose data augmentation as an efficient method for fair clustering. The method involves augmenting the dataset using a small subset of data to achieve a fairer clustering output after applying the algorithm. The authors propose a general bi-level formulation to address two problem settings: 1) using convex group-level fairness notions and convex center-based clustering objectives, and 2) using general group-level fairness notions and general center-based clustering objectives.





\paragraph{In-processing Strategies} %We have categorized the current techniques for in-process fair clustering into two primary groups: one based on regularizers and constraints, and the other using alternating objectives. The first approach involves incorporating additional regularizers during clustering to attain fairness, while the second approach focuses on entirely alternating between fairness and clustering objectives.


{\bf Regularizers and Constraints.} Built on the Balance metric, the authors in~\cite{kleindessner2019guarantees} incorporate fairness constraints in spectral clustering as well as providing empirical evidence that it is possible to achieve higher demographic proportionality at minimal additional cost in the clustering objective. Li et al.~\cite{li2020deep} introduce a fairness-adversarial term encouraging soft assignments that remain constant across various protected subgroups, resulting in a model that is not influenced by sensitive attributes. Zhang et al.~\cite{zhang2021deep} propose an approach for fairness in deep clustering. A regularization term based on the Balance notion is proposed and is combined with the clustering objective. Chai et al~\cite{chaifair} propose to use Sinkhorn divergence to reduce differences in predicted soft labels among various demographic groups and to develop representations that are conducive to clustering. The requirement of equalized confidence is modeled as a regularization term during training using Sinkhorn divergence, with several additional regularizers for ensuring accuracy.


{\bf Alternating Objective.} This approach focuses on entirely alternating between fairness and clustering objectives during unsupervised learning. Liu et al.~\cite{liu2023stochastic} formulate the cost of clustering and fairness as a bi-objective optimization problem to achieve balance. Their approach is based on mini-batch $k$-means clustering, where the algorithm performs clustering based on the $k$-means during mini-batch updates. However, the algorithm also includes a series of swap-based steps to enhance the balance in clusters. The routine involves exchanging data points between the least balanced and well-balanced clusters after the mini-batch update. The method described in~\cite{ziko2021variational} combines the Kuulback-Leibler fairness term with the objective of center-based and graph-based algorithms. The approach involves conducting a separate update for the assignment of datapoints based on the fairness objective and clustering objective. The heuristic algorithm proposed in~\cite{chen2019proportionally} focuses on achieving the proportionality fairness notion by alternating between fairness and clustering objective. The proposed algorithm achieves a $(1+\sqrt{2})$-proportional solution. %Anderson et al.~\cite{anderson2020distributional} propose an approach for fair clustering based on the individual fairness metric presented in~\cite{dwork2012fairness}. Their method uses an alternating approach where the vanilla clustering algorithm generates a set of clusters first. Then, a constraint optimization problem based on individual fairness is formulated, and its solution is combined with the original clustering algorithm to produce the final clustering output.

















%Geometric approach: fairlet (features are known)
%constraint based: add constraint as they are in instance level 


%In-process algorithms tend impose fairness on clustering objective function or algorithm. 


%Built upon the disparate impact, the authors in~\cite{schmidt2019fair} use the idea of corsets in~\cite{har2004coresets} for fair clustering. Roughly speaking a corset is a summary of a point set that approximates well for any possible solution candidate. As the definition of corsets suggests, it allows for achieving fairness in large scale datasets.The combination of corsets and fair clustering algorithms allow for scalable fair clustering.


%The authors in~\cite{schmidt2019fair} propose a variant of LIoyd's algorithm that computes fair clustering and extend it to fair $k$-means++ clustering algorithm. The combination of corsets and fair clustering algorithms allow for scalable fair clustering. Built on the Balance metric, the authors in~\cite{kleindessner2019guarantees} incorporate fairness constraints in spectral clustering as well as providing empirical evidence that it is possible to achieve higher demographic proportioniliy at minimal additional cost in the clustering objective. 


%The authors in~\cite{thejaswi2021diversity} formulate the diversity-aware k-median problem. In this problem, set of clients $C$, set of facilities $\mathcal{F}$, and a collection of facility groups $\{F_1, ..., F_t \}$ is provided, and the idea is to select $k$ cluster facility such that the number of selected facilities from each facility remains above a certain threshold and $k$-median objective function between clients and facilities are minimized. The authors show that in general case where the facility groups can overlap the problem is NP-hard, and develop approximation for the case of disjoint facility groups.

%Series of works focus on achieving fairness in deep clustering as it poses unique set of challenges due to absence of priori knownledge about features as opposed approaches. 


%Initial works on fair deep clustering such as~\cite{wang2019towards} approach the problem with geometric perspective by learning a fair representation with multi-state protected status variables. %In~\cite{li2020deep}, deep fair visual clustering model with adverserial learning to conduct clustering statistically independent from the sensitive attributes. 



%Zhang and Davidson~\cite{zhang2021deep} formulate group level fairness objective function for deep clustering as an integer linear programming and show that it can be solved efficiently.  



%102-111



\paragraph{Post-processing Strategies} The majority of the methods proposed for the post-processing stage involve using linear programming to reassign data points according to fairness metrics. This approach is referred to as the LP formulation. Additionally, Simoes et al.~\cite{simoes2022exploring} have recently explored an alternative approach based on {\em Data Perturbation} for fair clustering. The core concept of this approach is to perturb the assignment of data points to clusters in several iterations based on the "Rawls' difference principle"~\cite{altham1973rawls}.

%The post-processing approach proposed in~\cite{simoes2022exploring} propose to perturb the assignment of datapoints once the clustering is done. The purturbation is conducted based on  

{\bf LP formulation.} Considering disparate impact doctrine, the authors in~\cite{bera2019fair} show that for a given clustering with $l_p$-norm objective including center-based approaches such as $k$-means and $k$-medoids, it is possible to have fair algorithms with a slight sacrifice in fairness constraint. In more detail, given any $\rho$-approximation algorithm for a given clustering objective, a $(\rho+2)$-approximation solution exist for the best clustering, which satisfies fairness constraints. The objective is achieved by formulating and solving an LP optimization problem for fair assignment after clustering. Approaches in~\cite{ahmadian2019clustering} and~\cite{harb2020kfc} also use alternative LP formulations for fair assignment of datapoints to centers. Esmaeili et al.~\cite{esmaeili2020probabilistic} extend the approach to a scenario where data points are probabilistically assigned to groups instead of having a priori information on the group assignment. In~\cite{esmaeili2021fair}, first the center-based clustering algorithm is applied to maximize the clustering objective. Then, using an LP formulation, clustering is improved considering the fairness objective. This is done by searching for the cluster with maximum violation of the fairness objective considering the upper bound required for the clustering objective and rounding the possibly fractional solution to a feasible integer solution using a network flow algorithm. 







%{\bf Data Perturbation.} The post-processing approach proposed in~\cite{simoes2022exploring} propose to perturb the assignment of datapoints once the clustering is done. The purturbation is conducted based on `Rawls' difference principle' over several iterations. 




\













%\newpage

\subsection{Fairness in Reinforcement Learning}
RL is concerned with learning how to make decisions in an environment by maximizing some cumulative reward (or equivalently, minimizing some regret)  through interacting with the environment and receiving feedback. The decisions made by RL agents can have a significant impact on individuals and society, making it essential to ensure that these decisions are unbiased and fair. Compared to other methods in which only the immediate impact of the decision-making algorithm is studied to mitigate unfairness, algorithms for fair RL aim to account for the long-term consequences of the agent's actions to ensure that they are unbiased \cite{wang2023survey}. 

In the context of fairness in RL, a significant focus is dedicated to addressing unfairness across various variants of the bandit problem. Bandit problems involve an agent repeatedly selecting from a set of arms, each associated with unknown reward distributions. The agent's objective is to determine an optimal \emph{policy} that maximizes cumulative reward while receiving limited feedback on unchosen arms \cite{lattimore2020bandit}. Bandit scenarios provide a tractable framework for exploring and developing fair decision-making algorithms in RL.

%This is particularly important when it comes to sensitive domains such as healthcare, criminal justice, hiring, and college admissions, where the decisions have a profound impact on people's lives. Another challenge in this domain is to optimize competing objectives. The RL agent's primary objective is to maximize rewards by learning from interactions with the environment and observing states. But, without considering fairness, this can result in biased decisions if the rewards function is not carefully designed. Therefore, ensuring fairness requires the agent to meet additional constraints, which may conflict with its primary goal of maximizing rewards, leading to a trade-off between fairness and utility. To address these challenges, researchers have proposed various fairness notions and mitigation techniques for the fair RL field, which we will introduce in the following subsections.


\subsubsection{Notions and Definitions}

% \textbf{TODO: What to do about meritocratic fairness, as it seems more like a philosophy?}
% \textbf{TODO: Better categorization: 1- Meritocratic Fairness: A worse arm should have no chance of being selected over a better arm. 2- Individual Fairness: Similar individuals (arms) be treated similarly 3- Constraint on the proportions in which arms should be selected (see \cite{chien2022multi})}
In recent years, several perspectives have emerged for evaluating and improving the fairness of RL algorithms. Specifically, researchers have proposed and evaluated notions of fairness in RL from three distinct perspectives: \emph{Meritocratic Fairness}, \emph{Individual Fairness}, and \emph{Proportional Fairness}. These perspectives can be quantified in different ways, depending on the specific goals and objectives of the RL algorithm. %We will introduce each of these perspectives in more detail in the following sections and provide notions that can be used to quantify them.

% \begin{defn} \label{def: meritocratic fairness}
% (Meritocratic Fairness). 
% This notion of fairness requires avoiding favoring less qualified individuals over more qualified ones. For example, in the context of bandits, this fairness notion indicates that it is unfair to preferentially select an arm that has a lower expected reward over other available arms that have higher expected rewards. Similar to individual fairness, various studies employ different metrics to achieve this notion of fairness. The following definitions are some examples used in the literature.
% \end{defn}

{\bf{Meritocratic Fairness.}}
This perspective requires avoiding favoring less qualified individuals over more qualified ones. For example, in the context of bandits, this fairness notion indicates that it is unfair to preferentially select an arm with a lower expected reward over other available arms with higher expected rewards~\cite{joseph2016fairness}. This ensures that the rewards are allocated fairly based on the arms' abilities. The following definitions are some examples of evaluating fairness from this aspect in literature.



\begin{defn} \label{def: delta-fairness classic bandits}
($\delta$-Fairness in Classic Bandits~\cite{joseph2016fairness}).
An algorithm $\mathcal{A}$ is deemed $\delta$-fair if, with a probability of at least $1-\delta$ over history $h$, for all distributions $\mathcal{D}_1,...,\mathcal{D}_k$, every $t \in [T]$, and all $j,j' \in [k]$:
\begin{equation}
\pi^t_{j|h} > \pi^t_{j'|h} \text{ only if
} \mu_j > \mu_j'
\end{equation}
Here, $T$ represents a known horizon, $[k]={1,...,k}$ denotes the set of arms, and $\mathcal{D}_1,...,\mathcal{D}k$ are the unknown reward distributions of arms. $\mu_i$ is the unknown average reward of the $i$-th arm, and $\pi_{i|h}^t$ is the probability that $\mathcal{A}$ selects arm $i$ given history $h$.
\end{defn}
In essence, this definition suggests that selecting one arm over another is considered unfair if there is sufficient confidence to indicate that the chosen arm has a lower expected reward compared to the unselected one.


\begin{defn} \label{def: delta-fairness contextual bandits}
($\delta$-Fairness in Contextual Bandits~\cite{joseph2016fairness}).
An algorithm $\mathcal{A}$ is considered $\delta$-fair if, with a probability of at least $1-\delta$ over history $h$, for all sequences of contexts $x^1,...,x^t$, all payoff distributions $\mathcal{D}_1^t,...,\mathcal{D}_k^t$, every round $t \in [T]$, and all pairs of arms $j,j' \in [k]$:
\begin{equation}
\pi^t_{j|h} > \pi^t_{j'|h} \text{ only if
} f_j(x_j^t) > f_{j'}(x_{j'}^t),
\end{equation}
where $f_i:x_i \rightarrow [0,1]$ denotes an unknown mapping from the context to the reward for each arm.
\end{defn}

{\bf{Individual Fairness.}}
Individual fairness mandates that similar individuals should be treated similarly~\cite{dwork2012fairness}. In the context of bandits, it means that arms with similar qualities should be selected by the algorithm with similar probability~\cite{liu2017calibrated}. This ensures that no particular arm is consistently preferred over others that have similar expected rewards. The following definition provides a notion of individual fairness in the context of bandits.

\begin{defn} \label{def: smooth-fairness bandits}
"(Smooth Fairness~\cite{liu2017calibrated}). For a divergence function $D$, let $D(\pi_t(i) \mathbin\Vert \pi_t(j))$ denote the divergence between Bernoulli distributions with parameters $\pi_t(i)$ and $\pi_t(j)$, and let $D(r_i \mathbin\Vert r_j)$ denote the divergence between the reward distributions of the $i$-th and $j$-th arms. An algorithm $\mathcal{A}$ is $(\epsilon_1, \epsilon_2, \delta)$-fair with respect to the divergence function $D$ if $\epsilon_1, \epsilon_2 \geq 0$, and $0\leq \delta \leq 1$. With a probability of at least $1-\delta$ in every round $t$, for every pair of arms $i$ and $j$, the following inequality should hold:

\begin{equation}
D(\pi_t(i) \mathbin\Vert \pi_t(j)) \leq \epsilon_1 D(r_i\mathbin\Vert r_j) + \epsilon_2.
\end{equation}
\end{defn}
In other words, if two arms have comparable reward distributions, a fair decision rule should treat them similarly by assigning them similar selection probabilities.

{\bf{Proportional Fairness.}}
Proportional Fairness in RL aims to ensure that each user, or in the case of bandits, each arm, is allocated a minimum guaranteed share of resources or pulls over time \cite{li2019combinatorial, claure2020multi}. By doing so, it guarantees that each arm is played at least a certain proportion of the time, thus ensuring a minimum level of exploration for all arms and preventing any particular arm from being unfairly favored over others. We present the following fairness notion as a way to quantify Proportional Fairness:

% \begin{defn} \label{def: smooth-fairness bandits}
% (Smooth Fairness~\cite{liu2017calibrated}).
% Given a divergence function $D$, let $D(\pi_t(i) \mathbin\Vert \pi_t(j))$ represent the divergence between Bernoulli distributions with parameters $\pi_t(i)$ and $\pi_t(j)$, and let $D(r_i \mathbin\Vert r_j)$ denote the divergence between the reward distributions of the $i$-th and $j$-th arms. An algorithm $\mathcal{A}$ is $(\epsilon_1, \epsilon_2, \delta)$-fair with respect to the divergence function $D$, where $\epsilon_1, \epsilon_2 \geq 0$ and $0\leq \delta \leq 1$, if with a probability of at least $1-\delta$ in each round $t$, for every pair of arms $i$ and $j$:
% \begin{equation}
% D(\pi_t(i) \mathbin\Vert \pi_t(j)) \leq \epsilon_1 D(r_i\mathbin\Vert r_j) + \epsilon_2
% \end{equation}
% \end{defn}
% In simpler terms, if two arms exhibit similar reward distributions, a fair decision rule should treat them comparably by assigning them similar selection probabilities.
\begin{defn} \label{def: asymptotic fairness}
(Asymptotic Fairness~\cite{li2019combinatorial}). 
Let $d(t) = (d_1(t), \ldots, d_N(t))$ be a vector indicating whether each of the $N$ arms is pulled at round $t$, where $d_i(t)=1$ if arm $i$ is played and $d_i(t)=0$ otherwise. Moreover, let $r_i \in (0, 1)$ denote the required minimum fraction of rounds in which arm $i$ is played. Algorithm $\mathcal{A}$ is called asymptotically fair if:
\begin{equation}
\liminf_{T \to \infty} \frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}[d_i(t)] \geq r_i, \forall i \in [N].
\end{equation}
\end{defn}
This means that as the number of rounds $T$ approaches infinity, the expected fraction of rounds in which each arm is played should be equal to or greater than a prespecified fraction that is considered fair.

\subsubsection{Unfairness Mitigation Algorithms}

% \paragraph{Pre-processing Strategies}

% {\bf{Mitigation Strategies.}}
% \textbf{TODO: Some introduction}
% \textbf{TODO: A possible categorization: Reward modification techniques: modifying the rewards that are received by the agent during training to encourage fairness. e.g., penalize the agent for making decisions that are unfair towards certain groups.
% Action constraint techniques: placing constraints on the actions that the agent can take (or modification to the action selection strategy) during training to ensure fairness. e.g., a constraint on the proportion of decisions for a certain group to avoid under-representation.
% Algorithmic fairness techniques: modifying the RL algorithm itself to encourage fairness. E.g., design an algorithm to optimize for a fairness metric in addition to the standard reward objective. Can we also have a category in which the selection strategy is modified?}
% \textbf{TODO: Another possible categorization: Either modification is based on state, reward (or regret), or other constraints.}
% Joseph et al. \cite{joseph2016fairness} suggest using \bfph{FAIRBANDITS} as a way to achieve $\delta$-fairness in the context of stochastic bandits. This involves modifying the standard UCB algorithm by maintaining empirical means and confidence intervals for each arm. If there are overlapping confidence intervals, the authors propose playing the corresponding arms with equal probability rather than selecting the arm with the highest upper confidence bound. The authors further propose a method for achieving fairness in contextual bandit problems that involves a reduction from the KWIK (Knows When It Knows) algorithm to a $\delta$-fair contextual bandit algorithm and vice versa. \textbf{TODO: cat: 2- constraint on the actions}
% Jabbari et al. \cite{jabbari2017fairness} adapt the concept of $\delta$-fairness introduced by \cite{joseph2016fairness} and apply it to Markov Decision Processes (MDPs). Their definition of fairness ensures that no action will be favored over another if it leads to a lower discounted reward in the long run. The authors propose an algorithm called \bfph{Fair-E\textsuperscript{3}} to achieve fairness based on an approximate definition of this concept. \textbf{TODO: cat: 2- constraint on the actions}
% Liu et al. \cite{liu2017calibrated} propose a new fairness constraint based on arms' reward distributions rather than the expected reward. This notion, known as "smooth fairness," mandates that if two arms have a similar reward distribution, then they should have similar probabilities of being selected. Moreover, the authors suggest the concept of "fairness regret" to quantify the degree to which an algorithm is not calibrated. Perfect calibration requires that the probability of selecting an arm be equal to the probability with which that arm has the optimal reward and leads to calibrated fairness. They then address these fairness constraints in the case of Bernoulli and Dueling bandit settings. \textbf{TODO: cat: 2- constraint on the actions}

% One key challenge in achieving the notion of individual fairness proposed by \cite{dwork2012fairness} is how to quantify the similarity between individuals. Although many studies assume that such a metric is already given, Gillen et al. \cite{gillen2018online} suggest learning a similarity metric during the decision-making process in the contextual bandits setting. In particular, the authors assume a scenario that an algorithm has access to an oracle that intuitively understands fairness and can identify fairness violations after each decision but cannot explicitly enunciate a quantitative fairness metric for individuals. \textbf{TODO: cat: 3: changing algorithm itself}

% Several studies focus on achieving a fairness notion that requires guarantees for the minimum number of times each arm is chosen in multi-armed bandit problems. For instance, researchers in \cite{chen2020fair} suggest incorporating constraints into the contextual multi-armed bandit problem to secure a minimum selection rate for each arm. They then propose an algorithm that minimizes regret jointly for multiple different contexts at the same time while satisfying the fairness constraint \textbf{TODO: cat: 3: modifying algo itself}. In \cite{patil2021achieving}, researchers propose a method for addressing fairness in stochastic multi-armed bandit problems. They introduce an extension to the conventional notion of regret, named \bfph{$r$-regret}, which incorporates fairness constraints to ensure that each arm is selected at least a pre-specified fraction of the time at each time step. \textbf{TODO: cat: 1: modifying rewards (or regret)}

% Other researchers have concentrated on achieving fairness metrics at the group level in decision-making carried out by RL systems. Huang et al. \cite{huang2022achieving} approached the personalized recommendation problem by formulating it as a modified contextual bandit problem and establishing a fairness constraint to maintain parity in the expected mean reward of both the protected and unprotected groups. To fulfill this constraint, the authors created an algorithm named \bfph{Fair-LinUCB} that accomplishes group-level fairness while preserving high efficiency. \textbf{TODO: cat: 3- modifying algorithm itself} In another study, Wen et al. \cite{wen2021algorithms} suggest algorithms for fair sequential decision-making in MDPs. The authors first differentiate between the quality of outcome for the decision-maker (such as a bank) and the individuals (such as loan applicants). This differentiation permits enforcing fairness constraints based on the average outcome quality for individuals in different subpopulations to achieve objectives related to demographic parity and equalized opportunity while attempting to maximize the decision-makers gain (such as the bank's profits) simultaneously. \textbf{TODO: cat: 3- modifying algorithm itself}

% Ge et al. \cite{ge2022toward} investigate the challenge of balancing fairness and utility objectives in a Multi-Objective Markov Decision Process (MOMDP) using the Pareto principle. They propose MoFIR, a fairness-aware recommendation framework that employs Multi-Objective Reinforcement Learning (MORL) to learn a single parametric representation of the optimal recommendation policy. MoFIR considers a recommendation objective and a fairness objective concurrently to learn a representation of the optimal recommendation policy within the vast space of possible user preferences. To achieve this, the authors extend the Deep Deterministic Policy Gradient (DDPG) algorithm by introducing a conditioned network (CN) that directly conditions the networks on the decision-makers preferences and outputs Q-value vectors. \textbf{TODO: cat: 3- modifying algorithm itself}
% The authors of a related study on Interactive Recommender Systems introduced a novel RL-based framework named FairRec, aimed at combining accuracy and fairness in the rewards function for making recommendations \cite{liu2020balancing}. FairRec achieves a dynamic balance between accuracy and fairness by incorporating both user preferences and system fairness status into its state representations. This approach enables the framework to improve fairness while preserving recommendation quality over the long term \textbf{TODO: cat 1: modifying the rewards}.

Owing to the inherent characteristics of RL methods, most techniques in the ML category are in-processs. We have classified the mitigation algorithms into three groups: Reward modification, action constraint, and algorithmic adjustments.


{\bf Reward modification.}  Methods in this category involve modifying the notion of regret or rewards that the agent receives during training to encourage fairness. For instance, in \cite{patil2021achieving}, researchers propose an extension to the conventional notion of regret, called \emph{$r$-regret}, which incorporates fairness constraints to ensure that each arm is selected at least a pre-specified fraction of the time at each time step in stochastic multi-armed bandit problems. In a separate study on Interactive Recommender Systems, a novel RL-based framework named FairRec is introduced to combine accuracy and fairness in the rewards function for making recommendations \cite{liu2020balancing}. FairRec dynamically balances accuracy and fairness by incorporating user preferences and system fairness status into its state representations, which helps to improve fairness while preserving recommendation quality over time. 

{\bf Action constraint.} %Techniques in this category involve placing constraints on the actions that an agent can take or modifying the action selection strategy during training to ensure fairness. Joseph et al. \cite{joseph2016fairness} propose a method called FAIRBANDITS to achieve $\delta$-fairness in stochastic bandits. This involves modifying the standard UCB algorithm by maintaining empirical means and confidence intervals for each arm. If the confidence intervals overlap, the authors suggest playing the corresponding arms with equal probability instead of selecting the arm with the highest upper confidence bound. The authors also propose a method to achieve fairness in contextual bandit problems by reducing the KWIK algorithm to a $\delta$-fair contextual bandit algorithm and vice versa. In a separate study, Jabbari et al. \cite{jabbari2017fairness} extend the concept of $\delta$-fairness to Markov Decision Processes (MDPs). Their definition of fairness ensures that no action will be favored over another if it leads to a lower discounted reward in the long run. They propose an algorithm called Fair-E\textsuperscript{3} to achieve fairness based on an approximate definition of this concept. Additionally, Liu et al. \cite{liu2017calibrated} introduce a new fairness constraint, known as "smooth fairness," that is based on the reward distributions of the arms rather than the expected reward. This constraint mandates that if two arms have similar reward distributions, then they should have similar probabilities of being selected. The authors also propose the concept of "fairness regret" to quantify the degree to which an algorithm is not calibrated. They demonstrate how to address these fairness constraints in Bernoulli and Dueling bandit settings. 
This category encompasses techniques that either constrain an agent's actions or adjust the action selection strategy during training to promote fairness. Joseph et al. \cite{joseph2016fairness} introduce \emph{FAIRBANDITS} for achieving $\delta$-fairness in stochastic bandits by modifying the UCB algorithm. When confidence intervals overlap, they recommend playing corresponding arms with equal probability. They also present a method for fairness in contextual bandit problems by converting the KWIK algorithm to a $\delta$-fair contextual bandit algorithm and vice versa. Jabbari et al. \cite{jabbari2017fairness} extend $\delta$-fairness to MDPs, ensuring no action is favored if it results in lower long-term discounted rewards. Their Fair-E\textsuperscript{3} algorithm achieves fairness based on an approximate definition. Liu et al. \cite{liu2017calibrated} propose the notion of \emph{smooth fairness}, a constraint based on the reward distributions as described earlier, and \emph{fairness regret}, to measure calibration deviations. They show how to address these constraints in Bernoulli and Dueling bandit settings.


{\bf Algorithmic Modifications.} This category involves modifying RL algorithms themselves to promote fairness. Some research in this area aims to guarantee a minimum number of times each arm is chosen in multi-armed bandit problems. For example, Chen et al. \cite{chen2020fair} incorporate constraints to ensure a minimum selection rate for each arm in contextual multi-armed bandit problems, suggesting an algorithm that minimizes regret for multiple contexts while maintaining fairness. Other studies target group-level fairness in RL-based decision-making. Huang et al. \cite{huang2022achieving} frame personalized recommendations as a modified contextual bandit problem, introducing a fair algorithm called \emph{Fair-LinUCB} to maintain parity in the expected mean reward of both the protected and unprotected groups. Wen et al. \cite{wen2021algorithms} propose fair sequential decision-making algorithms in MDPs that enforce fairness constraints based on average outcome quality for different subpopulations, aiming for demographic parity and equalized opportunity.

A key challenge in individual fairness is quantifying individual similarity. While many studies assume such a metric, Gillen et al. \cite{gillen2018online} propose learning a similarity metric during decision-making in contextual bandits setting, relying on an oracle that can identify fairness violations without explicitly providing a quantitative metric. 
% Ge et al. \cite{ge2022toward} address balancing fairness and utility in Multi-Objective Markov Decision Processes (MOMDPs) using the Pareto principle. They introduce MoFIR, a fairness-aware recommendation framework that uses Multi-Objective Reinforcement Learning (MORL) to learn an optimal recommendation policy representation. MoFIR concurrently considers recommendation and fairness objectives, extending the Deep Deterministic Policy Gradient (DDPG) algorithm with a conditioned network (CN) that conditions the networks on decision-maker's preferences and outputs Q-value vectors.
Ge et al.~\cite{ge2022toward} introduce \emph{MoFIR}, a framework that balances fairness and utility in recommendation systems. The authors use Multi-Objective Reinforcement Learning to learn an optimal recommendation policy. MoFIR extends the Deep Deterministic Policy Gradient algorithm by incorporating a conditioned network that considers decision-maker preferences and outputs Q-value vectors. This approach aims to address fairness concerns while maximizing the effectiveness of recommendations.

% \paragraph{Post-processing Strategies}


%\bibliographystyle{abbrv}
%\bibliographystyle{ACM-Reference-Format}
% \bibliography{Ref_main}