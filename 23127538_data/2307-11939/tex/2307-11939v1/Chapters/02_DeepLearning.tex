\chapter{Deep Learning}
\label{chapter:Deeplearning}

In this chapter, we would like to talk about Deep Learning neural network components as well as how to train these network in order to do specific tasks such as Image Classification, 
\section{Deep Learning}
Deep learning is a subset of machine learning that involves the use of neural networks to learn from data. A neural network is a mathematical model that is designed to simulate the behavior of neurons in the brain. It consists of layers of interconnected nodes, each of which performs a simple mathematical operation on its inputs.

In a deep neural network, there are multiple layers of nodes, allowing the network to learn increasingly complex representations of the data. Each layer in the network transforms the input it receives from the previous layer, eventually producing an output that represents the network's prediction or classification of the input.

Deep learning neural networks have been used to achieve state-of-the-art performance on a wide range of tasks, including image and speech recognition, natural language processing, and game playing. They have also been applied to fields such as finance, healthcare, and transportation, where they are used to identify patterns and make predictions based on large amounts of data.

However, deep learning neural networks require a large amount of data and a good learning method to optimize the neural network's parameters. In this chapter, we will cover a various of learning method for a deep learning model as well as model's architecture for image classification task.

To sum it up, to train a deep learning model, we need to define the model's architecture, optimizer, loss function on a training dataset we want the model to learn about it.
\subsection{Model Architecture}
Since this work only focus on image classification task, we will only talk about Fully Connected Layer, Convolutional Layer, Batch Normalization which we used our differential private training on.
\subsubsection{Neuron (perceptron)}
The neural networks is built based on a set of dependent neurons (or perceptions). In other words, a neuron, is a basic unit in a deep learning neural networks. Mathematically, a neuron is a non-linear function which consists of 4 main parameters: Input values $x$, weights and Bias $w and b$, net sum function and activation function $f$. We have the output of a neuron as:
\[y = f(\sum_{1=1}^{n} w_i x_i + b)\] assuming $x$ and $w$ have $n$ elements/rows

There are many types of Activation functions:
\begin{itemize}
    \item Sigmoid function: $f(x) = \frac{1}{1+e^{-x}}$
    \item Relu: $f(x) = \left\{\begin{array}{lr}
        x, & \textit{for } x \geq 0\\
        0, & \textit{for } x < 0
        \end{array}\right. $
    \item Tanh: $f(x) = \frac{e^x â€“ e^{-x}}{e^x + e^{-x}} $
    \item Sign function: $f(x) = \left\{\begin{array}{lr}
        -1, & \textit{for } x < 0\\
        0, & \textit{for } x = 0 \\
        1, & \textit{for } x \geq 0
        \end{array}\right. $
\end{itemize}

These activation functions is chosen depended on different problems and the desired outputs. Moreover, they may also cause the slow learning process or vanishing gradients or exploding gradients which are very bad effects during the backpropagation process in the training process \cite{ExplodingGradpaper}. 

\subsubsection{Fully Connected Layer}
In fully connected layers, the neurons first perform a linear computation on the input vector though the weights matrix. After that, a non-linear transformation is applied through the activation function $f$. Note that the input of the fully connected layer is flattened into a single vector in order to perform the mentioned computation. Therefore, in fully connected neural networks, all neurons is connected to all neurons in the next layers. On the other hand, this layer also is used as the last layer in the networks and outputs the classification decision which called "softmax" layer:
\[softmax(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_i}}\]
where $n$ is the number of classes.

\subsubsection{Convolutional Layer}
A convolutional layer performs a convolution operation of the inputs using filters and kernels. This operation is simply sliding each filter over the input data and computing the dot product between the filter and the input data in the kernel. This results in a single scalar value which is a new value of the chosen input data.

Since the filter can only slide over the input data until it reaches the edge, this makes the output of a convolutional layer has smaller dimension than the input data. Moreover, the border pixels are also used less frequently than the center pixels which may cause some information loss while training on the input data. To overcome this problem, the input data is often padded with zeros as the border in order to maintain the size of the feature map and use more information from pixels at the edges. Usually, to do this, the padding size is usually equal to kernel size minus 2.

Similarly to fully connected layer, after the convolution operation, the convolutional layer also applies a non-linear activation function such as ReLu to the output feature maps. This enables the non-linearlity to the network and allows it to learn nmore complex features from the input data.

However, because of the convolution operation, the output of features map can be diverges and makes the model learn slower and may not have good performance. Therefore, in some very deep network such as Resnet-18,Resnet-34,... \cite{resnet18paper}. The batch normalization layer is often used in order to normalize all the output features and thus accelerate the training process of the convolutional neural network. \cite{ioffe2015batch}. 

\subsubsection{Batch Normalization}
In this layer, the input data to this layer is normalized as follows:
\[y = \frac{x-E[x]}{\sqrt{Var[x]+\epsilon}} \times \gamma + \beta\]

The mean and the standard-deviation are calculated per-dimension over the minibatches and $\gamma$, $\beta$ are learnable parameter. By default, $\gamma = 1, \beta = 0$ and will be updated during the training process.

However, Opacus \cite{Opacus}, a popular Differentially private Deep Learning training framework, could not apply this layer because it violates the differential privacy if we do not put a privacy mechaism for this layer. This is because this layer makes each sample's normalized value depend on its peers in a batch which yeild a vioation in DPSGD training algorithm. We will discuss about this problem in chapter \ref{chapter:DPSGD}
\subsection{Optimizer}
Given a loss function, the goal of the neural network training process is minimizing/maximizing the loss function. Therefore, optimizers are used to adjust the model's parameters in order to minimizing/maximizing the loss function. The loss function can be strongly convex, convex, concave,... etc. Depends on type of loss function, there are many optimizers are introduced in order to reach the closest point to the global solution. However, because this works only focus on differentially private stocastic gradient descend algorithm. We will only introduce about gradient descent, Stocastic gradient descend method and minibatch stocastic gradient descent.

Asuming we have the model $w$ and we want to minimize the loss function $L$ with the dataset $D = (x,y)$:
\[ min_{w}\ L(w(x),y)\]
\subsubsection{Gradient Descent}
Gradient descent is one of the most popular method to minimize the loss function of a model. It is a method for finding the optimal values of the parameters in a model by iteratively adjusting them in the direction of the steepest descent of the loss function.

The idea behind gradient descent is to start with an initial set of parameter values, and then repeatedly update them in the direction of the negative gradient of the loss function with respect to the parameters. This means that the parameters are adjusted in the direction that reduces the loss function the most.

The gradient descent algorithm works by computing the gradient of the loss function with respect to each parameter in the model, and then adjusting each parameter by subtracting a small fraction of the gradient (known as the learning rate). This process is repeated for a fixed number of iterations or until the change in the loss function falls below a certain threshold. Let the learning rate is $\eta$, we have the gradient descent update as follow:

\[w \leftarrow w - \eta \nabla_{w} L(w,x)\]

Here, the learning rate $\eta$ plays an important role in minimizing the loss function. If the learning rate is too big, the update will be diverged, i.e. updating $w$ will increase the loss instead of minimize it. On the other hand, if the learning rate is too small, then there will be very small update on $w$, thus leads to slow convergence in the training process and will cost a lot of updates on $w$ to have a good result. Therefore, we use diminising learning rate in our algorithm where we start with a learning rate $\eta_0$ big enough, then decrease it after a certain update iterations. This method not only preventing the diverging problem but also give us better results because we can reach a closer point to the optimal solution.

However, if the loss function is non-convex, we usually reach the local minima since the gradient is diminishing when reaching the extreme points of the function. This depends on the seed which is used to initiallize the weights of the deep learning model. Therefore, many methods have been proposed to find a better results by overcome the loss function's valleys such as momentum, choosing seeds based on loss function analyzing tasks, etc. 

There are different variations of gradient descent, such as stochastic gradient descent (SGD) and mini-batch gradient descent, which involve updating the parameters based on a subset of the training data instead of the entire dataset. These variations can be more efficient than batch gradient descent, which updates the parameters based on the entire dataset.

\subsubsection{Stochastic Gradient descent}

Stochastic gradient descent (SGD) is a variation of the gradient descent algorithm used in machine learning and deep learning. It is an optimization algorithm that updates the parameters of a model by computing the gradient of the loss function with respect to a randomly selected subset of the training data, rather than using the entire dataset.

In stochastic gradient descent, the model parameters are updated after each iteration, based on the gradient of the loss function with respect to a randomly selected sample of the training data. This means that the gradient is computed on a smaller subset of the data, which can lead to faster convergence and better generalization performance than batch gradient descent. In detail, stochastic gradient descent have the following steps:
\begin{enumerate}
    \item Randomly sample a example $x_i$ from dataset $D$
    \item Compute the loss: $L(w(x_i),y_i)$
    \item Perform gradient descent step $w \leftarrow w \nabla L(w(x_i),y_i)$
    \item Repeat step 1 to 3 till the end of an epoch.
\end{enumerate}

The random sampling of the training data also helps to reduce the risk of the algorithm getting stuck in a local minimum, as it is more likely to escape from a poor local minimum and converge to a better global minimum.

However, the randomness in stochastic gradient descent can introduce some noise into the parameter updates, which can lead to fluctuations in the optimization process. To address this issue, techniques such as learning rate schedules, momentum, and adaptive learning rates are often used in conjunction with stochastic gradient descent to improve its stability and convergence performance.

\subsubsection{Minibatch Stocastic Gradient descent}

In Minibattch Stocastic Gradient Descent, instead of feeding one sample at a time like SGD, we sample a minibatch randomly and execute stocastic gradient descent method on this minibatch. This helps us to speed up the training process compared to SGD and also allows us to train on large dataset.

\begin{enumerate}
    \item Randomly sample a minibatch $B$ from dataset $D$
    \item Compute the losses: $L(w(x_i),y_i)$ for $(x_i,y_1) \in B$
    \item Perform gradient descent step $w \leftarrow w \frac{1}{|B|}(\sum_{i=1}^{|B|}\nabla L(w(x_i),y_i))$
    \item Repeat step 1 to 3 till the end of an epoch.
\end{enumerate}