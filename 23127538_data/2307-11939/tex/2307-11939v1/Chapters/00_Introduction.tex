\section{Introduction}
\label{section:Introduction}

%TODO: Intro for ML and DP

% Add the citations here
% Machine Learning and Deep Learning have had a huge impact on both research and the technology industry~\cite{GoodBengCour16}. Many machine learning and deep learning models have been developed and improved for problems such as regression, classification, and recognition, with higher accuracy rates~\cite{resnet18paper}. However, these models are vulnerable to many attacks, such as membership-inference attacks~\cite{shokri2017membership}, model inversion [CITE], model poisoning[CITE], and training data poisoning[CITE], and deep-leakage attacks~\cite{ligengzhu201deepleakage}. These attacks aim to gain access to the training databases used to train the deep learning models, replicate those models, or even weaken those models during the training phase.

% In this work, we focus on protecting the privacy of the training database of deep learning models, preventing individual data in the training database from being leaked in the training phase by using Differential Privacy (DP) mechanisms. 


% Differential Privacy (DP)\citep{dwork2006calibrating, dwork2011firm,dwork2014algorithmic,dwork2006our} is a privacy-preserving data analysis method that helps us measure privacy attributes and provide privacy-preserving mechanisms that we can combine with machine learning training algorithms~\citep{mcmahan,nguyen2018sgd,nguyen2018new}. Differential privacy ensures that adding or removing a single database item does not affect the outcome of any analysis. 

Differential Private Stochastic Gradient Descent (DPSGD)~\cite{abadi2016deep} combines Stochastic Gradient Decent (SGD)~\cite{RM1951}
%,nguyen2018sgd,nguyen2018new} 
and Differential Privacy (DP)\citep{dwork2006our}
%\citep{dwork2006calibrating, dwork2011firm,dwork2014algorithmic,dwork2006our} 
to train deep neural networks privately. It has been widely studied since its introduction.
%in 2016. 

In each round of DPSGD a mini-batch of $m$ samples $\xi_{i_1},\dotsc,\xi_{i_m}$ from a larger data set $d=\{\xi_i\}_{i=1}^N$ of $N$ samples is randomly subsampled:
\begin{equation}
    \{i_1,\ldots, i_m\} \leftarrow {\tt Sample}_m(N). \label{eq:sample}
\end{equation}
The global model $w$ is updated by first computing gradients
$\nabla_w f(w;\xi_{i_j})$  for each sample $\xi_{i_j}$ given loss function $f$. Each gradient is clipped by using a clipping operation $[x]_C= x/\max\{1,\|x\|/C\}$ where $C$ denotes a fixed clipping constant. The clipped gradients are aggregated in a sum after which a noise vector $n$ drawn
from a Gaussian distribution\footnote{$\mathbf{I}$ is the identity correlation matrix of the multivariate Gaussian distribution. The factor $2$ is due to sampling exactly $m$ data points every round; by using a Poisson process to  probabilistically sample $\xi \in d$ such that in expectation the mini-batch has size $m$ removes the factor 2 and leads to better DP. This is implemented in \citep{Opacus}. In order to simplify our exposition we work with deterministic sampling and keep the factor 2.} $\mathcal{N}(0,(2C\sigma\mathbf{I})^2)$ is added. The resulting "noised and clipped" update $U$ 
is transmitted to a central server where it is averaged over the size $m$ of the used mini-batch and multiplied by a step-size $\mu$ before subtracting it from the global model:
% \begin{equation}
% \label{algo:DPSGDx}
%     w := w - \eta (\frac{1}{m} \sum_{i=1}^m [\nabla f_w(w;\xi_i)]_C + n), 
% \end{equation}
%
%\textcolor{red}{[TO DO: I think we need the formula below and say that $n$ is a vector with entries drawn from ... But we need a different symbol since we already use $n$ in the abstract.]}
%
%\textcolor{red}{Ha to Marten} Can we say that we abuse the notion $n$, i.e., $n$ can be a number or vector depending on the context we are discussing? It may make the reading smoothly. About the formula, we should use your proposed formula because I did not correctly remember the formula of the weight updating. 
%
\begin{eqnarray}
\label{algo:DPSGDx}
U &:=& n+\sum_{j=1}^m [\nabla_w f(w;\xi_{i_j})]_C 
%% Do not change to f_w !!!!!!!
\ \ \ \ \mbox{ and } \ \ \ \
    w := w -  \frac{\eta}{m} U.
\end{eqnarray}
%where $\eta$ is a step-size, $\nabla f_w(w;\xi_i)$ is the gradient of sample $\xi_i$ given loss function $f$, $n$ is a noise drawn from a Gaussian distribution\footnote{In the abstract we use $n$ for Gaussian noise which is drawn from $\mathcal{N}(0,(C\sigma\mathbf{I})^2)$ for each clipped  } $\mathcal{N}(0,(C\sigma\mathbf{I})^2)$, and where $[x]_C= x/\max\{1,\|x\|/C\}$ is a clipping operation with value $C$. 
In DPSGD, 
$\sigma$ translates to a DP guarantee and is chosen carefully by the designer to balance DP and the accuracy of the final global model.

% \textcolor{red}{HA To Marten: We have found that our proposed adaptive is not completely new. People already knows how to use published dataset U and layer gradient norms for deriving layerwise clipping constants $C_i$. Compared to the existing works, we combine all three following ideas together: decaying master constant C, published dataset U and layer gradient norms for deriving layerwise clipping constants $C_i$. The derivation of $C_i$ from the master $C$ and layer gradient norms is new. More importantly, it seems there are no papers providing the rigorous DP proof. Anyway, the new adaptive clipping method seems to be a minor contribution. The major contribution now is batch clipping because batch clipping allows us to any deep neural networks and existing training method without any modifications due to the restriction of individual clipping mode. However, we have the following problem. We still want to write that new clipping method is important because we cannot significantly change the title and abstract. The thing we want to add to title and abstract is we have to mention the batch clipping there. By combining batch clipping and adaptive clipping method, we can train deep neural network with high accuracy very fast with low privacy budget}


% In each round, SGD computes gradients of a mini-batch of samples first and then clips each gradient with a clipping constant $C$ and its norm. Finally, for each entry in the sum of clipped gradients is added. The parameter $\sigma$ is privacy guarantee chosen by the designer. 

%==== TO DO - edit text below ====



The global model is represented by a weight vector $w$ which can be written as a concatenation $(w_1 | \ldots | w_L)$ where $w_h$ corresponds to the $h$-th layer in the neural network which is being trained. DPSGD uses a fixed clipping constant $C$ for clipping  gradients 
$$
\nabla_w f(w;\xi_{i_j})
= (\nabla_{w_1} f(w;\xi_{i_j}) | \ldots | \nabla_{w_L} f(w;\xi_{i_j}) ).
$$
In expectation the norms $\|\nabla_{w_h} f(w;\xi_{i_j}) \|$ of different layers $h$  in a deep neural network model vary layer by layer. This makes different layers have different sensitivity to the added Gaussian noise. For this reason, \citep{van2018three,Xu2021,Zhang2018CoRR} proposed\footnote{For completeness, per-layer clipping was originally introduced in DP-FEDAVG \citep{mcmahan2018learning} where a clipping budget is {\em evenly} (not customized) distributed  among all layers and a DP guarantee is proven based on the moment accountant from \citep{abadi2016deep}.} Adaptive Layerwise Clipping (ALC) which tunes the added Gaussian noise with respect to the (sensitivity of the) layer. Each layer of the model has its own customized clipping constant based on estimating the expectation of the gradient norms of each layer by sampling  a given small public dataset $\mathcal{D}_{pub}$. 
%Unfortunately, there are 
We notice that  \citep{van2018three,Xu2021,Zhang2018CoRR} have not provided rigorous DP proofs 
%provided 
for ALC. In this paper we prove DP guarantees for ALC and we enhance the ALC method of \citep{Zhang2018CoRR} for improved test accuracy.



% If $\mathcal{D}$ is private, i.e., it is a small subset of the training dataset $d$, then the layer clipping constants must be computed privately; Gaussian noise is drawn and added to the layer clipping constants\footnote{\cite{van2018three}  computes layer
% clipping constants $C_i$  based on the layer gradient norms from the previous round (${\cal D}$ is the mini-batch used in the previous round).
% DPSGD-F \citep{Xu2021} calculates the layerwise clipping constants $C_i$ proportional to the ratio of gradients exceeding a threshold.}. The main shortcoming of this approach is that these noised layer clipping constants can become large because the sampled noise can be large. This hurts the convergence of the training process (see the experiments in~\citep{van2018three} for more details). 
% We also notice that \citep{van2018three,Xu2021} do not have rigorous DP proofs. We remedy this situation by using a public $D$ which leads to high accuracy and by providing a DP analysis for LC.

%\textbf{Motivation:} 
Besides the enhanced ACL, we introduce Batch Clipping (BC) where 
(\ref{algo:DPSGDx}) is replaced by
\begin{eqnarray}
\label{algo:DPSGDxBC}
U &:=& n+\left[\frac{1}{m} \sum_{j=1}^m  \nabla_w f(w;\xi_{i_j}) \right]_C 
%% Do not change to f_w !!!!!!!
\ \ \ \ \mbox{ and } \ \ \ \
    w := w -  \eta U.
\end{eqnarray}
BC allows us to first compute an average of a batch of gradients before clipping, as opposed to (\ref{algo:DPSGDx}) which averages a sum of clipped individual gradients; for this reason we call (\ref{algo:DPSGDx}) the Individual Clipping (IC) approach.
BC gives us the ability to properly train Batch Normalization Layers (BNL) \citep{ioffe2015batch} in a neural network; 
%BNL is crucial for high accuracy needed 
in  very deep neural networks the use of  BNLs is crucial for achieving high accuracy. 
The BC and ALC techniques are complimentary and can be implemented in parallel, also BC and ALC do not require any changes to be made in deep neural networks, i.e., BC and ALC apply directly without modifications to the neural network that we wish to train. This makes our training framework with BC and ALC flexible.
BC allows us to use resnet-18\citep{resnet18paper} (which uses BNLs) in our experiments and have a DP guarantee. 
%while  still maintaining the high accuracy of standard SGD without DP. 
DPSGD without BC cannot properly train BNLs, therefore, DPSGD with IC (even with ALC) for resnet-18 
%without batch normalization  
does not converge and leads to poor accuracy, while DPSGD with BC and the enhanced ALC does converge. In the context of DPSGD we are the first to introduce BC together with proving its DP guarantee; no prior related work on DPSGD with BC exists.

%Besides BC we analyse a complimentary technique for achieving high accuracy.

%== merge with above ===

%Per-layer clipping is introduced in DP-FEDAVG \citep{mcmahan2018learning} where the clipping budget is distribute equally among all model layers and proves privacy guarantee based on moment accountant. \cite{van2018three} proposed the adaptive clipping method where each layer in the neural network has its own adaptive clipping constant $C_i$. The layer clipping constants $C_i$ are computed privately before each new round, i.e., these 

%== write about BC ==



Our main contribution are:
\begin{itemize}
    \item  We propose an enhanced Adaptive Layerwise Clipping method based on~\cite{Zhang2018CoRR}. Our experiments show that DPSGD with our enhanced ALC  converges faster to 
 a higher accuracy. 
    
    % allows us to use a significantly large amount of Gaussian noise without any compromise of testing accuracy. 

    \item  We prove and characterize the DP guarantee for ALC
    %Layerwise Clipping 
    by using the $f$-DP framework~\cite{dong2019gaussian}. We explain how layerwise clipping degrades DP and we show how to set the noise parameters of DPSGD without ALC and DPSGD with ALC so that they have the same DP guarantee and their test accuracy can be fairly compared.
    
    %provide a rigorous privacy guarantee proof for all existing type of adaptive layer clipping methods based on Gaussian DP notion or $f$-DP~\cite{dong2019gaussian}.

    \item  
    We introduce and propose to use Batch Clipping during training.
    %We develop one new training mode named as batch clipping mode. Basically, this batch clipping mode 
    We also define General Batch Clipping (GBC) of which BC and IC are special cases and notice that GBC  is compatible with  \textit{any first order optimizers} in mini-batch mode.
BC allows us to implement Batch Normalization Layers which are crucial for attaining high accuracy for deep neural networks (BNLs cannot be trained properly by the original DPSGD which uses Individual Clipping). 
    
    %This type of work is never done before and significantly important. As explained in~\cite{ioffe2015batch}, Batch Normalization Layer working with a mini-batch dataset is very important to guarantee a high testing accuracy for deep neural networks. The current DPSGD works with individual clipping mode and Batch Normalization Layer is prohibited. Moreover, 
    %%%%%%% ??????? %%%%%
    %the individual clipping mode makes DPSGD only work with mini-batch type first order optimizers. We develop a rigorous DP guarantee for DPSGD with batch clipping mode. 

   \item  
   %\textcolor{red}{[TO DO -- this is commonly already understood -- We notice that it has been commonly understood that ... etc. This argument also applies to our BC and GBC approach.]} We prove and characterize the DP guarantee for BC and, more generally, GBC by using the $f$-DP framework~\cite{dong2019gaussian}. 
    For proving differential privacy guarantees it is commonly understood that the privacy argument does not depend on how the gradients in the clipped values in (\ref{algo:DPSGDx}) are computed; the to-be-clipped values may as well be computed as in (\ref{algo:DPSGDxBC}).
   % In particular 
    This shows that (the original) DPSGD with IC given by (\ref{algo:DPSGDx}) and (the new) DPSGD with BC given by (\ref{algo:DPSGDxBC}) offer the exact same DP guarantee (also if both implement ALC).

    \item Our experiments show that DPSGD modified by using  our enhanced ACL and using BC allows us to train the deep neural network resnet-$18$~\cite{resnet18paper} (which uses BNL) on CIFAR10~\cite{CIFAR10dataset} while DPSGD with ALC and IC does not converge. This shows that BC outperforms IC  in practice. 
    
    
 %    and achieve the same 
 %    %prediction accuracy on testing dataset 
 %    test accuracy
 %    as a standard training method that runs classical SGD without DP (i.e., without clipping and without noise) in the same training framework. 
 %    %When the number of epochs are of 20 and 50 
 %    For 20 and 50 epochs, respectively, with
 % $\sigma=8, 10, 12, \dots, 62$, the test accuracy of our DPSGD vs standard no-DP SGD are $86\%$ vs $88\%$ and $90\%$ vs $90\%$, respectively. This result has not yet been achieved for the original DPSGD. 
    
\end{itemize}
Our main conclusion is that ACL and BC are two techniques that provide a better balance between DP and accuracy. However, our experiments are for small $\sigma$ which corresponds to weak differential privacy. We still need additional techniques beyond ALC and BC for training a deep neural network like resnet-18 with CIFAR10  in order to achieve a practical balance between test accuracy and DP guarantee.

\textbf{Outline:}
We first provide the necessary background on $f$-DP in Section \ref{sec:background}. Section \ref{sec:modifiedDPSGD} introduces BC (and GBC), layerwise clipping and our ACL, and proves DP guarantees in the $f$-DP framework.
Experiments are in Section \ref{sec:experiments}. Section~\ref{chapter:Conclusion} concludes our paper. 
%The organization of this paper is as follows. 
%We first introduce basic concept of DP, $f$-DP, DPSGD and how the norms of layers vary layer by layer in deep neural network and in this case is resnet-18 in Section~\ref{sec:background}. After that we discuss about the our proposed clipping method in details in Section~\ref{section:layerwiseclipping} with the privacy guarantee argument. We demonstrate our experiments in Section~\ref{section:Experiments} and conclude our paper in Section~\ref{sec:conclusion}. 

% In this paper, We explain the basic concepts of Privacy, Differential privacy, Deep Learning in chapters \ref{chapter:Differentialprivacy}, \ref{chapter:Deeplearning}, show how differential privacy stochastic gradient descent \cite{Abadi_2016} \cite{vandijk2023generalizing} works in chapter \ref{chapter:DPSGD} and then introduce our method in chapter \ref{section:layerwiseclipping} and finally show our experiments in section \ref{section:Experiments}.







% In this paper, we use a lot of parameters which are defined in table \ref{tbl:notations}
% \begin{table}[H]
% \centering
% \label{tbl:notations}
% \begin{tabular}{|l|l|}
% \hline
%  Parameters & Description    \\ \hline
%  $N$ & Training database size    \\ \hline
%  $K$ & Number of training iterations    \\ \hline
%  $D$ & database \\ \hline
%  $\eta$ & Learning rate \\ \hline
%  $C$ & Gradient Clipping Constant \\ \hline
%  $\sigma$ & Gaussian noise variance scaling \\ \hline
 
% \end{tabular}
% \caption{List of notations}
% \end{table}


% \section{Gaussian Differential Privacy}
% \section{Batch Clipping}