\chapter{Differentially Private Stocastic Gradient Descent}
\label{chapter:DPSGD}

\section{DPSGD}
Abadi \cite{Abadi_2016} has proposed a differentially private training method for minibatch stochatic gradient descent. There is two main steps adding in the training process which are clipping step and add noise step which is shown in the algorithm \ref{alg:DPSGD}.

According to algorithm \ref{alg:DPSGD}, although we sampled mini-batches but we still have to compute per sample's gradients in order to perform clipping step in line 8. This computation is expensive and we can not apply parallelism training of the Deep Learning training frameworks such as Tensorflow or Pytorch. 
\begin{algorithm}[H]
\caption{Differential Private Stochastic Gradient Descent}
\label{alg:DPSGD}
\begin{algorithmic}[1]
 \Procedure{DPSGD}{$d$}
%\Procedure{LocalSGDwithDP}{$d,C,s,\sigma,T$}
\State {\bf Input:} database $D = \{x_1,x_2, \dots, x_n\}$, loss function $\ell(\theta) = \frac{1}{N} \sum_{i} \ell(\theta,x_i)$, learning rate $\eta_t$, noise scale $\sigma$, group size L, gradient norm bound C.
\State {\bf Initialize:} $\theta_0$ randomly.
 
\For {$t \in T$}
    \State Take a random sample $L_t$ with sampling probability $q = L/N$
    \For{$i \in L_t$}
        \State $g_t(x_i) \leftarrow \nabla_{\theta_t} \ell(\theta,x_i)$ \Comment{Compute gradient}
        \State $\bar{g_t}(x_i) \leftarrow g_t(x_i)/ max(1, \frac{||g_t(x_i)||_2}{C})$ \Comment{Clipping Norm}
    \EndFor
    \State $\tilde{g_t} \leftarrow \frac{1}{L} (\sum_{i} \bar{g}_t(x_i) + \mathcal{N}(0,\sigma^2 C^2 I))$  \Comment{Add noise}
    \State $\theta_{t+1} = \theta_{t} - \eta_t \tilde{g_t} $
\EndFor
\State {\bf Output:} $\theta_K$ and compute the overall privacy cost $(\epsilon,\delta)$ using Moment Accountant method.
   
\EndProcedure
\end{algorithmic}
\end{algorithm}



% TODO: write about fast clipping papers?

\subsubsection{Moment Accountant}
The algorithm \ref{alg:DPSGD} is proven to be $(\epsilon,\delta)-DP$ of we choose $\sigma = \Omega(\sqrt{T \log(1/\delta) \log(T/\delta)} /\epsilon)$ where T is the number of iterations. This implies we have to fix 3 in 4 parameters: $T,\sigma , \epsilon, \delta$ before training and we will have limited training iterations.  
Therefore, Abadi \cite{Abadi_2016} also provides Moment Accountant method in order to compute the privacy budget $(\epsilon,\delta)$ after each epoch which means we will fix the number of iterations, $\sigma$ value and one privacy parameter $\epsilon$ or $\delta$ before training. This shows us a limited choice of parameters and it is very hard to find a good settings which helps our neural network have good testing accuracy.

\subsection{f-DP Background}
However, the $(\epsilon,\delta)$-DP definition does not provide a clear measure of a model's privacy level, making it challenging to choose an appropriate privacy budget for DPSGD against strong attacks like Membership Inference Attack\cite{shokri2017membership} and Deep Leakage attacks \cite{zhu2019deep}. Furthermore, in this definition, probabilities rely solely on the randomness of the mechanism ${\cal M}$. Therefore, the level of noise added to the mechanism ${\cal M}$ must be large enough to conceal all characteristics of individual data in dataset $D$. If ${\cal M}$ is a model focused on improving accuracy in specific tasks like image classification, excessive noise can significantly reduce the model's accuracy.

In contrast, Dong et al. \cite{dong2019gaussian} proposed the state-of-the-art Differential Privacy notation, $f$-Differential Privacy, based on hypothesis testing, which is a key idea behind statistical attacks. This framework surpasses all other existing frameworks based on $(\epsilon,\delta)$-DP. Specifically, in hypothesis testing, an adversary attempts to differentiate between two adjacent datasets $D$ and $D'$ based on the output of a differentially private mechanism ${\cal M}$.:

\[H_0: \mbox{The underlying dataset is}\ D\ \mbox{and}\ H_1: \mbox{The underlying dataset is}\ D'\]

Similarly to the $(\epsilon, \delta)$-DP definition, the neighboring datasets are those which differ by only one data row: $|D \cup D'| = 1$. Here, we define Type I and Type II errors as:

\[\alpha_\phi = \mathbb{E}_{o\sim {\cal M}(D)}[\phi(o)]  \mbox{ and } \beta_\phi = 1- \mathbb{E}_{o\sim {\cal M}(D')}[\phi(o)] \]

where $\phi(o) \in [0,1]$ is the rejection rule that takes the output $o$ of the DP mechanism ${\cal M}$ as input. Here, we toss a coin and reject the null hypothesis with probability $\phi$. The optimal trade-off function between Type I and Type II errors is defined as:

\[T({\cal M}(D),{\cal M}(D'))(\alpha) = \inf_\phi \left \{ \beta_\phi : \alpha_\phi \leq \alpha \right\}\]
for $\alpha \in [0,1]$ and the infimum is taken over all measureable rejection rules $\phi$.

If the statistical attack succeeds, it means that the two hypotheses are fully indistinguishable. This leads to the trade-off function $1-\alpha$. A function $f: [0,1] \rightarrow [0,1]$ is a trade-off function if and only if it is convex, continuous, non-increasing, and $f(x) \leq 1-x$ for $x \in [0,1]$. Then, we have the definition of $f$-differential privacy as follows:
\begin{definition}
\label{def:f-DP}
f-differential privacy: Let $f$ be a trade-off function. A mechanism $\cal{M}$ is f-differentially private if
\[T({\cal M}(D), {\cal M}(D')) \geq f\]
for all neighboring datasets $D$ and $D'$
\end{definition}

Moreover, Dong et al. \cite{dong2019gaussian} also defines the Gaussian DP as a special case of $f-DP$ where $f$ is a trade-off function:

\[G_\mu(\alpha) = T({\cal N}(0,1),{\cal N}(\mu,1))(\alpha) = \Phi( \Phi^{-1}(1-\alpha) - \mu) \]
where $\Phi$ is the standard normal cumulative distribution of ${\cal N}(0,1)$. 


Here we have the GDP definition:
\begin{definition}
    A mechanism $\cal M$ is $\mu$- Gaussian Differential Privacy ($\mu$-GDP) if it is $G_{\mu}$-DP:
    \[T({\cal M}(D), {\cal M}(D') \geq G_\mu\]
    for all neighboring dataset $D$ and $D'$
    
\end{definition}

Using the GDP definition, we can easily determine which value of $\mu$ guarantees a reasonable amount of privacy, as shown in Figure 3 (\cite{dong2019gaussian}). For example, $\mu=0.5$ and $\mu=1$ are reasonably private values, while $\mu=3$ or $\mu=6$ are basically non-private because an adversary can control Type I and Type II errors at very small values such as $0.07$ and $0.001$, respectively.

Similar to Abadi et al. \cite{Abadi_2016}, Dong et al. \cite{dong2019gaussian} also prove in Theorem 2.7 that the Gaussian mechanism operating on a statistic $\theta$ as ${\cal M}(D) = \theta(D) + \xi$, where $\xi \sim {\cal N}(0,sens(\theta)^2/\mu^2)$, is $\mu$-GDP. Here, the sensitivity of $\theta$ is defined as:

\[ sens(\theta) = \sup_{D,D'}|\theta(D) - \theta(D')|\]

where the supremum is taken over all neighboring datasets.
Therefore, GDP offers the tightest privacy bound for the Gaussian mechanism.

However, in the case of deep learning where we add Gaussian noise to each layer, we need to compute the composition of the GDPs of all $n$ layers: $\mu = g(\mu_1,\mu_2,\dots,\mu_n)$. Here, let $y_i \leftarrow {\cal M}i(aux,D)$ with $aux = (y_1,...,y{i-1})$, Theorem 3.2 in \cite{dong2019gaussian} shows that if the mechanism ${\cal M}(aux,D)$ is $f_i$-DP for all $aux$, then the $n$-fold composed mechanism $\cal M$, which applies ${\cal M}i$ in the order from $i=1$ to $i=n$, is $(f_1 \otimes f_2 \otimes \dots \otimes f_n)$-DP because the tensor product is commutative. In the special case of the composition of Gaussian operators $G{\mu_i}$, Corollary 3.3 in \cite{dong2019gaussian} states that the $n$-fold composition of $\mu_i$-GDP mechanisms is $\sqrt{\mu_1^2 + \dots + \mu_n^2}$-GDP.

Moreover, if datasets $D$ and $D'$ differ by more than one sample, say $g$ samples where $g>1$, Dong et al. \cite{dong2019gaussian} also show that if a mechanism is $G_{\mu}$-DP for $g=1$, then it is $G_{g\mu}$-DP for groups of size $g$. This is a very nice result because it shows the linear dependency in $g$, and it is a backbone theory for our batch clipping method.

