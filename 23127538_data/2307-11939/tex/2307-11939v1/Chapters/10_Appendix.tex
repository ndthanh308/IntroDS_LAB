
\section{Supplementary Material} \label{appendix}
As shown in the main paper, the training of resnet-18 for DP-SGD with BC and ALC converges when the added Gaussian noise is small enough.
%in Batch Clipping mode and ALC. 
We suspect that this is related to the size of the networks and the complexity of the training dataset. In Section \ref{app:convnet}, we work with shallow networks and see if the training of shallow networks on CIFAR10 can converge for larger Gaussian noise. In Section \ref{app:MNIST} we train a simple network on the simpler dataset MNIST and investigate whether the Gaussian noise can even be larger.  (We remind the reader that the larger the added Gaussian noise, the better the DP guarantee.) In Section \ref{app:BCandBNL}, we give evidence that DPSGD with batch clipping preserves the merits of using batch normalization layers in convolutional neural networks. To complete the work, Section~\ref{subsec:ALCvsFGC} compares ALC with Full Gradient Clipping (FGC) showing that ALC outperforms FGC.  


\subsection{Lightweight Network on a Complex Dataset: convnet with CIFAR10}
\label{app:convnet}

We conduct the same experiments as for resnet-18 on the CIFAR10 dataset with a lightweight network (convnet) which consists of 5 layers. The first 4 layers are the combination of a convolutional layer, a batch normalization layer and an average pooling layer followed by CONV-BN-POOLING order. The last layer is a softmax layer. The convent model architecture is defined in Table \ref{tab:convnet}.

\begin{table}[ht]
\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Operation Layer & \#Filters  & Kernel size & Stride & Padding & Output size & Activation function \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $32$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{3}{*}{$16 \times 16 \times 3$} & ReLu\\ 
 $BatchnNorm2d$&   & $32 \times 32$ &  &  &  & \\ 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ &  &  & \\ \hline 
\parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm}}& $64$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{3}{*}{$8 \times 8 \times 32$} & ReLu\\ 
 $BatchnNorm2d$&   & $64 \times 64$ &  &  &  & \\ 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ &  &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm} }& $64$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{3}{*}{$4 \times 4 \times 64$} & ReLu\\  
  $BatchnNorm2d$&   & $64 \times 64$ &  &  &  & \\ 
 $AvgPool2d$&  & $2 \times 2$ & $2 \times 2$ &  &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $128$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{3}{*}{$1 \times 1 \times 128$} & ReLu\\ 
  $BatchnNorm2d$&   & $128 \times 128$ &  &  &  & \\ 
 $AdaptiveAvgPool2d$&   & $1 \times 1$ & $1 \times 1$ &    &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC2$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ & $10$ & $softmax$\\ \hline 
\end{tabular}
}
\caption{convnet model architecture with batch normalization layers}
\label{tab:convnet}
\end{table}

% Figure environment removed
In our first experiment we choose the noise multiplier $\sigma = 0.01875$, batch size $m=1024$, diminishing learning rate $\eta = 0.025$ with decay value $\eta_{decay} = 0.9$. We vary the master clipping value $C$ as shown in Figure \ref{fig:convnet_small_sigma_vary_C}. For $C = 0.9$, we achieve $59.02\%$ test accuracy after 50 epochs, which is  less than the $67\%$  test accuracy achieved by the resnet-18 model. We observe that the test accuracy  increases significantly for $0< C \leq 0.2$, is stable with some fluctuations for $0.2 < C < 0.9$, and decreases slightly for $C> 0.9$. We conclude that, for noise multiplier $\sigma=0.01875$, there is a range of $C$ where we see stable performance in terms of test accuracy.


%LET'S NOT ADD THIS OPEN PROBLEM ...
%\footnote{Rather than the experiment's trial and error process in order to find such a range of $C$, we leave it as an open problem to find a more efficient method.}

%we do not figure out how to find such a range of $C$ effectively rather than doing the trial and error process. We consider solving this problem as future works.

% Figure environment removed

%\textcolor{red}{Figures 6 and 7 and 9 say "ours." Use the wording of the main body. "BC, ALC" instead of "ours, sigma=0..."}

In our second experiment we want to push the lightweight convnet model to the limit where we choose a relatively large $\sigma = 0.5$ with all other hyper-parameters remaining the same. As shown in figure \ref{fig:convnet_big_sigma_vary_C}, we only achieve $40.58\%$ test accuracy for $C = 0.1$ and we observe that the test accuracy decreases for $C> 0.1$.

% Figure environment removed

See Figure \ref{fig:convnet_benchmark}, our third experiment studies  mini-batch SGD without DP for convnet with CIFAR10 as our benchmark. We see  that smaller batch sizes yield better accuracy. Specifically, we achieve $80.55\%$ test accuracy for batch size $m= 64$ and $61.19\%$ for batch size $m = 1024$.

% Figure environment removed


This leads us to the fourth experiment, where, by choosing a smaller batch size,  we try to increase the test accuracy of DP-SGD with BC and ALC for convnet and CIFAR10 with $\sigma = 0.5$. For example, in Fig \ref{fig:convnet_vary_C_batchsize_64}.a we choose $m = 64$ and vary the master clipping constant $C$ to find the value which gives best test accuracy:  We achieve $43.42\%$ test accuracy for $C = 0.14$. Moreover, we also vary the noise multiplier value $\sigma$ with fixed master clipping value $C= 0.14$ to see whether our model can sustain larger noise. The Figure \ref{fig:convnet_vary_C_batchsize_64}.a shows that the testing accuracy drops $\approx 15\%$ when we increase $\sigma$ from $0.01$ to $0.2$ and decreases $\approx 10\%$ more from $\sigma=0.2$ to $\sigma = 0.6$. Therefore, we choose $\sigma = 0.5$ for the next experiments, where we can achieve $\approx 35\%$ testing accuracy and have better DP guarantee than our resnet-18 experiment.

% Figure environment removed


Our final  experiment uses DP-SGD with BC and ALC to train convnet with CIFAR10 for different batch sizes $m=(64,128,256,512,1024)$; 
%\textcolor{red}{Why not include $m=64$ of the previous experiment which motivates $C=0.14$? Should we change the red 64 into 128 in the previous text? => There was a mistake on the settings file I create so we ran from batch size 128 onward => just retrain the model for batchsize 64 an reuploaded it} 
We use master clipping constant $C = 0.14$, initial learning rate $\eta = 0.025$ with decay rate $\eta_{decay}=0.9$, noise multiplier $\sigma = 0.5$, and number of epochs $E=50$. We compare  with mini-batch SGD without DP with $m=64$ in Figure \ref{fig:convnet_BCversusbenchmark}. Although the accuracy is hurt badly by the Gaussian noise,
%${\cal N}(0,(2C\sigma)^2{\bf I})$, 
the lightweight  convnet model is still able to converge to $\approx 40\%$ test accuracy while the test accuracy for resnet-18  starts to fall below $40\%$ for $\sigma \geq 0.2$ and only achieves $\approx 20\%$ test accuracy for $\sigma = 0.5$. 


%Through this process, we are able to prove 
Our experiments show that the deep  resnet-18 network is more sensitive to the added Gaussian noise than the lightweight convnet network. This observation opens a new research direction where we want to simplify the neural network model as much as possible for a given dataset type (and corresponding learning task) while maintaining test accuracy and allowing a  large enough $\sigma$ for a reasonable DP guarantee. We expect (given our experiments) to be able to train  simpler network models with a larger noise multiplier $\sigma$ and this yields better privacy. The network simplification should not be too much in that the test accuracy of a trained model with DP noise should still be "good enough."
%while not hurting the model's performance too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{\color{red}Toan to Marten: I tried to run IC under these three settings: 
%\begin{itemize}
%    \item  $\sigma = 0.5$ + IC + diminishing stepsize + nor\_convnet (Batchnorm): diverged
%    \item  $\sigma = 0.5$ + IC + constant stepsize + convnet (no Batchnorm): converged to $20\%$
%    \item $\sigma = 0.5$ + IC + constant stepsize + nor\_convnet (Batchnorm):diverged
%\end{itemize}
%
%Here, we may not want to include IC because it appears that IC mode only converge when we remove the batch normalization layer. This may cause conflict to our comparison in the mainbody (where we run IC and BC with resnet-18 which has BatchNorm layer).
%
%Another note that we may not want to explain why shallow network works better with larger noise. Simply because we do not have the proof for that and we may just want to state it as an observation. Ha and I are thinking about a simple implementation to proof our intuition where shallow networks is less sensitive to DP noise. (Maybe for another paper I guess?) 
%}

\subsection{Lightweight Network on a Simple Dataset: BN-LeNet-5 with MNIST}
\label{app:MNIST}

We investigate how well our method performs on a simpler dataset compared to CIFAR-10. For this reason we conduct the same experiments of Section \ref{app:convnet} on the MNIST dataset.
%as we do on CIFAR-10 dataset.

MNIST consists of 60,000 training examples and 10,000 testing examples of handwritten digits \cite{Lenet5model}. Each example is a 28x28 gray-level image. For training,
%this dataset, 
we use the modified version of LeNet-5  \cite{Lenet5model}, where we add a batch normalization layer after each convolutional layer. The details of the modified LeNet-5 architecture (BN-LeNet-5) are described in Table \ref{tab:BN-LeNet-5}. For each training image, we crop a $32 \times 32$ region from it with padding of 4, apply a random horizontal flip to the image, and then normalize it with 
\[(mean, std) = (0.1307, 0.3081).\]

\begin{table}[ht]
\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Operation Layer & \#Filters  & Kernel size & Stride & Padding & Output size & Activation function \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $6$ & $5 \times 5$ & $1 \times 1$ & $0$ & $28 \times 28 \times 6$ & $tanh$\\ 
 $BatchnNorm2d$&   & $6 \times 6$ &  &  &  & \\ 
 \hline 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ &  & $14 \times 14 \times 6$ & \\ \hline 
\parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm}}& $16$ & $5 \times 5$ & $1 \times 1$ & $0$ & $10 \times 10 \times 16$ & $tanh$\\ 
$BatchnNorm2d$&   & $16 \times 16$ &  &  &  & \\ \hline 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ & &  $10 \times 10 \times 16$ &\\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm} }& $120$ & $5 \times 5$ & $1 \times 1$ & $0$ &  $5 \times 5 \times 120$ & $tanh$\\ 
 $BatchnNorm2d$&   & $120 \times 120$ &  &  &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC1$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ & $84$ & $tanh$\\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC2$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ & $10$ & $softmax$\\ \hline 
\end{tabular}
}
\caption{BN-LeNet-5 model architecture}
\label{tab:BN-LeNet-5}
\end{table}

As before, we fix the noise multiplier $\sigma = 0.5$ and   search for a good master clipping constant $C$. We use DP-SGD with BC and ALC to train the BN-LeNet-5 model with batch size $m=64$, diminishing step size $\eta = 0.025$ with decaying value $\eta_{decay} = 0.9$ in 50 epochs. See Figure \ref{fig:LeNet5varyC}, we achieve the best test accuracy $84.80\%$ for $C = 0.2$.  
%and this $C_{master}$ value gives the best accuracy because the testing accuracy decreases for $C<0.2$ and $C > 0.2$.

% Figure environment removed
%%% C = 0.2, max_acc = 84.80

Given $C = 0.2$, we push the BN-LeNet-5 model to the limit by choosing a relatively large noise multiplier $\sigma$ for which  the test accuracy does not drop below $50\%$. This allows us to see the effect of having a simpler dataset by comparing to the experiments in Section \ref{app:convnet}.
%to see how far $\sigma$ can reach for the MNIST dataset and BN-LeNet-5 model. 
We achieve $50.38\%$ test accuracy for  $\sigma = 2.5$ as shown in Figure \ref{fig:LeNet5varySigma}.b.

% Figure environment removed

Let $C = 0.2$ and $\sigma = 2.5$, we train the BN-Lenet-5 model with various batch size $m = (64,128,256,512,1024)$. As shown in Figure \ref{fig:LeNet5varySigma}.a, the test accuracy decreases from $50.38\%$ to $45.11\%$ when we increase the batch size from $m=64$ to $m=1024$. 

Our main conclusion is that 
%More importantly, 
the BN-LeNet-5 model still converges for the large noise multiplier $\sigma =2.5$ when training on the MNIST dataset.
% % Figure environment removed
Therefore, the simpler dataset allows us to use more Gaussian noise for differential privacy and this 
%a higher Gaussian Differential Privacy noise multiplier
%which 
yields an improved Differential Privacy guarantee.


%\textcolor{red}{Toan to Marten: We have experiment result which shows adding batch normalization layer yields better accuracy, should we include this in the appendix?}

%\textcolor{red}{
%Not sure ... I think BNL have been introduced to improve accuracy and everyone knows this -- so, why would this not also be the case if clipping and noise is added? Of course you have more evidence because of your experiment(s). But it may deviate from the main message of the paper?
%}
%\textcolor{red}{
%Ha suggests adding it because it shows that batch clipping + BNL is meaningful
%}
%Okay, this also makes sense. We can add just a very short A.3 (and also add a line to the intro of appendix A)

%\textcolor{red}{
%Toan to Marten: Just talked to Ha, We will add two more figures which compare the performance of the model between with and without BNL in no DP scenario. We can infer whether we can maintain the effect of the batch normalization layer in the batch clipping environment. Currently, I am re run the benchmark experiment for convnet without BNL.
%}
\subsection{Batch clipping and Batch Normalization Layer}
\label{app:BCandBNL}
The concept of a Batch Normalization Layer (BNL) has been introduced in \cite{ioffe2015batch} to improve the training speed and testing accuracy for convolutional neural networks. Figure \ref{fig:BNbenchmark} shows that indeed for normal training with SGD without DP batch normalization layers allow a high test accuracy.
In this section we  investigate how using BNLs helps attaining a higher test accuracy when using DP-SGD 
%differentially private training case where we perform the batch clipping 
with BC and ALC.
%and add the noise to the gradient at each iteration. 

% Figure environment removed



We compare training  convnet \ref{tab:convnet}   with and without BNLs for CIFAR10 by using DP-SGD with BC and ALC. 
Figure \ref{fig:BCandBN} shows that we achieve $\approx 5\%$ higher test accuracy for DP-SGD with batch size $m=64$, 
diminishing step size $\eta = 0.025$ with decaying value $\eta_{decay}=0.9$, master clipping constant $C = 0.14$, noise multiplier $\sigma = 0.5$ and total number of epochs $E=50$.


% Figure environment removed

We also run experiments with convnet and resnet18 after removing all BNLs for the set-up in Table \ref{tab:hyperparameter} (as in Section \ref{sec:experiments} in the main body) where we also consider a diminishing master clipping constant with initial value $C=0.095$ decaying with rate $C_{decay}=0.9$ after each epoch.
%our experiments with resnet18 and simple net after removing all the batch normalization layers in these models under both IC and BC modes. We also use the same experiments on the main body which are described in table  \ref{tab:hyperparameter}
%
\begin{table}[ht]
\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|}
\hline
 Learning Rate $\eta$ & $0.025$ \\ \hline 
 Master Clipping value ($C$) & $0.095$ \\ \hline 
 Noise multiplier $\sigma$ & $0.01875$ \\ \hline 
 Learning rate decay ($\eta_{decay})$ & $0.9$ \\ \hline 
 Clipping value decay ($C_{decay}$) & $0.9$ \\ \hline 
 Batch size ($m$) & $[64,128,256,512,1028]$\\ \hline 
 Epochs ($E$) & $50$\\ \hline 
\end{tabular}
}
\caption{Hyperparameter settings}
\label{tab:hyperparameter}
\end{table}

Figure \ref{fig:convnetnonBNICresult} is for convnet with IC+ALC (as compared to Figure \ref{fig:BCandBN} which is for BC+ALC).
After removing batch normalization layers in the convnet model, the testing accuracy for IC cannot converge to an acceptable value.
%as shown in Figure \ref{fig:convnetnonBNICresult} for individual clipping. 
After 50 epochs, we only achieve $21.12\%$ test accuracy if we train the model with constant step size and constant master clipping value $C$
%with our ALC method 
for mini-batch size $m=64$. This shows that batch clipping outperforms individual clipping  for  convnet without BNLs.
% Figure environment removed

Next, we ask ourselves whether batch clipping  still outperforms individual clipping for the more complicated model such resnet18 without BNLs. As shown in Figure \ref{fig:resnet18nonBNBCresult}, we achieve $30\% \sim 40\%$ test accuracy for the various combinations of constant and diminishing step size and master constant, respectively (with the best test accuracy close to $40\%$ for constant step size and non-decaying master constant).  
%After switching between constant and diminishing stepsize, master clipping value $C$. 
On the other hand, we only achieve $18\% \sim 22\%$ test accuracy for individual clipping  as shown in Figure \ref{fig:resnet18nonBNICresult}.


% Figure environment removed

% Figure environment removed


\subsection{ALC versus FGC}
\label{subsec:ALCvsFGC}

We investigate how ALC compares with Full Gradient Clipping (FGC) under the same Gaussian Noise $N(0,(2C\sigma)^2)$. (FGC means that we clip the full gradient and do not separately clip layers as in ALC.) As shown in Figure \ref{fig:ALCversusFGCSameAnddiscountednoise}, we used DP-SGD with BC for 50 epochs in order to train resnet18 with CIFAR10; we use a non-decaying master clipping constant $C= 0.095$, noise multiplier $\sigma = 0.01875$, mini-batch size $m = 64$ and diminishing step size $\eta = 0.025$ with $\eta_{decay}=0.9$.
%under batch clipping mode. 
We achieve $66.66\%$ with ALC and $46.35\%$ with FGC method. This shows evidence that training with ALC leads to better convergence rate as well as better test accuracy. 
% % Figure environment removed

However, in the above experiment the privacy budget is not the same for ALC versus FGC: See Section \ref{sec:layerwiseclipping} in ALC we have an extra factor $\sqrt{L}$ in the DP guarantee, 
%privacy budget for ALC method 
where $L$ is the number of layers in the neural network model. Therefore, we also run the same experiment for FGC  with discounted noise multiplier $\bar{\sigma} = \sigma/\sqrt{L} = \sigma/\sqrt{62}$ so that both ALC and FGC correspond to the same DP guarantee. The result is shown in Figure \ref{fig:ALCversusFGCSameAnddiscountednoise} and shows that ALC outperforms FGC.

% \textcolor{red}{I correct to division by $\sqrt{62}$ and this should also be written in Figure \ref{fig:ALCversusFGCSameAnddiscountednoise}.b. I hope you indeed used division by $\sqrt{62}$ (which leads to worse test accuracy compared to division by 62. Why does FGC not improve from (a) to (b)? Did you plot the wrong curve in (b)?}

% \textcolor{red}{Ha to Marten: I think the results in 18(b) are correct, i.e., (almost) no improvement in accuracy. The dependence between accuracy and $\sigma$ is shown in Figs 9(b) and 12(b). Typically, the accuracies of small $\sigma$s are roughly the same after 50 epoches. I want to say the results in 9,12,18(b) are quite aligned with each other.}


% % Figure environment removed

% Figure environment removed



% TODO: 
% = convnet vary C (after MNIST)
% = MNIST dataset? (Done)

\section{Towards Balancing DP Guarantees and Accuracy}

In this paper we have discussed how ALC and BC can bring us closer to balancing DP with accuracy. In particular, the focus of this paper is on the ALC and BC techniques for improving robustness against added Gaussian noise for differential privacy.
However, we still need additional techniques in order to achieve a practical balance which allows a reasonable accuracy (say at most 10\% or 20\% drop) together with a DP guarantee which shows a trade-off curve reasonably close to the ideal $1-\alpha$ curve which represents perfect security. 

In Section \ref{sec:balance} we show that ALC+BC as discussed in this paper is not yet sufficient on themselves to find a practical balance: Even though ALC+BC allow us to be significantly more robust against added Gaussian noise for bootstrapping DP, just using these techniques will not yet make training of even lightweight neural network models with less complex training datasets sufficiently robust against the required DP noise for good/solid differential privacy.

One direction of tackling this problem is to enhance and/or optimize the ALC+BC techniques. In Section \ref{sec:optimization} we offer suggestions that focus on improving the presented ALC+BC. We leave it to future work to empirically study these possible improvements and optimizations. And we leave it to future work to find altogether new techniques that are complimentary and go beyond ALC+BL. 


\subsection{Lightweight Neural Network Model with Less Complex Training Dataset} \label{sec:balance}


We show that even moving to a lightweight neural network with a less complex training data set still requires additional techniques beyond our BC+ALC and/or improvements  of the BC+ALC techniques. We show that BC+ALC as presented in this paper is on its own not yet sufficient even though they make the gap between DP guarantee and accuracy significantly smaller:
%\textcolor{red}{Ha to Marten: in my personal opinion, we do not want to have this subsection because we never have any fair comparison and good results. Current DPSGD does not give us good DP guarantee at the beginning. As researcher, we have to keep working on it to make it practical.}


We notice that Corollary 5.4 in \cite{dong2021gaussian} shows that, for $c>$, if $E=(N/m)\cdot c^2\rightarrow \infty$, then (\ref{eq:DPIC}) is asymptotically $G_{\mu}$-DP with $\mu=\sqrt{2}\cdot c\cdot h(\sigma)$ for 
$$ h(\sigma) = \sqrt{e^{\sigma^{-2}} \Phi(3\sigma^{-1}/2)+3\Phi(-\sigma^{-1}/2)-2}.$$
By using their interpretation of their result, we may conclude that (\ref{eq:DPIC}) is approximately $G_{\sqrt{2}\cdot c \cdot h(\sigma)}$ for $c=\sqrt{Em/N}$ for concrete large $N$ and relatively small $E$ and $m$. Including ALC means that we need to substitute $\sigma/\sqrt{L}$ for $\sigma$.
%%%%%% CORRECT %%%%%%

For  $\sigma = 0.01875$, we have $h(\sigma/\sqrt{L})\approx \sqrt{e^{(\sigma/\sqrt{L})^{-2}}}=e^{L\sigma^{-2}/2}$, i.e., a very very large number leading to no useful DP guarantee even for large datasets (with large $N$). In other words, $\sigma$ cannot be too small. Even $\sigma=0.5$ with $L=62$ for resnet-18  leads to 
$h(\sigma/\sqrt{L})= \sqrt{e^{62\cdot 4} \Phi(3\cdot \sqrt{62}) + 3\Phi(-\sqrt{62}) -2}$ which is prohibitively large for achieving a good DP guarantee. 
%$=\sqrt{ 54.60\cdot 0.9987 +3\cdot 0.1587 -2}=7.28$. 
%$h(\sigma)= \sqrt{e^4 \Phi(3) + 3\Phi(-1) -2}=\sqrt{ 54.60\cdot 0.9987 +3\cdot 0.1587 -2}=7.28$. 
%For our parameter setting $E=50$, $m=64$, and $N=\frac{9}{10}\cdot 50000$, we obtain $\approx G_{\sqrt{2Em/N}\cdot h(\sigma)}=G_{2.75}$-DP. This is still a rather weak DP guarantee, but for a factor 10 larger dataset we would already see a much more reasonable $G_{0.87}$-DP property. 
Figure \ref{fig:ourDPSGDwithDiffNoises} shows that choosing a larger $\sigma>0.5$ for obtaining a better DP guarantee gives too much noise resulting in a poor test accuracy of at most $20\%$, which is unacceptable.
%The main problem is that Figure \ref{fig:ourDPSGDwithDiffNoises} shows that $\sigma=0.5$ gives too much noise resulting in a poor test accuracy of about $20\%$, which is unacceptable. 
We conclude that resnet-18 and CIFAR10 represent a too deep neural network and complex dataset for a good balance between test accuracy and DP guarantee when using DP-SGD with our BC and ALC improvements. For now, we see that BC and ALC are two steps toward a better balance (after we are able to achieve convergence where this was not possible before for the original DP-SGD with IC) and that more techniques are needed for deep neural network models with complex training datasets. 

Section \ref{app:convnet} shows experiments for the lightweight convnet model with the complex CIFAR10 dataset which achieves a better $\approx 40\%$ test accuracy -- this demonstrates that a lightweight neural network model is more robust against noise and is better suitable for training with DP-SGD with BC and ALC.  Section \ref{app:MNIST} shows experiments for the lightweight BN-LeNet-5 model with the simple MNIST dataset which, if restricted to $50\%$ accuracy (for proper comparison with the $40\%$ test accuracy for convnet with CIFAR10), allows a much larger $\sigma=2.5$ resulting in a much improved $G_{0.52}$-DP guarantee ($h(\sigma/\sqrt{L})=1.513$ with $L=8$; $m=64$, $N=\frac{9}{10}\cdot 60000$, $E=50$). 

A smaller $\sigma=1.5$ (see Figure \ref{fig:LeNet5varySigma}.b in Section \ref{app:MNIST}) achieves a better balance of $\approx 67\%$ test accuracy with $\approx G_{1.99}$-DP ($h(\sigma/\sqrt{L})=5.783$). For a factor 15 larger dataset we would be able to improve $G_{1.99}$-DP to the better $G_{0.51}$-DP guarantee. We conclude that a more lightweight model and/or less complex (and larger) training dataset can potentially lead to a better balance between test accuracy and DP guarantee using the proposed BC and ALC techniques. Nevertheless, we will even want to improve $G_{0.51}$ to some trade-off function more like $G_{0.01}$ such that hypothesis testing indeed resembles a random guess. To date, this remains an open problem -- our ALC+BC techniques provide a step forward, but more complimentary techniques are needed.

\subsection{Towards Improving/Optimizing ALC+BC}
\label{sec:optimization}

From an accuracy perspective we see that  $\sigma=1.5$ (as mentioned above) or larger $\sigma$ may be needed even for a more lightweight network model with a less complex training dataset. For large $\sigma$, we notice that a Taylor series expansion of $h(\sigma)$ shows a linear dependency on $1/\sigma$. This shows that $\mu$ in a $G_{\mu}$-DP guarantee for ALC scales with $\sqrt{L}$ (since, as discussed before, ALC requires a factor $\sqrt{L}$ smaller $\sigma$ if we want to keep the same $G_\mu$-DP guarantee).
%, and this translates in the $\sqrt{L}$ dependency in $\mu$).
%\textcolor{red}{TO DO: we now understand that $\sigma$ is at least in the range $\geq 1$. For large $\sigma$, if this were possible giving good test accuracy, shows a Taylor series approximation with $h(\sigma)= ...$ and we clearly see its linear dependence on $1/\sigma$ implying a DP guarantee that will scale with $\sqrt{L}$ (its effect is not weakened by function $h$}
%Therefore, as a final remark, we observe that ALC effectively lowers $\sigma$ by a factor $\sqrt{L}$ in the DP analysis. 
The linear dependency of $\mu$ on $\sqrt{L}$ is due to the fact that there is no subsampling effect for the separate layers within a gradient computation; the leakage is directly composed over all the layers for a single gradient computation (without using a subsampling operator as is done in the general analysis leading to (\ref{eq:DPIC})). For this reason, it is advantageous to group layers that have similar clipping constant and clip the group rather than the individual layers within the group. E.g., Figure \ref{fig:resnet18gradientnorm} indicates we may use about 4 groups of layers representing a very small norm, to medium and larger norms. This reduces $L=62$ down to $L=4$ for resnet-18. Also, notice that the unexplored sparsification trick mentioned in Section \ref{sec:sparse} may offer another improvement.

Our discussion shows that as future work, we need to further optimize the promising ALC technique. One tempting direction is to not use a single training sample $\xi$ for updating all the layer gradients, but to use $\xi$ for a single layer gradient. So, rather than using a data sample $\xi$ for computing updates for all the layer gradients, we can think of using a data sample $\xi$ for updating just one of the layer gradients. Together with $\xi$ we choose one of the $L$ layers at random. In this way we still train all the layers. We notice that this approach means that we have subsampling for each layer gradient and we do not pay the composition price of $\sqrt{L}$ as explained in \ref{sec:layerwiseclipping}. However, one can think of this as $L$ separate learning tasks, each costing the same amount of training as the original learning task which learn the full weight vector across all layers at once. This means a composition of $L$ leakages and we again pay the price of $\sqrt{L}$ since $G_\mu^{\otimes L}=G_{\sqrt{L}\cdot \mu}$. Or, equivalently, one can argue that we need $L$ times more rounds in order to train the full weight vector, i.e., a factor $L$ more epochs, hence, the $\sqrt{L}$ factor penalty after composing over all the epochs. So,  this idea still does not improve the sought-after balance between DP guarantee and accuracy since we now have proper amplification from subsampling but at the price of $L$ times more rounds, and this cancels out, that is, no improvement. 


Based on the above discussion we would like to somehow  only clip the overall full gradient while still keeping the better robustness against DP noise of ALC which requires layerwise clipping. 
Suppose that we associate a multiplication factor $m_j\geq 1$ to each layer $j$. We proceed as follows:
\begin{enumerate}
    \item As before, we first compute the full gradient composed of layer gradients:
$$\nabla_w f(w;\xi)  = ( \nabla_{w_1} f(w;\xi)  ||   .... || \nabla_{w_L} f(w;\xi) ).
$$
\item We use the multiplication factors $m_j$, $1\leq j\leq L$, to compute
$$\{ \nabla_w f(w;\xi)  \}_{m_1,...,m_L} = ( m_1\cdot \nabla_{w_1} f(w;xi)  ||  .... || m_L \cdot \nabla_{w_L} f(w;\xi) ).
$$
\item Now we perform full gradient clipping (FGC) with clipping constant $C$:
$$[ \{ \nabla_w f(w;\xi)  \}_{m_1,...,m_L} ]_C.
$$
We use this in computing formulas (the $a_h$) leading to the noised update $U$. Notice again that we can use the IC or BC approach in these formulas.
%(over a mini-batch for IC or we use BC).
\end{enumerate} 
The differential privacy argument follows the line of thinking of our analysis of BC in Section \ref{sec:BC}. Since we use FGC, we do not pay the $\sqrt{L}$ penalty.  This will significantly improve the trade-off function as discussed in Section \ref{sec:balance}.

What about robustness against the added Gaussian DP noise? The server receives a noised update of the form
$$U = ( U_1 || ... || U_L).$$
The server divides by the multiplication factors and computes
$$( U_1/m_1 || ... || U_L/m_L )$$
with which the global model is updated.
The effect of the proposed trick using multiplication factors $m_j$ is that dividing by the multiplication factors retrieves the original layer gradients -- if there is no clipping noise. 
%(potentially normalized together by the clipping constant). 
In this process we  reduce the added Gaussian noise, since the noises are divided by the factor $m_j\geq 1$.
In our ALC we estimate the expected norm of each layer $j$ denoted by $e_j$. We equate $M$ to the maximum of all $e_j$. We compute layer clipping constants $C\cdot e_j/M$. In the above approach based on multiplication factors we may define $m_j=M/e_j$. Since this will increase each layer norm to $M$ in expectation, we will want to choose a higher overall clipping constant $C$ in FGC (as compared\footnote{We can use the same collecting-layers-into-4-groups argument for resnet18 with which we started this section and conclude that the clipping constant corresponding to the use of multiplication factors is about $\sqrt{4}=2$ larger.} to the master clipping constant $C$ used in ALC). We see that the proposed new trick on one hand remains robust to Gaussian noise added to layers that have a small norm compared to other layers. On the other hand $C$ needs to be fine-tuned and may be larger than the $C$ of ALC, which means that the overall added noise is larger and makes this solution less robust.  
Concluding, we have the original DP guarantee without $\sqrt{L}$ penalty, while we make sure that layer gradients with small norms get multiplied by a large $m_j$ so that the effect of the added noise for that layer is not going to be overpowering. This is also the goal which ALC wants to achieve. We leave it to future work to experiment with multiplication factors and in this sense optimize over ALC and/or find a better balance between DP guarantee and test accuracy.


As a final remark, we notice that we do not need to restrict ourselves to using BC, we may use GBC which allows momentum based update rules. We leave it to future work to find out whether this can lead to more robustness against added Gaussian DP noise.




%Concluding, we still need additional complimentary techniques for a deep neural network like resnet-18 with CIFAR10 in combination with (optimized) BC and ALC in order to achieve a practical balance between test accuracy and DP guarantee. 

%\textcolor{red}{TO DO: some experiments showing how test accuracy is improved by ALC in appendix for the simple neural network and simple dataset; extend Figure 4 with DPSGD+IC and without ALC allows an even smaller delta -- did we optimize for $C$? The comparison does not seem fair to me.}
%
%%%%%%%% END EXTRA TEXT %%%%% OLD TEXT BELOW
%For  $\sigma = 0.01875$, we have $h(\sigma)\approx 1$, hence, we obtain $\approx G_{\sqrt{2Em/N}}$-DP which is $G_{0.38}$-DP for our parameter setting $E=50$, $m=64$, and $N=\frac{9}{10} \cdot 50000$.

% % Figure environment removed
%\textcolor{red}{TO DO: edit}
%We have $\sigma_i = \sigma_{initial}\cdot 2^i, i=0,\dotsc,7$ where $\sigma_{intial} = 0.01875$. The result is depicted in Figure~\ref{fig:ourDPSGDwithDiffNoises}.

% First, we run our experiments on different hyper-parameter settings defined in table \ref{tab:hyperparameter} where we vary master clipping value $C_{master}$, noise multiplier $\sigma$ and the mini-batch size.

% \begin{table}[ht]

% \centering
% \scalebox{0.8}{
% \begin{tabular}{|c|c|}
% \hline
%  Learning Rate $\eta$ & $0.025$ \\ \hline 
%  Master Clipping value ($C_{master}$) & $[0.005,0.01,0.05,0.1,0.5,1.0]$ \\ \hline 
%  $\sigma$ & $[2,4,8]$ \\ \hline 
%  Learning rate decay ($\eta_{decay})$ & $0.9$ \\ \hline 
%  Clipping value decay ($C_{decay}$) & $0.9$ \\ \hline 
%  Batch size ($|S_b|$) & $[64,128,256,512,1028]$\\ \hline 
% \end{tabular}
% }
% \caption{Hyperparameter settings}
% \label{tab:hyperparameter}
% \end{table}

% According to Figure \ref{fig:BCaccuracy}, we achieved $89.85\%$ testing accuracy with batch size $m=64$ after 50 training epochs, noise multiplier $\sigma = 2$, master clipping value $C_{master} = 0.1$, learning rate $\eta = 0.025$


% Available:
% = BC + layerwise + diminishing C (2 < sigma < 62) (done)
% = Batch clipping + layerwise + diminishing C (done)
% = IC + layerwise + diminishing C (2 < sigma < 62) (done)
% benchmark (done)
% BC + layerwise + diminishing C (sigma= 64,128,256,512,1024) (Done) (setting 1=> 16)
% BC + Zhang (sigma= 64,128,256,512,1024)(Done)
% ==> Add 1a,1b,2 add benchmark (done)
% subsection adaptive => gradient graph for resnet 18 with 50 epochs (done)
% ********TODO: tutorial on github: batch clipping(best setting) + subsampling + vary sigma (2=> 1 mil)
% TODO: 
% BC + layerwise + diminishing C (sigma= $2^{10} ,..., 2^{30}$) (on going)






