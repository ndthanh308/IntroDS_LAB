\chapter{Generalized Framework for DP-SGD}
\label{chapter:generalizedframeworkDPSGD}
Van Dijk et al. \cite{vandijk2023generalizing} have proposed a generalized framework for DPSGD that allows us to train any machine learning algorithm differentially privately. Moreover, this framework also supports shuffling sampling, which has better performance than subsampling methods \cite{safran2021good}. Therefore, we can use this framework to train our method (shuffling + layerwise batch clipping) without violating privacy. We simplified algorithm 1 in \cite{vandijk2023generalizing}, which is written for the federated learning setting, to a single client training algorithm, shown in Algorithm \ref{alg:genDPSGD}.

In \cite{vandijk2023generalizing}, two sampling methods are analyzed:

\begin{itemize}
    \item Subsampling (SS): In this method, a batch $S_B$ with $|S_B| = sm$ is randomly sampled from the data set $D$ with replacement. Thus, the batches sampled by this method may have intersections. Moreover, this is the sampling method which is used in \cite{Abadi_2016}.
    \item Shuffling (SH): In this method, the whole data set $D$ is shuffled by a random permutation $\pi^e$ and then divided into equally-sized batches $S_b$ with $|S_b| = sm$ in a random way. Therefore, the batches sampled by this method are disjoint, i.e., $D= \cup^{N/(ms)}_{b=1} S_b$.
\end{itemize}
%%%% Shuffling Batch clipping algorithm %%%%%%
\begin{algorithm}[t]
\caption{%Differential Private SGD -- 
Generalized Framework for DP-SGD}
\label{alg:genDPSGD}
\begin{algorithmic}[1]
%\Procedure{LocalSGDwithDP}{$d,C,s,\sigma,T$}
\Procedure{DP-SGD-General}{}
   \State \textbf{Input:} Dataset size $N$,dataset $(D=\{\xi_i\}_{i=1}^{N})$, number of epochs $(E)$, diminishing round step size sequence $\{\eta_i\}_{i=1}^{T}$, the default initial model $(w)$, Gradient clipping value $(C)$, noise scale $\sigma$
    \For{$e\in \{1,\ldots, E\}$}
    \State Let $\pi^e$ be a random permutation 
    \State re-index data samples: $\{\xi_i\leftarrow \xi_{\pi^e(i)}\}_{i=1}^N$  
    \State $\{S_{b,h}\}_{b=1,h=1}^{N/(ms),m}\leftarrow$ ${\tt Sample}_{s,m}$ with $S_{b,h}\subseteq \{1,\ldots, N\}$, 
    \Comment{ $|S_{b,h}|=s$, $|S_b|=sm$ with $S_b=\bigcup_{h=1}^m S_{b,h}$ }
    \For{$b\in \{1,\ldots, \frac{N}{ms}\}$}
%    \State Set $w$ equal to last received global model
 %   \State $U=0$
 \State Start of round $(e-1)\frac{N}{ms}+b$:
    \For{$h\in \{1,\ldots,m\}$}
    \State $a_h\leftarrow {\cal A}(w,\{\xi_i\}_{i\in S_{b,h}})$
    \State $[a_h]_C = a_h/\max(1,\frac{||a_h||_2}{C})$ \Comment{Clipping gradient}
 %   \State $U\leftarrow U+[a]_C$
    \EndFor
    
    \State $U=\sum_{h=1}^m [a_h]_C$
    \State $\bar{U}\leftarrow U+ {\cal N}(0,(2C\sigma)^2{\bf I})$ \Comment{Add Noise}
    \State $w\leftarrow w- \eta_{(e-1)\frac{N}{ms}+b} \cdot \bar{U}/m$
    \EndFor
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

The algorithm ${\cal A}$ could be any optimization algorithm (such as SGD, ADAM, etc.) and only depends on $w$ and the batch of data samples $S_{b,h}$. Moreover, based on the DP analysis in \cite{vandijk2023generalizing}, this generalized framework also enables us to apply batch normalization, where the normalization is performed on the samples in batch $S_{b,h}$. Thus, we can even use a batch normalization layer in our neural network's architecture.

\subsubsection{Clipping}
In algorithm \ref{alg:genDPSGD}, the gradients $a_h$ are clipped as follow:
\[[a_h]_C = a_h/\max(1,\frac{||a_h||_2}{C})\]
and then added into the updated value $U = \sum_{h=1}^m [a_h]_C = \sum_{h=1}^m [a_h/\max(1,\frac{||a_h||_2}{C}]_C$

Note that the batch size $|S_b| = sm$ and the micro-batch size $|S_{b,h}|=s$, allowing us to switch the clipping settings by choosing the appropriate values of $s$ and $m$. Specifically, we have the following options:
\begin{itemize}
    \item Individual Clipping ($s=1$): In this setting, each micro-batch $S_{b,h}$ contains only one sample. As a result, $[a_h]_C$ represents the clipped value of each sample's gradient, leading to an individual clipping setting similar to \cite{Abadi_2016}.
    \item Batch Clipping ($m=1$): With only one micro-batch $S_{b,h}$ that contains all samples in the batch ($S_b = S_{b,h} = s$), the gradient values of the whole batch computed by the optimization algorithm ${\cal A}$ are used for clipping ($a_h$). This approach can leverage parallel computation provided by popular frameworks such as Tensorflow and PyTorch, leading to significantly reduced training time. However, the entire batch's gradients are clipped with the same value $C$, which may result in a higher gradient norm compared to individual clipping, potentially losing more information in the gradients.
    \item Mixing Clipping ($1 < s,m < sm$): Finally, Van Dijk et al. \cite{vandijk2023generalizing} also provides a mixing clipping method where we have $m$ micro-batches with size $s$. This method attempts to combine the benefits of individual and batch clipping methods.
\end{itemize}

However, based on the experimental results presented in \cite{vandijk2023generalizing}, the batch clipping method showed the highest performance for a CNN architecture with 4 convolutional layers. Therefore, we chose to use the batch clipping method to evaluate our new algorithm, which is described in chapter \ref{chapter:layerwiseclipping}.

\subsubsection{DP guarantee}

In \cite{vandijk2023generalizing}, the adversary is stronger compared to the adversary in \cite{dong2019gaussian}. This adversary knows which sampling method was used and how it works. They can derive a joint probability distribution of the number of differentiating data samples for each sampling round within the sequence of rounds. In contrast, the adversary in the $f$-DP analysis \cite{dong2019gaussian} considers each round separately and analyzes a single round. In other words, the adversary in \cite{vandijk2023generalizing} has extra information from the sampling operator by observing the outcome of multiple rounds and can use it to enhance the statistical attack.

Therefore, Van Dijk et al. \cite{vandijk2023generalizing} enables us to use other sampling methods rather than mini-batch SGD, which was introduced in the original DPSGD paper \cite{Abadi_2016}. Applying the $f$-DP terminology, the general algorithmic framework is $G_{\sqrt{gE}/\sigma}$-DP with a $\sqrt{gE}$ dependency, where $g$ is the group size, $E$ is the total number of epochs, and $\sigma$ is the noise multiplier in Gaussian noise.
