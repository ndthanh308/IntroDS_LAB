\chapter{Differential Privacy}
\label{chapter:Differentialprivacy}
\section{What is Privacy?}

At first thought, you may wonder why we need data privacy and what it exactly means. Unfortunately, there is no exact definition for privacy because it depends on the context and what information we consider to be "private." For example, each person has their own different set of private information, such as address, phone number, name, and so on. Hence, this information is considered sensitive data and no one wants this information to be revealed to any other party without their permission. Therefore, Dwork \cite{DPbook} proposed a formal mathematical definition of privacy called Differential Privacy, which assures that there is hardly a behavior change when an individual's data is added to or removed from the dataset.

\section{Differential Privacy}
Let the randomized mechanism $M$ be the algorithm applied by the curator when releasing information, thus we do not need to distinguish interactive setting and non-interactive setting. Moreover, we say the databases $D_1$ and $D_2$ are neighboring databases if they just differ at most one element. 
\begin{definition}
\label{DP:definition}
A randomized mechanism $M$ gives $(\epsilon,\delta)$-differential privacy if for \textbf{all} neighboring databases $D_1$ and $D_2$, and all $S \subseteq Range(F)$:
\[Pr[M(D_1) \in S] \leq e^\epsilon Pr[M(D_2) \in S] + \delta \]
When $\delta = 0$, the guarantee is simply called $\epsilon$-DP
\end{definition}

A mechanism M satisfying this definition can ensure the privacy for any participant data even if he/she either adds his/her data to the database or removes his/her data from the database. For example, the presence or absence of a patient profile in a hospital database does not affects significantly the outcomes of the differential  private mechanism M which is used by that hospital.

Differential privacy provides a very strong guarantee of privacy. To be detailed, it is just a statistical property about the behavior of the randomized mechanism, thus it is independent of the computational power and auxiliary information available to the adversary/attacker. However, Differential privacy does not give absolute guarantee of privacy and we measures the cost as privacy loss given the output $o$ of the mechanism M:
\begin{equation}
    % \begin{aligned}
    PL_{M(D_1)||M(D_2)}^{o} = \ln \left(\frac{Pr[M(D_1) = o]}{Pr[M(D_2) = o]}\right)
    % \end{aligned}
\end{equation}

Dwork and Roth have shown that any $(\epsilon,\delta)-$differentially private mechanism ensures that for all neighboring database $D_1,D_2$, the absolute value of privacy loss will be bounded by $\epsilon$ with probability at least $1 - \delta$.

Next, we introduce the properties of differential privacy which helps it extremely useful and be applicable to solve many privacy problems.
\subsection{Properties}
There are four majors properties of differential privacy which are: Post-processing, Group Privacy, Basic Composition and Advance Composition.
\begin{properties} \textbf{Post-processing:} Let $M: \mathbb{N}^{|X|} \rightarrow R$  be a randomized algorithm that is $(\epsilon,\delta)$-differentially private. Let $f: R \rightarrow R'$ be an arbitrary deterministic mapping. Then $f \circ M : \mathbb{N}^{|X|} \rightarrow R'$ is $(\epsilon,\delta)$-differentially private.
\end{properties}


\begin{properties} \textbf{Group Privacy:} Let $M: \mathbb{N}^{|X|} \rightarrow R$ be a randomized algorithm that is $(\epsilon,\delta)$-differentially private. Then, M is $(k\epsilon,ke^{k-1}\delta)$-differentially  private for groups of size k. That is, for databases $x,y \in \mathbb{N}^{|X|}$ such that $||x=y||_1 \leq k $ and for all $S \subseteq R$ we have:
\[Pr[M(x_1) \in S] \leq e^{k\epsilon} Pr[M(x_2) \in S]  + ke^{k-1}\delta\]
\end{properties}
\begin{properties} \textbf{Basic Composition:} Let $M_i: \mathbb{N}^{|X|} \rightarrow R_i$ be a randomized algorithm that is $(\epsilon_i,\delta_i)$-differentially private for $i = 1,2,3,...,k$. Then, the composition $M_{[k]}: \mathbb{N}^{|X|} \rightarrow R_1 \times R_2 \times \dots \times R_k$ defined as:
\[M_{[k]}(x) = (M_1(x),M_2(x),M_3(x),\dots ,M_k(x))\] is $(\sum_{i=1}^k\epsilon_i,\sum_{i=1}^k\delta_i)$-DP
\end{properties}
\begin{properties} \textbf{Advanced Composition:} Let $M: \mathbb{N}^{|X|} \rightarrow R^K$ a k-fold adaptive composition of $(\epsilon_i,\delta_i)$-DP mechanisms. Then, M is $\epsilon', k\delta + \delta'$-DP for:
\[\epsilon' = \epsilon \sqrt{2k \ln(\frac{1}{\delta'})} + k\epsilon(e^\epsilon -1)\] is $(\sum_{i=1}^k\epsilon_i,\sum_{i=1}^k\delta_i)$-DP
\end{properties}

% \pagebreak % TODO: automatically page break for algorithm

\subsection{Gaussian Mechanism}
Dwork \cite{DPbook} proposed many mechanisms such as Gaussian Mechanism, Laplace Mechanism and Exponential Mechanism which provides the privacy for and function $f$ by adding the Gaussian noise, Laplace noise or choosing the output "randomly" based on the a certain probability, respectively. However, in this work, we only focus on Gaussian Mechanism which is mainly used on our private deep learning training algorithm and DP analysis.

\begin{definition}
\label{def:gaussianmech}
    Let $f: \mathbb{N}^{|X|} \rightarrow R^d$ be an n arbitrary $d$-dimensional function and the $\ell_2$ sensitivity of $f$ be $||f||_2 = max_{x,y} ||f(x)-f(y)||_2$ for any neighboring datasets $x$ and $y$. The Gaussian Mechanism with parameter $\sigma$ adds the $\sigma$ adds the Gaussian noise $N(0,\sigma^2)$ to each of $d$ components of the output. Then with any $\epsilon \in (0,1)$, for $c^2 > 2\ln (1.25/\delta)$, the Gaussian Mechanism with $\sigma \geq c \frac{||f||_2}{\epsilon}  $ is $(\epsilon,\delta)$-differentially private.
\end{definition}

By injecting noise into the datasets, we can perform all possible statistical analyses without identifying any personal sensitive information. Differential Privacy motivates users to share their personal information without worrying about it being exposed by other parties. Therefore, it solves a significant problem for other computer science subjects that rely heavily on user data, such as Internet of Things (IoT) or Machine Learning/Deep Learning. Based on the above analysis, Differential Privacy is an ideal tool to provide privacy for Machine Learning and Deep Learning models, which is a primary topic we have been focusing on.




% \begin{mechanism} 
% \textbf{Gaussian Mechanism:} TBD
% \end{mechanism}
% \begin{mechanism} 
% \textbf{Laplace Mechanism:} TBD
% \end{mechanism}
% \begin{mechanism} 
% \textbf{Exponential Mechanism:} TBD
% \end{mechanism}
% \section{Differential Privacy Variants}
% \begin{definition} Local Differential Privacy (LDP): A randomized algorithm $M$ gives $(\epsilon,\delta)$-differential privacy if for \textbf{all pairs} of user's possible private data $x_1$, $x_2$, and all subsets $S \subseteq Range(M)$:
% \[Pr[M(x_1) \in S] \leq e^\epsilon Pr[M(x_2) \in S] + \delta \]

% \end{definition}
% \subsection{\href{https://arxiv.org/pdf/1702.07476.pdf}{\textcolor{blue}{Renyi Differential Privacy}}}
% \begin{definition}
% \textbf{Renyi Divergence}: For two probability distributions P and Q defined over R (the image of the DP mechanism), the Renyi divergence of order $\alpha > 1$ is 
% \[D_\alpha (P||Q) = \frac{1}{\alpha - 1} \log E_{x \sim Q} (\frac{P(x)}{Q(x)})^\alpha\]
% \end{definition}
% \begin{definition}
% $(\alpha,\epsilon)-$\textbf{RDP}: A randomized mechanism $M:D \rightarrow R$ is said to have  $\epsilon$-Renyi differential privacy of order $\alpha$  if for \textbf{all} data sets $D_1$ and $D_2$ differing only on at most one element, it holds that:
% \[D_\alpha(M(D)||M(D') \leq \epsilon\]
% Or
% \[Pr[M(D) \in S] \leq (e^\epsilon Pr[M(D') \in S])^{\frac{\alpha-1}{\alpha}} \] for any subset $S \subseteq R$
% \end{definition}
% \begin{properties}
% \textbf{Post Processing and Group privacy:} the same as $(\epsilon,\delta)$-DP
% \end{properties}
% \begin{properties}
% \textbf{Basic Composition:} Let $f: D \rightarrow R_1$  be $(\alpha,\epsilon_1)$-RDP and $g: D \rightarrow R_2$  be  $(\alpha,\epsilon_2)$-RDP, then the mechanism defined as (X,Y), where $X \sim f(D)$, $Y \sim g(D)$, satisfies $(\alpha, \epsilon_1 + \epsilon_2)$-RDP
% \end{properties}
% \textbf{Advanced Composition:}:  Let $f: D \rightarrow R$  be an adaptive composition of n mechanisms all satisfying $\epsilon$-RDP. Let D and D' be two adjacent inputs. Then for and $S \subseteq R$:
% \[Pr(f(D) \in S] \leq exp\left(2 \epsilon \sqrt{n \log (1/Pr[f(D') \in S])}\right) \cdot Pr[f(D') \in S]\]
% \textbf{Moment Accountant:} For $\alpha \leq 1 +\sigma^2\ln{\frac{1}{q\sigma}}$, the DPSGD algorithm is $(\alpha, q^2 \frac{\alpha}{(1-q)\sigma^2}+ O(q^3\alpha^3/\sigma^3)$-RDP for $q \rightarrow 0$
% \subsection{RDP versus DP}
% % % Figure environment removed
% \subsection{\href{https://projects.iq.harvard.edu/files/privacytools/files/bun_mark_composable_.pdf}{\textcolor{blue}{Concentrated Differential Privacy (CDP)}}}
% Let $M_{\alpha}(A, S,S') = exp(\alpha D_{\alpha+1}(A(S)||A(S')))$, 
% The following are equivalent definitions of CDP,zCDP,tCDP and RDP:
% \begin{definition}
% For all adjacent databases S,S', a mechanism A satisfies:
% \begin{itemize}
%     \item $(\mu,\tau)$-CDP if:
%     \[E_{z\sim A(S)} [c(z;A,S,S')] \leq \mu, M_\alpha(A,S,S') \leq exp(\frac{\alpha^2\beta^2}{2})\]. for all $\alpha \geq 1$
%     Where $c(z;A,S,S')$ is the privacy loss of mechanism A given output z.
%     \item $(\xi,\rho)$-zCDP if:
%     \[\forall \alpha > 0: M_\alpha(A,S,S') \leq exp(\alpha(\xi + \rho(\alpha+1)))  \]
%     \item $(\rho,\omega)$-tCDP if:
%     \[\forall \alpha \in (0,\omega): M_\alpha(A,S,S') \leq exp(\rho \alpha(\alpha +1))\]
%     \item $(\alpha,\epsilon)$-RDP if:
%     \[M_{\alpha-1} (A,S,S') \leq exp((\alpha-1)\epsilon)\]
% \end{itemize}
% \end{definition}


% \section{Deep Learning}
% \subsection{Federated Learning}
% % \subsection{Online Learning}

% \section{Differentially Private Training Methods}
% \subsection{Differential Private Stochastic Gradient Descent (DPSGD)}
% TBD: Add intro here


% \begin{definition}

%  \textbf{Moment Accountant \href{https://arxiv.org/pdf/1607.00133.pdf}{(\textcolor{blue}{Deep Learning with Differential Privacy})}:} There exist constants $c_1$ and $c_2$ so that given
% the sampling probability $q = L/N$ and the number of steps
% T, for any $\epsilon < c_1 q^2 T$, the DPSGD algorithm is $(\epsilon,\delta)$-DP for \textbf{any} $\delta > 0$ if we choose:
% \[\sigma \geq c_2 \frac{q\sqrt{T\log{1/\delta}}}{\epsilon}\]

% \end{definition}