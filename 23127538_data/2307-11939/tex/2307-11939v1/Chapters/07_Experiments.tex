\section{Experiments}
\label{section:Experiments}

This section reports on our evaluation of the batch clipping method, diminishing clipping value $C$, layerwise clipping values $C_i$, and results on two popular image datasets: MNIST and CIFAR-10. Moreover, we also show that our method outperforms Opacus settings in terms of testing accuracy. The code is available at \href{https://github.com/MisaNguyen/DPSGD/tree/master}{https://github.com/MisaNguyen/DPSGD/tree/master}.

% In order to demonstrate the power of the algorthm (TODO: Cite the algo), we run this algorithm under various optimization methods, deep learning models and hyper-parameters settings. Due to the dataset complexity, we also try out different deep learning model accordingly: Lenet-5, Convolutional Neural Network with 5 convolutional layers, Alex-net, Resnet-18, Resnet-50. More importantly, since we can apply any algorithm to compute the gradients, we also experiments with different model optimizers: Stocastic gradient descent (SGD), ADAM. Moreover, based on the dataset, model architecture and model optimizer we choose, we also do the sensitivity studies on the hyperparameters: Learning rate $\eta$, master clipping norm constant $C$, the Gaussian noise variance scale $\sigma$ and the batch size $|B|$. 
\subsection{Training Configuration}
Testing accuracy depends on various factors such as the batch size $|S_b|$, learning rate $\eta$, clipping norm $C$, Gaussian noise $N(0,(2C\sigma)^2)$, model architecture, number of epochs $E$ or training iterations, data augmentation, data normalization, sampling method, optimizer, and training algorithm. In our experiments, we define a set of parameters to explore how these variables affect testing accuracy. Our objective is to identify the optimal configuration that achieves the highest testing accuracy while keeping the privacy budget $(\epsilon,\delta)$ as small as possible.

To ensure consistency, we apply data augmentation and normalization on the training dataset before feeding the images to the neural network. We also shuffle the entire dataset at the beginning of each epoch before splitting it into training batches. These techniques are used in all of our experiments for the following reasons:

\begin{itemize}
\item Data Augmentation \cite{dataAugpaper}: This technique enhances the size and quality of the training dataset to avoid overfitting and improve the generalization ability of the neural network, which can lead to better testing accuracy.
\item Data Normalization \cite{dataNormpaper}: This technique helps the convolutional neural network learn faster and even improve testing accuracy in some scenarios. Since our experiments involve image classification using convolutional layers in our neural networks, we consider this technique to be helpful.
\item Random Shuffling in each epoch \cite{safran2021good}: This technique has proven to have smaller optimization error than subsampling (where we sample the data batches with replacements) for the SGD optimizer. Therefore, we also applied this technique to enhance our testing accuracy further.
\end{itemize}

Our main goal is to demonstrate the effectiveness of our model when utilizing the batch clipping method, diminishing clipping value, and layerwise clipping during the training process of our differentially private model. Therefore, we conduct four experiments for each dataset using the SGD optimizer and the methods listed in Table \ref{tab:Experiments}:

\begin{table}[ht]

\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
   \multicolumn{2}{|c|}{Sampling} & \multicolumn{2}{|c|}{Clipping method} & \multicolumn{2}{|c|}{Clipping value} & \multicolumn{2}{|c|}{Clipping mode} & \multicolumn{3}{|c|}{Testing Accuracy} & Note\\ \hline 
   SS & SH & BC \qquad & IC & Cons & Dim  & Layer & Full & MNIST& \multicolumn{2}{|c|}{CIFAR10} &\\ \hline
   \multicolumn{8}{|c|}{ } & Lenet-5 & convnet & resnet18 & \\ \hline
   x &   &   & x &   & x &   & x & $89.70\%$&$37.08\%$ & $51,24\%$& Opacus\\ \hline
   & x & x &   & x &   &   & x & $\approx 98\%$  & $\approx 65\%$ & N/A &\cite{vandijk2023generalizing}\\ \hline
   
    & x & x &   & x &   & x &   & $98.25\%$ & $68.54\%$  & $90.78\%$  &ours\\ \hline
   & x & x &   &   & x & x &   & $98.25\%$& $60.31\%$ & $91.06\%$ &ours\\ \hline
 % 4 & &   & x & x &   &   & x &   & x & ours\\ \hline
  
\end{tabular}
}
\caption{Experiments description: SS and SH stand for subsampling and shuffling. "BC" and "IC" are the batch clipping and individual clipping methods, respectively. "Cons" and "Dim" refer to the constant and diminishing clipping values. "Layer" and "Full" indicate whether we clipped the gradient at each layer or fully clipped the parameters of the entire model. The testing accuracy is achieved after 50 training epochs with a diminishing learning rate $\eta$ and different Gaussian noise multipliers $\sigma$, batch sizes $S_b$, and clipping values $C$ described in Table \ref{tab:hyperparameter}. with a privacy budget of $(\epsilon,\delta) = (1,1/N)$, where $N$ is the size of the training dataset. The testing accuracy is the best testing accuracy for $\sigma = 2$ where we have the same privacy $G_{\sqrt{gEL/}\sigma}$}
\label{tab:Experiments}
\end{table}
We do the experiments on the parameters defined in Table \ref{tab:hyperparameter}. Besides, for diminishing learning rate $\eta$ and clipping value $C$, we update these two parameter after each training epoch:
\[\eta = \eta \times \eta_{decay}\]
And
\[C = C \times C_{decay} \textit{\ in\ diminishing\ clipping\ value\ case}\] 
\begin{table}[ht]

\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|}
\hline
 Learning Rate $\eta$ & $0.025$ \\ \hline 
 Clipping value ($C$) & $[0.005,0.01,0.05,0.1,0.5,1.0]$ \\ \hline 
 $\sigma$ & $[2,4,8]$ \\ \hline 
 Learning rate decay ($\eta_{decay})$ & $0.9$ \\ \hline 
 Clipping value decay ($C_{decay}$) & $0.9$ \\ \hline 
 Batch size ($|S_b|$) & $[64,128,256,512,1028]$\\ \hline 
\end{tabular}
}
\caption{Hyperparameter settings}
\label{tab:hyperparameter}
\end{table}

For the layerwise clipping method, we compute the clipping value for each layer based on the gradient ratio between each layer. Therefore, these clipping values will leak information about the training data if the gradient ratio is computed based on the training data. To address this issue, we randomly split the training dataset $D$ into two subsets:

\begin{itemize}
    \item Subset for training the neural network $D_{train}$: We use this subset to train the neural network model and optimize the model based on the examples in it.
    \item Subset for computing gradient ratios $D_{dummy}$: We use this subset to compute the ratio of gradients by simply inputting it to the current model at a certain epoch and then computing the gradients. Note that we do not update the model using these gradients. In the case of diminishing clipping value $C$, we recompute the clipping value of each layer whenever we update a new value $C$.
\end{itemize}

In our experiments, the size of the training data $|D_{train}|$ is $\frac{9}{10}|D|$, and the size of the dummy data $|D_{dummy}|$ is $\frac{|D|}{10}$.
\subsection{Opacus comparison}
We want to run Opacus and our algorithm under the same privacy and then compare the performance based on the testing accuracy after 50 training epochs.

Since our algorithm \ref{alg:layerwiseDPSGD} is prove to be $G_{\sqrt{gEL}/\sigma}$, we choose the same noise added in the Opacus setting: $\bar{\sigma} = \sigma / \sqrt{L}$. For example, in resnet18 model, there are total 62 layers which have gradients where we add the Gaussian noise. Thus, $\bar{\sigma} = \sigma/\sqrt{62}$.

\textbf{Combining constant clipping value with layerwise batch clipping method:}
In this method, we do not decrease the clipping value $C$ and we only compute the layerwise clipping values $c_i$ for $i$-th layer once before training the model. After running a wide range of $C$, we are able to achieve the closest performance compared to the baseline result. Specifically, we achieved $98.25\%$,$98.07\%$, $97.68\%$ with $\sigma = 2,4,8$ and $C = 0.005$, respectively. This means given a good clipping value $C$, we can use larger Gaussian noise multiplier $\sigma$ without hurting the model's testing accuracy too much. This yields stronger privacy $G_{\sqrt{gEL}/\sigma}$-DP as $\sigma$ increases. However, finding this $C$ value maybe a hard task and it is heavily dependent on the each gradient value in each neuron in the model. The results are shown in figures \ref{fig:MNISTAccuracy_BC_CCV_L} and \ref{fig:CIFAR10Accuracy_BC_CCV_L}

\textbf{Combining diminishing clipping value with layerwise batch clipping method:}
In this experiments, we use the diminishing clipping value which starts with $C$ and then recomputes the layerwise clipping values $c_i$ whenever we update the new $C$. We also achieve the similar results compared with constant clipping value method. This implies that the value $C$ can move along with the trends of gradients without hurting the model's testing accuracy. However, we could not see the different in performance between two methods with MNIST dataset and Lenet-5 model which are the simple dataset and model architecture. The results are shown in figures \ref{fig:MNISTAccuracy_BC_DCV_L} and \ref{fig:CIFAR10Accuracy_BC_CCV_L}

\subsection{MNIST}
   Firstly, we ran our algorithm on a simple dataset, MNIST, which consists of 60,000 training examples and 10,000 testing examples of handwritten digits \cite{Lenet5model}. Each example is a 28x28 gray-level image. For this dataset, we used the LeNet-5 model \cite{Lenet5model}, which has 3 convolutional layers, 2 averaging layers, and 2 fully connected layers. The details of the LeNet-5 architecture are described in Table \ref{tab:Lenet5}. For each training image, we cropped a $32\times32$ image with padding of 4 from it, randomly horizontally flipped the image, and then normalized it with $(mean, std) = ((0.1307,), (0.3081,))$. In our experiments, data augmentation and data normalization were performed independently in each epoch.

\begin{table}[ht]

\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Operation Layer & \#Filters  & Kernel size & Stride & Padding & Output size & Activation function \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $6$ & $5 \times 5$ & $1 \times 1$ & $0$ & $28 \times 28 \times 6$ & $tanh$\\ \hline 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ &  & $14 \times 14 \times 6$ & \\ \hline 
\parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm}}& $16$ & $5 \times 5$ & $1 \times 1$ & $0$ & $10 \times 10 \times 16$ & $tanh$\\ \hline 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ & &  $10 \times 10 \times 16$ &\\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm} }& $120$ & $5 \times 5$ & $1 \times 1$ & $0$ &  $5 \times 5 \times 16$ & $tanh$\\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC1$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ & $84$ & $tanh$\\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC2$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ & $10$ & $softmax$\\ \hline 
\end{tabular}
}
\caption{Lenet-5 model architecture}
\label{tab:Lenet5}
\end{table}

\textbf{Baseline result:}
With our algorithm \ref{alg:layerwiseDPSGD}, we achieved  $98.09\%$ testing accuracy with cross-entropy loss and batch size $|S_b| = 64$ which is consistent with what have shown in \cite{Lenet5model}. 
% Figure environment removed

\textbf{Opacus result:}
For MNIST dataset, we observed a slightly decrease in accuracy when we choose $\sigma = 2,4,8$, respectively. In other words, we run the experiments using Opacus framework with $\bar{\sigma} = \sigma/\sqrt{L} = \bar{\sigma} = \sigma/\sqrt{10}$ (Note that convolution layer and fully connected layer has bias values and they also have the gradient, we count each bias as one layer as well) where $\sigma = 2,4,8$ and achieved the testing accuracy of $89.7\%, 87.67\%$ and $83.74\%$. We can clearly see that the testing accuracy is decrease significantly if we try to increase $\sigma$ which yields better privacy. The result of these experiments are shown in figure \ref{fig:OpacusMNIST}
% Figure environment removed

\textbf{Layerwise Clipping with constant clipping value result:}
On the other hand, with the same privacy where $\sigma = 2,4,8$, we achieved the testing accuracy $98.25\%, 98.09\%, 97.63\%$, respectively. This is the best testing accuracy which reaches the performance of the baseline model where no noise is added. Moreover, the accuracy is maintained while increasing the $\sigma$ value if we could find a suitable clipping value $C$
% Figure environment removed

\textbf{Layerwise Clipping with diminishing clipping value result:}
As explained in previous chapters, this method has the similar idea as diminishing step size. We would want to decrease the clipping norm since the gradients tends to be smaller when reaching the optimal point.
Our results also show that this method can also reach the performance of the base line model: We achieved the testing accuracy of $98.25\%,98.09\%$ and $97.63\%$ for $\sigma = 2,4,8$, respectively. However, we hardly see the different between diminishing clipping value and constant clipping value methods. Therefore, we decided to run our algorithm on more complex dataset and model architectures.
% Figure environment removed


%%%%%%%%%%%%%%----------------------------------------%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CIFAR10}

We conduct similar experiments on the CIFAR-10 dataset as we did on the MNIST dataset. The CIFAR-10 dataset consists of 50,000 training examples and 10,000 test examples, divided into 10 classes, with each example being a 32x32 image with three color channels (RGB) \cite{CIFAR10dataset}. For this dataset, we trained two different models: a convnet with four convolutional layer blocks (each consisting of a convolutional layer with ReLU activation function, followed by an average pool layer with a kernel size of 2x2 and stride value of 2) and one softmax layer, and a ResNet18 \cite{resnet18paper}. We wanted to explore how our method works with both simple and very deep model architectures that have shortcuts, since our work heavily depends on the model architecture with the layerwise clipping method. The details of the convnet and ResNet-18 model architectures are described in Tables \ref{tab:convnet} and \ref{tab:resnet18}, respectively.

For each training image, we crop a $32 \times 32$ region from it with padding of 4, apply a random horizontal flip to the image, and then normalize it with 
\[(mean,std) = ((0.4914, 0.4822, 0.4465), (0.2023,0.1994, 0.2010))\]. In our experiments, we perform data augmentation and data normalization independently in each epoch.
\begin{table}[ht]

\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Operation Layer & \#Filters  & Kernel size & Stride & Padding & Output size & Activation function \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $32$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{2}{*}{$16 \times 16 \times 3$} & ReLu\\ 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ &  &  & \\ \hline 
\parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm}}& $64$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{2}{*}{$8 \times 8 \times 32$} & ReLu\\ 
 $AvgPool2d$&   & $2 \times 2$ & $2 \times 2$ &  &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$\vspace{1mm} }& $64$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{2}{*}{$4 \times 4 \times 64$} & ReLu\\  
 $AvgPool2d$&  & $2 \times 2$ & $2 \times 2$ &  &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $128$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ & \multirow[c]{2}{*}{$1 \times 1 \times 128$} & ReLu\\ 
 $AdaptiveAvgPool2d$&   & $1 \times 1$ & $1 \times 1$ &    &  & \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC2$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ & $10$ & $softmax$\\ \hline 
\end{tabular}
}
\caption{convnet model architecture}
\label{tab:convnet}
\end{table}


\begin{table}[ht]

\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Operation Layer & \#Filters  & Kernel size & Stride & Padding & Output size & Activation function \\ \hline 
 \parbox[c]{3cm}{\vspace{1mm} \centering $Conv2D$ \vspace{1mm}}& $64$ & $7 \times 7$ & $2 \times 2$ & $1 \times 1$ & \multirow[c]{1}{*}{$112 \times 112 $} & ReLu\\ \hline
 $MaxPool2d$&   & $3 \times 3$ & $2 \times 2$ &  &  \multirow[c]{2}{*}{$56 \times 56$} & \\  
 \parbox[c]{5cm}{\vspace{1mm} \centering $2 \times (Conv2D + BatchNorm)$ \vspace{1mm}}& $64$ & $3 \times 3$ & $1 \times 1$ & $1 \times 1$ &  & ReLu\\ \hline
 
 \parbox[c]{5cm}{\vspace{1mm} \centering $2 \times (Conv2D + BatchNorm)$ \vspace{1mm}}& $128$ & $3 \times 3$ & $2 \times 2$ & $1 \times 1$ & $28 \times 28$ & ReLu\\ \hline
 \parbox[c]{5cm}{\vspace{1mm} \centering $2 \times (Conv2D + BatchNorm)$ \vspace{1mm}}& $256$ & $3 \times 3$ & $2 \times 2$ & $1 \times 1$ & $14 \times 14$ & ReLu\\ \hline
 \parbox[c]{5cm}{\vspace{1mm} \centering $2 \times (Conv2D + BatchNorm)$ \vspace{1mm}}& $512$ & $3 \times 3$ & $2 \times 2$ & $1 \times 1$ & $7 \times 7$ & ReLu\\ \hline
 $AvgPool$2D&   & $4 \times 4$ &  &    & \multirow[c]{3}{*}{$1 \times 1$} & \\ 
 \parbox[c]{3cm}{\vspace{1mm} \centering $1000-d\ FC1$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ &  & \\ 
 \parbox[c]{3cm}{\vspace{1mm} \centering $FC2$\vspace{1mm} }& $-$ & $-$ & $-$ & $-$ &  & $softmax$\\ \hline 
\end{tabular}
}
\caption{resnet18 model architecture}
\label{tab:resnet18}
\end{table}

\textbf{Baseline result: }For non-private training, we achieve \textbf{$69.93\%$} and \textbf{$89.57\%$} testing accuracy respectively for convnet and resnet18 with  batch size $64$ and SGD optimizer for our baseline result.

% Figure environment removed

\textbf{Opacus result:}
For CIFAR10, we can see that the model testing accuracy is very bad for convnet and resnet18 models. After 50 training epochs, we can only achieve $\approx 38\%$ and $\approx 48\%$ testing accuracy for convnet and resnet models when choosing $\sigma = 2,4,8$. Here, we have 
$L_{convnet} = 10$ and $L_{resnet} = 62$ and we run the experiments with $\bar{\sigma} = \sigma/\sqrt{L_{convnet}}$  and $\bar{\sigma} = \sigma/\sqrt{L_{resnet}}$ with Opacus framework. The detail of results shown in figure \ref{fig:OpacusCIFAR10}
% Figure environment removed
% We achieve around $\%$, $\%$, $\%$ accuracy with non-private baseline, DP with batch clipping method, DP with batch clipping and diminishing layerwise clipping constant $C_i$ for 50 epochs with batch size of $xxx$ examples; respectively

\textbf{Layerwise Clipping with constant clipping value result:}
As shown in Figure \ref{fig:CIFAR10Accuracy_BC_CCV_L}, the testing accuracy lines are fluctuating more than the Opacus's algorithm. However, although this effects show that complex models are very sensitive with our methods, our algorithm still converge to a good accuracy which can even reach the baseline accuracy. Specifically, we achieve $\approx 68\%$ and $\approx 90.7\%$ testing accuracy for CIFAR10 with convnet and resnet18 models with only 50 training epochs. Similarly to MNIST experiments, even with complex models, our methods still be able to adjust wide range of $\sigma$ value while maintaining the performance. In \cite{vandijk2023generalizing}, we do not have the experiment results for CIFAR10 + resnet18, therefore, we put the $N/A$ value in that cell.
% Figure environment removed

\textbf{Layerwise Clipping with diminishing clipping value result:}
We also try out of diminishing clipping value experiments performs compared to constant clipping value experiments. We observe that the testing accuracy curves becomes smoother and the fluctuation decreased significantly. However, The resnet18 model has the same testing accuracy while convnet model's testing accuracy dropped $\approx 8\%$ down when we switch from constant clipping value mode to diminishing clipping value mode. This implies that this method performs better with deeper neural networks.
% Figure environment removed

\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose a new clipping method which unlock the power of DPSGD. 



