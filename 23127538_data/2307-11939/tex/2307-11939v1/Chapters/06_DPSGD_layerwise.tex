\section{\texorpdfstring{Background on 
%$(\epsilon,\delta)$ and 
$f$-DP}{}} \label{sec:background}

DP literature  first introduced $\epsilon$-DP \cite{dwork2006calibrating}, later relaxed to $(\epsilon,\delta)$-DP \citep{dwork2014algorithmic}.
In order to have a better dependency on group privacy and to improve adaptive composibility, the notion of Concentrated Differential Privacy (CDP)  \citep{CDP} was introduced.
CDP was re-interpreted and relaxed by using Renyi entropy in \citep{BS15} and its authors followed up with the notion of zero-CDP (zCDP) in \citep{zCDP}. This notion admits
simple interpretable DP guarantees for adaptive composition and group privacy. 
After the introduction of $\rho$-zCDP, Renyi DP (RDP) was introduced by \citep{RDP}.
Combining the ideas that give rise to the zCDP and RDP definitions
leads naturally to the definition of $(\rho,\omega)$-tCDP \citep{tCDP} which relaxes zCDP. 
All of these various DP measures have been superseded by $f$-DP \citep{dong2021gaussian}since (1) $f$-DP can be transformed/translated into  divergence based DP guarantees (but generally not the other way around) and can be translated into $(\epsilon,\delta)$-DP, and since (2) $f$-DP analyses the underlying core hypothesis testing problem directly and derives a {\em tight} (or exact) DP guarantee (for the adversarial model considered in the proofs of DP guarantees in literature).
%In this sense 
The $f$-DP framework  is tight and contains all the information needed to derive other known DP metrics. 
Below we summarize $f$-DP and show how to use $f$-DP to prove and formulate the DP guarantee of our modified DPSGD algorithm. 







%%%%%%% BEGIN OLD %%%%%%
%\subsection{\texorpdfstring{$(\epsilon,\delta)$-Differential Privacy}{}} 
%\label{sec:edDP}
%
%Differential privacy \citep{dwork2006calibrating, dwork2011firm,dwork2014algorithmic,dwork2006our} defines privacy guarantees for algorithms on databases. Precisely, this guarantee quantifies into what extent the output of an algorithm can be used to differentiate among two adjacent data sets $d$ and $d'$ (i.e., where one set has one extra element compared to the other set).
%
%\begin{definition} \label{defDP} A randomized mechanism ${\cal M}: D \rightarrow R$ is $(\epsilon, \delta)$-DP (Differentially Private) if for any adjacent $d$ and $d'$ in $D$ and for any subset $S\subseteq R$ of outputs,
%$$ Pr[{\cal M}(d)\in S]\leq e^{\epsilon} Pr[{\cal M}(d')\in S] + \delta,$$
%where the probabilities are taken over the coin flips of mechanism ${\cal M}$.
%\end{definition}
%%%%% END OLD %%%%%%%%%%

% The privacy loss incurred by observing $o$ is given by
% $$ L^{o}_{{\cal M}(d) \| {\cal M}(d')} = \ln \left( \frac{Pr[{\cal M}(d)=o]}{Pr[{\cal M}(d')=o]} \right).
% $$
% As explained in \citep{dwork2014algorithmic} $(\epsilon, \delta)$-DP ensures that
% for all adjacent $d$ and $d'$ the absolute value of privacy loss will be bounded by $\epsilon$ with probability at least $1-\delta$. 
% The larger $\epsilon$ the more certain we are about which of $d$ or $d'$ caused observation $o$. 

%\subsection{\texorpdfstring{$f$-DP Framework}{}} 
%\label{sec:fDP}

%One of major limitations of $(\epsilon,\delta)$-DP is that it does not provide a clean description for privacy guarantee. As an simple example, in many cases we are not able to compare two different pairs of $(\epsilon,\delta)$. Therefore, many different DP notions are introduced such as Renyi-DP~\cite{Mironov2017RenyiDP}, zero Concentrated Differential Privacy DP~\cite{zCDP}, Gaussian DP or $f$-DP~\cite{dong2021gaussian}. 


%$f$-DP is the state-of-the-art DP formulation based on hypothesis testing. Basically, we  formulate this problem based on the attacker point of view who 

We call data sets $d=\{\xi_i\}_{i=1}^N$ and $d'=\{\xi'_i\}_{i=1}^N$ neighboring if they differ in one element;
%in : $N=|d|=|d'|$ and $|d\cap d'|=N-1$.
without loss of generality $\xi_i=\xi'_i$ for $1\leq i\leq N-1$ and $\xi_N\neq \xi'_N$ coined the {\em differentiating sample}.
In DP  a mechanism ${\cal M}$ is a process that takes either data set $d$ or data set $d'$ as input and outputs a sequence of observables which the adversary uses to distinguish which of $d$ or $d'$ has been used.  DPSGD is a mechanism which outputs a sequence of updates $U$  corresponding to each round, see (\ref{algo:DPSGDx}).
Below, we adopt the notion and explanation provided in~\cite{dong2019gaussian}
%vandijk2023generalizing}
for our short introduction of $f$-DP. We refer the reader  to \cite{dong2019gaussian}
%,vandijk2023generalizing} 
for a full description. 

\textbf{Hypothesis Testing:} From the attacker's perspective, it is natural to formulate the  problem of distinguishing two 
neighboring
data sets $d$ and $d'$ based on the output of a DP mechanism ${\cal M}$ as a hypothesis testing problem:
%
%distinguishes neighboring\footnote{$d$ and $d'$ differ in one element: $N=|d|=|d'|$ and $|d\cap d'|=N-1$.} data sets $d$ and $d'$  based on the output of a DP mechanism\footnote{DPSGD is a mechanism which outputs a sequence of updates $U$ corresponding to each round.} ${\cal M}$ as a hypothesis testing problem:
$$\mbox{versus } \begin{array}{l}
H_0: \mbox{ the underlying data set is }d \\
H_1: \mbox{ the underlying data set is }d'.
\end{array}
$$

% Here, neighboring means that either $|d\setminus d'|=1$ or $|d'\setminus d|=1$. 
% More precisely, in the context of mechanism ${\cal M}$, ${\cal M}(d)$ and ${\cal M}(d')$ take as input representations $r$ and $r'$ of data sets $d$ and $d'$ which are `neighbors.' The representations are mappings from a set of indices to data samples with the property that if $r(i)\in d\cap d'$ or $r'(i)\in d\cap d'$, then $r(i)=r'(i)$. This means that the mapping from indices to data samples in $d\cap d'$ is the same for the representation of $d$ and the representation of $d'$. In other words the mapping from indices to data samples for $d$ and $d'$ only differ for indices corresponding to the differentiating data samples in $(d\setminus d')\cup (d'\setminus d)$. In this sense the two mappings (data set representations) are neighbors.
% In our main theorem we will consider the general case $g=\max\{ |d\setminus d'|, |d'\setminus d|\}$ in order to analyse `group privacy.' 



We define the Type I and Type II errors by
$$\alpha_\phi = \mathbb{E}_{o\sim {\cal M}(d)}[\phi(o)]  \mbox{ and } \beta_\phi = 1- \mathbb{E}_{o\sim {\cal M}(d')}[\phi(o)],
$$
where $\phi$ in $[0,1]$ denotes the rejection rule which takes the output of the DP mechanism as input. We flip a coin and reject the null hypothesis with probability $\phi$. The optimal trade-off between Type I and Type II errors is given by the trade-off function
$$ T({\cal M}(d),{\cal M}(d'))(\alpha) = \inf_\phi \{ \beta_\phi \ : \ \alpha_\phi \leq \alpha \},$$ 
for $\alpha \in [0,1]$, where the infimum is taken over all measurable rejection rules $\phi$. If the two hypothesis are fully indistinguishable, then this leads to the trade-off function $1-\alpha$. We say a function $f\in [0,1]\rightarrow [0,1]$ is a trade-off function if and only if it is convex, continuous, non-increasing, at least $0$, and $f(x)\leq 1-x$ for $x\in [0,1]$. 
%
We define a mechanism ${\cal M}$ to be $f$-DP if $f$ is a trade-off function and for all neighboring $d$ and $d'$,
$$
 T({\cal M}(d),{\cal M}(d')) \geq f.
$$
%
% The $f$-DP framework supersedes all existing other frameworks in that a trade-off function contains all the information needed to derive known DP metrics such as $(\epsilon,\delta)$-DP and divergence based DPs. 
%
%\citep{dong2021gaussian} defines Gaussian DP as a special case of $f$-DP where $f$ is a trade-off function
%$$G_\mu(\alpha) = T({\cal N}(0,1),{\cal N}(\mu,1))(\alpha) = \Phi( \Phi^{-1}(1-\alpha) - \mu )$$
%with $\Phi$ the standard normal cumulative distribution of ${\cal N}(0,1)$. 
%
%--------------
%

\textbf{Gaussian DP:}
Gaussian DP is defined as a special case of $f$-DP where $f$ is  defined as a trade-off function
$$G_\mu(\alpha) = T({\cal N}(0,1),{\cal N}(\mu,1))(\alpha) = \Phi( \Phi^{-1}(1-\alpha) - \mu ),$$
for some $\mu\geq 0$, where $\Phi$ is the standard normal cumulative distribution of ${\cal N}(0,1)$. 
%We define a mechanism to be $\mu$-Gaussian DP if it is $G_\mu$-DP. 
%Corollary 2.13 in \citep{dong2021gaussian} shows  that a mechanism is $\mu$-Gaussian DP if and only if it is $(\epsilon, \delta(\epsilon))$-DP for all $\epsilon\geq 0$, where
%\begin{equation} \delta(\epsilon) = \Phi(-\frac{\epsilon}{\mu}+\frac{\mu}{2}) - e^{\epsilon} \Phi(-\frac{\epsilon}{\mu}-\frac{\mu}{2}).
%\label{eq:gdp}
%\end{equation}
%
Suppose that a mechanism ${\cal M}(d)$ computes some function $u(d)\in \mathbb{R}^n$ and adds Gaussian noise ${\cal N}(0,(c\sigma)^2{\bf I})$, that is,  the mechanism outputs $o\sim u(d)+{\cal N}(0,(C\sigma)^2{\bf I})$. Suppose that $c$ denotes the sensitivity of function $u(\cdot)$, that is, $$\|u(d)-u(d')\|\leq c$$ 
for neighboring $d$ and $d'$; the mechanism corresponding to one round update in DPSGD, where ${\tt Sample}_m(N)$ selects (as one of the $m$ randomly selected indices) the index $N$ of the differentiating sample, 
has {\em sensitivity} $c=2C$. After projecting the observed $o$  onto the line that connects $u(d)$ and $u(d')$ and after normalizing by dividing by $c$, we have that differentiating whether $o$ corresponds to $d$ or $d'$ is in the best case for the adversary (i.e., $\|u(d)-u(d')\|=c$) equivalent to differentiating whether a received output is from ${\cal N}(0,\sigma^2)$ or from  ${\cal N}(1,\sigma^2)$. Or, equivalently, from ${\cal N}(0,1)$ or from  ${\cal N}(1/\sigma,1)$. 
This is how the Gaussian trade-off function $G_{\sigma^{-1}}$ comes into the picture. 

\textbf{Subsampling:} Besides implementing Gaussian noise which bootstraps DP,   DPSGD also uses ${\tt Sample}_m$ for subsampling  which amplifies DP.
\citep{dong2021gaussian} defines a subsampling operator $C_{m/N}$ and  shows that if a mechanism ${\cal M}$ on data sets of size $N$ is $f$-DP, then the subsampled mechanism ${\cal M}\circ {\tt Sample}_{m}$ is $C_{m/N}(f)$-DP. We have that the mechanism corresponding to one round in DPSGD is $C_{m/N}(G_{1/\sigma})$-DP (and this is a tight analysis).

\textbf{Composition:} If DPSGD implements $T$ rounds, then the privacy leakage across rounds composes. 
\citep{dong2021gaussian} defines a commutative tensor product $\otimes$ for trade-off functions and shows this can be used to characterize adaptive composibility:
Let ${\cal M}_i$ be the mechanism corresponding to the $i$-th round with $y_i \leftarrow {\cal M}_i(\texttt{aux},d)$ where $\texttt{aux}=(y_1,\ldots, y_{i-1})$ (this captures adaptivity). If ${\cal M}_i(\texttt{aux},.)$ is $f_i$-DP for all $\texttt{aux}$, then the composed mechanism ${\cal M}$, which applies ${\cal M}_i$ in sequential order from $i=1$ to $i=T$, is 
$(f_1\otimes \ldots \otimes f_T)$-DP.
%$f^{\otimes T}$-DP.
%The tensor product is commutative.
This leads to a tight analysis of DPSGD.
We have  that DPSGD 
%(with subsampling, individual clipping and mini-batch SGD) 
as introduced in \cite{abadi2016deep} 
%with (the slightly more general) update $U$ defined in (\ref{eq:ind}) before adding Gaussian noise
%in Algorithm \ref{alg:??} [TO DO: work with $2\sigma$ in pseudo codes as in $f$-DP paper] 
is
%[TO DO -- we now use $|S_b|=ms$ instead of $s$ here]
%\footnote{TO DO: In appendix explain where DP-SGD paper goes wrong. Their DP-SGD algorithm uses noise ${\cal N}(0,C^2(2\sigma)^2 {\bf I})$ compared to ${\cal N}(0,C^2\sigma^2 {\bf I})$ in our version of the DP-SGD algorithm.}
$$ C_{m/N}(G_{1/\sigma})^{\otimes T}\mbox{-DP}.$$
Notice that if DPSGD computes a total of $E$ epochs of gradients, i.e., $EN$ gradient computations in total, then $T=(N/m)\cdot E$ (since each round computes $m$ gradients). We have
\begin{equation} C_{m/N}(G_{1/\sigma})^{\otimes (N/m)\cdot E}\mbox{-DP}.
\label{eq:DPIC}
\end{equation}

\section{Application $f$-DP toward a Modified DPSGD}
\label{sec:modifiedDPSGD}

We discuss two main  DPSGD modifications. The first is coined Batch Clipping (BC) and the second Layerwise Clipping (LC) leading to Adaptive LC (ACL).

\subsection{Batch Clipping}
\label{sec:BC}

The presented $f$-DP analysis is more general in that it holds for (\ref{algo:DPSGDx}) where $U$ is not just computed as a noised sum of clipped {\em gradients} but  computed as
\begin{equation}
U := n+\sum_{j=1}^m [g(w;\xi_{i_j})]_C
\label{eq:g}
\end{equation}
for some other fixed function $g$. 

Suppose that we partition data set $d$ of size $N$ into $N/s$ mini-sets of size $s$ each. We use this to define a new data set $d_s$ which has as elements the $N/s$ mini-sets, which we denote as $S_i$, $1\leq i\leq N/s$. Data set $d_s$ has size $N/s$ and its samples are mini-sets $S_i$ of size $s$. Each $S_i$ contains $s$ data points $\xi_j$ from $d$.
We apply DPSGD  to this new data set $d_s$ for the general (\ref{eq:g}) where we replace $m$ by $k$. This yields 
\begin{equation}
U := n+\sum_{j=1}^k [g(w;S_{i_j})]_C.
\label{eq:g1}
\end{equation}
%where mini-sets $S_{i_j}$ have size $s$. We call this General Clipping (GC).
We call this General Batch Clipping (GBC) since we clip vectors $g(w;S_{i_j})$ which are computed based on a batch (mini-set) $S_{i_j}\subseteq d$ of data points from $d$. Notice that in GBC, $g$ can implement any moment based SGD type algorithm that iteratively scans the data points in $S_{i_j}$.
%(while treating $w$ as a local variable within $g$'s evaluation and  updating the local $w$ from).

Applying the $f$-DP analysis for DPSGD for a data set $d_s$ of size $N/s$ with ${\tt Sample}_k$, see (\ref{eq:g1}), yields
$C_{k/(N/s)}(G_{1/\sigma})^{\otimes T}\mbox{-DP}$, see Section \ref{sec:background}. By setting $m=sk$, we conclude $C_{m/N}(G_{1/\sigma})^{\otimes T}\mbox{-DP}$.
Notice that if the modified DPSGD with GBC computes a total of $E$ epochs of gradients, then again $T=(N/m)\cdot E$ since each round still computes $m=sk$ gradients. For GBC with $m=sk$ we conclude the exact same DP guarantee as the one for 
%IC in 
(\ref{eq:DPIC}). 


In our experiments we use the special case
\begin{equation} g(w;S_{i_j}) = \frac{1}{s} \sum_{\xi \in S_{i_1}} \nabla_w
%% Do not change to f_w !!!!!!!
f(w;\xi) \label{eq:g2}
\end{equation}
with $k=1$ 
%in (\ref{eq:g1}) 
and $s=m$ in GBC (with $m=sk$).
We refer to this as (non-general) Batch Clipping (BC) (since we clip $g(w;S_{i_1})$ which computes and averages a batch of gradients), see (\ref{algo:DPSGDxBC}). We call the original DPSGD formulation (\ref{algo:DPSGDx}) Individual Clipping (IC) since single/individual gradients are clipped; for completeness, this is the case $s=1$ with $k=m$ in GBC (with $m=sk$).



%When referring to BC in our experiments, we consider the special case $k=1$ with $s=m$, that is, (\ref{eq:g1}) consists of a single computation (\ref{eq:g2}).


\subsection{Layerwise Clipping}
\label{sec:layerwiseclipping}

%Based on the main idea in we propose a 
Layerwise Clipping (LC)~\cite{mcmahan2018learning,van2018three,Xu2021,Zhang2018CoRR} splits vectors $g=g(w;S_{i,j})$ in parts $g=(g_1 | \ldots | g_L)$ (i.e., $g$ is equal to the concatenation of parts $g_1$, $\ldots$, $g_L$) and clip each part $g_h$ separately, i.e., we define
$$
[g]_{(C_1,\ldots,C_L)}=
([g_1]_{C_1} | \ldots | [g_L]_{C_L}).
$$
The different $g_i$ correspond to the different layers in the neural network. We compute noise $n$ in (\ref{eq:g1}) as the concatenation
$$
n = (n_1 | \ldots | n_L) \ \ \ \ \mbox{ with } \ \ \ \ n_h\sim {\cal N}(0,(2C_h\sigma){\bf I}),
$$
where the different matrices ${\bf I}$ have sizes that correspond to the number of entries in the various $g_h$. 

In order to understand how the DP guarantee is affected, we rewrite (\ref{eq:g1}) as follows:
$$
U:=(U_1 | \ldots | U_L) \ \ \ \ \mbox{ where } \ \ \ \ U_h=n_h + \sum_{j=1}^k [g_h(w;S_{i_j})]_{C_h}.
$$
In other words, transmission of $U$ is equal to transmitting $U_h$, $1\leq h\leq L$. Each $U_h$ can be considered as a round update where we use clipping constant $C_h$ and noise ${\cal N}(0,(2C_h\sigma){\bf I})$. The $f$-DP analysis shows that such a round is $G_{1/\sigma}$-DP (see the explanation of Gaussian DP with sensitivity $c=2C_h$). We have $L$ such sub-rounds that make up the whole transmission of $U$. By composition, we have that this gives $G_{1/\sigma}^{\otimes L}$-DP. \citep{dong2021gaussian} shows that\footnote{We can also vary the noise from layer to layer and use $\sigma/p_h$ instead of $\sigma$. This leads to $G_{p_1/\sigma}\otimes \ldots \otimes G_{p_L/\sigma}=G_{ \sqrt{\sum_{h=1}^L p_h^2}/\sigma}$.} $G_{1/\sigma}^{\otimes L}=G_{\sqrt{L}/\sigma}$.

We conclude that in the DP guarantee (\ref{eq:DPIC}) we need to replace $\sigma$ by $\sigma/\sqrt{L}$ for the modified DPSGD with BC/IC and LC. If we want to compare this in a fair way with the modified DPSGD with BC/IC and {\em no} LC (we keep the original gradient clipping approach), then we should use 
$$ \bar{\sigma}:=\sigma/\sqrt{L}$$
as the privacy parameter $\sigma$ in the modified DPSGD (as this will result in the same (\ref{eq:DPIC}) with $\sigma$ replaced by $\bar{\sigma}=\sigma /\sqrt{L}$).

Clearly, we want to be careful about the number of layers $L$ we can handle since the privacy parameter $\sigma$ is divided by $\sqrt{L}$. We notice that we can group layers together and split vectors $g=g(w;S_{i,j})$ into a smaller number of parts giving a smaller $L$ which leads to a better DP guarantee.

%Nevertheless, it is possible to assign separate clipping constants to smaller groups of weight entries in each layer -- thereby effectively increasing $L$ in our analysis.

% \textbf{Motivation:} \textcolor{red}{[TO DO: compress these two paragraphs -- probably removing the first altogether; For space, put the graphs in the appendix?]}
% %The heart of DPSGD is Equation~\ref{algo:DPSGDx} and it is briefly explained in Section~\ref{section:Introduction}. 
% If DPSGD converges, the direction information in each entry of $U=\sum_{i=1}^m[\nabla f(w, \xi)]_C$ becomes smaller and smaller throughout the training process. Therefore, keeping $C$ or adding the same amount noise to all entries of the sum of clipped gradients hurts the convergence of DPSGD and testing prediction accuracy of final model $w$. If $C$ and $\sigma$ are chosen small, then the privacy guarantee is very poor. Event if we accept the small $C$ and $\sigma$, the $U$ may still not be useful especially the network is deep and wide. The reason is that if $C$ is small, then $U$ contain only tiny useful direction information. Therefore, the convergence of DPSGD is slower or more training rounds are needed and it again ends up with poorer privacy guarantee. All of the mentioned reasons limits the effectiveness of DPSGD. Actually, the problem is still more complex or worse when DPSGD applies to deep neural networks. We explain this fact in next section. 

 % % Figure environment removed

%We are motivated by the observation and analysis in~\citep{KaiHu2022}. Basically, the authors 

% \citep{KaiHu2022} shows that the magnitude of gradients of different layers in deep neural networks are very different depending on the network architecture. 
% %This means the direction information in each entry is very different from each other. 

% We plot the gradient norm and gradient average norm at each layer of resnet-18~\citep{resnet18paper} with CIFAR10~\citep{CIFAR10dataset} in Figure~\ref{fig:gradientgraph}. Figure~\ref{fig:gradientgraph} shows there are many layers which have %comparatively 
% very small norm compared to other ones and the norms vary layer by layer. Therefore, if these entries are added same amount noise, then the convergence of training algorithm seriously gets hurt and it may explain why the accuracy of deep neural networks trained by DPSGD degenerate when the size of the model increases. Moreover, the norm of gradients is larger when the model becomes larger. Therefore, the norm of gradients is larger and the direction information of each entry becomes much smaller if we clip them with the same clipping constant $C$. Clearly, DPSGD imposes a very tight restriction on the training setup to get a model with high accuracy at the cost of small privacy leakage. This motivates us to propose a new clipping method called dynamic clipping method and discussed in next sections. 

\subsection{Adaptive LC}
\label{subsec:adaptiveclipping}

% Figure environment removed

% Tune C initially -- can make it pretty small
% C_i are decaying -- improves over Zhang
% Zhang multiplies norm with sigma --> large noise --> bad
% WE decaying C_i, hence
% less absolute noise, but the same relative noise
% sigma smaller than 60 no difference

The Adaptive LC (ALC) of~\cite{Zhang2018CoRR} uses a public dataset $\mathcal{D}_{pub}$ to estimate expectations of the layer gradient norms $\|\nabla_{w_h} f(w;\xi)\|$. These estimates are used as the layerwise clipping constants $C_h$.
%
%$C_1, \dotsc, C_L$. See 
%Figure~\ref{fig:resnet18gradientnorm},  our main observation is that the 
For standard SGD without DP, \cite{KaiHu2022} explains that even if model $w$ converges, the  gradient norms of different layers may not decrease throughout the training. We confirm this in
Figure~\ref{fig:resnet18gradientnorm}, which depicts layer gradient norms of resnet-18 trained without DP over 50 epochs. We observe that most layer gradient norms only slowly decrease from epoch to epoch.   Therefore, most of the $C_h$ in the ALC of~\cite{Zhang2018CoRR}
are only slightly adapted from epoch to epoch.
%which adapts each $C_i$ according to  the estimated expectation of the corresponding layer gradient norm,
%most of $C_1, \dotsc, C_L$ do not change much from epoch to epoch
This implies that, for such a layer $h$, the distribution ${\cal N}(0,(2C_h\sigma)^2{\bf I})$ of the Gaussian noise added to each of the weight entries in layer $h$  does not change much either. This setup does not allow the designer to optimize the clipping constants $C_1, \dotsc, C_L$ to make  convergence faster.

%This may hurt the convergence of DPSGD when $\sigma$ is large ($>60$) as shown in our experiments in Figure~\ref{fig:ZhangversusUs} in Section~\ref{subsec:ZhangUs}. 

We enhance the adaptive clipping strategy of ~\cite{Zhang2018CoRR}:
%from round to round proposed in in our modified DPSGD, 
%which consists of two parts. 
First, we determine a master clipping constant $C$ for each round. We do not impose a restriction on how $C$ is chosen, i.e., it can diminish from epoch to epoch  or $C$ can be the same constant for all rounds.  In our experiments $C$ is a constant throughout the whole training. Second, we use $C$ to derive clipping values $(C_1, C_2, ..., C_L)$ for the corresponding round. This has the property that each $C_h$ scales linearly with $C$.

Given a master clipping constant $C$ at the beginning of each round, we use a {\em public} dataset $\mathcal{D}_{pub}$ to derive clipping constants $(C_1, C_2, ..., C_L)$.
The reason for using a public dataset is that we do not need to worry about privacy leakage revealed by $(C_1, C_2, ..., C_L)$; DP analysis/proofs, where the adversary knows $(C_1, C_2, ..., C_L)$, may proceed as before. We estimate the expectation $e=(e_1, \ldots, e_L)$ of the layer gradient norms $(\|\nabla_{w_1} f(w;\xi)\|, \ldots, \|\nabla_{w_L} f(w;\xi)\|)$ over $\xi \in \mathcal{D}_{pub}$ for $w=(w_1 | \ldots | w_L)$. We compute the maximum gradient norm among all layers, i.e., $M = \max_{h=1}^{L} e_h$. Then, for each layer $h$ we define $C_h = C \cdot e_h/M$. See Section \ref{sec:Zhang}, Figure~\ref{fig:ZhangversusUs} shows that our enhanced ALC allows DPSGD with BC to converge faster to a higher accuracy compared to DPSGD with BC and  the original ALC of ~\cite{Zhang2018CoRR}. 



%Note that we compute these clipping values ${C_i}$ based on the dataset $D$, which gives no information about the training dataset.

%Let's $x = \nabla f_w (w;\xi)$ and $x_{i}$ is the gradient at layer $i$ and $x_{i,j}$ is the entry $j$ at entry $j$ of layer $i$ where $i \in [1,L]$. For each layer $i$, we clip its entry $x_{i,j}$ using the norm of layer $i$ and clipping constant $C_i$ as follows. 
%\[[x_{i,j}]_{C_i} = x_{i,j}/\max(1,\frac{\|x_{i,j}\|}{C_i}).\]
%Therefore, we can denote the aggregated clipped $[x_{i,j}]_{C_i}, \forall (i,j)$ by $[x]_C$ to keep the notation of clipped gradients consistent to the one in DPSGD in Equation~\ref{algo:DPSGDx}.   

%\section{Related Work}

%\textbf{Related Work:}
%Per-layer clipping is introduced in DP-FEDAVG \citep{mcmahan2018learning} where the clipping budget is distribute equally among all model layers and proves privacy guarantee based on moment accountant. \cite{van2018three} proposed the adaptive clipping method where each layer in the neural network has its own adaptive clipping constant $C_i$. The layer clipping constants $C_i$ are computed privately before each new round, i.e., these clipping constants $C_i$ are computed based on the layer gradient norms from previous round added to a Gaussian noise. DPSGD-F \citep{Xu2021} calculates the layerwise clipping constants $C_i$ proportional to the ratio of gradients exceeding a threshold.

%% \textcolor{red}{[TO DO: finish table and text -- do we need a table or just words? -- in table one column for $D$, wher we mention public or private -- by DP proof/analysys you mean Y or N?]}


%% Current works on differentially private machine learning can be categorized into three main parts where differential privacy mechanisms are applied on: 1) input data, 2) output model queries, 3) inner model components. In the input data approach, the noise is added directly to the training data before feeding them to the model in the model training process. In the output model queries approach, the noise is added to the model after the training process finishes. On the other hand, the inner model components approach injects the noise during the model training process. For example, DPSGD \cite{abadi2016deep} clips and adds the noise to the gradient of each training step or the objective function is modified in \cite{Chaudhuri2011}.

%% However, adding noise to either the input data, model output or inner model components hurts the performance. Therefore, several works study has been published in order to improve learning accuracy and robustness of the differentially private machine learning models while maintaining the same or better privacy compared to previous works. 


%\begin{table}[ht]
%\centering
%\scalebox{0.8}{
%    \begin{tabular}{|c|c|c|c|}
%    \hline
%         & Decaying $C_{master}$ & Adaptive layerwise $C_i$ & DP Proof~$^{1}$ \\ \hline
%         \cite{mcmahan2018learning}& $\times$& $\times$ &  $\times$  \\ \hline
%         \cite{van2018three}& $\times$ & \checkmark &  $\times$ \\ \hline
%        \cite{Xu2021}& $\times$& \checkmark &  $\times$ \\ \hline
%         our & \checkmark& \checkmark&   \checkmark \\ \hline
%          \multicolumn{4}{l}{$^{1}$\footnotesize{we only consider DP proof for adaptive layerwise $C_i$ where $C_i$ is recomputed during the training process}}
%    \end{tabular} 
%}
%\caption{Comparison between existing adaptive clipping methods }
%\label{tab:my_label}
%\end{table}

%\cite{van2018three} proposed an adaptive 

%For completeness, we are the first providing the DP analysis of layerwise clipping in Section \ref{sec:layerwiseclipping} (and we are the first to introduce batch clipping in DPSGD together with its DP analysis).



\subsection{Sparsification}
\label{sec:sparse}

We notice that the public data set $\mathcal{D}_{pub}$ in adaptive clipping can also be used to find out whether certain weight entries in $w$ have converged sufficiently. That is, we say a weight entry has converged if, over a large number of recent rounds, it hoovers around an average with standard deviation corresponding to the added DP noise. As soon as this is the case, we may fix the weight entry to this average in all future computations/rounds (since this is a form of post processing, no additional DP leakage occurs). This reduces the number of weight entries over which we need to compute gradients and each level gets less weight entries as soon as convergence sets in. This allows us to use even smaller clipping constants per layer (depending on the number of active weight entries in the layers). We leave this optimization as an open problem.
%because without this optimization we already achieve good experimental results.


%extension to the whole modified DPSGD

%privacy parameters

%We have
%$$ C_{m/N}(G_{1/\sigma})^{\otimes (N/m)\cdot E}\mbox{-DP}.$$


%------------------



%Corollary 3.3 in \citep{dong2021gaussian} states that composition of multiple Gaussian operators $G_{\mu_i}$ results in $G_{\mu}$ where $\mu=\sqrt{\sum_i \mu_i^2}$. Moreover, suppose that $d$ and $d'$ do not differ in just one sample, but differ in $g$ samples. \citep{dong2021gaussian} shows that if a mechanism is $G_\mu$-DP, then it is $G_{g\mu}$-DP for groups of size $g$. This shows a linear dependency in $g$.

%\citep{dong2021gaussian} described the privacy accountant for DPSGD where the subsampling is a nice approach to amplify the privacy. As stated by the authors, there is no closed formula for computing the composition theory. Recently, the authors in~\cite{vandijk2023generalizing} provide a new DPSGD framework which allows us to run \textit{any} first order gradient optimizer with a nice closed formula for the composition theory. If DPSGD uses constant $C$ to bound the sensitivity of the gradients, i.e., $\|U-U'\|\leq C$ where $U,U'$ are two computed gradients, add the noise with variance $(2C\sigma)^2$ into each entry and running $E$ epochs, then it has $G_{\sqrt{gE}\mu}$-DP for groups of size $g$ where $\mu = 1/\sigma$. Since this framework is state-of-the art one, We will do experiment with this framework to show the efficiency of our new clipping method and their framework.

%\subsection{DPSGD}

% \begin{algorithm}[h]
% \caption{Differential Private SGD}
% \label{alg:DPSGDs}
% \begin{algorithmic}[1]
% \Procedure{DP-SGD}{}
%     \State $N=$ size training data set $d=\{\xi_i\}_{i=1}^{N}$
%     \State $E=$ total number of epochs
%     \State step size sequence $\{\eta_i\}$
%     \State initialize $w$ as the default initial model
%     \For{$e\in \{1,\ldots, E\}$}
%     \State $\{S_{b}\}_{b=1}^{N/m}\leftarrow$ ${\tt Sample}_{m}$ with $S_{b}\subseteq \{1,\ldots, N\}$, $|S_{b}|=m$
%     \For{$b\in \{1,\ldots, \frac{N}{m}\}$}
%  \State Start of round $(e-1)\frac{N}{m}+b$:
%     \For{$h\in S_b$}
%     \State 
%     $a_h= \nabla_w f(w;\xi_h)$
%     \EndFor
%     \State $U=\sum_{h=1}^m [a_h]_C$
%     \State $\bar{U}\leftarrow U+ {\cal N}(0,(2C\sigma)^2{\bf I})$
%     \State Locally update $w\leftarrow w- \eta_{(e-1)\frac{N}{m}+b} \cdot \bar{U}/m$
%     \EndFor
%     \EndFor
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% Algorithm~\ref{alg:DPSGDs} is the original DPSGD proposed in~\cite{abadi2016deep} with a slight modification to make it adapted to $f$-DP~\cite{dong2019gaussian,vandijk2023generalizing}. Rather than using the gradient $\nabla f(w, \xi)$ itself, DP-SGD uses its clipped version $[\nabla f(w, \xi)]_C$ where $[x]_C= x/\max\{1,\|x\|/C\}$. 


%The heart of DPSGD is Equation~\ref{algo:DPSGDx} and it is briefly explained in Section~\ref{section:Introduction}. If DPSGD converges, the direction information in each entry of $U=\sum_{i=1}^m[\nabla f(w, \xi)]_C$ becomes smaller and smaller throughout the training process. Therefore, keeping $C$ or adding the same amount noise to all entries of the sum of clipped gradients hurts the convergence of DPSGD and testing prediction accuracy of final model $w$. If $C$ and $\sigma$ are chosen small, then the privacy guarantee is very poor. Event if we accept the small $C$ and $\sigma$, the $U$ may still not be useful especially the network is deep and wide. The reason is that if $C$ is small, then $U$ contain only tiny useful direction information. Therefore, the convergence of DPSGD is slower or more training rounds are needed and it again ends up with poorer privacy guarantee. All of the mentioned reasons limits the effectiveness of DPSGD. Actually, the problem is still more complex or worse when DPSGD applies to deep neural networks. We explain this fact in next section. 

%\subsection{Norms of Layers of Deep Neural Network}
%\label{subsec:normGrad}



%\section{Layerwise Clipping for DPSGD}
%\label{section:layerwiseclipping}
% \begin{algorithm}[t]
% \caption{%Differential Private SGD -- 
% Layerwise Clipping DPSGD}
% \label{alg:layerwiseDPSGD}
% \begin{algorithmic}[1]
% %\Procedure{LocalSGDwithDP}{$d,C,s,\sigma,T$}
% \Procedure{DP-SGD-General}{}
%    \State \textbf{Input:} Dataset size $N$,dataset $(D=\{\xi_i\}_{i=1}^{N})$, number of epochs $(E)$, diminishing round step size sequence $\{\eta_i\}_{i=1}^{T}$, the default initial model $(w)$, Gradient clipping value sequence $\{C\}_{i=1}^{T}$, noise scale $\sigma$
%    \State ${D_{train} = =\{\xi_i\}_{i=1}^{P},D_{dummy}=\{\xi_i\}_{i=1}^{Q}} \leftarrow RandomSplit(D,p)$ \Comment{$P+Q = N$}
%    % \State ${c_1,c_2,\dots,c_L} \leftarrow ComputeLayerwiseC(D_{dummy},w,C_0)$ 
%     \For{$e\in \{1,\ldots, E\}$}
    
%     \State Let $\pi^e$ be a random permutation 
%     \State re-index data samples: $\{\xi_i\leftarrow \xi_{\pi^e(i)}\}_{i=1}^P$  
%     \State $\{S_{b,h}\}_{b=1,h=1}^{P/(ms),m}\leftarrow$ ${\tt Sample}_{s,m}$ with $S_{b,h}\subseteq \{1,\ldots, P\}$, 
%     \Comment{ $|S_{b,h}|=s$, $|S_b|=sm$ with $S_b=\bigcup_{h=1}^m S_{b,h}$ }
%     \State ${C_1,C_2,\dots,C_L} \leftarrow ComputeLayerwiseC(D_{dummy},w,C_e)$
%     \For{$b\in \{1,\ldots, \frac{P}{ms}\}$}
% %    \State Set $w$ equal to last received global model
%  %   \State $U=0$
%  \State Start of round $(e-1)\frac{P}{ms}+b$:
%     \For{$h\in \{1,\ldots,m\}$}
%     \State $a_h = [a_{h,1},a_{h,2},\dots,a_{h,L}]\leftarrow {\cal A}(w,\{\xi_i\}_{i\in S_{b,h}})$
%     \State $[a_h]_C =\left\{[a_{h,i}]_{C_i}\right\}_{i=1}^{L}$ \Comment{Layerwise Clipping gradient}
%  %   \State $U\leftarrow U+[a]_C$
%     \EndFor
%     \State $U=\sum_{h=1}^m [a_h]_C =\left\{\sum_{h=1}^m [a_{h,i}]_{C_i}\right\}_{i=1}^{L}$
%     \State $\bar{U}\leftarrow U+ \left\{{\cal N}(0,(2C_i\sigma)^2{\bf I})\right\}_{i=1}^{L} = \left\{\sum_{h=1}^m [a_{h,i}]_{C_i} + {\cal N}(0,(2C_i\sigma)^2{\bf I}) \right\}_{i=1}^{L}$ \Comment{Add layerwise Noise}
%     \State $w\leftarrow w- \eta_{(e-1)\frac{N}{ms}+b} \cdot \bar{U}/m$
     
%     \EndFor
%     \EndFor
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

%The key observations for motivating the new clipping methods are as follows. First, in each training round/iteration the constant $C$ only plays the role of scaling factor or it is used to bound the sensitivity of computed gradients while $\sigma$ represents the privacy guarantee. Therefore, we can vary $C$ round by round. As argued in Section~\ref{subsec:normGrad}, the norm of gradients at each layer varies layer by layer, we want to have different constants $C_i$ for different layer $i$ based on its norm. 
%Note that we can generalize this ideas as each weight $w_{i,j}$ can have its own clipping constant $C_{i,j}$ where $w_{i,j}$ is the weight of model $w$ at layer $i$ and coordinate $j$ in that layer. However, we just stop at layer-level constant $C_i$ rather than entry-level $C_{i,j}$ due to privacy guarantee reason as explain in next section. 


% Firstly, our intuition is to enhance the testing accuracy of the model by minimizing the Gaussian noise added to each layer while maintaining the same level of privacy as the original clipping method in \cite{vandijk2023generalizing}, \cite{abadi2016deep}, and \cite{dong2019gaussian}. From Figure \ref{fig:gradientgraph}, we observe that the gradients at each layer have different norms, which we use to decide whether to clip the gradient or not. Furthermore, different types of layers have significantly different gradient norms. For example, the gradient norm of the convolutional layer is more than $100\times$ the norm of the gradients in the batch normalization layer. Therefore, if we add Gaussian noise $N(0,(2C\sigma)^2I)$ to the gradient of each layer, it is impossible to choose $C$ in order to balance the noise added to these two types of layers. We believe this is the main reason why deep learning neural networks with many different types of layers and shortcuts suffer a significant decrease in testing accuracy when applying differential privacy in the training process.
 
%Our new clipping method is called dynamic clipping one. Before each training round, we first determine a proper master clipping constant $C$ and use it to derive clipping values $(C_1, C_2, ..., C_L)$ for each layer in the model $w$'s architecture with $L$ layers, along with the corresponding Gaussian noise for each layer's gradient. For the sake of explaination, we first discuss how to dynamically derive clipping constants $(C_1, C_2, ..., C_L)$ based on master clipping constant $C$ and then how to determine $C$ dynamically. 

% \subsection{Sampling}
% In this algorithm, we first randomly split the training dataset $D$ into two disjoint datasets: $D_{train} = {\xi_i}_{i=1}^{P}$ and $D_{dummy} = {\xi_i}{i=1}^{Q}$, with sizes $P$ and $Q$, respectively. The purpose of this step is to compute the layerwise clipping values $(C_1,C_2,...,C_L)$ without revealing information about the data used to train the model $w$. After that, we sample batches ${S{b,h}}$ from the $D_{train}$ dataset and use them to train our model. The sampling method is the same for both datasets $D_{train}$ and $D_{dummy}$ in order to have similar gradient patterns.

%\subsection{Layerwise Clipping}
%Given master constant $C$ at the beginning of each round, we use a public dataset $D$ to derive clipping constants $(C_1, C_2, ..., C_L)$. In other words, the dataset $D$ must be known and given to everybody. Now, we compute the full gradient $g$ of the loss function $f$ given model $w$ and $D$ and $g=(g_1,\dotsc,g_L)$ where $g_i$ is the gradient at the layer $i$. Next, we compute the maximum gradient norm among all layers, i.e., $G = \max_{i=1,\dotsc,L} \|g_i\|$. Then, for each layer $i$ we define $C_i = C \times \frac{\|g_i\|}{G}$.

%Note that we compute these clipping values ${C_i}$ based on the dataset $D$, which gives no information about the training dataset.

%Let's $x = \nabla f_w (w;\xi)$ and $x_{i}$ is the gradient at layer $i$ and $x_{i,j}$ is the entry $j$ at entry $j$ of layer $i$ where $i \in [1,L]$. For each layer $i$, we clip its entry $x_{i,j}$ using the norm of layer $i$ and clipping constant $C_i$ as follows. 
%\[[x_{i,j}]_{C_i} = x_{i,j}/\max(1,\frac{\|x_{i,j}\|}{C_i}).\]
%Therefore, we can denote the aggregated clipped $[x_{i,j}]_{C_i}, \forall (i,j)$ by $[x]_C$ to keep the notation of clipped gradients consistent to the one in DPSGD in Equation~\ref{algo:DPSGDx}.   

% Finally, we aggregate the clipped gradient $[a_h]_C = {[a_{h,i}]_{C_i}}_{i=1}^L$ to a sum $U$:
% \[U=\sum_{h=1}^m [a_h]_C =\left\{\sum_{h=1}^m [a_{h,i}]_{C_i}\right\}_{i=1}^{L}\]
% \subsubsection{Gaussian Noise}

% After computing $U$, we add layerwise Gaussian noise $N(0,(2c_i\sigma)^2)$ to each gradient value in layer $i$. We then use the resulting value $\bar{U}$ to update the model $w$.

%\subsection{Diminishing Clipping Value}

%Moreover, as we move towards an optimal point, the gradients become smaller. Therefore, we use a diminishing value for the clipping constant $C$, similar to the diminishing step size $\eta$, to further improve testing accuracy. Note that the layerwise clipping values are computed based on $C$, which means that we need to recompute these values every time we update the value of $C$.

% \subsection{DP Guarantee}

% Basically, we only replace the clipping method of DPSGD by a dynamic one where each layer has its own clipping constant $c_i$. If the network has $L$ layers, then we can interpret one round has $L$ different sub-rounds which have their own clipping constant $c_i$ but the same privacy parameter $\sigma$. Therefore, we can now apply the composition theory proposed in~\cite{dong2021gaussian,vandijk2023generalizing} in a straightforward way, i.e., if the new DPSGD runs $E$ epochs, then the privacy guarantee is $G_{\sqrt{gLE}/\sigma}$-DP. Note that this type of argument can be applied to any existing DP notion. 



% Note that in \cite{vandijk2023generalizing}, the generalized algorithm \ref{alg:genDPSGD} is $G_{\sqrt{gE}/\sigma}$-DP with dependency on $q$, $E$, and $\sigma$ if the update value $U=\sum_{h=1}^m [a_h]_C$ is bounded by $2kC$, where $k$ is the number of $a_h$ and $C$ is the clipping constant. In an attacker's perspective, the adversary needs to perform hypothesis testing between $U$ and $U'$, which are the update outcomes when the samples are drawn from the dataset $D$ and $D'$, respectively. Section D.4 in \cite{vandijk2023generalizing} proves that $||U' - U||$ is upper bounded by $2kC$ for the generalized DPSGD algorithm if $||[a_h]_C||_2$ is bounded by $C$.

% Assuming the gradient of each layer $[a_{h,i}]{c_i}$ has $J$ elements $a{h,ij}$, in our case, the gradient of the $i$-th layer $[a_{h,i}]_{c_i}$ is bounded by $c_i$.

% \[||[a_{h,i}]_{c_i}||_2 = \sqrt{a_{h,i1}^2 + a_{h,i2}^2 + \dots + a_{h,iJ}^2} = \sqrt{\sum_{j=1}^J a_{h,ij}^2} \leq c_i\]

% Then, the full gradients $[a_h]_C$ is bounded by $C = \sqrt{c_1^2 + c_2^2 + \dots + c_L^2}$:

% \[||[a_h]_C = \sqrt{\sum_{i=1}^L \sum_{j=1}^J a_{h,ij}^2} \leq \sqrt{\sum_{i=1}^L c_i^2} \]

% Therefore, the update value $U = \sum_{h=1}^m [a_h]_C = \left\{\sum_{h=1}^m [a_{h,i}]_{c_i} + {\cal N}(0,(2c_i\sigma)^2{\bf I}) \right\}_{i=1}^{L}$ is bounded by $2kC = 2k\sqrt{\sum_{i=1}^L c_i^2}$.

% \textbf{From layerwise noise to f-DP Gaussian operator}:
% Note that, it is quite different from the DP analysis in \cite{vandijk2023generalizing} where we add the same noise $N(0,(2C\sigma)^2I)$ to each layer. In our work, we instead add different noise $N(0,(2c_i\sigma)^2I)$ to each entry of layer $i$. Therefore, we have to do the hypothesis testing as follows:

% Let $||U'- U|| = {U_1,U_2,\dots, U_L}$ , we consider hypothesis testing for worst case scenario: $||U_i|| = ||[a_{h,i}]_{c_i}|| = c_i$., then the adversary will do the hypothesis testing on:

% \[\left\{U_i + N(0,(2c_i\sigma)^2I)\right\}_{i=1}^{L} \mbox{versus} \left\{0 + N(0,(2c_i\sigma)^2I)\right\}_{i=1}^{L}\]
% After dividing two sides by $c_i$ we will have:
% \[\left\{U_i/c_i + N(0,(2\sigma)^2I)\right\}_{i=1}^{L} \mbox{versus}\ \left\{0 + N(0,(2\sigma)^2I)\right\}_{i=1}^{L}\]
% \[\Leftrightarrow \left\{U_i/c_i + N(0,(2\sigma)^2I)\right\}_{i=1}^{L} \mbox{versus}\ 0 + N(0,(2\sigma)^2I)\]

% Let $e = \{U_i/c_i\}_{i=1}^{L}$, then we have the norm $||e|| = \sqrt{\sum_{i=1}^{L} ||U_i||^2/c_i^2 } = \sqrt{L}$ and the hypothesis testing become:
% \[e +  N(0,(2\sigma)^2I)\ \mbox{versus}\ 0 + N(0,(2\sigma)^2I)\]
% Next, we divide the two sides by the norm $||e||$:
% \[\frac{e}{||e||} +  N\left(0,(\frac{2\sigma}{||e||})^2I\right) \mbox{versus}\ \frac{0}{||e||} + N\left(0,(\frac{2\sigma}{||e||})^2I\right)\]
% which is equivalent to 
% \begin{equation}\label{eqn:hypotesting}
%     % \begin{aligned}
%     1 + N\left(0,(\frac{2\sigma}{\sqrt{L}})^2I\right) \mbox{versus}\ 0 + N\left(0,(\frac{2\sigma}{\sqrt{L}})^2I\right)
%     % \end{aligned}
% \end{equation}

% In order to use the $f$-DP trade off function $G_{1/\bar{\sigma}} $ where the hypothesis testing is performed on:
% \[1 + N(0,2\bar{\sigma}^2I) \mbox{versus}\ 0 + N(0,2\bar{\sigma}^2I)\]

% Therefore, the hypothesis testing in \ref{eqn:hypotesting} is $G_{1/\bar{\sigma}} = G_{\sqrt{L}/\sigma}$ where $\bar{\sigma} = \sigma/\sqrt{L}$ and $L$ is the number of layers that computes the gradients.

% Following that, the algorithm \ref{alg:layerwiseDPSGD} is $G_{\sqrt{gEL}/\sigma}$-DP for $E$ is the number of epochs and $g$ is the group size.








