\section{Experiments}
\label{sec:experiments}

\subsection{Setup}
\label{sec:setup}

\textbf{Data preprocessing:} 
%In this paper, 
We perform 
experiments on the CIFAR-10 dataset which consists of 50,000 training examples and 10,000 test examples, divided into 10 classes, with each example being a 32x32 image with three color channels (RGB)(\cite{CIFAR10dataset}). In our experiments, we perform data augmentation and data normalization independently. Specifically, for each training image, we crop a $32 \times 32$ region from it with padding of 4, apply a random horizontal flip to the image, and then normalize it with 
\[(mean,std) = ((0.4914, 0.4822, 0.4465), (0.2023,0.1994, 0.2010)).\]
Next, we divide the CIFAR-10 dataset $\mathcal{D}$ into two dataset $\mathcal{D}_{pub}$ and $\mathcal{D}_{train}$ where $|\mathcal{D}_{pub}| = \frac{1}{10}|\mathcal{D}|$  and $|\mathcal{D}_{train}| = \frac{9}{10}|\mathcal{D}|$.


As explained in Section \ref{subsec:adaptiveclipping}, 
%similar to \cite{Zhang2018CoRR}, 
we  estimate the expectation $(e_1,\ldots,e_L)$  of 
the layer gradient norms 
$(\|\nabla_{w_1} f(w;\xi)\|,\ldots, \|\nabla_{w_L} f(w;\xi)\|)$ over\footnote{We notice that we may not need a large sized ${\cal D}_{pub}$ for a good estimate.} $\xi \in {\cal D}_{pub}$, where $w=(w_1 | \ldots | w_L)$ is the current model. We compute the layerwise clipping constants $(C_1=C\cdot e_1/M,\dots,C_L=C\cdot e_L/M)$ with $M=\max_{h=1}^L e_h$ and master clipping constant $C$ .

%based on gradients computed from the public dataset $\mathcal{D}_{pub}$ and the current model $w$. %'s parameters. 

We sample the $m$-sized training data batches with replacement from $\mathcal{D}_{train}$ at the beginning of each epoch and  feed them to the machine learning model to train the model.

\textbf{Diminishing Learning Rate and Fixed Master Clipping Constant:} We fix the master clipping constant $C$ throughout the training process and after each epoch we update the learning rate $\eta$ as 
\begin{eqnarray*}
\eta &:=&  \eta \cdot \eta_{decay},
\end{eqnarray*}
%
% \begin{eqnarray*}
% \eta &:=&  \eta \cdot \eta_{decay}
% \ \ \ \ \mbox{ and } \ \ \ \
% C :=  C \cdot C_{decay},
% \end{eqnarray*}
where $\eta_{decay}$ is a decaying factor. After each epoch, we re-compute the layerwise clipping constants $(C_1,C_2,\dots,C_L)$ as explained above. In our experiments, we set $\eta_{decay} = 0.9$, $C = 0.095$ and $\sigma=0.01875$.

\textbf{Model Update:} resnet-18 is updated by our modified DPSGD with BC and ALC,
%as described in (\ref{algo:DPSGDxBC}), 
i.e., for a batch (mini-set) $\{\xi_{i_1},\ldots, \xi_{i_m}\}$ we compute
\begin{eqnarray*}
U &:=& n+\left[\frac{1}{m} \sum_{j=1}^m  \nabla_w f(w;\xi_{i_j}) \right]_{(C_1,\dotsc,C_L)}
\ \ \ \ \mbox{ and } \ \ \ \
    w := w -  \eta U
\end{eqnarray*}
with 
$n\sim {\cal N}(0,(2C_1\sigma)^2{\bf I})\times \ldots \times {\cal N}(0,(2C_L\sigma)^2{\bf I})$.




% In order to demonstrate our idea, we first train the light-weighted convnet model \ref{tab:convnet} with batch size $m=64$ and print out per layer gradient average norm after each epoch, which is shown in Figure \ref{fig:gradientgraph}.a. Here, we can clearly see the different between layers gradient norm magnitudes so we decide to apply our method, layerwise clipping and diminishing clipping constant $C_{master}$, instead of full gradient clipping in \cite{abadi2016deep}.

% Moreover, our method outperforms full gradient clipping method when we apply both method to train the convnet model with individual clipping under the same hyper-parameters settings: Specifically, we choose, noise multiplier $\sigma= 1.5$, constant clipping value $C =1.2$, starting dimishing clipping value $C_{master}= 1.2$, batch size $m = 64$, number of epochs $E=50$, diminishing learning rate $\eta = 0.025$ where $m$ is the mini-batch size and $C_{master}$ decreases after each epoch by $10\%$. The comparison between two methods is shown in \ref{fig:gradientgraph}.b.

% Note that batch clipping method enable us to apply batch normalization layer in the model architecture. 
% However, the convnet model does not have the batch normalization layer so we conduct all experiments with resnet18 model (\cite{resnet18paper}).

\subsection{Benchmark versus BC}
\label{subsec:benchmarkvsBC}

We compare our DPSGD with BC and ALC versus mini-batch SGD without DP, i.e., without clipping and without adding Gaussian noise. In both cases, we train resnet-18 on the CIFAR-10 dataset.

%Firstly, we compare how good is our method, batch clipping and diminishing master clipping value $C_{master}$, with the no DP configuration where we use mini-batch SGD to train the resnet-18 model on CIFAR-10 dataset without clipping the gradients and adding the Gaussian noise. 

For mini-batch SGD with diminishing learning rate without DP we experiment with
%train
%resnet-18 
%model with normal training (no clipping, no added noise) with 
mini-batch sizes $m = (64,128,256,512,1024)$ and initial learning rate $\eta = 0.025$.  The best test accuracy
%and achieved the best testing accuracy 
at epochs 20 and 50 is realized by mini-batch size $m=64$ and achieves $88.31\%$ and $90.24\%$ (see Figure~\ref{fig:BCaccuracyA}).
%, respectively.
%, respectively, is : $88.31\%$ and $90.24\%$ with batch size $m = 64$. 
%Next, we also run the 
The results of our DPSGD with BC and ALC and diminishing learning rate, with fixed master clipping constant $C=0.0095$, with standard deviation $\sigma=0.01875$, and with $m = (64,128,256,512,1024)$ and initial learning rate $\eta = 0.025$ are presented in Figure~\ref{fig:BCaccuracyB}. We  achieve $60\%$ and $67\%$ at epochs 20 and 50  for $m=64$.

%% Figure environment removed


% Figure environment removed

% Figure environment removed




\subsection{IC versus BC}
We compare our DPSGD with BC and ALC versus DPSGD with IC and the same ALC. The latter is the original DPSGD \cite{abadi2016deep} combined with ALC.
%\cite{van2018three,Xu2021,Zhang2018CoRR}.
%Beside that, we also wonder how good is our batch clipping method compared with the individual clipping method which in widely used in many literature in (\cite{abadi2016deep},\cite{van2018three},\cite{Xu2021},\cite{Zhang2018CoRR}). 
Based on the result from Section \ref{subsec:benchmarkvsBC}, we choose 
%the mini batch size 
$m=64$ which gives  the best test accuracy for fixed master clipping constant $C=0.0095$ and initial learning rate $\eta=0.025$. We report the test accuracies after $E=50$ epochs for 
$\sigma = 0.01875$ in Figure \ref{fig:ZhangversusUs}. We observe that BC  converges while IC does not.
%
%Here, we choose learning rate $\eta = 0.025$, learning decaying $\eta_{decay}=0.9$, diminishing master clipping value $C_{master} = 0.005$, number of epochs $E=20$ and we vary the noise multiplier value $\sigma = 2,4,6,\dots, 60$. According to Figure \ref{fig:BCICcomparison}.a, 
%we can see that batch clipping outperformed individual clipping method by having better accuracy. 
% We zoom in on $\sigma=60$ in Figure \ref{fig:BCICcomparison}.b
% %, we choose $\sigma = 60$, then 
% where we achieve $85.97\%$ test accuracy for BC and $60.80\%$ test accuracy for IC.
% % Figure environment removed

\subsection{Zhang's ALC versus ours}
\label{sec:Zhang}

As discussed in Section~\ref{subsec:adaptiveclipping}, the layerwise clipping constants $C_1,\dotsc,C_L$ in~\cite{Zhang2018CoRR} may not change from epoch to epoch because the layer gradient norms of resnet18 do not significantly change throughout the training process as depicted in Figure~\ref{fig:resnet18gradientnorm}. 
We explained that this may hurt the convergence if $C_1,\dotsc,C_L$ are not optimized.
%of training algorithm if the noise variance $\sigma$ is sufficient large. 
Based on this observation, our  proposed enhanced ALC implements a master clipping constant $C$ which allows us to optimize layer clipping constants $C_1, \dotsc, C_L$. 
%We show that 
By tuning the initial/fixed master clipping constant 
%manipulating 
$C$, DPSGD with BC and our enhanced ALC offers a better performance compared to
DPSGD with BC and the ALC method of ~\cite{Zhang2018CoRR}.




%We observe that our enhanced ALC
%allows us to choose a very small initial master clipping constant $C=0.005$. This in turn leads to a wide range of possibles choices for 
%$\sigma$. We notice  that, the Gaussian noise added  to the gradient of layer $h$ is drawn from $N(0,(2C_h\sigma)^2{\bf I})$. Experiments seem to confirm that 
%we can keep 
%
%increase $\sigma$ at least to the point where $C^2\sigma^2 = 1$ without hurting the model's gradient too much.

We run DPSGD with BC and the our enhanced ALC method  with initial learning rate  $\eta = 0.025$, fixed master clipping constant $C = 0.0095$, and $m=64$
%\eta_{decay} = 0.9$ and $C_{decay} = 0.9$ 
over 50 epochs for resnet-18 and CIFAR-10.
We run DPSGD with BC and the ALC of \cite{Zhang2018CoRR} with the same learning rate  $\eta = 0.025$ and $m=64$ over 50 epochs for resnet-18 and CIFAR-10.
%We work with very large $\sigma$ because our ALC and Zhang's one give good testing accuracy when $\sigma$ are smaller than 60. 
The results are shown in Figure~\ref{fig:ZhangversusUs}. 


% Therefore, we investigate how large the noise multiplier $\sigma$ we can achieve until we witness the degradation in test accuracy while comparing with the method in \cite{Zhang2018CoRR}:
% {\color{red} TODO: Should we add the comparison image for small sigma (2 to 64)?}


\label{subsec:ZhangUs}
% Figure environment removed


\subsection{Different noises}
% Figure environment removed

We study the relationship between the test accuracy and $\sigma$ 
for DPSGD with BC and the enhanced ALC
with $\eta=0.025$, $m=64$, and $C=0.095$ over 10 epochs. The result is depicted in Figure~\ref{fig:ourDPSGDwithDiffNoises} and shows that for convergence resnet-18 needs a relatively small $\sigma$ and as a consequence can only achieve a weak DP guarantee. 
This shows that a proper balance between test accuracy and DP guarantee for very deep neural networks with complex datasets is still an open problem. Our BC and ALC techniques help towards achieving a practical balance, but more complimentary tricks and methods are still needed.
%One solution may be to work with much larger data sets (large $N$) which yields more privacy amplification due to subsampling.
%This means that we need more privacy amplification by using subsampling, that is, we need to work with large data sets (large $N$). 
%However, 

