\documentclass{article}


\usepackage{arxiv}
\usepackage[numbers]{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{caption}
% \graphicspath{ {./images/} }
%--- https://tex.stackexchange.com/questions/30720/footnote-without-a-marker%
\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% \newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\title{Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent}


\author{Toan N. Nguyen$^{1^{*}}$, \textbf{Phuong Ha Nguyen}$^{2^{*}}$, \textbf{Lam M. Nguyen}$^{3}$,Marten van Dijk$^{4,5,6}$ \\ 
   \\
$^{1}$ Department of Computer Science and Engineering, University of Connecticut, CT, USA \\
$^{2}$ eBay, CA, USA\\
$^{3}$ IBM Research, Thomas J. Watson Research Center, Yorktown Heights, NY, USA\\
$^{4}$ CWI Amsterdam, The Netherlands\\
$^{5}$ Department of Computer Science, Vrije Universiteit Amsterdam, The Netherlands \\
$^{6}$
Department of Electrical and Computer Engineering, University of Connecticut, CT, USA\\
\\
 \texttt{toan.nguyen@uconn.edu}, \texttt{phuongha.ntu@gmail.com},\\
\texttt{LamNguyen.MLTD@ibm.com}, \texttt{marten.van.dijk@cwi.nl}
}

\begin{document}
\maketitle

\blfootnote{$^{*}$ these authors contributed equally.}
% \blfootnote{$^{\dagger}$ Supported by NSF grant CNS-1413996 “MACS: A Modular
% Approach to Cloud Security.”}
% \blfootnote{}
\begin{abstract}
Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18  cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks  for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a new ALC and provide rigorous DP proofs for both BC and ALC}. Experiments show that our modified DPSGD with BC and ALC  for CIFAR-$10$ with resnet-$18$ converges while DPSGD with IC and ALC does not.  
\end{abstract}


% Content %
\input{Chapters/00_Introduction.tex}
\input{Chapters/06_DPSGD_layerwise}
\input{Chapters/09_RelatedWorks}
\input{Chapters/07_Experiments_new}
\input{Chapters/08_Conclusion}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

% \clearpage
\bibliography{bibliography}
\bibliographystyle{plainnat}

\clearpage
 \newpage
 \appendix
 \onecolumn

 \input{Chapters/10_Appendix}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}





\end{document}
