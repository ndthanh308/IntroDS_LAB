
\section{Experimental Results}
In this section, we present the following results: evaluation of the re-localisation module on a large-scale natural dataset, Wild-Places~\cite{knights2022wild} (consisting of \emph{Venman} and \emph{Karawatha} sequences) and its comparison with Scan Context~\cite{kim2018scan} (as a handcrafted PR approach widely integrated with lidar SLAM), evaluation of cross-modal localisation on the same dataset. Finally, we evaluate the entire proposed \textit{R3Loc} pipeline on a wake-up problem scenario on a robot system. 

Both EgoNN and SLidR were trained on the Wild-Places dataset. For EgoNN we followed the training split described in~\cite{knights2022wild}. For testing, however, we evaluated the model on two sequences of Venman collected in opposite directions. This inter-sequence PR evaluation mimics the wake-up problem when the robot operating in the \textit{revisit} session travels within the prior map generated in the opposite direction. Same sequences were used to evaluate Scan Context following the default setting. For evaluation, we define a true positive revisit when a prediction is within 3 m of a positive ground truth.


For SLidR we trained and validated the network using about 1750 matched lidar-image pairs (pairs captured at the same time)
on one sequence from Venman. 
We created three test sets from the validation section by augmenting the relative transformation between the image and point cloud to create matched and mismatched pairs, and by randomly pairing images and point clouds captured in different places for unmatched pairs. This allows testing SLidR for the three most common EgoNN output cases. We also tested the proposed verification pipeline on a new dataset collected in an unstructured area at the Queensland Centre for Advanced Technology (QCAT), Brisbane, Australia.









\subsection{Evaluation of EgoNN Offline}
\figref{fig:recall} illustrates the top-K Recall curves between EgoNN and Scan Context. As seen, the performance of EgoNN is almost twice higher than Scan Context, indicating the limitation of Scan Context to produce distinctive and rotation-invariant descriptors in cluttered environments such as forests. To evaluate re-localisation accuracy, we compare the estimated relative transformation with ground truth and compute a success rate if the rotation and translation errors are within $5^{\circ}$ and $2$~m, respectively. This evaluation was not performed for Scan Context due to the inability of this approach to estimate 6DoF rotation and translation only based on global descriptors. 

The success rate for EgoNN, when the relative transformation was only estimated using keypoints and via RANSAC, is about $40\%$. However, after refining the estimated transformation using ICP (we downsampled point clouds to 40 cm spatial resolution for online registration), the success rate increased to $78\%$. This indicates that although EgoNN achieved high performance in place recognition, the extracted keypoints are not well-repeated in unstructured environments for accurate re-localisation. 


% Figure environment removed



% Figure environment removed


% Figure environment removed


\subsection{Evaluation of SLidR Offline}
\figref{fig:slidr} illustrates an example of matched, unmatched and mismatched pairs in the top, middle and bottom rows, respectively. As seen, the similarity matrices (second column) and the projection vectors (third column) obtained from the procedure described in~\secref{sec:slidr} are good measures to distinguish between matched, mismatched and unmatched pairs.~\figref{fig:valid} shows the boxplots over MCS and $\nu$ computed for about 250 matched pairs and 230 mismatched and unmatched pairs on the validation set (above 700 pairs in total). The substantial difference in MCS between unmatched and matched/mismatched pairs allows classifying unmatched ones with high confidence. Additionally, the large $\nu$ for the matched pairs helps classify them from the other pairs. We, however, observed if both MSC and $\nu$ are used together, it improves generalisation when training and testing environments are different. Hence, we trained  a multi-class fifth-degree polynomial SVC model, $\mathcal{K}(\text{MCS}, \nu)$, to predict if a pair belongs to matched, mismatched or unmatched categories.







% Figure environment removed

\subsection{Evaluation of the Entire Pipeline Online}
To evaluate our pipeline, a tracked robot (as seen in~\figref{fig:diagram}), equipped with a lidar sensor and four cameras, was teleoperated in an unstructured area once as the \textit{initial} session and once as the \textit{revisit} session at QCAT. Time difference between the two sessions was reasonably chosen large, allowing us to evaluate the performance of our verification under various lighting conditions.
For the cross-modal perception, we only used the camera frames of the front camera. Submaps were generated by Wildcat, and Robot Operating System (ROS) was used for communication between different components. Our re-localisation pipeline is triggered through a rosservice command. Upon requesting re-localisation, the query submap and the submaps existing in the prior map were fed into the already trained model of EgoNN. By performing a forward pass using the weights and benefiting from kd-tree, the top candidate was selected and the relative pose was estimated. Since PR is only performed based on root nodes, there were at most 20 submaps in the prior map from \textit{initial} session. To thoroughly test the pipeline, the recorded data of the \textit{revisit} session played back, and the re-localisation was requested for every root node spawned out, resulting in testing the entire pipeline 20 times (\ie~20 ``wake-up'' locations). Following this process, the average Recall@1 of EgoNN is $100\%$. However, the success rate for re-localisation was $70\%$, evidencing the necessity of the hypothesis verification. 

The pose estimate is not transferred to our lidar-inertial SLAM unless it passes through the hypothesis verification. For this, the top-candidate submap and the query image (which is already rectified) are fed into our pre-trained verification models. For the QCAT dataset, after 20 trials, the proposed hypothesis verification detected all the mismatched pairs for which EgoNN could not produce an accurate pose estimate.~\figref{fig:qcat_examples} shows a sample matched (top) and mismatched (bottom) scenario. The proposed verification pipeline, including the pre-trained feature matching  and the SVC model $\mathcal{K}$,  successfully separated these cases and detected the re-localisation failure.

Upon a verified re-localisation, the pose graph generated by the \emph{revisit} session is safely merged into the existing map as shown in~\figref{fig:qcat_multiagent}.
\figref{fig:qcat_multiagent_paths} presents qualitative results after merging \textit{revisit} session robot into the map generated from the \textit{initial} session, evidencing the proposed pipeline feasibility for multi-agent re-localisation. 

% Figure environment removed

% Figure environment removed


\subsection{Runtime Analysis}
\label{tab:runtime}
\begin{table}[t]
    \caption{\small{Runtime Analysis.}}
    \label{tab:runtime}
    \resizebox{\linewidth}{!}{\begin{tabular}{c|cc|cccc|c}
        \hline
        {\textbf{Processing}}& \multicolumn{2}{|c|}{EgoNN}& \multicolumn{4}{|c|}{SLidR}& {\textbf{Total}}\\ 
        \cline{2-4} \cline{5-7} 
        \multirow{1}{*}{\textbf{time}}& Description& Localisation& Superpixel & Description& MCS & Verification& \textbf{time}\\\hline
        & &  & & & && \\
         \textbf{Mean}(s)& 0.087&  0.408& 0.186 &0.209& 0.013 & 0.019 & 0.905\\
         \textbf{std}(s) & $\pm$0.002& $\pm$0.207& $\pm$0.085 & $\pm$0.022 &$\pm$0.037 & $\pm$0.002 & $\pm$0.215\\
        \hline
    \end{tabular}}
    \vspace{-0.5cm}
\end{table}

To demonstrate that our presented system can run online, we evaluated the computation time for each
component. The timing results are collected by running the pre-trained models on a single NVIDIA Quadro T2000 GPU and the rest of the pipeline on a unit with an Intel Xeon W-10885M CPU.
~\tabref{tab:runtime} reports a breakdown of individual modules' runtime in our pipeline. 
Together, the total runtime (for the QCAT experiment with the scale shown in~\figref{fig:hero}) is less than a second, allowing the system to run for online operation. 


