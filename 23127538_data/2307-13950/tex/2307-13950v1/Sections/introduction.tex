\section{Introduction}

(Re)-localisation in robotics refers to a process of determining a robot's current pose (position and orientation) in a known environment that has been previously mapped. This task is crucial for robots to perform their operations seamlessly, even if they experience temporary difficulty in tracking their location. 
For example, the ``wake-up'' problem involves a robot that needs to determine its location after being turned off or losing power. 
Despite significant advances in learning-based approaches for re-localisation that rely on vision~\cite{galvez2012bags, li2023hot, doan2019scalable, zhang2023etr} or lidar data~\cite{uy2018pointnetvlad, jacek20minkloc, vidanapathirana2022logg3d, hui2021pyramid},  designing a robust and reliable re-localisation technique remains a challenge, especially in unstructured, natural environments. 
Such environments lack distinctive features and change over time due to vegetation growth and weather conditions, affecting re-localisation's robustness~\cite{knights2022wild}. 




Due to the inherent limitations of both lidar and image, it is difficult to extract appropriate features (in complex natural scenes) relying on a single modality for re-localisation. To address this, we propose integrating a self-supervised image-to-lidar feature matching process to predict re-localisation failure in a pipeline consisting of three modules of place recognition, pose estimation, and hypothesis verification, each leveraging learning methods. 
For place recognition and pose estimation modules, we use EgoNN~\cite{komorowski2021egonn}, an end-to-end deep re-localisation network. With the power of our lidar SLAM system, namely Wildcat~\cite{ramezani2022wildcat}, we generate lidar submaps and a pose graph with the robot nodes and geometric information in between and store them in a database. EgoNN is trained on Wildcat submaps offline. At inference time, the pre-trained network is used for re-localisation by comparing a query submap with the submaps in the database. Once the relative pose between the query and top-candidate submaps is estimated (\figref{fig:hero}), the proposed hypothesis verification module evaluates the correctness of the transformation with a cross-modality comparison between an image captured at the same time as the query submap and the top-candidate submap. 
Experimental results demonstrate the effectiveness of the proposed pipeline in achieving accurate re-localisation. The main contributions of this work can be summarised as follows:


% Figure environment removed


\begin{itemize}
    \item We propose integrating a self-supervised image-to-lidar feature matching process to predict re-localisation failure.
    \item We present a full pipeline of a deep re-localisation method (\textit{R3Loc}) to address multi-robot re-localisation.
    \item We demonstrate our pipeline's effectiveness in a large-scale natural dataset offline and in a forest-like environment on real robots online.
\end{itemize}
