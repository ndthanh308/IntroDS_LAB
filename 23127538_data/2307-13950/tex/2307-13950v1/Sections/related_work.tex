\section{Related Work}


This section reviews the existing Lidar Place Recognition (LPR) algorithms and discusses existing research into re-localisation. Finally, works related to image-lidar modal perception in cross-modal PR and registration are reviewed. 

\subsection{Lidar-Based Localisation}

A range of algorithms has been proposed for LPR. Conventional approaches~\cite{kim2018scan, rusu2009fast, rusu2008aligning, salti2014shot, dube2017segmatch} encode point clouds into either a global descriptor representing the entire point cloud or several local descriptors by segmenting the point cloud into patches. However, these handcrafted methods are often rotation dependent and are not effective in generating discriminative descriptors for unstructured environments.

Deep LPR has demonstrated outstanding results in the past few years. These methods process point clouds through a deep neural network to extract local features. Features are either directly used for place recognition, such as works in~\cite{dube2018segmap, tinchev2019learning} or aggregated using either a first-order pooling technique,~\eg~GeM~\cite{radenovic2018fine}, NetVLAD~\cite{arandjelovic2016netvlad} or second order pooling employed in~\cite{vidanapathirana2021locus, vidanapathirana2022logg3d}, to generate a global descriptor of the point cloud~\cite{vidanapathirana2022logg3d, jacek20minkloc, hui2021pyramid, uy2018pointnetvlad, zhang2019pcan}. Methods such as EgoNN~\cite{komorowski2021egonn} and LCDNet~\cite{cattaneo2022lcdnet} estimate relative pose between two point clouds upon place recognition. EgoNN computes keypoint coordinates, local descriptors, and saliency in a local head. It later estimates 6DoF relative transformation between the query and top-candidate point clouds by matching keypoints and employing RANSAC to remove outliers. LCDNet trains local features end-to-end utilising the Optimal Transport (OT) theory %
for matching features and finally estimating the relative pose using Singular Value Decomposition (SVD), allowing the entire pipeline to be differentiable, therefore, learnable. However, at test-time, LCDNet employs RANSAC for relative pose estimation prone to divergence in natural environments. 
Focusing on re-ranking top-k retrieval candidates, SpectralGV~\cite{vidanapathirana2023spectral} introduces a computationally efficient spectral re-ranking method to improve localisation.




\subsection{Cross-Modal Localisation} 



There are PR-related works that aim to enhance place recognition by leveraging lidar scans and images captured in the same place. Works such as~\cite{yin2022bioslam, bernreiter2021spherical, pan2021coral} integrates lidar and visual measurements at an early stage of multi-modal fusion to encode them into a global descriptor using a projection technique; however, at the cost of dimension loss. In contrast, works such as~\cite{komorowski2021minkloc++, xie2020large, ratz2020oneshot} encode lidar and visual data separately (late fusion) into image and point cloud embeddings and later aggregate them to create the bimodal global descriptor. To deal with lighting conditions (affecting the quality of image features), AdaFusion~\cite{lai2022adafusion} employs an attention mechanism avoiding two modalities to be considered equally important when image quality is poor for recognition and vice versa. 


In computer vision, works such as I2P~\cite{li2021deepi2p} and 2D3D-MatchNet~\cite{feng20192d3d} have been proposed with a focus on image-to-lidar registration. I2P trains a network to estimate the pose between a pair of images and point clouds in two steps of classification and inverse camera projection. I2P uses an attention mechanism to classify lidar points in and out of the camera frustum. It optimises pose in the lidar frame using inverse camera projection and classification prediction. 2D3D-MatchNet learns 2D image and 3D point cloud descriptors in a triplet loss (anchor image, positive and negative point clouds) as similar image-lidar descriptors are pushed closer while negative pairs are pushed apart. Recently, SLidR~\cite{sautier2022image} proposed to find similarities between point cloud and image pairs based on locally similar regions on 2D images and their corresponding 3D patches obtained knowledge distillation.  
