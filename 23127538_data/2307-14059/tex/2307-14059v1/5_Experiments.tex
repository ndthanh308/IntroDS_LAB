\section{Experimental Results \& Discussion}\label{sec:experiments}
We wish to answer the following research questions experimentally:
\begin{description}
\item[\textbf{RQ1}] \textit{Is our proposed probabilistic method able to model exposure probabilities more accurately than existing methods?}
\item[\textbf{RQ2}] \textit{Can the model leverage contextual signals effectively?}
\item[\textbf{RQ3}] \textit{Are the obtained position biases useful for downstream tasks, such as unbiased offline evaluation?}
\end{description}

Naturally, position biases are heavily influenced by specific use-cases, platforms and interface choices.
The methods we propose in this work are motivated by a short-video feed recommendation use-case, and even though our proposed framework is generally applicable, we expect the Yule-Simon instantiation to only hold merit in similar use-cases.

In order to empirically validate the performance of both our and earlier proposed methods, we require \emph{interventional} data with logged views $V$, ranks $R$, and contexts $X$.
To the best of our knowledge and at the time of writing, we are unaware of any such datasets being publicly available.
Existing Learning-to-Rank (LTR) datasets do not contain rank interventions and deal with web search use-cases, which imply very different modalities to ours.
For this reason, we need to resort to proprietary datasets, but additionally release an open-source Jupyter notebook to reproduce the position bias curves visualised in Figure~\ref{fig:yulesimon} at \href{https://github.com/olivierjeunen/C-3PO-recsys-2023}{github.com/olivierjeunen/C-3PO-recsys-2023}.

\subsection{Estimating Exposure Probabilities (RQ1--2)}
We obtain a sample of 1 million sessions of feed view events on a social media platform, where rank interventions occurred following Fig.~\ref{fig:PGM}, collected over five days in February 2023.
We perform an 80-20\% train-test split, aiming to predict whether recommendations were viewed based on their rank and contextual information.

We compare several non-contextual variants: the standard DCG discount function as well as the logarithmic and exponential forms in Eq.~\ref{eq:dcg}, and the probabilistic method based on the Yule-Simon distribution introduced in Eq.~\ref{eq:prob}.
The latter three methods include a single parameter ($\alpha, \gamma, \rho$ respectively), which we learn to minimise NLL@$K$ on the training set, following the procedure laid out in \S\ref{sec:learning}.
We implement this in Python 3.9 with the SciPy library~\cite{Virtanen2020}.

As an additional baseline, we include a non-parametric method that predicts the empirical average from the training data.
This approach should be expected to outperform the aforementioned methods, but it requires a hard-coded probability at every rank instead of the single parameter that the logarithmic, exponential, or probabilistic forms require.
Additionally, this approach cannot easily be extended to incorporate contextual information $X$.

For the contextual case, we adopt a single continuous user-based feature describing users' past average scroll depth, as well as a single continuous context-based feature, describing average scroll depth at the time of day. 
We adopt a simple linear model to estimate the distribution parameter from this input $X$: $\rho_{\theta}(x) = \theta^{\intercal}x$.
The functional forms for the parameters $\alpha$ and $\gamma$ are analogous.
As such, the contextual and personalised methods consist of only \emph{three} parameters each (assuming $x$ includes a constant 1-feature, emulating a bias term in $\theta$).
Even in this simplistic scenario, the contextual and personalised methods significantly outperform those that do not consider this information, as shown in Table~\ref{tab:results}.
Our contextual, personalised, probabilistic position bias model \texttt{C-3PO} achieves the lowest NLL@$K$ for a wide range of $K$, whilst requiring a minimum of learnable parameters or computing resources.
This yields a desirable trade-off between parsimony and model expressiveness when compared to complex model classes like neural networks (which would typically require orders of magnitude more parameters). 
We observe that this additionally allows us to be sample-efficient, as our method already performs well with only $\mathcal{O}(10^{4})$ samples.
Indeed, instead of modelling the entire curve at every possible value of $r$, our proposed method outputs a single scalar which can be used to obtain position bias estimates for all natural numbers.
The inductive bias we enjoy from well-motivated mathematical models greatly improves the methods' real-world usability, when compared to neural-network based alternatives.

\begin{table}[t]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{5}{c}{\textbf{Negative Log-Likelihood (NLL)}}\\
     ~& \textbf{@5} & \textbf{@10} & \textbf{@25} & \textbf{@50} & \textbf{@100} \\
    \cline{2-6}

    $\widehat{\mathsf{P}}_{{\rm dcg}}(V|R)$ & 0.5453 & 0.6320 & 0.5998 & 0.4973 & 0.3763\\
    $\widehat{\mathsf{P}}_{\log}(V|R)$ & \underline{0.5159} & \underline{0.6001} & 0.5900 & 0.5036 & 0.3833\\
    $\widehat{\mathsf{P}}_{\exp}(V|R)$ & 0.5202 & 0.6158 & 0.6089 & 0.5101 & 0.3673\\
    $\widehat{\mathsf{P}}_{\rm prob}(V|R)$ & \underline{0.5162} & \underline{0.6002} &\underline{0.5873} & \underline{0.4891} & \underline{0.3495}\vspace{1ex}\\
    \cdashline{1-6}
    \vspace{-2ex}~\\
    $\widehat{\mathsf{P}}_{{\rm empirical}}(V|R)$ & 0.5157 & 0.5999 & 0.5843 & 0.4813 & 0.3369\vspace{1ex}\\
    \cdashline{1-6}
    \vspace{-2ex}~\\
    $\widehat{\mathsf{P}}_{\log}(V|R,X)$ & \textbf{0.4852} & \textbf{0.5620} & 0.5577 & 0.4806 & 0.3555\\ 
    $\widehat{\mathsf{P}}_{\exp}(V|R,X)$ & 0.4883 & 0.5761 & 0.5778 & 0.4959 & 0.3652\\ 
    $\widehat{\mathsf{P}}_{\rm prob}(V|R,X)$ & \textbf{0.4850} & \textbf{0.5620} &\textbf{0.5551} & \textbf{0.4651} & \textbf{0.3325}\\ 
\bottomrule
    \end{tabular}
    \caption{NLL for position bias models on observed data, lower is better.
    The top-group are independent of contextual information, the middle baseline is a non-parametric method that predicts a sample average, the bottom-group include three parameters that were optimised via linear regression. 
    Marked fields indicate stat. sig. improvements over other methods in the same group at a 99\% level.}
    \label{tab:results}
\end{table}

\subsection{Unbiased Offline Evaluation (RQ3)}
The main task position bias models need to perform, is to deliver offline estimates of online performance.
Given a dataset of logged impressions $\mathcal{D}\coloneqq \{(x_{i}, a_{i}, r_{i}, c_{i})_{i=1}^{N}\}$ (contexts, actions, ranks, rewards) , we wish to estimate the expected reward we would have obtained under some different ranking policy $\pi$.
This policy maps a context $X$ and set of \emph{candidate} items $\mathcal{A}_{x}$ to a ranked list.
We will denote with the shorthand notation $\pi(a|x)$ the rank that item $a$ will be placed at when $\pi$ is presented with context $x$ (assuming $\mathcal{A}_{x}$ given).
Note that this framing is easily extended to more general stochastic ranking policies~\cite{Oosterhuis2021}.
Then, a dataset $\mathcal{D}$ and position bias model $\widehat{\mathsf{P}}$ can be used to to obtain an unbiased estimate of the reward we would obtain under $\pi$:
\vspace{-1ex}
\begin{equation}\label{eq:unbiased_dcg}
    \mathop{\mathbb{E}}\limits_{r \sim \pi}[C] \stackrel{1}{\approx} {\rm DCG}_{\widehat{\mathsf{P}}}(\mathcal{D}, \pi) \stackrel{2}{\approx}
    \frac{1}{N}\sum_{i=1}^{N}
    c_{i} \cdot \frac{\widehat{\mathsf{P}}(V=1|R=\pi(a_{i}|x_{i}), X=x_{i})}{\widehat{\mathsf{P}}(V=1|R=r_{i}, X=x_{i})} .
\end{equation}
Here, the first approximation $\stackrel{1}{\approx}$ is due to the inherent assumptions of the DCG metric (compared to, e.g., cascade-based alternatives), whereas the second only exists because we resort to an empirical average over the observed data $\mathcal{D}$ and estimated position biases via $\widehat{\mathsf{P}}$.
Assuming unbiasedness of $\widehat{\mathsf{P}}$, the unbiasedness of the metric in Eq.~\ref{eq:unbiased_dcg} is easily recognised, as it is an application of importance sampling or IPS~\cite{Owen2013}.
As is typical for IPS-based methods, techniques like capping or introducing control variates can improve their finite-sample performance by reducing variance~\cite{Gilotte2018, Swaminathan2015snips}.
We do not consider such extensions in this short article, but remark that they are likely to further improve performance.

To validate the utility of these offline estimates, we perform an online experiment on a social media platform that operates a short-video recommendation feed.
Thus, we obtain samples from the reward distribution by sampling $\mathbb{E}_{r \sim \pi}[C]$ directly and taking an empirical average per day, for five days.
Then, for varying context-independent position bias models (optimised $@100$), we obtain offline estimates of online reward via Eq.~\ref{eq:unbiased_dcg}, and evaluate the offline estimates by Pearson's correlation coefficient between the ground truth and the offline estimate, over 5 days.

Table~\ref{tab:results2} shows relative improvements in correlation over the classical DCG formulation.
We observe that our probabilistically motivated position bias model is able to significantly improve the offline-online correlation compared to existing methods, and conjecture that the context-dependent variant can lead to further improvements.
This highlights the importance of a well-motivated position bias model, and is a strong argument in favour of our proposed methods.

\begin{table}[t]
    \vspace{-3ex}
    \centering
    \begin{tabular}{lllccccccc}
    \toprule
    \textbf{Position Bias Model} &~&~& $\widehat{\mathsf{P}}_{{\rm dcg}}(V|R)$ &~& $\widehat{\mathsf{P}}_{{\rm log}}(V|R)$ &~& $\widehat{\mathsf{P}}_{{\rm exp}}(V|R)$ &~& $\widehat{\mathsf{P}}_{{\rm prob}}(V|R)$ \\
         \cline{4-10}
\textbf{Relative Improvement} &~&~& 100\% &~& -3\% &~& -20\% &~& \textbf{+16\%} \\
    
\bottomrule
    \end{tabular}
    \caption{Relative correlation improvement over $\widehat{\mathsf{P}}_{{\rm dcg}}$ between DCG estimates and online metrics, higher is better.}
    \label{tab:results2}
\end{table}