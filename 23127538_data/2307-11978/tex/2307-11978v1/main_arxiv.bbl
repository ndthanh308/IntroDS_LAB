\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{arazo2019unsupervised}
Eric Arazo, Diego Ortego, Paul Albert, Noel Oâ€™Connor, and Kevin Mcguinness.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In {\em ICML}, 2019.

\bibitem{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In {\em ECCV}, 2014.

\bibitem{chang2017active}
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum.
\newblock Active bias: Training more accurate neural networks by emphasizing
  high variance samples.
\newblock In {\em NeurIPS}, 2017.

\bibitem{cimpoi2014DTD}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em CVPR}, 2014.

\bibitem{dosovitskiy2020ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2020.

\bibitem{du2022Detectiong}
Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
\newblock Learning to prompt for open-vocabulary object detection with
  vision-language model.
\newblock In {\em CVPR}, 2022.

\bibitem{fei2004cal101}
Li Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In {\em CVPR-W}, 2004.

\bibitem{ghosh2017robust_loss}
Aritra Ghosh, Himanshu Kumar, and P~Shanti Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In {\em AAAI}, 2017.

\bibitem{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{hendrycks2019using}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In {\em ICML}, 2019.

\bibitem{hendrycks2018using}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock In {\em NeurIPS}, 2018.

\bibitem{Huang2022UPL}
Tony Huang, Jack Chu, and Fangyun Wei.
\newblock Unsupervised prompt learning for vision-language models.
\newblock {\em arXiv preprint}, 2022.

\bibitem{jia2021_align}
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em ICML}, 2021.

\bibitem{ju2022prompting_video}
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In {\em ECCV}, 2022.

\bibitem{li2019learning}
Junnan Li, Yongkang Wong, Qi Zhao, and Mohan~S Kankanhalli.
\newblock Learning to learn from noisy labeled data.
\newblock In {\em CVPR}, 2019.

\bibitem{li2022bridge}
Muheng Li, Lei Chen, Yueqi Duan, Zhilan Hu, Jianjiang Feng, Jie Zhou, and Jiwen
  Lu.
\newblock Bridge-prompt: Towards ordinal action understanding in instructional
  videos.
\newblock In {\em CVPR}, 2022.

\bibitem{li2017learning}
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia
  Li.
\newblock Learning from noisy labels with distillation.
\newblock In {\em ICCV}, 2017.

\bibitem{lu2022prompt_dist}
Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian.
\newblock Prompt distribution learning.
\newblock In {\em CVPR}, pages 5206--5215, 2022.

\bibitem{ma2020normalized}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James
  Bailey.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In {\em ICML}, 2020.

\bibitem{ma2020normalized_loss}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James
  Bailey.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In {\em ICML}, 2020.

\bibitem{ma2018dimensionality}
Xingjun Ma, Yisen Wang, Michael~E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia,
  Sudanthi Wijewickrema, and James Bailey.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In {\em ICML}, 2018.

\bibitem{maji2013aircraft}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock {\em arXiv preprint}, 2013.

\bibitem{shu2022tpt}
Shu Manli, Nie Weili, Huang De-An, Yu Zhiding, Goldstein Tom, Anandkumar Anima,
  and Xiao Chaowei.
\newblock Test-time prompt tuning for zero-shot generalization in
  vision-language models.
\newblock In {\em NeurIPS}, 2022.

\bibitem{nilsback2008flower}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em ICVGIP}, 2008.

\bibitem{parkhi2012pets}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar.
\newblock Cats and dogs.
\newblock In {\em CVPR}, 2012.

\bibitem{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In {\em CVPR}, 2017.

\bibitem{smoothing}
Gabriel Pereyra, George Tucker, Jan Chorowski, {\L}ukasz Kaiser, and Geoffrey
  Hinton.
\newblock Regularizing neural networks by penalizing confident output
  distributions.
\newblock {\em arXiv preprint}, 2017.

\bibitem{Radford2021CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, 2021.

\bibitem{rao2022denseclip}
Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang,
  Jie Zhou, and Jiwen Lu.
\newblock Denseclip: Language-guided dense prediction with context-aware
  prompting.
\newblock In {\em CVPR}, 2022.

\bibitem{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em ICML}, 2019.

\bibitem{bootstrapping}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock {\em arXiv preprint}, 2014.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em ICML}, 2018.

\bibitem{shu2019meta}
Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock In {\em NeurIPS}, 2019.

\bibitem{song2019selfie}
Hwanjun Song, Minseok Kim, and Jae-Gil Lee.
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In {\em ICML}, 2019.

\bibitem{soomro2012ucf101}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock Ucf101: A dataset of 101 human actions classes from videos in the
  wild.
\newblock {\em arXiv preprint}, 2012.

\bibitem{sun2022dualcoop}
Ximeng Sun, Ping Hu, and Kate Saenko.
\newblock Dualcoop: Fast adaptation to multi-label recognition with limited
  annotations.
\newblock {\em arXiv preprint}, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em NeurIPS}, 2017.

\bibitem{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em ICCV}, 2019.

\bibitem{wang2022continual}
Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,
  Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister.
\newblock Learning to prompt for continual learning.
\newblock In {\em CVPR}, 2022.

\bibitem{xia2020robust}
Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi
  Chang.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In {\em ICLR}, 2020.

\bibitem{yao2021filip}
Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan
  Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.
\newblock Filip: Fine-grained interactive language-image pre-training.
\newblock In {\em ICLR}, 2021.

\bibitem{yao2020dual}
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and
  Masashi Sugiyama.
\newblock Dual t: Reducing estimation error for transition matrix in
  label-noise learning.
\newblock {\em NeurIPS}, 2020.

\bibitem{yi2019probabilistic}
Kun Yi and Jianxin Wu.
\newblock Probabilistic end-to-end noise correction for learning with noisy
  labels.
\newblock In {\em CVPR}, 2019.

\bibitem{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock {\em arXiv preprint}, 2022.

\bibitem{Zhang2017Understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em ICML}, 2017.

\bibitem{mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock Mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{GenXEnt}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In {\em NeurIPS}, 2018.

\bibitem{zhang2020distilling}
Zizhao Zhang, Han Zhang, Sercan~O Arik, Honglak Lee, and Tomas Pfister.
\newblock Distilling effective supervision from severe label noise.
\newblock In {\em CVPR}, 2020.

\bibitem{zheng2021meta}
Guoqing Zheng, Ahmed~Hassan Awadallah, and Susan Dumais.
\newblock Meta label correction for noisy label learning.
\newblock In {\em AAAI}, 2021.

\bibitem{zhou2022CoCoOp}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In {\em CVPR}, 2022.

\bibitem{Zhou2022CoOp}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock {\em IJCV}, 2022.

\end{thebibliography}
