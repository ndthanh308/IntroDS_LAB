\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{subcaption}

\usepackage{csquotes}
\usepackage{makecell}
\usepackage{authblk}

\usepackage[dvipsnames]{xcolor}
\usepackage{color, colortbl}
\definecolor{citecolor}{HTML}{0071bc}
\definecolor{tabhighlight}{HTML}{e5e5e5}

\renewcommand*{\Authsep}{\qquad}
\renewcommand*{\Authand}{\qquad}
\renewcommand*{\Authands}{\qquad}
\setlength{\affilsep}{0.5em}

\newcommand{\std}[1]{\tiny{$\pm$#1}}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{5737} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?}

\author{Cheng-En Wu$^1$\thanks{Work done during an internship at ByteDance Inc.} \;\, Yu Tian$^2$ \;\, Haichao Yu$^2$ \;\, Heng Wang$^2$\;\, Pedro Morgado$^1$ \\ Yu Hen Hu$^1$\;\, Linjie Yang$^2$ \\
\vspace{0.5em}
$^1$University of Wisconsin-Madison \,\; $^2$ByteDance Inc. \\
{\tt\small \{cwu356, pmorgado, yhhu\}@wisc.edu \,\{yutian.yt, haichaoyu, heng.wang, linjie.yang\}@bytedance.com

}\vspace{-1em}
}



\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Vision-language models such as CLIP~\cite{Radford2021CLIP} learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 
1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 
2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. 
Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at \url{https://github.com/CEWu/PTNL}.
\end{abstract}

%%%%%%%%% BODY TEXT
% %--------------------------Introduction----------------------------------
\input{sections/01.Introduction}

% %--------------------------Related Work----------------------------------
\input{sections/02.Relatetd}

% % %--------------------------Methodology-----------------------------------
\input{sections/03.Method_noise}

% % %--------------------------Experiments-----------------------------------
\input{sections/04.Experiments}

% %--------------------------Conclusion---------------------------------
\input{sections/05.Conclusion}

%-------------------------------------------------------------------------


{\small
\bibliographystyle{ieee_fullname}
\bibliography{main_arxiv}
}

\clearpage
\appendix 
\input{sections/06.Supplement}


\end{document}
