% Figure environment removed


\section{Analysis of Prompt Tuning with Label Noise}
\label{sec:exp-robustness}
Methods based on prompt tuning for CLIP~\cite{Radford2021CLIP} have been shown to be effective in few-shot learning~\cite{Zhou2022CoOp,zhou2022CoCoOp}. However, these methods have been studied on datasets with perfect labels. It remains unknown how prompt tuning performs under label noise. We explore this practical training setting and present our key findings.

\subsection{Experimental Settings}

\noindent\textbf{Datasets.} We conduct in-depth studies on a diverse set of visual tasks, including generic object classification, fine-grained recognition, action recognition, and texture identification. We conduct our experimental analysis on eight datasets, OxfordPets~\cite{parkhi2012pets}, Food101~\cite{bossard2014food}, DTD~\cite{cimpoi2014DTD}, UCF101~\cite{soomro2012ucf101}, Flowers102~\cite{nilsback2008flower}, FGVCAircraf~\cite{maji2013aircraft}, Caltech101~\cite{fei2004cal101} and ImageNet~\cite{recht2019imagenet}.
Since one of the main benefits of prompt tuning is its data efficiency~\cite{Huang2022UPL}, we focus our studies on a 16-shot image classification problem, i.e.~for each dataset, we select 16 images per class as our training set. To examine the effect of noise in prompt tuning, we randomly perturb training labels with different levels of noise rate (12.5\%, 25\%, and 50\%). Unless otherwise specified, noisy labels are drawn uniformly at random from other categories of the dataset. We report average results over four runs with different training sets in all experiments.

\noindent\textbf{Backbone.} We adopt pre-trained CLIP models, namely using the 63M parameter text Transformer~\cite{vaswani2017attention} as the text encoder, and either a ResNet-50~\cite{he2016resnet} or a ViT-B/32~\cite{dosovitskiy2020ViT} as the visual encoder. Following CoOp~\cite{zhou2022CoCoOp}, we use 16 learnable tokens in each prompt shared across all categories. 

\noindent\textbf{Optimization.} Models are trained with a batch size of 32 for 50 epochs, using stochastic gradient descent (SGD) with momentum of 0.9 and an initial learning rate of 0.002, annealed to zero with a cosine decay schedule.


\subsection{Prompt Tuning Is Robust to Noisy Labels}
\label{sec:method_noise}


The core observation of this paper is that prompt tuning vision-language models, such as CLIP, is surprisingly robust to noisy labels.
This can be observed by comparing prompt tuning for CLIP with two traditional transfer learning approaches: 1) training a linear classifier on CLIP's visual representations (denoted CLIP Linear Probe); and 2) fine-tuning the same visual backbone pre-trained on ImageNet. 
The results on two datasets, DTD and UCF101, are shown in Figure~\ref{fig:CLIP-PTvsRN5-FT} (a) and (b), respectively. As can be seen, although linear probes and fine-tuning achieve competitive performance with perfectly labeled data (0\% noise rate), both procedures suffer from a significant accuracy drop with higher noise rates of 25\% and 50\%.
This result shows that prompt tuning is naturally more resistant to noisy labels than the alternatives. We show nevertheless that its robustness can be further enhanced by training the prompt using the robust generalized cross-entropy loss (denoted CLIP Prompt Tuning (GCE) in Figure~\ref{fig:CLIP-PTvsRN5-FT}). As can be seen, when combining Prompt Tuning and GCE, the model's performance remains highly competitive, even for noise rates as high as 50\%.
Furthermore, we observe that this robustness stems from the combination between Prompt Tuning and GCE, and not from GCE alone. This can be seen in Figure~\ref{fig:GCE_PTvsLP}, which depicts the noise robustness of Prompt Tuning and Linear Probes both trained under the cross-entropy and GCE losses on four datasets. While the robustness of the linear probe also improves with a GCE loss, the performance drop at high noise rates is significantly smaller when learning through prompt tuning.

Now that we have established the noise robustness of prompt tuning, the remainder of this Section is dedicated to providing intuitions and experimental analysis to answer the why question.
\begin{displayquote}
\textbf{\textit{Question:}} \textit{Why is prompt tuning for CLIP-like vision-language models more robust than traditional transfer learning against noisy labels?}
\end{displayquote}


\input{tables/clip_components}

% \subsection{Which module contributes to noise robustness}
\subsection{Robustness Attribution}
\label{sec:reason}
To answer this question, we begin by analyzing two key components of CLIP in isolation, namely the generated class embeddings and learnable prompts. 
% Specifically, we only allow one of the units to update its corresponding parameters during few-shot learning while the parameters of the other two units remain fixed. 

\noindent \textbf{Pre-trained CLIP generates effective class embeddings.} 
We first analyse the impact of the class embeddings generated by the CLIP text encoder. To this end, in addition to the class embeddings generated through prompt tuning, we assess the noise robustness of three different models:
\begin{description}
    \setlength\itemsep{0.05em}
    \item[Classifier-R] Trains a linear probe on the output of CLIP's pre-trained visual encoder. The class embeddings (i.e., the classifier weights) are initialized at \emph{random}, and learned without constrains. See Figure~\ref{fig:PT_ablation_TEnc_Prompt} (a).
    \item[Classifier-C] Similar to Classifier-R, but the classifier weights are initialized using the text embeddings $\mathbf{f}_c^t$ obtained from CLIP's pre-trained text encoder for the handcrafted prompt. Note that Classifier-C only uses the CLIP text encoder for initializing its weights. See Figure~\ref{fig:PT_ablation_TEnc_Prompt} (b).
    \item[TEnc-FT] Trains a CLIP classifier, by associating the image embedding ${\bf f}^v$ with the CLIP text embedding ${\bf f}^t$ of the correct class through the posterior of eq.~(\ref{eq:cos_sim}).
    In this case, the entire CLIP text encoder is \emph{fine-tuned} on an hand-crafted prompt of the form ``A photo of a $<$CLS$>$''.
    See Figure~\ref{fig:PT_ablation_TEnc_Prompt} (c). 
\end{description}

\input{tables/prompt-ablation}

Table~\ref{tbl:classifier_comp} compares the various models on four datasets under different levels of label noise. 
The linear classifier with CLIP initialization (Classifier-C) outperformed random initialization across all levels of noise. This shows that CLIP class embeddings provide a strong initialization for few-shot learning. Furthermore, although both Classifiers degrade considerably with high noise ratios, the CLIP initialization is also more robust to noise. 
As for TEnc-FT, it achieved competitive performance at zero noise rates, but its accuracy also dropped significantly as the noise rate increased. This highlights (unsurprisingly) that the highly expressive CLIP text encoder can easily overfit to the noisy labels. 
Finally, Prompt Tuning outperformed all alternative strategies across all noise rates. The advantage of prompt tuning was especially large for high noise levels. These observations confirm that (a) the text encoder is essential for providing a strong but informative regularization of the text embeddings to combat noisy inputs (Prompt Tuning v.s. classifiers); and (b) the text encoder should be fixed to prevent overfitting (Prompt Tuning v.s. TEnc-FT). 


% Figure environment removed

\noindent \textbf{Effectiveness of prompt.} The previous experiment showed that the class embeddings generated by CLIP pre-trained text encoder plays a critical role in noise robustness. 
Next, we keep the text encoder fixed, and attempt to answer another question: \textit{Which components of the prompt provide noise robustness to prompt tuning?}

We hypothesize that the {{classname}} token $\mathbf{w}_c$ provides a strong regularization to the model, since it is leveraged by the text encoder to encode relationships between the different visual concepts (e.g.~how similar or different classes are from each other). Respecting this structure could help the model avoid fitting noisy data during training. To verify our hypothesis, we assess the noise robustness of two additional models:
\begin{description}
    \setlength\itemsep{0.05em}
    \item[Full Prompt Tuning] Learns the {{classname}} token jointly with the original learnable tokens (see Figure~\ref{fig:PT_ablation_TEnc_Prompt} (e)).
    \item[CLS Tuning] Adopts a \textit{fixed} template prompt ``A photo of a $<$CLS$>$'' and optimizes only the classname token (see Figure~\ref{fig:PT_ablation_TEnc_Prompt}(f)).
\end{description}

Table~\ref{tbl:prompt_comp} shows the analysis on four dataset for different noise levels. Compared to prompt-tuning, which optimizes only learnable tokens shared across all classes, both CLS-Tuning and Full-Prompt-Tuning models struggle at high noise rate. Even when the training data is clean, learning the {{classname}} tokens produces worse performance on two of the four datasets (OxfordPets and Food101).
This analysis validates our assumption that the fixed {{classname}} token is indeed a critical regularization for the prompt tuning. Learnable classname tokens can be fitted to the noisy training data, perturbing the class embeddings and leading to worse performance. 



\subsection{Prompt Tuning Suppresses Noisy Gradients}
\label{sec:supress}

The previous section provided clear evidence of the robustness of the prompt tuning framework in comparison to other alternatives.
These findings suggest that, by learning only shared prompt tokens, prompt tuning focuses better on clean samples than noisy samples. In other words, prompt tuning can suppress gradient updates from noisy samples, while aggregating gradients from clean samples.
To verify this hypothesis, we measure the gradients with respect to the learnable parameters of both CLIP prompt tuning and linear probing using 50\% noise rate. Specifically, we measure the ratio between the gradient norm induced by noisy samples and that induced by clean samples. A ratio above one indicates that noisy samples play a bigger role in the optimization than clean samples. 


Figure \ref{fig:gradient} shows the noisy-to-clean gradient norm ratio as models are trained on four datasets. As can be seen, prompt tuning displays significantly lower ratios than linear probing. This indicates that noisy samples play a comparatively small role with prompt tuning compared to linear probes.
This property likely arises from the highly constrained prompt tuning optimization, which restricting the model to fit the noisy labels. 



% Figure environment removed

\input{tables/archs-dbs}

\subsection{Generalization Across Model Architectures}
Previous sections have focused on four datasets (OxfordPets, Food101, DTD, and UCF101) and a ResNet-50 image encoder. We now show that these findings generalize across model architectures and datasets.


\noindent\textbf{Context length.} We first assess the noise robustness of prompt tuning with increasing numbers of learnable tokens. We also evaluate a baseline without any learnable tokens by directly feeding the classname into the model (denoted as Ctx-0). Figure \ref{fig:ctx_length} shows that the optimal context lenght is dataset dependent, but all context lengths achieve superior performance compared to traditional linear probing. Ctx-0 outperforms some prompt tuning variants under large noise rates at 50\%, suggesting fixed prompts may be a good choice when the labeling noise is too strong on the downstream task. 


\noindent\textbf{Image encoders.} To validate whether the noise robustness of prompt tuning is backbone-agnostic, we also assess CLIP with ViT-B/32 for prompt tuning (denoted ViT-B/32-PT). Table \ref{tbl:PT_on_more_datasets} shows the comparison with RN50-PT. ViT-B/32-PT outperforms RN50-PT under most settings. Moreover, both methods do not suffer from a large performance drop and maintain competitive accuracy at high noise rates.
\input{tables/noise-type}


\subsection{Robustness to Correlated Label Noise}
So far, we assumed white label noise (i.e., noisy labels are uniformly drawn from the label space). However, label noise produced by either human annotators or machine-generated pseudo labels often displays correlations between similar visual concepts.
For example, UPL~\cite{Huang2022UPL} observed that pre-trained CLIP prefers some classes over others during zero-shot transfer. Inspired by this observation, we examine whether CLIP inherent preferences affect the performance of prompt tuning when confronted with CLIP-generated label noise. 

We begin by measuring the confusion matrix of CLIP zero-shot predictions with \emph{randomly initialized} learnable tokens on the OxfordPets and UCF101 datasets (see Figure \ref{fig:confusion}). 
Next, we introduce a challenging type of label noise, named \emph{Confusion} noise, where each mislabeled sample is labeled as the incorrect class that is most favored by zero-shot CLIP. 
Finally, we examine the transfer performance of prompt tuning with both random and confusion noise at a 50\% noise rate. Table~\ref{tbl:noise_type_ablation} presents the results on four datasets. As can be seen, confusion noise presents a bigger challenge to transfer learning, leading to larger degradation of classification accuracy at high noise ratios compared to random noise.
Such degradation is visible both for prompt tuning and linear probes. However, among the two, prompt tuning still achieves the best overall performance, providing further evidence for its robustness even to more challenging types of noise.



% Figure environment removed


% Figure environment removed


\section{Application to Unsupervised Prompt Tuning}
\label{sec:UPT}
Prior work UPL~\cite{Huang2022UPL} demonstrated that unsupervised prompt tuning can outperform the transfer performance of zero-shot transfer based on CLIP. However, UPL does not fully utilize the noise robustness of prompt tuning. 

\noindent\textbf{Baseline UPL.} UPL~\cite{Huang2022UPL} proposed a framework to adapt CLIP for downstream tasks without any labeled images. An overview of the framework is shown in Figure \ref{fig:upl_diagram}. This framework is divided into two phases. 
In phase 1, UPL leverages pre-trained CLIP to generate pseudo labels for unlabeled images. 
Then, in phase 2, a set of $K$ pseudo-labels are chosen to optimize the learnable tokens through the typical prompt-tuning optimization process (described in CoOp~\cite{Zhou2022CoOp}). 
To increase the quality of training examples, UPL ranks all pseudo-labeled images based on their confidence score (Eq.~\ref{eq:cos_sim}) and selects the $K$ most confident samples per class.
Furthermore, inspired by prompt ensembling in CLIP~\cite{Radford2021CLIP}, UPL improved transfer performance by ensembling multiple predictions generated by models with different learnable prompts. 

\noindent\textbf{Robust UPL.} 
In Section~\ref{sec:exp-robustness}, we showed that prompt tuning can be robust to noisy labels. Furthermore, we showed that prompt tuning robustness can be further strengthened using the generalized cross-entropy loss (GCE). Given these observations, we propose to perform unsupervised prompt tuning by 
1) randomly sample training samples  and 2)  optimizing the prompt with the robust GCE loss.
Random sampling has two effects. On the one hand, it increases the diversity of training samples which benefits learning. On the other hand, it increases the amount of label noise. However, we expect the label noise to be tolerable by our robust prompt tuning framework. 


\noindent\textbf{Experimental Settings.} We experiment with the unsupervised prompt tuning following the same training setting of Section \ref{sec:exp-robustness}. Pseudo-labels are generated by CLIP zero-transfer with ResNet50 image encoder. We follow the prompt engineering used by CLIP. There are three types of hand-crafted prompts, with more details listed in the supplementary material.
$K$ is set to 16 in all experiments. During the inference stage, we employ the ensemble-average approach following UPL~\cite{GenXEnt} to generate predictions combining the outputs of four distinct models. Each model has a distinct learnable prompt that was initialized with a unique random seed.

\noindent\textbf{Experimental Results.}
We compared UPL~\cite{Huang2022UPL} and the proposed Robust UPL on a diverse set of visual tasks, including generic object classification, fine-grained recognition, and texture identification. We also assessed Robust UPL using both a cross-entropy (CE) and generalized cross-entropy (GCE) losses. Table \ref{tbl:top16vsrandom} shows that all three unsupervised prompt tuning methods can improve transfer learning over zero-shot predictions, at no additional labeling cost. Among the three methods, Robust UPL trained under GCE loss obtains the best performance on average. We highlight once again that Robust UPL randomly samples pseudo labeled images for training, instead of using high-confidence samples as in UPL. As a result, UPL training pseudo-labels are less diverse, but have less noise. For example, the pseudo-labels used to train UPL on Caltech were 93\% correct, while the pseudo-labels used to train Robust UPL were only 83\% correct. Nevertheless, these errors did not harm final performance of Robust UPL; on the contrary,  learning from a more diverse set, while being robust to the noise enhanced prompt tuning. 


\input{tables/upt}