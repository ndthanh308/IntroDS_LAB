\section{Extended Experimental Results}

\noindent\textbf{Robustness Attribution.}
The findings presented in Table~\ref{tbl:supp_classifier_comp} indicate that the reason behind the noise robustness observed in prompt tuning can be attributed to the structured form imposed on class embeddings by CLIP's pre-trained text encoder.

\setlength{\tabcolsep}{3pt}
\begin{table}[ht]
\small
\centering
\begin{tabular}{cl|cccccc}
\toprule
\multirow{2}{*}{\bf Dataset} &
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} \bf Method \end{tabular}}
&\multicolumn{4}{c}{\bf Noise rate} \\ 
\multicolumn{2}{l|}{}                                 
& 0 & 12.5 & 25 & 50 \\ \cline{3-6} \hline

% \multirow{4}{*}{OxfordPets} 
% & VEnc-LP 
% & 76.59
% & 65.11
% & 57.62
% & 35.70 \\ 
& Classifier-R
& 74.82
& 64.10
& 55.96
& 36.63 \\
& Classifier-C
& 81.47
& 70.29
& 61.87
& 44.21\\
& TEnc-FT
& 84.38
& 70.73
& 61.11
& 41.21 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} OxfordPets} 
& Prompt Tuning
& \textbf{87.89}
& \textbf{84.62}
& \textbf{81.20}
& \textbf{73.13} \\ \hline

& Classifier-R
& 88.19
& 74.48
& 61.14
& 42.68 \\
& Classifier-C
& 89.94
& 77.04
& 63.81
& 45.96\\
& TEnc-FT
& \textbf{90.75}
& 76.67
& 62.76
& 46.45 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} Caltech101} 
& Prompt Tuning
& 90.65
& \textbf{82.51}
& \textbf{78.70}
& \textbf{70.13} \\ \hline

& Classifier-R
& 83.40
& 69.58
& 60.85
& 37.74 \\
& Classifier-C
& 94.11
& 80.26
& 69.18
& 47.55\\
& TEnc-FT
& \textbf{94.62}
& 80.91
& 70.83
& 49.54 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} Flowers102} 
& Prompt Tuning
& 91.71
& \textbf{86.24}
& \textbf{81.92}
& \textbf{71.80} \\ \hline


& Classifier-R
& 63.80
& 54.66
& 46.23
& 28.97 \\
& Classifier-C
& 69.36
& 60.46
& 51.85
& 34.37\\
& TEnc-FT
& 71.30
& 61.60
& 52.64
& 34.74 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} Food101} 
& Prompt Tuning
& \textbf{76.99}
& \textbf{73.63}
& \textbf{71.07}
& \textbf{64.30} \\ \hline

& Classifier-R
& 30.38
& 24.84
& 20.89
& 13.32 \\
& Classifier-C
& 34.46
& \textbf{29.97}
& \textbf{25.91}
& 17.36\\
& TEnc-FT
& \textbf{35.30}
& 29.66
& 25.31
& 17.42 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} FGVCAircr} 
& Prompt Tuning
& 27.13
& 25.07
& 23.34
& \textbf{19.05} \\ \hline


& Classifier-R
& 48.02
& 44.30
& 40.32
& 30.10 \\
& Classifier-C
& \textbf{63.83}
& 57.14
& 50.36
& 34.86\\
& TEnc-FT
& 63.61
& 55.47
& 48.21
& 33.12 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} DTD} 
& Prompt Tuning
& 62.86
& \textbf{58.90}
& \textbf{53.62}
& \textbf{46.19} \\ \hline

& Classifier-R
& 67.16
& 58.33
& 50.34
& 31.07 \\
& Classifier-C
& 71.87
& 64.12
& 54.79
& 38.01\\
& TEnc-FT
& \textbf{73.74}
& 64.52
& 56.10
& 37.88 \\
\rowcolor{tabhighlight}
\multirow{-4}{*}{\cellcolor{white} UCF101} 
& Prompt Tuning
& 73.12
& \textbf{68.73}
& \textbf{67.66}
& \textbf{60.93} \\ 

\bottomrule
\end{tabular}
\caption{Comparison of transfer performance at incremental noise rates between different variants.}
\label{tbl:supp_classifier_comp}
\end{table}


\setlength{\tabcolsep}{2pt}
\begin{table}[h]
\small
\centering
\begin{tabular}{ll|cccccc}
\toprule
\multirow{2}{*}{Dataset} &
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Method \end{tabular}}
&\multicolumn{4}{c}{Noise rate} \\ \cline{3-6} 
\multicolumn{2}{l|}{}                                 
& 0 & 12.5 
& 25
& 50 \\ \cline{3-6} \hline
& Full-Prompt-Tuning
& 85.39
& 74.00
& 68.66
& 50.50\\
& CLS-Tuning
& 85.04
& 77.02
& 71.03
& 53.15 \\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} OxfordPets} 
& Prompt Tuning
& \textbf{87.89}
& \textbf{84.62}
& \textbf{81.20}
& \textbf{73.13} \\ \hline

& Full-Prompt-Tuning
& 89.21
& 74.20
& 61.26
& 45.92 \\
& CLS-Tuning
& 89.13
& 76.84
& 62.27
& 48.64\\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} Caltech101} 
& Prompt Tuning
& \textbf{90.65}
& \textbf{82.51}
& \textbf{78.70}
& \textbf{70.13}\\ \hline

& Full-Prompt-Tuning
& \textbf{93.93}
& 83.58
& 77.00
& 59.52 \\
& CLS-Tuning
& 93.47
& 84.19
& 78.74
& 61.79\\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} Flowers102} 
& Prompt Tuning
& 91.71
& \textbf{86.24}
& \textbf{81.92}
& \textbf{71.80}\\ \hline

& Full-Prompt-Tuning
& 72.36
& 63.14
& 55.29
& 38.69 \\
& CLS-Tuning
& 72.07
& 63.91
& 56.97
& 41.73\\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} Food101} 
& Prompt Tuning
& \textbf{76.99}
& \textbf{73.63}
& \textbf{71.07}
& \textbf{64.30} \\ \hline


& Full-Prompt-Tuning
& \textbf{32.28}
& \textbf{28.16}
& \textbf{24.67}
& 16.76 \\
& CLS-Tuning
& 30.84
& 27.86
& 24.51
& 17.63\\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} FGVCAircraft} 
& Prompt Tuning
& 27.13
& 25.07
& 23.34
& \textbf{19.05}\\ \hline

& Full-Prompt-Tuning
& 62.80
& 55.50
& 49.01
& 34.66 \\
& CLS-Tuning
& 62.78
& 56.15
& 48.46
& 35.43\\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} DTD} 
& Prompt Tuning
& \textbf{62.86}
& \textbf{58.90}
& \textbf{53.62}
& \textbf{46.19}\\ \hline

& Full-Prompt-Tuning
& 73.02
& 64.31
& 57.11
& 40.42 \\
& CLS-Tuning
& 72.73
& 65.64
& 58.91
& 44.55\\
\rowcolor{tabhighlight}
\multirow{-3}{*}{\cellcolor{white} UCF101} 
& Prompt Tuning
& \textbf{73.12}
& \textbf{68.73}
& \textbf{67.66}
& \textbf{60.93}\\

\bottomrule
\end{tabular}
\caption{Comparison of transfer performance at incremental noise rates between different prompt designs.}
\label{tbl:supp_prompt_comp}
\end{table}


Table~\ref{tbl:supp_prompt_comp} validates two observations: (a) the significance of the text encoder in offering robust regularization of the text embeddings to tackle noisy inputs (Prompt Tuning versus classifiers); and (b) the necessity of fixing the text encoder to prevent overfitting (Prompt Tuning versus TEnc-FT).

 \noindent\textbf{Robustness to Correlated Label Noise.}
 Table~\ref{tbl:supp_noise_type_ablation} shows that transfer learning faces a greater challenge with confusion noise, resulting in a great decline in classification accuracy at higher noise ratios as opposed to random noise. This decline is evident in both prompt tuning and linear probes. The robustness of prompt tuning is evident in its ability to outperform linear probes, even when faced with more challenging noise types.

\noindent\textbf{Integration with Noise-Robust Losses.}
We examine the effectiveness of a robust loss function
applied to prompt tuning with noisy labels. In this study, We adopt  Generalized Cross Entropy (GCE)~\cite{GenXEnt} as a representative of robust loss functions for noise-robust learning. Specifically, cross-entropy loss in Eq.~2 is replaced with GCE loss during the training process. Figure~\ref{fig:supp_GCE_comp} shows results of applying GCE loss to prompt tuning and linear probing for CLIPâ€™s vision encoder. We observe that both transfer learning methods obtain an improvement of noise robustness by training with GCE loss. In particular, prompt tuning further enhances its inherent noise robustness. This outcome suggests that prompt tuning offers great applicability to couple with existing noise-robust loss functions. In addition to GCE, Figure~\ref {fig:rubust_losses} shows two additional robust loss functions: Symmetric Cross Entropy (SCE)~\cite{wang2019symmetric} and Normalized Cross Entropy (NCE) combined with a Reverse Cross Entropy (RCE)~\cite{ma2020normalized_loss}. Both losses also improve the noise robustness of prompt tuning, but GCE still achieves slightly better performance.

% Figure environment removed


\setlength{\tabcolsep}{9pt}
\begin{table}[h]
\small
\centering
\begin{tabular}{l|l|c}
\toprule
Dataset &  Method  & 0-Shot\\ \hline
OxfordPets &  Random Prompt  & 40.93\std{11.19}
 \\ \hline
Caltech101 &  Random Prompt  & 59.65\std{9.56}
 \\ \hline
 Flowers102 &  Random Prompt   & 14.86\std{8.40}
 \\ \hline
 Food101 &  Random Prompt  & 34.63\std{11.10}
 \\ \hline
 FGVCAircraft &  Random Prompt   & 3.43\std{1.97}
 \\ \hline
 DTD &  Random Prompt  & 21.20\std{3.51}
 \\ \hline
 UCF101 &  Random Prompt   & 32.93\std{5.48}
 \\ 
\bottomrule
\end{tabular}
\caption{CLIP zero-shot with random prompts.}
\label{tbl:supp_zero-shot_random}
\end{table}

\setlength{\tabcolsep}{7pt}
\begin{table}[h]
\centering
\small
\begin{tabular}{ll|cc}
\toprule
\bf Dataset & \bf Method & \bf Random & \bf Confusion \\ \cline{1-4}
\multirow{2}{*}{OxfordPets}
& Linear Probe  & 46.42\std{0.88} & 41.39\std{1.87} \\
& Prompt Tuning & 73.13\std{3.76} & 66.55\std{2.02} \\
\hline
\multirow{2}{*}{Caltech101}
& Linear Probe  & 56.24\std{1.96} & 56.25\std{6.92} \\
& Prompt Tuning & 70.13\std{3.76} & 70.86\std{1.83} \\
\hline
\multirow{2}{*}{Flowers102}
& Linear Probe  & 68.92\std{0.76} & 45.94\std{0.69} \\
& Prompt Tuning & 71.80\std{1.00} & 69.63\std{1.31} \\
\hline
\multirow{2}{*}{Food101} 
& Linear Probe & 42.63\std{0.89} & 37.71\std{0.52} \\
& Prompt Tuning & 64.30\std{2.58} & 63.93\std{1.45} \\
\hline
\multirow{2}{*}{FGVCAircraft} 
& Linear Probe & 21.98\std{0.48} & 15.38\std{0.71} \\
& Prompt Tuning & 19.05\std{1.06} & 18.04\std{1.32} \\
\hline
\multirow{2}{*}{DTD}
& Linear Probe & 42.29\std{2.12}  & 37.69\std{1.70} \\
& Prompt Tuning & 46.19\std{2.12}  & 45.76\std{1.23} \\
\hline
\multirow{2}{*}{UCF101}
& Linear Probe & 54.05\std{1.19}  & 50.90\std{1.45} \\
& Prompt Tuning & 60.93\std{0.94} & 59.11\std{0.70} \\
\bottomrule
\end{tabular}
\caption{The impact of random and confusion label noise at a 50\% noise rate on Linear Probing and Prompt Tuning strategies.}
\label{tbl:supp_noise_type_ablation}
\end{table}

% Figure environment removed


\section{Capacity of classifiers with random prompt tokens} 
Prompt tuning has a limited parameter space given by the length of the prompt tokens.
This parameter space is fundamentally different from that of a whole neural network and may present special properties related to the robustness of the model. 
Instead of updating the learnable prompts with noisy data, we evaluate the classifiers with \emph{random prompts }. Table~\ref{tbl:supp_zero-shot_random} summarizes the average zero-shot performance over 100 runs. Surprisingly, the results show that CLIP can achieve non-trivial zero-shot performance, even with random prompts. 
This indicates that as long as the classname token is provided to the pre-trained text encoder, CLIP is capable of computing non-trivial class embeddings for generic image classification.

\section{Unsupervised Prompt Tuning Settings} Pseudo-labels are generated by CLIP zero-transfer with ResNet50 image encoder. We follow the prompt engineering used by CLIP. There are three types of hand-crafted prompts: "A photo of a $<$label name$>$" for generic object datasets; "A photo of a $<$label name$>$, a type of $<$collective name$>$" for fine-grained object datasets (e.g., prompts for OxfordPets are appended "a type of dog" or "a type of cat"); and "$<$label name$>$ texture" for the DTD dataset.
$K$ is set to 16 in all experiments.
