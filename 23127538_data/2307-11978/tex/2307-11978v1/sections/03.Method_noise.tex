% Figure environment removed

%\section{Robustness of Prompt Tuning}
\section{Prompt Tuning}
\label{sec:method}
CLIP~\cite{Radford2021CLIP} can perform zero-shot transfer by prompt engineering -- the practice of designing text inputs for downstream tasks. Specifically, in the case of image classification, a normalized image embedding $\mathbf{f}^v$ is obtained by passing an image $\mathbf{x}$ through CLIP's visual encoder, and a set of normalized class embeddings $\{\mathbf{f}^t_{i}\}^K_{i=1}$ by feeding template prompts of the form ``A photo of a $<$CLS$>$'' into CLIP's text encoder.
The class posterior is then estimated as
\begin{equation}
Pr(y=i|\mathbf{x}) =  \frac{\exp \Bigl(\mathtt{sim}(\mathbf{f}^v, \mathbf{f}^t_i) / \tau \Bigr) }{\sum_{j=1}^{K} \exp \Bigl(\mathtt{sim}(\mathbf{f}^v, \mathbf{f}^t_j)/\tau\Bigr)},
\label{eq:cos_sim}
\end{equation}
where $\tau$ is a temperature factor learned by CLIP and $\mathtt{sim}$ denotes cosine similarity.

\paragraph{Prompt Tuning}
Although CLIP is capable of zero-shot transfer, its performance is sensitive to designed text prompts. 
To avoid the need for hand-crafted prompts and improve transfer performance, CoOp~\cite{Zhou2022CoOp} showed that text prompts can be replaced with continuous soft prompts that can be optimized on a target dataset. 
Specifically, the name of a class $c$ is first converted into a classname embedding $\mathbf{w}_c \in \mathbb{R}^{d}$ and prepended with a sequence of $M$ learnable tokens $\mathbf{p}_m \in \mathbb{R}^{d}$ shared across all classes. The full prompt $\mathrm{P_c}=[\mathbf{p}_1,\mathbf{p}_2,...\mathbf{p}_M,\mathbf{w}_c]$ for each class $c$ is then processed by CLIP's text encoder to compute the corresponding text embedding $\mathbf{f}^t_c$, and the class posteriors $Pr(y=i|\mathbf{x})$ are obtained once again through Eq.~\ref{eq:cos_sim}.
To adapt the prompt to the target dataset, CoOp~\cite{Zhou2022CoOp} optimizes the shared learnable tokens $\mathbf{p}_1,\mathbf{p}_2,...\mathbf{p}_M$ on a small labeled dataset $\mathcal{D}=\{({\bf x}_i, c_i)_{i=1}^N\}$ to minimize the cross-entropy loss
\begin{equation}
    \mathcal{L}_{CE} = -\mathbb{E}_{(\mathbf{x}, c)\in\mathcal{D}} \left[\log 
    Pr(y=c|\mathbf{x})\right].
    \label{eq:lossCE}
\end{equation}


\paragraph{Robust Prompt Tuning}
In this work, we show that the prompt tuning framework~\cite{Zhou2022CoOp}, describe above, displays surprising robustness to noisy labels. However, this robustness can be further enhanced by optimizing the learnable prompts using the generalized cross-entropy (GCE) loss~\cite{GenXEnt}, a robust generalization of cross-entropy loss. Formally, the GCE loss is defined as
\begin{equation}
    \mathcal{L}_{GCE} = \mathbb{E}_{(\mathbf{x}, c)\in\mathcal{D}} \left[
        \frac{1 - Pr(y=c|\mathbf{x})^q}{q}
    \right].
    \label{eq:lossGCE}
\end{equation}
As shown in~\cite{GenXEnt}, GCE is equivalent to the standard cross-entropy loss of Eq.~\ref{eq:lossCE} when $x \to 0$, and equivalent to the (robust) mean absolute error (MAE) loss ${\|1 - Pr(y=c|\mathbf{x})\|_1}$ when $q = 1$. The hyper-parameter $q$ can therefore control the tradeoff between the highly robust but less performing MAE loss and the less robust but highly performing CE loss. While the optimal value for $q$ could be adjusted to the amount of noise by cross-validation, we found that $q=0.7$ lead to overall good performance across several experimental settings.
