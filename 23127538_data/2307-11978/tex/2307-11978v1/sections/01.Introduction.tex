\section{Introduction}
\label{sec:intro}
% Figure environment removed

Large-scale vision-language models such as CLIP~\cite{Radford2021CLIP}, ALIGN~\cite{jia2021_align}, and CoCa~\cite{yu2022coca} are transforming how we learn and interact with visual representations. Since these models learn to align the representations of a broad set of natural images with their textual descriptions, they have shown an exceptional ability to solve a wide range of tasks in a data-efficient manner. For example, using the pre-trained text encoder, one can obtain a set of class embeddings by encoding a canonical sentence such as ``A photo of a $<$CLS$>$'' and use them to recognize objects without a labeled dataset. While promising, Zhou et al.~\cite{Zhou2022CoOp} showed that these human-defined sentences (also known as class prompts) can be unstable, with seemingly equivalent descriptions leading to different predictions. To address this issue, researchers have focused on prompt tuning~\cite{Zhou2022CoOp}, where a learnable prompt is learned from a small target dataset by back-propagation. Since only the prompt needs to be trained, this framework is very data-efficient. As a result, prompt-tuning has gained popularity for adapting vision-language models to downstream tasks like few-shot learning~\cite{Zhou2022CoOp,zhou2022CoCoOp}, continual learning~\cite{wang2022continual}, and object segmentation~\cite{rao2022denseclip}.


While prompt tuning has proven effective when training on downstream tasks with accurately annotated datasets, their robustness to noisy labels has been neglected. Since the quality of annotations for many applications can be low, learning with noisy labels is critical to solving real-world problems. In this work, we demonstrate that prompt tuning is robust to noisy labels, and investigate the mechanisms that enable this robustness. We hypothesize that the joint text and image embeddings of vision-language models can provide a well defined structure to the classification space (e.g., which classes are most similar and most distinct from each other). This model-informed structure compensates for
the degradation of the structure present in the data due to label noise. To verify this hypothesis, we conducted extensive experiments to study the impact of each component of a prompt tuning task with noisy labeled data. 
Beyond the robustness conferred by the structured label space, we show that this robustness can be further enhanced when the learnable prompts are trained using a robust loss function that mitigates the impact of outliers. Our study has revealed several interesting findings.


First, the classification performance obtained by tuning the prompt through a pre-trained CLIP model is significantly more robust to noisy labels than the traditional fine-tuning or linear probing paradigms (see Figure~\ref{fig:CLIP-PTvsRN5-FT}).
The robustness of prompt tuning is evident not only due to their smaller performance degradation with higher noise rates, but also due to its ability to diminish the gradients induced by noisy samples.
Second, while priming each class with a shared learnable prompt is necessary for adaptation, ensuring that the class name remains in the prompt strongly regularizes the class embeddings and prevents overfitting to the noisy labels. 
Finally, we demonstrate the benefits of this robustness by showing that CLIP zero-shot (noisy) predictions can be used to tune its own prompt, and significantly enhance CLIP prediction accuracy. In fact, we show that, instead of focusing on samples with confident predictions (as proposed in prior unsupervised prompt tuning approaches~\cite{Huang2022UPL}), prompt tuning benefits more from an increased diversity of training samples as it can tolerate the noisier predictions associated with them.


The main contributions of our work are as follows:
\begin{itemize}
    \setlength\itemsep{0.1em}
    \item We demonstrate that prompt tuning for pre-trained vision-language models (e.g., CLIP) is more robust to noisy labels than traditional transfer learning approaches, such as model fine-tuning and linear probes.
    \item We further demonstrate that prompt tuning robustness can be further enhanced through the use of a robust training objective.
    \item We conduct an extensive analysis on why prompt tuning is robust to noisy labels to discover which components contribute the most to its robustness.
    \item Motivated by this property, we propose a simple yet effective method for unsupervised prompt tuning, showing that randomly selected noisy pseudo labels can be effectively used to enhance CLIP zero-shot performance. The proposed robust prompt tuning outperformed prior work~\cite{Huang2022UPL} on a variety of datasets, even though noisier pseudo-labels are used for self-training.
\end{itemize}