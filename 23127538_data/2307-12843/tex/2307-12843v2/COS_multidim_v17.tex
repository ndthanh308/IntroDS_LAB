%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[american,english]{babel}
\usepackage{array}
\usepackage{verbatim}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{plain}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\newtheorem{assumption}{Assumption} 

\renewcommand\theassumption{A\arabic{assumption}}

\usepackage{stmaryrd}

\makeatother

\addto\captionsamerican{\renewcommand{\corollaryname}{Corollary}}
\addto\captionsamerican{\renewcommand{\definitionname}{Definition}}
\addto\captionsamerican{\renewcommand{\lemmaname}{Lemma}}
\addto\captionsamerican{\renewcommand{\propositionname}{Proposition}}
\addto\captionsamerican{\renewcommand{\remarkname}{Remark}}
\addto\captionsamerican{\renewcommand{\theoremname}{Theorem}}
\addto\captionsenglish{\renewcommand{\corollaryname}{Corollary}}
\addto\captionsenglish{\renewcommand{\definitionname}{Definition}}
\addto\captionsenglish{\renewcommand{\lemmaname}{Lemma}}
\addto\captionsenglish{\renewcommand{\propositionname}{Proposition}}
\addto\captionsenglish{\renewcommand{\remarkname}{Remark}}
\addto\captionsenglish{\renewcommand{\theoremname}{Theorem}}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\usepackage[round]{natbib}
\usepackage{breakurl}
\hypersetup{breaklinks=true, colorlinks=false, pdfusetitle=true }

\begin{document}
\title{The multidimensional COS method for option pricing}
\author{Gero Junike\thanks{Corresponding author. Carl von Ossietzky Universitt, Institut fr
Mathematik, 26129 Oldenburg, Germany, ORCID: 0000-0001-8686-2661,
E-mail: gero.junike@uol.de}, Hauke Stier\thanks{Carl von Ossietzky Universitt, Institut fr Mathematik, 26129 Oldenburg,
Germany.}}
\maketitle
\begin{abstract}
The multidimensional COS method is a numerical tool to price financial
options, which depend on several underlyings. The method makes use
of the characteristic function $\varphi$ of the logarithmic returns
of the underlyings and it is advantageous if the Fourier-cosine coefficients
$v_{\boldsymbol{k}}$ of the payoff function are given in closed-form.
However, in important cases, neither $\varphi$ nor $v_{\boldsymbol{k}}$
are given analytically but need to be recovered numerically. We make
the following main contributions: First, we prove the convergence
of the multidimensional COS method including numerical uncertainty
on $\varphi$ and $v_{\boldsymbol{k}}$. Second, we find explicit
formulas for two parameters required by the COS method in multivariate
dimensions: the truncation range and the number of terms. Third, we
show empirically and theoretically that the COS method is faster than
a Monte Carlo simulation in higher dimensions if a very high accuracy
is required.

\textbf{Keywords: }Fourier-transform, COS method, option pricing,
rainbow options, basket options\\
 \textbf{Mathematics Subject Classification} 65D30  65T40  60E10
 62P05%
\begin{comment}
91B25, 91G20, 91G60
\end{comment}
\end{abstract}

\section{Introduction}

\begin{comment}
todo:
\begin{itemize}
\item Numerische Experimente cpp (Haukle)
\item double check Parseval\textquoteright s identity by R (Hauke)
\item $\Xi$ bestimmen in Prop. (Hauke)
\item A1 erweitern (Gero)
\item show multivariate t-student is COS-admissible (Hauke)
\item Wie skaliert die COS methode mit der dimension? wie $O(N^{d})$ oder
strker, $c_{\boldsymbol{k}}$ skaliert ja wie $2^{d-1}$...
\end{itemize}
Experiments: fr $\varepsilon=10^{-5}$ und verschiedene Dimensionen:
$d=2,...,5$ und zunchst $\Sigma=id$ dann $\Sigma$ zufllige Covarianzmatrix: 
\begin{itemize}
\item bestimme cdf von multivariate Normal mit Mittelwert Null und Kovarianzmatrix
$\Sigma$ mit R-Packet bei einem bestimmten Punkt $\boldsymbol{K}\in\mathbb{R}_{+}^{d}$,
d.h.,
\[
F_{cdf}(\boldsymbol{K})=\int_{-\infty}^{K_{1}}...\int_{-\infty}^{K_{d}}f(\boldsymbol{x)}d\boldsymbol{x}=\int_{\mathbb{R}^{d}}\prod_{h=1}^{d}1_{(-\boldsymbol{\infty},K_{i}]}(x_{i})f(\boldsymbol{x)}d\boldsymbol{x}
\]
$f$ is dichte von mul. Normal.
\item Bestimme $\boldsymbol{L=M}$ und $\boldsymbol{N}$. Ansatz(?): $\boldsymbol{N}=(L_{1}\alpha,...,L_{d}\alpha)$
fr ein $\alpha>0$. Dann sollte es mglich sein, mit dem Lemma nach
$\alpha$ aufzulsen. 
\item Lse das Integral per COS methode und berprfe, ob die Fehlertol.
eingehalten wird.
\item Bestimme minimales $\text{\ensuremath{\alpha}}$ sodass Fehlertol.
eingehalten wird. Wie ntzlich ist Das Lemma?
\end{itemize}
\end{comment}
Rainbow or basket options are financial contracts that depend on several
underlyings. The price of a rainbow option can be expressed by an
integral $\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}$,
where $v$ is the payoff function describing the rainbow option and
$f$ is the density of the logarithmic returns of the underlyings.
The integral can be solved numerically using different techniques
such as Monte Carlo simulation, numerical quadrature and Fourier inversion,
see \cite{ruijter2012two} and references therein.

The COS method, see \citet{fang2009novel,junike2022precise}, is a
Fourier pricing technique. For most financial models, the density
$f$ is not known but only the characteristic function $\varphi$
of $f$ is given in closed-form. The COS method uses the characteristic
function in an efficient way to approximate the integral and compares
favorably to other Fourier pricing techniques, see \citet{fang2009novel}.
The COS method is particularly fast when not only $\varphi$ but also
the Fourier-cosine coefficients of the payoff functions $v$ are given
analytically. For instance, in multivariate dimensions, the Fourier-cosine
coefficients of geometric basket options or call-on-maximum options
can be obtained analytically, see \cite{ruijter2012two}. However,
for many rainbow options the Fourier-cosine coefficients are not given
in closed-form and need to be approximated numerically.

Furthermore, in some important cases, $\varphi$ must also be recovered
numerically and is approximated by some function $\tilde{\varphi}$.
This happens for example for some complex financial models, see, e.g.,
\cite{carr2004time}, where $\varphi$ is the solution of some ordinary
differential equation that has to be solved numerically before applying
the COS method.

This paper makes the following three main contributions: We generalize
\citet{junike2022precise} to the multidimensional case. In particular,
we prove the convergence of the COS method in a multidimensional setting.
Unlike \cite{ruijter2012two}, we include in our analysis numerical
uncertainty on the characteristic function $\varphi$ and on the Fourier-cosine
coefficients of the payoff function. This helps to understand how
approximations on $\varphi$ and the Fourier-cosine coefficients of
the payoff function affect the total error of the COS method.

The main idea of the COS method is to truncate the (unknown) density
$f$ and to approximate the truncated density by a finite cosine expansion.
As another contribution, we find explicit formulas for the two parameters
required by the COS method in multivariate dimensions: the truncation
range and the number of terms for the cosine expansion. 

We also show empirically and theoretically that the COS method is
faster than a Monte Carlo simulation in higher dimensions if a very
high accuracy is required. We provide some indications that a Monte
Carlo simulation compares favorable to the COS method in low dimensions
(like $d=3$) if a only a rough estimate of the price is sufficient.
The choice between a Monte Carlo simulation and the COS method depends
crucially on both the dimension and the error tolerance.

This article is structured as follows: In Section \ref{sec:Notation},
we fix some notation. In Section \ref{sec:Convergence-of-the} we
prove the convergence of the multidimensional COS method. In Sections
\ref{sec:How d=00003D2} we provide explicit formulas for the truncation
range and the number of terms of the cosine expansion. Using theoretic
arguments, Section \ref{sec:Monte-Carlo-versus} compares the computational
cost of the multivariate COS method to a Monte Carlo simulation. Section
\ref{sec:Conclusions} concludes.

\global\long\def\Cov{}%

\global\long\def\COS{}%


\section{\label{sec:Notation}Notation}

Let $d\in\mathbb{N}$. Let $\mathcal{L}^{1}$ and $\mathcal{L}^{2}$
denote the sets of integrable and square integrable functions from
$\mathbb{R}^{d}$ to $\mathbb{R}$, and by $\left\langle .,.\right\rangle $
and $\left\Vert .\right\Vert _{2}$ we denote the scalar product and
the (semi)norm on $\mathcal{L}^{2}$. The supremum norm of a function
$g:\mathbb{R}^{d}\to\mathbb{C}$ is defined by $\left\Vert g\right\Vert _{\infty}:=\sup_{\boldsymbol{x}\in\mathbb{R}^{d}}|g(\boldsymbol{x})|$.
By $\Re\{.\}$ we denote the real part of a complex number. $i$ is
the complex unit. By $\Gamma$, we denote the Gamma function. The
Euclidean norm on $\mathbb{R}^{d}$ is denoted by $|.|$. For $\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^{d}$
we define
\[
\boldsymbol{x}\geq\boldsymbol{y}:\Leftrightarrow x_{1}\geq y_{1},...,x_{d}\geq y_{d}
\]
and treat ``$\leq$'', ``$<$'' and ``$>$'' similarly. We set
$\mathbb{R}_{+}^{d}:=\{\boldsymbol{x}\in\mathbb{R}^{d},\boldsymbol{x}>\boldsymbol{0}\}$.
For $\boldsymbol{a},\boldsymbol{b}\in\mathbb{R}_{+}^{d}$ with $\boldsymbol{a}\leq\boldsymbol{b}$,
two complex vectors $\boldsymbol{x},\boldsymbol{y}\in\mathbb{C}^{d}$
and $\lambda\in\mathbb{C}$ we denote 
\begin{align*}
\boldsymbol{x}+\boldsymbol{y}:= & (x_{1}+y_{1},...,x_{d}+y_{d})\in\mathbb{C}^{d}\\
\boldsymbol{xy}:= & (x_{1}y_{1},...,x_{d}y_{d})\in\mathbb{C}^{d}\\
\boldsymbol{x}\cdot\boldsymbol{y}:= & x_{1}y_{1}+...+x_{d}y_{d}\in\mathbb{C}\\
\frac{\boldsymbol{x}}{\boldsymbol{y}}:= & \left(\frac{x_{1}}{y_{1}},...,\frac{x_{d}}{y_{d}}\right)\in\mathbb{C}^{d}\\
\lambda\boldsymbol{x}:= & (\lambda x_{1},...,\lambda x_{d})\in\mathbb{C}^{d}\\{}
[\boldsymbol{a},\boldsymbol{b}]:= & [-a_{1},b_{1}]\times...\times[-a_{d},b_{d}]\subset\mathbb{R}^{d}.
\end{align*}
Let $\mathbb{N}_{0}=\mathbb{N}\cup\{0\}$. For $\boldsymbol{N}=(N_{1},...,N_{d})\in\mathbb{N}_{0}^{d}$,
and a sequence $\left(\xi_{\boldsymbol{k}}\right)_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\subset\mathbb{C}$,
we define
\begin{align*}
\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\xi_{\boldsymbol{k}} & :=\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\frac{1}{2^{\Lambda(\boldsymbol{k})}}\xi_{\boldsymbol{k}},
\end{align*}
where $\Lambda(\boldsymbol{k})$ counts the number elements equal
to zero of the vector $\boldsymbol{k}$, i.e.,
\begin{equation}
\Lambda(\boldsymbol{k}):=\sum_{h=1}^{d}1_{\{k_{h}=0\}},\quad\boldsymbol{k}\in\mathbb{N}_{0}^{d}.\label{eq:Lambda}
\end{equation}
\begin{comment}
vektoren fett

i komplexe zahl

$p,k,l\in\mathbb{N}_{0}^{d}$ fr $a_{k},c_{k},e_{k}$

h fr 1,...,d

j fr 1,...,J+1
\end{comment}


\section{\label{sec:Convergence-of-the}Convergence }

Let $f:\mathbb{R}^{d}\to\mathbb{R}$ be a density with characteristic
function
\[
\varphi(\boldsymbol{u})=\int_{\mathbb{R}^{d}}f(\boldsymbol{x})e^{i\boldsymbol{u}\cdot\boldsymbol{x}}d\boldsymbol{x},\quad\boldsymbol{u}\in\mathbb{R}^{d}.
\]
For many models in finance, $\varphi$ is given in closed-form, see
\cite{ruijter2012two}. However, for some models $\varphi$ need to
be approximated by a function $\tilde{\varphi}:\mathbb{R}^{d}\to\mathbb{C}$,
e.g., in \cite{carr2004time} $\varphi$ can be expressed as the solution
to some ordinary differential equation, which itself need to be solved
numerically. Let $v:\mathbb{R}^{d}\to\mathbb{R}$ be at least locally
integrable. We aim to approximate the integral 
\begin{equation}
\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}.\label{eq:int}
\end{equation}
The function $v$ might describe the payoff of some rainbow option.
The function $f$ could be the density of the logarithmic returns
of some underlyings. The integral describes the price of the rainbow
option. Next, we describe the COS method detail; Table \ref{tab:Overview-multidimensional-COS}
provides an overview.

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|l|}
\hline 
Aim: & Approximate $\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}$
by $\sideset{}{'}\sum\tilde{c}_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}$\tabularnewline
\hline 
\hline 
\textbf{$\boldsymbol{L}$}, $\boldsymbol{M}$ & Truncation range for $f$ and $v$\tabularnewline
\hline 
$f$, $f_{\boldsymbol{L}}$ & Density and truncated density function\tabularnewline
\hline 
$v$, $v_{\boldsymbol{M}}$ & Payoff and truncated payoff function\tabularnewline
\hline 
$\varphi$ & Characteristic function of $f$\tabularnewline
\hline 
$\tilde{\varphi}$ & Numerical approximation of $\varphi$\tabularnewline
\hline 
$a_{\boldsymbol{k}}$ & \multicolumn{1}{c|}{Classical Fourier-cosine coefficients of $f_{\boldsymbol{L}}$}\tabularnewline
\hline 
$c_{\boldsymbol{k}}$ & Approximation of $a_{\boldsymbol{k}}$ through $\varphi$\tabularnewline
\hline 
$\tilde{c}_{\boldsymbol{k}}$ & Numerical approximation of $c_{\boldsymbol{k}}$ through $\tilde{\varphi}$\tabularnewline
\hline 
$v_{\boldsymbol{k}}$ & (Scaled) Fourier-cosine coefficients of $v_{\boldsymbol{M}}$\tabularnewline
\hline 
$\tilde{v}_{\boldsymbol{k}}$ & Numerical approximation of $v_{\boldsymbol{k}}$\tabularnewline
\hline 
$e_{\boldsymbol{k}}$, $e_{\boldsymbol{k}}^{\boldsymbol{L}}$ & (Truncated) cosine basis functions\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:Overview-multidimensional-COS}Overview multidimensional
COS method.}
\end{table}

Let $\boldsymbol{L}=(L_{1},...,L_{d})\in\mathbb{R}_{+}^{d}$, $\boldsymbol{x}=(x_{1},...,x_{d})\in\mathbb{R}^{d}$
and $\boldsymbol{k}=(k_{1},...,k_{d})\in\mathbb{\mathbb{N}}_{0}^{d}$.
Define the truncated density
\[
f_{\boldsymbol{L}}(\boldsymbol{x})=f(\boldsymbol{x})\prod_{h=1}^{d}1_{[-L_{h},L_{h}]}(x_{h}).
\]
We will assume that $f$ is centered around zero, then, intuitively,
$f\approx f_{\boldsymbol{L}}$ for large $\boldsymbol{L}$. Define
the basis functions
\[
e_{\boldsymbol{k}}(\boldsymbol{x})=\prod_{h=1}^{d}\cos\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right),
\]
and
\[
e_{\boldsymbol{k}}^{\boldsymbol{L}}(\boldsymbol{x})=e_{\boldsymbol{k}}(\boldsymbol{x})\prod_{h=1}^{d}1_{[-L_{h},L_{h}]}(x_{h}).
\]
The classical Fourier-cosine coefficients of $f_{\boldsymbol{L}}$
are defined by
\[
a_{\boldsymbol{k}}=\frac{1}{\prod_{h=1}^{d}L_{h}}\int_{[\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}
\]
and we approximate the coefficients $a_{\boldsymbol{k}}$ by integrating
over $\mathbb{R}^{d}$ instead of $[\boldsymbol{L},\boldsymbol{L}]$,
i.e.,
\[
c_{\boldsymbol{k}}=\frac{1}{\prod_{h=1}^{d}L_{h}}\int_{\mathbb{R}^{d}}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}.
\]
By Lemma \ref{lem:int fe} in the Appendix it follows that
\begin{align}
c_{\boldsymbol{k}} & =\frac{1}{2^{d-1}\prod_{h=1}^{d}L_{h}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\Re\left\{ \varphi\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\exp\left(i\frac{\pi}{2}\boldsymbol{s}\cdot\boldsymbol{k}\right)\right\} .\label{eq:ck}
\end{align}
The coefficients $c_{\boldsymbol{k}}$ are given analytically, if
$\varphi$ is given in closed-form. If $\varphi$ need to be approximated
by some function $\tilde{\varphi}$, we define $\tilde{c}_{\boldsymbol{k}}$
by replacing $\varphi$ by $\tilde{\varphi}$ in Equation (\ref{eq:ck}).

For $\boldsymbol{M}=(M_{1},...,M_{d})\in\mathbb{R}_{+}^{d}$ such
that $\boldsymbol{M}\leq\boldsymbol{L}$, define the (scaled) Fourier-cosine
coefficients of the payoff function $v$ by
\begin{equation}
v_{\boldsymbol{k}}=\int_{[\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})e_{\boldsymbol{k}}^{\boldsymbol{L}}(\boldsymbol{x})d\boldsymbol{x}.\label{eq:vk}
\end{equation}
The truncated payoff function is defined by 
\[
v_{\boldsymbol{M}}(\boldsymbol{x})=v(\boldsymbol{x})\prod_{h=1}^{d}1_{[-M_{h},M_{h}]}(x_{h}).
\]
In some cases, the coefficients $v_{\boldsymbol{k}}$ can be obtained
explicitly. If $v_{\boldsymbol{k}}$ are not given analytically, the
integral in Equation (\ref{eq:vk}) can also be solved numerically
and the coefficients $v_{\boldsymbol{k}}$ are approximated by some
$\tilde{v}_{\boldsymbol{k}}$, see \cite{ruijter2012two}. The idea
of the multidimensional COS method is to approximate the integral
in (\ref{eq:int}) by
\[
\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}\approx\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}},
\]
see \cite{ruijter2012two}. In the remainder of the article, we will
prove under which conditions the integral can be approximated by the
COS method. 
\begin{defn}
\label{def:COS_admissible}Let $\boldsymbol{L}=(L_{1},...,L_{d})\in\mathbb{R}_{+}^{d}$.
A function $f\in\mathcal{L}^{1}$ is called \emph{COS-admissible},
if
\[
B(\boldsymbol{L}):=\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\to0,\quad\min_{h=1,...,d}L_{h}\to\infty.
\]
\end{defn}

\begin{rem}
The definition of $B(\boldsymbol{L})$ extends \citet[Def. 1]{junike2022precise}
to the multidimensional setting. However, here we include the scaling
factor $\frac{1}{2}$ if $k_{h}=0$, $h=1,..,.d$.
\end{rem}

\begin{prop}
\label{prop:COSadmissible}Assume $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
with 
\begin{equation}
\int_{\mathbb{R}^{d}}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}<\infty.\label{eq:propBed}
\end{equation}
Let $\boldsymbol{L}=(L_{1},...,L_{d})\in\mathbb{R}_{+}^{d}$. Then
$f$ is COS-admissible and
\begin{align}
B(\boldsymbol{L}) & \leq\Xi\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\prod_{h=1}^{d}\max\left\{ \frac{x_{h}^{2}}{L_{h}^{2}},1\right\} |f(\boldsymbol{x})|^{2}d\boldsymbol{x}\label{eq:B(L)_1}\\
 & \leq\Xi\frac{1}{d\underset{h=1,...,d}{\min}L_{h}^{2d}}\bigg(\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}\bigg),\label{eq:B(L)_2}
\end{align}
where $\Xi=\frac{\pi^{2}}{3}\sum_{h=1}^{d}\left(\frac{\pi^{2}}{3}+1\right)^{h-1}$.
\end{prop}

\begin{proof}
Let $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$ and $\boldsymbol{j}\in\mathbb{Z}^{d}$.
It follows by Parseval's identity%
\begin{comment}
\#case d=2

L=rexp(2)

\#L=c(1.5,1.34)

ek=function(x,k)\{prod(cos(k{*}pi{*}(x+L)/(2{*}L)))\}

j=c(0,1)

f=function(x)\{x{[}1{]}+x{[}1{]}{*}x{[}2{]}\textasciicircum 2\}

library(cubature)

M=3

z=0

for(k1 in 0:M)\{

for(k2 in 0:M)\{

f\_tmp2=function(x)\{f(x){*}ek(x,c(k1,k2))\}

fac=1

if(k1==0)

fac=fac{*}0.5

if(k2==0)

fac=fac{*}0.5

z=z+fac{*}1/prod(L){*}(cubintegrate(f\_tmp2,2{*}j{*}L-L,2{*}j{*}L+L,maxEval=10\textasciicircum 8,relTol=1e-8)\$integral)\textasciicircum 2

\}

\}

z \#Rechte Seite

f\_tmp1=function(x)\{(f(x))\textasciicircum 2\}

cubintegrate(f\_tmp1,2{*}j{*}L-L,2{*}j{*}L+L,maxEval=10\textasciicircum 8,relTol=1e-8)\$integral
\#Linke Seite
\end{comment}
\begin{align}
 & \int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}\nonumber \\
= & \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\bigg|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})\prod_{h=1}^{d}\underbrace{\cos\left(k_{h}\pi\frac{x_{h}-(2j_{h}L_{h}-L_{h})}{2L_{h}}\right)}_{=(-1)^{j_{h}k_{h}}\cos\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right)}d\boldsymbol{x}\bigg|^{2}\nonumber \\
= & \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\bigg|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\bigg|^{2}.\label{eq:parseval}
\end{align}
By the Cauchy-Schwarz inequality, we obtain with $g(\boldsymbol{j}):=\prod_{h=1}^{d}\max\{|j_{h}|,1\}$,
\begin{align}
 & \left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\nonumber \\
 & =\left|\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\frac{g(\boldsymbol{j})}{g(\boldsymbol{j})}\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\nonumber \\
 & \leq\bigg(\underbrace{\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\frac{1}{(g(\boldsymbol{j}))^{2}}}_{=\varXi}\bigg)\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}(g(\boldsymbol{j}))^{2}\left|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}.\label{eq:CS}
\end{align}
The fact that $\Xi=\frac{\pi^{2}}{3}\sum_{h=1}^{d}\left(\frac{\pi^{2}}{3}+1\right)^{h-1}$
can be shown by mathematical induction over $d$.%
\begin{comment}
It holds that 
\begin{align*}
\Xi & =\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\prod_{h=1}^{d}\frac{1}{\max\{|j_{h}|,1\}^{2}}=\frac{\pi^{2}}{3}\sum_{h=1}^{d}\left(\frac{\pi^{2}}{3}+1\right)^{h-1}
\end{align*}

Proof: For $d=2$ it holds that
\begin{align*}
 & \sum_{\boldsymbol{j}\in\mathbb{Z}^{2}\setminus\{\boldsymbol{0}\}}\prod_{h=1}^{2}\frac{1}{\max\{|j_{h}|,1\}^{2}}\\
= & \sum_{j_{1}\in\mathbb{Z}\backslash\{0\}}\sum_{j_{2}\in\mathbb{Z}}\prod_{h=1}^{2}\frac{1}{\max\{|j_{h}|,1\}^{2}}+\sum_{j_{1}=0}\sum_{j_{2}\in\mathbb{Z}\backslash\{0\}}\prod_{h=1}^{2}\frac{1}{\max\{|j_{h}|,1\}^{2}}\\
= & \frac{\pi^{2}}{3}(\frac{\pi^{2}}{3}+1)+\frac{\pi^{2}}{3}.
\end{align*}
$d-1\rightarrow d$
\begin{align*}
 & \sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\prod_{h=1}^{d}\frac{1}{\max\{|j_{h}|,1\}^{2}}\\
= & \sum_{j_{d}\in\mathbb{Z}\backslash\{0\}}\sum_{\boldsymbol{j}\in\mathbb{Z}^{d-1}}\prod_{h=1}^{d}\frac{1}{\max\{|j_{h}|,1\}^{2}}+\sum_{j_{d}=0}\sum_{\boldsymbol{j}\in\mathbb{Z}^{d-1}\setminus\{\boldsymbol{0}\}}\prod_{h=1}^{d}\frac{1}{\max\{|j_{h}|,1\}^{2}}\\
= & \frac{\pi^{2}}{3}(\frac{\pi^{2}}{3}+1)^{d-1}+\frac{\pi^{2}}{3}\bigg((\frac{\pi^{2}}{3}+1)^{d-2}+\dots+(\frac{\pi^{2}}{3}+1)^{0}\bigg)\\
= & \frac{\pi^{2}}{3}\bigg((\frac{\pi^{2}}{3}+1)^{d-1}+(\frac{\pi^{2}}{3}+1)^{d-2}+\dots+(\frac{\pi^{2}}{3}+1)^{0}\bigg)
\end{align*}
\end{comment}
{} Then it follows that
\begin{align*}
B(\boldsymbol{L}) & =\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\\
 & \overset{(\ref{eq:CS})}{\leq}\,\Xi\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}(g(\boldsymbol{j}))^{2}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\left|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\\
 & \overset{(\ref{eq:parseval})}{=}\Xi\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}(g(\boldsymbol{j}))^{2}\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}.
\end{align*}
For $\boldsymbol{j}\in\mathbb{Z}^{d}$ and $\boldsymbol{x}\in[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]$,
one has $|j_{h}|\leq\frac{|x_{h}|}{L_{h}}$, $h=1,...,d$.%
\begin{comment}
Let $\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}$.
Let $h\in\{1,...,d\}.$ Let $x=x_{h}$, $L=L_{h}$ , $j=j_{h}$. 

If $j=0$ then $|j|=0\leq\frac{|x|}{L}$.

If $j>0$ then 
\[
L\leq2jL-L\leq x\Rightarrow1\leq\frac{x}{L}\Rightarrow\frac{1}{2}\leq\frac{x}{2L}.
\]
Further, 
\[
2jL-L\leq x\Rightarrow j\leq\frac{x}{2L}+\frac{1}{2}\leq\frac{x}{2L}+\frac{x}{2L}=\frac{x}{L}.
\]
Hence
\[
|j|=j\leq\frac{x}{L}.
\]
If $j<0$ then
\[
x\leq2jL+L\Rightarrow\frac{x}{L}\leq2j+1\leq-1\Rightarrow\frac{x}{2L}\leq-\frac{1}{2}.
\]
Further,
\[
x\leq2jL+L\Rightarrow\frac{x}{2L}-\frac{1}{2}\leq j\Rightarrow\frac{x}{L}\leq j.
\]
Hence
\[
|j|=-j\leq-\frac{x}{L}=\frac{|x|}{L}.
\]

If $j\neq0$ then $\frac{|x|}{L}\geq1$.
\end{comment}
{} It follows that
\begin{align*}
(g(\boldsymbol{j)})^{2} & =\left(\prod_{h=1}^{d}\max\{|j_{h}|,1\}\right)^{2}\leq\prod_{h=1}^{d}\max\left\{ \frac{x_{h}^{2}}{L_{h}^{2}},1\right\} 
\end{align*}
\begin{comment}
show that $\frac{|\boldsymbol{x}|^{2}}{|\boldsymbol{L}|^{2}}\geq1$
for $\boldsymbol{x}\in\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]$.
Further, 
\[
\frac{x_{h}^{2}}{L_{h}^{2}}\leq\frac{|\boldsymbol{x}|^{2}}{|\boldsymbol{L}|^{2}}
\]
Young-Inequality:

\[
\prod_{h=1}^{d}\left(\max\left\{ \frac{x_{h}}{L_{h}},1\right\} ^{2d}\right)^{\frac{1}{d}}\leq\frac{1}{d}\sum_{h=1}^{d}\max\left\{ \frac{x_{h}}{L_{h}},1\right\} ^{2d}.
\]
One could define
\[
g(\boldsymbol{j}):=\sum_{h=1}^{d}|j_{h}|.
\]
 Then by Cauchy-Schwarz inequality
\[
(g(\boldsymbol{j)})^{2}=\left(\sum_{h=1}^{d}|j_{h}|\right)^{2}\leq\left(\sum_{h=1}^{d}|\frac{x_{h}}{L_{h}}|\right)^{2}\leq d\sum_{h=1}^{d}\left(\frac{x_{h}}{L_{h}}\right)^{2}=d\left|\frac{\boldsymbol{x}}{\boldsymbol{L}}\right|^{2}.
\]
What is easier to estimate?
\[
\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\frac{1}{(\max\{|j_{1}|,...,|j_{d}|\})^{2}}\quad\text{or}\quad\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\frac{1}{\left(\sum_{h=1}^{d}|j_{h}|\right)^{2}}
\]
For the second case, we might use the following
\begin{lem}
Let $\boldsymbol{a}\in\mathbb{R}^{d}$. Then
\[
\left(\sum_{h=1}^{d}a_{h}\right)^{2}\leq d\sum_{h=1}^{d}a_{h}^{2}.
\]
\end{lem}

\begin{proof}
Cauchy-Schwartz inequality.
\end{proof}
\end{comment}
and therefore
\begin{align*}
B(\boldsymbol{L}) & \leq\Xi\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}\prod_{h=1}^{d}\max\left\{ \frac{x_{h}^{2}}{L_{h}^{2}},1\right\} |f(\boldsymbol{x})|^{2}d\boldsymbol{x}\\
 & =\Xi\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\prod_{h=1}^{d}\max\left\{ \frac{x_{h}^{2}}{L_{h}^{2}},1\right\} |f(\boldsymbol{x})|^{2}d\boldsymbol{x}.
\end{align*}
This proves Inequality (\ref{eq:B(L)_1}). By the monotonicity of
the $p$-norm it follows that
\begin{equation}
\left(\sum_{h=1}^{d}x_{h}^{2d}\right)^{\frac{1}{2d}}\leq\left(\sum_{h=1}^{d}x_{h}^{2}\right)^{\frac{1}{2}}\Rightarrow\sum_{h=1}^{d}x_{h}^{2d}\leq\left|\boldsymbol{x}\right|^{2d}.\label{eq:monoton}
\end{equation}
 %
\begin{comment}
By Young's inequality it holds for any $a_{h}\geq0$ and $\lambda_{h}\geq0$
with $\sum\lambda_{h}=1$ that
\[
\prod_{h=1}^{d}a_{h}^{\lambda_{h}}\leq\sum_{h=1}^{d}\lambda_{h}a_{h}.
\]
Set $\lambda_{h}=\frac{1}{d}$. 
\end{comment}
By Young's inequality, it holds that 
\begin{align*}
\prod_{h=1}^{d}\max\left\{ \frac{x_{h}^{2}}{L_{h}^{2}},1\right\}  & =\prod_{h=1}^{d}\left(\max\left\{ \frac{x_{h}^{2d}}{L_{h}^{2d}},1\right\} \right)^{\frac{1}{d}}\\
 & \overset{\text{Young}}{\leq}\frac{1}{d}\sum_{h=1}^{d}\max\left\{ \frac{x_{h}^{2d}}{L_{h}^{2d}},1\right\} \\
 & \leq\frac{1}{d}\sum_{h=1}^{d}\frac{x_{h}^{2d}}{L_{h}^{2d}}+1\\
 & \overset{(\ref{eq:monoton})}{\leq}\frac{\left|\boldsymbol{x}\right|^{2d}}{d\underset{h=1,...,d}{\min}L_{h}^{2d}}+1
\end{align*}
in the second last inequality, we used $\max\{a,b\}\leq a+b$ for
any $a,b\geq0$. Hence, 
\[
B(\boldsymbol{L})\leq\Xi\frac{1}{d\underset{h=1,...,d}{\min}L_{h}^{2d}}\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+\Xi\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}.
\]
 Assumption (\ref{eq:propBed}) and $f\in\mathcal{L}^{2}$ imply $B(\boldsymbol{L})\to0$,
$\min_{h=1,...,d}L_{h}\to\infty$.
\end{proof}
\begin{cor}
\label{cor:finite_var}Let $\boldsymbol{X}$ be a $d$-dimensional
random variable such that the marginal moments of all orders exist
and $\boldsymbol{X}$ has bounded density $f$. Then $f$ is COS-admissible.
\end{cor}

\begin{proof}
It holds $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$ because $f$ is
a bounded density. We use
\[
|\boldsymbol{x}|^{2d}\leq d^{d}\max_{h=1,...,d}x_{h}^{2d}\leq d^{d}\sum_{h=1}^{d}x_{h}^{2d}
\]
to obtain
\[
\int_{\mathbb{R}^{d}}|\boldsymbol{x}|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}\leq\left\Vert f\right\Vert _{\infty}d^{d}\sum_{h=1}^{d}\int_{\mathbb{R}^{d}}x_{h}^{2d}f(\boldsymbol{x})d\boldsymbol{x}=\left\Vert f\right\Vert _{\infty}d^{d}\sum_{h=1}^{d}E\left[X_{h}^{2d}\right]<\infty.
\]
The assertion follows by Proposition \ref{prop:COSadmissible}.
\end{proof}
\begin{rem}
Corollary \ref{cor:finite_var} indicates that the class of $d$-dimensional,
COS-admissible densities is large; this is already observed for the
one-dimensional case in \citet[Cor. 3]{junike2022precise}. The proof
of Corollary \ref{cor:finite_var} shows that in one dimension it
is sufficient that the second moment exits, in two dimensions the
forth marginal moments and so forth. In particular, the multivariate
normal distribution is COS-admissible. %
\begin{comment}
{[}Wrong: Additionally, Example \ref{exa:t_distribution} shows that
the multivariate $t$-distribution, including the multivariate Cauchy
distribution as a special case, is COS-admissible.??{]}
\begin{example}
\label{exa:t_distribution}The (centered) multivariate $t$-distribution
with $\nu>\frac{d}{2}$ degrees of freedom is COS-admissible. {[}Wrong:
Note that if $\nu=1$ the $t$-distribution reduces to the multivariate
Cauchy distribution. In particular, the two dimensional multivariate
Cauchy distribution is COS-admissible.??{]}
\end{example}

\begin{proof}
Idee fr einen Beweis, der fr alle $\nu>0$ funktionieren knnte:

Let $\Sigma$ be a symmetric, positive definite real $d\times d$
matrix. The probability density function of the (centered) $t$-distribution
with $\nu$ degrees of freedom and scale matrix $\Sigma$ is given
by
\[
f(\boldsymbol{x})=\frac{C}{(1+\frac{1}{\nu}\boldsymbol{x}\cdot\Sigma^{-1}\boldsymbol{x})^{\frac{\nu+d}{2}}},\quad C:=\frac{\Gamma\big(\frac{\nu+d}{2}\big)}{\Gamma\big(\frac{\nu}{2}\big)(\nu\pi)^{\frac{d}{2}}\sqrt{|\Sigma|}},\quad\boldsymbol{x}\in\mathbb{R}^{d}.
\]
By Courant--Fischer--Weyl min-max principle, see, e.g., \citet[Thm. 8.1.2]{van1996matrix},
it holds that
\begin{equation}
\lambda_{\min}|\boldsymbol{y}|^{2}\leq\boldsymbol{y}\cdot\Sigma\boldsymbol{y}\leq\lambda_{\max}|\boldsymbol{y}|^{2},\quad\boldsymbol{y}\in\mathbb{R}^{d}\label{eq:CFW-2}
\end{equation}
where $\lambda_{\min}$ and $\lambda_{\max}$ are the smallest and
greatest eigenvalues of $\Sigma$.

\[
B(\boldsymbol{L})=\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}
\]
die Summe aufteilen, in alle Bereiche, die $k_{h}=0$ und $k_{h}>0$
enthalten. Wir bestimmen
\begin{align*}
\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\\
=\int_{\mathbb{R}\setminus[-L_{1},L_{1}}...\underbrace{\left(\int_{\mathbb{R}\setminus[-L_{d},L_{d}]}f(\boldsymbol{x})\cos\left(k_{d}\pi\frac{x_{d}+L_{d}}{2L_{d}}\right)dx_{d}\right)}_{\text{per integration-by-parts bestimmen}}\prod_{h=1}^{d-1}\cos\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right)dx_{d-1}...dx_{1}
\end{align*}
\begin{align*}
\int_{\mathbb{R}\setminus[-L_{d},L_{d}]}f(\boldsymbol{x})\cos\left(k_{d}\pi\frac{x_{d}+L_{d}}{2L_{d}}\right)dx_{d}\\
=\frac{2L_{d}}{k_{d}\pi}\left(\left[f(\boldsymbol{x})\sin\left(k_{d}\pi\frac{x_{d}+L_{d}}{2L_{d}}\right)\right]_{L_{d}}^{\infty}+\left[f(\boldsymbol{x})\sin\left(k_{d}\pi\frac{x_{d}+L_{d}}{2L_{d}}\right)\right]_{-\infty}^{-L_{d}}\right)-\frac{2L_{d}}{k_{d}\pi}\int_{\mathbb{R}\setminus[-L_{d},L_{d}]}\frac{\partial f(\boldsymbol{x})}{\partial x_{d}}\sin\left(k_{d}\pi\frac{x_{d}+L_{d}}{2L_{d}}\right)dx_{d}
\end{align*}
evtl. kommt sowas wie:
\begin{align*}
B(\boldsymbol{L}) & =\underbrace{...}_{\text{k=0 teile}}+\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\prod_{h=1}^{d}\left\{ \frac{4L_{h}^{2}}{k_{h}^{2}\pi^{2}}\right\} \left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\frac{\partial f(\boldsymbol{x})}{\partial x_{1}...\partial x_{d}}\prod_{h=1}^{d}\sin\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right)d\boldsymbol{x}\right|^{2}\\
 & \leq\frac{1}{\prod_{h=1}^{d}L_{h}}\prod_{h=1}^{d}\left\{ \frac{4L_{h}^{2}}{\pi^{2}}\right\} \left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\frac{\partial f(\boldsymbol{x})}{\partial x_{1}...\partial x_{d}}\right|d\boldsymbol{x}\right|^{2}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\prod_{h=1}^{d}\left\{ \frac{1}{k_{h}^{2}}\right\} 
\end{align*}
\end{proof}
%
\begin{proof}
Let $\Sigma$ be a symmetric, positive definite real $d\times d$
matrix. The probability density function of the (centered) $t$-distribution
with $\nu$ degrees of freedom and scale matrix $\Sigma$ is given
by
\[
f(\boldsymbol{x})=\frac{C}{(1+\frac{1}{\nu}\boldsymbol{x}\cdot\Sigma^{-1}\boldsymbol{x})^{\frac{\nu+d}{2}}},\quad C:=\frac{\Gamma\big(\frac{\nu+d}{2}\big)}{\Gamma\big(\frac{\nu}{2}\big)(\nu\pi)^{\frac{d}{2}}\sqrt{|\Sigma|}},\quad\boldsymbol{x}\in\mathbb{R}^{d}.
\]
By Courant--Fischer--Weyl min-max principle, see, e.g., \citet[Thm. 8.1.2]{van1996matrix},
it holds that
\begin{equation}
\lambda_{\min}|\boldsymbol{y}|^{2}\leq\boldsymbol{y}\cdot\Sigma\boldsymbol{y}\leq\lambda_{\max}|\boldsymbol{y}|^{2},\quad\boldsymbol{y}\in\mathbb{R}^{d}\label{eq:CFW}
\end{equation}
where $\lambda_{\min}$ and $\lambda_{\max}$ are the smallest and
greatest eigenvalues of $\Sigma$. Hence,
\begin{align*}
\int_{\mathbb{R}^{d}}|\boldsymbol{x}|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x} & \leq C^{2}\int_{\mathbb{R}^{d}}\frac{|\boldsymbol{x}|^{2d}}{(1+\frac{1}{\nu\lambda_{\min}}|\boldsymbol{x}|^{2})^{\nu+d}}d\boldsymbol{x}.
\end{align*}
The integral of the right hand-side in the last inequality is rotationally
symmetric and it follows by \citet[Thm. 8.11]{amann2009analysis}
that
\[
\int_{\mathbb{R}^{d}}|\boldsymbol{x}|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}\leq C^{2}\frac{d\pi^{\frac{d}{2}}}{\Gamma(1+\frac{d}{2})}\int_{0}^{\infty}\frac{r^{2d+d-1}}{(1+\frac{1}{\nu\frac{1}{\nu\lambda_{\min}}}r^{2})^{\nu+d}}dr<\infty.
\]
Note that $f$ is bounded by $C$ and therefore $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$.
Apply Proposition \ref{prop:COSadmissible} to conclude.
\end{proof}
\end{comment}
\end{rem}

The following theorem generalizes \citet[Thm. 7]{junike2022precise}
to the multidimensional case including numerical uncertainty on the
characteristic function $\varphi$. It shows that multivariate densities
can be approximated by a cosine expansion.
\begin{thm}
\label{thm:approx f}Assume that $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
and that $f$ is COS-admissible. For any $\varepsilon>0$ there is
a $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$, a $\boldsymbol{N}\in\mathbb{N}^{d}$
and a $\gamma>0$ so that $\left\Vert \varphi-\tilde{\varphi}\right\Vert _{\infty}<\gamma$
implies
\[
\left\Vert f-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}<\varepsilon.
\]
Note that $\boldsymbol{N}$ depends on $\boldsymbol{L}$ and $\gamma$
depends on both $\boldsymbol{L}$ and $\boldsymbol{N}$.
\end{thm}

\begin{proof}
For $k,l\in\mathbb{N}_{0}$ and $L>0$, it holds that
\[
\int_{-L}^{L}\cos\left(k\pi\frac{x+L}{2L}\right)\cos\left(l\pi\frac{x+L}{2L}\right)dx=\begin{cases}
2L & ,k=l=0\\
L & ,k=l\neq0\\
0 & ,k\neq l.
\end{cases}
\]
Thereby, we obtain
\begin{align*}
\left\langle e_{\boldsymbol{k}}^{\boldsymbol{L}},e_{\boldsymbol{l}}^{\boldsymbol{L}}\right\rangle  & =\int_{-L_{1}}^{L_{1}}...\int_{-L_{d}}^{L_{d}}\prod_{h=1}^{d}\bigg\{\cos\big(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\big)\cos\big(l_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\big)\bigg\} dx_{d}...dx_{1}\\
 & =\begin{cases}
0 & ,k_{h}\neq l_{h}\text{ for any }h\\
2^{\Lambda(\boldsymbol{k})}\prod_{h=1}^{d}L_{h} & ,k_{1}=l_{1},...,k_{d}=l_{d},
\end{cases}
\end{align*}
where $\Lambda$ is defined as in Equation (\ref{eq:Lambda}). For
any $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$ and $\boldsymbol{N}\in\mathbb{N}^{d}$,
it holds that
\begin{align*}
\left\Vert f-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}\leq & \underbrace{\left\Vert f-f_{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{1}(\boldsymbol{L})}+\underbrace{\left\Vert f_{\boldsymbol{L}}-\sideset{}{'}\sum_{k\in\mathbb{N}_{0}^{d},k\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{2}(\boldsymbol{L},\boldsymbol{N})}\\
 & +\underbrace{\left\Vert \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}(a_{\boldsymbol{k}}-c_{\boldsymbol{k}})e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{3}(\boldsymbol{L},\boldsymbol{N})}+\underbrace{\left\Vert \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}(c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}})e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{4}(\boldsymbol{L},\boldsymbol{N})}.
\end{align*}
Further,
\begin{align*}
A_{3}(\boldsymbol{L},\boldsymbol{N})^{2} & =\left\langle \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}(a_{\boldsymbol{k}}-c_{\boldsymbol{k}})e_{\boldsymbol{k}}^{\boldsymbol{L}},\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}(a_{\boldsymbol{k}}-c_{\boldsymbol{k}})e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\rangle \\
 & =\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\sum_{\boldsymbol{l}\in\mathbb{N}_{0}^{d},\boldsymbol{l}\leq\boldsymbol{N}}\frac{1}{2^{\Lambda(\boldsymbol{k})+\Lambda(\boldsymbol{l})}}(a_{\boldsymbol{k}}-c_{\boldsymbol{k}})(a_{\boldsymbol{l}}-c_{\boldsymbol{l}})\left\langle e_{\boldsymbol{k}}^{\boldsymbol{L}},e_{\boldsymbol{l}}^{\boldsymbol{L}}\right\rangle \\
 & \leq\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\prod_{h=1}^{d}\{L_{h}\}\left|a_{\boldsymbol{k}}-c_{\boldsymbol{k}}\right|^{2}=B(\boldsymbol{L}),
\end{align*}
see Definition \ref{def:COS_admissible}. For $\varepsilon>0$, choose
$\boldsymbol{L}\in\mathbb{R}_{+}^{d}$ such that $A_{1}(\boldsymbol{L})<\frac{\varepsilon}{4}$
and $B(\boldsymbol{L})<\big(\frac{\varepsilon}{4}\big){}^{2}$. Hence,
$A_{3}(\boldsymbol{L},\boldsymbol{N})<\frac{\varepsilon}{4}$. Then
choose $\boldsymbol{N}\in\mathbb{N}^{d}$ such that $A_{2}(\boldsymbol{L},\boldsymbol{N})<\frac{\varepsilon}{4}$.
By the definition of $c_{\boldsymbol{k}}$ and $\tilde{c}_{\boldsymbol{k}}$,
see Equation (\ref{eq:ck}), it follows that
\begin{align*}
|c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}}| & \leq\frac{1}{2^{d-1}\prod_{h=1}^{d}L_{h}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\left|\varphi\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)-\tilde{\varphi}\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\right|\\
 & \leq\frac{\left\Vert \varphi-\tilde{\varphi}\right\Vert _{\infty}}{\prod_{h=1}^{d}L_{h}}.
\end{align*}
Similarly to the analysis of $A_{3}$, we have
\begin{align*}
A_{4}(\boldsymbol{L},\boldsymbol{N}) & ^{2}\leq\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\prod_{h=1}^{d}\{L_{h}\}\left|c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}}\right|^{2}\\
 & \leq\max_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\left|c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}}\right|^{2}\prod_{h=1}^{d}\{L_{h}\}\prod_{h=1}^{d}\{N_{h}+1\}\\
 & \leq\frac{\left\Vert \varphi-\tilde{\varphi}\right\Vert _{\infty}^{2}}{\prod_{h=1}^{d}L_{h}}\prod_{h=1}^{d}\{N_{h}+1\}.
\end{align*}
Choose
\[
\gamma=\frac{\varepsilon}{4}\frac{\sqrt{\prod_{h=1}^{d}L_{h}}}{\sqrt{\prod_{h=1}^{d}\{N_{h}+1\}}}.
\]
Then $\left\Vert \varphi-\tilde{\varphi}\right\Vert _{\infty}<\gamma$
implies $A_{4}(\boldsymbol{L},\boldsymbol{N})\leq\frac{\varepsilon}{4}$,
which concludes the proof.
\end{proof}
The following Corollary generalizes \citet[Cor. 8]{junike2022precise}
to the multidimensional case including numerical uncertainty on the
characteristic function $\varphi$ and the cosine coefficients of
the payoff function $v$. The corollary provides sufficient conditions
in order to ensure that the COS method approximates the price of a
rainbow option within a predefined error tolerance $\varepsilon>0$.
\begin{cor}
\label{cor:generalCase}(Convergence of the COS method). Let $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
be COS-admissible and $v:\mathbb{R}^{d}\to\mathbb{R}$ be locally
in $\mathcal{L}^{2}$, that is, $v_{\boldsymbol{M}}\in\mathcal{L}^{2}$
for any $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$. Assume $vf\in\mathcal{L}^{1}$,
then the integral of the product of $f$ and $v$ can be approximated
by a finite sum as follows: Let $\varepsilon>0$. Let $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$
and $\xi>0$ such that
\begin{equation}
\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}\left|v(\boldsymbol{x})f(\boldsymbol{x})\right|d\boldsymbol{x}\leq\frac{\varepsilon}{3},\quad\left\Vert v_{\boldsymbol{M}}\right\Vert _{2}\leq\xi.\label{eq:Mgeneral}
\end{equation}
Let $\boldsymbol{L}\geq\boldsymbol{M}$ such that
\begin{equation}
\left\Vert f-f_{\boldsymbol{L}}\right\Vert _{2}\leq\frac{\varepsilon}{12\xi}\label{eq:L1general}
\end{equation}
and
\begin{equation}
B(\boldsymbol{L})\leq\left(\frac{\varepsilon}{12\xi}\right)^{2}.\label{eq:L2general}
\end{equation}
Choose $\boldsymbol{N}\in\mathbb{N}^{d}$ large enough, so that 
\[
\left\Vert f_{\boldsymbol{L}}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}\leq\frac{\varepsilon}{12\xi}.
\]
Assume
\begin{equation}
\left\Vert \varphi-\tilde{\varphi}\right\Vert _{\infty}\leq\frac{\varepsilon}{12\xi}\frac{\sqrt{\prod_{h=1}^{d}L_{h}}}{\sqrt{\prod_{h=1}^{d}\{N_{h}+1\}}}.\label{eq:phi_tilde}
\end{equation}
Let $\eta>0$ such that $\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{c}_{\boldsymbol{k}}|\leq\eta.$
Assume
\[
|v_{\boldsymbol{l}}-\tilde{v}_{\boldsymbol{l}}|\leq\frac{\varepsilon}{3\eta},\quad\boldsymbol{l}\leq\boldsymbol{N}.
\]
Then it follows that
\begin{equation}
\left|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}\right|\leq\varepsilon.\label{eq:Ngeneral}
\end{equation}
\end{cor}

\begin{proof}
Let $A_{1}(\boldsymbol{L})$, $A_{2}(\boldsymbol{L},\boldsymbol{N})$
and $A_{4}(\boldsymbol{L},\boldsymbol{N})$ be as in the proof of
Theorem \ref{thm:approx f}. By Inequality (\ref{eq:phi_tilde}) it
follows that $A_{4}(\boldsymbol{L},\boldsymbol{N})\leq\frac{\varepsilon}{12\xi}$.
Due to $v_{\boldsymbol{k}}=\langle v_{\boldsymbol{M}},e_{\boldsymbol{k}}^{\boldsymbol{L}}\rangle$
and by Theorem \ref{thm:approx f} we have that
\begin{align*}
\Big|\int_{\mathbb{R}^{d}}v(\boldsymbol{x}) & f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}\Big|\\
= & \bigg|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}+\langle v_{\boldsymbol{M}},f\rangle-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\langle v_{\boldsymbol{M}},e_{\boldsymbol{k}}^{\boldsymbol{L}}\rangle\\
 & -\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}(\tilde{v}_{\boldsymbol{k}}-v_{\boldsymbol{k}})\bigg|\\
\leq & \int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}|v(\boldsymbol{x})f(\boldsymbol{x})|d\boldsymbol{x}+\Big|\Big\langle v_{\boldsymbol{M}},f-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Big\rangle\Big|\\
 & +\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{c}_{\boldsymbol{k}}(\tilde{v}_{\boldsymbol{k}}-v_{\boldsymbol{k}})|\\
< & \frac{\varepsilon}{3}+\|v_{\boldsymbol{M}}\|_{2}\,\Big\Vert f-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Big\Vert_{2}+\max_{\boldsymbol{l}\in\mathbb{N}_{0}^{d},\boldsymbol{l}\leq\boldsymbol{N}}|\tilde{v}_{\boldsymbol{l}}-v_{\boldsymbol{l}}|\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{c}_{\boldsymbol{k}}|\\
< & \frac{\varepsilon}{3}+\xi\left(A_{1}(\boldsymbol{L})+A_{2}(\boldsymbol{L},\boldsymbol{N})+\sqrt{B(\boldsymbol{L})}+A_{4}(\boldsymbol{L},\boldsymbol{N})\right)+\frac{\varepsilon}{3}\\
\leq & \frac{\varepsilon}{3}+\xi\left(\frac{\varepsilon}{12\xi}+\frac{\varepsilon}{12\xi}+\frac{\varepsilon}{12\xi}+\frac{\varepsilon}{12\xi}\right)+\frac{\varepsilon}{3}=\varepsilon.
\end{align*}
\end{proof}
\begin{rem}
\label{rem:no uncertainty}If there is no uncertainty on the characteristic
function, i.e., $\varphi=\tilde{\varphi}$ and no uncertainty on the
Fourier-cosine coefficients of the payoff function $v$, i.e., $v_{\boldsymbol{k}}=\tilde{v}_{\boldsymbol{k}}$,
we may replace in Corollary \ref{cor:generalCase} the term $\frac{\varepsilon}{3}$
by $\frac{\varepsilon}{2}$ and the term $\frac{\varepsilon}{12\xi}$
by $\frac{\varepsilon}{6\xi}$.
\end{rem}


\section{\label{sec:How d=00003D2}How to find $\boldsymbol{L}$ and $\boldsymbol{N}$}

In \cite{junike2022precise}, it is assumed that $f$ has semi-heavy
tails. Here, we make the same assumption in multivariate dimensions:

\begin{assumption}\label{A1} 	
There are $C_1,C_2,m>0$ such that for $|\textbf{x}|>m$  it holds that \begin{equation}  
|f(\boldsymbol{x})|\leq C_{1}e^{-C_{2}(|\boldsymbol{x}|)}.  
\end{equation}
\end{assumption}

As in the one-dimensional case, Markov's inequality may be applied
to find an explicit formula for $\boldsymbol{M}$ such that Equation
(\ref{eq:Mgeneral}) holds. In Theorem \ref{thm:(Multidimensional-COS-method},
we assume no uncertainty on neither the characteristic function $\varphi$
nor on the Fourier-cosine coefficients of the payoff function.
\begin{thm}
\emph{\label{thm:(Multidimensional-COS-method}(Multidimensional COS
method, Markov). }Let $(X_{1},...,X_{d})$ be a random vector with
bounded density $f$ and characteristic function $\varphi$. Let $v:\mathbb{R}^{d}\to\mathbb{R}$
be bounded, with $|v(\boldsymbol{x})|\leq K$ for all $\boldsymbol{x}\in\mathbb{R}^{d}$
and some $K>0$. Let $n\geq2$ be some even number and assume the
$n^{th}-$moment of each $X_{j}$, $j=1,...,d$, exists. Assume $f$
satisfies Assumption \ref{A1}. Let $\varepsilon>0$ be small enough.
Define
\begin{equation}
M_{h}:=\left(\frac{2dK}{\varepsilon}E\left[X_{h}^{n}\right]\right)^{\frac{1}{n}},\quad h=1,...,d,\label{eq:m-1-1}
\end{equation}
and $\boldsymbol{M}=(M_{1},...,M_{d})\in\mathbb{R}_{+}^{d}$. Set
$\boldsymbol{L}:=\boldsymbol{M}$. Define
\[
\xi:=\sqrt{K^{2}2^{d}\prod_{h=1}^{d}M_{h}}.
\]
Let $\boldsymbol{N}\in\mathbb{N}^{d}$ be large enough so that
\begin{equation}
\Vert f_{\boldsymbol{L}}-\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Vert_{2}\leq\frac{\varepsilon}{6\xi}.\label{eq:f_l-akek}
\end{equation}
Then it holds that
\begin{equation}
\left|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}v_{\boldsymbol{k}}\right|\leq\varepsilon.\label{eq:endRes-1-1}
\end{equation}
\end{thm}

\begin{proof}
It holds applying Markov's inequality
\begin{align*}
\frac{1}{K}\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}\left|v(\boldsymbol{x})f(\boldsymbol{x})\right|d\boldsymbol{x} & \leq\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}f(\boldsymbol{x})d\boldsymbol{x}\\
 & =P\left(\cup_{h=1}^{d}\left\{ |X_{h}|\geq M_{h}\right\} \right)\\
 & \leq\sum_{h=1}^{d}P(|X_{h}|\geq M_{h})\\
 & \leq\sum_{h=1}^{d}\frac{E[X_{h}^{n}]}{M_{h}^{n}}=\frac{\varepsilon}{2K}.
\end{align*}
The last inequality follows by the definition of $\boldsymbol{M}$.
\begin{comment}
As
\begin{align*}
\sqrt{L^{\prime}\Sigma^{-1}L} & =\sqrt{(\alpha M)^{\prime}\Sigma^{-1}\alpha M}=\alpha\sqrt{M^{\prime}\Sigma^{-1}M}\\
 & =\sqrt{(\alpha M_{1}...\alpha M_{d})\left(\begin{array}{c}
\sum_{i=1}^{d}\Sigma_{1i}^{-1}\alpha M_{i}\\
\\
\sum_{i=1}^{d}\Sigma_{di}^{-1}\alpha M_{i}
\end{array}\right)}\\
 & =\sqrt{\sum_{k=1}^{d}\alpha M_{k}\sum_{i=1}^{d}\Sigma_{ki}^{-1}\alpha M_{i}}\\
\\
\end{align*}
\end{comment}
As $v$ is bounded by $K$, it holds that $\left\Vert v_{\boldsymbol{M}}\right\Vert _{2}\leq\xi.$
Hence, the inequalities in (\ref{eq:Mgeneral}) are satisfied. %
\begin{comment}
\begin{proof}
It follows by the definition of $\eta$ and by Lemma \ref{lem:eigenvalues}
that
\[
\lambda_{\Sigma}(x)=\frac{c_{d}}{\sqrt{\det\Sigma}}e^{-\sqrt{x^{\prime}\Sigma^{-1}x}}\leq\frac{c_{d}}{\sqrt{\det\Sigma}}e^{-\frac{1}{\eta}|x|},\quad x\in\mathbb{R}^{d}.
\]
It holds by Lemma \ref{lem:eigenvalues} and Lemma \ref{lem:integral}
and because $\ell\leq\min_{j}L_{j}$
\begin{align*}
\left\Vert f-f_{L}\right\Vert _{2}^{2} & =\int_{\mathbb{R}^{d}\setminus[-L,L]}f^{2}(x)dx\\
 & \leq\int_{\mathbb{R}^{d}\setminus[-L,L]}\lambda_{\Sigma}^{2}(x)dx\\
 & \leq\frac{c_{d}^{2}}{\det\Sigma}\int_{\mathbb{R}^{d}\setminus[-L,L]}e^{-\frac{2}{\eta}|x|}dx\\
 & \leq\frac{c_{d}^{2}}{\det\Sigma}\int_{\{x\in\mathbb{R}^{d}:|x|>\ell\}}e^{-\frac{2}{\eta}|x|}dx\\
 & \overset{\text{remove this line}}{=}\frac{c_{d}^{2}}{\det\Sigma}\frac{1}{(d-1)!c_{d}}e^{-\frac{2}{\eta}\ell}(d-1)!\left(\frac{\eta}{2}\right)^{d}\sum_{k=0}^{d-1}\frac{\left(\frac{2}{\eta}\ell\right)^{k}}{k!}\\
 & =\frac{c_{d}}{\det\Sigma}e^{-\frac{2}{\eta}\ell}\left(\frac{\eta}{2}\right)^{d}\sum_{k=0}^{d-1}\frac{\left(\frac{2}{\eta}\ell\right)^{k}}{k!}\\
 & \leq\frac{\varepsilon^{2}}{36K^{2}2^{d}\prod_{j=1}^{d}M_{j}}=\left(\frac{\varepsilon}{6\xi}\right)^{2},
\end{align*}
where we used (?? wo benutzt)
\[
\frac{\Gamma\left(\frac{d}{2}\right)}{\Gamma\left(\frac{d}{2}+1\right)}=\frac{\Gamma\left(\frac{d}{2}\right)}{\frac{d}{2}\Gamma\left(\frac{d}{2}\right)}=\frac{2}{d},\quad d=1,2,...,
\]
to simplfy the terms, compare with Equation (6.1.15) in \cite{abramowitz1972handbook}.
\end{proof}
\end{comment}
For $\varepsilon$ small enough, $\boldsymbol{L}$ is large enough
and by Assumption \ref{A1} and Lemma \ref{lem:integral}, with $\ell:=\min_{h=1,...,d}L_{h}$,
it follows that
\begin{align}
\left\Vert f-f_{\boldsymbol{L}}\right\Vert _{2}^{2} & \leq C_{1}^{2}\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}e^{-2C_{2}|\boldsymbol{x}|}d\boldsymbol{x}\nonumber \\
 & \leq C_{1}^{2}\frac{d\pi^{\frac{d}{2}}}{\Gamma\left(1+\frac{d}{2}\right)}\int_{\ell}^{\infty}e^{-2C_{2}r}r^{d-1}dr\nonumber \\
 & =C_{1}^{2}\frac{d\pi^{\frac{d}{2}}}{\Gamma\left(1+\frac{d}{2}\right)}e^{-2C_{2}\ell}\frac{(d-1)!}{(2C_{2}\sqrt{d})^{d}}\sum_{k=0}^{d-1}\frac{(2C_{2}\ell)^{k}}{k!}\label{eq:||f-f_l}\\
 & \leq\frac{\varepsilon}{6\xi},\label{eq:f-f_l eps}
\end{align}
the last inequality holds true if $\varepsilon$ is small enough because
the right hand-side of Equation (\ref{eq:||f-f_l}) decreases exponentially
in $\varepsilon$, while the right hand-side of Inequality (\ref{eq:f-f_l eps})
goes to zero like $\varepsilon^{1+\frac{d}{n}}$ for $\varepsilon\searrow0$.
Hence, Inequality (\ref{eq:L1general}) holds. Further, by Assumption
\ref{A1} and by Lemma \ref{lem:integral} it holds that
\begin{align}
\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x} & \leq C_{1}^{2}\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}e^{-2C_{2}|\boldsymbol{x}|}d\boldsymbol{x}\nonumber \\
 & \leq C_{1}^{2}\frac{d\pi^{\frac{d}{2}}}{\Gamma\left(1+\frac{d}{2}\right)}e^{-2C_{2}\ell}\frac{(2d)!}{(2C_{2}\sqrt{d})^{2d+1}}\sum_{k=0}^{2d}\frac{(2C_{2}\ell)^{k}}{k!}.\label{eq:x^2df(x)^2}
\end{align}
Let $\Xi$ be defined as in Proposition \ref{prop:COSadmissible}.
It follows that
\begin{align}
B(\boldsymbol{L}) & \leq\frac{\Xi}{d\underset{h=1,...,d}{\min}L_{h}^{2d}}\bigg(\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}\bigg)\label{eq:B(L)_d=00003D2}\\
 & \leq\left(\frac{\varepsilon}{12\xi}\right)^{2}\label{eq:B(L)_d=00003D2 eps}
\end{align}
the last inequality holds true if $\varepsilon$ is small enough because
the right hand-side of Equation (\ref{eq:B(L)_d=00003D2}) decreases
exponentially in $\varepsilon$ using Inequalities (\ref{eq:||f-f_l})
and (\ref{eq:x^2df(x)^2}). Meanwhile, the right hand-side of Inequality
(\ref{eq:B(L)_d=00003D2 eps}) goes to zero like $\varepsilon^{2+\frac{2d}{n}}$
for $\varepsilon\searrow0$. Hence, Inequality (\ref{eq:L2general})
holds. Apply Corollary \ref{cor:generalCase} and Remark \ref{rem:no uncertainty}
to conclude.
\end{proof}
To solve Inequality (\ref{eq:f_l-akek}) for $\boldsymbol{N}$, we
need the Lemma \ref{lem: norm f_L}. In Lemma \ref{lem: norm f_L}
we make an independence assumption and we assume that both $f$ and
its derivatives have compact support. The last assumptions greatly
simplifies notation and holds, at least approximately, for light tailed
distributions like the multivariate normal distribution. In a future
research, we plan to relax this Assumption. We use the following notation:
For a function $g:\mathbb{R}\to\mathbb{R}$, we denote by $g^{(j)}$,
$j=1,2,...,$ the $j^{th}$-derivative of $g.$ We use the convention
$g^{(0)}\equiv g$.
\begin{lem}
\label{lem: norm f_L}Let $J\in\mathbb{N}$. Assume
\begin{equation}
f(\boldsymbol{x})=\prod_{h=1}^{d}f_{h}(x_{h}),\quad\boldsymbol{x}\in\mathbb{R}^{d}\label{eq:f prod}
\end{equation}
and suppose $f_{h}$ is bounded and $J+1$ times continuously differentiable
with bounded derivatives and bounded support. Let $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$
and $\boldsymbol{N}\in\mathbb{N}^{d}$. Assume
\[
|f_{h}^{j}(x)|=0,\quad|x|\geq L_{h},\quad j=0,...,J+1,\quad h=1,...,d.
\]
\begin{comment}
It holds that 
\begin{align*}
\bigg\Vert f_{\boldsymbol{L}}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\bigg\Vert_{2} & \leq\sqrt{\prod_{h=1}^{d}L_{h}}\left(\sum_{h=1}^{d}c_{h}\frac{1}{JN_{h}^{J}}\prod_{\underset{n\neq h}{n=1}}^{d}\big(c_{n}\frac{\pi^{2}}{6}+d_{n}\big)\right),
\end{align*}
\end{comment}
Let $c_{h}$ and $d_{h}$ be defined as in Equations (\ref{eq:a_k prod})
and (\ref{eq:dh}). Further, let $\varepsilon>0$ and $\xi>0$. Define
\[
\lambda=\left(\sqrt{\prod_{h=1}^{d}L_{h}}\left(\sum_{h=1}^{d}c_{h}\frac{1}{JL_{h}^{J}}\prod_{\underset{n\neq h}{n=1}}^{d}\big(c_{n}\frac{\pi^{2}}{6}+d_{n}\big)\right)\frac{6\xi}{\varepsilon}\right)^{\frac{1}{J}}
\]
and let $\boldsymbol{N}\geq\lambda\boldsymbol{L}$. Then
\[
\bigg\Vert f_{\boldsymbol{L}}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\bigg\Vert_{2}\leq\frac{\varepsilon}{6\xi}.
\]
\end{lem}

\begin{proof}
By Equality (\ref{eq:f prod}) it holds that
\[
a_{\boldsymbol{k}}=\prod_{h=1}^{d}\frac{1}{L_{h}}\int_{-L_{h}}^{L_{h}}f_{h}(x)\cos\left(k_{h}\pi\frac{x+L_{h}}{2L_{h}}\right)dx.
\]
It follows as in the proof of \citet[Lemma 3.5]{junike2023handle}
that
\begin{align}
|a_{\boldsymbol{k}}|\leq & \prod_{\underset{k_{h}=0}{h=1}}^{d}\left\{ 2\left\Vert f_{h}\right\Vert _{\infty}\right\} \prod_{\underset{k_{h}\neq0}{h=1}}^{d}\bigg\{\sum_{j=1}^{J}\frac{2^{j+1}}{\pi^{j+1}}\frac{L_{h}^{j}}{k_{h}^{j+1}}\nonumber \\
 & \quad\left(|f_{h}^{(j)}(-L_{h})|+|f_{h}^{(j)}(L_{h})|\right)+\underbrace{\frac{2^{J+2}\left\Vert f_{h}^{(J+1)}\right\Vert _{\infty}L_{h}^{J+1}}{\pi^{J+1}}}_{=:c_{h}}\frac{1}{k_{h}^{J+1}}\bigg\}\nonumber \\
 & \leq\prod_{\underset{k_{h}=0}{h=1}}^{d}d_{h}\prod_{\underset{k_{h}\neq0}{h=1}}^{d}\bigg\{\frac{b_{h}}{k_{h}^{2}}+\frac{c_{h}}{k_{h}^{J+1}}\bigg\},\label{eq:a_k prod}
\end{align}
where 
\begin{equation}
d_{h}:=2\left\Vert f_{h}\right\Vert _{\infty}\label{eq:dh}
\end{equation}
 and we define
\[
b_{h}:=(\frac{2}{\pi})^{2}J\max_{j}\big\{ L_{h}^{j}\big\}\max_{j}\big\{|f_{h}^{(j)}(-L_{h})|+|f_{h}^{(j)}(L_{h})|\big\},\quad h=1,...,d.
\]
By assumption, $b_{h}=0$. We use $\sum_{k=1}^{\infty}\frac{1}{k^{2}}=\frac{\pi^{2}}{6}$
and
\[
\sum_{k=N+1}^{\infty}\frac{1}{k^{j+1}}\leq\int_{N}^{\infty}x^{-j-1}dx=\frac{1}{jN^{j}},\quad N,j\geq1,
\]
to obtain by mathematical induction over $d$ that %
\begin{comment}
It follows that
\begin{align*}
 & \sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{1}>N_{1}}\prod_{\underset{k_{n}=0}{n=1}}^{d}d_{n}\prod_{\underset{k_{n}\neq0}{n=1}}^{d}\frac{c_{n}}{k_{n}^{J+1}}\\
\leq & \sum_{k_{1}>N_{1}}\sum_{k_{2}\in\mathbb{N}_{0}}\dots\sum_{k_{d}\in\mathbb{N}_{0}}\prod_{\underset{k_{n}=0}{n=1}}^{d}d_{n}\prod_{\underset{k_{n}\neq0}{n=1}}^{d}\frac{c_{n}}{k_{n}^{J+1}}\\
\leq & c_{d}\frac{\pi^{2}}{6}\sum_{k_{1}>N_{1}}\sum_{k_{2}\in\mathbb{N}_{0}}\dots\sum_{k_{d-1}\in\mathbb{N}_{0}}\prod_{\underset{k_{n}=0}{n=1}}^{d-1}d_{n}\prod_{\underset{k_{n}\neq0}{n=1}}^{d-1}\frac{c_{n}}{k_{n}^{J+1}}+d_{d}\sum_{k_{1}>N_{1}}\sum_{k_{2}\in\mathbb{N}_{0}}\dots\sum_{k_{d-1}\in\mathbb{N}_{0}}\prod_{\underset{k_{n}=0}{n=1}}^{d-1}d_{n}\prod_{\underset{k_{n}\neq0}{n=1}}^{d-1}\frac{c_{n}}{k_{n}^{J+1}}\\
\leq & \big(c_{d}\frac{\pi^{2}}{6}+d_{d}\big)\sum_{k_{1}>N_{1}}\sum_{k_{2}\in\mathbb{N}_{0}}\dots\sum_{k_{d-1}\in\mathbb{N}_{0}}\prod_{\underset{k_{n}=0}{n=1}}^{d-1}d_{n}\prod_{\underset{k_{n}\neq0}{n=1}}^{d-1}\frac{c_{n}}{k_{n}^{J+1}}\\
\leq & \prod_{h=2}^{d}\big(c_{h}\frac{\pi^{2}}{6}+d_{h}\big)\sum_{k_{1}>N_{1}}\frac{c_{1}}{k_{1}^{J+1}}\\
\leq & c_{1}\frac{1}{JN_{1}^{J}}\prod_{h=2}^{d}\big(c_{h}\frac{\pi^{2}}{6}+d_{h}\big)
\end{align*}
\end{comment}
\begin{align}
\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{h}>N_{h}}|a_{\boldsymbol{k}}|\overset{(\ref{eq:a_k prod})}{\leq} & \sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{h}>N_{h}}\prod_{\underset{k_{n}=0}{n=1}}^{d}d_{n}\prod_{\underset{k_{n}\neq0}{n=1}}^{d}\frac{c_{n}}{k_{n}^{J+1}}\nonumber \\
\leq & c_{h}\frac{1}{JN_{h}^{J}}\prod_{\underset{n\neq h}{n=1}}^{d}\big(c_{n}\frac{\pi^{2}}{6}+d_{n}\big),\quad h=1,...,d.\label{eq:ak h}
\end{align}
We obtain
\begin{align*}
 & \Vert f_{\boldsymbol{L}}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Vert_{2}\\
 & =\sqrt{\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{k_{1}>N_{1}\text{ or}\dots\text{or }k_{d}>N_{d}}|a_{\boldsymbol{k}}|^{2}}\\
 & \leq\sqrt{\prod_{h=1}^{d}L_{h}}\left(\sideset{}{'}\sum_{k_{1}>N_{1}\text{ or}\dots\text{or }k_{d}>N_{d}}|a_{\boldsymbol{k}}|\right)\\
 & \leq\sqrt{\prod_{h=1}^{d}L_{h}}\left(\sum_{h=1}^{d}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{h}>N_{h}}|a_{\boldsymbol{k}}|\right)\\
 & \overset{(\ref{eq:ak h})}{\leq}\sqrt{\prod_{h=1}^{d}L_{h}}\left(\sum_{h=1}^{d}c_{h}\frac{1}{JN_{h}^{J}}\prod_{\underset{n\neq h}{n=1}}^{d}\big(c_{n}\frac{\pi^{2}}{6}+d_{n}\big)\right).
\end{align*}
Assuming $\boldsymbol{N}\geq\lambda\boldsymbol{L}$, it follows that
\[
\Vert f_{\boldsymbol{L}}-\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Vert_{2}\leq\frac{1}{\lambda^{J}}\sqrt{\prod_{h=1}^{d}L_{h}}\left(\sum_{h=1}^{d}c_{h}\frac{1}{JL_{h}^{J}}\prod_{\underset{n\neq h}{n=1}}^{d}\big(c_{n}\frac{\pi^{2}}{6}+d_{n}\big)\right)\leq\frac{\varepsilon}{6\xi}
\]
 by definition of $\lambda$.%
\begin{comment}
\#Find N, M, L sind gleich ind allen dimensionen.

d=2

sigma=0.2

eps=10\textasciicircum -4

n=30

K\_v=1

mat=1

EXn=(sigma\textasciicircum 2{*}mat)\textasciicircum (n/2){*}prod(seq(1,n-1,by=2))

M=(2{*}d{*}K\_v{*}EXn/eps)\textasciicircum (1/n)

L=M

xi=sqrt(K\_v\textasciicircum 2{*}2\textasciicircum d{*}M\textasciicircum d)

k=30

\#h is bound of j derivative of normal density (1-dim)

H=function(j)\{

z=sigma{*}sqrt(mat)/sqrt(2)

gamma((j+1)/2)/(pi{*}2{*}z\textasciicircum (j+1))

\}

dh=2{*}H(0)

ch=2\textasciicircum (k+1){*}H(k+1){*}L\textasciicircum (k+1)/(pi\textasciicircum (k+1))

(sqrt(L\textasciicircum d)/k{*}(2{*}dh{*}ch+ch{*}ch{*}pi/3){*}(6{*}xi/eps))\textasciicircum (1/k)
\end{comment}
\end{proof}
\begin{rem}
To obtain an upper bound for $c_{h}$ and $d_{h}$ appearing in Lemma
\ref{lem: norm f_L}, we need a bound for $\left\Vert f_{h}^{(j)}\right\Vert _{\infty}$,
$j\in\{0,J+1\}$. Explicit expressions exists for several densities,
see \cite{junike2023handle}, e.g., if $f_{h}$ is a normal density
with mean zero and volatility $\sigma_{h}\sqrt{T}$ for some $\sigma_{h},T>0$,
it holds that
\[
\|f_{h}^{(j)}\|_{\infty}\leq\frac{\Gamma\left(\frac{j+1}{2}\right)}{2\pi\left(\frac{\sigma\sqrt{T}}{\sqrt{2}}\right)^{j+1}},\quad j=0,1,2,...
\]
\end{rem}


\section{\label{sec:Monte-Carlo-versus}Numerical experiments}

In this section, we compare the numerical effort of the COS method
to a Monte Carlo simulation and empirically verify the explicit bounds
for the parameters required by the COS method, i.e., $\boldsymbol{M}$,
$\boldsymbol{L}$ and $\boldsymbol{N}$.

For this purpose, we consider a \emph{cash-or-nothing put option.
}First, we present the multivariate Black-Scholes model, see \cite{karatzas1998methods}.
Let $r\in\mathbb{R}$ be some continuously compounded interest rate.
Let $\boldsymbol{B}$ be a $d$-dimensional standard Brownian motion
and let $\Sigma$ be a symmetric positive definite matrix defined
by
\[
\Sigma_{hj}=\begin{cases}
\sigma^{2} & ,h=j\\
\rho\sigma^{2} & ,h\neq j,
\end{cases}\quad h,j=1,...,d,
\]
where we set $\sigma:=0.2$ and $\rho=0.5.$ Let $A$ such that $\Sigma=AA{}^{\prime}$.
Let $T>0$ be some time horizon. We consider $d$ asset prices $\boldsymbol{S}(T):\Omega\to\mathbb{R}_{+}^{d}$
at time $T>0$ in the multivariate Black-Scholes model, i.e., the
price of stock $h$ at time $T$ is given by
\[
S_{h}(T)=S_{h}(0)\exp\bigg((r-\frac{1}{2}\Sigma_{hh})T+\sum_{j=1}^{d}A_{hj}B_{j}(T)\bigg),\quad S_{h}(0)>0,\quad h=1,\dots,d,
\]
\begin{comment}
\begin{align*}
 & \Cov\bigg(\sigma_{i}\sum_{k=1}^{d}A_{i,k}B_{k}(t),\sigma_{j}\sum_{h=1}^{d}A_{j,h}B_{h}(t)\bigg)\\
= & \sigma_{i}\sigma_{j}\sum_{k=1}^{d}\sum_{h=1}^{d}A_{i,k}A_{j,h}\Cov\bigg(B_{k}(t),B_{h}(t)\bigg)\\
= & \sigma_{i}\sigma_{j}\sum_{k=1}^{d}A_{i,k}A_{j,k}t
\end{align*}

r=0

t=0.5

sigma=c(0.1,0.2,0.3)

a=matrix(c(1,0,0,0.5,1,0,0.3,0.4,1),byrow=TRUE,ncol=3)

S0=1

h=1

s=numeric(1000000)

for (i in 1:1000000)\{

B=rnorm(3,mean=0,sd=sqrt(t))

S{[}i{]}=S0{*}exp((r-0.5{*}sigma{[}h{]}\textasciicircum 2){*}t+sigma{[}h{]}{*}sum(a{[}h,{]}{*}B))

\}

mean(S)
\end{comment}
The market is complete and arbitrage free, see \cite{karatzas1998methods}.
Let $\boldsymbol{X}$ be the $d$-dimensional centralized log-returns
defined by
\[
X_{h}:=\log\left(S_{h}(T)\right)-\underbrace{E\left[\log\left(S_{h}(T)\right)\right]}_{=:\mu_{h}},\quad h=1,....,d.
\]
The characteristic function, the cumulative distribution function
and the density function of $\boldsymbol{X}$ are denoted by $\varphi$,
$F$ and $f$, respectively. $\boldsymbol{X}$ is multivariate normally
distributed with mean zero and covariance matrix $T\Sigma$.

Let $\boldsymbol{K}\in\mathbb{R}_{+}^{d}$ be some strikes. We consider
a \emph{cash-or-nothing put option} defined by the payoff $g(\boldsymbol{S}(T))$
at maturity $T$, where 
\[
g(\boldsymbol{s})=\prod_{h=1}^{d}1_{[0,K_{h}]}(s_{h}),\quad\boldsymbol{s}\in\mathbb{R}_{+}^{d}.
\]
$g(\boldsymbol{S}(T))$ pays $1\$$ at maturity if all underlyings
$S_{1}(T),...,S_{d}(T)$ are below the corresponding strikes $K_{1},...,K_{d}$
and nothing otherwise. See \cite{ruijter2012two} for other examples
of rainbow options. Let 
\[
v(\boldsymbol{x})=g(\exp(x_{1}+\mu_{1}),...,\exp(x_{d}+\mu_{d})),\quad\boldsymbol{x}\in\mathbb{R}^{d}.
\]
The price of the rainbow option $g$ can be computed by 
\begin{align*}
e^{-rT}E[v(\boldsymbol{X})] & =e^{-rT}\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}.
\end{align*}
For this special choice of $v$, we can obtain the price explicitly;
it holds that 

\begin{align*}
\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x} & =\int_{\mathbb{R}^{d}}\prod_{h=1}^{d}1_{[0,K_{h}]}(\exp(x_{h}+\mu_{h}))f(\boldsymbol{x})d\boldsymbol{x}\\
 & =\int_{-\infty}^{\log(K_{1})-\mu_{1}}...\int_{-\infty}^{\log(K_{d})-\mu_{d}}f(\boldsymbol{x})d\boldsymbol{x}\\
 & =F(\log(\boldsymbol{K})-\boldsymbol{\mu}),
\end{align*}
where $\log(\boldsymbol{K}):=(\log(K_{1}),...,\log(K_{d}))\in\mathbb{R}^{d}$.
We approximate $e^{-rT}E[v(\boldsymbol{X})]$ by the COS method, i.e.,
\[
\int_{\mathbb{R}^{d}}v(\boldsymbol{x)}f(\boldsymbol{x})d\boldsymbol{x}\approx\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}v_{\boldsymbol{k}},
\]
for some $\boldsymbol{N}\in\mathbb{N}^{d}$. The approximation by
the COS method can be compared to $F(\log(\boldsymbol{K})-\boldsymbol{\mu})$.
The coefficients $v_{\boldsymbol{k}}$ of the cash-or-nothing put
option can be obtained in closed-form: let $\boldsymbol{k}\in\mathbb{N}_{0}^{d}$
and $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$ then it holds that
\begin{align*}
v_{\boldsymbol{k}} & =\int_{[-\boldsymbol{M},\boldsymbol{\tilde{M}}]}\prod_{h=1}^{d}\cos\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right)d\boldsymbol{x}\\
 & =\prod_{\underset{k_{h}=0}{h=1}}^{d}\{\tilde{M}_{h}+M_{h}\}\prod_{\underset{k_{h}>0}{h=1}}^{d}\bigg\{\frac{2L_{h}}{\pi k_{h}}\bigg(\sin\big(k_{h}\pi\frac{\tilde{M}_{h}+L_{h}}{2L_{h}}\big)-\sin\big(k_{h}\pi\frac{-M_{h}+L_{h}}{2L_{h}}\big)\bigg)\bigg\},
\end{align*}
where $\tilde{M}_{h}:=\min(\log(K_{h})-\mu_{h},M_{h})$, $h=1,..,d$. 

We set $\boldsymbol{M}$ and $\boldsymbol{L}$ for the COS method
according to Theorem \ref{thm:(Multidimensional-COS-method} using
the error tolerance $\varepsilon$ and the parameters $T=1$, $S_{0}=100$,
$K_{1}=...=K_{d}=100$ and $n=30$. The number of terms $\boldsymbol{N}$
can be obtained from Lemma \ref{lem: norm f_L}, where we set $J=30$.
The parameters $\boldsymbol{M}$, $\boldsymbol{L}$ and $\boldsymbol{N}$
for various $d$ and $\varepsilon$ can be found in Table \ref{tab:L,N}.
Tables \ref{tab:Monte-Carlo-vs-COS_empirical-closed} and \ref{tab:Monte-Carlo-vs-COS_empirical}
show empirically that the bounds for $\boldsymbol{M}$, $\boldsymbol{L}$
and $\boldsymbol{N}$ are sufficiently large to ensure the convergence
of the COS method within the predefined error tolerance, even though
not all assumptions in Lemma \ref{lem: norm f_L} are satisfied. 

Given the error tolerance $\varepsilon>0$ and dimension $d\in\mathbb{N}$,
we compare the \emph{computational cost} of the COS method with a
Monte Carlo simulation. We estimate $e^{-rT}E[v(\boldsymbol{X})]$
by a Monte Carlo simulation, i.e., for some $U\in\mathbb{N}$, we
approximate
\[
E[v(\boldsymbol{X})]\approx\frac{1}{U}\sum_{u=1}^{U}Y_{u},
\]
where $Y_{u}$, $u=1,...,U$, are independent copies of $v(\boldsymbol{X})$.
We choose $U$ depending on the error tolerance: Using the central
limit theorem, we choose 
\begin{equation}
U\geq\left(\Phi^{-1}\left(\frac{p+1}{2}\right)\frac{\sigma}{\varepsilon}\right)^{2}\label{eq:U-1}
\end{equation}
to ensure that $\big|\frac{1}{U}\sum_{u=1}^{U}Y_{u}-F(\log(\boldsymbol{K})-\boldsymbol{\mu})\big|\leq\varepsilon$
with probability $p:=0.99$, where 
\[
\sigma=\sqrt{F(\log(\boldsymbol{K})-\boldsymbol{\mu})(1-F(\log(\boldsymbol{K})-\boldsymbol{\mu})}
\]
is the standard deviation of $v(\boldsymbol{X})$ and $\Phi$ is the
cumulative distribution function of a (one-dimensional) standard normal
random variable.

In Tables \ref{tab:Monte-Carlo-vs-COS_empirical-closed} and \ref{tab:Monte-Carlo-vs-COS_empirical}
we analyze the CPU time of both methods and observe that for a moderate
error tolerance $\varepsilon=10^{-4}$, the COS method is faster than
a Monte Carlo simulation in two or three dimensions, however in four
dimensions, a Monte Carlo simulation is less expensive.

In Table \ref{tab:Monte-Carlo-vs}, we compare the computational complexity
of both methods. The computational complexity of a Monte Carlo simulations
scales like $O(Ud)$. The COS method consist of $d$-nested sums.
Due to Equation (\ref{eq:ck}) and ignoring the computational complexity
to obtain $v_{\boldsymbol{k}}$, the computational complexity of the
COS method scales like $O(\prod_{h=1}^{d}\{N_{h}\}2^{d-1})$. A Monte
Carlo simulation converges rather slowly but hardly depends on the
dimension. On the other hand, the complexity of the COS method grows
exponentially in the dimension however the COS method also converges
exponentially. 

In conclusion, Table \ref{tab:Monte-Carlo-vs} indicates that the
COS method is significantly faster than a Monte Carlo simulation,
even for higher dimensions like $d=4$, when a high accuracy is required.
Table \ref{tab:Monte-Carlo-vs} also indicates that a Monte Carlo
simulation may be advantageous compared to the COS method if the error
tolerance is moderate. From our theoretical analysis, we conclude
that a Monte Carlo simulation might outperform the COS method already
in three dimensions if the error tolerance is equal to $10^{-2}$.

\begin{comment}
\begin{align*}
P\left(-\varepsilon\leq\frac{1}{U}\sum_{u=1}^{U}Y_{u}-E[v(\boldsymbol{X})]\leq\varepsilon\right)\geq p & \Leftrightarrow P\left(-\frac{\sqrt{U}\varepsilon}{\sigma}\leq\frac{\sum_{u=1}^{U}Y_{u}-UE[v(\boldsymbol{X})]}{\sqrt{U}\sigma}\leq\frac{\sqrt{U}\varepsilon}{\sigma}\right)\geq p\\
 & \Leftrightarrow2\Phi\left(\frac{\sqrt{U}\varepsilon}{\sigma}\right)-1\geq p\\
 & \Leftrightarrow U\geq\left(\Phi^{-1}\left(\frac{p+1}{2}\right)\frac{\sigma}{\varepsilon}\right)^{2}
\end{align*}
Note that $v(\boldsymbol{X})=(v(\boldsymbol{X}))^{2}$ implies
\[
\text{Var}(v(\boldsymbol{X}))=E[v(\boldsymbol{X})^{2}]-(E[v(\boldsymbol{X})])^{2}=F(\log(\boldsymbol{K}-\boldsymbol{\mu}))(1-F(\log(\boldsymbol{K}-\boldsymbol{\mu})).
\]

eps=10\textasciicircum -2

lam=0.3

U=round((qnorm( (0.99+1)/2){*}(1/lam)/eps)\textasciicircum 2)

n=100000

z=0

for(i in 1:n)\{

y=rexp(U,rate=lam)

if(abs(mean(y)-1/lam)>eps)

z=z+1

\}

z/n

~

$X_{h}\sim N(0,\sum_{k=1}^{d}\sigma_{h}^{2}a_{h,k}^{2}t)$ Damit gilt
$E[X_{h}^{8}]=105(\sum_{k=1}^{d}\sigma_{h}^{2}a_{h,k}^{2}t)^{4}$
\end{comment}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|c|c|}
\hline 
$\varepsilon$ & $d=2$ & $d=3$ & $d=4$ & $d=5$\tabularnewline
\hline 
\hline 
$10^{-2}$ & 0.82/18 & 0.83/94 & 0.83/568 & 0.84/3889\tabularnewline
\hline 
$10^{-3}$ & 0.88/23 & 0.89/129 & 0.90/851 & 0.91/6323\tabularnewline
\hline 
$10^{-4}$ & 0.95/29 & 0.96/178 & 0.97/1273 & 0.98/10275\tabularnewline
\hline 
$10^{-5}$ & 1.03/37 & 1.04/246 & 1.05/1906 & 1.06/16727\tabularnewline
\hline 
$10^{-6}$ & 1.11/47 & 1.13/339 & 1.14/2858 & 1.15/27155\tabularnewline
\hline 
$10^{-7}$ & 1.20/60 & 1.22/468 & 1.23/4283 & 1.24/44159\tabularnewline
\hline 
$10^{-8}$ & 1.30/76 & 1.31/646 & 1.33/6415 & 1.34/71799\tabularnewline
\hline 
$10^{-9}$ & 1.40/96 & 1.42/892 & 1.43/9617 & 1.44/116675\tabularnewline
\hline 
$10^{-10}$ & 1.51/122 & 1.53/1230 & 1.55/14405 & 1.56/1899798\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:L,N}We set $\sigma=0.2$ and $\rho=0.5$. We show $L/N$,
where $N_{1}=...=N_{d}=N$ and $L_{1}=...=L_{d}=L$ are obtained from
Theorem \ref{thm:(Multidimensional-COS-method} and Lemma \ref{lem: norm f_L}.}
\end{table}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|>{\centering}p{0.5cm}|>{\centering}p{0.5cm}|>{\centering}p{1.6cm}|>{\centering}p{1.8cm}|>{\centering}p{0.9cm}|>{\centering}p{1.6cm}|>{\centering}p{1.7cm}|}
\hline 
$d$ & $N$ & $L$ & abs. error COS & CPU time COS in sec. & $U$ & abs. error MC & CPU time MC in sec.\tabularnewline
\hline 
\hline 
2 & 100 & 0.96 & 1.1e-16 & 0.09 & 1.4e8 & 5.6e-06 & 119.7\tabularnewline
\hline 
3 & 100 & 0.97 & 1.7e-16 & 14.82 & 8.8e7 & 5.0e-05 & 111.0\tabularnewline
\hline 
4 & 100 & 0.98 & 1.7e-15 & 3215.55 & 5.2e7 & 4.7e-05 & 86.2\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:Monte-Carlo-vs-COS_empirical-closed}We set $\varepsilon=10^{-4}$,
$\sigma=0.2$ and $\rho=0.0$. We use $N_{1}=...=N_{d}=N$ and $L_{1}=...=L_{d}=L$.
Computational cost and absolute errors of the COS method and a Monte
Carlo simulation. Due to $\rho=0$, the reference value can be obtained
in closed form.}
\end{table}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|>{\centering}p{0.5cm}|>{\centering}p{0.5cm}|>{\centering}p{1.6cm}|>{\centering}p{1.8cm}|>{\centering}p{0.9cm}|>{\centering}p{1.6cm}|>{\centering}p{1.7cm}|}
\hline 
$d$ & $N$ & $L$ & abs. error COS & CPU time COS in sec. & $U$ & abs. error MC & CPU time MC in sec.\tabularnewline
\hline 
\hline 
2 & 100 & 0.96 & 5.6e-17 & 0.10 & 1.6e8 & 4.1e-05 & 139.08\tabularnewline
\hline 
3 & 100 & 0.97 & 7.1e-06 & 15.27 & 1.4e8 & 8.3e-06 & 169.51\tabularnewline
\hline 
4 & 100 & 0.98 & 4.1e-05 & 3324.75 & 1.2e8 & 6.5e-05 & 205.67\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:Monte-Carlo-vs-COS_empirical}We set $\varepsilon=10^{-4}$,
$\sigma=0.2$ and $\rho=0.5$. We use $N_{1}=...=N_{d}=N$ and $L_{1}=...=L_{d}=L$.
The computational cost of the COS method and a Monte Carlo simulation
are shown. We use the R package \emph{pmvnorm} to obtain a reference
value. Thus, the reference value itself is obtained by a Monte Carlo
simulation and the reported absolute errors should be interpreted
accordingly.}
\end{table}

\begin{comment}
library(mvtnorm)

library(Rcpp)

sourceCpp('/home/Cloud/Professur/Forschung/COS/COS\_multidimensional/R\_Code/cos.cpp')

d\_max=4 \#maximal dimension

repetitions=1 \#repetitions to obtain CPU time.

myLs=matrix(numeric(d\_max{*}d\_max),ncol=d\_max)

myUs=numeric(d\_max)

cpu\_times\_COS=numeric(d\_max)

abs\_errors\_COS=numeric(d\_max)

cpu\_times\_MC=numeric(d\_max)

abs\_errors\_MC=numeric(d\_max)

closed\_forms=numeric(d\_max)

MC=T

for(d in 2:d\_max)\{

\#model Parameters

sigma=0.2

rho=0.5

Sig=diag(rep(sigma\textasciicircum 2,d))+matrix(rep(1,d{*}d),ncol=d){*}rho{*}sigma\textasciicircum 2-diag(rep(sigma\textasciicircum 2{*}rho,d))

mat=1

r=0

S0s=rep(100,d)

Ks=rep(100,d)

p=mat{*}Sig

mus=log(S0s)+(r-0.5{*}diag(Sig)){*}mat

eps=10\textasciicircum -4

\#MC

\#d=3 U=10\textasciicircum 6 takes about 0.8sec

A=t(chol(Sig))

closed\_form\_estimate=pmvnorm(rep(-Inf,d),log(Ks)-mus, mean=rep(0,d),
sigma=p){[}{[}1{]}{]}

U=function(eps)\{ceiling( (qnorm((0.99+1)/2){*}sqrt(closed\_form\_estimate{*}(1-closed\_form\_estimate))/eps)\textasciicircum 2)\}

myUs{[}d{]}=U(eps)

if(rho==0)\{

closed\_form=prod(pnorm(log(Ks)-mus,sd=sigma))

\}else\{

print(paste(\textquotedbl compute reference price by MC. This could
take about \textquotedbl ,U(eps/10)/10\textasciicircum 6{*}0.8/60/60,\textquotedbl hours\textquotedbl ))

\#closed\_form=put\_digi\_MC\_cpp(Ks,mat,mus,U(eps/10),A)

closed\_form=closed\_form\_estimate

\}

closed\_forms{[}d{]}=closed\_form

if(MC)\{

tic<-as.numeric(Sys.time())

for(k in 1:repetitions)\{

mc\_aprox=put\_digi\_MC\_cpp(Ks,mat,mus,U(eps),A) \#MC approximation

\}

tac<-as.numeric(Sys.time())

cpu\_times\_MC{[}d{]}=(tac-tic)/repetitions

abs\_errors\_MC{[}d{]}=abs(mc\_aprox-closed\_form)

\}

print(\textquotedbl vor COS\textquotedbl )

\#COS method

\#d=3 and N1=N2=N3=100, i.e., 10\textasciicircum 6 iterations, takes
about 8sec

n=30

K\_v=1 \#bound for v

EXn=diag(p)\textasciicircum (n/2){*}prod(seq(1,n-1,by=2))

Ms=(2{*}d{*}K\_v{*}EXn/eps)\textasciicircum (1/n)

Ls=Ms

Ns=rep(100,d)

myLs{[}d,1:d{]}=Ls \#save Ls

tic<-as.numeric(Sys.time())

for(k in 1:repetitions)\{

cos\_aprox=put\_digi\_COS\_cpp(Ks,Ls,mat,Ms,mus,Ns,p) \#COS approximation

\}

tac<-as.numeric(Sys.time())

cpu\_times\_COS{[}d{]}=(tac-tic)/repetitions

abs\_errors\_COS{[}d{]}=abs(cos\_aprox-closed\_form)

print(d)

\}

\#assume all Ls and Ns are equal in all dimensions:

mydata\_COS=matrix(c(Ns{[}2:d\_max{]},myLs{[}2:d\_max,1{]},abs\_errors\_COS{[}2:d\_max{]},cpu\_times\_COS{[}2:d\_max{]}),nrow=(d\_max-1))

mydata\_COS

mydata\_MC=matrix(c(myUs{[}2:d\_max{]},abs\_errors\_MC{[}2:d\_max{]},cpu\_times\_MC{[}2:d\_max{]},closed\_forms{[}2:d\_max{]}),nrow=(d\_max-1))

mydata=cbind(mydata\_COS,mydata\_MC)

write.table(mydata,\textquotedbl tmp\_ref\_depend.csv\textquotedbl ,col.names=c(\textquotedbl N\textquotedbl ,\textquotedbl L\textquotedbl ,\textquotedbl err\_COS\textquotedbl ,\textquotedbl CPU\_COS\textquotedbl ,\textquotedbl U\textquotedbl ,\textquotedbl err\_MC\textquotedbl ,\textquotedbl CPU\_MC\textquotedbl ,\textquotedbl Ref\_price\textquotedbl ),row.names=F)
\end{comment}
\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|c|c|}
\hline 
$\varepsilon$ & $d=2$ & $d=3$ & $d=4$ & $d=5$\tabularnewline
\hline 
\hline 
$10^{-2}$ & 49 & 0.012 & <0.001 & <0.001\tabularnewline
\hline 
$10^{-3}$ & >1000 & 0.47 & <0.001 & <0.001\tabularnewline
\hline 
$10^{-4}$ & >1000 & 18 & <0.001 & <0.001\tabularnewline
\hline 
$10^{-5}$ & >1000 & 690 & <0.001 & <0.001\tabularnewline
\hline 
$10^{-6}$ & >1000 & >1000 & 0.08 & <0.001\tabularnewline
\hline 
$10^{-7}$ & >1000 & >1000 & 0.17 & <0.001\tabularnewline
\hline 
$10^{-8}$ & >1000 & >1000 & 3.5 & <0.001\tabularnewline
\hline 
$10^{-9}$ & >1000 & >1000 & 69 & <0.001\tabularnewline
\hline 
$10^{-10}$ & >1000 & >1000 & >1000 & <0.001\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:Monte-Carlo-vs}We set $\sigma=0.2$ and $\rho=0.5$. We
compare the computational complexity of the COS method to the computational
complexity of a Monte Carlo Simulation, i.e., we show the term $Ud(\prod_{h=1}^{d}\{N_{h}\}2^{d-1})^{-1}$
for different values of the dimension $d$ and the error tolerance
$\varepsilon$. We obtained $\boldsymbol{N}$ from Lemma \ref{lem: norm f_L}.}
\end{table}


\section{\label{sec:Conclusions}Conclusions}

In this article, we provided sufficient conditions to approximate
certain integrals appearing in mathematical finance using the COS
method in multidimensions. The COS method requires several parameters:
In particular, one has to specify a truncation range $\boldsymbol{L}$
for the density $f$ of the logarithmic returns, a truncation range
$\boldsymbol{M}$ for the payoff function and the number $\boldsymbol{N}$
of cosine functions to approximate the truncated density. Corollary
\ref{cor:generalCase} provides sufficient conditions on $\boldsymbol{M}$,
$\boldsymbol{L}$ and $\boldsymbol{N}$ to ensure the convergence
of the COS method within a given error tolerance $\varepsilon>0$.
Theorem \ref{thm:(Multidimensional-COS-method} provides explicit
formulas for the truncation ranges $\boldsymbol{M}$ and $\boldsymbol{L}$
extending \cite{junike2022precise} to multivariate dimensions.

Furthermore, an error tolerance for approximating the characteristic
function $\varphi$ of $f$ by some function $\tilde{\varphi}$ in
the supremum norm need to be provided. This error tolerance depends
on $\boldsymbol{N}$, i.e., the higher the number of terms, the better
$\tilde{\varphi}$ has to approximate $\varphi$ to ensure the convergence
of the COS method. Finally, the Fourier-cosine coefficients of the
payoff function must also be approximated in some applications. Corollary
\ref{cor:generalCase} provides conditions on the maximal tolerable
distance between the Fourier-cosine coefficients and their numerical
approximations.

We show both empirically and theoretically that the choice between
a Monte Carlo simulation and the COS method depends crucially on both
the dimension and the error tolerance.

\appendix

\section{Auxiliary results}

The following lemma generalizes some results in \cite{ruijter2012two}
for $d\leq3$ to arbitrary dimensions.
\begin{lem}
\label{lem:int fe}Let $f\in\mathcal{L}^{1}$, $\mathcal{A}\subset\mathbb{R}^{d}$
and $\boldsymbol{k}\in\mathbb{N}_{0}^{d}$. Then it holds that
\[
\int_{\mathcal{A}}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}=\frac{1}{2^{d-1}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\Re\bigg\{\int_{\mathcal{A}}f(\boldsymbol{x})e^{i\frac{\pi}{2}\boldsymbol{\frac{\boldsymbol{sk}}{\boldsymbol{L}}}\cdot\boldsymbol{x}}d\boldsymbol{x}\exp\big(i\frac{\pi}{2}\boldsymbol{s}\cdot\boldsymbol{k}\big)\bigg\}.
\]
\end{lem}

\begin{proof}
It follows by mathematical induction, the fact that cosine is an even
function and the well-known identity,
\[
\cos\theta_{1}\cos\theta_{2}=\frac{1}{2}\left(\cos(\theta_{1}+\theta_{2})+\cos(\theta_{1}-\theta_{2})\right),\quad\theta_{1},\theta_{2}\in\mathbb{R},
\]
see Equations (4.3.17) and (4.3.31) in \cite{abramowitz1972handbook},
that
\begin{align*}
\prod_{h=1}^{d}\cos\theta_{h} & =\frac{1}{2^{d-1}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\cos\left(\boldsymbol{s}\cdot\boldsymbol{\theta}\right),\quad\boldsymbol{\theta}\in\mathbb{R}^{d}.
\end{align*}
\begin{comment}
$d=2$, then
\begin{align*}
\prod_{i=1}^{d}\cos\theta_{i} & =\cos\theta_{1}\cos\theta_{2}=\frac{1}{2}\left(\cos(\theta_{1}+\theta_{2})+\cos(\theta_{1}-\theta_{2})\right)\\
 & =\frac{1}{2^{1}}\sum_{s_{2}=\pm1}\cos\left(\theta_{1}+s_{s}\theta_{2}\right)
\end{align*}

$d\to d+1$
\begin{align*}
 & \prod_{i=1}^{d+1}\cos\theta_{i}\\
 & =\prod_{i=1}^{d}\cos\theta_{i}\cos(\theta_{d+1})\\
 & =\left(\frac{1}{2^{d-1}}\sum_{s_{2}=\pm1,...,s_{d}=\pm1}\cos\left(\theta_{1}+s_{s}\theta_{2}+...+s_{d}\theta_{d}\right)\right)\cos(\theta_{d+1})\\
 & =\frac{1}{2^{d-1}}\sum_{s_{2}=\pm1,...,s_{d}=\pm1}\cos\left(\theta_{1}+s_{s}\theta_{2}+...+s_{d}\theta_{d}\right)\cos(\theta_{d+1})\\
 & =\frac{1}{2^{d-1}}\sum_{s_{2}=\pm1,...,s_{d}=\pm1}\frac{1}{2}\left(\cos\left(\theta_{1}+s_{s}\theta_{2}+...+s_{d}\theta_{d}+\theta_{d}\right)+\cos\left(\theta_{1}+s_{s}\theta_{2}+...+s_{d}\theta_{d}-\theta_{d}\right)\right)\\
 & =\frac{1}{2^{d}}\sum_{s_{2}=\pm1,...,s_{d}=\pm1.s_{d+1}=\pm1}\cos\left(\theta_{1}+s_{s}\theta_{2}+...+s_{d}\theta_{d}+s_{d+1}\theta_{d+1}\right)
\end{align*}
\end{comment}
Hence,
\begin{align*}
\int_{\mathcal{A}}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}= & \int_{\mathcal{A}}f(\boldsymbol{x})\prod_{h=1}^{d}\cos\left(k_{h}\frac{\pi}{2}\frac{x_{h}+L_{h}}{L_{h}}\right)d\boldsymbol{x}\\
= & \frac{1}{2^{d-1}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\int_{\mathcal{A}}f(\boldsymbol{x})\cos\left(\frac{\pi}{2}\boldsymbol{s}\cdot\big(\boldsymbol{k}\frac{\boldsymbol{x}+\boldsymbol{L}}{\boldsymbol{L}}\big)\right)d\boldsymbol{x}\\
= & \frac{1}{2^{d-1}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\Re\left\{ \int_{\mathcal{A}}f(\boldsymbol{x})e^{i\frac{\pi}{2}\boldsymbol{\frac{\boldsymbol{sk}}{\boldsymbol{L}}}\cdot\boldsymbol{x}}d\boldsymbol{x}\exp\left(i\frac{\pi}{2}\boldsymbol{s}\cdot\boldsymbol{k}\right)\right\} .
\end{align*}
\end{proof}
\selectlanguage{american}%
\begin{lem}
\label{lem:integral}Let $s\geq0$, $a>0$ and $n\in\mathbb{N}_{0}$
and $d\in\mathbb{N}$. Then it holds that
\begin{align*}
\int_{\{x\in\mathbb{R}^{d}:|\boldsymbol{x}|>s\}}e^{-a|\boldsymbol{x}|}|\boldsymbol{x}|^{n}d\boldsymbol{x} & =\frac{d\pi^{\frac{\pi}{2}}}{\Gamma\left(1+\frac{d}{2}\right)}\int_{s}^{\infty}e^{-ar}r^{n+d-1}dr\\
 & =\frac{d\pi^{\frac{\pi}{2}}}{\Gamma\left(1+\frac{d}{2}\right)}e^{-as}\frac{(n+d-1)!}{a^{n+d}}\sum_{k=0}^{n+d-1}\frac{(as)^{k}}{k!}.
\end{align*}
\end{lem}

\begin{proof}
The first equation follows by Theorem 8.11 in \cite{amann2009analysis}.
The second equation follows by mathematical induction over $n$. %
\begin{comment}
checked numerically. R-Code

a=rexp(1)

n=sample(1:3,1)

d=sample(1:3,1)

wd=pi\textasciicircum (d/2)/gamma(1+d/2)

s=rexp(1){*}a

f=function(arg)\{

res=0

if(sqrt(sum(arg\textasciicircum 2))>s)\{

res=( exp(-a{*}sqrt(sum(arg\textasciicircum 2))){*} sqrt(sum(arg\textasciicircum 2))\textasciicircum n
) \#(d+1)\textasciicircum (d/2)/(sigma\textasciicircum d{*}wd{*}factorial(d)){*}

\}

res

\}

library(cubature)

\#cubintegrate(f,rep(-Inf,d),rep(Inf,d),maxEval=10\textasciicircum 6,relTol=1e-6)

vegas(f,lowerLimit=rep(-Inf,d),upperLimit=rep(Inf,d),maxEval=10\textasciicircum 6,relTol=1e-6)

k=0:(n+d-1)

d{*}wd{*}exp(-a{*}s){*}factorial(n+d-1)/a\textasciicircum (n+d){*}sum((a{*}s)\textasciicircum k/factorial(k))
\end{comment}
\end{proof}
\selectlanguage{english}%
\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}
