%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[american,english]{babel}
\usepackage{array}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\newtheorem{assumption}{Assumption} 

\renewcommand\theassumption{A\arabic{assumption}}

\usepackage{stmaryrd}

\makeatother

\addto\captionsamerican{\renewcommand{\corollaryname}{Corollary}}
\addto\captionsamerican{\renewcommand{\definitionname}{Definition}}
\addto\captionsamerican{\renewcommand{\examplename}{Example}}
\addto\captionsamerican{\renewcommand{\lemmaname}{Lemma}}
\addto\captionsamerican{\renewcommand{\propositionname}{Proposition}}
\addto\captionsamerican{\renewcommand{\remarkname}{Remark}}
\addto\captionsamerican{\renewcommand{\theoremname}{Theorem}}
\addto\captionsenglish{\renewcommand{\corollaryname}{Corollary}}
\addto\captionsenglish{\renewcommand{\definitionname}{Definition}}
\addto\captionsenglish{\renewcommand{\examplename}{Example}}
\addto\captionsenglish{\renewcommand{\lemmaname}{Lemma}}
\addto\captionsenglish{\renewcommand{\propositionname}{Proposition}}
\addto\captionsenglish{\renewcommand{\remarkname}{Remark}}
\addto\captionsenglish{\renewcommand{\theoremname}{Theorem}}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\usepackage[round]{natbib}
\usepackage{breakurl}
\hypersetup{breaklinks=true, colorlinks=false, pdfusetitle=true }

\begin{document}
\title{From characteristic functions to multivariate distribution functions
and European option prices by the damped COS method}
\author{Gero Junike\thanks{Corresponding author. Carl von Ossietzky Universitt, Institut fr
Mathematik, 26129 Oldenburg, Germany, ORCID: 0000-0001-8686-2661,
E-mail: gero.junike@uol.de}, Hauke Stier\thanks{Carl von Ossietzky Universitt, Institut fr Mathematik, 26129 Oldenburg,
Germany.}}
\maketitle
\begin{abstract}
We provide a unified framework for the computation of the distribution
function and the computation of prices of financial options from the
characteristic function of some density by the COS method. The classical
COS method is numerically very efficient in one-dimension but cannot
deal very well with certain financial options in general dimensions.
Therefore, we introduce the damped COS method which can handle a large
class of integrands very efficiently. We prove the convergence of
the (damped) COS method and study its order of convergence. The (damped)
COS method converges exponentially if the characteristic function
decays exponentially. To apply the (damped) COS method, one has to
specify two parameters: a truncation range for the multivariate density
and the number of terms to approximate the truncated density by a
cosine series. We provide an explicit formula for the truncation range
and an implicit formula for the number of terms. Numerical experiments
up to five dimensions confirm the theoretical results.

\textbf{Keywords: }Fourier-transform, numerical integration, inversion
theorem, COS method, CDF, rainbow options\\
 \textbf{Mathematics Subject Classification} 65D30  65T40  60E10
62P05
\end{abstract}

\section{Introduction}

We aim to solve the following integral numerically:
\begin{equation}
\int_{\mathbb{R}^{d}}w(\boldsymbol{x})g(\boldsymbol{x})d\boldsymbol{x},\label{eq:int}
\end{equation}
The function $g$ is usually a density and the function $w$ is called
\emph{function of interest.} Integrals as in (\ref{eq:int}) appear
in a wide range of applications: The integral is equal to the cumulative
distribution function (CDF) of the density $g$ if $w$ is an indicator
function. CDFs appear in many scientific disciplines. In a financial
context, the function $w$ might also describe some \emph{rainbow
option}, which depends on several assets, and $w$ is also called
\emph{payoff function}. The function $g$ is then the density of the
logarithmic returns of the assets and the integral describes the price
of the rainbow option. 

In many cases, the precise structure of $g$ is unknown but the Fourier
transform $\widehat{g}$ is often given in closed-form. For example:
while the joint density of the sum of two independent random variables
can only be expressed as a convolution and is usually not given explicitly,
the joint characteristic function is much simpler to obtain, it is
just the product of the marginal characteristic functions. Moreover,
the characteristic function of a Lvy process at a particular time-point
is usually given explicitly thanks to the Lvy-Khinchin formula.

The integral in (\ref{eq:int}) can be solved numerically using different
techniques such as (quasi) Monte Carlo simulation, numerical quadrature
and Fourier inversion, see \cite{ruijter2012two, eberlein2010analysis}
and references therein. Special Fourier-inversion methods exists in
case of a CDF, e.g., the Gil-Pelaez formula, see \cite{gil1951note}
and extensions, e.g., \cite{schorr1975numerical,abate1992fourier,waller1995obtaining,hughett1998error}
in one-dimension and \cite{shephard1991characteristic,shephard1991numerical}
in $d$ dimensions.

The COS method, see \citet{fang2009novel} for $d=1$ and \cite{ruijter2012two}
for $d>1$, is a Fourier inversion technique. The COS method has been
applied extensively in computational finance and economics, see \citet{fang2009pricing,fang2011fourier,grzelak2011heston,ruijter2012two,zhang2013efficient,leitao2018data,liu2019neural,liu2019pricing,oosterlee2019mathematical,bardgett2019inferring}.
Other one-dimensional Fourier pricing techniques can be found in \citet{carr1999option,lord2008fast,ortiz2013robust,ortiz2016highly}.
The COS method compares favorably to other Fourier inversion techniques,
see \citet{fang2009novel}.

The main idea of the COS method is to truncate the integration range
in (\ref{eq:int}) to some finite hypercube and to approximate the
density $g$ on the finite truncation range by a classical Fourier-cosine
expansion. There is a clever trick to approximate the cosine coefficients
for $g$ in a very fast and robust way using $\widehat{g}$. The COS
method is particularly fast when the Fourier-cosine coefficients of
the function of interest $w$ are given analytically, too. For instance,
in multivariate dimensions, the Fourier-cosine coefficients of a CDF
can be obtained analytically. However, for many rainbow options the
Fourier-cosine coefficients are not given in closed-form (e.g. arithmetic
basket options). \cite{ruijter2012two} propose in this case to obtain
the Fourier coefficients of the function of interest numerically by
a discrete cosine transform but this significantly slows the COS method
down. In this article, we introduce the \emph{damped COS method},
which is able to avoid the expensive application of the discrete cosine
transform if the Fourier transform of the function of interest is
given in closed-form. The Fourier transform $\widehat{w}$ is known
for many rainbow options, see \citet{hurd2010fourier,eberlein2010analysis}.

In moderate dimensions, the COS method is a fast, robust and straightforward
to implement alternative to the $d$-dimensional Gil-Pelaez formula,
see \cite{shephard1991characteristic,shephard1991numerical} or the
multivariate Lewis formula, see \citet{eberlein2010analysis}, in
particular if $g$ is smooth and has semi-heavy tails.

This article makes the following main contributions: We prove the
convergence of the multidimensional (damped) COS method, we analyze
the order of convergence of the (damped) COS method and provide explicit
and implicit formulas for the truncation range and the number of terms,
respectively. Unlike \cite{ruijter2012two}, who analyze the classical
COS method, we include in our analysis numerical uncertainty on the
characteristic function $\widehat{g}$ and on the Fourier-cosine coefficients
of the function of interest. This helps to understand how approximations
on $\widehat{g}$ and the Fourier-cosine coefficients of the function
of interest affect the total error of the COS method. 

This article is structured as follows: In Section \ref{sec:Notation},
we fix some notation. In Section \ref{sec:Damped-COS-method} we introduce
the multidimensional (damped) COS method, prove its convergence, analyze
the order of convergence and provide explicit and implicit formulas
for the truncation range and the number of terms. In Section \ref{sec:Characteristic-functions}
we discuss some examples for $g$ and $\widehat{g}$. In Section \ref{sec:Functions-of-interest}
we discuss some functions of interest, i.e., examples for $w$. In
Section \ref{sec:Numerical} we provide numerical experiments and
compare the COS method to a Monte Carlo simulation. Section \ref{sec:Conclusions}
concludes.

\global\long\def\Cov{}%

\global\long\def\COS{}%


\section{\label{sec:Notation}Notation}

Let $d\in\mathbb{N}$. Let $\mathcal{L}^{1}$ and $\mathcal{L}^{2}$
denote the sets of integrable and square integrable functions from
$\mathbb{R}^{d}$ to $\mathbb{R}$, and by $\left\langle .,.\right\rangle $
and $\left\Vert .\right\Vert _{2}$ we denote the scalar product and
the (semi)norm on $\mathcal{L}^{2}$, respectively. The supremum norm
of a function $g:\mathbb{R}^{d}\to\mathbb{C}$ is defined by $\left\Vert g\right\Vert _{\infty}:=\sup_{\boldsymbol{x}\in\mathbb{R}^{d}}|g(\boldsymbol{x})|$.
By $\Re\{z\}$ and $\Im\{z\}$ we denote the real part and imaginary
part of a complex number $z\in\mathbb{C}$. The complex unit is denoted
by $i$. By $\Gamma$, we denote the Gamma function. The Euclidean
norm and the maximum norm on $\mathbb{R}^{d}$ is denoted by $|.|$
and by $|.|_{\infty}$ respectively. For $\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^{d}$
we define
\[
\boldsymbol{x}\geq\boldsymbol{y}:\Leftrightarrow x_{1}\geq y_{1},...,x_{d}\geq y_{d}
\]
and treat ``$\leq$'', ``$<$'', ``$>$'', ``$=$'' and ``$\neq$''
similarly. We set $\mathbb{R}_{+}^{d}:=\{\boldsymbol{x}\in\mathbb{R}^{d},\boldsymbol{x}>\boldsymbol{0}\}$.
For $\boldsymbol{a},\boldsymbol{b}\in\mathbb{R}^{d}$ with $\boldsymbol{a}\leq\boldsymbol{b}$,
two complex vectors $\boldsymbol{z},\boldsymbol{y}\in\mathbb{C}^{d}$
and $\lambda\in\mathbb{C}$ we define $\boldsymbol{z}+\boldsymbol{y}:=(z_{1}+y_{1},...,z_{d}+y_{d})\in\mathbb{C}^{d}$
and treat $\boldsymbol{zy}$ and $\frac{\boldsymbol{z}}{\boldsymbol{y}}$
similarly. We further define
\begin{align*}
\boldsymbol{z}\cdot\boldsymbol{y}:= & z_{1}y_{1}+...+z_{d}y_{d}\in\mathbb{C}\\
\lambda\boldsymbol{z}:= & (\lambda z_{1},...,\lambda z_{d})\in\mathbb{C}^{d}\\{}
[\boldsymbol{a},\boldsymbol{b}]:= & [a_{1},b_{1}]\times...\times[a_{d},b_{d}]\subset\mathbb{R}^{d}\\
(-\boldsymbol{\infty},\boldsymbol{b}]:= & (-\infty,b_{1}]\times...\times(-\infty,b_{d}]\subset\mathbb{R}^{d}\\
\exp(\boldsymbol{x}) & :=\left(\exp(x_{1}),....,\exp(x_{d})\right),\quad\boldsymbol{x}\in\mathbb{R}^{d}\\
\log(\boldsymbol{x}) & :=\left(\log(x_{1}),....,\log(x_{d})\right),\quad\boldsymbol{x}\in\mathbb{R}_{+}^{d}.
\end{align*}
For a subset $A\subset\mathbb{R}^{d},$ we define the indicator function
$1_{A}(\boldsymbol{x})$ by one if $\boldsymbol{x}\in A$ and zero
otherwise. Let $\mathbb{N}_{0}=\mathbb{N}\cup\{0\}$. For $\boldsymbol{N}=(N_{1},...,N_{d})\in\mathbb{N}_{0}^{d}$
and a sequence $\left(a_{\boldsymbol{k}}\right)_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\subset\mathbb{C}$,
we define
\begin{align*}
\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}} & :=\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\frac{1}{2^{\Lambda(\boldsymbol{k})}}a_{\boldsymbol{k}},
\end{align*}
where $\Lambda(\boldsymbol{k})$ is the number of components of the
vector $\boldsymbol{k}$ which are equal to zero, i.e., $\Lambda(\boldsymbol{k}):=\sum_{h=1}^{d}1_{\{0\}}(k_{h})$.
For an integrable function $g:\mathbb{R}^{d}\to\mathbb{C}$ define
its \emph{Fourier transform} by
\begin{equation}
\widehat{g}(\boldsymbol{u}):=\int_{\mathbb{R}^{d}}g(\boldsymbol{x})e^{i\boldsymbol{u}\cdot\boldsymbol{x}}d\boldsymbol{x},\quad\boldsymbol{u}\in\mathbb{R}^{d}.\label{eq:FourierTransform}
\end{equation}
This definition of the Fourier transform also appear in \citet[Def. 22.6]{bauer1996probability}
and \cite{eberlein2010analysis}. Provided the integral in (\ref{eq:FourierTransform})
exists, the domain of $\widehat{g}$ may also be extended to parts
of the complex plane. If $g\geq0$ and $\int g(\boldsymbol{x})d\boldsymbol{x}=1$
then $g$ is called \emph{density}, $\widehat{g}$ is called \emph{characteristic
function} and the map $\boldsymbol{y}\mapsto\int_{(-\boldsymbol{\infty},\boldsymbol{y}]}g(\boldsymbol{x})d\boldsymbol{x}$
is called \emph{cumulative distribution function }(CDF)\emph{.} 

\section{\label{sec:Damped-COS-method}Damped COS method}

Typically, the function of interest, $w$, is only locally integrable
but $w\notin\mathcal{L}^{1}$. We provide two examples: The integral
in (\ref{eq:int}) is equal to the CDF of $g$ evaluated at $\boldsymbol{y}\in\mathbb{R}^{d}$
if $w(\boldsymbol{x})=1_{(\boldsymbol{-\infty},\boldsymbol{y}]}(\boldsymbol{x})$
for $\boldsymbol{x}\in\mathbb{R}^{d}$. A rainbow option like an arithmetic
basket put option with strike $K>0$ is defined by $w(\boldsymbol{x})=\max(K-\sum_{h=1}^{d}e^{x_{h}},0)$,
$\boldsymbol{x}\in\mathbb{R}^{d}$. To introduce the \emph{damped
COS method}, we will consider a damped function of interest which
is assumed to be integrable. The idea introducing a damping factor
and to consider modified (integrable) functions of interest dates
back at least to \cite{carr1999option}. Note that for many models
and many rainbow options, both $\widehat{g}$ and $\widehat{w}$ are
given in closed-form, see, e.g., \cite{ruijter2012two,eberlein2010analysis}
and references therein. 

For a scaling factor $\lambda>0$, shift parameter $\boldsymbol{\mu}\in\mathbb{R}^{d}$
and a damping factor $\boldsymbol{\alpha}\in\mathbb{R}^{d}$, define
the \emph{damped} \emph{density} by 
\begin{equation}
f(\boldsymbol{x})=\lambda e^{\boldsymbol{\alpha}\cdot(\boldsymbol{x}+\boldsymbol{\mu})}g(\boldsymbol{x}+\boldsymbol{\mu}),\quad\boldsymbol{x}\in\mathbb{R}^{d}\label{eq:dampedf}
\end{equation}
and the\emph{ damped} \emph{function of interest} by
\begin{equation}
v(\boldsymbol{x})=\frac{1}{\lambda}e^{-\boldsymbol{\alpha}\cdot(\boldsymbol{x}+\boldsymbol{\mu})}w(\boldsymbol{x}+\boldsymbol{\mu}),\quad\boldsymbol{x}\in\mathbb{R}^{d}.\label{eq:dampedv}
\end{equation}
By definition, it follows that
\begin{equation}
\int_{\mathbb{R}^{d}}w(\boldsymbol{x})g(\boldsymbol{x})d\boldsymbol{x}=\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}.\label{eq:wg=00003Dvf}
\end{equation}

Thanks to Proposition \ref{prop:centr}, $f$ is a density centered
around zero if we choose $\lambda$ and $\boldsymbol{\mu}$ carefully
and $\widehat{f}$ is given in closed-form if $\widehat{g}$ is given
in closed-form. 
\begin{prop}
\label{prop:centr}Let $g\in\mathcal{L}_{1}$ and $\boldsymbol{\alpha}\in\mathbb{R}_{+}^{d}$.
Assume that $g$ is a density and that $\boldsymbol{x}\mapsto|\boldsymbol{x}|e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})$
is integrable. Let $\lambda=(\widehat{g}(-i\boldsymbol{\alpha}))^{-1}$
then $\lambda\in(0,\infty)$. Choose $\boldsymbol{\mu}\in\mathbb{R}^{d}$
by
\begin{equation}
\mu_{h}=-\lambda i\left.\frac{\partial}{\partial u_{h}}\widehat{g}(\boldsymbol{u}-i\boldsymbol{\alpha})\right|_{\boldsymbol{u}=\boldsymbol{0}},\quad h=1,...,d.\label{eq:mu_lam_partial}
\end{equation}
Define $f(\boldsymbol{x})=\lambda e^{\boldsymbol{\alpha}\cdot(\boldsymbol{x}+\boldsymbol{\mu})}g(\boldsymbol{x}+\boldsymbol{\mu})$,
$\boldsymbol{x}\in\mathbb{R}^{d}$. Then $f$ is a density with characteristic
function 
\begin{equation}
\widehat{f}(\boldsymbol{u})=\lambda e^{-i\boldsymbol{u}\cdot\boldsymbol{\mu}}\widehat{g}(\boldsymbol{u}-i\boldsymbol{\alpha}),\quad\boldsymbol{u}\in\mathbb{R}^{d}.\label{eq:Fourier_f}
\end{equation}
Further, the moments of $f$ of first order are zero, i.e., $\int_{\mathbb{R}^{d}}f(\boldsymbol{x})x_{h}d\boldsymbol{x}=0$,
$h=1,...,d$.
\end{prop}

\begin{proof}
Use $\int|\boldsymbol{x}|e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})d\boldsymbol{x}<\infty$
and split the integration range into $\mathbb{R}^{d}\setminus B_{1}$
and $B_{1}$, where $B_{1}$ is the unit ball, to see that $\boldsymbol{x}\mapsto e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})$
is integrable. Since $\lambda=(\int_{\mathbb{R}^{d}}e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})d\boldsymbol{x})^{-1}$
and $g$ is a density we have $\lambda\in(0,\infty)$. By the definition
of $\lambda$, $f$ is a density. Since $f\in\mathcal{L}^{1}$, $\widehat{f}$
exists. A direct analysis shows (\ref{eq:Fourier_f}). By \citet[Thm 25.2]{bauer1996probability},
the partial derivatives in Equation (\ref{eq:mu_lam_partial}) exist
and it holds that $\mu_{h}=\lambda\int_{\mathbb{R}^{d}}e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})x_{h}d\boldsymbol{x}$.
Finally, we have that
\begin{align*}
\int_{\mathbb{R}^{d}}f(\boldsymbol{x})x_{h}d\boldsymbol{x}= & \int_{\mathbb{R}^{d}}\lambda e^{\boldsymbol{\alpha}\cdot(\boldsymbol{x}+\boldsymbol{\mu})}g(\boldsymbol{x}+\boldsymbol{\mu})x_{h}d\boldsymbol{x}\lambda\int_{\mathbb{R}^{d}}e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})x_{h}d\boldsymbol{x}-\mu_{h}\lambda\int_{\mathbb{R}^{d}}e^{\boldsymbol{\alpha}\cdot\boldsymbol{x}}g(\boldsymbol{x})d\boldsymbol{x}=0.
\end{align*}
\end{proof}
For some models $\widehat{g}$ need to be approximated numerically,
e.g., in \cite{duffie2003affine}, $\widehat{g}$ is the solution
to some ordinary differential equation, which itself need to be solved
numerically before applying the COS method. From now on, we assume
that $f\in\mathcal{L}_{1}$ and $\widehat{f}$ is given explicitly
or can be approximated numerically efficiently by some function $\vartheta$
and that $v$ is (at least) locally integrable.\textbf{ }At several
places, we assume $v\in\mathcal{L}_{1}$, which usually can be achieved
by setting $\boldsymbol{\alpha}\neq\boldsymbol{0}$. We describe the
COS method in detail to approximate the right hand-side of Equation
(\ref{eq:wg=00003Dvf}). Let $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$
large enough, so that
\begin{equation}
\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}\approx\int_{[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}.\label{eq:approx_integrate_M}
\end{equation}
Let $\boldsymbol{L}\geq\boldsymbol{M}$. If $f$ is centered around
zero, we truncate $f$ and on $[-\boldsymbol{L},\boldsymbol{L}]$
and approximate the truncated damped density by a Fourier-series.
We intuitively have that
\begin{equation}
f\approx f1_{[-\boldsymbol{L},\boldsymbol{L}]}\approx\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\approx\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\approx\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]},\label{eq:f_approx}
\end{equation}
where we define the basis functions
\[
e_{\boldsymbol{k}}(\boldsymbol{x})=\prod_{h=1}^{d}\cos\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right),\quad\boldsymbol{x}\in\mathbb{R}^{d},\quad\boldsymbol{k}\in\mathbb{\mathbb{N}}_{0}^{d},
\]
and the classical Fourier-cosine coefficients of $f1_{[-\boldsymbol{L},\boldsymbol{L}]}$
are given for $\boldsymbol{k}\in\mathbb{\mathbb{N}}_{0}^{d}$ by

\begin{align}
a_{\boldsymbol{k}} & =\frac{1}{\prod_{h=1}^{d}L_{h}}\int_{[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\}\nonumber \\
 & \approx\frac{1}{\prod_{h=1}^{d}L_{h}}\int_{\mathbb{R}^{d}}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\label{eq:integral_ck}\\
 & =\frac{1}{2^{d-1}\prod_{h=1}^{d}L_{h}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\Re\left\{ \widehat{f}\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\exp\left(i\frac{\pi}{2}\boldsymbol{s}\cdot\boldsymbol{k}\right)\right\} =:c_{\boldsymbol{k}}\label{eq:ck}\\
 & \approx\frac{1}{2^{d-1}\prod_{h=1}^{d}L_{h}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\Re\left\{ \vartheta\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\exp\left(i\frac{\pi}{2}\boldsymbol{s}\cdot\boldsymbol{k}\right)\right\} =:\tilde{c}_{\boldsymbol{k}}.\label{eq:ck_tilde}
\end{align}
Sometimes it is necessary to choose $\boldsymbol{L}>\boldsymbol{M}$
to ensure that $c_{\boldsymbol{k}}$ is close enough to $a_{\boldsymbol{k}}$.
The key insight of the COS method is the fact that the integral at
the right-hand side of Equation (\ref{eq:integral_ck}) can be solved
explicitly\footnote{We use $\prod_{h=1}^{d}\cos\theta_{h}=\frac{1}{2^{d-1}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\cos\left(\boldsymbol{s}\cdot\boldsymbol{\theta}\right)$,
$\boldsymbol{\theta}\in\mathbb{R}^{d}$, which follows by mathematical
induction, the fact that cosine is an even function and the trigonometric
identities stated in Equations (4.3.17) and (4.3.31) in \citet[Eqs. (4.3.17, 4.3.31)]{abramowitz1972handbook}.}. If $\widehat{f}$ need to be approximated by some function $\vartheta$,
we use $\tilde{c}_{\boldsymbol{k}}$ instead of $c_{\boldsymbol{k}}$. 

The idea of the multidimensional COS method is to approximate $f$
as in (\ref{eq:f_approx}) and hence the right-hand side of Equation
(\ref{eq:approx_integrate_M}) by

\begin{align}
\int_{[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x} & \approx\int_{[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\label{eq:int_vf_approx}\\
 & =\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\underbrace{\int_{[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}}_{=:v_{\boldsymbol{k}}}\label{eq:vk}\\
 & \approx\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\underbrace{\int_{\mathbb{R}^{d}}v(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}}_{=:\tilde{v}_{\boldsymbol{k}}}.\label{eq:vk_tilde}
\end{align}

\emph{Classical COS method}: If $\boldsymbol{\alpha}=\boldsymbol{0}$,
we speak of the classical COS method. In important cases, the coefficients
$v_{\boldsymbol{k}}$ can be obtained explicitly, i.e., the integral
at right hand-side of Equation (\ref{eq:vk}) can be solved analytically.
Examples in finance include in one dimension plain vanilla put and
call options and digital options, see \citet{fang2009novel} and in
two dimensions geometric basket options, call-on-maximum options or
put-on-minimum options, see \cite{ruijter2012two}. In general dimensions,
$v_{\boldsymbol{k}}$ of a CDF are given in closed-form, see Example
\ref{exa:CDF}. In the case that integral in Equation (\ref{eq:vk})
cannot be solved directly (e.g. for arithmetic basket options), \cite{ruijter2012two}
propose to solve the integral in Equation (\ref{eq:vk}) numerically
to obtain $v_{\boldsymbol{k}}$, e.g., by the discrete cosine transform
or some quadrature rule. However, solving the integral in (\ref{eq:vk})
numerically for each $\boldsymbol{k}$ is expensive and slows down
the COS method significantly. 

\emph{Damped COS method}: If $\boldsymbol{\alpha}\neq\boldsymbol{0}$,
we speak of the damped COS method. Assume that $v$ is integrable,
which usually can be achieved by setting $\boldsymbol{\alpha}\neq\boldsymbol{0}$.
Then we propose to approximate $v_{\boldsymbol{k}}$ by $\tilde{v}_{\boldsymbol{k}}$.
This works if $\boldsymbol{M}$ is large enough. Similar to the solution
presented in Equation (\ref{eq:ck}), the coefficients $\tilde{v}_{\boldsymbol{k}}$
are given analytically:
\begin{equation}
\tilde{v}_{\boldsymbol{k}}=\frac{1}{2^{d-1}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\Re\left\{ \widehat{v}\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\exp\left(i\frac{\pi}{2}\boldsymbol{s}\cdot\boldsymbol{k}\right)\right\} .\label{eq:vkTilde}
\end{equation}
In finance, the function $\hat{v}$ is known explicitly for many rainbow
options, e.g., arithmetic basket options, spread options and put and
call options on the minimum or maximum of $d$ assets.

In the remainder of the article, we will prove under which conditions
the integral (\ref{eq:int}) can be approximated by the (damped) COS
method. 
\begin{rem}
In the special case that $\widehat{f}$ only takes real values, the
computational cost of the COS method can be reduced by (about) the
factor a half, since $c_{\boldsymbol{k}}=0$ if $\sum_{h=1}^{d}k_{h}$
is odd.
\end{rem}

In order to prove the convergence of the COS method in Theorem \ref{thm:approx f}
and Corollary \ref{cor:generalCase}, we need the concept of \emph{COS-admissibility},
which is introduced in Definition \ref{def:COS_admissible} and extends
\citet[Def. 1]{junike2022precise} to the multidimensional setting. 
\begin{defn}
\label{def:COS_admissible}Let $\boldsymbol{L}=(L_{1},...,L_{d})\in\mathbb{R}_{+}^{d}$.
A function $f\in\mathcal{L}^{1}$ is called \emph{COS-admissible},
if
\[
B_{f}(\boldsymbol{L}):=\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\to0,\quad\min_{h=1,...,d}L_{h}\to\infty.
\]
\end{defn}

By Proposition \ref{prop:COSadmissible}, it follows that bounded
densities with existing moments are COS-admissible, which indicates
that the class of $d$-dimensional, COS-admissible densities is large.
\begin{prop}
\label{prop:COSadmissible}Assume $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
with 
\begin{equation}
\int_{\mathbb{R}^{d}}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}<\infty.\label{eq:propBed}
\end{equation}
Then $f$ is COS-admissible. Let $\boldsymbol{L}=(L_{1},...,L_{d})\in\mathbb{R}_{+}^{d}$
then it holds that
\begin{align}
B_{f}(\boldsymbol{L}) & \leq\Xi\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\prod_{h=1}^{d}\max\left\{ x_{h}^{2}L_{h}^{-2},1\right\} |f(\boldsymbol{x})|^{2}d\boldsymbol{x}\label{eq:B(L)_1}\\
 & \leq\frac{\Xi}{d\underset{h=1,...,d}{\min}L_{h}^{2d}}\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+\Xi\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x},\label{eq:B(L)_2}
\end{align}
where $\Xi=\frac{\pi^{2}}{3}\sum_{h=1}^{d}\left(\frac{\pi^{2}}{3}+1\right)^{h-1}$.
\end{prop}

\begin{proof}
Let $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$ and $\boldsymbol{j}\in\mathbb{Z}^{d}$.
It follows by Parseval's identity
\begin{align}
\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}= & \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\bigg|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})\prod_{h=1}^{d}\underbrace{\cos\left(k_{h}\pi\frac{x_{h}-(2j_{h}L_{h}-L_{h})}{2L_{h}}\right)}_{=(-1)^{j_{h}k_{h}}\cos\left(k_{h}\pi\frac{x_{h}+L_{h}}{2L_{h}}\right)}d\boldsymbol{x}\bigg|^{2}\nonumber \\
= & \sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\bigg|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\bigg|^{2}.\label{eq:parseval}
\end{align}
By the Cauchy-Schwarz inequality, we obtain with $g(\boldsymbol{j}):=\prod_{h=1}^{d}\max\{|j_{h}|,1\}$,
\begin{align}
\left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2} & =\left|\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\frac{g(\boldsymbol{j})}{g(\boldsymbol{j})}\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\nonumber \\
 & \leq\bigg(\underbrace{\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}\frac{1}{(g(\boldsymbol{j}))^{2}}}_{=\Xi}\bigg)\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}(g(\boldsymbol{j}))^{2}\left|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}.\label{eq:CS}
\end{align}
The fact that $\Xi=\frac{\pi^{2}}{3}\sum_{h=1}^{d}\left(\frac{\pi^{2}}{3}+1\right)^{h-1}$
can be shown by mathematical induction over $d$. Then it follows
that
\begin{align*}
B_{f}(\boldsymbol{L}) & \overset{(\ref{eq:CS})}{\leq}\,\Xi\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}(g(\boldsymbol{j}))^{2}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\frac{1}{\prod_{h=1}^{d}L_{h}}\left|\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\\
 & \overset{(\ref{eq:parseval})}{=}\Xi\sum_{\boldsymbol{j}\in\mathbb{Z}^{d}\setminus\{\boldsymbol{0}\}}(g(\boldsymbol{j}))^{2}\int_{[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}.
\end{align*}
For $\boldsymbol{j}\in\mathbb{Z}^{d}$ and $\boldsymbol{x}\in[2\boldsymbol{jL}-\boldsymbol{L},2\boldsymbol{jL}+\boldsymbol{L}]$,
one has $|j_{h}|\leq\frac{|x_{h}|}{L_{h}}$, $h=1,...,d$. It follows
that $(g(\boldsymbol{j)})^{2}\leq\prod_{h=1}^{d}\max\left\{ x_{h}^{2}L_{h}^{-2},1\right\} $,
which implies Inequality (\ref{eq:B(L)_1}). By Young's inequality,
it holds that 
\begin{align*}
\prod_{h=1}^{d}\left(\max\left\{ x_{h}^{2d}L_{h}^{-2d},1\right\} \right)^{\frac{1}{d}} & \leq\frac{1}{d}\sum_{h=1}^{d}\max\left\{ x_{h}^{2d}L_{h}^{-2d},1\right\} \leq\frac{\left|\boldsymbol{x}\right|^{2d}}{d\underset{h=1,...,d}{\min}L_{h}^{2d}}+1.
\end{align*}
In the last inequality, we used $\max\{a,b\}\leq a+b$ for any $a,b\geq0$
and $\sum_{h=1}^{d}x_{h}^{2d}\leq\left|\boldsymbol{x}\right|^{2d}$,
which follows from the monotonicity of the $p$-norm. Hence, Inequality
(\ref{eq:B(L)_2}) holds. Assumption (\ref{eq:propBed}) and $f\in\mathcal{L}^{2}$
imply $B_{f}(\boldsymbol{L})\to0$, $\min_{h=1,...,d}L_{h}\to\infty$.
\end{proof}
The following theorem shows that multivariate densities can be approximated
by a cosine expansion. The theorem also includes numerical uncertainty
on on the Fourier transform $\widehat{f}$.
\begin{thm}
\label{thm:approx f}Assume that $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
is COS-admissible. Let $\vartheta:\mathbb{R}^{d}\to\mathbb{C}$ and
define $\tilde{c}_{\boldsymbol{k}}$ as in Equation (\ref{eq:ck_tilde}).
For any $\varepsilon>0$ there is a $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$,
a $\boldsymbol{N}\in\mathbb{N}^{d}$ and a $\gamma>0$ so that $\left\Vert \widehat{f}-\vartheta\right\Vert _{\infty}<\gamma$
implies
\[
\left\Vert f-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\right\Vert _{2}<\varepsilon.
\]
Note that $\boldsymbol{N}$ depends on $\boldsymbol{L}$ and $\gamma$
depends on both $\boldsymbol{L}$ and $\boldsymbol{N}$.
\end{thm}

\begin{proof}
Define $e_{\boldsymbol{k}}^{\boldsymbol{L}}=e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}$.
It holds for $\boldsymbol{l},\boldsymbol{k}\in\mathbb{N}_{0}^{d}$
that
\begin{align}
\left\langle e_{\boldsymbol{k}}^{\boldsymbol{L}},e_{\boldsymbol{l}}^{\boldsymbol{L}}\right\rangle  & =\begin{cases}
2^{\Lambda(\boldsymbol{k})}\prod_{h=1}^{d}L_{h} & ,\boldsymbol{k}=\boldsymbol{l},\\
0 & ,\text{otherwise}
\end{cases}\label{eq:ekel}
\end{align}
where $\Lambda$ is defined Section \ref{sec:Notation}. For any $\boldsymbol{L}\in\mathbb{R}_{+}^{d}$
and $\boldsymbol{N}\in\mathbb{N}^{d}$, it holds that
\begin{align*}
\left\Vert f-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}\leq & \underbrace{\left\Vert f-f1_{[-\boldsymbol{L},\boldsymbol{L}]}\right\Vert _{2}}_{=:A_{1}(\boldsymbol{L})}+\underbrace{\left\Vert f1_{[-\boldsymbol{L},\boldsymbol{L}]}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{2}(\boldsymbol{L},\boldsymbol{N})}\\
 & +\underbrace{\left\Vert \sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}(a_{\boldsymbol{k}}-c_{\boldsymbol{k}})e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{3}(\boldsymbol{L},\boldsymbol{N})}+\underbrace{\left\Vert \sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}(c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}})e_{\boldsymbol{k}}^{\boldsymbol{L}}\right\Vert _{2}}_{=:A_{4}(\boldsymbol{L},\boldsymbol{N})}.
\end{align*}
Further,
\begin{align*}
A_{3}(\boldsymbol{L},\boldsymbol{N})^{2} & =\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{l}\leq\boldsymbol{N}}\frac{1}{2^{\Lambda(\boldsymbol{k})+\Lambda(\boldsymbol{l})}}(a_{\boldsymbol{k}}-c_{\boldsymbol{k}})(a_{\boldsymbol{l}}-c_{\boldsymbol{l}})\left\langle e_{\boldsymbol{k}}^{\boldsymbol{L}},e_{\boldsymbol{l}}^{\boldsymbol{L}}\right\rangle \leq\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\prod_{h=1}^{d}\{L_{h}\}\left|a_{\boldsymbol{k}}-c_{\boldsymbol{k}}\right|^{2}=B_{f}(\boldsymbol{L}),
\end{align*}
see Definition \ref{def:COS_admissible}. For $\varepsilon>0$, choose
$\boldsymbol{L}\in\mathbb{R}_{+}^{d}$ such that $A_{1}(\boldsymbol{L})<\frac{\varepsilon}{4}$
and $B_{f}(\boldsymbol{L})<\big(\frac{\varepsilon}{4}\big){}^{2}$.
Hence, $A_{3}(\boldsymbol{L},\boldsymbol{N})<\frac{\varepsilon}{4}$.
Then choose $\boldsymbol{N}\in\mathbb{N}^{d}$ such that $A_{2}(\boldsymbol{L},\boldsymbol{N})<\frac{\varepsilon}{4}$.
Such $\boldsymbol{N}$ exists by classically Fourier analysis. By
the definition of $c_{\boldsymbol{k}}$ and $\tilde{c}_{\boldsymbol{k}}$,
see Equation (\ref{eq:ck}), it follows that
\begin{align*}
|c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}}| & \leq\frac{1}{2^{d-1}\prod_{h=1}^{d}L_{h}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\left|\widehat{f}\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)-\vartheta\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\right|\leq\frac{\left\Vert \widehat{f}-\vartheta\right\Vert _{\infty}}{\prod_{h=1}^{d}L_{h}}.
\end{align*}
Similarly to the analysis of $A_{3}$, we have
\begin{align}
A_{4}(\boldsymbol{L},\boldsymbol{N}) & ^{2}\leq\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\prod_{h=1}^{d}\{L_{h}\}\left|c_{\boldsymbol{k}}-\tilde{c}_{\boldsymbol{k}}\right|^{2}\leq\frac{\left\Vert \widehat{f}-\vartheta\right\Vert _{\infty}^{2}}{\prod_{h=1}^{d}L_{h}}\prod_{h=1}^{d}\{N_{h}+1\}.\label{eq:A4}
\end{align}
Choose $\gamma=\frac{\varepsilon}{4}\sqrt{\prod_{h=1}^{d}L_{h}}\left(\prod_{h=1}^{d}\{N_{h}+1\}\right)^{-\frac{1}{2}}.$
Then $\left\Vert \widehat{f}-\vartheta\right\Vert _{\infty}<\gamma$
implies $A_{4}(\boldsymbol{L},\boldsymbol{N})<\frac{\varepsilon}{4}$,
which concludes the proof.
\end{proof}
Corollary \ref{cor:generalCase} provides sufficient conditions in
order to ensure that the COS method approximates the price of a rainbow
option within a predefined error tolerance $\varepsilon>0$ including
numerical uncertainty on $\widehat{f}$ and numerical uncertainty
on the cosine coefficients of the function of interest $v$: either
because the $v_{\boldsymbol{k}}$ are approximated by solving the
integral in Equation (\ref{eq:vk}) numerically or because $v_{\boldsymbol{k}}$
are approximated by $\tilde{v}_{\boldsymbol{k}}$ defined in Equation
(\ref{eq:vk_tilde}).
\begin{cor}
\label{cor:generalCase}(Convergence of the COS method). Let $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
be COS-admissible and $v:\mathbb{R}^{d}\to\mathbb{R}$ be locally
in $\mathcal{L}^{2}$, that is, $v1_{[-\boldsymbol{M},\boldsymbol{M}]}\in\mathcal{L}^{2}$
for any $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$. Assume $vf\in\mathcal{L}^{1}$,
then the integral of the product of $f$ and $v$ can be approximated
by a finite sum as follows: Let $\varepsilon>0$. Let $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$
and $\xi>0$ such that
\begin{equation}
\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}\left|v(\boldsymbol{x})f(\boldsymbol{x})\right|d\boldsymbol{x}\leq\frac{\varepsilon}{3},\quad\left\Vert v1_{[-\boldsymbol{M},\boldsymbol{M}]}\right\Vert _{2}\leq\xi.\label{eq:Mgeneral}
\end{equation}
Let $\boldsymbol{L}\geq\boldsymbol{M}$ such that
\begin{equation}
\left\Vert f-f1_{[-\boldsymbol{L},\boldsymbol{L}]}\right\Vert _{2}\leq\frac{\varepsilon}{12\xi}\label{eq:L1general}
\end{equation}
and
\begin{equation}
B_{f}(\boldsymbol{L})\leq\left(\frac{\varepsilon}{12\xi}\right)^{2}.\label{eq:L2general}
\end{equation}
Choose $\boldsymbol{N}\in\mathbb{N}^{d}$ large enough, so that 
\begin{equation}
\left\Vert f1_{[-\boldsymbol{L},\boldsymbol{L}]}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\right\Vert _{2}\leq\frac{\varepsilon}{12\xi}.\label{eq:Nlargeenough}
\end{equation}
For some $\vartheta:\mathbb{R}^{d}\to\mathbb{C}$ assume
\begin{equation}
\left\Vert \widehat{f}-\vartheta\right\Vert _{\infty}\leq\frac{\varepsilon}{12\xi}\frac{\sqrt{\prod_{h=1}^{d}L_{h}}}{\sqrt{\prod_{h=1}^{d}\{N_{h}+1\}}}.\label{eq:phi_tilde}
\end{equation}
Let $\eta>0$ such that $\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{c}_{\boldsymbol{k}}|^{2}\leq\eta.$
Let $(\tilde{v}_{\boldsymbol{k}})_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}$
such that
\begin{equation}
\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{v}_{\boldsymbol{k}}-v_{\boldsymbol{k}}|^{2}\leq\frac{\varepsilon^{2}}{9\eta}.\label{eq:v_l_v_l_tilde}
\end{equation}
Then it follows that
\begin{equation}
\left|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}\right|\leq\varepsilon.\label{eq:Ngeneral}
\end{equation}
\end{cor}

\begin{proof}
Define $e_{\boldsymbol{k}}^{\boldsymbol{L}}=e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}$.
Let $A_{1}(\boldsymbol{L})$, $A_{2}(\boldsymbol{L},\boldsymbol{N})$
and $A_{4}(\boldsymbol{L},\boldsymbol{N})$ be as in the proof of
Theorem \ref{thm:approx f}. By Inequalities (\ref{eq:A4}, \ref{eq:phi_tilde})
it follows that $A_{4}(\boldsymbol{L},\boldsymbol{N})\leq\frac{\varepsilon}{12\xi}$.
Due to $v_{\boldsymbol{k}}=\langle v1_{[-\boldsymbol{M},\boldsymbol{M}]},e_{\boldsymbol{k}}^{\boldsymbol{L}}\rangle$
and applying Theorem \ref{thm:approx f} and the Cauchy-Schwarz inequality,
we have that
\begin{align*}
\Big|\int_{\mathbb{R}^{d}}v(\boldsymbol{x}) & f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}\Big|\\
= & \bigg|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}+\langle v1_{[-\boldsymbol{M},\boldsymbol{M}]},f\rangle-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}\langle v1_{[-\boldsymbol{M},\boldsymbol{M}]},e_{\boldsymbol{k}}^{\boldsymbol{L}}\rangle-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}(\tilde{v}_{\boldsymbol{k}}-v_{\boldsymbol{k}})\bigg|\\
\leq & \underbrace{\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}|v(\boldsymbol{x})f(\boldsymbol{x})|d\boldsymbol{x}}_{=:D_{1}(\boldsymbol{M})}+\Big|\Big\langle v1_{[-\boldsymbol{M},\boldsymbol{M}]},f-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Big\rangle\Big|+\underbrace{\sqrt{\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{v}_{\boldsymbol{k}}-v_{\boldsymbol{k}}|^{2}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{c}_{\boldsymbol{k}}|^{2}}}_{=:D_{2}(\boldsymbol{N},\boldsymbol{L},\boldsymbol{M})}\\
< & \frac{\varepsilon}{3}+\|v1_{[-\boldsymbol{M},\boldsymbol{M}]}\|_{2}\,\Big\Vert f-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}\tilde{c}_{\boldsymbol{k}}e_{\boldsymbol{k}}^{\boldsymbol{L}}\Big\Vert_{2}+\frac{\varepsilon}{3}\\
< & \frac{\varepsilon}{3}+\xi\left(A_{1}(\boldsymbol{L})+A_{2}(\boldsymbol{L},\boldsymbol{N})+\sqrt{B_{f}(\boldsymbol{L})}+A_{4}(\boldsymbol{L},\boldsymbol{N})\right)+\frac{\varepsilon}{3}\\
\leq & \frac{\varepsilon}{3}+\xi\left(\frac{\varepsilon}{12\xi}+\frac{\varepsilon}{12\xi}+\frac{\varepsilon}{12\xi}+\frac{\varepsilon}{12\xi}\right)+\frac{\varepsilon}{3}=\varepsilon.
\end{align*}
\end{proof}
In \cite{junike2022precise} and \cite{junike2023handle}, it is assumed
that $f$ has semi-heavy tails, i.e., $f$ decays exponentially or
faster. Here, we make the same assumption in multivariate dimensions
to be able to estimate $\boldsymbol{M}$, $\boldsymbol{L}$ and $\boldsymbol{N}$.
\begin{defn}
A function $f:\mathbb{R}^{d}\to\mathbb{R}$ \emph{decays exponentially}
if there are $C_{1},C_{2},m>0$ such that for $|\boldsymbol{x}|>m$
 it holds that $|f(\boldsymbol{x})|\leq C_{1}e^{-C_{2}|\boldsymbol{x}|}$. 
\end{defn}

\begin{lem}
\label{lem:fMinusAkEk}Let $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$.
Let $\boldsymbol{M},\boldsymbol{L}\in\mathbb{R}_{+}^{d}$ with $\boldsymbol{M}\leq\boldsymbol{L}$
then it holds that
\[
\Vert f1_{[-\boldsymbol{L},\boldsymbol{L}]}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\Vert_{2}^{2}\leq\int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}-\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|c_{\boldsymbol{k}}|^{2}+G(\boldsymbol{L}),
\]
where 
\begin{equation}
G(\boldsymbol{L}):=B_{f}(\boldsymbol{L})+2\sqrt{B_{f}(\boldsymbol{L})\int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}}.\label{eq:eta_l}
\end{equation}
\end{lem}

\begin{proof}
Let
\[
\phi_{\boldsymbol{k}}:=\frac{1}{\prod_{h=1}^{d}L_{h}}\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}f(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x},\quad\boldsymbol{k}\in\mathbb{N}_{0}^{d}.
\]
It holds that $c_{\boldsymbol{k}}=a_{\boldsymbol{k}}+\phi_{\boldsymbol{k}}$.
It follows by the Cauchy-Schwarz inequality that
\begin{align}
\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|c_{\boldsymbol{k}}|^{2}= & \prod_{h=1}^{d}L_{h}\left(\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|a_{\boldsymbol{k}}|^{2}+\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\phi_{\boldsymbol{k}}|^{2}+2\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\phi_{\boldsymbol{k}}||a_{\boldsymbol{k}}|\right)\nonumber \\
\leq & \prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|a_{\boldsymbol{k}}|^{2}+B_{f}(\boldsymbol{L})+2\prod_{h=1}^{d}L_{h}\sqrt{\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\phi_{\boldsymbol{k}}|^{2}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|a_{\boldsymbol{k}}|^{2}}\nonumber \\
\overset{(\ref{eq:parseval})}{\leq} & \prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|a_{\boldsymbol{k}}|^{2}+\underbrace{B_{f}(\boldsymbol{L})+2\sqrt{B_{f}(\boldsymbol{L})\int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}}}_{=G(\boldsymbol{L})}\label{eq:a_k_c_k}\\
\text{\ensuremath{\overset{(\ref{eq:parseval})}{\leq}}} & \int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+G(\boldsymbol{L}).\label{eq:bound_ck2}
\end{align}
Hence,
\begin{align*}
\Vert f1_{[-\boldsymbol{L},\boldsymbol{L}]}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\Vert_{2}^{2}\overset{(\ref{eq:ekel})}{\leq} & \prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{k_{1}>N_{1}\text{ or}\dots\text{or }k_{d}>N_{d}}|a_{\boldsymbol{k}}|^{2}\\
=\,\,\, & \prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}|a_{\boldsymbol{k}}|^{2}-\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|a_{\boldsymbol{k}}|^{2}\\
\overset{(\ref{eq:parseval},\ref{eq:a_k_c_k})}{\leq} & \int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}-\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|c_{\boldsymbol{k}}|^{2}+G(\boldsymbol{L}).
\end{align*}
\end{proof}
\begin{thm}
\emph{\label{thm:(Multidimensional-COS-method}(Classical COS method:
Find $\boldsymbol{M}$ and $\boldsymbol{L}$). }Let $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
be a nonnegative function satisfying Inequality (\ref{eq:propBed}).
Let $v:\mathbb{R}^{d}\to\mathbb{R}$ be bounded with $\left\Vert v\right\Vert _{\infty}\in(0,\infty)$.
Let $n\geq2$ be some even number and assume the moments of $f$ of
$n^{th}-$order exist, i.e.,
\begin{equation}
m_{h}(n):=\int_{\mathbb{R}^{d}}x_{h}^{n}f(\boldsymbol{x})d\boldsymbol{x}=i^{-n}\left.\frac{\partial^{n}}{\partial u_{h}^{n}}\widehat{f}(\boldsymbol{u})\right|_{\boldsymbol{u}=\boldsymbol{0}}\in(0,\infty),\quad h=1,...,d.\label{eq:moments}
\end{equation}
Assume that $f$ decays exponentially. Let $\varepsilon>0$ be small
enough. Define
\begin{equation}
M_{h}:=\left(\frac{3d\left\Vert v\right\Vert _{\infty}}{\varepsilon}m_{h}(n)\right)^{\frac{1}{n}},\quad h=1,...,d,\label{eq:m-1-1}
\end{equation}
and $\boldsymbol{L}=\boldsymbol{M}=(M_{1},...,M_{d})\in\mathbb{R}_{+}^{d}$.
There is a $\boldsymbol{N}\in\mathbb{N}_{0}^{d}$ so that
\begin{equation}
\left|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}v_{\boldsymbol{k}}\right|\leq\varepsilon.\label{eq:endRes}
\end{equation}
\end{thm}

\begin{cor}
\label{cor:vk_tilde_M_N}\emph{(Damped COS method: Find $\boldsymbol{M}$
and $\boldsymbol{L}$).} Assume that all assumptions in Theorem \ref{thm:(Multidimensional-COS-method}
hold and that $v\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$, $v$ satisfies
Inequality (\ref{eq:propBed}) and $v$ decays exponentially. Define
$\tilde{v}_{\boldsymbol{k}}$ as in (\ref{eq:vk_tilde}). Then Inequality
(\ref{eq:endRes}) still holds, if we replace $v_{\boldsymbol{k}}$
by $\tilde{v}_{\boldsymbol{k}}$.
\end{cor}

\begin{cor}
\label{cor:NrTerms}\emph{(Find $\boldsymbol{N}$).} Assume that all
assumptions in Corollary \ref{cor:vk_tilde_M_N} hold. The number
of terms can be chosen by any $\boldsymbol{N}\in\mathbb{N}_{0}^{d}$
such that
\begin{equation}
\left|(2\pi)^{-d}\int_{\mathbb{R}^{d}}|\widehat{f}(\boldsymbol{u})|^{2}d\boldsymbol{u}-\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|c_{\boldsymbol{k}}|^{2}\right|\leq\frac{\varepsilon^{2}}{162\left\Vert v\right\Vert _{2}^{2}}.\label{eq:f_l-akek}
\end{equation}
\end{cor}

\begin{proof}
\emph{We first prove Theorem \ref{thm:(Multidimensional-COS-method}}:
Equation (\ref{eq:moments}) follows by \citet[Thm 25.2]{bauer1996probability}.
For $h\in\{1,...,d\}$ let $\pi_{h}:\mathbb{R}^{d}\to\mathbb{R}$,
$\boldsymbol{x}\mapsto x_{h}$. Let $\lambda^{d}$ be the Lebesgue
measure on $\mathbb{R}^{d}$ and define the finite and positive measure
$\mu:=f\lambda^{d}$. By Markov's inequality, see \citet[Lemma 20.1]{bauer1992mass},
it follows that
\[
\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}\left|v(\boldsymbol{x})f(\boldsymbol{x})\right|d\boldsymbol{x}\leq\left\Vert v\right\Vert _{\infty}\sum_{h=1}^{d}\mu\left(\left\{ \boldsymbol{x}\in\mathbb{R}^{d}:|\pi_{h}(\boldsymbol{x})|\geq M_{h}\right\} \right)\leq\left\Vert v\right\Vert _{\infty}\sum_{h=1}^{d}\frac{m_{h}(n)}{M_{h}^{n}}=\frac{\varepsilon}{3}.
\]
The last inequality follows by the definition of $\boldsymbol{M}$.
Define $\xi:=\left\Vert v\right\Vert _{\infty}\sqrt{2^{d}\prod_{h=1}^{d}M_{h}}$.
It holds that $\left\Vert v1_{[-\boldsymbol{M},\boldsymbol{M}]}\right\Vert _{2}\leq\xi.$
Hence, the inequalities in (\ref{eq:Mgeneral}) are satisfied. Next,
we use the following auxiliary result: \foreignlanguage{american}{Let
$s\geq0$, $a>0$ and $n\in\mathbb{N}_{0}$ and $d\in\mathbb{N}$.
Then it holds by mathematical induction over $n$ and \citet[Theorm 8.11 ]{amann2009analysis}
that
\begin{align}
\int_{\{\boldsymbol{x}\in\mathbb{R}^{d}:|\boldsymbol{x}|>s\}}e^{-a|\boldsymbol{x}|}|\boldsymbol{x}|^{n}d\boldsymbol{x} & =\frac{d\pi^{\frac{d}{2}}}{\Gamma\left(1+\frac{d}{2}\right)}e^{-as}\frac{(n+d-1)!}{a^{n+d}}\sum_{k=0}^{n+d-1}\frac{(as)^{k}}{k!}.\label{eq:amann}
\end{align}
}For $\varepsilon$ small enough, $\boldsymbol{L}$ is large enough.
Using that $f$ decays exponentially and applying Equation (\ref{eq:amann}),
we obtain with $\ell:=\min_{h=1,...,d}L_{h}$ that
\begin{align}
\left\Vert f-f1_{[-\boldsymbol{L},\boldsymbol{L}]}\right\Vert _{2} & \leq C_{1}\sqrt{\int_{\{\boldsymbol{x}\in\mathbb{R}^{d}:|\boldsymbol{x}|>\ell\}}e^{-2C_{2}|\boldsymbol{x}|}d\boldsymbol{x}}\leq\frac{\varepsilon}{12\xi}.\label{eq:||f-f_l}
\end{align}
The last inequality holds true if $\varepsilon$ is small enough because,
thanks to Inequality (\ref{eq:amann}), the term in the middle of
(\ref{eq:||f-f_l}) decreases exponentially in $\varepsilon$, while
the term at the right-hand side of (\ref{eq:||f-f_l}) goes to zero
like $\varepsilon^{1+\frac{d}{2n}}$ for $\varepsilon\searrow0$.
Hence, Inequality (\ref{eq:L1general}) holds. By Inequality (\ref{eq:B(L)_2})
it holds that $B_{f}(\boldsymbol{L})\leq\varepsilon^{2}(12\xi)^{-2}$
if $\varepsilon$ is small enough because $B_{f}(\boldsymbol{L})$
decreases exponentially in $\varepsilon$: use Inequality (\ref{eq:||f-f_l})
and observe that the term $\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{L},\boldsymbol{L}]}\left|\boldsymbol{x}\right|^{2d}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}$
converges exponentially thanks to Inequality (\ref{eq:amann}). Hence,
Inequality (\ref{eq:L2general}) holds. By classical Fourier analysis,
there is a $\boldsymbol{N}\in\mathbb{N}_{0}^{d}$ such that Inequality
(\ref{eq:Nlargeenough}) is satisfied. By assumption we have $\vartheta=\widehat{f}$
and $v_{\boldsymbol{k}}=\tilde{v}_{\boldsymbol{k}}$. Inequalities
(\ref{eq:phi_tilde}) and (\ref{eq:v_l_v_l_tilde}) hold trivially.
Apply Corollary \ref{cor:generalCase} to finish the proof of Theorem
\ref{thm:(Multidimensional-COS-method}. 

\emph{We prove Corollary \ref{cor:vk_tilde_M_N}}: We have to show
that Inequality (\ref{eq:v_l_v_l_tilde}) holds to proof Corollary
\ref{cor:vk_tilde_M_N}. Let $G(\boldsymbol{L})$ be as in Equality
(\ref{eq:eta_l}). Observe $G(\boldsymbol{L})\to0$, $\min_{h}L_{h}\to\infty$
because $f$ is COS-admissible by Proposition \ref{prop:COSadmissible}.
There is $\boldsymbol{P}\in\mathbb{R}_{+}^{d}$ and a $\gamma>0$
such that $G(\boldsymbol{L})\leq\gamma$ for all $\boldsymbol{L}\geq\boldsymbol{P}$.
By Inequality (\ref{eq:bound_ck2}), it follows for all $\boldsymbol{N}\in\mathbb{N}^{d}$
and all $\boldsymbol{L}\geq\boldsymbol{P}$ that
\begin{equation}
\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|c_{\boldsymbol{k}}|^{2}\leq\frac{\int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+\gamma}{\prod_{h=1}^{d}P_{h}}=:\eta<\infty.\label{eq:eta}
\end{equation}
It follows by Proposition \ref{prop:COSadmissible} for all $\boldsymbol{N}\in\mathbb{N}^{d}$
that
\begin{align}
\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|\tilde{v}_{\boldsymbol{k}}-v_{\boldsymbol{k}}|^{2} & \leq\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}\left|\int_{\mathbb{R}^{d}\setminus[-\boldsymbol{M},\boldsymbol{M}]}v(\boldsymbol{x})e_{\boldsymbol{k}}(\boldsymbol{x})d\boldsymbol{x}\right|^{2}\leq\prod_{h=1}^{d}M_{h}B_{v}(\boldsymbol{M})\leq\frac{\varepsilon^{2}}{9\eta}\label{eq:v_tile-vk}
\end{align}
the last inequality holds true if $\varepsilon$ is small enough because
the term in the middle of (\ref{eq:v_tile-vk}) decreases exponentially
in $\varepsilon$ since $v$ decays exponentially, while the right-hand
side of (\ref{eq:v_tile-vk}) goes to zero like $\varepsilon^{2}$
for $\varepsilon\searrow0$. 

\emph{We prove Corollary \ref{cor:NrTerms}}: For $\varepsilon$ small
enough, $\boldsymbol{M}$ and $\boldsymbol{L}$ are large enough.
Apply the Plancherel theorem and Corollary \ref{cor:vk_tilde_M_N}
to $f$ and $v:=f$ to see that there is a $\boldsymbol{N}\in\mathbb{N}^{d}$
such that Inequality (\ref{eq:f_l-akek}) holds. Let $G(\boldsymbol{L})$
be defined as in Equation (\ref{eq:eta_l}). Note that $\left\Vert v1_{[-\boldsymbol{M},\boldsymbol{M}]}\right\Vert _{2}\leq\left\Vert v\right\Vert _{2}$
for any $\boldsymbol{M}\in\mathbb{R}_{+}^{d}$. By Lemma \ref{lem:fMinusAkEk}
and the Plancherel theorem, it follows that
\begin{align}
\Vert f1_{[-\boldsymbol{L},\boldsymbol{L}]}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}a_{\boldsymbol{k}}e_{\boldsymbol{k}}1_{[-\boldsymbol{L},\boldsymbol{L}]}\Vert_{2}^{2}\leq & \left|(2\pi)^{-d}\int_{\mathbb{R}^{d}}|\widehat{f}(\boldsymbol{u})|^{2}d\boldsymbol{u}-\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}|c_{\boldsymbol{k}}|^{2}\right|+G(\boldsymbol{L})\nonumber \\
\leq & \frac{\varepsilon^{2}}{162\left\Vert v\right\Vert _{2}^{2}}+\frac{\varepsilon^{2}}{162\left\Vert v\right\Vert _{2}^{2}}=\left(\frac{\varepsilon}{9\left\Vert v\right\Vert _{2}}\right)^{2}.\label{eq:fakekPlan}
\end{align}
The last inequality holds because for $\varepsilon>0$ small enough,
$\boldsymbol{L}$ is large enough so that $G(\boldsymbol{L})\leq\frac{\varepsilon^{2}}{162\left\Vert v\right\Vert _{2}^{2}}$.
Note that we may we may replace the term $\frac{\varepsilon}{12\xi}$
in Inequalities (\ref{eq:L1general}, \ref{eq:L2general}, \ref{eq:Nlargeenough})
in Corollary \ref{cor:generalCase} by $\frac{\varepsilon}{9\left\Vert v\right\Vert _{2}}$
since $\widehat{f}=\vartheta$ and $\left\Vert v\right\Vert _{2}<\infty$.
Apply Inequality (\ref{eq:fakekPlan}) to conclude.
\end{proof}
\begin{rem}
\label{rem:lessStable}Provided the expression $(2\pi)^{-d}\int_{\mathbb{R}^{d}}|\widehat{f}(\boldsymbol{u})|^{2}d\boldsymbol{u}$
can be obtained precisely, Inequality (\ref{eq:f_l-akek}) makes it
possible to define a stopping criterion for $\boldsymbol{N}$. In
particular, Inequality (\ref{eq:f_l-akek}) enables us to determine
$\boldsymbol{N}$ \emph{while} computing the coefficients $c_{\boldsymbol{k}}$:
incrementally increase $\boldsymbol{N}$ and compute $|c_{\boldsymbol{k}}|$
and $|c_{\boldsymbol{k}}|^{2}$ simultaneously. Stop when Inequality
(\ref{eq:f_l-akek}) is met. However, since right-hand side of Equation
(\ref{eq:f_l-akek}) converges to zero like $O\left(\varepsilon^{2}\right)$,
rounding-off errors makes it difficult to find $\boldsymbol{N}$ by
Inequality (\ref{eq:f_l-akek}) for very small $\varepsilon$. Using
arbitrary-precision arithmetic instead of fixed-precision arithmetic
should overcome this drawback.
\end{rem}

The next theorem implies that the COS method converges exponentially
if $\widehat{f}$ decays exponentially, i.e., if Inequality (\ref{eq:f_hat_p})
holds for all $p>0$. The cases (i) and (ii) in Theorem \ref{thm:order of convergence}
treat the classical and the damped COS method, respectively. The bound
for the order of convergence of the damped COS method is slightly
sharper.
\begin{thm}
\label{thm:order of convergence}(Order of convergence). Assume $f\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$
satisfies Inequality (\ref{eq:propBed}) and decays exponentially.
Assume $v$ is bounded. Let $\gamma>0$ and $\beta\in(0,1)$. For
$n\in\mathbb{N}$, let \textbf{$\boldsymbol{N}=(n,...,n)\in\mathbb{N}^{d}$}
and $\boldsymbol{M}=\boldsymbol{L}=(\gamma n^{\beta},...,\gamma n^{\beta}).$
Define $\tilde{v}_{\boldsymbol{k}}$ as in Equation (\ref{eq:vk})
or Equation (\ref{eq:vk_tilde}). Assume for some $p>\frac{d}{2}$
that
\begin{equation}
|\widehat{f}(\boldsymbol{u})|\leq O\left(|\boldsymbol{u}|_{\infty}^{-p}\right),\quad|\boldsymbol{u}|_{\infty}\to\infty.\label{eq:f_hat_p}
\end{equation}
(i) Define $v_{\boldsymbol{k}}$ as in Equation (\ref{eq:vk}). Then
it holds that
\[
\left|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}v_{\boldsymbol{k}}\right|\leq O\bigg(n^{-(1-\beta)p+\frac{d}{2}}\bigg),\quad n\to\infty.
\]
(ii) Assume that $v\in\mathcal{L}^{1}\cap\mathcal{L}^{2}$, $v$ satisfies
Inequality (\ref{eq:propBed}) and $v$ decays exponentially. Define
$\tilde{v}_{\boldsymbol{k}}$ as in Equation (\ref{eq:vk_tilde}).
Then it holds that
\[
\left|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}\right|\leq O\bigg(n^{-(1-\beta)(p-\frac{d}{2})}\bigg),\quad n\to\infty.
\]
\end{thm}

\begin{proof}
Let $A_{1}(\boldsymbol{L})$, $A_{2}(\boldsymbol{L},\boldsymbol{N})$,
$D_{1}(\boldsymbol{M})$ and $D_{2}(\boldsymbol{N},\boldsymbol{L},\boldsymbol{M})$
be as in the proof of Corollary \ref{cor:generalCase}. Since $v_{\boldsymbol{k}}=\langle v1_{[-\boldsymbol{M},\boldsymbol{M}]},e_{\boldsymbol{k}}^{\boldsymbol{L}}\rangle$
and similarly to the proof of Corollary \ref{cor:generalCase} we
have that
\begin{align}
 & \bigg|\int_{\mathbb{R}^{d}}v(\boldsymbol{x})f(\boldsymbol{x})d\boldsymbol{x}-\sideset{}{'}\sum_{\boldsymbol{\boldsymbol{0}}\leq\boldsymbol{k}\leq\boldsymbol{N}}c_{\boldsymbol{k}}\tilde{v}_{\boldsymbol{k}}\bigg|\nonumber \\
\leq & D_{1}(\boldsymbol{M})+\|v1_{[-\boldsymbol{M},\boldsymbol{M}]}\|_{2}\,\bigg(A_{1}(\boldsymbol{L})+A_{2}(\boldsymbol{L},\boldsymbol{N})+\sqrt{B_{f}(\boldsymbol{L})}\bigg)+D_{2}(\boldsymbol{N},\boldsymbol{L},\boldsymbol{M}).\label{dsd}
\end{align}
We will analyze the order of convergence of each term at the right-hand
side of Inequality (\ref{dsd}): Since $v$ is bounded and $f$ decays
exponentially, $D_{1}(\boldsymbol{M})$, $A_{1}(\boldsymbol{L})$
and $\sqrt{B_{f}(\boldsymbol{L})}$ decay exponentially, i.e., can
be bounded by $O\big(\exp(-C_{3}n^{\beta})\big),\quad n\to\infty$
for some $C_{3}$, see proof of Theorem \ref{thm:(Multidimensional-COS-method}.
By Inequality (\ref{eq:eta}), the term $\sideset{}{'}\sum|c_{\boldsymbol{k}}|^{2}$
is bounded. If $w_{\boldsymbol{k}}=v_{\boldsymbol{k}}$ then $D_{2}(\boldsymbol{N},\boldsymbol{L},\boldsymbol{M})=0$.
If $w_{\boldsymbol{k}}=\tilde{v}_{\boldsymbol{k}}$ then $D_{2}(\boldsymbol{N},\boldsymbol{L},\boldsymbol{M})$
decays exponentially, see proof of Corollary \ref{cor:vk_tilde_M_N}.
Last, we treat $A_{2}(\boldsymbol{L},\boldsymbol{N})$. Let $j\in\{1,...,d\}$.
Let $n$ be large enough. Let $\boldsymbol{k}\in\mathbb{N}_{0}^{d}$
such that $k_{j}>n$. By Equation (\ref{eq:ck}) and Inequality (\ref{eq:f_hat_p}),
there is a constant $a_{1}>0$ so that
\begin{align*}
|c_{\boldsymbol{k}}|^{2}\overset{(\ref{eq:ck})}{\leq}\bigg(\frac{1}{2^{d-1}\prod_{h=1}^{d}L_{h}}\sum_{\boldsymbol{s}=(1,\pm1,...,\pm1)\in\mathbb{R}^{d}}\bigg|\widehat{f}\left(\frac{\pi}{2}\frac{\boldsymbol{sk}}{\boldsymbol{L}}\right)\bigg|\bigg)^{2}\overset{(\text{\ref{eq:f_hat_p}})}{\leq} & a_{1}n^{2\beta(p-d)}|\boldsymbol{k}|_{\infty}^{-2p}.
\end{align*}
By mathematical induction over $d$ and the applying the integral
test of convergence, one can show that
\begin{align}
\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{j}>n}|\boldsymbol{k}|_{\infty}^{-2p} & \leq\frac{2^{d-1}}{(2p-d)n^{2p-d}}.\label{eq:induction}
\end{align}
It follows by Inequality (\ref{eq:induction}) for some $a_{2}>0$
that
\begin{align}
\prod_{h=1}^{d}L_{h}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{j}>n}|c_{\boldsymbol{k}}|^{2}\leq & a_{2}n^{-(1-\beta)(2p-d)}.\label{eq:k_j_N_j}
\end{align}
Let \textbf{$G(\boldsymbol{L})$ }be defined as in Equality (\ref{eq:eta_l}).
By Equality (\ref{eq:ekel}), the Cauchy-Schwarz (CS) inequality and
Inequality (\ref{eq:bound_ck2}), we obtain
\begin{align*}
A_{2}(\boldsymbol{L},\boldsymbol{N})^{2}\overset{(\ref{eq:ekel})}{\leq} & \prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{k_{1}>N_{1}\text{ or}\dots\text{or }k_{d}>N_{d}}|a_{\boldsymbol{k}}+c_{\boldsymbol{k}}-c_{\boldsymbol{k}}|^{2}\\
\overset{(\text{CS})}{\leq} & \prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{k_{1}>N_{1}\text{ or}\dots\text{or }k_{d}>N_{d}}|c_{\boldsymbol{k}}|^{2}+\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}|a_{\boldsymbol{k}}-c_{\boldsymbol{k}}|^{2}\\
 & +2\sqrt{\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}|c_{\boldsymbol{k}}|^{2}\prod_{h=1}^{d}L_{h}\sideset{}{'}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d}}|a_{\boldsymbol{k}}-c_{\boldsymbol{k}}|^{2}}\\
\overset{(\ref{eq:bound_ck2})}{\leq} & \sum_{j=1}^{d}\bigg(\prod_{h=1}^{d}L_{h}\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{d},k_{j}>n}|c_{\boldsymbol{k}}|^{2}\bigg)+B_{f}(\boldsymbol{L})+2\sqrt{\left(\int_{\mathbb{R}^{d}}|f(\boldsymbol{x})|^{2}d\boldsymbol{x}+G(\boldsymbol{L})\right)B_{f}(\boldsymbol{L})}\\
\overset{(\ref{eq:k_j_N_j})}{\leq} & O\left(n^{-(1-\beta)(2p-d)}\right),\quad n\to\infty,
\end{align*}
since $B_{f}(\boldsymbol{L})$ and $G(\boldsymbol{L})$, converge
exponentially to zero. Since $v$ is bounded, we have that $\|v1_{[-\boldsymbol{M},\boldsymbol{M}]}\|_{2}\leq O\left(n^{\frac{d\beta}{2}}\right)$,
$n\to\infty$. Noting that $\frac{-(1-\beta)(2p-d)+d\beta}{2}=-(1-\beta)p+\frac{d}{2}$,
shows (i). It holds $\|v1_{[-\boldsymbol{M},\boldsymbol{M}]}\|_{2}\leq\|v\|_{2}$
if $v\in\mathcal{L}_{2}$, which implies (ii).
\end{proof}

\section{\label{sec:Characteristic-functions}Characteristic functions}

In this section, in Examples \ref{exa:(Normal-distribution).-Let}
and \ref{exa:(Variance-Gamma-distribution).}, we recall the normal
and the Variance Gamma distribution from the literature. Remarks \ref{rem:In-a-financial},
\ref{rem:BS} and \ref{rem:VG} provide a financial context.
\begin{example}
\label{exa:(Normal-distribution).-Let}(Normal distribution). Let
$\boldsymbol{Z}$ be a $d$-dimensional, standard normal random variable.
Let $\boldsymbol{\eta}\in\mathbb{R}^{d}$ and let $\Sigma\in\mathbb{R}^{d}\times\mathbb{R}^{d}$
be a symmetric positive definite matrix. Consider $\boldsymbol{X}=\boldsymbol{\eta}+\Sigma\boldsymbol{Z}$
with characteristic function $\widehat{g}(\boldsymbol{u})=\exp\left(i\boldsymbol{\eta}\cdot\boldsymbol{u}-\frac{1}{2}\boldsymbol{u}\cdot\Sigma\boldsymbol{u}\right)$,
which can be extended to $\mathbb{C}^{d}$, i.e., $\widehat{g}(\boldsymbol{u}-i\boldsymbol{\alpha})$
exists for all $\boldsymbol{\alpha}\in\mathbb{R}^{d}$. By Proposition
\ref{prop:centr}, we set $\lambda=\exp\left(-\boldsymbol{\eta}\cdot\boldsymbol{\alpha}-\frac{1}{2}\boldsymbol{\alpha}\cdot\Sigma\boldsymbol{\alpha}\right)$
and $\boldsymbol{\mu}=\boldsymbol{\eta}+\Sigma\boldsymbol{\alpha}$.
The characteristic function of the damped density $f$, defined in
Equation (\ref{eq:dampedf}), is given by $\widehat{f}(\boldsymbol{u})=\exp\left(-\frac{1}{2}\boldsymbol{u}\cdot\Sigma\boldsymbol{u}\right)$.
A straightforward computation shows that
\begin{align*}
(2\pi)^{-d}\int_{\mathbb{R}^{d}}|\widehat{f}(\boldsymbol{u})|^{2}d\boldsymbol{u} & =\frac{2^{-d}}{\sqrt{\pi{}^{d}\det(\Sigma)}}.
\end{align*}
\end{example}

\begin{example}
\label{exa:(Variance-Gamma-distribution).}(Variance Gamma distribution).
Let $\boldsymbol{Z}$ be a $d$-dimensional, standard normal random
variable. Let $G$ be a Gamma distributed random variable, independent
of $\boldsymbol{Z}$, with shape $a>0$ and scale $s>0$. Let $\boldsymbol{\eta},\boldsymbol{\theta}\in\mathbb{R}^{d}$
and $\boldsymbol{\sigma}\in\mathbb{R}_{+}^{d}$. Consider $\boldsymbol{X}=\boldsymbol{\eta}+\boldsymbol{\theta}G+\sqrt{G}\boldsymbol{\sigma}\boldsymbol{Z}$.
The distribution of $\boldsymbol{X}$ is denoted by $\text{VG}(a,s,\boldsymbol{\eta},\boldsymbol{\theta},\boldsymbol{\sigma})$.
Define $\Sigma\in\mathbb{R}^{d}\times\mathbb{R}^{d}$ such that $\Sigma_{ii}=\sigma_{i}^{2}$
and $\Sigma_{ij}=0$ for $i\neq j$. Then $\boldsymbol{X}$ has characteristic
function

\[
\widehat{g}(\boldsymbol{u})=\exp\left(i\boldsymbol{\eta}\cdot\boldsymbol{u}\right)\big(1-is\boldsymbol{\theta}\cdot\boldsymbol{u}+\frac{1}{2}s\boldsymbol{u}\cdot\Sigma\boldsymbol{u}\big)^{-a},
\]
see \citet{luciano2006multivariate}. The (extended) Fourier transform
$\widehat{g}(\boldsymbol{u}-i\boldsymbol{\alpha})$ exists for all
$\boldsymbol{\alpha}\in\mathbb{R}^{d}$ with $\zeta:=1-s\boldsymbol{\theta}\cdot\boldsymbol{\alpha}-\frac{1}{2}s\boldsymbol{\alpha}\cdot\Sigma\boldsymbol{\alpha}>0$,
see \citet{bayer2022optimal}. By Proposition \ref{prop:centr}, we
set $\lambda=\exp\left(-\boldsymbol{\eta}\cdot\boldsymbol{\alpha}\right)\zeta^{a}$
and $\boldsymbol{\mu}=\boldsymbol{\eta}+as\zeta^{-1}(\boldsymbol{\theta}+\Sigma\boldsymbol{\alpha})$.
The characteristic function of the damped density $f$, defined in
Equation (\ref{eq:dampedf}), is given by 
\begin{align*}
\widehat{f}(\boldsymbol{u})= & \exp\left(-i\frac{as}{\zeta}\big(\boldsymbol{\theta}+\Sigma\boldsymbol{\alpha}\big)\cdot\boldsymbol{u}\right)\left(1-i\frac{s}{\zeta}\big(\boldsymbol{\theta}+\Sigma\boldsymbol{\alpha}\big)\cdot\boldsymbol{u}+\frac{1}{2}\frac{s}{\zeta}\boldsymbol{u}\cdot\Sigma\boldsymbol{u}\right)^{-a}.
\end{align*}
Apply the Courant--Fischer--Weyl min-max principle to
see that $|\widehat{f}(\boldsymbol{u})|\leq O\left(|\boldsymbol{u}|_{\infty}^{-2a}\right)$
for $|\boldsymbol{u}|_{\infty}\to\infty$.
\end{example}

\begin{rem}
\label{rem:In-a-financial}In a financial context, we model a stock
price over time by a $d-$dimensional positive semimartingale $(\boldsymbol{S}(t))_{t\geq0}$
on a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_{t})_{t\geq0})$.
The filtration $(\mathcal{F}_{t})_{t\geq0}$ satisfies the usual conditions
with $\mathcal{F}_{0}=\{\Omega,\emptyset\}$. We define the \emph{log-returns}
$\boldsymbol{X}(t):=\log(\boldsymbol{S}(t))$, $t\geq0$. There is
a bank account paying continuous compound interest $r\in\mathbb{R}$.
By $Q$ we denote a risk-neutral measure. Expectations are taken under
$Q$. There is a European rainbow option $w:\mathbb{R}^{d}\to\mathbb{R}$
with maturity $T>0$ and payoff $w\big(\boldsymbol{X}(T)\big)$ at
time $T$. We denote by $g$ the density of $\log(\boldsymbol{S}(T))$.
The time-0 price of the European option with payoff $w$ is then given
by 
\begin{align}
e^{-rT}E[w(\boldsymbol{X}(T))] & =e^{-rT}\int_{\mathbb{R}}w(\boldsymbol{x})g(\boldsymbol{x})d\boldsymbol{x}.\label{eq:price}
\end{align}
The integral at the right-hand side of Equation (\ref{eq:price})
can be approximated by the (damped) COS method.
\end{rem}

\begin{rem}
\label{rem:BS}Let $\Sigma\in\mathbb{R}^{d}\times\mathbb{R}^{d}$
be a symmetric positive definite matrix. For the Black-Scholes (BS)
model, see \cite{karatzas1998methods}, the log-returns $\boldsymbol{X}(T)$
are normally distributed with location $\boldsymbol{\eta}:=\log(\boldsymbol{S}(0))+(\boldsymbol{r}-\frac{1}{2}\text{diag}(\Sigma))T$
and covariance matrix $T\Sigma$, where $\boldsymbol{r}=(r,...,r)\in\mathbb{R}^{d}$
and $\text{diag}(\Sigma)\in\mathbb{R}^{d}$ denotes the diagonal of
$\Sigma$.
\end{rem}

\begin{rem}
\label{rem:VG}Let $\nu>0$, $\boldsymbol{\sigma}\in\mathbb{R}_{+}^{d}$
and $\boldsymbol{\theta}\in\mathbb{R}^{d}$. In the multivariate Variance
Gamma (VG) model, see \citet{luciano2006multivariate}, the log-returns
$\boldsymbol{X}(T)$ follow a $\text{VG}(\frac{T}{\nu},\nu,\boldsymbol{\eta},\boldsymbol{\theta},\boldsymbol{\sigma})$
distribution, where
\[
\eta_{h}:=\log(S_{h}(0))+\big(r+\frac{1}{\nu}\log\big(1-\frac{1}{2}\sigma_{h}^{2}\nu-\theta_{h}\nu\big)\big)T,\quad h=1,\dots,d.
\]
\end{rem}


\section{\label{sec:Functions-of-interest}Functions of interest}
\begin{example}
\label{exa:CDF}(CDF). Let $w(\boldsymbol{x})=1_{(-\infty,\boldsymbol{y}]}(\boldsymbol{x})$,
$\boldsymbol{x}\in\mathbb{R}^{d}$ for some $\boldsymbol{y}\in\mathbb{R}^{d}$.
The integral in (\ref{eq:int}) is interpreted as the CDF of the density
$g$ evaluated at $\boldsymbol{y}$. The coefficients $v_{\boldsymbol{k}}$,
defined in Equation (\ref{eq:vk}), can be obtained in closed-form
if $\boldsymbol{\alpha}=\boldsymbol{0}$. Let $\boldsymbol{M},\boldsymbol{L}\in\mathbb{R}_{+}^{d}$
as in Section \ref{sec:Damped-COS-method}. Let $\gamma_{h}:=\min(y_{h}-\mu_{h},M_{h})$,
$h=1,..,d$. It holds for $\boldsymbol{k}\in\mathbb{N}_{0}^{d}$ that
$v_{\boldsymbol{k}}=0$ if $\gamma_{h}<-M_{h}$ for any $h$ and otherwise
\begin{align*}
v_{\boldsymbol{k}} & =\lambda^{-1}\prod_{\underset{k_{h}=0}{h=1}}^{d}\{\gamma_{h}+M_{h}\}\prod_{\underset{k_{h}>0}{h=1}}^{d}\bigg\{\frac{2L_{h}}{\pi k_{h}}\bigg(\sin\big(k_{h}\pi\frac{\gamma_{h}+L_{h}}{2L_{h}}\big)-\sin\big(k_{h}\pi\frac{-M_{h}+L_{h}}{2L_{h}}\big)\bigg)\bigg\}.
\end{align*}
\end{example}

Next, we assume for some $\boldsymbol{\alpha}\in\mathbb{R}^{d}$ that
the map $\boldsymbol{x}\mapsto w(\boldsymbol{x})e^{-\boldsymbol{\alpha}\cdot\boldsymbol{x}}$
is integrable. The Fourier-transform of $w$ then exists at all points
$\boldsymbol{u}+i\boldsymbol{\alpha}\in\mathbb{C}^{d}$, $\boldsymbol{u}\in\mathbb{R}^{d}$.
Let $\lambda>0$ and $\boldsymbol{\mu}\in\mathbb{R}^{d}$. Let $v$
be as in Equation (\ref{eq:dampedv}). The Fourier-transform of $v$
is given by $\widehat{v}(\boldsymbol{u})=\lambda^{-1}e^{-i\boldsymbol{u}\cdot\boldsymbol{\mu}}\widehat{w}(\boldsymbol{u}+i\boldsymbol{\alpha})$.
Hence, a closed-form expression for $\widehat{w}$ is sufficient to
obtain a closed-form expression for $\widehat{v}$. We can then directly
obtain $\tilde{v}_{\boldsymbol{k}}$, defined in Equation (\ref{eq:vk_tilde}).
For many functions of interest, $\widehat{w}$ is known in closed-form
in $d$ dimensions. We provide some examples from finance, where integral
in (\ref{eq:int}) is then interpreted as a price: Digital cash-or-nothing
put options and arithmetic basket options are discussed in Examples
\ref{exa:CDF} and \ref{exa:w_hat_basket}, respectively. For put
and call options on the maximum or minimum of $d$ assets, see \cite{eberlein2010analysis};
for spread options, see \cite{hurd2010fourier}. Note that a digital
cash-or-nothing put option with strike $\boldsymbol{K}$ is equal
to the CDF of $g$ evaluated at $\log(\boldsymbol{K})$. We include
Example \ref{exa:(Digital-cash-or-nothing-put} to test the damped
COS method for $\boldsymbol{\alpha}\neq\boldsymbol{0}$.
\begin{example}
\label{exa:(Digital-cash-or-nothing-put}(Digital cash-or-nothing
put option). The payoff function of a cash-or-nothing put option is
defined by $w(\boldsymbol{x})=1_{[0,\boldsymbol{K}]}(e^{\boldsymbol{x}})$,
$\boldsymbol{x}\in\mathbb{R}^{d}$ for some strikes $\boldsymbol{K}\in\mathbb{R}_{+}^{d}$.
The option pays $1\$$ at maturity if $\boldsymbol{S}(T)\leq\boldsymbol{K}$
and nothing otherwise. The integral at the right-hand side of Equation
(\ref{eq:price}) is given by $G\big(\log(\boldsymbol{K})\big)$,
where $G$ is the CDF of $g$. A simple calculation shows that the
Fourier-transform of $w$ exists for $\boldsymbol{z}\in\mathbb{C}^{d}$
such that $\Im\{z_{h}\}<0$ and is given by $\widehat{w}(\boldsymbol{z})=\prod_{h=1}^{d}\frac{K_{h}^{iz_{h}}}{iz_{h}}.$
For $\lambda>0$ and $\boldsymbol{\mu}\in\mathbb{R}^{d}$, let $v$
be as in Equation (\ref{eq:dampedv}). It holds for $\boldsymbol{\alpha}<\boldsymbol{0}$
that $\left\Vert v\right\Vert _{\infty}\leq\lambda^{-1}e^{-\boldsymbol{\alpha}\cdot\log(\boldsymbol{K})}$
and
\begin{align*}
\left\Vert v\right\Vert _{2}^{2} & =\lambda^{-2}\prod_{h=1}^{d}\frac{\exp\left(-2\alpha_{h}\big(\log(K_{h})\big)\right)}{-2\alpha_{h}}.
\end{align*}
 
\end{example}

\begin{example}
\label{exa:w_hat_basket}(Arithmetic basket put option). The payoff
function of an arithmetic  basket put option is defined by $w(\boldsymbol{x})=\max(K-\sum_{h=1}^{d}e^{x_{h}},0)$,
$\boldsymbol{x}\in\mathbb{R}^{d}$ for some strike $K>0$. The Fourier-transform
of $w$ exists for $\boldsymbol{z}\in\mathbb{C}^{d}$ such that $\Im\{z_{h}\}<0$
and is given by
\begin{align}
\widehat{w}(\boldsymbol{z}) & =\int_{\mathbb{R}^{d}}e^{i\boldsymbol{z}\cdot\boldsymbol{x}}w(\boldsymbol{x})d\boldsymbol{x}=K^{(1+i\sum_{h=1}^{d}z_{h})}\frac{\prod_{h=1}^{d}\Gamma(iz_{h})}{\Gamma\left(i\sum_{h=1}^{d}z_{h}+2\right)}.\label{eq:w_fourier_basket}
\end{align}
Equation (\ref{eq:w_fourier_basket}) follows by an elementary substitution\footnote{We thank Friedrich Hubalek from TU Wien for pointing this out to us.}
from \citet[Eq. (5.14.1)]{olver2010nist} and is also mentioned in
a similar form in \cite{hubalek2003variance}. If $\boldsymbol{\alpha}<\boldsymbol{0}$
it holds using \citet[Eq. (5.14.1)]{olver2010nist} and
\[
\max\big(K-\sum_{h=1}^{d}e^{x_{h}+\mu_{h}},0\big)\leq\begin{cases}
0 & ,\sum_{h=1}^{d}e^{x_{h}+\mu_{h}}\geq K\\
K & ,\text{otherwise}.
\end{cases}
\]
that
\[
\left\Vert v\right\Vert _{2}^{2}\leq\frac{K^{2-2\sum_{h=1}^{d}\alpha_{h}}}{\lambda^{2}}\frac{\prod_{h=1}^{d}\Gamma\big(-2\alpha_{h}\big)}{\Gamma\big(1+\sum_{h=1}^{d}(-2\alpha_{h})\big)}.
\]
We further have, using $v(\boldsymbol{x})=0$ for $\boldsymbol{x}\geq(\log(K),...,\log(K))-\boldsymbol{\mu}$,
that $\left\Vert v\right\Vert _{\infty}\leq\lambda^{-1}K^{1-\sum_{h=1}^{d}\alpha_{h}}.$
\end{example}


\section{\label{sec:Numerical}Numerical experiments}

We provide several numerical experiments to solve the integral in
(\ref{eq:int}) by the COS method. Reference values are obtained by
\citet[Theorem 3.2]{eberlein2010analysis} with damping factor $\boldsymbol{R}=(-4,\dots,-4)$,
who express the integral in (\ref{eq:int}) by another integral involving
the Fourier-transforms $\widehat{g}$ and $\widehat{w}$. To obtain
reference values and to solve the integral in Corollary \ref{cor:NrTerms},
we use the command \emph{cubintegrate }with the method \emph{cuhre}
from the R-package \emph{cubature} with relative tolerance $10^{-11}$.
We confirm all reference values using the COS method with $\boldsymbol{N}=(2000,\dots,2000)$
and a truncation range obtained from Equation (\ref{eq:m-1-1}) with
$\varepsilon=10^{-10}$ using $n=8$ moments. For the normal distribution,
reference values are also given in closed-form in the uncorrelated
case for a CDF and the digital cash-or-nothing put option. All experiments
are performed on a laptop with Intel i7-11850H processor and 32 GB
RAM. The COS method and Monte Carlo simulations are implemented in
C++ using for-loops without parallelization. The memory requirements
are minimal.

We first investigate the influence of the damping factor $\boldsymbol{\alpha}$
on the accuracy of the COS method to obtain the price of a cash-or-nothing
put option with strike $\boldsymbol{K}\in\mathbb{R}_{+}^{d}$ in the
BS model, which is just the CDF of a normal distribution evaluated
at $\log(\boldsymbol{K})$, hence reference values can be obtained
in close-form. Figure \ref{fig:alpha} shows the behavior of the COS
method for different damping factors in dimensions $d\in\{2,3,4\}$.
If $\boldsymbol{\alpha}$ is too close to zero, almost no damping
takes place and the difference between $v_{\boldsymbol{k}}$ and $\tilde{v}_{\boldsymbol{k}}$
is large, which implies a relative high error of the COS method. If
$|\boldsymbol{\alpha}|$ is too big, $\left\Vert v\right\Vert _{\infty}$
and $\left\Vert v\right\Vert _{2}$ become very large and the truncation
error increases. However, we observe in the example that a wide range
of damping factor $\boldsymbol{\alpha}$ work well in various dimensions.
Further, fixing the number of terms $\boldsymbol{N}$ and the truncation
range $\boldsymbol{L}$, the accuracy of the classical COS method
with $\boldsymbol{\alpha}=\boldsymbol{0}$ and the damped COS method
with $\boldsymbol{\alpha}\neq\boldsymbol{0}$ is very similar for
some damping factors. 

We illustrate the order of convergence of the COS method for an arithmetic
basket put option in the VG model. We compare three different maturities.
In Figure \ref{fig:alpha} we can see that the theoretical bound from
Theorem \ref{thm:order of convergence} for the order of convergence
is sharp and close the empirical order of convergence.

% Figure environment removed


\subsection{Other methods}

We compare the COS method with a Monte Carlo (MC) simulation to obtain
the price of a cash-or-nothing put option in the BS model, which is
equal to the CDF of the normal distribution evaluated at $\log(\boldsymbol{K})$,
hence reference values can be obtained in close-form. The computational
complexity of a MC simulation with $U\in\mathbb{N}$ runs scales like
$O(Ud)$. We estimate $U$ by the central limit theorem and a statistical
error of $0.99$. The COS method consist of $d$-nested sums. According
to Equation (\ref{eq:ck}), the computational complexity of the COS
method scales like $O\left(\prod_{h=1}^{d}\{N_{h}\}2^{d-1}\right)$.
A MC simulation converges relative slowly but hardly depends on the
dimension. On the other hand, the complexity of the COS method grows
exponentially in the dimension, however, the COS method also converges
exponentially for the BS model. The choice between MC and the COS
method depends both on the dimension and the error tolerance $\varepsilon$:
the higher $d$ the faster MC compared to the COS method but the smaller
$\varepsilon$, the better performs the COS method. In Table \ref{tab:Monte-Carlo-vs-COS_empirical-closed-1}
and Figure \ref{fig:Computational-cost-of}, we observe that the COS
method is faster than MC for $d\leq3$ and $\varepsilon\leq10^{-2}$.
For $d=4$ ($d=5)$, the COS method outperforms MC for $\varepsilon\leq10^{-3}$
($\varepsilon\leq10^{-5}$), otherwise a MC simulation is faster.
If $\varepsilon=10^{-9}$ and $d=4$, the COS method needs $220$
number of terms in each dimensions to stay below the error tolerance
and the CPU time is about one hour. We estimate that a MC simulation
would need more than $20,000$ years.

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|r|r|r|c|}
\hline 
{\small{}$d$} & {\small{}$N$} & {\small{}$L$} & $U$ & {\small{}$\tau_{\text{COS}}$} & {\small{}$\tau_{\text{MC}}$} & {\small{}Reference value}\tabularnewline
\hline 
\hline 
{\small{}1} & {\small{}30} & {\small{}2.0} & 16481995016 & {\small{}8.9e-6} & {\small{}5.1e+3} & {\small{}0.539827}\tabularnewline
\hline 
{\small{}2} & {\small{}30} & {\small{}2.4} & 13700525367 & {\small{}3.7e-4} & {\small{}1.6e+4} & {\small{}0.291414}\tabularnewline
\hline 
{\small{}3} & {\small{}40} & {\small{}3.0} & 8795611829 & {\small{}4.9e-2} & {\small{}1.2e+4} & {\small{}0.157313}\tabularnewline
\hline 
{\small{}4} & {\small{}50} & {\small{}3.6} & 5156004587 & {\small{}1.1e+1} & {\small{}8.1e+3} & {\small{}0.084922}\tabularnewline
\hline 
{\small{}5} & {\small{}50} & {\small{}4.2} & 2902219256 & {\small{}1.4e+3} & {\small{}7.8e+3} & {\small{}0.045843}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:Monte-Carlo-vs-COS_empirical-closed-1}CPU time of the
COS method $(\tau_{\text{COS}})$ and CPU time of a MC simulation
$(\tau_{\text{MC}})$ for the BS model to price a cash-or-nothing
put option. We set $\varepsilon=10^{-5}$, $\boldsymbol{\alpha}=(-7,\dots,-7)$,
$\Sigma_{ii}=\sigma^{2}$, $\Sigma_{ij}=0$, $i\protect\neq j$, where
$\sigma=0.2$, and $\boldsymbol{S}(0)=\boldsymbol{K}=(100,\dots,100)$.
We set $N_{1}=\dots=N_{d}=N$. We obtain the truncation range $\boldsymbol{L}=(L,...,L)$
from Inequality (\ref{eq:m-1-1}) using $n=8$ moments. The reference
value can be obtained in closed-form. CPU time is measured in seconds.}
\end{table}
% Figure environment removed


\subsection{On the choice of $\boldsymbol{N}$}

In this section, we consider an arithmetic basket put option in the
BS and the VG models. The methodology can also be applied to other
rainbow options or a CDF. We compare different strategies to choose
the number of terms $\boldsymbol{N}$. For $d=1$ we also consider
the bound for $N$ from \citet{junike2023handle}, which can be obtained
as follows:

If the (damped) density $f$ is $J+1$ times differentiable with bounded
derivatives, the number of terms can be chosen by 

\begin{equation}
N\geq\left(\frac{2^{s+\frac{5}{2}}\left\Vert f^{(s+1)}\right\Vert _{\infty}L^{s+2}}{s\pi^{s+1}}\frac{12Ke^{-rT}}{\varepsilon}\right)^{\frac{1}{s}},\label{eq:N_Junike2023}
\end{equation}
where $s\in\{1,...,J\}$. The term $\left\Vert f^{(s+1)}\right\Vert _{\infty}$
can be bounded by
\begin{equation}
\|f^{(s+1)}\|_{\infty}\leq\frac{1}{2\pi}\int_{\mathbb{R}}|u|^{s+1}|\varphi(u)|du.\label{eq:boundf}
\end{equation}

For the BS model, we choose $s=40$. According to \citet{kuchler2008shapes},
the density of the VG model is $J+1$ times continuously differentiable
if $J$ is equal to the largest natural number which is less than
$\frac{2T}{\nu}-2$. For the VG model, we use $s=J$.

In Table \ref{tab:Nopt} one can see that Corollary \ref{cor:NrTerms}
provides a sharper bound for $\boldsymbol{N}$ than \citet{junike2023handle}.
This is particularly noticeable for the VG model, which is less smooth
than the BS model. However, the formula in \citet{junike2023handle}
is more stable, compare with Remark \ref{rem:lessStable}. The number
of terms obtained by Corollary \ref{cor:NrTerms} is at most three
times larger than the minimal number of terms necessary to stay below
the error tolerance.

\begin{table}[H]
\begin{centering}
\begin{tabular}{|l|c|>{\centering}p{0.7cm}|>{\centering}p{0.5cm}|>{\centering}p{2.2cm}|c|c|>{\centering}p{1.5cm}|}
\hline 
 & {\small{}$d$} & {\small{}Model} & {\small{}$N$} & {\small{}CPU time COS }\\
{\small{}(num. int.) } & {\small{}Parameters} & {\small{}$\boldsymbol{L}$} & {\small{}Ref. value}\tabularnewline
\hline 
\hline 
{\small{}Minimal $N$} & {\small{}1} & {\small{}BS} & {\small{}25} & {\small{}0.02} & {\small{}$\sigma=0.2$} & {\small{}$1.8$} & {\small{}7.965567}\tabularnewline
\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
{\small{}Cor. \ref{cor:NrTerms}} & {\small{}1} & {\small{}BS} & {\small{}28} & {\small{}0.03} &  &  & \tabularnewline
\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
{\small{}\citet{junike2023handle}} & {\small{}1} & {\small{}BS} & {\small{}34} & {\small{}0.03} &  &  & \tabularnewline
\hline 
{\small{}Minimal $N$} & {\small{}1} & {\small{}VG} & {\small{}95} & {\small{}0.12} & {\small{}$\sigma=0.1213$, } & {\small{}$5.5$} & {\small{}5.195700}\tabularnewline
\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
{\small{}Cor. \ref{cor:NrTerms}} & {\small{}1} & {\small{}VG} & {\small{}264} & {\small{}0.28 (1.95)} & {\small{}$\theta=-0.1436$,} &  & \tabularnewline
\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
{\small{}\citet{junike2023handle}} & {\small{}1} & {\small{}VG} & {\small{}729} & {\small{}0.96 (12.23)} & {\small{}$\nu=0.1686$} &  & \tabularnewline
\hline 
{\small{}Minimal $\boldsymbol{N}$} & {\small{}2} & {\small{}BS} & {\small{}65} & {\small{}5.35} & {\small{}$\Sigma_{11}=0.2^{2},$ $\Sigma_{22}=0.4^{2}$} & {\small{}$(5.7,11.5)$ } & {\small{}21.010354}\tabularnewline
\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
{\small{}Cor. \ref{cor:NrTerms}} & {\small{}2} & {\small{}BS} & {\small{}116} & {\small{}17.02} & {\small{}$\Sigma_{12}=\Sigma_{21}=\frac{1}{2}\sqrt{\Sigma_{11}\Sigma_{22}}$} &  & \tabularnewline
\hline 
{\small{}Minimal $\boldsymbol{N}$} & {\small{}2} & {\small{}VG} & {\small{}55} & {\small{}8.55} & {\small{}$\boldsymbol{\sigma}=(0.2,0.25)$, $\nu=0.1$} & {\small{}$(5.8,7.5)$ } & {\small{}12.670179}\tabularnewline
\cline{1-5} \cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
{\small{}Cor. \ref{cor:NrTerms}} & {\small{}2} & {\small{}VG} & {\small{}154} & {\small{}68.20 (1092.2)} & {\small{}$\boldsymbol{\theta}=(-0.03,-0.05)$,} &  & \tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:Nopt}Comparison of different strategies to choose $\boldsymbol{N}$
for an arithmetic  basket put option in the BS model and the VG model.
We use the error tolerance $\varepsilon=10^{-3}$. We set $\boldsymbol{N}=(N,\dots,N)$,
$\boldsymbol{S}(0)=(100,...,100)$, $K=100d$, $\boldsymbol{\alpha}=(-4,\dots,-4)$,
$T=1$ and $r=0$. We obtain the truncation range $\boldsymbol{L}=(L,...,L)$
from Inequality (\ref{eq:m-1-1}) using $n=8$ moments. Reference
values are obtained by \citet{eberlein2010analysis}. We average over
ten runs to obtain the CPU time, which is measured in milliseconds.}
\end{table}


\section{\label{sec:Conclusions}Conclusions}

In this article we introduced and discussed the damped COS method,
which is a numerical tool to solve certain multidimensional integrals
numerically, e.g., to obtain a CDF from a characteristic function
or to price rainbow options in a financial context. The (damped) COS
method requires several parameters: In particular, one has to specify
a truncation range $\boldsymbol{L}$ for the density $f$, a truncation
range $\boldsymbol{M}$ for the integral and the number of terms $\boldsymbol{N}$
of cosine functions to approximate the truncated density. Corollary
\ref{cor:generalCase} provides sufficient conditions on $\boldsymbol{M}$,
$\boldsymbol{L}$ and $\boldsymbol{N}$ to ensure the convergence
of the COS method within a given error tolerance $\varepsilon>0$.
Theorem \ref{thm:(Multidimensional-COS-method} and Corollary \ref{cor:NrTerms}
provide formulas for the truncation ranges $\boldsymbol{M}$ and $\boldsymbol{L}$
and the number of terms $\boldsymbol{N}$, respectively. Theorem \ref{thm:order of convergence}
provides an upper bound of the order of convergence of the COS method.
Numerical experiments indicate that the bound is sharp. In particular,
the (damped) COS method converges exponentially if the Fourier transform
$\widehat{f}$ decays exponentially.

\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}
