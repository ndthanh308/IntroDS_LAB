\section{Preliminaries}

% \subsection{\textcolor{red}{Formalize the sim-to-real TSC problems}}


This section will formalize the traffic signal control (TSC) problem and its RL solutions and introduce the grounded action transformation (GAT) framework for sim-to-real transfer.

\subsection{Concepts of TSC and  RL Solutions}
    % \paragraph{\textcolor{red}{Traffic Signal Control(TSC) problem define}}

    % \textcolor{blue}{Given a set of traffic demands at an intersection, the objective of TSC details...
    
    % . }
In the TSC problem, each traffic signal controller decides the phase of an intersection, which is a set of pre-defined combinations of traffic movements that do not conflict while passing through the intersection. Given the current condition of this intersection, the traffic signal controller will choose a phase for the next time interval $\Delta t$ to minimize the average queue length on lanes around this intersection. Following existing work~\cite{wei2018intellilight,chen2020toward,wei2019colight}, an agent is assigned to each traffic signal, and the agent will choose actions to decide the phase in the next $\Delta t$. The TSC problem is defined as an MDP which could be characterized by $\mathcal{M}  = \langle \mathcal{S}, \mathcal{A}, P, r, \gamma \rangle$ where the state space $\mathcal{S}$ contains each lane's number of vehicles and the current phase of the intersection in the form of the one-hot code, $s_t \in \mathcal{S}$. Action space (discrete) $\mathcal{A}$ is the phase chosen for the next time interval $\Delta t$. Transition dynamics $P(s_{t+1}| s_t, a_t)$ maps $\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, describing the probability distribution of next state $s_{t+1} \in \mathcal{S}$. Reward $r$ is an immediate scalar return from the environment calculated as $r_t = -\sum_l w_t^l$, where $l$ is the lane belonging to the intersection and $w_t^l$ is the queue length at each lane. And the $\gamma$ is the discount factor. Policy $\pi_{\theta}$ could be represented as the logic of: $\mathcal{S} \rightarrow \mathcal{A}$. An RL approach solves this problem by maximizing the long-term expectation of discounted accumulation reward adjusted $\gamma$. The discounted accumulated reward is $\mathbb{E}_{(s_t, a_t)\sim (\pi_{\theta},\mathcal{M})}[ \sum_{t=0}^T \gamma^{T-t} r_t(s_t, a_t) ]$. Since the action space $\mathcal{A}$ is discrete, we follow the past work using Deep Q-network (DQN)~\cite{wei2018intellilight} to optimize the $\pi_{\theta}$, the above procedure is conducted in simulation environment $E_{sim}$.


    % \item System: The traffic state space consists of the number of vehicles on each lane belonging to the intersection and the current phase of the intersection in the form of the one-hot code. Thus the state at time t could be represented as $s_t \in \mathcal{S}$.
    % \item Set of action space $\mathcal{A}$: In this paper, we define the action as the phase for the next time interval $\Delta t$. The action space $\mathcal{A}$ is discrete since, in practice, the phases are a set of pre-defined combinations of traffic movements. %The action $a_t \in \mathcal{A}$ at time $t$ is discrete, indicating a certain phase from a set of predefined phases $\mathcal{A}$ for the intersection from $t$ to $t+\Delta t$.
    % \item Transition dynamics $P$: Given current state $s_t \in \mathcal{S}$ and action $a_t \in \mathcal{A}$, the transition dynamics $P(s_{t+1}| s_t, a_t)$ maps $\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, describing the probability distribution of next state $s_{t+1} \in \mathcal{S}$. 
    % In this paper, we use transition dynamics and transition probability distribution interchangeably. 
    % \item \label{pre:rewarddefine}Reward $r$: The reward is typically a scalar return from the environment which can be described as $\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The reward we used in the traffic signal control problem is an immediate return from the environment. The reward at time t could be calculated from $r_t = -\sum_l w_t^l$, where $l$ is the lane belonging to the intersection and $w_t^l$ is the queue length at each lane.
    % \item Policy $\pi_{\theta}$ and discount factor $\gamma$: The policy $\pi_\theta$ parameterized by $\theta$ aims to  minimize the expected queue length $r_t$. Specifically, it maps the given state $s_t$ to the action $a_t$ taken at this time step. It could be represented $\pi_{\theta}: \mathcal{S} \rightarrow \mathcal{A}$.


% \subsection{\textcolor{red}{Domain Gap between Simulation and Real-world}}

% Domain gap impedes the generalization of models trained in the source domain to the target domain. In RL, the gap between the simulation (the source domain to improve a policy) and the real world (the target domain where trained policy is implemented) hinders most research transacted from the simulation into the real world concerning the cost and public safety. Here we define the sim-to-real transfer problem as below:
% \begin{itemize}
%     \item Modeling simulation and real-world environment: We first define simulation and real-world environment as $E_{sim}$ and $E_{real}$. We consider the two environments to share the same MDP except for the transition dynamics characterized by the transition probability distribution. The MDP in $E_{sim}$ could be characterized as $\mathcal{M}_{sim}  = \langle \mathcal{S}, \mathcal{A}, P_{\phi}, r, \gamma \rangle$, where the simulation's transition dynamic is controlled by a set of parameters $\phi$.
%     And the MDP in $E_{real}$ could be characterized as $\mathcal{M}_{real}  = \langle \mathcal{S}, \mathcal{A}, P^*, r, \gamma \rangle$, where $P^*$ is the transition dynamics in the real world.
% \end{itemize}
% The goal of sim-to-real transfer is to find the best policy $\pi_{\theta^*}$ such that:
%     \begin{equation}
%         \pi_{\theta^*} = \arg \max_{\pi_{\theta}} \eta_{P^*}(\pi_{\theta}), P^* \neq P_{\phi}
%     \end{equation}
%     where $\eta_P(\pi_{\theta})$ is used to evaluate the policy's performance in the $E_{real}$:
%     \begin{equation}
%         \eta_{P}(\pi_{\theta}) = \mathbb{E}_{(s_t,a_t) \sim (\pi_{\theta}, \mathcal{M}_{real}}) [ \sum^T_{t=0} \gamma^{T-t}r_t(s_t, a_t) ]
%     \end{equation}


     % \paragraph{\textcolor{blue}{Describe why the gap exists with reality examples and refer to our preliminary study results to justify the problem's seriousness. And mathematically model how the gap appears and causes problems.}}  \\
    


     % \paragraph{}

% \subsection{Potential Solutions}
% Domain Randomization, Transfer Learning, Data Augmentation, Multi-Task Learning, etc.

\subsection{Grounded Action Transformation}

% 3. GAT is trying to bridge the gap between transition dynamics, which suits our TSC problem. Explain how grounded action solves the dynamics differences. 

Grounded action transformation (GAT) is a framework originally proposed in robotics to improve robotic learning by using trajectories from the physical world $E_{real}$ to modify $E_{sim}$. Under the GAT framework, MDP in $E_{sim}$ is imperfect and modifiable, and it can be parameterized as a transition dynamic $P_{\phi}(\cdot|s, a)$. Given real-world dataset $\mathcal{D}_{real} =  \{\tau^{1}, \tau^{2}, \dots, \tau^{I}\}$, where $\tau^{i} = (s_0^{i}, a_0^{i}, s_1^{i}, a_1^{i}, \dots, s_{T-1}^{i}, a_{T-1}^{i}, s_T^{i})$ is a trajectory collected by running a policy $\pi_{\theta}$ in $E_{real}$, GAT aims to minimize differences between transition dynamics by finding $\phi^*$:
\vspace{-2mm}
\begin{equation}
\phi^* = \arg \min_{\phi} \sum_{\tau^i \in \mathcal{D}_{real}} \sum_{t=0}^{T-1} d(P^*(s^i_{t+1}|s^i_t, a^i_t), P_{\phi}(s^i_{t+1}|s^i_t, a^i_t))
\end{equation}

% \textcolor{red}{wirte full function $g_{\phi}()$}

\noindent where $d(\cdot)$ is the distance between two dynamics, $P^*$ is the real world transition dynamics, and $P_{\phi}$ is the simulation transition dynamics. 

To find $\phi$ efficiently, GAT takes the agent's state $s_t$ and action $a_t$ predicted by policy $\pi_\theta$ as input and outputs a grounded action $\hat{a}_t$. Specifically, it uses an action transformation function parameterized with $\phi$: 
\vspace{-2mm}
\begin{equation}
\label{eq:gat}
    \hat{a}_t = g_{\phi}(s_t, a_t) = h_{\phi^{-}}(s_t, f_{\phi^{+}}(s_t, a_t))
    \vspace{-2mm}
\end{equation}
 which includes two specific functions: a forward model $f_{\phi^{+}}$, and an inverse model $h_{\phi^{-}}$, as is shown in Fig.~\ref{fig:intro}.   
 
\noindent $\bullet$ \textit{The forward model} $f_{\phi^{+}}$ is trained with the data from $E_{real}$, aiming to predict the next possible state $\hat{s}_{t+1}$ given current state $s_t$ and action $a_t$:
\vspace{-2mm}
\begin{equation}
\label{eq:forward}
     \hat{s}_{t+1} =  f_{\phi^{+}}(s_t, a_t)
     \vspace{-2mm}
\end{equation}
% $s_t, a_t$ are current state and action and $\hat{s}_{t+1}$ is the predicted next state in $\mathcal{M}_{real}$ 

\noindent $\bullet$ \textit{The inverse model} $h_{\phi^{-}}$ is trained with the data from $E_{sim}$, aiming to predict the possible action $\hat{a}_t$ that could lead the current state $s_t$ to the given next state. Specifically, the inverse model in GAT takes $\hat{s}_{t+1}$, the output from the forward model, as its input for the next state: 
\vspace{-2mm}
\begin{equation}
\label{eq:inverse}
     \hat{a}_t =  h_{\phi^{-}}(\hat{s}_{t+1}, s_t)
     \vspace{-2mm}
\end{equation}

%takes the next state $\hat{s}_{t+1}$ calculated from the forward model and the current state $s_t$ in simulation as input, and predicts a grounded action $\hat{a}_t$ under the transition dynamics in $E_{sim}$:

% The output from the inverse model, $\hat{a}_t$, is the grounded action which makes the resulted $s_{t+1}$ under the simulation transition dynamics $P_{\phi}(s_{t+1}|s_t, \hat{a}_t)$ close to the predicted next state $\hat{s}_{t+1}$ under real world transition dynamics $P^*(\hat{s}_{t+1}|s_t, a_t)$.

Given current state $s_t$ and the action $a_t$ predicted by the policy $\pi_\theta$, the grounded action $\hat{a}_t$ takes place in $E_{sim}$ will make the resulted $s_{t+1}$ in $E_{sim}$ close to the predicted next state $\hat{s}_{t+1}$ in $E_{real}$, which makes the dynamics $P_{\phi}(s_{t+1}|s_t, \hat{a}_t)$ in simulation close to the real-world dynamics $P^*(\hat{s}_{t+1}|s_t, a_t)$. Therefore, the policy $\pi_\theta$ is learned in $E_{sim}$ with $P_{\phi}$ close to $P^*$ will have a smaller performance gap when transferred to $E_{real}$ with $P^*$.


% The new grounded action $\hat{a}_t$ in the $E_{sim}$ will result in the agents transitioning close to the next states, which would happen in $E_{real}$ given the current states and actions. \hua{need more intuition}
% Given current state $s_t$, the grounded action $\hat{a}_t$ will make the resulted $s_{t+1}$ under the simulation transition dynamics $P_{\phi}(s_{t+1}|s_t, \hat{a}_t)$ close to the predicted next state $\hat{s}_{t+1}$ under real world transition dynamics $P^*(\hat{s}_{t+1}|s_t, a_t)$. GAT then proceeds to improve $p_\theta$ within the grounded simulator.
