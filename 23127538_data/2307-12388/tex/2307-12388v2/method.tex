\section{Methods}

% To mitigate the gap in the transition dynamics between the simulation and the real world, we propose an uncertainty

% \textcolor{blue}{we use \ours to mitigate the gap between sim and real transition dynamics and add into uncertainty to help mitigate the transformation model overconfidence problem and improve its performance.}


To mitigate the gap in the transition dynamics between traffic simulations and complex real-world traffic systems, we proposed an uncertainty-aware grounded action transformation (\ours) approach to improve the generalizability of policies learned in simulation. Inspired by the grounded action transformation method (GAT), we leverage uncertainty quantification to take grounding action dynamically, which could overcome the GAT's overconfidence problem and stably optimize the policy. In the rest of this section, we will illustrate the basic idea of GAT, the utilization of uncertainty for \ours, and corresponding implementations. 

\subsection{\textcolor{red}{Grounded Action Transformation}}

% Though TSC problem has been studied a lot recently and many researches have shown RL-based method has achieved promising results, there are still concerns about the these methods performances when they are deployed in real life. We will discuss current RL-based researches limitation on generalizability and propose an effective way to mitigate this problem in the below paragraphs. 
% % \textcolor{blue}{introduce the problem of transition dynamic gaps in TSC in the past work}

% % 1. there is a dynamic transition gap existing between sim and real in TSC. TSC exploration is at a high cost, so we use the simulation. 

% \subsubsection{\textcolor{red}{Generalization problem in past RL-based Traffic signal control}}

In reinforcement learning (RL), training a policy needs rounds of exploration to make agents learn from these experiences. However, in some domains, like traffic signal control, the exploration is expensive and unrealistic for public safety. As a result, conventional approaches only train and test their proposed traffic signal control algorithms in simulations with traffic data collected from real-life. There is no guarantee that the superior performance could be generalized to the new environment~\cite{zhao2019investigating}. The gap between simulation and the real world hinders the application of RL-based  methods in the real world.


% 3. To improve the policy trained in the simulation and generalize it to the real-word, we need to modify the simulationâ€˜s $\mathcal{M}_{sim}$ to make it higher fidelity and close to real-world $\mathcal{M}_{real}$.

To enhance the simulation's accuracy and establish a closer resemblance to the physical world, it is essential to minimize the disparity between the transition dynamics in both domains. This enables the simulation's model, denoted as $\mathcal{M}_{sim}$, to approach the real-world model, represented as $\mathcal{M}_{real}$. This procedure is commonly referred to as grounding.

\subsubsection{Basic GAT Framework}

% 3. GAT is trying to bridge the gap between transition dynamics, which suits our TSC problem. Explain how grounding action solves the dynamic differences. 

Grounded action transformation (GAT) is an algorithm originally proposed in robotics to improve robotic learning by using trajectories from the physical world $\mathcal{M}_{real}$ to modify $\mathcal{M}_{sim}$. In this study, we follow the idea of grounded action transformation and make our traffic signal control policy $\pi_{\theta}$ learned in our simulation $E_{sim}$ without loss of generality to deployed in $E_{real}$.

% % Figure environment removed

In the GAT framework, it assumes the simulation MDP $\mathcal{M}_{sim}$ is imperfect and modifiable. Specifically formulate it as a parameterized transition probability model $P_{\phi}(\cdot|s,a) =  P_{sim}(\cdot|s,a;\phi)$. Given real-world dataset $\mathcal{D}_{real} =  \{\tau^{1}, \tau^{2}, \dots, \tau^{I}\}$, where $\tau^{i} = (s_0^{i}, a_0^{i}, s_1^{i}, a_1^{i}, \dots, s_{T-1}^{i}, a_{T-1}^{i}, s_T^{i})$ is a trajectory collected by running a policy $\pi_{\theta}$ in $\mathcal{M}_{real}$, GAT aims to minimize the transition dynamics by finding $\phi^*$:

\begin{equation}
\phi^* = \arg \min \sum_{\tau^i \in \mathcal{M}_{real}} \sum_{t=0}^{T-1} d(P(s^i_{t+1}|s^i_t, a^i_t), P_{\phi}(\cdot|\pi_{\theta}; \phi))
\end{equation} 

% \textcolor{red}{wirte full function $g_{\phi}()$}

\noindent where $d(\cdot)$ is the distance between two distributions, $P$ is the real world transition distribution, and $P_{\phi}$ is the simulation transition distribution. 

To find $\phi$ efficiently, GAT takes agent's state $s_t$ and action $a_t$ as input, outputs a grounding action $\hat{a}_t$. Specifically, it uses an action transformation function parameterized with $\phi$: 

\begin{equation}
\label{eq:gat}
    \hat{a}_t = g_{\phi}(s_t, a_t) = f^{-1}_{sim}(s_t, f_{real}(s_t, a_t))
\end{equation}
 which includes two specific functions: a forward model $f_{real}$, and an inverse model $f^{-1}_{sim}$, as is shown in Fig.~\ref{fig:intro}.   
 
\noindent $\bullet$ \textit{The forward model} $f_{real}$ predicts the next state $\hat{s}_{t+1}$ given the current state $s_t$ and action $a_t$ under the transition dynamics in the real world $\mathcal{M}_{real}$:

\begin{equation}
\label{eq:forward}
     \hat{s}_{t+1} = f_{real}(s_t, a_t)
\end{equation}

% $s_t, a_t$ are current state and action and $\hat{s}_{t+1}$ is the predicted next state in $\mathcal{M}_{real}$ 

\noindent $\bullet$ \textit{The inverse model} $f^{-1}_{sim}$ takes the next state $\hat{s}_{t+1}$ calculated from the forward model and the current state $s_t$ in simulation as input, and predicts a grounded action $\hat{a}_t$ under the transition dynamic in $\mathcal{M}_{sim}$:
\begin{equation}
\label{eq:inverse}
     \hat{a}_t =  f^{-1}_{sim}(\hat{s}_{t+1}, s_t)
\end{equation}

The new grounded action $\hat{a}_t$ in the $E_{sim}$ will result in the agents transitioning close to the next states, which would happen in $E_{real}$ given the current states and actions. \hua{need more intuition}

% \textcolor{green}{formula to define forward and inverse model}

\subsubsection{\textcolor{red}{GAT for Traffic Signal Control}}


% \noindent1. Initialize policy $\pi_0$, real-world traffic dataset $\mathcal{D}_{real}$, simulation traffic dataset $\mathcal{D}_{sim}$, and train policy in the $E_{sim}$ till converge. 

% \noindent2. Execute policy $\pi_i$ to control traffic light in both $E_{sim} \text{ and } E_{real}$ and append collected traffic states and actions (phases) pairs ${(s^i_0, a^i_0), \dots, (s^i_t, a^i_t)}$ into $\mathcal{D}_{real}$ and $\mathcal{D}_{sim}$.


% \noindent3. Optimize action transformation function $g_{\phi} = f^{-1}_{sim}(s^i_t, f_{real}(s^i_t, a^i_t))$ to find the $\phi^*$ which minimize equation~\ref{eq:gat}. 

% \noindent4. Optimize policy $\pi_i$ in $E_{sim}$. At each simulation step $t$, if there is an $a_t^* = \arg \max_{a_{t}} Q(s_t, a_t)$ returned from $\pi_i$, replace $a_t^*$ with grounding action $\hat{a}_t = g_{\phi}(s_t, a^*_t)$.

% \noindent5. After optimization, return updated policy $\pi_{i+1}$ got to the step 2 until the policy did not improve in the $E_{real}$.


We adapt GAT for the traffic signal control problem by specifying the learning of $f_{real}$ and $f^{-1}_{sim}$: 

The forward model $f_{real}(s_t, a_t)$ in traffic signal control problem predicts the next traffic state $\hat{s}_{t+1}$ in the real world given taken action $a_t$ and observation of current traffic state $s_t$. We use a feed-forward neural network to approximate $f_{real}$ and learn to optimize the parameters with supervised learning, where $f_{real}$ is trained with:
\begin{equation}
\label{eq:forward-loss}
     MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})
\end{equation}
by data collected from $E_{real}$.

The inverse model $f^{-1}_{sim}(\hat{s}_{t+1}, s_t)$ in TSC predicts the grounding action in the simulation to reproduce the same traffic states $\hat{s}_{t+1}$ in the real world. We also use a feed-forward neural network and optimize it with supervised learning, where the inverse model is trained with:
\begin{equation}
\label{eq:inverse-loss}
    CrossEnropy(f^{-1}_{sim}(\hat{s}^i_{t+1}, s^i_t), a^i_t)
\end{equation}
with data collected from low-fidelity $E_{sim}$.

% Details of \ours are shown in Algorithm~\ref{algo:UGAT}.

% an inverse model $f^{-1}_{sim}(\hat{s}_{t+1}, s_t)$, which predicts the grounding action in the simulation to reproduce the same traffic states $\hat{s}_{t+1}$ in the real world. Thus the optimization on parameter $\phi$ could be formulated as a supervised learning problem, where the forward model is trained with the gradient from $MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})$ by data collected from real-world. And the inverse model is trained with the gradient from $CrossEnropy(f^{-1}_{sim}(\hat{s}^i_{t+1}, s^i_t), a^i_t)$ with data collected from low-fidelity simulation environment. Details of UGAT are shown in Algorithm~\ref{}

% 4. How TSC could benefit from grounding action transformation.


% Following the definition in the preliminary. We optimize our policy $\pi$ in the $E_{sim}$ with MDP $\mathcal{M}_{sim} = \left \langle \mathcal{S, A, r, P_{sim}}, \pi, \gamma \right \rangle$ to archive optimal in $E_{real}$: 

% \begin{align}
%     \pi^* &= \arg \max_{\pi} \eta_{P_{real}}(\pi), \\
%     \eta_{P_{real}}(\pi) &= \mathbb{E}_{\tau\sim(\pi)} \left[ R(\tau) \right] 
% \end{align}

% We achieve this by taking grounding action to make trajectory $\tau$ in $E_{sim}$ close to $E_{real}$:

% \begin{equation}
% \phi^* = \arg \min \sum_{\tau_{i} \in \mathcal{D}} \sum^{L}_{t=0} d(P(s_{t+1}^{i}| s^i_t, a^i_t), P_{\phi}(s^i_{t+1}|s^i_t, a^i_t))
% \end{equation} 

% \textcolor{red}{introduce gat module and how }


% \textcolor{red}{a picture to show how grounding action find $\theta^*$ }
% % 1/6
% % Figure environment removed


% where $d(c\dot)$ is the distance between two distributions, $s^i_t \text{ and } s^i_{t+1} \in \mathcal{S}$ are current state and next time state in state space, $a^i_t \in \mathcal{A}$ is the action taken at current time step belong to action space.

% To find $\phi$ efficiently, GAT uses parameterized action transformation function, which takes in agents' states and actions and outputs new actions. The new actions, grounding actions, will result in the agents in the $E_{sim}$ transitioning close to the next states, which would happen in $E_{real}$ given the current states and actions. This transition is called the grounding procedure.

% In the GAT grounding procedure is achieved by training an action transformation function with supervised learning:
% \begin{equation}
% g_{\phi}(s^i_t, a^i_t) = f^{-1}_{sim}(s^i_t, f_{real}(s^i_t, a^i_t))
% \end{equation}
% % where $f^{-1}_{sim}() \text{ and } f_{real}()$ are forward and inverse models explained below:


% Our implementation of GAT in TSC follows the steps described below:

% \begin{itemize}
%     \item[1] First train $\pi$ in the $E_{sim}$ to improve and get initial policy $\pi^0$.
%     \item[2] Execute policy $\pi$ in $E_{real}$ and $E_{sim}$ to collect data $D_{sim}$ and $D_{real}$
%     \item[3] train forward transition model $f_{real}$ with $D_{real}$.
%     \item[4] Train inverse transition model $f^-1_{sim}$ with $D_{sim}$.
%     \item[5] Update policy $\pi$ in $S_{sim}$ 
%     \item[6] iteratively run between steps 2-5 till performance stop improving in $E_{real}$
    
% \end{itemize}


% The forward model is trained with the gradient from $MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})$ by data collected from real-world. The inverse model is trained with the gradient from $CrossEnropy(f^{-1}_{sim}(s^i_{t+1}, s^i_t), a^i_t)$ with data collected from low-fidelity simulation environment.

% \textcolor{blue}{Introduce the problem that we have, and make the connection with what GAT is trying to solve, then we changed something of GAT to fit the task.
% (when explaining the methods, use defined TSC actions/states/terms. }

\subsection{\textcolor{red}{Uncertainty-aware Action Transformation}}

Though GAT is an effective method to bridge the sim-to-real gap, limitations still exist in the current framework, which could be improved in our \ours task. 
% In the below paragraph, we will introduce an uncertainty-aware mechanism and improve the current \ours structure. 
\hua{repetitive}

% \textcolor{blue}{introduce the existing GATs' flaws and challenges, and then introduce how we think about uncertainty could address such a challenge}


\subsubsection{\textcolor{red}{Model Uncertainty on $g_{\phi}$}}

% \textcolor{red}{add in Annotation with description}
% 1. Current \ours has model uncertainty and overconfidence in some actions. This will result in enlarging the gap instead of mitigating it. Thus learning is not stable and hard to converge.

The vanilla GAT takes supervised learning to train the action transformation function $g_{\phi}(\cdot, \cdot)$, and grounded action transformation $\hat{a}$ is taken at each step while improving in the $E_{sim}$. However, the action transformation function $g_{\phi}(\cdot, \cdot)$ could have high model uncertainty on unseen state and action inputs $s_t, a_t$. \hua{suddenly use `samples'}, upon which the grounded action $\hat{a}$ in Equation~\eqref{eq:gat} is likely to enlarge dynamics gap instead of mitigating it. This behavior will make policy learning unstable and hard to converge.


% \textcolor{red}{uncertatinty decrease performance of }
% To overcome this problem, inspired by uncertainty-aware control methods (reference here), we proposed an uncertainty-induced action transformation mechanism that will only execute grounding action while the inference has high certainty. By doing so, we can 1) keep the exploration rate close to the original $\epsilon$ and thus improve the policy learning step in the GAT framework. 2) By doing grounding action only at high certainty, the new actions will only be taken in $E_{sim}$ when the resulting next state is closed to the next state, which would be reached in $E_{real}$.


 % \textcolor{red}{model uncertainty exists, overconfident- we introduce uncertainty to avoid overconfident}
% 2. Using uncertainty quantification to evaluate model uncertainty can help distinguish these actions and avoid these grounding actions with high model uncertainty.

To overcome the enlarged gap induced by the model uncertainty, we need uncertainty quantification methods to~\cite{kabir2018neural} to alleviate this gap and propose an uncertainty-aware GAT method that will only execute grounding action $\hat{a}$ while the inference $g_{\phi}(s_t, a_t)$ has low model uncertainty. Such uncertainty allows us to 1) take grounding action only at low uncertainty. 2) stabilize the policy improvement and converge the learning policy. 


% 3. We improve GAP-TSC to uncertainty-aware \ours to mitigate the dynamic gaps and, at the same time, achieve stable learning. We keep track of uncertainty and plugin the uncertainty into GAT. (Details description of how we set alpha)

% To make GAP-TSC avoid executing grounding action $a_grounding$ while model uncertainty is high, we extend our framework to uncertainty-aware \ours. 


\subsubsection{\textcolor{red}{Dynamic Grounding Rate $\alpha$}}
Introducing uncertainty quantification allows us to evaluate the reliability of the transformation model and take grounded actions $\hat{a}$ when the model is certain that the resulting transition $P_{\phi}(s_t, \hat{a}_t)$  would mirror that of the real-world environment $E_{real}$ transition $P^*(s_t, a_t)$. This process enables us to minimize the gap in Equation~\eqref{eq:gat} between the simulation environment $E_{sim}$ and the real-world environment $E_{real}$, thereby improving the fidelity of the simulation. Details are described in Algorithm~\ref{algo:UGAT}.

Hereby, we use an $\alpha$ to represent the grounding rate which is the threshold for taking $\hat{a}$. In the uncertainty-aware \ours, we dynamically adjust $\alpha$, since the action transformation model $g_{\phi}(\cdot, \cdot)$ is updating; as a result, the model uncertainty will adjust with the learning sample increasing. A fixed grounding rate cannot synchronously adjust with the updating $g_{\phi}(\cdot, \cdot)$, which would not be able to achieve the regularization on grounding action $\hat{a}$. Also, a dynamically adjusted grounding rate will release the efforts on hyper-parameter tuning. \hua{gabbish}


\subsection{\textcolor{red}{Implementations}}
In this section, we introduce the detailed implementation of model uncertainty quantification and dynamically adjusted grounding rate.

\subsubsection{\textcolor{red}{Choice of Uncertainty Model}}
Since the uncertainty model is highly correlated to the performance of the uncertainty method, we explored different state-of-the-art uncertainty quantification methods and used the evidential deep learning (EDL) method as the implementation of the uncertainty model in the inverse model of \ours.

% Inspired by the Theory of Evidence (DST) \cite{TheoryE}, Subjective Logic (SL) allows one to quantify belief masses and uncertainty through a well-defined theoretical framework and the overall uncertainty mass of $u$ can be written as $u = \frac{K}{S}$,  where $ S = \sum_{i=1}^K (e_i+ 1) $, evidence $e_i$ is a measure of the amount of support collected from data in favor of a sample to be classified into a certain class and $K$ refers to mutually exclusive class labels $k = 1, . . . , K$.

Evidential deep learning (EDL)~\cite{sensoy2018evidential} is an uncertainty quantification method from the theory of evidence perspective for classification problems, quantifying belief mass and uncertainty. Specifically, in a K-class multi-classification problem, SL considers class labels mutually exclusive singletons by providing belief mass $b_k$ for each class $k$. And it, at the same time, provides an overall uncertainty mass of $u$. The relationship between belief mass and uncertainty is: 
\begin{equation}
    u = \frac{K}{S} \text{ and } b_k = \frac{e_k}{S}
\end{equation}
where $S = \sum_{i=1}^K(e_i +1)$. In the EDL, the $Softmax$ activation on the output layer of a neural network is replaced with $ReLU$ to calculate the evidence vector $[e_1, e_2, \dots, e_K ]$ for each class. And the uncertainty is calculated by:
\begin{equation}
\label{eq:edl}
    u = \frac{K}{\sum_{i =1}^K (e_i +1)}
\end{equation}


To track the model uncertainty of grounding transformation function $g_{\phi}(\cdot, \cdot)$, we see the action space $\mathcal{A}$ as classes and integrate EDL into the inverse model $f^{-1}_{sim}(\hat{s}_{t+1}, s_t)$ to quantify the uncertainty of output grounding action $\hat{a} \in \mathcal{A}$. At the last layer for predicting the output $logits$, we use $ReLU$ as the activation layer \cite{sensoy2018evidential} to calculate the evidence. Model uncertainty $u$ can be easily computed from Equation~\eqref{eq:edl}. \textcolor{red}{After adding EDL to the inverse model, we modify the inverse model into:
\begin{equation}
\label{eq:update inverse}
    \hat{a}_t, u_t =  \tilde{f}^{-1}_{sim}(\hat{s}_{t+1}, s_t)
\end{equation}
and grounded transformation could is modified into:
\begin{equation}
    \hat{a}_t, u_t = \tilde{g}_{\phi}(s_t, a_t) = \tilde{f}^{-1}_{sim}(f_{real}(s_t, a_t), s_t)
\end{equation}
}


% In the policy improvement step, at round $r$, assume for each epoch $i \in N$, containing $T$ steps, we can calculate $u_r$ from estimation on each step $u_i$:
% \begin{equation}
%     u_r = \frac{\sum_{n=1}^N{\sum_{i=1}^M} u_i }{N*M}
% \end{equation}
% And then, we update the parameter $\alpha = u_r$; if the uncertainty at round $t$: $u_t < u_r$, the model conducts action grounding. 

\subsubsection{\textcolor{red}{Calculation of Grounding Rate}}

To dynamically adjust the grounding rate with the changing of model uncertainty, we keep track of the transformation function $g_{\phi}(s_t, a_t)$'s uncertainty $u_t$ when it outputs grounding action $\hat{a}_t$  \hua{never mentioned $u_t$ is output from $g_{\phi}$} in the action grounding step in Algorithm~\ref{algo:UGAT}. 
 % there is a high chance of making the next states $s_{t+1}$ in $E_{sim}$ diverge to the next state $\hat{s}_{t+1}$, which would happen in $E_{real}$. As a result, we reject to take it. And low uncertainty means that by taking $\hat{a}_t$, we have a high chance of pushing the next state in $E_{sim}$ and $E_{real}$ close. 
%When we work in the uncertainty-aware \ours framework, in the first iteration, during the policy improvement step, whenever model $g_{\phi}(s_t, a_t)$ returns a grounding action $\hat{a}_t$, we track the model uncertainty $u_t$ and take the original action $a_t$. 
At the end of each iteration $i$, we decide the grounding rate $\alpha$ based on the past record of model uncertainty by calculating the mean
\begin{equation}
\label{eq:u-update}
    \mu^i = \frac{\sum^E_{e=1} \sum^{T-1}_{t=0} u^e_t}{T \times E}
\end{equation}
from the logged uncertainties in the last $E$ epochs, and update $\alpha$ for evaluating grounding action in the next iteration. During action grounding, whenever model $g_{\phi}(s_t, a_t)$ returns a grounding action $\hat{a}_t$, if the uncertainty $u_t$ is less than the threshold $\alpha$, the grounded action $\hat{a}_t$ will be taken in the real environment; otherwise, we will reject $\hat{a}_t$ and take the original $a_t$.

% Thus introducing uncertainty-aware grounding action is beneficial to learn a policy in $E_{sim}$, which could be generalized to $E_{real}$.

% \textcolor{red}{why dynamically change $\alpha$}

% \textcolor{red}{fixed cannot tune with updated transformation model. no need to tune hyper-parameter  }
%  In the first iteration, during the policy improvement step, we track the uncertainty of each simulation step and calculate the first quantile as $\alpha$. At this round, we do not take grounding actions. In the rest iterations, during the policy improvement step, we calculate grounding action with its uncertainty. If the uncertainty is less than the threshold $\alpha$, we take grounding action; if less, then not. And take the new first quantile as $\alpha$ for the next iteration. 

% Since the uncertainty model is highly correlated to the performance of the uncertainty quantification method, we explored different state-of-the-art methods, including {Methods}.

\begin{algorithm}[h!]
\DontPrintSemicolon
\caption{Algorithm for \ours with model uncertainty quantification}
\label{algo:UGAT}
\KwIn{Initial policy $\pi_{\theta}$, forward model $f_{real}$, inverse model $\tilde{f}^{-1}_{sim}$, Real dataset $\mathcal{D}_{real}$, Sim dataset $\mathcal{D}_{sim}$, grounding rate $\alpha = \inf$}
\KwOut{Policy $\pi_{\theta}$, $f_{real}$, $\tilde{f}^{-1}_{sim}$}

    % \For {e = 1,2, \dots}{
    % \# \textbf{\textit{Improving initial policy  in $E_{sim}$}} \;
    %     Update policy $\pi$ in the $E_{sim}$ \;
    % }
    Pre-train policy $\pi_{\theta}$ for $\textcolor{red}{M}$ iterations in $E_{sim}$ \;
    
	\For {i = 1,2, \dots, I}
	{
        % Reset $E_{real}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate real-world rollout with current $\pi$}} \;
        % Take actions with $a_t = \pi(s_t) $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{real}$ \;
        % }
        Generate real-world rollout $\tau^i_{real}$ with policy $\pi_{\theta}$ and add $\tau^i_{real}$ into $\mathcal{D}_{real}$ \; \\
        
        % Reset $E_{sim}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate simulation rollout with current $\pi$ }} \;
        % Take actions with $a_t = \pi(s_t)  $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{sim}$ \;
        % }
        Generate simulation rollout $\tau^i_{sim}$ with policy $\pi_{\theta}$, and add $\tau^i_{sim}$ into $\mathcal{D}_{sim}$ \\

        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train forward model $f_{real}$}} \;
        %     Sample minibatch $B_l$ from $D_{real}$ \;
        %     Update $f_{real}$ with $MSE(f_{real}(s_t,a_t), s_{t+1}$) \;
        % }
        \# \textbf{\textit{Transformation function improvement step}} \;
        
        Update $f_{real}$ with Equation~\eqref{eq:forward-loss} \;
        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train inverse model $f^{-1}_{sim}$ \;
        %     Sample minibatch $B_l$ from $D_{sim}$ }} \;
        %     Update $f_{sim}$ with $CrossEntropy(f^{-1}_{sim}(s_t, s_{t+1|}), a_t)$ \;
        % }
        Update $f^{-1}_{sim}$ with Equation~\eqref{eq:inverse-loss}  \;
        
        Reset logged uncertainty $U = [ \ ]$ \;
        
        \For {e = 1, 2, \dots, \textcolor{red}{E}}
        {
        \# \textbf{\textit{Action grounding step}} \;
            \For {t = 1,2 ,\dots, T}
            {
            $a_t = \pi(s_t)$, $\hat{s}_{t+1} = f_{real}(s_t, a_t)$ \;
            
            $\hat{a}_{t}, u^e_t = \tilde{f}^{-1}_{sim}(\hat{s}_{t+1}, s_t)$ \;
            \lIf{$u^e_t \geq \alpha $}  
                {$\hat{a}_t = a_t$ \textbf{\textit{\# Reject}}}
            $U.append(u^e_t)$ \;
            }
            \# \textbf{\textit{Policy improvement step}}\;
            Improve policy $\pi_{\theta}$ with reinforcement learning \;
        }  
       Update $\alpha$ with Equation~\eqref{eq:u-update} \;
    }
\end{algorithm} 

% \subsubsection{\textcolor{red}{Example: evidential deep learning (EDL) in \ours}}
% 4. We provide one example of an uncertainty modeling method (how we use uncertainty modeling here.




% In the Algorithm~\ref{} step, when we calculate grounding action from grounded action transformation function $g_{\phi}$, at the same time, we simultaneously calculate the uncertainty of taking this action. High uncertainty means by taking the grounding action; there is a high chance of making the next states in $E_{sim}$ diverge to the next state, which would happen in $E_{real}$. As a result, we reject to take it. And low uncertainty means that by taking grounding action, we have a high chance of pushing the next state in $E_{sim}$ and $E_{real}$ close. Introducing uncertainty-aware grounding action is beneficial to learn a policy in $E_{sim}$, which could be generalized to $E_{real}$.

% We set a dynamically changing threshold value $\alpha$ to decide whether to take a grounding action. In the first iteration, during the policy improvement step, we track the uncertainty of each simulation step and calculate the first quantile as $\alpha$. At this round, we do not take grounding actions. In the rest iterations, during the policy improvement step, we calculate grounding action with its uncertainty. If the uncertainty is less than the threshold $\alpha$, we take grounding action; if less, then not. And take the new first quantile as $\alpha$ for the next iteration. 

% Since the uncertainty model is highly correlated to the performance of the GAT method, we explored different state-of-the-art methods including {Methods}.

% \textcolor{red}{taking one and combine the above part for demonstration later.}

% % 1 / 6 pseudo code 

% \textcolor{blue}{strengthen how we use uncertainty with exploited various uncertainty quantification methods.}

% During the action grounding phase, due to the imperfection of the forward and inverse models, the grounded action is prone to enlarge instead of mitigating the difference of states between the simulation and real-life environment. A more severe situation is the grounding action with low belief will make this transformation behave like random exploration, resulting in instability during training. To quantify belief masses and uncertainty, we introduce a dynamically adjusted hyperparameter $\alpha$ to dynamically determine taking grounded action with a qualified uncertainty value during inference. In the framework, the uncertainty of $u^r_t$ at time $t$ after training round $r$ is quantified based on the Evidential Deep Learning method~\cite{EDL}. If $u^r_t < u^r$, where $u^r$ is the average uncertainty of all $u$ during the training round $r$, the model conducts action grounding. 
% At round $r$, assume for each epoch $n \in N$, containing $M$ steps, we can calculate $u_r$ from estimation on each step $u_i$:
% \begin{equation}
%     u_r = \frac{\sum_{n=1}^N{\sum_{i=1}^M} u_i }{N*M}
% \end{equation}
% Based on $\alpha = u_r$, if the uncertainty at round $t$: $u_t < u_r$, the model conducts action grounding. 
% Uncertainty quantification involves identifying the sources of uncertainty in the model, propagating the uncertainties through the model, and quantifying the uncertainty in the model output. This process can involve a variety of techniques, including statistical analysis, probabilistic modeling, and sensitivity analysis.

% \paragraph{\textcolor{red}{introduce one specific uncertainty and how we use it in our method, leave an open space to explore the uncertainty quantification methods}
% }


% 1/4

