%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% Para1: Traffic Signal Control domain development and real-world significance.
With the growing availability of traffic data and advancements in deep reinforcement learning techniques, there is a developing trend toward utilizing reinforcement learning (RL) for traffic signal control (TSC). However, current research on reinforcement learning-based TSC is limited to simulators. Although simulation-based training is cost-effective, it suffers from inherent discrepancies with real-world settings due to the complex nature of real-life dynamics. As a result, a critical challenge in implementing real-life RL is finding ways to apply simulation-based training in real-world scenarios.
% ~\cite{Sim-to-real transfer}

% Para2: What has been previously focused on and studied, still existing challenges.
In order to bridge the gap, much effort has been made. Some researchers adopt domain randomization ideas, which intend to cover the actual distribution of real-world data by randomizing the simulation multiple times. On the other hand, domain adaptation methods exploit the source domain data through feature representation and transfer to improve the model's target domain performance, whose data is practically scarce \cite{Domain adapt1}.
% Para3: Introduce the Sim2Reak problem and its importance.
However, the above exploration is mainly applied to the robotics domain, and few studies have been conducted on the TSC area, even though it is also suffering such a plight. We conducted a preliminary study on a specific trained policy to demonstrate the gap it may have when performing in the simulator and real-world settings, as shown in Fig.~\ref{fig:intro}(a). Solving the sim2real problem in TSC is an inevitable way to the policy's practical deployment. 
% Figure environment removed

% Para4: What we propose in this paper and contribution.
In this paper, we propose \ours to use grounded action transformation to bridge such a gap in the training process, helping to calibrate the dynamics distribution shifting. Furthermore, we quantify the model's parameter uncertainty and leverage it to dynamically adjust the action grounding rate, contributing to the training efficiency and stability. The improvement using GAT-TSC-DQN in an example is shown in Fig 1(b).

% sim: params, real:params (explain) real world meaning