\section{Methods}

% To mitigate the gap in the transition dynamics between the simulation and the real world, we propose an uncertainty

% \textcolor{blue}{we use \ours to mitigate the gap between sim and real transition dynamics and add into uncertainty to help mitigate the transformation model overconfidence problem and improve its performance.}
To mitigate the gap in the transition dynamics between traffic simulations and complex real-world traffic systems, we proposed an uncertainty-aware grounded action transformation (\ours) approach to improve the generalizability of policies learned in simulation. Inspired by the grounded action transformation method (GAT), we leverage uncertainty quantification to take grounded action dynamically, which could overcome the GAT's overconfidence problem and stably optimize the policy. In the rest of this section, we will illustrate GAT in TSC problem, \ours, and its corresponding implementations. 

\subsection{\textcolor{red}{GAT for Traffic Signal Control}}

% Though TSC problem has been studied a lot recently and many researches have shown RL-based method has achieved promising results, there are still concerns about the these methods performances when they are deployed in real life. We will discuss current RL-based researches limitation on generalizability and propose an effective way to mitigate this problem in the below paragraphs. 
% % \textcolor{blue}{introduce the problem of transition dynamic gaps in TSC in the past work}

% % 1. there is a dynamic transition gap existing between sim and real in TSC. TSC exploration is at a high cost, so we use the simulation. 

% \subsubsection{\textcolor{red}{Generalization problem in past RL-based Traffic signal control}}

 The gap between simulation and the real world hinders the application of RL-based  methods in the real world.
% 3. To improve the policy trained in the simulation and generalize it to the real-word, we need to modify the simulationâ€˜s $\mathcal{M}_{sim}$ to make it higher fidelity and close to real-world $\mathcal{M}_{real}$.
To establish a closer resemblance between the simulation and the physical world, minimizing the disparity between the transition dynamics in both domains is essential. The procedure enabling the transition dynamics in simulation $P_{\phi}$ close to the real-world transition dynamics $P^*$ is commonly called grounding.



% \textcolor{green}{formula to define forward and inverse model}

% \subsubsection{\textcolor{red}{GAT for Traffic Signal Control}}


% \noindent1. Initialize policy $\pi_0$, real-world traffic dataset $\mathcal{D}_{real}$, simulation traffic dataset $\mathcal{D}_{sim}$, and train policy in the $E_{sim}$ till converge. 

% \noindent2. Execute policy $\pi_i$ to control traffic light in both $E_{sim} \text{ and } E_{real}$ and append collected traffic states and actions (phases) pairs ${(s^i_0, a^i_0), \dots, (s^i_t, a^i_t)}$ into $\mathcal{D}_{real}$ and $\mathcal{D}_{sim}$.


% \noindent3. Optimize action transformation function $g_{\phi} = f^{-1}_{sim}(s^i_t, f_{real}(s^i_t, a^i_t))$ to find the $\phi^*$ which minimize equation~\ref{eq:gat}. 

% \noindent4. Optimize policy $\pi_i$ in $E_{sim}$. At each simulation step $t$, if there is an $a_t^* = \arg \max_{a_{t}} Q(s_t, a_t)$ returned from $\pi_i$, replace $a_t^*$ with grounded action $\hat{a}_t = g_{\phi}(s_t, a^*_t)$.

% \noindent5. After optimization, return updated policy $\pi_{i+1}$ got to the step 2 until the policy did not improve in the $E_{real}$.


We adapt GAT for the traffic signal control problem by specifying the learning of $f_{\phi^+}$ and $h_{\phi^-}$: 


The forward model $f_{\phi^+}(s_t, a_t)$ in traffic signal control problem predicts the next traffic state $\hat{s}_{t+1}$ in the real world given taken action $a_t$ and observation of current traffic state $s_t$. We use a deep neural network to approximate $f_{\phi^+}$ and learn to optimize the parameters with supervised learning, where $f_{\phi^+}(s_t, a_t)$ is trained with:
\begin{equation}
\label{eq:forward-loss}
     \mathcal{L}(\phi^+) = MSE(f_{\phi^+}(s^i_t, a^i_t), s^i_{t+1})
\end{equation}
by trajectories collected from $E_{real}$.

The inverse model $h_{\phi^-}(\hat{s}_{t+1}, s_t)$ in TSC predicts the grounded action in the simulation to reproduce the same traffic states $\hat{s}_{t+1}$ in the real world. We also use a deep neural network and optimize it with supervised learning, where the inverse model is trained with:
\begin{equation}
\label{eq:inverse-loss}
    \mathcal{L}(\phi^-) = CrossEnropy(h_{\phi^-}(\hat{s}^i_{t+1}, s^i_t), a^i_t)
\end{equation}
with trajectories collected from $E_{sim}$.

% Details of \ours are shown in Algorithm~\ref{algo:UGAT}.

% an inverse model $f^{-1}_{sim}(\hat{s}_{t+1}, s_t)$, which predicts the grounded action in the simulation to reproduce the same traffic states $\hat{s}_{t+1}$ in the real world. Thus the optimization on parameter $\phi$ could be formulated as a supervised learning problem, where the forward model is trained with the gradient from $MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})$ by data collected from real-world. And the inverse model is trained with the gradient from $CrossEnropy(f^{-1}_{sim}(\hat{s}^i_{t+1}, s^i_t), a^i_t)$ with data collected from low-fidelity simulation environment. Details of UGAT are shown in Algorithm~\ref{}

% 4. How TSC could benefit from grounded action transformation.


% Following the definition in the preliminary. We optimize our policy $\pi$ in the $E_{sim}$ with MDP $\mathcal{M}_{sim} = \left \langle \mathcal{S, A, r, P_{sim}}, \pi, \gamma \right \rangle$ to archive optimal in $E_{real}$: 

% \begin{align}
%     \pi^* &= \arg \max_{\pi} \eta_{P_{real}}(\pi), \\
%     \eta_{P_{real}}(\pi) &= \mathbb{E}_{\tau\sim(\pi)} \left[ R(\tau) \right] 
% \end{align}

% We achieve this by taking grounded action to make trajectory $\tau$ in $E_{sim}$ close to $E_{real}$:

% \begin{equation}
% \phi^* = \arg \min \sum_{\tau_{i} \in \mathcal{D}} \sum^{L}_{t=0} d(P(s_{t+1}^{i}| s^i_t, a^i_t), P_{\phi}(s^i_{t+1}|s^i_t, a^i_t))
% \end{equation} 

% \textcolor{red}{introduce gat module and how }


% \textcolor{red}{a picture to show how grounded action find $\theta^*$ }
% % 1/6
% % Figure environment removed


% where $d(c\dot)$ is the distance between two distributions, $s^i_t \text{ and } s^i_{t+1} \in \mathcal{S}$ are current state and next time state in state space, $a^i_t \in \mathcal{A}$ is the action taken at current time step belong to action space.

% To find $\phi$ efficiently, GAT uses parameterized action transformation function, which takes in agents' states and actions and outputs new actions. The new actions, grounded actions, will result in the agents in the $E_{sim}$ transitioning close to the next states, which would happen in $E_{real}$ given the current states and actions. This transition is called the grounded procedure.

% In the GAT grounding procedure is achieved by training an action transformation function with supervised learning:
% \begin{equation}
% g_{\phi}(s^i_t, a^i_t) = f^{-1}_{sim}(s^i_t, f_{real}(s^i_t, a^i_t))
% \end{equation}
% % where $f^{-1}_{sim}() \text{ and } f_{real}()$ are forward and inverse models explained below:


% Our implementation of GAT in TSC follows the steps described below:

% \begin{itemize}
%     \item[1] First train $\pi$ in the $E_{sim}$ to improve and get initial policy $\pi^0$.
%     \item[2] Execute policy $\pi$ in $E_{real}$ and $E_{sim}$ to collect data $D_{sim}$ and $D_{real}$
%     \item[3] train forward transition model $f_{real}$ with $D_{real}$.
%     \item[4] Train inverse transition model $f^-1_{sim}$ with $D_{sim}$.
%     \item[5] Update policy $\pi$ in $S_{sim}$ 
%     \item[6] iteratively run between steps 2-5 till performance stop improving in $E_{real}$
    
% \end{itemize}


% The forward model is trained with the gradient from $MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})$ by data collected from real-world. The inverse model is trained with the gradient from $CrossEnropy(f^{-1}_{sim}(s^i_{t+1}, s^i_t), a^i_t)$ with data collected from low-fidelity simulation environment.

% \textcolor{blue}{Introduce the problem that we have, and make the connection with what GAT is trying to solve, then we changed something of GAT to fit the task.
% (when explaining the methods, use defined TSC actions/states/terms. }

\subsection{\textcolor{red}{Uncertainty-aware GAT}}


In the below section, we will introduce the limitations of the current GAT and propose an uncertainty-aware GAP (\ours) which can benefit from quantifying model uncertainty.

% \textcolor{blue}{introduce the existing GATs' flaws and challenges, and then introduce how we think about uncertainty could address such a challenge}

\subsubsection{\textcolor{red}{Model Uncertainty on $g_{\phi}$}}

% \textcolor{red}{add in Annotation with description}
% 1. Current \ours has model uncertainty and overconfidence in some actions. This will result in enlarging the gap instead of mitigating it. Thus learning is not stable and hard to converge.

The vanilla GAT takes supervised learning to train the action transformation function $g_{\phi}(\cdot, \cdot)$, and grounded action transformation $\hat{a}$ is taken at each step while improving in the $E_{sim}$. However, the action transformation function $g_{\phi}(\cdot, \cdot)$ could have high model uncertainty on unseen state and action inputs $s_t, a_t$, upon which the grounded action $\hat{a}$ in Equation~\eqref{eq:gat} is likely to enlarge the dynamics gap instead of mitigating it. The enlarged gap will make policy learning unstable and hard to converge.


% \textcolor{red}{uncertatinty decrease performance of }
% To overcome this problem, inspired by uncertainty-aware control methods (reference here), we proposed an uncertainty-induced action transformation mechanism that will only execute grounded action while the inference has high certainty. By doing so, we can 1) keep the exploration rate close to the original $\epsilon$ and thus improve the policy learning step in the GAT framework. 2) By doing grounded action only at high certainty, the new actions will only be taken in $E_{sim}$ when the resulting next state is closed to the next state, which would be reached in $E_{real}$.


 % \textcolor{red}{model uncertainty exists, overconfident- we introduce uncertainty to avoid overconfident}
% 2. Using uncertainty quantification to evaluate model uncertainty can help distinguish these actions and avoid these grounded actions with high model uncertainty.

To overcome the enlarged gap induced by the model uncertainty, we need uncertainty quantification methods to~\cite{kabir2018neural} to keep track of the uncertainty of grounded transformation function $g_{\phi}$. This uncertainty quantification allows us to evaluate the reliability of the transformation model and take grounded actions $\hat{a}$ when the model is certain that the resulting transition $P_{\phi}(s_t, \hat{a}_t)$  would mirror that of the real-world environment $E_{real}$ transition $P^*(s_t, a_t)$. This process enables us to minimize the gap in Equation~\eqref{eq:gat} between the simulation environment $E_{sim}$ and the real-world environment $E_{real}$, thereby improving the fidelity of the simulation. Details are described in Algorithm~\ref{algo:UGAT}.

% and propose an uncertainty-aware GAT method that will only execute grounded action $\hat{a}$ while the inference $g_{\phi}(s_t, a_t)$ has low model uncertainty. Such uncertainty quantification allows us to 1) take grounded action only at low uncertainty. 2) stabilize the policy improvement and converge the learning policy. 

% Introducing uncertainty quantification allows us to evaluate the reliability of the transformation model and take grounded actions $\hat{a}$ when the model is certain that the resulting transition $P_{\phi}(s_t, \hat{a}_t)$  would mirror that of the real-world environment $E_{real}$ transition $P^*(s_t, a_t)$. This process enables us to minimize the gap in Equation~\eqref{eq:gat} between the simulation environment $E_{sim}$ and the real-world environment $E_{real}$, thereby improving the fidelity of the simulation. Details are described in Algorithm~\ref{algo:UGAT}.

% 3. We improve GAP-TSC to uncertainty-aware \ours to mitigate the dynamic gaps and, at the same time, achieve stable learning. We keep track of uncertainty and plugin the uncertainty into GAT. (Details description of how we set alpha)

% To make GAP-TSC avoid executing grounded action $a_grounding$ while model uncertainty is high, we extend our framework to uncertainty-aware \ours. 

\subsubsection{\textcolor{red}{Choice of Uncertainty Model}}
Since the uncertainty model is highly correlated to the performance of the uncertainty method, we explored different state-of-the-art uncertainty quantification methods and found UGAT performs well across different methods. We use the evidential deep learning (EDL) method to demonstrate how we integrate the uncertainty model into the inverse model of \ours.

% Inspired by the Theory of Evidence (DST) \cite{TheoryE}, Subjective Logic (SL) allows one to quantify belief masses and uncertainty through a well-defined theoretical framework and the overall uncertainty mass of $u$ can be written as $u = \frac{K}{S}$,  where $ S = \sum_{i=1}^K (e_i+ 1) $, evidence $e_i$ is a measure of the amount of support collected from data in favor of a sample to be classified into a certain class and $K$ refers to mutually exclusive class labels $k = 1, . . . , K$.

Evidential deep learning (EDL)~\cite{sensoy2018evidential} is an uncertainty quantification method from the theory of evidence perspective for classification problems, quantifying belief mass and uncertainty. Specifically, in a K-class multi-classification problem, subjective logic (SL) considers class labels mutually exclusive singletons by providing belief mass $b_k$ for each class $k$. And it, at the same time, provides an overall uncertainty mass of $u$. The relationship between belief mass and uncertainty is: 
\begin{equation}
    u = \frac{K}{S} \text{ and } b_k = \frac{e_k}{S}
\end{equation}
where $S = \sum_{i=1}^K(e_i +1)$. In the EDL, the $Softmax$ activation on the output layer of a neural network is replaced with $ReLU$ to calculate the evidence vector $[e_1, e_2, \dots, e_K ]$ for each class. And the uncertainty is calculated by:
\begin{equation}
\label{eq:edl}
    u = \frac{K}{\sum_{i =1}^K (e_i +1)}
\end{equation}


To track the model uncertainty of grounded transformation function $g_{\phi}(\cdot, \cdot)$, we see the action space $\mathcal{A}$ as classes and integrate uncertainty into the inverse model $h_{\phi^-}(\hat{s}_{t+1}, s_t)$ to quantify the uncertainty of output grounded action $\hat{a} \in \mathcal{A}$. At the last layer for predicting the output $logits$, we use $ReLU$ as the activation layer \cite{sensoy2018evidential} to calculate the evidence. Model uncertainty $u$ can be easily computed from Equation~\eqref{eq:edl}. After adding EDL to the inverse model, we modify the inverse model into:
\begin{equation}
\label{eq:update inverse}
    \hat{a}_t, u_t =  h^u_{\phi^-}(\hat{s}_{t+1}, s_t)
\end{equation}
and grounded transformation could is modified into:
\begin{equation}
    \hat{a}_t, u_t = g^u_{\phi}(s_t, a_t) = h^u_{\phi^-}(f_{\phi^+}(s_t, a_t), s_t)
\end{equation}


\subsubsection{\textcolor{red}{Dynamic Grounding Rate $\alpha$}}

With the integrated uncertainty quantification, UGAT can keep track of the model uncertainty of grounded action $\hat{a}_t$, then filter out ones with uncertainty higher than a threshold denoted a $\alpha$. We used two methods to set the threshold.
First, we set the threshold $\alpha$ fixed, which is a naive method. However, since the inverse model $h^u_{\phi^-}$ is updating, a fixed grounding rate cannot adjust with the model uncertainty dynamically changing in each iteration. An alternative method is to set grounding rate $\alpha$ dynamically changing. This method can synchronously adjust $\alpha$ with the updating $h^u_{\phi^-}$ and relief efforts on hyper-parameter tuning.

% Hereby, we use an $\alpha$ to represent the grounding rate which is the threshold for taking $\hat{a}$. In the uncertainty-aware \ours, we dynamically adjust $\alpha$, since the action transformation model $g_{\phi}(\cdot, \cdot)$ is updating; as a result, the model uncertainty will adjust with the learning sample increasing. A fixed grounding rate cannot synchronously adjust with the updating $g_{\phi}(\cdot, \cdot)$, which would not be able to achieve the regularization on grounded action $\hat{a}$. Also, a dynamically adjusted grounding rate will release the efforts on hyper-parameter tuning. \hua{gabbish}


\subsection{\textcolor{red}{Implementations}}
In this section, we introduce the detailed implementation of model uncertainty quantification and dynamically adjusted grounding rate.



% In the policy improvement step, at round $r$, assume for each epoch $i \in N$, containing $T$ steps, we can calculate $u_r$ from estimation on each step $u_i$:
% \begin{equation}
%     u_r = \frac{\sum_{n=1}^N{\sum_{i=1}^M} u_i }{N*M}
% \end{equation}
% And then, we update the parameter $\alpha = u_r$; if the uncertainty at round $t$: $u_t < u_r$, the model conducts action grounded. 

\subsubsection{\textcolor{red}{Calculation of Grounding Rate}}

To dynamically adjust the grounding rate with the changing of model uncertainty, we keep track of the model uncertainty $u_t$ of $g^u_{\phi}(s_t, a_t)$  when it outputs grounding action $\hat{a}_t$ in the action grounding step in Algorithm~\ref{algo:UGAT}. 
 % there is a high chance of making the next states $s_{t+1}$ in $E_{sim}$ diverge to the next state $\hat{s}_{t+1}$, which would happen in $E_{real}$. As a result, we reject to take it. And low uncertainty means that by taking $\hat{a}_t$, we have a high chance of pushing the next state in $E_{sim}$ and $E_{real}$ close. 
%When we work in the uncertainty-aware \ours framework, in the first iteration, during the policy improvement step, whenever model $g_{\phi}(s_t, a_t)$ returns a grounding action $\hat{a}_t$, we track the model uncertainty $u_t$ and take the original action $a_t$. 
At the end of each iteration $i$, we decide the grounding rate $\alpha$ based on the past record of model uncertainty by calculating the mean
\begin{equation}
\label{eq:u-update}
    U^i = \frac{\sum^E_{e=1} \sum^{T-1}_{t=0} u^e_t}{T \times E}
\end{equation}
from the logged uncertainties in the last $E$ epochs, update $\alpha$ for evaluating grounding action in the next iteration. During action grounding, whenever model $g^u_{\phi}(s_t, a_t)$ returns a grounding action $\hat{a}_t$, if the uncertainty $u_t$ is less than the threshold $\alpha$, the grounded action $\hat{a}_t$ will be taken in the simulation environment; otherwise, we will reject $\hat{a}_t$ and take the original $a_t$.

% Thus introducing uncertainty-aware grounding action is beneficial to learn a policy in $E_{sim}$, which could be generalized to $E_{real}$.

% \textcolor{red}{why dynamically change $\alpha$}

% \textcolor{red}{fixed cannot tune with updated transformation model. no need to tune hyper-parameter  }
%  In the first iteration, during the policy improvement step, we track the uncertainty of each simulation step and calculate the first quantile as $\alpha$. At this round, we do not take grounding actions. In the rest iterations, during the policy improvement step, we calculate grounding action with its uncertainty. If the uncertainty is less than the threshold $\alpha$, we take grounding action; if less, then not. And take the new first quantile as $\alpha$ for the next iteration. 

% Since the uncertainty model is highly correlated to the performance of the uncertainty quantification method, we explored different state-of-the-art methods, including {Methods}.

\begin{algorithm}[h!]
\DontPrintSemicolon
\caption{Algorithm for \ours with model uncertainty quantification}
\label{algo:UGAT}
\KwIn{Initial policy $\pi_{\theta}$, forward model $f_{\phi^+}$, inverse model $h_{\phi^-}$, Real dataset $\mathcal{D}_{real}$, Sim dataset $\mathcal{D}_{sim}$, grounding rate $\alpha = \inf$}
\KwOut{Policy $\pi_{\theta}$, $f_{\phi^+}$, $h_{\phi^-}$}

    % \For {e = 1,2, \dots}{
    % \# \textbf{\textit{Improving initial policy  in $E_{sim}$}} \;
    %     Update policy $\pi$ in the $E_{sim}$ \;
    % }
    Pre-train policy $\pi_{\theta}$ for M iterations in $E_{sim}$ \;
    
	\For {i = 1,2, \dots, I}
	{
        % Reset $E_{real}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate real-world rollout with current $\pi$}} \;
        % Take actions with $a_t = \pi(s_t) $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{real}$ \;
        % }
        Generate real-world rollout $\tau^i_{real}$ with policy $\pi_{\theta}$ and add $\tau^i_{real}$ into $\mathcal{D}_{real}$ \;
        
        % Reset $E_{sim}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate simulation rollout with current $\pi$ }} \;
        % Take actions with $a_t = \pi(s_t)  $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{sim}$ \;
        % }
        Generate simulation rollout $\tau^i_{sim}$ with policy $\pi_{\theta}$, and add $\tau^i_{sim}$ into $\mathcal{D}_{sim}$ \\

        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train forward model $f_{real}$}} \;
        %     Sample minibatch $B_l$ from $D_{real}$ \;
        %     Update $f_{real}$ with $MSE(f_{real}(s_t,a_t), s_{t+1}$) \;
        % }
        \# \textbf{\textit{Transformation function improvement step}} \;
        
        Update $f_{\phi^+}$ with Equation~\eqref{eq:forward-loss} \;
        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train inverse model $f^{-1}_{sim}$ \;
        %     Sample minibatch $B_l$ from $D_{sim}$ }} \;
        %     Update $f_{sim}$ with $CrossEntropy(f^{-1}_{sim}(s_t, s_{t+1|}), a_t)$ \;
        % }
        Update $h_{\phi^-}$ with Equation~\eqref{eq:inverse-loss}  \;
        
        Reset logged uncertainty $U^i = List()$ \;
        
        \For {e = 1, 2, \dots, \textcolor{red}{E}}
        {
        \# \textbf{\textit{Action grounding step}} \;
            \For {t = 1,2 ,\dots, T}
            {
            $a_t = \pi(s_t)$ \;
            $\hat{a}_t, u_t = g^u_{\phi}(s_t, a_t) = h^u_{\phi^-}(f_{\phi^+}(s_t, a_t), s_t)$\;
            \lIf{$u^e_t \geq \alpha $}  
                {$\hat{a}_t = a_t$ \textbf{\textit{\# Reject}}}
            $U.append(u^e_t)$ \;
            }
            \# \textbf{\textit{Policy improvement step}}\;
            Improve policy $\pi_{\theta}$ with reinforcement learning\;
        }  
       Update $\alpha$ with Equation~\eqref{eq:u-update} \;
    }
\end{algorithm} 

% \subsubsection{\textcolor{red}{Example: evidential deep learning (EDL) in \ours}}
% 4. We provide one example of an uncertainty modeling method (how we use uncertainty modeling here.




% In the Algorithm~\ref{} step, when we calculate grounding action from grounded action transformation function $g_{\phi}$, at the same time, we simultaneously calculate the uncertainty of taking this action. High uncertainty means by taking the grounding action; there is a high chance of making the next states in $E_{sim}$ diverge to the next state, which would happen in $E_{real}$. As a result, we reject to take it. And low uncertainty means that by taking grounding action, we have a high chance of pushing the next state in $E_{sim}$ and $E_{real}$ close. Introducing uncertainty-aware grounding action is beneficial to learn a policy in $E_{sim}$, which could be generalized to $E_{real}$.

% We set a dynamically changing threshold value $\alpha$ to decide whether to take a grounding action. In the first iteration, during the policy improvement step, we track the uncertainty of each simulation step and calculate the first quantile as $\alpha$. At this round, we do not take grounding actions. In the rest iterations, during the policy improvement step, we calculate grounding action with its uncertainty. If the uncertainty is less than the threshold $\alpha$, we take grounding action; if less, then not. And take the new first quantile as $\alpha$ for the next iteration. 

% Since the uncertainty model is highly correlated to the performance of the GAT method, we explored different state-of-the-art methods including {Methods}.

% \textcolor{red}{taking one and combine the above part for demonstration later.}

% % 1 / 6 pseudo code 

% \textcolor{blue}{strengthen how we use uncertainty with exploited various uncertainty quantification methods.}

% During the action grounding phase, due to the imperfection of the forward and inverse models, the grounded action is prone to enlarge instead of mitigating the difference of states between the simulation and real-life environment. A more severe situation is the grounding action with low belief will make this transformation behave like random exploration, resulting in instability during training. To quantify belief masses and uncertainty, we introduce a dynamically adjusted hyperparameter $\alpha$ to dynamically determine taking grounded action with a qualified uncertainty value during inference. In the framework, the uncertainty of $u^r_t$ at time $t$ after training round $r$ is quantified based on the Evidential Deep Learning method~\cite{EDL}. If $u^r_t < u^r$, where $u^r$ is the average uncertainty of all $u$ during the training round $r$, the model conducts action grounding. 
% At round $r$, assume for each epoch $n \in N$, containing $M$ steps, we can calculate $u_r$ from estimation on each step $u_i$:
% \begin{equation}
%     u_r = \frac{\sum_{n=1}^N{\sum_{i=1}^M} u_i }{N*M}
% \end{equation}
% Based on $\alpha = u_r$, if the uncertainty at round $t$: $u_t < u_r$, the model conducts action grounding. 
% Uncertainty quantification involves identifying the sources of uncertainty in the model, propagating the uncertainties through the model, and quantifying the uncertainty in the model output. This process can involve a variety of techniques, including statistical analysis, probabilistic modeling, and sensitivity analysis.

% \paragraph{\textcolor{red}{introduce one specific uncertainty and how we use it in our method, leave an open space to explore the uncertainty quantification methods}
% }


% 1/4

\subsubsection{\textcolor{red}{UGAT algorithm}}

Before starting the grounded action transformation, we train policy $\pi_{\theta}$ for $t = 100$ episodes in the simulation environment $E_{sim}$.  



\begin{algorithm}[h!]
\DontPrintSemicolon
\caption{Algorithm for \ours with model uncertainty quantification}
\label{algo:UGAT}
\KwIn{Initial policy $\pi_{\theta}$, forward model $f_{\phi^+}$, inverse model $h_{\phi^-}$, Real dataset $\mathcal{D}_{real}$, Sim dataset $\mathcal{D}_{sim}$, grounding rate $\alpha = \inf$}
\KwOut{Policy $\pi_{\theta}$, $f_{\phi^+}$, $h_{\phi^-}$}

    % \For {e = 1,2, \dots}{
    % \# \textbf{\textit{Improving initial policy  in $E_{sim}$}} \;
    %     Update policy $\pi$ in the $E_{sim}$ \;
    % }
    Pre-train policy $\pi_{\theta}$ for M iterations in $E_{sim}$ \;
    
	\For {i = 1,2, \dots, I}
	{
        % Reset $E_{real}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate real-world rollout with current $\pi$}} \;
        % Take actions with $a_t = \pi(s_t) $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{real}$ \;
        % }
        Generate real-world rollout $\tau^i_{real}$ with policy $\pi_{\theta}$ and add $\tau^i_{real}$ into $\mathcal{D}_{real}$ \;
        
        % Reset $E_{sim}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate simulation rollout with current $\pi$ }} \;
        % Take actions with $a_t = \pi(s_t)  $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{sim}$ \;
        % }
        Generate simulation rollout $\tau^i_{sim}$ with policy $\pi_{\theta}$, and add $\tau^i_{sim}$ into $\mathcal{D}_{sim}$ \\

        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train forward model $f_{real}$}} \;
        %     Sample minibatch $B_l$ from $D_{real}$ \;
        %     Update $f_{real}$ with $MSE(f_{real}(s_t,a_t), s_{t+1}$) \;
        % }
        \# \textbf{\textit{Transformation function improvement step}} \;
        
        Update $f_{\phi^+}$ with Equation~\eqref{eq:forward-loss} \;
        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train inverse model $f^{-1}_{sim}$ \;
        %     Sample minibatch $B_l$ from $D_{sim}$ }} \;
        %     Update $f_{sim}$ with $CrossEntropy(f^{-1}_{sim}(s_t, s_{t+1|}), a_t)$ \;
        % }
        Update $h_{\phi^-}$ with Equation~\eqref{eq:inverse-loss}  \;
        
        Reset logged uncertainty $U^i = List()$ \;
        
        \For {e = 1, 2, \dots, \textcolor{red}{E}}
        {
        \# \textbf{\textit{Action grounding step}} \;
            \For {t = 1,2 ,\dots, T}
            {
            $a_t = \pi(s_t)$ \;
            $\hat{a}_t, u_t = g^u_{\phi}(s_t, a_t) = h^u_{\phi^-}(f_{\phi^+}(s_t, a_t), s_t)$\;
            \lIf{$u^e_t \geq \alpha $}  
                {$\hat{a}_t = a_t$ \textbf{\textit{\# Reject}}}
            $U.append(u^e_t)$ \;
            }
            \# \textbf{\textit{Policy improvement step}}\;
            Improve policy $\pi_{\theta}$ with reinforcement learning\;
        }  
       Update $\alpha$ with Equation~\eqref{eq:u-update} \;
    }
\end{algorithm} 