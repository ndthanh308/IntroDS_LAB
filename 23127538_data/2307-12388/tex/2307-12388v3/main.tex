%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins

% \usepackage[bottom=0.5in]{geometry}
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{microtype}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{booktabs} % for professional 
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{longtable}
\usepackage{etoolbox}
\usepackage[ruled,linesnumbered,lined]{algorithm2e}
\usepackage{algpseudocode} 
\usepackage{hyperref}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother


\newcommand{\hua}[1]{{\bf\color{blue} [#1]}}
\newcommand{\hao}[1]{{\bf\color{red} [#1]}}

\newcommand{\longchao}[1]{{\bf\color{green} [#1]}}

\newcommand{\datasetFont}{\text}
\newcommand{\ours}{\datasetFont{UGAT}\xspace}

% \urldef{\myurl}\url{https://sumo.dlr.de/docs/Definition_of_Vehicles,_Vehicle_Types,_and_Routes.html}

\title{\LARGE \bf
Uncertainty-aware Grounded Action Transformation \\ towards Sim-to-Real Transfer for Traffic Signal Control
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Longchao Da, Hao Mei, Romir Sharma and Hua Wei% <-this % stops a space
% \thanks{This work was supported by xxxxx}% <-this % stops a space
\thanks{\boldmath Hua Wei with Longchao Da, Hao Mei, are at  
School of Computing and Augmented Intelligence (SCAI), Arizona State University, USA,
        {\tt\small \{hua.wei, longchao, hmei7\}@asu.edu.} Romir Sharma is with the West Windsor-Plainsboro High School South, West Windsor, USA, {\tt\small sharmaromir@gmail.com}}%
\thanks{The work was partially supported by NSF award \#2153311. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\input{abs}
% 1/4

\input{intro_new}

% TSC problem--> RL method good --> real-world gap --> sim2real 

% 1

\input{preliminary_new}

% 1. We formalize tsc as a reinforcement learning (RL) problem (Sutton and Barto 1998).

    

%2. Deinfe \pi_{sim, \theta} for sim2real use later: We assume π is parameterized by a vector θ and denote the
%parameterized policy as πθ

%3. Raise sim2real problem E_sim to E_real: In this paper, learning takes place in a simulator which is in environment, Esim, that models E. Specifically Esim has the same state-action space as E but inevitably a different dynamics distribution, Psim.. Thus θ with Jsim(θ) < Jsim(θ0) does not imply J(θ) < J(θ0) – in fact J(θ) could even be worse than J(θ0). In practice and in the literature, learning in a simulator frequently leads to catastrophic degradation of J. This paper explores methods for learning in Esim that result in lower policy cost. 


% 3/4 (together)
\vspace{-4mm}
\input{method_new}
% 2.5
% section title (report first)

% 1. introduce our sim and the real environment - CityLlow: low-fidelity, deterministic simulator(sim), SUMO: high-fidelity stochastic simulator (real), and how we construct a believable sim2real environment.
% 1.1 we used simulators to construct the environment of the simulator and real-world - this follows the procedure of past Sim2Real works. 

% 1.2 simulator introduction: why cityflow - sim, sumo -real

% 1. Inspired by grounded action transformation (GAT), how we solve the problem in TSC domain, (when explain the methods, use defined TSC actions/states/terms. 

% 3/4 - 1

% 3. uncertainty-induced grounded transformation
% 3.1 high probability of more exploration if taking grounding action if underfitting reward and inverse model
% (we take $\epsilon$-greedy exploration which $\epsilon$ = 0.1 while training policy. but if taking grounding action, the actual $\epsilon$ will be higher than the setting if our forward and inverse model is underfitting. (grounding action with less certainty equals random exploration). Thus learning will not be stable and will be hard to converge. 

% 3.2 we use two schemes to lower the grounding action rate to improve stability and improve the action transformation quality (high certainty action is allowed) 

% % 3.2.1 fixed 
% % we set $\alpha$ fixed as a threshold and grounding action when uncertainty at inference time is lower than this value.

% 3.2.2 dynamic 
% we track uncertainty at the last policy updating step and take the first quantile as a threshold $\alpha$ for the next iterations. Thus this threshold will be dynamically changing. We take grounding action when uncertainty is smaller than this threshold.

% strengthen how we use uncertainty with exploited various uncertainty quantification methods.
%1


% # introduce how uncertainty could help to benefit the GAT
% # justify without uncertainty and with uncertainty the action grounding effiencecy's improvement
% # what we applied (one example of uncertainty) briefly introduce
% # other uncertaity methods' exploratiob in experiments






\input{results}

% 1. experimental settings with explanation
% 2. main table showing the gap and improvement
% 
% 3. The ablation study shows each part of the method's effectiveness
% 4. The case study shows the usefulness on specific crossing
% 5. 


% 2.5
\vspace{-5mm}
\input{related}

\vspace{-4.5mm}
\input{conclusion_future}
\vspace{-5mm}
% \section*{ACKNOWLEDGMENTS}

% \bibliography{software}

% \input{bib}
\bibliographystyle{IEEEtran}
\vspace{-5mm}
\bibliography{software.bib}

\end{document}
