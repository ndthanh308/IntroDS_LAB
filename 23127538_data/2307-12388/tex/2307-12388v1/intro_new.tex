\section{Introduction}
Traffic Signal Control (TSC) is crucial for improving traffic flow, reducing congestion in modern transportation systems, and benefiting individuals and society as a whole. Traffic signal control remains an active research topic because of the high complexity of the problem. The traffic situations are highly dynamic and require traffic signal plans to adapt to different situations, making it necessary to develop effective algorithms that can adjust to changing traffic conditions.

Recent advances in reinforcement learning (RL) techniques have shown superiority over traditional approaches in TSC~\cite{noaeen2022reinforcement}. In RL, an agent aims to learn a policy through trial and error by interacting with an environment to maximize the cumulative expected reward over time. The biggest advantage of RL is that it can directly learn how to generate adaptive signal plans by observing the feedback from the environment. 


One major issue of applying current RL-based traffic signal control approaches in the real world is that these methods are mostly trained in simulation and suffer from the performance gap between simulation and the real world. 
Training in the simulation provides an efficient way to avoid the cost of learning RL-based policies in the real world. However, due to the high complexity of real-world dynamics, simulations are not always representative of real-world scenarios~\cite{jiang2021simgan}, which can limit the performance of RL-based TSC models in practice. For example, a traffic simulator might have default acceleration or deceleration settings for vehicles, while in the real world, the vehicle settings can vary widely depending on weather conditions, vehicle types, and many other factors. The inherent mismatches between simulation and real-world hinder RL-based models trained in simulation from achieving similar performance in the real world, as is shown in Figure~\ref{fig:intro}.




To close this gap, existing literature in TSC has been focusing on modifying the traffic simulator to better match the real world with real-world data~\cite{zhang2019cityflow} so that the policy or model can be transferred from simulation-to-real-world (sim-to-real) without much performance gap. However, in practice, the internal parameters of the simulator cannot be easily modified. To address this challenge, Grounded Action Transformation is a popular technique that seeks to induce simulator transitions to more closely match the real world. However, the current GAT technique is mainly applied to robotics, and few studies have been conducted on the traffic signal control problem. %Unlike most sim-to-real transfer techniques in robotics which mainly tackle the domain gap in visual representations, traffic signal control mainly faces a domain gap in the transition dynamics, as real-world traffic dynamics can be complex and hard to simulate accurately.
% Grounded Simulation Learning is a promising paradigm for addressing the sim-to-real problem in robotics which aims to modify the simulator to better match the real world based on data from the real world. However, in practice, the internal parameters of the simulator cannot be easily modified. To address this challenge, the state-of-the-art approach is Grounded Action Transformation (GAT). Unlike GSL, GAT performs grounding not by modifying the simulator but rather by augmenting it with a learned action transformer that seeks to induce simulator transitions that more closely match the real world. GAT has shown promising results in bridging the gap between simulation and reality, making it a popular technique for sim-to-real transfer in robotics.
%the state-of-the-art approach is Grounded Action Transformation (GAT)
%much effort has been made to transfer a learned policy or model from a simulated environment to a real-world environment with simulation-to-real-world (sim-to-real) transfer techniques~\cite{}. Among them, 


In this paper, we present Uncertainty-aware Grounded Action Transformation (\ours), an approach that bridges the domain gap of transition dynamics by dynamically transforming actions in the simulation with uncertainty. 
\ours learns to mitigate the discrepancy between the simulated and real-world dynamics under the framework of grounded action transformation (GAT), which learns an inverse model that can generate an action to ground the next state in the real world with a desired next state predicted by the forward model learned in simulation. Specifically, to avoid enlarging the transition dynamics gap induced by the grounding actions with high uncertainty, \ours dynamically decides when to transform the actions by quantifying the uncertainty in the forward model. Our experiments demonstrate the existence of the performance gap in traffic signal control problems and further show that \ours has a good performance in mitigating the gap with higher efficiency and stability.

% Figure environment removed
