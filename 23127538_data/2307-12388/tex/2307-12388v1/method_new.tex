\section{Methods}

% To mitigate the gap in the transition dynamics between the simulation and the real world, we propose an uncertainty

% \textcolor{blue}{we use \ours to mitigate the gap between sim and real transition dynamics and add into uncertainty to help mitigate the transformation model overconfidence problem and improve its performance.}
To mitigate the gap in the transition dynamics between traffic simulations and real-world traffic systems, we use the vanilla GAT and analyze its limitations. To overcome the problem in vanilla GAT, we 
propose \ours to further leverage uncertainty quantification to take grounded action dynamically. %In the rest of this section, we will illustrate GAT in TSC problem, \ours, and its corresponding implementations. 

\subsection{Vanilla GAT for TSC}

% Though TSC problem has been studied a lot recently and many researches have shown RL-based method has achieved promising results, there are still concerns about the these methods performances when they are deployed in real life. We will discuss current RL-based researches limitation on generalizability and propose an effective way to mitigate this problem in the below paragraphs. 
% % \textcolor{blue}{introduce the problem of transition dynamic gaps in TSC in the past work}

% % 1. there is a dynamic transition gap existing between sim and real in TSC. TSC exploration is at a high cost, so we use the simulation. 

% \subsubsection{\textcolor{red}{Generalization problem in past RL-based Traffic signal control}}

 % The gap between simulation and the real world hinders the application of RL-based  methods in the real world.
% 3. To improve the policy trained in the simulation and generalize it to the real-word, we need to modify the simulationâ€˜s $\mathcal{M}_{sim}$ to make it higher fidelity and close to real-world $\mathcal{M}_{real}$.
% To establish a closer resemblance between the simulation and the physical world, minimizing the disparity between the transition dynamics in both domains is essential. The procedure enabling the transition dynamics in simulation $P_{\phi}$ close to the real-world transition dynamics $P^*$ is commonly called grounding.



% \textcolor{green}{formula to define forward and inverse model}

% \subsubsection{\textcolor{red}{GAT for Traffic Signal Control}}


% \noindent1. Initialize policy $\pi_0$, real-world traffic dataset $\mathcal{D}_{real}$, simulation traffic dataset $\mathcal{D}_{sim}$, and train policy in the $E_{sim}$ till converge. 

% \noindent2. Execute policy $\pi_i$ to control traffic light in both $E_{sim} \text{ and } E_{real}$ and append collected traffic states and actions (phases) pairs ${(s^i_0, a^i_0), \dots, (s^i_t, a^i_t)}$ into $\mathcal{D}_{real}$ and $\mathcal{D}_{sim}$.


% \noindent3. Optimize action transformation function $g_{\phi} = f^{-1}_{sim}(s^i_t, f_{real}(s^i_t, a^i_t))$ to find the $\phi^*$ which minimize equation~\ref{eq:gat}. 

% \noindent4. Optimize policy $\pi_i$ in $E_{sim}$. At each simulation step $t$, if there is an $a_t^* = \arg \max_{a_{t}} Q(s_t, a_t)$ returned from $\pi_i$, replace $a_t^*$ with grounded action $\hat{a}_t = g_{\phi}(s_t, a^*_t)$.

% \noindent5. After optimization, return updated policy $\pi_{i+1}$ got to the step 2 until the policy did not improve in the $E_{real}$.


We use the vanilla GAT for the traffic signal control problem by specifying the learning of $f_{\phi^+}$ and $h_{\phi^-}$: 

\noindent $\bullet$ \textit{The forward model} $f_{\phi^+}(s_t, a_t)$ in traffic signal control problem predicts the next traffic state $\hat{s}_{t+1}$ in the real world given taken action $a_t$ and the current traffic state $s_t$. We approximate $f_{\phi^+}$ with a deep neural network and optimize $\phi^+$ by minimizing the Mean Squared Error (MSE) loss:
\begin{equation}
\label{eq:forward-loss}
     \mathcal{L}(\phi^+) = MSE(\hat{s}^i_{t+1}, s^i_{t+1}) = MSE(f_{\phi^+}(s^i_t, a^i_t), s^i_{t+1})
\end{equation}
where $s^i_t$, $a^i_t$, $s^i_{t+1}$ are sampled from the trajectories collected from $E_{real}$.

\noindent $\bullet$ \textit{The inverse model} $h_{\phi^-}(\hat{s}_{t+1}, s_t)$ in traffic signal control predicts the grounded action $\hat{a}^i_t$ in simulation $E_{sim}$ to reproduce the same traffic states $\hat{s}_{t+1}$. 
We approximate $h_{\phi^-}$ with a deep neural network and optimize $\phi^-$ by minimizing the Categorical Cross-Entropy (CCE) loss since the target $a^i_t$ is a discrete value:
\begin{equation}
\label{eq:inverse-loss}
    \mathcal{L}(\phi^-) = CCE(\hat{a}^i_t, a^i_t) = CCE(h_{\phi^-}(s^i_{t+1}, s^i_t), a^i_t)
\end{equation}
where $s^i_t$, $a^i_t$, $s^i_{t+1}$ are sampled from the trajectories collected from $E_{sim}$.

% Details of \ours are shown in Algorithm~\ref{algo:UGAT}.

% an inverse model $f^{-1}_{sim}(\hat{s}_{t+1}, s_t)$, which predicts the grounded action in the simulation to reproduce the same traffic states $\hat{s}_{t+1}$ in the real world. Thus the optimization on parameter $\phi$ could be formulated as a supervised learning problem, where the forward model is trained with the gradient from $MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})$ by data collected from real-world. And the inverse model is trained with the gradient from $CrossEnropy(f^{-1}_{sim}(\hat{s}^i_{t+1}, s^i_t), a^i_t)$ with data collected from low-fidelity simulation environment. Details of UGAT are shown in Algorithm~\ref{}

% 4. How TSC could benefit from grounded action transformation.


% Following the definition in the preliminary. We optimize our policy $\pi$ in the $E_{sim}$ with MDP $\mathcal{M}_{sim} = \left \langle \mathcal{S, A, r, P_{sim}}, \pi, \gamma \right \rangle$ to archive optimal in $E_{real}$: 

% \begin{align}
%     \pi^* &= \arg \max_{\pi} \eta_{P_{real}}(\pi), \\
%     \eta_{P_{real}}(\pi) &= \mathbb{E}_{\tau\sim(\pi)} \left[ R(\tau) \right] 
% \end{align}

% We achieve this by taking grounded action to make trajectory $\tau$ in $E_{sim}$ close to $E_{real}$:

% \begin{equation}
% \phi^* = \arg \min \sum_{\tau_{i} \in \mathcal{D}} \sum^{L}_{t=0} d(P(s_{t+1}^{i}| s^i_t, a^i_t), P_{\phi}(s^i_{t+1}|s^i_t, a^i_t))
% \end{equation} 

% \textcolor{red}{introduce gat module and how }


% \textcolor{red}{a picture to show how grounded action find $\theta^*$ }
% % 1/6
% % Figure environment removed


% where $d(c\dot)$ is the distance between two distributions, $s^i_t \text{ and } s^i_{t+1} \in \mathcal{S}$ are current state and next time state in state space, $a^i_t \in \mathcal{A}$ is the action taken at current time step belong to action space.

% To find $\phi$ efficiently, GAT uses parameterized action transformation function, which takes in agents' states and actions and outputs new actions. The new actions, grounded actions, will result in the agents in the $E_{sim}$ transitioning close to the next states, which would happen in $E_{real}$ given the current states and actions. This transition is called the grounded procedure.

% In the GAT grounding procedure is achieved by training an action transformation function with supervised learning:
% \begin{equation}
% g_{\phi}(s^i_t, a^i_t) = f^{-1}_{sim}(s^i_t, f_{real}(s^i_t, a^i_t))
% \end{equation}
% % where $f^{-1}_{sim}() \text{ and } f_{real}()$ are forward and inverse models explained below:


% Our implementation of GAT in TSC follows the steps described below:

% \begin{itemize}
%     \item[1] First train $\pi$ in the $E_{sim}$ to improve and get initial policy $\pi^0$.
%     \item[2] Execute policy $\pi$ in $E_{real}$ and $E_{sim}$ to collect data $D_{sim}$ and $D_{real}$
%     \item[3] train forward transition model $f_{real}$ with $D_{real}$.
%     \item[4] Train inverse transition model $f^-1_{sim}$ with $D_{sim}$.
%     \item[5] Update policy $\pi$ in $S_{sim}$ 
%     \item[6] iteratively run between steps 2-5 till performance stop improving in $E_{real}$
    
% \end{itemize}


% The forward model is trained with the gradient from $MSE(f_{real}(s^i_t, a^i_t), s^i_{t+1})$ by data collected from real-world. The inverse model is trained with the gradient from $CrossEnropy(f^{-1}_{sim}(s^i_{t+1}, s^i_t), a^i_t)$ with data collected from low-fidelity simulation environment.

% \textcolor{blue}{Introduce the problem that we have, and make the connection with what GAT is trying to solve, then we changed something of GAT to fit the task.
% (when explaining the methods, use defined TSC actions/states/terms. }

\subsection{Uncertainty-aware GAT}
In this section, we will introduce the limitations of the vanilla GAT and propose an uncertainty-aware method on GAT that can benefit from quantifying model uncertainty.

% \textcolor{blue}{introduce the existing GATs' flaws and challenges, and then introduce how we think about uncertainty could address such a challenge}

\subsubsection{\textbf{Model Uncertainty on $g_{\phi}$}}

% \textcolor{red}{add in Annotation with description}
% 1. Current \ours has model uncertainty and overconfidence in some actions. This will result in enlarging the gap instead of mitigating it. Thus learning is not stable and hard to converge.

The vanilla GAT takes supervised learning to train the action transformation function $g_{\phi}$, and grounded action transformation $\hat{a}$ is taken at each step while improving in the $E_{sim}$. However, the action transformation function $g_{\phi}$ could have high model uncertainty on unseen state and action inputs, which is likely to happen during the exploration of RL. With high model uncertainty on $g_{\phi}$, the grounded action $\hat{a}$ in Equation~\eqref{eq:gat} is likely to enlarge the performance gap instead of mitigating it if the high uncertainty action is taken because it will make policy learning unstable and hard to converge.


% \textcolor{red}{uncertatinty decrease performance of }
% To overcome this problem, inspired by uncertainty-aware control methods (reference here), we proposed an uncertainty-induced action transformation mechanism that will only execute grounded action while the inference has high certainty. By doing so, we can 1) keep the exploration rate close to the original $\epsilon$ and thus improve the policy learning step in the GAT framework. 2) By doing grounded action only at high certainty, the new actions will only be taken in $E_{sim}$ when the resulting next state is closed to the next state, which would be reached in $E_{real}$.


 % \textcolor{red}{model uncertainty exists, overconfident- we introduce uncertainty to avoid overconfident}
% 2. Using uncertainty quantification to evaluate model uncertainty can help distinguish these actions and avoid these grounded actions with high model uncertainty.

To overcome the enlarged gap induced by $\hat{a}$ with high model uncertainty in $g_{\phi}$, we need uncertainty quantification methods~\cite{kabir2018neural} to keep track of the uncertainty of $g_{\phi}$. Specifically, we would like the action transformation function to output an uncertainty value $u_t$ in addition to $\hat{a}_t$:
\begin{equation}
\label{eq:ugat-uncertainty}
    \hat{a}_t, u_t = g_{\phi}(s_t, a_t) = h_{\phi^-}(f_{\phi^+}(s_t, a_t), s_t)
\end{equation}

In general, any methods capable of quantifying the uncertainty of a predicted class from a deep neural network (since $h_{\phi^-}$ is implemented with deep neural networks) could be utilized, like evidential deep learning (EDL), Concrete Dropout~\cite{gal2017concrete}, Deep Ensembles~\cite{lakshminarayanan2017simple}, etc. In this paper, we explored different state-of-the-art uncertainty quantification methods and found out that they all perform well with our method (their experimental results can be found in Section~\ref{sec:exp:uncertainty}). We adopted EDL as the default in our method as it performs the best with our method.

Intuitively, during action grounding, whenever model $g_{\phi}(s_t, a_t)$ returns a grounded action $\hat{a}_t$, if the uncertainty $u_t$ is less than the threshold $\alpha$, the grounded action $\hat{a}_t$ will be taken in the simulation environment $E_{sim}$ for policy improvement; otherwise, we will reject $\hat{a}_t$ and take the original $a_t$. This uncertainty quantification allows us to evaluate the reliability of the transformation model and take grounded actions $\hat{a}$ when the model is certain that the resulting transition $P_{\phi}(s_t, \hat{a}_t)$ would mirror that of the real-world environment $E_{real}$ transition $P^*(s_t, a_t)$. This process enables us to minimize the gap in Equation~\eqref{eq:gat} between the policy training environment $E_{sim}$ and the policy testing environment $E_{real}$, thereby mitigating the performance gap. 


%Details are described in Algorithm~\ref{algo:UGAT}.

% and propose an uncertainty-aware GAT method that will only execute grounded action $\hat{a}$ while the inference $g_{\phi}(s_t, a_t)$ has low model uncertainty. Such uncertainty quantification allows us to 1) take grounded action only at low uncertainty. 2) stabilize the policy improvement and converge the learning policy. 

% Introducing uncertainty quantification allows us to evaluate the reliability of the transformation model and take grounded actions $\hat{a}$ when the model is certain that the resulting transition $P_{\phi}(s_t, \hat{a}_t)$  would mirror that of the real-world environment $E_{real}$ transition $P^*(s_t, a_t)$. This process enables us to minimize the gap in Equation~\eqref{eq:gat} between the simulation environment $E_{sim}$ and the real-world environment $E_{real}$, thereby improving the fidelity of the simulation. Details are described in Algorithm~\ref{algo:UGAT}.

% 3. We improve GAP-TSC to uncertainty-aware \ours to mitigate the dynamic gaps and, at the same time, achieve stable learning. We keep track of uncertainty and plugin the uncertainty into GAT. (Details description of how we set alpha)

% To make GAP-TSC avoid executing grounded action $a_grounding$ while model uncertainty is high, we extend our framework to uncertainty-aware \ours. 

% \subsubsection{\textbf{Choice of Uncertainty Model}}
% Since the uncertainty model is highly correlated to the performance of the uncertainty method, we explored different state-of-the-art uncertainty quantification methods and found UGAT performs well across different methods. We use the evidential deep learning (EDL) method to demonstrate how we integrate the uncertainty model into the inverse model of \ours.

% % Inspired by the Theory of Evidence (DST) \cite{TheoryE}, Subjective Logic (SL) allows one to quantify belief masses and uncertainty through a well-defined theoretical framework and the overall uncertainty mass of $u$ can be written as $u = \frac{K}{S}$,  where $ S = \sum_{i=1}^K (e_i+ 1) $, evidence $e_i$ is a measure of the amount of support collected from data in favor of a sample to be classified into a certain class and $K$ refers to mutually exclusive class labels $k = 1, . . . , K$.

% Evidential deep learning (EDL)~\cite{sensoy2018evidential} is an uncertainty quantification method from the theory of evidence perspective for classification problems, quantifying belief mass and uncertainty. Specifically, in a K-class multi-classification problem, subjective logic (SL) considers class labels mutually exclusive singletons by providing belief mass $b_k$ for each class $k$. And it, at the same time, provides an overall uncertainty mass of $u$. The relationship between belief mass and uncertainty is: 
% \begin{equation}
%     u = \frac{K}{S} \text{ and } b_k = \frac{e_k}{S}
% \end{equation}
% where $S = \sum_{i=1}^K(e_i +1)$. In the EDL, the $Softmax$ activation on the output layer of a neural network is replaced with $ReLU$ to calculate the evidence vector $[e_1, e_2, \dots, e_K ]$ for each class. And the uncertainty is calculated by:
% \begin{equation}
% \label{eq:edl}
%     u = \frac{K}{\sum_{i =1}^K (e_i +1)}
% \end{equation}


% To track the model uncertainty of grounded transformation function $g_{\phi}(\cdot, \cdot)$, we see the action space $\mathcal{A}$ as classes and integrate uncertainty into the inverse model $h_{\phi^-}(\hat{s}_{t+1}, s_t)$ to quantify the uncertainty of output grounded action $\hat{a} \in \mathcal{A}$. At the last layer for predicting the output $logits$, we use $ReLU$ as the activation layer \cite{sensoy2018evidential} to calculate the evidence. Model uncertainty $u$ can be easily computed from Equation~\eqref{eq:edl}. After adding EDL to the inverse model, we modify the inverse model into:
% \begin{equation}
% \label{eq:update inverse}
%     \hat{a}_t, u_t =  h^u_{\phi^-}(\hat{s}_{t+1}, s_t)
% \end{equation}
% and grounded transformation could is modified into:
% \begin{equation}
%     \hat{a}_t, u_t = g_{\phi}(s_t, a_t) = h_{\phi^-}(f_{\phi^+}(s_t, a_t), s_t)
% \end{equation}


\subsubsection{\textbf{Dynamic Grounding Rate $\alpha$}}
The threshold $\alpha$, which we referred to as the grounding rate,  helps us to decide when to filter out $\hat{a}_t$ with uncertainty $u_t$. One naive approach of deciding the grounding rate $\alpha$ is to treat it as a hyperparameter for training and keep it fixed during the training process. However, since $g_{\phi}(s_t, a_t)$ keeps being updated during the training process, the model uncertainty of $g_{\phi}$ is dynamically changing. Even with the same $s_t$ and $a_t$, the output $u_t$ and $\hat{a}_t$ from $g_{\phi}(s_t, a_t)$ could be different in different training iterations. 

An alternative yet feasible approach is to set grounding rate $\alpha$ dynamically changing with the model uncertainty during different training iterations.
% Hereby, we use an $\alpha$ to represent the grounding rate which is the threshold for taking $\hat{a}$. In the uncertainty-aware \ours, we dynamically adjust $\alpha$, since the action transformation model $g_{\phi}(\cdot, \cdot)$ is updating; as a result, the model uncertainty will adjust with the learning sample increasing. A fixed grounding rate cannot synchronously adjust with the updating $g_{\phi}(\cdot, \cdot)$, which would not be able to achieve the regularization on grounded action $\hat{a}$. Also, a dynamically adjusted grounding rate will release the efforts on hyper-parameter tuning. \hua{gabbish}
To dynamically adjust the grounding rate with the changing of model uncertainty, we keep track of the model uncertainty $u_t$ of $g_{\phi}(s_t, a_t)$ during each training iteration. 
 % there is a high chance of making the next states $s_{t+1}$ in $E_{sim}$ diverge to the next state $\hat{s}_{t+1}$, which would happen in $E_{real}$. As a result, we reject to take it. And low uncertainty means that by taking $\hat{a}_t$, we have a high chance of pushing the next state in $E_{sim}$ and $E_{real}$ close. 
%When we work in the uncertainty-aware \ours framework, in the first iteration, during the policy improvement step, whenever model $g_{\phi}(s_t, a_t)$ returns a grounded action $\hat{a}_t$, we track the model uncertainty $u_t$ and take the original action $a_t$. 
At the end of each iteration $i$, we update the grounding rate $\alpha$ for the next iteration based on the past record of model uncertainty by calculating the mean
\begin{equation}
\label{eq:u-update}
    \alpha = \frac{\sum^E_{e=1} \sum^{T-1}_{t=0} u^e_t}{T \times E}
\end{equation}
from the logged uncertainties in the last $E$ epochs.
This dynamic grounding rate $\alpha$ can synchronously adjust $\alpha$ with the update of $g_{\phi}$ and relief efforts on hyper-parameter tuning.


% \subsection{\textcolor{red}{Implementations}}
% In this section, we introduce the detailed implementation of model uncertainty quantification and dynamically adjusted grounding rate.



% % In the policy improvement step, at round $r$, assume for each epoch $i \in N$, containing $T$ steps, we can calculate $u_r$ from estimation on each step $u_i$:
% % \begin{equation}
% %     u_r = \frac{\sum_{n=1}^N{\sum_{i=1}^M} u_i }{N*M}
% % \end{equation}
% % And then, we update the parameter $\alpha = u_r$; if the uncertainty at round $t$: $u_t < u_r$, the model conducts action grounded. 

% \subsubsection{\textcolor{red}{Calculation of Grounding Rate}}



% Thus introducing uncertainty-aware grounding action is beneficial to learn a policy in $E_{sim}$, which could be generalized to $E_{real}$.

% \textcolor{red}{why dynamically change $\alpha$}

% \textcolor{red}{fixed cannot tune with updated transformation model. no need to tune hyper-parameter  }
%  In the first iteration, during the policy improvement step, we track the uncertainty of each simulation step and calculate the first quantile as $\alpha$. At this round, we do not take grounded actions. In the rest iterations, during the policy improvement step, we calculate grounded action with its uncertainty. If the uncertainty is less than the threshold $\alpha$, we take grounded action; if less, then not. And take the new first quantile as $\alpha$ for the next iteration. 

% Since the uncertainty model is highly correlated to the performance of the uncertainty quantification method, we explored different state-of-the-art methods, including {Methods}.



% \subsubsection{\textcolor{red}{Example: evidential deep learning (EDL) in \ours}}
% 4. We provide one example of an uncertainty modeling method (how we use uncertainty modeling here.




% In the Algorithm~\ref{} step, when we calculate grounded action from grounded action transformation function $g_{\phi}$, at the same time, we simultaneously calculate the uncertainty of taking this action. High uncertainty means by taking the grounded action; there is a high chance of making the next states in $E_{sim}$ diverge to the next state, which would happen in $E_{real}$. As a result, we reject to take it. And low uncertainty means that by taking grounded action, we have a high chance of pushing the next state in $E_{sim}$ and $E_{real}$ close. Introducing uncertainty-aware grounding action is beneficial to learn a policy in $E_{sim}$, which could be generalized to $E_{real}$.

% We set a dynamically changing threshold value $\alpha$ to decide whether to take a grounded action. In the first iteration, during the policy improvement step, we track the uncertainty of each simulation step and calculate the first quantile as $\alpha$. At this round, we do not take grounded actions. In the rest iterations, during the policy improvement step, we calculate grounded action with its uncertainty. If the uncertainty is less than the threshold $\alpha$, we take grounded action; if less, then not. And take the new first quantile as $\alpha$ for the next iteration. 

% Since the uncertainty model is highly correlated to the performance of the GAT method, we explored different state-of-the-art methods including {Methods}.

% \textcolor{red}{taking one and combine the above part for demonstration later.}

% % 1 / 6 pseudo code 

% \textcolor{blue}{strengthen how we use uncertainty with exploited various uncertainty quantification methods.}

% During the action grounding phase, due to the imperfection of the forward and inverse models, the grounded action is prone to enlarge instead of mitigating the difference of states between the simulation and real-life environment. A more severe situation is the grounded action with low belief will make this transformation behave like random exploration, resulting in instability during training. To quantify belief masses and uncertainty, we introduce a dynamically adjusted hyperparameter $\alpha$ to dynamically determine taking grounded action with a qualified uncertainty value during inference. In the framework, the uncertainty of $u^r_t$ at time $t$ after training round $r$ is quantified based on the Evidential Deep Learning method~\cite{EDL}. If $u^r_t < u^r$, where $u^r$ is the average uncertainty of all $u$ during the training round $r$, the model conducts action grounding. 
% At round $r$, assume for each epoch $n \in N$, containing $M$ steps, we can calculate $u_r$ from estimation on each step $u_i$:
% \begin{equation}
%     u_r = \frac{\sum_{n=1}^N{\sum_{i=1}^M} u_i }{N*M}
% \end{equation}
% Based on $\alpha = u_r$, if the uncertainty at round $t$: $u_t < u_r$, the model conducts action grounding. 
% Uncertainty quantification involves identifying the sources of uncertainty in the model, propagating the uncertainties through the model, and quantifying the uncertainty in the model output. This process can involve a variety of techniques, including statistical analysis, probabilistic modeling, and sensitivity analysis.

% \paragraph{\textcolor{red}{introduce one specific uncertainty and how we use it in our method, leave an open space to explore the uncertainty quantification methods}
% }


% 1/4
\subsection{Training Algorithm}
The overall algorithm for \ours is shown in Algorithm~\ref{algo:UGAT}.
% We first initialize a policy with the DQN algorithm~\cite{wei2018intellilight}, a feed-forward neural network as the forward model, and another feed-forward neural network as the inverse model with $ReLU$ activation for the last layer to output uncertainty $u$. And we initialize a real-world dataset $D_{real}$, a simulation dataset $D_{sim}$, and a dynamic grounding rate $\alpha$ as $\inf$.
We firstly pre-train the RL policy $\pi_{\theta}$ for $M$ epochs in the simulation environment $E_{sim}$. Then each training iteration of \ours starts with collecting datasets for $E_{sim}$ and $E_{real}$. Note that here we follow the data collection process in~\cite{hanna2017grounded} while the data collection in $E_{real}$ does not necessarily happen in the training process and could be done from existing offline logged data. With the collected data, we can update $g_{\phi}$ by training the forward model $f_{\phi^+}$ and inverse model $h_{\phi^-}$. With the updated $g_{\phi}$, we start to use the policy $\pi_\theta$ to interact with $E_{sim}$ for policy training. Before the action $a_t$ outputted by $\pi_\theta(s_t)$ is taken into the environment $E_{sim}$, \ours grounds the actions through $\hat{a}_t$ and $u_t$ from $g_{\phi}(s_t, a_t)$. If the model uncertainty $u_t$ is greater than the grounding rate $\alpha$, the grounded action $\hat{a}_t$ is rejected and we execute origin action $a_t$ in the simulation $E_{sim}$. Then $u_t$ is added into logged uncertainty $U$. The RL policy $\pi_{\theta}$ updates during the interaction with $E_{sim}$. After $E$ rounds of intersections, we update $\alpha$ with Equation~\eqref{eq:u-update} for the next round of policy training.


\begin{algorithm}[h!]
\DontPrintSemicolon
\caption{Algorithm for \ours with model uncertainty quantification}
\label{algo:UGAT}
\KwIn{Initial policy $\pi_{\theta}$, forward model $f_{\phi^+}$, inverse model $h_{\phi^-}$, real-world dataset $\mathcal{D}_{real}$, simulation dataset $\mathcal{D}_{sim}$, grounding rate $\alpha = \inf$}
\KwOut{Policy $\pi_{\theta}$, $f_{\phi^+}$, $h_{\phi^-}$}

    % \For {e = 1,2, \dots}{
    % \# \textbf{\textit{Improving initial policy  in $E_{sim}$}} \;
    %     Update policy $\pi$ in the $E_{sim}$ \;
    % }
    Pre-train policy $\pi_{\theta}$ for M iterations in $E_{sim}$ \;
    
	\For {i = 1,2, \dots, I}
	{
        % Reset $E_{real}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate real-world rollout with current $\pi$}} \;
        % Take actions with $a_t = \pi(s_t) $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{real}$ \;
        % }
        Rollout policy $\pi_{\theta}$ in $E_{sim}$ and add data to $\mathcal{D}_{sim}$ \;
        
        % Reset $E_{sim}$ \;
        % \For {t = 1,2, \dots}
        % {
        % \# \textbf{\textit{Generate simulation rollout with current $\pi$ }} \;
        % Take actions with $a_t = \pi(s_t)  $ \;
        % Add $(s_t, a_t, s_{t+1})$ into $\mathcal{D}_{sim}$ \;
        % }
        Rollout policy $\pi_{\theta}$ in $E_{real}$ and add data to $\mathcal{D}_{real}$ \\

        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train forward model $f_{real}$}} \;
        %     Sample minibatch $B_l$ from $D_{real}$ \;
        %     Update $f_{real}$ with $MSE(f_{real}(s_t,a_t), s_{t+1}$) \;
        % }
        \# \textbf{\textit{Transformation function update step}} \;
        
        Update $f_{\phi^+}$ with Equation~\eqref{eq:forward-loss} \;
        % \For {l = 1,2, \dots} {
        % \# \textbf{\textit{Train inverse model $f^{-1}_{sim}$ \;
        %     Sample minibatch $B_l$ from $D_{sim}$ }} \;
        %     Update $f_{sim}$ with $CrossEntropy(f^{-1}_{sim}(s_t, s_{t+1|}), a_t)$ \;
        % }
        Update $h_{\phi^-}$ with Equation~\eqref{eq:inverse-loss}  \;
        
        Reset logged uncertainty $U^i = List()$ \;
        
        \# \textbf{\textit{Policy training}}\;
        \For {e = 1, 2, \dots, E}
        {
        \# \textbf{\textit{Action grounding step}} \;
            \For {t = 0, 1 ,\dots, T-1}
            {
            $a_t = \pi(s_t)$ \;
            Calculate $\hat{a}_t$ and $u_t$ with Equation~\eqref{eq:ugat-uncertainty} \;
            % $\hat{a}_t, u_t = g^u_{\phi}(s_t, a_t) = h_{\phi^-}(f_{\phi^+}(s_t, a_t), s_t)$\;
            \If{$u^e_t \geq \alpha $ }   
                {
                
                $\hat{a}_t = a_t$  \textbf{\textit{\# Reject grounded action}}
                }
            $U.append(u^e_t)$ \;
            }
            \# \textbf{\textit{Policy update step}}\;
            Improve policy $\pi_{\theta}$ with reinforcement learning\;
        }  
       Update $\alpha$ with Equation~\eqref{eq:u-update} \;
    }
\end{algorithm} 