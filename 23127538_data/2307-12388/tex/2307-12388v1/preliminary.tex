\section{Preliminaries}

% \subsection{\textcolor{red}{Formalize the sim-to-real TSC problems}}


This section will formalize the traffic signal control (TSC) problem and its RL solutions and characterize the sim-to-real issues behind them.

\subsection{\textcolor{red}{Concepts of TSC and  RL Solutions}}
    % \paragraph{\textcolor{red}{Traffic Signal Control(TSC) problem define}}

    % \textcolor{blue}{Given a set of traffic demands at an intersection, the objective of TSC details...
    
    % . }
We consider the traffic signal control problem as a Markov decision process (MDP). Each intersection is controlled by an agent and based on the current state of this intersection. The agent will proactively choose an action for the next time interval $\Delta t$ to minimize the average queue length on lanes around this intersection. The chosen action is a phase describing which intersection traffic movements are allowed. Following existing work~\cite{}, we define this TSC problem as an MDP which could be characterized by $\mathcal{M}  = \langle \mathcal{S}, \mathcal{A}, P, r, \gamma \rangle$ where each component is described below:

\begin{itemize}
    \item System state space $\mathcal{S}$: The traffic state space consists of the number of vehicles on each lane belonging to the intersection and the current phase of the intersection in the form of the one-hot code. Thus the state at time t could be represented as $s_t \in \mathcal{S}$.
    \item Set of action space $\mathcal{A}$: In this paper, we define the action as the phase for the next time interval $\Delta t$. The action space $\mathcal{A}$ is discrete since in practice, the phases are a set of pre-defined combinations of traffic movements that do not conflict while passing through the intersection. %The action $a_t \in \mathcal{A}$ at time $t$ is discrete, indicating a certain phase from a set of predefined phases $\mathcal{A}$ for the intersection from $t$ to $t+\Delta t$.
    \item Transition probability distribution $P$: Given current state $s_t \in \mathcal{S}$ and action $a_t \in \mathcal{A}$, the transition probability distribution $P(s_{t+1}| s_t, a_t)$ maps $\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, describing the probability distribution of next state $s_{t+1} \in \mathcal{S}$.
    \item Reward r: The reward is typically a scalar return from the environment which can be described as $\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The reward we used in the traffic signal control problem is an immediate return from the environment. The reward at time t could be calculated from $r_t = -\sum_l w_t^l$, where $l$ is the lane belonging to the intersection and $w_t^l$ is the queue length at each lane.
    \item Policy $\pi_{\theta}$ and discount factor $\gamma$: The policy $\pi_\theta$ parameterized by $\theta$ aims to  minimize the expected queue length $r_t$. Specifically, it maps the given state $s_t$ to the action $a_t$ taken at this time step. It could be represented $\pi_{\theta}: \mathcal{S} \rightarrow \mathcal{A}$.
\end{itemize}
An RL approach solves this problem by maximizing the long-term expectation of discounted accumulation reward adjusted by discount factor $\gamma$. The discounted accumulated reward is $\mathbb{E}_{(s_t, a_t)\sim (\pi_{\theta},\mathcal{M})}[ \sum_{t=0}^T \gamma^{T-t} r_t(s_t, a_t) ]$. In the past RL-based TSC works, the above-described procedure is conducted in the simulation environment $E_{sim}$

\subsection{\textcolor{red}{Domain Gap between Simulation and Real-world}}

Domain gap impedes the generalization of models trained in the source domain to the target domain. In RL, the gap between the simulation (the source domain to improve a policy) and the real world (the target domain where trained policy is implemented) hinders most research transacted from the simulation into the real world concerning the cost and public safety. Here we define the sim-to-real transfer problem as below:
\begin{itemize}
    \item Modeling simulation and real-world environment: We first define simulation and real-world environment as $E_{sim}$ and $E_{real}$. We consider the two environments to share the same MDP except for the transition dynamics characterized by the transition probability distribution. The MDP in $E_{sim}$ could be characterized as $\mathcal{M}_{sim}  = \langle \mathcal{S}, \mathcal{A}, P_{\phi}, r, \gamma \rangle$, where the simulation's transition dynamic is controlled by a set of parameters $\phi$.
    And the MDP in $E_{real}$ could be characterized as $\mathcal{M}_{real}  = \langle \mathcal{S}, \mathcal{A}, P^*, r, \gamma \rangle$, where $P^*$ is the transition dynamics in the real world.
\end{itemize}
The goal of sim-to-real transfer is to find the best policy $\pi_{\theta^*}$ such that:
    \begin{equation}
        \pi_{\theta^*} = \arg \max_{\pi_{\theta}} \eta_{P^*}(\pi_{\theta}), P^* \neq P_{\phi}
    \end{equation}
    where $\eta_P(\pi_{\theta})$ is used to evaluate the policy's performance in the $E_{real}$:
    \begin{equation}
        \eta_{P}(\pi_{\theta}) = \mathbb{E}_{(s_t,a_t) \sim (\pi_{\theta}, \mathcal{M}_{real}}) [ \sum^T_{t=0} \gamma^{T-t}r_t(s_t, a_t) ]
    \end{equation}


     % \paragraph{\textcolor{blue}{Describe why the gap exists with reality examples and refer to our preliminary study results to justify the problem's seriousness. And mathematically model how the gap appears and causes problems.}}  \\
    


     % \paragraph{}

% \subsection{Potential Solutions}
% Domain Randomization, Transfer Learning, Data Augmentation, Multi-Task Learning, etc.