\section{Experiments}


% We evaluate performance on multiple downstream tasks.
\subsection{Experimental Setup}
We evaluate our model on four datasets: CIFAR-10~\cite{cifar10}, FFHQ-256~\cite{DBLP:conf/cvpr/KarrasLA19}, LSUN-Bedroom-256~\cite{LSUN} and LSUN-Church-256~\cite{LSUN}.
Following the standard practice in existing works, we generate 50,000 images on each dataset randomly for evaluation.
We compare the model performance in the generation quality by reporting Fr{\'{e}}chet Inception Distance (FID)~\cite{DBLP:conf/nips/HeuselRUNH17}. 
We also report Precision (Prec.) and Recall (Rec.) metrics~\cite{DBLP:conf/nips/KynkaanniemiKLL19} to separately measure the sample fidelity and diversity.


We train our model on CIFAR-10 at $32\times 32$ resolution, and FFHQ, LSUN-Bedroom, LSUN-Church on $256\times 256$ resolution.
For all of our experiments, similar to~\cite{DBLP:conf/nips/HoJA20}, we use the encoder-middle-decoder architecture to construct the U-Net with our proposed convolution and attention layers.
% Hyperparam
For $32 \times 32$ image resolution, we detail the architecture as follows. The downsampling block is 4-step, each with 3 residual blocks. The upsampling block mirrors the downsampling one. 
From highest to lowest resolution, the U-Net stages adopt the channel size of $[c,2c,2c,2c]$, respectively.
We use four attention heads at the $16\times 16$ and $8\times 8$ resolution.
For model architecture with $256 \times 256$ image resolution, the down/up-sampling block is 6-step with channel sizes of $[c,c,2c,2c,4c,4c]$, and 2 residual blocks for each step, respectively. We use a single attention head at the $16\times 16$ resolution. $C$ is set as 128 for all models.

% GPU
We use Adam~\cite{kingma2014adam} optimizer to train all models with a learning rate of $10^{-4}$ and an exponential moving average (EMA) over model parameters with rate $0.9999$. We adopt the linear noise scheduler in~\cite{DBLP:conf/nips/HoJA20} with $T=1000$ timesteps. 
% For CIFAR-10, we set the dropout as 0.1. 
Our CIFAR-10 model is trained on 8 Nvidia V100 32GB GPUS for 500K iterations, with a batch size of 128 and dropout of 0.1.
To accommodate for larger resolution in  FFHQ, LSUN-Bedroom and LSUN-Church, we train our models on 32 GPUS for 250K iterations.

% we train $32\times 32$ and $128\times 128$ models \linjie{256 x 256?} on 8 and 32 Nvidia V100 32GB GPUS, respectively. 
% we train the models for 500K iterations on CIFAR-10, and 250K iterations for FFHQ-256, LSUN-Bedroom-256 and LSUN-Church-256 with batch size of 128.

\begin{table}[tbh]
\begin{center}
\vspace{-4mm}
\tablestyle{3.5pt}{1.0} 
\caption{Results on CIFAR-10 dataset.}
\label{tab:c10:result}
\begin{tabular}%{L{0.01cm}C{2.0cm}C{2.0cm}C{2.0cm}C{1.01cm}}
{lccccccccc}
\toprule
&\multicolumn{1}{c}{{Architecture}}
&\multicolumn{1}{c}{{Model Type}}
&\multicolumn{1}{c}{{Denoising Space}}
&\multicolumn{1}{c}{FID$ \downarrow$}
&\multicolumn{1}{c}{Perc. $\uparrow$}
&\multicolumn{1}{c}{Rec. $\uparrow$} \\
% &\multicolumn{1}{c}{FLOPs(\%)}\\
%&\multicolumn{1}{c}{Pruned(\%)}
% \midrule
% &Real Images &- &35.26 &6.09\\
\midrule
&DDGAN~\cite{DBLP:conf/iclr/XiaoKV22} &Diffusion + GAN &Pixel &3.75(-0.00) &- &0.57(+0.00)  \\
&WaveDiff~\cite{DBLP:journals/corr/abs-2211-16152} &Diffusion + GAN &Wavelet &4.01(+0.26) &- &0.55(-0.02) \\
\midrule
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (default) &Diffusion &Pixel &3.53(-0.00) &0.62(+0.00) &0.55(+0.00) \\
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (concat) &Diffusion &Wavelet &9.29(+5.76) &0.64(+0.02) &0.51(-0.04)\\
% &Conditioned StyleGAN2 &24  &21.59 &17.86  &31.23 &47.22\\
% &\ourmodel-DG &24 &26.28 &13.37 \\
&\ourmodel (ours) &Diffusion &Wavelet &4.88(+1.35) &0.60(-0.02) &0.55(+0.00)\\
% &\ourmodel-hyper-guidance-perceptual &25 &17.82 &23.31 \\
\bottomrule
\end{tabular}
\vspace{-5mm}
\end{center}
\end{table}


\subsection{Quantitative Results}
We report generation quality in terms of FID, Precision and Recall, and compare with several baselines. With the same denoising objective and training process, the default DDPM U-Net architecture in pixel space is a natural and trivial baseline. We also design a simple variant of the denoising U-Net to process noisy wavelet inputs in a 4D form with all subbands concatenated along the channel dimension. Such data structure results in a corresponding change in the number of first/last convolution's input/output channels. (i.e. from 3 to 12). We denote this baseline as DDPM U-Net (concat). On CIFAR-10 and LSUN-Church-256, we additionally compare with the results reported in DDGAN~\cite{DBLP:conf/iclr/XiaoKV22} and WaveDiff~\cite{DBLP:journals/corr/abs-2211-16152}. Note that the DDPM U-Net (concat) is a simplified version of WaveDiff, removing the excessive D/IWT operators and the discriminator for GAN training.

As shown in Table~\ref{tab:c10:result}, though \ourmodel and WaveDiff can achieve plausible performance on CIFAR-10, both of them cannot beat the corresponding pixel-space counterparts (\textit{i.e.}, DDPM U-Net (default) and DDGAN). We hypothesize that this is due to the low image resolution in CIFAR-10, where transforming from pixel space to wavelet space further lowers the input resolution by half, thereby can not provide much benefit. \ourmodel also achieves comparable performance to WaveDiff on CIFAR-10, it is worth noting that \ourmodel adopts a simpler architectural design and training objective than WaveDiff, which embeds DWT and IWT as differentiable operators multiple times inside the network, and is trained with adversarial loss against an auxiliary discriminator.
% \ourmodel completely operates in the wavelet space for both training and inference, while WaveDiff depends on an auxiliary discriminator that takes in real/generated images in the pixel space. 
Furthermore, the `concat' baseline yields a poorer generation performance than \ourmodel, which suggests naively modifying the input channel to the standard U-Net cannot facilitate the denoising process in wavelet space. 

Table~\ref{tab:ffhq:result} shows a significant improvement of FID from \ourmodel over baselines on FFHQ-256.
These results deliver several findings, (1) the benefit of directly denoising in wavelet space is more substantial for higher resolution image generation; and (2) \ourmodel is able to fully exploit the information from wavelet space hence producing more realistic face images, with much lower FID.
Similar conclusions can be drawn from Table~\ref{tab:church:result} and ~\ref{tab:bedroom:result} on LSUN datasets, \ourmodel consistently improves over the pixel-space counterpart and the `concat' baseline, which validates the proposed architecture in \ourmodel  can successfully model the wavelets information within the context of standard DDPM training.
% We also show the generation comparisons on LSUN datasets in Table~\ref{tab:church:result} and ~\ref{tab:bedroom:result}, which validates \ourmodel is a plausible design to connect wavelets information to U-Net architecture within the context of standard DDPM training.


\begin{table}[tbh]
\begin{center}
\vspace{-4mm}
\tablestyle{4pt}{1.0} 
% \vspace{-5mm}
\caption{Results on FFHQ-256 dataset.}
\label{tab:ffhq:result}
\begin{tabular}%{L{0.01cm}C{2.0cm}C{2.0cm}C{2.0cm}C{1.01cm}}
{lccccccccc}
\toprule
&\multicolumn{1}{c}{{Architecture}}
&\multicolumn{1}{c}{{Model Type}}
&\multicolumn{1}{c}{{Denoising Space}}
&\multicolumn{1}{c}{FID$ \downarrow$}
&\multicolumn{1}{c}{Perc. $\uparrow$}
&\multicolumn{1}{c}{Rec. $\uparrow$} \\
% &\multicolumn{1}{c}{FLOPs(\%)}\\
%&\multicolumn{1}{c}{Pruned(\%)}
% \midrule
% &Real Images &- &35.26 &6.09\\
\midrule
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (default) &Diffusion &Pixel &13.53(-0.00) &0.52(+0.00) &0.31(+0.00) \\
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (concat) &Diffusion &Wavelet &23.18(+9.65) &0.55(+0.03) &0.30(-0.01)\\
% &Conditioned StyleGAN2 &24  &21.59 &17.86  &31.23 &47.22\\
% &\ourmodel-DG &24 &26.28 &13.37 \\
&\ourmodel (ours) &Diffusion &Wavelet &7.12(-6.41) &0.54(+0.02) &0.38(+0.07)\\
% &\ourmodel-hyper-guidance-perceptual &25 &17.82 &23.31 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-8mm}
\end{table}


\begin{table}[htb]
\begin{center}
\tablestyle{5pt}{1.0} 
\caption{Results on LSUN-Church-256 dataset.}
\label{tab:church:result}
\begin{tabular}%{L{0.01cm}C{2.0cm}C{2.0cm}C{2.0cm}C{1.01cm}}
{lccccccccc}
\toprule
&\multicolumn{1}{c}{{Architecture}}
&\multicolumn{1}{c}{{Model Type}}
&\multicolumn{1}{c}{{Denoising Space}}
&\multicolumn{1}{c}{FID$ \downarrow$}
&\multicolumn{1}{c}{Perc. $\uparrow$}
&\multicolumn{1}{c}{Rec. $\uparrow$} \\
% &\multicolumn{1}{c}{FLOPs(\%)}\\
%&\multicolumn{1}{c}{Pruned(\%)}
% \midrule
% &Real Images &- &35.26 &6.09\\
\midrule
&DDGAN~\cite{DBLP:conf/iclr/XiaoKV22} &Diffusion + GAN &Pixel &5.25(-0.00) &- &-\\
&WaveDiff~\cite{DBLP:journals/corr/abs-2211-16152} &Diffusion +GAN &Wavelet &5.06(-0.19) &- &0.40\\
\midrule
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (default) &Diffusion &Pixel &7.89(-0.00) &- &-  \\
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (concat) &Diffusion &Wavelet &18.96(+11.07) &0.61 &0.41 \\
% &Conditioned StyleGAN2 &24  &21.59 &17.86  &31.23 &47.22\\
% &\ourmodel-DG &24 &26.28 &13.37 \\
&\ourmodel (ours) &Diffusion &Wavelet &6.11(-1.78) &0.60 &0.44\\
% &\ourmodel-hyper-guidance-perceptual &25 &17.82 &23.31 \\
\bottomrule
\end{tabular}
\vspace{-6mm}
\end{center}
\end{table}


\begin{table}[htb]
\begin{center}
\tablestyle{4pt}{1.0} 
\caption{Results on LSUN-Bedroom-256 dataset.}
\label{tab:bedroom:result}
\begin{tabular}%{L{0.01cm}C{2.0cm}C{2.0cm}C{2.0cm}C{1.01cm}}
{lccccccccc}
\toprule
&\multicolumn{1}{c}{{Architecture}}
&\multicolumn{1}{c}{{Model Type}}
&\multicolumn{1}{c}{{Denoising Space}}
&\multicolumn{1}{c}{FID$ \downarrow$}
&\multicolumn{1}{c}{Perc. $\uparrow$}
&\multicolumn{1}{c}{Rec. $\uparrow$} \\
% &\multicolumn{1}{c}{FLOPs(\%)}\\
%&\multicolumn{1}{c}{Pruned(\%)}
% \midrule
% &Real Images &- &35.26 &6.09\\
\midrule
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (default) &Diffusion &Pixel &4.89(-0.00) &0.60(+0.00) &0.45(+0.00) \\
&DDPM~\cite{DBLP:conf/nips/HoJA20} U-Net (concat) &Diffusion &Wavelet &18.23(+13.34) &0.44(-0.16) &0.41(-0.04) \\
% &\ourmodel-DG &24 &26.28 &13.37 \\
&\ourmodel (ours) &Diffusion &Wavelet &3.88(-1.01) &0.62(-0.02) &0.48(+0.03) \\
% &\ourmodel-hyper-guidance-perceptual &25 &17.82 &23.31 \\
\bottomrule
\end{tabular}
\vspace{-4mm}
\end{center}
\end{table}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
\subsection{Qualitative Results}
\ourmodel is designed to directly facilitate the denoising task in wavelet sapce. As such, during sampling, we perform sequential reversed diffusion process to generate wavelets first.  
In Figure~\ref{fig:cifar_a},~\ref{fig:ffhq_a},~\ref{fig:church_a} and~\ref{fig:bedroom_a}, we generate high-quality wavelets with all four subbands information from noise (Top left: $\bU_{ll}$, Top right: $\bU_{lh}$, Bottom left: $\bU_{hl}$, Bottom right: $\bU_{hh}$).
These visualizations show that our model is able to (1) capture the spatial information to provide  realistic content in the synthesized image; and (2) approximate the complementary information from all four subbands to represent one image, including the highly semantic low-frequency content and the fine-grained details in high-frequency components.
% This also validates the spatial and frequency doamin-agnostic property of our design: spatial information is well captured to provide a realistic content; All four bands provide complementary information representing one image: low-frequency content approximation and the high-frequency details.
% We then perform inverse wavelet transform (IWT) to produce realistic image samples in Figure~\ref{fig:cifar_b},~\ref{fig:ffhq_b},~\ref{fig:church_b},~\ref{fig:bedroom_b}.
The final generated images are obtained by performing inverse wavelet transform onto the wavelets, shown in Figure~\ref{fig:cifar_b},~\ref{fig:ffhq_b},~\ref{fig:church_b},~\ref{fig:bedroom_b}.






\begin{table*}[htb]
\footnotesize
\begin{center}
\tablestyle{6pt}{1.0} 
   % \vspace{-4mm}
\caption{We ablate the effectiveness of different architectural components on FFHQ-256.}
\label{tab:ablation:result}
\begin{tabular}%{L{0.01cm}C{2.0cm}C{2.0cm}C{2.0cm}C{1.01cm}}
{lccccccccc}
\toprule
&\multicolumn{1}{c}{{Methods}}
&\multicolumn{1}{c}{Spatial Conv.}
&\multicolumn{1}{c}{Frequency Conv.}
&\multicolumn{1}{c}{Spatial Attn.}
&\multicolumn{1}{c}{Frequency Attn.}
&\multicolumn{1}{c}{FID $\downarrow$} \\
% &\multicolumn{1}{c}{FLOPs(\%)}\\
%&\multicolumn{1}{c}{Pruned(\%)}
% \midrule
% &Real Images &- &35.26 &6.09\\
\midrule
&Spatial-only Baseline &\cmark &\xmark &\cmark &\xmark &23.18\\
&+Freq-Conv. &\cmark &\cmark &\cmark  &\xmark &13.66\\
&+Freq-Attn.  &\cmark &\xmark &\cmark &\cmark &15.22\\
&\ourmodel &\cmark &\cmark &\cmark &\cmark  &\textbf{7.12}\\
% &\ourmodel-hyper-guidance-perceptual &25 &17.82 &23.31 \\
\bottomrule
\end{tabular}
   \vspace{-5mm}
\end{center}
\end{table*}


% Figure environment removed
\subsection{Ablation Study}
We ablate the effectiveness of the spatial-frequency convolutions and attentions in Table~\ref{tab:ablation:result} on FFHQ-256.
We first set up a simple baseline architecture with only spatial convolutions and attentions.
Then, we extend it to explicitly exploit the correlation along frequency dimension, by adding frequency convolutions and attentions, one at a time. The results show that all components need to work together to achieve the lowest FID score.
% into a principled architecture design successfully.
In addition, we visualize the generated wavelets and images by the spatial-only baseline architecture in Figure~\ref{fig:baseline}. As shown in the top left of Figure~\ref{fig:baseline_a}, the model with only spatial modules can still learn the low-frequency facial content (low-sub $\bU_{ll}$), however, it does not model the correlation among different frequency components, resulting in a noisy sample (especially for the high-subs) due to the failure of spatial-frequency alignment. Both quantitative and qualitative comparisons demonstrate that the proposed spatial-frequency-aware architecture is a necessary design for wavelet-based DDPMs.


% Figure environment removed
\subsection{Wavelets Refinement}
In Figure~\ref{fig:wave_refine}, we examine whether our method can explicitly recover the distribution of wavelets driven by the simple denoising objective, in contrast to the  reconstruction loss and the auxiliary adversarial loss in WaveDiff.
We demonstrate this by visualizing the predicted wavelets along the timesteps of reversed diffusion process. The generated images on the first row are from applying IWT to the generated wavelets at the corresponding timesteps.
We observe the wavelet generations in all four subbands get refined as the reversed diffusion process continues, in turn revealing more details in the generated images.
% Subsequently, we can gradually refine the face samples with more details by simply applying IWT to generated wavelets. 











