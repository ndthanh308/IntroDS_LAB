\section{Introduction} \label{sec:intro}
% supervised learning/self-sup. learning; Labeling heavy; finetune.
Denoising Diffusion probabilistic model (DDPM), as a deep generative model, has achieved a new state-of-the-art performance, surpassing GANs~\cite{} in generation tasks. 
% with more stabilized training when scaling up.
% recently.   
% Facilitating the stabilization of training process with model scaling up, diffusion models have surpassed GANs~\cite{} in generation tasks. 
% With this topic becoming popular, more research efforts have 
Recent efforts have shown great potentials of DDPMs in advancing machine learning and computer vision tasks such as image synthesis~\cite{}, multi-modal learning~\cite{}, 3D Vision~\cite{} and representation learning~\cite{}.
Despite its success, corruption (forward diffusion process) and restoration (reversed diffusion process) of the probabilistic distribution in the RGB pixel space has been the \textit{de facto} setting and little attention has been paid to performing DDPM in the wavelet space.

Among limited explorations in wavelet-based generations, WaveDiff~\cite{} aims to achieve a trade-offs between efficiency and sample quality by combining Discrete/Inverse Wavelet Transformation (D/IWT) and DDGAN~\cite{}, instead of the standard DDPM~\cite{}.
% by combining wavelets and DDGAN~\cite{}, instead of the standard DDPM~\cite{}.
The training of Wavediff (Figure~\ref{}) requires an auxiliary discriminator, guided by adversarial and reconstruction losses.
DiWa~\cite{}  leverages the strengths of DDPMs and DWT on a different task, Single-Image Super-Resolution (SISR) to effectively exploit high-frequency information for more detailed image reconstruction. Despite the promises shown in these works, directly performing standard DDPM training in wavelets is not trivial, as the computational operators, attention mechanism and denoising objective by default are designed for the pixel space diffusion. 

% cnn architecture design
For DDPM U-Net design, 2D convolutions have been the gold standard in processing 2D data such as images~\cite{}. 
As a natural extension, 3D convolutions are used to process 3D data, \textit{e.g.} videos~\cite{}. 
Besides, for specific tasks, the attention mechanisms need to be hand-designed to connect information embedded in the corresponding data structure.
For example, 2D convolutions in Figure~\ref{} extract the spatial information from RGB data, followed by the attention module modeling the correlation between different pixel features in 2D image data.
% , capturing global coherence and consistency in image data.
On videos, VideoDiffusion~\cite{} simply extends the U-Net architecture by incorporating the spatial-temporal concept~\cite{} in network design, resulting in a 3D U-Net with space-only 3D convolution, spatial self-attention and temporal self-attention.  Such architectural changes have been proven effective with the standard denoising objective for video generation.
% , resulting in a 3D U-Net with space-only 3D convolution, spatial self-attention and temporal self-attention. 
% VideoDiffusion~\cite{} further extends the U-Net architecture to accommodate video generation task by incorporating the spatial-temporal concept~\cite{} in network design, resulting in a 3D U-Net with space-only 3D convolution, spatial self-attention and temporal self-attention. 
% As an analogy of extension to 2D U-Net with training process compatibility, plausible
Similarly, if one views wavelet as a distinct data space from RGB pixels to represent a image, careful design choices to replace/upgrade the standard 2D U-Net should be made, in order to maintain the compatibility to the DDPM training process.

% Figure environment removed
In this paper, our goal is to perform denoising task in the wavelet space to generate realistic images, while maintain the compatibility with the standard DDPM training process, as shown in Figure~\ref{}.
% which is able to generate realistic images while maintaining the compatibility with the standard DDPM training process, as shown in Figure~\ref{}. 
To this regard, we design a new architecture by framing both low-subbands and high-subbands signals within a concept of joint spatial and frequency, so that the training and sampling can benefit from the complementary information provided by both low-frequency and high-frequency subspaces.
There's an one-on-one mapping between our operator/attention modules, and the spatial operator/attention modules in the standard U-Net architecture.
As such, our model transforms and parallelly processes the wavelets of spatial and frequency subspaces, while maintaining the default construction of DDPM U-Net.
During training, the model is optimized by the simple MSE loss of noise prediction over all diffusion timesteps.
We summarize our contributions as two-fold:
\vspace{-0.5em}
\begin{itemize}[leftmargin=.15in]
\setlength\itemsep{0.2em}
    \item{%
      \textbf{Spatial-Frequency-aware Architectural Design.}~
      The architecture is specifically and carefully designed for wavelet data. By explicitly processing and exploiting the information from both spatial and frequency subspaces, the distribution of image contents (\textit{i.e,} spatial components across low and high frequencies)  and local details (\textit{i.e,} high-frequency components) can be better converged to reverse the forward diffusion process. The newly designed modules can be easily dropped in to DDPM U-Net without affecting the default structure.
   }%
   \item{%
      \textbf{High Quality of Image Generation.}~
      Our model can generate high-quality images with clear details, achieving excellent quantitative and qualitative results under common evaluation protocols. 
   }%
\end{itemize}
\vspace{-0.5em}
We demonstrate these advantages on multiple datasets including CIFAR-10, FFHQ-256, LSUN-Bedroom-256 and LSUN-Church-256.



