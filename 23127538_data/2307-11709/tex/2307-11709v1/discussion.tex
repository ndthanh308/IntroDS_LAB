\vspace{-0.1cm}
\section{Discussion\slash Future Work}

This paper improves upon  state-of-the-art in four ways. First, a memory network outperforms very recent baseline work in encoding statement-level information for source code summarization. Second, statement based memory network improves a subset of summaries significantly, while other subsets may benefit from other approaches. Multiple approaches can be ensembled to take advantage of these orthogonal improvements. Third, memory network relies on learning which statements are more important to the summary among all statements. Each memory gives maximum attention to one statement. We posit that too many memories can decrease performance. Fourth, explicit encoding of the position of a statement is crucial to model performance.

Our approach outperforms recent baseline work in statement-based encoders on datasets of two different programming languages.  Our approach also improves several state of the art non-statement-based approaches. Additional intellectual merit of this paper is in the evaluation of the subsets of both datasets where different approaches excel. We explore the difference set for the purpose of intellectual curiosity. The results provide an insight into both the dataset and the advantages of different approaches. Future work could benefit from this way of evaluating source code summarization models. We see that different models improve niche subset of summaries and our approach improves every other model.

One possible avenue of future work is to apply memory networks to external context such as approaches described in Section~\ref{sub:codesummary}. Bansal~\textit{et al.}~\cite{bansal2021project} show that specific subroutines in various files in a project can help generate better summaries. A memory network could learn which of these subroutines is most important and could produce interesting results. 