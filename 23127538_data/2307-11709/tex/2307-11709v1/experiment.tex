\vspace{-0.15cm}
\section{Experiment Design}
\label{experiment}
In this section we describe our experimental design including the research questions, methodology, baselines, and metrics used to evaluate our model against the baseline.
\vspace{-0.25cm}
\subsection{Research Questions}
\label{sec:rqs}
 Our main research objective is to evaluate if statement based memory network (SMN) improves source code summarization. Also, we aim to learn about the subset of summaries most improved by our approach and how our design choices affect performance. To that end, we ask three research questions:

\begin{description}
	\item[RQ1] What is the difference between our approach and the baseline when ensembled with non-statement-based models as measured by automated metrics?
	\item[RQ2] What are the features of the subset of summaries most improved by our approach?
	\item[RQ3] How do the different design choices and configurations affect the performance of our encoder, as measured by automated metrics?
	\end{description}

The rationale behind RQ1 is to evaluate our approach in a way consistent with previous approaches. We designed our encoder to add to, not compete with existing approaches. To that end, a vast majority of papers in the past 5 years use ensembles to evaluate orthogonal improvements by combining different approaches ~\cite{leclair2021ensemble,zhou2022summarizing,gao2021code}.  We use automated metrics to quantify the differences between our approach and the baseline, consistent with literature ~\cite{haque2022semantic,roy2021reassessing}.
 
The rationale for RQ2 is to further investigate the impact of our encoder in improving a subset of summaries when compared against the basic models. While we designed our approach to add to, and not compete with other approaches, competitive comparison helps place our approach among the state-of-the-art. Non-ensembled evaluation helps highlight the subset of summaries most improved by our approach.

The rationale behind RQ3 is that a neural network is a complex system and several factors may impact performance. We make several design choices informed by literature to construct our model. It could be useful to know which of these factors are most crucial to the performance of our encoder.   We evaluate three different design choices against alternatives informed by best practices in literature to aid any future work that may benefit from our approach. We test these configurations for our encoder without ensembles because minor differences can be overshadowed by ensembling.

\vspace{-0.2cm}
\subsection{Methodology}
\label{sec:methodology}
\vspace{-0.1cm}

Our methodology to answer RQ1 is informed by both the procedure established by most source code summarization approaches ~\cite{hu2018deep,alon2019code2seq,leclair2019neural} and the latest research that suggests improvements~\cite{haque2022semantic,roy2021reassessing,9286030}. We start with an established dataset of subroutine-summary pairs and curate it for our experiment (see~Section~\ref{sec:dataset}). We then train models of each of the non-statement based basic models and our statement based approach as well as another statement-based baseline. These models are independently trained till convergence. Convergence is popularly accepted as the epoch at which validation accuracy peaks without substantially higher validation loss. 

Minor performance improvements can be achieved by simply making the model bigger, i.e. increasing the dimensions of the decoder vectors. Consequently, we keep the size of all embedding and dense layers constant. The penultimate step is to ensemble each combination of statement based and non-statement based models using techniques from literature~\cite{leclair2021ensemble}. Lastly, we use automated metrics for evaluation. Although the BLEU metric has been popular in the past, recent studies suggest BLEU scores may not accurately reflect an improvement in quality of summaries~\cite{wieting-etal-2019-beyond,haque2022semantic,roy2021reassessing}. Therefore, we also evaluate using METEOR, recommended by Roy~\textit{et al.}~\cite{roy2021reassessing} and USE, recommended by Haque~\textit{et al.}~\cite{haque2022semantic}.  

To aid the evaluation of our ensembles, we also discuss the features of the ``difference set'' between ensemble with our approach and the baseline statement-based approach.  The difference set is the set of predicted summaries which are different for any two approaches.  Typically, there exists a subset of generic or otherwise very easy to summarize subroutines, which a majority of neural models will summarize in exactly the same way . This group of easy subroutines is the ``same set''  that may exaggerate the overall scores to a degree that may distort the reported results.

We use automated metrics for our RQs in favor of a human study for three reasons. First, we have several thousands of predictions to test against several baselines, and configurations. A small subset that a human participant could reasonably be expected to evaluate would not be representative of the performance of our approach. Second, human studies tend not to be reproducible because they are opinion-based. Any results obtained may be specific to the demographic of participants recruited. Third, it is expensive to conduct a reliable human study that would require a large number of participants. Cost-decreasing methods lead to unreliable evaluations by either recruiting participants without enough experience with source code or a very small number of participants with low agreement scores within their ratings.

To answer RQ2 we evaluate our approach in two ways. First, we evaluate our model against each of the other baselines without the use of ensembles and present the results using the same automated metrics as RQ1. Second, we report the features of the ``improved set'', a subset where our approach achieves significantly higher score compared to the basic models. This subset is obtained by taking each pair of predicted summary and comparing to the reference on the basis of scores alone. Note that this is not the same as the difference set in RQ1, which considers differences between the words in the summary. We only use METEOR scores because: 1) BLEU is only reliable at the corpus level, and 2) USE is volatile because of the learnt weights that are used to create the vectors.

For RQ3  we conduct three tests for different configurations of our encoder. Each test is of one design choice: 1) $h$ iterations for dynamic network, 2) randomly initialized matrix $Q$, and 3) Positional encoding. We compare alternatives to these design choices against the best performing model configuration described in Section~\ref{sec:approach}. For the first test, we change  $h$ that is the number of iterations used to generate memories, described  in Section~\ref{sec:details}. We compare models H1, H2, H4, and H5 trained with four different $h$ values against our optimal configuration. For the second test, we replace the $Q$ matrix (recall Equation \ref{eq:1} in Section~\ref{sec:details}) with the vector representing summary generated so far, a configuration inspired by Kumar~\textit{et al.}~\cite{kumar2016ask}  we call this configuration {\small \texttt{SMN-summary vector}} . This is in contrast to the best performing model inspired by the works of Sukhbaatar~\textit{et al.}~\cite{sukhbaatar2015end}. For the third test, we compare End of Sentence (EOS) encoding as described by Kumar~\textit{et al.}~\cite{kumar2016ask} against positional encoding used in our best configuration described in Section~\ref{sec:details}.  

\vspace{-0.2cm}
\subsection{Metrics}
\label{sec:metrics}
We use three metrics for evaluation of our model. First, BLEU~\cite{papineni2002bleu}, a natural language similarity metric that computes n-gram overlap. Second, METEOR~\cite{banerjee2005meteor} uses unigram precision and recall and computes the harmonic mean. Third and latest, USE~\cite{cer2018universal}, which encodes summaries into a vectors and computes the distance between predicted and reference summary vectors. Each of these metrics is backed by human studies to ensure the metrics correspond to human perception. 

Recent study by Haque~\textit{et al.}~\cite{haque2022semantic} compares these metrics for source code summarization. Their study suggests unigram or n-gram approaches may not generalize to source code summarization jargon as well as they do to natural language summaries. We follow their recommendation to evaluate generated summaries with USE. We use the augmented implementation of USE provided as part of their replication package. We follow recommendations by Roy ~\textit{et al.}~\cite{roy2021reassessing} to use METEOR and Graham~\textit{et al.} ~\cite{graham2014randomized} to compute statistical significance of the USE and METEOR score difference.

\vspace{-0.2cm}
\subsection{Basic Models : Non-Statement-based}
\label{sec:baselines}
Our basic models consist of four recent neural network based approaches in source code summarization literature that do not use statement based information. We faithfully re-implement each of these approaches, taking the novel contributions of their approach (mostly encoder) and incorporate them within our sequence to sequence framework:

\begin{description}
	\item[code2seq] is the approach proposed by Alon ~\textit{et al.}~\cite{alon2019code2seq} that represents a family of papers that use the AST to represent source code. They use AST path tokens as input and encode this information using multiple GRU. They use a decoder similar to the \textit{attendgru} baseline. 
	\item[attendgru] was proposed by LeClair ~\textit{et al.} ~\cite{leclair2019neural} as a typical sequence to sequence model. They use source code as a sequence of text tokens as input . They process these inputs with a learnt word embedding and use GRUs to encode the inputs and a GRU for the decoder.
	\item[transformer] proposed by Ahmad \textit{et al.}~\cite{ahmad2020transformer} represents a family of transformer based approaches . They use source code text tokens as input and the use multi-headed self attention between all tokens in the input. We faithfully re-implement the model within our framework.
	\item[codegnngru] by LeClair ~\textit{et al.}~\cite{leclair2020improved} is a representative example of graph neural networks for source code summarization. They encode both AST and source code as text in form of a graph. They use the nodes and adjacency list for this graph as input. They present several configurations in their paper. We re-implement the GRU based approach for posterity from the replication package provided. 

\end{description}

This is not a complete list of recent basic non-statement based models, but each of them is representative of a family of approaches as explained in Section~\ref{sub:codesummary}. We do not replicate a basic model to represent external context because those techniques benefit from additional data that our framework does not have such as UML~\cite{wang2021cocosum} or an indexed corpus~\cite{li2021editsum}. We considered using CodeBERT by Feng \emph{et al.}~\cite{feng2020codebert} as a basic approach. We did not replicate their approach because: 1) the resource requirements for their approach are huge and would take us months to train a replication of the pre-trained embedding presented, and 2) the improvements for source code summarization is similar to those reported by other baselines. Therefore we decided not to replicate their approach in favor of other low resource approaches.


\subsection{Baseline: Hierarchical Attention Network}
\label{han}
Our main baseline is {\small \texttt{HANcode}}, a statement based approach by Zhou {et al.}~\cite{zhou2022summarizing}. It was released in early 2022 and is the most recent approach that uses similar techniques to our approach such as intra-attention and statements-flow. We chose HANcode as our main competitor because the paper establishes the benefits of using statement-based approach. We design our experiment to evaluate our hypothesis that memory networks are better at encoding statement-level information.

HANcode is inspired by the Hierarchical Attention Network (HAN) proposed for document classification by Yang~\textit{et al.}~\cite{yang2016hierarchical}. Their approach is a statement based approach like ours, however applies word level self-attention much like the {\small \texttt{transformer}} baseline. We faithfully replicate their approach within our own framework using GRUs. We ensemble this approach with the four non-statement-based approaches to serve as baseline against which we evaluate our statement-based memory network. 
\vspace{-0.1cm}
\subsection{Threats to Validity}
\label{sec:threats}
The major threats to validity of our study are: 1) limitations of our datasets 2) metrics used to evaluate performance, and 3) the hyperparameters and configurations. 

First, although we curate the Java dataset from a well-established dataset in the field, similar results may not be seen over other datasets. For example, datasets that mainly consists of very small subroutines with one or two statements, may not see any improvement from a statement based approach like ours. Datasets in other programming languages may not have information embedded between statements or may need different model configurations. To mitigate this our dataset has been through multiple rounds of filters to create a high quality subset. We created the Python dataset that has similar features as the Java dataset to further mitigate this.

Second, automated metrics like METEOR and BLEU have been established in NMT. However, they only provide a comparison to the reference summary, where a high score means the generated summary has mostly the same words as the reference. This does not take into consideration that some words in the summary may be more important than others. This is specially true to technical jargon one would expect to see in a code summary. To mitigate these challenges we evaluate using the latest USE metric and follow the most recent guidelines for using NMT metrics for source summarization~\cite{roy2021reassessing,haque2022semantic}.

Third, implementation configurations are another threat to validity. It is possible to get minor improvements in performance by tweaking hyperparameters and increasing the size of the model. Therefore we replicate each baseline very carefully ensuring similar dimensions for all vectors involved. The ensemble process we used to combine our statement based encoder with a non-statement based model can distort the results. We attempt to mitigate this threat by individually comparing all the models we trained to create ensembles in RQ2 and then compare different configurations of our encoder without any basic models in RQ3.