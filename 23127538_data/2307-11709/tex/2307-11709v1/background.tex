

\vspace{-0.1cm}
\section{Background \& Related Work}
\label{sec:background}
This section discusses key background technologies and related work, such as encoder-decoder models, source code summarization, and memory networks.

\vspace{-0.1cm}
\subsection{Source Code Summarization}
\label{sub:codesummary}

Automated source code summarization has interested the software engineering community for decades. Early approaches used Information Retrieval based techniques~\cite{sridhara2011automatically,mcburney2014automatic,rodeghero2014improving}. The neural machine translation (NMT) problem served as an early inspiration for a shift towards neural code summarization. Since 2014,  encoder-decoder models have formed the state-of-the-art in NMT as documented in the survey by Cho \textit{et al.}~\cite{cho2014properties}. A typical encoder-decoder model takes an input to the encoder in one language, for example French and translates the phrase into another language, such as English. For source code summarization, the input is source code of a subroutine and the output is a natural language summary. Figure~\ref{tab:screlated} provides an overview of the state-of-the-art in neural source code summarization over the last five years. These papers can be broadly classified into four categories based on the information encoded by the neural model:

% Figure environment removed

\textbf{Source Code as Text:} The earliest neural approach in literature is to input source code as text to an encoder-decoder model. These approaches typically use a Recurrent Neural Network (RNN), Transformer, or other neural networks to encode source code as if it were one long sequence. Then, the decoder translates that sequence into a natural language summary. In 2019 LeClair \textit{et al.}~\cite{leclair2019neural} introduced \textit{attendgru}, an implementation inspired from seq2seq model first proposed in 2014 by Bahdanau \textit{et al.}~\cite{bahdanau2014neural} for general purpose NMT. They observe that attention mechanism helps find important words in source code and generates better natural language summaries. However, these approaches are designed to learn common text sequence. Source code contains structure, data flow, and other information that this approach does not capture. We use this as a starting point for our approach in this paper.

\textbf{Abstract Syntax Tree (AST):} Abstract syntax trees are structural representations of source code. ASTs represent structure instead of text specific to a given piece of code, such as variables. Therefore, AST is a more generalizable representation of source code. In 2018 Alon \textit{et al.}~\cite{alon2018code2seq} introduced \textit{code2seq} that learns a structured representation of source code for summarization using AST paths. They observe that generalizability helps improve larger number of summaries overall . Then in 2020, LeClair \textit{el al.}~\cite{leclair2020improved} presented ~\textit{codegnngru} a bi-directional graph neural network (GNN) based approach. They observe that AST and source code can be attended to separately to provide orthogonal improvement. Codegnngru represents a family of GNN based techniques ~\cite{zhou2022automatic}. AST works well for simple and generalizable subroutines, but longer more complex subroutines may benefit from additional knowledge.

\textbf{External Context:} External context is information from sources other than the piece of code being summarized. Humans use external context during program comprehension~\cite{latoza2006maintaining}, so researchers are motivated to build models that use external context as well. In 2020, Haque~\textit{et~al.}~\cite{haque2020improved} introduced ``file context''  that encodes the file with the target source code. They use an attention mechanism to learn from other methods in the same file as the subroutine being summarized. Then in 2021, Bansal \textit{et al.}~\cite{bansal2021project} introduced ``project context'' extending upon the concept of file context. Their model encodes files and specific methods inside those files to add to the knowledge base. Although they show there is important information in context, the data and resource requirements are very high.

\textbf{Self-Attention:} Self-Attention techniques learn dependencies among tokens in the subroutine. Self-Attention techniques are hypothesized to learn structure without explicitly parsing AST, for example by Ahmad ~\textit{et al.}~\cite{ahmad2020transformer}. Essentially, self-attention mechanisms learn relationships between different parts of the input itself. Transformer~\cite{vaswani2017attention} models are a popular example of self-attention networks. In 2020 Ahmad \textit{et al.}~\cite{ahmad2020transformer} proposed a Transformer based model, that uses self-attention mechanism to summarize source code. On a larger scale Feng \emph{et al.}~\cite{feng2020codebert} introduce \textit{codeBERT}, an implementation of BERT~\cite{devlin2019bert}. They use multi-headed self attention to learn representation of source code for several code intelligence tasks. Resource and data requirements for CodeBERT are huge compared to other recent approaches. However, there are other lower cost self-attention techniques in literature. One of these is memory networks that we discuss in the next sub-section.

\vspace{-0.4cm}
\subsection{Memory Networks}
\label{sub:memorybackground}
\vspace{-0.15cm}
A memory network is a neural architecture that links information in different chunks of data that the network sees in sequence. These chunks of data are often referred to as \textit{facts}. The model sees the input as a sequence of facts and then learns connections among them. They are linked together by a series of learned attention units called \textit{memories} that are generated iteratively. Each memory represents the most important fact in that iteration. Each iteration attends to the memory from previous iteration and each of the facts. This allows the network to pick the most important fact from the series of memories, then the second most important fact given the previous fact, and so forth.

In 2015 Bordes \textit{et al.}~\cite{bordes2015large} introduce memory networks for a question answering system. The initial input to their system is a paragraph, where each sentence is a fact. Their approach stores an indexed array of facts they call the memory. In their approach , the memory vector gets updated every time a fact is added to the network. Similar facts are grouped together in memory to populate a hypergraph. Their approach attained improvements over the state-of-the-art approaches that append facts to the original sequence. 

Then in 2016, Kumar \textit{et al.}~\cite{kumar2016ask} introduced Dynamic Memory Networks (DMN). The input to their approach is a series of sentences, each one a fact. Their model is trained to learn weights for each sentence given a question-answer pair and is informed by the previous sentence. This self-attention mechanism is applied between the question and hidden states of all the sentences assigning weights to the most relevant ones iteratively. They observe that self-attention is beneficial to preserving temporal information in sequence of events. Their approach improves over the state-of-the-art at the time by linking temporal events together. This paper serves as an inspiration for our memory network. Application of memory networks is not limited to NLP. In literature, the benefits of applying memory networks can be seen across various domains .~\cite{yang2018learning,zhu2019cvpr}.


