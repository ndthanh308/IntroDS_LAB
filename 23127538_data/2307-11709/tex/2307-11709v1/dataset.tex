\section{Dataset Preparation}
\label{sec:dataset}
We used two datasets specifically curated for our paper : 1) Java dataset, and 2) Python dataset.

The Java dataset we use consists of 190K Java methods that we curated from a larger dataset of 2.1m methods proposed in 2019 by LeClair \textit{el al.}~\cite{leclair2019recommendations} named \textit{funcom}. We selected the ~\textit{funcom} dataset for three reasons. First, the summaries were generated with the help of JavaDocs, with high quality standardized documentation. Second, all subroutines in the dataset are split at project-level not at method level to reduce the risk of  data leaks. Third, the dataset is well established and follows best practices from literature~\cite{leclair2019neural,allamanis2018learning}. 

We curated our dataset by first removing code clones from the dataset as proposed by Bansal~\textit{et al.}~\cite{bansal2021project} using the technique presented by Microsoft Research ~\cite{allamanis2019adverse}. We also select 190k of the longest methods from the filtered dataset. This reduces the likelihood of smaller get/set/return methods with generic summaries. The \textit{funcom} dataset contains a large number of these methods as observed by Haque et al.~\cite{haque2021action}. They observed that it was comparatively easier to generate summaries that would achieve high metric scores for these methods. This is due to the distinct structure and limited number of action words describing the purpose of the method. Longer methods also make better candidates for statement-based memory networks as the network is designed to learn attention between multiple statements.

We created the Python dataset using programs we extracted from Github. To keep the dataset comparable with Java, we apply the same filters to remove duplicates and smaller methods with limited number of sentences. The Python dataset consists of 270k total python functions. We selected the 270k largest functions from a dataset of 1.7 million filtered python functions. These functions are split at the project-level to reduce the risk of data leaks following the same best practices used for the Java dataset. 