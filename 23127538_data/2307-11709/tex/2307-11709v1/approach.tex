\section{Approach}
\label{sec:approach}

This section describes the approach of our paper. We list our model design, mathematical notation, parameters, and other details needed to replicate our proposed memory network.
\vspace{-0.2cm}
\subsection{Overview}
\label{sub:overview}
\vspace{-0.1cm}

% Figure environment removed

Figure ~\ref{fig:overview} provides a high-level overview model. Our model accepts two inputs: 1) source code as text, and 2) summary sequence predicted so far. The model produces one output i.e. a matrix indicating the most likely candidate for the next word. Figure ~\ref{fig:overview} Area 1 follows the same architecture as ~\textit{attendgru} model described in Section~\ref{sub:codesummary}. We use the same embedding layer for the memory network because the tokens i.e. words in the sequences are exactly the same.

The novel contribution of this paper is the grey part in Figure ~\ref{fig:overview} area 2 and 3. First, we divide the source code of the java method into statements. Each statement is a line of code as it appears in the program. These serve as facts to our memory network. Second, we faithfully replicate positional encoding technique as defined by Sukhbaatar~\textit{et al.}~\cite{sukhbaatar2015end}. This encodes temporal information into the sequence i.e. the order in which the statements occur in the initial sequence. It is necessary to explicitly encode this information within each statement vector because the memory network attends to one statement at a time.

The memory network learns which of the statements are most important to predict a given word. This is achieved using a gated self-attention unit iteratively.  Figure ~\ref{fig:overview}  area 3 shows the inner workings of dynamic network. Over the first iteration, the learned attention unit i.e. the memory represents the most important statement. Over the subsequent iterations, it attends to the statements of secondary importance given high attention to the statement in previous memories. The number of iterations is a hardcoded value for all samples regardless of the number of statements.

As seen in Figure ~\ref{fig:overview} area 4, we use an attention mechanism twice. First, between the output of GRU encoder and the summary. Second, between the memories from the memory network and the summary. Attention mechanism helps the model learn parts of both GRU encoder and memories that are most important to the decoder. Finally we concatenate the outputs from these attention layers as well as the output of the summary GRU from Figure ~\ref{fig:overview} area 1. This large vector is passed to a dense layer used to predict the next word.

\vspace{-0.1cm}
\subsection{Model Details}
\label{sec:details}
\vspace{-0.1cm}
We explain the details of our model with the equations below. These details cover the key novel aspects to our model in Figure ~\ref{fig:overview} area 2. There are three steps. First, we split the sequence into statements, a process we describe in the next subsection. Second, we feed these statements to the positional encoder represented by these equations:
\vspace{-0.1cm}
\[\Big[\Big[P_{xy} = (1- y/Y) - (x/X)\times(1 - 2\times y/Y)\Big]_{x=0}^X\Big]_{y=0}^Y\]
\[F= S \otimes P' \]

Here $P$ is the positional encoding matrix, $S$ is the vector of statements, $X$ is the size of the word embedding, and $Y$ is the maximum length of a statement. The $ [  ]_{x=0}^X$ notation describes a \textit{for loop} from $0$ to $X$. The $\otimes$ symbol represents element wise multiplication. $P'$ indicates transpose of the positional encoding matrix. Third, we present the memory network layer that receives a set of statements from the model. We represent our memory network with these equations:
\vspace{-0.1cm}
\[{\Big[Memories_i= Update(F,Q,M_{(i-1)})}\Big]_{i=0}^h  \tag{1}\label{eq:1}\]

Here $h$ is the number of ``hobs'', i.e. pre-defined iterations the memory network goes through and $Q$ is a matrix where 0.1 is numerical the value of each element . The function ``Update'' can be represented by the following equation:
\vspace{-0.1cm}
\[{\Big[m = G'_t \times GRU(F_t ,m) + (1-G'_t)\times m}\Big]_{t=0}^n\]

Here $m$ is initialized a matrix of zeros. After iterating through the attention for all statements using a GRU, the final $m$ matrix is returned as an output to the Update function i.e. $Memories_i$ from equation~\ref{eq:1}. $G'_t$ is output of the gated attention unit for that fact, represented by equations:
\vspace{-0.1cm}
\[G_k = (F_k \times Q_k) \oplus (F_k \times M_k) \oplus |F_k-Q_k| \oplus |F_k - A_k|\]
\[G'_k = \Sigma (tanh(G_k))\]

Here, $\oplus$ represents the tensor concatenation operation and $G'_k$ is the gated attention learned between statement k($F_k$) and memory k($M_k$). This is iterated via a for loop from $0$ to $n$, where $n$ is the maximum number of statements. The $\Sigma$ and $tanh$ computations are described in the Keras library, version details for which are in Section~\ref{sec:versions}.

At the decoder side seen in Figure ~\ref{fig:overview} area 4, the output memories vector is concatenated with the output of the summary GRU in Figure ~\ref{fig:overview} area 1. This concatenated vector is used to compute the attention using softmax activations. The attention vector is then applied back to the memories vector using a dot product. This approach is similar and in addition to the attention mechanism in the \textit{attendgru} encoder. In the final step we concatenate both post-attention vectors as well as the output from the summary GRU into a \textit{context} vector. A keras dense layer with the softmax function takes this vector and produces the output word.

\vspace{-0.1cm}
\subsection{Input\slash Output}
\label{sec:io}
\vspace{-0.1cm}

Our model accepts two inputs: 1) the method source code as tokenized text and 2) the summary predicted so far as tokenized text. This tokenization occurs during pre-processing. Input 1 also contains ``<NL> '' tokens which mark the end of a line of code. We partition code at these tokens to extract statements ensuring each statement is a vector representing a line of code and the statements are kept in order to form a $m \times n$ matrix, where $m$ is the number of statements and $n$ is the maximum number of words in a statement. For our experiment we use $m=70$ and $n=30$. This design configuration is explored further in Section~{sec:quantexp}. 

Our model outputs a $v \times v$ matrix where $v$ is the size of tokenized vocabulary for summaries. The max value in this vector is selected as the word added to the previously generated sequence, for the next iteration during testing. For the Java dataset, vocabulary size $t$ for source code is 69725 and vocabulary size $v$ for summaries is 10908. For the Python dataset  vocabulary size $t$ for source code is 100000 and vocabulary size $v$ for summaries is 25000.  These limits on vocabulary strike a delicate balance between limiting resource exhaustion and representing the most common words. The model sequentially generates a maximum of 12 tokens for a total summary length of 13 words where the \textit{<\slash s>} token marks the end of the generated summary.


\vspace{-0.15cm}
\subsection{Combining with non-statement encoders }
\label{sec:ensembles}
\vspace{-0.05cm}

We combine our statement based memory with non statement encoders from recent literature. We use an ensemble technique as recommended for code summarization by LeClair~\textit{et al.}~\cite{leclair2021ensemble}. Ensemble of models is used in a wide range of research areas such as Gene Identification ~\cite{le2020computational} and Computer vision~\cite{qummar2019deep} . The core reason ensembles are popular is that they take advantage of two or more approaches where each one might excel at a niche use case. LeClair~\textit{et al.} found that ensembles are able to capture and merge orthogonal contributions of various approaches in source code summarization. We follow best practices laid out in their paper to create ensemble models for source code summarization.

We created ensemble models by combining a non-statement based model and a statement based approach. We follow the methods listed in  LeClair ~\textit{et al.} and choose a "mean" aggregation method, where softmax output from both approaches is averaged to generate the output word. We choose this method of aggregating ensembles because it is simpler than other methods and easier to standardize across different encoders. More importantly, creating ensembles this way has negligible resource overhead because these trained models can be reused to create any number of ensembles. Every model we used to create ensembles was trained independently and only aggregated during prediction. For example, {\small \texttt{transformer+SMN}} uses {\small \texttt{SMNcode }} i.e. a model trained with our statement based memory network encoder as described in Section~\ref{sub:overview} ,  and {\small \texttt{transformer}} a model trained with a transformer based encoder (see Section ~\ref{sec:baselines}). 
 
 \vspace{-0.2cm}
\subsection{Hyperparameters}
\label{sec:hyper}
Our system is designed with the following hyperparameters:

\begin{table}[h!]
	\vspace{-0.3cm}
	\centering
	\begin{tabular}{lll}
		$tdatlen$	&200	  & maximum length of code sequence\\
		$comlen$	&13	& maximum length of summary\\
		$e_{dim}$ & 100	& dimension of word embedding\\
		$l_{dim}$  &100	& dimension of encoding layers\\
		$h$		& 3	& iterations for memory self-attention \\
		$n$   	& 70    & maximum number of statements \\
		$Y$  	& 30   & maximum length of a statement \\
		$RNN$ 	& GRU   & type of RNN \\
		$b$	& 100	& batch size \\
		$p$		&Adam		& parameter optimization technique \\
		$l$		&cross-entropy		& loss function\\
	\end{tabular}
	\vspace{-0.3cm}
\end{table}


\vspace{-0.3cm}
\subsection{Hardware\slash Software Versions}
\label{sec:versions}
Our models have been trained and tested on the following Hardware and Software versions :

\textbf{Hardware:} For training, validation and testing we used a single TITAN RTX GPU with 24 GBs of memory , and Intel(R) Xeon(R) CPU E5-1620 v3 @ 3.50GHz CPU.

\textbf{Software:} We used these software versions: Ubuntu LTS 18.04 , CUDA 11,  Tensorflow 2.4.1,  Python 3.6.9, Keras 2.4.0, NLTK 3.6.2, numpy 1.19.5, scipy 1.7.3,  and sklearn 1.0.2