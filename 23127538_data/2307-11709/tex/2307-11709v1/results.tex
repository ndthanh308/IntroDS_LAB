\begin{table*}[t]
	\centering
\caption{Metric scores for SMN encoder ensembled with each basic models compared against HAN encoder ensembled with the same baselines. T-test is conducted between +SMN and +HAN to reject a null hypothesis with a P-value <0.05 }
\vspace{-0.2cm}
\begin{tabular}{lllllllllllll} 
 	\cline{2-12}
	& \multicolumn{1}{|p{2.5cm}|}{Java}                                 	& \multicolumn{4}{c|}{METEOR}                                                                                  			& \multicolumn{4}{c|}{USE}                                        			&\multicolumn{2}{c|}{BLEU}   	&  \\
	& \multicolumn{1}{|p{2.5cm}|}{\textbf{Basic}} 	&{+HAN}  		&{+SMN} 			& {T-test}        		&\multicolumn{1}{c|}{P-value}  	   	& {+HAN}		& {+SMN} 		& {T-test}   	&\multicolumn{1}{c|}{P-value}       	& {+HAN}		&\multicolumn{1}{c|}{+SMN} 	&  \\ \cline{2-12}
	& \multicolumn{1}{|l|}{code2seq}     			& {32.01}		& {35.82} 			& {23.60}   		&\multicolumn{1}{l|}{<0.01} 	    	& {47.01}		& {52.61}         		& {33.09}        	&\multicolumn{1}{l|}{<0.01}       		& {16.87}     	&\multicolumn{1}{l|}{19.95}  	&    \\
	& \multicolumn{1}{|l|}{attendgru}     			& \textbf{34.30}	& {36.16} 			& {12.20}   		&\multicolumn{1}{l|}{<0.01} 	    	& \textbf{50.59}	& {53.28}         		& {18.16}        	&\multicolumn{1}{l|}{<0.01}    	    	& \textbf{19.23}	&\multicolumn{1}{l|}{\textbf{20.52}}  	&    \\
	& \multicolumn{1}{|l|}{transformer}     		& {34.00}		& \textbf{36.19} 	& {14.42}   		&\multicolumn{1}{l|}{<0.01} 		& {50.45}		& \textbf{53.40}         	& {20.12}        	&\multicolumn{1}{l|}{<0.01}            	& {18.60}     	&\multicolumn{1}{l|}{20.31}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru}     		& {33.40}		& {36.08} 			& {17.13}   		&\multicolumn{1}{l|}{<0.01} 	 	& {48.80}		& {52.91}          		& {26.10}        	&\multicolumn{1}{l|}{<0.01}	    	& {18.29}		&\multicolumn{1}{l|}{20.17}  	&    \\ \cline{2-12}
\end{tabular}

\hspace{0.01cm}
\begin{tabular}{lllllllllllll} 
 	\cline{2-12}
	& \multicolumn{1}{|p{2.5cm}|}{Python}                                 	& \multicolumn{4}{c|}{METEOR}                                                                                  			& \multicolumn{4}{c|}{USE}                                        			&\multicolumn{2}{c|}{BLEU}   	&  \\
	& \multicolumn{1}{|p{2.5cm}|}{\textbf{Basic}} 	&{+HAN}  		&{+SMN} 			& {T-test}        			&\multicolumn{1}{c|}{P-value}  	   	& {+HAN}		& {+SMN} 		& {T-test}   	&\multicolumn{1}{c|}{P-value}       	& {+HAN}		&\multicolumn{1}{c|}{+SMN} 	&  \\ \cline{2-12}
	& \multicolumn{1}{|l|}{code2seq}     			& {23.93}		& {27.14} 			& {14.89}   			&\multicolumn{1}{l|}{<0.01} 	    	& {37.39}		& {41.73}         		& {21.01}        	&\multicolumn{1}{l|}{<0.01}       		& {16.50}     	&\multicolumn{1}{l|}{19.61}  	&    \\
	& \multicolumn{1}{|l|}{attendgru}     			& {28.74}		& {29.76} 			& {05.94}   			&\multicolumn{1}{l|}{<0.01} 	    	& {42.57}		& {44.51}         		& {11.68}        	&\multicolumn{1}{l|}{<0.01}    	    	& {21.27}		&\multicolumn{1}{l|}{22.41}  	&    \\
	& \multicolumn{1}{|l|}{transformer}     		& \textbf{30.18}	& \textbf{30.8} 		& {{\color{white}0}3.82}   	&\multicolumn{1}{l|}{<0.01} 		&\textbf{43.49}	& \textbf{45.30}         	& {11.40}        	&\multicolumn{1}{l|}{<0.01}            	& \textbf{22.77}	&\multicolumn{1}{l|}{\textbf{23.30}}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru}     		& {28.9}		& {29.79} 			& {{\color{white}0}5.19}   	&\multicolumn{1}{l|}{<0.01} 	 	& {41.76}		& {43.72}          		& {11.42}        	&\multicolumn{1}{l|}{<0.01}	    	& {22.08}		&\multicolumn{1}{l|}{23.12}  	&    \\ \cline{2-12}
\end{tabular}
\label{tab:rq1}
\vspace{-0.6cm}
\end{table*}
\begin{table*}[t]
	\vspace{0.3cm}
	\centering
	\caption{Features of the Difference set. Diff(\%) column contains the size of the difference set. Same column metric scores for the same set. +HAN and +SMN indicate metric scores over the difference set for ensemble of the basic models with the baseline and our approach respectively. T-test is conducted between the +SMN and +HAN scores over the difference set.}
	\vspace{-0.2cm}
\begin{tabular}{@{}lllllllllllllll|ll} 
 	\cline{2-16}
	& \multicolumn{1}{|p{1.25cm}|}{Java}                     &\multicolumn{1}{c|}{Diff}        						& \multicolumn{5}{c|}{METEOR}                                                                 					& \multicolumn{5}{c|}{USE}           													&\multicolumn{3}{c|}{BLEU}  	&  \\
	& \multicolumn{1}{|p{1.25cm}|}{\textbf{Basic}} &\multicolumn{1}{c|}{ (\%)}        		&{Same}		&{+HAN} 			&{+SMN } 	& {T-test}   	&\multicolumn{1}{c|}{P-val} 		& {Same} 		& {+HAN} 			& {+SMN }	& {T-test} 		&\multicolumn{1}{c|}{P-val}  	 & {Same}		&\multicolumn{1}{c}{+HAN}  		&\multicolumn{1}{c|}{+SMN } 	&  \\ \cline{2-16}
	& \multicolumn{1}{|l|}{code2seq}     			&\multicolumn{1}{l|}{87.13}		&{60.11}		&{27.86} 			&{32.24}		& {23.71}   	&\multicolumn{1}{l|}{<0.01}		&{71.93}		& {43.33}                	& {49.76} 		& {33.40} 		&\multicolumn{1}{l|}{<0.01}         &{49.17}		&\multicolumn{1}{c}{12.36}  		&\multicolumn{1}{l|}{15.49}    \\
	& \multicolumn{1}{|l|}{attendgru}     			&\multicolumn{1}{l|}{79.43}		&{56.58}		&{28.53} 			&{30.88}		& {12.23}   	&\multicolumn{1}{l|}{<0.01} 		&{69.36}		& {45.73}                 	& {49.12} 		& {18.25} 		&\multicolumn{1}{l|}{<0.01}  	&{46.60}		&\multicolumn{1}{c}{11.95}  		&\multicolumn{1}{l|}{12.39}    \\
	& \multicolumn{1}{|l|}{transformer}     		&\multicolumn{1}{l|}{80.91}		&{57.90}		&{28.36} 			&{31.06}		& {14.46}   	&\multicolumn{1}{l|}{<0.01} 		&{69.40}		& {45.98}                 	& {49.63} 		& {20.23}  		&\multicolumn{1}{l|}{<0.01}          &{48.23}		&\multicolumn{1}{c}{12.45}  		&\multicolumn{1}{l|}{13.65}    \\
	& \multicolumn{1}{|l|}{codegnngru}     		&\multicolumn{1}{l|}{82.67}		&{59.02}		&{28.03} 			&{31.28}		& {17.20}   	&\multicolumn{1}{l|}{<0.01} 		&{71.05}		& {44.14}                 	& {49.11} 		& {26.32}    	&\multicolumn{1}{l|}{<0.01}	&{48.62}		&\multicolumn{1}{c}{12.24}  		&\multicolumn{1}{l|}{12.24}    \\ \cline{2-16}
\end{tabular}
\begin{tabular}{@{}lllllllllllllll|ll} 
 	\cline{2-16}
	& \multicolumn{1}{|p{1.25cm}|}{Python}                         &\multicolumn{1}{c|}{Diff}        						& \multicolumn{5}{c|}{METEOR}                                                                 & \multicolumn{5}{c|}{USE}           								&\multicolumn{3}{c|}{BLEU}  	&  \\
	& \multicolumn{1}{|p{1.25cm}|}{\textbf{Basic}} &\multicolumn{1}{c|}{(\%)}        	&{Same}		&{+HAN} 			&{+SMN } 	& {T-test}   			&\multicolumn{1}{c|}{P-val} 		&{Same}			& {+HAN} 				& {+SMN }	& {T-test} 			&\multicolumn{1}{c|}{P-val}  	 	&{Same}    		&\multicolumn{1}{c}{+HAN}  		&\multicolumn{1}{c|}{+SMN } 	&  \\ \cline{2-16}
	& \multicolumn{1}{|l|}{code2seq}     			&\multicolumn{1}{l|}{90.15}	&{94.45}		&{16.23} 			&{19.79}		& {14.90}   			&\multicolumn{1}{l|}{<0.01}	&{95.73}		& {31.02}                	& {35.83} 		& {21.05} 		&\multicolumn{1}{l|}{<0.01}        &{93.49}       	&\multicolumn{1}{c}{7.97}  		&\multicolumn{1}{l|}{11.54}    \\
	& \multicolumn{1}{|l|}{attendgru}     			&\multicolumn{1}{l|}{84.98}	&{91.08}		&{17.72} 			&{18.92}		& {{\color{white}0}5.94}   	&\multicolumn{1}{l|}{<0.01} 	&{93.07}		& {33.64}                 	& {35.92} 		& {11.70} 		&\multicolumn{1}{l|}{<0.01}  	&{89.66}		&\multicolumn{1}{c}{8.71}  		&\multicolumn{1}{l|}{10.12}    \\
	& \multicolumn{1}{|l|}{transformer}     		&\multicolumn{1}{l|}{82.94}	&{89.28}		&{18.02} 			&{18.78}		& {{\color{white}0}3.82}   	&\multicolumn{1}{l|}{<0.01} 	&{92.00}		& {33.51}                 	& {35.69} 		& {11.41}  		&\multicolumn{1}{l|}{<0.01}        &{87.50}        	&\multicolumn{1}{c}{9.04}  		&\multicolumn{1}{l|}{{\color{white}0}9.56}    \\
	& \multicolumn{1}{|l|}{codegnngru}     		&\multicolumn{1}{l|}{82.97}	&{91.34}		&{16.09} 			&{17.16}		& {{\color{white}0}5.20}   	&\multicolumn{1}{l|}{<0.01} 	&{93.34}		& {31.18}                 	& {33.53} 		& {11.43}    	&\multicolumn{1}{l|}{<0.01}	&{90.06}		&\multicolumn{1}{c}{7.46}  		&\multicolumn{1}{l|}{{\color{white}0}8.73}    \\ \cline{2-16}
\end{tabular}
\vspace{0.1cm}
\label{tab:rq2}
\vspace{-0.3cm}
\end{table*}
%\newpage
\section{Experimental Results}
This section includes experimental results for all three of the RQs we ask in Section~\ref{sec:rqs}.


\subsection{RQ1: Comparison against HANcode baseline}
\label{sec:rq1}
We report our evaluation results in Table~\ref{tab:rq1}. The statistical tests are relative to models ensembled with HAN instead of SMN. We make three interesting observations for the Java dataset. First, every ensemble with {\small \texttt{SMNcode}}  achieves statistically significant performance gains when compared to the individual baseline models. Second, the highest performing ensemble for USE and METEOR scores is {\small \texttt{transformer+SMNcode}} slightly better than {\small \texttt{attendgru+SMNcode}}. We posit that {\small \texttt{transformer}} as a word-level self attention model complements our statement-level approach more so than {\small \texttt{attendgru}}~. 

For the Python dataset, in addition to the observations from ensembles over Java dataset, one observation stands out. The ensemble {\small \texttt{transformer+HAN}} achieves scores higher than {\small \texttt{attendgru+HAN}} model which performed best among HAN combination for the Java dataset. This may indicate that python dataset might specially benefit from token-level self-attention. We further evaluate the differences between the two datasets by comparing the difference set, as explained in Section~\ref{sec:methodology}.

In Table~\ref{tab:rq2} we present the features of the difference set between our basic models ensembled with {\small \texttt{SMNcode}} and the same models ensembled with {\small \texttt{HANcode}} for both datasets. Overall, we observe that our approach improves performance over a large number of summaries. For both datasets, we observe a large difference set. 

For the Java dataset, we observe that 79-87\% of the summaries are part of the difference set. We observe each difference set is improved by a statistically significant score for all three metrics when compared to the baseline. We found that the small set with same summaries for both approaches has a really high score for every model. One possible reason is that these summaries have descriptive names. For example, method name : DrawCartesianMap() has the reference summary ``Draws Cartesian Map''. These are relatively easy for any approach to get right. This makes the difference set a more reliable way to look under the hood and see where our approach really makes a difference. With a 10\% or higher score difference over the baseline HAN, we posit our memory network approach improves the state-of-the-art in statement based encoders.

For the Python dataset we observe that  82-90\% of the summaries are part of the difference set. We observe a similar trend to the Java dataset in terms of improvement in metric scores when compared to the baseline. We observe a three key differences : 1) a slightly larger difference set, 2) much higher score for the same set, and 3) lower delta and scores for the difference set. We created this dataset explicitly for this paper, so these observations are very important. First, the slightly larger difference set indicates that overall there are less number of functions that are easy to summarize. This makes sense as the Java summaries were generated by JavaDocs, but the Python summaries were extracted from various repositories, and thus are less likely to follow consistent naming conventions as observed for Java. Second, upon further investigation of the same set we found that the high scores on the same set are from functions with generic summaries like ``Takes input X and Y''. We split the dataset by project to reduce the likelihood of data leaks between training and testing sets. However, within the project some generic summaries tend to repeat for non-duplicate functions. Third, the delta between our approach and the baseline as well as overall scores are lower for the difference set. Although the overall scores in Table~\ref{tab:rq1} seem similar for Python and Java, difference set analysis reveals that Python dataset is more challenging. However, the SMN encoder improves over the baseline in each instance.

\begin{table*}[t]
	\centering
	\caption{Metric scores for non-ensembled basic models as well as our baseline HANcode. SMNcode is the standalone model for our statement based memory encoder. T-tests are conducted between each other model and the SMNcode model. }
	\vspace{-0.2cm}
\begin{tabular}{llllllllllll} 
 	\cline{2-9}
	& \multicolumn{1}{|p{3cm}|}{Java} 								& \multicolumn{3}{p{3cm}|}{METEOR}   							& \multicolumn{3}{p{3cm}|}{USE}                                        			&\multicolumn{1}{p{1cm}|}{BLEU}  	&  \\
	& \multicolumn{1}{|p{3cm}|}{\textbf{Models}}         	&{Score} 		& {T-test}   			&\multicolumn{1}{c|}{P-value}       		& {Score} 		& {T-test}        			&\multicolumn{1}{c|}{P-value}  	     		&\multicolumn{1}{c|}{Score} 	&  \\ \cline{2-9}
	& \multicolumn{1}{|l|}{code2seq}     				&{32.07} 		& {14.11}   			&\multicolumn{1}{l|}{<0.01} 	    		& {47.65}          	& {20.59}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{16.71}  	&    \\
	& \multicolumn{1}{|l|}{attendgru}     				&{34.14} 		& {{\color{white}0}3.05}   	&\multicolumn{1}{l|}{<0.01} 	    		& {51.13}          	& {{\color{white}0}2.31}    	&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{19.07}  	&    \\
	& \multicolumn{1}{|l|}{transformer}     			&{33.97} 		& {{\color{white}0}3.87}   	&\multicolumn{1}{l|}{<0.01} 			& {51.16}         	& {{\color{white}0}2.06}     	&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{18.48}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru}     			&{33.22} 		& {{\color{white}0}8.07}   	&\multicolumn{1}{l|}{<0.01} 	 		& {49.06}          	& {12.49}        			&\multicolumn{1}{l|}{{\color{white}0}0.02}	&\multicolumn{1}{l|}{18.03}  	&    \\
	& \multicolumn{1}{|l|}{HANcode}     				&{27.38} 		& {36.22}   			&\multicolumn{1}{l|}{<0.01} 	     		& {40.33}          	& {53.52}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{14.07}  	&    \\
	& \multicolumn{1}{|l|}{SMNcode}     				&\textbf{34.68} 		& {-}   				&\multicolumn{1}{l|}{-} 				& {\textbf51.53}          	& {-}        				&\multicolumn{1}{l|}{-}                		&\multicolumn{1}{l|}{\textbf{19.47}}  	&    \\ \cline{2-9}
\end{tabular}

\begin{tabular}{llllllllllll}
 	\cline{2-9}
	& \multicolumn{1}{|p{3cm}|}{Python} 								& \multicolumn{3}{p{3cm}|}{METEOR}   							& \multicolumn{3}{p{3cm}|}{USE}                                        			&\multicolumn{1}{p{1cm}|}{BLEU}  	&  \\
	& \multicolumn{1}{|p{3cm}|}{\textbf{Models}}         	&{Score} 		& {T-test}   			&\multicolumn{1}{c|}{P-value}       		& {Score} 		& {T-test}        			&\multicolumn{1}{c|}{P-value}  	     		&\multicolumn{1}{c|}{Score} 	&  \\ \cline{2-9}
	& \multicolumn{1}{|l|}{code2seq}     				&{13.38} 		& {51.33}   			&\multicolumn{1}{l|}{<0.01} 	    		& {28.05}          	& {39.94}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{{\color{white}0}5.02}  	&    \\
	& \multicolumn{1}{|l|}{attendgru}     				&{26.25} 		& {{\color{white}0}3.81}   	&\multicolumn{1}{l|}{<0.01} 	    		& {41.28}          	& {{\color{white}0}2.26}    	&\multicolumn{1}{l|}{{\color{white}0}0.01}  	&\multicolumn{1}{l|}{17.62}  	&    \\
	& \multicolumn{1}{|l|}{transformer}     			&\textbf{27.96} 		&{{\color{white}0}4.91}   	&\multicolumn{1}{l|}{<0.01} 			& \textbf{42.43}         	& {{\color{white}0}3.79}     	&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{19.70}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru}     			&{25.85} 		& {10.01}   			&\multicolumn{1}{l|}{<0.01} 	 		& {39.15}          	& {{\color{white}0}9.21}    	&\multicolumn{1}{l|}{<0.01}			&\multicolumn{1}{l|}{19.12}  	&    \\
	& \multicolumn{1}{|l|}{HANcode}     				&{23.56} 		& {15.56}   			&\multicolumn{1}{l|}{<0.01} 	     		& {36.90}          	& {22.87}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{16.16}  	&    \\
	& \multicolumn{1}{|l|}{SMNcode}     				&{27.01} 		& {-}   				&\multicolumn{1}{l|}{-} 				& {41.71}          	& {-}        				&\multicolumn{1}{l|}{-}                		&\multicolumn{1}{l|}{\textbf{19.81}}  	&    \\ \cline{2-9}

\end{tabular}
%\vspace{0.1cm}

\label{tab:basic}
\vspace{-0.3cm}
\end{table*}

\vspace{-0.1cm}
\subsection{RQ2: Investigation of the improved summaries}
\label{sec:rq2}


To investigate the summaries improved by our encoder we 1) compare our non-ensembled encoder against basic models, and 2) discuss the features of the improved set.

\begin{comment}
\begin{table*}[!ht]
	\centering
	\caption{Metric scores for ensemble models compared against standalone basic models and the HAN baseline. T-test is conducted between the scores distributions for ensemble and corresponding the basic model in Table~\ref{tab:basic}.}
\begin{tabular}{llllllllllll} 
 	\cline{2-9}
	& \multicolumn{1}{|p{3cm}|}{Java}                                 	& \multicolumn{3}{p{3cm}|}{METEOR}                                                                                  & \multicolumn{3}{p{3cm}|}{USE}                                        				&\multicolumn{1}{p{1cm}|}{BLEU}   	&  \\
	& \multicolumn{1}{|p{3cm}|}{\textbf{Ensembles}}   	& {Score} 		& {T-test}        			&\multicolumn{1}{c|}{P-value}  	     		&{Score} 		& {T-test}   			&\multicolumn{1}{c|}{P-value}       		&\multicolumn{1}{c|}{Score} 	&  \\ \cline{2-9}
	& \multicolumn{1}{|l|}{code2seq+SMNcode}     		&{35.82} 		& {24.31}   			&\multicolumn{1}{l|}{<0.01} 	    		& {52.61}          	& {30.60}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{19.95}  	&    \\
	& \multicolumn{1}{|l|}{attendgru+SMNcode}     		&{36.16} 		& {13.78}   			&\multicolumn{1}{l|}{<0.01} 	    		& {53.28}         	& {15.52}        			&\multicolumn{1}{l|}{<0.01}    			&\multicolumn{1}{l|}{20.52}  	&    \\
	& \multicolumn{1}{|l|}{transformer+SMNcode}     	&{36.19} 		& {14.78}   			&\multicolumn{1}{l|}{<0.01} 			& {53.40}         	& {15.84}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{20.31}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru+SMNcode}     	&{36.08} 		& {18.67}   			&\multicolumn{1}{l|}{<0.01} 	 		& {52.91}          	& {25.30}        			&\multicolumn{1}{l|}{<0.01}			&\multicolumn{1}{l|}{20.17}  	&    \\
	& \multicolumn{1}{|l|}{HANcode+SMNcode}     		&{34.85} 		& {42.35}   			&\multicolumn{1}{l|}{<0.01} 	     		& {51.00}         	& {56.88}        			&\multicolumn{1}{l|}{<0.01}                		&\multicolumn{1}{l|}{19.53}  	&    \\ \cline{2-9}
\end{tabular}

\begin{tabular}{llllllllllll} 
 	\cline{2-9}
	& \multicolumn{1}{|p{3cm}|}{Python}                                 	& \multicolumn{3}{p{3cm}|}{METEOR}                                                                                  & \multicolumn{3}{p{3cm}|}{USE}                                        				&\multicolumn{1}{p{1cm}|}{BLEU}   	&  \\
	& \multicolumn{1}{|p{3cm}|}{\textbf{Ensembles}}   	& {Score} 		& {T-test}        			&\multicolumn{1}{c|}{P-value}  	     		&{Score} 		& {T-test}   			&\multicolumn{1}{c|}{P-value}       	&\multicolumn{1}{c|}{Score} 	&  \\ \cline{2-9}
	& \multicolumn{1}{|l|}{code2seq+SMNcode}     		& {27.14}          	& {45.29}        			&\multicolumn{1}{l|}{<0.01}                		&{41.73} 		& {50.14}   			&\multicolumn{1}{l|}{<0.01} 	    	&\multicolumn{1}{l|}{19.61}  	&    \\
	& \multicolumn{1}{|l|}{attendgru+SMNcode}     		& {29.76}         	& {20.42}        			&\multicolumn{1}{l|}{<0.01}    			&{44.51} 		& {19.94}   			&\multicolumn{1}{l|}{<0.01} 	    	&\multicolumn{1}{l|}{22.41}  	&    \\
	& \multicolumn{1}{|l|}{transformer+SMNcode}     	& {30.80}         	& {18.24}        			&\multicolumn{1}{l|}{<0.01}                		&{45.30} 		& {19.02}   			&\multicolumn{1}{l|}{<0.01} 		&\multicolumn{1}{l|}{23.30}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru+SMNcode}     	& {29.79}          	& {22.44}        			&\multicolumn{1}{l|}{<0.01}			&{43.72} 		& {26.55}   			&\multicolumn{1}{l|}{<0.01} 	 	&\multicolumn{1}{l|}{23.12}  	&    \\
	& \multicolumn{1}{|l|}{HANcode+SMNcode}     		& {29.63}         	& {31.13}        			&\multicolumn{1}{l|}{<0.01}                		&{43.34} 		& {36.41}   			&\multicolumn{1}{l|}{<0.01} 	     	&\multicolumn{1}{l|}{22.45}  	&    \\ \cline{2-9}
\end{tabular}
\vspace{0.1cm}
\label{tab:ensembles}
\vspace{-0.6cm}
\end{table*}
\end{comment}

We observe that our SMN encoder without ensemble with a non-statement based approach achieves higher USE, METEOR, and BLEU scores compared to all the basic models for Java dataset and all but one basic model for the Python dataset. We show the results of our evaluation in Table~\ref{tab:basic}. The top subtable reports the results for the Java dataset and bottom subtable reports the results for the Python dataset. The statistical t-tests are performed between our approach {\small \texttt{SMNcode}} and other models, so we only report those for the basic models and our baseline.

For the Java dataset we found {\small \texttt{SMNcode}} outperforms every other approach. The biggest difference we observe is against {\small \texttt{HANcode}}, another statement based approach, with a 7.30 METEOR, 11.20 USE, and 5.40 BLEU score improvement. We posit that {\small \texttt{HANcode}} relies on the pre-trained embedding mentioned in their paper or requires a much larger dataset to learn statement level information compared to {\small \texttt{SMNcode}}. We also observe that {\small \texttt{SMNcode}} achieves significantly higher score than every model with a P-value <0.02. This observation indicates that although we combine our statement based encoder with other approaches in order to take advantage of previous advancements in literature, our encoder can be perform just as well or even better than state-of-the-art independently. These results corroborate our initial results, that memory networks encode statement-based information better than HAN.

For the Python dataset we found that {\small \texttt{SMNcode}} outperforms every baseline except {\small \texttt{transformer}}. We observe that {\small \texttt{SMNcode}} achieves 3.45 METEOR, 4.81 USE, and  3.65 BLEU higher than {\small \texttt{HANcode}}. We also observe that {\small \texttt{transformer}} achieves  0.95 METEOR, 0.72 USE higher than our approach, but 0.11 BLEU lower than our approach. It appears that in contrast to the Java dataset, Python dataset benefits from token level self-attention more than statement-level self-attention overall. Recall from Section~\ref{sec:rq2} we noticed some unique features of the Python dataset. We posit that the small set of methods that are easier to summarize may benefit from copy-attention mechanism in the transformer models. Python dataset also has a larger number of challenging summaries that are harder to summarize because of lack of a generalizable structure which may further benefit from token-level self-attention. However, as we observed in RQ1,  {\small \texttt{transformer}} is further improved by addition of {\small \texttt{SMNcode}}. The ensemble {\small \texttt{transformer+SMNcode}} achieves 8\% higher scores compared to {\small \texttt{transformer}}. We suspect that for a subset of summaries, statement-level information is more important than token-level information. We test our hypothesis further by evaluating this improved subset in Table~\ref{tab:improvement}.

\begin{table}[h]
\centering
\vspace{-0.2cm}
\caption{METEOR scores for the improved set, where our approach SMNcode achieves higher scores when compared to the basic models and our baseline.}
\vspace{-0.2cm}
\begin{tabular}{lllllllllll} 
 	\cline{2-8}
	& \multicolumn{1}{|p{1.5cm}|}{}                                 	& \multicolumn{3}{c|}{Java}                                                                               & \multicolumn{3}{c|}{Python}                       &  \\
	& \multicolumn{1}{|p{1.5cm}|}{\textbf{Models}}  	& {Size} 		& {Base}    	&\multicolumn{1}{c|}{SMN}  	&{Size} 		& {Base} 		&\multicolumn{1}{c|}{SMN} &  \\ \cline{2-8}
	& \multicolumn{1}{|l|}{code2seq}     			& {47.20}          		& {25.21}        	&\multicolumn{1}{l|}{39.80}                	&{53.10} 		& {10.73} 		&\multicolumn{1}{l|}{41.97}  	&    \\
	& \multicolumn{1}{|l|}{attendgru}     			& {40.43}         		& {23.27}        	&\multicolumn{1}{l|}{36.69}    		&{38.19} 		& {17.99}   	&\multicolumn{1}{l|}{33.89}  	&    \\
	& \multicolumn{1}{|l|}{transformer}     		& {42.24}         		& {25.18}        	&\multicolumn{1}{l|}{38.87}                	&{36.28} 		& {16.09}   	&\multicolumn{1}{l|}{29.95}  	&    \\
	& \multicolumn{1}{|l|}{codegnngru}     		& {44.21}          		& {24.11}        	&\multicolumn{1}{l|}{37.99}		&{39.90} 		& {12.94}   	&\multicolumn{1}{l|}{28.74}  	&    \\
	& \multicolumn{1}{|l|}{HANcode}     			& {61.21}         		& {22.81}        	&\multicolumn{1}{l|}{39.78}                	&{44.66} 		& {13.15}   	&\multicolumn{1}{l|}{31.36}  	&    \\ \cline{2-8}
\end{tabular}
\vspace{-0.4cm}
\label{tab:improvement}
\end{table}

\begin{table*}[t]
	\centering
	\caption{Comparison of different configuration and statistical tests against best-performing configuration SMNcode}
\begin{tabular}{llllllllllll} 
 	\cline{2-9}
	& \multicolumn{1}{|p{3cm}|}{Java}                                 	& \multicolumn{3}{p{3cm}|}{METEOR}                                                                                  & \multicolumn{3}{p{3cm}|}{USE}                                        				&\multicolumn{1}{p{1cm}|}{BLEU}   	&  \\
	& \multicolumn{1}{|p{1.85cm}|}{\textbf{Models}}         	& {Score} 		& {T-test}        	&\multicolumn{1}{c|}{P-value}  	     			&{Score} 	& {T-test}   	&\multicolumn{1}{c|}{P-value}       		&\multicolumn{1}{c|}{Score} 	&  \\ \cline{2-9}
	& \multicolumn{1}{|l|}{SMNcode}     				& {34.68}         	& {-}        		&\multicolumn{1}{l|}{-}                			&{51.53} 	& {-}   		&\multicolumn{1}{l|}{-} 	    			&\multicolumn{1}{l|}{19.47}  	&    \\
	& \multicolumn{1}{|l|}{SMNcode-H1}     			& {34.39}         	& {2.66}    		&\multicolumn{1}{l|}{<0.01}              			&{50.85}	& {3.84}  		&\multicolumn{1}{l|}{<0.01}	    		&\multicolumn{1}{l|}{19.11} 	&    \\
	& \multicolumn{1}{|l|}{SMNcode-H2}     			& {34.62}          	& {0.37}       	&\multicolumn{1}{l|}{{\color{white}0}0.35}  		&{51.35}	& {1.04}  		&\multicolumn{1}{l|}{{\color{white}0}0.14}	&\multicolumn{1}{l|}{19.23} 	&    \\
	& \multicolumn{1}{|l|}{SMNcode-H4}     			& {34.45}          	& {1.25}       	&\multicolumn{1}{l|}{{\color{white}0}0.10}         	&{50.66}	& {4.91}  		&\multicolumn{1}{l|}{<0.01}	    		&\multicolumn{1}{l|}{19.42} 	&    \\
	& \multicolumn{1}{|l|}{SMNcode-H5}     			& {34.15}         	& {3.03}       	&\multicolumn{1}{l|}{<0.01}              			&{51.27}	& {1.49}  		&\multicolumn{1}{l|}{{\color{white}0}0.07}	&\multicolumn{1}{l|}{19.07} 	&    \\
	& \multicolumn{1}{|l|}{SMNcode-summary vector}     	& {34.06}         	& {3.57}       	&\multicolumn{1}{l|}{<0.01}              			&{50.28}	& {7.18}  		&\multicolumn{1}{l|}{<0.01}	    		&\multicolumn{1}{l|}{18.92} 	&    \\
	& \multicolumn{1}{|l|}{SMNcode-EOS Encoding}     	& {34.18}         	& {2.76}       	&\multicolumn{1}{l|}{<0.01}              			&{50.54}	& {5.60}  		&\multicolumn{1}{l|}{<0.01}	    		&\multicolumn{1}{l|}{19.01} 	&    \\ \cline{2-9}
\end{tabular}
\label{tab:smnrq3}
\vspace{-0.3cm}
\end{table*}

The way to interpret Table ~\ref{tab:improvement} is that for the first entry, there is a subset of  47.20\% of the methods in the Java dataset where our standalone encoder {\small \texttt{SMNcode}} performs better than {\small \texttt{code2seq}} where the average METEOR score for our approach is 39.80 compared to 25.21 by {\small \texttt{code2seq}}. We observe that compared to the {\small \texttt{transformer}} baseline there are 36.28 percent of the functions in the Python dataset where our encoder achieves higher METEOR,USE, and BLEU scores. This observation confirms our suspicion as for subset  {\small \texttt{SMNcode}} achieves almost 90\% higher METOR scores on average than {\small \texttt{transformer}}. This observation also means there is a comparable subset where {\small \texttt{transformer}} does better in order to get slightly higher METEOR and USE  score in Table~\ref{tab:basic}. The ensembles in RQ1 benefit from both of these subsets and hence achieve the highest score.

\vspace{-0.1cm}
\subsection{RQ3: Design Decisions and Configurations}

In Table~\ref{tab:smnrq3} we compare the different design decisions and configurations against our best performing approach {\small \texttt{SMNcode}}. For our first test, we observe that $h=3$ iterations provide achieves the best overall performance. We find that {\small \texttt{SMNcode-H2}} where $h=2$ performs almost as well as {\small \texttt{SMNcode}} where $h=3$. The difference between these could not be confirmed as statistically significant with a p-value greater than 0.05. While  {\small \texttt{SMNcode-H4}} is a close third, we see that {\small \texttt{SMNcode-H5}} gets the lowest scores The performance the performance peaks around $h=3$. We posit that {\small \texttt{SMNcode-H2}} attends to the two most important statements in the input and while a third statement adds some improvement, this selection of the most important facts leads of the improvements shown by our approach. Therefore, too many memory iterations decreases the quality of this selection. The size of the model and training time also increase with the number of iterations, this could make $h=2$ a good candidate for future work if there are resource constraints.

For the second test, we see that {\small \texttt{SMN-summary vector}}  achieves lower scores than our best performing configuration. We posit that because the summary vector already updates the GRU within the memory network because of RNN back-propagation, it does not add any new information for the memory network. In fact, the scores are even lower than the baseline {\small \texttt{attendgru}}. This suggests that summary so far may be acting as a distractor to the memory network. However, a fixed $Q$ matrix in {\small \texttt{SMNcode}} allows the model to learn which statements are more important using self-attention between statements within the memory network.

For the third test, we find that positional encoding achieves significantly higher scores for all three metrics compared to EOS encoding. We observe POS encoding in {\small \texttt{SMNcode}}  achieves 0.5 METEOR, 0.99 USE and 0.46 BLEU improvements over the EOS encoding. With p-values below 0.01 these improvements are statistically significant. We find that EOS encoding saw improvements in natural language application~\cite{kumar2016ask} , it links every statement with the representation of the previous statement. However, it does not encode the order in which the statement occurs in code. We posit that this explicit positional information better informs the memory network as source code is executed sequentially.

