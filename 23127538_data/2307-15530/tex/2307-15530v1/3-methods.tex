% \vspace{-6pt}
\section{Methodology}
% \vspace{-6pt}

% In this part, we give the problem setup in Section~\ref{prob-setup}. Section \ref{subsec:framework} introduces the high-level idea and the framework of the proposed CoS. We describe a group consensus generation framework in Section~\ref{subsec:vqgc}, and a group consensus-guided strategy in Section~\ref{subsec:gcs}.



\subsection{Problem Setup}
\label{prob-setup}
\paragraph{\textbf{Group Consensus-guided Dec-POMDP.}}
We formalize our problem with multiple agents as
a group consensus-guided decentralized partially observable Markov decision process (GC-Dec-POMDP). A GC-Dec-POMDP is given by a tuple $(\mathcal{I}, \mathcal{S}, \mathcal{O}, \mathcal{G}, \mathcal{A}, P, R, \gamma)$. 
$\mathcal{I}$ denotes the set of agents indexed by $i \in {1,...,N}$.
% $N$ denotes the number of agents indexed by $i \in \{1,...,N\}$. 
Let $\mathcal{S}$ represent the global state space and $\mathcal{A}$ be the action space of the agent. $\mathcal{G}={\{e_1, e_2, ..., e_{K}\}}$ denotes the group codebook to generate the group consensus embedding $e^j$ belonging to the group $j$ for each agent $i$ in every time step, which divides all the agents into various parts to realize the corresponding density of coordination. 
At each time step $t$, each agent $i$ chooses its action $a^i_t \in \mathcal{A}$ based on the group consensus policy $\pi^{{gc}^j_i}$ conditioned only on the group consensus embedding $e^j$ and the group guided policy $\pi^{{gg}^j_i}$ conditioned on $e^j$ and its observation drawn from the observation function $O(s_t, i)$ where $s_t \in \mathcal{S}$, and receive the environmental reward $r_t$ given by its reward function $R^i:\mathcal{O} \times \mathcal{A} \to \mathbb{R}$.
$P(s_{t+1}|s_t,\bm{a}_t)$ is the dynamics function that gives the distribution of the next state $s_{t+1}$ at the current state $s_t$ to execute the joint action $\bm{a}_t=\{a^i_t\}_{i=1}^N$.
In summary, each agent learns the group consensus policy $\pi^{{gc}^i}$ and the group guided policy $\pi^{gg^i}$ simultaneously, denoted as $\pi^i=(\pi^{{gc}^i}, \pi^{{gg}^i})$
, and the overall objective is to find the optimal joint policy $\bm{\pi}=(\pi_1,...,\pi_N)$ such that the discounted returns of each agent $G_i = {\sum\limits_{t = 1}^\infty  {{\gamma ^{t-1}}r_t^i}}$ are maximized, where $\gamma \in [0,1)$ is a discounted factor. 

\paragraph{\textbf{Optimization Target.}}
The overall goal is to maximize the cumulative return, denoted as:
\begin{equation}
\label{eq:ret}
\eta  = {\mathbb{E}_{\bm{a} \sim \bm{\pi} }}\left[ {\sum\limits_{k = 0}^\infty  {{\gamma ^k}} r({s_{t + k}},{\bm{a}_{t + k}})} \right],
\end{equation}
where $\bm{\pi}$ is the joint policy for all the agents, formulated as:
\begin{equation}
    \bm{\pi}(\bm{a}|\{o^i\}_{i=1}^N) \buildrel \Delta \over = \{\pi^{{gc}^i}(a^{gc^i_j} | e^j)  +  \pi^{{gg}^i}(a^{gg^i_j}|o^i,e^j) \}_{i=1}^N
\end{equation}
where $e^j$ is the group consensus embedding. Here, we drop the time step $t$ for simplification.


% Figure environment removed

% \vspace{-6pt}
\subsection{The Motivation and Framework}
\label{subsec:framework}
% \vspace{-6pt}



Effective collaboration is a crucial aspect of multi-agent systems and relies heavily on well-coordinated teamwork. One approach to achieving effective collaboration is through the use of consensual groups, which consist of one or more agents guided by a shared pattern to make higher-level decisions for completing a particular sub-task. To ensure the success of the consensual groups, we consider three fundamental principles:

a) \textbf{Group Consensus}: Agents within a group should strive to reach a consensus to accomplish a specific sub-task. The group consensus should be well-defined and distinguishable, with sufficient knowledge.

b) \textbf{Behavioral Diversity}: Both inter- and intra-group agents should exhibit diverse and varied behavioral patterns to promote exploration and better solutions.

c) \textbf{Dynamic Integration}: The multi-agent decision-making process should involve a dynamic combination of a group consensus policy and an individual policy guided by group knowledge. This approach will allow for flexibility in adapting to changing circumstances and enable the agents to make informed decisions in a collaborative manner.

% Effective collaboration is essential in multi-agent systems and usually relies on well-coordinated teamwork.
% A natural concept that occurs to mind is the consensual groups specialized in certain sub-tasks. The consensual group includes one or more agents guided by a common pattern to make higher-level decisions for finishing a certain sub-task.
% Moreover, behavioral diversity is indispensable and individual patterns should be incorporated into the consensual group.
% In summary, we consider three principles to reach more effective collaboration in multi-agent systems:

% a) \textbf{Group consensus}: Agents in the same group are expected to reach a consensus to accomplish a certain sub-task. The group consensus should contain enough messages and be identifiable.

% b) \textbf{Behavioral diversity}: Agents in both between and within groups should encompass different and varied behavior patterns, which are conducive to exploring and reaching a better solution space.

% c) \textbf{Dynamic Integration}: The multi-agent decision-making process should be equipped with the dynamic mixture of a group consensus policy and an individual policy guided by group knowledge.



% The mixing policies 
% Identifiable

Taking the above factors into consideration, we present the \textbf{C}onsensus-\textbf{o}riented \textbf{S}trategy (CoS) learning framework, shown in Figure~\ref{fig:main}.
The upper part is designed for extracting knowledgeable and distinguishable group consensus embedding. The lower part illustrates the group consensus-oriented strategy. 
Compare to common MARL methods, we innovatively introduce the VQ-VAE for learning group consensus embedding and propose an additional group consensus policy. This embedding module can be considered as observation augmentation, and the group consensus policy can be incorporated into most MARL algorithms to complement their policy.
% The proposed method can be plugged into any multi-agent algorithm.
Moreover, the network architecture and the detailed parameter settings can be found in Appendix~\ref{app:net} and~\ref{app:para}, respectively. The pseudo-code for CoS is provided in~\ref{app:algo}.

% \vspace{-6pt}
\subsection{Vector Quantized Group Consensus}
\label{subsec:vqgc}
% \vspace{-6pt}


Drawing inspiration from the vector-quantized variational autoencoder (VQ-VAE) proposed by~\cite{van2017vqvae}, we introduce a novel module called Vector Quantized Group Consensus (VQGC) to achieve identifiable and rich group consensus. The motivations behind this include: 
a) The discrete codebook in VQ-VAE can effectively group features with similar semantics into tighter clusters with better separation than traditional continuous representation methods. This enables more robust and efficient feature extraction in multi-agent settings.
b) The range of VQ encodings is highly controllable, and training the VQ encodings as group embeddings of agents leads to reduced variance, thus enhancing the stability of multi-agent collaboration.
c) The use of a discrete feature representation can mitigate the effects of semantic noise in stochastic environments by limiting the number of possible bias vectors. This, in turn, ensures the extraction of richer knowledge from the features.



% Inspired by the powerful vector-quantized variational autoencoder (VQ-VAE)~\cite{van2017vqvae}, we designed a module named Vector Quantized Group Consensus (VQGC) for realizing identifiable and rich group consensus. The motivations behind this include: 
% a) The discrete codebook in VQ-VAE can categorize features with similar semantics into tighter clusters with better separation compared to classic continuous representation methods;
% b) The range of VQ encodings is more controllable, and training the VQ encodings as group embeddings of agents can reach a smaller variance, which makes multi-agent collaboration more stable;
% c) The discrete feature representation can combat semantic noise from stochastic environments due to limited bias vectors, which further guarantee the feature extraction of rich knowledge.


Thus, building on the powerful properties of VQ-VAE, we design the VQGC to extract the group consensus embeddings,  which enables neural models to learn similarities and differences between states better and generate a higher-level consensual representation to guide policy learning. Next, the design details are elaborated as follows.

% \vspace{-6pt}
\paragraph{\textbf{History Encoder}.}
At time step $t$, given the history transitions $\tau^i$ including observations $o^i$, action $a^i$, and reward $r^i$, we first aim to extract the feature by a history encoder parameterized by $\theta$, denoted as:
\begin{equation}
    z_e(\bm{\tau}^i) = q_\theta(\cdot|\bm{\tau}^i),
\end{equation}
where $\bm{\tau}^i:=\{ o^i_l, a^i_{l-1}, r^i_{l-1} \}_{l=t-c}^t$ and $c$ is the window size of the history chunk.

% \vspace{-6pt}
\paragraph{\textbf{Group Codebook}.}
The group codebook refers to a set of vectors used in the VQGC module. 
Specifically, it is defined as $\bm{e} = \{e^1,e^2,...,e^K\}, \forall e^j \in \mathbb{R}^{1 \times D}$, where $j$ is the $j^{th}$ entry and $D$ is the dimensionality of each entry.  
To obtain a powerful and robust group codebook, we introduce a  regularizer inspired by~\cite{fan2021multi} into the training process.
Euclidean space has limited representation capacity in a fixed dimension, and increasing the dimensionality will bring a large computation budget and training overfitting issues. Thus, we consider a hyperbolic space constraint to generate more powerful and knowledgeable embedding representations.
Thus, we give a definition of the Poincar\'e ball model~\cite{poincare1882theorie}.
\begin{definition}
The Poincar\'e ball model is a model of $n$-dimensional ($n \ge 3$) hyperbolic geometry in which the points of the geometry are in the $n$-dimensional unit ball.
\end{definition}

Let a Poincar\'e ball with dimension $d$ and radius $1$ be $\mathcal{P}^{d,1}:=\{\bm{e} \in \mathbb{R}^d, ||\bm{e}|| < 1\}$, and the operator $||\cdot||$ denotes the Euclidean $L^2$ norm. 
This corresponds to the Riemannian manifold $(\mathcal{P}^{d,1}, g_{\bm{e}})$, where $g_{\bm{e}} = (2/(1-||\bm{e}||^2))^2 g^E$ is the Riemannian metric tensor and $g^E$ denotes the Euclidean metric tensor. 
Then the distance between two vector $\bm{x}, \bm{y} \in \mathcal{P}^{d,1}$ can be computed as:
\begin{equation}
    d(\bm{x},\bm{y})=arcosh\left ( 1+2\frac{||\bm{x}-\bm{y}||^2}{(1-||\bm{x}||^2)(1-||\bm{y}||^2)}  \right ) .
\end{equation}

For inducing an appropriate structural bias on the group embedding space, we introduce a novel metric $\mathcal{L}_{\mathcal{P}}$ to constrain the embeddings into hyperbolic space with the Poincar\'e ball model, which is well-suited for the gradient-based optimization~\cite{nickel2017poincare}, formulated as:
\begin{equation}
\mathcal{L}_{\mathcal{P}}=\sum_{(\bm{e}^I,e^j)\in \mathcal{B}_e}  log\sigma( Sgn((\bm{e}^I,e^j)) \cdot D(\bm{e}^I,e^j))  ,
\end{equation}
where $D(\bm{e}^I,e^j) = \min_{i \in I} d(e^i, e^j)$ is the shortest Poincar\'e distance for the anchor $e^j$ and other entries $\bm{e}^I$ sampled from the buffer $\mathcal{B}_e$ that is used to save the latest $L$ group codebooks. This buffer at time step $t$ is denoted as $\mathcal{B}_t^e = \{e^1_l,e^2_l,...,e^K_l\}_{l=t-L}^t$.  $\sigma$ is a logistic activation function and $Sgn$ is a symbolic function defined as follows.
\begin{equation}
    Sgn((\bm{e}^I,e^j))=\left\{\begin{matrix}
        1, \ \ \ \ if \ \bm{e}^I \in Pos
         \\
        -1, \ \ \ if \ \bm{e}^I \in Neg
    \end{matrix}\right.
\end{equation}
where the set $Pos$ denotes the identical group embedding $e^i_l$ with different time steps. The set $Neg$ represents the different group embeddings $e^j_l$, where $j \ne i, \forall l \in (t-L,t]$.

By minimizing $\mathcal{L}_{\mathcal{P}}$, we can pull together the group embeddings with the same identifier to relieve the non-stationarity of drastic changes and push apart different group embeddings to obtain distinguishable representations.

% the group codebook and $I \in \{1,2,...,K\} \setminus j$.

    % D(\bm{e}^I,e^j) = \min_{i \in I} d(e^i, e^j), \\

Given the current group codebook $\bm{e}$ and the encoded feature $z_e(\bm{\tau}^i)$, the nearest matching mechanism can be denoted as $Quantize$, formulated as follows.
\begin{equation}
\label{eq:quantize}
    \begin{matrix}
    Quantize(z_e(\bm{\tau}^i)) = e^j , \ \  \
    j=argmin_{k} ||z_e(\bm{\tau}^i) - e^k|| ,
    \end{matrix}
\end{equation}
where $k$ is the index of the length of the group codebook. In the following, we denote Equation~(\ref{eq:quantize}) as $e^j$ to represent the nearest quantized vector for brevity.

% \vspace{-6pt}
\paragraph{\textbf{History Decoder}.}

The history decoder takes the $z_q(\bm{\tau}^i)$ as the input to reconstruct the input $\bm{\tau}^i$ that is to maximize the log-likelihood:
\begin{equation}
    \mathcal{L}_{recon}=\mathbb{E}_{z_q}\left [  log p(\hat \tau^i | z_q(\tau^i))  \right ].
\end{equation}
Thus, the decoding of the history transitions can capture the dynamics of the environment to a large extent.

% The dynamic contained in the history transitions can be well-captured by our VQGC.

% \vspace{-6pt}
 
\paragraph{\textbf{Training Objective}.}
Based on VQVAE, the training of VQGC is equipped with the stop gradient technique, summarized as follows.
\begin{equation}
\label{eq:vqgc}
    \begin{matrix}
    \mathcal{L}_{VQGC}=\underbrace{log p _\theta(\hat {\tau}^i | z_q({\tau}^i))}_{history \ decoder} + \beta \underbrace{||sg[z_e({\tau}^i)]-e^j||^2_2}_{history \ encoder}
     \\
    +  \underbrace{||z_e({\tau}^i),sg[e^j]||^2_2 + \mathcal{L}_{\mathcal{P}} }_{group \ codebook}
    \end{matrix},
\end{equation}
where the operator $sg$ indicates the stop of gradient backpropagation. 
$\beta$ is the hyper-parameter preventing the encoder outputs from fluctuating between different code vectors.

So far, the group consensus embedding is outputted by our VQGC. Through training, the group consensus embedding fully incorporates rich dynamical knowledge, as well as robust and distinguishable representations. 
Next, we will introduce the group consensus-oriented strategy to use this extracted knowledge to provide global intention and individual guidance for the multi-agent decision-making process.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Group Consensus-oriented Strategy}
\label{subsec:gcs}

Our group consensus-oriented strategy includes two parts: group consensus policy aims to generate the higher-level decision from the global and long-term perspective and group-guided policy is used to make individual decisions condition on the group consensus, elaborated as follows.

\paragraph{\textbf{Group Consensus Policy (GCP)}.}

Here, we propose using a hyper-network architecture where a primary network $f_h$ generates the weights to  parameterize all the base layer $f_b^i$, where $ i \in \{1,2,..., K\}$. 
A hyper-network is a network that generates the weights for another network.
Specifically, VQGC generates the group consensus embedding $e^j$ for each agent $i$. Then the hyper-MLP takes the group consensus embeddings $e^j$ as input and produces the weights for each Base-MLP network which performs fine-grained control denoted as $f_{b^j}=\pi^{gc^i}(a^{gc_j^i}|e^j)$.
The hyper-MLP can be formulated as $f_h: \mathbb{R}^d \mapsto \mathbb{R}^{dim(b^j)}$ which is parameterized by $h$, $d$ and $dim(b^j)$ are the dimensions of group embedding and base network, respectively.
The formulation of a Base-MLP $f_{b^j}$ is given by $f_{b^j}(e^j) = \bm{W}_1(\sigma (\bm{W}^T_2 e^T_j))$, where the weight matrix $\bm{W}_1$ and $\bm{W}_2$ are produced by Hyper-MLP $f_h$.
 
% 超网络提供统一的参数生成空间
%  超网络提供高阶的全局交互
% 超网络
The motivation for such a design centrally involved two aspects. On the one hand, agents from different groups are expected to solve different sub-tasks on different parameter spaces, which escalates the difficulties of policy learning. The hyper-network fed with different group consensus embeddings can well coordinate the potentially conflicting parameters in a unified space, which reduces the training complexity to some extent. On the other hand, the hyper-network captures higher-order interaction among group consensus embeddings, which is conducive to making global decisions.

% As a high-level idea, as for the agent $i$, VQGC generates the group consensus embedding $e^j$, and the primary Base-MLP network performs fine-grained control denoted as $f_{b^j}=\pi^{gc^i}(a^{gc_j^i}|e^j)$, and the parameter of this network is produced by the hypernet $f_h: \mathbb{R}^d \mapsto \mathbb{R}^{dim(b^j)}$ parameterized by $h$, where $d$ and $dim(b^j)$ are the dimensions of group embedding and primary network, respectively. Specifically, the formulation of a Base-MLP $f_{b^j}$ is given by $f_{b^j}(e^j) = \bm{W}_1(\sigma (\bm{W}^T_2 e^T_j))$, where $\bm{W}_1$ and $\bm{W}_2$ are produced by Hyper-MLP $f_h$.


\paragraph{\textbf{Group-Guided Policy (GGP)}.}

Besides the higher-level decision, the individual policy guided by the group that emphasizes the underlying dynamic of the environment is also essential. Here, we utilize the group consensus embedding $e^j$ as the context prompt to augment the observation $o^i$ for agent $i$ belonging to the group $j$, denoted as $\pi^{gg^i}(a^{gg^i_j}|o^i,e^j)$.
The group-guided policy is used to make individual decisions conditioned on the group consensus. 


% emphasizes the underlying dynamic

\paragraph{\textbf{Training Objective}.}
Borrow the policy gradient for perturbation network~\cite{fujimoto2019off}, the final executed action of the agent $i$ can be denoted as follows.
\begin{equation}
\label{eq:pi}
  u^i = \pi^{{gc}^i}( e^j)  +  \pi^{{gg}^i}(o^i,e^j)   
\end{equation}

With Equation~(\ref{eq:ret}) and (\ref{eq:pi}), the overall optimization objective for the expected cumulative return can be written as:
\begin{equation}
    \mathcal{J}=\mathbb{E}_{s,e^j,\bm{u}}\left [ \prod_{i=1}^{N}\left (  \pi^{gc^i}(e^j)+\pi^{gg^i}(o^i,e^j) \right ) \cdot Q_{\bm{\pi}}(s,\bm{u})  \right ] ,
\end{equation}
where $s\sim p^{\bm{\pi}}$ is the state sampled from the stationary distribution $p^{\pi}$, $e^j \sim z_q$ is the generated group embedding, and $\bm{u} \sim \{\pi^{gc^i}+\pi^{gg^i}\}_{i=1}^N$ denotes the joint action.

Thus, the gradient for the group consensus policy of agent $i$ parameterized by $\phi$ can be derived by applying the mini-batch technique to the off-policy training:
\begin{equation}
\label{eq:gcp}
    \bigtriangledown_{\phi}^i \mathcal{J} = \mathbb{E}
\left [ \bigtriangledown_{\bm{u'}}Q_{\bm{\pi}}(s,\bm{u}')|_{\bm{u}'=\{\pi^{gc^i}_{\phi}+\pi^{gg^i}_{\varphi}\}} 
\bigtriangledown_{\phi}log\pi^{gc^i}_{\phi}(e^j)
 \right ] .
\end{equation}

Also, the gradient for the group-guided policy is shown as:
\begin{equation}
\label{eq:ggp}
    \bigtriangledown_{\varphi}^i \mathcal{J} = \mathbb{E}
\left [ \bigtriangledown_{\bm{u'}}Q_{\bm{\pi}}(s,\bm{u}')|_{\bm{u}'=\{\pi^{gc^i}_{\phi}+\pi^{gg^i}_{\varphi}\}} 
\bigtriangledown_{\phi}log\pi^{gg^i}_{\varphi}(o^i,e^j)
 \right ] .
\end{equation}

\paragraph{\textbf{Jump-start Dynamic Integration}.}
However, evidence from supervised learning suggests hypernetwork performance is highly sensitive to input and initialization.
Thus, we perform a trick called Jump-start Dynamic Integration (JDI) to alleviate the issue inspired by~\cite{uchendu2022jump}.
Here, we define a hyper-parameter $jump \in [0,1]$ to represent a proportion factor of the full training horizon $T$, which indicates it should be changed while the current time step $t$ satisfies $t \ge jump * T$. The change can be formulated as:
\begin{equation}
    \pi^{finall} = \begin{cases}
    \pi^{gg^i}(o^i,e^j), \ \  if \  t < jump*T
     \\
    \pi^{gc^i}(e^j)+\pi^{gg^i}(o^i,e^j), \ \ else
    \end{cases}.
\end{equation}

In our case, we adopt this trick to stabilize the training process of our group consensus policy by warming up the training of the group codebook.

















