\section{Introduction}
% 许多现实的复杂问题都可以建模成合作多智能体系统，比如。
% 这些问题可以采用多智能体强化学习的技术手段进行求解，已经取得广泛的应用；
% Popular approaches for multi-agent cooperation 可以划分为be classified into two categories: CTDE智能体之间缺乏显式的合作，执行阶段没有一个好的知识引导； communication-based有设计通信对象以及通信带宽的限制。 
% 然而这些方法有一个common问题是 缺乏对整体任务的合理分工。
% 为了实现良好的协同，很多现实的多智能体系统问题都需要良好的分工。
% 比如足球比赛中需要进行合理分工，团结协作，教练的作用是统筹规划给一个前瞻性全局性的指导，后卫的职责重点就是防守，前锋的职责主要就是进攻射门。
% 一些分工的方法也被提出，
% 但是这些分工的方法缺乏全局的指导。
% 好的团队协作往往需要良好的分工去解决不同的子任务，专攻某个子任务的前瞻性的指导以及个人的策略。
% 为了解决以上问题，我们提出了。。。既克服了CTDE执行缺乏指导的问题，又解决了通信方法的。
% % 组共识策略 加上 组引导的个人策略
% 
% \centering
% \begin{flushright}
% \raggedright

Many applications, such as multi-player games~\cite{kurach2020google}, 
% vinyals2019grandmaster
traffic signal control~\cite{wu2020multi},
% chen2020toward
and sensor networks~\cite{zhang2011coordinated},
% chen2010directional
can be modeled as cooperative multi-agent systems (MASs), where a team of agents performs a shared task to reach a common goal. A promising solution is cooperative multi-agent reinforcement learning (MARL) which has shown exceptional results, of which the popular approach for multi-agent cooperation is communication-based MARL. One line of research lies on fine-grained communication channels, such as graph neural networks~\cite{jiang2018graph,niu2021multi}, 
% tu2021adversarial
attention mechanisms~\cite{liu2022self,jiang2018atoc}, etc. Another line~\cite{das2019tarmac,du2021flowcomm,liu2020when2com} 
% sukhbaatar2016CommNet
yields agents that learn communication protocols to determine which messages to transmit, and who to communicate with to assist decision-making.
However, a common issue is a lack of reasonable labor division for multiple agents.

In order to achieve better collaboration, many realistic multi-agent problems require a reasonable division of labor to enhance team-level cooperation~\cite{ferber1998meta,alam2016multi}. 
% jang2005dynamic
For example, in search and rescue missions, multiple agents (e.g. drones, robots, doctors) need to coordinate their efforts to locate and rescue survivors in a disaster-stricken area. Multiple drones should be deployed to search the area to ensure complete coverage of the search area and avoid collisions, while the doctors and robots should be assigned dynamically to different areas to ensure efficient rescuing. Thus, a well-designed cooperative multi-agent system can dynamically group agents with similar abilities based on the specific situation, while also allowing individual agents to make rational decisions based on their specific observations. 

Some methods have been proposed from the perspective of roles or groups. ROMA~\cite{wang2020roma} and RODE~\cite{wang2021rode} focus on task decomposition and specialize the agent associated with a role to resolve a certain sub-task. LDSA~\cite{yang2022ldsa} learns dynamic subtask assignments, which can dynamically group agents with similar abilities into the same subtask. However, on the one hand, role (group) representations are not well constrained and vulnerable to dynamic changes from the environment or policy training. On the other hand, the above methods rarely consider extracting higher-level policy guidance from the natural properties of groups (sub-tasks). In summary, good teamwork often requires a good division of labor, forward-looking guidance to specialize in a certain subtask, and rational individual decision-making. This poses a challenge for MARL for providing stable and powerful group representations, higher guidance at the team level, and better decisions at the individual level.

Therefore, we propose CoS, a consensus-oriented strategy in MARL. 
First, we use the vector quantized variational autoencoder (VQ-VAE)~\cite{van2017vqvae} to extract the group consensus embedding, which captures the shared objective that agents in the same group should pursue and  is essential for promoting effective teamwork.
% denotes the group objective that agents in the same group should target.
% which incorporates the transition dynamic to the learning process for the knowledgeable, distinguishable, and stable representation. 
Then, we perform the policy learning from two levels. At the higher level, we propose using a hyper-network architecture~\cite{ha2016hypernetworks}
% hao2022api
to transform group consensus embedding into group-level decisions from global and long-term perspectives. At the lower level, we utilize the group consensus embedding as the context prompt to augment the observation to make the individual and refined policy. The combination of these two policies guarantees the foresight and precision of the decision-making process. Moreover, CoS is a pluggable module and is suitable for both discrete and continuous action spaces.
We evaluate our method in three challenging MARL environments including discrete cooperative navigation, continuous cooperative navigation~\cite{lowe2017multi}, and google research football~\cite{kurach2020google}. The results show that our CoS significantly improves the learning performance on these benchmarks compared to some competitive baselines.

% First, we claim that the consensual representation~\cite{li2019survey} should be knowledgeable, distinguishable, and stable in the training process. Thus we utilize the vector quantized variational autoencoder (VQ-VAE)~\cite{van2017vqvae} to extract the group consensus embedding, which incorporates the transition dynamic to maintain a group codebook. Then, we perform the policy learning from two levels. At the higher level, we propose using a hyper-network architecture to transform group consensus representations into group-level decisions from global and long-term perspectives. At the lower level, we utilize the group consensus embedding as the context prompt to augment the observation to make the individual and refined policy. The combination of these two policies guarantees the foresight and precision of the decision-making process. Moreover, CoS is a pluggable module and is suitable for both discrete and continuous action spaces.

% We evaluate our method in three challenging MARL environments including discrete cooperative navigation, continuous cooperative navigation~\cite{lowe2017multi}, and google research football~\cite{kurach2020google}. The results show that our CoS significantly improves the learning performance on these benchmarks compared to some competitive baselines.

