% \vspace{-10pt}
\section{Related Work}
% \vspace{-10pt}

\paragraph{\textbf{Multi-agent Grouping}.}
We classify existing algorithms into three categories: (i) prior knowledge-based, (ii) role-based, and (iii) subtask-based methods.
Some early methods~\cite{odell2002role,lhaksmana2013role,ferber2004agents,pavon2003agent} utilize the domain knowledge to group the agents, and are limited by human labor.
To solve the issue, one line of work groups multiple agents by generating diverse roles responsible for different parts. 
ROMA~\cite{wang2020roma} learns a role-specific policy where the roles are captured from agents' local observations.
ROGC~\cite{liu2022rogc} introduces the graph convolutional network for classifying agents into different roles.
RODE~\cite{wang2021rode} decomposes action spaces with the learned roles.
\cite{hu2022policy}~define the role and role diversity to measure a cooperative MARL task and help diagnose the current policy.
LILAC~\cite{fu2022lilac} learns a leader to assign roles.
Another line of work such as~\cite{yang2022ldsa,yuan2022multi,phan2021vast,iqbal2021randomized}, divides the agents into some groups that carry out similar sub-tasks with a specific policy or value function.
In our work, we learn more stable and distinguishable group embeddings and further consider the integration of team-level strategy and individual-level decision.

% \paragraph{VQVAE-based Representation Learning.}
\vspace{-6pt}

\paragraph{\textbf{Representation Learning in MARL}.}
The representations for observations, actions, or underlying messages are widely studied in MARL.
Some works ~\cite{aman2021knowledge,fervari2022bisimulations} use bisimulation metrics to extract the latent embeddings from observations.
\cite{li2022ace,agarwal2021contrastive} attempt to learn  action representations to assist multi-agent policy learning. 
% ~\cite{zhang2022common,xu2022consensus,lo2022learning} 
\cite{xu2022consensus,jiang2022multi} propose represent underlying messages to conduct effective communication in MAS.
There are also some works like~\cite{tianhypertron,zhang2022common} that use VAE to encode the trajectory message to make representation more knowledgeable.
Unlike the above methods, we introduce VQ-VAE to maintain a quantized hidden space to extract stable,  and distinguishable group consensus embeddings to associate a more powerful strategy.

% \paragraph{\textbf{VQ-VAE}.}




% Efficient Multi-Agent Communication via Shapley Message Value


% \paragraph{Hypernetworks.}
% % https://arxiv.org/pdf/2210.11348.pdf


% \vspace{-10pt}
\section{Preliminary}
% \vspace{-6pt}

\paragraph{\textbf{Vector Quantised-Variational AutoEncoder}.}
The VQ-VAE is a type of variational autoencoder that uses vector quantization to obtain a discrete latent representation. 
It includes an encoder $z_e$, a decoder $z_d$, and a codebook $e \in \mathbb{R}^{K \times D}$, where $K$ is the number of embeddings in the codebook and $D$ is the dimension of the embeddings.
The encoder maps the input data $x$ to a sequence $z$ of discrete codes from a codebook and the decoder takes the responsibility of reconstruction.
In our paper, we use the encodings in the codebook as stable and distinguishable representations.

Formally, $z_e(x)$ is mapped via nearest-neighbor into the quantized codebook, denoted as:
\begin{equation}
    z_q(x) = e^j \ where \ j = argmin_k || z_e(x) - e^k||_2^2.
\end{equation}

This discretized process is not differentiable, thus copying the gradient of $z_q(x)$ to $z_e(x)$ is a suitable approximation similar to the straight-through estimator. The loss can be written as follows.
\begin{equation}
    \mathcal{L} = ||z_d(z_q(x))-x||^2_2 + \beta ||sg[z_e(x)]-e^j||^2_2 + ||z_e(x),sg[e^j]||^2_2,
\end{equation}
where $sg$ is a stop gradient operator, and $\beta$ is a parameter that regulates the rate of code change.

% \paragraph{\textbf{Temporal Difference (TD) learning}.}
% The centralized action-value function $Q_\pi$ can be updated through TD learning, formalized as follows.
% \begin{equation}
% \label{eq:critic}
% {\mathcal{L}}(\phi ) = {\mathbb{E}_{(s,\bm{o}, {\bm{u}})\sim \mathcal{D}}}\left[ {{{\left( {Q_{\bm{\pi}} ^i(s,\bm{u}) - {y^i}} \right)}^2}} \right]
% \end{equation}
% where the transition $(s,\bm{o},\bm{u})$ is sampled from the replay buffer $\mathcal{D}$, ${y^i} = r + \gamma {\max _{\bm{u}'}}Q_{{{\bm{\pi}} ^ - }}^i\left( {s',\bm{u}'} \right)$ is the learning target 
% and $Q_{{{\bm{\pi}} ^ - }}^i$ is the target network parameterized by $\theta^{i-}$.


