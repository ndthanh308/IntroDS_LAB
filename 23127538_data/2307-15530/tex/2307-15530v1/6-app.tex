\appendix
\setcounter{tocdepth}{2}
\onecolumn
{\centering\section{Appendix}}
\label{sec:app}

% 11111

\subsection{Network Architecture}
\label{app:net}

There is a summary of all the neural networks used in our framework about the network structure, layers, and activation functions.
% list all the network architecture
\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
       \toprule
       ~ & Network Structure &  Layers & Hidden Size & Activation Functions  \\
       \midrule
       History Encoder & CausalSelfAttention & 1 &  -  & ReLu \\
       History Decoder & MLP & 2 &  64  & ReLu   \\
       Group Codebook &   nn.Embedding & 1 & [N, 32] & None \\
       Feature Extractor & CNN+FiLMedBlock & - & - & ReLu \\
       Group-Guided Policy &   Hyper-MLP & 2 & 32 & Tanh \\
       Group Consensus Policy  &   MLP+RNN+MLP & 3 & [64]+[64]+[32] & Tanh \\
       Policy Critic & MLP & 2 & 64 & Tanh\\
       \bottomrule
    \end{tabular}
   \caption{The Summary for Network Architecture}
   \label{tab:net}
%   \vspace{-2em}
\end{table}

Please note that '$-$' denote that we refer readers to check the open source repository CausalSelfAttention, see \href{https://github.com/sachiel321/Efficient-Spatio-Temporal-Transformer}{https://github.com/sachiel321/Efficient-Spatio-Temporal-Transformer}.


\subsection{Parameter Settings}
\label{app:para}

There are our hyper-parameter settings for the training of Vector Quantized Group Consensus and Group Consensus-oriented Strategy, shown in Table~\ref{tab:para1} and Table~\ref{tab:para2}, respectively.

\vspace{10pt}

\begin{minipage}{\textwidth}
\begin{minipage}[t]{0.5\textwidth}

\centering
% \begin{table}[h!]
    % \centering
    \begin{tabular}{cc}
       \toprule
       Description &  Value  \\
       \midrule
       optimizer & $AdamW$  \\
       lr & $5*10^{-4}$  \\
        group embedding size  &   $32$ \\
        context length         &    $1$ \\
        model type        &     $state \ only$ \\
        attention head &    $4$ \\
        actor embedding size &    $32$ \\
        group codebook $K$ &    $agent \ number$ \\
        encoder coefficient $\beta$ &    $1$ \\
       \bottomrule
    \end{tabular}
\makeatletter\def\@captype{table}\makeatother\caption{Hyper-parameters of Vector Quantized Group Consensus}
\label{tab:para1}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\centering
% \begin{table}[h!]
%     \centering
    \begin{tabular}{cc}
       \toprule
       Description &  Value \\
       \midrule
       optimizer       &    Adam   \\
        $\alpha$        &    $10^{-4}$  \\
        $\beta_1$       &    $0.9$  \\
        $\beta_2$       &    $0.999$  \\
        $\varepsilon$-greedy $\varepsilon$   &    $10^{-5}$  \\
        clipping $\epsilon$       &   $0.2$ \\
        seed             &   $[0,10)$  \\
        number of process &  $64$  \\
        lr             &     $10^{-4}$  \\
        eval interval  &     $400000$  \\
        eval episodes  &     $100$   \\
        jump & $0.3$ \\
       \bottomrule
    \end{tabular}

   
\makeatletter\def\@captype{table}\makeatother\caption{Hyper-parameters of Group Consensus-oriented Strategy}
\label{tab:para2}
% \end{table}
\end{minipage}
\end{minipage}

By the way, 
we refer readers to the source code in the supplementary to check the detailed hyper-parameters.

\subsection{The Pseudo-code of CoS}
\label{app:algo}

The main procedures of our proposed CoS are summarized as Algorithm~\ref{algo:cos}.
% Our ultimate goal is to obtain the~\namefp~ $\{\pi^i\}_{i=1}^N$.
% % and the intermediate goal is to get the optimal graph generation policy $\rho$. 
% The~\namegg~$\rho$ is an intermediate for accessing
% a graph structure that guides the order of decisions among agents to achieve a high degree of coordination. 
% The graph generator is a pluggable module that is replaceable while solving DAGs. Note that the DAG is necessary because we need an execution structure that can determine a clear sequence of the actions, and therefore it should be directed rather than circular. The policy solver is a universal module, which, in general, one can choose from a diverse set of cooperative MARL algorithms~\cite{lowe2017maddpg,yu2021mappo,iqbal2019maac}.



\begin{algorithm}[ht!]
\caption{The pseudo-code of CoS.}
\label{algo:cos}
\textbf{Ensure} vector quantized group consensus $\rho=\{q, \{e^j\}_{j=1}^K, p\}$;\\
\textbf{Ensure} group consensus policy $\pi^{gc^i}$, group-guided policy $\pi^{gg^i}$ for agent $i$, and critic $Q_{\pi}$;\\
\textbf{Initialize}~the parameters $\theta$ 
for the vector quantized group consensus, the parameter $\phi^{gc^i}, \phi^{gg^i}$ for the group consensus policy and group-guided policy, where $i=1,...,N$, and
$\varphi$ for the critic network;\\
\textbf{Initialize}~jump\_rate=0.3;\\
\For {each episode}{
Initial state;\\
 \For{ each timestep}{
Compute history encoder output $z_e(\tau^i)=q_{\theta}(\cdot|\tau^i)$;\\
Quantize $z_e$ as $z_q$ with the group codebook $\bm{e}$;\\
Obtain the group consensus embedding $e^j$;\\
Reconstruct the input $\hat \tau^i = p_{\theta}(\cdot|z^q)$;\\
Compute the group-guided action $a^{{gg}^i_j}=\pi^{gg^i}(\cdot | o^i, e^j)$;\\
Obtain action for agent $i$ as $u^i=a^{gg^i_j}$;\\
\If{ $\#episodes > jump\_rate*total\_number$}{
Compute the group consensus action $a^{gc^i_j} = \pi^{gc^i}(\cdot|e^j)$;\\
Obtain action for the agent $i$ as $u^i=a^{gc^i_j}+a^{gg^i_j}$; \\
// Note: the sum of actions for continuous action space; the sum of action distributions for discrete action space;
}
Obtain the joint action $\bm{u}=\{u^1,u^2,...,u^N\}$;\\
Receive reward $r_t$ and observe next state $\{o_t^i\}_{i=1}^N$;\\
}
// Training \\
Update the group consensus policy with Equation~(\ref{eq:gcp});\\ 
Update the group-guided policy with Equation~(\ref{eq:ggp});\\ 
Update the vector quantized group consensus with Equation~(\ref{eq:vqgc});\\
Update the critic network with TD error.
}

\end{algorithm}




\subsection{The Detailed Description of GRF}
\label{app:grf}

\paragraph{\textbf{Observations}}
The environment exposes the raw observations as Table~\ref{tab:grf-info}.
We use the \textit{Simple115StateWrapper}\footnote{We refer the reader to:\url{https://github.com/google-research/football} for details of encoded information.} as the simplified representation of a game state encoded with 115 floats. 





\begin{sidewaystable}[!ht]
    \centering
    \caption{The detailed descriptions of the information included in the raw observation of GRF.}
\label{tab:grf-info}
    \begin{tabular}{cc|cc}
    \toprule
        \textbf{Information} & \textbf{Descriptions} & \textbf{Information} & \textbf{Descriptions} \\ \hline
        ~ & position of ball & ~ & ~ \\ 
        ~ & direction of ball & ~ & controlled player index \\ 
        Ball & rotation angles of ball & Controlled player & designated player index \\ 
        ~ & owned team of ball & ~ & active action \\ 
        ~ & owned player of ball & ~ & ~ \\ \hline
        ~ & position of players in left team & ~ & position of players in right team \\ 
        ~ & direction of players in left team & ~ & direction of players in right team \\ 
        Left Team & tiredness factor of players & Right Team & tiredness factor of players \\ 
        ~ & numbers of players with yellow card & ~ & numbers of players with yellow card \\ 
        ~ & whether a player got a red card & ~ & whether a player got a red card \\ 
        ~ & roles of players & ~ & roles of player \\ \hline
        ~ & goals of left and right teams & ~ & ~ \\ 
        Match state & left steps & Screen & rendered screen \\ 
        ~ & current game mode & ~ & ~ \\ \bottomrule
    \end{tabular}
\end{sidewaystable}






\paragraph{\textbf{Actions}}
The number of actions available to an individual agent can be denoted as $|\mathcal{A}| = 19$.
The standard move actions (in $8$ directions) include $\mathcal{A}_{move} = \{Top, Bottom, Left, Right,Top-Left, Top-Right, Bottom-Left, Bottom-Right\}$.
Moreover, the actions represent different ways to kick the ball is 
$\mathcal{A}_{kick} = \{Short Pass, High Pass,Long Pass, Shot,Do-Nothing,Sliding,Dribble,Stop-Dribble,Sprint, Stop-Moving, Stop-Sprint\}$.

\paragraph{\textbf{Rewards}}
The reward function mainly includes two parts. The first is $SCORING$, which corresponds to the natural reward where the team obtains $+1$ when scoring a goal and $-1$ when losing one to the opposing team. The second part is $CHECKPOINT$, which is proposed to address the issue of sparse rewards. It is encoded with  domain
knowledge by an additional auxiliary reward contribution.



\paragraph{\textbf{Scenarios}}
We conduct the experiments on the following scenarios.
\begin{itemize}
    \item academy\_3\_vs\_1\_with\_keeper: Three of our players try to score from the edge of the box, one on each side, and the other at the center. Initially, the player at the center has the ball and is facing the defender. There is an opponent keeper.
    \item academy\_counterattack\_easy: 4 versus 1 counter-attack with keeper; all the remaining players of both teams run back towards the ball.
    \item academy\_counterattack\_hard: 4 versus 2 counter-attack with keeper; all the remaining players of both teams run back towards the ball. 
\end{itemize}