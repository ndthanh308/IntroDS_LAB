

\setlength{\tabcolsep}{2.8pt}
\setlength{\abovecaptionskip}{0.5pt}
\setlength{\belowcaptionskip}{0.5pt}
\begin{table*}[!ht]
\vspace{-6pt}
    \centering
    \caption{End steps of various methods on d-CN.}
    \begin{tabular}{ccccccc}
    \toprule
      \#agents & CoS & MAPPO & VDN & QMIX & RODE & ROMA \\ \hline
     4 & \textbf{19.8} $\pm$ \scriptsize{1.1} & 21.2 $\pm$ \scriptsize{0.7} & 24.6 $\pm$ \scriptsize{0.4} & 24.3 $\pm$  \scriptsize{0.3} & 23.6 $\pm$ \scriptsize{1.2} & 24.8 $\pm$ \scriptsize{0.9} \\ 
     6 & \textbf{23.1} $\pm$ \scriptsize{0.9} & 23.2 $\pm$ \scriptsize{0.8} & 24.9 $\pm$ \scriptsize{0.6} & 24.7 $\pm$ \scriptsize{0.5} & 24.9 $\pm$ \scriptsize{0.8} & 24.9 $\pm$ \scriptsize{0.6} \\ 
    10 & 25 $\pm$ \scriptsize{0} & 25 $\pm$ \scriptsize{0} & 25 $\pm$ \scriptsize{0} & 25 $\pm$ \scriptsize{0} & 25 $\pm$ \scriptsize{0} & 25 $\pm$ \scriptsize{0} \\   
    \bottomrule
    \end{tabular}
    \label{tab:endstep-d-CN}
\end{table*}
% \vspace{-26pt}

% \vspace{-10pt}

\section{Experimental Evaluations}
% \vspace{-10pt}

We evaluate the effectiveness of our algorithm in three environments including both discrete and continuous action space: discrete Cooperative Navigation (d-CN), continuous Cooperative Navigation (c-CN), and Google Research Football (GRF), illustrated as Figure~\ref{fig:draw}.


\vspace{-10pt}
% Figure environment removed
\vspace{-20pt}
\subsection{Experimental Settings}

\paragraph{\textbf{d-CN and c-CN}.}
We modify the classic Cooperative Navigation (CN) implemented in the multi-agent particle world~\cite{lowe2017multi} to a more challenging environment, which requires more collaboration among agents.
We initialize the CN world with $n$ landmarks and $2*n$ agents with random locations at the beginning of each episode.
Each agent can only observe its velocity, position, and displacement from other agents and landmarks. 
The final objective is to occupy all the landmarks and each landmark contains two agents.
The reward function can be formulated as $r_t = -0.1 + 3*single + 10*double$. The variables $single$ and $double$ denote the number of landmarks that are occupied by only one and two agents, which corresponds to the reward $3$ and $10$, respectively. 
$-0.1$ is the step punishment.
Obviously, the game can reach the maximum reward while each landmark contains two agents, and can be over. 
d-CN denotes that the world has a discrete action space $\mathcal{A}_d$ including five actions \textit{[up, down, left, right, stop]}.
c-CN represents that the world has a continuous action space $\mathcal{A}_c$.
We set the length of each episode as $25$ time steps.

\paragraph{\textbf{GRF}.}
GRF~\cite{kurach2020google} is a realistically complicated and dynamic multi-agent testbed. 
Agents should have a division of labor and plan to coordinate the time and location to complete the scoring.
In the experiments, we control left-side players except for the goalkeeper while the right-side players are built-in
bots controlled by the game engine. 
Here, each player has 19 actions to control,
including the standard move actions and different ball-kicking techniques.
The observation contains the positions and moving directions of the ego-agent, other agents, and the ball.
We use the Floats wrappers to represent the state that contains a 115-dimensional vector.
The rewards include the SCORING reward $\{-1, +1\}$, and
the CHECKPOINT reward, which is the shaped reward that specifically addresses the sparsity of SCORING. 
The detailed descriptions of GRF can be found in Appendix~\ref{app:grf}.

\paragraph{\textbf{Baselines}.}
We compared our results with several baselines as
follows. {VDN} and {QMIX} are state-of-the-art (SOTA) value factorization approaches, with which it is difficult to obtain coordinated behaviors.
{MAPPO} is the multi-agent competitive SOTA algorithm extended from PPO by setting the sharing actor and a centralized critic. 
{MADDPG} and {MATD3} are classic continuous control algorithms.
% \textbf{LDSA} 
{ROMA} and {RODE} are role-based grouping algorithms.
Note that VDN, QMIX, ROMA, and RODE are designed for the discrete action space, while MADDPG and MATD3 are designed for the continuous action space.


\subsection{Does CoS Perform Better?}

For validating our CoS, we conduct empirical experiments on d-CN, c-CN, and GRF.
The benchmarks we choose include discrete and continuous action spaces, realistically complicated and stochastic worlds.
All experiments are repeated for 10 runs with different random seeds.





% \begin{table*}[!ht]
%     \centering
%     \caption{End steps of various methods. The episode limit is set to 25. The '-' denotes the dismatch action space.}
%     \begin{tabular}{c|c|cccccccc}
%     \toprule
%         ~ & \#agents & CoS & MAPPO & VDN & QMIX & MADDPG & MATD3 & RODE & ROMA \\ \hline
%         ~ & 4 & \textbf{19.8} & 21.2 & 24.6 & 24.3 & - & - & 23.6 & 24.8 \\ 
%         d-CN & 6 & \textbf{23.1} & 23.2 & 24.9 & 24.7 & - & - & 24.9 & 24.9 \\ 
%         ~ & 10 & 25 & 25 & 25 & 25 & - & - & 25 & 25 \\ \hline
%         ~ & 4 & \textbf{19.6} & 20.3 & - & - & 22.3 & 22.1 & - & - \\ 
%         c-CN & 6 & \textbf{22.1} & 23.8 & - & - & 24.3 & 24.2 & - & - \\ 
%         ~ & 10 & 25 & 25 & - & - & 25 & 25 & - & - \\ 
%     \bottomrule
%     \end{tabular}
%     \label{tab:endstep}
% \end{table*}



% Figure environment removed
% \protect\footnotemark. 
% \footnotetext{Note that we don't conduct the experiments with RODE and ROMA on c-CN because RODE and ROMA is only suitable for the discrete action space as reported in the original paper.}



\subsubsection{Performance on d-CN.}
We conduct the experiments across $4,6$ and $10$ agents in d-CN with discrete action space as shown in Figure~\ref{fig:exp-cn}(a-c). 
CoS substantially gets a better average reward than all the baselines, indicating that the group consensus strategy increasingly enhances the superiority 
of our method.


Moreover, as shown in Table~\ref{tab:endstep-d-CN}, we report the mean end steps of these methods after testing 100 episodes, and the episode limit is set to 25. 
CoS basically completes the task faster with the least number of steps.
These results show that the group consensus strategy of Cos can group agents with different intentions toward various landmarks, which boosts the training performance.


\subsubsection{Performance on c-CN.}
As shown in Figure~\ref{fig:exp-cn}(d-f), the results in continuous space also exhibit better performance. 
Obviously, our method has a quicker convergence speed and a smaller variance than others, which indicates the decision of CoS to consider both global and individual guidance can further exploit the underlying properties and facilitate cooperative behaviors among agents.

In Table~\ref{tab:endstep-c-CN}, our method still get the least number of steps. Unfortunately, all the methods have failed with 10 agents because of the stochasticity and difficulty of the environment. It will be an interesting direction to study how to obtain the optimal solution in such complicated scenarios in the fastest time steps.

 % \vspace{-12pt}
\setlength{\tabcolsep}{5.8pt}
\setlength{\abovecaptionskip}{0.5pt}
\setlength{\belowcaptionskip}{0.5pt}
\begin{table}[!ht]
% \vspace{-16pt}
    \centering
    \caption{End steps of various methods on c-CN.}
    \begin{tabular}{ccccccc}
    \toprule
      \#agents & CoS & MAPPO & MADDPG & MATD3 \\ \hline
    4 & \textbf{19.6} $\pm$ 2.1  & 20.3 $\pm$ 2.3  & 22.3 $\pm$ 1.2  & 22.1 $\pm$ 1.6   \\ 
    6 & \textbf{22.1} $\pm$ 1.4  & 23.8 $\pm$ 1.5   & 24.3  $\pm$ 1.3 & 24.2 $\pm$ 1.3   \\ 
    10 & 25 $\pm$ 0  & 25 $\pm$ 0   & 25 $\pm$ 0  & 25 $\pm$ 0   \\ 
\bottomrule
    \end{tabular}
    \label{tab:endstep-c-CN}
\end{table}

 % \vspace{-15pt}


% Figure environment removed




% \textbf{}
\vspace{-20pt}

\subsubsection{Performance on GRF.}

Further, we conduct experiments in google research football, a more dynamic and complicated benchmark to validate the effectiveness of our proposed CoS, shown in Figure~\ref{fig:exp-grf}.
Specifically, we choose three popular scenarios, including \textit{academy\_3\_vs\_1\_with\_keeper} (3vs1), \textit{academy\_counterattack\_easy} (c\_easy), and \textit{academy\_coun-terattack\_hard} (c\_hard).

In Figure~\ref{fig:exp-grf}(a-c), we observed that CoS consistently obtains higher performance than all the baselines in different scenarios of GRF, indicating that our method is robust and effective in complex and dynamic environments. Moreover, this performance improvement in GRF demonstrates that our approach excels at handling stochasticity, as the dynamic grouping strategy can extract more underlying details and make informed high-level decisions.  
% The additional results and the detailed descriptions of GRF can be found in Appendix~\ref{app:grf}.
Additionally, we report the average end steps of CoS in the three scenarios, shown in Figure~\ref{fig:grf-end}.
We test the trained model for 100 episodes and count the average end steps of every algorithm.
% CoS can complete the football game in the fewest steps compared to all baselines, which further demonstrates the superiority of our proposed algorithm.
Specifically, we tested the trained model for 100 episodes and counted the average end steps of each algorithm. Notably, CoS completed the football game in the fewest steps compared to all the baselines, further validating the superiority of our proposed algorithm.


% \subsection{Is Grouping Effective?}


\subsection{Does the components of CoS Really Work?}

% Figure environment removed



To evaluate the effectiveness of the components in CoS, we conduct the ablation study with the following configurations.
\begin{itemize}
    \item \textit{CoS(ours)}: The proposed framework.
    \item \textit{CoS w/o GGP}: Remove the group-guided policy.
    \item \textit{CoS w/o GCP}: Remove the group consensus policy.
    % \item \textit{CoS+GCP(MLP)}: Replace hypernet in GCP with MLP.
\end{itemize}


As shown in Figure~\ref{fig:exp-abl}, the ablation study of 4 and 6 agents on d-CN shows the components in the CoS really work. The performance degradations of \textit{CoS w/o GGP} and \textit{CoS w/o GCP} show that the combination of these two modules is necessary, mainly due to two aspects. (1) GCP utilizes the extracted group consensus embeddings to make high-level decisions, which is conducive to long-term utility. (2) GGP perceives the individual observation from the true world, which can exploit underlying dynamics to make accurate decisions. Therefore, the fine-grained combination of these two modules
can achieve forward-looking guidance to specialize in a certain subtask and rational individual decision-making. 

% Moreover, the blue line represents the results of \textit{CoS+GCP(MLP)}, which indicate it is more effective to use the hypernet to make global guidance in CoS.
% From the perspective of training, the hypernet can well coordinate the potentially conflicting parameters in a unified space through feeding fed with different group consensus embeddings. 
% At the same time,  the hyper-network can also capture the higher-order interaction among group consensus embeddings from another perspective.

\subsection{How Well Does the Group Consensus Embedding Perform?}


To investigate the discriminative power of the CoS model's group consensus embeddings, we visualize the embeddings of several scenarios on two tasks collected in the later stages of the training process. The goal is to show whether CoS can distinguish different groups and achieve separate consensus among every group, aiding in the collaboration of intra-groups in the decision-making process.

Due to the assumption that $K$ agents can group into $K$ groups maximally, the number of group clusters is the same as the number of agents in each scenario. As shown in Figure~\ref{fig:exp-vis1}, the 3D visualizations demonstrate that CoS can consistently produce distinct group embeddings in all the scenarios. 
We showcase the visualizations obtained by applying the T-SNE technique to the group consensus embeddings saved from the last 5000 training steps.
Each cluster in the plots represents a specific group. Each point in the cluster corresponds to a vector in the group codebook. The distinguishable border highlights the ability of CoS to learn consistent group consensus representations.
% The plots show that each group forms a separate cluster, indicating that the model's learned embeddings are effective in discriminating between different groups.
% Additionally, we provide a 2D projection of the same visualizations, which makes the clusters more easily distinguishable.


Overall, these results demonstrate that our proposed method can facilitate intra-group collaboration and decision-making by producing effective group consensus embeddings that can discriminate between different groups in the decision-making process.


% To demonstrate whether CoS can discriminate different groups and achieve separate consensus among every group, we visualize the group consensus embeddings of several scenarios on two tasks collected in the later stages of the training process.
% Due to the assumption that $K$ agents can group into $K$ groups maximally, there are the same group clusters as the agents in each scenario.
% As shown in Figure~\ref{fig:exp-vis1}, CoS can consistently obtain the discriminative group embeddings on all the scenarios, which further indicates that our proposed method can facilitate the collaboration of intra-groups in the decision-making process conditioned on them.



% % Figure environment removed



% Figure environment removed
 \vspace{-16pt}



% % Figure environment removed




\subsection{Has CoS Learned to Group?}

To assess CoS's ability to group and its impact on the collaborative behavior of agents, we conducted some visualizations and analyses. First, at time step $t=20$ during one testing episode, we visualize the grouping effect, as shown in Figure~\ref{fig:d-CN-vis2-1}, which illustrates that agents grouped by the same number in yellow exhibit similar behavior, indicating that CoS has learned to divide the agents into groups and promote collaboration.

To further examine the effect of grouping on performance, we evaluate the average rewards of 100 episodes at different checkpoints with varying numbers of groups, as shown in Figure~\ref{fig:d-CN-vis2-2}. Our results show that in the scenario with 10 agents, dividing them into 5 groups achieves the highest utility, consistent with the environmental setting. This finding highlights the importance of an effective grouping mechanism and demonstrates the capability of CoS to optimize the group sizes based on the task requirements.

% To show whether CoS has learned to group and the grouping effect, we give some visualizations. As shown in Figure~\ref{fig:d-CN-vis2-1}, we visualize one time step ($t=20$) at the testing process and find that agents grouping with the same number in yellow have the same intention to cover a landmark, which indicates CoS has the ability to divide the groups and promote collaboration. Numerically, we count the average rewards of 100 episodes on the checkpoints with different numbers of groups and give a histogram as shown in Figure~\ref{fig:d-CN-vis2-2}. The results show that in the scenario of 10 agents, dividing into 5 groups reaches the highest utility, which is consistent with the environmental setting. It also demonstrates the importance of the proper grouping mechanism.

% Figure environment removed
 \vspace{-16pt}











