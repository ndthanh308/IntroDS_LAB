{
  "title": "Learning to Collaborate by Grouping: a Consensus-oriented Strategy for Multi-agent Reinforcement Learning",
  "authors": [
    "Jingqing Ruan",
    "Xiaotian Hao",
    "Dong Li",
    "Hangyu Mao"
  ],
  "submission_date": "2023-07-28T12:41:36+00:00",
  "revised_dates": [],
  "abstract": "Multi-agent systems require effective coordination between groups and individuals to achieve common goals. However, current multi-agent reinforcement learning (MARL) methods primarily focus on improving individual policies and do not adequately address group-level policies, which leads to weak cooperation. To address this issue, we propose a novel Consensus-oriented Strategy (CoS) that emphasizes group and individual policies simultaneously. Specifically, CoS comprises two main components: (a) the vector quantized group consensus module, which extracts discrete latent embeddings that represent the stable and discriminative group consensus, and (b) the group consensus-oriented strategy, which integrates the group policy using a hypernet and the individual policies using the group consensus, thereby promoting coordination at both the group and individual levels. Through empirical experiments on cooperative navigation tasks with both discrete and continuous spaces, as well as Google research football, we demonstrate that CoS outperforms state-of-the-art MARL algorithms and achieves better collaboration, thus providing a promising solution for achieving effective coordination in multi-agent systems.",
  "categories": [
    "cs.MA"
  ],
  "primary_category": "cs.MA",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15530",
  "pdf_url": "https://arxiv.org/pdf/2307.15530v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 8990008,
  "size_after_bytes": 196716
}