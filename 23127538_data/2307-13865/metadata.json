{
  "title": "Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT",
  "authors": [
    "Taha Emre",
    "Marzieh Oghbaie",
    "Arunava Chakravarty",
    "Antoine Rivail",
    "Sophie Riedl",
    "Julia Mai",
    "Hendrik P. N. Scholl",
    "Sobha Sivaprasad",
    "Daniel Rueckert",
    "Andrew Lotery",
    "Ursula Schmidt-Erfurth",
    "Hrvoje BogunoviÄ‡"
  ],
  "submission_date": "2023-07-25T23:46:48+00:00",
  "revised_dates": [],
  "abstract": "In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of architectures and associated pretraining on a task of predicting progression to wet age-related macular degeneration (AMD) within a six-month period on two large longitudinal OCT datasets.",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "primary_category": "cs.CV",
  "doi": "10.1007/978-3-031-44013-7_14",
  "journal_ref": null,
  "arxiv_id": "2307.13865",
  "pdf_url": null,
  "comment": "Accepted at OMIA-X MICCAI'23 Workshop",
  "num_versions": null,
  "size_before_bytes": 591270,
  "size_after_bytes": 127173
}