
@inproceedings{wada-etal-2022-unsupervised,
    title = "Unsupervised Lexical Substitution with Decontextualised Embeddings",
    author = "Wada, Takashi  and
      Baldwin, Timothy  and
      Matsumoto, Yuji  and
      Lau, Jey Han",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    year = "2022",
    pages = "4172--4185"
}
@inproceedings{lin-etal-2022-improving,
    title = "Improving Contextual Representation with Gloss Regularized Pre-training",
    author = "Lin, Yu  and
      An, Zhecheng  and
      Wu, Peihao  and
      Ma, Zejun",
    booktitle = "NAACL",
    year = "2022",
    pages = "907--920"
}
@inproceedings{gu-etal-2020-token,
    title = "Token-level Adaptive Training for Neural Machine Translation",
    author = "Gu, Shuhao  and
      Zhang, Jinchao  and
      Meng, Fandong  and
      Feng, Yang  and
      Xie, Wanying  and
      Zhou, Jie  and
      Yu, Dong",
    booktitle = "EMNLP",
    year = "2020",
    pages = "1035--1046",
 }

 @inproceedings{jiang2019improving,
  title={Improving neural response diversity with frequency-aware cross-entropy loss},
  author={Jiang, Shaojie and Ren, Pengjie and Monz, Christof and de Rijke, Maarten},
  booktitle={The World Wide Web Conference},
  pages={2879--2885},
  year={2019}
}
@inproceedings{pavlick2016simple,
  title={Simple PPDB: A paraphrase database for simplification},
  author={Pavlick, Ellie and Callison-Burch, Chris},
  booktitle={ACL},
  pages={143--148},
  year={2016}
}

@article{devlin1998the,
  title={The use of a psycholinguistic database in the simpli cation of text for aphasic readers},
  author={Siobhan {Devlin} and John {Tait}},
  journal={Linguistic Databases},
  volume={1},
  pages={161–173},
  year={1998}
}

@inproceedings{mccarthy2002lexical,
  title={Lexical substitution as a task for WSD evaluation},
  author={McCarthy, Diana},
  booktitle={Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions},
  pages={89--115},
  year={2002}
}

@inproceedings{hu-etal-2019-large,
    title = "Large-Scale, Diverse, Paraphrastic Bitexts via Sampling and Clustering",
    author = "Hu, J. Edward  and
      Singh, Abhinav  and
      Holzenberger, Nils  and
      Post, Matt  and
      Van Durme, Benjamin",
    booktitle = "CoNLL",
    year = "2019",
    pages = "44--54"
}

@inproceedings{Ganitkevitch2013,
  title={PPDB: The paraphrase database},
  author={Ganitkevitch, Juri and Durme, Benjamin Van and Callison-Burch, Chris},
  booktitle={NAACL-HLT},
  pages={758--764},
  year={2013}
}

@inproceedings{pavlick2015ppdb,
  title={PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification},
  author={Pavlick, Ellie and Rastogi, Pushpendre and Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
  booktitle={ACL},
  pages={425--430},
  year={2015}
}

@inproceedings{rello2013frequent,
  title={Frequent words improve readability and short words improve understandability for people with dyslexia},
  author={Rello, Luz and Baeza-Yates, Ricardo and Dempere-Marco, Laura and Saggion, Horacio},
  booktitle={IFIP Conference on Human-Computer Interaction},
  pages={203--219},
  year={2013},
  organization={Springer}
}

@inproceedings{horn2014learning,
  title={Learning a lexical simplifier using wikipedia},
  author={Horn, Colby and Manduca, Cathryn and Kauchak, David},
  booktitle={ACL},
  pages={458--463},
  year={2014}
}

@article{paetzold2017survey,
  title={A survey on lexical simplification},
  author={Paetzold, Gustavo H and Specia, Lucia},
  journal={Journal of Artificial Intelligence Research},
  volume={60},
  pages={549--593},
  year={2017}
}

@inproceedings{zhou2019bert,
  title={BERT-based Lexical Substitution},
  author={Zhou, Wangchunshu and Ge, Tao and Xu, Ke and Wei, Furu and Zhou, Ming},
  booktitle={ACL},
  pages={3368--3373},
  year={2019}
}

@article{yuan2021bartscore,
  title={Bartscore: Evaluating generated text as text generation},
  author={Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  journal={NeuraIPS},
  volume={34},
  year={2021}
}

@inproceedings{carroll1998practical,
  title={Practical simplification of English newspaper text to assist aphasic readers},
  author={Carroll, John and Minnen, Guido and Canning, Yvonne and Devlin, Siobhan and Tait, John},
  booktitle={AAAI Workshop},
  pages={7--10},
  year={1998}
}

@inproceedings{jawahar2019does,
  title={What does BERT learn about the structure of language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e}},
  booktitle={ACL},
  year={2019}
}

@inproceedings{de2010text,
  title={Text simplification for children},
  author={De Belder, Jan and Moens, Marie-Francine},
  booktitle={SIGIR},
  pages={19--26},
  year={2010}
}

@inproceedings{glavavs2015simplifying,
  title={Simplifying lexical simplification: Do we need simplified corpora?},
  author={Glava{\v{s}}, Goran and {\v{S}}tajner, Sanja},
  booktitle={ACL},
  pages={63--68},
  year={2015}
}

@inproceedings{paetzold2016unsupervised,
  title={Unsupervised lexical simplification for non-native speakers},
  author={Paetzold, Gustavo and Specia, Lucia},
  booktitle={AAAI},
  volume={30},
  number={1},
  year={2016}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{devlin1998use,
  title={The use of a psycholinguistic database in the simplification of text for aphasic readers},
  author={Devlin, Siobhan},
  journal={Linguistic databases},
  year={1998},
  publisher={CSLI}
}

@inproceedings{rello2013simplify,
  title={Simplify or help? Text simplification strategies for people with dyslexia},
  author={Rello, Luz and Baeza-Yates, Ricardo and Bott, Stefan and Saggion, Horacio},
  booktitle={Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility},
  pages={1--10},
  year={2013}
}

@inproceedings{amancio2014analysis,
  title={An analysis of crowdsourced text simplifications},
  author={Amancio, Marcelo Adriano and Specia, Lucia},
  booktitle={Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)},
  pages={123--130},
  year={2014}
}

@inproceedings{zhu2010monolingual,
  title={A monolingual tree-based translation model for sentence simplification},
  author={Zhu, Zhemin and Bernhard, Delphine and Gurevych, Iryna},
  booktitle={Coling},
  pages={1353--1361},
  year={2010}
}

@inproceedings{coster2011learning,
  title={Learning to simplify sentences using wikipedia},
  author={Coster, William and Kauchak, David},
  booktitle={Proceedings of the workshop on monolingual text-to-text generation},
  pages={1--9},
  year={2011}
}

@inproceedings{hwang2015aligning,
  title={Aligning sentences from standard wikipedia to simple wikipedia},
  author={Hwang, William and Hajishirzi, Hannaneh and Ostendorf, Mari and Wu, Wei},
  booktitle={NAACL},
  pages={211--217},
  year={2015}
}

@inproceedings{vstajner2015deeper,
  title={A deeper exploration of the standard PB-SMT approach to text simplification and its evaluation},
  author={{\v{S}}tajner, Sanja and Bechara, Hannah and Saggion, Horacio},
  booktitle={ACL},
  pages={823--828},
  year={2015}
}

@inproceedings{evans2014evaluation,
  title={An evaluation of syntactic simplification rules for people with autism},
  author={Evans, Richard and Orasan, Constantin and Dornescu, Iustin},
  year={2014},
  organization={Association for Computational Linguistics}
}

@inproceedings{gala2020alector,
  title={Alector: A Parallel Corpus of Simplified French Texts with Alignments of Misreadings by Poor and Dyslexic Readers},
  author={Gala, N{\'u}ria and Tack, Ana{\"\i}s and Javourey-Drevet, Ludivine and Fran{\c{c}}ois, Thomas and Ziegler, Johannes C},
  booktitle={Language Resources and Evaluation for Language Technologies (LREC)},
  year={2020}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={arXiv preprint arXiv:1409.3215},
  year={2014}
}

@inproceedings{kalchbrenner2013recurrent,
  title={Recurrent continuous translation models},
  author={Kalchbrenner, Nal and Blunsom, Phil},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1700--1709},
  year={2013}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{martin2020multilingual,
  title={Multilingual unsupervised sentence simplification},
  author={Martin, Louis and Fan, Angela and de la Clergerie, {\'E}ric and Bordes, Antoine and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:2005.00352},
  year={2020}
}

@inproceedings{espla2019paracrawl,
  title={ParaCrawl: Web-scale parallel corpora for the languages of the EU},
  author={Espl{\`a}-Gomis, Miquel and Forcada, Mikel L and Ram{\'\i}rez-S{\'a}nchez, Gema and Hoang, Hieu},
  booktitle={Proceedings of Machine Translation Summit XVII Volume 2: Translator, Project and User Tracks},
  pages={118--119},
  year={2019}
}

@inproceedings{ziemski2016united,
  title={The united nations parallel corpus v1. 0},
  author={Ziemski, Micha{\l} and Junczys-Dowmunt, Marcin and Pouliquen, Bruno},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={3530--3534},
  year={2016}
}

@inproceedings{koehn2005europarl,
  title={Europarl: A parallel corpus for statistical machine translation},
  author={Koehn, Philipp},
  booktitle={MT summit},
  volume={5},
  pages={79--86},
  year={2005},
  organization={Citeseer}
}

@article{sennrich2015improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
  year={2015}
}

@article{bentz2016comparison,
  title={A comparison between morphological complexity measures: typological data vs. language corpora},
  author={Bentz, Christian and Soldatova, Tatjana and Koplenig, Alexander and Samard{\v{z}}i{\'c}, Tanja},
  year={2016}
}

@article{ng2019facebook,
  title={Facebook FAIR's WMT19 News Translation Task Submission},
  author={Ng, Nathan and Yee, Kyra and Baevski, Alexei and Ott, Myle and Auli, Michael and Edunov, Sergey},
  journal={arXiv preprint arXiv:1907.06616},
  year={2019}
}

@inproceedings{ott-etal-2019-fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53",
    abstract = "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@inproceedings{paetzold2017lexical,
  title={Lexical simplification with neural ranking},
  author={Paetzold, Gustavo and Specia, Lucia},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={34--40},
  year={2017}
}

@article{brysbaert2009moving,
  title={Moving beyond Ku{\v{c}}era and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English},
  author={Brysbaert, Marc and New, Boris},
  journal={Behavior research methods},
  volume={41},
  number={4},
  pages={977--990},
  year={2009},
  publisher={Springer}
}

@article{wei2019imitation,
  title={Imitation learning for non-autoregressive neural machine translation},
  author={Wei, Bingzhen and Wang, Mingxuan and Zhou, Hao and Lin, Junyang and Xie, Jun and Sun, Xu},
  journal={arXiv preprint arXiv:1906.02041},
  year={2019}
}

@inproceedings{zhou2021paraphrase,
  title={Paraphrase generation: A survey of the state of the art},
  author={Zhou, Jianing and Bhat, Suma},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5075--5086},
  year={2021}
}

@inproceedings{wang2019task,
  title={A task in a suit and a tie: paraphrase generation with semantic augmentation},
  author={Wang, Su and Gupta, Rahul and Chang, Nancy and Baldridge, Jason},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={7176--7183},
  year={2019}
}

@inproceedings{zhou2021paraphrase,
  title={Paraphrase generation: A survey of the state of the art},
  author={Zhou, Jianing and Bhat, Suma},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5075--5086},
  year={2021}
}

@inproceedings{hu2019large,
  title={Large-scale, diverse, paraphrastic bitexts via sampling and clustering},
  author={Hu, J Edward and Singh, Abhinav and Holzenberger, Nils and Post, Matt and Van Durme, Benjamin},
  booktitle={CoNLL},
  pages={44--54},
  year={2019}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{qiang2021lsbert,
  title={LSBert: Lexical Simplification Based on BERT},
  author={Qiang, Jipeng and Li, Yun and Zhu, Yi and Yuan, Yunhao and Shi, Yang and Wu, Xindong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3064--3076},
  year={2021},
  publisher={IEEE}
}

@inproceedings{gooding2019recursive,
  title={Recursive context-aware lexical simplification},
  author={Gooding, Sian and Kochmar, Ekaterina},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4853--4863},
  year={2019}
}

@article{xu2016optimizing,
  title={Optimizing statistical machine translation for text simplification},
  author={Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={401--415},
  year={2016},
  publisher={MIT Press}
}

@techreport{kincaid1975derivation,
  title={Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel},
  author={Kincaid, J Peter and Fishburne Jr, Robert P and Rogers, Richard L and Chissom, Brad S},
  year={1975},
  institution={Naval Technical Training Command Millington TN Research Branch}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@article{gers1999learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year={1999},
  publisher={IET}
}

@inproceedings{alva-manchego-etal-2020-asset,
    title = "{ASSET}: {A} Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations",
    author = "Alva-Manchego, Fernando  and
      Martin, Louis  and
      Bordes, Antoine  and
      Scarton, Carolina  and
      Sagot, Beno{\^\i}t  and
      Specia, Lucia",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.424",
    doi = "10.18653/v1/2020.acl-main.424",
    pages = "4668--4679",
    abstract = "In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.",
}

@article{wubben2012sentence,
  title={Sentence simplification by monolingual machine translation},
  author={Wubben, Sander and Krahmer, EJ and van den Bosch, APJ},
  year={2012},
  publisher={Stroudsburg, PA, USA: Association for Computational Linguistics}
}

@inproceedings{katsuta2019improving,
  title={Improving text simplification by corpus expansion with unsupervised learning},
  author={Katsuta, Akihiro and Yamamoto, Kazuhide},
  booktitle={2019 International Conference on Asian Language Processing (IALP)},
  pages={216--221},
  year={2019},
  organization={IEEE}
}

@inproceedings{palmero2019neural,
  title={Neural text simplification in low-resource conditions using weak supervision},
  author={Palmero Aprosio, Alessio and Tonelli, Sara and Turchi, Marco and Negri, Matteo and Di Gangi Mattia, A},
  booktitle={Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen)},
  pages={37--44},
  year={2019},
  organization={Association for Computational Linguistics (ACL)}
}

@article{wieting2017learning,
  title={Learning paraphrastic sentence embeddings from back-translated bitext},
  author={Wieting, John and Mallinson, Jonathan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1706.01847},
  year={2017}
}

@article{wieting2017paranmt,
  title={ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations},
  author={Wieting, John and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1711.05732},
  year={2017}
}

@inproceedings{hu2019parabank,
  title={ParaBank: Monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation},
  author={Hu, J Edward and Rudinger, Rachel and Post, Matt and Van Durme, Benjamin},
  booktitle={AAAI},
  volume={33},
  number={01},
  pages={6521--6528},
  year={2019}
}

@article{kajiwara2018text,
  title={Text simplification without simplified corpora},
  author={Kajiwara, Tomoyuki and Komachi, M},
  journal={The Journal of Natural Language Processing},
  volume={25},
  pages={223--249},
  year={2018}
}

@inproceedings{zhao2020semi,
  title={Semi-supervised text simplification with back-translation and asymmetric denoising autoencoders},
  author={Zhao, Yanbin and Chen, Lu and Chen, Zhi and Yu, Kai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={9668--9675},
  year={2020}
}

@article{saggion2017automatic,
  title={Automatic text simplification},
  author={Saggion, Horacio},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={10},
  number={1},
  pages={1--137},
  year={2017},
  publisher={Morgan \& Claypool Publishers}
}

@article{saggion2015making,
  title={Making it simplext: Implementation and evaluation of a text simplification system for spanish},
  author={Saggion, Horacio and {\v{S}}tajner, Sanja and Bott, Stefan and Mille, Simon and Rello, Luz and Drndarevic, Biljana},
  journal={ACM Transactions on Accessible Computing (TACCESS)},
  volume={6},
  number={4},
  pages={1--36},
  year={2015},
  publisher={ACM New York, NY, USA}
}


@article{qiang2019unsupervised,
  title={Unsupervised Statistical Text Simplification},
  author={Qiang, Jipeng and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={4},
  pages={1802--1806},
  year={2021}
}

@article{qiang2020AAAI,
  title={Lexical Simplification with Pretrained Encoders},
  author={Qiang, Jipeng and Li, Yun and Zhu, Yi and Yuan, Yunhao and Wu, Xindong},
  journal={Thirty-Fourth AAAI Conference on Artificial Intelligence},
  pages={8649–8656},
  year={2020}
}

@inproceedings{lu-etal-2021-unsupervised-method,
    title = "An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages",
    author = "Lu, Xinyu  and
      Qiang, Jipeng  and
      Li, Yun  and
      Yuan, Yunhao  and
      Zhu, Yi",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.22",
    doi = "10.18653/v1/2021.findings-emnlp.22",
    pages = "227--237"
    }
@article{qiang2020STTP,
  title =  {Short Text Topic Modeling Techniques, Applications, and Performance: A Survey },
  author = {Qiang, Jipeng and 
            Qian Zhenyu and
            Li, Yun and 
            Yuan, Yunhao and 
            Wu, Xindong},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages={DOI:10.1109/TKDE.2020.2992485},
  year  =  {2020}
}

@inproceedings{nisioi2017exploring,
  title={Exploring neural text simplification models},
  author={Nisioi, Sergiu and {\v{S}}tajner, Sanja and Ponzetto, Simone Paolo and Dinu, Liviu P},
  booktitle={Proceedings of the 55th annual meeting of the association for computational linguistics (volume 2: Short papers)},
  pages={85--91},
  year={2017}
}

@article{Wenzek2019,
  author    = {Guillaume Wenzek and
               Marie{-}Anne Lachaux and
               Alexis Conneau and
               Vishrav Chaudhary and
               Francisco Guzm{\'{a}}n and
               Armand Joulin and
               Edouard Grave},
  title     = {CCNet: Extracting High Quality Monolingual Datasets from Web Crawl
               Data},
  journal   = {CoRR},
  volume    = {abs/1911.00359},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.00359},
  archivePrefix = {arXiv},
  eprint    = {1911.00359}
}


@article{2015Unsupervised,
  title={Unsupervised Sentence Simplification Using Deep Semantics},
  author={ Narayan, S.  and  Gardent, C. },
  booktitle={Proceedings of The 9th International Natural Language Generation conference},
  pages={111--120},
  year={2015},
}

@inproceedings{aluisio2008towards,
  title={Towards brazilian portuguese automatic text simplification systems},
  author={Alu{\'\i}sio, Sandra M and Specia, Lucia and Pardo, Thiago AS and Maziero, Erick G and Fortes, Renata PM},
  booktitle={Proceedings of the eighth ACM symposium on Document engineering},
  pages={240--248},
  year={2008}
}

@inproceedings{brunato2015design,
  title={Design and annotation of the first Italian corpus for text simplification},
  author={Brunato, Dominique and Dell’Orletta, Felice and Venturi, Giulia and Montemagni, Simonetta},
  booktitle={Proceedings of The 9th Linguistic Annotation Workshop},
  pages={31--41},
  year={2015}
}

@inproceedings{kusner2015word,
  title={From word embeddings to document distances},
  author={Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
  booktitle={International conference on machine learning},
  pages={957--966},
  year={2015},
  organization={PMLR}
}

@inproceedings{artetxe-etal-2018-unsupervised,
    title = "Unsupervised Statistical Machine Translation",
    author = "Artetxe, Mikel  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1399",
    doi = "10.18653/v1/D18-1399",
    pages = "3632--3642"
    }
    
    
@article{goto2015japanese,
  title={Japanese news simplification: Task design, data set construction, and analysis of simplified text},
  author={Goto, Isao and Tanaka, Hideki and Kumano, Tadashi},
  journal={Proceedings of MT Summit XV},
  volume={1},
  pages={17--31},
  year={2015}
}

@misc{mehta2020simplifythentranslate,
      title={Simplify-then-Translate: Automatic Preprocessing for Black-Box Machine Translation}, 
      author={Sneha Mehta and Bahareh Azarnoush and Boris Chen and Avneesh Saluja and Vinith Misra and Ballav Bihani and Ritwik Kumar},
      year={2020},
      eprint={2005.11197},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{chandrasekar1996motivations,
  title={Motivations and methods for text simplification},
  author={Chandrasekar, Raman and Doran, Christine and Bangalore, Srinivas},
  booktitle={COLING 1996 Volume 2: The 16th International Conference on Computational Linguistics},
  pages={1041--1044},
  year={1996}
}

@inproceedings{niklaus-etal-2016-sentence,
    title = "A Sentence Simplification System for Improving Relation Extraction",
    author = "Niklaus, Christina  and
      Bermeitinger, Bernhard  and
      Handschuh, Siegfried  and
      Freitas, Andr{\'e}",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-2036",
    pages = "170--174"
}

@inproceedings{jiang2019improving,
  title={Improving neural response diversity with frequency-aware cross-entropy loss},
  author={Jiang, Shaojie and Ren, Pengjie and Monz, Christof and de Rijke, Maarten},
  booktitle={The World Wide Web Conference},
  pages={2879--2885},
  year={2019}
}

% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz

@inproceedings{woodsend2011learning,
  title={Learning to simplify sentences with quasi-synchronous grammar and integer programming},
  author={Woodsend, Kristian and Lapata, Mirella},
  booktitle={Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  pages={409--420},
  year={2011}
}

@inproceedings{kauchak2013improving,
  title={Improving text simplification language modeling using unsimplified text data},
  author={Kauchak, David},
  booktitle={Proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: Long papers)},
  pages={1537--1546},
  year={2013}
}

@inproceedings{zhang-lapata-2017-sentence,
    title = "Sentence Simplification with Deep Reinforcement Learning",
    author = "Zhang, Xingxing  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1062",
    doi = "10.18653/v1/D17-1062",
    pages = "584--594",
    abstract = "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for \textbf{D}eep \textbf{RE}inforcement \textbf{S}entence \textbf{S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.",
}

@article{xu-etal-2015-problems,
    title = "Problems in Current Text Simplification Research: New Data Can Help",
    author = "Xu, Wei  and
      Callison-Burch, Chris  and
      Napoles, Courtney",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    url = "https://www.aclweb.org/anthology/Q15-1021",
    doi = "10.1162/tacl_a_00139",
    pages = "283--297",
    abstract = "Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on Wikipedia limits simplification research. We back up our arguments with corpus analysis and by highlighting statements that other researchers have made in the simplification literature. We introduce a new simplification dataset that is a significant improvement over Simple Wikipedia, and present a novel quantitative-comparative approach to study the quality of simplification data resources.",
}

@inproceedings{martin-etal-2020-controllable,
    title = "Controllable Sentence Simplification",
    author = "Martin, Louis  and
      de la Clergerie, {\'E}ric  and
      Sagot, Beno{\^\i}t  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.577",
    pages = "4689--4698",
    abstract = "Text simplification aims at making a text easier to read and understand by simplifying grammar and structure while keeping the underlying information identical. It is often considered an all-purpose generic task where the same simplification is suitable for all; however multiple audiences can benefit from simplified text in different ways. We adapt a discrete parametrization mechanism that provides explicit control on simplification systems based on Sequence-to-Sequence models. As a result, users can condition the simplifications returned by a model on attributes such as length, amount of paraphrasing, lexical complexity and syntactic complexity. We also show that carefully chosen values of these attributes allow out-of-the-box Sequence-to-Sequence models to outperform their standard counterparts on simplification benchmarks. Our model, which we call ACCESS (as shorthand for AudienCe-CEntric Sentence Simplification), establishes the state of the art at 41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best previously reported score.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{ganitkevitch2013ppdb,
  title={PPDB: The paraphrase database},
  author={Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
  booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={758--764},
  year={2013}
}

@inproceedings{pavlick-etal-2015-ppdb,
    title = "{PPDB} 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification",
    author = "Pavlick, Ellie  and
      Rastogi, Pushpendre  and
      Ganitkevitch, Juri  and
      Van Durme, Benjamin  and
      Callison-Burch, Chris",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2070",
    doi = "10.3115/v1/P15-2070",
    pages = "425--430",
}

@inproceedings{mallinson2017paraphrasing,
  title={Paraphrasing revisited with neural machine translation},
  author={Mallinson, Jonathan and Sennrich, Rico and Lapata, Mirella},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages={881--893},
  year={2017}
}

@inproceedings{post2018call,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={Post, Matt},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={186--191},
  year={2018}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{sulem-etal-2018-bleu,
    title = "{BLEU} is Not Suitable for the Evaluation of Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1081",
    doi = "10.18653/v1/D18-1081",
    pages = "738--744",
    abstract = "BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments. We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences.",
}

@inproceedings{ott-etal-2018-scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{wubben-etal-2012-sentence,
    title = "Sentence Simplification by Monolingual Machine Translation",
    author = "Wubben, Sander  and
      van den Bosch, Antal  and
      Krahmer, Emiel",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-1107",
    pages = "1015--1024",
}

@inproceedings{surya-etal-2019-unsupervised,
    title = "Unsupervised Neural Text Simplification",
    author = "Surya, Sai  and
      Mishra, Abhijit  and
      Laha, Anirban  and
      Jain, Parag  and
      Sankaranarayanan, Karthik",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1198",
    doi = "10.18653/v1/P19-1198",
    pages = "2058--2068",
    abstract = "The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. It also outperforms viable unsupervised baselines. Adding a few labeled pairs helps improve the performance further.",
}


@inproceedings{dong-etal-2019-editnts,
    title = "{E}dit{NTS}: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing",
    author = "Dong, Yue  and
      Li, Zichao  and
      Rezagholizadeh, Mehdi  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1331",
    doi = "10.18653/v1/P19-1331",
    pages = "3393--3402"
}

@inproceedings{scarton2018learning,
  title={Learning simplifications for specific target audiences},
  author={Scarton, Carolina and Specia, Lucia},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={712--718},
  year={2018}
}

@inproceedings{zhao-etal-2018-integrating,
    title = "Integrating Transformer and Paraphrase Rules for Sentence Simplification",
    author = "Zhao, Sanqiang  and
      Meng, Rui  and
      He, Daqing  and
      Saptono, Andi  and
      Parmanto, Bambang",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1355",
    doi = "10.18653/v1/D18-1355",
    pages = "3164--3173",
    abstract = "Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normal-simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state-of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at \url{https://github.com/Sanqiang/text_simplification}.",
}

@inproceedings{kumar-etal-2020-iterative,
    title = "Iterative Edit-Based Unsupervised Sentence Simplification",
    author = "Kumar, Dhruv  and
      Mou, Lili  and
      Golab, Lukasz  and
      Vechtomova, Olga",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.707",
    doi = "10.18653/v1/2020.acl-main.707",
    pages = "7918--7928",
    abstract = "We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.",
}

@inproceedings{gu-etal-2020-token,
    title = "Token-level Adaptive Training for Neural Machine Translation",
    author = "Gu, Shuhao  and
      Zhang, Jinchao  and
      Meng, Fandong  and
      Feng, Yang  and
      Xie, Wanying  and
      Zhou, Jie  and
      Yu, Dong",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.76",
    doi = "10.18653/v1/2020.emnlp-main.76",
    pages = "1035--1046",
    abstract = "There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.",
}

@inproceedings{Kriz2018Simplification,
  title={Simplification Using Paraphrases and Context-Based Lexical Substitution},
  author={Kriz, Reno and Miltsakaki, Eleni and Apidianaki, Marianna and Callisonburch, Chris},
  booktitle={NAACL},
  pages={207--217},
  year={2018},
}

@inproceedings{woodsend2011learning,
  title={Learning to simplify sentences with quasi-synchronous grammar and integer programming},
  author={Woodsend, Kristian and Lapata, Mirella},
  booktitle={Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  pages={409--420},
  year={2011}
}

@inproceedings{ghazvininejad-etal-2019-mask,
    title = "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
    author = "Ghazvininejad, Marjan  and
      Levy, Omer  and
      Liu, Yinhan  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1633",
    doi = "10.18653/v1/D19-1633",
    pages = "6112--6121",
    abstract = "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.",
}

@inproceedings{gooding-kochmar-2019-recursive,
    title = "Recursive Context-Aware Lexical Simplification",
    author = "Gooding, Sian  and
      Kochmar, Ekaterina",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1491",
    doi = "10.18653/v1/D19-1491",
    pages = "4853--4863",
    abstract = "This paper presents a novel architecture for recursive context-aware lexical simplification, REC-LS, that is capable of (1) making use of the wider context when detecting the words in need of simplification and suggesting alternatives, and (2) taking previous simplification steps into account. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and outperforms the current state-of-the-art systems in lexical simplification.",
}


@inproceedings{qian-etal-2021-glancing,
    title = "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
    author = "Qian, Lihua  and
      Zhou, Hao  and
      Bao, Yu  and
      Wang, Mingxuan  and
      Qiu, Lin  and
      Zhang, Weinan  and
      Yu, Yong  and
      Li, Lei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.155",
    doi = "10.18653/v1/2021.acl-long.155",
    pages = "1993--2003",
    abstract = "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8{\mbox{$\times$}}-15{\mbox{$\times$}} speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.",
}

@inproceedings{kauchak2013improving,
  title={Improving text simplification language modeling using unsimplified text data},
  author={Kauchak, David},
  booktitle={Proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: Long papers)},
  pages={1537--1546},
  year={2013}
}

@inproceedings{zhang-lapata-2017-sentence,
    title = "Sentence Simplification with Deep Reinforcement Learning",
    author = "Zhang, Xingxing  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1062",
    doi = "10.18653/v1/D17-1062",
    pages = "584--594",
    abstract = "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for \textbf{D}eep \textbf{RE}inforcement \textbf{S}entence \textbf{S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.",
}

@article{xu-etal-2015-problems,
    title = "Problems in Current Text Simplification Research: New Data Can Help",
    author = "Xu, Wei  and
      Callison-Burch, Chris  and
      Napoles, Courtney",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    url = "https://www.aclweb.org/anthology/Q15-1021",
    doi = "10.1162/tacl_a_00139",
    pages = "283--297",
    abstract = "Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on Wikipedia limits simplification research. We back up our arguments with corpus analysis and by highlighting statements that other researchers have made in the simplification literature. We introduce a new simplification dataset that is a significant improvement over Simple Wikipedia, and present a novel quantitative-comparative approach to study the quality of simplification data resources.",
}

@inproceedings{martin-etal-2020-controllable,
    title = "Controllable Sentence Simplification",
    author = "Martin, Louis  and
      de la Clergerie, {\'E}ric  and
      Sagot, Beno{\^\i}t  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.577",
    pages = "4689--4698",
    abstract = "Text simplification aims at making a text easier to read and understand by simplifying grammar and structure while keeping the underlying information identical. It is often considered an all-purpose generic task where the same simplification is suitable for all; however multiple audiences can benefit from simplified text in different ways. We adapt a discrete parametrization mechanism that provides explicit control on simplification systems based on Sequence-to-Sequence models. As a result, users can condition the simplifications returned by a model on attributes such as length, amount of paraphrasing, lexical complexity and syntactic complexity. We also show that carefully chosen values of these attributes allow out-of-the-box Sequence-to-Sequence models to outperform their standard counterparts on simplification benchmarks. Our model, which we call ACCESS (as shorthand for AudienCe-CEntric Sentence Simplification), establishes the state of the art at 41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best previously reported score.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{ganitkevitch2013ppdb,
  title={PPDB: The paraphrase database},
  author={Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
  booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={758--764},
  year={2013}
}

@inproceedings{pavlick-etal-2015-ppdb,
    title = "{PPDB} 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification",
    author = "Pavlick, Ellie  and
      Rastogi, Pushpendre  and
      Ganitkevitch, Juri  and
      Van Durme, Benjamin  and
      Callison-Burch, Chris",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2070",
    doi = "10.3115/v1/P15-2070",
    pages = "425--430",
}

@inproceedings{mallinson2017paraphrasing,
  title={Paraphrasing revisited with neural machine translation},
  author={Mallinson, Jonathan and Sennrich, Rico and Lapata, Mirella},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages={881--893},
  year={2017}
}

@inproceedings{post2018call,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={Post, Matt},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={186--191},
  year={2018}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{bao-etal-2022-textit,
    title = "{latent-GLAT}: Glancing at Latent Variables for Parallel Text Generation",
    author = "Bao, Yu  and
      Zhou, Hao  and
      Huang, Shujian  and
      Wang, Dongqi  and
      Qian, Lihua  and
      Dai, Xinyu  and
      Chen, Jiajun  and
      Li, Lei",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.575",
    doi = "10.18653/v1/2022.acl-long.575",
    pages = "8398--8409",
}

@inproceedings{sulem-etal-2018-bleu,
    title = "{BLEU} is Not Suitable for the Evaluation of Text Simplification",
    author = "Sulem, Elior  and
      Abend, Omri  and
      Rappoport, Ari",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1081",
    doi = "10.18653/v1/D18-1081",
    pages = "738--744",
    abstract = "BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments. We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences.",
}

@inproceedings{ott-etal-2018-scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{wubben-etal-2012-sentence,
    title = "Sentence Simplification by Monolingual Machine Translation",
    author = "Wubben, Sander  and
      van den Bosch, Antal  and
      Krahmer, Emiel",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-1107",
    pages = "1015--1024",
}

@inproceedings{surya-etal-2019-unsupervised,
    title = "Unsupervised Neural Text Simplification",
    author = "Surya, Sai  and
      Mishra, Abhijit  and
      Laha, Anirban  and
      Jain, Parag  and
      Sankaranarayanan, Karthik",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1198",
    doi = "10.18653/v1/P19-1198",
    pages = "2058--2068",
    abstract = "The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. It also outperforms viable unsupervised baselines. Adding a few labeled pairs helps improve the performance further.",
}


@inproceedings{dong-etal-2019-editnts,
    title = "{E}dit{NTS}: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing",
    author = "Dong, Yue  and
      Li, Zichao  and
      Rezagholizadeh, Mehdi  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1331",
    doi = "10.18653/v1/P19-1331",
    pages = "3393--3402"
}

@inproceedings{scarton2018learning,
  title={Learning simplifications for specific target audiences},
  author={Scarton, Carolina and Specia, Lucia},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={712--718},
  year={2018}
}

@inproceedings{zhao-etal-2018-integrating,
    title = "Integrating Transformer and Paraphrase Rules for Sentence Simplification",
    author = "Zhao, Sanqiang  and
      Meng, Rui  and
      He, Daqing  and
      Saptono, Andi  and
      Parmanto, Bambang",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1355",
    doi = "10.18653/v1/D18-1355",
    pages = "3164--3173",
    abstract = "Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normal-simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state-of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at \url{https://github.com/Sanqiang/text_simplification}.",
}

@inproceedings{kumar-etal-2020-iterative,
    title = "Iterative Edit-Based Unsupervised Sentence Simplification",
    author = "Kumar, Dhruv  and
      Mou, Lili  and
      Golab, Lukasz  and
      Vechtomova, Olga",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.707",
    doi = "10.18653/v1/2020.acl-main.707",
    pages = "7918--7928",
    abstract = "We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.",
}

@inproceedings{gu-etal-2020-token,
    title = "Token-level Adaptive Training for Neural Machine Translation",
    author = "Gu, Shuhao  and
      Zhang, Jinchao  and
      Meng, Fandong  and
      Feng, Yang  and
      Xie, Wanying  and
      Zhou, Jie  and
      Yu, Dong",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.76",
    doi = "10.18653/v1/2020.emnlp-main.76",
    pages = "1035--1046",
    abstract = "There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.",
}


@inproceedings{thompson2020paraphrase,
  title={Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity},
  author={Thompson, Brian and Post, Matt},
  booktitle={Proceedings of the Fifth Conference on Machine Translation},
  pages={561--570},
  year={2020}
}

@inproceedings{lu-etal-2022-neurologic,
    title = "{N}euro{L}ogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics",
    author = "Lu, Ximing  and
      Welleck, Sean  and
      West, Peter  and
      Jiang, Liwei  and
      Kasai, Jungo  and
      Khashabi, Daniel  and
      Le Bras, Ronan  and
      Qin, Lianhui  and
      Yu, Youngjae  and
      Zellers, Rowan  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.57",
    doi = "10.18653/v1/2022.naacl-main.57",
    pages = "780--799",
    abstract = "The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the $A^*$ search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-$k$ sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.",
}

@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  pages={100--107},
  year={1968}
}

@inproceedings{whistely2022presiuniv,
  title={PresiUniv at TSAR-2022 Shared Task: Generation and Ranking of Simplification Substitutes of Complex Words in Multiple Languages},
  author={Whistely, Peniel and Mathias, Sandeep and Poornima, Galiveeti},
  booktitle={TSAR},
  pages={213--217},
  year={2022}
}



@inproceedings{ferres2022alexsis,
  title={ALEXSIS: a dataset for lexical simplification in Spanish},
  author={Ferr{\'e}s, Daniel and Saggion, Horacio},
  booktitle={Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  pages={3582--3594},
  year={2022}
}

@inproceedings{north2022alexsis,
  title={ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification},
  author={North, Kai and Zampieri, Marcos and Ranasinghe, Tharindu},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={6057--6062},
  year={2022}
}

@article{qiang2021chinese,
  title={Chinese lexical simplification},
  author={Qiang, Jipeng and Lu, Xinyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={1819--1828},
  year={2021},
  publisher={IEEE}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{li-etal-2022-mantis,
    title = "{MANTIS} at {TSAR}-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders",
    author = "Li, Xiaofei  and
      Wiechmann, Daniel  and
      Qiao, Yu  and
      Kerz, Elma",
    booktitle = "TSAR",
    year = "2022",
    pages = "243--250"
}


@inproceedings{lin-etal-2022-improving,
    title = "Improving Contextual Representation with Gloss Regularized Pre-training",
    author = "Lin, Yu  and
      An, Zhecheng  and
      Wu, Peihao  and
      Ma, Zejun",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.68",
    doi = "10.18653/v1/2022.findings-naacl.68",
    pages = "907--920",
    abstract = "Though achieving impressive results on many NLP tasks, the BERT-like masked language models (MLM) encounter the discrepancy between pre-training and inference. In light of this gap, we investigate the contextual representation of pre-training and inference from the perspective of word probability distribution. We discover that BERT risks neglecting the contextual word similarity in pre-training. To tackle this issue, we propose an auxiliary gloss regularizer module to BERT pre-training (GR-BERT), to enhance word semantic similarity. By predicting masked words and aligning contextual embeddings to corresponding glosses simultaneously, the word similarity can be explicitly modeled. We design two architectures for GR-BERT and evaluate our model in downstream tasks. Experimental results show that the gloss regularizer benefits BERT in word-level and sentence-level semantic representation. The GR-BERT achieves new state-of-the-art in lexical substitution task and greatly promotes BERT sentence representation in both unsupervised and supervised STS tasks.",
}

@article{saggion2023findings,
  title={Findings of the tsar-2022 shared task on multilingual lexical simplification},
  author={Saggion, Horacio and {\v{S}}tajner, Sanja and Ferr{\'e}s, Daniel and Sheang, Kim Cheng and Shardlow, Matthew and North, Kai and Zampieri, Marcos},
  journal={arXiv preprint arXiv:2302.02888},
  year={2023}
}

@article{costa2022no,
  title={No language left behind: Scaling human-centered machine translation},
  author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

@inproceedings{paetzold2017lexical,
  title={Lexical simplification with neural ranking},
  author={Paetzold, Gustavo and Specia, Lucia},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={34--40},
  year={2017}
}

@inproceedings{aumiller-gertz-2022-unihd,
    title = "{U}ni{HD} at {TSAR}-2022 Shared Task: Is Compute All We Need for Lexical Simplification?",
    author = "Aumiller, Dennis  and
      Gertz, Michael",
    booktitle = "TSAR",
    year = "2022",
    pages = "251--258"
}


@article{vstajner2021automatic,
  title={Automatic text simplification for social good: progress and challenges},
  author={{\v{S}}tajner, Sanja},
  journal={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={2637--2652},
  year={2021}
}

@inproceedings{carroll1998practical,
  title={Practical simplification of English newspaper text to assist aphasic readers},
  author={Carroll, John and Minnen, Guido and Canning, Yvonne and Devlin, Siobhan and Tait, John},
  booktitle={AAAI Workshop},
  pages={7--10},
  year={1998},
}

@article{qiang2021chinese,
  title={Chinese lexical simplification},
  author={Qiang, Jipeng and Lu, Xinyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={1819--1828},
  year={2021},
  publisher={IEEE}
}

@article{vstajner2022lexical,
  title={Lexical simplification benchmarks for English, Portuguese, and Spanish},
  author={{\v{S}}tajner, Sanja and Ferr{\'e}s, Daniel and Shardlow, Matthew and North, Kai and Zampieri, Marcos and Saggion, Horacio},
  journal={Frontiers in artificial intelligence},
  volume={5},
  pages={991242},
  year={2022}
}


@inproceedings{vamvas-sennrich-2022-nmtscore,
    title = "{NMTS}core: A Multilingual Analysis of Translation-based Text Similarity Measures",
    author = "Vamvas, Jannis  and
      Sennrich, Rico",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.15",
    pages = "198--213",
    abstract = "Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.",
}

@inproceedings{wieting2018paranmt,
  title={ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations},
  author={Wieting, John and Gimpel, Kevin},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={451--462},
  year={2018}
}

@inproceedings{wieting-etal-2019-simple,
    title = "Simple and Effective Paraphrastic Similarity from Parallel Translations",
    author = "Wieting, John  and
      Gimpel, Kevin  and
      Neubig, Graham  and
      Berg-Kirkpatrick, Taylor",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1453",
    doi = "10.18653/v1/P19-1453",
    pages = "4602--4608",
    abstract = "We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.",
}

@inproceedings{hu2019parabank,
  title={Parabank: Monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation},
  author={Hu, J Edward and Rudinger, Rachel and Post, Matt and Van Durme, Benjamin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6521--6528},
  year={2019}
}

@inproceedings{hao2022parazh,
  title={ParaZh-22M: A Large-Scale Chinese Parabank via Machine Translation},
  author={Hao, Wenjie and Xu, Hongfei and Xiong, Deyi and Zan, Hongying and Mu, Lingling},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={3885--3897},
  year={2022}
}

@inproceedings{zhou2021paraphrase,
  title={Paraphrase generation: A survey of the state of the art},
  author={Zhou, Jianing and Bhat, Suma},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5075--5086},
  year={2021}
}

@software{robyn_speer_2022_7199437,
  author       = {Robyn Speer},
  title        = {rspeer/wordfreq: v3.0},
  month        = sep,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v3.0.2},
  doi          = {10.5281/zenodo.7199437},
  url          = {https://doi.org/10.5281/zenodo.7199437}
}

@article{bojanowski2017enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  year={2017},
  issn={2307-387X},
  pages={135--146}
}

@inproceedings{whistely-etal-2022-presiuniv,
    title = "{P}resi{U}niv at {TSAR}-2022 Shared Task: Generation and Ranking of Simplification Substitutes of Complex Words in Multiple Languages",
    author = "Whistely, Peniel  and
      Mathias, Sandeep  and
      Poornima, Galiveeti",
    booktitle = "Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.tsar-1.22",
    pages = "213--217",
    abstract = "In this paper, we describe our approach to generate and rank candidate simplifications using pre-trained language models (Eg. BERT), publicly available word embeddings (Eg. FastText), and a part-of-speech tagger, to generate and rank candidate contextual simplifications for a given complex word. In this task, our system, PresiUniv, was placed first in the Spanish track, 5th in the Brazilian-Portuguese track, and 10th in the English track. We upload our codes and data for this project to aid in replication of our results. We also analyze some of the errors and describe design decisions which we took while writing the paper.",
}

@inproceedings{north-etal-2022-gmu,
    title = "{GMU}-{WLV} at {TSAR}-2022 Shared Task: Evaluating Lexical Simplification Models",
    author = "North, Kai  and
      Dmonte, Alphaeus  and
      Ranasinghe, Tharindu  and
      Zampieri, Marcos",
    booktitle = "Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.tsar-1.30",
    pages = "264--270",
    abstract = "This paper describes team GMU-WLV submission to the TSAR shared-task on multilingual lexical simplification. The goal of the task is to automatically provide a set of candidate substitutions for complex words in context. The organizers provided participants with ALEXSIS a manually annotated dataset with instances split between a small trial set with a dozen instances in each of the three languages of the competition (English, Portuguese, Spanish) and a test set with over 300 instances in the three aforementioned languages. To cope with the lack of training data, participants had to either use alternative data sources or pre-trained language models. We experimented with monolingual models: BERTimbau, ELECTRA, and RoBERTA-largeBNE. Our best system achieved 1st place out of sixteen systems for Portuguese, 8th out of thirty-three systems for English, and 6th out of twelve systems for Spanish.",
}

@inproceedings{li-etal-2022-mantis,
    title = "{MANTIS} at {TSAR}-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders",
    author = "Li, Xiaofei  and
      Wiechmann, Daniel  and
      Qiao, Yu  and
      Kerz, Elma",
    booktitle = "Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.tsar-1.27",
    pages = "243--250",
    abstract = "In this paper we present our contribution to the TSAR-2022 Shared Task on Lexical Simplification of the EMNLP 2022 Workshop on Text Simplification, Accessibility, and Readability. Our approach builds on and extends the unsupervised lexical simplification system with pretrained encoders (LSBert) system introduced in Qiang et al. (2020) in the following ways: For the subtask of simplification candidate selection, it utilizes a RoBERTa transformer language model and expands the size of the generated candidate list. For subsequent substitution ranking, it introduces a new feature weighting scheme and adopts a candidate filtering method based on textual entailment to maximize semantic similarity between the target word and its simplification. Our best-performing system improves LSBert by 5.9{\%} accuracy and achieves second place out of 33 ranked solutions.",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{horn2014learning,
  title={Learning a lexical simplifier using wikipedia},
  author={Horn, Colby and Manduca, Cathryn and Kauchak, David},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={458--463},
  year={2014}
}

@inproceedings{horn2014learning,
  title={Learning a lexical simplifier using wikipedia},
  author={Horn, Colby and Manduca, Cathryn and Kauchak, David},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={458--463},
  year={2014}
}

@inproceedings{paetzold2017lexical,
  title={Lexical simplification with neural ranking},
  author={Paetzold, Gustavo and Specia, Lucia},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={34--40},
  year={2017}
}

@inproceedings{qiang2020lexical,
  title={Lexical simplification with pretrained encoders},
  author={Qiang, Jipeng and Li, Yun and Zhu, Yi and Yuan, Yunhao and Wu, Xindong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8649--8656},
  year={2020}
}


@inproceedings{yimam2018report,
  title={A Report on the Complex Word Identification Shared Task 2018},
  author={Yimam, Seid Muhie and Biemann, Chris and Malmasi, Shervin and Paetzold, Gustavo and Specia, Lucia and {\v{S}}tajner, Sanja and Tack, Ana{\"\i}s and Zampieri, Marcos},
  booktitle={Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={66--78},
  year={2018}
}

@inproceedings{finnimore2019strong,
  title={Strong Baselines for Complex Word Identification across Multiple Languages},
  author={Finnimore, Pierre and Fritzsch, Elisabeth and King, Daniel and Sneyd, Alison and Rehman, Aneeq Ur and Alva-Manchego, Fernando and Vlachos, Andreas},
  booktitle={NAACL},
  pages={970--977},
  year={2019}
}

@article{dabre2020survey,
  title={A survey of multilingual neural machine translation},
  author={Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={5},
  pages={1--38},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{johnson2017google,
  title={Google’s multilingual neural machine translation system: Enabling zero-shot translation},
  author={Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={339--351},
  year={2017},
  publisher={MIT Press}
}

@inproceedings{tan-etal-2019-multilingual,
    title = "Multilingual Neural Machine Translation with Language Clustering",
    author = "Tan, Xu  and
      Chen, Jiale  and
      He, Di  and
      Xia, Yingce  and
      Qin, Tao  and
      Liu, Tie-Yan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1089",
    doi = "10.18653/v1/D19-1089",
    pages = "963--973",
    abstract = "Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.",
}


@article{fan2021beyond,
  title={Beyond english-centric multilingual machine translation},
  author={Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and others},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={4839--4886},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{vamvas-sennrich-2022-nmtscore,
    title = "{NMTS}core: A Multilingual Analysis of Translation-based Text Similarity Measures",
    author = "Vamvas, Jannis  and
      Sennrich, Rico",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.15",
    pages = "198--213",
    abstract = "Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.",
}

@inproceedings{vijayakumar2018diverse,
  title={Diverse beam search for improved description of complex scenes},
  author={Vijayakumar, Ashwin and Cogswell, Michael and Selvaraju, Ramprasaath and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{biran2011putting,
  title={Putting it simply: a context-aware approach to lexical simplification},
  author={Biran, Or and Brody, Samuel and Elhadad, No{\'e}mie},
  booktitle={ACL},
  pages={496--501},
  year={2011}
}

@inproceedings{hao2022parazh,
  title={ParaZh-22M: A Large-Scale Chinese Parabank via Machine Translation},
  author={Hao, Wenjie and Xu, Hongfei and Xiong, Deyi and Zan, Hongying and Mu, Lingling},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={3885--3897},
  year={2022}
}

@article{qiang2021unsupervised,
  title={Unsupervised Statistical Text Simplification},
  author={Qiang, Jipeng and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={4},
  pages={1802--1806},
  year={2021}
}

@article{qiang2023natural,
  title={Natural Language Watermarking via Paraphraser-based Lexical Substitution},
  author={Qiang, Jipeng and Zhu, Shiyu and Li, Yun and Zhu, Yi and Yuan, Yunhao and Wu, Xindong},
  journal={Artificial Intelligence},
  pages={103859},
  year={2023},
  publisher={Elsevier}
}

@article{qiang2021chinese,
  title={Chinese Lexical Simplification},
  author={Qiang, Jipeng and Lv, Xinyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
  journal={IEEE Transactions on Audio, Speech and Language Processing.},
  volume={29},
  number={},
  pages={1819-1828},
  year={2021}
}