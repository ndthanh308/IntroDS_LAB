% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Haghani18SLU}
P.~Haghani, A.~Narayanan, M.~A.~U. Bacchiani, G.~Chuang, N.~Gaur, P.~J.~M.
  Mengibar, D.~Qu, R.~Prabhavalkar, and A.~Waters, ``From audio to semantics:
  Approaches to end-to-end spoken language understanding,'' in \emph{Proc.
  SLT}, 2018.

\bibitem{serdyuk2018endtoend}
D.~Serdyuk, Y.~Wang, C.~Fuegen, A.~Kumar, B.~Liu, and Y.~Bengio, ``Towards
  end-to-end spoken language understanding,'' 2018.

\bibitem{Potdar2021ASE}
N.~Potdar, A.~R. Avila, C.~Xing, D.~Wang, Y.~Cao, and X.~Chen, ``A streaming
  end-to-end framework for spoken language understanding,'' in \emph{IJCAI},
  2021.

\bibitem{Radfar2021FANSFA}
M.~Radfar, A.~Mouchtaris, S.~Kunzmann, and A.~Rastrow, ``Fans: Fusing asr and
  nlu for on-device slu,'' in \emph{Interspeech}, 2021.

\bibitem{Wang2021Speech2SlotAE}
P.~Wang, X.~Ye, X.~Zhou, J.~Xie, and H.~Wang, ``Speech2slot: An end-to-end
  knowledge-based slot filling from speech,'' \emph{ArXiv}, vol.
  abs/2105.04719, 2021.

\bibitem{2020NeuralInterface}
M.~Rao, A.~Raju, P.~Dheram, B.~Bui, and A.~Rastrow, ``Speech to semantics:
  Improve asr and nlu jointly via all-neural interfaces,'' in \emph{Proc.
  INTERSPEECH}, 2020.

\bibitem{raju2021end}
A.~Raju, G.~Tiwari, M.~Rao, P.~Dheram, B.~Anderson, Z.~Zhang, B.~Bui, and
  A.~Rastrow, ``End-to-end spoken language understanding using rnn-transducer
  asr,'' \emph{arXiv preprint arXiv:2106.15919}, 2021.

\bibitem{arora2022two}
S.~Arora, S.~Dalmia, X.~Chang, B.~Yan, A.~Black, and S.~Watanabe, ``Two-pass
  low latency end-to-end spoken language understanding,'' \emph{Interspeech},
  2022.

\bibitem{xu2022introducing}
D.~Xu, S.~Dong, C.~Wang, S.~Kim, Z.~Lin, A.~Shrivastava, S.-W. Li, L.-H. Tseng,
  A.~Baevski, G.-T. Lin \emph{et~al.}, ``Introducing semantics into speech
  encoders,'' \emph{arXiv preprint arXiv:2211.08402}, 2022.

\bibitem{le2022deliberation}
D.~Le, A.~Shrivastava, P.~Tomasello, S.~Kim, A.~Livshits, O.~Kalinli, and M.~L.
  Seltzer, ``Deliberation model for on-device spoken language understanding,''
  \emph{Interspeech}, 2022.

\bibitem{desot2022end}
T.~Desot, F.~Portet, and M.~Vacher, ``End-to-end spoken language understanding:
  Performance analyses of a voice command task in a low resource setting,''
  \emph{Computer Speech \& Language}, vol.~75, p. 101369, 2022.

\bibitem{sainath2019two}
T.~N. Sainath, R.~Pang, D.~Rybach, Y.~He, R.~Prabhavalkar, W.~Li, M.~Visontai,
  Q.~Liang, T.~Strohman, Y.~Wu \emph{et~al.}, ``Two-pass end-to-end speech
  recognition,'' \emph{arXiv preprint arXiv:1908.10992}, 2019.

\bibitem{li2020parallel}
W.~Li, J.~Qin, C.-C. Chiu, R.~Pang, and Y.~He, ``Parallel rescoring with
  transformer for streaming on-device speech recognition,'' \emph{arXiv
  preprint arXiv:2008.13093}, 2020.

\bibitem{xu2022rescorebert}
L.~Xu, Y.~Gu, J.~Kolehmainen, H.~Khan, A.~Gandhe, A.~Rastrow, A.~Stolcke, and
  I.~Bulyko, ``Rescorebert: Discriminative speech recognition rescoring with
  bert,'' in \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 6117--6121.

\bibitem{kim2022joint}
S.~Kim, K.~Li, L.~Kabela, R.~Huang, J.~Zhu, O.~Kalinli, and D.~Le, ``Joint
  audio/text training for transformer rescorer of streaming speech
  recognition,'' \emph{EMNLP}, 2022.

\bibitem{stop2022}
P.~Tomasello, A.~Shrivastava, D.~Lazar, P.-C. Hsu, D.~Le, A.~Sagar, A.~Elkahky,
  J.~Copet, W.-N. Hsu, Y.~Mordechay, R.~Algayres, T.~A. Nguyen, E.~Dupoux,
  L.~Zettlemoyer, and A.~Mohamed, ``{STOP: A dataset for Spoken Task Oriented
  Semantic Parsing},'' in \emph{CoRR}.

\bibitem{graves2012sequence}
A.~Graves, ``Sequence transduction with recurrent neural networks,''
  \emph{arXiv preprint arXiv:1211.3711}, 2012.

\bibitem{Prabhavalkar17}
R.~Prabhavalkar, K.~Rao, T.~Sainath, B.~Li, L.~Johnson, and N.~Jaitly, ``{A
  Comparison of Sequence-to-Sequence Models for Speech Recognition},'' in
  \emph{Interspeech}, 2017, pp. 939--943.

\bibitem{battenberg2017exploring}
E.~Battenberg, J.~Chen, R.~Child, A.~Coates, Y.~G.~Y. Li, H.~Liu, S.~Satheesh,
  A.~Sriram, and Z.~Zhu, ``Exploring neural transducers for end-to-end speech
  recognition,'' in \emph{ASRU}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2017, pp. 206--213.

\bibitem{he2019streaming}
Y.~He, T.~N. Sainath, R.~Prabhavalkar, I.~McGraw, R.~Alvarez, D.~Zhao,
  D.~Rybach, A.~Kannan, Y.~Wu, R.~Pang \emph{et~al.}, ``Streaming end-to-end
  speech recognition for mobile devices,'' in \emph{ICASSP}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2019, pp. 6381--6385.

\bibitem{li2019improving}
J.~Li, R.~Zhao, H.~Hu, and Y.~Gong, ``Improving rnn transducer modeling for
  end-to-end speech recognition,'' in \emph{ASRU}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2019, pp. 114--121.

\bibitem{kim2021improved}
S.~Kim, Y.~Shangguan, J.~Mahadeokar, A.~Bruguier, C.~Fuegen, M.~L. Seltzer, and
  D.~Le, ``Improved neural language model fusion for streaming recurrent neural
  network transducer,'' in \emph{ICASSP 2021-2021 IEEE International Conference
  on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2021, pp. 7333--7337.

\bibitem{see2017get}
A.~See, P.~J. Liu, and C.~D. Manning, ``Get to the point: Summarization with
  pointer-generator networks,'' in \emph{Proceedings of the 55th Annual Meeting
  of the Association for Computational Linguistics (Volume 1: Long Papers)},
  2017.

\bibitem{decoupled}
A.~Aghajanyan, J.~Maillard, A.~Shrivastava, K.~Diedrick, M.~Haeger, H.~Li,
  Y.~Mehdad, V.~Stoyanov, A.~Kumar, M.~Lewis, and S.~Gupta, ``{Conversational
  Semantic Parsing},'' in \emph{Proceedings of the Conference on Empirical
  Methods in Natural Language Processing and the International Joint Conference
  on Natural Language Processing (EMNLP-IJCNLP)}, 2020.

\bibitem{chen-2020-topv2}
X.~Chen, A.~Ghoshal, Y.~Mehdad, L.~Zettlemoyer, and S.~Gupta, ``{Low-Resource
  Domain Adaptation for Compositional Task-Oriented Semantic Parsing},'' in
  \emph{Proceedings of the Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2020.

\bibitem{gulati2020conformer}
A.~Gulati, J.~Qin, C.-C. Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang,
  Z.~Zhang, Y.~Wu \emph{et~al.}, ``Conformer: Convolution-augmented transformer
  for speech recognition,'' \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{Shi2022conformer}
Y.~Shi, C.~Wu, D.~Wang, A.~Xiao, J.~Mahadeokar, X.~Zhang, C.~Liu, K.~Li,
  Y.~Shangguan, V.~Nagaraja \emph{et~al.}, ``Streaming transformer transducer
  based speech recognition using non-causal convolution,'' \emph{Proc. ICASSP},
  2022.

\bibitem{kudo-richardson-2018-sentencepiece}
T.~Kudo and J.~Richardson, ``{{S}entence{P}iece: A simple and language
  independent subword tokenizer and detokenizer for Neural Text Processing},''
  in \emph{Proc. EMNLP: System Demonstrations}, 2018.

\bibitem{Mahadeokar2021AR-RNNT}
J.~Mahadeokar, Y.~Shangguan, D.~Le, G.~Keren, H.~Su, T.~Le, C.~Yeh, C.~Fuegen,
  and M.~L. Seltzer, ``{Alignment Restricted Streaming Recurrent Neural Network
  Transducer},'' in \emph{Proc. SLT}, 2021.

\bibitem{park2019specaugment}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le,
  ``Specaugment: A simple data augmentation method for automatic speech
  recognition,'' \emph{arXiv preprint arXiv:1904.08779}, 2019.

\bibitem{le21_interspeech}
D.~Le, M.~Jain, G.~Keren, S.~Kim, Y.~Shi, J.~Mahadeokar, J.~Chan, Y.~Shangguan,
  C.~Fuegen, O.~Kalinli, Y.~Saraf, and M.~L. Seltzer, ``{Contextualized
  Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and
  Shallow Fusion},'' in \emph{Proc. Interspeech}, 2021, pp. 1772--1776.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{NeurIPS}, 2017.

\bibitem{lin2017focal}
T.-Y. Lin, P.~Goyal, R.~Girshick, K.~He, and P.~Doll{\'a}r, ``Focal loss for
  dense object detection,'' in \emph{Proceedings of the IEEE international
  conference on computer vision}, 2017, pp. 2980--2988.

\bibitem{reddy2021interspeech}
C.~K. Reddy, H.~Dubey, K.~Koishida, A.~Nair, V.~Gopal, R.~Cutler, S.~Braun,
  H.~Gamper, R.~Aichner, and S.~Srinivasan, ``Interspeech 2021 deep noise
  suppression challenge,'' \emph{arXiv preprint arXiv:2101.01902}, 2021.

\end{thebibliography}
