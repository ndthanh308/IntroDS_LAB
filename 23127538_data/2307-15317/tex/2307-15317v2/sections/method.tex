\section{DiffKenall: Learning with Differentiable Kendall's Rank Correlation}
We have shown that the mere utilization of Kendall's rank correlation at test time yields a substantial enhancement in few-shot learning performance.
In this section, we investigate the potential for further improvements through the integration of Kendall's rank correlation into the meta learning process.
The main challenge is that the calculation of channel importance ranking is non-differentiable, which hinders the direct optimization of Kendall's rank correlation for training.
To tackle this problem, in our study, we propose a differentiable Kendall's rank correlation by approximating Kendall's rank correlation with smooth functions, hence enabling optimizing ranking consistency directly in episodic training.




\subsection{A Differentiable Approximation of Kendall's Rank Correlation}

Given two $n$-dimensional vectors $\bx = (x_1,...,x_n)$, $\by = (y_1,...,y_n)$, we define $\Tilde{\tau}_\alpha(\bx,\by)$ as:
\begin{align}
\Tilde{\tau}_\alpha(\bx,\by) =\frac{1}{N_0} \sum_{i=2}^{n}\sum_{j=1}^{i-1}\frac{e^{\alpha(x_i-x_j)}-e^{-\alpha(x_i-x_j)}}{e^{\alpha(x_i-x_j)}+e^{-\alpha(x_i-x_j)}} \frac{e^{\alpha(y_i-y_j)}-e^{-\alpha(y_i-y_j)}}{e^{\alpha(y_i-y_j)}+e^{-\alpha(y_i-y_j)}}, 
\label{dif_k}
\end{align}
where $\alpha > 0$ is a hyperparameter, and $N_0 =\frac{n(n-1)}{2} $ represents the total number of channel pairs.


\begin{restatable}{lemma}{mylemma}
$\Tilde{\tau}_\alpha(\bx,\by)$ is a differentiable approximation of Kendall's rank correlation $\tau(\bx, \by)$,
\begin{align*}
\tau (\bx, \by)=\lim_{\alpha\to +\infty }\Tilde{\tau}_\alpha(\bx,\by).
 \end{align*}
\end{restatable}
 
Please refer to the appendix for the proof.
The main idea of the proof involves using a sigmoid to approximate the $\operatorname{sgn}$ function in the $\operatorname{sgn}$ expression of Kendall's rank correlation where $\tau(\bx,\by)=\frac{2}{n(n-1)}\sum_{i<j}\operatorname{sgn}(x_i-x_j)\operatorname{sgn}(y_i-y_j)$.
 















\subsection{Integrating Differentiable Kendall's Rank Correlation with Meta-Baseline}
By approximating Kendall's rank correlation using Eq.~\eqref{dif_k}, we are able to directly optimize Kendall's rank correlation in training, addressing the issue of non-differentiability in rank computation. 
This implies that our method can integrate with numerous existing approaches that rely on geometric similarity, by \emph{replacing geometric similarity with differentiable Kendall's rank correlation} in episodic training, thereby achieving further improvements.

A straightforward application is the integration of our method with Meta-Baseline \cite{metabaseline}, which is a simple and widely-adopted baseline in few-shot learning. In Meta-Baseline, cosine similarity $\cos(\bx,\by)$ is employed as the similarity measure $\text{sim}(\cdot )$ in Eq.~\eqref{eq1}, to determine the semantic similarity between query samples' embedding and prototypes in episodic training and testing. 
Hence, we replace the cosine similarity originally utilized in Meta-Baseline with Kendall's rank correlation by employing differentiable Kendall's rank correlation during the meta-training phase and adopting Kendall's rank correlation during the testing phase.
The outline of calculating the episodic-training loss with differentiable Kendall's rank correlation $\Tilde{\tau}_a(\bx,\by)$ is demonstrated in Algorithm \ref{algor}.



\begin{algorithm}[t]
    \caption{Episodic training with differentiable Kendall's rank correlation $\Tilde{\tau}_a(\bx,\by)$}
    \label{algor}
    \LinesNumbered
    \KwIn{Base class dataset $\Dbase=\{(x_i,y_i)| i=1,...,N \}$.}
    \KwOut{The episodic-traing loss $\mathcal{L}$.}
    Randomly sample $n$ categories from base class dataset $\Dbase$\;

    Randomly sample $m_s$ samples in each category to build the support set $\mathcal{S}$\;
    Randomly sample $m_q$ samples in each category to build the query set $\mathcal{Q}$\;
    Compute class prototypes: $c_k=\frac{1}{|\mathcal{S}_k| } \sum\limits_{(x,y)\in \mathcal{S}_k } f_{\theta }(x)$. $\mathcal{S}_k$ denotes the subset of $\mathcal{S}$ where $y = k$\;  
    Compute the episodic-training loss: $\mathcal{L} = -\frac{1}{|\mathcal{Q} |} \sum\limits_{(x,y)\in Q}\log p(y|x)$. $p(y|x)$ is obtained by Eq. \eqref{eq1}, where $\Tilde{\tau}_a(\bx,\by)$ is used as the similarity measure $\text{sim}(\cdot )$.
\end{algorithm}




\subsection{Results}
\textbf{Settings.} We conduct extensive experiments on mini-ImageNet \cite{matching} and tiered-ImageNet \cite{tiered} for performance evaluation, both of which are widely used in previous studies. We use ResNet-12 as the backbone network and first pre-train the feature extractor on the base dataset.
$\alpha$ in Eq.~\eqref{dif_k} is set to 0.5 for the differentiable approximation of Kendall rank correlation.
The performance evaluation is conducted on randomly sampled tasks from the test set, where the average accuracy and the $95\%$ confidence interval are reported.

\begin{table}[t]
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{8pt}
\centering
\caption{Comparison studies on \textbf{mini-ImageNet} and \textbf{tiered-ImageNet}. The average accuracy ($\%$) with $95\%$ confidence interval of the 5-way 1-shot setting and the 5-way 5-shot setting is reported.}
\label{train compare mini}
\resizebox{0.85\linewidth}{!}{\begin{tabular}{c|l|c|cc}
\toprule
\textbf{Dataset} &\textbf{Method}              & \textbf{Backbone} & \textbf{5-way 1-shot} & \textbf{5-way 5-shot} \\ \midrule[\heavyrulewidth]
\multirow{16}*{\rotatebox{90}{mini-ImageNet}}&ProtoNet \cite{protonet}                    & Conv-4            & $49.42 \pm 0.78$               & $68.20 \pm  0.66$                 \\
&MatchingNet \cite{matching}                 & Conv-4            & $43.56 \pm 0.84$                & $55.31 \pm  0.73$                  \\
&MAML \cite{maml}                       & Conv-4            & $48.70 \pm  1.84$                 & $63.11 \pm  0.92$                 \\ 
&GCR \cite{gcr}                       & Conv-4            & $53.21 \pm 0.40$                 & $ 72.34 \pm 0.32$                 \\ 
&SNAIL \cite{snail}                        & ResNet-12         & $55.71 \pm  0.99$                  & $68.88 \pm  0.92$                 \\
&AdaResNet \cite{adaresnet}                   & ResNet-12         & $56.88 \pm  0.62$                 & $71.94 \pm  0.57$                 \\
&TADAM \cite{tadam}                       & ResNet-12         & $58.50 \pm 0.30$                 & $76.70 \pm 0.30$                 \\
&MTL \cite{mtl}                        & ResNet-12         & $61.20 \pm 1.80$                 & $75.50 \pm  0.80$                 \\
&MetaOptNet \cite{metaopt}                  & ResNet-12         & $62.64 \pm 0.61$                 & $78.63 \pm 0.46$                 \\
&TapNet \cite{tapnet}                  & ResNet-12         & $61.65 \pm 0.15$                 & $76.36 \pm 0.10$                 \\
&CAN \cite{can}                         & ResNet-12         & $63.85 \pm 0.48$                 & $79.44 \pm 0.34$                 \\
&ProtoNet + TRAML \cite{traml}            & ResNet-12         & $60.31 \pm 0.48$                 & $77.94 \pm 0.57$                 \\
&SLA-AG \cite{sla-ag}                         & ResNet-12         & $62.93 \pm 0.63$                 & $79.63 \pm 0.47$                 \\ 
&ConstellationNet \cite{constell}                         & ResNet-12        & $64.89 \pm 0.23$                 & $79.95 \pm 0.17$                 \\
\cmidrule{2-5}
&Meta-Baseline \cite{metabaseline}               & ResNet-12         & $63.17 \pm 0.23$                 & $79.26 \pm 0.17$                 \\
&Meta-Baseline + DiffKendall (Ours) & ResNet-12         & \textbf{65.56 $\pm$ 0.43}         & \textbf{80.79 $\pm$ 0.31}         \\ 
\midrule[\heavyrulewidth]

\multirow{7}*{\rotatebox{90}{tiered-ImageNet~}}
&ProtoNet \cite{protonet}                    & Conv-4            & $53.31 \pm 0.89$                 & $72.69 \pm 0.74$                \\
&Relation Networks \cite{relationnet}           & Conv-4            & $54.48 \pm 0.93$                  & $71.32 \pm 0.78$                  \\
&MAML \cite{maml}                         & Conv-4            & $51.67 \pm 1.81$                  & $70.30 \pm 1.75$                 \\
&MetaOptNet \cite{metaopt}                   & ResNet-12         & $65.99 \pm 0.72$         & $81.56 \pm 0.53$         \\
&CAN \cite{can}                          & ResNet-12         & $69.89 \pm 0.51$          & $84.23 \pm 0.37$          \\ 
\cmidrule{2-5}
&Meta-Baseline \cite{metabaseline}                & ResNet-12         & $68.62 \pm 0.27$                 & $83.74 \pm 0.18$                  \\
&Meta-Baseline + DiffKendall (Ours) & ResNet-12         & \textbf{70.76 $\pm$ 0.43} & \textbf{85.31 $\pm$ 0.34}        \\ 
\bottomrule
\end{tabular}}
\end{table}


\textbf{Comparison Studies.} Table \ref{train compare mini} shows the results of the comparison studies.
It can be seen that on both the datasets, compared with the original Meta-Baseline that uses cosine similarity in episodic training, we achieve a clear improvement by replacing cosine similarity with the proposed differentiable Kendall's rank correlation, with $2.39\%$, $2.16\%$ in the 1-shot and $1.53\%$, $1.57\%$ in the 5-shot, respectively. Moreover, our method also outperforms methods like CAN \cite{can} and ConstellationNet \cite{constell}, where cross-attention and self-attention blocks are used. It is worth noting that there are no additional architectures or learnable parameters introduced in our method, just like the original Meta-Baseline.


Furthermore, we also conduct a comprehensive comparison to demonstrate the role of incorporating differentiable Kendall's rank correlation during episodic training.
The results are presented in Figure \ref{fig:comparison}.
Compared with solely adopting Kendall's rank correlation at test time, it can be observed that leveraging the differentiable Kendall's rank correlation in episodic training leads to a $1\%$-$2\%$ improvement under 5-way 1-shot on test sets with varying domain discrepancies. This clearly demonstrates that the proposed differentiable Kendall's rank correlation can effectively serve as a soft approximation to directly optimize ranking consistency for further performance improvements in few-shot learning.










\subsection{Analysis} \label{analysis}
\textbf{Channel-Wise Ablation Studies: A closer look at the performance improvements.} We aim to carry out an in-depth analysis to uncover the underlying reasons behind the performance gains observed in few-shot learning upon utilizing Kendall's rank correlation. 
By determining semantic similarities between features with the correlation of channel importance ranking, we can effectively distinguish the role and contribution of small-valued channels that overwhelmingly occupy the feature space of novel samples for classification. 
As a result, these previously neglected small-valued channels can be fully harnessed to enhance classification performance. 
To validate this, we propose a channel-wise ablation study in which we test the performance of models on few-shot tasks using the small-valued and large-valued channels separately, allowing for a more detailed and nuanced understanding of their respective roles in classification. 
Concretely, given an $n$-dimensional feature $\bm{x} = (x_1,...,x_n)$, we define two types of masks $\bm{l} = (l_1,...,l_n)$, $\bm{h} = (h_1,...,h_n)$, as follows,
\begin{align*}
l_i=\begin{cases}
  0& \text{ if } x_i<L_0 \\
  1& \text{ else }
\end{cases}
\quad
h_i=\begin{cases}
  0& \text{ if } x_i>H_0 \\
  1& \text{ else }
\end{cases}
\end{align*}
The masked feature is then calculated as $\bar{\bm{x}} = \bm{x}\odot  \bm{l}$ or $\bar{\bm{x}} = \bm{x}\odot  \bm{h}$, where $\odot $ denotes Hadamard Product. 
We selectively preserve channels with large values in mask $\bm{l}$ while masking out small-valued channels. 
Conversely, we exclude channels with large values in mask $\bm{h}$ to exclusively evaluate the performance of small-valued channels on classification. 
Subsequently, we utilize the masked embedding to compare the performance of Kendall's rank correlation and cosine similarity in few-shot learning under various settings of threshold $L_0$ and $H_0$ with the corresponding results illustrated in Figure \ref{fig:channel-wise ablation}.





% Figure environment removed

% Figure environment removed










Figure \ref{fig:channel-wise ablation}(a) shows that when small-valued channels are masked during testing, both cosine similarity and Kendall's rank correlation achieve similar performance, but significant improvements are observed by utilizing differentiable Kendall's rank correlation for episodic training. 
As small-valued channels are gradually unmasked, Kendall's rank correlation significantly outperforms cosine similarity. 
This demonstrates that the improvement in performance achieved by utilizing Kendall's rank correlation is due to a more effective utilization of small-valued channels in novel sample features. 
This effect is further reflected in Figure \ref{fig:channel-wise ablation}(b), where masking only large-valued channels and utilizing only small-valued channels for classification results in a substantial improvement of approximately $9\%$ in performance using Kendall's rank correlation compared to cosine similarity.











\textbf{Calculating Kendall's Rank Correlation within Linear Time Complexity.} 
Kendall's rank correlation requires us to compute the importance ranking concordance of any pair of channels. 
This results in a higher time complexity compared to cosine similarity, increasing quadratically with the total number of channels. 
We investigate whether this time complexity could be further reduced to improve the computational efficiency of Kendall's rank correlation at test time. 
Specifically, we propose a simple approach to calculate the importance ranking concordance by randomly sampling a subset of channel pairs instead of using all channel pairs. 
The experimental results are presented in Figure \ref{fig:linear time}, where $n$ represents the total number of channels in the features of novel samples. 
It can be observed that by randomly sampling $5n$ channel pairs, we achieve a performance that is very close to using all channel pairs. 
It should be noted that this performance has already surpassed that of the original Meta-Baseline method while the time complexity is maintained linear, equivalent to cosine similarity. 

\textbf{Hyperparameter Sensitivity.} 
We also investigate the impact of the hyperparameter $\alpha$ in Eq. \eqref{dif_k}, and the experimental results are presented in Figure \ref{fig:ablation a}. 
The best results are obtained around a value of $0.5$, and the performance is found to be relatively insensitive to variations of $\alpha$ within a certain range. 
Setting a value for $\alpha$ that is too large or too small may lead to a decrease in performance. 
When a value for $\alpha$ is too large, the model may overfit to the base classes during episodic training, which can result in decreased generalization performance on novel classes. 
Conversely, if a value that is too small is used, this may lead to a poor approximation of Kendall's rank correlation.
\subsection{Visualization}
% Figure environment removed
% Figure environment removed
Further visual analysis is demonstrated in Figure~\ref{fig:visual1}. 
Specifically, we employ Kendall's rank correlation and cosine similarity to visualize the feature maps of the query samples. 
By computing the semantic similarity between the class prototype and each local feature of the query samples, the regions wherein the salient targets are located on the feature maps are illustrated. 
Hence, we can observe whether the significant features in the query samples are accurately detected. 
It is evident that the utilization of Kendall's rank correlation results in a more precise localization of the distinctive regions within the query sample. 

Furthermore, we conduct in-depth visual experiments involving channel ablation. 
We mask the channels with values greater than $H_0$ in the features of both the class prototype and query sample, just like the channel-wise ablation experiments in Section~\ref{analysis}.
The results are shown in Figure~\ref{fig:visual2}, from which we can observe that Kendall's rank correlation captures the discriminative features in the query sample when only utilizing the small-valued channels. 
In contrast, cosine similarity ignores these critical features, resulting in an inability to correctly locate salient regions when all channels are used. 
Therefore, we can infer that the small-valued channels that occupy the majority of the features indeed play a vital role in few-shot learning. 
This also explicitly demonstrates that the improvement achieved by Kendall's rank correlation in few-shot learning is essentially due to its ability to fully exploit the small-valued channels in features.
\section{Conclusion}
This paper exposes a key property of the features of novel samples in few-shot learning, resulting from the fact that values on most channels are small and closely distributed, making it arduous to distinguish their importance in classification. 
To overcome this, we propose to replace the commonly used geometric similarity metric with the correlation of the channel importance ranking to determine semantic similarities.
Our method can integrate
with numerous existing few-shot approaches without increasing training costs and has the potential to integrate with future
state-of-the-art methods that rely on geometric similarity metrics to achieve additional improvement.

























