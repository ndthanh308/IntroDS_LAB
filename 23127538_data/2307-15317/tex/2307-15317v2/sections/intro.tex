\section{Introduction}

Deep learning has achieved remarkable success in various domains. 
However, obtaining an adequate amount of labeled data is essential for attaining good performance.
In many real-world scenarios, obtaining sufficient labeled data can be exceedingly challenging and laborious.
This makes it a major bottleneck in applying deep learning models in real-world applications. 
In contrast, humans are able to quickly adapt to novel tasks 
by leveraging prior knowledge and experience, with only a very small number of samples. 
As a result, few-shot learning has received increasing attention recently.












The goal of few-shot learning is to adapt models trained on the base dataset to novel tasks where only a few labeled data are available. 
Previous works mainly focus on classification tasks and have been extensively devoted to metric-learning-based methods \cite{protonet,relationnet,can,metabaseline}.
In particular, they learn enhanced embeddings of novel samples by sampling tasks with a similar structure to the novel task from a sufficiently labeled base dataset for training. 
Geometric similarity metrics, such as negative Euclidean distance \cite{protonet} and cosine similarity \cite{s2m2,constell,prototypecompletion,metabaseline}, are commonly utilized to determine the semantic similarities between feature embeddings. 
Recent studies \cite{Chen,baseline20,simple_shot} also show that simply employing cosine similarity instead of inner product in linear classifier also yields competitive performance of novel tasks, when the model is pre-trained on the base dataset.





Since the categories of the base class and novel class are disjoint in few-shot learning, there exists a significant gap between training and inference for the model.
We found that compared to base classes, when the feature extractor faces a novel class that is unseen before, the feature channel values become more concentrated, i.e., for a novel class, most non-core features' channels have small and closely clustered values in the range [0.1, 0.3] (see Figure~\ref{fig:small value}(a)).
This phenomenon occurs because the model is trained on the base data, and consequently exhibits reduced variation of feature values when dealing with novel data. 
Empirically, we additionally compare the variance of feature channel values between the base dataset and various novel datasets (see Figure~\ref{fig:small value}(b)). The results reveal a significantly smaller variance in feature channel values in the novel datasets compared to the base dataset.
A smaller variance means values are closer to each other, demonstrating that this is a universally valid conclusion.
This situation creates a challenge in employing geometric similarity to accurately distinguish the importance among non-core feature channels.
To provide a concrete example, consider distinguishing between dogs and wolves. While they share nearly identical core visual features, minor features play a vital role in differentiating them. Suppose the core feature, two minor features are represented by channels 1, 2, and 3, respectively, in the feature vector. A dog prototype may have feature (1, 0.28, 0.2), and a wolf prototype may have feature (1, 0.25, 0.28). Now, for a test image with feature (0.8, 0.27, 0.22), it appears more dog-like, as the 2nd feature is more prominent than the 3rd. 
However, cosine distance struggles to distinguish them clearly, misleadingly placing this test image closer to the wolf prototype (distance=0.0031) rather than the dog prototype (distance=0.0034).
Contrastingly, the importance ranking of feature channels is able to distinguish dogs and wolves.
The test image shares the same channel ranking (1, 2, 3) as the dog prototype, whereas the wolf prototype's channel ranking is (1, 3, 2).  



















% Figure environment removed

Motivated by the above observations, we aim to boost the few-shot learning based on the importance ranking of feature channels in this paper.
Specifically, we propose a simple and effective method, 
which replaces the commonly-used geometric similarity metric (e.g., cosine similarity) with Kendall's rank correlation to determine how closely two feature embeddings are semantically related. 
We demonstrate that using Kendall’s rank correlation at test time can lead to performance improvements on a wide range of datasets with different domains. 
Furthermore, we investigate the potential benefits of employing Kendall’s rank correlation in episodic training. 
One main challenge is that the calculation of channel importance ranking is non-differentiable, which prevents us from directly optimizing Kendall’s rank correlation for training. To address this issue, we propose a smooth approximation of Kendall’s rank correlation and hence make it differentiable. We verify that using the proposed differentiable Kendall’s rank correlation at the meta-training stage instead of geometric similarity achieves further performance improvements.




In summary, our contributions to this paper are threefold as follows:
1) We reveal an intrinsic property of novel sample features in few-shot learning, whereby the vast majority of feature values are closely distributed across channels, leading to difficulty in distinguishing their importance;
2) We demonstrate that the importance ranking of feature channels can be used as a better indicator of semantic correlation in few-shot learning. By replacing the geometric similarity metric with Kendall's rank correlation at test time, significant improvements can be observed in multiple few-shot learning methods on a wide range of popular benchmarks with different domains;
3) We propose a differentiable loss function by approximating Kendall's rank correlation with a smooth version. This enables Kendall's rank correlation to be directly optimized. We verify that further improvements can be achieved by using the proposed differentiable Kendall’s rank correlation at the meta-training stage instead of geometric similarity.




































\section{Related Works}
\textbf{Meta-Learning-Based Few-Shot Learning.} Previous research on few-shot learning has been extensively devoted to meta-learning-based methods. They can be further divided into optimization-based methods \cite{maml,metaopt,leo} and metric learning-based methods \cite{protonet,tadam,relationnet,can,metabaseline}, with metric learning-based methods accounting for the majority of them. ProtoNets \cite{protonet} proposes to compute the Euclidean distance between the embedding of query samples and the prototypes on the support set for nearest-neighbor classification. Meta-baseline \cite{metabaseline} uses cosine similarity instead and proposes a two-stage training paradigm consisting of pre-training and episodic training, achieving competitive performance compared to state-of-the-art methods. Recent research \cite{knn,can} has begun to focus on aligning support and query samples by exploiting the local information on feature maps. \citet{knn} represent support samples with a set of local features and use a $k$-nearest-neighbor classifier to classify query samples. CAN \cite{can} uses cross-attention to compute the similarity score between the feature maps of support samples and query samples. ConstellationNet \cite{constell} proposes to learn enhanced local features by constellation models and then aggregate the features through self-attention blocks. Several studies \cite{bias1,prototypecompletion} investigate methods for calibrating novel sample features in few-shot learning. \citet{bias1} learn a transformation that moves the embedding of novel samples towards the class prototype by episodic training. \citet{prototypecompletion} propose to use additional attribute annotations that are generalizable across classes to complete the class prototypes. However, this requires additional labeling costs and the common properties will not exist when the categories of base class data and novel class data are significantly different. Unlike previous studies, we explore determining semantic similarities between novel sample features with the correlation of channel importance ranking in few-shot learning, which has never been studied before.


\textbf{Transfer-Learning-Based Few-Shot Learning.} Transfer-learning-based few-shot learning methods have recently received increasingly widespread attention. Prior research \cite{Chen,baseline20,simple_shot} has verified that competitive performance in few-shot learning can be achieved by pre-training models on the base dataset with cross-entropy loss and using cosine similarity for classification on novel tasks, without relying on elaborate meta-learning frameworks. Recent studies \cite{associate,distractoriccv,distractornips} also focus on introducing data from the pre-training phase to assist in test-time fine-tuning. For instance, \citet{associate} propose to introduce samples from the base dataset in fine-tuning that are similar to the novel samples in the feature space. POODLE \cite{distractornips}, on the other hand, uses the samples in the base dataset as negative samples and proposes to pull the embedding of novel samples away from them during fine-tuning.

















