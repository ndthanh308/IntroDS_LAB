\section{Learning with Differentiable Kendall's Rank Correlation}
We have shown that the mere utilization of Kendall's rank correlation at test time yields a substantial enhancement in few-shot learning performance.
In this section, we investigate the potential for additional enhancements through the integration of Kendall's rank correlation into the training process.
The main challenge is that the calculation of channel importance ranking is non-differentiable, which hinders the direct utilization of Kendall's rank correlation for training.
To tackle this problem, we propose a differentiable Kendall's rank correlation by approximating it with smooth functions in our study, hence enabling the optimization of Kendall's rank correlation. 

\begin{table}[t]
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{8pt}
\centering
\caption{Comparison studies on \textbf{mini-ImageNet}. The average accuracy ($\%$) with $95\%$ confidence interval of the 5-way 1-shot setting and the 5-way 5-shot setting is reported}
\label{train compare mini}
\begin{tabular}{c|c|cc}
\toprule
\textbf{Method}              & \textbf{backbone} & \textbf{5-way 1-shot} & \textbf{5-way 5-shot} \\ \midrule
ProtoNet \cite{protonet}                    & Conv-4            & $49.42 \pm 0.78$               & $68.20 \pm  0.66$                 \\
MatchingNet \cite{matching}                 & Conv-4            & $43.56 \pm 0.84$                & $55.31 \pm  0.73$                  \\
MAML \cite{maml}                       & Conv-4            & $48.70 \pm  1.84$                 & $63.11 \pm  0.92$                 \\ 
SNAIL \cite{snail}                        & ResNet-12         & $55.71 \pm  0.99$                  & $68.88 \pm  0.92$                 \\
AdaResNet \cite{adaresnet}                   & ResNet-12         & $56.88 \pm  0.62$                 & $71.94 \pm  0.57$                 \\
TADAM \cite{tadam}                       & ResNet-12         & $58.50 \pm 0.30$                 & $76.70 \pm 0.30$                 \\
MTL \cite{mtl}                        & ResNet-12         & $61.20 \pm 1.80$                 & $75.50 \pm  0.80$                 \\
MetaOptNet \cite{metaopt}                  & ResNet-12         & $62.64 \pm 0.61$                 & $78.63 \pm 0.46$                 \\
TapNet \cite{tapnet}                  & ResNet-12         & $61.65 \pm 0.15$                 & $76.36 \pm 0.10$                 \\
CAN \cite{can}                         & ResNet-12         & $63.85 \pm 0.48$                 & $79.44 \pm 0.34$                 \\
ProtoNet + TRAML \cite{traml}            & ResNet-12         & $60.31 \pm 0.48$                 & $77.94 \pm 0.57$                 \\
SLA-AG \cite{sla-ag}                         & ResNet-12         & $62.93 \pm 0.63$                 & $79.63 \pm 0.47$                 \\ 
ConstellationNet \cite{constell}                         & ResNet-12        & $64.89 \pm 0.23$                 & $79.95 \pm 0.17$                 \\
P-Transfer \cite{p-transfer}                         & ResNet-12        & $64.21 \pm 0.77$                 & $80.38 \pm 0.59$                 \\
\midrule
Meta-Baseline \cite{metabaseline}               & ResNet-12         & $63.17 \pm 0.23$                 & $79.26 \pm 0.17$                 \\
Meta-Baseline + Kendall (Ours, test only)              & ResNet-12         & $63.36 \pm 0.45$                 & $79.41 \pm 0.32$                 \\
Meta-Baseline + DiffKendall (Ours) & ResNet-12         & \textbf{65.56 $\pm$ 0.43}         & \textbf{80.79 $\pm$ 0.31}         \\ \bottomrule
\end{tabular}
\end{table}

\subsection{A Differentiable Approximation of Kendall's Rank Correlation}

Given two $n$-dimensional vectors $\bx = (x_1,...,x_n)$, $\by = (y_1,...,y_n)$, we define $\Tilde{\tau}_\alpha(\bx,\by)$ as:
\begin{align*}
\Tilde{\tau}_\alpha(\bx,\by) =\frac{1}{N_0} \sum_{i=2}^{n}\sum_{j=1}^{i-1}\frac{e^{\alpha(x_i-x_j)}-e^{-\alpha(x_i-x_j)}}{e^{\alpha(x_i-x_j)}+e^{-\alpha(x_i-x_j)}} \frac{e^{\alpha(y_i-y_j)}-e^{-\alpha(y_i-y_j)}}{e^{\alpha(y_i-y_j)}+e^{-\alpha(y_i-y_j)}},\label{dif_k}
\end{align*}
where $\alpha > 0$ is a hyperparameter, and $N_0 =\frac{n(n-1)}{2} $ represents the total number of channel pairs.


\begin{restatable}{lemma}{mylemma}
$\Tilde{\tau}_\alpha(\bx,\by)$ is a differentiable approximation of Kendall's rank correlation $\tau(\bx, \by)$,
\begin{align*}
\tau (\bx, \by)=\lim_{\alpha\to +\infty }\Tilde{\tau}_\alpha(\bx,\by).
 \end{align*}
\end{restatable}
 
Please refer to the appendix for the proof.
 










\begin{table}[]
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{8pt}
\centering
\caption{Comparison studies on \textbf{tiered-ImageNet}. The average accuracy ($\%$) with $95\%$ confidence interval of the 5-way 1-shot setting and the 5-way 5-shot setting is reported}
\label{train compare tiered}
\begin{tabular}{c|c|cc}
\toprule
\textbf{Method}              & \textbf{backbone} & \textbf{5-way 1-shot} & \textbf{5-way 5-shot} \\ \midrule
ProtoNet \cite{protonet}                    & Conv-4            & $53.31 \pm 0.89$                 & $72.69 \pm 0.74$                \\
Relation Networks \cite{relationnet}           & Conv-4            & $54.48 \pm 0.93$                  & $71.32 \pm 0.78$                  \\
MAML \cite{maml}                         & Conv-4            & $51.67 \pm 1.81$                  & $70.30 \pm 1.75$                 \\
MetaOptNet \cite{metaopt}                   & ResNet-12         & $65.99 \pm 0.72$         & $81.56 \pm 0.53$         \\
CAN \cite{can}                          & ResNet-12         & $69.89 \pm 0.51$          & $84.23 \pm 0.37$          \\ \midrule
Meta-Baseline \cite{metabaseline}                & ResNet-12         & $68.62 \pm 0.27$                 & $83.74 \pm 0.18$                  \\
Meta-Baseline + DiffKendall (Ours) & ResNet-12         & \textbf{70.76 $\pm$ 0.43} & \textbf{85.31 $\pm$ 0.34}        \\ \bottomrule
\end{tabular}
\end{table}




\subsection{Plugging-In Differentiable Kendall's Rank Correlation to Meta-Baseline}
We thus obtain a differentiable approximation of Kendall's rank correlation by Eq. \eqref{dif_k}. 
To validate its effectiveness, we propose to use $\Tilde{\tau}_\alpha(\bx,\by)$ in episodic training to directly optimize Kendall's rank correlation. 
Meta-Baseline \cite{metabaseline} is a representative method in few-shot learning, which proposes a simple and effective two-stage training paradigm. Specifically, the model is first pre-trained on the base dataset using cross-entropy loss as in conventional supervised learning. For the meta-training stage, it samples tasks from the base dataset, mimicking the construction of the test task in the form of $N$-way $K$-shot. 
The training goal is to accurately classify the query samples from the sampled tasks using cross-entropy loss as the loss function, where the probability is computed via Eq. \eqref{eq1}.
In Meta-Baseline, cosine similarity $\cos(\bx,\by)$ is employed as the similarity measure $\text{sim}(\cdot )$ in Eq. \eqref{eq1}, to determine the semantic similarity between query samples' embedding and prototypes for nearest-neighbor classification.
We hence replace the $\text{sim}(\cdot )$ in Eq. \eqref{eq1} from the cosine similarity $\cos(\bx,\by)$ to the proposed differentiable Kendall's rank correlation $\Tilde{\tau}_a(\bx,\by)$. 
The outline of calculating the episodic-training loss with differentiable Kendall's rank correlation $\Tilde{\tau}_a(\bx,\by)$ is demonstrated in Algorithm \ref{algor}.


\textbf{Implementation Details.} We conduct extensive experiments on mini-ImageNet \cite{matching} and tiered-ImageNet \cite{tiered} for performance evaluation, both of which are widely used in previous studies. We use ResNet-12 as the backbone network and first pre-train the feature extractor with cross-entropy loss, followed by episodic training using the proposed differentiable Kendall's rank correlation. We set $\alpha=0.5$ in Eq. \eqref{dif_k} and use SGD as the optimizer for episodic training, where the learning rate is set to $0.001$. In the testing phase, we use Kendall's rank correlation to compute the similarity between the embedding of query samples and class prototypes for nearest-neighbor classification. The performance evaluation is conducted on $2000$ randomly sampled tasks from the test set, where the average accuracy and the $95\%$ confidence interval are reported.

\begin{algorithm}[tbp]
    \caption{Episodic training with differentiable Kendall's rank correlation $\Tilde{\tau}_a(\bx,\by)$}
    \label{algor}
    \LinesNumbered
    \KwIn{Base class dataset $\Dbase=\{(x_i,y_i)| i=1,...,N \}$.}
    \KwOut{The episodic-traing loss $\mathcal{L}$.}
    Randomly sample $n$ categories from base class dataset $\Dbase$\;

    Randomly sample $m_s$ samples in each category to build the support set $\mathcal{S}$\;
    Randomly sample $m_q$ samples in each category to build the query set $\mathcal{Q}$\;
    Compute class prototypes: $c_k=\frac{1}{|\mathcal{S}_k| } \sum\limits_{(x,y)\in \mathcal{S}_k } f_{\theta }(x)$. $\mathcal{S}_k$ denotes the subset of $\mathcal{S}$ where $y = k$\;  
    Compute the episodic-training loss: $\mathcal{L} = -\frac{1}{|\mathcal{Q} |} \sum\limits_{(x,y)\in Q}\log p(y|x)$. $p(y|x)$ is obtained by Eq. \eqref{eq1}, where $\Tilde{\tau}_a(\bx,\by)$ is used as the similarity measure $\text{sim}(\cdot )$.




\end{algorithm}



\subsection{Comparison Studies}
We conduct extensive comparison studies to demonstrate the effectiveness of the differentiable Kendall's rank correlation. The results are shown in Table \ref{train compare mini} and Table \ref{train compare tiered}. It can be seen that on both the datasets, compared with the original Meta-Baseline that uses cosine similarity in episodic training, we achieve a clear improvement by replacing cosine similarity with the proposed differentiable Kendall's rank correlation, with $2.39\%$, $2.16\%$ in the 1-shot and $1.53\%$, $1.57\%$ in the 5-shot, respectively. Moreover, our method outperforms the state-of-the-art methods CAN \cite{can} and ConstellationNet \cite{constell}, where cross-attention and self-attention blocks are used. It is worth noting that there are no additional architectures or learnable parameters introduced in our method, just like the original Meta-Baseline.

Furthermore, we conduct a comprehensive comparison between our approach and the performance achieved by solely adopting Kendall's rank correlation at test time, without leveraging the differentiable Kendall's rank correlation in episodic training. The results are presented in Figure \ref{fig:comparison}, where it can be observed that leveraging the differentiable Kendall's rank correlation in episodic training leads to a $1\%$-$2\%$ improvement under 5-way 1-shot on test sets with varying domain discrepancies. This clearly demonstrates that the proposed differentiable Kendall's rank correlation can effectively serve as a soft approximation to directly optimize Kendall's rank correlation in few-shot learning for further performance improvements.


% Figure environment removed

% Figure environment removed







\subsection{Analysis}

\textbf{Channel-Wise Ablation Studies: A closer look at the performance improvements.} We aim to carry out an in-depth analysis to uncover the underlying reasons behind the performance gains observed in few-shot learning upon utilizing Kendall's rank correlation. 
By determining semantic similarities between features with the correlation of channel importance ranking, we can effectively distinguish the role and contribution of small-valued channels that overwhelmingly occupy the feature space of novel samples for classification. 
As a result, these previously neglected small-valued channels can be fully harnessed to enhance classification performance. 
To validate this, we propose a channel-wise ablation study in which we test the performance of models on few-shot tasks using the small-valued and large-valued channels separately, allowing for a more detailed and nuanced understanding of their respective roles in classification. 
Concretely, given an $n$-dimensional feature $\bm{x} = (x_1,...,x_n)$, we define two types of masks $\bm{l} = (l_1,...,l_n)$, $\bm{h} = (h_1,...,h_n)$, as follows,
\begin{align}
l_i=\begin{cases}
  0& \text{ if } x_i<L_0 \\
  1& \text{ else }
\end{cases}
\quad
h_i=\begin{cases}
  0& \text{ if } x_i>H_0 \\
  1& \text{ else }
\end{cases}
\end{align}
The masked feature is then calculated as $\bar{\bm{x}} = \bm{x}\odot  \bm{l}$ or $\bar{\bm{x}} = \bm{x}\odot  \bm{h}$, where $\odot $ denotes Hadamard Product. We selectively preserve channels with large values in mask $\bm{l}$ while masking out small-valued channels. Conversely, we exclude channels with large values in mask $\bm{h}$ to exclusively evaluate the performance of small-valued channels on classification. Subsequently, we utilize the masked embedding to compare the performance of Kendall's rank correlation and cosine similarity in few-shot learning under various settings of threshold $L_0$ and $H_0$ with the corresponding results illustrated in Figure \ref{fig:channel-wise ablation}.
















Figure \ref{fig:channel-wise ablation}(a) shows that when small-valued channels are masked during testing, both cosine similarity and Kendall's rank correlation achieve similar performance, but significant improvements are observed by utilizing differentiable Kendall's rank correlation for episodic training. As small-valued channels are gradually unmasked, Kendall's rank correlation significantly outperforms cosine similarity. This demonstrates that the improvement in performance achieved by utilizing Kendall's rank correlation is due to a more effective utilization of small-valued channels in novel sample features. This effect is further reflected in Figure \ref{fig:channel-wise ablation}(b), where masking only large-valued channels and utilizing only small-valued channels for classification results in a substantial improvement of approximately $9\%$ in performance using Kendall's rank correlation compared to cosine similarity.











\textbf{Calculating Kendall's Rank Correlation within Linear Time Complexity.} Kendall's rank correlation requires us to compute the importance ranking concordance of any pair of channels. This results in a higher time complexity compared to cosine similarity, increasing quadratically with the total number of channels. We investigated whether this time complexity could be further reduced to improve the computational efficiency of Kendall's rank correlation at test time. Specifically, we propose a simple approach to calculate the importance ranking concordance by randomly sampling a subset of channel pairs instead of using all channel pairs. The experimental results are presented in Figure \ref{fig:linear time}, where $n$ represents the total number of channels in the features of novel samples. It can be observed that by randomly sampling $5n$ channel pairs, we achieve a performance that is very close to using all channel pairs. In addition, it should be noted that this performance has already surpassed that of the original Meta-Baseline method, where the time complexity is linear, equivalent to cosine similarity. 

\textbf{Hyperparameter Sensitivity.} We also investigate the impact of the hyperparameter $\alpha$ in Eq. \eqref{dif_k}, and the experimental results are presented in Figure \ref{fig:ablation a}. The best results are obtained around a value of $0.5$, and the performance is found to be relatively insensitive to variations of $\alpha$ within a certain range. Setting a value for $\alpha$ that is too large or too small may lead to a decrease in performance. When a value for $\alpha$ is too large, the model may overfit to the base classes during episodic training, which can result in decreased generalization performance on novel classes. Conversely, if a value that is too small is used, this may lead to a poor approximation of Kendall's rank correlation.




\section{Conclusion}
This paper exposes a key property of the features of novel samples in few-shot learning, resulting from the fact that values on most channels are small and closely distributed, making it arduous to distinguish their importance in classification. 
To overcome this, we propose to replace the commonly-used geometric similarities with the correlation of the channel importance ranking to determine semantic similarities.
We verify that significant improvements can be observed in few-shot learning by simply using Kendall's rank correlation at test time.
Since channel importance ranking is non-differentiable, we also introduce a differentiable version of Kendall's rank correlation, enabling direct optimization during episodic training for further improvements. Our proposed method is validated through extensive experiments, confirming its effectiveness.























