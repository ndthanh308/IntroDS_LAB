
\section{Problem Definition}
Typically, a few-shot task $\mathcal{T}$ consists of a support set $\mathcal{S}$ and a query set $\mathcal{Q}$. 
The objective of the few-shot task is to accurately predict the category for each query sample $x_i \in \mathcal{Q}$ based on the support set $\mathcal{S}$.
The support set $\mathcal{S}$ is commonly organized as $N$-way $K$-shot, which means that there are a total of $N$ categories of samples included in this task, with each class containing $K$ annotated samples. 
In few-shot scenarios, $K$ is usually a very small number, indicating that the number of samples available for each category is extremely small. 

It is infeasible to train a feature extractor $f_{\theta}$ on a few-shot task directly from scratch. 
Thus, the feature extractor $f_{\theta}$ is usually trained on a base dataset $\Dbase$ with sufficient annotation to learn a priori.  
The training process typically involves pre-training and episodic training.
After that, few-shot tasks are sampled on the novel dataset $\Dnovel$ for performance evaluation. 
The categories of the samples in $\Dbase$ are entirely distinct from those in $\Dnovel$.
The trained feature extractor $f_{\theta}$ is then used to obtain the embedding of the samples for novel tasks, followed by calculating the similarity between the embedding of support samples and query samples for a nearest-neighbor classification, namely,
\begin{align}
P(y = k \mid \bx) = \frac{\exp( \text{sim}(f_{\theta}(\bx), \bm{c}_k)\cdot t  )}{\sum_{j=1}^{N} \exp( \text{sim}(f_{\theta}(\bx), \bm{c}_j)\cdot t  )}, \label{eq1}
\end{align}
where $\bx \in \mathcal{Q}$ denotes the query sample, $\bm{c}_k$ represents the class prototype, which is usually represented by the mean feature of the support set samples in each category, $\text{sim}(\cdot )$ denotes a similarity measure, $N$ denotes the total number of classes included in the few-shot task, and $t$ is used to perform a scaling transformation on the similarities.


In supervised learning, since there is no distribution gap between the training and test data, we can obtain high-quality embedding of test data by training the model with a large number of in-domain samples. However, in few-shot scenarios, the categories in the training data and the test data have no overlap. 
As there is no direct optimization towards the novel category, the features of novel samples in few-shot learning can be distinct from those learned under conventional supervised learning \cite{bias1,prototypecompletion,channel_imp}.


Geometric similarities are commonly used to determine semantic similarities between features in few-shot learning, among which cosine similarity is widely employed in recent studies \cite{Chen,constell,metabaseline}. 
Given two vectors with the same dimension $\bx = (x_1,...,x_n)$ and $\by = (y_1,...,y_n)$, denoting the features of novel samples, cosine similarity is calculated as $ \frac{1}{\| \bx\| \cdot \| \by \|}\sum_{i=1}^n x_iy_i$. 
It can be seen that the importance of each channel is directly correlated with the numerical value $x_i$ and $y_i$. However, as shown in Figure \ref{fig:small value} (a), the features of the novel class samples are largely different from those of the base class samples. Small values are observed on most channels and are very close to each other, making it difficult for cosine similarity to differentiate their importance in classification. On the other hand, there are also very few channels with large values. This means that only these large-valued channels will play a decisive role in the classification, while the small-valued channels, which occupy the majority of the features, will be underutilized. Although the embedding has already been projected onto the unit sphere in cosine similarity to reduce this effect, we verify that the role of the small-valued channels in classification is still largely underestimated.




\begin{table}[]
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{3pt}
\centering
\caption{Performance improvements by using Kendall's rank correlation at test time. The training set of mini-ImageNet is used as the base dataset and the average accuracy ($\%$) of 2000 randomly sampled 5-way 1-shot tasks on test sets with different domains is reported.}
\label{test compare}
\begin{tabular}{cc|cccccc}

\toprule
\multicolumn{1}{c|}{\textbf{Method}} & \textbf{Backbone} & \small \textbf{mini-test} & \small \textbf{CUB}   & \small \textbf{Traffic Signs} & \small \textbf{VGG Flowers} & \small \textbf{Quick Draw} & \small \textbf{Fungi} \\ \midrule
\multicolumn{1}{c|}{CE + cosine}       & Conv-4            & 48.57              & 36.97          & 38.89                  & 59.92                & 45.75     & 35.99         \\
\multicolumn{1}{c|}{CE + CIM}          & Conv-4            & 48.94              & 37.41          & \textbf{39.35}         & 59.79                & 45.56   &  35.89           \\
\multicolumn{1}{c|}{CE + Kendall}      & Conv-4            & \textbf{51.50}     & \textbf{39.01} & 39.04                  & \textbf{61.55}       & \textbf{46.00}  & \textbf{36.73}    \\ \midrule
\multicolumn{1}{c|}{CE + cosine}       & ResNet-12         & 62.2               & 45.12          & 55.54                  & 69.41                & 53.73     & 40.68          \\
\multicolumn{1}{c|}{CE + CIM}          & ResNet-12         & 60.4               & 45.20          & 56.64                  & 69.61                & 55.14     & 40.34          \\
\multicolumn{1}{c|}{CE + Kendall}      & ResNet-12         & \textbf{63.3}      & \textbf{47.07} & \textbf{60.84}         & \textbf{71.38}       & \textbf{55.99}   & \textbf{41.68}   \\ \midrule
\multicolumn{1}{c|}{Meta-B + cosine}       & ResNet-12         & 62.84              & 45.38          & 54.88                  & 69.14                & 53.27        & 40.57       \\
\multicolumn{1}{c|}{Meta-B + CIM}          & ResNet-12         & 61.60              & 45.24          & 55.31                  & 68.87                & 54.08    & 40.41           \\
\multicolumn{1}{c|}{Meta-B + Kendall}      & ResNet-12         & \textbf{63.36}     & \textbf{47.15} & \textbf{59.70}         & \textbf{70.57}       & \textbf{55.78}  &  \textbf{41.70}    \\ \midrule
\multicolumn{1}{c|}{CE + cosine}       & ResNet-18         & \textbf{62.92}     & 43.7           & 47.17                  & 62.35                & 52.33     & 38.89          \\
\multicolumn{1}{c|}{CE + CIM}          & ResNet-18         & 61.91              & 43.75          & 47.23                  & 61.89                & 52.21       & 39.07        \\
\multicolumn{1}{c|}{CE + Kendall}      & ResNet-18         & 62.83              & \textbf{45.54} & \textbf{54.32}         & \textbf{67.08}       & \textbf{54.15} & \textbf{39.64}     \\ \midrule
\multicolumn{1}{c|}{CE + cosine}       & WRN-28-10         & 60.08              & 43.64          & 47.01                  &  66.03                    & 47.99         & 39.27      \\
\multicolumn{1}{c|}{CE + CIM}          & WRN-28-10         & 59.34              & 43.43          &    46.30                   &                       64.42 & 48.37    & 39.42           \\
\multicolumn{1}{c|}{CE + Kendall}      & WRN-28-10         & \textbf{61.68}     & \textbf{45.80} & \textbf{51.39}         &  \textbf{69.72}                    & \textbf{53.52}  & \textbf{41.52}             \\ \midrule
\multicolumn{1}{c|}{S2M2 + cosine}     & WRN-28-10         & \textbf{64.52}     & 47.44          & 52.30                  & 68.93                & 51.41     & 41.76          \\
\multicolumn{1}{c|}{S2M2 + CIM}        & WRN-28-10         & 63.60              & 47.59          & 53.84                  & 70.91                & 53.89  & 42.54             \\
\multicolumn{1}{c|}{S2M2 + Kendall}    & WRN-28-10         & 63.97              & \textbf{47.74} & \textbf{57.88}         & \textbf{71.48}       & \textbf{54.63}  & \textbf{43.49}     \\ \midrule
\multicolumn{2}{c|}{Avg Improvements  (Kendall vs. cosine)  }                &     0.92 $\uparrow$                &      1.69 $\uparrow$          &         4.56 $\uparrow$               &       2.67 $\uparrow$              &  2.60 $\uparrow$ &  1.27 $\uparrow$                    \\ 
\multicolumn{2}{c|}{Avg Improvements (Kendall vs. CIM)  }                &     1.81 $\uparrow$               &       1.63 $\uparrow$         &         4.13 $\uparrow$               &       2.88  $\uparrow$             &  1.80 $\uparrow$ & 1.18 $\uparrow$                    \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Warm-Up: Using Kendall's Rank Correlation During Inference}


In this paper, we explore Kendall's Rank Correlation as a new similarity metric for few-shot learning. 
Specifically, we propose to represent channel importance in few-shot learning utilizing the ranking of numerical values. 
Converting numerical differences into ranking differences enables effective discrimination between small-valued channels that exhibit similar values, and reduces the large numerical differences between large-valued and small-valued channels.
Subsequently, ranking correlation can be employed to assess the semantic similarity between two features.
To implement the above idea, we start with investigating the use of Kendall's rank correlation during inference in this section, and address the training issue of Kendall's rank correlation in the following section.






\subsection{Kendall's Rank Correlation}





Given two $n$-dimensional feature vectors $\bx = (x_1,...,x_n)$, $\by = (y_1,...,y_n)$, Kendall's rank correlation is determined by measuring the consistency of pairwise rankings for every channel pair $(x_i, x_j)$ and $(y_i,y_j)$. This coefficient can be defined as the disparity between the number of channel pairs $(x_i, x_j)$ and $(y_i,y_j)$ that exhibit concordant ordering versus discordant ordering, namely,
\begin{align}
    \tau (\bx, \by) =\frac{N_{\text{con}}-N_{\text{dis}}}{N_{\text{total}}}, \label{eq2}
\end{align}
where $N_{\text{con}}$ represents the count of channel pairs with consistent importance ranking, i.e., either $(x_i>x_j) \wedge (y_i>y_j)$ or $(x_i<x_j) \wedge (y_i<y_j)$, $N_{\text{dis}}$ reflects the count of channel pairs with inconsistent ordering represented by either $(x_i>x_j) \wedge (y_i<y_j)$ or $(x_i<x_j) \wedge (y_i>y_j)$. $N_{\text{total}}$ represents the total number of channel pairs. 

% Figure environment removed

\subsection{Performace Improvements by Using Kendall's Rank Correlation at Test Time}

We conduct comprehensive experiments and verify that directly using Kendall's rank correlation at test time can significantly improve performance in few-shot learning.

Specifically, recent studies \cite{Chen,baseline20,simple_shot} have confirmed that pre-training the model on the base dataset with cross-entropy loss, and utilizing cosine similarity for classification on novel tasks, can yield competitive performance.
This approach has proven to outperform a number of meta-learning-based methods. Therefore, our initial comparison involves assessing the performance of cosine similarity and Kendall's rank correlation when the model is pre-trained with cross-entropy loss (CE) on the base dataset.
The evaluation is conducted on different backbone networks that are commonly used in previous studies, including Conv-4, ResNet-12, ResNet-18, and WRN-28-10. 
In addition, we also compare cosine similarity with Kendall's rank correlation based on Meta-Baseline (Meta-B) \cite{metabaseline}, a representative meta-learning based approach in few-shot learning, and a state-of-the-art method S2M2 \cite{s2m2}. 
We keep all other settings unchanged and replace the originally utilized cosine similarity with Kendall's rank correlation solely during the testing phase. 
Furthermore, we expand our comparison to include a recently proposed method (CIM) \cite{channel_imp}, which suggests a test-time transformation for feature calibration in few-shot learning.
The train set of mini-ImageNet \cite{matching} is used as the base dataset to train the model. 
Since the novel task confronted by the model can be arbitrary in real-world applications, we employ a wide range of datasets for testing that exhibit significant differences in both category and domain, including the test set of mini-ImageNet (mini-test), CUB \cite{cub}, Traffic Signs \cite{traffic_signs}, VGG Flowers \cite{vgg_flower}, Quick Draw \cite{quick_draw} and Fungi \cite{fungi}. 
In the testing phase, we randomly sample 5000 few-shot tasks from the test dataset in the form of 5-way 1-shot and report the average accuracy on all tasks for performance evaluation. 
The results are shown in Table \ref{test compare}. 
It can be seen that simply using Kendall's rank correlation at test time in few-shot learning achieves significant improvements compared with cosine similarity. It also vastly outperforms the test-time feature transformation method CIM.















Moreover, we conduct another in-depth study that clearly demonstrates the effectiveness of using Kendall's rank correlation in few-shot learning. Specifically, we pre-train the model on the base dataset from scratch using standard cross-entropy loss and make a comparison between the performance of Kendall's rank correlation and cosine similarity on few-shot tasks after each training epoch. The results are shown in Figure \ref{fig:train from scratch}. It can be seen that almost throughout the entire training process, Kendall's rank correlation achieves better performance compared to cosine similarity, demonstrating that the importance ranking of feature channels is more reliable by models than exact values in few-shot learning.















































