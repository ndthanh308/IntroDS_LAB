\section{Introduction}

Deep learning has achieved remarkable success in various domains. 
However, obtaining an adequate amount of labeled data is essential for attaining good performance.
In many real-world scenarios, obtaining sufficient labeled data can be exceedingly challenging and laborious.
This makes it a major bottleneck in applying deep learning models in real-world applications. 
In contrast, humans are able to quickly adapt to novel tasks 
by leveraging prior knowledge and experience, with only a very small number of samples. 
As a result, few-shot learning has received increasing attention recently.












The goal of few-shot learning is to adapt models trained on the base dataset to novel tasks where only a few labeled data are available. 
Previous works mainly focus on classification tasks and have been extensively devoted to metric-learning-based methods \cite{protonet,relationnet,can,metabaseline}.
In particular, they learn enhanced embeddings of novel samples by sampling tasks with a similar structure to the novel task from a sufficiently labeled base dataset for training. 
Geometric similarity metrics, such as negative Euclidean distance \cite{protonet} and cosine similarity \cite{s2m2,constell,prototypecompletion,metabaseline}, are commonly utilized to determine the semantic similarities between feature embeddings. 
Recent studies \cite{Chen,baseline20,simple_shot} also show that simply employing cosine similarity instead of inner product in linear classifier also yields competitive performance of novel tasks, when the model is pre-trained on the base dataset.





Since the categories of the base class and novel class are disjoint in few-shot learning, there exists a significant gap between training and inference for the model.
When we compare the values of different feature channels on the base dataset and novel dataset, we observe that the novel dataset has a much more uniform value distribution than the base dataset (Figure~\ref{fig:small value}(a)).
This means that a model trained on the base dataset has difficulty distinguishing the channel importance on the novel classes.
Under such circumstances, the importance ranking of feature channels is a more reliable indicator than geometric similarity  to determine how closely two novel samples are semantically related.
For example, dogs and wolves have almost identical core visual features,
and thus minor features are crucial to distinguish them.
It is known that dogs have broad heads whereas wolves have large heads.
In this case, broad head and large head are two minor features of them.
If the feature of broad head is more apparent than the feature of large head, then it is more likely to be a dog.
Since their core features are similar, cosine similarities between different kinds of dogs and between dogs and wolves should be very close, and hence hard to use for classification.
In contrast, the importance ranking of feature channels is able to distinguish dogs and wolves.
Figure~\ref{fig:small value}(b) shows a real example that two similar images (i.e., two dogs) have a lower cosine similarity than two dissimilar ones (i.e., dog and crab), while similar images have higher Kendall's rank correlations.



















Motivated by the above observations, we aim to boost the few-shot learning based on the importance ranking of feature channels in this paper.
Specifically, we propose a simple and effective method, 
which replaces the commonly-used geometric similarity metric (e.g., cosine similarity) with Kendall's rank correlation to determine how closely two feature embeddings are semantically related. 
We demonstrate that using Kendall’s rank correlation at test time can lead to performance improvements on a wide range of datasets with different domains. 
Furthermore, we investigate the potential benefits of employing Kendall’s rank correlation in episodic training. 
One main challenge is that the calculation of channel importance ranking is non-differentiable, which prevents us from directly optimizing Kendall’s rank correlation for training. To address this issue, we propose a smooth approximation of Kendall’s rank correlation and hence make it differentiable. We verify that using the proposed differentiable Kendall’s rank correlation at the meta-training stage instead of geometric similarity achieves further performance improvements.




In summary, our contributions to this paper are threefold as follows:
1) We reveal an intrinsic property of novel sample features in few-shot learning, whereby the vast majority of feature values are closely distributed across channels, leading to difficulty in distinguishing their importance;
2) We demonstrate that the importance ranking of feature channels can be used as a better indicator of semantic correlation in few-shot learning. By replacing the geometric similarity metric with Kendall's rank correlation at test time, significant improvements can be observed in multiple few-shot learning methods on a wide range of popular benchmarks with different domains;
3) We propose a differentiable loss function by approximating Kendall's rank correlation with a smooth version. This enables Kendall's rank correlation to be directly optimized. We verify that further improvements can be achieved by using the proposed differentiable Kendall’s rank correlation at the meta-training stage instead of geometric similarity.


























% Figure environment removed









\section{Related Works}
\textbf{Meta-Learning-Based Few-Shot Learning.} Previous research on few-shot learning has been extensively devoted to meta-learning-based methods. They can be further divided into optimization-based methods \cite{maml,metaopt,leo} and metric learning-based methods \cite{protonet,tadam,relationnet,can,metabaseline}, with metric learning-based methods accounting for the majority of them. ProtoNets \cite{protonet} proposes to compute the Euclidean distance between the embedding of query samples and the prototypes on the support set for nearest-neighbor classification. Meta-baseline \cite{metabaseline} uses cosine similarity instead and proposes a two-stage training paradigm consisting of pre-training and episodic training, achieving competitive performance compared to state-of-the-art methods. Recent research \cite{knn,can} has begun to focus on aligning support and query samples by exploiting the local information on feature maps. \citet{knn} represent support samples with a set of local features and use a $k$-nearest-neighbor classifier to classify query samples. CAN \cite{can} uses cross-attention to compute the similarity score between the feature maps of support samples and query samples. ConstellationNet \cite{constell} proposes to learn enhanced local features by constellation models and then aggregate the features through self-attention blocks. Several studies \cite{bias1,prototypecompletion} also investigate methods for calibrating novel sample features in few-shot learning. \citet{bias1} learn a transformation that moves the embedding of novel samples towards the class prototype by episodic training. \citet{prototypecompletion} propose to use additional attribute annotations that are generalizable across classes to complete the class prototypes. However, this requires additional labeling costs and the common properties will not exist when the categories of base class data and novel class data are significantly different. Unlike previous studies, we explore determining semantic similarities between novel sample features with the correlation of channel importance ranking in few-shot learning, which has never been studied before.


\textbf{Transfer-Learning-Based Few-Shot Learning.} Transfer-learning-based few-shot learning methods have recently received increasingly widespread attention. prior research \cite{Chen,baseline20,simple_shot} has verified that competitive performance in few-shot learning can be achieved by pre-training models on the base dataset with cross-entropy loss and using cosine similarity for classification on novel tasks, without relying on elaborate meta-learning frameworks. Recent studies \cite{associate,distractoriccv,distractornips} also focus on introducing data from the pre-training phase to assist in test-time fine-tuning. For instance, \citet{associate} propose to introduce samples from the base dataset in fine-tuning that are similar to the novel samples in the feature space. POODLE \cite{distractornips}, on the other hand, uses the samples in the base dataset as negative samples and proposes to pull the embedding of novel samples away from them during fine-tuning.

















