\section{Experiments}

For our experiments, we sought to closely emulate a real-world e-commerce setting, where the main source of training data is implicit user feedback from query logs, and models are evaluated under a realistic query popularity distribution. Unfortunately, most public search datasets do not reflect a realistic query distribution and rarely have implicit user feedback as training data. While recommendation system datasets have implicit user feedback, they do not have a text query that is usable by BM25 or dense retrieval, which retrieve based on query to listing text similarity. Therefore, we curated a training and evaluation dataset from our e-commerce platform.

\subsection{Dataset Creation}

For training data, we collected 365 days of implicit feedback data, comprising of records of queries and the product listings that were clicked, added to cart, or purchased from a given query. 
Queries are represented by their query text. Listings are represented by their unique ID and the title of the product. In addition, as mentioned in Section~\ref{section:shoptags}, listings are associated with seller-provided tags, and each listing belongs to exactly one seller's shop.

Over the time period used for this experiment there were 137,824,871 unique listings, 147,174,817 unique queries, 62,803,463 unique tag, and 3,018,713 unique shops. 
There were a total of 1,349,734,328 query-listing interactions recorded, where 3.46\% were purchases, 6.19\% were cart adds and 90.3\% were clicks. Altogether, there were 1,395,759,140 edges. 
Example records are found in Table \ref{sample_record}.

\begin{table*}[!h]
\begin{tabular}{ c c  c  c  c c } \hline 
   query  &  listing ID & listing title & interaction type & shop & tags  \\ \hline \hline
   wedding dress & l12 & beautiful bridal wedding gown & click & s00 & white, gown \\
   wedding gown & l12 & custom embroidered wedding dress & purchase & s00 & white, gown \\
   wedding dress & l34 & ethereal champagne dress with chiffon skirt & click & s11 & fancy, dress, chiffon \\
   \hline 
\end{tabular}
\caption{Hand-created example of implicit feedback training data.}
\label{sample_record}
\vspace{-2em}
\end{table*}

% Each interaction is a query, a listing, and an associated interaction.  All together this comprises a bipartite graph with four types of nodes: query, listing, shop and tag. Edges exist between listing node and a query node if an interaction occurred in the lookback period; edges exist between a listing node and a shop node if the listing is sold by the shop; edges exist between a listing node and a tag node if the listing’s seller described their listing using that tag. 

Evaluation data was curated to be a representative query distribution, sampled from a single day immediately following the last day of the training data window. We randomly sampled 11,521 queries that resulted in at least one purchase. As the sample is intended to be reflective of the true query popularity distribution, we did not de-duplicate the query set. Figure~\ref{querycount} shows the distribution of the query frequency in the evaluation set.

For each query, the listings that were purchased from that query are considered the relevant document. 82.3\% of queries had only one purchase, 12.0\% had two purchases and 5.6\% had more than two purchases. For each one of these queries, we assigned them to a head/torso/tail frequency bin based on how frequently they occurred in the previous 365 day period. The bins were created such that the total counts of requests are roughly equal among those bins. Of the evaluation queries, 31.0\% were in the head bin, 47.9\% were in the torso bin, and 43.9\% were in the tail bin.

% Figure environment removed

\subsection{Experiment set up}

We compare XWalk against two other methods of candidate retrieval. First is lexical retrieval using BM25 scoring (BM25). We use Pyserini \cite{Lin_etal_SIGIR2021_Pyserini} to build a Lucene index based on listing titles in our dataset and then retrieve candidates using BM25 rankings using bag-of-words representations. We used the default analyzer and default BM25 parameters (k1=1.2, b=0.75).

% The details regarding the neural baseline (NIR) were not explained clearly and in full detail. What was the technique, how much was the training data used? for how many epochs was it trained for? Without knowing these details, it's very hard to believe the claim that the XWalk technique performs better than NIR.

The second baseline is a state of the art neural dense retrieval system \cite{nigam2019} trained on search traffic for candidate retrieval (NIR). NIR uses a smaller time window of training data (30 days) due to the time and expense of training on larger data sets. NIR is a Transformer-based, two tower model that uses a multi-part hinge loss to distinguish between interactions that involve a purchase, cart add, favorite, click, or nothing. The model was trained over one epoch. %for YYY hours. 
It was designed for better semantic matching between queries and listings by incorporating title, query as well as additional features such as tags, and listing taxonomy.

In addition to the above, we also compare results against hybrid systems of NIR+BM25 \cite{chen_out--domain_2022}. 
To ensemble the results from each retrieval engine, we use Reciprocal Rank Fusion, a simple but effective fusion technique \cite{chen_out--domain_2022}.
%We emphasize now that our argument in this paper is to demonstrate that the marriage of pure lexical search with a non-semantic interaction based retrieval outperforms each individually, so we explore how the combination of our fast graph-based retrieval service, together with the benchmark lexical model produces high quality results. 
Higher recall in candidate retrieval result in higher overall search accuracy~\cite{chen_out--domain_2022}.
As we are focusing on the first-pass candidate retrieval stage of search, 
we use recall and mean average precision (MAP) at 100, and 1000 to measure the quality of candidates retrieved. %Note that our dataset defines previously purchased listings as ``relevant'', thus recall and MAP measure recall of previously purchased listings.

%In showing how these forms of recall can be combined, candidate sets are generated by stochastically combining the candidate sets from XWalk and from BM25. For demonstrative purposes, we computationally flip a biased coin (with heads probability equal to $\alpha$) to determine whether to draw Xwalk (heads) or BM25 (tails) for each position, with higher values of $\alpha$ leading to more weight on XWalk candidates and lower values of $\alpha$ leading to more weight on BM25 candidates.


\section{Analysis}

As shown in Table~\ref{tab:main}, when compared independently against other methods, XWalk out-performs other methods in most metrics despite the fact that it is unable to return results for novel queries, due to its strength in the head query bin. When combined with BM25, it outperforms in every metric, both NIR and the hybrid NIR+BM25. Finally, the ensemble of all three methods (XWalk+BM25+NIR) substantially outperforms all other configurations.

%The reason for the above is evident when examining queries segmented by their popularity bins in Table~\ref{bins_recall}.
We see in Table~\ref{tab:main} that BM25 is significantly weaker in performance compared to NIR and XWalk and does not always improve the overall results of NIR and XWalk, especially for MAP. While BM25 can improve recall by adding listings that were not retrieved by NIR and XWalk, its poor ranking drags down MAP in the hybrid systems. For the most popular short queries, BM25 is not able to distinguish between the many listings with titles that token match similarly to the query. Whereas methods like XWalk and NIR are able to provide a more reliable ranking of the highly purchaseable listings based on training data. 

However, in Table~\ref{bins_recall} we see that XWalk is complementary to BM25; XWalk is stronger in the head and torso bins while BM25 outperforms XWalk in the tail bin. This is due to the fact that XWalk suffers from cold start problems: it performs best with many prior examples and is unable to handle novel queries. BM25, as a lexical matching system, is more able to handle novel queries. Furthermore, XWalk+BM25 is still yet complementary with NIR. The semantic matching of dense retrieval excels in the tail, where queries are typically longer. When all three systems are ensembled, it is the highest performing across all query bins. XWalk's success in the head query bin is particularly notable -- in an e-commerce setting, the head query bin is responsible for a large majority of merchandise sales.

%Despite that XWalk is not able to 
%
%Unsurprisingly, using only a lexical model misses much of the meaningful information contained in historic user interactions. As a standalone model, lexical matching trails far behind the random walk based approach. Looking deeper, however, we find that combining XWalk with lexical matching exceeds XWalk’s performance on it’s own - specifically in the areas where it performs worst. Whereas XWalk has highest recall and MAP for the most popular queries (top.01 and top.1), for BM25 the highest metrics are for the least common queries (tail and no bin). This is evident in figures \ref{recall100} - \ref{map1000} where $\alpha=0$ is pure lexical matching and $\alpha=1$ is pure XWalk.

\begin{table}[!h]
\begin{tabular}{ l  c  c   c  c  }
 \hline
 & r@100 & r@1000 & M@100 & M@1000 \\
 \hline
 \hline
BM25 & 0.192 & 0.394  & 0.034 & 0.035 \\
NIR & 0.547 & 0.740  & 0.107  & 0.109 \\
XWalk & 0.600 & 0.723 & 0.153 & 0.154 \\
NIR+BM25 & 0.497 & 0.780  & 0.097 & 0.100 \\
XWalk+BM25 & 0.599 & 0.829 & 0.129 & 0.132  \\
% XWalk+NIR &  0.621 & 0.902 & 0.193 &  0.197 \\
XWalk+BM25+NIR & \textbf{0.701} & \textbf{0.915} & \textbf{0.194} & \textbf{0.198}  \\
 \hline
\end{tabular}
\caption{recall (r@100, r@1000) and MAP (M@100, M@1000) for retrieval models and combinations.}
\label{tab:main}
\end{table}
\vspace{-3em}
\begin{table}[!h]
\begin{tabular}{ l  c  c  c  }
\hline 
 & tail & torso & head \\
 \hline
 \hline
BM25 & 0.471 & 0.420 & 0.299 \\
NIR & 0.738 & 0.728 & 0.759 \\
XWalk & 0.260 & 0.813 & 0.899 \\
NIR+BM25 & 0.804 & 0.779 & 0.762 \\
XWalk+BM25 & 0.595 & 0.875 & 0.914 \\
% XWalk+NIR & 0.794 & 0.923 & 0.941 \\
XWalk+BM25+NIR & \textbf{0.836} & \textbf{0.931} & \textbf{0.942} \\
\hline
\end{tabular}
\caption{Comparison of retrieval models in terms of recall@1000 stratified by query popularity.}
\label{bins_recall}
\end{table}

\vspace{-2em}

% \begin{table}[!h]
% \begin{tabular}{ l  c  c  c  }
% \hline 
%  & tail & torso & head \\
%  \hline
%  \hline
% BM25 & 0.070  & 0.039 & 0.006 \\
% NIR & 0.135  & 0.122 & 0.072 \\
% XWalk & 0.062 & 0.181 & 0.174 \\
% NIR+BM25 & 0.146 & 0.114 & 0.049\\
% XWalk+BM25 & 0.107 & 0.150 & 0.115 \\
% % XWalk+NIR & 0.170 & \textbf{0.224} & \textbf{0.177}\\
% XWalk+BM25+NIR & \textbf{0.175} & \textbf{0.222} & \textbf{0.173} \\
% \hline
% \end{tabular}
% \label{bins_map}
% \caption{Comparison of retrieval models in terms of MAP@1000 stratified by query popularity}
% \end{table}

%The critical region of these plots is for high values of $\alpha$ where we find that we gain almost all the benefits of XWalk with a boost from lexical signals coming from BM25. Of specific note is the difference between high and low frequency queries. While for popular queries it is not possible to differentiate between pure XWalk and high values of $\alpha<1$, there is a sudden shift for the least common queries. This in turn leads to a jump in recall and MAP over all queries when $\alpha \approx 0.95$ versus when $\alpha=1$.


\section{Online Testing}

%In an online A/B experiment, visiting browsers are placed into a control or variant bucket and are given the experience for the entire duration of the online experiment. At the conclusion of the experiment, we compare the conversion rate of browsers (i.e. the percentage of browsers that made a purchase) placed in the control bucket vs. the variant bucket.

We tested XWalk in a live online A/B experiment on a large e-commerce platform. The experiment ran for 23 and 25 days on our mobile and web version of our platform, respectively. Our search system is a two-stage search system, which uses an ensemble of candidate retrievers in the first pass, followed by a second pass re-ranker.  In our A/B experiment, an ensemble of NIR+Solr as the candidate retrieval system was compared against an ensemble of XWalk+NIR+Solr.

We saw a statistically significant and substantial increase in conversion rate for the search system including XWalk in both the web and mobile platforms, $+1.2\%$ on web and $+1.98\%$ on mobile. In addition, in a production setting, we saw that XWalk was our lowest latency retrieval engine. The 99th percentile latency is only 58\% of the NIR engine and 22\% that of our Solr inverted index. 

%We ran several online tests on a large e-Commerce website across all queries to test our hypotheses regarding the contribution of interaction-based retrieval. We observed significantly positive increases in conversion rate, click-through rate, revenue, and many other secondary metrics. We also anecdotally found that search results from XWalk presented some clever examples of non-semantically similar relevant listings. For example, when users searched for a TV show, some results featured actors on the show or spinoff series.
%
%Some engineering effort was expended in developing a service for mixing retrieved candidates from different sources but we have reaped the dividends of this effort as we continue to explore new ways to combine complementary candidate retrieval services. One challenging regime has been learning how to apply XWalk to novel queries. Subsequent work improving our lexical retrieval service coupled with developments in neural IR have mitigated what might otherwise have been an insurmountable block in using a graph-based retrieval service. 
%
%Perhaps most importantly, we observed massive latency gains as projected offline. This is attributed to using a low-level language like Rust\cite{matsakis2014rust} for implementation. Specifically, we found that XWalk returned candidates on average >75\% more quickly than Apache SOLR. 

% % Figure environment removed

