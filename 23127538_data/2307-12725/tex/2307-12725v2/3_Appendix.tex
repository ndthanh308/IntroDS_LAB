\appendix
\begin{center}
        \LARGE \bf APPENDIX
    \end{center}

\section{Auxiliary Facts and Results}

    In this section we list auxiliary facts and results that we use several times in our~proofs.
    
    \subsection{Squared norm of the sum} For all $a_1,...,a_n \in \mathbb{R}^d$, where $n=\{2,3\}$
    \begin{equation}
        \label{eq:squared_norm_sum}
        \norms{a_1 + ... + a_n }^2 \leq n \norms{ a_1 }^2 + ... + n \norms{a_n}^2.
    \end{equation}
    

    \subsection{$L$ smoothness function}
        Function $f$ is called $L$-smooth on $\mathbb{R}^d$ with $L~>~0$ when it is differentiable and its gradient is $L$-Lipschitz continuous on $\mathbb{R}^d$, i.e.\ 
        \begin{equation}
            \norms{\nabla f(x) - \nabla f(y)} \leq L \norms{x - y},\quad \forall x,y\in \mathbb{R}^d. \label{eq:L_smoothness}
        \end{equation}
         It is well-known that $L$-smoothness implies (see e.g., \cite{Nesterov_2003})
        \begin{eqnarray*}
            f(y) \leq f(x) + \dotprod{\nabla f(x)}{y-x} + \frac{L}{2}\norms{y-x}^2\quad \forall x,y\in \mathbb{R}^d,
        \end{eqnarray*} 
        and if $f$ is additionally convex, then
        \begin{eqnarray*}
            \norms{ \nabla f(x) - \nabla f(y) }^2 \leq 2L \left( f(x) - f(y) - \dotprod{ \nabla f(y)}{x-y} \right) \quad \forall x,y \in \mathbb{R}^d. 
        \end{eqnarray*}

    \subsection{Wirtinger-Poincare inequality}
        Let $f$ is differentiable, then for all $x \in \mathbb{R}^d$, $\tau e \in S^d_2(\tau)$:
        \begin{equation}\label{eq:Wirtinger_Poincare}
            \expect{f(x+ \tau e)^2} \leq \frac{\tau^2}{d} \expect{\norms{\nabla f(x + \tau e)}^2}.
        \end{equation}
        

    
    % \subsection{Fact from concentration of the measure}
    % Let $\ee$ is uniformly distributed on the Euclidean unit sphere, then, for $d \geq 8$, $\forall s \in \mathbb{R}^d$
    % \begin{equation}
    %     \label{Concentration_measure}
    %     \mathbb{E}_\ee \left( \dotprod{s}{\ee}^2\right) \leq \frac{\| s \|_2^2}{d}. 
    % \end{equation}
    \newpage
\section{Proof Theorem \ref{th:biased_AC_SA}} \label{Appendix:proof_th1}
    In this section, our reasoning will be based on the proof from \cite{Woodworth_2021_over}. Initially, let us formally define a batched biased gradient oracle (see Definition \ref{def:biased_oracle}):
    \begin{equation}
        \label{eq:batched_gradient_oracle}
        \gg^B(x) := \frac{1}{B} \sum_{i=1}^{B} \gg(x, \xi_i), \quad \text{for i.i.d. } \xi_1, \xi_2, ..., \xi_B \sim \mathcal{D}. 
    \end{equation}
    Then Algorithm \ref{alg:AC_SA} presents a Biased Accelerated Mini-batch Stochastic Gradient Descent (Biased AC-SA method) under the overparameterization condition. 
    \begin{algorithm}
        \caption{Biased AC-SA}\label{alg:AC_SA}
        \textbf{Input}: Start point $x^{ag}_0 = x_0 \in \mathbb{R}^d$, maximum number of iterations $N \in \mathbb{Z}_+$.\\
        \hspace*{\algorithmicindent} Let stepsize $ \al{\gamma}_k > 0$, parameters $\beta_k, \gamma > 0$, batch size $B \in \mathbb{Z}_+$. 
        \begin{algorithmic}[1]
        \For{$k=0,...,N-1$}
        \State $\beta_k = 1 + \frac{k}{6}$ and $\al{\gamma}_k = \al{\gamma} (k+1)$ for $\gamma = \min \left\{ \frac{1}{12 L}, \frac{B}{24 L(N+1)}, \sqrt{\frac{B R^2}{Lf^* N^3}} \right\}$  
        \State $x^{md}_k = \beta^{-1}_k x_k + (1 - \beta^{-1}_k) x_k^{ag}$
        \State $\tilde{x}_{k+1} = x_k - \al{\gamma}_k \gg_k^B(x_k^{md})$, where $\gg_k^B(x_k^{md})$ is defined from \eqref{eq:batched_gradient_oracle}
        \State $x_{k+1} = \min \left\{ 1, \frac{R}{\norms{\tilde{x}_{k+1}}} \right\} \tilde{x}_{k+1}$
        \State $x_{k+1}^{ag} = \beta^{-1}_k x_{k+1} + (1 - \beta^{-1}_k) x_{k}^{ag}$
        \EndFor
        \end{algorithmic}
        \textbf{Output}: $x_N^{ag}$.
    \end{algorithm}
    
    Then it is not hard to show that the following Lemma is also correct for the batched biased gradient oracle \eqref{eq:batched_gradient_oracle}. Therefore, to avoid repetition, we formulate this lemma without proof, by referring to the original proof. 
    \begin{lemma}[see Lemma 1, \cite{Woodworth_2021_over}]\label{lemma:lem1}
    Let $x_{k+1}, x_k$ and $x_k^{md}$ be updated as in Algorithm \ref{alg:AC_SA}. Then for any $x \in \left\{ x: \norms{x}\leq R \right\}$
        \begin{eqnarray*}
            \gamma_k \dotprod{\gg^B(x^{md}_k)}{x_{k+1} - x_{k}^{md}} &\leq& \gamma_k \dotprod{\gg^B(x^{md}_k)}{x - x_{k}^{md}} + \frac{1}{2} \norms{x - x_k}^2 \\
            && \quad - \frac{1}{2} \norms{x - x_{k+1}}^2 - \frac{1}{2} \norms{x_{k+1} - x_k}^2.
        \end{eqnarray*} 
    \end{lemma}
    Next, we provide some auxiliary lemma before presenting proof of Theorem~\ref{th:biased_AC_SA}.
    \begin{lemma}\label{lemma:lem2}
        Let function $f(x,\xi)$ satisfy Assumptions \ref{ass:convex}-\ref{ass:smooth} and the gradient oracle (see Definition \ref{def:biased_oracle}) satisfy Assumption \ref{ass:stoch_noise}, and let $\gg^B(x^{md}_k)$ be defined in \eqref{eq:batched_gradient_oracle}. Then
        \begin{eqnarray*}
            \expect{\norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}^2} \leq \frac{8 L^2 R^2}{B \beta_k^2} + \frac{8 L}{B} \expect{f(x_k^{ag}) - f^*} + \frac{4 \sigma^2_{*}}{B} + \norms{\bb(x_k^{md})}^2
        \end{eqnarray*}
    \end{lemma}
    \begin{proof}
        \begin{align}
            \mathbb{E}\|&\gg^B(x_k^{md}) - \nabla f(x_k^{md})\|^2 \nonumber \\
            &= \expect{\norms{\frac{1}{B} \sum_{i=1}^{B} \nabla f(x^{md}_k, \xi_i) - \nabla f(x_k^{md})}^2} 
            \nonumber \\ 
            & \quad \quad + \expect{\norms{\gg^B(x_k^{md}) - \frac{1}{B} \sum_{i=1}^{B} \nabla f(x^{md}_k, \xi_i)}^2}
            \nonumber \\ 
            &\overset{\eqref{eq:batched_gradient_oracle}}{=} \expect{\norms{\frac{1}{B} \sum_{i=1}^{B} \nabla f(x^{md}_k, \xi_i) - \nabla f(x_k^{md})}^2}  + \expect{\norms{\frac{1}{B} \sum_{i=1}^{B} \bb(x_k^{md})}^2}
            \nonumber \\ 
            &= \frac{1}{B^2} \sum_{i=1}^{B} \expect{\norms{ \nabla f(x^{md}_k, \xi_i) - \nabla f(x_k^{md})}^2} + \norms{\bb(x_k^{md})}^2
            \nonumber \\ 
            &\leq \frac{1}{B} \expect{\norms{\nabla f(x_k^{md}, \xi_1)}^2} + \norms{\bb(x_k^{md})}^2
            \nonumber \\ 
            &\overset{\eqref{eq:squared_norm_sum}}{\leq} \frac{2}{B} \expect{\norms{\nabla f(x_k^{md}, \xi_1) - \nabla f(x_k^{ag}, \xi_1)}^2}  + \frac{2}{B} \expect{\norms{\nabla f(x_k^{ag}, \xi_1)}^2} + \norms{\bb(x_k^{md})}^2
            \nonumber \\ 
            & \overset{\eqref{eq:L_smoothness}}{\leq} \frac{2L^2}{B} \expect{\norms{x_k^{md} - x_k^{ag}}^2} + \frac{4}{B} \expect{\norms{\nabla f(x_k^{ag}, \xi_1) - \nabla f(x^*, \xi_1)}^2}
            \nonumber \\ 
            & \quad \quad + \frac{4}{B} \expect{\norms{\nabla f(x^*, \xi_1)}^2} + \norms{\bb(x_k^{md})}^2.
            \label{eq:lemma2}
        \end{align}
        For the first term on the right hand side:
        \begin{equation}
            \label{eq:lemma2_1}
            x^{md}_k = \beta^{-1}_k x_k + (1 - \beta^{-1}_k) x^{ag}_k  \Rightarrow  \norms{x_k^{md} - x^{ag}_k} = \beta_k^{-1} \norms{x_k - x_k^{ag}} \leq 2 R \beta^{-1}_k.
        \end{equation}
        For second term, we apply Theorem 2.1.5 from \cite{Nesterov_2003}:
        \begin{align}
            \mathbb{E}\|\nabla f(x_k^{ag}, \xi_1) &- \nabla f(x^*, \xi_1) \|^2
            \nonumber \\
            & \leq 2L \expect{f(x_k^{ag}, \xi_1) - f(x^*, \xi_1) - \dotprod{\nabla f(x^*, \xi_1)}{x_k^{ag} - x^*}}
            \nonumber \\
            & = 2 L \expect{f(x^{ag}_k) - f^*}. \label{eq:lemma2_2}
        \end{align}
        For third term, we apply Assumption \ref{ass:stoch_noise}:
        \begin{equation}\label{eq:lemma2_3}
            \expect{\norms{\nabla f(x^*, \xi_1)}^2} = \expect{\norms{\nabla f(x^*, \xi_1) - \nabla f(x^*)}^2} \leq \sigma_*^2.
        \end{equation}
        Substituting \eqref{eq:lemma2_1}-\eqref{eq:lemma2_3} into \eqref{eq:lemma2} we complete the proof of the Lemma. 
        \\ \qed
    \end{proof}
    We can now proceed to prove the main theorem of Section \ref{sec:biased_gradient}.\\
    \begin{flushleft}
        \textit{ Proof of the Theorem \ref{th:biased_AC_SA}}. 
    \end{flushleft}
    Using the convexity and $L$-smoothness of the function $f$ we can obtain the following upper bound:
    \begin{align}
        \beta_k \gamma_k f(x_{k+1}^{ag}) &\leq \beta_{k} \gamma_{k} \left[ f(x_k^{md}) + \dotprod{\nabla f(x_{k}^{md})}{x_{k+1}^{ag} - x_{k}^{md}} + \frac{L}{2} \norms{x_{k+1}^{ag} - x_{k}^{md}}^2 \right]
        \nonumber \\
        & = \beta_{k} \gamma_{k} \left[ f(x_k^{md}) + \dotprod{\nabla f(x_{k}^{md})}{x_{k+1}^{ag} - x_{k}^{md}}\right] + \frac{L \gamma_k}{2 \beta_k} \norms{x_{k+1} - x_{k}}^2 
        \nonumber \\
        & = \beta_{k} \gamma_{k} \left[ f(x_k^{md}) + \dotprod{\nabla f(x_{k}^{md})}{\beta^{-1}_k x_{k+1} + (1-\beta^{-1})x^{ag}_k - x_{k}^{md}}\right] 
        \nonumber \\
        & \quad \quad + \frac{L \gamma_k}{2 \beta_k} \norms{x_{k+1} - x_{k}}^2 
        \nonumber \\
        & = (\beta_{k} - 1) \gamma_{k} \left[ f(x_k^{md}) + \dotprod{\nabla f(x_{k}^{md})}{x_k^{ag} - x_{k}^{md}}\right] 
        \nonumber \\
        & \quad \quad + \gamma_{k} \left[ f(x_k^{md}) + \dotprod{\nabla f(x_{k}^{md})}{x_{k+1} - x_{k}^{md}}\right]  + \frac{L \gamma_k}{2 \beta_k} \norms{x_{k+1} - x_{k}}^2 
        \nonumber \\
        & \leq (\beta_k - 1) \gamma_k f(x_k^{ag}) + \gamma_k \left[ f(x_k^{md}) + \dotprod{\gg^B(x_k^{md})}{x_{k+1} - x_k^{md}} \right] 
        \nonumber \\
        & \quad \quad - \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x_{k+1} - x_{k}^{md}} + \frac{L \gamma_k}{2 \beta_k} \norms{x_{k+1} - x_{k}}^2
        \nonumber
    \end{align}
    Using Lemma \ref{lemma:lem1} with $x = x^* \in \argmin_{x: \norms{x} \leq R} f(x)$ for second term we obtain:
    \begin{align}
        \gamma_k  f(x_k^{md}) &+ \gamma_k  \dotprod{\gg^B(x_k^{md})}{x_{k+1} - x_k^{md}}
        \nonumber \\
        & = \gamma_k  f(x_k^{md}) + \gamma_k  \dotprod{\gg^B(x_k^{md})}{x^* - x_k^{md}}
        \nonumber \\
        & \quad \quad + \frac{1}{2} \norms{x^*-x_k}^2 - \frac{1}{2} \norms{x^*-x_{k+1}}^2 - \frac{1}{2} \norms{x_{k+1}-x_k}^2
        \nonumber \\
        & = \gamma_k  f(x_k^{md}) + \gamma_k  \dotprod{\nabla f(x_k^{md})}{x^* - x_k^{md}} + \gamma_k  \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_k^{md}}
         \nonumber \\
        & \quad \quad + \frac{1}{2} \norms{x^*-x_k}^2 - \frac{1}{2} \norms{x^*-x_{k+1}}^2 - \frac{1}{2} \norms{x_{k+1}-x_k}^2
        \nonumber \\
        & \leq \gamma_k f^* + \gamma_k  \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_k^{md}}
        \nonumber \\
        & \quad \quad + \frac{1}{2} \norms{x^*-x_k}^2 - \frac{1}{2} \norms{x^*-x_{k+1}}^2 - \frac{1}{2} \norms{x_{k+1}-x_k}^2.
        \nonumber
    \end{align}
    Substituting the obtained upper bound we obtain:
\allowdisplaybreaks
    \begin{align}
        \beta_k \gamma_k f(x_{k+1}^{ag}) &\leq (\beta_k - 1) \gamma_k f(x_k^{ag}) + \gamma_k f^* + \frac{L \gamma_k}{2 \beta_k} \norms{x_{k+1} - x_{k}}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_{k+1}}
        \nonumber \\
        & \quad \quad + \frac{1}{2} \left( \norms{x^*-x_k}^2 - \norms{x^*-x_{k+1}}^2 - \norms{x_{k+1}-x_k}^2 \right) .
        \nonumber
    \end{align}
    Adding $\beta_k \gamma_k f^*$ to both sides we can obtain:
    \begin{align}
        \beta_k \gamma_k \left[ f(x_{k+1}^{ag}) - f^* \right] &\leq (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_{k+1}}
        \nonumber \\
        & \quad \quad + \frac{L \gamma_k - \beta_k}{2 \beta_k} \norms{x_{k} - x_{k+1}}^2
        \nonumber \\
        & = (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x_{k} - x_{k+1}}
        \nonumber \\
        & \quad \quad + \frac{L \gamma_k - \beta_k}{2 \beta_k} \norms{x_{k} - x_{k+1}}^2 
        \nonumber \\
        & \leq (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \gamma_k \norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})} \norms{x_{k} - x_{k+1}}
        \nonumber \\
        & \quad \quad + \frac{L \gamma_k - \beta_k}{2 \beta_k} \norms{x_{k} - x_{k+1}}^2. 
        \nonumber
    \end{align}
    Since $\beta_k = 1 + \frac{k}{6} > \frac{1+k}{6} \geq 2L\gamma_k$, then 
    \begin{equation*}
        \gamma_k \norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})} \norms{x_{k} - x_{k+1}} + \frac{L \gamma_k - \beta_k}{2 \beta_k} \norms{x_{k} - x_{k+1}}^2
    \end{equation*} 
    is a quadratic polynomial of the form: $-\frac{a}{2}y^2 + by$ (where $y = \norms{x_k - x_{k+1}}$), which can be upper bounded by $-\frac{a}{2}y^2 + b y\leq \max_{y} \left\{ -\frac{a}{2}y^2 + b y \right\} = \frac{b^2}{2a}$. We get
    \begin{align}
        \beta_k \gamma_k \left[ f(x_{k+1}^{ag}) - f^* \right] &\leq (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \frac{\beta_k \gamma_k^2}{2 (\beta_k - L \gamma_k)} \norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}^2
        \nonumber \\ 
        & \leq (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \gamma_k^2 \norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}^2
        \nonumber \\
         & \leq (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\frac{1}{B}\sum_{i=1}^{B} \nabla f(x_{k}^{md}, \xi_i) - \nabla f(x_k^{md})}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\gg^B(x_k^{md}) - \frac{1}{B}\sum_{i=1}^{B} \nabla f(x_{k}^{md}, \xi_i)}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \gamma_k^2 \norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}^2
        \nonumber \\
        & \leq (\beta_k - 1) \gamma_k \left[ f(x_k^{ag}) - f^* \right] +\frac{1}{2} \norms{x_k - x^*}^2 - \frac{1}{2} \norms{x_{k+1} - x^*}^2
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\frac{1}{B}\sum_{i=1}^{B} \nabla f(x_{k}^{md}, \xi_i) - \nabla f(x_k^{md})}{x^* - x_{k}}
        \nonumber \\
        & \quad \quad + \gamma_k^2 \norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}^2 + \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}}.
        \nonumber
    \end{align}
    Taking the expectation of both sides we have:
    \begin{align}
        \beta_k \gamma_k  \expect{f(x_{k+1}^{ag}) - f^*} & \leq (\beta_k - 1) \gamma_k \expect{ f(x_k^{ag}) - f^*} +\frac{1}{2} \expect{\norms{x_k - x^*}^2} 
        \nonumber \\
        & \quad \quad - \frac{1}{2} \expect{\norms{x_{k+1} - x^*}^2} + \gamma_k^2 \expect{\norms{\gg^B(x_k^{md}) - \nabla f(x_k^{md})}^2} 
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}}.
        \nonumber
    \end{align}
    Using the Lemma \ref{lemma:lem2} we obtain:
    \begin{align}
        \beta_k \gamma_k  \expect{f(x_{k+1}^{ag}) - f^*} & \leq (\beta_k - 1) \gamma_k \expect{ f(x_k^{ag}) - f^*} +\frac{1}{2} \expect{\norms{x_k - x^*}^2} 
        \nonumber \\
        & \quad \quad - \frac{1}{2} \expect{\norms{x_{k+1} - x^*}^2} +  \frac{8 L^2 R^2 \gamma_k^2}{B \beta_k^2}
        \nonumber \\
        & \quad \quad  + \frac{8 L \gamma_k^2}{B} \expect{f(x_k^{ag}) - f^*} + \frac{4 \sigma^2_{*} \gamma_k^2}{B} 
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}} + \gamma_k^2 \norms{\bb(x_k^{md})}^2
        \nonumber \\
        & \leq \left(\beta_k - 1 + \frac{8 L \gamma_k}{B} \right) \gamma_k \expect{ f(x_k^{ag}) - f^*} 
        \nonumber \\
        & \quad \quad +\frac{1}{2} \expect{\norms{x_k - x^*}^2}  - \frac{1}{2} \expect{\norms{x_{k+1} - x^*}^2} 
        \nonumber \\
        & \quad \quad + \frac{8 L^2 R^2 \gamma_k^2}{B \beta_k^2} + \frac{4 \sigma^2_{*} \gamma_k^2}{B} 
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}} + \gamma_k^2 \norms{\bb(x_k^{md})}^2.
        \nonumber
    \end{align}
    We now remind that
    \begin{align}
        \beta_k &= 1 + \frac{k}{6};
        \nonumber \\
        \gamma_k &= \gamma(k+1);
        \nonumber \\
        \gamma &\leq \min\left\{ \frac{1}{12 L}, \frac{B}{24 L (N+1)} \right\} .
        \nonumber
    \end{align}
    This ensure that $\forall k:$ $\beta_k \geq 1$ and $2L \gamma_k \leq \beta_k$. Moreover, for $k \in [0; N-1]$:
    \begin{align}
        \left(\beta_{k+1} - 1 + \frac{8 L \gamma_{k+1}}{B} \right) &\gamma_{k+1} - \beta_k \gamma_k
        \nonumber \\
        & = \left( \beta_k - \frac{5}{6} + \frac{8 L \gamma_{k+1}}{B} \right) \gamma(k+2) - \beta_k \gamma(k+1)
        \nonumber \\
        & = \gamma \left( 1 + \frac{k}{6} - \frac{5 (k+2)}{6} + \frac{8 L \gamma (k+2)^2}{B} \right)
        \nonumber \\
        & = \gamma \left( - \frac{2}{3} - \frac{2k}{3} + \frac{(k+2)}{3} \cdot \frac{24 L (k+2) \gamma}{B} \right)
        \nonumber \\
        & \leq \gamma \left( -\frac{k}{3} \right) \leq 0.
        \nonumber
    \end{align}
    Thus we have shown that for $k \in [0; N-1]$: $\left(\beta_{k+1} - 1 + \frac{8 L \gamma_{k+1}}{B} \right) \gamma_{k+1} \leq \beta_k \gamma_k$. Therefore, we can conclude the following:
    \begin{align}
        \beta_k \gamma_k  \expect{f(x_{k+1}^{ag}) - f^*} & \leq \left(\beta_k - 1 + \frac{8 L \gamma_k}{B} \right) \gamma_k \expect{ f(x_k^{ag}) - f^*} 
        \nonumber \\
        & \quad \quad +\frac{1}{2} \expect{\norms{x_k - x^*}^2}  - \frac{1}{2} \expect{\norms{x_{k+1} - x^*}^2} 
        \nonumber \\
        & \quad \quad + \frac{8 L^2 R^2 \gamma_k^2}{B \beta_k^2} + \frac{4 \sigma^2_{*} \gamma_k^2}{B} 
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}} + \gamma_k^2 \norms{\bb(x_k^{md})}^2
        \nonumber \\
        & \leq \beta_{k-1} \gamma_{k-1} \expect{ f(x_k^{ag}) - f^*} 
        \nonumber \\
        & \quad \quad +\frac{1}{2} \expect{\norms{x_k - x^*}^2}  - \frac{1}{2} \expect{\norms{x_{k+1} - x^*}^2} 
        \nonumber \\
        & \quad \quad + \frac{8 L^2 R^2 \gamma_k^2}{B \beta_k^2} + \frac{4 \sigma^2_{*} \gamma_k^2}{B} 
        \nonumber \\
        & \quad \quad + \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}} + \gamma_k^2 \norms{\bb(x_k^{md})}^2.
        \nonumber
    \end{align}
    Summing the both sides over $k$ we obtain:
    \begin{align}
        \sum_{k = 0}^{N-1} \beta_k \gamma_k  \expect{f(x_{k+1}^{ag}) - f^*} 
        & \leq \sum_{k = 0}^{N-1} \beta_{k-1} \gamma_{k-1} \expect{ f(x_k^{ag}) - f^*} 
        \nonumber \\
        & \quad \quad + \sum_{k = 0}^{N-1} \left(\frac{1}{2} \expect{\norms{x_k - x^*}^2}  - \frac{1}{2} \expect{\norms{x_{k+1} - x^*}^2} \right)
        \nonumber \\
        & \quad \quad + \sum_{k = 0}^{N-1} \frac{8 L^2 R^2 \gamma_k^2}{B \beta_k^2} + \sum_{k = 0}^{N-1} \frac{4 \sigma^2_{*} \gamma_k^2}{B} 
        \nonumber \\
        & \quad \quad + \sum_{k = 0}^{N-1} \gamma_k \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}} + \sum_{k = 0}^{N-1} \gamma_k^2 \norms{\bb(x_k^{md})}^2.
        \nonumber
    \end{align}
    Simplifying the expression we have
    \begin{align}
        \beta_{N-1} \gamma_{N-1}  \expect{f(x_{N}^{ag}) - f^*} 
        & \leq \frac{1}{2} \expect{\norms{x_0 - x^*}^2}  + \sum_{k = 0}^{N-1} \frac{288 L^2 R^2 \gamma^2(k+1)^2}{B (k+6)^2} + \sum_{k = 0}^{N-1} \frac{4 \sigma^2_{*} \gamma^2(k+1)^2}{B} 
        \nonumber \\
        & \quad \quad + \sum_{k = 0}^{N-1} \gamma (k+1) \dotprod{\bb(x_{k}^{md})}{x^* - x_{k}} + \sum_{k = 0}^{N-1} \gamma^2(k+1)^2 \norms{\bb(x_k^{md})}^2
        \nonumber \\
        & \leq \frac{R^2}{2}  + \frac{288 L^2 R^2 \gamma^2 N}{B} + \frac{12 \sigma^2_{*} \gamma^2 N^3}{B} + 2 \gamma N^2 R \zeta + \gamma^2 N^3 \zeta^2,
        \nonumber 
    \end{align}
    where $\norms{\bb(x_k^{md})}^2 \leq \zeta^2$. Divide the left and right side by $\beta_{N-1} \gamma_{N-1} \simeq \gamma N^2$:
    \begin{align}
        \expect{f(x_{N}^{ag}) - f^*} 
        & \leq \frac{R^2}{2 \gamma N^2}  + \frac{288 L^2 R^2 \gamma}{B N} + \frac{12 \sigma^2_{*} \gamma N}{B} + 2 R \zeta + \gamma N \zeta^2.
        \nonumber 
    \end{align}
    With our choice of $\gamma = \min \left\{ \frac{1}{12 L}, \frac{B}{24 L (N+1)}, \sqrt{\frac{BR^2}{\sigma_*^2 N^3}} \right\}$ we obtain:
    \begin{align}
        \expect{f(x_{N}^{ag}) - f^*} 
        & \lesssim \frac{L R^2}{ N^2}  + \frac{L R^2 }{BN} + \frac{\sigma_{*}R}{\sqrt{BN}} +  \zeta R + \frac{\zeta^2 N}{2L}.
        \nonumber 
    \end{align}
    \qed

    \section{Proof Theorem on the convergence of AZO-SGD} \label{Appendix:proof_th2}
    In this section, we present a detailed proof of the results of Theorem \ref{th:AZO_SGD}. First, we find the bias and the second moment of the gradient approximation \eqref{eq:gradient_approximation} based on the improved analysis of the paper \cite{Akhavan_2023}.
    \paragraph{Bias of gradient approximation}
    Using the variational representation of the Euclidean norm, and definition of gradient approximation \eqref{eq:gradient_approximation} we can write:
    \begin{align}
        \norms{\expect{\gg(x_k,\xi,e)} - \nabla f(x_k)} &= \norms{\expect{\frac{d}{2 \tau}\left( f_\delta(x_k + \tau e, \xi) - f_\delta(x_k - \tau e, \xi) \right) e} - \nabla f(x_k)}
        \nonumber \\
        & \overset{\circledOne}{=} \norms{\expect{\frac{d}{\tau}\left( f(x_k + \tau e, \xi) + \delta(x_k + \tau e) \right) e} - \nabla f(x_k)}
        \nonumber \\
        & \overset{\circledTwo}{\leq}  \norms{\expect{\frac{d}{\tau} f(x_k + \tau e, \xi) e} - \nabla f(x_k)} + \frac{d \Delta}{\tau}
        \nonumber \\
        & \overset{\circledThree}{=}   \norms{\expect{\nabla f(x_k + \tau u, \xi)} - \nabla f(x_k)} + \frac{d \Delta}{\tau}
        \nonumber \\
        & = \sup_{z \in S_2^d(1)} \expect{| \nabla_z f(x_k + \tau u, \xi) - \nabla_z f(x_k)|} + \frac{d \Delta}{\tau}
        \nonumber \\
        & \overset{\eqref{eq:L_smoothness}}{\leq} L \tau \expect{\norms{u}} + \frac{d \Delta}{\tau} 
        \nonumber \\
        & \leq L \tau + \frac{d \Delta}{\tau}, \label{eq:proof_bias}
    \end{align}
    where $u \in B_2^d(1)$, $\circledOne =$ the equality is obtained from the fact, namely, distribution of $e$ is symmetric, $\circledTwo =$ the inequality is obtain from bounded noise $|\delta(x)| \leq \Delta$, $\circledThree =$ the equality is obtained from a version of Stokesâ€™ theorem \cite{Zorich_2016}.

    \paragraph{Bounding second moment of gradient approximation} By definition gradient approximation \eqref{eq:gradient_approximation} and Wirtinger-Poincare inequality \eqref{eq:Wirtinger_Poincare} we have
    \begin{align}
        \expect{\norms{\gg(x^*,\xi,e)}^2} & = \frac{d^2}{4 \tau^2} \expect{\norms{\left(f_\delta(x^* + \tau e, \xi) - f_\delta(x^* - \tau e, \xi)\right) e}^2}
        \nonumber \\
        & = \frac{d^2}{4 \tau^2} \expect{\left(f(x^* + \tau e, \xi) - f(x^* - \tau e, \xi) + \delta (x^* + \tau e) - \delta (x^* -\tau e)\right)^2}
        \nonumber \\
        & \overset{\eqref{eq:squared_norm_sum}}{\leq} \frac{d^2}{2 \tau^2} \left( \expect{\left(f(x^* + \tau e, \xi) - f(x^* - \tau e, \xi)\right)^2} + 2 \Delta^2 \right)
        \nonumber \\
        & \overset{\eqref{eq:Wirtinger_Poincare}}{\leq} \frac{d^2}{2 \tau^2} \left( \frac{\tau^2}{d} \expect{\norms{ \nabla f(x^* + \tau e, \xi) + \nabla f(x^* - \tau e, \xi)}^2} + 2 \Delta^2 \right) 
        \nonumber \\
        & = \frac{d^2}{2 \tau^2} \left( \frac{\tau^2}{d} \expect{\norms{ \nabla f(x^* + \tau e, \xi) + \nabla f(x^* - \tau e, \xi) \pm 2 \nabla f(x^*, \xi)}^2} + 2 \Delta^2 \right)
        \nonumber \\
        & \overset{\eqref{eq:L_smoothness}}{\leq} 4d \norms{\nabla f(x^*, \xi)}^2 + 4 d  L^2 \tau^2 \expect{\norms{e}^2}  + \frac{d^2 \Delta^2}{\tau^2}  
        \nonumber \\
        & \overset{\circledOne}{\leq} 4d \sigma^2_* + 4 d  L^2 \tau^2 \expect{\norms{e}^2}  + \frac{d^2 \Delta^2}{\tau^2}, \label{eq:proof_variance}
    \end{align}
    where $\circledOne =$ the inequality is obtain from Assumption \ref{ass:stoch_noise}.

   We can now explicitly write down the convergence of the gradient-free AZO-SGD method (see Section \ref{sec:Main_Result}, Algorithm \ref{alg:AZO_SGD}) by substituting upper bounds on the bias \eqref{eq:proof_bias} and second moment \eqref{eq:proof_variance} for the gradient approximation \eqref{eq:gradient_approximation} in the convergence of the first-order method: Biased AC-SA (see Theorem \ref{th:biased_AC_SA}):
   \begin{align}
       \expect{f(x_{N}^{ag}) - f^*} 
        & \lesssim \underbrace{\frac{L R^2}{ N^2}}_{\circledOne}  + \underbrace{\frac{L R^2 }{BN}}_{\circledTwo} + \underbrace{\frac{\sqrt{d} \sigma_{*}R}{\sqrt{BN}}}_{\circledThree} + \underbrace{\frac{\sqrt{d} L \tau R}{\sqrt{BN}}}_{\circledFour} + \underbrace{\frac{d \Delta R}{\tau \sqrt{BN}}}_{\circledFive}  
        \nonumber \\
        & \quad \quad +  \underbrace{L \tau R}_{\circledSix} + \underbrace{\frac{d \Delta R}{\tau}}_{\circledSeven} + \underbrace{L \tau^2 N}_{\circledEight} + \underbrace{\frac{d^2 \Delta^2 N}{\tau^2 L}}_{\circledNine}.
        \nonumber 
   \end{align}
    \begin{flushleft}
        \textit{ Proof of the Theorem \ref{th:AZO_SGD}}. 
    \end{flushleft}
    \textbf{From term $\circledOne$}, we find the number of iterations $N$ required for Algorithm \ref{alg:AZO_SGD} to achieve $\varepsilon$-accuracy:
    \begin{align}
        \circledOne: \quad \frac{L R^2}{ N^2} \leq \varepsilon \quad & \Rightarrow \quad N \geq \sqrt{\frac{L R^2}{\varepsilon}};
        \nonumber \\
         N &= \mathcal{O}\left( \sqrt{\frac{L R^2}{\varepsilon}} \right). \label{eq:proof_iterations}
    \end{align}
    \textbf{From terms $\circledTwo$ and $\circledThree$}, we find the batch size $B$ required to achieve optimality in iteration complexity $N$: 
    \begin{align}
        &\circledTwo: \quad \frac{L R^2 }{BN} \leq \varepsilon \quad \Rightarrow \quad B \geq \frac{L R^2}{\varepsilon N} 
        \overset{\eqref{eq:proof_iterations}}{=} \mathcal{O}\left( \sqrt{\frac{L R^2}{\varepsilon}}\right);
        \nonumber \\
        &\circledThree: \quad \frac{\sqrt{d} \sigma_{*}R}{\sqrt{BN}} \quad  \Rightarrow \quad B \geq \frac{d \sigma^2_* R^2}{\varepsilon^2 N} 
        \overset{\eqref{eq:proof_iterations}}{=}  \mathcal{O}\left( \frac{d \sigma^2_* R}{\varepsilon^{3/2} L^{1/2}} \right);
        \nonumber \\
        & \quad \quad \quad B = \max \left\{ \mathcal{O}\left( \sqrt{\frac{L R^2}{\varepsilon}}\right),  \mathcal{O}\left( \frac{d \sigma^2_* R}{\varepsilon^{3/2} L^{1/2}} \right)\right\}. \label{eq:proof_batch_size}
    \end{align}
    \textbf{From terms $\circledFour$, $\circledSix$ and $\circledEight$} we find the smoothing parameter $\tau$:
    \begin{align}
        &\circledFour: \quad \frac{\sqrt{d} L \tau R}{\sqrt{BN}} \leq 
        \varepsilon \quad \Rightarrow \quad \tau \leq \frac{\varepsilon \sqrt{BN}}{\sqrt{d} LR} \overset{\eqref{eq:proof_iterations}, \eqref{eq:proof_batch_size}}{=} \max \left\{ \sqrt{\frac{\varepsilon}{d L}}, \frac{\sigma_*}{L} \right\};
        \nonumber \\
        & \circledSix: \quad L \tau R \leq 
        \varepsilon \quad \Rightarrow \quad \tau \leq \frac{\varepsilon}{L R};
        \nonumber \\
        & \circledEight: \quad L \tau^2 N \leq \varepsilon \quad \Rightarrow \quad \tau \leq \sqrt{\frac{\varepsilon}{L N}} \overset{\eqref{eq:proof_iterations}}{=} \frac{\varepsilon^{3/4}}{L^{3/4} R^{1/2}};
        \nonumber \\
        & \quad \quad \quad \tau \leq \min \left\{ \max \left\{ \sqrt{\frac{\varepsilon}{d L}}, \frac{\sigma_*}{L} \right\}, \frac{\varepsilon}{L R}, \frac{\varepsilon^{3/4}}{L^{3/4} R^{1/2}} \right\} = \frac{\varepsilon}{L R}. \label{eq:proof_smoothing_parameter}
    \end{align}
    \textbf{From the remaining terms $\circledFive$, $\circledSeven$, and $\circledNine$}, we find the maximum allowable level of adversarial noise $\Delta$ that still guarantees the convergence of the Accelareted Zero-Order Stochastic Gradient Descent Method to desired accuracy~$\varepsilon$:
    \begin{align}
        &\circledFive: \quad \frac{d \Delta R}{\tau \sqrt{BN}} \leq \varepsilon \quad \Rightarrow \quad \Delta \leq \frac{\varepsilon \tau \sqrt{B N}}{d R} \overset{\eqref{eq:proof_iterations}, \eqref{eq:proof_batch_size}, \eqref{eq:proof_smoothing_parameter}}{=} \max \left\{ \frac{\varepsilon^{3/2}}{d \sqrt{L} R}, \frac{\varepsilon \sigma_*}{d L R} \right\};
        \nonumber \\
        & \circledSeven: \quad \frac{d \Delta R}{\tau} \leq \varepsilon \quad \Rightarrow \quad \Delta \leq \frac{\varepsilon \tau}{d R} \overset{\eqref{eq:proof_smoothing_parameter}}{=} \frac{\varepsilon^2}{d L R};
        \nonumber \\
        &\circledNine: \quad \frac{d^2 \Delta^2 N}{\tau^2 L} \leq \varepsilon \quad \Rightarrow \quad \Delta \leq \sqrt{\frac{\varepsilon \tau^2 L}{d^2 N}} \overset{\eqref{eq:proof_iterations}, \eqref{eq:proof_smoothing_parameter}}{=} \frac{\varepsilon^{7/4}}{d L^{3/4} R^{3/2}};
        \nonumber \\
        & \quad \quad \quad \Delta \leq \min \left\{ \max \left\{ \frac{\varepsilon^{3/2}}{d \sqrt{L} R}, \frac{\varepsilon \sigma_*}{d L R} \right\}, \frac{\varepsilon^2}{d L R}, \frac{\varepsilon^{7/4}}{d L^{3/4} R^{3/2}} \right\} = \frac{\varepsilon^2}{d L R}. 
        \label{eq:proof_noise_level}
    \end{align}
    In this way, the Accelareted Zero-Order Stochastic Gradient Descent (AZO-SGD) Method (see Algorithm \ref{alg:AZO_SGD}) achieves $\varepsilon$-accuracy: $\expect{f(x_{N}^{ag}) - f^*} \leq \varepsilon$ after 
        \begin{equation*}
            N = \mathcal{O}\left( \sqrt{\frac{L R^2}{\varepsilon}} \right), \quad T = N \cdot B = \max \left\{ \mathcal{O}\left( \frac{LR^2}{\varepsilon} \right), \mathcal{O}\left( \frac{d \sigma_*^2 R^2}{\varepsilon^{2}} \right) \right\}
        \end{equation*}
        number of iterations \eqref{eq:proof_iterations}, total number of gradient-free oracle calls \eqref{eq:proof_batch_size} and at
        \begin{equation*}
            \Delta \leq \frac{\varepsilon^2}{d L R^2}
        \end{equation*}
        the maximum level of noise \eqref{eq:proof_noise_level} with smoothing parameter $\tau = \frac{\varepsilon}{L R}$ \eqref{eq:proof_smoothing_parameter}. \\
    \qed
     