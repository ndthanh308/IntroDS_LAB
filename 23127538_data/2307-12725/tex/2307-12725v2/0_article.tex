% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\input{macros.tex}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[breaklinks=true,colorlinks=true,linkcolor=red, citecolor=blue, urlcolor=blue]{hyperref}%

\usepackage[backend=biber,bibencoding=utf8,sorting=none,style=gost-numeric,language=autobib,autolang=other,clearlang=true,sortcites=true,doi=false,isbn=false,date=year]{biblatex}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\addbibresource{2_reference.bib}

\usepackage{xcolor}
\newcommand{\al}[1]{{\color{black}#1}} % Aleksandr Lobanov

\begin{document}
%
\title{Accelerated Zero-Order SGD Method for Solving the Black Box Optimization Problem under ``Overparametrization'' Condition\thanks{The research was supported by Russian Science Foundation (project No. 21-71- 30005), \url{https://rscf.ru/en/project/21-71-30005/}.}}%\thanks{Supported by organization x.}}
%
\titlerunning{AZO-SGD Method for Overparametrization Problems}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}

\author{Aleksandr Lobanov\inst{1,2}\orcidID{0000-0003-1620-9581}  \and
Alexander Gasnikov\inst{1,2,3}\orcidID{0000-0002-7386-039X}}

%
\authorrunning{A. Lobanov et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Moscow Institute of Physics and Technology, Dolgoprudny, Russia
 \and 
 ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia 
\and 
Institute for Information Transmission Problems RAS, Moscow, Russia \\
\email{\{lobanov.av,gasnikov.av\}@mipt.ru}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% Today it is impossible to imagine modern machine learning, optimization, etc. problems without big data. Often when talking about machine learning we already mean big data. That is why it is important to solve such problems efficiently. A significant breakthrough in solving such problems was the concept of batching, that is, to parallelize the solution of the problem on several machines, thereby reducing the solution time. In this paper, we study the solution of a convex black-box problem in a overparameterization condition when the gradient is not available, but a zero-order oracle (the value of the objective function) is available. Considering the smooth case, we use gradient approximation via $l_2$ randomization to create a novel gradient-free algorithm. We also show the effectiveness of the proposed algorithm and verify the theoretical results of convergence using a model example in the Experiments Section.
This paper is devoted to solving a convex stochastic optimization problem in a overparameterization setup for the case where the original gradient computation is not available, but an objective function value can be computed. For this class of problems we provide a novel gradient-free algorithm, whose creation approach is based on applying a gradient approximation with $l_2$ randomization instead of a gradient oracle in the biased Accelerated SGD algorithm, which generalizes the convergence results of the AC-SA algorithm to the case where the gradient oracle returns a noisy (inexact) objective function value. We also perform a detailed analysis to find the maximum admissible level of adversarial noise at which we can guarantee to achieve the desired accuracy. We verify the theoretical results of convergence using a model example.

\keywords{Black-Box Optimization  \and Overparametrization \and Accelerated Zero-Order SGD Method \and Biased Gradient Oracle.}
\end{abstract}
%
%
%

\input{1_main}

%\newpage
\printbibliography

\newpage

\input{3_Appendix}


\end{document}
