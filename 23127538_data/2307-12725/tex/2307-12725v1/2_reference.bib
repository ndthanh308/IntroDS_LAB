@article{Audet_2017,
  title={Derivative-free and blackbox optimization},
  author={Audet, Charles and Hare, Warren},
  year={2017},
  publisher={Springer}
}

@book{Conn_2009,
  title={Introduction to derivative-free optimization},
  author={Conn, Andrew R and Scheinberg, Katya and Vicente, Luis N},
  year={2009},
  publisher={SIAM}
}

@inproceedings{Gasnikov_2022_ICML,
  title={The power of first-order smooth optimization for black-box non-smooth problems},
  author={Gasnikov, Alexander and Novitskii, Anton and Novitskii, Vasilii and Abdukhakimov, Farshed and Kamzolov, Dmitry and Beznosikov, Aleksandr and Takac, Martin and Dvurechensky, Pavel and Gu, Bin},
  booktitle={International Conference on Machine Learning},
  pages={7241--7265},
  year={2022},
  organization={PMLR}
}

@inproceedings{Bach_2016,
  title={Highly-smooth zero-th order online optimization},
  author={Bach, Francis and Perchet, Vianney},
  booktitle={Conference on Learning Theory},
  pages={257--283},
  year={2016},
  organization={PMLR}
}

@article{Akhavan_2022,
  title={A gradient estimator via L1-randomization for online zero-order optimization with two point feedback},
  author={Akhavan, Arya and Chzhen, Evgenii and Pontil, Massimiliano and Tsybakov, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7685--7696},
  year={2022}
}

@article{Rosenbrock_1960,
  title={An automatic method for finding the greatest or least value of a function},
  author={Rosenbrock, HoHo},
  journal={The computer journal},
  volume={3},
  number={3},
  pages={175--184},
  year={1960},
  publisher={Oxford University Press}
}

@article{Bogolubsky_2016,
  title={Learning supervised pagerank with gradient-based and gradient-free optimization methods},
  author={Bogolubsky, Lev and Dvurechenskii, Pavel and Gasnikov, Alexander and Gusev, Gleb and Nesterov, Yurii and Raigorodskii, Andrei M and Tikhonov, Aleksey and Zhukovskii, Maksim},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{Chen_2017,
  title={Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  author={Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 10th ACM workshop on artificial intelligence and security},
  pages={15--26},
  year={2017}
}

@inproceedings{Patel_2022,
  title={Distributed online and bandit convex optimization},
  author={Patel, Kumar Kshitij and Saha, Aadirupa and Wang, Lingxiao and Srebro, Nathan},
  booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
  year={2022}
}

@article{Lobanov_2023_SADOM,
  title={Non-Smooth Setting of Stochastic Decentralized Convex Optimization Problem Over Time-Varying Graphs},
  author={Lobanov, Aleksandr and Konin, Georgiy and Gasnikov, Alexander and Kovalev, Dmitry},
  journal={arXiv preprint arXiv:2307.00392},
  year={2023}
}

@inproceedings{Gao_2018,
  title={Black-box generation of adversarial text sequences to evade deep learning classifiers},
  author={Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={50--56},
  year={2018},
  organization={IEEE}
}

@article{Scaman_2019,
  title={Optimal convergence rates for convex distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={1--31},
  year={2019}
}

@article{Lobanov_2022,
  title={Gradient-Free Federated Learning Methods with $ l\_1 $ and $ l\_2 $-Randomization for Non-Smooth Convex Stochastic Optimization Problems},
  author={Lobanov, Aleksandr and Alashqar, Belal and Dvinskikh, Darina and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2211.10783},
  year={2022}
}

@article{Hazan_2017,
  title={Hyperparameter optimization: A spectral approach},
  author={Hazan, Elad and Klivans, Adam and Yuan, Yang},
  journal={arXiv preprint arXiv:1706.00764},
  year={2017}
}

@article{Elsken_2019,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{Papernot_2017,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Proceedings of the 2017 ACM on Asia conference on computer and communications security},
  pages={506--519},
  year={2017}
}

@article{Papernot_2016,
  title={Transferability in machine learning: from phenomena to black-box attacks using adversarial samples},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1605.07277},
  year={2016}
}

@article{Duchi_2015,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}

@article{Shibaev_2022,
  title={Zeroth-order methods for noisy H{\"o}lder-gradient functions},
  author={Shibaev, Innokentiy and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={Optimization Letters},
  volume={16},
  number={7},
  pages={2123--2143},
  year={2022},
  publisher={Springer}
}

@article{Shamir_2017,
  title={An optimal algorithm for bandit and zero-order convex optimization with two-point feedback},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={1703--1713},
  year={2017},
  publisher={JMLR. org}
}

@article{Bubeck_2012,
  title={Regret analysis of stochastic and nonstochastic multi-armed bandit problems},
  author={Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={5},
  number={1},
  pages={1--122},
  year={2012},
  publisher={Now Publishers, Inc.}
}

@inproceedings{Bartlett_2008,
  title={High-probability regret bounds for bandit online linear optimization},
  author={Bartlett, Peter and Dani, Varsha and Hayes, Thomas and Kakade, Sham and Rakhlin, Alexander and Tewari, Ambuj},
  booktitle={Proceedings of the 21st Annual Conference on Learning Theory-COLT 2008},
  pages={335--342},
  year={2008},
  organization={Omnipress}
}

@inproceedings{Choromanski_2018,
  title={Structured evolution with compact architectures for scalable policy optimization},
  author={Choromanski, Krzysztof and Rowland, Mark and Sindhwani, Vikas and Turner, Richard and Weller, Adrian},
  booktitle={International Conference on Machine Learning},
  pages={970--978},
  year={2018},
  organization={PMLR}
}

@article{Mania_2018,
  title={Simple random search of static linear policies is competitive for reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{Gasnikov_2022,
  title={Randomized gradient-free methods in convex optimization},
  author={Gasnikov, Alexander and Dvinskikh, Darina and Dvurechensky, Pavel and Gorbunov, Eduard and Beznosikov, Aleksander and Lobanov, Alexander},
  journal={arXiv preprint arXiv:2211.13566},
  year={2022}
}

@article{Robbins_1951,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{Bottou_2018,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@inproceedings{Dvinskikh_2022,
  title={Noisy zeroth-order optimization for non-smooth saddle point problems},
  author={Dvinskikh, Darina and Tominin, Vladislav and Tominin, Iaroslav and Gasnikov, Alexander},
  booktitle={International Conference on Mathematical Optimization Theory and Operations Research},
  pages={18--33},
  year={2022},
  organization={Springer}
}

@article{Lobanov_2023_WAIT,
  title={Stochastic Adversarial Noise in the ``Black Box" Optimization Problem},
  author={Lobanov, Aleksandr},
  journal={arXiv preprint arXiv:2304.07861},
  year={2023}
}

@article{Kornilov_2023,
  title={Gradient Free Methods for Non-Smooth Convex Optimization with Heavy Tails on Convex Compact},
  author={Kornilov, Nikita and Gasnikov, Alexander and Dvurechensky, Pavel and Dvinskikh, Darina},
  journal={arXiv preprint arXiv:2304.02442},
  year={2023}
}

@article{Lan_2012,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1-2},
  pages={365--397},
  year={2012},
  publisher={Springer}
}

@inproceedings{Woodworth_2021,
  title={The min-max complexity of distributed stochastic convex optimization with intermittent communication},
  author={Woodworth, Blake E and Bullins, Brian and Shamir, Ohad and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={4386--4437},
  year={2021},
  organization={PMLR}
}

@article{Gorbunov_2020,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15042--15053},
  year={2020}
}

@article{Gasnikov_2016,
  title={Universal fast gradient method for stochastic composit optimization problems},
  author={Gasnikov, Alexander and Nesterov, Yurii},
  journal={arXiv preprint arXiv:1604.05275},
  year={2016}
}

@article{Ajalloeian_2020,
  title={On the convergence of SGD with biased gradients},
  author={Ajalloeian, Ahmad and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2008.00051},
  year={2020}
}

@article{Polyak_1963,
  title={Gradient methods for the minimisation of functionals},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={3},
  number={4},
  pages={864--878},
  year={1963},
  publisher={Elsevier}
}

@article{Lojasiewicz_1963,
  title={Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques r{\'e}els},
  author={Lojasiewicz, Stanislaw},
  journal={Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  pages={87--89},
  year={1963}
}

@article{Yue_2022,
  title={On the Lower Bound of Minimizing Polyak-$\{$$\backslash$L$\}$ ojasiewicz functions},
  author={Yue, Pengyun and Fang, Cong and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2212.13551},
  year={2022}
}

@article{Woodworth_2021_over,
  title={An even more optimal stochastic optimization algorithm: minibatching and interpolation learning},
  author={Woodworth, Blake E and Srebro, Nathan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7333--7345},
  year={2021}
}

@inproceedings{Rakhlin_2012,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1571--1578},
  year={2012}
}

@article{Hazan_2014,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  publisher={JMLR. org}
}

@article{Stich_2019,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@book{Bertsekas_1996,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}

@article{Schmidt_2013,
  title={Fast convergence of stochastic gradient descent under a strong growth condition},
  author={Schmidt, Mark and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:1308.6370},
  year={2013}
}

@article{Srebro_2010,
  title={Optimistic rates for learning with a smooth loss},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1009.3896},
  year={2010}
}

@article{Lobanov_2023,
  title={Highly Smoothness Zero-Order Methods for Solving Optimization Problems under PL Condition},
  author={Lobanov, Aleksandr and Gasnikov, Alexander and Stonyakin, Fedor},
  journal={arXiv preprint arXiv:2305.15828},
  year={2023}
}

@inproceedings{Vaswani_2019,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1195--1204},
  year={2019},
  organization={PMLR}
}

@article{Zhang_2022,
  title={Adam can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28386--28399},
  year={2022}
}

@article{Fatkhullin_2022,
  title={Sharp analysis of stochastic optimization under global Kurdyka-Lojasiewicz inequality},
  author={Fatkhullin, Ilyas and Etesami, Jalal and He, Niao and Kiyavash, Negar},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15836--15848},
  year={2022}
}

@inproceedings{Tran_2022,
  title={Nesterov accelerated shuffling gradient method for convex optimization},
  author={Tran, Trang H and Scheinberg, Katya and Nguyen, Lam M},
  booktitle={International Conference on Machine Learning},
  pages={21703--21732},
  year={2022},
  organization={PMLR}
}

@article{Cotter_2011,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{Allen_2019,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Belkin_2019,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{Jacot_2018,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{Vasin_2023,
  title={Accelerated gradient methods with absolute and relative noise in the gradient},
  author={Vasin, Artem and Gasnikov, Alexander and Dvurechensky, Pavel and Spokoiny, Vladimir},
  journal={Optimization Methods and Software},
  pages={1--50},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{Dvinskikh_2021,
  title={Decentralized and parallel primal and dual accelerated methods for stochastic convex programming problems},
  author={Dvinskikh, Darina and Gasnikov, Alexander},
  journal={Journal of Inverse and Ill-posed Problems},
  volume={29},
  number={3},
  pages={385--405},
  year={2021}
}

@phdthesis{Devolder_2013,
  title={Exactness, inexactness and stochasticity in first-order methods for large-scale convex optimization},
  author={Devolder, Olivier},
  year={2013},
  school={CORE UCLouvain Louvain-la-Neuve, Belgium}
}

@book{Nesterov_2003,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{Akhavan_2023,
  title={Gradient-free optimization of highly smooth functions: improved analysis and a new algorithm},
  author={Akhavan, Arya and Chzhen, Evgenii and Pontil, Massimiliano and Tsybakov, Alexandre B},
  journal={arXiv preprint arXiv:2306.02159},
  year={2023}
}

@book{Zorich_2016,
  title={Mathematical analysis II},
  author={Zorich, Vladimir Antonovich and Paniagua, Octavio},
  volume={220},
  year={2016},
  publisher={Springer}
}

@article{Ilandarideva_2023,
  title={Accelerated stochastic approximation with state-dependent noise},
  author={Ilandarideva, Sasila and Juditsky, Anatoli and Lan, Guanghui and Li, Tianjiao},
  journal={arXiv preprint arXiv:2307.01497},
  year={2023}
}