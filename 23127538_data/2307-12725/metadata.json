{
  "title": "Accelerated Zero-Order SGD Method for Solving the Black Box Optimization Problem under \"Overparametrization\" Condition",
  "authors": [
    "Aleksandr Lobanov",
    "Alexander Gasnikov"
  ],
  "submission_date": "2023-07-24T12:09:06+00:00",
  "revised_dates": [
    "2024-02-14T01:05:44+00:00"
  ],
  "abstract": "This paper is devoted to solving a convex stochastic optimization problem in a overparameterization setup for the case where the original gradient computation is not available, but an objective function value can be computed. For this class of problems we provide a novel gradient-free algorithm, whose creation approach is based on applying a gradient approximation with $l_2$ randomization instead of a gradient oracle in the biased Accelerated SGD algorithm, which generalizes the convergence results of the AC-SA algorithm to the case where the gradient oracle returns a noisy (inexact) objective function value. We also perform a detailed analysis to find the maximum admissible level of adversarial noise at which we can guarantee to achieve the desired accuracy. We verify the theoretical results of convergence using a model example.",
  "categories": [
    "math.OC"
  ],
  "primary_category": "math.OC",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12725",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 579924,
  "size_after_bytes": 508088
}