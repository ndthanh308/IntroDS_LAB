\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{tablefootnote}
\usepackage{color, colortbl}
\usepackage[misc]{ifsym} % for envolope sign
\usepackage{soul}%for \ul
\usepackage{amsmath}%for aligning eq
\usepackage{fixltx2e}%sub and super scripts
\usepackage{afterpage}
\usepackage{caption}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

\definecolor{brown}{RGB}{229,225,224}

\definecolor{LRed}{RGB}{227, 167, 125}
\definecolor{DRed}{RGB}{134, 37, 0}
\definecolor{ORed}{RGB}{255,204,170}

\definecolor{LGreen}{RGB}{227, 234, 238}%209,255,214
\definecolor{DGreen}{RGB}{0,82,147}%0,80,10
\definecolor{OGreen}{RGB}{132,178,214}%207,225,190



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
%\fancyhead[C]{\textit{arXiv} Template \hrule}
%\fancyhead[R]{\textsc{\undertitle}}


% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Annika Ries$^{* 1, 2}$, \Letter \hspace{0.3em} Tina Dorosti$^{* 1-3}$, Johannes Thalhammer$^\mathrm{{1-4}}$, Daniel Sasse$^\mathrm{{3}}$, Andreas Sauter$^\mathrm{{3}}$,\\
  \textbf{Felix Meurer$^\mathrm{{3}}$, Ashley Benne$^\mathrm{{3, 4}}$, Tobias Lasser$^\mathrm{{2, 5}}$, Franz Pfeiffer$^\mathrm{{1-4}}$},\\
  \textbf{Florian Schaff$^\mathrm{{1, 2}}$, Daniela Pfeiffer$^\mathrm{{3, 4}}$} \\\\
  1 Chair of Biomedical Physics, Department of Physics, School of Natural Sciences\\
  2 Munich Institute of Biomedical Engineering\\
  3 Department of Diagnostic and Interventional Radiology, School of Medicine, Klinikum rechts der Isar\\
  4 Institute for Advanced Study\\
  5 Computational Imaging and Inverse Problems, Department of Computer Science, School of Computation,\\ Information, and Technology\\\\
  Technical University of Munich, Germany\\
  * Authors contributed equally to this work\\
  %\texttt{\{Author1, Author2\}email@email} \\
  \Letter \hspace{0.3em} \texttt{tina.dorosti@tum.de} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle
\section*{Summary statement:}
Artifacts in sparse-view lung cancer computed tomography images were corrected with a U-Net model, and the best trade-off between the number of projection views, image quality, and diagnostic confidence was determined to be 64 views in a reader study.
\section*{Key points:}
\begin{enumerate}
  \item The results of our reader study suggest that with a post-processing method by the dual-frame U-Net, the number of views used for CT reconstruction can be reduced from 2048 to 64 while maintaining diagnostically accurate image quality for lung nodule detection (sensitivity = 0.94).
  \item Dice Similarity Coefficients show that lung nodule markings by the readers did not significantly improve for the images post-processed by the dual-frame U-Net compared to the unprocessed sparse-view images.
  \item Correcting sparse-view artifacts in CT scans with the dual-frame U-Net drastically increased the readers’ confidence in the diagnosis for the detection of lung nodules.\\
\end{enumerate}

\textbf{Abbreviations:} Confidence Interval (CI), Convolutional Neural Network (CNN), Computed Tomography (CT), Dice Similarity Coefficient (DSC), Mean Squared Error (MSE), Sensitivity (Se), Specificity (Sp)

% keywords can be removed
\keywords{Artifact Correction \and Lung Cancer \and Post-processing \and Sparse-view CT \and Dual-frame U-Net}

\newpage
\begin{abstract}
\textbf{Purpose:} To improve the image quality of sparse-view computed tomography (CT) images with a U-Net for lung cancer detection and to determine the best trade-off between number of views, image quality, and diagnostic confidence.\\\\
\textbf{Methods:} CT images from 41 subjects (34 with lung cancer, seven healthy) were retrospectively selected ($\mathrm{01.2016 - 12.2018}$) and forward projected onto 2048-view sinograms. Six corresponding sparse-view CT data subsets at varying levels of undersampling were reconstructed from sinograms using filtered backprojection with 16, 32, 64, 128, 256, and 512 views, respectively. A dual-frame U-Net was trained and evaluated for each subsampling level on 8,658 images from 22 diseased subjects. A representative image per scan was selected from 19 subjects (12 diseased, seven healthy) for a single-blinded reader study. The selected slices, for all levels of subsampling, with and without post-processing by the U-Net model, were presented to three readers. Image quality and diagnostic confidence were ranked using pre-defined scales. Subjective nodule segmentation was evaluated utilizing sensitivity (Se) and Dice Similarity Coefficient (DSC) with 95\% confidence intervals (CI).\\\\
\textbf{Results:} The 64-projection sparse-view images resulted in Se $=0.89$ and DSC $=0.81 [0.75, 0.86]$ while their counterparts, post-processed with the U-Net, had improved metrics (Se $=0.94$, DSC $=0.85 [0.82, 0.87]$). Fewer views lead to insufficient quality for diagnostic purposes. For increased
views, no substantial discrepancies were noted between the sparse-view and post-processed images.\\\\
\textbf{Conclusion:} Projection views can be reduced from 2048 to 64 while maintaining image quality and the
confidence of the radiologists on a satisfactory level.
\end{abstract}



\section{Introduction}
Lung cancer maintains the highest mortality rate for malignancies around the globe, with more than 2.2 million new cases recorded worldwide in 2020 \cite{WHO}\cite{WCRF}. More than half of all lung cancer diagnoses present as symptomatic once the patient has reached a progressive stage \cite{GEKID}. Regular screenings enable early detection and thereby increase survival rates \cite{GEKID}\cite{ACS}. \\

X-ray computed tomography (CT) is considered standard practice in present-day medicine for diagnosing lung nodules \cite{ACS}\cite{ONKO}\cite{NHS}, yet it comes at the cost of radiation exposure \cite{Hamada2014}\cite{FDA}. To make regular screenings possible, a trade-off between dose and image quality must be found \cite{ACS}. Sparse-view CT is a technique for dose reduction. However, this technique leads to a degradation of image quality due to distinct streak artifacts caused by a limited number of projection views in the reconstruction process \cite{Kudo2013}\cite{Zhang2018}. \\

Machine learning approaches have shown promising results for sparse-view artifact correction \cite{Kudo2013}\cite{Zhang2018}\cite{Jin2016}\cite{Han2016}\cite{Han2018}\cite{Koetzier2023}. Specifically, residual learning has delivered superior results compared to the direct approach \cite{Jin2016}\cite{Han2016}. The goal of the network in residual learning is to estimate the difference between sparse-view and full-view images. In a direct approach, the network aims to predict the artifact-free image. The simpler topological structure of residual images allows for more efficient learning \cite{Han2016}. A popular network architecture for such artifact-correction tasks is the U-Net \cite{Ronneberger2015}. With a large receptive field, the model is capable of handling global artifacts such as the given sparse-view streak artifacts \cite{Jin2016}\cite{Han2016}. The dual-frame U-Net was proposed as a more robust variant of the standard U-Net for the task at hand \cite{Han2018}. \\

We hypothesize that by post-processing sparse-view lung cancer CT images with the dual-frame U-Net, the image quality can be substantially improved, allowing radiologists to diagnose lung tumor nodules at greatly reduced radiation exposure. In this pilot study, we assess the performance of the given architecture on correcting for streak artifacts present in sparse-view lung cancer CT scans. An image reconstructed from 2048 views, later referred to as a full-view image, was taken to calculate the residual image. Six levels of subsampled input images were reconstructed from 16, 32, 64, 128, 256, and 512 views, respectively. By conducting a reader study with the unprocessed sparse-view images and their U-Net post-processed counterpart images, we aim to find the best trade-off between the number of views, image quality, and confidence of the participating radiologists on their diagnosis.

\section{Methods}%Guidelines for Manuscript Preparation

\subsection{Dataset}%Abbreviations and Acronyms
The dataset consisted of 8,677 images from 41 subjects (seven healthy, 34 suffering from lung cancer). Approval from an ethics committee and patient informed consent were obtained. All data was selected retrospectively ($\mathrm{01.2016 - 12.2018}$) and anonymized. The data selection flowchart is given in \autoref{fig:flowchart}. Independent datasets were utilized for model assessment and the reader study. Additional 9,481 images from the Luna16 external dataset were utilized for testing the model’s robustness \cite{vanGinneken2016}\cite{Armato2011}. Table \ref{tab:demographics} shows the subject demographics for the internal datasets.

% Figure environment removed 

\begin{table}[h!]
\caption{\small Subject Demographics for Internal Datasets (n = 41)}
\label{tab:demographics}
%\scriptsize
\small
  \centering
    \begin{tabular}{lcccccc}
    \toprule 
    Dataset & \multicolumn{3}{c}{Model assessment} & \multicolumn{2}{c}{Reader study} \\
    \midrule
    \rowcolor{brown}Parameter \textbackslash    subset &  Train  &  Validation & Test &  Healthy  &  Diseased \\
    \midrule
    \midrule
    Male & 5 & 2 & 4 & 5 & 7\\
    \rowcolor{brown}Female & 7 & 0 & 4 & 2 & 5\\
    Age \tablefootnote{Note. Age is given in terms of each set's mean and age range [youngest, oldest].} (years) & 65.8 [37, 79] & 77.0 [73, 81] & 61.9 [46, 83] & 44.3 [28, 70] & 64.8 [44, 79]\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Data Preparation}
The CT images were forward projected onto 2048-view sinograms. Sparse-view CT data subsets at varying levels of undersampling were generated using the filtered backprojection algorithm with 16, 32, 64, 128, 256, and 512 views, respectively. The full-view data was generated using 2048 views. All operations were performed using the Astra toolbox (2.1.1) \cite{vanAarle2015}\cite{vanAarle2016}\cite{Palenstijn2011}. Images were of size 512x512 pixels. The intensity values of all images were clipped to the lung window, [-1450, 250] HU, and
normalized between zero and one.\\

22 of the diseased subjects were split on CT scan level into train (n=12, images=4,723), validation (n=2, images=787), and test sets (n=8, images=3,148). The residual ground truth label images were calculated as the difference between the full-view and the sparse-view images for each projection view. The final post-processed image was the pure-artifact U-Net prediction subtracted from the sparse-view input.

\subsection{Network Architecture}
The dual-frame U-Net was utilized, as depicted in \autoref{fig:dualUnet}. The contracting path consists of four subsequently applied encoder blocks, each with two convolution layers (3x3 kernels, followed by batch normalization and a rectified linear unit activation). A 2x2 max pooling layer is applied after each encoder block. Following the two convolution layers in the bottleneck, the features are upsampled with four subsequently applied decoder blocks mirroring the contracting path, followed by a 2x2 unpooling before each decoder block. The dual-frame U-Net introduces additional skip connections, bridging the output of each encoder block after pooling to the input of the associated decoder block before unpooling. These additional connections ensure the frame condition is met, thereby reducing blurring and image artifacts. The final image is obtained with a 1x1 convolution \cite{Han2018}.\\

The dual-frame U-Net was chosen as it generated robust outputs and had a comparable computational effort as the standard U-Net. The network was implemented with Keras (2.4.0) in TensorFlow, randomly initialized, and trained individually for each number of projection views \cite{Keras}\cite{Tensorflow}. The sparse-view images were taken as input, and the residual images were taken as labels. No data augmentation was applied. Mean squared error (MSE) loss with an adaptive moment estimation optimizer was utilized. Early stopping was implemented if validation loss did not improve. Training took place for a maximum of $n = 30$ epochs and a batch size of six. The initial learning rate
$lr_0$ was set to $lr_0 = 0.001$ and decayed exponentially per epoch following $lr_n = lr_{n-1} \cdot e^{-0.1}$ . The
model with the smallest validation loss among all epochs was chosen for inference on the test sets
and the reader study. The quality of post-processed images was evaluated with the MSE and the structural similarity index measure (SSIM) metrics \cite{SSIM}.

% Figure environment removed

\subsection{Reader Study}
CT scans from 19 subjects (seven healthy, 12 diseased) were considered for this single-blinded study. Three board-certified radiologists and an in-training radiologist, respectively with 15 (DP), 11, 10, and 5 years of experience in chest radiology, participated in the study. Using the full-view images, DP selected a representative slice per subject and marked the ground truth lung nodule segmentation (mean diameter = $1.11 [0.91, 1.31] cm$) for the diseased subjects. All nodules were metastases. The sparse-view images reconstructed from 16, 32, 64, 128, and 256 views and post-processed by the U-Net were presented to the other three radiologists, resulting in a total of 190 evaluated images.\\

Full-view and all sparse-view images of an exemplary slice are shown in \autoref{fig:exampleSlices}. Slices reconstructed and post-processed using 512 views were excluded from the study as DP determined that even without any post-processing, they are of comparable quality to the full-view images.\\

Readers were asked to independently annotate each slice using our in-house tool by rating every image on quality, the confidence of diagnosis, and the severity of artifacts present in the image according to pre-defined labels in Table \ref{tab:labels}. Furthermore, the radiologists were asked to independently segment perceived suspect pulmonary nodules. The following metrics were considered to compare the diagnostic reliability of images for different views. Sensitivity (Se) and specificity (Sp) values were calculated using true positive (TP), true negative (TN), false positive (FP), and false negative (FN) cases as \cite{Parikh2008}:

\begin{equation}
\begin{aligned}
    \small
    Se = \frac{TP}{TP+FN} \quad Sp = \frac{TN}{TN+FP} \quad .
    \label{eq:SeSp}
\end{aligned}
\end{equation}

For all TP cases, the segmentation overlaps were calculated with the Dice Similarity Coefficient (DSC) \cite{Dice1945}\cite{Fleiss2003}. For two segmentations, X and Y, the DSC was calculated as

\begin{equation}
\begin{aligned}
    \small
    DSC = \frac{2|X\cap Y|}{|X|+|Y|} \quad. 
    \label{eq:DSC}
\end{aligned}
\end{equation}

Here, $|X\cap Y|$ is the number of pixels marked in both segmentations. In case of no overlap, or if one of the segmentations was empty, the resulting DSC was zero. 95\% confidence intervals (CI) and P-values from the Wilcoxon signed-rank test were calculated with Python’s SciPy library (1.4.1) \cite{Wilcoxon}\cite{Virtanen2020}.

% Figure environment removed

\begin{table}[h!]
\caption{\small Defined Scale for Describing the Image Quality, Confidence of Diagnosis, and Artifact Severity for
Sparse-view CT Images with and without Post-processing}
\label{tab:labels}
%\scriptsize
\small
  \centering
    \begin{tabular}{cccc}
    \toprule 
    Scale & Quality & Confidence & Artifacts \\
    \midrule
    \midrule
    \rowcolor{brown} 1 & Not diagnostic & Not confident at all & No artifacts \\
    2 & Highly impaired & Slightly confident & Few artifacts and quality not impaired\\
    \rowcolor{brown} 3 & Impaired & Somewhat confident & Some artifacts and reduced quality\\
    4  & Sufficient & Fairly confident & A lot of artifacts and reduced quality \\
    \rowcolor{brown} 5 & High & Very confident & $-$\\
    6 & Very high & Surely confident & $-$\\
    \bottomrule
    \end{tabular}
\end{table}

\section{Results}
The following results show the model's performance on 3,148 images from eight diseased subjects and 9,481 images from the Luna16 dataset. Furthermore, results of the reader study on 19 CT-wise images from 12 diseased and seven healthy subjects are described.

\subsection{Network Performance}

\autoref{fig:exampleSlices} shows an example slice with varying levels of sub-sampling alongside the corresponding U-Net post-processed results. It can be observed that fewer projection views result in more artifacts. The sparse-view images from extremely limited views also lead to a loss of structural integrity in their post-processed counterparts. This was especially prominent for 16 views, as lung nodule distortion and microvascular structures generate diminished performance capabilities. Tumor composition and primary anatomical characteristics can better be amassed once reconstruction views have increased to 32. For 64 views, streak artifacts do not impact the tumor’s visibility due to tissue density, but minimalistic structural identification, such as small vessels, are not clearly portrayed. Minor features are displayed for 128 and 256 views; however, for 128 views, some streak artifacts remain present. For the post-processed image of 32 views, the tumor's shape is mostly correct, and the display of the vascular structures is improved. For 64 or more views, the tumor appearance in the post-processed image is similar to the full-view image. Furthermore, vascular distinction on imaging can be detected with the post-processed 128 views image. The post-processed image from 256 views is very close in quality to the full-view image. For 512 views, no qualitative differences can be detected.\\

%A directly proportional relationship is observed between increasing calculated image quality metric,
%MSE, and higher views. Mean MSE [95\% CI] on the internal test data for 16, 32, 64, 128, 256, and 512 views are measured as $2.32 [1.95, 2.78] \cdot 10^{-3}$ , $7.95 [6.51, 9.39] \cdot 10^{-4}$ , $2.40 [1.96, 2.84] \cdot 10^{-4}$ ,
%$8.46 [6.31, 10.6] \cdot 10^{-5}$ , $2.28 [1.54, 3.01] \cdot 10^{-5}$ , and $3.78 [2.81, 4.75] \cdot 10^{-6}$ , respectively. External testing with the Luna16 dataset results in mean MSE [95\% CI] values of $6.52 [4.93, 8.10] \cdot 10^{-3}$ ,
%$3.46 [2.49, 4.43] \cdot 10^{-3}$ , $1.04 [0.75, 1.34] \cdot 10^{-3}$ , $6.19 [4.18, 8.19] \cdot 10^{-4}$ , $1.07 [0.81, 1.32] \cdot 10^{-4}$ , and $5.34 [3.93, 6.75] \cdot 10^{-5}$ for the respective views.

A directly proportional relationship is observed between improved image quality and higher views. As shown in \autoref{tab:MSE_SSIIM}, calculated mean MSE and SSIM values increase with more projection views for the internal test set and the external Luna16 dataset. Although mean MSE and SSIM values are marginally higher for the internal test set, the model achieves comparable results on the external Luna16 dataset.

\begin{table*}[h!]
\captionof{table}{\small Mean Squared Error (MSE) and Structural Similarity Index Measure (SSIM) Metrics for Post-processed Images in the Internal Test Set and the External Luna16 Dataset for All Projection Views Presented by the Mean values and the Corresponding 95\% Confidence Intervals \\}
\label{tab:MSE_SSIIM}
  \small
  \centering
   \begin{tabular}{l|l|ccc}
    \toprule
     Metric & Dataset & 16 projections& 32 projections& 64 projections \\
    \midrule
    \midrule
    \rowcolor{brown} MSE & Test set & $2.36 [1.95, 2.78] \cdot 10^{-3}$ & $7.95 [6.51, 9.39] \cdot 10^{-4}$ &$2.40 [1.96, 2.84] \cdot 10^{-4}$ \\
    & Luna16 & $6.52 [4.93, 8.10] \cdot 10^{-3}$ & $3.46 [2.49, 4.43] \cdot 10^{-3}$ &$1.04 [0.746, 1.34] \cdot 10^{-3}$ \\
    \rowcolor{brown} SSIM & Test set & $0.799 [0.749, 0.809]$ & $0.834 [0.808, 0.861]$ & $0.895 [0.873, 0.917]$ \\
    & Luna16 & $0.782 [0.758, 0.805]$ & $0.816 [0.792, 0.840]$ & $0.873 [0.852, 0.895]$ \\
    \midrule
    \midrule
     &  & 128 projections& 256 projections & 512 projections \\
    \rowcolor{brown} MSE & Test set & $8.46 [6.31, 10.6] \cdot 10^{-5}$ & $2.28 [1.54, 3.01] \cdot 10^{-5}$ &$3.78 [2.81, 4.75] \cdot 10^{-6}$ \\
    & Luna16 & $6.19 [4.18, 8.19] \cdot 10^{-4}$ & $1.07 [0.810, 1.32] \cdot 10^{-4}$ &$5.34 [3.93, 6.75] \cdot 10^{-5}$ \\
    \rowcolor{brown} SSIM & Test set & $0.938 [0.920, 0.955]$ & $0.979 [0.973, 0.985]$ & $0.997 [0.996, 0.997]$ \\
    & Luna16 & $0.908 [0.889, 0.927]$ & $0.960 [0.950, 0.969]$ & $0.983 [0.980, 0.986]$ \\
    \bottomrule
    \end{tabular}
\end{table*}


\subsection{Reader Study}
The resulting mean values for quality, confidence, and artifacts reported by the readers are shown in \autoref{fig:labels}. The labeled mean quality for sparse-view images decreases linearly from roughly “sufficient” to approximately “not diagnostic” for decreasing number of projection views. The mean quality for the post-processed images is similar for 256, 128, and 64 views at “sufficient” or “high.” For 32 views, the mean quality decreases to below “sufficient,” and for 16 to “sufficient” or “impaired.” The tendency for the mean confidence is similar for both sparse-view and post-processed images. For the sparse-view images, the confidence again decreases linearly with decreasing number of views, ranging from “fairly confident” or “very confident” to “not confident at all” or “slightly confident.” The post-processed 256, 128, and 64 view images lead to “fairly” or “very” confident results. For 32 and 16 views, the confidence decreases respectively to below “fairly confident.” The subjective quality and confidence of post-processed images are significantly higher than their unprocessed pairs for 64 and fewer views (\textit{P}<0.001). The presence of artifacts increases for the sparse-view images with fewer views. Fewer sparse-view artifacts are visible for 256 views (“few artifacts”). More artifacts are present in 32 and 16 views (approximately “a lot of artifacts”). For the post-processed images, 256 and 128 views lead to a similar representation of artifacts (“no” or “few artifacts”). The artifacts are further reduced for 64, 32, and 16 views (“no artifacts”). Post-processed images have significantly fewer subjective artifacts than their unprocessed pairs for all projection views (\textit{P}<0.001).\\

%\afterpage{
% Figure environment removed
%\end{center}



The confusion matrices and the corresponding Se and Sp values are shown in Tables \ref{tab:CMs} and \ref{tab:SeSp}, respectively. In some images, incorrect subjective segmentation by the readers resulted in falsely marked pixels in an alternate location. Such cases are counted as FN and mostly appeared for the sparse-view images reconstructed with 16 views. An example of such an inaccurately marked image, as well as a correctly marked image, and an image with an extra perceived nodule, are shown in \autoref{fig:failureAnalysis}.\\


% Figure environment removed

The confusion matrices in Table \ref{tab:CMs} show increasing FN cases with decreasing number of views for the sparse-view images and their post-processed counterparts. This leads to a decreased Se, as seen in Table \ref{tab:SeSp}. The number of FP is mostly independent of the number of views, which leads to Sp values between 0.86 and 1.00. Regarding Se and Sp, there are no notable differences between the tendencies of sparse-view and post-processed images. 256 and 128 views lead to Se values between 0.97 and 1.00 and Sp values between 0.86 and 0.95. For 64 views, there are four falsely classified instances for sparse-view and post-processed cases each. In the case of the sparse-view images, they all count as FN, leading to Se = 0.89 and Sp = 1.00. In the case of the post-processed images, two count as FN and two as FP, resulting in a comparably higher Se and lower Sp (Se = 0.94, Sp = 0.90). For 32 views, the number of FN instances increases to six in both cases, leading to Se = 0.83. For 16 views, the FN value rises to 20 (Se = 0.44) and 29 (Se = 0.19) for sparse-view and post-processed images, respectively.\\

\autoref{fig:DSC} shows the mean DSC and 95\% CI for sparse-view images with and without post-processing by the model. The mean DSC shows only slight differences between sparse-view images with and without post-processing for 32 or more views. For instance, in the case of 64 views, sparse-view images without post-processing resulted in DSC = 0.81 [0.75,0.86], while images post-processed by the model had improved DSC = 0.85 [0.82,0.87] (\textit{P}>0.1). Additionally, the 95\% CI lines show that neither of those differences is statistically significant, as the individual intervals overlap in almost all cases. It must be noted that although no statistically significant discrepancy in segmentation overlap is observed, subjective quality and confidence assessment was markedly higher in the post-processed images of 64 views and fewer (\textit{P}<0.001).\\

% Figure environment removed

\begin{table*}[h!]
\captionof{table}{\small Confusion Matrices for Sparse-view CT Images and their Post-processed Counterpart Images for All
Projection Views Calculated Over 19 Subject-wise Images Presented to Three Readers (n = 57)\\}
\label{tab:CMs}
%\scriptsize
  \small
  \centering
   \begin{tabular}{c|c|ccccccccccccccc}
    \toprule
       \multicolumn{1}{c}{}& \multicolumn{1}{c}{}& \multicolumn{3}{c}{16} & \multicolumn{3}{c}{32} & \multicolumn{3}{c}{64} & \multicolumn{3}{c}{128} & \multicolumn{3}{c}{256} \\
      \multicolumn{1}{c}{}&  \multicolumn{1}{c}{}& \multicolumn{3}{c}{projections} & \multicolumn{3}{c}{ projections} & \multicolumn{3}{c}{projections} & \multicolumn{3}{c}{projections} & \multicolumn{3}{c}{projections}\\
    \midrule
    \rowcolor{brown} \multicolumn{1}{c}{} & \multicolumn{1}{c}{ reader \textbackslash true} & + &  -& sum & + & - & sum & + & - & sum & + & - & sum & + & - & sum\\
    \midrule
    \midrule
    & +& \cellcolor{LGreen}\textcolor{DGreen}{7}& \cellcolor{ORed}2& 9& \cellcolor{LGreen}\textcolor{DGreen}{30}& \cellcolor{ORed}1& 31& \cellcolor{LGreen}\textcolor{DGreen}{34}& \cellcolor{ORed}2& 36& \cellcolor{LGreen}\textcolor{DGreen}{35}& \cellcolor{ORed}2& 37& \cellcolor{LGreen}\textcolor{DGreen}{35}& \cellcolor{ORed}2& 37 \\
    \rowcolor{brown} Processed & - & \cellcolor{LRed}\textcolor{DRed}{29} &\cellcolor{OGreen}19& 48& \cellcolor{LRed}\textcolor{DRed}{6}& \cellcolor{OGreen}20& 26& \cellcolor{LRed}\textcolor{DRed}{2}& \cellcolor{OGreen}19& 21& \cellcolor{LRed}\textcolor{DRed}{1}& \cellcolor{OGreen}19& 20& \cellcolor{LRed}\textcolor{DRed}{1}& \cellcolor{OGreen}19& 20\\
    &sum & 36 & 21& & 36& 21& & 36& 21& & 36& 21& & 36& 21&\\
    \rowcolor{brown} & +& \cellcolor{LGreen}\textcolor{DGreen}{16}& \cellcolor{ORed}0& 16& \cellcolor{LGreen}\textcolor{DGreen}{30}& \cellcolor{ORed}0& 30& \cellcolor{LGreen}\textcolor{DGreen}{32}&\cellcolor{ORed} 0& 32& \cellcolor{LGreen}\textcolor{DGreen}{36}& \cellcolor{ORed}1& 37& \cellcolor{LGreen}\textcolor{DGreen}{36}& \cellcolor{ORed}3& 39\\
    Sparse & - & \cellcolor{LRed}\textcolor{DRed}{20}& \cellcolor{OGreen}21& 41& \cellcolor{LRed}\textcolor{DRed}{6}& \cellcolor{OGreen}21& 27& \cellcolor{LRed}\textcolor{DRed}{4}& \cellcolor{OGreen}21& 25& \cellcolor{LRed}\textcolor{DRed}{0}& \cellcolor{OGreen}20& 20& \cellcolor{LRed}\textcolor{DRed}{0}& \cellcolor{OGreen}18& 18\\
    \rowcolor{brown} &sum & 36 & 21& & 36& 21& & 36& 21& & 36& 21& & 36& 21&\\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[h!]
\captionof{table}{\small Sensitivity and Specificity for Sparse-view CT Images and their Post-processed Counterpart Images for All Projection Views Calculated Over 19 Subject-wise Images Presented to Three Readers (n =
57)\\}
\label{tab:SeSp}
  \small
  \centering
   \begin{tabular}{l|l|ccccc}
    \toprule
     \multicolumn{1}{c}{}& \multicolumn{1}{c}{}& 16 & 32 & 64 & 128 & 256 \\
     \multicolumn{1}{c}{}&  \multicolumn{1}{c}{}& projections & projections & projections & projections & projections\\
    \midrule
    \midrule
    \rowcolor{brown} Processed & Sensitivity & 0.19 & 0.83 & 0.94 & 0.97 & 0.97\\
    & Specificity & 0.90 & 0.95 & 0.90 & 0.90 & 0.90\\
    \rowcolor{brown} Sparse & Sensitivity & 0.44 & 0.83 & 0.89 & 1.00 & 1.00\\
    & Specificity & 0.86 & 1.00 & 1.00 & 0.95 & 0.86\\
    \bottomrule
    \end{tabular}
\end{table*}

%}


\section{Discussion}
To improve the image quality of lung cancer sparse-view CT images, post-processing correction with a dual-frame U-Net based on a residual approach was implemented. External evaluation with a public dataset demonstrated the model’s robustness. Furthermore, a single-blinded reader study determined a trade-off between number of projection views, image quality, and diagnostic confidence. The results suggest that post-processing by the U-Net can reduce the number of views from 2048 to 64 while maintaining diagnostically accurate image quality for nodule detection (Se = 0.94). Although the DSC for the lung nodule segmentations by the readers did not significantly improve for the post-processed images, the sparse-view artifact-corrected images drastically increased the readers’ confidence in detecting lung nodules.\\

It must be noted that every image labeled as “not diagnostic” in terms of image quality or “not
confident at all” in terms of confidence of diagnosis would not be considered in a clinical workflow.
This is especially the case for sparse-view images reconstructed from 16 views but also for some
sparse-view images reconstructed from 32 views. Thus, these instances will not be considered for
further discussion.\\

All images post-processed by the model are labeled with better image quality and confidence of
diagnosis. More precisely, the difference between sparse-view images with and without post-processing is the most prominent result for all assigned labels. It indicates that the radiologists prefer working with the post-processed images over the unprocessed sparse-view ones: They rate their quality higher, see fewer artifacts in the images, and, most importantly, are more confident in their diagnosis. Especially the higher quality and the increased confidence could be accompanied by a shorter processing time and, in the long run, lead to fewer signs of fatigue compared to working with unprocesssed sparse-view images. Since 256, 128, and 64 views lead to very similar results regarding the quality and confidence labels and worse results are achieved with 32 views, 64-view images appear to be the best choice.\\

To define a threshold providing a reasonable trade-off between a reduced number of projection views
and diagnostic value, Sp and Se values should be maximized. Typically values of about 0.95 are chosen
as acceptable thresholds for Sp and Se values. Accordingly, FP and FN values should be minimized: FP
cases should be avoided as these cause unnecessary follow-up procedures, potentially exposing the
patient to more radiation if a full-view scan is required. However, it is of utmost importance to avoid
FN cases since these would lead to afflicted patients not getting diagnosed. Low FP cases are
correlated with high Sp, and low FN values are associated with high Se. Therefore, Sp values of about
0.90 were deemed acceptable in this work. The overall sample size was comparatively small, leading
to a considerable decrease of either Sp or Se given an FP or FN case, respectively. According to these
thresholds, the lowest possible number of projection views allowing reliable diagnosis would be
achieved for post-processed images of 64 views, leading to Se = 0.94 and Sp = 0.90. The selected Se
value is slightly lower than the typical threshold of 0.95. Nonetheless, we consider 64 views an
acceptable threshold due to our small sample size.\\

The mean DSC values did not consistently show a trend of improvement between the post-processed
and the unprocessed sparse-view images. Yet, these findings support the choice of the trade-off
threshold at 64 views: The mean DSC values for the post-processed images of 64 views result in the
greatest improvement over the mean DSC values of their unprocessed counterparts in comparison to
the other projection views.\\

Some limitations were present in this study. In clinical practice, radiologists often search the entire
stack of images for malignancies. The present reader study could have modeled the clinical workflow
more precisely as it only considered single CT images. Including neighboring slices would come closer
to clinical diagnosis based on CT scans and most likely reduce the amount of falsely classified patients.
Furthermore, the sparse-view data generated for this study was obtained using simplified conditions
not reflective of the complex reconstruction processes in clinical settings. Therefore, only the reduced
number of projection views compared to the full-view images can be reported, and an exact measure
of dose reduction is hence unachievable.\\

Overall, the amount of projection views can be reduced by a factor of 32 compared to the full-view
image with post-processing by a dual-frame U-Net, while keeping the diagnostic value and the
confidence of the radiologists at a satisfactory level. Regarding the radiologists' confidence, the images
post-processed with the model lead to drastically better results than the unprocessed sparse-view
images. These findings suggest that post-processed sparse-view CT images by the dual-frame U-Net
could help enable dose-efficient screening for lung cancer detection.

\section*{Acknowledgments}
This work was funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the Länder, the German Research Foundation (GRK2274), as well as by the Technical University of Munich–Institute for Advanced Study.


%%%%%

%Bibliography
%\bibliographystyle{unsrt}  
%\bibliography{references}  
\newpage
\begin{thebibliography}{00}

%1.
\bibitem{WHO}World Health Organization. Cancer. World Health Organization. February 3, 2022. Available at:\ul{https://www.who.int/news-room/fact-sheets/detail/cancer}. Accessed February 10, 2023

%2.
\bibitem{WCRF}World Cancer Research Fund International. Lung cancer. 2022. Available at:
\ul{https://www.wcrf.org/cancer-trends/lung-cancer-statistics/}. Accessed March 20, 2023.

%3.
\bibitem{GEKID}Gesellschaft der epidemiologischen Krebsregister e.V. (GEKID) und Zentrum für Krebsregisterdaten
(ZfKD) im Robert Koch-Institut. Lunge, ICD10 C33-C34. Krebs in Deutschland. 2018. Available at: \ul{https://www.krebsdaten.de/Krebs/DE/Content/Publikationen/Krebs\_in\_Deutschland/kid\_2021/kid\_2021\_c33\_c \\ 34 \_lunge.pdf?\_\_blob=publicationFile}. Accessed Feb 10, 2023.

%4.
\bibitem{ACS}American Cancer Society. Lung cancer. 2023. Available at: \ul{https://www.cancer.org/cancer/lung-
cancer.html}. Accessed February 10, 2023.

%5.
\bibitem{ONKO}Deutsche Krebsgesellschaft. Lungenkrebs / Lungenkarzinom. 2013. Available at: \ul{https://www.krebsgesellschaft.de/onko-internetportal/basis-informationen-krebs/krebsarten/lungenkrebs.html}. Accessed February 10, 2023

%6.
\bibitem{NHS}
National Health Service. Overview - Lung cancer. November 1, 2022. Available at: \ul{https://www.nhs.uk/conditions/lung-cancer/}. Accessed Feb 10, 2023.

%7.
\bibitem{Hamada2014}Hamada N, Fujimichi Y. Classification of radiation effects for dose limitation purposes: history, current situation and future prospects. \textit{J Radiat Res}. 2014;55(4):629-640. DOI: 10.1093/jrr/rru019.

%8.
\bibitem{FDA}US Food and Drug Administration, Center for Devices and Radiological Health. What are the radiation risks from CT? 2018. Available at: \ul{https://www.fda.gov/radiation-emitting-products/medical-x-ray-imaging/what-are-radiation-risks-ct}. Accessed Feb 10, 2023.

%9.
\bibitem{Kudo2013}Kudo H, Suzuki T, Rashed EA. Image reconstruction for sparse-view CT and interior CT-introduction to
compressed sensing and differentiated backprojection. \textit{Quant Imaging Med Surg}. 2013;3(3):147-61. DOI: 10.3978/j.issn.2223-4292.2013.06.01.

%10.
\bibitem{Zhang2018}Zhang Z, Liang X, Dong X, et al. A sparse-view CT reconstruction method based on combination of denseNet and deconvolution. \textit{IEEE Trans Med Imaging}. 2018 ;37(6):1407-1417. DOI:
10.1109/TMI.2018.2823338.

%11.
\bibitem{Jin2016}Jin KH, McCann MT, Froustey E, et al. Deep convolutional neural network for inverse problems in imaging. \textit{IEEE Trans Image Process}. 2016 Jun 15;26(9):4509-5422. DOI: 10.1109/TIP.2017.2713099.

%12.
\bibitem{Han2016}Han Y, Yoo J, Ye JC. Deep residual learning for compressed sensing CT reconstruction via persistent homology analysis. arXiv. November 25, 2016. Available at: \ul{https://arxiv.org/abs/1611.06391}.Accessed February 10, 2023.

%13.
\bibitem{Han2018}Han Y, Ye JC. Framing U-Net via deep convolutional framelets: Application to sparse-view CT. \textit{IEEE Trans Med Imaging}. 2018;37(6):1418-1429. DOI: 10.1109/TMI.2018.2823768

%14.
\bibitem{Koetzier2023}Koetzier LR, Mastrodicasa D, Szczykutowicz TP, et al. Deep learning image reconstruction for CT: Technical principles and clinical prospects. \textit{Radiology}. 2023;306(3):e221257. DOI: 10.1148/radiol.221257.

%15.
\bibitem{Ronneberger2015}Ronneberger O, Fischer P, Brox T. U-Net: Convolutional networks for biomedical image segmentation. 18th International Conference on Medical Image Computing and Computer-Assisted Intervention MICCAI; 2015 Oct 5- Oct 9; Munich, Germany. \textit{Springer, Cham}; November 2015. Proceedings, Part III 18, 234-241 p.

%16.
\bibitem{vanGinneken2016}van Ginneken B, Setio AAA, Jacobs C. Lung nodule analysis 2016; 2016. Available at: \ul{https://luna16.grand-challenge.org}. Accessed January 16, 2020.

%17.
\bibitem{Armato2011}Armato SG 3rd, McLennan G, Bidaut L, et al. The Lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans. \textit{Med
Phys}. 2011;38(2):915-31. DOI: 10.1118/1.3528204.

%18.
\bibitem{vanAarle2015}van Aarle W, Palenstijn WJ, De Beenhouwer J, et al. The ASTRA toolbox: A platform for advanced algorithm development in electron tomography. \textit{Ultramicroscopy}. 2015;157:35-47. DOI: 10.1016/j.ultramic.2015.05.002.

%19.
\bibitem{vanAarle2016}van Aarle W, Palenstijn WJ, Cant J, et al. Fast and flexible X-ray tomography using the ASTRA toolbox. \textit{Opt Express}. 2016;24(22):25129-25147. DOI: 10.1364/OE.24.025129.

%20.
\bibitem{Palenstijn2011}Palenstijn WJ, Batenburg KJ, Sijbers J. Performance improvements for iterative electron tomography reconstruction using graphics processing units (GPUs). \textit{J Struct Biol}. 2011;176(2):250-3. DOI: 10.1016/j.jsb.2011.07.017.

%21.
\bibitem{Keras}Chollet F, et al. Keras. Keras.io; 2015. Available at: \ul{https://keras.io}. Accessed February 10, 2023.

%22.
\bibitem{Tensorflow}Abadi M, Agarwal A, Barham P, et al. TensorFlow: Large-scale machine learning on heterogeneous systems; Published 2015. Available at: \ul{https://www.tensorflow.org/}. Accessed February 10, 2023.

%23.
\bibitem{SSIM} Wang Z, Bovik AC, Sheikh HR, et al. Image quality assessment: from error visibility to structural similarity. \textit{IEEE Trans. Image Process}. 2004; 13(4):600–612. DOI: 10.1109/TIP.2003.819861.

%24.
\bibitem{Parikh2008}Parikh R, Mathai A, Parikh S, et al. Understanding and using sensitivity, specificity and predictive values. \textit{Indian J Ophthalmol}. 2008;56(1):45-50. DOI: 10.4103/0301-4738.37595.

%25.
\bibitem{Dice1945}Dice LR. Measures of the Amount of Ecologic Association Between Species. \textit{Ecology}. 1945; 26(3)3:297–302. DOI: 10.2307/1932409.

%26.
\bibitem{Fleiss2003}Fleiss JL, Levin B, Paik MC. The measurement of interrater agreement. In: Shewart WA, Wilks SS, editors. Statistical methods for rates and proportions. John Wiley \& Sons, Ltd.;2003. p. 598-626.

%27
\bibitem{Wilcoxon} Wilcoxon F. Individual Comparisons by Ranking Methods. Biometrics Bulletin. 1945;1(6):80-83. DOI: \ul{https://doi.org/10.2307/3001968}.

%28.
\bibitem{Virtanen2020}Virtanen P, Gommers R, Oliphant TE, et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing
in Python. \textit{Nat Methods}. 2020; 17(3): 261-272. DOI: 10.1038/s41592-019-0686-2.

%----------------------------------


\end{thebibliography}


\end{document}
