\section{Discussion}

In this work we have presented \ours{}, a method for aligning LLMs using simulated pairwise preference data obtained from prompting the same LLM. \revision{Based on the fundamental idea that we can improve the preference data simulation by not generating response pairs i.i.d.,} \ours{} follows a similar pipeline to RLAIF while adding ideas reminiscent of context distillation. In particular, we simulate pairwise preference data using a positive prompt $p_+$ and a negative prompt $p_-$, aiming to amplify the difference between outputs $o_+, o_-$ by encouraging opposite-directional changes on a desired attribute such as harmlessness. Empirical results on three diverse alignment tasks across multiple model scales for preference data simulation confirm our intuitions that \ours{} can be highly effective, outperforming both RLAIF and context distillation baselines. \revision{Especially at 7B model scale---where we find that RLAIF performs very poorly---\ours{} already works quite decently, potentially enabling researchers and practitioners to experiment with RLAIF-style pipelines much faster and at lower cost.} %We are excited to continue exploring improvements to \ours{} and related approaches as they apply to additional tasks, larger models, and other methods for leveraging preference data.

However, despite our strong empirical results, we think that \ours{} only scratches the surface of what is possible for automatic preference data simulation in RLHF pipelines. For instance, across the several experimental settings in this work, our current RLCD approach benefits from intuitively pushing $o_+$ and $o_-$ farther apart to reduce label noise. In cases where reranking outputs post hoc is easy, or where one has a sufficiently strong scoring LLM to provide accurate labels even close to the classification boundary, one could alternatively attempt to create harder training examples by intentionally pushing $o_+$ and $o_-$ \textit{closer together} compared to whatever RLAIF achieves by random chance. Additionally, it could prove useful to simulate preference labels in formats other than a single binary label, such as by ranking more than two outputs at a time or using more fine-grained annotations on longer outputs, and we are excited to investigate these and other possibilities for continuing to improve automatic data simulation procedures for LLM alignment.

% \todo{discuss experimenting with larger models, other tasks?, combining multiple tasks/qualities (sort of did this already in anthropic harmlessness and outlines)}