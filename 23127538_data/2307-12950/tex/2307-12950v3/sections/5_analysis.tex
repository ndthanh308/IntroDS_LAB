\section{Analysis}

We run two additional analyses to provide further insight into \ours{}. 

\subsection{Preference Model Evaluation}\label{sec:analysis_preference_model}

% \medskip
% \noindent\textbf{Preference Model Evaluation.}
For the harmlessness and helpfulness tasks, we evaluate \ours{}'s preference model compared to \rlaif{}'s on 2000 gold human-labeled preference data examples from \citet{bai2022training}, based on the same prompts as used for preference data simulation. We check average binary prediction accuracy (i.e., whether the gold human-preferred output is assigned higher preference probability) as well as the average probability that each preference model assigns to the gold output.

\begin{table}[htbp]
\small
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
& \multicolumn{2}{c}{\textit{\textbf{Harmlessness Prompts}}}   & \multicolumn{2}{c}{\textit{\textbf{Helpfulness Prompts}}}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Method}     &   \textbf{Acc.} & \textbf{Prob.} & \textbf{Acc.} & \textbf{Prob.} \\
\midrule
\rlaif{}\sevenb{}      & 35.6 & 0.492 & 60.6 & 0.508\\
\ours{}\sevenb{}     & \textbf{52.4} & \textbf{0.516} & \textbf{64.4} & \textbf{0.601} \\
\midrule
\rlaif{}\thirtyb{}      & 45.7 & 0.489 & 66.2 & 0.551\\
\ours{}\thirtyb{}     & \textbf{55.9} & \textbf{0.542} & \textbf{66.7} & \textbf{0.628} \\
\bottomrule
\end{tabular}
\caption{Average binary accuracy and probability for favoring gold human-preferred output on harmlessness and helpfulness data, for \rlaif{} and \ours{} preference models. \ours{}'s preference models perform better on both datasets.}
\label{tab:reward_model}
% \vspace{-0.5em}
\end{table}

As shown in Table \ref{tab:reward_model}, \ours{}'s preference models exhibit higher agreement with human preferences compared to \rlaif{}'s, whether measured by binary accuracy or by probability of agreement. 

Perhaps surprisingly, \rlaif{}'s harmlessness preference models actually perform worse than chance, even for \rlaif{}\thirtyb{}, even though \rlaif{}\thirtyb{} performs quite reasonably downstream for mitigating harmful outputs (e.g., examples in Appendix \ref{appendix:output_examples}).\footnote{On the other hand, \rlaif{}\sevenb{}'s downstream performance is quite poor, more closely reflecting its preference model's low agreement with humans.} In fact, this low agreement may not be entirely unexpected, as \citet{bai2022constitutional} also observe that both (1) few-shot prompting for the scoring LLM and (2) well over 10B model scale seem necessary for \rlaif{}'s preference model to achieve higher than chance agreement with humans on harmlessness. It is also not impossible for \rlaif{}\thirtyb{} to successfully mitigate harm downstream despite low preference model agreement with humans, as human labels may also contain errors or biases. See Appendix \ref{appendix:fewshot} for further discussion, as well as experiments with a version of \rlaif{} using few-shot prompts for scoring.

% In fact, this result is perhaps not unexpected, as corroborated to some degree by \citet{bai2022constitutional}; see Appendix \ref{appendix:fewshot} for further discussion, as well as experiments with a few-shot version of \rlaif{}. 

% Even if the preference model's level of human agreement may not correlate perfectly to downstream performance, we suppose that high agreement should be desirable in and of itself. %, which translates into stronger downstream performance. 

% Perhaps surprisingly, \rlaif{}'s harmlessness model performs worse than chance. Upon inspection, we noticed that the preference labels from \rlaif{}'s data generation often seemed counterintuitive. We suspect that asking pretrained LLaMA-7B questions such as e.g., which output is ``less harmful'' may sometimes result in it preferring the worse output simply due to seeing the word ``harmful''; similar phenomena have been observed in e.g., \citet{welbl2021challenges,pei2023preadd}, although the issue may be alleviated in stronger LLMs. Additionally, the harmlessness scoring prompts (following \citet{bai2022constitutional}) encourage helpfulness to some degree as well (Appendix \ref{appendix:prompts_harmlessness}), which may at times be at odds with harmlessness~\citep{bai2022training}.

In any case, \ours{}'s learned preference models do not exhibit the same lower-than-chance human agreement as \rlaif{}'s on the harmlessness prompts. Moreover, \ours{}'s preference models exhibit higher agreement with humans compared to \rlaif{}'s on the helpfulness prompts as well. Even if the preference model's level of human agreement may not correlate perfectly to downstream performance, we suppose that high human agreement should be somewhat desirable in and of itself.

Finally, \ours{}'s preference models make judgments with higher polarity compared to \rlaif{}'s, likely due to our use of discrete binary preference labels as opposed to continuous probabilities (Figure \ref{fig:rlcd_differences}). We explore a version of \rlaif{} that also uses binary preference labels in Appendix \ref{appendix:rlaifnp}.

% \todo{describe results}. \todo{maybe refer to rlaifnp in appendix if needed here} \todo{what this says about our model and the reasons for why it's good}

\subsection{Rescoring Variant of \ours{}}

% \medskip
% \noindent\textbf{Rescoring Ablation.}
We additionally investigate a variant of \ours{}, \oursrescore{}, in which we generate preference data $(o_+, o_-)$ using our prompts $p_+, p_-$ but re-label using the same scoring prompts as in \rlaif{}. We compare pairwise against \ours{} on all three tasks using GPT-4.

\begin{table}[htbp]
\small
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
& \multicolumn{2}{c}{\textit{\textbf{Harmlessness Prompts}}}   & \textit{\textbf{Helpfulness Prompts}}      & \textit{\textbf{Outlining Prompts}}  \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
\textbf{Method}        & \textbf{Harm} & \textbf{Help} & \textbf{Help} & \textbf{Qual}\\
\midrule
% \oursrescore{}      & 14.0  & 24.2 & 13.7 &  11.2 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \oursrescore{}\sevenb{}   & \textbf{86.0} / 14.0  & \textbf{75.8} / 24.2 & \textbf{86.3} / 13.7 &  \textbf{88.8} / 11.2 \\
\midrule
% \oursrescore{}     & 45.4  & 46.8 & \textbf{52.7} &  \textbf{63.6} \\
\ours{}\thirtyb{} vs. \oursrescore{}\thirtyb{}    & \textbf{54.6} / 45.4  & \textbf{53.2} / 46.8 & 47.3 / \textbf{52.7} &  36.4 / \textbf{63.6} \\
\bottomrule
\end{tabular}
\caption{Percentage of outputs preferred in GPT-4 pairwise comparisons for \ours{} vs. \oursrescore{} variant (re-labeling outputs using \rlaif{} scoring prompts). \ours{} dramatically outperforms \oursrescore{} at 7B scale for preference data simulation, but rescoring becomes a viable alternative at 30B scale.}
\label{tab:rescoring_ablation}
% \vspace{-0.5em}
\end{table}

As shown in Table \ref{tab:rescoring_ablation}, \ours{} substantially outperforms \oursrescore{} at 7B model scale for preference data simulation, indicating that labeling $o_+,o_-$ based on the initial prompts $p_+,p_-$ used for output generation is much more effective compared to the post hoc rescoring used in \rlaif{}. At least in the settings we examine, LLaMA-7B appears to be more capable of generating contrasting outputs $o_+, o_-$ than labeling them after the fact. 

However, rescoring becomes a viable alternative at 30B scale, as the scoring LLM becomes more capable of labeling examples closer to the boundary. At such model scales, it may also be possible to run a version of \ours{} that mixes labels from the two options (\ours{} and \oursrescore{}), or to use a method such as PREADD~\citep{pei2023preadd} to modulate the control strength of the prompts $p_+,p_-$ to obtain accurately labeled preference pairs closer to the classification boundary. On the other hand, it may also be the case that the larger effective context window requirement for post hoc labeling (Sec. \ref{sec:methods_motivation}) could cause \oursrescore{}'s performance to degrade compared to \ours{} when $o_+, o_-$ are much longer than in our current experiments, even when using LLaMA-30B.

% \todo{describe results}. \todo{what this says about our model and the reasons for why it's good}
