We propose \oursfull{} (\ours{}), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. 
% Different from reinforcement learning from AI feedback (RLAIF), 
\ours{} creates preference pairs from two contrasting model outputs, one using a positive prompt designed to encourage following the given principles, and one using a negative prompt designed to encourage violating them. Using two different prompts causes model outputs to be more differentiated on average, resulting in cleaner preference labels in the absence of human annotations. We then use the preference pairs to train a preference model,  
% \ours{} trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is 
which is in turn used to improve a base unaligned language model via reinforcement learning. Empirically, \ours{} outperforms RLAIF~\citep{bai2022constitutional} and context distillation~\citep{huang2022large} baselines across three diverse alignment tasks---harmlessness, helpfulness, and story outline generation---and when using both 7B and 30B model scales for simulating preference data.
% \violet{it's interesting that you only have citations for CD but not RLAIF.}

% Empirically, \ours{} outperforms both RLAIF and context distillation~\cite{huang2022large} baselines on three diverse tasks---harmlessness, helpfulness, and story outline generation---by at least 9\% absolute in all pairwise comparisons, and often more than 30\%.

%\todo{silly alternative name: RLNF, for RL with No Feedback}

