\section{Introduction}

% Figure environment removed

Reinforcement Learning from Human Feedback (RLHF) has recently been used to great effect to align pretrained large language models (LLMs) to human preferences, optimizing for desirable qualities like harmlessness and helpfulness~\citep{bai2022training} and achieving state-of-the-art results across a variety of natural language tasks~\citep{openai2023gpt4}. %RLHF approaches fundamentally rely on collecting pairs of LLM outputs $(o_1, o_2)$ from a shared prompt $p$, with a human indicating which output in each pair is better on a specified attribute.
% A fundamental component of RLHF is a preference model derived from human labels, typically formatted as pairs of LLM outputs $(o_1, o_2)$ generated from a shared prompt $p$.

A standard RLHF procedure fine-tunes an initial unaligned LLM using an RL algorithm such as PPO~\citep{schulman2017proximal}, optimizing the LLM to align with human preferences. %\violet{not sure whether we need to provide this detail in the intro, especially this has nothing to do with our contribution.} % i feel like this context is useful later when e.g. explaining that context distillation is SFT
RLHF is thus critically dependent on a reward model derived from human-labeled preferences, typically \textit{pairwise preferences} on LLM outputs $(o_1, o_2)$ generated from a shared prompt $p$. % and labeled by humans. 

However, collecting human pairwise preference data, especially high-quality data, may be expensive and time consuming at scale. To address this problem, approaches have been proposed to obtain labels without human annotation, such as Reinforcement Learning from AI Feedback (RLAIF) and context distillation. 

\iffalse
raising the question of whether we can generate high-quality data for RLHF without using human labeling. %accurately-labeled preference pairs $(o_1, o_2)$
%, motivating model alignment approaches that aim to generate accurately-labeled preference pairs $(o_1, o_2)$ without human involvement. 
Two major categories of such approaches are . 
\fi

RLAIF approaches (e.g.,~\citet{bai2022constitutional}) simulate human pairwise preferences by scoring $o_1$ and $o_2$ with an LLM (Figure \ref{fig:rlcd_differences} center); the scoring LLM is often the same as the one used to generate the original pairs $(o_1, o_2)$. Of course, the resulting LLM pairwise preferences will be somewhat noisier compared to human labels. However, this problem is exacerbated by using the same prompt $p$ to generate both $o_1$ and $o_2$, causing $o_1$ and $o_2$ to often be of very similar quality and thus hard to differentiate (e.g., Table~\ref{tab:rlaif_bad_example}). Consequently, training signal can be overwhelmed by label noise, yielding lower-quality preference data. 

% While it avoids human labeling efforts, it has weakness. First, LLM preference labels will naturally be somewhat noisier compared to human labels. Furthermore, since the same prompt $p$ is used to generate both $o_1$ and $o_2$, their quality is often very similar and hard to differentiate (See Table~\ref{tab:rlaif_bad_example}). As a result, training signals can be overwhelmed by label noise, yielding lower-quality preference data. 

Meanwhile, context distillation methods (e.g., \citet{sun2023principle}) create more training signal by modifying the initial prompt $p$. 
%to create more significant training signal. 
The modified prompt $p_+$ typically contains additional context encouraging a \textit{directional attribute change} in the output $o_+$ (Figure \ref{fig:rlcd_differences} right). However, context distillation methods only generate a single output $o_+$ per prompt $p_+$, which is then used for supervised fine-tuning, losing the pairwise preferences which help RLHF-style approaches to 
%rather than using a RLHF-style preference model to 
derive signal from the contrast between outputs. 
Multiple works have observed that RL approaches using preference models for pairwise preferences can substantially improve over supervised fine-tuning by itself when aligning LLMs~\citep{ouyang2022training,dubois2023alpacafarm}. 

% conduct alignment by running supervised fine-tuning on model outputs $o_+$ generated from a modified prompt $p_+$. $p_+$ typically contains additional context encouraging desirable attributes (Figure \ref{fig:rlcd_differences} right), such as in \citet{sun2023principle}. However, multiple works have observed that RLHF-style approaches can substantially improve over supervised fine-tuning by itself when aligning LLMs~\citep{ouyang2022training,dubois2023alpacafarm}. 

Therefore, while both RLAIF and context distillation approaches have already been successfully applied in practice to align language models, we posit that it may be even more effective to combine the key advantages of both. That is, we will use RL with \textit{pairwise preferences}, while also using modified prompts to encourage \textit{directional attribute change} in outputs. %In particular, we will adapt the RLAIF data generation process with two different prompts rather than a single $p$, modifying both prompts similarly to context distillation. %\violet{this motivation is a little unexciting. I think we can more specifically discuss the potential benefits of our approach, like the benefits from RL: exploration/data generation; benefits from contrast. I don't think we get too much benefits from context distillation since we switched to the RL framework.} 

Concretely, we propose \oursfull{} (\ours{}). 
\ours{} generates preference data as follows. Rather than producing two i.i.d.\ model outputs $(o_1, o_2)$ from the same prompt $p$ as in RLAIF, \ours{} creates two variations of $p$: a \textit{positive prompt} $p_+$ similar to context distillation which encourages directional change toward a desired attribute, and a \textit{negative prompt} $p_-$ which encourages directional change \textit{against} it (Figure \ref{fig:rlcd_differences} left). We then generate model outputs $(o_+, o_-)$ respectively, and automatically label $o_+$ as preferred---that is, \ours{} automatically ``generates'' pairwise preference labels by construction. %, without further post hoc labeling.\violet{should make it clearer that our approach `generates' labels by construction} 
We then follow the standard RL pipeline of training a preference model followed by PPO. 

Compared to RLAIF-generated preference pairs $(o_1, o_2)$ from the same input prompt $p$, there is typically a clearer difference in the quality of $o_+$ and $o_-$ generated using \ours{}'s directional prompts $p_+$ and $p_-$, which may result in less label noise. %which may result in better training signal for the preference model. 
That is, intuitively, \ours{} exchanges having examples be \textit{closer to the classification boundary} for much more \textit{accurate labels} on average. Compared to standard context distillation methods, on top of leveraging pairwise preferences for RL training, \ours{} can derive signal not only from the positive prompt $p_+$ which improves output quality, but also from the negative prompt $p_-$ which degrades it. %\ours{} is not learning to imitate $o_+$, but to distill the \textit{contrast} between $o_+$ and $o_-$. 
Positive outputs $o_+$ don't need to be perfect; they only need to contrast with $o_-$ on the desired attribute while otherwise following a similar style.

% \todo{discuss our method and why intuitively it may be better.}

We evaluate the practical effectiveness of \ours{} through both human and automatic evaluations on three tasks, aiming to improve the ability of LLaMA-7B~\citep{touvron2023llama} to generate harmless outputs, helpful outputs, and high-quality story outlines. %\ours{} outperforms both RLAIF and context distillation baselines in pairwise comparisons on 
As shown in Sec. \ref{sec:experiments}, \ours{} substantially outperforms both RLAIF and context distillation baselines in pairwise comparisons when simulating preference data with LLaMA-7B, while still performing equal or better when simulating with LLaMA-30B. 
%On all three tasks, \ours{} substantially outperforms both RLAIF and context distillation baselines in pairwise comparisons---by a margin of at least 9\% and often more than 30\%---validating our method's efficacy. 
We will release all code at a later date, although in any case \ours{} is fairly easy to implement by modifying any reference RLAIF codebase. %We release all code at \todo{github link}.