\section{Related Work}

Lately, several RL approaches leveraging reward models trained on human preferences~\citep{ouyang2022training,bai2022training,zhu2023principled,rafailov2023direct} have been applied to align strong pretrained LLMs~\citep{stiennon2020learning,openai2022chatgpt,openai2023gpt4,anthropic2023claude,touvron2023llamatwo}. However, it can be expensive to collect human pairwise preferences.

\medskip
\noindent\textbf{Reinforcement Learning from AI Feedback.} %One natural modification to RLHF to lessen dependence on human-labeled data is 
RLAIF simulates human pairwise preferences using a LLM, whether the same LLM to be aligned later~\citep{bai2022constitutional} or a stronger LLM as an oracle~\citep{dubois2023alpacafarm}. Such methods typically obtain pairwise preferences by scoring two i.i.d.\ outputs $(o_1, o_2)$. \ours{} instead generates outputs $(o_+, o_-)$ from different distributions, obviating the need for post hoc scoring (whether human or AI). %\todo{find more citations?}

\medskip
\noindent\textbf{Context Distillation.} \ours{} is related to context distillation approaches, which generate data for supervised fine-tuning by prompting a language model with different contexts~\citep{askell2021general,choi2022prompt,snell2022learning,huang2022large}. In contrast to knowledge distillation approaches using stronger models as a teacher~\citep{kim2016sequence,chang2023learning}, context distillation methods often generate data using the same LLM being aligned or fine-tuned later. In particular, \citet{sun2023principle} apply this approach to align LLaMA-65B~\citep{touvron2023llama}. Unlike existing context distillation approaches, \ours{} generates pairwise preference data to train a preference model followed by applying RL. Consequently, \ours{} can derive training signal from the \textit{contrast} in output distributions for two different context-modified prompts $p_+$ and $p_-$. %\todo{find more citations?}

\medskip
\noindent\textbf{Reinforcement Learning with Contrastive Objective.} Using a contrastive loss in RL has proven effective in various scenarios~\citep{oord2018representation,laskin2020curl,liu2021return,laskin2022cic,eysenbach2022contrastive}. Compared to standard reward signals that may lead to insufficient numerical differences between good and bad cases, contrastive loss naturally focuses on sample pairs with similar appearances but different underlying semantics with current representations~\citep{tian2022understanding}, thus improving sample efficiency and model quality. \ours{} employs a similar idea to improve the generation of simulated preference data in the RLHF pipeline.

% \todo{any other categories of stuff i should discuss? maybe some related RL works using contrastive ideas? https://arxiv.org/pdf/2306.11816.pdf and also some knowledge distillation stuff from stronger models which we don't use}