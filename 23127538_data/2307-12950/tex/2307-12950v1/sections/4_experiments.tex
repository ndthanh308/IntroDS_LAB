\section{Experiments}\label{sec:experiments}

We now evaluate \ours{}'s effectiveness in practice.

\medskip
\noindent\textbf{Tasks.} We run experiments on three tasks, corresponding to three different sets of prompts:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item \textit{Harmlessness Prompts.} Dialogues frequently containing offensive or otherwise socially unacceptable text. The goal is to generate outputs that are socially acceptable, ethical, and/or inoffensive, even when given such toxic context. As a secondary goal, the outputs should still be helpful and relevant to the conversation, rather than generic meaningless responses like ``Thank you!'' and ``Sorry.''
    \item \textit{Helpfulness Prompts.} Dialogues where the human is typically asking for information or advice. The goal is to generate outputs that are helpful.
    \item \textit{Outlining Prompts.} Dialogues where the human provides a story premise and asks for an outline. The goal is to write a well-formed and interesting story outline for the premise.
\end{enumerate}

All prompts are framed as generating the next assistant response at some point in the given human-assistant conversation, as shown in e.g., ``Initial Prompts'' and ``Completions'' in Figure \ref{fig:rlcd_differences}.

Our harmlessness and helpfulness prompt sets are inspired by \citet{bai2022training}, and we use their training sets to derive the initial prompts for preference data simulation; each training set contains slightly over 40000 conversations.\footnote{It is likely that these initial prompts could also be generated procedurally from a much smaller seed set~\citep{bai2022training,sun2023principle}, although we do not empirically investigate this possibility in this work.} We also include the outlining prompt set because we believe it may have higher requirements on long-range planning, in addition to simultaneously composing multiple different attributes (e.g., interestingness, well-formedness, relevance to the premise). 
For the outlining prompts we leverage 40000 existing premises from the internet, 
%Prompts for the outlining task are derived from 40000 %premises from WritingPrompts~\citep{fan2018hierarchical},
and assistant responses automatically start with ``Here is a possible outline:\textbackslash n\textbackslash n1.'' to encourage correct basic formatting regardless of which method is being evaluated.

\medskip
\noindent\textbf{\ours{} Positive and Negative Prompts.}
For the harmlessness task, we write 16 pairs of context phrases for constructing $p_+$ and $p_-$ (sampling a random pair for each use); these pairs are written to be similar to the 16 scoring prompts used in \citet{bai2022constitutional}, who implement RLAIF for harmlessness. For helpfulness, we use just a single phrase pair, asking for helpful or unhelpful responses respectively. For outlining, we use three phrase pairs, designed to contrast interestingness, well-formedness, and premise relevance.

For harmlessness and helpfulness, we create training signal while roughly matching the surface forms of $p_+$ and $p_-$ by simply placing contrasting descriptions in parentheses before the colon in ``Assistant:'' indicators, as shown for example in Figure \ref{fig:rlcd_differences}. In the outlining task, we end all prompts with ``1.'' to indicate the beginning of a numbered outline. All prompts are zero-shot.
See Appendix \ref{appendix:prompts} for full details on preference data simulation prompt formats.

\medskip
\noindent\textbf{\ours{} Implementation and Hyperparameters.}
For each task we run two variations of \ours{}---\ours{}\sevenb{} and \ours{}\thirtyb{}---which simulate preference data using the base (pretrained, unaligned) LLaMA-7B and LLaMA-30B respectively. As \ours{} is a method for simulating preference data, but does not touch the downstream preference model and PPO training, we use base LLaMA-7B as the initial LLM to be aligned via \ours{} regardless of the model used in preference data simulation.\footnote{Alternatively, simulating preference data with LLaMA-30B while aligning LLaMA-7B downstream can be viewed as distillation from a teacher model. In this view, we are evaluating \ours{} compared to baselines on effectiveness when used for model distillation.} 

Our implementation is based on the AlpacaFarm codebase~\citep{dubois2023alpacafarm}. We optimize the training parameters for PPO, in particular the number of training steps and KL-regularization term, following \citet{dubois2023alpacafarm}'s observations that these parameters can have some impact on final output quality. We otherwise use AlpacaFarm's default hyperparameters for PPO and for supervised fine-tuning; see Appendix \ref{appendix:hyperparameters} for full details on hyperparameters.



% \begin{table*}[htbp]
% \small
% \centering
% \begin{tabular}{@{}lcccccccc@{}}
% \toprule
% & \multicolumn{4}{c}{\textit{\textbf{Harmless}}}   & \multicolumn{2}{c}{\textit{\textbf{Helpful}}}      & \multicolumn{2}{c}{\textit{\textbf{Outline}}}  \\ 
% \cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
% \textbf{Method}        & \textbf{Harm\emojismile{}} & \textbf{Harm\emojirobot{}} & \textbf{Help\emojismile{}} &\textbf{Help\emojirobot{}} & \textbf{Help\emojismile{}} & \textbf{Help\emojirobot{}} & \textbf{Qual\emojismile{}} &\textbf{Qual\emojirobot{}}\\
% \midrule
% \base{}      & 3.56 & 17.2 & 3.70 & 23.0 & 2.48 & \phantom{0}9.3 & 2.98 & 24.0\\
% \ours{}\sevenb{}     & \textbf{5.44} & \textbf{82.8} & \textbf{5.30} & \textbf{77.0} & \textbf{6.52} & \textbf{90.7} & \textbf{6.02} & \textbf{76.0}  \\
% \midrule
% \rlaif{}\sevenb{}      & 3.38 & 15.2 & 4.36 & 29.0 & 3.12 & 14.6 & 3.03 & 21.5 \\
% \ours{}\sevenb{}     & \textbf{5.62} & \textbf{84.8} & \textbf{4.64} & \textbf{71.0} & \textbf{5.88} & \textbf{85.4} & \textbf{5.97} & \textbf{78.5} \\
% \midrule
% \contextdist{}\sevenb{}      & 4.49 & 30.3 & 4.31 & 32.3 & 3.27 & 10.5 & 3.33 & 28.2 \\
% \ours{}\sevenb{}     & \textbf{4.51} & \textbf{69.7} & \textbf{4.69} & \textbf{67.7} & \textbf{5.73} & \textbf{89.5} & \textbf{5.67} & \textbf{71.8} \\
% \midrule
% \base{}        & 3.41 & 21.1 & 3.55 & 21.7 & 2.58 & 18.7 & 3.97 & 44.3 \\
% \ours{}\thirtyb{}      & \textbf{5.59} & \textbf{78.9} & \textbf{5.45} & \textbf{78.3} & \textbf{6.42} & \textbf{81.3} & \textbf{5.03} & \textbf{55.7} \\
% \midrule
% \rlaif{}\thirtyb{}       & 4.29 & 39.7 & 4.50 & 44.7 & 4.49 & \textbf{52.2} & 4.24 & \textbf{64.1} \\
% \ours{}\thirtyb{}        & \textbf{4.71} & \textbf{60.3} & 4.50 & \textbf{55.3} & \textbf{4.51} & 47.8 & \textbf{4.76} & 35.9 \\
% \midrule
% \contextdist{}\thirtyb{} & 4.20 & 35.5 & 4.12 & 30.7 & 3.28 & 25.1 & 3.22 & 37.8 \\
% \ours{}\thirtyb{}        & \textbf{4.80} & \textbf{64.3} & \textbf{4.88} & \textbf{69.3} & \textbf{5.72} & \textbf{74.9} & \textbf{5.78} & \textbf{62.2} \\
% \bottomrule
% \end{tabular}
% \caption{Pairwise comparisons for \ours{} against each baseline when conducting alignment on LLaMA-7B, showing average human (\emojismile{}) comparisons on a 1-8 scale, and automatic (\emojirobot{}) comparison winrate for each attribute. Subscripts indicate LLaMA size used for preference data simulation. %Human numbers are average preference labels on a scale of 1 (dispreferred) to 8 (preferred), while automatic numbers are the percentage of outputs from each method preferred by a ; higher is better for both. 
% \ours{} substantially outperforms baselines 
%  according to both human and automatic evaluations when simulating preference data using LLaMA-7B, while remaining equal or better when simulating using LLaMA-30B.%\todo{double check everything once we get the rest of the results}
% }
% \label{tab:main_results}
% \vspace{-0.5em}
% \end{table*}

% \begin{table*}[htbp]
% \small
% \centering
% \begin{tabular}{@{}lcccccccc@{}}
% \toprule
% & \multicolumn{4}{c}{\textit{\textbf{Harmless}}}   & \multicolumn{2}{c}{\textit{\textbf{Helpful}}}      & \multicolumn{2}{c}{\textit{\textbf{Outline}}}  \\ 
% \cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
% \textbf{Method}        & \textbf{Harm\emojismile{}} & \textbf{Harm\emojirobot{}} & \textbf{Help\emojismile{}} &\textbf{Help\emojirobot{}} & \textbf{Help\emojismile{}} & \textbf{Help\emojirobot{}} & \textbf{Qual\emojismile{}} &\textbf{Qual\emojirobot{}}\\
% \midrule
% \base{}        & 3.41 & 21.1 & 3.55 & 21.7 & 2.58 & 18.7 & 3.97 & 44.3 \\
% \ours{}-30B        & \textbf{5.59} & \textbf{78.9} & \textbf{5.45} & \textbf{78.3} & \textbf{6.42} & \textbf{81.3} & \textbf{5.03} & \textbf{55.7} \\
% \midrule
% \rlaif{}-30B       & 4.29 & 39.7 & 4.50 & 44.7 & 4.49 & \textbf{52.2} & 4.24 & \textbf{64.1} \\
% \ours{}-30B        & \textbf{4.71} & \textbf{60.3} & 4.50 & \textbf{55.3} & \textbf{4.51} & 47.8 & \textbf{4.76} & 35.9 \\
% \midrule
% \contextdist{}-30B & 4.20 & 35.5 & 4.12 & 30.7 & 3.28 & 25.1 & 3.22 & 37.8 \\
% \ours{}-30B        & \textbf{4.80} & \textbf{64.3} & \textbf{4.88} & \textbf{69.3} & \textbf{5.72} & \textbf{74.9} & \textbf{5.78} & \textbf{62.2} \\
% \bottomrule
% \end{tabular}
% \caption{LLaMA-30B pairwise comparisons for \ours{} against each baseline, showing percentage of outputs preferred in both human (\emojismile{}) and automatic (\emojirobot{}) evaluations for each attribute. %Human numbers are average preference labels on a scale of 1 (dispreferred) to 8 (preferred), while automatic numbers are the percentage of outputs from each method preferred by a ; higher is better for both. 
% Compared to baselines, \ours{} performs substantially better on harmlessness without sacrificing helpfulness. \ours{} also performs substantially better on helpfulness alone, as well as outlining. %\todo{double check everything once we get the rest of the results}
% }
% \label{tab:main_results_30b}
% \vspace{-0.5em}
% \end{table*}

% \begin{table*}[htbp]
% \small
% \centering
% \begin{tabular}{@{}lcccccccc@{}}
% \toprule
% & \multicolumn{2}{c}{\textit{\textbf{Harmless}}}   & \multicolumn{1}{c}{\textit{\textbf{Helpful}}}      & \multicolumn{1}{c}{\textit{\textbf{Outline}}} & \multicolumn{2}{c}{\textit{\textbf{Harmless}}}   & \multicolumn{1}{c}{\textit{\textbf{Helpful}}}      & \multicolumn{1}{c}{\textit{\textbf{Outline}}}  \\ 
% \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}
% \textbf{Method}        & \textbf{Harm\emojismile{}}  & \textbf{Help\emojismile{}}  & \textbf{Help\emojismile{}} & \textbf{Qual\emojismile{}} & \textbf{Harm\emojirobot{}} &\textbf{Help\emojirobot{}} &\textbf{Help\emojirobot{}}  &\textbf{Qual\emojirobot{}}\\
% \midrule
% \base{}      & 3.90 & 3.92 & 2.48 & 2.98 & 31.8 & 31.8 & \phantom{0}9.3 & 23.8\\
% \ours{}     & \textbf{5.10} & \textbf{5.08}& \textbf{6.52}& \textbf{6.02}& \textbf{68.2}  & \textbf{68.2}  & \textbf{90.7}  & \textbf{76.2}  \\
% \midrule
% \rlaif{}      & 3.87 & 4.42& 3.85& 3.56& 24.8  & 35.8  & 25.8  & 23.0 \\
% \ours{}     & \textbf{5.13} & \textbf{4.58}& \textbf{5.15}& \textbf{5.44}& \textbf{75.2}  & \textbf{64.2}  & \textbf{74.2}  & \textbf{77.0} \\
% \midrule
% \contextdist{}      & 4.25 & 3.75& 3.27& 3.33& 33.8  & 31.3  & 12.3  & 28.3 \\
% \ours{}     & \textbf{4.75} & \textbf{5.25}& \textbf{5.73}& \textbf{5.67}& \textbf{66.2}  & \textbf{68.7}  & \textbf{87.7}  & \textbf{71.7} \\
% \bottomrule
% \end{tabular}
% \caption{Pairwise comparisons with both human (\emojismile{}) and automatic (\emojirobot{}) evaluations for \ours{} against each baseline. Human numbers are average preference labels on a scale of 1 (dispreferred) to 8 (preferred) as described in \citet{rajani2023llm_labels}, while automatic numbers are the percentage of outputs from each method preferred by a; higher is better for both. Compared to baselines, \ours{} performs substantially better on harmlessness without sacrificing helpfulness. \ours{} also performs substantially better on helpfulness alone, as well as outlining.}
% \label{tab:main_results}
% \vspace{-0.5em}
% \end{table*}


\medskip
\noindent\textbf{Baselines.}
We compare \ours{} to three baselines:

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item \base{}, i.e., just directly generating outputs using the base unaligned LLaMA-7B (the same initial LLM to be aligned by \ours{} and other baselines), included as a sanity check.
\item \rlaif{}, following Constitutional AI~\citep{bai2022constitutional}. Since their code and models are non-public, we re-implement using AlpacaFarm. We use the exact same prompt templates as \citet{bai2022constitutional} for harmlessness scoring, although we use zero-shot prompting to match \ours{}. For helpfulness and outlining scoring we use prompts as similar as possible to those used in \ours{} (Appendix \ref{appendix:prompts}).
\item \contextdist{}, a context distillation baseline which conducts supervised fine-tuning on only the outputs $o_+$ from positive prompts $p_+$ in \ours{}.
\end{enumerate}

As with \ours{}, we experiment with simulating preference data using both LLaMA-7B and LLaMA-30B for \rlaif{} and \contextdist{} (again denoted by subscripts, e.g., \rlaif{}\sevenb{}), though the base model to be aligned remains LLaMA-7B in all cases.

\medskip
\noindent\textbf{Metrics.}
For each task, we run pairwise evaluations for \ours{} compared to each baseline. As the harmlessness prompts from \citet{bai2022constitutional}---while focusing primarily on harmlessness---additionally encourage helpfulness to some degree (Appendix \ref{appendix:prompts_harmlessness}), we measure both harmlessness (\textit{Harm}) and helpfulness (\textit{Help}) for the harmlessness task.\footnote{See Appendix \ref{appendix:focused_harmlessness} for a version of the harmlessness task which focuses more exclusively on harmlessness.} For the helpfulness and outlining tasks we collect just one set of labels for overall helpfulness (\textit{Help}) and outline quality (\textit{Qual}) respectively. 

For each pairwise evaluation, we ask humans to compare 200 examples on a Likert scale from 1 (output A is much better) to 8 (output B is much better) following Anthropic as described in \citet{rajani2023llm_labels}. %, binary comparisons of 200 outputs from test set prompts via both human evaluations %\footnote{Our human annotations are actually on a scale from 1 to 8 following Anthropic as described in \citet{rajani2023llm_labels}, but we binarize them for interpretability. See Appendix \ref{appendix:raw_human_scores} for the raw scores, which yield similar qualitative conclusions.} 
We additionally query GPT-4 for binary judgments on 1000 examples. Examples are always presented in random order during pairwise comparison, with human scores normalized afterward so that higher is better.\footnote{For example, if the human label is 3 (output A is moderately better), then output B's normalized score is 3 while output A's normalized score is 6.} See Appendix \ref{appendix:evaluation_details} for complete evaluation details. 
    
\begin{table*}[htbp]
\small
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
& \multicolumn{2}{c}{\textit{\textbf{Harmlessness Prompts}}}   & \textit{\textbf{Helpfulness Prompts}}      & \textit{\textbf{Outlining Prompts}}  \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
\textbf{Methods}        & \textbf{Harm} & \textbf{Help} &\textbf{Help}  &\textbf{Qual}\\
\midrule
% \base{}      & 3.56 & 3.70 & 2.48 & 2.98 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \base{}  & \textbf{5.44} / 3.56 & \textbf{5.30} / 3.70 & \textbf{6.52} / 2.48 & \textbf{6.02} / 2.98 \\
\midrule
% \rlaif{}\sevenb{}      & 3.38 & 4.36 & 3.12 & 3.03 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \rlaif{}\sevenb{}    & \textbf{5.62} / 3.38 & \textbf{4.64} / 4.36 & \textbf{5.88} / 3.12  & \textbf{5.97} / 3.03 \\
\midrule
% \contextdist{}\sevenb{}      & 4.49 & 4.31 & 3.27 & 3.33 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \contextdist{}\sevenb{}     & \textbf{4.51} / 4.49 & \textbf{4.69} / 4.31 & \textbf{5.73} / 3.27  & \textbf{5.67} / 3.33 \\
\toprule
% \base{}        & 3.41 & 3.55 & 2.58 & 3.97 \\
\ours{}\thirtyb{}  vs. \base{}    & \textbf{5.59} / 3.41 & \textbf{5.45} / 3.55 & \textbf{6.42} / 2.58 & \textbf{5.03} / 3.97 \\
\midrule
% \rlaif{}\thirtyb{}       & 4.29 & 4.50 & 4.49 & 4.24 \\
\ours{}\thirtyb{}  vs. \rlaif{}\thirtyb{}      & \textbf{4.71} / 4.29 & 4.50 / 4.50 & \textbf{4.51} / 4.49 & \textbf{4.76} / 4.24 \\
\midrule
% \contextdist{}\thirtyb{} & 4.20 & 4.12 & 3.28 & 3.22 \\
\ours{}\thirtyb{} vs. \contextdist{}\thirtyb{}       & \textbf{4.80} / 4.20 & \textbf{4.88} / 4.12 & \textbf{5.72} / 3.28 & \textbf{5.78} / 3.22 \\
\bottomrule
\end{tabular}
\caption{Human comparison results for \ours{} against each baseline, evaluating harmlessness and helpfulness on harmlessness prompt set; helpfulness on helpfulness prompt set; and outline quality on story outlining prompt set. Annotators indicated which output was better, and by how much, on a 1-8 scale; scores here are normalized so that higher is better. \ours{} is in all cases equal or better---often substantially better---compared to baselines, for all tasks and for preference data simulation at both 7B and 30B model scale.%Subscripts indicate LLaMA size used for preference data simulation. %Human numbers are average preference labels on a scale of 1 (dispreferred) to 8 (preferred), while automatic numbers are the percentage of outputs from each method preferred by a ; higher is better for both. 
% \ours{} substantially outperforms baselines 
%  according to both human and automatic evaluations when simulating preference data using LLaMA-7B, while remaining equal or better when simulating using LLaMA-30B.%\todo{double check everything once we get the rest of the results
}
\label{tab:main_results_human}
% \vspace{-0.5em}
\end{table*}


\begin{table*}[htbp]
\small
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
& \multicolumn{2}{c}{\textit{\textbf{Harmlessness Prompts}}}   & \textit{\textbf{Helpfulness Prompts}}      & \textit{\textbf{Outlining Prompts}}  \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
\textbf{Methods}        & \textbf{Harm} & \textbf{Help} &\textbf{Help}  &\textbf{Qual}\\
\midrule
% \base{}      & 3.56 & 3.70 & 2.48 & 2.98 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \base{}  & \textbf{82.8} / 17.2 & \textbf{77.0} / 23.0 & \textbf{90.7} / \phantom{0}9.3 & \textbf{76.0} / 24.0 \\
\midrule
% \rlaif{}\sevenb{}      & 3.38 & 4.36 & 3.12 & 3.03 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \rlaif{}\sevenb{}    & \textbf{84.8} / 15.2 & \textbf{71.0} / 29.0 & \textbf{85.4} / 14.6  & \textbf{78.5} / 21.5 \\
\midrule
% \contextdist{}\sevenb{}      & 4.49 & 4.31 & 3.27 & 3.33 \\
\ours{}\sevenb{}\padtothirtyb{} vs. \contextdist{}\sevenb{}     & \textbf{69.7} / 30.3 & \textbf{67.7} / 32.3 & \textbf{89.5} / 10.5  & \textbf{71.8} / 28.2 \\
\toprule
% \base{}        & 3.41 & 3.55 & 2.58 & 3.97 \\
\ours{}\thirtyb{}  vs. \base{}    & \textbf{78.9} / 21.1 & \textbf{78.3} / 21.7 & \textbf{81.3} / 18.7 & \textbf{55.7} / 44.3 \\
\midrule
% \rlaif{}\thirtyb{}       & 4.29 & 4.50 & 4.49 & 4.24 \\
\ours{}\thirtyb{}  vs. \rlaif{}\thirtyb{}      & \textbf{60.3} / 39.7 & \textbf{55.3} / 44.7 & 47.8 / \textbf{52.2} & 35.9 / \textbf{64.1} \\
\midrule
% \contextdist{}\thirtyb{} & 4.20 & 4.12 & 3.28 & 3.22 \\
\ours{}\thirtyb{} vs. \contextdist{}\thirtyb{}       & \textbf{64.5} / 35.5 & \textbf{69.3} / 30.7 & \textbf{74.9}  /  25.1 & \textbf{62.2} / 37.8 \\
\bottomrule
\end{tabular}
\caption{Percentage of outputs preferred in GPT-4 binary evaluations when comparing \ours{} to baselines for aligning LLaMA-7B. \ours{}'s outputs are substantially preferred by GPT-4 across all tasks and data simulation model scales, except comparing \ours{}\thirtyb{} to \rlaif{}\thirtyb{}, where performance is relatively equal.
}
\label{tab:main_results_auto}
% \vspace{-0.5em}
\end{table*}


\medskip
\noindent\textbf{Results.}
\ours{} outperforms all of our baselines according to both human (Table \ref{tab:main_results_human}) and GPT-4 (Table \ref{tab:main_results_auto}) evaluations, validating the effectiveness of our proposed data generation procedure at this model scale. On harmlessness prompts, \ours{} is better on the main harmlessness metric, while not sacrificing helpfulness compared to baselines. \ours{} is also much better than baselines on helpfulness by itself as well as the somewhat more complex task of story outlining. 

The gap between \ours{} and all baselines is especially large when using LLaMA-7B for preference data simulation. Even our strongest baseline, \rlaif{}, comes close to \ours{} only at 30B model scale for data simulation, while substantially underperforming at 7B model scale. Even at 30B scale, although GPT-4 prefers \rlaif{}\thirtyb{} to \ours{}\thirtyb{} in some cases, humans always consider \ours{} to be at least equal or better; GPT-4 evaluations may show greater disagreement with humans at this level of model quality (Appendix \ref{appendix:agreement}).

The difference between \ours{} and baselines can be observed qualitatively as well. For instance, the example shown in Table \ref{tab:main_output_example} makes it abundantly clear why \ours{} is so strongly preferred over our baselines on the helpfulness data at 7B model scale for preference data simulation. See Appendix \ref{appendix:output_examples} for additional, i.i.d.\ example outputs for both \ours{} and baselines.

\begin{table}[htbp]
\small
\begin{tabular}{@{}Y{0.95\linewidth}@{}}
\toprule
\texttt{Human: What did Thomas Edison invent?}

\texttt{Assistant:}\\
\midrule
\base{}: \texttt{An alkaline battery.}\\
\midrule
\rlaif{}\sevenb{}: \texttt{Thomas Alva Edison was an American inventor and businessman.}\\
\midrule
\contextdist{}\sevenb{}: \texttt{The light bulb.}\\
\midrule
\ours{}\sevenb{}: \texttt{Edison is often remembered for inventing the light bulb, but he actually invented over a thousand different things throughout his lifetime, including the phonograph, electric power grid, microphone, video camera, and the motion picture camera. Edison was the first to take advantage of incandescent lighting in an industrial setting, opening up a new era of work environments in the 1870s and 1880s. His most well-known invention, however, was the incandescent light bulb, which he invented in 1879 and brought to market by 1880. He saw the potential for widespread use in homes, offices, and street lighting, and the bulb changed the way people lived and worked across the world. Edison had experimented with the incandescent light bulb since the mid-1870s, eventually perfecting the design with a filament made from carbonized thread.}\\
\bottomrule
\caption{Outputs generated by baselines and \ours{} for an example from our helpfulness test set. All methods use the same LLaMA-7B during preference data simulation, but \ours{}'s final output is clearly more comprehensive in this case.}
% \vspace{-2em}
\label{tab:main_output_example}
\end{tabular}
\end{table}

% In general, the gap between \ours{} and baselines is especially large when simulating preference data at 7B model scale, as is readily apparent when inspecting examples such as in Table \ref{tab:main_output_example}. Qualitatively, we observe that \ours{} already exhibits quite strong performance 

% \ours{} continues to exhibit strong performance compared to baselines when simulating preference data with LLaMA-30B, continuing to substantially outperform \contextdist{} on all tasks. While \rlaif{} closes much of the gap compared to 7B scale, and GPT-4 prefers \rlaif{}\thirtyb{} to \ours{}\thirtyb{} in some cases, humans consider \ours{}\thirtyb{} to be equal or better; GPT-4 evaluations are perhaps less reliable at this level of model quality.

% \ours{}'s strong performance holds up upon qualitative inspection as well. For example, on the the harmlessness task, it often mitigates toxicity even in previously toxic dialogue contexts, without becoming completely unhelpful. On the helpfulness task, \ours{}'s outputs are consistently helpful. %On the outlining task, \ours{} generates several reasonable short story outlines. 
% See Appendix \ref{appendix:output_examples} for several example outputs for both \ours{} and all baselines.%\todo{should we include any examples here in main text?}

    % \todo{qualitative summary of results; maybe note human and automatic agreement for later}

    % \todo{examples- refer to appendix output examples, consider including a couple ones here}
        
    % - [ ] summarize results qualitatively, discuss exps / examples