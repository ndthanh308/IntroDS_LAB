We propose \oursfull{} (\ours{}), a method for aligning language models to follow natural language principles without using human feedback. 
% Different from reinforcement learning from AI feedback (RLAIF), 
\ours{} trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, \ours{} outperforms RLAIF~\citep{bai2022constitutional} and context distillation~\citep{huang2022large} baselines across three diverse alignment tasks---harmlessness, helpfulness, and story outline generation---and on both 7B and 30B model scales for preference data simulation.
% \violet{it's interesting that you only have citations for CD but not RLAIF.}

% Empirically, \ours{} outperforms both RLAIF and context distillation~\cite{huang2022large} baselines on three diverse tasks---harmlessness, helpfulness, and story outline generation---by at least 9\% absolute in all pairwise comparisons, and often more than 30\%.

%\todo{silly alternative name: RLNF, for RL with No Feedback}

