\section*{Limitations} 

While we have carefully investigated the effectiveness of \ours{} compared to several baselines on three tasks for LLaMA-7B, and even run experiments with preference data simulation using LLaMA-30B, state-of-the-art pretrained LLMs are still much larger, and we have not yet empirically verified our conclusions when aligning larger pretrained LLMs. It would also be interesting to test other algorithms for leveraging preference data such as DPO~\citep{rafailov2023direct}.

The performance of both \ours{} and baselines also depends on the prompts used for pairwise preference simulation and scoring, so the results could change with different prompts. While it is difficult to entirely eliminate the impact of prompt design on performance, we have attempted to limit this impact in our pairwise comparison experiments by matching the prompt contexts used in \ours{} and baselines where possible (Appendix \ref{appendix:prompts}). We use prompts with similar meanings for \ours{} and \rlaif{}, and use the same $p_+$ for context distillation as in \ours{}. We also use zero-shot prompting throughout our experiments to avoid any influence from few-shot examples.

Additionally, all outputs in this work are limited to 300 tokens, which is sufficient for the overwhelming majority of model outputs in our tasks, but may be insufficient for tasks requiring longer context. It remains to be investigated how well \ours{} works when generating much longer outputs during preference data simulation. 

Finally, all of our prompts, data, and outputs are in English, and we have not tested in other languages. Performance may be worse especially in low-resource languages for which pretrained LLMs have seen less data during pretraining.

% \todo{scaling to larger models than LLaMA-7B is unproven; also, longer contexts / better outlines than the simple ones in this work}

% \todo{is this needed? i suppose it can't hurt?}