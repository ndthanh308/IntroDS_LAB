% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{snell2022learning,
  title={Learning by distilling context},
  author={Snell, Charlie and Klein, Dan and Zhong, Ruiqi},
  journal={arXiv preprint arXiv:2209.15189},
  year={2022}
}

@article{sun2023principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2305.03047},
  year={2023}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2305.14387},
  year={2023}
}

@article{zhang2023wisdom,
  title={The Wisdom of Hindsight Makes Language Models Better Instruction Followers},
  author={Zhang, Tianjun and Liu, Fangchen and Wong, Justin and Abbeel, Pieter and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2302.05206},
  year={2023}
}

@article{chen2023extending,
  title={Extending Context Window of Large Language Models via Positional Interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{anthropic2023claude,
      title={Introducing Claude}, 
      author={Anthropic},
      year={2023},
      url={https://www.anthropic.com/index/introducing-claude}
}

@misc{openai2022chatgpt,
      title={Introducing ChatGPT}, 
      author={OpenAI},
      year={2022},
      url={https://www.openai.com/blog/chatgpt}
}

@misc{mosaic202364k,
      title={Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs}, 
      author={MosaicML},
      year={2023},
      url={https://www.mosaicml.com/blog/mpt-7b}
}

@article{rajani2023llm_labels,
  author = {Rajani, Nazneen and Lambert, Nathan and Han, Sheon and Wang, Jean and Nitski, Osvald and Beeching, Edward and Tunstall, Lewis},
  title = {Can foundation models label data like humans?},
  journal = {Hugging Face Blog},
  year = {2023},
  note = {https://huggingface.co/blog/llm-v-human-data}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{choi2022prompt,
  title={Prompt Injection: Parameterization of Fixed Inputs},
  author={Choi, Eunbi and Jo, Yongrae and Jang, Joel and Seo, Minjoon},
  journal={arXiv preprint arXiv:2206.11349},
  year={2022}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{zhu2023principled,
  title={Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons},
  author={Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2301.11270},
  year={2023}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{eysenbach2022contrastive,
  title={Contrastive learning as goal-conditioned reinforcement learning},
  author={Eysenbach, Benjamin and Zhang, Tianjun and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35603--35620},
  year={2022}
}

@inproceedings{laskin2020curl,
  title={Curl: Contrastive unsupervised representations for reinforcement learning},
  author={Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={5639--5650},
  year={2020},
  organization={PMLR}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{liu2021return,
  title={Return-based contrastive representation learning for reinforcement learning},
  author={Liu, Guoqing and Zhang, Chuheng and Zhao, Li and Qin, Tao and Zhu, Jinhua and Li, Jian and Yu, Nenghai and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2102.10960},
  year={2021}
}

@article{laskin2022cic,
  title={Cic: Contrastive intrinsic control for unsupervised skill discovery},
  author={Laskin, Michael and Liu, Hao and Peng, Xue Bin and Yarats, Denis and Rajeswaran, Aravind and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2202.00161},
  year={2022}
}

@article{laskin2020reinforcement,
  title={Reinforcement learning with augmented data},
  author={Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19884--19895},
  year={2020}
}

@article{tian2022understanding,
  title={Understanding deep contrastive learning via coordinate-wise optimization},
  author={Tian, Yuandong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19511--19522},
  year={2022}
}

@article{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016}
}

@article{chang2023learning,
  title={Learning to Generate Better Than Your LLM},
  author={Chang, Jonathan D and Brantley, Kiante and Ramamurthy, Rajkumar and Misra, Dipendra and Sun, Wen},
  journal={arXiv preprint arXiv:2306.11816},
  year={2023}
}

@article{welbl2021challenges,
  title={Challenges in detoxifying language models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  journal={arXiv preprint arXiv:2109.07445},
  year={2021}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@misc{pei2023preadd,
      title={PREADD: Prefix-Adaptive Decoding for Controlled Text Generation}, 
      author={Jonathan Pei and Kevin Yang and Dan Klein},
      year={2023},
      eprint={2307.03214},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2023lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={arXiv preprint arXiv:2307.03172},
  year={2023}
}

@article{touvron2023llamatwo,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}