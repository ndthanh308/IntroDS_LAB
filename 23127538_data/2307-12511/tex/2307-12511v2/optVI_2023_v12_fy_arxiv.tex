\documentclass[11pt,letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{dsfont}
\usepackage[maxfloats=100]{morefloats}[2015/07/22]% v1.0h
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{enumerate}
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{cuted}
%\usepackage{tabu}
\usepackage{float}
\usepackage[dvipsnames,table,xcdraw]{xcolor}
\usepackage{bbm}
\usepackage{varioref}
\usepackage{hyperref}
\newcounter{counter}[section]                                
\usepackage{amssymb} 
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
\let\proof\relax 
\let\endproof\relax
\usepackage{amsthm}
\DeclareUnicodeCharacter{00A0}{~}
\usepackage{algorithm}
\usepackage{algorithmic}



\newtheorem{assumption}{Assumption}
%\newtheorem{remark}{Remark}\theoremstyle{remark}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\theoremstyle{plain}
\newtheorem{remark}{Remark}
%\newtheorem{proof}{Proof}
%\usepackage{cleveref}
\RequirePackage[capitalize,nameinlink]{cleveref}[0.19]

\newcommand{\fy}[1]{{\color{black}#1}}
\newcommand{\ses}[1]{{\color{black}#1}}

\usepackage[textsize=small]{todonotes}
%%% Blank footnote command is the following:
\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}
\usepackage{tcolorbox}




% Per SIAM Style Manual, "section" should be lowercase
\crefname{section}{section}{sections}
\crefname{subsection}{subsection}{subsections}
\Crefname{section}{Section}{Sections}
\Crefname{subsection}{Subsection}{Subsections}

% Per SIAM Style Manual, "Figure" should be spelled out in references
\Crefname{figure}{Figure}{Figures}

% Per SIAM Style Manual, don't say equation in front on an equation.
\crefformat{equation}{\textup{#2(#1)#3}}
\crefrangeformat{equation}{\textup{#3(#1)#4--#5(#2)#6}}
\crefmultiformat{equation}{\textup{#2(#1)#3}}{ and \textup{#2(#1)#3}}
{, \textup{#2(#1)#3}}{, and \textup{#2(#1)#3}}
\crefrangemultiformat{equation}{\textup{#3(#1)#4--#5(#2)#6}}%
{ and \textup{#3(#1)#4--#5(#2)#6}}{, \textup{#3(#1)#4--#5(#2)#6}}{, and \textup{#3(#1)#4--#5(#2)#6}}

% But spell it out at the beginning of a sentence.
\Crefformat{equation}{#2Equation~\textup{(#1)}#3}
\Crefrangeformat{equation}{Equations~\textup{#3(#1)#4--#5(#2)#6}}
\Crefmultiformat{equation}{Equations~\textup{#2(#1)#3}}{ and \textup{#2(#1)#3}}
{, \textup{#2(#1)#3}}{, and \textup{#2(#1)#3}}
\Crefrangemultiformat{equation}{Equations~\textup{#3(#1)#4--#5(#2)#6}}%
{ and \textup{#3(#1)#4--#5(#2)#6}}{, \textup{#3(#1)#4--#5(#2)#6}}{, and \textup{#3(#1)#4--#5(#2)#6}}

% Make number non-italic in any environment.
\crefdefaultlabelformat{#2\textup{#1}#3}



\title{\LARGE \bf
Improved guarantees for optimal Nash equilibrium seeking and bilevel variational inequalities
}
\author{Sepideh Samadi\thanks{PhD student in the Department of Industrial and Systems Engineering, Rutgers University, Piscataway, NJ 08854, USA  ({\tt\small{sepideh.samadi@rutgers.edu}}).}
\and Farzad Yousefian\thanks{Assistant Professor in the Department of Industrial and Systems Engineering, Rutgers University, Piscataway, NJ 08854, USA
  ({\tt\small{farzad.yousefian@rutgers.edu}}).
 This work was funded in part by the NSF under CAREER grant ECCS-$1944500$, in part by the ONR under grant N$00014$-$22$-$1$-$2757$, and in part by the DOE under grant DE-SC$0023303$.}% <-this % stops a space  
%\thanks{$^{2}$Bernard D. Researcher is with the Department of Electrical Engineering, Wright State University, Dayton, OH 45435, USA {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}
%\scrollmode
\sloppy
\maketitle
\thispagestyle{empty}
\pagestyle{plain}

       



\maketitle
\begin{abstract}We consider a class of hierarchical variational inequality (VI) problems that subsumes VI-constrained optimization and several other important problem classes including the optimal solution selection problem, the optimal Nash equilibrium (NE) seeking problem, and the generalized NE seeking problem. Our main contributions are threefold. (i) We consider bilevel VIs with merely monotone and Lipschitz continuous mappings and devise a single-timescale iteratively regularized extragradient method (IR-EG). We improve the existing iteration complexity results for addressing both bilevel VI and VI-constrained convex optimization problems. (ii) Under the strong monotonicity of the outer level mapping, we develop a variant of IR-EG, called R-EG, and derive significantly faster guarantees than those in (i). These results appear to be new for both bilevel VIs and VI-constrained optimization. (iii) To our knowledge, complexity guarantees for computing the optimal NE in nonconvex settings do not exist. Motivated by this lacuna, we consider VI-constrained nonconvex optimization problems and devise an inexactly-projected gradient method, called IPR-EG, where the projection onto the unknown set of equilibria is performed using R-EG with prescribed adaptive termination criterion and regularization parameters. We obtain new complexity guarantees in terms of a residual map and an infeasibility metric for computing a stationary point. We validate the theoretical findings using preliminary numerical experiments for computing the best and the worst Nash equilibria. 
\end{abstract}

 
 
\section{Introduction}\label{sec:intro}
Consider the class of optimization problems with variational inequality constraints (also known as VI-constrained optimization~\cite{facchinei2014vi}) of the form
\begin{align}\label{eqn:optVI} 
&\hbox{minimize}\quad f(x)\\
&\hbox{subject to}\quad x \in \mbox{SOL}(X,F),\nonumber
\end{align}
where $X \subseteq \mathbb{R}^n$ is a closed convex set, $f:X \to \mathbb{R}$ is a continuously differentiable (possibly nonconvex) function, and $F:X\to \mathbb{R}^n$ is a continuous and  merely monotone mapping. Here, $\mbox{SOL}(X,F)$ denotes the solution set of the variational inequality problem $\mbox{VI}(X,F)$ where
\begin{align}\label{eqn:SOLVI}
\mbox{SOL}(X,F) \triangleq \left\{x \in X \mid F(x)^T(y-x)\geq 0, \ \hbox{for all } y \in X\right\}.
\end{align}
One of the goals in this work is to devise a fully iterative method with complexity guarantees for addressing problem~\cref{eqn:optVI} when $f$ is nonconvex. In addition, when $f$ is merely convex or strongly convex, we aim at considerably improving the existing complexity guarantees for solving this class of problems. Another goal in this work lies in improving the existing performance guarantees in solving a generalization of~\cref{eqn:optVI}, known as the {\it bilevel variational inequality problem}, given as follows.
\begin{align}\label{eqn:bilevelVI} 
&\hbox{Find}\quad x \in \mbox{SOL}(X^*_F,H) \qquad \hbox{where } X^*_F \triangleq \mbox{SOL}(X,F),
\end{align}
where $H:X\to \mathbb{R}^n$ is a continuous and merely monotone mapping. In fact, note that when $f$ is convex and differentiable, \cref{eqn:optVI} is an instance of~\cref{eqn:bilevelVI} where $H(x):=\nabla f(x)$.

\subsection{Motivating problem classes}\label{subsection:motivation} Our motivation arises from some important problem classes in optimization and game theory presented as follows. 


\subsubsection{Optimal solution selection problem} In many optimization problems, the optimal solution may not be unique. When multiple optimal solutions exist, one may seek among the optimal solutions, one that minimizes a secondary metric. Let us formalize this problem and show how it can be formulated as~\cref{eqn:optVI}. Consider the minimization problem of the form
\begin{align}\label{prob:opt_select}
&\hbox{minimize} \quad \phi(y) \\
& \hbox{subject to} \quad y \in Y^*,\notag
\end{align}
where $\phi:\mathbb{R}^p \to \mathbb{R}$ is a continuous function (possibly nonconvex) and $Y^*$ denotes the optimal solution set of the canonical constrained convex program of the form
\begin{align}\label{prob:cst_cvx}
&\hbox{minimize} \quad g(y) \\
& \hbox{subject to} \quad y \in Y, \qquad h_j(y) \leq 0, \qquad j=1,\ldots,q.\notag
\end{align}
Assume that $g:\mathbb{R}^p \to \mathbb{R}$, $h_j:\mathbb{R}^p \to \mathbb{R}$, $j=1,\ldots,q$, are continuously differentiable convex functions. Suppose the Slater condition holds (see~\cite[Assum.~6.4.2]{bertsekas2003convex}) and the set $Y \subseteq \mathbb{R}^p$ is closed and convex. Then, in view of the strong duality theorem~\cite[Prop.~6.4.3]{bertsekas2003convex}, the duality gap is zero. From the Lagrangian saddle point theorem~\cite[Prop.~6.2.4]{bertsekas2003convex}, any optimal solution-multiplier pair, denoted by $(y^*,\lambda^*)$, is a saddle-point of the Lagrangian function $\mathcal{L}:\mathbb{R}^{p+q} \to \mathbb{R}$ defined as $\mathcal{L}(y,\lambda) \triangleq g(y) + \lambda^Th(y)$ where $\lambda\triangleq [\lambda_1;\ldots;\lambda_q]$ and $h(y)\triangleq [h_1(y);\ldots;h_q(y)]$. Then, problem~\cref{prob:opt_select} can be written as a VI-constrained optimization of the form~\cref{eqn:optVI} as follows. Let us define
\begin{align}\label{eqn:def_example1}
&x\triangleq [y;\lambda], \qquad X\triangleq Y\times \mathbb{R}^q_+, \qquad f(x)\triangleq \phi(y),\\
& F(x) \triangleq \left[\nabla_y \mathcal{L}(y,\lambda);-\nabla_\lambda \mathcal{L}(y,\lambda)\right]= \left[\nabla g(y) + \nabla h(y)\lambda;-h(y)\right], \nonumber
\end{align}
where $\nabla h(y) \triangleq [\nabla h_1(y),\ldots,\nabla h_q(y)]$. Notably, if functions $h_j$ are linear and $g$ is smooth, then $F$ is Lipschitz continuous on $X$. Further, given that $g$ and $h_j$ are convex in $y$, $F$ is merely monotone on $X$. For completeness, this is shown in \cref{lem:example1_monotone_F}. %\fy{TODO:add the Lipschiz property result for F when h is linear}
\subsubsection{Optimal Nash equilibrium seeking problem} Nash games may admit multiple equilibria~\cite{papadimitriou2001algorithms}. A storied and yet, challenging question in noncooperative game theory pertains to seeking the best and the worst equilibrium with respect to a given social welfare function. Addressing this question led to the emergence of efficiency estimation metrics, including Price of Anarchy (PoA) and Price of Stability (PoS), introduced in~\cite{koutsoupias1999worst,papadimitriou2001algorithms} and~\cite{anshelevich2008price}, respectively. Let us first formally recall the standard Nash equilibrium problem (NEP) and then show how~\cref{eqn:optVI} captures the optimal NE seeking problem. Consider a canonical noncooperative game among $d$ players where player $i$, $i=1,\ldots,d$, is associated with a cost function $g_i(x^i,x^{-i})$, $x^i$ denotes the strategy of player $i$, and $x^{-i}\triangleq (x^{1},\ldots, x^{i-1}, x^{i+1}, \ldots,x^d)$ denotes the strategy of other players. Let $X_i \subseteq \mathbb{R}^{n_i}$ denote the strategy set of player $i$. In an NEP, player $i$ seeks to solve
\begin{align}\label{prob:NEP}
&\hbox{minimize}_{\ x^i} \quad g_i(x^{i},x^{-i}) \\
& \hbox{subject to} \quad x^i \in X_i.\notag
\end{align}
A Nash equilibrium is a tuple of specific strategies, $\bar{x}\triangleq (\bar{x}^1,\ldots,\bar{x}^d)$, where no player can reduce her cost by unilaterally
deviating from her strategy. This is mathematically characterized as follows. For all $i=1,\ldots,d$, 
\begin{align*}
g_i(\bar{x}^{i},\bar{x}^{-i}) \leq g_i(x^{i},\bar{x}^{-i}), \qquad \hbox{for all } x^{i} \in X_i.
\end{align*}
The preceding characterization of NE can be captured by a VI problem. This is explained as follows. Suppose that, for each player $i$, the set $X_i$ is closed and convex, and function $g_i(x^{i},x^{-i})$ is continuously differentiable and convex in $x^i$, for every fixed $x^{-i} \in X_{-i}\triangleq \prod_{j\neq i} X_j$. Then, in view of~\cite[Prop.~1.4.2]{FacchineiPang2003}, $\mbox{SOL}(X,F)$ is equal to the set of Nash equilibria, where $X \subseteq \mathbb{R}^n$ and $F:X \to \mathbb{R}^n$ are defined as 
\begin{align}\label{eqn:def_NE_VI}
&X\triangleq \prod_{i=1}^dX_i, \quad  F(x) \triangleq \left[\nabla_{x^{1}} g_1(x^{1},x^{-1});\ldots;\nabla_{x^{d}} g_d(x^{d},x^{-d})\right], \quad  n\triangleq \sum_{i=1}^d n_i .  
\end{align}
Notably, under the convexity of the players' objectives, mapping $F$ is merely monotone and as such, $\mbox{SOL}(X,F)$ may contain a spectrum of equilibria  (an example is provided in \cref{sec:num}).  Let $\psi:\mathbb{R}^n \to \mathbb{R}$ denote a global metric for the group of the players. Some popular examples for $\psi$ include the following choices. (i) utilitarian approach: $\psi(x)\triangleq \frac{1}{d}\sum_{i=1}^d g_i(x)$. (ii) egalitarian approach: $\psi(x)\triangleq \max_{i=1,\ldots, d} g_i(x)$. (iii) risk-averse approach: $\psi(x)\triangleq \rho(g_1(x),\ldots,g_d(x))$ where $\rho:\mathbb{R}^d\to \mathbb{R}$ denotes a suitably defined risk measure for the group of the players. In all these cases, the problem of computing the best (or worst) NE with respect to the metric $\psi(x)$ is precisely captured by problem~\cref{eqn:optVI} where $f(x):= \psi(x)$ (or $f(x):= -\psi(x)$), respectively.

\subsubsection{Generalized Nash equilibrium problem (GNEP)}\label{sec:GNEPs} Another important class of problems that can be captured by problem~\cref{eqn:bilevelVI} is the generalized Nash game~\cite{facchinei2010generalized} where the strategy sets of the players may involve shared constraints. To this end, consider a GNEP, for $i=1,\ldots,d$, of the form
\begin{align}\label{prob:GNEP}
&\hbox{minimize}_{\ x^i} \quad g_i(x^{i},x^{-i}) \\
& \hbox{subject to} \quad x^i \in \mathcal{X}_i	(x^{-i}).\notag 
\end{align}
where $\mathcal{X}_i(x^{-i}) \subseteq \mathbb{R}^{n_i}$ denotes the strategy set of player $i$ that depends on the strategies of the rival players. Let us define the point-to-set mapping $\mathcal{X}(x)\triangleq \prod_{i=1}^d \mathcal{X}_i(x^{-i})$. Then, $\bar{x}\triangleq (\bar{x}^1,\ldots,\bar{x}^d)$ is a GNE if and only if $\bar{x} \in \mathcal{X}(\bar{x})$ and for all $i=1,\ldots,d$,
\begin{align*}
g_i(\bar{x}^{i},\bar{x}^{-i}) \leq g_i(x^{i},\bar{x}^{-i}), \qquad \hbox{for all } x^{i} \in \mathcal{X}_i	(\bar{x}^{-i}).
\end{align*}
Below, we provide three instances of GNEPs that can be formulated as a bilevel VI of the form \cref{eqn:bilevelVI}. In all cases, we define $H(x) \triangleq \left[\nabla_{x^{1}} g_1(x^{1},x^{-1});\ldots;\nabla_{x^{d}} g_d(x^{d},x^{-d})\right]$.

\noindent (i) GNEP with explicit functional constraints: Let the set of the shared constraints  be of the form
$$\mathcal{S}\triangleq \left\{x \in \mathbb{R}^n \mid \sum_{\ell=1}^d {A_\ell x^\ell} =b, \   h_j(x^{1},\ldots,x^d) \leq 0, \  j=1,\ldots,q, \ x^i \in X_i, \ i=1,\ldots,d\right\},$$
 where $A_i \in \mathbb{R}^{m \times n_i}$ and $b \in \mathbb{R}^{m \times 1}$ and $h_j(\bullet)$ is a continuously differentiable convex function. Let us denote $A:= [A_1,A_2,\ldots,A_d] \in \mathbb{R}^{m \times n}$. Then, the GNEP given by \cref{prob:GNEP} can be formulated as \cref{eqn:bilevelVI} where the mapping $F$ and the set $X$ are given as 
\begin{align}\label{eqn:def_GNE_VI}
&  F(x) \triangleq A^T(Ax-b) +\sum_{j=1}^q\max\{0,h_j(x)\}\nabla h_j(x), \qquad X\triangleq \prod_{i=1}^dX_i.  
\end{align}
If $\mathcal{S}\neq \emptyset$, then $x^* \in \mathcal{X}$ solves the GNEP \cref{prob:GNEP} if and only if it is a solution to \cref{eqn:bilevelVI}. To show this claim, it suffices to show that $\mbox{SOL}(X,F) = \mathcal{S}$ and that $F$ is merely monotone. This can be shown in a similar vein to the proof of Lemma~1.3 in~\cite{doi:10.1137/20M1357378}. Also, note that when the set $\mathcal{S}$ is linear, Lipschitz continuity of $F$ can be established. 
 
\noindent (ii) GNEP over nonlinear system of equations: Consider the case when the shared constraints set is the solution set to a system of nonlinear equations, given as $\mathcal{S}\triangleq \left\{x \in \mathbb{R}^n \mid F(x)=0\right\},$ where $F$ is a continuous and merely monotone mapping. Then, in view of $\mbox{SOL}(\mathbb{R}^n,F) = \mathcal{S}$, the GNEP given by \cref{prob:GNEP} can be formulated as \cref{eqn:bilevelVI}. 

\noindent (iii) GNEP with complementarity constraints: Also, when the shared constraints set is of the form $\mathcal{S}\triangleq \left\{x \in \mathbb{R}^n \mid  0 \leq x \perp F(x) \geq 0 \right\},$ noting that $\mbox{SOL}(\mathbb{R}^n_{+},F) = \mathcal{S}$, the GNEP \cref{prob:GNEP} can be cast as \cref{eqn:bilevelVI}.  



%\fy{Explain that the nested VI can capture Nash games with generalized constraints. Use Lemma 1.3 in the siopt paper to show that the generalized constraint set can be captured by the lower level SOL. We need to show that the lower level F is Lipschitz. If we can show that then our first theorem achieves the same rate as in the other recent papers on constraints VIs. Actually F is Lipscchitz if the  coupling constraints are linear. For the nonlinear case, i am not sure. Check Yangyang's paper on ALM. Then we can mention that in the contributions as well.}

\subsection{Related work and gaps} Next, we briefly review the literature on VIs and then, provide an overview of the prior research in addressing~\cref{eqn:optVI} and~\cref{eqn:bilevelVI}. %This will pave our way to highlight the gaps and our contributions. 

\noindent {\bf (i) Background on VIs.} The theory, applications, and methods for the VIs have been extensively studied in the past decades~\cite{FacchineiPang2003,rockafellar2009variational,kinderlehrer2000introduction}. Traditionally, a key challenge in the design and analysis of methods for solving VIs lies in the absence of some of the mathematical tools that are often available in the convergence theory of methods for solving optimization problems. Perhaps a good example on this is the Descent Lemma that has been widely utilized in deriving performance guarantees for first-order methods in smooth and structured nonsmooth optimization and yet, it may not be utilized in addressing VIs. Efforts on addressing such challenges with a focus on weakening the strong assumptions on the VIs led to the design of the extragradient method~\cite{korpelevich1976extragradient} where an extrapolation technique is utilized. Interestingly, in addressing deterministic VIs with merely monotone and Lipschitiz continuous maps, the extragradient-type methods improve the iteration complexity of the gradient methods from $\mathcal{O}(\epsilon^{-2})$ to $\mathcal{O}(\epsilon^{-1})$~\cite{nemirovski2004prox}. Employing Monte Carlo sampling, stochastic variants of the gradient and extragradient methods have been developed more recently in~\cite{jiang2008stochastic,juditsky2011solving,yousefian2018stochastic,yousefian2014optimal,kannan2019optimal,kotsalis2022simple}
and their variance-reduced extensions~\cite{iusem2017extragradient,alacaoglu2022stochastic}, among others. 

\noindent {\bf Gap (i).} Despite these advances, the abovementioned references do not provide iterative methods for addressing the two formulations considered in this work.

  

\noindent {\bf (ii) Methods for the optimal solution selection problem.} Problem~\cref{prob:opt_select} has been recently studied as a class of bilevel optimization problems~\cite{yamada2005hybrid,solodov2007explicit,solodov2007bundle,
beck2014first,sabach2017first,yousefian2021bilevel,jiang2023conditional,
chen2023bilevel,shen2023online,latafat2023adabim,merchav2023convex} and appears to have its roots in the study of ill-posed optimization problems and the notion of exact regularization~\cite{ferris1991finite,friedlander2008exact,tikhonov1963solution,amini2019iterative}.
%One shortcoming of these references lies in the following. 

\noindent {\bf Gap (ii).}  It appears that there are no methods endowed with complexity guarantees for addressing~\cref{prob:opt_select} with explicit lower level functional constraints and possibly, with a nonconvex objective function $\phi(\bullet)$. %Observing that~\eqref{prob:opt_select} is an instance of \cref{eqn:optVI}, in this work we are interested in addressing this gap. 
%
%here mention the references on simple bilevel (Erfan, Fatma, Beck, Sabach) and your prior work. also comment on Mangasarian and Tseng's work
%
%
%
%Beck and Sabach \cite{beck2014first}
%A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem \cite{jiang2023conditional}\\
%Bilevel Distributed Optimization in Directed Networks \cite{yousefian2021bilevel}\\
%%Complexity guarantees for an implicit smoothing-enabled method for stochastic MPECs\cite{cui2023complexity}\\
%On Bilevel Optimization without Lower-level Strong Convexity~\cite{chen2023bilevel}\\
%An online convex optimization-based framework for convex bilevel optimization~\cite{shen2023online}\\
%AdaBiM: An adaptive proximal gradient method for structured convex bilevel optimization~ \cite{latafat2023adabim}\\
%Solodov's bundle method~\cite{solodov2007bundle}\\
%Solodov's An explicit descent method for bilevel convex optimization~\cite{solodov2007explicit}



\noindent {\bf (iii) Methods for VI-constrained optimization.} Naturally, solving problem~\cref{eqn:optVI} is challenging, due to the following two reasons. (i) The feasible solution set in \cref{eqn:optVI} is itself the solution set of a VI problem and so, it is often unavailable. Indeed, as mentioned in~\cite{feinstein2023characterizing}, there does not seem to exist any algorithm that can compute all Nash equilibria. (ii) The standard Lagrangian duality theory can be employed when the feasible set of the optimization problem is characterized by a finite number of functional constraints. However, as observed in~\cref{eqn:SOLVI}, the solution set of a VI problem is characterized by infinitely many, and possibly nonconvex, inequality constraints. Indeed, there are only a handful of papers that have developed efficient iterative methods for computing the optimal equilibrium~\cite{yamada2005hybrid,facchinei2014vi,yousefian2017smoothing,doi:10.1137/20M1357378,kaushik2023incremental,jalilzadeh2022stochastic}. Among these, only the methods in~\cite{doi:10.1137/20M1357378,kaushik2023incremental,jalilzadeh2022stochastic} are equipped with suboptimality and infeasibility error bounds. 

\noindent {\bf Gap (iii).} The abovementioned three references do not address the case when the objective $f$ is nonconvex. Also, when $f$ is convex, the best known iteration complexity for suboptimality and infeasibility metrics appears to be $\mathcal{O}(\epsilon^{-4})$. In this work, our aim is to improve such guarantees for the convex case and also, achieve new guarantees for the strongly convex and nonconvex cases. 

  
%
%Yamada's work~\cite{yamada2005hybrid}\\
%VI-constrained hemivariational inequalities~\cite{facchinei2014vi}
%Stochastic Approximation for Estimating the Price of Stability in Stochastic Nash Games\cite{jalilzadeh2022stochastic}\\
%FY MPAR~\cite{yousefian2017smoothing}
%Distributed coordination for seeking the optimal Nash equilibrium of aggregative games \cite{ma2022distributed}\\
%Design and Analysis of Bilevel Optimisation Algorithms\cite{giangdesign}\\
%FY work~\cite{doi:10.1137/20M1357378,kaushik2023incremental,jalilzadeh2022stochastic}


\noindent {\bf (iv) Methods for bilevel VIs and GNEPs.} The bilevel VI problem~\cref{eqn:bilevelVI} is a powerful mathematical model that subsumes several important problem classes, including the optimal solution selection problem~\cref{prob:opt_select}, the optimal NEP and its generalization~\cref{eqn:optVI}, and special cases of the GNEP~\cref{prob:GNEP}, as discussed in \cref{subsection:motivation}. Methods for addressing bilevel VIs and their closely-related variants have been recently developed in references including~\cite{facchinei2014vi,thong2020strong,van2021regularization,lampariello2022solution}. The GNEP has been extensively employed in several application areas, including economic sciences and telecommunication systems~\cite{facchinei2010generalized}. Under convexity assumptions, it can be cast as the so-called quasi-VI problem~\cite{chan1982generalized}. While the research on GNEPs has been more active in recent years~\cite{benenati2023optimal,benenati2023semi}, there are a limited number of methods equipped with provable rates for solving GNEPs, including~\cite{alizadeh2023randomized}, where Lagrangian duality theory is employed for relaxing the shared functional constraints. 

\noindent {\bf Gap (iv).} Among the references on bilevel VIs, only the work in~\cite{lampariello2022solution} appears to provide complexity guarantees, that is $\mathcal{O}(\epsilon^{-8})$ when $H$ is merely monotone. We aim at improving these guarantees significantly and also, providing new guarantees when $H$ is strongly monotone. With regard to GNEPs, our work provides a new avenue for solving these challenging problems even when the shared constraints do not admit the conventional form, examples of which are provided in \cref{sec:GNEPs}~(ii) and (iii).

\subsection{Contributions} Motivated by these gaps, our contributions are as follows. 

\noindent {\bf (i) Improved guarantees for monotone bilevel VIs.} When mappings $F$ and $H$ are both merely monotone and Lipschitz continuous, we develop a single-timescale iteratively regularized extragradient method (IR-EG) for addressing problem~\cref{eqn:bilevelVI} and derive new iteration complexity of $\mathcal{O}\left(\epsilon^{-2}\right)$ for the inner and outer VI problem, in terms of the dual gap function. For bilevel VIs, this result improves the existing complexity $\mathcal{O}(\epsilon^{-8})$ for this class of problems in prior work~\cite{lampariello2022solution}. Also, when $H(x):=\nabla f(x)$, addressing VI-constrained optimization, this improves the existing complexity $\mathcal{O}(\epsilon^{-4})$ in~\cite{doi:10.1137/20M1357378,kaushik2023incremental,jalilzadeh2022stochastic} by leveraging smoothness of $f$ and Lipschitz continuity of $F$. 

\noindent {\bf (ii) New guarantees for strongly monotone bilevel VIs.} When $H$ is strongly monotone, we show that a variant of IR-EG, called R-EG, under a prescribed weighted averaging scheme, can be endowed with a linear convergence speed in terms of an error bound on the dual gap function of the outer VI problem. We also show that under a suitable choice of the regularization parameter, complexities of $\mathcal{O}\left(\epsilon^{-1/p}\right)$ and $\mathcal{O}\left(\epsilon^{-1/(p+1)}+\epsilon^{-1}\right)$ can be obtained for the outer and inner VI problem, respectively, for any arbitrary $p\geq 1$. These guarantees appear to be new for both \cref{eqn:optVI} and \cref{eqn:bilevelVI} formulations.  

\noindent {\bf (iii) New guarantees for VI-constrained nonconvex optimization.} We develop an iterative method, called IPR-EG, for addressing problem~\cref{eqn:optVI} when $f$ is smooth and nonconvex. We show that this method can achieve an overall iteration complexity of nearly $\mathcal{O}\left(\epsilon^{-5}\right)$ in computing a stationary point (see \cref{theorem:NC}). Our method appears to be the first fully iterative scheme equipped with iteration complexity for solving VI-constrained nonconvex optimization problems, and in particular, for computing the worst equilibrium in monotone Nash games. 


\subsection*{Outline of the paper} The remainder of the paper is organized as follows. In \cref{sec:algs}, we present the outline of the proposed methods for addressing bilevel VIs (with a monotone and a strongly monotone mapping $H$) and the VI-constrained nonconvex optimization problem. We then present the convergence rate results of these methods in \cref{sec:conv}. Some preliminary numerical results are presented in \cref{sec:num}. Lastly, we provide the concluding remarks in \cref{sec:conclude}.   

\subsection*{Notation} Throughout, a vector $x \in \mathbb{R}^n$ is assumed to be a column vector. 	The transpose of $x$ is denoted by $x^T$. Given two vectors $x\in \mathbb{R}^n$ and $y\in \mathbb{R}^m$, we use $[x;y] \in \mathbb{R}^{(n+m)\times 1}$ to denote their concatenated column vector. When $n=m$, we use $[x,y] \in \mathbb{R}^{n\times 2}$ to denote their side-by-side concatenation. We let $\|\cdot\|$ denote the Euclidean norm. A mapping $F:X \to \mathbb{R}^n$ is said to be monotone on a convex set $X \subseteq \mathbb{R}^n$ if $(F(x)-F(y))^T(x-y)\geq 0$ for any $x,y \in X$. The mapping $F$ is said to be $\mu$-strongly monotone on a convex set $X \subseteq \mathbb{R}^n$ if $\mu>0$ and $(F(x)-F(y))^T(x-y)\geq \mu\|x-y\|^2$ for any $x,y \in X$. Also, $F$ is said to be Lipschitz continuous with parameter $L>0$ on the set $X$ if $\|F(x)-F(y)\|\leq L\|x-y\|$ for any $x,y \in X$. A continuously differentiable function $f:X \to \mathbb{R}$ is called $\mu$-strongly convex on a convex set $X$ if $f(x) \geq f(y)+\nabla f(y)^T(x-y)+\frac{\mu}{2}\|x-y\|^2$. The Euclidean projection of vector $x$ onto a closed convex set $X$ is denoted by $ \Pi_{X}(x) $, where $\Pi_{X}(x)\triangleq  \mbox{arg}\min_{ y \in  X}\| x- y\|$. We let $\mbox{dist}(x,X)\triangleq \|x-\Pi_{X}(x)\|$ denote the distance of $x$ from the set $X$. 

 

\section{Outline of algorithms}\label{sec:algs} 
In this section, we present the outline of the new algorithms and highlight some key techniques leveraged in designing these schemes. 

\subsection{Description of \cref{alg:IR-EG}}
We devise \cref{alg:IR-EG} to address two problems. (i) The bilevel VI problem \cref{eqn:bilevelVI} where both $F$ and $H$ are assumed to be merely monotone and Lipschitz continuous. (ii) The VI-constrained optimization problem~\cref{eqn:optVI} when $f$ is $L$-smooth and convex, by setting $H(x):=\nabla f(x)$. \cref{alg:IR-EG} is essentially an iteratively regularized extragradient (IR-EG) method. A key idea lies in employing the regularization technique to incorporate both mappings $H$ and $F$. At iteration $k$, the vectors $y_k$ and $x_k$ are updated by using the regularized map $F(\bullet) +\eta_kH(\bullet)$. The regularization parameter $\eta_k$ is updated iteratively. Intuitively, the update rule of $\eta_k$ regulates the trade-off between incorporating the information of mapping $F$ and $H$. Interestingly, this will be theoretically supported in \cref{thm:bilevelVI}, where the main convergence results for this algorithm are presented. 



\begin{algorithm}[H]
\caption{IR-EG for bilevel VI~\cref{eqn:bilevelVI} with merely monotone $H$ and $F$}
\label{alg:IR-EG}
\begin{algorithmic}[1]
\STATE \textbf{input:} Initial vectors $x_0, y_0 \in X$, a stepsize $\gamma > 0$, an initial regularization parameter $\eta_0 > 0$, $\bar{y}_0 := y_0$, and $0\leq b<1$.
\FOR{$k = 0, 1, \ldots, K-1$}
%\STATE Generate $\xi_k$  as realizations of the random vector $\xi$. 
\STATE $y_{k+1}  := \Pi_X\left[x_k - {\gamma}\left(F(x_k)+\eta_k H(x_k)\right)\right]   \label{line:update_y}$
 
\STATE $x_{k+1} := \Pi_X\left[x_k - {\gamma}\left(F(y_{k+1}) +\eta_k H(y_{k+1})\right)\right]  \label{line:update_x}$ 
 
\STATE $\bar{y}_{k+1} := \left(k \bar{y}_k +  y_{k+1}\right)/(k+1)$
\STATE $\eta_{k+1} :=\frac{\eta_0}{(k+1)^b}$ 
\ENDFOR
\STATE {\bf return} $\bar{y}_K$
\end{algorithmic}
\end{algorithm}

\subsection{Description of  \cref{alg:IR-EG-s}}
The R-EG method, presented by \cref{alg:IR-EG-s}, is devised to address two classes of problems as follows. (i) The bilevel VI problem \cref{eqn:bilevelVI} when $H$ is $\mu_H$-strongly monotone and Lipschitz continuous, and $F$ is merely monotone and Lipschitz continuous. (ii) The VI-constrained optimization problem~\cref{eqn:optVI} when $f$ is $L$-smooth and strongly convex, by setting $H(x):=\nabla f(x)$. Note that R-EG is a variant of IR-EG with two slight differences described as follows. (1) In R-EG, we employ a constant regularization parameter $\eta$. (2) We also employ a weighted averaging sequence where the weights, $\theta_k$, are updated geometrically characterized by the stepsize $\gamma$, regularization parameter $\eta$, and $\mu_H$. The main convergence guarantees for this method will be presented in \cref{theorem: H is SC} and \cref{theorem: H is SC2}.

\begin{algorithm}[H]
\caption{R-EG for~\cref{eqn:bilevelVI} with strongly monotone $H$ and merely monotone $F$}
\label{alg:IR-EG-s}
\begin{algorithmic}[1]
\STATE \textbf{input:} Initial vectors $x_0, y_0 \in X$, a stepsize $\gamma > 0$, a regularization parameter $\eta > 0$, such that {${\gamma}^2L_F^2+{\gamma}\eta\mu_H+{\gamma}^2\eta^2L_H^2\leq 0.5$},  $\theta_0 = \frac{1}{1- \gamma \eta \mu_H }$, and $ \Gamma_0 =0$.
\FOR{$k = 0, 1, \ldots, K-1$}

\STATE $y_{k+1}  := \Pi_X\left[x_k - {\gamma}\left(F(x_k)+\eta H(x_k)\right)\right]   \label{line:update_y_S H}$
 

\STATE $x_{k+1} := \Pi_X\left[x_k - {\gamma}\left(F(y_{k+1}) +\eta H(y_{k+1})\right)\right]  \label{line:update_x_S H}$ 


\STATE $\bar{y}_{k+1} := \frac{\Gamma_k \bar{y}_{k} + \theta_{k} y_{k+1}}{\Gamma_{k+1}}$
 
\STATE $ \Gamma_{k+1} := \Gamma_k + \theta_k $ 

\STATE $\theta_{k+1} :=\frac{\theta_k}{1 - \gamma \eta \mu_H} $
\ENDFOR
\STATE {\bf return} $\bar{y}_K$
\end{algorithmic}
\end{algorithm}


\subsection{Description of \cref{alg:ncvx-IR-GD}}\label{sec:IPREG_desc}
 We propose the IPR-EG method, presented by \cref{alg:ncvx-IR-GD}, for solving~the VI-constrained optimization problem~\cref{eqn:optVI} when $f$ is $L$-smooth and nonconvex. This is an inexactly-projected gradient method that employs R-EG, at each iteration, with prescribed adaptive termination criterion and regularization parameters, to approximate the Euclidean projection onto the VI-constraint set. In the following, we elaborate on the key ideas employed in this method and highlight some challenges in the analysis. Consider problem~\cref{eqn:optVI}. It can be compactly written as $\min_{x \in X^*_F} \ f(x)$ where $X^*_F\triangleq \mbox{SOL}(X,F)$. First, we may naively consider the standard projected gradient method given as 
$$\hat{x}_{k+1}:=\Pi_{X_F^*}\left[\hat{x}_k -\hat{\gamma}\nabla f(\hat{x}_k)\right].$$ 
However, the set $X^*_F$ is unknown. Interestingly, at any iteration $k$, the R-EG method can be employed to inexactly compute $\Pi_{X^*_F}(z_k)$ given  $z_k:=\hat{x}_k -\hat{\gamma}\nabla f(\hat{x}_k)$. To this end, let us assume that $k$ is fixed and define $H(x):=x-z_k$. Note that $H$ is strongly monotone and Lipschitz continuous with $\mu_H=L_H=1$. Observing that $H(x) =\nabla_x \left(\tfrac{1}{2}\|x-z_k\|^2\right)$, the unique solution to the bilevel VI problem~\cref{eqn:bilevelVI} is the optimal solution to the projection problem given as
\begin{align}\label{eqn:proj_problem_nc}
\min_{x \in X^*_F} \ \tfrac{1}{2}\|x-z_k\|^2.
\end{align}
This implies that the unique solution to the bilevel VI problem is $\Pi_{X^*_F}(z_k)$. Motivated by this observation, we employ R-EG to compute $\Pi_{X^*_F}(z_k)$ inexactly. However, it is crucial to control the level of this inexactness for establishing the convergence and deriving rate statements for computing a stationary point to problem~\cref{eqn:optVI}. Indeed, we may obtain a bound on the inexactness, as it will be shown in \cref{theorem: H is SC2}. A key question in the design of the IPR-EG method lies in finding out that at any given iteration $k$ of the underlying gradient descent method, how many iterations of R-EG are needed. A challenge in addressing this question lies in the fact that, because of performing inexact projections, $\hat{x}_k$ may not be feasible to problem~\cref{eqn:optVI}. This infeasibility needs to be carefully treated in the analysis to establish the convergence to a stationary point of the original VI-constrained problem. This will be precisely addressed in \cref{sec:ncvx}. 
 

 
\begin{algorithm}[H]
  \caption{IPR-EG for VI-constrained nonconvex optimization~\cref{eqn:optVI}}
  \label{alg:ncvx-IR-GD}
  \begin{algorithmic}[1]
    \STATE \textbf{Input:} Initial vectors $\hat x_{0}, x_{0,0}, y_{0,0} \in X$, $\bar{y}_{0,0}:= y_{0,0}$, outer level stepsize $\hat{\gamma}:=\frac{1}{\sqrt{K}} \leq {\frac{1}{ 2L  }}$, inner level stepsize $ \gamma$ such that $\gamma \leq \frac{1}{2L_F}$.
   \FOR{$k=0,1,\ldots, K-1$}
     \STATE $z_k := \hat x_k - \hat{\gamma} \nabla f(\hat x_k)$
     \STATE {$T_k := \max\{k\sqrt{k},151\}$ and $\eta_k :=  {6\ln(T_k)} /( \gamma T_k)$}
	\STATE $\theta_{k,0} = \frac{1}{1- 0.5\gamma \eta_0 }$ and $ \Gamma_{k,0} =0$
 
      \FOR{$t = 0, 1, \ldots, T_k -1$}

	\STATE $y_{k, t+1}  := \Pi_X\left[x_{k,t} - {\gamma}\left(F(x_{k,t})+\eta_k \left( x_{k,t} - z_k\right)\right)\right]  $
 

	\STATE $x_{k, t+1} := \Pi_X\left[x_{k,t} - {\gamma}\left(F(y_{k, t+1}) +\eta_k \left( y_{k,t+1} - z_k\right)\right)\right] $ 


	\STATE $\bar{y}_{k, t+1} := \left( {\Gamma_{k,t} \bar{y}_{k,t} + \theta_{k,t} y_{k, t+1}}\right)/{\Gamma_{k, t+1}}$
 
	\STATE $ \Gamma_{k, t+1} := \Gamma_{k, t} + \theta_{k, t} $  and $\theta_{k, t+1} :=\frac{\theta_{k,t}}{1 - 0.5\gamma \eta_k } $
	\ENDFOR
         \STATE $\hat x_{k+1} := \bar{y}_{k,T_k } $ and $y_{k+1,0}:=\bar{y}_{k,T_k} $
    \ENDFOR
    \STATE {\bf return} $\hat{x}_K$
  \end{algorithmic}
  %\textbf{return:} $x_{K}$
\end{algorithm}





%\begin{algorithm}[H]
%\caption{nestedEG {\scriptsize (Projected Averaging Iteratively Regularized Extragradient Method)}}
%\label{alg:IR-EG}
%\begin{algorithmic}[1]
%\STATE \textbf{input:} Initial points $x_0, y_0 \in X$, an initial stepsize $\gamma_0 > 0$, an initial regularization parameter $\eta_0 > 0$, a scalar $r < 1$, $y_0 := \bar{y}_0$, and $\Gamma_0 := 0$.
%\FOR{$k = 0, 1, \ldots, K-1$}
%%\STATE Generate $\xi_k$  as realizations of the random vector $\xi$. 
%\STATE $y_{k+1}  := \Pi_X\left[x_k - {\gamma}\left(\tilde{F}(x_k)+\eta_k \tilde{H}(x_k)\right)\right]   \label{line:update_y}$
%
%where $\tilde{F}(\bullet)\triangleq \frac{1}{M_k}\sum_{j=1}^{M_k} F(\bullet, \tilde{\xi}_{j})$ and $\tilde{H}(\bullet)\triangleq \frac{1}{M_k}\sum_{j=1}^{M_k} H(\bullet,\tilde{\xi}_{j})$
%\STATE $x_{k+1} := \Pi_X\left[x_k - {\gamma}\left(\hat{F}(y_{k+1}) +\eta_k\hat{H}(y_{k+1})\right)\right]  \label{line:update_x}$ 
%
%where $\hat{F}(\bullet)\triangleq \frac{1}{M_k}\sum_{j=1}^{M_k} F(\bullet, \hat{\xi}_{j})$ and $\hat{H}(\bullet)\triangleq \frac{1}{M_k}\sum_{j=1}^{M_k} H(\bullet,\hat{\xi}_{j})$
%\STATE $\Gamma_{k+1} := {\gamma} + {\gamma}^r$
%\STATE $\bar{y}_{k+1} := \left({\gamma} \bar{y}_k + {\gamma}^r y_{k+1}\right)/\Gamma_{k+1}$
%\STATE $\gamma_{k+1} :=\frac{\gamma_0}{\sqrt{k+1}}$ and $\eta_{k+1} :=\frac{\eta_0}{(k+1)^b}$ where $0<b<0.5$
%\ENDFOR
%\STATE {\bf return} $\bar{y}_K$
%\end{algorithmic}
%\end{algorithm}

\section{Convergence theory}\label{sec:conv}
In this section, we establish the convergence properties and derive the rate statements for each of the three proposed methods. 
\subsection{Preliminaries}
In the following, we provide some definitions and preliminary results that will be utilized in the analysis in this section. 
\begin{lemma}[Projection theorem {\cite[Prop.~2.2.1]{bertsekas2003convex}}]\label{lem:Projection theorm}\em
Let $X$ be a closed convex set. Given any arbitrary $x \in \mathbb{R}^n$, a vector $x^* \in X$ is equal to $\Pi_X(x)$ if and only if $$(y-x^*)^T(x-x^*) \leq 0, \qquad \hbox{for all }y\in X.$$ 
\end{lemma}
To quantify the quality of the generated iterates by the proposed algorithms, we use the dual gap function~\cite{juditsky2011solving,yousefian2017smoothing,doi:10.1137/20M1357378}, defined as follows. 
\begin{definition}[Dual gap function]\em
Let $X\subseteq \mathbb{R}^n$ be a nonempty, closed, and convex set and $F:X \to \mathbb{R}^n$ be a continuous mapping. Then, the dual gap function associated with $\mbox{VI}(X,F)$ is an extended-valued function denoted by $\mbox{Gap}:\mathbb{R}^n \to \mathbb{R}\cup\{+\infty\}$ and is given as $$ \mbox{Gap}(x,X,F)\triangleq \sup_{ y\in  {X}} F(y)^T(x-y).$$
\end{definition}
\begin{remark} \label{rem:gap}
Note that when $F$ is continuous and monotone, if $x \in X$, then $\mbox{Gap}(x,X,F)=0$ if and only if $x \in \mbox{SOL}(X,F)$~\cite{juditsky2011solving}. Similar to~\cite[page 167]{FacchineiPang2003}, here we define the  dual gap function for vectors both inside and outside of the set $X$. Note that if $x \in X$, then $ \mbox{Gap}(x,X,F) \geq 0$. However, this may not be the case for $x \notin X$. In the case when the generated iterate by an algorithm is possibly infeasible to the set of the VI (e.g., this emerges in solving bilevel VIs), it is important to build both a lower bound and an upper bound on the dual gap function.
\end{remark}
In some of the rate results, we utilize the weak sharpness property, defined as follows. 
\begin{definition}[Weak sharpness property{~\cite{marcotte1998weak,kannan2019optimal}}]\label{def:weaksharp}\em
Let $X\subseteq \mathbb{R}^n$ be a nonempty, closed, and convex set and $F:X \to \mathbb{R}^n$ be a continuous mapping. The weak-sharpness property holds for $\mbox{VI}(X,F)$ if there exists $\alpha>0$ such that for all $x \in X$ and all $x^* \in  \mbox{SOL}(X,F)$, we have 
$F(x^*)^T(x- x^*) \geq \alpha\  \mbox{dist}(x, \mbox{SOL}(X,F)).$
\end{definition}
Throughout, in deriving some of the rate results, we may utilize the following terms. 
\begin{definition}\label{def:terms_bounded}\em
Let $D_X^2\triangleq \sup_{x,y \in X}\ \frac{1}{2}\|x-y\|^2$ and define $B_F, B_H, B_f>0$ such that $B_F\triangleq \sup_{x \in X^*_F} \|F(x)\|$, $B_H\triangleq \sup_{x \in X^*_F} \|H(x)\|$, and $B_f \triangleq \sup_{x \in X^*_F} \|\nabla f(x)\|$. We also define $C_F, C_H, C_f>0$ similarly, but with respect to the set $X$. 
\end{definition}
%\fy{In lemma 3.8, we should use B defined only on the SOL of X and F. However, later in obtaining the gap function bound for lower level VI,  B should be defined only on the entire set X. This needs to be corrected throughout the paper.}
Note that when the set $X$ is bounded, these scalars are finite. 
\subsection{Monotone bilevel VIs and VI-constrained convex optimization} 
The main assumptions in this section are presented in the following. 
\begin{assumption}\label{assum:bilevelVI_m} \em
Consider problem~\cref{eqn:bilevelVI}. Let the following statements hold. 

\noindent (a) Set $X$ is nonempty, closed, and convex.

\noindent (b) Mapping $F:X \to \mathbb{R}^n$ is $L_F$-Lipschitz continuous and merely monotone on $X$.

\noindent  (c) Mapping $H:X \to \mathbb{R}^n$ is  $L_H$-Lipschitz continuous and merely monotone on $X$.

\noindent (d) The solution set of problem~\cref{eqn:bilevelVI} is nonempty.
\end{assumption}
\begin{remark}\label{rem:assum_monotone}
We note that Assumption~\ref{assum:bilevelVI_m}~(d) can be met under several settings. We provide an example in the following. Let $X$ be a closed convex set and $F$ be monotone. Then, in view of~\cite[Theorem~2.3.5]{FacchineiPang2003}, $\mbox{SOL}(X,F)$ is convex. If, additionally, $X$ is bounded, then from~\cite[Corollary~2.2.5]{FacchineiPang2003}, $\mbox{SOL}(X,F)$ is nonempty and compact. Now, let us consider $\mbox{VI}(\mbox{SOL}(X,F),H)$ and let $H$ be monotone. Invoking the preceding two results once again, we can conclude that $\mbox{SOL}(\mbox{SOL}(X,F),H)$ is nonempty, compact, and convex. 
\end{remark}

In the bilevel VI problem~\cref{eqn:bilevelVI}, $\mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)  \geq 0$ for all $x \in \mbox{SOL}(X,F)$. This follows directly from \cref{rem:gap}. However, $\mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right) $ might be negative for  $x \notin \mbox{SOL}(X,F)$. In the following, we establish a lower bound on the dual gap function of the outer VI problem for any vectors $x\in X$ that might not belong to $\mbox{SOL}(X,F)$. This result will be utilized in deriving a subset of the main rate statements in this work. 
\begin{lemma}\label{lem:bilevelVI_gap_ws}\em
Consider problem~\cref{eqn:bilevelVI}. Let Assumption~\ref{assum:bilevelVI_m} hold. Also, suppose $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp. Then, the following results hold. 

\noindent (i) $\ \mbox{dist}\left(x,\mbox{SOL}(X,F)\right)\leq   \alpha^{-1}\mbox{Gap}\left(x,X,F\right)$ for any $x \in X$.

\noindent (ii) $\mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)   \geq - B_H\ \mbox{dist}\left(x,\mbox{SOL}(X,F)\right) $.

In particular,  $\mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)  \geq 0$ for any $x \in \mbox{SOL}(X,F)$. 
%Further, if $H$ is $\mu_H$-strongly monotone, then 
%$$\mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)   \geq - C_H\ \mbox{dist}\left(x,\mbox{SOL}(X,F)\right)+\mu_H\|x-x^*\|^2 ,$$
%where $x^*$ denotes the unique solution to problem~\cref{eqn:bilevelVI}. 

\noindent (iii)  Consider the special case $H(x) := \nabla f(x)$ where $f$ is a continuously differentiable and convex function. Then, (i) holds, and for any $x \in X$ we have 
$$f(x) -\inf_{\bar{x} \in {\tiny\mbox{SOL}(X,F)}} f(\bar{x}) \geq - B_f\ \mbox{dist}\left(x,\mbox{SOL}(X,F)\right). $$
Further, if $f$ is $\mu$-strongly convex, then for any $x \in X$ we have 
$$f(x) -f(x^*) \geq - \|\nabla f(x^*)\|\ \mbox{dist}\left(x,\mbox{SOL}(X,F)\right)+\frac{\mu}{2}\|x-x^*\|^2, $$
where $x^*$ denotes the unique optimal solution to problem~\cref{eqn:optVI}.  
\end{lemma}
\begin{proof}
(i) Let $x \in X$ be given. Let $x^*_F \in \mbox{SOL}(X,F)$ be an arbitrary vector. Then, from \cref{def:weaksharp}, we have $F(x^*_F)^T(x- x^*_F) \geq \alpha\  \mbox{dist}\left(x,\mbox{SOL}(X,F)\right)$. Invoking the definition of the dual gap function and the preceding relation, we obtain  
\begin{align}\label{eqn:ineq_ws_iii_1}\mbox{Gap}\left(x,X,F\right) = \sup_{ y\in  {X}} F(y)^T(x-y) \geq F(x^*_F)^T(x- x^*_F) \geq \alpha\  \mbox{dist}\left(x,\mbox{SOL}(X,F)\right).
\end{align} 

\noindent (ii) From the definition of the dual gap function, for any $\hat{x} \in \mbox{SOL}(X,F)$, we have
\begin{align}\label{eqn:hatx_monotoneH} 
H(\hat{x} )^T\left(x-\hat{x} \right) \leq  \sup_{ y\in  {\tiny \mbox{SOL}(X,F)}} H(y)^T(x-y)= \mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)   .
\end{align}
Invoking the Cauchy-Schwarz inequality, from the preceding inequality we obtain 
\begin{align*}
 -B_H \|x-\hat{x} \| \leq \mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)       .
\end{align*}
Now, let us choose $\hat{x} :=\Pi_{\tiny \mbox{SOL}(X,F)}(x)$. The preceding relation implies that 
\begin{align*}%\label{eqn:ineq_ws_iii_2}
 -B_H \ \mbox{dist}\left(x,\mbox{SOL}(X,F)\right) \leq \mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)       .
\end{align*}
%To show the strongly monotone case, consider \eqref{eqn:hatx_monotoneH}. 
%\begin{align*} 
%H(\hat{x} )^T\left(x-\hat{x} \right)  \leq \mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)   .
%\end{align*}
%From the strong monotonicity of $H$, we have $\mu_H\|x-\hat{x}\|^2 \leq (H(x)- H(\hat{x} ))^T (x-\hat{x}) .$
%Combining~\eqref{eqn:ineq_ws_iii_1} and \eqref{eqn:ineq_ws_iii_2}, we obtain the result in (ii).
If $x \in \mbox{SOL}(X,F)$, then $\mbox{Gap}\left(x,\mbox{SOL}(X,F),H\right)  \geq 0$  follows  from the above relation. 

\noindent (iii) We show the result for both merely and strongly convex cases by letting $\mu \geq 0$. Let us define $\hat{x}\triangleq \Pi_{\tiny\mbox{SOL}(X,F)} (x)$. Let $x^*$ denote an optimal solution to problem~\cref{eqn:optVI}. Since $\hat{x} \in \mbox{SOL}(X,F)$, in view of the optimality condition, we have $\nabla f(x^*)^T(\hat{x}-x^*) \geq 0$. From (strong) convexity of $f$ and the Cauchy-Schwarz inequality, we may write 
\begin{align*}
f(x) - f(x^*) & \geq \nabla f(x^*)^T(x-x^*) +\frac{\mu}{2}\|x-x^*\|^2%= \nabla f(x^*)^T(x-\hat{x}+\hat{x}-x^*)+\frac{\mu}{2}\|x-x^*\|^2
\\
&=\nabla f(x^*)^T(x-\hat{x})+\nabla f(x^*)^T(\hat{x}-x^*)+\frac{\mu}{2}\|x-x^*\|^2\\ 
&\geq \nabla f(x^*)^T(x-\hat{x})+\frac{\mu}{2}\|x-x^*\|^2\\
%& \geq -\|\nabla f(x^*)\|\|x-\hat{x}\|+\frac{\mu}{2}\|x-x^*\|^2 
&\geq -\|\nabla f(x^*)\|\ \mbox{dist}(x,\mbox{SOL}(X,F))+\frac{\mu}{2}\|x-x^*\|^2.
\end{align*}
\end{proof}
The next result provides a recursive inequality that will be useful in the analysis. 
\begin{lemma}\label{lem:monotone_lemma_ineq}\em
Consider problem~\cref{eqn:bilevelVI}. Let the sequence $\{\bar{y}_k\}$ be generated by \cref{alg:IR-EG}. Let Assumption~\ref{assum:bilevelVI_m} hold. The following statements hold.

\noindent (i) Assume that ${\gamma}^2(L_F^2+\eta_k^2L_H^2) \leq 0.5$ for all $k\geq 0$. Then, for any $x\in X$ and for all $k\geq 0$, we have
\begin{align}\label{ineq:EG_lemma_merelymonotone}
2{\gamma}\left(F(x)+\eta_kH(x)\right)^T\left(y_{k+1}-x\right)   & \leq  \|x_k - x\|^2   -\|x_{k+1} - x\|^2 .
\end{align}

\noindent (ii) Consider the special case $H(x) := \nabla f(x)$ where $f$ is a convex and $L$-smooth function. Assume that ${\gamma}^2(L_F^2+\eta_k^2L^2) \leq 0.5$ for all $k\geq 0$. Then, for any $x\in X$ and for all $k\geq 0$, we have
\begin{align}\label{ineq:EG_lemma_merelymonotone2}
2{\gamma}F(x)^T\left(y_{k+1}-x\right) +2{\gamma}\eta_k(f(y_{k+1}) -f(x))  & \leq  \|x_k - x\|^2   -\|x_{k+1} - x\|^2 .
\end{align} 

\end{lemma}

\begin{proof}
(i) Let $x \in X$ be an arbitrary vector. For $k \geq 0$, we have
\begin{align*}
 \|x_{k+1} - x\|^2 &= \|x_{k+1} -x_k + x_k - x\|^2 \\
 &= \|x_{k+1} - x_k\|^2 + \|x_k - x\|^2 + 2(x_{k+1} - x_k)^T(x_k - x) \\ 
  & =\|x_{k+1} - x_k\|^2 + \|x_k - x\|^2 + 2(x_{k+1} - x_k)^T(x_k -x_{k+1} + x_{k+1} - x)  \\
 & =\|x_k - x\|^2 - \|x_{k+1} - x_k\|^2 + 2(x_{k+1} - x_k)^T(x_{k+1} - x).
\end{align*}
We also have
$$\|x_{k+1} - x_k\|^2  =
\|x_{k+1} - y_{k+1}\|^2 + \|y_{k+1} - x_k\|^2 + 2(x_{k+1} - y_{k+1})^T(y_{k+1} - x_k).
$$
From the preceding relations, we may write 
\begin{align}\label{eqn:ES_main1}
 \|x_{k+1} - x\|^2 & =\|x_k - x\|^2 - \|x_{k+1} - y_{k+1}\|^2 - \|y_{k+1} - x_k\|^2\nonumber\\
 & + 2(y_{k+1}-x_{k} )^T( y_{k+1}-x_{k+1}) + 2(x_{k+1} - x_k)^T(x_{k+1} - x).
\end{align}
In the next step, we invoke \cref{lem:Projection theorm} twice, for establishing an upper bound on $(x_{k+1} - y_{k+1})^T( x_k-y_{k+1} )$ and $(x_{k+1} - x_k)^T(x_{k+1} - x)$. Recall that from the projection theorem, for all vectors $u\in\mathbb{R}^n$ and $\hat{x} \in X$ we have 
$$\left(\Pi_X(u)-u\right)^T\left(\hat{x}-\Pi_X(u)\right) \geq 0.$$
By substituting $u:=x_k-{\gamma}(F(y_{k+1})+\eta_kH(y_{k+1}))$, $\hat{x}:=x$, and noting that $x_{k+1}=\Pi_X(u)$, we obtain
$$\left(x_{k+1}-x_k+{\gamma}(F(y_{k+1})+\eta_kH(y_{k+1}))\right)^T\left(x-x_{k+1}\right) \geq 0.$$
Rearranging the terms, we obtain 
\begin{align}\label{eqn:ES_main2}
(x_{k+1} - x_k)^T(x_{k+1} - x)\leq {\gamma}\left(F(y_{k+1})+\eta_kH(y_{k+1})\right)^T\left(x-x_{k+1}\right).
\end{align}
Let us invoke the projection theorem again by substituting $u:=x_k-{\gamma}(F(x_k)+\eta_kH(x_k))$, $\hat{x}:=x_{k+1}$, and noting that $y_{k+1}=\Pi_X(u)$. We obtain
\begin{align}\label{eqn:ES_main3}
(y_{k+1}-x_{k} )^T( y_{k+1}-x_{k+1})\leq {\gamma}\left(F(x_k)+\eta_kH(x_k)\right)^T\left(x_{k+1}-y_{k+1}\right).
\end{align}
From the equations~\cref{eqn:ES_main1}, \cref{eqn:ES_main2}, and \cref{eqn:ES_main3}, we obtain 
\begin{align*}
 \|x_{k+1} - x\|^2 & \leq\|x_k - x\|^2 - \|x_{k+1} - y_{k+1}\|^2 - \|y_{k+1} - x_k\|^2\nonumber\\
 &+2{\gamma}\left(F(x_k)+\eta_kH(x_k)\right)^T\left(x_{k+1}-y_{k+1}\right)\\
 & + 2{\gamma}\left(F(y_{k+1})+\eta_kH(y_{k+1})\right)^T\left(x-x_{k+1}\right).
\end{align*}
Adding and subtracting $y_{k+1}$ inside $\left(x-x_{k+1}\right)$ in the preceding relation, we obtain
\begin{align*}
 \|x_{k+1} - x\|^2 & \leq\|x_k - x\|^2 - \|x_{k+1} - y_{k+1}\|^2 - \|y_{k+1} - x_k\|^2\nonumber\\
 &+2{\gamma}\left(F(x_k)+\eta_kH(x_k)-F(y_{k+1})-\eta_kH(y_{k+1})\right)^T\left(x_{k+1}-y_{k+1}\right)\\
 & + 2{\gamma}\left(F(y_{k+1})+\eta_kH(y_{k+1})\right)^T\left(x-y_{k+1}\right).
\end{align*}
Recall that for any $a,b \in \mathbb{R}^n$, $-\|a\|^2+2a^Tb \leq \|b\|^2$. Utilizing this relation, we obtain 
\begin{align*} 
 \|x_{k+1} - x\|^2 & \leq\|x_k - x\|^2   - \|y_{k+1} - x_k\|^2\nonumber\\
 &+ {\gamma}^2\left\|F(x_k)+\eta_kH(x_k)-F(y_{k+1})-\eta_kH(y_{k+1})\right\|^2 \\
 & + 2{\gamma}\left(F(y_{k+1})+\eta_kH(y_{k+1})\right)^T\left(x-y_{k+1}\right).
\end{align*}
Invoking the Lipschitzian property of $F$ and $H$, we obtain
\begin{align}\label{eqn:EG_lemma_mm_last}
 \|x_{k+1} - x\|^2 & \leq\|x_k - x\|^2   - \|y_{k+1} - x_k\|^2+ 2{\gamma}^2(L_F^2+\eta_k^2L_H^2)\|y_{k+1} - x_k\|^2 \\
 & + 2{\gamma}\left(F(y_{k+1})+\eta_kH(y_{k+1})\right)^T\left(x-y_{k+1}\right).\notag
\end{align}
In view of the assumption ${\gamma}^2(L_F^2+\eta_k^2L_H^2) \leq 0.5$ and invoking the monotonicity of the mappings $F$ and $H$, we obtain 
\begin{align*} 
{\gamma}\left(F(x)+\eta_kH(x)\right)^T\left(y_{k+1}-x\right)   & \leq 0.5\|x_k - x\|^2   -0.5\|x_{k+1} - x\|^2 .
\end{align*}
This completes the proof for part (i).

\noindent (ii) Consider equation \cref{eqn:EG_lemma_mm_last} where $H(\bullet):=\nabla f(\bullet)$. From the convexity of $f$, we have 
$$ \nabla f(y_{k+1})^T(x-y_{k+1}) \leq f(x) - f(y_{k+1}). $$
Invoking this relation and the monotonicity of $F$, from \cref{eqn:EG_lemma_mm_last} we obtain 
\begin{align*} 
 \|x_{k+1} - x\|^2 & \leq\|x_k - x\|^2   - \|y_{k+1} - x_k\|^2+ 2{\gamma}^2(L_F^2+\eta_k^2L_H^2)\|y_{k+1} - x_k\|^2 \\
 & + 2{\gamma} F(x)^T\left(x-y_{k+1}\right)+2\gamma\eta_k (f(x) - f(y_{k+1})).\notag
\end{align*}
Rearranging the terms, we obtain the inequality in part (ii).
\end{proof}
The main convergence result for the IR-EG method is presented in the following. 
 \begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
\begin{theorem}[IR-EG's error bounds for bilevel VIs and VI-constrained optimization]\label{thm:bilevelVI}\em 
Consider problem~\cref{eqn:bilevelVI}. Let $\bar{y}_K$ be generated by \cref{alg:IR-EG}, let Assumption~\ref{assum:bilevelVI_m} hold, and let $X$ be bounded. Assume that $\{\eta_k\}$ is nonincreasing and ${\gamma}^2(L_F^2+\eta_0^2L_H^2) \leq 0.5$. Then, the following results hold. 

\noindent (i) $\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right)      \leq  (\gamma\eta_{K-1} K)^{-1}  D_X^2$ for all $K\geq 1$. 
 
 \noindent (ii) $ 0 \leq \mbox{Gap}\left(\bar{y}_K,X,F\right)       \leq  (\gamma K)^{-1}D_X^2 +\sqrt{2} C_HD_X K^{-1}\sum_{k=0}^{K-1}\eta_k$ for all $K\geq 1$. 
 
 \noindent (iii)  Further, if $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp, then for all $K\geq 1$ we have 
$$\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right) \geq -\frac{B_H}{\alpha}\left((\gamma K)^{-1}D_X^2 +\sqrt{2} C_HD_X K^{-1}\textstyle\sum_{k=0}^{K-1}\eta_k\right).$$

\noindent (iv) In the special case when $H$ is the gradient map of a convex $L$-smooth function $f$, problem~\cref{eqn:bilevelVI} captures problem~\cref{eqn:optVI} and the bounds in (i), (ii), and (iii) hold for $L_H:=L$, $C_H:=C_f$, and $B_H:=B_f$. In particular, the bounds in (i) and (iii) hold for the metric $f(\bar{y}_K) - \inf_{x \in {\tiny \mbox{SOL}(X,F)}} f(x)$. 
\end{theorem}
 \end{tcolorbox}
\begin{proof}
\noindent (i) Consider the inequality~\cref{ineq:EG_lemma_merelymonotone}. Let $x^*_F \in X^*_F\triangleq \mbox{SOL}(X,F)$ denote an arbitrary solution to $\mbox{VI}(X,F)$. Thus, we have $F(x^*_F)^T\left(y_{k+1}-x^*_F\right)\geq 0$, where we recall that $y_{k+1} \in X$. Invoking this relation, from~\cref{ineq:EG_lemma_merelymonotone}, we have for all $k\geq 0$
\begin{align}\label{ineq:thm1_separated_ineq1}
 H(x^*_F)^T\left(y_{k+1}-x^*_F\right)   & \leq 0.5(\gamma\eta_k)^{-1}\|x_k - x^*_F\|^2   -0.5(\gamma\eta_k)^{-1}\|x_{k+1} - x^*_F\|^2 ,
\end{align}
where we divided both sides by $\gamma\eta_k$. Adding and subtracting $0.5\gamma^{-1}\eta_{k-1}^{-1}\|x_{k}-x^*_F\|^2$, we obtain
\begin{align*} 
 H(x^*_F)^T\left(y_{k+1}-x^*_F\right)   & \leq 0.5\gamma^{-1}\eta_{k-1}^{-1}\|x_k - x^*_F\|^2 -0.5\gamma^{-1}\eta_k^{-1}\|x_{k+1} - x^*_F\|^2 \\
 &+0.5\gamma^{-1}\left(\eta_k^{-1}  - \eta_{k-1}^{-1}\right)\|x_k - x^*_F\|^2  .
\end{align*}
Note that $\eta_k^{-1}  - \eta_{k-1}^{-1}>0$, because $\eta_k$ is nonincreasing. We obtain
\begin{align*} 
 H(x^*_F)^T\left(y_{k+1}-x^*_F\right)   & \leq 0.5\gamma^{-1}\eta_{k-1}^{-1}\|x_k - x^*_F\|^2 -0.5\gamma^{-1}\eta_k^{-1}\|x_{k+1} - x^*_F\|^2 \\
 &+ \gamma^{-1}\left(\eta_k^{-1}  - \eta_{k-1}^{-1}\right)D_X^2  .
\end{align*}
Summing both sides for $k=1,\ldots,K-1$, we obtain
 \begin{align*} 
 H(x^*_F)^T\textstyle\sum_{k=1}^{K-1}\left(y_{k+1}- x^*_F\right)   & \leq 0.5\gamma^{-1}\eta_{0}^{-1}\|x_1 - x^*_F\|^2 -0.5\gamma^{-1}\eta_{K-1}^{-1}\|x_{K} - x^*_F\|^2 \\
 &+ \gamma^{-1}\left(\eta_{K-1}^{-1}  - \eta_{0}^{-1}\right)D_X^2.
\end{align*}
Substituting $k:=0$ in \cref{ineq:thm1_separated_ineq1} and summing the resulting inequality with the preceding relation yields
  \begin{align*} 
 H(x^*_F)^T\left(\textstyle\sum_{k=0}^{K-1}y_{k+1}-Kx^*_F\right)   & \leq 0.5\gamma^{-1}\eta_{0}^{-1}\|x_0 - x^*_F\|^2 -0.5\gamma^{-1}\eta_{K-1}^{-1}\|x_{K} - x^*_F\|^2 \\
 &+ \gamma^{-1}\left(\eta_{K-1}^{-1}  - \eta_{0}^{-1}\right)D_X^2.
\end{align*}
 Noting that $0.5\gamma^{-1}\eta_{0}^{-1}\|x_0 - x^*_F\|^2 \leq \gamma^{-1}\eta_{0}^{-1}D_X^2$ and dropping the nonpositive term $-0.5\gamma^{-1}\eta_{K-1}^{-1}\|x_{K} - x^*_F\|^2 $, we obtain 
\begin{align}\label{eqn:before_taking_sup}
 H(x^*_F)^T\left(\bar{y}_{K}-x^*_F\right)   & \leq  (\gamma\eta_{K-1} K)^{-1}  D_X^2.
\end{align}
Taking the supremum on the both sides with respect to the set ${X}^*_F$ and invoking the definition of the dual gap function, we obtain $
 \mbox{Gap}\left(\bar{y}_K,X^*_F,H\right)      \leq  (\gamma\eta_{K-1} K)^{-1}  D_X^2$.

\noindent (ii) Next we derive the bound on the infeasibility in the inner level VI. Consider the inequality~\cref{ineq:EG_lemma_merelymonotone}. From the Cauchy-Schwarz inequality, we obtain
\begin{align*}%\label{ineq:partii_thm_nested}
F(x) ^T\left(y_{k+1}-x\right)   & \leq 0.5\gamma^{-1}\|x_k - x\|^2   -0.5\gamma^{-1}\|x_{k+1} - x\|^2 +\sqrt{2} \eta_kC_HD_X.
\end{align*}
Summing both sides for $k=0,\ldots,K-1$ and dividing both sides by $K$, we obtain 
\begin{align*} 
F(x) ^T\left(\bar{y}_K-x\right)   & \leq 0.5(\gamma K)^{-1}\left(\|x_0 - x\|^2   -\|x_{K} - x\|^2\right) +\sqrt{2} C_HD_X K^{-1}\sum_{k=0}^{K-1}\eta_k .
\end{align*}
Dropping the nonpositive term and invoking the definition of $D_X$ again, we obtain 
\begin{align*} 
F(x) ^T\left(\bar{y}_K-x\right)   & \leq  (\gamma K)^{-1}D_X^2 +\sqrt{2} C_HD_X K^{-1}\sum_{k=0}^{K-1}\eta_k .
\end{align*}
Taking the supremum on the both sides with respect to the set $X$ and invoking the definition of the dual gap function, we obtain the result in (ii). 
%\begin{align*} 
% \mbox{Gap}\left(\bar{y}_K,X,F\right)   &   \leq  (\gamma K)^{-1}D_X^2 +\sqrt{2} C_HD_X K^{-1}\sum_{k=0}^{K-1}\eta_k.
%\end{align*}

\noindent (iii) This result is obtained by applying \cref{lem:bilevelVI_gap_ws} and invoking the bound in (ii).

\noindent (iv) Note that the optimality condition of problem~\cref{eqn:optVI} can be captured by the inequality $\nabla f(x^*)^T(x-x^*) \geq 0$ for all $x \in \mbox{SOL}(X,F)$. Letting $H(x):=\nabla f(x)$, the preceding inequality is equivalent to $x^* \in \mbox{SOL}(\mbox{SOL}(X,F),H)$, i.e., $x^*$ solves problem~\cref{eqn:bilevelVI}. As such (i)--(iii) hold. To show that the bound in (i) and (iii) hold for the metric $f(\bar{y}_K) - \inf_{x \in {\tiny \mbox{SOL}(X,F)}} f(x)$, consider the equation~\cref{ineq:EG_lemma_merelymonotone2}. Consider a replication of the proof of part (i) where we choose $x^*_F \in \mbox{SOL}(X,F)$ such that $x^*_F$ is an optimal solution to problem~\cref{eqn:optVI}. Following similar steps in the proof of part (i), we obtain $f(\bar{y}_K) - \inf_{x \in {\tiny \mbox{SOL}(X,F)}} f(x)\leq  (\gamma\eta_{K-1} K)^{-1}  D_X^2$ for all $K\geq 1$. 
\end{proof}
Next, we derive the convergence rate statements for the IR-EG method. 
 \begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
\begin{corollary}[IR-EG's rate statements for solving bilevel VIs and VI-constrained optimization]\label{cor:nested}\em 
Consider \cref{thm:bilevelVI} and let $\eta_{k}:=\frac{\eta_0}{(k+1)^{b}}$ where $b \in [0,1)$ is arbitrary. Then, the following results hold for all $K\geq 2^{1/(1-b)}$. 

\noindent (i) $\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right)      \leq  \left(\frac{ D_X^2}{\gamma\eta_0}\right)\frac{1}{{K}^{1-b}}$. 
 
 \noindent (ii) $ 0 \leq \mbox{Gap}\left(\bar{y}_K,X,F\right)       \leq   \left(\frac{D_X^2}{\gamma }\right)\frac{1}{K} +\left(\frac{\sqrt{2}\eta_0 C_HD_X}{1-b}\right)\frac{1}{K^b}$. 
 
 \noindent (iii)  Further, if $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp, then  
 
\noindent $\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right) \geq -B_H\alpha^{-1}\left( \left(\frac{D_X^2}{\gamma }\right)\frac{1}{K} +\left(\frac{\sqrt{2}\eta_0 C_HD_X}{1-b}\right)\frac{1}{K^b}\right)$.

\noindent (iv) \cref{thm:bilevelVI} (iv) holds true with the above updated bounds.
 
\end{corollary}
 \end{tcolorbox}
\begin{proof} Let us consider the results in \cref{thm:bilevelVI}. The proof follows by noting that $\eta_{K-1}=\frac{\eta_0}{{K}^b}$ and also that
$$\textstyle\sum_{k=0}^{K-1} \eta_k =\eta_0\textstyle\sum_{k=0}^{K-1} (k+1)^{-b} \leq \frac{\eta_0 K^{1-b}}{1-b},\qquad \hbox{for } K\geq 2^{1/(1-b)},$$ 
where the last inequality follows from \cref{lem:harmonic_series_bound} in the appendix.
\end{proof}
\begin{remark}
The results in \cref{cor:nested} imply that when $b:=0.5$, an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ can be achieved for both the outer and inner level VIs. This improves the existing complexity $\mathcal{O}(\epsilon^{-8})$ in~\cite{lampariello2022solution} for bilevel VIs. Also, for VI-constrained optimization, this improves the existing complexity $\mathcal{O}(\epsilon^{-4})$ in~\cite{doi:10.1137/20M1357378,kaushik2023incremental,jalilzadeh2022stochastic} by leveraging smoothness of $f$ and Lipschitz continuity of $F$. 
\end{remark}
\subsection{Strongly monotone bilevel VIs and VI-constrained strongly convex optimization}
In this section, we provide the rate analysis for the R-EG method. %The main assumptions are presented in the following.
\begin{assumption}\label{assum:bilevelVI_sm} \em
Consider problem~\cref{eqn:bilevelVI}. Let the following statements hold. 

\noindent (a) Set $X$ is nonempty, closed, and convex.

\noindent (b) Mapping $F:X \to \mathbb{R}^n$ is $L_F$-Lipschitz and merely monotone on $X$.

\noindent  (c) Mapping $H:X \to \mathbb{R}^n$ is  $L_H$-Lipschitz and $\mu_H$-strongly monotone on $X$.

\noindent (d) The set $\mbox{SOL}(X,F)$ is nonempty.
\end{assumption}
\begin{remark}
Let us briefly comment on the existence and uniqueness of solutions to \cref{eqn:bilevelVI} under the above assumptions. In view of \cref{rem:assum_monotone}, Assumption~\ref{assum:bilevelVI_sm} implies that $\mbox{SOL}(X,F)$ is nonempty, closed, and convex. Then, from the strong monotonicity of the mapping $H$, it follows that a solution to problem~\cref{eqn:bilevelVI} exists and is unique. 
\end{remark}
The next result provides a recursive relation that will be utilized in the rate analysis. 
\begin{lemma}\label{lem:s_monotone_lemma_ineq}\em
Consider problem~\cref{eqn:bilevelVI}. Let the sequence $\{\bar{y}_k\}$ be generated by \cref{alg:IR-EG-s}. Let Assumption~\ref{assum:bilevelVI_sm} hold. The following statements hold.


\noindent (i) Assume that {${\gamma}^2L_F^2+{\gamma}\eta\mu_H+{\gamma}^2\eta^2L_H^2\leq 0.5$}. Then, for any $x\in X$ and for all $k\geq 0$, we have
\begin{align}\label{ineq:EG_lemma_smonotone}
2{\gamma}\left(F(x)+\eta H(x)\right)^T\left(y_{k+1}-x\right)   & \leq (1-{\gamma}\eta\mu_H)\|x_k - x\|^2   -\|x_{k+1} - x\|^2.
\end{align}

\noindent (ii) Consider the special case $H(x) := \nabla f(x)$ where $f$ is a $\mu$-strongly convex and $L$-smooth function. Assume that {${\gamma}^2L_F^2+0.5{\gamma}\eta\mu+{\gamma}^2\eta^2L^2\leq 0.5$}. Then, for any $x\in X$ and for all $k\geq 0$, we have
\begin{align}\label{ineq:EG_lemma_smonotone2}
2{\gamma} F(x)^T\left(y_{k+1}-x\right) +2{\gamma}\eta(f(y_{k+1})-f(x))  & \leq (1-0.5{\gamma}\eta\mu)\|x_k - x\|^2   -\|x_{k+1} - x\|^2.
\end{align}
\end{lemma}

\begin{proof}
\noindent (i) From the monotonicity of $F$ and the strong monotonicity of $H$, we have
\begin{align*}
2{\gamma}\left(F(y_{k+1})+ {{\eta}}H(y_{k+1})\right)^T\left(  x-y_{k+1}  \right) &\leq 2{\gamma}\left(F(x)+ {{\eta}}H(x)\right)^T\left( x -y_{k+1}\right) \\
&- 2{\gamma}\eta\mu_H\|y_{k+1}-x\|^2.
\end{align*}
We can also write $-2\|y_{k+1}-x\|^2 \leq -\|x_k-x\|^2+ 2\|x_k-y_{k+1}\|^2.$ From~\cref{eqn:EG_lemma_mm_last} and the preceding two relations, we obtain 
%In view of the assumption ${\gamma}^2(L_F^2+\eta^2L_H^2) \leq 0.5$ and invoking the monotonicity of the mappings $F$ and $H$, we obtain 
\begin{align*} 
2{\gamma}\left(F(x)+\eta H(x)\right)^T\left(y_{k+1}-x\right)   & \leq (1-{\gamma}\eta\mu_H)\|x_k - x\|^2   -\|x_{k+1} - x\|^2 \\
&-(1-2{\gamma}\eta\mu_H-2{\gamma}^2(L_F^2+\eta^2L_H^2)) \|x_k-y_{k+1}\|^2.
\end{align*}
From the assumptions, $1-2{\gamma}\eta\mu_H-2{\gamma}^2(L_F^2+\eta^2L_H^2)\geq 0$. This completes the proof.  

\noindent (ii) From the strong convexity of $f$, we have 
$$ \nabla f(y_{k+1})^T(x-y_{k+1}) +\frac{\mu}{2}\|y_{k+1}-x\|^2\leq f(x) - f(y_{k+1}). $$
From the monotonicity of $F$ and the preceding relation, we have
\begin{align*}
2{\gamma}\left(F(y_{k+1})+ {{\eta}}\nabla f(y_{k+1})\right)^T\left(  x-y_{k+1}  \right) &\leq 2{\gamma} F(x)^T\left( x -y_{k+1}\right) +2\gamma\eta (f(x)-f(y_{k+1})) \\
&- {\gamma}\eta\mu\|y_{k+1}-x\|^2.
\end{align*}
We can also write $-\|y_{k+1}-x\|^2 \leq -0.5\|x_k-x\|^2+ \|x_k-y_{k+1}\|^2.$ From~\cref{eqn:EG_lemma_mm_last} and the preceding two relations, we obtain 
%In view of the assumption ${\gamma}^2(L_F^2+\eta^2L_H^2) \leq 0.5$ and invoking the monotonicity of the mappings $F$ and $H$, we obtain 
\begin{align*} 
2{\gamma}( F(x)^T\left(y_{k+1}-x\right) +\eta(f(y_{k+1})-f(x)))    & \leq(1-0.5{\gamma}\eta\mu)\|x_k - x\|^2   -\|x_{k+1} - x\|^2 \\
-(1&-{\gamma}\eta\mu-2{\gamma}^2(L_F^2+\eta^2L^2)) \|x_k-y_{k+1}\|^2.
\end{align*}
But we assumed that $1-{\gamma}\eta\mu-2{\gamma}^2(L_F^2+\eta^2L^2)\geq 0$. This completes the proof.
%The rest of the proof can be done in a similar vein to the proof of part (i). Note that one small change is that the multiplier of $\|y_{k+1}-x\|^2$ is half of that in part (i). 
\end{proof}
In the following result, we show that the generated iterate by R-EG is indeed a weighted average sequence. 
\begin{lemma}[Weighted averaging in R-EG]\label{lem:ave_REG}\em 
Let the sequence $\{\bar{y}_k\}$ be generated by \cref{alg:IR-EG-s} where $\theta_k \triangleq \frac{1}{(1- \gamma \eta \mu_H )^{k+1}}$ for $k\geq 0$. Let us define the weights $\lambda_{k,K} \triangleq \frac{\theta_k}{\sum_{j=0}^{K-1}\theta_j}$ for $k \in \{0,\ldots, K-1\}$ and $K\geq 1$. Then, for any $K\geq 1$, we have $\bar{y}_{K} = \sum_{k=0}^{K-1} \lambda_{k,K} y_{k+1}$. Also, when $X$ is a convex set, we have $\bar y_K \in X$.
%Then, we have $\bar{y}_K = \sum_{k=0}^{K-1} \frac{\theta_k}{\sum_{j=0}^{K-1} \theta_j} y_{k+1}.$ 
%$\bar{y}_{k+1} =\frac{\Gamma_k \bar{y}_{k} + \theta_{k} y_{k+1}}{\Gamma_{k+1}},$ while  $ \Gamma_{k+1} := \Gamma_k + \theta_k $  and $ \Gamma_{k+1} := \Gamma_k + \theta_k $.
\end{lemma}
\begin{proof}
We use induction on $K\geq 1$. For $K=1$, $\sum_{k=0}^0 {\lambda_{k,1}} y_{k+1} = \lambda_{0,1}y_{1} = y_{1},$ where we used $\lambda_{0,1}=1$. From \cref{alg:IR-EG-s} and the initialization $\Gamma_0 =0$, we have
\begin{align*}
&\bar y_{1}:=\frac{\Gamma_0 \bar y_0+\theta_0 y_{1}}{\Gamma_{1}}=\frac{0+\theta_0 y_{1}}{\Gamma_{0}+\theta_0} = y_{1}.
\end{align*}
From the preceding two relations, the hypothesis statement holds for $K=1$. Next, suppose the relation holds for some $K\geq 1$. In view of $\Gamma_{K}=\sum_{k=0}^{K-1}\theta_k$, we may write
\begin{align*}
	\bar{y}_{K+1} &= \frac{\Gamma_K\bar{y}_K + \theta_K y_{K+1}}{\Gamma_{K+1}} 
	 = \frac{\left(\sum_{k=0}^{K-1}\theta_k\right)\sum_{k=0}^{K-1} \lambda_{k,K} y_{k+1}+ \theta_K y_{K+1}}{\Gamma_{K+1}}\\
	 &= \frac{\sum_{k=0}^{K}\theta_k y_{k+1}}{\sum_{j = 0}^{K}\theta_j} = \sum_{k=0}^{K}\left(\tfrac{\theta_k }{\sum_{j = 0}^{K}\theta_j}\right)y_{k+1}= \sum_{k=0}^{K} \lambda_{k,K+1} y_{k+1},
	\end{align*}
implying that the induction hypothesis holds for $K+1$. In view of $\sum_{k=0}^{K-1} \lambda_{k,K}=1$, under the convexity of the set $X$, we have $\bar y_K \in X$.  
\end{proof}
We now present the error bounds for the R-EG method. Unlike in the previous section where we presented the results for bilevel VIs and VI-constrained optimization in one place, here we present the results for these problems separately, to emphasize on some distinctions. In particular, it turns out that the conditions and bounds in the following results slightly differ between the two cases. 


 \begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
\begin{theorem}[R-EG's error bounds for bilevel VIs]\label{theorem: H is SC}\em 
Consider problem~\cref{eqn:bilevelVI}. Let $\bar{y}_K$ be generated by \cref{alg:IR-EG-s}, let Assumption~\ref{assum:bilevelVI_sm} hold, and let the set $X$ be bounded. Suppose {${\gamma}^2L_F^2+{\gamma}\eta\mu_H+{\gamma}^2\eta^2L_H^2\leq 0.5$}. Then, the following results hold for all $K\geq 1$. 

\noindent (i) $\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right)      \leq    \frac{D_X^2}{ \gamma \eta}  (1- \gamma \eta \mu_H)^K $. 
 
 \noindent (ii) $ 0 \leq \mbox{Gap}\left(\bar{y}_K,X,F\right)       \leq \frac{D_X^2}{ \gamma}  (1- \gamma \eta \mu_H)^K+  \sqrt{2} C_HD_X\eta$. 
 
 \noindent (iii)  Further, if $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp, then 

 \noindent$\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right) \geq  -B_H\alpha^{-1}\left(\frac{D_X^2}{ \gamma}  (1- \gamma \eta \mu_H)^K+  \sqrt{2} C_HD_X\eta \right).$ 
\end{theorem}
   \end{tcolorbox}
\begin{proof}

\noindent (i)  Consider \cref{ineq:EG_lemma_smonotone}. Substituting $x:= x^*_F \in \mbox{SOL}(X,F)$, we have
%\begin{align*}
%2\gamma\left(F(x^*_F)+\eta H(x^*_F)\right)^T\left(y_{k+1}-x^*_F\right) &\leq (1-\gamma\eta \mu_H)\|x_k - x^*_F\|^2 -\|x^*_{k+1} -  x^*_F\|^2.
%\end{align*}
%Dividing both sides of the preceding relation by $2\gamma$, we obtain
\begin{align*}
\left(F(x^*_F)+\eta H(x^*_F)\right)^T\left(y_{k+1}-x^*_F\right) &\leq \frac{1}{2\gamma} (1-\gamma\eta\mu_H)\|x_k - x^*_F\|^2 - \frac{1}{2\gamma}\|x_{k+1} -  x^*_F\|^2.
\end{align*}
Since $x^*_F \in \mbox{SOL}(X,F)$ and $y_{k+1} \in X$, we have $F(x^*_F)^T\left(y_{k+1}-x^*_F\right) \geq 0$. We obtain
\begin{align*}
\eta H(x^*_F)^T\left(y_{k+1}-x^*_F\right) &\leq  \frac{1}{2\gamma} (1-\gamma\eta\mu_H)\|x_k - x^*_F\|^2 - \frac{1}{2\gamma}\|x_{k+1} -  x^*_F\|^2.
\end{align*}
Multiplying both sides of the preceding inequality by $\theta_k=1/{(1-\gamma \eta\mu_H)}^{k+1}$, for $k\geq 0$, we obtain 
\begin{align*}
\eta H(x^*_F)^T\left(\theta_ky_{k+1}-\theta_kx^*_F\right) &\leq  \frac{1}{2\gamma} \left( \frac{\|x_k - x^*_F\|^2}{{(1-\gamma \eta\mu_H)}^{k}}  -  \frac{\|x_{k+1} - x^*_F\|^2}{{(1-\gamma \eta\mu_H)}^{k+1}}\right).
\end{align*}
Summing the both sides over $k=0,\ldots, K-1$, for $K\geq 1$, we obtain 
\begin{align*}
\eta H(x^*_F)^T\left(\sum_{k=0}^{K-1}\theta_ky_{k+1}-\sum_{k=0}^{K-1}\theta_kx^*_F\right) &\leq  \frac{1}{2\gamma} \left(  \|x_0 - x^*_F\|^2  -  \theta_{K-1}\|x_{K} - x^*_F\|^2 \right).
\end{align*}
Dropping the nonpositive term and invoking \cref{lem:ave_REG}, we obtain  
\begin{align*}
 & H(x^*_F)^T(\bar y_{K} - x^*_F  ) \leq    (\gamma\eta)^{-1} D_X^2/\left(\textstyle\sum_{j=0}^{K-1} \theta_j\right),
\end{align*}
where we used the definition of $D_X$. Taking the supremum on both sides with respect to $x^*_F \in\mbox{SOL}( X, F)$ and recalling the definition of dual gap function, we obtain
\begin{equation*} 
\text{Gap}(\bar y_K,\mbox{SOL}( X, F), H) \leq  (\gamma\eta)^{-1} D_X^2/\left(\textstyle\sum_{j=0}^{K-1} \theta_j\right).
\end{equation*}
Next, we estimate $\sum_{j=0}^{K-1} \theta_j$. Let us define $\rho \triangleq  1/(1-\gamma \eta \mu_H) $. From the assumption ${\gamma}^2L_F^2+{\gamma}\eta\mu_H+{\gamma}^2\eta^2L_H^2\leq 0.5$, we have $\rho \in (1,2)$. We have  
\begin{align}\label{eqn:sum_theta}
\sum_{j=0}^{K-1} \theta_j = \rho+\ldots +\rho^{K} = \frac{\rho^{K+1}-\rho}{\rho-1} = \frac{\rho^K -1}{1- \frac{1}{\rho}} = \frac{1 - {(1- \gamma \eta \mu_H)}^K}{{(1- \gamma \eta \mu)}^K \gamma \eta \mu}.
\end{align} 
From the preceding two relations, we obtain 
\begin{align*} 
&\text{Gap}(\bar y_K,\mbox{SOL}( X, F), H) \leq \frac{{(1- \gamma \eta \mu_H)}^K \gamma \eta \mu_H}{1 - {(1- \gamma \eta \mu_H)}^K} \left(\frac{1}{\gamma\eta}\right) D_X^2   \\ \notag
 & = \frac{{(1- \gamma \eta \mu_H)}^K \mu_H}{1 - {(1- \gamma \eta \mu_H)}^K}     D_X^2 \leq \frac{{(1- \gamma \eta \mu_H)}^K\mu_H }{1 - {(1- \gamma \eta \mu_H)}}    D_X^2 =\frac{D_X^2}{ \gamma \eta}  (1- \gamma \eta \mu_H)^K.
\end{align*}
%This completes the proof for part (i). 

\noindent (ii) Consider \cref{ineq:EG_lemma_smonotone}. For any $x \in X$, for $k\geq 0$, we have
\begin{align*}
2{\gamma}\left(F(x)+\eta H(x)\right)^T\left(y_{k+1}-x\right)   & \leq (1-{\gamma}\eta \mu_H)\|x_k - x\|^2   -\|x_{k+1} - x\|^2.
\end{align*}
Using the Cauchy-Schwarz inequality, and invoking \cref{def:terms_bounded}, we obtain
\begin{align*}
 F(x) ^T\left(y_{k+1}-x\right)   & \leq \frac{1}{2{\gamma}} \left((1-{\gamma}\eta \mu_H)\|x_k - x\|^2   -\|x_{k+1} - x\|^2\right)+\eta \|H(x)\|\|y_{k+1}-x\| \\
 & \leq   \frac{1}{2\gamma}\left((1-\gamma\eta\mu_H)\|x_k - x\|^2-  \|x_{k+1} -  x_F\|^2\right) +  \eta C_H \sqrt{2} D_X
\end{align*}
Recall that $\theta_k = 1/{(1-\gamma \eta\mu_H)}^{k+1}$ for $k\geq 0$. Multiplying both sides by $\theta_k$, we have
\begin{align*}
 F(x) ^T\left(\theta_ky_{k+1}-\theta_kx\right)    \leq   \frac{1}{2\gamma} \left( \frac{\|x_k - x\|^2}{{(1-\gamma \eta\mu_H)}^{k}}  -  \frac{\|x_{k+1} - x\|^2}{{(1-\gamma \eta\mu_H)}^{k+1}}\right) +  \eta C_H \sqrt{2} D_X\theta_k
\end{align*}
Summing the both sides over $k=0,\ldots, K-1$, for $K\geq 1$, we obtain 
\begin{align*}
    F(x)^T\sum_{k=0}^{K-1}\theta_k\left(y_{k+1} - x\right) \leq \frac{1}{2\gamma} \left(  \|x_0 - x  \|^2  -  \theta_{K-1}\|x_{K} - x \|^2 \right)+\eta C_H \sqrt{2} D_X \sum_{k=0}^{K-1} \theta_k. 
\end{align*}
Dropping the nonpositive term and invoking \cref{lem:ave_REG}, we obtain  
\begin{align*}
 & F(x )^T(\bar y_{K} - x  ) \leq      \left({\gamma }^{-1} D_X^2 +\eta C_H \sqrt{2} D_X\right)/\left(\textstyle\sum_{j=0}^{K-1} \theta_j\right),
\end{align*}
where we used the definition of $D_X$. Taking the supremum on both sides with respect to $x  \in X$ and invoking~\cref{eqn:sum_theta} we obtain the bound in part (ii).
%\begin{equation*} 
%\text{Gap}(\bar y_K,X, F) \leq  \left((\gamma )^{-1} D_X^2+\eta C_H \sqrt{2} D_X\right)/\left(\textstyle\sum_{j=0}^{K-1} \theta_j\right).
%\end{equation*}



\noindent (iii) This result is obtained by applying \cref{lem:bilevelVI_gap_ws} and using the bound in part (i).

  

\end{proof}
 \begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
  %  \vspace{-0.1in}
\begin{corollary}[R-EG's rate statements for bilevel VIs]\label{cor: convergance rate H is SC}\em 
Consider \cref{theorem: H is SC}  and let {$\gamma \leq \frac{1}{2L_F}$, $\eta := \frac{(p+1)\ln(K)}{ \gamma\mu_H K}$} for some arbitrary $p \geq 1$. Then, the following results hold for all integers $K$ such that $\frac{K}{\ln(K)} \geq 5(p+1)\frac{L_H}{\mu_H}$. 


\noindent (i) $\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right)      \leq     \left(\frac{\mu_H D_X^2}{ p+1 }\right)\frac{1}{\ln(K)K^p} $. 
 
 \noindent (ii) $ 0 \leq \mbox{Gap}\left(\bar{y}_K,X,F\right)       \leq \left(\frac{D_X^2}{\gamma}\right)\frac{1}{ K^{p+1}}+\left(\frac{(p+1)\sqrt{2} C_H  D_X}{ \gamma\mu_H } \right)\frac{\ln(K)}{K}$. 
 
 \noindent (iii) Further, if $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp, we have 
 
\noindent $\mbox{Gap}\left(\bar{y}_K,\mbox{SOL}(X,F),H\right) \geq -B_H\alpha^{-1}\left(\left(\frac{D_X^2}{\gamma}\right)\frac{1}{ K^{p+1}}+\left(\frac{(p+1)\sqrt{2} C_H  D_X}{ \gamma\mu_H } \right)\frac{\ln(K)}{K}\right).$

\end{corollary}
\end{tcolorbox}
\begin{proof}
Note that using $\eta := \frac{(p+1)\ln(K)}{ \gamma\mu_H K}$, we may write 
\begin{align*}
&(1-\eta\gamma\mu_H)^K = \left(1-\tfrac{(p+1)\ln(K)}{K}\right)^K
=  \left(\left(1-\tfrac{(p+1)\ln(K)}{K}\right)^{\tfrac{K}{(p+1)\ln(K)}}\right)^{\ln(K^{(p+1)})}\\
&\leq \left(\lim_{K\to \infty}\left(1-\tfrac{(p+1)\ln(K)}{K}\right)^{\tfrac{K}{(p+1)\ln(K)}}\right)^{\ln(K^{(p+1)})} = (\exp(-1))^{{\ln(K^{(p+1)})}}
= \tfrac{1}{K^{(p+1)}}.
\end{align*}
 The results follow by applying \cref{theorem: H is SC}. To complete the proof, it suffices to show that {${\gamma}^2L_F^2+{\gamma}\eta\mu_H+{\gamma}^2\eta^2L_H^2\leq 0.5$}. From $\gamma \leq \frac{1}{2L_F}$, $\eta := \frac{(p+1)\ln(K)}{ \gamma\mu_H K}$, we have 
\begin{align*}
 {\gamma}^2L_F^2+{\gamma}\eta\mu_H+{\gamma}^2\eta^2L_H^2 &\leq 0.25 + \tfrac{(p+1)\ln(K)}{  K} + \tfrac{(p+1)^2\ln(K)^2L_H^2}{ \mu_H^2 K^2} \\
 &\leq 0.25+0.2+0.04\leq 0.5, 
 \end{align*}
where we used ${K}/{\ln(K)} \geq {5(p+1) L_H/\mu_H}$.
\end{proof}
We now provide the main results for addressing VI-constrained strongly optimization. 
 \begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
\begin{theorem}[R-EG's error bounds VI-constrained optimization]\label{theorem: H is SC2}\em 
Consider problem~\cref{eqn:optVI}. Let Assumption~\ref{assum:bilevelVI_sm} hold where $H$ is the gradient map of a $\mu$-strongly convex $L$-smooth function $f$. Let the set $X$ be bounded. Let $\bar{y}_K$ be generated by \cref{alg:IR-EG-s} with $\mu_H:=0.5\mu$ and $L_H:=L$.  Suppose {${\gamma}^2L_F^2+0.5{\gamma}\eta\mu+{\gamma}^2\eta^2L^2\leq 0.5$}. Then, the following results hold for all $K\geq 1$. 
 
 \noindent (i) $f(\bar{y}_K) -f(x^*)   \leq    \frac{\|x_0-x^*\|^2}{ \gamma \eta}  (1- 0.5\gamma \eta \mu)^K $. 
 
 \noindent (ii) $ 0 \leq \mbox{Gap}\left(\bar{y}_K,X,F\right)       \leq \frac{D_X^2}{ \gamma}  (1- 0.5\gamma \eta \mu)^K+  \sqrt{2} C_fD_X\eta$. 
 
 \noindent (iii)  Further, if $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp, then 
\begin{align*}
f(\bar{y}_K) -f(x^*)  & \geq  - \tfrac{\|\nabla f(x^*)\|}{\alpha}\left(\tfrac{D_X^2(1- 0.5\gamma \eta \mu)^K}{ \gamma}  +  \sqrt{2} C_fD_X\eta \right)+\tfrac{\mu}{2}\|\bar{y}_K-x^*\|^2.
\end{align*}
\end{theorem}
   \end{tcolorbox}



\begin{proof}
To show that the bound in (i) holds for the metric $f(\bar{y}_K) -  f(x^*)$, consider~\cref{ineq:EG_lemma_smonotone2}. In the proof of \cref{theorem: H is SC} (i), choose $x^*_F \in \mbox{SOL}(X,F)$ to be the optimal solution to~\cref{eqn:optVI}, denoted by $x^*$. Following similar steps in that proof, we obtain the bound for part (i). The bound in (iii) follows from \cref{lem:bilevelVI_gap_ws} and (ii). 
\end{proof}






\begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
  %  \vspace{-0.1in}
\begin{corollary}[R-EG's rate statements for VI-constrained optimization]\label{cor: convergance rate H is SC2}\em 
Consider \cref{theorem: H is SC2} and let {$\gamma \leq \frac{1}{2L_F}$ and $\eta := \frac{2(p+1)\ln(K)}{ \gamma\mu K}$} for some arbitrary $p \geq 1$. Then, the following results hold for all integers $K$ such that $\frac{K}{\ln(K)} \geq 10(p+1)\frac{L}{\mu}$. 
 
 \noindent (i) $f(\bar{y}_K) -f(x^*)   \leq     \left(\frac{\mu \|x_0-x^*\|^2}{ 2(p+1)  }\right)\frac{1}{\ln(K)K^p} $. 
 
 \noindent (ii) $ 0 \leq \mbox{Gap}\left(\bar{y}_K,X,F\right)       \leq \left(\frac{D_X^2}{\gamma}\right)\frac{1}{ K^{p+1}}+\left(\frac{2(p+1)\sqrt{2} C_f  D_X}{ \gamma\mu } \right)\frac{\ln(K)}{K}$.
 
 \noindent (iii)  Further, if $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp, then 
\begin{align*}
\|\bar{y}_K-x^*\|^2 &\leq    \tfrac{2\|\nabla f(x^*)\|}{\mu\alpha}\left(\tfrac{D_X^2}{\gamma K^{p+1}}+\left(\tfrac{2(p+1)\sqrt{2} C_f  D_X}{ \gamma\mu } \right)\tfrac{\ln(K)}{K} \right)  + \tfrac{\|x_0-x^*\|^2}{(p+1 )\ln(K)K^p}.
\end{align*}
%
% \noindent (iv) In the special case where $H$ is the gradient map of a $\mu$-strongly convex $L$-smooth function $f(x)$, problem~\cref{eqn:bilevelVI} captures problem~\cref{eqn:optVI}. 
% Let $\gamma \leq \frac{1}{2L_F}$, $\eta := \frac{2(p+1)\ln(K)}{ \gamma\mu K}$ for $p \geq 1$. Then, he bounds in (i), (ii), (iii) hold for $L_H:=L$ and $\mu_H:=0.5\mu$, for all integers $K$ such that $\tfrac{K}{\ln(K)} \geq 8(p+1)\left({L}/{\mu}\right)^2$. In particular, the bound in (i) holds for the metric $f(\bar{y}_K) - \inf_{x \in {\tiny \mbox{SOL}(X,F)}} f(x)$. 
 
\end{corollary}
\end{tcolorbox}
\begin{proof}
Consider the results of \cref{theorem: H is SC2}. First, when $\eta := \frac{2(p+1)\ln(K)}{ \gamma\mu K}$, similar to the proof of \cref{cor: convergance rate H is SC}, we have $(1-0.5\eta\gamma\mu)^K \leq   {K^{-(p+1)}}$. Second, we show that the condition ${\gamma}^2L_F^2+0.5{\gamma}\eta\mu+{\gamma}^2\eta^2L^2\leq 0.5$ is met. We have 
 \begin{align*}
 {\gamma}^2L_F^2+0.5{\gamma}\eta\mu+{\gamma}^2\eta^2L^2 &\leq 0.25 + \tfrac{(p+1)\ln(K)}{  K} + \tfrac{4(p+1)^2\ln(K)^2L^2}{ \mu^2 K^2} \\
 &\leq 0.25+0.1+0.04\leq 0.5, 
 \end{align*}
 where we used the assumption ${K}/{\ln(K)} \geq 10(p+1) {L}/{\mu} $. The bound in (iii) can be obtained by combining (i) and (iii) in \cref{theorem: H is SC2}.
 \end{proof}

\begin{remark}
 \cref{cor: convergance rate H is SC2} equips R-EG with iteration complexities of $\mathcal{O}\left(\epsilon^{-1/p}\right)$ and $\mathcal{O}\left(\epsilon^{-1/(p+1)}+\epsilon^{-1}\right)$ for VI-constrained optimization in terms of the outer and inner level, respectively, for any arbitrary $p\geq 1$. These guarantees appear to be new and are significantly faster than the existing ones. Also, similar complexities are provided in \cref{cor: convergance rate H is SC} that appear to be novel for bilevel VIs. %Note that the inner level complexity is nearly optimal for VIs.
\end{remark}
%\fy{TODO: add a remark to talk about relaxing boundedness of X for the upper level. The lower level bound can hold for restricted dual gap function when X is unbounded}
 
 
\subsection{Convergence theory for VI-constrained nonconvex optimization}\label{sec:ncvx}
Our goal in this section is to derive convergence rate statements for \cref{alg:ncvx-IR-GD} in computing a stationary point to the VI-constrained nonconvex optimization problem~\cref{eqn:optVI} under the following assumptions.
\begin{assumption}\label{assum:bilevelVI_nc} \em
Consider problem~\cref{eqn:optVI}. Let the following statements hold. 

\noindent (a) Set $X$ is nonempty, compact, and convex.

\noindent (b) Mapping $F:X \to \mathbb{R}^n$ is $L_F$-Lipschitz continuous and merely monotone on $X$.

\noindent  (c) Function $f:X \to \mathbb{R}$ is real-valued, possibly nonconvex, and $L$-smooth on $X$.

\noindent (d) The set $\mbox{SOL}(X,F)$ is $\alpha$-weakly sharp. 
\end{assumption}
Next, we define a metric for qualifying stationary points to problem~\cref{eqn:optVI}.
\begin{definition}[Residual mapping]\label{def:res_map}\em
Let Assumption~\ref{assum:bilevelVI_nc} hold and $0<\hat{\gamma} \leq \frac{1}{L}$. For all $x \in X$, let $G_{1/\hat{\gamma}}:X \to \mathbb{R}^n$ be defined as 
$$G_{1/\hat{\gamma}}(x) \triangleq \tfrac{1}{\hat{\gamma}}\left(x-\Pi_{\footnotesize \mbox{SOL}(X,F)}\left[x-\hat{\gamma}\nabla f(x)\right]\right).$$
\end{definition}
\begin{remark}
Note that under Assumption~\ref{assum:bilevelVI_nc}, $\mbox{SOL}(X,F)$ is nonempty, compact, and convex. Also, $f^*\triangleq \inf_{x\, \in \, {\footnotesize \mbox{SOL}}(X,F)} f(x)>-\infty$. 
In view of Theorem~10.7 in~\cite{book:AmirB}, $x^* \in \mbox{SOL}(X,F)$ is a stationary point to problem~\cref{eqn:optVI} if and only if $G_{1/\hat{\gamma}}(x^*)=0$. Utilizing this result, we consider $\|G_{1/\hat{\gamma}}(\bullet)\|^2$ as a well-defined error metric in the analysis of the proposed method. 
\end{remark}
Next, we introduce some terms that will help with the analysis of \cref{alg:ncvx-IR-GD}. 
\begin{definition}\label{def:IPREG_terms}\em Let us define the following terms for $k\geq 0$. 
\begin{align*}
 & \hat{x}_{k+1} \triangleq  \bar y_{k,T_k}, \qquad z_k \triangleq \hat x_k - \hat{\gamma} \nabla f(\hat x_k),\\  
  & \delta _k\triangleq\hat x_{k+1}-\Pi_{X^*_F}\left(z_k\right) , \qquad e_k \triangleq \hat x_k - \Pi_{X^*_F}\left[\hat x_k\right], \qquad \hbox{where }X^*_F \triangleq \mbox{SOL}(X,F).
\end{align*}
\end{definition}
\begin{remark}\label{rem:e_k_sol_rel}
As explained earlier in \cref{sec:IPREG_desc}, at each iteration of IPR-EG, we employ R-EG to compute an inexact but increasingly accurate projection onto the unknown set $X^*_F$. Accordingly, $\delta_k$ measures the inexactness of projection at iteration $k$ of the underlying gradient descent method. Also, $e_k$ measures the infeasibility of the generated iterate $\hat{x}_k$ at iteration $k$ with respect to the unknown constraint set $X^*_F$. Indeed, by definition it follows that $\|e_k\| = \mbox{dist}(\hat{x}_k,\mbox{SOL}(X,F))$ for $k\geq 0$.
\end{remark}
To establish the convergence of the generated iterate by IPR-EG to a stationary point, the term $\|\hat{x}_{k+1}-\hat{x}_k\|^2$ may seem relevant. The following result builds a relation between this term and the residual mapping. 
\begin{lemma}\label{subsection:error-metric}\em
Let $\{{\hat x}_{k}\}$ be generated by \cref{alg:ncvx-IR-GD}. Then, for all $k \geq 0$,
\begin{align}\tfrac{\hat\gamma^2}{2}  \left\| G_{{1}/{\hat\gamma}}(\hat x_k) \right\|^2 \leq    \|\hat  x_{k+1} - \hat x_k\|^2 +  \| \delta_k \|^2.
\end{align}
\end{lemma}
\begin{proof}
From \cref{def:res_map} and~\cref{def:IPREG_terms}, we have
$$G_{{1}/{\hat\gamma}}(\hat x_k) = \tfrac{1}{\hat\gamma}{\left(\hat x_k -   \Pi_{X^*_F}\left[\hat x_k - \hat\gamma \nabla{f\left(\hat x_k\right)}\right] \right)} = \tfrac{1}{\hat\gamma}{\left(\hat x_k - {\hat x_{k+1} + \delta_k}\right)} .$$
The result follows by invoking $\|u+v\|^2 \leq 2\|u\|^2+2\|v\|^2$ for all $u,v \in \mathbb{R}^n$.
\end{proof}
In the following, we obtain an error bound on $\|G_{1/\hat{\gamma}}(\bullet)\|^2$ characterized by $\delta_k$ and $e_k$. 
\begin{proposition}\label{prop:upper_bound_gradient}\em
Consider problem~\cref{eqn:optVI}. Let Assumption~\ref{assum:bilevelVI_nc} hold and let the sequence $\{{\hat x}_{k}\}$ be generated by \cref{alg:ncvx-IR-GD}. Suppose  $\hat{\gamma} \leq \tfrac{1}{2L}$. Then, for all $k\geq 0$ we have 
%\begin{align}\label{prop: uper G before sum}
%& \tfrac{\hat\gamma}{4}  \| G_{{1}/{\hat\gamma}}(\hat x_k) \|^2  \leq f(\hat x_k) - f(\hat x_{k+1})+ \tfrac{\hat\gamma^2C_f^2}{2 }  +\left(\tfrac{5+5\hat\gamma}{2\hat\gamma^2}\right)  \| \delta _k\|^2  + \left(\tfrac{2+\hat\gamma}{\hat\gamma^2}  \right)\| e_k \|^2.
%\end{align}
\begin{align}\label{prop: uper G before sum}
  \| G_{{1}/{\hat\gamma}}(\hat x_k) \|^2  \leq \frac{4}{\hat\gamma} \left(f(\hat x_k) - f(\hat x_{k+1})\right)+  L\hat\gamma C_f^2  +\frac{80}{L\hat{\gamma}^3} ( \| \delta _k\|^2  + \| e_k \|^2).
\end{align}
\end{proposition}

\begin{proof} 
(a) In view of $y:=\Pi_{{X^*_F}}(\hat{x}_k) \in {X^*_F}$, from \cref{lem:Projection theorm}, we have for $k\geq 0$
\begin{equation*}%\label{inequality:projection-d}
   \left( \Pi_{X^*_F}\left(z_k\right) - \Pi_{X^*_F}\left(\hat x_k\right)\right) ^T  \left(z_k- \Pi_{X^*_F}\left(z_k\right)\right) \geq 0.
\end{equation*}
Recall that $\delta _k =\hat x_{k+1} -\Pi_{X^*_F}\left(z_k\right)  $ and
$e_k = \hat x_k - \Pi_{X^*_F}\left(\hat x_k\right)$. We obtain
\begin{equation*}%\label{inequality:projection-sub-d}
\left(\left(\hat x_{k+1} - \delta_k\right) -\left(\hat x_k - e_k\right) \right)^T \left( z_k - \left(\hat x_{k+1} - \delta_k \right)\right) \geq 0.
\end{equation*}
From the preceding inequality, we have
\begin{equation}
\left(\hat x_{k+1} - \hat x_{k}\right) ^T \left(z_k - \hat x_{k+1}\right) + \left(\hat x_{k+1} - \hat x_{k}\right) ^T \delta_k +   \left(e_k - \delta_k\right) ^T \left(z_k - \hat x_{k+1} + \delta_k\right)
\geq 0.
\end{equation}
From \cref{alg:ncvx-IR-GD}, we have that $z_k = \hat x_k - \hat{\gamma} \nabla f(\hat x_k)$. We obtain
\begin{align*}%\label{inequality:projection-sub2-d}
&\underbrace{\left(\hat x_{k+1} - \hat x_{k}\right) ^T \left(\hat x_k - \hat{\gamma} \nabla f(\hat x_k) - \hat x_{k+1}\right) }_\text{Term 1} + \underbrace{\left(\hat x_{k+1} - \hat x_{k}\right) ^T \delta_k}_\text{Term 2} \\
&+  \underbrace{  \left(e_k - \delta_k\right) ^T \left(\hat x_k - \hat{\gamma} \nabla f(\hat x_k) - \hat x_{k+1}\right) }_\text{Term 3} +  \underbrace{\left( e_k -\delta_k \right) ^T  \delta_k}_\text{Term 4}
\geq 0.
\end{align*}
Here we expand Term~1, apply the inequality $u^T v \leq \frac{1}{2\theta} \|u\|^2 + \frac{\theta}{2} \|v\|^2$ in bounding Term~2 and  Term~3, for some arbitrary $\theta>0$, and apply the inequality $u^T v \leq \frac{1}{2} \|u\|^2 + \frac{1}{2} \|v\|^2$ in bounding Term~4. We have
\begin{align*}
&\underbrace{-\hat{\gamma}\nabla f(\hat x_k)^T(\hat x_{k+1}-\hat x_{k}) - \|\hat x_{k+1}-\hat x_k\|^2}_\text{Term 1} 
 +\underbrace{\frac{1}{2\theta} \|\hat x_{k+1} - \hat x_k\|^2 +\frac{\theta}{2}  \|\delta_k\|^2}_{  \text{Term 2} \leq}  
\\ &
 +\underbrace{\theta \| e_k-\delta_k \|^2 + \frac{1}{2\theta}\|\hat x_{k+1} - \hat x_k\|^2 + \frac{\hat{\gamma}^2}{2\theta}\|\nabla f(\hat x_k)\|^2   }_{  \text{Term 3} \leq} 
+ \underbrace{\frac{1}{2}\| e_k -\delta_k\|^2 + \frac{1}{2}\|\delta_k\|^2 }_{  \text{Term 4} \leq}
\geq 0.
\end{align*}
Hence, we have 
\begin{align*}
 \hat \gamma\nabla f(\hat x_k)^T(\hat x_{k+1}-\hat x_{k}) &\leq \left(\theta^{-1} -1 \right)\|\hat x_{k+1}-\hat x_k\|^2+ \frac{\hat\gamma^2}{2\theta } \| \nabla f(\hat x_k)\|^2 + 0.5\left( \theta  +1 \right)  \| \delta _k\|^2 \\
 & + \left(\theta +0.5 \right ) ( 2\| e_k \|^2+2\| \delta _k\|^2).
\end{align*}
Let us choose {$\theta := \frac{2}{\hat\gamma L }$}. Note that from the assumption $\hat\gamma\leq \tfrac{1}{2L}$, we have $\theta\geq 4$. Before we substitute $\theta$, using $\theta\geq 4$ we obtain
\begin{align*}
 \hat \gamma\nabla f(\hat x_k)^T(\hat x_{k+1}-\hat x_{k}) &\leq \left(\theta^{-1} -1 \right)\|\hat x_{k+1}-\hat x_k\|^2+ \frac{\hat\gamma^2C_f^2}{2\theta }   + 5\theta ( \| \delta _k\|^2  + \| e_k \|^2) .
\end{align*}
Dividing both sides by $\hat{\gamma}$ and substituting $\theta := \frac{2}{\hat\gamma L }$ we obtain
\begin{align*}
\nabla f(\hat x_k)^T(\hat x_{k+1}-\hat x_{k}) \leq
\left(\tfrac{L}{2}-\tfrac{1}{\hat{\gamma}}\right  )\|\hat x_{k+1}-\hat x_k\|^2+ \tfrac{L\hat\gamma^2C_f^2}{4 }  +\tfrac{10}{L\hat{\gamma}^2} ( \| \delta _k\|^2  + \| e_k \|^2).
\end{align*}
From the $L$-smoothness property of $f$, we have 
\begin{equation*}
f(\hat x_{k+1}) \leq f(\hat x_k) + \nabla f(\hat x_k)^T (\hat x_{k+1} - \hat x_k) + \tfrac{L}{2} \|\hat x_{k+1} - \hat x_k\|^2.
\end{equation*}
From the two preceding relations, we obtain
%\begin{equation}
%f(\hat x_{k+1}) \leq f(\hat x_k) + \left( \frac{\hat\gamma - 1}{\hat\gamma}\right  )\|\hat x_{k+1}-\hat x_k\|^2+ \frac{\hat\gamma^2C_f^2}{2 }  +(\frac{5+3\hat\gamma}{2 \hat\gamma^2})  \| \delta _k\|^2  + (\frac{2+\hat\gamma}{\hat\gamma^2}   )\| e_k \|^2+ \frac{L}{2} \|\hat x_{k+1} - \hat x_k\|^2,
%\end{equation}
%
%
%which implies
%
%
\begin{equation*}
\left(\tfrac{1}{\hat{\gamma}}-L\right) \|\hat x_{k+1}-\hat x_k\|^2 \leq f(\hat x_k) - f(\hat x_{k+1})+ \tfrac{L\hat\gamma^2C_f^2}{4 } +\tfrac{10}{L\hat{\gamma}^2} ( \| \delta _k\|^2  + \| e_k \|^2).
\end{equation*}
Note that $\tfrac{1}{2\hat{\gamma}} \leq \left(\tfrac{1}{\hat{\gamma}}-L\right) $. Utilizing this bound, then adding $\tfrac{1}{2\hat{\gamma}} \| \delta_k\|^2$ to the both sides, and invoking \cref{subsection:error-metric}, we obtain 
\begin{equation*}
 \tfrac{\hat\gamma}{4}  \| G_{{1}/{\hat\gamma}}(\hat x_k) \|^2  \leq f(\hat x_k) - f(\hat x_{k+1})+ \tfrac{L\hat\gamma^2C_f^2}{4 } +\tfrac{20}{L\hat{\gamma}^2} ( \| \delta _k\|^2  + \| e_k \|^2),
\end{equation*}
where we used $\tfrac{1}{2\hat{\gamma}} \leq \tfrac{10}{L\hat{\gamma}^2} $ in view of the assumption $\hat\gamma\leq \tfrac{1}{2L}$.
%
%\begin{equation}\label{inequality:err}
%\underbrace{(\frac{1-\hat\gamma}{ \hat\gamma} - \frac{L}{2})}_{\text{Term 1}} \left( \|\hat  x_{k+1} - \hat x_k\|^2 +  \| \delta_k \|^2 \right) \leq f(\hat x_k) - f(\hat x_{k+1})+ \frac{\hat\gamma^2C_f^2}{2 }  +(\frac{5+5\hat\gamma-2\hat\gamma^2-L\hat\gamma^2}{2\hat\gamma^2})  \| \delta _k\|^2  + (\frac{2+\hat\gamma}{\hat\gamma^2}   )\| e_k \|^2.
%\end{equation}
%
%
%Let us suppose that $\hat\gamma \leq \frac{1}{2+L}$, then we have $\frac{1} {2 \hat\gamma} \leq \text{Term 1} $. Therefore, we obtain
%
%\begin{equation}\label{inequality:err01}
%\frac{1} {2 \hat\gamma} \underbrace{\left( \| \hat x_{k+1} - \hat x_k\|^2 +  \| \delta_k \|^2 \right)}_{\text{Term 1}} \leq  f(\hat x_k) - f(\hat x_{k+1})+ \frac{\hat\gamma^2C_f^2}{2 }  +(\frac{5+5\hat\gamma-2\gamma^2-L\hat\gamma^2}{2\hat\gamma^2})  \| \delta _k\|^2  + (\frac{2+\hat\gamma}{\hat\gamma^2}   )\| e_k \|^2.
%\end{equation}
%
%By invoking Lemma \ref{subsection:error-metric}, we have $\text{Term 1} \geq  \frac{\hat\gamma^2}{2}  \| G_{\tfrac{1}{\hat\gamma}}(\hat x_k) \|^2 $. So, we obtain
\end{proof} 
In the next step, we derive bounds on the feasibility error term $e_k$ and the inexactness error term $\delta_k$. Of these, the bound on $e_k$ is obtained by directly invoking the rate results we developed for R-EG. The bound on $\delta_k$, however, is less straightforward.  
\begin{proposition} \label{Upper bound for e_k}\em
Consider problem~\cref{eqn:optVI}. Let Assumption~\ref{assum:bilevelVI_nc} hold and let the sequence $\{{\hat x}_{k}\}$ be generated by \cref{alg:ncvx-IR-GD}. Then, for all $k\geq 1$ we have 

\noindent  (i) $    \|e_k\| \leq  \left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) \tfrac{\ln(T_{k})}{T_{k}}.$
%\left(\frac{25 D_X^2}{\alpha\gamma}\right) \frac{\ln(T_{k})}{T_{k}} . 

\noindent  (ii) $       \| \delta_k \|^2  \leq  \left( \frac{ D_X^2}{ 3  } + \left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) ^2 \right)\frac{2(\ln(T_{k}))^2}{T_{k}^2}+\left(\tfrac{2\hat{\gamma} C_f(25 D_X^2+17C_fD_X\hat{\gamma})}{\alpha\gamma}\right) \frac{\ln(T_{k})}{T_{k}}.$

 
%\begin{align*}
%& (i)\     \|e_k\|^2 \leq \left(\frac{2D_X^4}{\alpha^2\gamma^2}\right)\frac{1}{ T_{k-1}^6}+\left(\frac{\fy{72 C_f^2}  D_X^2}{ \alpha^2\gamma^2 } \right)\frac{(\ln(T_{k-1}))^2}{T_{k-1}^2}. \\
%& (ii)\         \| \delta_k \|^2  \leq  \frac{2D_X^2}{3\ln(T_{k-1})T_{k-1}^2} + \left(\frac{4D_X^4}{\alpha^2\gamma^2}\right)\frac{1}{ T_{k-1}^6} + \left(\frac{72 B_H^2  D_X^2}{ \alpha^2\gamma^2 } \right)\frac{{(\ln(T_{k-1}))}^2}{T_{k-1}^2}\\ 
%& \qquad\quad\quad +
%    \left(\frac{\hat \gamma C_f 2D_X^2 }{\gamma\alpha}\right)\frac{1}{ T_{k-1}^3} +     \left(\frac{\hat \gamma C_f6\sqrt{2} B_H  D_X }{ \gamma\alpha } \right)\frac{\ln(T_{k-1})}{T_{k-1}}.
%\end{align*}
\end{proposition}

\begin{proof}
\noindent (i) Recall that $\|e_k\| = \mbox{dist}(\hat{x}_k,\mbox{SOL}(X,F))$. Invoking \cref{lem:bilevelVI_gap_ws}, we have
\begin{align}\label{eqn:e_k_gap}
\alpha \|e_k\| \leq \mbox{Gap}(\hat{x}_k,X,F), \qquad \hbox{for all } k\geq 0.
\end{align} 
From \cref{alg:ncvx-IR-GD}, we have $\hat{x}_k=\bar y_{{k-1}, T_{k-1}}$ for $k \geq 1$, where $\bar y_{{k-1}, T_{k-1}}$ is the output of R-EG after $T_{k-1}$ number of iterations employed for solving the projection problem~\cref{eqn:proj_problem_nc}. Consider the projection problem~\cref{eqn:proj_problem_nc} and let $k\geq 1$ be fixed. We apply \cref{cor: convergance rate H is SC2} where we choose $p:=2$ and $K:=T_{k-1}$. Note that the condition $\frac{T_{k-1}}{\ln(T_{k-1})} \geq 10(p+1)L/\mu=30$ is met as $L=\mu=1$ for the objective function of the projection problem and that $T_{k-1} \geq 151$ (see \cref{alg:ncvx-IR-GD}). Let us define $h(x) \triangleq \tfrac{1}{2}\|x-z_k\|^2$. Therefore, from \cref{cor: convergance rate H is SC2} (ii), we have 
$${\mbox{Gap}}\left(\bar y_{{k-1}, T_{k-1}},X,F\right)       \leq\left(\tfrac{D_X^2}{\gamma}\right)\tfrac{1}{ T_{k-1}^3}+\left(\tfrac{6\sqrt{2} C_h  D_X}{ \gamma } \right)\tfrac{\ln(T_{k-1})}{T_{k-1}}, \qquad \hbox{for all } k\geq 1,$$
where $C_h \triangleq \sup_{x \in X}\|\nabla h(x)\|$. Recalling $z_k \triangleq \hat x_k - \hat{\gamma} \nabla f(\hat x_k)$, we have 
$$C_h = \sup_{x \in X} \|x-z_k\| \leq  \sup_{x \in X} \|x-\hat{x_k}\| + \hat{\gamma}\| \nabla f(\hat x_k)\| \leq  \sqrt{2}D_X + \hat{\gamma}C_f.$$ From the preceding relation and the equation~\cref{eqn:e_k_gap}, we have for $k \geq 1$ 
\begin{align*}
\|e_k\|  &\leq
 \tfrac{D_X^2}{\alpha\gamma T_{k-1}^3} +\tfrac{\left(12D_X^2+6\sqrt{2}C_fD_X\hat{\gamma}\right)\ln(T_{k-1})}{\alpha\gamma T_{k-1}}  \leq \left(\tfrac{12.5 D_X^2+6\sqrt{2}C_fD_X\hat{\gamma}}{\alpha\gamma}\right) \tfrac{\ln(T_{k-1})}{T_{k-1}},
\end{align*}  where we used $\tfrac{1}{ T_{k-1}^3}\leq\tfrac{0.5\ln(T_{k-1})}{T_{k-1}}$ in view of $T_{k-1} \geq 151$. Next we relate the preceding bound to $T_k$ (This is done to simplify the presentation of the next results). Note that from $T_k:=\max\{k\sqrt{k},151\}$, we have  $\frac{\ln(T_{k-1})}{T_{k-1}} \leq 2\frac{\ln(T_{k-1})}{T_{k}} \leq 2\frac{\ln(T_{k})}{T_{k}}$ for all $k \geq 1$.  


 %, and invoking the inequality $u^T v \leq \frac{1}{2} \|u\|^2 + \frac{1}{2} \|v\|^2$.


\noindent (ii) Let us define $y^*_h\triangleq \Pi_{X^*_F}\left(z_k\right)$. Invoking \cref{cor: convergance rate H is SC2} (iii) for the projection problem (where $L=\mu=1$, $p=2$) and using the bound in part (i), we obtain
\begin{align}\label{bound_delta_in_proof_lemma}
 \|\bar{y}_{k,T_{k}}-y^*_h\|^2 &\leq  \left(\tfrac{2 D_X^2}{ 3  }\right)\tfrac{1}{\ln(T_{k})T_{k}^2}    +   2\|\nabla h(y^*_h)\| \left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) \tfrac{\ln(T_{k})}{T_{k}}.
\end{align}
 Note that $\bar{y}_{k,T_{k}}-y^*_h = \hat x_{k+1} -\Pi_{X^*_F}\left(z_k\right) =\delta_k$ and $\nabla h(y^*_h)=z_k-y^*_h$.  Next, we derive a bound on $\|y^*_h-z_k\|$. Using $z_k=\hat{x}_k -\hat{\gamma}\nabla f(\hat{x}_k)$, we have
\begin{align*}
  \|z_k - y^*_h\|  &= \| z_k - \Pi_{X^*_F}(z_k) \|  \leq \| z_k - \Pi_{X^*_F}(\hat{x}_k) \| = \| \hat{x}_k -\hat{\gamma}\nabla f(\hat{x}_k) - \Pi_{X^*_F}(\hat{x}_k) \| \\
& \leq \| \hat{x}_k - \Pi_{X^*_F}(\hat{x}_k) \|+\hat{\gamma}\|\nabla f(\hat{x}_k)\|  \leq \|e_k\| + \hat{\gamma} C_f,
\end{align*}
where we used \cref{def:IPREG_terms} and that $\hat{x}_k \in X$. This is because $\hat{x}_k = y_{k-1,T_{k-1}}$ and that, from \cref{lem:ave_REG},  $y_{k-1,T_{k-1}} \in X$. Thus, from \cref{bound_delta_in_proof_lemma} we have 
\begin{align*} 
 \|\delta_k \|^2 &\leq  \left(\tfrac{ 2D_X^2}{ 3  }\right)\tfrac{1}{\ln(T_{k})T_{k}^2}    + 2 (\|e_k\| + \hat{\gamma} C_f)\left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right)\tfrac{\ln(T_{k})}{T_{k}} .
\end{align*}
The result is obtained from part (i) and that $\tfrac{1}{\ln(T_{k})T_{k}^2} \leq \tfrac{(\ln(T_{k}))^2}{T_{k}^2} $ for $k \geq 1$.
\end{proof}
Next, we derive iteration complexity guarantees for the IPR-EG method in computing a stationary point to problem~\cref{eqn:optVI} when the objective function is nonconvex. 
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!55!black]
\begin{theorem}[IPR-EG's guarantees for VI-constrained nonconvex optimization]\label{theorem:NC}\em 
Consider problem~\cref{eqn:optVI}. Let Assumption~\ref{assum:bilevelVI_nc} hold. Let the sequence $\{\hat {x}_k\}$ be generated by \cref{alg:ncvx-IR-GD}.
 Then, the following results hold.

\noindent (i) [Infeasibility bound] For all $1 \leq k \leq K$, we have 
$$ \mbox{dist} (\hat x_k, \mbox{SOL}(X,F)) \leq  \left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) \frac{\ln(\max\{k\sqrt{k},151\})}{k\sqrt{k}}.$$

\noindent (ii) [Residual mapping bound] Let $\hat \gamma = \frac{1}{\sqrt{K}}$ where $K\geq \max\{2,4L^2\}$. Then, 
\begin{align*}%\label{theorem-nc- bound G} 
    \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2  &\leq   \frac{\theta_1 (\ln(K))^2}{{K}\sqrt {K}}   +\frac{\theta_2\ln(K) }{\sqrt {K}} +\frac{\theta_3}{\sqrt {K}},
\end{align*}
where $\theta_1\triangleq \left(   D_X^2  + 3\left(\tfrac{25 D_X^2+17C_fD_X}{\alpha\gamma}\right) ^2 \right)\tfrac{6560\, }{L  }$, $\theta_2\triangleq \left(\tfrac{1920\, C_f(25 D_X^2+17C_fD_X)}{\alpha\gamma L}\right)     $, $\theta_3 \triangleq 8D_f+2 LC_f^2$,  $D_f \triangleq \sup_{x,y \in X} (f(x)-f(y))$, and we define $\hat x_K^*$ such that $$ \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2= \min_{k=\lfloor {\frac{K}{2}}\rfloor, \ldots, K-1} \| G_{{1}/{\hat\gamma}}(\hat x_k) \|^2.$$

\noindent (iii) [Overall iteration complexity] Let $\epsilon>0$ be an arbitrary scalar. To achieve $ \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2 \leq \epsilon$, the total number of projections onto the set $X$ is $ {\tilde{\mathcal{O}}}(\epsilon^{-5})$ where $\tilde{\mathcal{O}}$ ignores polylogarithmic and constant numerical factors.
\end{theorem}

\end{tcolorbox}









\begin{proof}
\noindent (i) This result follows from \cref{Upper bound for e_k} and \cref{rem:e_k_sol_rel}.


\noindent (ii) Consider equation \cref{prop: uper G before sum}. Taking the sum on the both sides for $k=\lfloor {\frac{K}{2}}\rfloor, \ldots, K-1$, we obtain 
\begin{align*} 
 \sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  \| G_{{1}/{\hat\gamma}}(\hat x_k) \|^2  \leq \frac{4}{\hat\gamma} \left(f(\hat x_{\lfloor {\frac{K}{2}}\rfloor}) - f(\hat x_{K})\right)+  K L\hat\gamma C_f^2  +\frac{80}{L\hat{\gamma}^3}\sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  ( \| \delta _k\|^2  + \| e_k \|^2).
\end{align*}
From the definition of $\hat x_K^*$ and $D_f$ and that $\tfrac{K}{2} \leq K-\lfloor\tfrac{K}{2}\rfloor$, we obtain 
\begin{align}\label{eqn:thm_G_1}
\frac{K}{2}    \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2  \leq \frac{4D_f}{\hat\gamma} +  K L\hat\gamma C_f^2  +\frac{80}{L\hat{\gamma}^3}\sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  ( \| \delta _k\|^2  + \| e_k \|^2).
\end{align}
From \cref{Upper bound for e_k}, we have
\begin{align}\label{eqn:thm_G_2} \| e_k \|^2+   \| \delta_k \|^2  &\leq \left(   D_X^2  + 3\left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) ^2 \right)\tfrac{(\ln(T_{k}))^2}{T_{k}^2}\notag\\
&+\left(\tfrac{2 C_f(25 D_X^2+17C_fD_X\hat{\gamma})}{\alpha\gamma}\right) \tfrac{\hat{\gamma}\ln(T_{k})}{T_{k}}.
\end{align}
Recall that $T_k = \max\{k^{1.5},151\}\geq k$. Invoking \cref{lemma:ineqHarmonic_beta_negative}, for $K\geq 2$ we have
\begin{align}\label{eqn:thm_G_3}
& \textstyle\sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  \frac{(\ln(T_{k}))^2}{T_{k}^2}  \leq (\ln(K^{1.5}))^2 \textstyle\sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  \tfrac{1}{{k}^3} =2.25(\ln(K))^2 \textstyle\sum_{k= \lfloor {\frac{K}{2}}\rfloor -1}^{K-2}  \tfrac{1}{{(k+1)}^3} \\
 &= 2.25(\ln(K))^2 \left(\lfloor K/2\rfloor^{-3} +\lfloor{K/2}\rfloor^{-2}-K^{-2} \right) \leq 4.5(\ln(K))^2 \lfloor {K/2}\rfloor^{-2} \leq \tfrac{41(\ln(K))^2}{K^2},\notag
\end{align}
where we used $\lfloor {K/2}\rfloor \geq K/3$ for $K\geq 2$. From \cref{lemma:ineqHarmonic_beta_negative}, we may also write 
\begin{align}\label{eqn:thm_G_4}
& \textstyle\sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  \tfrac{\ln(T_{k})}{T_{k}} \leq \ln(K^{1.5})\textstyle\sum_{k= \lfloor {\frac{K}{2}}\rfloor}^{K-1}  \frac{1}{k^{1.5}} = 1.5\ln(K)\textstyle\sum_{k= \lfloor {\frac{K}{2}}\rfloor-1}^{K-2}  \frac{1}{(k+1)^{1.5}} \\
&= 1.5\ln(K)  \left(\lfloor K/2\rfloor^{-1.5} +\lfloor{K/2}\rfloor^{-0.5}-K^{-0.5} \right) \leq 3\ln(K)\lfloor {K/2}\rfloor^{-0.5} \leq \tfrac{6\ln(K)}{\sqrt{K}},\notag
\end{align}
using $\lfloor {K/2}\rfloor \geq K/4$ for $K\geq 2$. From \cref{eqn:thm_G_1}, \cref{eqn:thm_G_2}, \cref{eqn:thm_G_3}, and \cref{eqn:thm_G_4}, we obtain 
\begin{align*} 
    \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2  & \leq \tfrac{8D_f}{\hat\gamma K} +   2 L\hat\gamma C_f^2  +  \left(   D_X^2  + 3\left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) ^2 \right)\tfrac{6560\, (\ln(K))^2}{L\hat{\gamma}^3 K^3}
     \\
    &       +\left(\tfrac{2 C_f(25 D_X^2+17C_fD_X\hat{\gamma})}{\alpha\gamma}\right) \tfrac{960\, \hat{\gamma}\ln(K)}{L\hat{\gamma}^3K\sqrt{K}}  .
\end{align*}
Now, substituting $\hat \gamma := \frac{1}{\sqrt {K}}$ for $K\geq \max\{4L^2,2\}$, we obtain 
 

\begin{align*} 
    \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2  &  \leq \tfrac{8D_f+2 L\, C_f^2}{\sqrt {K}}    +  \left(   D_X^2  + 3\left(\tfrac{25 D_X^2+17C_fD_X\hat{\gamma}}{\alpha\gamma}\right) ^2 \right)\tfrac{6560\, (\ln(K))^2}{L  K\sqrt{K} }
     \\
    &       +\left(\tfrac{2 C_f(25 D_X^2+17C_fD_X\hat{\gamma})}{\alpha\gamma}\right) \tfrac{960\, \ln(K)}{L \sqrt{K}}  .
\end{align*}
 
\noindent (iii) Note that from (ii), we have $  \| G_{{1}/{\hat\gamma}}(\hat x_K^*) \|^2 \leq \mathcal{O} \left( \frac{\ln(K)}{\sqrt{K}}\right) =\tilde{\mathcal{O}} \left( \frac{1}{\sqrt{K}}\right)$. Consider \cref{alg:ncvx-IR-GD}. At each iteration in the outer loop, $2T_k$, i.e, $\max\{2k^{1.5},302\}$, projections are performed onto the set $X$. Invoking \cref{lemma:ineqHarmonic_beta_negative}, we conclude that the total number of projections onto the set $X$ is $\tilde{\mathcal{O}}(\epsilon^{-5})$. 
\end{proof}
\begin{remark}
\cref{theorem:NC} provides new complexity guarantees in computing a stationary point to VI-constrained optimization problems with a smooth nonconvex objective function. To our knowledge, such guarantees for computing the optimal NE in nonconvex settings did not exist before.
\end{remark}
\section{Numerical experiments}\label{sec:num}
In this section, we provide preliminary empirical results to validate the performance of the proposed methods in computing optimal equilibria. As mentioned in \cref{sec:intro}, in monotone games, the set of all equilibria is often unavailable which renders a challenge in validating the performance of our methods in computing the true optimal equilibrium. To circumvent this issue, we consider a simple illustrative example of a two-person zero-sum Nash game, provided in~\cite{jalilzadeh2022stochastic}, where the set $\mbox{SOL}(X,F)$ is explicitly available. This game is as follows. 

\noindent\begin{minipage}{.47\linewidth}
\begin{equation*}%\label{eqn:player1problem}
\Bigg\{\begin{aligned}
 &\min_{x_1} \   f_1(x_1,x_2) \triangleq 20-0.1x_1x_2+x_1 \\
& \hbox{s.t.}  \   x_1 \in X_1 \triangleq \{x_1 \mid 11 \leq x_1\leq 60\}, 
\end{aligned}
\end{equation*}
\end{minipage} 
\begin{minipage}{.47\linewidth}
\begin{equation*}%\label{eqn:player2problem}
\Bigg\{\begin{aligned}
 &\min_{x_2} \   f_2(x_1,x_2) \triangleq -20+0.1x_1x_2-x_1 \\
& \hbox{s.t.}  \  x_2 \in X_2 \triangleq \{x_2 \mid 10 \leq x_2\leq 50\}. 
\end{aligned}
\end{equation*}
\end{minipage}
\vspace{.1in}

Note that the above game can be formulated as $\mbox{VI}(X,F)$ where $F(x)\triangleq Ax+b$, with $A=[ 0,-0.1  ;0.1,0 ]$ and $b=[1;0]$, and the set $X\triangleq X_1\times X_2$. The mapping $F$ is merely monotone, in view of $u^TAu=0$ for all $u \in \mathbb{R}^2$. Also, $F$ is $L_F$-Lipschitz with $L_F = \|A\|=0.1$, where $\|\bullet\|$ denotes the spectral norm. Further, this game has a spectrum of equilibria given by $\mbox{SOL}(X,F) = \{(x_1,x_2)\mid 11 \leq x_1 \leq 60, \ x_2=10\}$.
 
In the experiments, we seek the best and the worst NE with respect to the metric $\psi(x) \triangleq \frac{1}{2}\|x\|^2$. The best-NE seeking problem is $\min_{x \in {\tiny \mbox{SOL}(X,F)}} \ f(x)\triangleq \psi(x)$ with the unique best NE as $x^*_{{\tiny \mbox{B-NE}} }= (11,10)$,  while the worst-NE seeking problem is $\min_{x \in {\tiny \mbox{SOL}(X,F)}} \ f(x)\triangleq -\psi(x)$ with the unique worst NE as $x^*_{{\tiny \mbox{W-NE}}} = (60,10)$. This is the unique stationary point to $\min_{x \in {\tiny \mbox{SOL}(X,F)}} \ -\psi(x)$, in view of the condition $0 \in -\nabla \psi(x^*_{{\tiny \mbox{W-NE}}} ) + \mathcal{N}_{\tiny \mbox{SOL}(X,F)}(x^*_{{\tiny \mbox{W-NE}}})$, where $\mathcal{N}$ denotes the normal cone. Accordingly, the true value of the PoS and the PoA ratios are given as 
$$\mbox{PoS} = \tfrac{\psi(x^*_{{\tiny \mbox{B-NE}}})}{\min_{x \in X} \ \psi(x)\ } = \tfrac{0.5\|(11,10)\|^2}{0.5\|(11,10)\|^2} = 1, \quad \mbox{PoA} = \tfrac{\psi(x^*_{{\tiny \mbox{W-NE}}})}{\min_{x \in X} \ \psi(x)\ } = \tfrac{0.5\|(60,10)\|^2}{0.5\|(11,10)\|^2} = 16.74.$$
This implies that at the best NE, the game attains full efficiency, while at the worst NE, the efficiency deteriorates by more than $16$ times. 
 \subsection*{The methods}  We implement our proposed methods and compare them with the methods in~\cite{facchinei2014vi}, where two schemes are developed for solving VI-constrained problem~\cref{eqn:optVI} in convex and nonconvex cases. Note that complexity guarantees are not provided in~\cite{facchinei2014vi}. We implement inexact variants of Algorithms 1 and 3 in~\cite{facchinei2014vi}, denoted by ISR-cvx and ISR-ncvx, respectively, where ISR denotes an inexact sequential regularization technique that is employed in these methods to approximate the Tikhonov trajectory~\cite{FacchineiPang2003,doi:10.1137/20M1357378}. Of these, ISR-cvx is suitable for addressing~\cref{eqn:optVI} with a convex objective and is essentially a two-loop regularized gradient scheme where in the outer loop, the regularization parameter is updated and decreased by a factor $\rho \in (0,1)$, while in the inner loop, a gradient method is employed for inexactly computing the unique solution to the regularized VI. The ISR-ncvx is suitable for the nonconvex case and employs ISR-cvx at each iteration to inexactly compute a projection onto the set of equilibria. ISR-ncvx is characterized by two parameters $\delta$ and $\beta$~\cite[Algorithm~3]{facchinei2014vi}.  
 
 \subsection*{The experiments and set-up} We run three sets of experiments as follows. {\bf (E1)} We implement IR-EG, R-EG, and ISR-cvx for computing the best NE. For these methods, we use the same constant stepsize $\gamma:=1/(2\|A\|_{\mathcal{F}})$ where $\|\bullet\|_{\mathcal{F}}$ denotes the Frobenius norm. For IR-EG and ISR-cvx, we let $\eta_0=0.01$ and for R-EG, we use the self-tuned constant regularization parameter in \cref{cor: convergance rate H is SC2}. For {ISR-cvx}, we consider the choices $\rho \in \{0.1,0.5,0.9\}$. {\bf (E2)} We implement IPR-EG and ISR-ncvx for computing the worst NE where for IPR-EG, we follow the parameter choices outlined in \cref{alg:ncvx-IR-GD} and for ISR-ncvx, we use $\delta=\beta=0.5$. {\bf (E3)} Lastly, we employ R-EG for approximating the PoS and use IPR-EG for approximating PoA. In all the experiments, we use the initial iterate $(40,40) \in X$.
 
\subsection*{Results and insights} %The results of experiments (E1), (E2), and (E3) are presented in \cref{fig:E1}, \cref{fig:E2}, and \cref{fig:E3}, respectively. 
A summary of the key insights is provided as follows. 
% Figure environment removed



% Figure environment removed





% Figure environment removed

\noindent {\bf (E1)} In computing the best NE in \cref{fig:E1}, R-EG outperforms the other schemes. This observation is indeed aligned with the accelerated rate statements provided in \cref{cor: convergance rate H is SC2} for R-EG. The IR-EG method performs relatively fast, but slower than R-EG. This is because unlike in R-EG where we utilize the strong convexity of the objective function $f$, IR-EG is devised primarily for addressing merely convex objectives and it does not leverage the strong convexity property of $f$. As observed, the performance of ISR-cvx appears to be significantly sensitive to the decay factor $\rho$, emphasizing on the importance of the update rules for the regularization parameter. Indeed, one of the key contributions in our work is to provide prescribed update rules for the regularization parameter in the single-timescale methods IR-EG and R-EG. 

\noindent {\bf (E2)} In computing the worst NE in \cref{fig:E2}, the sequence of iterates generated by IPR-EG appears to successfully converge to the unique stationary point $x^*_{{\tiny \mbox{W-NE}}} = (60,10)$. This is indeed aligned with the theoretical guarantees in \cref{theorem:NC}. We observe, however, that the sequence of iterates generated by ISR-ncvx appears to converge to a vector in the set $\mbox{SOL}(X,F)$ that deviates from the worst NE. A similar observation was observed when we used different values for the initial point, $\beta$, or $\delta$. To plot the error in terms of the residual mapping, we use $\ln(\bullet +\exp(1))$ to better present the performance of the metric when the residual mapping is approaching zero. Notably, even though both methods start from the same initial point, the initial residual mapping takes different values between the two methods. This is because, unlike in IPR-EG where a constant stepsize is used, ISR-ncvx leverages a line search technique to find a suitable stepsize at each iteration. 

\noindent {\bf (E3)} Lastly, in \cref{fig:E3}, we observe that the PoS ratio approximated by R-EG and PoA ratio approximated by IPR-EG converge to their true values, i.e., $1$ and $16.74$, respectively, as evaluated earlier. We note that in this particular experiment, the stationary point to the worst-NE seeking problem is unique. In general, however, a stationary point may not necessarily be among the worst equilibria.

\section{Concluding remarks}\label{sec:conclude}
In noncooperative game theory, understanding the quality of Nash equilibrium has been a storied area of research and has led to the emergence of popular metrics including the Price of Anarchy and the Price of Stability. The evaluation of these ratios is complicated by the need to compute the worst and the best equilibrium, respectively. In this paper, our goal is to devise a class of iteratively regularized extragradient methods with performance guarantees for computing the optimal equilibrium. To this end, we consider optimization problems with merely monotone variational inequality (VI) constraints when the objective function is either (i) merely convex, (ii) strongly convex, or (iii) nonconvex. In (i), we considerably improve the existing complexity guarantees. In  (ii) and (iii), we derive new complexity guarantees for solving this class of problems. Notably, this appears to be the first work where nonconvex optimization with monotone VI constraints is addressed with complexity guarantees. We further show that our results in (i) and (ii) can be generalized to address a class of bilevel VIs that subsumes challenging problem classes such as the generalized Nash equilibrium problem. Extensions of the results in this work to stochastic regimes are among interesting directions for future research.


%\noindent Problems~\eqref{eqn:player1problem} and~\eqref{eqn:player2problem} together construct a  two-person zero-sum Nash game. From~\cite[1.4.2 Proposition]{facchinei02finite}, the set of saddle-points are the solutions to the variational inequality problem $\mbox{VI}(X,F)$ where we define $$F(x_1,x_2) \triangleq (\nabla_{x_1} f_1(x), \nabla_{x_2} f_2(x))= (-0.1x_2+1, 0.1x_1) \quad \hbox{and} \quad X\triangleq X_1\times X_2.$$ 
%Note that the mapping $F$ is merely monotone, in view of $(F(x)-F(y))^T(x-y) =0$ for all $x \in \mathbb{R}^2$ and $y \in \mathbb{R}^2$. We observe that the set of all the saddle-points is given by $\mbox{SOL}(X,F) = \{(x_1,x_2)\mid x_1 \in [11,60], \ x_2=10\}$, implying that there are infinitely many Nash equilibria to this game characterized by the convex set $\mbox{SOL}(X,F)$.

 

%\section*{Acknowledgments}
%We would like to acknowledge the assistance of volunteers in putting
%together this example manuscript and supplement.

\bibliographystyle{siamplain}
\bibliography{references}


\appendix
\section{Supplementary material} 
\begin{lemma}\label{lem:example1_monotone_F}\em
Consider problem~\cref{prob:opt_select} and let $X$ and $F$ be given by~\cref{eqn:def_example1} where the set $Y\subseteq \mathbb{R}^p$ is closed and convex and functions $g$, $h_j$, $j=1,\ldots,q$ are continuously differentiable and convex. Then, the mapping $F$ is merely monotone on $X$. %Moreover, suppose that $g$ is $L_g$-smooth on $Y$ and $h_j$ is linear, i.e., $h_j = $
\end{lemma}
\begin{proof}
Suppose $x_1,x_2 \in X$ are arbitrary given vectors. From \cref{eqn:def_example1}, we can write 
\begin{align*}
 (F(x_1)-F(x_2))^T(x_1-x_2) &=  (\nabla g(y_1) -\nabla g(y_2) )^T(y_1-y_2)\\
 &+ \sum_{j=1}^q(\nabla h_j(y_1)\lambda_{1,j}-\nabla h_j(y_2)\lambda_{2,j})^T(y_1-y_2)\\
 & +\sum_{j=1}^q(h_j(y_2)-h_j(y_1))(\lambda_{1,j}-\lambda_{2,j}),
\end{align*} 
where $\lambda_{1,j}$ and $\lambda_{2,j}$ denote the $j$th dual variable in $x_1$ and $x_2$, respectively. Let us define $\mathcal{J}_{+} \triangleq \{j \in [q]\mid \lambda_{1,j}-\lambda_{2,j} \geq 0\}$ and $\mathcal{J}_{-} \triangleq \{j \in [q]\mid \lambda_{1,j}-\lambda_{2,j} < 0\}$. Then, adding and subtracting the two terms $\sum_{j \in \mathcal{J}_{+}}\nabla h_j(y_1)\lambda_{2,j} $ and  $\sum_{j \in \mathcal{J}_{-}}\nabla h_j(y_2)\lambda_{1,j} $, we obtain
\begin{align*}
& (F(x_1)-F(x_2))^T(x_1-x_2) =  (\nabla g(y_1) -\nabla g(y_2) )^T(y_1-y_2)\\
 &+ \sum_{j \in \mathcal{J}_{+}}^q(\nabla h_j(y_1)\lambda_{1,j}-\nabla h_j(y_1)\lambda_{2,j}+\nabla h_j(y_1)\lambda_{2,j}-\nabla h_j(y_2)\lambda_{2,j})^T(y_1-y_2)\\
  &+ \sum_{j \in \mathcal{J}_{-}}^q(\nabla h_j(y_1)\lambda_{1,j}-\nabla h_j(y_2)\lambda_{1,j}+\nabla h_j(y_2)\lambda_{1,j}-\nabla h_j(y_2)\lambda_{2,j})^T(y_1-y_2)\\
 & +\sum_{j=1}^q(h_j(y_2)-h_j(y_1))(\lambda_{1,j}-\lambda_{2,j}).
\end{align*}
Rearranging the terms, we obtain 
\begin{align*}
& (F(x_1)-F(x_2))^T(x_1-x_2) =  (\nabla g(y_1) -\nabla g(y_2) )^T(y_1-y_2)\\
 &+ \sum_{j \in \mathcal{J}_{+}}^q(\lambda_{1,j}-\lambda_{2,j})\nabla h_j(y_1)^T(y_1-y_2)+ \sum_{j \in \mathcal{J}_{+}}^q\lambda_{2,j}(\nabla h_j(y_1)-\nabla h_j(y_2))^T(y_1-y_2)\\
  &+ \sum_{j \in \mathcal{J}_{-}}^q\lambda_{1,j}(\nabla h_j(y_1)-\nabla h_j(y_2))^T(y_1-y_2)+\sum_{j \in \mathcal{J}_{-}}^q(\lambda_{1,j}-\lambda_{2,j})\nabla h_j(y_2)^T(y_1-y_2)\\
 & +\sum_{j=1}^q(h_j(y_2)-h_j(y_1))(\lambda_{1,j}-\lambda_{2,j}).
\end{align*}
From the nonnegativity of the Lagrange multipliers and convexity of functions $g$ and $h_j$, we have 
\begin{align*}
& (F(x_1)-F(x_2))^T(x_1-x_2) \geq  \\
 &+ \sum_{j \in \mathcal{J}_{+}}^q(\lambda_{1,j}-\lambda_{2,j})\left(h_j(y_2)-h_j(y_1)+\nabla h_j(y_1)^T(y_1-y_2)\right)\\
  &+\sum_{j \in \mathcal{J}_{-}}^q(\lambda_{1,j}-\lambda_{2,j})\left(h_j(y_2)-h_j(y_1)+\nabla h_j(y_2)^T(y_1-y_2)\right).
\end{align*}
In view of convexity of $h_j$, we have that $h_j(y_2)-h_j(y_1)+\nabla h_j(y_1)^T(y_1-y_2) \geq 0$ and $h_j(y_2)-h_j(y_1)+\nabla h_j(y_2)^T(y_1-y_2) \leq 0$.  Invoking the definitions of $\mathcal{J}_{+}$  and $\mathcal{J}_{-} $, we have $(F(x_1)-F(x_2))^T(x_1-x_2) \geq 0$ and thus, $F$ is merely monotone on $X$. 
\end{proof}
{\begin{lemma}[{\cite[Lemma~2.14]{doi:10.1137/20M1357378}}]\label{lem:harmonic_series_bound}\em Let $\alpha \in [0,  1)$ and $K\geq 1$ be an integer. Then for all $K\geq   {2}^{1/(1-\alpha)}$, we have $\frac{{K}^{1-\alpha}}{2(1-\alpha)} \leq \textstyle\sum_{k = 0}^{K-1}(k+1)^{-\alpha}\leq \frac{K^{1-\alpha}}{1-\alpha}.$
\end{lemma}
\begin{lemma}[{\cite[Lemma~9]{yousefian2017smoothing}}]\label{lemma:ineqHarmonic_beta_negative}\em
For any scalar $\alpha$ and integers $\ell$ and $N$ where $0\leq \ell \leq N-1$, the following results hold.
\begin{itemize}
\item [(a)] $\ln\left( \frac{N+1}{\ell+1}\right) \leq \sum_{k=\ell}^{N-1}\frac{1}{k+1} \leq \frac{1}{\ell+1}+\ln\left( \frac{N}{\ell+1}\right)$.
\item [(b)] $\frac{N^{\alpha+1}-(\ell+1)^{\alpha+1}}{\alpha+1}\leq \sum_{k=\ell}^{N-1}(k+1)^\alpha \leq (\ell+1)^\alpha+\frac{(N+1)^{\alpha+1}-(\ell+1)^{\alpha+1}}{\alpha+1}$, for $\alpha \neq -1$. 

\end{itemize}
\end{lemma}}
\end{document}
