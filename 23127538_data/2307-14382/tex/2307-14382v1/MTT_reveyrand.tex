%
% IEEE Transactions on Microwave Theory and Techniques example
% Tibault Reveyrand - http://www.microwave.fr
%
% http://www.microwave.fr/LaTeX.html
% ---------------------------------------



% ================================================
% Please HIGHLIGHT the new inputs such like this :
% Text :
%  \hl{comment}
% Aligned Eq. 
% \begin{shaded}
% \end{shaded}
% ================================================



\documentclass[journal]{IEEEtran}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}  
\usepackage{xcolor,soul,framed} %,caption

\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb} % for \mathbb
\usepackage{comment}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{soul}

\newcommand{\miaojing}[1]{\textcolor{red}{MS: #1}}
\newcommand{\papertomention}[1]{\textcolor{green}{Paper: #1}}

\hyphenation{op-tical net-works semi-conduc-tor}

%\bstctlcite{IEEE:BSTcontrol}
\newcommand{\mynorm}[1]{ \left\| #1 \right\| }
\usepackage{soul}
\newcommand{\MWS}[2]{{\color[rgb]{0.4,0,0}\st{#1}} \color[rgb]{0,0.33,0}#2\normalcolor}
\newcommand{\Correction}[2]{{\color[rgb]{0.4,0,0}\st{#1}} \color[rgb]{0,0.5,0.5}#2\normalcolor} 

\usepackage{xspace}
\newcommand{\etc}{{\it etc.}\xspace}
\newcommand{\ie}{{\it i.e.},\xspace}
\newcommand{\eg}{{\it e.g.},\xspace}
\newcommand{\via}{{\it via}\xspace}
\newcommand{\viz}{{\it viz.}\xspace}
\newcommand{\cf}{{\it cf.},\xspace}
\newcommand{\etal}{{\it et al}\xspace}
\newcommand{\apriori}{{\it a priori}\xspace}
\newcommand{\adhoc}{{\it ad hoc}\xspace}

%=== TITLE & AUTHORS ====================================================================
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
    \title{When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review}
  \author{
  %Maxime Fontana, Michael Spratling, and Miaojing Shi ~\IEEEmembership{}% <-this % stops a space
  \IEEEauthorblockN{
  \textsuperscript{1}Maxime Fontana, 
  \textsuperscript{1}Michael Spratling, 
  \textsuperscript{1,2}Miaojing Shi} \\
    \IEEEauthorblockA{\textsuperscript{1}Department of Informatics, King's College London
    \\\ \textsuperscript{2}College of Electronic and Information Engineering, Tongji University} \\
    \IEEEauthorblockA{\fontfamily{qcr}\selectfont maxime.fontana@kcl.ac.uk; michael.spratling@kcl.ac.uk; miaojing.shi@kcl.ac.uk}  
  \thanks{}}
 %(e-mail: maxime.fontana@kcl.ac.uk, michael.spratling@kcl.ac.uk, miaojing.shi@kcl.ac.uk)}  


% The paper headers
%\markboth{ 
%}{Roberg \MakeLowercase{\textit{et al.}}: High-Efficiency Diode and Transistor Rectifiers}


% ====================================================================
\maketitle

% === ABSTRACT ====================================================================
% =================================================================================
\begin{abstract}
%\boldmath
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. 
This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising from such a multi-objective optimisation scheme. Third, it introduces how task groupings can be achieved by analysing task relationships. Fourth, it focuses on how partially supervised methods applied to MTL can tackle the aforementioned challenges. Lastly, this review presents the available datasets, tools and benchmarking results of such methods.
\end{abstract}


% === KEYWORDS ====================================================================
% =================================================================================
\begin{IEEEkeywords}
Multi-Task Learning; Deep Learning; Minimal Supervision; Autonomous Driving; Visual Understanding; Medical Imaging; Robotic Surgery
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:IEEE transactions template
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
% The paper headers
\markboth{M.Fontana et al.}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpeerreviewmaketitle


% ====================================================================
% ====================================================================
% ====================================================================




% === I. INTRODUCTION =============================================================
% =================================================================================
\section{Introduction}

%\IEEEPARstart{C}{} 
Convolutional Neural Networks (CNNs) have achieved great success in numerous and diverse computer vision tasks such as classification \cite{classification-sota, classification-success-1, classification-success-2, classifcation-success-3}, semantic segmentation \cite{semantic-segmentation-sota, Fast-R-CNN, unet, semantic-success1} and object-detection \cite{object-detection-sota, YOLOv3, semantic-success1}. These models have the common characteristic of being task specific. However, systems should ideally be capable of sharing knowledge between tasks.

Multi-Task Learning (MTL) \cite{Caruana} aims at providing computational models able to learn multiple tasks. To achieve this, MTL seeks to partition representations into task-agnostic and task-specific features so that each task can utilise a common representation. This is justified by previous work investigating the learning of representations in CNNs that distinguish two types of features. Firstly, shallow layers, which learn simple patterns (\ie edges and colors), are task-agnostic and should be shared. Secondly, deep layers which learn complex patterns (\ie objects), should be kept task-specific \cite{visualizing-CNN}. However, determining how to partition a specific network hierarchy is not trivial and depends on the tasks at hand \cite{cross-stich}. 
Nonetheless, MTL could help discover relationships and structure amongst tasks \cite{predictive-structures, task-relationships} which could improve performance compared to task-specific models. From a computational efficiency perspective, sharing representations results in enhanced memory efficiency and a significant reduction in inference time as shared representations only need to be inferred once to predict multiple tasks. 

Deep Learning (DL) models generally suffer from a high data-dependency during training, but acquiring large volumes of labeled data is not always feasible. This has motivated the development of various partial supervision configurations, with the unifying goal to create data-efficient DL solutions \cite{partly-supervised-MTL, MTPSL, semi-supervised-MTL-for-semantics-depth, Semi-supervised-MTL, semi-MTL-for-lung-cancer}. MTL brings a new opportunity for such techniques: by leveraging relationships between tasks, MTL can use the available supervisory signals for one task to aid the learning of other tasks.

\textbf{Applications.}
MTL is currently being employed in Computer Vision (CV) due to its success in achieving advanced scene understanding. Its most studied area is urban scene detection \cite{multinet, end-to-end-autonomous-driving, unified-autonomous-driving, Andrej-talk}, specifically to address autonomous driving related tasks such as road segmentation and object detection. MTL has also been successfully used in robotics, specifically in robotic-assisted surgery \cite{AP-MTL,ST-MTL} to predict diverse effects from a surgery scene (instruments, tissues etc.). Additionally, this paradigm has been heavily studied in the context of face recognition \cite{face_recognition-facial-expression, face-parsing, privacy-preserving-face, MTLFace} to enable, for instance, the simultaneous prediction of facial expression, face detection, and identification. MTL has also been explored in medical applications, such as in medical image segmentation in the area of gastroenterology for detection of polyps \cite{upper-gastro, explicable-capsule-endoscopy, esophageal}, or in cardiology for atrial segmentation \cite{3d-cardiac}. In addition, MTL has been applied to non-CV scenarios such as Natural Language Processing (NLP) \cite{MTL-NLP-networks, NLP-MTL-ADAPTIVE, NLP-NER, Vietnamese-POS, NLP-SEQ, NLP-POS} and recommendation systems \cite{MTL-recommendation-systems-1, MTL-recommendation-systems-2, MTL-recommendation-systems-3}.

\textbf{Related Work.} 
MTL has been the subject of numerous and diverse review papers \cite{MTL-overview-2017,MTL-in-DNN-2020, MTL-survey-2021,MTL-dense-predic-tasks, Deep-reinforcement-learning-survey, MTL-NLP-survey, MTL-NLP-overview}. Some of these previous works have focused on specific domains such as NLP. For instance, \cite{MTL-NLP-overview} focuses on MTL-based solutions for various NLP tasks and provides a classification for available solutions, whilst \cite{MTL-NLP-survey} focuses on NLP-related training procedures and task relatedness. Alternatively, \cite{Deep-reinforcement-learning-survey} reviews MTL in the domain of deep reinforcement learning (DRL). Other published reviews have focused on MTL from an optimisation perspective, for instance by comparing the different loss weighting techniques \cite{a-comparison-of-loss-weightin-strategies} or by evaluating task-specific transfer learning strategies \cite{taskonomy, factors-of-influence}. 

Some works have, however, aimed at providing a less constrained review of MTL. For instance, \cite{MTL-overview-2017, MTL-in-DNN-2020} reviews fully-supervised MTL methods as well as the inherent optimisation challenges under the deep learning framework. Moreover, \cite{MTL-survey-2021} provides a full-fledged and comprehensive review on both linear and DL solutions as well as the underlying optimisation techniques.

Previous work has focused on area more closely related to this study. For example, a CV-focused review \cite{MTL-dense-predic-tasks} analyses how MTL has been applied to pixel-wise prediction tasks and provides benchmark results on common fully-supervised MTL architecture. \cite{MTL-dense-predic-tasks} further differentiates MTL architectures based on the location where task interactions take place (encoder vs decoder). This paper, in contrast, does not highlight such differentiation as architectural issues are not the focus of our analysis. This review instead focuses on partially-supervised learning paradigms applied to CV tasks in a multi-task fashion. Although the vast majority of multi-task learning solutions has been applied to dense prediction tasks, this work aims at providing a comprehensive understanding of how MTL's future improvement might be underpinned by increasing the number and diversity of tasks. This study is the first, to the best of our knowledge, to focus on partially-supervised MTL for CV.

\textbf{Paper overview.} \cref{chapter:MT-parameter-sharing} reviews traditional fully-supervised MTL methods from a parameter-sharing perspective. 
\cref{chapter:Optimisation} introduces challenges arising from such multi-objective optimisation. 
\cref{sec:task-grouping} analyses relationships between common CV tasks, and how task groupings can be used to identify mutually beneficial tasks. 
\cref{chapter:partial-supervision} discusses how MTL can be used under partially-supervised paradigms. Last, \cref{sec:datasets} is dedicated to an introduction to available datasets, code repositories and tools as well as a comparison of the solutions introduced in this review.



% === II. Multi-Task Parameter Sharing =============================================================
% =================================================================================
\section{Multi-Task Parameter Sharing}
\label{chapter:MT-parameter-sharing}
In order to understand the underlying challenges to MTL, \cref{sec:non-neural-MTL} reviews cross-task parameter sharing introduced in traditional settings. Subsequently, \cref{sec:feature-fusion} will review feature fusion paradigms under two major frameworks: CNN and Vision Transformers. Then, \cref{sec:knowledge-decomposition} investigates how learned representations can be partitioned and further shared. Finally, \cref{sec:parameter-NAS} will focus on architecture search based strategies as a way to share parameters across different tasks. 

\subsection{Traditional Parameter Sharing}
\label{sec:non-neural-MTL}
\subsubsection{Sparse Multi-Task Representations}
\label{sec:sparse}
The core of the early work in MTL has focused on obtaining a sparse multi-task parameter matrix generally obtained by linear models such as support vector machines (SVM) or ridge regression. The sparsity objective is based on the assumption that only a low-dimensional sub-representation of parameters should be shared across all the tasks. For example, \textit{Multi-Task Feature Learning} (MTFL) \cite{MT-feature-learning} defines the objective as a regularisation optimisation using the \textit{$L_{1}$-normalisation}. Considering a linear feature matrix $\mathbf{U} \in \mathbb{R}^{d x d}$ where $d$ is the parameter dimension, MTFL \cite{MT-feature-learning} aims at learning a transformation matrix $\mathbf{A} \in \mathbb{R}^{d x T}$ where $T$ is the number of tasks, such that $\mathbf{W}$ = $\mathbf{UA}$, with $\mathbf{W} \in \mathbb{R}^{d x T}$. Formally, such optimisation can be defined as:

\begin{equation} \label{eq:sparse}
f(\textbf{A}, \textbf{U}) = \sum_{t=1}^{T} \sum_{i=1}^{m} L(y_{ti}, a_{t} \cdot (\textbf{U}^{T}x_{ti})) + \gamma||\textbf{A}||^{2}_{1}
\end{equation}
where the first term is the empirical error for the $i^{th}$ data-label pair $(x_{ti}, y_{ti})$ for a task $t$. In the second term, the transformation matrix $\textbf{A}$ is constrained by the $L_{1}$\textit{-norm}. 
The sparsity is controlled by the non-negative parameter $\gamma$. Intuitively, the sparsity imposed on the transformation matrix $\mathbf{A}$ will lead to most rows in $\mathbf{A}$ being equal to 0. After the transformation $\textbf{W = UA}$, these rows will represent task-specific parameters whilst others represent the shared low-dimensional subspace $\textbf{W}$ across tasks. 
Finally, MTFL \cite{MT-feature-learning} introduces a convex formulation of this optimisation and jointly learns both the partition and the parameters. Alternatively, the authors suggest that non-linear features can be obtained through the use of kernels \cite{learning-multiple-tasks-with-kernel-methods}. 

Following this optimisation objective, previous work has investigated using different linear models such as the Group Lasso Method \cite{taking-advantage-of-sparsity}, by improving over the convergence and speed of the sparsity constraint, or by minimising the trace-norm of $\mathbf{A}$ \cite{MTL-l21, trace-norm-minimization}. 
Nonetheless, this paradigm is essentially constrained to only a small subset of shared features. Moreover, it also assumes tasks are related as some features are shared anyway. However, intuition suggests it should not always be the case. To counter this, some works \cite{dirty-model, learning-task-grouping-and-overlap} allow for an adaptive and partial overlapping of the task parameters to only share parameters when necessary.


\subsubsection{Clustering}
\label{sec:clustering}
To mitigate the a-priori assumption that all tasks are related, some works have investigated how to identify task relationships under a task clustering framework. Such methods are motivated by the assumption that similar tasks have similar weight vectors. Obtaining such clusters helps narrow down the search space for the shared low-dimensional parameter space.
For instance, \cite{clustering-learning-tasks} introduces a Task Clustering (TC) algorithm based on K-nearest Neighbours (KNN) in which information is shared within clusters. Specifically, given two tasks $T_{1}$ and $T_{2}$, performance gain (PG) is calculated for the task pair through transfer learning (\ie $PG_{T_{1} \rightarrow T_{2}}$ if knowledge is transferred from $T_{1}$ to $T_{2}$). The task clusters are formed based on such pair-wise performance gains, then knowledge transfer is performed only within the most related tasks.
Similarly, \cite{dirichlet-process-priors} introduces an automatic identification of such clusters based on the Dirichlet Process (DP) prior distribution. 
Later, with the aim of providing a convex formulation to this framework, \cite{clustered-MTL} suggests regularising the multi-task parameter space $\mathbf{W}$ by imposing norms $\Omega$ to model several orthogonal properties: the mean weight vector size, the between-cluster and the within-cluster variance. This leads to a decomposition of \textbf{W} such that similar tasks are close in parameter space. Therefore, in addition to the minimisation of the empirical loss, the clustering objective can be simplified as in \cref{eq:clustering}:

\begin{equation} \label{eq:clustering}
min \sum_{i=1}^{m} L(t_{ti}, (W^{T}x_{ti}) +  \sum_{y \in {mean, between, within}}^{m'} \gamma_{y}\Omega_{y}(A) 
%\gamma_{2}\Omega_{between}(A) +8 
%\gamma_{3}\Omega_{within}(A)
\end{equation}

where $\lambda$ is a weight parameter for the norm $\Omega$ over the weight matrix $W$.

To explicitly model the distributions of the tasks to better identify their relationships, \cite{kernels-for-MTL,learning-a-kernel} introduces a kernel learning strategy to find a Reproducing Kernel Hilbert Space (RKHS) in which task-respective distributions are close together in parameter space if their relatedness is high enough. 
Finally, \cite{CMTL-via-ASO} interestingly derives the relationships between Clustered Multi-Task Learning (CMTL) in which similar tasks are clustered and sparse multi-task representations are learnt within clusters, as seen in \cref{sec:sparse}. The work introduces three algorithms to perform CMTL and demonstrates how the clustering approach is significantly more efficient than the low-dimensional subspace learning solution, especially under high-dimensional data settings.

\subsubsection{Common-Trunk}
% Figure environment removed
Early DL methods involved attaching task-specific heads to a CNN encoder's latent representation as shown in \cref{soft-hard} (top) \cite{Caruana}. For example, Ubernet \cite{Ubernet} introduced a CNN designed to tackle seven CV tasks. Many subsequent studies followed this design \cite{3d-cardiac, uncertainty, multinet, multitask-centernet}. This architecture shares a CNN backbone which gets updated by gradients aggregated by multiple tasks. As a result, all the tasks pull features from this backbone, which makes a global learned representation critical, although not trivial to obtain as different tasks need different representations to perform well. 
Hence, recent works suggest sharing parameters as part of multi-task encoder-decoder architectures at the decoder level \cite{PAD-net, PAP, MTI-NET, exploring-relational-context, invPT} to exchange high-level semantic features. For instance, Prediction-and-distillation Network (PAD-NET) \cite{PAD-net} suggest sharing knowledge after predictions and allows the training of a distillation module to learn what to share. \cite{MTI-NET} expends on this idea whilst incorporating multi-scale prediction for better dense prediction task performance. Similarly, at the prediction level, Pattern-Affinitive-Propagation (PAP) \cite{PAP} proposes learning pair-wise task relationship to produce affinity matrices for each task to further guide the sharing strategy.  

\subsection{Feature Fusion}
\label{sec:feature-fusion}
This section introduces parameter fusion techniques used in the two most pre-dominant vision models. First, \cref{subsec:CNN} introduces methods to share parameters across CNNs. Then, \cref{subsec:transformer} reviews recent attention-based methods to fuse parameters in Vision Transformers (ViTs) \cite{ViT}.
\subsubsection{CNN Sharing Strategies}
\label{subsec:CNN}
Cross-stitch networks \cite{cross-stich} introduce a model-agnostic fusion technique. As opposed to the hard-parameter sharing paradigm, in which task-decoders are attached to a shared backbone encoder (Fig.~\ref{soft-hard} (top)), \cite{cross-stich} introduces a soft-parameter sharing paradigm in which task networks are processed independently and through which parameter fusion is executed in parallel at a similar level of abstraction (Fig.~\ref{soft-hard} (bottom)). Given two task activation maps $\mathbf{A}$ and $\mathbf{B}$, cross-stitch units \cite{cross-stich} compute the dot product between a vector representing their respective values $x^{i,j}_{A}$ and $x^{i,j}_{B}$  at a shared location (i, j) and a trainable weight matrix $\mathbf{W} \in \mathbb{R}^{kxk}$ , where \textit{k} is the number of tasks. The values in $\mathbf{W}$ represent task-specific (diagonal entries) and shared parameters (non-diagonal entries). \cref{eq:cross-stitch} illustrates the process for k = 2

\begin{equation} \label{eq:cross-stitch}
\begin{bmatrix}
\tilde{x}^{i,j}_{A}\\
\tilde{x}^{i,j}_{B}
\end{bmatrix}
= 
\begin{bmatrix}
w_{AA} & w_{AB}\\
w_{BA} & w_{BB}
\end{bmatrix}
\begin{bmatrix}
x^{i,j}_{A}\\
x^{i,j}_{B}
\end{bmatrix}
\end{equation}
Despite being a locally-flexible, easy-to-implement and model-agnostic method, its design results in a complex and expensive model. First, studies investigating CNN layers have shown that shallow layers are usually task-agnostic and cross-stitch units would eventually represent such task-agnostic parameters, but at an expensive of training cost.  Second, the overall solution is expensive as the training costs increase with the number of tasks and the size of the network.

Sluice Networks \cite{sluice} generalise cross-stitch units by increasing the flexibility and sharing parameter space. In particular, \cite{sluice} allows for selective sharing of layers, parameter subspaces and skip connections.
%define task-specific subspaces in order to narrow down the search space and complexity and learn feature fusing weights across those subspaces.
%Neural Discriminative Dimensionality Reduction
To expand on this soft-parameter sharing structure, \cite{NDDR} proposes a solution based on the principle of Discriminative Dimensionality Reduction (NDDR). This principle attempts to minimise the number of features whilst keeping the maximum amount of representative information, similarly to Linear Discriminant Analysis (LDA) or Principal Component Analysis (PCA). Therefore, NDDR \cite{NDDR} formulates the multi-task feature fusion problem as a discriminative dimensionality reduction problem by first concatenating parallel feature maps, then task-specific 1x1 convolutions \cite{network-in-network} are run on such representation to perform dimensionality reduction. In addition, the authors employ batch normalisation and $L_{2}$ weight decay to stabilise learning. This method is illustrated in \cref{NDDR}.
% Figure environment removed

%NDDR-layers have been utilised in different multi-task models at various locations. For instance, Multi-Task Attention Network (MTAN) \cite{MTAN} introduces NDDR-layers to fuse task-specific soft-attention modules in an encoder-decoder architecture to jointly perform instance segmentation and depth prediction. 
As a result, NDDR \cite{NDDR} enables learnable local representation parameter sharing in a similar manner to cross-stitch and sluice networks \cite{cross-stich, sluice}. However, these techniques hypothesise that all tasks should be processed together, computational cost could therefore be reduced using prior knowledge on task groupings to avoid redundant computation. 

%% Figure environment removed

\subsubsection{Attention-based Sharing Strategies}
\label{subsec:transformer}
With the advent of the transformer model \cite{attention-is-all-you-need}, originally applied to NLP, and subsequently to CV \cite{ViT}, there has been a great improvement in dense prediction tasks in CV \cite{PVT, PVT-v2, swin-transformer, focal-transformer} due to the non-local feature acquisition inherent to these models as well as their capacity to exploit long-range dependencies. 
%MTAN \cite{MTAN} was first to incorporate attention mechanisms into a MTL framework.
Similar to the aforementioned soft-parameter sharing techniques \cite{cross-stich, sluice, NDDR}, Multi-Task Attention Network (MTAN) first trains a shared CNN network which is designed to learn general features. This shared framework is attached and at each convolutional operations, a task-specific network with attention modules which learn soft-attention masks over the shared features. 

\textit{Unified Transformer} (UniT) \cite{UniT} learns a multi-modal encoder-decoder transformer model. UniT \cite{UniT} learns modality-specific encoders using multi-head self-attention, the modalities are then simply concatenated before a joint decoder performs cross-attention to mix the multiple representations. Similarly, \textit{Multi-Task Transformer Network} \cite{MulT} (MulT) performs feature fusion at the decoding level and introduces a shared attention mechanism. Specifically, MulT chooses a reference task $t^{ref}$,
%(in their case, \textit{surface normal estimation} is empirically chosen), 
then the reference task encoded representation $x$ is used to compute a query $q^{t^{ref}}_{x}$ and a key $k^{t^{ref}}_{x}$. Let us denote $v^{t}$ the values for the other tasks based on the previous stage output. The attention values for this task are then calculated as:
\begin{equation} \label{eq:attention}
A^{t^{ref}}_{x} = softmax\left(\frac{q^{t^{ref}}_{x} \cdot {k^{t^{ref}}_{x}}^{T}}{\sqrt{C_{qkv}^{t^{ref}}}}\right) + B^{t^{ref}}\
\end{equation}
Subsequently, for any task $t$, the shared representation is obtained as follows:  $\tilde{x}^{t} = A^{t^{ref}}_{x}v^{t}$.  The term $x^{t}$ is then used for the multi-head attention.

MTFormer \cite{MTFormer} also chooses to compute cross-task interactions at task-specific heads. However, the authors choose to concatenate the projected representations at each transformer block, based on multi-head self-attention operations. To merge the attention maps of $n$ tasks, the authors show it is beneficial to consider self-task attention as a primary task and to consider cross-task attention as playing an auxiliary role in order to perform cross-task feature propagation. To reflect this, the authors choose to reduce the number of projected feature channels $C$ of auxiliary tasks such that $C' = \frac{C}{n-1}$, whilst keeping the original dimension for the main task. 

Finally, motivated by the success of pyramid-based transformer-based encoded representations for dense prediction tasks \cite{swin-transformer, PVT, PVT-v2}, InvPT \cite{invPT} proposes a cross-scale self-attention mechanism for multiple tasks. In this method, the attention maps are linearly combined by learnable weights, the result is also constrained by a residual feature map from the input image.


\subsection{Knowledge Decomposition}
\label{sec:knowledge-decomposition}
Knowledge Decomposition aims at partitioning a large set of features into smaller and meaningful components. In the context of MTL, one might be interested in recycling large models into smaller multi-task models. 
First, \cref{sec:tensor-factorization} reviews how tensor factorization can operate over CNN kernels to construct MTL components. Second, \cref{sec:knowledge-distillation} introduces methods to transfer information from a large single-task teacher model to a smaller multi-task student model. 
Last, \cref{sec:adapters} reviews how adapters can be used to achieve multi-task continual learning by fine-tuning a large single-task model.  

\subsubsection{Tensor Factorization}
\label{sec:tensor-factorization}
\cref{sec:sparse} reviewed solutions employing the low-rank approximation of a multi-task weight matrix using linear models. Deep Multi-Task Representation Learning (DMTRL) \cite{deep-multi-task-representation-learning} generalises this idea to tensors (N-way arrays with $N \ge 3$). In fact, as per the nature of a CNN, kernels are N-way tensors and fully convolutional (FC) layers are 2-way tensors, stacking those by a number of tasks $T$, usually resulting in large tensors. Tensor Factorization (TF) is a generalisation of some form of matrix decomposition, such as Singular Value Decomposition (SVD) \cite{SVD}.
%, however, due to the usual high-order of tensors, this renders such process complicated. 
DMTRL \cite{deep-multi-task-representation-learning} accomplishes soft-parameter sharing in a layer-wise manner between parallel and identical CNNs, similarly to \cite{cross-stich, NDDR}. First, single-task CNNs are trained, then layer-wise parameters are concatenated during backpropagation and subsequently fed as input to SVD-based solutions for decomposition. DMTRL \cite{deep-multi-task-representation-learning} uses multiple sharing strategies, including one based on the Tucker Decomposition (TD) \cite{Tuck1966c}, to learn parameters of this SVD-based solution to generate the decomposed units. 

Further to this strategy, \cite{Trace-norm-DML} uses the tensor trace norm (the sum of a matrix's singular values) as a proxy of the tensor rank on the layer-wise parameters' concatenation. In this way, each CNN is encouraged to use the other network's parameters. However, these methods have the same drawback as the previously introduced parameter-fusion based techniques \cite{cross-stich, NDDR, sluice} as parameters are shared in a layer-wise fashion which introduces constraints including architectural parallelism and locality in the parameter sharing strategy. 

\subsubsection{Knowledge Distillation}
\label{sec:knowledge-distillation}
Another perspective to parameter sharing is to design strategies based on Knowledge Distillation (KD). KD is a form a model compression that transfers knowledge from a large model to a smaller model. Early KD work in MTL explored how to compress DRL methods. For instance, \cite{policy-distillation, actor-mimic, distral} introduced a policy distillation strategy to derive lighter multi-task policies from task-specific deep Q-network (DQN) policies. However, as per the nature of DRL, these strategies approach tasks for which the set of actions was finite and would therefore struggle in more complex prediction visual tasks. 
As a result, \cite{PAD-net} introduces, as part of a multi-task multi-modal network, a distillation module to merge predictions from intermediate and complementary tasks from different modalities to subsequently pass representations on to task-specific decoders. The variations for this distillation module include cross-prediction reasoning as well as attention-guided mechanisms. 
Hence, \cite{knowledge-distillation-for-mtl} suggests a two-step solution in which: (1) task-specific models are first trained before freezing their respective parameters; (2) a multi-task model is optimized to minimise a multi-loss objective through the use of \textit{adaptors} (reviewed in \cref{sec:adapters}) that align task-specific and task-agnostic parameters together in order for the multi-task model to use the same features as the task-specific models. Following a similar strategy, \cite{MuST} extends this strategy to a self-supervised pre-training procedure through the use of intermediate pseudo-labeling. 

Recently, \cite{factorizing-knowledge-in-NNs} introduces a new alternative to KD, namely, \textit{Knowledge Factorization} (KF). Instead of distilling knowledge from a task-specific teacher model to a multi-task student model, KF aims at decomposing a pre-trained, large multi-task model into \textit{k} task-disentangled factor networks modelling both task-agnostic and task-specific parameters of the teacher model. The resulting lightweight networks can be assembled to create custom multi-task models.

\subsubsection{Adapters}
\label{sec:adapters}
With the aim of learning universal representations that can perform well across multiple domains, \cite{learning-multiple-visual-domains} introduces \textit{residual adapter modules}. Adapters are small neural networks that learn to recognise task-specific parameters given a model pre-trained on another task. Inspired by the ResNet \cite{resnet} architecture where residual connections are introduced across the sequential process of a CNN, adapters are modules attached after each convolutional block that learn to select parameters to be utilised for a downstream task. This presents an alternative to traditional \textit{fine-tuning} as only the adapters are trained. \cite{learning-multiple-visual-domains} demonstrates the capacity of adapter modules to maintain performance across 10 domains by just tuning a small portion of domain-specific parameters, and also their capacity to overcome the challenge of \textit{learning without forgetting} \cite{learning-without-forgetting}. 

\cite{efficient-parametrization-of-multi-domain-nn} introduces \textit{parallel} adapters as a simpler variant and show that only a few parameters need to be re-trained. As opposed to domain learning, \cite{attentive-single-tasking} shows how adapters can be used in Incremental Multi-Task Learning (I-MTL). As a new task is optimised, \cite{attentive-single-tasking} trains task-specific adapters to identify what parameters to retrain and \textit{Squeeze-and-Excitation} \cite{squeeze-and-excitation} modulation blocks perform channel-wise attention. Furthermore, to address the challenges raised by I-MTL, AdapterFusion \cite{adapter-fusion}, inspired by the multi-task objective adapter training strategy proposed by \cite{bert-and-pals}, introduces a 2-stage algorithm that enables task-specific parameters inside a transformer model to re-use other task-specific parameters contained in adapters. 
It is worth noting that, apart from the few aforementioned studies, adaptors have been studied far less in CV than in NLP. There is thus scope for exploiting this efficient parameter-sharing more fully in CV applications

%\papertomention{[MT-Uncertainty Sampling] Pilault, J., Elhattami, A., and Pal, C. J. Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters and Less Data. ICLR, 2021.}

\subsection{Neural Architecture Search}
\label{sec:parameter-NAS}
Neural Architecture Search (NAS) generally attempts to find the best network architecture given a specific problem by manipulating neural modules. However, in case of a multi-task objective, NAS can be seen as a way to partition the parameter space. For instance, \cite{soft-layer-ordering} introduces parameter sharing through \textit{soft ordering} (as opposed to \textit{parallel ordering}). The idea is to learn individual weight scalars per shared layers to \textit{soft-merge} parameters at different depths of a network. This comes down to learning a N-way tensor of task-specific parameters. Alternatively, Multi-gate Mixture of Experts (MMoE) \cite{MMoE} embeds the Mixture of Experts (MoE) framework \cite{MoE-layer} in MTL by sharing expert task-specific networks and optimising a gating network to select what features to use for each task. Following the same framework, \cite{DSelect-k} further improves the efficiency and stability of the selection of experts process and demonstrates its significant improvement on large-scale multi-task datasets. 

With the aim of learning an even more flexible assembling strategy, evolutionary algorithms have been proposed as a training strategy in which agents are network inference routes consisting of a set of computational blocks \cite{path-net, evolutionary-architecture-search-CTR}. Similarly, to learn large-scale MTL systems that tackle \textit{catastrophic forgetting} in the I-MTL paradigm, \cite{dynamic-introduction-of-tasks-in-large-scale} adopts an evolutionary algorithm to dynamically optimise a model each time a new task is added. Moreover, motivated by even more flexible ways to share features, some work has investigated using computational operations inherent to CNN layers as modulation units. For instance, \cite{flexible-mtl-by-learning-parameter-allocation} introduces the Gumbel-SoftMax Matrix model by modulating inner components of a layer, and shows how their activation is learned to optimise tasks through logits. Alternatively, \cite{adashare} shows how routing policies can be learned through the Gumbel-Softmax sampling method \cite{gumbel-softmax} taking into account computational resources. Recently \cite{AutoMTL} uses this trick to integrate the learning of such policies into its programming framework. \cite{stochastic-filter-groups} modulates networks the same way as \cite{gumbel-softmax}, however \textit{stochastic filter groups} are introduced as a  way to model the distributions, approximated via \textit{variational inference} \cite{variational-inference}, over the possible kernel groupings. More recently, Adashare \cite{adashare} introduced MTL in such architecture search-based systems to model relationships between tasks by studying the partitioned feature space. As a result, recent studies have focused on leveraging these relationships to route information through networks. For instance, \cite{fully-adaptive-feature-sharing} incrementally expands on an initially small network, at each step, grouping similar tasks based on a measure of task affinity. Similarly, \cite{branched-mtl} implements a low-resource, layer-wise sharing strategy driven by NAS, exploiting task affinity measures. In a CV context, \cite{synergistic-MTL-NAS} leverages hardware-aware NAS \cite{h-NAS-survey} together with MTL to improve the accuracy of dense-prediction tasks on edge devices.
\\
\\
\section{Optimisation Challenges}
\label{chapter:Optimisation}
MTL has underlying optimisation challenges due to it being a Multi-Objective problem. MTL is subject to two major optimization issues. First, overall performance is dependent on the relatedness of the tasks being optimised. Unrelated tasks can have conflicting gradients that will lead to a non-convergence of a multi-task solution. This phenomenon is called \textit{negative transfer}. 
Second, multi-objective performance relies on a thorough task balancing problem as respective complexities interfere during training. For example, easier tasks converge faster, resulting in larger gradients being back-propagated across all tasks. This makes the acquisition of aggregated representations for different task gradients non-trivial.
This section reviews solutions aiming to tackle the aforementioned challenges. 
First, \cref{sec:loss-based-methods} reviews how individual losses can be adjusted to balance tasks in a MTL training. Second, \cref{sec:grad-based-methods} focuses on techniques that directly operate over gradient updates during training. Third, \cref{sec:moo} reviews techniques directly inspired from Multi-Objective Optimisation to perform gradient-descent under a MTL configuration. Last, \cref{sec:other-task-balancing-techs} introduces other task balancing approaches. 


% Figure environment removed

\subsection{Loss-based Techniques}
\label{sec:loss-based-methods}
Early work in deep MTL studied a weighted average of the task-specific losses. By considering $\mathit{I}$ tasks and their respective weights $\mathit{w_{i}}$, this model objective can be summarised in \cref{eq:basic-mtl}.
\begin{equation} \label{eq:basic-mtl}
L_{MTL} = \sum_{i} w_{i} L_{i} 
\end{equation}

Rather than setting weights manually, solutions have been proposed to incorporate the weights into the objective function to adaptively weigh tasks during training. For example, AdaLoss \cite{adaloss} suggests adaptively tweaking weights in such a way that they are inversely proportional to the average of each loss in order to project losses onto the same scale. Alternatively, \cite{uncertainty} introduces learnable scalar parameters into the minimisation objective. The authors derive their loss weighting strategy based on the \textit{Homoscedastic (or task-dependent) uncertainty} which captures the uncertainty of a model, this type of uncertainty is invariant to different inputs. The authors follow a Gaussian likelihood maximisation setting and show that the loss optimisation given two tasks can be approximated as shown in \cref{eq:uncertainty}.
\begin{equation} \label{eq:uncertainty}
L_{uncert} (W, \sigma_{1}. \sigma_{2}) = \frac{1}{2\sigma_{1}^{2}} L_{1} + \frac{1}{2\sigma_{2}^{2}} L_{2} + log \sigma_{1} \sigma_{2}
\end{equation}
Following the same strategy, \cite{auxiliary-tasks-in-MTL} suggests a slight difference in the $log$ regularisation term, by changing it to $ln(1+\sigma)$. This is to prevent values of $\sigma < 1$ yielding negative loss values. We refer to this method as \textit{revised uncertainty}.
However, uncertainty-based task balancing strategies have certain drawbacks and in practice, task-wise terms need to be changed in \cref{eq:uncertainty} depending on the type of task (classification or regression) and depending on the task-specific loss.  As a result, IMTL \cite{impartial-MTL} introduces a hybrid method using both gradient methods and adaptive loss tuning. The loss component IMTL-L updates task-specific parameters and learns task-wise scaling parameters $s_{t}$ by minimising a function $g$ for each task $t$ as demonstrated in \cref{eq:IMTL-L}.
\begin{equation} \label{eq:IMTL-L}
g(s) = e^{s}L(\theta) - s
\end{equation}
\cref{eq:IMTL-L} shows that each task loss is scaled by $e^{s}$ and regularised by the term $- s$ to avoid trivial solutions. In practice, this technique allows tasks to all have comparable scales. Moreover, as opposed to uncertainty weighting \cite{uncertainty}, IMTL-L does not bias towards any type of task such as regression or classification. 
Alternatively, \cite{geometric} introduces a Geometric Loss Strategy (GLS), using the geometric loss to weigh $n$ individual losses. The geometric loss is invariant to individual loss scales which makes it an easy way to balance tasks, as shown in \cref{eq:geometric}
\begin{equation} \label{eq:geometric}
L_{geometric} = \Pi_{i=1}^{n} \sqrt[n]{L_{i}}
\end{equation}
Additionally, the authors introduce a variant to focus on $m$ ($m < n$) `more important' tasks and therefore attribute more weighting to these as demonstrated as in \cref{eq:geometric-prime},
\begin{equation} \label{eq:geometric-prime}
L'_{geometric} = \Pi_{i=1}^{n} \sqrt[n]{L_{i}} \times \Pi_{j=1}^{m} \sqrt[m]{L_{j}}
\end{equation}

Alternatively, balance of tasks can be achieved through averaging task weights over time by considering the rate of change in the respective task-specific loss. \cite{MTAN} introduces \textit{Dynamic Weight Average} (DWA). DWA \cite{MTAN} calculates a specific task-specific weight $\lambda_{k}$ for a task $k$ by obtaining a relative descending rate compared to other tasks with respect to the previous iteration (averaged over multiple epochs) as in \cref{eq:DWA}
\begin{equation} \label{eq:DWA}
\lambda_{k}(t) = \frac{K exp(w_{k}(t-1) / T)}{\sum_{i}exp(w_{i}(t-1) / T)} , w_{k}(t-1) = \frac{L_{k}(t-1)}{L_{k}(t-2)}
\end{equation}
where $T$ is a temperature parameter controlling the stiffness of the weighting distribution and $K$ ensures that $\sum_{i}\lambda_{i}(t) = K$.

More recently, Random Loss Weighting (RLW) \cite{RLW-RLG} has drawn task-specific weights from a probability distribution at each epoch before normalising them and shows comparable results to SOTA loss-weighting strategies. As a result, \cite{RLW-RLG} provides a more generalisable solution than the baseline (\cref{eq:basic-mtl}), due to its additional randomness. Finally, \cite{a-comparison-of-loss-weightin-strategies} provides benchmark results comparing Single Task Learning (STL) to DWA \cite{MTAN}, uncertainty \cite{uncertainty} and revised uncertainty \cite{auxiliary-tasks-in-MTL} and suggests that, given careful task selection, the revised uncertainty method \cite{auxiliary-tasks-in-MTL} generally performs best but suffers when there is lack of training samples.

\subsection{Gradient-based Techniques}
\label{sec:grad-based-methods}
Weighting losses is an indirect way of changing the model's gradients. Therefore, a line of work has investigated how to optimise MTL models by directly operating over the gradients. Throughout this section, we refer to the illustration in \cref{optimisation-techs} which provides a visualisation of the gradient update techniques introduced by the presented methods. Formally, the problem is that during multi-task optimisation, a subset of parameters $\theta$ is shared across multiple tasks, as a result, $\theta$ generally receives gradient updates to optimise all tasks at once. In practice, this is achieved by finding an aggregated representation of the vectors. However, finding such representation is not trivial as task-respective gradients might conflict. Hence, GradNorm \cite{gradnorm} proposes a method that balances training by automatically tuning the gradient magnitudes. Considering a subspace of weights of a model $W$ (generally chosen as the last shared layer for computational purposes), GradNorm \cite{gradnorm} defines the $L_{2}$ norm of the gradient for a particular weighted task loss $\textit{i}$, and similarly defines $\overline{G_{W}}(t)$ the average gradient norm across all tasks at time $t$. Additionally, GradNorm \cite{gradnorm} defines training rates as $\widetilde{L}_{i}(t) = L_{i}(t)/L(0)$, the loss ratio for a task $i$ at time $t$ and defines a relative training rate for a task $i$ as shown in \cref{eq:training-rate}:
\begin{equation} \label{eq:training-rate}
r_{i}(t) = L_{i}(t) / E_{task}[\widetilde{L}_{i}(t)]
\end{equation}
Subsequently, GradNorm \cite{gradnorm} calculates new task-specific gradients for the weight subspace $W$ based on the update rule shown in \cref{eq:gradnorm}:
\begin{equation} \label{eq:gradnorm}
G_{W}^{i}(t) = \overline{G}_{W}(t) \times [r_{i}(t)]^{\alpha} 
\end{equation}
where $\alpha$ is a hyper-parameter controlling the force of traction towards a similar training rate for all tasks. This method, by directly operating over gradients during training, adaptively tunes the speed to which tasks are being trained. 
However, solely balancing tasks does not prevent conflicting gradients (negative transfer).

GradDrop \cite{GradDrop} proposes adding a modular layer that operates during back-propagation to first select a sign (positive or negative) based on the initial distribution of gradient values. It then proposes masking out all gradient values of the opposite sign. 
Similarly, \cite{adapting-aux-losses-using-gradient-similarity} leverages auxiliary tasks in order to optimise a main task. During training, \cite{adapting-aux-losses-using-gradient-similarity} only minimises the auxiliary losses if their gradient update at epoch $t$ is non-conflicting with the main task gradient update. Specifically, \cite{adapting-aux-losses-using-gradient-similarity} uses the \textit{cosine similarity} to measure the gradients relation. Conceptually, if the cosine similarity between the main and auxiliary gradients is positive, it suggests that the auxiliary loss should be minimised alongside the main loss, otherwise, it should not. \cite{regularising-DMTL-orthogonal-gradients} uses a similar strategy in a more conventional MTL setting, in which multiple tasks are optimised simultaneously. \cite{regularising-DMTL-orthogonal-gradients} uses the cosine similarity to ensure shared gradients are near orthogonal, leading to a non-destructive training between tasks. Unlike \cite{regularising-DMTL-orthogonal-gradients} which ensures `near orthogonal' properties of the gradients via the minimisation of the loss, PCGrad \cite{gradient-surgery} projects the conflicting gradients of task $i$ onto the normal plane of task $j$ as shown in \cref{PCgrad-figure} (b). Formally, such projection can be defined as in \cref{eq:pcgrad}:

\begin{equation} \label{eq:pcgrad}
\Delta g_{i} = g_{i} - \frac{g_{i} \cdot g_{j}}{\lVert g_{j} \rVert^{2}} g_{j}
\end{equation}

However, imposing such strong orthogonality constraint upon gradients implies that all tasks at hand should benefit from similar gradient interactions, ignoring complex relationships and destructing natural optimisation behaviour. Moreover, PCGrad \cite{gradient-surgery} stays idle when the gradients have positive cosine similarity, which still might not be optimal as a more desirable similarity (closer a positive cosine similarity) might be preferred. Hence, GradVac \cite{gradient-vaccine} leverages both directions and magnitudes in an adaptive strategy. Specifically, given two tasks $i$ and $j$, a similarity goal $\phi^{T}_{i,j}$ is fixed between two gradients $\mathbf{g_{i}}$ and $\mathbf{g_{j}}$ such that $\phi^{T}_{i,j} > \phi_{i,j}$ for which $\phi_{i,j}$ is the cosine similarity, as computed in PCGrad \cite{gradient-surgery}. To achieve this, GradVac \cite{gradient-vaccine} derives the projection equation (\cref{eq:pcgrad} by fixing the gradient of $\mathbf{g_{i}}$ and rather estimates the weight of $\mathbf{g_{j}}$ via the Law of Sines in the gradients plane. This process can be summarised as in \cref{eq:vcgrad}:

\begin{equation} \label{eq:vcgrad}
\Delta g_{i} = g_{i} + \frac{ \lVert g_{i} \rVert (\phi_{ij} \sqrt{1 - \phi_{ij}^{2}} - \phi_{ij} \sqrt{1 - (\phi_{ij}^{T})^{2})}}{\lVert g_{j} \rVert \sqrt{1 - (\phi_{ij}^{T})^{2}}} \cdot g_{j}
\end{equation}

Furthermore, using an Exponential Moving Average (EMA) (similar to DWA \cite{MTAN}), $\phi^{T}_{ij}$ is estimated in an adaptive manner during training, over a subset of shared parameters $\mathbf{w}$ belonging to the same layer as illustrated in \cref{eq:ema-update-vcgrad}:

\begin{equation} \label{eq:ema-update-vcgrad}
\Delta \phi_{ijw} = (1 - \beta) \phi^{t}_{ijw} + \beta \phi^{t-1}_{ijw}
\end{equation}

% Figure environment removed

Similarly, \cite{impartial-MTL} suggests a hybrid method leveraging both loss and gradient tweaking, IMTL \cite{impartial-MTL} chooses, in their gradient component IMTL-G, to make all the projections from each task equal to balance the tasks. 
Recently, RotoGrad \cite{rotograd} proposed a solution to both homogenise gradients magnitude and resolve conflicting ones. To achieve this, a 2-step algorithm is implemented. The first step consists in homogenising the gradients such that the tasks that have progressed the least are encouraged to learn more. Therefore, to project the gradients $\mathbf{G_{k}}$, for a task $k$, Rotograd \cite{rotograd} assigns weights to gradients such that their weighted combination is $\mathbf{C = \sum_{k} \alpha_{k} \lVert G_{k} \rVert}$. Precisely, $\alpha$ is adaptively calculated every $i^{th}$ iteration as shown in \cref{eq:rotograd-alpha-update}:

\begin{equation} \label{eq:rotograd-alpha-update}
\alpha_{k} = \frac{\lVert G_{k} \rVert / \lVert G_{k}^{0} \rVert}{\sum_{i} \lVert G_{i} \rVert / \lVert G_{i}^{0} \rVert}
\end{equation}

In the second step, Rotograd \cite{rotograd} tunes the gradients by learning a task-specific rotation matrix $\mathbf{R_{k}}$ on the last shared representation $\mathbf{z}$. Hence, $\mathbf{R_{k}}$ aims to maximise the cosine similarity between the gradients across tasks given a batch of size $n$; or equivalently, to minimise the loss function shown in \cref{eq:rotograd}.

\begin{equation} \label{eq:rotograd}
L_{rot}^{k} = - \sum \left\langle R^{T}_{k} g_{n,k} , v_{k} \right\rangle
\end{equation}

 
\subsection{Multi-Objective Optimisation}
\label{sec:moo}
Multi-Objective Optimisation (MOO) addresses the challenge of optimising a set of possibly conflicting objectives. This section reviews gradient-based multi-objective optimisation methods applied to MTL. First, \cref{sec:pareto-optimality} formally defines Pareto optimisation and how it is relevant to MTL under gradient descent techniques. Then, \cref{sec:gradient-descent-solutions} reviews gradient-descent optimisation solutions applied to MTL.


\subsubsection{Pareto Optimality}
\label{sec:pareto-optimality}
% Here you define what is multi-objective, what is pareto optimisation, how you can do that in Multiple Gradient Descent. What are pareto stationary points, the pareto front etc... Define it formally.
As presented in \cref{sec:loss-based-methods}, tuning the task-specific weights is not trivial and usually comes at the cost of computational overhead. One way to remedy this is to reframe the MTL optimisation into a MOO problem.
Motivation to use MOO for MTL comes from the fact that global optimality for multiple tasks is unconceivable unless a pair-wise equivalence between tasks exists, which is unrealistic. For a hard-parameter sharing network as depicted in \cref{soft-hard} (top), $\theta^{sh}$ represents parameters that are shared and where $\theta^{s}$ are task-specific parameters. Additionally, $\widehat{L^{t}}(\theta^{sh}, \theta^{s})$ is the empirical loss for a specific task $t$ in the task set $T$. Then, a multi-objective loss function can be defined as shown in \cref{eq:moo-minimisation}:

\begin{equation} \label{eq:moo-minimisation}
\min_{\theta^{s}, ... ,\theta^{T}} (\widehat{L}^{t}(\theta^{sh},\theta^{t}), ... , \widehat{L}^{T}(\theta^{sh},\theta^{T}))^{\mathbf{T}}
\end{equation}

% Figure environment removed

%Following such objective, the minimisation is achieved when \textit{Pareto Optimality} is obtained. 
%Such optimality can be defined as follows. \\
%A solution $\mathbf{\theta}$ is said to be Pareto optimal if there exists no solution 

%$\mathbf{\bar{\theta}}$ such that $\widehat{L}(\theta^{s}, \theta^{t}) < \widehat{L}(\bar{\theta}^{s}, \bar{\theta}{t}) \forall t$ and $L(\theta^{s}, \theta^{1}, ... \theta^{T}) \neq L(\bar{\theta}^{s}, \bar{\theta}^{1}, ... \bar{\theta}^{T})$.

\noindent Minimising \cref{eq:moo-minimisation} leads to Pareto-optimal solutions. In other words, in a MTL setting, considering both shared and specific parameters $\theta_{i}^{sh,s}$ and $\theta_{j}^{sh,s}$ for task $i$ and $j$ respectively, a Pareto-optimal solution is one for which a change in $\theta^{i}_{sh,s}$ would damage the performance of task $j$ and vice-versa. The set of Pareto-optimal solutions can therefore be considered as a set of trade-offs between tasks. This set is called the \textit{Pareto front} ($P_{\theta}$).

Pareto optimality has extensively been studied leveraging the Multiple Gradient Descent Algorithm (MGDA) \cite{MGDA_paper} which supports the Karush-Khun-Tucker (KKT) conditions that are necessary conditions for Pareto optimality. 
\cite{MGDA_paper} demonstrates that minimising \cref{eq:minimisation-mgda} supports the KKT constraints and either leads to null or to a descent direction that improves all tasks.
\begin{equation} \label{eq:minimisation-mgda}
\min_{\alpha^{1}, ...,\alpha^{T}} 
\left\{
\mynorm{ \sum_{t=1}^{T}\alpha^{t}\nabla_{\theta^{sh}}\widehat{L}^{t}(\theta^{sh}, \theta^{s})}^{2}_{2} 
\right\}
\end{equation}
where $\alpha^{t}$ are non-negative scaling factors such that: $\sum^{T}_{t} \alpha^{t} = 1$.
%In the context of MTL, the pioneering work \cite{MTL-as-MOO} state these conditions in the context of MGDA
%for shared $\theta^{sh}$ and task-specific $\theta^{s}$ parameters for $T$ tasks.

\begin{comment}
\begin{itemize}
  \item There exist a set $\alpha^{1}, ...,\alpha^{T} \geq 0$ such that $\sum_{t=1}^{T} \alpha^{t} = 1$ and $\sum_{t=1}^{T}\alpha^{t}\nabla_{\theta^{s}}\widehat{L}^{t}(\theta^{s}, \theta^{t}) = 0$  
  \item For all tasks $t$, $\nabla_{\theta^{t}} \widehat{L}^{t} (\theta^{s}, \theta^{t}) = 0$
\end{itemize}
\end{comment}

%Such condition, in words, a convex combination of gradients at a particular point that equals 0, introduce the principle of \textit{Pareto stationarity} which is necessary for pareto optimality. Specifically, the pareto optimisation problem can be summarised as follows.

%\cite{MGDA_paper} demonstrates that minimising this objective, under the constraints discussed before, either lead to 0 or as a descent direction that improves over all tasks.

\subsubsection{Gradient Descent Solutions}
\label{sec:gradient-descent-solutions}
In a MTL context, \cite{MTL-as-MOO} shows that MTL optimisation can be regarded as a MOO problem using MGDA and demonstrates that solving \cref{eq:moo-minimisation} is equivalent to finding the min-norm point in the convex hull formed by the input points. That is, finding the closest point in a convex hull to a query point. As a result, \cite{MTL-as-MOO} obtains the aggregated projection of the task-specific gradient vector updates. Subsequently, to solve \cref{eq:moo-minimisation}, \cite{MTL-as-MOO} uses the Frank-Wolfe solver \cite{Frank-Wolfe} and ensures, with negligible additional training time, the convergence to a Pareto-optimal solution.    
CAGrad \cite{conflict-averse-gradient-descent-for-mtl} generalises the MGDA algorithm and chooses to ensure the convergence of the MTL objective to the equally weighted average of task-respective losses. To achieve this, CAGrad \cite{conflict-averse-gradient-descent-for-mtl} first obtains an average vector $d$ of individual task updates $g_{i}$. Then, it aims to find an update vector $g_{w}^{t}$ on a pre-defined ball around $d$, which maximises the conflict between ${T}$ tasks defined as: $- \min_{i \in {T}} \langle g_{i},d \rangle$. Furthermore, the authors show the dominance of CAGrad in a semi-supervised setting compared to MGDA \cite{MTL-as-MOO}.
However, this approach ensures the convergence to any point along the Pareto front which might not be representative of the desired task balance, an unbalanced solution might be preferred to enhance a target task.
Therefore, Pareto MTL \cite{Pareto-MTL} proposes generalising MGDA to generate a set of multiple Pareto optimal solutions along the Pareto front which would serve as different trade-offs to choose from. To achieve this, \cite{Pareto-MTL} takes inspiration from \cite{mmo-decomposition} and decomposes the objective space into $K$ well-distributed unit preference vectors $u_{k}$ to guide solutions. Formally, this is achieved through a sub-problem to \cref{eq:moo-minimisation} where a dot-product maximisation constraint is imposed between $u_{k}$ and a given vector $v$ to guide the learning onto a targeted area of the Pareto front. A sub-region is defined as shown in \cref{eq:partition-pareto}:
\begin{equation} \label{eq:partition-pareto}
\Omega_{k} = {v \in \mathbb{R}_{+}^{m} | u_{k}^{T}v \leq u_{k}^{T}v, \forall j = 1, ..., K}
\end{equation}

In contrast to its predecessors, \cite{continuous-pareto-exploration-in-MTL} suggests generating continuous Pareto optimal solutions along the Pareto front. To achieve this, \cite{continuous-pareto-exploration-in-MTL} proposes a 2-stage training algorithm that, in its first stage, generates a single Pareto stationary point $x_{0}$ from a network's initialisation. Then, a set of points $x_{n}$ is explored along the tangent plane direction $v_{i}$ and the points are calculated as follows: $x_{i} = x_{0} + sv_{i}$ where $s$ is a step size. As a result, a set of directions is obtained. Finally, \cite{continuous-pareto-exploration-in-MTL} combines the tangent vectors acquired in the previous step through linear combination to form convex hulls in which Pareto solutions are obtained, resulting in a continuous approximation of a larger Pareto front. 

All the solutions introduced thus far in this section initialise network parameters per trade-off, resulting in a large storage demand and making solutions computationally inefficient. Additionally, the generated solutions are either singular \cite{MTL-as-MOO} or subject to the practitioner's preferences \cite{Pareto-MTL, continuous-pareto-exploration-in-MTL}. To alleviate both issues, \cite{controllable-pareto-mtl} proposes utilising a HyperNet \cite{HyperNetworks}, a type of neural network that learns to generate the weights of another network, rendering storage less demanding. Additionally, \cite{controllable-pareto-mtl} introduces preference-based training to perform trade-off selection in real-time. More specifically, the objective space is sampled into $K$ subspaces (similarly to \cite{continuous-pareto-exploration-in-MTL}). Specifically, given a preference vector $\mathbf{p}$, the goal is to find a local Pareto optimal solution within such subspace for which the angle is the smallest to $\mathbf{p}$. To train the network on representative trade-off preference vectors, vectors are sampled using Monte Carlo methods and are given as input to the HyperNetwork $\mathbf{G}$. Then, the network is updated using normal gradient descent with the loss $\widehat{L}(G(p|\theta))$ in which $\theta$ represents the parameters for the MTL network. Specifically, \cite{controllable-pareto-mtl} uses a standard hard-parameter sharing strategy and such a process is depicted in \cref{controllable-pareto-fig}. Similarly, along the lines of preference-driven Pareto optimal solution, \cite{multi-objective-multi-task-learning-framework-induced-by-pareto} chooses to directly cast the MOO optimisation as a Weighted Chebyshev (WC) problem which consists of finding the Pareto front by minimising the $l_{+\infty}$-norm between the initialisation point and the Pareto front. 

Recently, Nash-MTL \cite{bargaining-game} suggests a different approach to obtain an Pareto optimal solution. Inspired by the game theory literature, the authors directly aim at obtaining the \textit{Nash Bargaining Solution} \cite{Nash-bargaining-solution}. This solution can be found on the Pareto front and translates to a proportionally fair solution where any change to the state results in a negative update for at least one task. As a result, the authors show that the aggregated update vector can be obtained as in \cref{eq:nash}. 
\begin{equation} \label{eq:nash}
\begin{aligned}
\theta^{(t)} = \quad & \theta^{(t)} - \eta G^{(t)} \alpha^{(t)} \\
\textrm{s.t.} \quad &  \textit{solve for $\alpha$ : } (G^{(t)})^{\textbf{T}} G^{(t)} \alpha = \frac{1}{\alpha}
\end{aligned}
\end{equation}
where $t$ is a training time-step, $\theta$ represents the shared parameter networks, $\theta$ the learning rate. $G^{t}$ is a task-specific vector update matrix with columns $g^{t}_{i}, i \in T$. 
The results obtained by \cite{bargaining-game} suggest Nash-MTL achieves current state of the art weighting strategy under many MTL configurations.

However, recently \cite{even-help} instead demonstrated that most MTL optimisation strategies \cite{MTL-as-MOO, gradnorm, gradient-surgery, GradDrop} do not improve MTL training beyond what careful choice of scalar weights in MTL weighted average (\cref{eq:basic-mtl} can achieve. Rather, \cite{even-help} identifies MTL optimisation is particularly sensitive to the choice of hyper-parameters.


\subsection{Other Task Balancing Techniques}
\label{sec:other-task-balancing-techs}
\subsubsection{Stopping Criterion Techniques} 
Previous techniques balanced tasks either by finding a combination of the task weights or through gradient manipulation to prevent destructive learning. However, these techniques globally penalise some tasks over others by constraining certain parameters in the objective space. 
Therefore, \cite{task-wise-early-stopping}, as part of their solution leveraging multiple auxiliary tasks to perform facial landmark detection, proposes a task-wise early stopping strategy. The intuition is that once a task starts to overfit a dataset, it will harm the main task as it will force the optimisation to be stuck in a non-global optimum. Hence, a task is stopped if its performance, measured as the product between the training error tendency, noted as $L_{tr}$ and the generalisation error \textit{w.r.t} $L_{tr}$, noted as $L_{val}$, has not exceeded a certain threshold $\epsilon$. 
Formally, a training error rate $E_{tr}$ is calculated over a patience epoch length $k$ \textit{w.r.t} a current epoch $t$. Intuitively, the smaller $E_{tr}$, the greater the signal to continue the training for the task as the training loss substantially drops over the period of time $k$ as shown in \cref{eq:early-stopping-tr}:
\begin{equation} \label{eq:early-stopping-tr}
E_{tr} = \frac{k \cdot med_{j=t-k}^{t}L_{tr}(j)}{\sum_{j=t-k}^{t}L_{tr}(j) - k \cdot med_{j=t-k}^{t}L_{tr}(j)}
\end{equation}
where $med$ represents the median operation. 
Similarly, $E_{val}$ measures the overfitting w.r.t $L_{tr}$. \cite{task-wise-early-stopping} defines $\lambda$ as an additional learnable parameter to measure the importance of the task's loss. This process is shown in \cref{eq:early-stopping-val}. \\
\begin{equation} \label{eq:early-stopping-val}
E_{val} = \frac{L_{val}(t) - \min_{j = 1..t}L_{tr}(j)}{\lambda \cdot \min_{j = 1..t}L_{tr}(j)} 
\end{equation}
Overall, if $E_{tr} \cdot E_{val} > \epsilon$, the stopping criterion is met.

In a MTL configuration in which all the tasks are aimed to be optimised equally, stopping a task might result in \textit{catastrophic forgetting}. Therefore, \cite{12-in-1} proposes a simple dynamic Stop-and-Go procedure that continually checks for task-wise improvement and degradation during training. 
Precisely, if performance, measured as the task-wise validation loss term for a given epoch $n$, noted as $L_{t}^{n}$, has not met the performance threshold $\epsilon_{stop}$ over the patience parameter $k$ such that $L_{t}^{n \rightarrow k} < \epsilon_{stop}$. Then, task $t$ is set to \textit{STOP} mode.
If during \textit{STOP} mode, $L_{t}^{n}$ is degraded and meets the degradation threshold $\epsilon_{go}$ such that $L_{t}^{n} < \epsilon_{go}$, then task $t$ is set back within the MTL training and is set to \textit{GO} mode.
In \cite{12-in-1}, the authors set $\epsilon_{stop}$ to be 0.1\% and $\epsilon_{go}$ to be a degradation of 0.5\% of the task's best performance.

\subsubsection{Prioritisation Techniques}
An alternative to balancing the learning of multiple tasks simultaneously is to instead focus on easier or complex tasks to benefit the training for all tasks. For example, \cite{self-paced-MTL} chooses to guide their MTL training by gradually incorporating both harder tasks and harder instances into the objective function. By considering a number of tasks $m$ and a number of instances per task $n$, the authors propose a regularisation $f$ over $\mathbf{W} \in \mathbb{R}^{m x n}$ as shown below:
\begin{equation} \label{eq:self-paced}
f(W, \lambda, \gamma) = - \lambda \sum_{i=1}^{m} \lVert W^{\mathbf{T}} \rVert_{1} + \gamma \sum_{i=1}^{m} \frac{\lVert W \rVert_{2}}{\sqrt{n_{i}}}
\end{equation}
in which the first term imposes the $L_{1}$-norm on the instances $n$, leading to sparsity. This is motivated by the fact that easy instances, for which the empirical loss will be small, have bigger gradients. As training continues, gradually increasing $\lambda$ will introduce harder instances. Similarly, the second term imposes the $L_{2-1}$-norm on $\mathbf{W}$, motivated by the fact that harder tasks exhibit larger empirical losses and gradually reducing $\gamma$ will introduce harder tasks as the corresponding vectors in $\mathbf{W}$ will exhibit near-0 vectors.

On the other hand, some works have focused on starting with harder tasks to benefit easier tasks. For instance, \cite{dynamic-task-prio} proposes a loss weighting strategy leveraging the \textit{focal loss} \cite{focal-loss}. Although the focal loss is primarily intended for classification, \cite{dynamic-task-prio} suggests using key performance metrics (KPIs) per task $t$ to generalise the method. Specifically, they adjust these task-specific KPIs $\kappa_{t}$ in an EMA approach as shown below: 
\begin{equation} \label{eq:FL}
\bar{\kappa}_{t}^{(\tau)} = \alpha \kappa_{t}^{(\tau)} + (1 - \alpha) \kappa_{t}^{(\tau - 1)}
\end{equation}
where $\alpha$ is a discount factor and $\tau$ is the iteration. Therefore, \cite{dynamic-task-prio} utilises $\bar{\kappa}_{t}^{(\tau)}$ to change the cross-entropy classification probability $\mathbf{p}$ originally defined as shown in \cref{eq:original-cross-entropy}:
\begin{equation} \label{eq:original-cross-entropy}
FL(\mathbf{p},\alpha) = -(1-\mathbf{p})^{\alpha}\log(\mathbf{p})
\end{equation}
Finally, \cite{dynamic-task-prio} defines a task difficulty as a combination of the task-specific loss and its respective KPI-based focal loss as shown in \cref{eq:DTP}:
\begin{equation} \label{eq:DTP}
L_{DTP} = \sum_{t=1}^{T} FL(\bar{\kappa_{t}};\gamma_{t}) \widehat{L}_{t}
\end{equation}

Alternatively, \cite{learning-to-MTL-active-sampling} proposes prioritising harder tasks through \textit{active sampling} (\ie choosing what data to train a model with at a particular time $t$ during training). More specifically, the model keeps track of two performance estimations: $t_{i}$ and $c_{i}$ which are a target performance and current performance, respectively, for a task $i$. The task performance is measured as follows: $m_{i} = \frac{t_{i} - c_{i}}{t_{i}}$, where a higher value of $m_{i}$ indicates the model is currently bad at task $i$. Therefore, to encourage the model to prioritise harder tasks, a task-wise sampling strategy is modeled by a distribution $p_{i}$ at every $k$ decision steps which is calculated as follows.
\begin{equation} \label{eq:MTL-active-sampling}
p_{i} = \frac{\exp^{\frac{m_{i}}{\tau}}}{\sum_{c=1}^{k} \exp^{\frac{m_{c}}{\tau}}}
\end{equation}

\section{Task Grouping}
\label{sec:task-grouping}

As explained in \cref{chapter:Optimisation}, the overall performance of a MTL model heavily depends on the set of tasks. The optimisation space could be simplified by only processing related tasks together. This chapter focuses on how Task Relationship Learning (TRL) can be achieved through Task Grouping (TG). 

Thus far, most works relied on human judgment concerning the relatedness of the tasks. However, these assumptions can be mitigated by quantitatively measuring task relationships. Early attempts in this area aimed to model task relationships (TR) based on vectors in a shared low-dimensional subspace. For example, \cite{learning-with-whom-to-share} explicitly builds upon MTFL \cite{MT-feature-learning} (introduced in \cref{sec:non-neural-MTL}) and frames the task grouping problem as a mixed integer programming problem. GO-MTL \cite{learning-task-grouping-and-overlap} learns a linear combination of task-specific vectors. 
Later, \cite{MRN} expands on previous works modelling TRs using matrix-variate normal distribution over task-specific parameters regularisation techniques to identify positive task correlations \cite{MTRL}. However, to embed this regularisation technique into DL, \cite{MRN} uses the tensor normal distribution \cite{tensor-normal-distribution} as a prior over task-specific tensors and learns task relationships by learning task covariance matrices. 
Similarly, \cite{MMoE} learns gating networks in a MoE framework to implicitly model task interactions. However, these works model relationships from a high-level perspective and generally poorly describe pair-wise relatedness.
To tackle TG, a body of work focused on studying relationship based on Transfer Task Learning (TTL) by directly learning a mapping between the learned parameters for a task $a$ to a target task $b$ in a MTL setting. For instance, Taskonomy \cite{taskonomy} introduced a computational approach to perform TG based on finding transfer learning dependencies between tasks. More specifically, \cite{taskonomy}, after training task-specific networks, the encoder parts of the networks are frozen and transfer task functions and dependencies are estimated via a target task decoder. Motivated by the idea that multiple source tasks can help provide a more meaningful dependency estimation for a mutual source task, the authors include high-order transfers where a mapping function receives the five best representation as inputs (from the five best first-order source tasks mappings), as illustrated in \cref{transfer-mapping}.
% Figure environment removed
Additionally, Taskonomy \cite{taskonomy} derives a vision task clustering architecture and shows that 4 major clusters stand out, namely: 3D tasks, 2D tasks, low dimensional geometric tasks and semantic tasks. 
Calculating the affinities in such a way is extremely computationally expensive. To alleviate such demand for computation, as opposed to analysing the performance of TTL, Representation Similarity Analysis (RSA) \cite{RSA-taxonomy} directly investigates the feature maps learned by the task specific networks. The authors choose to leverage RSA to frame the task relationship problem by computing correlation through task-specific inferences on pairs of images. As a result, a dissimilarity matrix is obtained for each task-specific network and a similarity score is obtained through the Spearmans correlation. 
However, these latter works only highlight the relationships from a transfer-learning perspective and do not present performance in a multi-task setting. 
Hence, \cite{which-tasks-should-be-learned-together} proposes an alternative to transfer-learning based solutions to highlight task relationships. This alternative is motivated by two findings. First, results obtained by \cite{which-tasks-should-be-learned-together} do not show any correlation in the performances between measured \textit{task affinities} and multi-task learning setting. Second, transfer-learning affinities highlight high-level semantic dependencies as only the bottleneck of the source task encoder is used for the mapping. However, MTL should benefit from clean structural dependencies in all abstraction levels of the features. 
Instead, the authors in \cite{which-tasks-should-be-learned-together} frame this TG problem as an architecture search. Specifically, given an input image, \cite{which-tasks-should-be-learned-together} aims to determine the best combinations of encoder backbones and task-specific decoders and perform an exhaustive search over these components. The process is constrained by a search time budget value given a number of tasks $T$. Moreover, \cite{which-tasks-should-be-learned-together} optimises the search space using a branch-and-bound procedure and trains between $\binom{T}{2} + T$ and $2^{T} - 1$ networks given $T$ tasks before performing TG. However, this search performance is computationally expensive and as a result, \cite{efficiently-identifying-task-groupings-for-MTL} directly builds upon this framework and obtains task groups in a single run only. To achieve this, the authors introduce \textit{Task Affinity Grouping} (TAG) which is a \textit{look-ahead} algorithm that tracks changes in the MTL loss (in this case, \cref{eq:basic-mtl}) under different task groupings. Therefore, the authors introduce the notion of \textbf{task affinity} between two tasks $a$ and $b$ defined by $\mathcal{\hat{Z}}_{a \rightarrow b}$ as shown in \cref{eq:affinity-a-b}:
\begin{equation} \label{eq:affinity-a-b}
\mathcal{\hat{Z}}^{t}_{a \rightarrow b} = 1 - \frac{L_{b}(X^{t}, \theta^{t+1}_{s|a}, \theta^{t}_{b})}{L_{b}(X^{t}, \theta^{t}_{s}, \theta^{t}_{b})}
\end{equation}
In which $t$ is the step during the estimation procedure and where the loss $L_{b}$ for task $b$ is parameterised by $X, \theta_{s}, \theta_{b}$ which represents the input, the shared parameters and task-specific parameters for task b, respectively. The look-ahead term $\theta^{t+1}_{s|a}$ represents the update of the shared parameters w.r.t. the update on task $a$. Subsequently, a network selection procedure is implemented to maximise the total inter-task affinity score. For instance, for a number of tasks $|T|$ the affinity scores onto a task $a$ are averaged over all the tasks.
\begin{equation} \label{eq:overall_affinity}
\mathcal{Z}_{a} = \frac{\sum_{t}^{|T|}\mathcal{\hat{Z}}_{t \rightarrow a}}{|T|} , a \in |T|, t \neq a
\end{equation}
This problem is NP-Hard and can therefore be solved by a branch-and-bound algorithm. 


\section{Partially Supervised Multi-Task Learning}
\label{chapter:partial-supervision}
Methods reviewed so far have mainly focused on a fully-supervised setup which assumes that data is sufficient and all task labels are available. However, this setting is not always realistic as both acquiring data and task labels is an expensive process in certain cases. 
In practice, the diversity of the task set is limited as required data and labels generally do not co-exist within the same datasets and therefore, not in the same quantities and/or domains. Thus, there is a need to explore MTL in settings that utilise all available source of information. 
Thankfully, MTL systems can mitigate their data dependency by using available supervisory information of one task to enhance the training of the unlabelled tasks by leveraging task relationships. 
Therefore, in this chapter, \cref{sec:representation-learning} reviews how leveraging multiple auxiliary tasks in a self-supervised manner can help obtain a general representation tailored to downstream tasks. 
Then, \cref{sec:semi-supervised} studies MTL solutions in a semi-supervised settings in which all tasks are optimised.
Finally, \cref{sec:few-shot-learning} introduces how MTL can be framed in a low-data availability learning paradigm: Few-Shot Learning.
Throughout this chapter, we refer to \textit{Partial Supervision} as an umbrella term encompassing self-supervised learning, semi-supervised learning and few-shot learning.


\subsection{Self-Supervised Representation Learning}
\label{sec:representation-learning}
As seen in this review, finding a task-agnostic representation suitable for all the tasks is crucial. However, most previous work in MTL assumed high availability of data and focused on obtaining such representations without diminishing the demand for labels. To remedy this issue, an alternative way to obtain a shared representation is to exploit tasks in a self-supervised fashion. Self-supervised tasks are tasks for which labels can be created without manual annotations. Such tasks hold a strong advantage in the context of MTL as downstream tasks benefit from the representation induced by multiple tasks \cite{cross-stich}. As a result, Self-supervised Multi Task Learning (Self-MTL) can be leveraged as a pre-training strategy. 

% Multi-Task Representation
For instance, \cite{MT-self-supervised-VL} suggests leveraging 4 self-supervised vision tasks as a pre-training procedure.\textit{Relative Position} \cite{relative-position} is a task which consists of finding the relative positions of a pair of patches sampled from the same unlabeled image. \cite{relative-position} claims to perform well at this task enhances object recognition. \textit{Colorization} \cite{colorization} which requires predicting the original RGB pixel color values given a greyscale image. This task acts as a cross-channel encoder and helps pixel-level dense prediction tasks. The `\textit{Exemplar}' task \cite{examplar-task} where pseudo-classes are estimated for each sample and the network is trained to discriminate between these. This task aims to improving classification properties in the learned representation. Last, \textit{Motion Segmentation} \cite{motion-segmentation} is a task that learns, given an image $I_{t}$ at a time $t$ to recognise pixels that will move in $I_{t+1}$. This task helps refine the features necessary to both object detection and segmentation prediction through movement cues. 

The authors of \cite{MT-self-supervised-VL} identify two possible sources of conflict in a Self-MTL setting. First, there are conflicts in the task respective inputs, as for instance, the colorization tasks receive greyscale images whilst others receive RGB images. This results in an network architectural problem. To resolve this conflict, the authors suggest performing \textit{input harmonisation} by duplicating the greyscale image over the RGB channels. Second, there is conflict in whether the features being trained should generalise to the class at hand or to the specific input image. To resolve this, the authors incorporate their CNN into a lasso regularisation block where each task-specific decoder receives a layer-wise linear combination of the shared backbone convolutional blocks. Hence, a matrix $A \in \mathbb{R}^{TxD}$ is trained to be sparse where $T$ is the number of task-specific decoders and $D$ is the number of convolutional blocks being shared. This regularisation allows the network to factorise the features to enhance the generalisation of the network. \cite{MT-self-supervised-VL} presents results matching fully-supervised single-task performance on diverse CV tasks such as classification, detection and depth prediction. The authors' solution is illustrated in \cref{self-supervised-visual-learning}. 

% Figure environment removed

MuST \cite{MuST} uses specialised teacher models to pseudo-label unlabeled multi-task datasets and suggests a pre-training strategy based on the following tasks: classification, detection, segmentation and depth estimation. Subsequently, a multi-task student model is trained on the pseudo-labeled dataset. Fine-tuning on downstream tasks shows that the self-supervised pre-training outperforms traditional ImageNet pre-training baseline \cite{ImageNet} and additionally, the authors identify that a large number of tasks and datasets benefit the representation for downstream tasks.

% Targeted-Task Examples
% Figure environment removed

This capacity to leverage MTL to enhance the shared representation of tasks has motivated applications in diverse areas. For instance, \cite{self-supervised-monocular-road-segmentation} pre-trains a CNN encoder on stereo-paired images from the well-known road object detection dataset KITTI \cite{KITTI} to perform monocular road segmentation. To achieve this, the authors choose to learn two tasks; \textit{Drivable Space Estimation} and \textit{Surface Normal Estimation}. Given a stereo-pair of images $(I_{left}, I_{right})$, the authors obtain a pseudo disparity map $I_{disparity}$ by using semi-global matching (SGM) \cite{SGM}. Subsequently, the authors run the Stixel World algorithm \cite{Stixel-world} which, given a RGB image $I_{RGB}$ ($I_{left}$ or $I_{right}$), exploits the corresponding disparity map $I_{disparity | I_{RGB}}$ to return a semantically segmented representation. MAP estimation is then performed based on the resulting distribution of the predicted pixel labels to extract the drivable area. Subsequently, surface normals are obtained by following the method introduced by \cite{surface-normal-unsupervised}. Specifically, given camera-related information such as the baseline distance $D$ and the focal length $D_{focal}$, the previously calculated diversity map $I_{disparity}$ is converted into a depth map $I_{depth}$. This depth map is later projected onto 3D world space $W$ given $D$ and $I_{normals}$ and is obtained via calculating the least-squares plane within $W$ and allocating the planes to neighbouring set of pixels. The authors fine-tune the learned features to perform monocular road segmentation and show impressive results whilst heavily reducing the demand for data.

\cite{recycling-bounding-box-annotations} utilises Self-MTL as a way to refine preliminary Object Detection (OD) predictions. In particular, assuming bounding box labels ${A_{OD}}$ are only available for object detection, 3 auxiliary tasks recycle ${A_{OD}}$ to produce their own respective labels ${A^{t}}$. Such a strategy has two main goals: (1) to learn robust discriminatory features for OD, (2) to refine the preliminary OD prediction. These auxiliary tasks are carefully chosen as follows. First, \textit{Multi-Object Labelling} randomly produces bounding boxes over the input image, constrained by the fact that one must overlap with at least one Ground Truth (GT) bounding box. Then, labels are assigned to the sampled bounding boxes based on GT Bounding Box area it overlaps the most with. The intuition behind this task is to perform augmentation on the input image to enhance globalisation. Second, \textit{Closeness Labeling} accounts for the inherent proximity in object classes in an image. This task consists in iterating over the GT bounding box annotations to provide a one-hot encoding based on the proximity of neighbouring GT bounding boxes. Finally, \textit{Foreground labeling} encodes the foreground and background, assigning 1's to pixels within GT bounding boxes and 0's otherwise. These tasks are illustrated in \cref{recycling-auxiliary-tasks}. Information encoded by these tasks is concatenated into a representation $f$ and is used to update the original prediction $x$ via a 1-layer FC layer to obtain a final refined prediction $z$ such that: $z = f \oplus x$. 

These methods demonstrate how effectively leveraging multiple self-supervised objectives can improve a shared representation suitable for MTL. Such efficiency has motivated some works to employ Self-MTL for diverse target downstream tasks in CV. For example, \cite{self-supervised-image-aesthetic-assessment} suggests a meaningful self-supervised pre-training strategy for Image Aesthetic Assessment (IAA). IAA models, which are usually trained an aesthetic-labeled ImageNet dataset \cite{ImageNet}, do not provide much information for why an image is not aesthetically good, for example, intrinsic image characteristics (\ie brightness, blurriness, contrast etc). Therefore, the authors train a comparative network of 2 distorted images, the distortion is chosen as one of the aforementioned characteristics and the networks aim at estimating the type of distortion as well as its intensity in an unsupervised manner. The goal of the MTL system is to recognise the less distorted image. Moreover, additional tasks are added to recognise the type and intensity of the distortion operation applied to the two input images. The authors report a decrease in 47\% in the number of epochs necessary for convergence compared to a IAA network pre-trained on Imagenet \cite{ImageNet}, notwithstanding the reduced need for data.

Alternatively, self-MTL framework has shown state-of-the-art  results in real-time applications. For example, SSMTL \cite{anomaly-detection} tackles anomaly detection in videos. Acquiring anomalous labels is difficult and as a result, the authors leverage self-supervised tasks to train a 3D CNN to recognise anomaly in videos. SSTML \cite{anomaly-detection} first runs a pre-trained YOLOv3 \cite{YOLOv3} to identify objects on a set of object-level frames ${I_{n}}$. Then, the authors choose three tasks to identify anomalous objects. First, irregularity is identified through the \textit{arrow of time} task, which involves obtaining an abnormal label by training the 3D CNN on the video in reverse mode. Second, \textit{motion irregularity detection} for which abnormal events are obtained via skipping frames is used to identify irregular motions such as someone running, falling etc. Third, a \textit{middle box prediction} task is implemented to predict the middle frame. Last, the authors enhance their multi-task 3D CNN through \textit{knowledge distillation} where the object detector YOLOv3  \cite{YOLOv3} is trained to predict the last layer of a ResNet-50 \cite{resnet}, which predicts whether the middle box frame is abnormal or not. The key point is that, in the knowledge distillation head, the authors expect a high difference between the object-level predictions of the 3D CNN and the ResNet-50 predictions when an anomaly is observed. The results significantly outperform previous state-of-the-art methods. Moreover, SSMMTL++ \cite{SSMTL++} recently reviews this framework and further improves it through the introduction of different tasks such as optical flow and advanced architectures such as the ViT \cite{ViT}. 

% Multi-Modal Representation
In addition to using multiple auxiliary tasks to enhance the learned representation, multiple modalities can be utilised to provide even more useful sources of information for models to learn. Multi-modal representation learning can be achieved by pre-training on diverse datasets. For instance, \cite{12-in-1} obtains a vision-language representation by pre-training on 12 vision-linguistic datasets and shows impressive results on common multi-modal tasks such as visual question answering and caption-based image retrieval. The authors utilise multi-modal self-supervision, inspired by \cite{Vilbert}, by masking proportional amounts of both image and word tokens and also by performing \textit{multi-modal alignment}, by predicting if two instances belong together. Similarly, \cite{sound-and-visual-rep-learning} introduces Multi-SSL, a multi-modal (sound and image) pre-training strategy aiming to provide a shared representation for both sound and image modalities that could be used for downstream tasks. 

\cite{MultiMAE} leverages the recent the success of Masked Auto-Encoders (MAEs) \cite{MAE}. MAEs \cite{MAE} are asymmetric encoder-decoder models in which the encoder only operates on a small portion (about 15 \%) of a patch-wise masked input image and  the decoder aims at regenerating the missing patches. In particular, \cite{MultiMAE} proposes Multi-Task MAE (MultiMAE), a pre-training strategy reconstructing diverse image modalities. To achieve this, given a set of RGB images, image modalities are acquired solely via \textit{Pseudo-labeling}. First, the depth modality is approximated by running a pre-trained DPT-Hybrid \cite{ViT-for-dense-prediction}, a ViT-based model. Similarly, Semantic Segmentation pseudo-labels are obtained via Mask2Former \cite{Mask2Former} trained on the COCO dataset \cite{COCO}. Once these labels are obtained, similar to original MAE \cite{MAE}, the authors sample a large portion of the image modalities divided into 16x16 patches. Subsequently, a number of tokens corresponding to approximately $\frac{1}{8}$ of the entire number of tokens for the 3 modalities (RGB, depth and semantic) are kept visible. The sampling strategy follows a symmetric Dirichlet distribution (with $\alpha$=1), equivalent to a uniform distribution so that no modality is prioritised. Then, the authors perform a 2D-sine-cosine linear embedding on the patches which are fed as input to the multimodal ViT encoder which operates only on the visible tokens, tremendously reducing the cost of computation \cite{MAE}. For downstream tasks, the multi-modal self-trained encoder can be used to fine tune a single task whilst benefiting from geometrical cues induced by other modalities. This framework is illustrated in \cref{MultiMAE-figure}.

% Figure environment removed

%\papertomention{Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic Imagery, 2018} \\
%\papertomention{Generative Modeling for Multi-task Visual Learning, 2022}
% No CV Applications
In addition, Muli-Task Self-Supervised pre-training has been investigated in medical applications \cite{retinal-vessel-segmentation, MUSCLE, skin-lesion}, in music classification \cite{music-classification} or in NLP for multilingual reverse dictionaries \cite{multilingual-reserved-dictionary}.


\subsection{Semi-Supervised Learning Methods}
\label{sec:semi-supervised}
\subsubsection{Traditional Methods}

\cite{Semi-supervised-MTL} proposes the first semi-supervised MTL framework. The framework consists of $T$ classifiers whose parameters share a joint probability distribution based on a soft variant of a Dirichlet Process. This allows for the parameters to be trained together and for the predictions to be obtained all at once. 
The probability distribution variant retains the inherent clustering property of Dirichlet Processes and as a result, the authors process unlabeled data via Parameterized Neighborhood-based Classification (PNBC). More specifically, the authors perform a Markov random walk over neighbouring data points obtained via supervised training, then, classifiers learn to assign unlabeled data to its closest point. 
Later, \cite{task-regularizations} expands on this setting by framing MTL as a clustering problem. To achieve this, after training $T$ linear classifiers, the authors improve their generalisation w.r.t. to their respective data by imposing a norm over the classification weights. Subsequently, the algorithm follows the same procedure, frames the respective classifiers into clusters via K-means clustering and assigns unlabeled points to nearby classifiers within that space. The authors also show this framework can be extended to non-linear classification through the use of kernels. 
It is worth noting that these traditional methods had a different notion of the MTL problem. In fact, the tasks are classification tasks in which `tasks' are either different datasets \cite{task-regularizations} or classes, resulting in multi-class classification \cite{Semi-supervised-MTL}. 
As a result, only one loss function is used for the optimisation which significantly differs from the contemporary definition of MTL.

\subsubsection{Self-Supervised-Semi-Supervised Methods}
% A good example for a CV target task 
The methods introduced in \cref{sec:representation-learning} highlight how multi-task learning can be used with self-supervised auxiliary tasks to minimise the overall training cost and demand for data. This characteristic has motivated numerous works to leverage both semi-supervised learning and self-supervised learning.

% Leveraging geometrical tasks for semantic
As explained in \cref{sec:task-grouping}, some tasks provide global understanding of scene geometry (\ie \textit{surface normals, depth prediction ...}) and when trained adequately, translate into low-level features tailored for dense prediction tasks. 
Therefore, there has been effort to investigate these tasks to improve an important CV task: \textit{Semantic Segmentation} (SS). For instance, \cite{urban-scene-understanding, dynamic-object-problem} use depth prediction as a proxy task for supervised urban scene understanding tasks such as car detection, road and semantic segmentation. Similarly, \cite{boostinc-semantic} uses both depth estimation and colorization as a pre-training strategy for semantic segmentation in autonomous driving. 
To expand upon the idea that self-supervised depth estimation (SDE) can be effective to reduce data dependency, \cite{three-ways} introduces three ways to leverage SDE to improve semantic segmentation in a semi-supervised learning paradigm. 
%Therefore, the following description first introduces the different strategies introduced by \cite{three-ways} before presenting their semi-supervised network combining these strategies.

First, the authors suggest an active learning strategy based on depth prediction. Specifically, given a set of images of the same domain ${G}$, the authors aim to split it into two image subsets. On the one hand, ${G_{A}} \subset {G}$ will be used for pseudo-labeled annotations for SDE, whilst ${G_{U}} \subset {G}$ is the set of unlabelled images. To obtain these, the authors iteratively choose ${G_{A}}$ through diversity sampling. Precisely, diversity is obtained when the chosen images are most representative of the dataset distribution. In urban scene understanding, this could result in the most frequent types of buildings, cars, bicycles, etc being chosen. To achieve diversity, the authors first populate ${G_{A}}$ with a random image and iteratively select the farthest $L_2$ distance between two sets of features of both ${G_{A}}$ and ${G_{U}}$. The features correspond to the same layer $l$ of a pre-trained SDE network $f_{SDE}(\theta, I)$. Formally, the active learning procedure can be formulated as in \cref{eq:active-learning}:

\begin{equation} \label{eq:active-learning}
G_{A_{n+1}} = \arg \max \min \lVert f_{SDE}(\theta, G_{U}) - f_{SDE}(\theta, G_{A})\rVert_{2}
\end{equation}

Subsequently, the authors aim to incorporate another important aspect to this active sampling: \textit{Uncertainty Sampling} which consists in choosing samples that are hard to learn for the current state of the model: formally, instances in ${G_{U}}$ for which the model's decision is close to the decision boundary. To achieve this, a student model $f'_{SDE}(\theta, I)$ is trained on ${G_{A}}$. The authors then measure the disparity, on ${G_{U}}$, of both the predictions of $f_{SDE}$ and those of $f'_{SDE}$. Formally, the difference is calculated using the $L_1$ distance as shown in equation \cref{eq:depthmix}:

\begin{equation} \label{eq:depthmmix}
E(i) = \lVert log(1+f_{SDE}(\theta, I)) - log(1+f'_{SDE}(\theta, I)) \rVert_{1}
\end{equation}

The authors choose to use the $log$ regulator to avoid close-range objects dominating the disparity difference. 
Conceptually, sampling based on these two characteristics benefits from diversified, complex and representative instances which results in a decreased demand for data samples. 

Second, inspired by the success of pair-wise data augmentation in CV \cite{CutMix, ClassMix}, \cite{three-ways} introduces \textit{DepthMix} as a way to further reduce this labeling demand. In this method, considering 2 images $I_{source}$ and $I_{target}$, the goal is to learn a binary mask $M$ over $I_{source}$. Specifically, the positive values in $M$ represent regions to be copied over $I_{target}$. As a result, the augmented image $I_{augmented}$ is obtained as shown in \cref{eq:goal-depthmix}:
\begin{equation} \label{eq:goal-depthmix}
I_{augmented} = M \odot I_{source} + (1 - M) \odot I_{target}
\end{equation}
where $\odot$ is the element-wise product. In contrast to existing data augmentation methods, \cite{three-ways} leverages depth to avoid violating geometric semantic relationships between objects. For example, it is undesirable to have a distant object in $I_{source}$ to be copied onto the forefront of $I_{target}$, or worse, to result in geometrically implausible situations like a close-range motorbike copied on the top of a close-range car. To mitigate this problem, the authors use depth predictions for both images noted as $D_{source}$ and $D_{target}$. To achieve this, given a shared location $(x,y)$, $M$ is constrained to select only pixels for whose depth values are smaller on $I_{source}$ than on $I_{target}$. This process is demonstrated in \cref{eq:depthmix}:
\begin{equation} \label{eq:depthmix}
M(a,b) =\left\{
    \begin{array}{ll}
        1 & \mbox{if } D_{source}(a,b) <  D_{target}(a,b) + \epsilon\\
        0 & \mbox{otherwise}
    \end{array}
\right.
\end{equation}
where $\epsilon$ is a small noise value to avoid conflicts of objects that are the same depth plane on both images such as curb, road and sky.

The final component introduced in \cite{three-ways} is a semi-supervised MTL network to perform both Depth Estimation and Semantic Segmentation.
% Figure environment removed
The authors train their MTL network in 2 stages. The first stage is  depth pre-training. This stage consists in a self-supervised training for both depth estimation and pose estimation on an unlabeled sequence of images. As part of this procedure, a shared encoder $f_{\theta}^{E}$ is initialised with ImageNet \cite{ImageNet}. Additionally, in order not to forget the semantic features during training, the initialised features, noted as $f_{I}^{E}$, serve as a regulator for the SDE pre-training and the authors use the $L_2$-norm in order to guide the multi-task representation. The resulting loss term is formulated as shown in \cref{eq:three-ways-regulation}:
\begin{equation} \label{eq:three-ways-regulation}
L_{SDE} = \lVert f_{\theta}^{E} - f_{I}^{E} \rVert_{2}
\end{equation}
In the second stage, the authors introduce semantic segmentation to form a semi-supervised network. In this stage, the network is trained on depth estimation on both labeled and pseudo-labeled (using the mean teacher algorithm \cite{mean-teacher-algorithm}) instances. Their solution is illustrated in \cref{three-ways-MTL}.
As a result, the authors manage to achieve 92\% accuracy on a baseline fully-supervised model whilst using 1/30 of labeled image segmentation instances. Furthermore, whilst using 1/8 of the SS labels, it outperforms this supervised baseline by a small margin. The authors then improve their solution to perform domain adaptation \cite{semi-supervised-domain-adaptive}. 

Recently, \cite{MTL-for-image-segmentation-task} leverages both depth and surface normals estimation to improve on semantic segmentation. In addition, the authors show how Nash-MTL \cite{bargaining-game} can lead to efficient solutions.

\subsubsection{Generative Modeling}
Recent advances of general self-supervised methods such as adversarial training with Generative Adversarial Networks (GANs) \cite{GANs}, as well as the ability of generative modeling to learn useful visual representations from unlabeled images \cite{adversarial-feature-learning}, have motivated the investigation of generative modeling in MTL to lower the demand for labeled data \cite{adver-learning-semantic-semi-supervised, robust-adver-learning-for-semantic-semi}. 

For example, \cite{partly-supervised-MTL} proposes a self-supervised semi-supervised MTL ($S^{4}MTL$) solution leveraging adversarial learning and semi-supervision to teach simultaneously two commonly tackled CV tasks, namely: Image Classification (for diagnostic classification) and Semantic Segmentation. By considering two datasets, one labeled $D_{A}$ and one unlabeled $D_{U}$, the authors define their respective losses as $L_{A}$ and $L_{U}$. If $\theta$ and $\upsilon$ define the network parameters for semantic segmentation and diagnostic classification respectively, then the overall objective can be summarised as:
\begin{equation} \label{eq:partly-supervised-MTL}
\min_{\upsilon, \theta} L_{A}(D_{A},(\upsilon, \theta)) + \alpha L_{U}(D_{U},(\upsilon, \theta))
\end{equation}
where $\alpha$ is a positive weight for the unsupervised loss. 
Subsequently, the authors train two networks: $G$, a mask generator for semantic segmentation and $D$ a classifier which is trained in an adversarial fashion. These two networks are divided into two branches. 
For supervised images, $G$ wants $D$ to maximise the likelihood of the segmentation masks given a regular image-label pair.
For the  unsupervised images, the model performs a transformation $t(x)$ over the input image $x$ such as rotation to enable $G$ to make predictions. Such a framework is illustrated in \cref{S4MTL}. 
% Figure environment removed
Using this framework, the model is claimed to outperform fully-supervised single task models whilst diminishing the availability of data/label up to 50\%.


\cite{semi-supervised-MTL-for-semantics-depth} extends on this framework and introduces SemiMTL. This method performs urban scene segmentation and depth estimation. However, the authors leverage multiple datasets in a heterogeneous (trained on different datasets) MTL framework and train their discriminator in a domain-aware fashion to compensate for the domain shift inherent to this environment. To do this, the authors add a inter-domain loss between the labeled dataset $A$ and unlabeled dataset $B$ which can be illustrated as below:
\begin{equation} \label{eq:inter-domain-loss}
L_{inter}^{t} = - \sum_{h,w} \log (D_{t}(\hat{y}^{B}_{t})^{(h,w,y^{A}_{t})})
\end{equation}
where the ground-truth value for the datasets $A$ and $B$ for an arbitrary task $t$ is noted as $y^{A}_{t}$ and $y^{B}_{t}$, respectively. Moreover, the respective two predictions are noted $\hat{y}^{A}_{t}$ and $\hat{y}^{B}_{t}$. Conceptually, the loss in \cref{eq:inter-domain-loss} aligns the unlabelled task prediction $\hat{y}^{B}_{t}$ onto the labelled task ground-truth $y^{A}_{t}$ to compensate for domain shifts. 
Additionally, the authors introduce different ground-truth and prediction alignment strategies such as aligning the unlabelled prediction $\hat{y}^{B}_{t}$ onto the labelled task prediction $\hat{y}^{A}_{t}$ or aligning $\hat{y}^{B}_{t}$ onto the intersection of the labelled ground-truth $y^{A}_{t}$ and prediction $\hat{y}^{A}_{t}$.


\subsubsection{Discriminative Methods}
Discriminative methods aim at determining boundaries between image representations by directly comparing them.
This section focuses on MTL works introducing this technique under semi-supervised training paradigms. 
One type of discriminative method that has shown great success in many CV tasks is \textit{Contrastive Learning} (CL). CL was  originally introduced by \cite{original-contrastive}. It involves learning a joint-space in which similar pairs of images are close to each other and in which different pairs are far part. Momentum Contrast (MoCo) \cite{Moco} extends this concept for unsupervised visual learning and sees this framework as a dictionary look-up problem where an image $I$ is encoded by a network $f$, this is denoted as the query $q = f(I)$. Then, a queue of size $n$ of image representations ${I_{k}}$, or keys, chosen as the preceding mini-batch, which are encoded by a momentum encoder $f_{m}$ are compared $k_{n} = f_{m}({I_{k}})$. Subsequently, the matching key $k_{+}$ is noise-augmented. Finally, $f$ is updated via the InfoNCE \cite{InfoNCE} loss shown in \cref{eq:infoNCE}.
\begin{equation} \label{eq:infoNCE}
L_{InfoNCE} = - \log \frac{\exp(q \cdot k_{+} / \tau)}{\sum_{i=1}^{N} \exp(q \cdot k_{i} / \tau)}
\end{equation}
SimCLR \cite{SimCLR} suggests a simpler version comparing diverse augmented versions of the same image, however it requires larger batch sizes.

\begin{table*} % Test table
\centering
\caption{MTFormer\cite{MTFormer} treats a \textcolor{red}{target task} annotations as few-shot samples whilst keeping two other tasks fully-supervised. \\
Results are reported on the PASCAL dataset \cite{PASCAL}.}
\label{tab.4}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllllc c c c c}
    \toprule
    \multirow{2}{*}{Method} &&
    \multirow{2}{*}{Few-Shot Task} &&
    \multirow{2}{*}{SS $\uparrow$} &&
    \multirow{2}{*}{Human Part Seg. $\uparrow$} &&
    \multirow{2}{*}{Saliency $\uparrow$} \\
    \\
    \cline{5-5} \cline{7-7} \cline{9-9}
    &&&& {mIoU $\uparrow$} && {mIoU $\uparrow$} && {mIoU $\uparrow$} \\
    \midrule
      STL && SS && \textcolor{red}{3.34} && 63.90 && 66.71 \\
      MTFormer \cite{MTFormer} && SS && \textcolor{red}{35.26} && 64.26 && 67.26 \\ 
    \midrule
      STL && Human Part Seg. && 71.17 && \textcolor{red}{11.27} && 66.71 \\
      MTFormer \cite{MTFormer} && Human Part Seg. && 73.36 && \textcolor{red}{51.74} && 67.74 \\ 
    \midrule
      STL && Saliency && 71.17 && 63.90 && \textcolor{red}{44.39} \\
      MTFormer \cite{MTFormer} && Saliency && 76.00 && 66.89 && \textcolor{red}{55.55} \\ 
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

Motivated by the aforementioned approaches, MTSS \cite{MTSS} suggests a simple, yet effective, semi-supervised MTL framework to optimise a discriminative self-supervised auxiliary task and a supervised main task simultaneously.
% Figure environment removed
Specifically, the authors choose to maximise the similarity between two different views of the same image. First, two augmentations on the same image are performed, these views are $x_{1}$ and $x_{2}$. Then, a shared CNN classifier process them leading to two representations $z_{1}$ and $z_{2}$. One, for example $z_{1}$, is chosen to be processed by the supervised main task. Similarly to SimCLR \cite{SimCLR}, the authors choose to attach a Multi-Layer Perceptron (MLP) in order to map representations to a similar space, let us denote the resulting representations as $p_{1}$ or $p_{2}$. Finally, the cosine similarity $D$ between $p_{1}$ and $z_{2}$ is calculated as shown below: \begin{equation} \label{eq:similarity-MTSS}
D(p_{1}, z_{2}) = - \frac{p_{1}}{\lVert p_{1} \rVert_{2}} \cdot \frac{z_{2}}{\lVert z_{2} \rVert_{2}}
\end{equation}
to minimise the cosine similarity between the representations of augmented views. The symmetric auxiliary loss, introduced by BYOL \cite{BYOL} and depicted in \cref{eq:BYOL}, is used as follows:
\begin{equation} \label{eq:BYOL}
L_{aux} = \frac{1}{2} D(p_{1}, z_{2}) + \frac{1}{2} D(p_{2}, z_{1})
\end{equation}
This auxiliary loss is then added to the overall MTL objective. The semi-supervised  framework is depicted in \cref{MTSS-figure}.

% Figure environment removed

Another recent task discriminatory approach, Cross-Task Consistency (XTC), is introduced by \cite{X-task-consistency}. Conceptually, this notion comes from the dependency between two tasks. For instance, in the context of urban scene semantic segmentation with depth estimation, there would be inconsistency if depth estimation evaluated a flat surface where a car is detected. Therefore, \cite{X-task-consistency} aims to compute task pair-wise mapping to map the prediction from a source task to the label of the target task. 
However, each of those mapping functions are parameterised by two DNNs and leverage labels from each task. To mitigate the use of labeled data, \cite{MTPSL} leverages cross-task relations in a semi-supervised framework. Specifically, \cite{MTPSL} suggests a framework to map the prediction of an unlabeled task $\hat{y}^{s}$ to the ground truth of another task $y^{t}$ through an adaptive encoder which embeds only shared parameters. Therefore, the two representations $\hat{y}^{s}$ and $y^{t}$ are mapped on to a joint space and their  cosine distance is minimised.

The authors in \cite{MTPSL} leverage XTC in their framework for semantic segmentation, depth estimation and surface normals estimation. Let us consider a partially-supervised image $I$, for which only $y^{depth}$ or $y^{semantic}$ is available. $I$ is then processed through a shared backbone network $f_{\Theta}$ to which task-specific decoders $h_{\vartheta}^{depth}$ and $h_{\vartheta'}^{semantic}$ are attached. 
The obtained predictions are noted as $\hat{y}^{depth}$ and $\hat{y}^{semantic}$. For the sake of illustration, let us consider 
$\hat{y}^{semantic}$ not to be labelled and therefore to leverage the available ground-truth from the depth estimation task. Now describing the XTC mechanism, let us consider a matrix $A$ for which entries correspond to $source \rightarrow target$, (in our example, $A[semantic,depth] = 1$) and all other entries are 0. An auxiliary network $k_{\theta}$ is used to conditionally parameterise a mapping network $m_{\psi}$. Similar to \cite{film}, $k_{\theta}$ is used to update the layers of $m_{\psi}$. This mechanism is to allow for a conditional source-to-target mapping. The two resulting representations are then projected on to the same joint-space $J$. The authors use the cosine similarity to minimise their distance. 
Additionally, to avoid trivial mappings, the features from $f_{\Theta}$ are used as a regularisation term of the distance between the mapping function's output and the encoded features $f_{\Theta}(I)$. The explained mapping is illustrated in \cref{MTPSL-fig}.


\subsection{Few-Shot Learning Methods}
\label{sec:few-shot-learning}
% Description of Few-Shot Learning
Few-Shot Learning (FSL) is a learning paradigm that aims to learn unseen classes from a few examples. This training paradigm is motivated by the fact that humans do not need hundreds or thousands of exemplar images to learn to recognise an object. Typically, FSL systems consist of two stages. First, a general feature extractor is learned from a large annotated dataset in a stage called \textit{meta-training}. Second, an adaption strategy is used to classify the new sample/class (also known as the query sample) based on a small labeled support set. This stage is called \textit{meta-testing}. A similarity function is then used on the support set to identify the matching class given the query sample. 
% How Few-Shot Learning relate to MTL, defining the problem
Traditionally, in FSL-MTL, the goal is to adapt to unseen classes for a specific task within a MTL model. In the context of MTL, cross-task interactions within a multi-task system could help enhance the generalisation to the few-shot target task. In fact, \cite{natural-language-decathlon} shows that MTL models generally focus on tasks that have the least training samples, which is due to the feature sharing process across tasks. 

Recently, the FSL literature has heavily focused on the initial meta-training stage in which multiple datasets serve to train a model to obtain global representations for a target few-shot task, most commonly being \textit{image classification}. For example, \cite{improving-fsl-with-self-pretext-tasks} suggests training such a model in a MTL fashion by leveraging self-supervised tasks (similar to solutions introduced in \cref{sec:representation-learning}), on both labelled and unlabelled images. The shared encoder is regularised by the contrastive learning method: BYOL \cite{BYOL}. Subsequently, the MTL system is evaluated on traditional few-shot image classification. 

MTFormer \cite{MTFormer} suggests different dense prediction tasks as few-shot tasks and evaluates a MTL system leveraging a cross-task attention mechanism at the decoder level of a ViT \cite{ViT} on the PASCAL dataset \cite{PASCAL}. The authors evaluate three tasks, in turn, as a few-shot sampled task by randomly sampling about 1 \% of the annotated data for the few-shot task and keeping all available labels for other tasks. MTFormer \cite{MTFormer} chooses to evaluate Semantic Segmentation, Human Part Segmentation and Saliency Detection which consists of identifying interesting points in an image (points that the human eye would focus on straight away). The results, presented in \cref{tab.4}, display an impressive improvement over the single-task FSL baseline. This improvement is explained by two techniques: the feature propagation across tasks to enhance the few-shot task representation, and the use of CL in \cite{MTFormer}, in which different task representations of the same image are considered as positive samples, which further reinforces the shared representation's quality. 

\textit{Visual Token Matching} (VTM) \cite{visual-token-matching} proposes a continual few-shot learning framework for dense prediction vision tasks. In this setting, a universal few-shot learner can learn new dense prediction tasks given extremely limited labelled task images, most often only using 10 labelled examples of image-label pairs. VTM employs a encoder-decoder architecture using ViT encoders \cite{ViT} to encode both image and label. As a way to propagate features across the model hierarchies, the authors perform token matching using an attention mechanism similar to MTFormer \cite{MTFormer}. More specifically, given a target few-shot task $t$, a query image $Q_{t}$ and support set of image-label pairs of length $N$ ($(X,Y)^{1...N}_{t}$), a task-specific shared encoder $f_{t}$ is used to process both $Q_{t}$ and $X^{i}_{t}$. On the other hand, a label encoder $g$ is used to encode $Y^{i}_{t}$. Subsequently, the token matching mechanism based on attention operates on ViT blocks representations. The block-wise query label predictions are then concatenated before a classification head provides the final prediction. Finally, the results reported by \cite{visual-token-matching} suggest similar strategies should be elevated to the simultaneous MTL settings.

\begin{table*} % Test table
\caption{Self-supervised MTL}
\label{tab.1}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllllc c c c c c c c c c}
    \toprule
    \multirow{2}{*}{Method} &
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Tasks} &
    \multirow{2}{*}{Labels} &
    \multirow{2}{*}{Backbone} &
      \multicolumn{1}{c}{SS} &&
      %\multicolumn{1}{c}{Depth} &&
      \multicolumn{1}{c}{OD} &&
      \multicolumn{2}{c}{C} \\
      \cline{6-6} \cline{8-8} \cline{10-11}
      \\
      &&&&& {mIoU $\uparrow$} && {mAp $\uparrow$} && {Top-1 Acc $\uparrow$} &  {Top-5 Acc $\uparrow$}\\ 
      \midrule

    \multirow{ 2}{*}{\cite{MT-self-supervised-VL}} & ImageNet \cite{ImageNet} & C & - & Resnet-101 \cite{resnet} & - && - && - & 85.10 \\
    & Pascal VOC \cite{PASCAL} & OD & - & Resnet-101 \cite{resnet} & - && 74.17 && - & - \\
    \midrule
    
    \cite{self-supervised-monocular-road-segmentation} & KITTI \cite{KITTI} & SS (Drivable Space) & - & VGG-net \cite{resnet} & 94.23 && - && - & -\\
    \midrule
    
    \multirow{ 2}{*}{\cite{recycling-bounding-box-annotations}} & Pascal VOC \cite{PASCAL} & OD & - & Fast R-CNN \cite{Fast-R-CNN} & - && 77.5 && - & - \\
    & COCO \cite{COCO} & OD & - & Fast R-CNN \cite{Fast-R-CNN} & - && 34.6 && - & - \\
    \midrule
    
    \cite{three-ways} & Cityscapes \cite{cityscapes} & SS & 1/30 & ResNet \cite{resnet} & 62 && - && - & - \\
    \midrule

    \multirow{ 3}{*}{\cite{MultiMAE}}
    & NYUv-2 \cite{NYUv2} & SS & - & ViT \cite{ViT} (RGB-only) & 50.6 && - && - & - \\
    & NYUv-2 \cite{NYUv2} & SS & - & ViT \cite{ViT} (RGB+Depth) & 53.6 && - && - & - \\
    & ImageNet-1K \cite{ImageNet} & C & - & ViT \cite{ViT} (RGB-only) & - && - && 83.3 & -\\
    \midrule

    \multirow{ 2}{*}{\cite{MTL-for-image-segmentation-task}}
    & NYUv-2 \cite{NYUv2} & SS & - & ResNet \cite{resnet} (Nash-MTL) & 80.0 && - && - & - \\
    & NYUv-2 \cite{NYUv2} & SS & - & ResNet \cite{resnet} (Unc.W) & 74.8 && - && - & - \\
    
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

\begin{table*}
\centering
\caption{NYUv2 and Cityscapes}
\label{tab.2}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllcc c ccc c}
    \toprule
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Method} &
      \multicolumn{1}{c}{SS} &&
      \multicolumn{2}{c}{Depth} &&
      \multicolumn{1}{c}{SN} && 
      \\
      \cline{3-3} \cline{5-6} \cline{8-8} \\
      && {mIoU $\uparrow$} && {aErr $\downarrow$} & {AbR $\downarrow$} && {mErr $\downarrow$} \\ 
      \midrule
    \multirow{ 6}{*}{NYUv2 \cite{NYUv2}}
    & $STL_{SS}$ & 37.45 && - & - && - \\
    & $STL_{Depth}$ & - && 0.61 & - && - \\
    & $STL_{SN}$ & - && -  & - && 25.94 \\
    & $MTL_{CNN}$ & 36.95 && 0.55 & -  && 29.5 \\
    \\
    \cline{2-10}
    \\
    & \cite{MTPSL} MTPSL (1/3) & 28.43 && 0.63 & -  && 33.01 \\
    & \cite{MTPSL} MTPSL (one) & 31.00 && 0.51 & -  && 28.58 \\
    \midrule
    \multirow{ 8}{*}{Cityscapes \cite{cityscapes}}
    & $STL_{Seg}$ & 74.19 && - & - && - \\
    & $STL^{SegNet}_{Depth}$ & - && 0.012 & - && - \\
    & $STL^{ResNet}_{Depth}$ & - && - & 0.41 && - \\
    & $MTL_{CNN}$ & 73.36 && 0.016 & - && - \\
    \\
    \cline{2-10}
    \\
    & \cite{MTPSL} MTPSL (one) & 74.90 && 0.016 & - && - \\
    & \cite{MTPSL} MTPSL (1:9) & 71.89 && 0.013 & - && -  \\
    & \cite{MTPSL} MTPSL (9:1) & 74.23 && 0.026 & - && - \\
    & \cite{semi-supervised-MTL-for-semantics-depth} SemiMTL & 71.9 && - & 0.28 && - \\
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}


\section{Datasets \& Tools}
%This chapter aims at providing the interested reader with a list of available tools and analysis of the current state of MTL research.

\cref{sec:datasets} refers the reader to a list of datasets commonly utilised in MTL for computer vision. Additionally, \cref{sec:discussion} provides a summary of the results achieved by partially-supervised MTL solutions. Based on these results, we discuss and analyse common trends and suggest interesting paths of exploration to further improve MTL. Last, we introduce a table summarising the different open-source MTL code. 


\subsection{Datasets}
\label{sec:datasets}
%\subsubsection{Dense Prediction Tasks}
%MTL has heavily focused on pixel-level (\textit{aka} dense) prediction tasks (\textit{semantic segmentation, depth estimation, surface normal estimation}). 
Below is a list of common multi-task CV datasets.
\begin{enumerate}
  \item \textbf{Taskonomy. }\cite{taskonomy} This dataset is the largest multi-task dataset. It contains 4.5 million indoor scene images, each labeled with 25 annotations. These images include: scene annotations, camera information, 2D/3D keypoints, surface normals and various-level object annotations. The foundational work \cite{taskonomy} on this dataset performed experiments on 26 diverse tasks.
  
  \item \textbf{NYUv2-Depth v2. }\cite{NYUv2} This dataset comprises 1449 labeled images drawn from indoor scene videos for which each pixel is annotated with a depth value and an object class. Additionally, there are 407,024 unlabeled images which contain RGB, depth and accelerometer data, rendering this dataset useful for real-time applications as well.  

  \item \textbf{Cityscapes. }\cite{cityscapes} This dataset consists of 5000 urban scenes. Each image is annotated with pixel-level labels for 30 classes. Additionally, the dataset includes image stereo pairs associated camera shift metadata. Therefore, \cite{cityscapes} leverages stereo-paired information to produce accurate depth labels. As a result, Cityscapes \cite{cityscapes} is typically used as a 7-class semantic segmentation class and depth estimation task. 

  \item \textbf{KITTI. }\cite{KITTI} This dataset is one of the most popular datasets for Autonomous Driving. The images result from hours of driving in diverse traffic environments. This dataset has been utilised for 3-class \cite{3-class-kitti}, 10-class \cite{10-class-kitti} or 11-class \cite{11-class-kitti} semantic segmentation or object detection. Additionally, the dataset includes 3D labeled point clouds for 15,000 images.  
\end{enumerate}

\subsection{Results and Discussion}
\label{sec:discussion}
This section presents results for partially supervised MTL. Moreover, an attempt to derive both general performance guidelines and future areas of investigation is made. 

\cref{tab.1} summarises some of the self-supervised methods introduced in \cref{sec:representation-learning}. The methods covered in this table leverage MTL as a pre-training strategy and subsequently fine-tune on a single target task. This table considers three common CV tasks: \textbf{Semantic Segmentation} (SS), \textbf{Object Detection} (OD) and \textbf{Classification} (C). Task metrics reported are respectively, mean Intersection over Union (mIoU), mean Average Precision (mAP) and classification accuracy on top-1 and top-5 images. 
By analysing the aforementioned methods, we first find that studies aiming to acquire global representations almost never leverage a large number of tasks and often leverage tasks based on the authors' assumption of the relatedness of the tasks. As a result, we find that there is a lack of adaptive methods to deal with possibly unrelated tasks too. For instance, an interesting path would be to explore multi-task representations based on the task affinities (as presented in \cref{sec:task-grouping}).
Furthermore, we identify that, although the methods in \cref{tab.1} train the representations in a self-supervised MTL setting, the fine-tuning procedure often aims at optimising a single/target task. Therefore, it would be interesting to see how a large number of tasks could enhance a representation in a MTL setting \cite{MultiMAE}.
Last, we point out that for pseudo-labeling, authors generally use single-task pre-trained networks. An interesting path of exploration would be to explore pre-trained MTL networks to give accurate labels to further improve the accuracy of pseudo-labels.

\begin{table*} % Test table
\centering
\caption{MTL open-source code repositories }
\label{tab.3}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{c c p{9cm}}
    \toprule
    %\multirow{1}{*}{Type} &
    %\multirow{1}{*}{Description} &
    \multicolumn{1}{c}{Type} &
    \multicolumn{1}{c}{Link} & 
    \multicolumn{1}{c}{Description}  \\
    \midrule
    \multirow{ 2}{*}{Paper Repository} 
    & \href{https://github.com/WeiHongLee/Awesome-Multi-Task-Learning}{Awesome Multi-Task Learning 1} & This repository regroups MTL-related papers in a chronological order.\\
    & \href{https://github.com/Manchery/awesome-multi-task-learning}{Awesome Multi-Task Learning 2} & This repository gathers MTL papers and provides a categorisation. \\
    \midrule
    
    \multirow{ 2}{*}{Programming Framework} 
    & \href{https://github.com/zhanglijun95/AutoMTL}{AutoMTL} \cite{AutoMTL} & This solution performs automatic MTL model compression given an arbitrary backbone and a set of tasks. \\
    &
    \href{https://github.com/median-research-group/LibMTL}{LibMTL} \cite{LibMTL} & This is a Python library for MTL built on Pytorch. The implementation supports a large number of SOTA solutions, weighting strategies and data loaders. \\
    \midrule
    \multirow{ 2}{*}{Benchmarking} 
    & \href{https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch}{Dense Prediction Tasks} \cite{MTL-dense-predic-tasks} & This solution benchmarks a 2 MTL solutions on CV dense prediction tasks on 2 datasets. It is implemented in Pytorch.\\
    & 
    \href{http://taskonomy.stanford.edu}{Taskonomy} \cite{taskonomy} & In addition to providing web-based visualisations. Taskonomy \cite{taskonomy} introduces a API to group 25 vision tasks. Pre-trained models are available in Tensorflow and Pytorch. \\ 
    \midrule
    \multirow{ 1}{*}{Self/Semi-supervision} 
    & \href{https://github.com/VICO-UoE/MTPSL}{MTPSL} \cite{MTPSL} & This solution implements different cross-task mapping under balanced and imbalanced semi-supervised settings for dense prediction tasks. This solution is implemented in Pytorch and supports two datasets.\\
     & \href{https://multimae.epfl.ch}{MultiMAE} \cite{MultiMAE} & This solution implements a pre-trained strategy inspired by Masked Auto-Encoders (MAEs). In addition to visualisations, tutorials are presented. The solution is implemented in Pytorch.\\
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

\cref{tab.2} presents some of the results obtained by MTPSL \cite{MTPSL} and SemiMTL \cite{semi-supervised-MTL-for-semantics-depth} on two commonly used MTL datasets: NYUv2 \cite{NYUv2} and Cityscapes \cite{cityscapes}. The results are reported on three tasks for NYUv2 \cite{NYUv2} including semantic segmentation, depth estimation and surface normals. Additionally, the results are reported on semantic segmentation and depth estimation for Cityscapes \cite{cityscapes}. 
First, MTPSL \cite{MTPSL} evaluates its cross-task consistency mapping method under two data availability settings. The first configuration consists of $\frac{1}{3}$ of the images, labelled with the three tasks, noted as MTPSL (1/3). The results reported in this setting suggest a degradation in performance compared to the single task learning (STL) baselines. However, the other setting, consisting of all images being labelled with only one of the tasks and noted as MTPSL (one) present better results closer to the STL baseline for all tasks. Although the two data settings present the same labeling demand, they showcase different performance. Therefore, this difference demonstrates that the joint space mapping is efficient \cite{MTPSL} under semi-supervised settings. 
Moreover, MTPSL \cite{MTPSL} displays, as part of their evaluation on Cityscapes \cite{cityscapes}, that some tasks are worth being shared more than others. The authors introduce an imbalanced supervision paradigm option and choose to use only 10\% of a task whilst keeping 90\% of the other task, noted as MTPSL (1:9), meaning 10\% of input images are annotated with segmentation ground truth and 90\% are labelled with depth ground truth. The results for imbalanced tasks present strong robustness, whereas the advantaged tasks outperform STL baselines.

\cref{tab.3} shows a range of publicly available code repositories for MTL including paper repositories, programming framework, benchmarking and partially-supervised code resources.

\section{Conclusion}
This review provided an extensive and comprehensive analysis of MTL systems in Computer Vision. 
Firstly, this work studied how architectural implications impact parameter sharing across tasks. 
Second, we analysed the concept of negative transfer and introduced MTL methods to remedy this issue through balancing the pace to which tasks learn during the training of a MTL system.
Third, this paper briefly reviewed how task relationships can be leveraged to provide new insights to task hierarchies to further improve the performance of MTL systems.
Fourth, we extensively reviewed how MTL can be utilised under partially supervised settings, for instance, as a self-supervised pre-training strategy for representation learning, or by exploiting task relationships to reduce the demand for labelled tasks in semi-supervised learning or finally by enhancing few-shot target tasks through cross-task parameter sharing.
Last, we summarised common multi-task datasets and code repositories to provide the interested reader with the necessary toolkits. 
We provide an analysis of results for partially-supervised MTL techniques. Our key insights for future work under this paradigm are as follows: (1) MTL generally processes a small and constrained set of presumably related tasks. We identify there is a lack of adaptive methods, capable of learning relevant features from a large pool of tasks; otherwise, (2) research in partially-supervised learning suggest MTL strongly enhances established single-task baselines (\ie \textit{Few-Shot Learning, Self-MTL}) but is rarely applied to conventional MTL systems where multiple tasks are trained simultaneously. Finally, (3) we identify that MTL requires more benchmarking tools on large datasets. Taskonomy \cite{taskonomy} is the first step towards this direction and similar work could bring new insights to future research in MTL. 


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% ============================================
%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here %\cite{Roberg2010}.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank D. Root for the loan of the SWAP. The SWAP that can ONLY be usefull in Boulder...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% ====== REFERENCE SECTION

%\begin{thebibliography}{1}

% IEEEabrv,

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Bibliography}
%\end{thebibliography}
% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{% Figure removed}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission
\begin{comment}
\begin{IEEEbiography}[{% Figure removed}]{Michael Roberg}
(S'09) received the B.S.E.E degree from Bucknell University, Lewisburg, PA, in 2003, the M.S.E.E. degree from the University of Pennsylvania, Philadelphia, in 2006, and the Ph.D. degree from the University of Colorado at Boulder in 2012. From 2003 to 2009, he was an Engineer with Lockheed MartinMS2, Moorestown, NJ, where he was involved with advanced phased-array radar systems. His current research interests include high efficiency microwave PA theory and design, microwave power rectifiers, MMIC design, and high-efficiency radar and communication system transmitters. He is currently employed by TriQuint Semiconductor - Defense Products and Foundry Services in Richardson, TX working on wideband high efficiency GaN MMIC PA design.
\end{IEEEbiography}
\end{comment}

%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{Ignacio Ramos}
%(S'12) received the B.S. degree in electrical engineering from the University of Illinois at Chicago in 2009, and is currently working toward the Ph.D. degree at the University of Colorado at Boulder. From 2009 to 2011, he was with the Power and Electronic Systems Department at Raytheon IDS, Sudbury, MA. His research interests include high-efficiency microwave power amplifiers, microwave DC/DC converters, radar systems, and wireless power transmission.
%\end{IEEEbiographynophoto}

%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\end{document}


