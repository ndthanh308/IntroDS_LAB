%
% IEEE Transactions on Microwave Theory and Techniques example
% Tibault Reveyrand - http://www.microwave.fr
%
% http://www.microwave.fr/LaTeX.html
% ---------------------------------------



% ================================================
% Please HIGHLIGHT the new inputs such like this :
% Text :
%  \hl{comment}
% Aligned Eq. 
% \begin{shaded}
% \end{shaded}
% ================================================



\documentclass[journal]{IEEEtran}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}  
\usepackage{xcolor,soul,framed} %,caption

\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb} % for \mathbb
\usepackage{comment}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage{epsfig}
\usepackage[ruled,lined]{algorithm2e}
%\usepackage{algpseudocode}% 
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{soul}
\usepackage[numbers]{natbib}
\usepackage{pifont}
\usepackage{longtable}
\SetKwComment{Comment}{/* }{ */}

\newcommand{\miaojing}[1]{\textcolor{red}{MS: #1}}
\newcommand{\papertomention}[1]{\textcolor{green}{Paper: #1}}

\hyphenation{op-tical net-works semi-conduc-tor}

%\bstctlcite{IEEE:BSTcontrol}
\newcommand{\mynorm}[1]{ \left\| #1 \right\| }
\usepackage{soul}
\newcommand{\MWS}[2]{{\color[rgb]{0.4,0,0}\st{#1}} \color[rgb]{0,0.33,0}#2\normalcolor}
\newcommand{\Correction}[2]{{\color[rgb]{0.4,0,0}\st{#1}} \color[rgb]{0,0.5,0.5}#2\normalcolor} 

\usepackage{xspace}
\newcommand{\etc}{{\it etc.}\xspace}
\newcommand{\ie}{{\it i.e.},\xspace}
\newcommand{\eg}{{\it e.g.},\xspace}
\newcommand{\via}{{\it via}\xspace}
\newcommand{\viz}{{\it viz.}\xspace}
\newcommand{\cf}{{\it cf.},\xspace}
\newcommand{\etal}{{\it et al}\xspace}
\newcommand{\apriori}{{\it a priori}\xspace}
\newcommand{\adhoc}{{\it ad hoc}\xspace}

%=== TITLE & AUTHORS ====================================================================
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
\title{When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review}
\author{
  Maxime Fontana, Michael Spratling, and Miaojing Shi,~\IEEEmembership{Senior Member,~IEEE}
  \thanks{Manuscript received June 23, 2023; revised March 14, 2024; accepted July 7, 2024. \textit{(Corresponding Author : Miaojing Shi)}}
  \thanks{{Maxime Fontana} is with the Department of Informatics, King's College London, London WC2B 4BG, United Kingdom (e-mail: maxime.fontana@kcl.ac.uk)} 
  \thanks{{Michael Spratling} is with the Department of Behavioural and Cognitive Sciences, University of Luxembourg, L-4366 Esch-sur-Alzette, Luxembourg and the Department of Informatics, King's College London, London WC2B 4BG, United Kingdom (e-mail: michael.spratling@uni.lu)} 
  \thanks{{Miaojing Shi} is with the College of Electronic and Information Engineering, Zip code : 201804, Tongji University, and with the Shanghai Institute of Intelligent Science and Technology, Tongji University (e-mail: mshi@tongji.edu.cn)} 
}

\markboth{M.Fontana et al.}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% ====================================================================
\maketitle

% === ABSTRACT ====================================================================
% =================================================================================


\begin{abstract}
%\boldmath
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. 
This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising from such a multi-objective optimisation scheme. Third, it introduces how task groupings can be achieved by analysing task relationships. Fourth, it focuses on how partially supervised methods applied to MTL can tackle the aforementioned challenges. Lastly, this review presents the available datasets, tools and benchmarking results of such methods. The reviewed papers, categorised following our work, are available: \href{https://github.com/Klodivio355/MTL-CV-Review}{https://github.com/Klodivio355/MTL-CV-Review}.
\end{abstract}


% === KEYWORDS ====================================================================
% =================================================================================
\begin{IEEEkeywords}
Multi-Task Learning; Deep Learning; Minimal Supervision; Autonomous Driving; Visual Understanding; Medical Imaging; Robotic Surgery
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:IEEE transactions template
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
% The paper headers


\IEEEpeerreviewmaketitle


% ====================================================================
% ====================================================================
% ====================================================================




% === I. INTRODUCTION =============================================================
% =================================================================================
\section{Introduction}

%\IEEEPARstart{C}{} 
Convolutional Neural Networks (CNNs) have achieved great success in numerous and diverse computer vision tasks such as classification \cite{classification-sota, classification-success-1, classification-success-2, classifcation-success-3}, semantic segmentation \cite{semantic-segmentation-sota, Fast-R-CNN, unet, semantic-success1} and object-detection \cite{object-detection-sota, YOLOv3, semantic-success1}. These models have the common characteristic of being task specific. However, systems should ideally be capable of sharing knowledge between tasks.

Multi-Task Learning (MTL) \cite{Caruana} aims at providing computational models able to learn multiple tasks. To achieve this, MTL seeks to partition representations into task-agnostic and task-specific features so that each task can utilise a common representation. This is justified by previous work investigating the learning of representations in CNNs that distinguish two types of features. Firstly, shallow layers, which learn simple patterns (\ie edges and colors), are task-agnostic and should be shared. Secondly, deep layers which learn complex patterns (\ie objects), should be kept task-specific \cite{visualizing-CNN}. However, determining how to partition a specific network hierarchy is not trivial and depends on the tasks at hand \cite{cross-stich}. 
Nonetheless, MTL could help discover relationships and structure amongst tasks \cite{predictive-structures, task-relationships} which could improve performance compared to task-specific models. From a computational efficiency perspective, sharing representations results in enhanced memory efficiency and a significant reduction in inference time as shared representations only need to be inferred once to predict multiple tasks. 

Deep Learning (DL) models generally suffer from a high data-dependency during training, but acquiring large volumes of labeled data is not always feasible. This has motivated the development of various partial supervision configurations, with the unifying goal to create data-efficient DL solutions \cite{partly-supervised-MTL, MTPSL, semi-supervised-MTL-for-semantics-depth, Semi-supervised-MTL, semi-MTL-for-lung-cancer}. MTL brings a new opportunity for such techniques: by leveraging relationships between tasks, MTL can use the available supervisory signals for one task to aid the learning of other tasks.

\textbf{Applications.}
MTL is currently being employed in Computer Vision (CV) due to its success in achieving advanced scene understanding. Its most studied area is urban scene detection \cite{multinet, end-to-end-autonomous-driving, unified-autonomous-driving, Andrej-talk}, specifically to address autonomous driving related tasks such as road segmentation and object detection. MTL has also been successfully used in robotics, specifically in robotic-assisted surgery \cite{AP-MTL,ST-MTL} to predict diverse effects from a surgery scene (instruments, tissues etc.). Additionally, this paradigm has been heavily studied in the context of face recognition \cite{face_recognition-facial-expression, face-parsing, privacy-preserving-face, MTLFace} to enable, for instance, the simultaneous prediction of facial expression, face detection, and identification. MTL has also been explored in medical applications, such as in medical image segmentation in the area of gastroenterology for detection of polyps \cite{upper-gastro, explicable-capsule-endoscopy, esophageal}, or in cardiology for atrial segmentation \cite{3d-cardiac}. In addition, MTL has been applied to non-CV scenarios such as Natural Language Processing (NLP) \cite{MTL-NLP-networks, NLP-MTL-ADAPTIVE, NLP-NER, Vietnamese-POS, NLP-SEQ, NLP-POS} and recommendation systems \cite{MTL-recommendation-systems-1, MTL-recommendation-systems-2, MTL-recommendation-systems-3}.

% Figure environment removed

% Figure environment removed

\textbf{Related Work.} 
MTL has been the subject of numerous and diverse review papers \cite{MTL-overview-2017,MTL-in-DNN-2020, MTL-survey-2021,MTL-dense-predic-tasks, Deep-reinforcement-learning-survey, MTL-NLP-survey, MTL-NLP-overview}. Some of these previous works have focused on specific domains such as NLP. For instance, \citet{MTL-NLP-overview} focus on MTL-based solutions for various NLP tasks and provide a classification for available solutions, whilst \citet{MTL-NLP-survey} focus on NLP-related training procedures and task relatedness. Alternatively, \citet{Deep-reinforcement-learning-survey} review MTL in the domain of deep reinforcement learning (DRL). Other published reviews have focused on MTL from an optimisation perspective, for instance by comparing the different loss weighting techniques \cite{a-comparison-of-loss-weightin-strategies} or by evaluating task-specific transfer learning strategies \cite{taskonomy, factors-of-influence}. 

Some works have, however, aimed at providing a less constrained review of MTL. For instance, \cite{MTL-overview-2017, MTL-in-DNN-2020} reviews fully-supervised MTL methods as well as the inherent optimisation challenges under the deep learning framework. Moreover, \cite{MTL-survey-2021} provides a full-fledged and comprehensive review on both linear and DL solutions as well as the underlying optimisation techniques.

Previous work has focused on area more closely related to this study. For example, a CV-focused review \cite{MTL-dense-predic-tasks} analyses how MTL has been applied to pixel-wise prediction tasks and provides benchmark results on common fully-supervised MTL architecture. \cite{MTL-dense-predic-tasks} further differentiates MTL architectures based on the location where task interactions take place (encoder vs decoder). This paper, in contrast, does not highlight such differentiation as architectural issues are not the focus of our analysis. This review instead focuses on partially-supervised learning paradigms applied to CV tasks in a multi-task fashion. Although the vast majority of multi-task learning solutions has been applied to dense prediction tasks, this work aims at providing a comprehensive understanding of how MTL's future improvement might be underpinned by increasing the number and diversity of tasks. This study is the first, to the best of our knowledge, to focus on partially-supervised MTL for CV. 
\\
The literature for this review was selected through a comprehensive search of academic databases and was further refined based on the relevance to the central themes of this study and the author's expert judgment, ensuring the inclusion of both foundational and cutting-edge research of significant interest.

\textbf{Paper overview.} \cref{chapter:MT-parameter-sharing} reviews traditional fully-supervised MTL methods from a parameter-sharing perspective. 
\cref{chapter:Optimisation} introduces challenges arising from such multi-objective optimisation. 
\cref{sec:task-grouping} analyses relationships between common CV tasks, and how task groupings can be used to identify mutually beneficial tasks. 
\cref{chapter:partial-supervision} discusses how MTL can be used under partially-supervised paradigms. Last, \cref{sec:datasets} is dedicated to an introduction to available datasets, code repositories and tools as well as a comparison of the solutions introduced in this review.
We provide an structural overview of this work in \cref{overview}.

Furthermore, we provide an outline of the varied landscape of related MTL reviews, contextualizing our research within this framework. See \cref{related-work} for more details.



% === II. Multi-Task Parameter Sharing =============================================================
% =================================================================================
\section{Multi-Task Parameter Sharing}
\label{chapter:MT-parameter-sharing}
In order to understand the underlying challenges to MTL, \cref{sec:non-neural-MTL} reviews cross-task parameter sharing introduced in traditional settings. Subsequently, \cref{sec:feature-fusion} will review feature fusion paradigms under two major frameworks: CNN and Vision Transformers. Then, \cref{sec:knowledge-decomposition} investigates how learned representations can be partitioned and further shared. Finally, \cref{sec:parameter-NAS} will focus on architecture search based strategies as a way to share parameters across different tasks. 

\subsection{Traditional Parameter Sharing}
\label{sec:non-neural-MTL}
\subsubsection{Sparse Multi-Task Representations}
\label{sec:sparse}
The core of the early work in MTL has focused on obtaining a sparse multi-task parameter matrix generally obtained by linear models such as support vector machines (SVM) or ridge regression. Concretely, a parameter matrix is said to be sparse if a large proportion of its values are close to 0. The sparsity objective is based on the assumption that only a low-dimensional sub-representation of parameters should be shared across all the tasks. For example, \textit{Multi-Task Feature Learning} (MTFL) \cite{MT-feature-learning} defines the objective as an optimisation using the L1 regularisation. Considering a linear feature matrix $U \in \mathbb{R}^{d \times d}$ where $d$ is the parameter dimension, MTFL \cite{MT-feature-learning} aims at learning a transformation matrix $A \in \mathbb{R}^{d \times T}$ where $T$ is the number of tasks, such that $W$ = $UA$, with $W \in \mathbb{R}^{d \times T}$. Formally, such objective can be defined as the minimisation of the following function:
\begin{equation} \label{eq:sparse}
f(A, U) = \sum_{t=1}^{T} \sum_{i=1}^{m} L(y_{ti}, a_{t} \cdot (U^{T}x_{ti})) + \gamma||A||^{2}_{1},
\end{equation}
where the first term is the empirical error for the $i^{th}$ data-label pair $(x_{ti}, y_{ti})$ for a task $t$. In the second term, the transformation matrix $A$ is constrained by the the regularisation term, which is itself controlled by the non-negative parameter $\gamma$. As a result, the sparsity imposed on the transformation matrix $A$ will lead to most rows in $A$ being equal to 0. After the transformation $W = UA$, these rows will represent task-specific parameters whilst others represent the shared low-dimensional subspace $W$ across tasks. 
%Finally, MTFL \cite{MT-feature-learning} introduces a convex formulation of this optimisation and 
However, such objective only partition features. MTFL \cite{MT-feature-learning} aims to jointly learn the parameters and their partition. The resulting strategy is therefore to minimise the function $f$ over the parameter $U$. However, although such strategy results in a bi-convex on $A$ and $U$ individually, the minimisation optimisation objective is not, rendering the optimisation challenging. Therefore, MTFL \cite{MT-feature-learning} introduces a convex formulation to their problem. To a further extent, the authors suggest non-linear features can be obtained through the use of kernel learning \cite{learning-multiple-tasks-with-kernel-methods} therefore allowing the model to learn non-linear relationship between parameters. 

Following this sparsity objective, previous work has investigated using different linear models such as the Group Lasso Method \cite{taking-advantage-of-sparsity}, by improving over the convergence speed of the sparsity objective, or by minimising the trace-norm of $A$ \cite{MTL-l21, trace-norm-minimization}. 
Nonetheless, this paradigm is essentially constrained to only a small subset of shared features. Moreover, it also assumes tasks are related as some features are shared anyway. However, intuition suggests it should not always be the case. To counter this, some works \cite{dirty-model, learning-task-grouping-and-overlap} allow for an adaptive and partial overlapping of the task parameters to only share parameters when necessary.

\subsubsection{Clustering}
\label{sec:clustering}
To mitigate the a-priori assumption that all tasks are related, some works have investigated how to identify task relationships under a task clustering framework. Such methods are motivated by the assumption that similar tasks have similar weight vectors. Obtaining such clusters helps narrow down the search space for the shared low-dimensional parameter space.
For instance, \citet{clustering-learning-tasks} introduce a Task Clustering (TC) algorithm based on K-Nearest Neighbours (KNN) in which information is shared within clusters. Specifically, given two tasks $T_{1}$ and $T_{2}$, performance gain (PG) is calculated for the task pair through transfer learning (\ie $PG_{T_{1} \rightarrow T_{2}}$ if knowledge is transferred from $T_{1}$ to $T_{2}$). The task clusters are formed based on such pair-wise performance gains. Then, knowledge transfer is performed only within the most related tasks.
Similarly, \citet{dirichlet-process-priors} introduce an automatic identification of such clusters based on the Dirichlet Process (DP) prior distribution. 
Later, with the aim of providing a convex formulation to this framework, \citet{clustered-MTL} suggest regularising the multi-task parameter space $W$ by imposing 3 different norms to model several orthogonal properties: the mean weight vector size $\Omega_{mean}$ which measures how large the weight vectors are on average by computing the trace over the $T$-task weight representation, 
\begin{equation} \label{eq:clustering1}
\Omega_{mean}(W) = tr(WUW^{T}),
\end{equation}
where $U \in \mathbb{R}^{T \times T}$ is a projection matrix which has all its entries equal $\frac{1}{T}$. Subsequently, the between-cluster and the within-cluster variance which respectively measures how close together the clusters are and how dense the clusters are. These measures can be formulated as follows:
\begin{equation} \label{eq:clustering2}
\Omega_{between}(W) = tr(W(M-U) W^{T}),
\end{equation}
\begin{equation} \label{eq:clustering3}
\Omega_{within}(W) = tr(W(I-M) W^{T}),
\end{equation}
where $M = L-I$ for which $L$ is the laplacian matrix and $I$ is an identity matrix. 
Finally, \citet{clustered-MTL} choose to combine these measures through a weighted sum as part of their minimisation objective : 
\begin{equation} \label{eq:clustering}
\min 
\left\{
\sum_{y \in {\{mean, between, within}\}} \gamma_{y}\Omega_{y}(W)
\right\},
\end{equation}
where $\lambda$ is a weight parameter for the norm $\Omega$ over the weight matrix $W$.
This multi-criteria weighting leads to a decomposition of $W$ such that similar tasks are close in parameter space. 

%Therefore, in addition to the minimisation of the empirical loss, the clustering objective can be simplified as in \cref{eq:clustering}:
\begin{comment}
\begin{equation} \label{eq:clustering}
min \sum_{i=1}^{m} L(t_{ti}, (W^{T}x_{ti}) +  \sum_{y \in {mean, between, within}}^{y} \gamma_{y}\Omega_{y}(A) 
%\gamma_{2}\Omega_{between}(A) +8 
%\gamma_{3}\Omega_{within}(A)
\end{equation}
\end{comment}

To explicitly model the distributions of the tasks to better identify their relationships, \citet{kernels-for-MTL,learning-a-kernel} introduce a kernel learning strategy to find a Reproducing Kernel Hilbert Space (RKHS) in which task-respective distributions are close together in parameter space if their relatedness is high enough. 
Finally, \citet{CMTL-via-ASO} interestingly derive the relationships between Clustered Multi-Task Learning (CMTL) in which similar tasks are clustered and sparse multi-task representations are learnt within clusters, as seen in \cref{sec:sparse}. The work introduces three algorithms to perform CMTL and demonstrates how the clustering approach is significantly more efficient than the low-dimensional subspace learning solution, especially under high-dimensional data settings.

\subsubsection{Common-Trunk}
% Figure environment removed
Early DL methods involved attaching task-specific heads to a CNN encoder's latent representation as shown in \cref{soft-hard} (top) \cite{Caruana}. For example, Ubernet \cite{Ubernet} introduced a CNN designed to tackle seven CV tasks. Many subsequent studies followed this design \cite{3d-cardiac, uncertainty, multinet, multitask-centernet}. This architecture shares a CNN backbone which gets updated by gradients aggregated by multiple tasks. As a result, all the tasks pull features from this backbone, which makes a global learned representation critical, although not trivial to obtain as different tasks need different representations to perform well. 
Hence, recent works suggest sharing parameters as part of multi-task encoder-decoder architectures at the decoder level \cite{PAD-net, PAP, MTI-NET, exploring-relational-context, invPT} to exchange high-level semantic features. For instance, Prediction-and-distillation Network (PAD-NET) \cite{PAD-net} suggests sharing knowledge after predictions and allows the training of a distillation module to learn what to share. \citet{MTI-NET} expend on this idea whilst incorporating multi-scale prediction for better dense prediction task performance. Similarly, at the prediction level, Pattern-Affinitive-Propagation (PAP) \cite{PAP} proposes learning pair-wise task relationship to produce affinity matrices for each task to further guide the sharing strategy.  

\subsection{Feature Fusion}
\label{sec:feature-fusion}
This section introduces parameter fusion techniques used in the two most pre-dominant vision models. First, \cref{subsec:CNN} introduces methods to share parameters across CNNs. Then, \cref{subsec:transformer} reviews recent attention-based methods to fuse parameters in Vision Transformers (ViTs) \cite{ViT}.
\subsubsection{CNN Sharing Strategies}
\label{subsec:CNN}
Cross-stitch Networks \cite{cross-stich} introduce a model-agnostic fusion technique. As opposed to the hard-parameter sharing paradigm, in which task-decoders are attached to a shared backbone encoder (Fig.~\ref{soft-hard} (top)), \citet{cross-stich} introduce a soft-parameter sharing paradigm in which task networks are processed independently and through which parameter fusion is executed in parallel at a similar level of abstraction (Fig.~\ref{soft-hard} (bottom)). Given two task activation maps $A$ and $B$, cross-stitch units \cite{cross-stich} compute the dot product between a vector representing their respective values $x^{i,j}_{A}$ and $x^{i,j}_{B}$  at a shared location (i, j) and a trainable weight matrix $W \in \mathbb{R}^{k \times k}$ , where \textit{k} is the number of tasks. The values in $W$ represent task-specific (diagonal entries) and shared parameters (non-diagonal entries). The process for $k=2$ is illustrated as:

\begin{equation} \label{eq:cross-stitch}
\begin{bmatrix}
\tilde{x}^{i,j}_{A}\\
\tilde{x}^{i,j}_{B}
\end{bmatrix}
= 
\begin{bmatrix}
w_{AA} & w_{AB}\\
w_{BA} & w_{BB}
\end{bmatrix}
\begin{bmatrix}
x^{i,j}_{A}\\
x^{i,j}_{B}
\end{bmatrix}.
\end{equation}
Despite being a locally-flexible, easy-to-implement and model-agnostic method, its design results in a complex and expensive model. First, studies investigating CNN layers have shown that shallow layers are usually task-agnostic and cross-stitch units would eventually represent such task-agnostic parameters, but at an expensive of training cost.  Second, the overall solution is expensive as the training costs increase with the number of tasks and the size of the network.

Sluice Networks \cite{sluice} generalise cross-stitch units by increasing the flexibility and sharing parameter space. In particular, \citet{sluice} allow for selective sharing of layers, parameter subspaces and skip connections.
%define task-specific subspaces in order to narrow down the search space and complexity and learn feature fusing weights across those subspaces.
%Neural Discriminative Dimensionality Reduction
To expand on this soft-parameter sharing structure, \citet{NDDR} propose a solution based on the principle of Neural Discriminative Dimensionality Reduction (NDDR). This principle attempts to minimise the number of features whilst keeping the maximum amount of representative information, similarly to Linear Discriminant Analysis (LDA) or Principal Component Analysis (PCA). Therefore, NDDR \cite{NDDR} formulates the multi-task feature fusion problem as a discriminative dimensionality reduction problem by first concatenating parallel feature maps, then task-specific $1 \times 1$ convolutions \cite{network-in-network} are run on such representation to perform dimensionality reduction. In addition, the authors employ batch normalisation and weight decay to stabilise learning. This method is illustrated in \cref{NDDR}.
% Figure environment removed

%NDDR-layers have been utilised in different multi-task models at various locations. For instance, Multi-Task Attention Network (MTAN) \cite{MTAN} introduces NDDR-layers to fuse task-specific soft-attention modules in an encoder-decoder architecture to jointly perform instance segmentation and depth prediction. 
As a result, NDDR \cite{NDDR} enables learnable local representation parameter sharing in a similar manner to cross-stitch and sluice networks \cite{cross-stich, sluice}. However, these techniques hypothesise that all tasks should be processed together, computational cost could therefore be reduced using prior knowledge on task groupings to avoid redundant computation. 

%% Figure environment removed

\subsubsection{Attention-based Sharing Strategies}
\label{subsec:transformer}
With the advent of the transformer model \cite{attention-is-all-you-need}, originally applied to NLP, and subsequently to CV \cite{ViT}, there has been a great improvement in dense prediction tasks in CV \cite{PVT, PVT-v2, swin-transformer, focal-transformer} due to the non-local feature acquisition inherent to these models as well as their capacity to exploit long-range dependencies. 
%MTAN \cite{MTAN} was first to incorporate attention mechanisms into a MTL framework.
Similar to the aforementioned soft-parameter sharing techniques \cite{cross-stich, sluice, NDDR}, Multi-Task Attention Network (MTAN) first trains a single CNN network which is designed to learn general features. Then, task-specific networks are derived by attaching attention modules, which learn soft-attention masks over the shared features, to each convolutional operation of the aforementioned CNN network.

\textit{Unified Transformer} (UniT) \cite{UniT} learns a multi-modal encoder-decoder transformer model. UniT \cite{UniT} learns modality-specific encoders using multi-head self-attention, the modalities are then simply concatenated before a joint decoder performs cross-attention to mix the multiple representations. Similarly, \textit{Multi-Task Transformer Network} \cite{MulT} (MulT) performs feature fusion at the decoding level and introduces a shared attention mechanism. Specifically, MulT chooses a reference task $t^{ref}$,
%(in their case, \textit{surface normal estimation} is empirically chosen), 
then the reference task encoded representation $x$ is used to compute a query $q^{t^{ref}}_{x}$ and a key $k^{t^{ref}}_{x}$. Let us denote $v^{t}$ the values for the other tasks based on the previous stage output. The attention values for this task are then calculated as:
\begin{equation} \label{eq:attention}
A^{t^{ref}}_{x} = softmax\left(\frac{q^{t^{ref}}_{x} \cdot {k^{t^{ref}}_{x}}^{T}}{\sqrt{C_{qkv}^{t^{ref}}}}\right) + B^{t^{ref}}.\
\end{equation}
Subsequently, for any task $t$, the shared representation is obtained as:  $\tilde{x}^{t} = A^{t^{ref}}_{x}v^{t}$.  The term $x^{t}$ is then used for the multi-head attention.

MTFormer \cite{MTFormer} also chooses to compute cross-task interactions at task-specific heads. However, the authors choose to concatenate the projected representations at each transformer block, based on multi-head self-attention operations. To merge the attention maps of $n$ tasks, the authors show it is beneficial to consider self-task attention as a primary task and to consider cross-task attention as playing an auxiliary role in order to perform cross-task feature propagation. To reflect this, the authors choose to reduce the number of projected feature channels $C$ of auxiliary tasks such that $C' = \frac{C}{n-1}$, whilst keeping the original dimension for the main task. 

Finally, motivated by the success of pyramid-based transformer-based encoded representations for dense prediction tasks \cite{swin-transformer, PVT, PVT-v2}, InvPT \cite{invPT} proposes a cross-scale self-attention mechanism for multiple tasks. In this method, the attention maps are linearly combined by learnable weights, the result is also constrained by a residual feature map from the input image.


\subsection{Knowledge Decomposition}
\label{sec:knowledge-decomposition}
Knowledge Decomposition aims at partitioning a large set of features into smaller and meaningful components. In the context of MTL, one might be interested in recycling large models into smaller multi-task models. 
First, \cref{sec:tensor-factorization} reviews how tensor factorization can operate over CNN kernels to construct MTL components. Second, \cref{sec:knowledge-distillation} introduces methods to transfer information from a large single-task teacher model to a smaller multi-task student model. 
Last, \cref{sec:adapters} reviews how adapters can be used to achieve multi-task continual learning by fine-tuning a large single-task model.  

\subsubsection{Tensor Factorization}
\label{sec:tensor-factorization}
\cref{sec:sparse} reviewed solutions employing the low-rank approximation of a multi-task weight matrix using linear models. Deep Multi-Task Representation Learning (DMTRL) \cite{deep-multi-task-representation-learning} generalises this idea to tensors (N-dimension arrays with $N \in \mathbb{N}$ and more specifically $N \ge 3$). In fact, as per the nature of a CNN, kernels are N-dimension tensors and fully convolutional (FC) layers are 2-way tensors, stacking those by a number of tasks $T$, usually resulting in large tensors. Tensor Factorization (TF) is a generalisation of some form of matrix decomposition, such as Singular Value Decomposition (SVD) \cite{SVD}.
%, however, due to the usual high-order of tensors, this renders such process complicated. 
DMTRL \cite{deep-multi-task-representation-learning} accomplishes soft-parameter sharing in a layer-wise manner between parallel and identical CNNs, similarly to \cite{cross-stich, NDDR}. First, single-task CNNs are trained, then layer-wise parameters are concatenated during backpropagation and subsequently fed as input to SVD-based solutions for decomposition. DMTRL \cite{deep-multi-task-representation-learning} uses multiple sharing strategies, including one based on the Tucker Decomposition (TD) \cite{Tuck1966c}, to learn parameters of this SVD-based solution to generate the decomposed units. 

Further to this strategy, \citet{Trace-norm-DML} use the tensor trace norm (the sum of a tensor's singular values) as a proxy of the tensor rank on the layer-wise parameters' concatenation. In this way, each CNN is encouraged to use the other network's parameters. However, these methods have the same drawback as the previously introduced parameter-fusion based techniques \cite{cross-stich, NDDR, sluice} as parameters are shared in a layer-wise fashion which introduces constraints including architectural parallelism and locality in the parameter sharing strategy. 

\subsubsection{Knowledge Distillation}
\label{sec:knowledge-distillation}
Another perspective to parameter sharing is to design strategies based on Knowledge Distillation (KD). KD is a form a model compression that transfers knowledge from a large model to a smaller model. Early KD work in MTL explored how to compress DRL methods. For instance, \cite{policy-distillation, actor-mimic, distral} introduced a policy distillation strategy to derive lighter multi-task policies from task-specific deep Q-network (DQN) policies. However, as per the nature of DRL, these strategies approach tasks for which the set of actions was finite and would therefore struggle in more complex prediction visual tasks. 
As a result, \citet{PAD-net} introduce, as part of a multi-task multi-modal network, a distillation module to merge predictions from intermediate and complementary tasks from different modalities to subsequently pass representations on to task-specific decoders. The variations for this distillation module include cross-prediction reasoning as well as attention-guided mechanisms. 
Hence, \citet{knowledge-distillation-for-mtl} suggest a two-step solution in which: (1) task-specific models are first trained before freezing their respective parameters; (2) a multi-task model is optimized to minimise a multi-loss objective through the use of \textit{adaptors} (reviewed in \cref{sec:adapters}) that align task-specific and task-agnostic parameters together in order for the multi-task model to use the same features as the task-specific models. Following a similar strategy, \citet{MuST} extend this strategy to a self-supervised pre-training procedure through the use of intermediate pseudo-labeling. 

Recently, \citet{factorizing-knowledge-in-NNs} introduce a new alternative to KD, namely, \textit{Knowledge Factorization} (KF). Instead of distilling knowledge from a task-specific teacher model to a multi-task student model, KF aims at decomposing a pre-trained, large multi-task model into \textit{k} task-disentangled factor networks modelling both task-agnostic and task-specific parameters of the teacher model. The resulting lightweight networks can be assembled to create custom multi-task models.

\subsubsection{Adapters}
\label{sec:adapters}
With the aim of learning universal representations that can perform well across multiple domains, \citet{learning-multiple-visual-domains} introduce \textit{residual adapter modules}. Adapters are small neural networks that learn to recognise task-specific parameters given a model pre-trained on another task. Inspired by the ResNet \cite{resnet} architecture where residual connections are introduced across the sequential process of a CNN, adapters are modules attached after each convolutional block that learn to select parameters to be utilised for a downstream task. This presents an alternative to traditional \textit{fine-tuning} as only the adapters are trained. \citet{learning-multiple-visual-domains} demonstrate the capacity of adapter modules to maintain performance across 10 domains by just tuning a small portion of domain-specific parameters, and also their capacity to overcome the challenge of \textit{learning without forgetting} \cite{learning-without-forgetting}. 

\citet{efficient-parametrization-of-multi-domain-nn} introduce \textit{parallel} adapters as a simpler variant and show that only a few parameters need to be re-trained. As opposed to domain learning, \citet{attentive-single-tasking} show how adapters can be used in Incremental Multi-Task Learning (I-MTL). As a new task is optimised, \citet{attentive-single-tasking} train task-specific adapters to identify what parameters to retrain and \textit{Squeeze-and-Excitation} \cite{squeeze-and-excitation} modulation blocks perform channel-wise attention. Furthermore, to address the challenges raised by I-MTL, AdapterFusion \cite{adapter-fusion}, inspired by the multi-task objective adapter training strategy proposed by \cite{bert-and-pals}, introduces a 2-stage algorithm that enables task-specific parameters inside a transformer model to re-use other task-specific parameters contained in adapters. 
It is worth noting that, apart from the few aforementioned studies, adaptors have been studied far less in CV than in NLP. There is thus scope for exploiting this efficient parameter-sharing more fully in CV applications

%\papertomention{[MT-Uncertainty Sampling] Pilault, J., Elhattami, A., and Pal, C. J. Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters and Less Data. ICLR, 2021.}

\subsection{Neural Architecture Search}
\label{sec:parameter-NAS}
Neural Architecture Search (NAS) generally attempts to find the best network architecture given a specific problem by manipulating neural modules. However, in case of a multi-task objective, NAS can be seen as a way to partition the parameter space. For instance, \citet{soft-layer-ordering} introduce parameter sharing through \textit{soft ordering} (as opposed to \textit{parallel ordering}). The idea is to learn individual weight scalars per shared layers to \textit{soft-merge} parameters at different depths of a network. This comes down to learning a N-dimension tensor of task-specific parameters. Alternatively, Multi-gate Mixture of Experts (MMoE) \cite{MMoE} embeds the Mixture of Experts (MoE) framework \cite{MoE-layer} in MTL by sharing expert task-specific networks and optimising a gating network to select what features to use for each task. Following the same framework, \citet{DSelect-k} further improve the efficiency and stability of the selection of experts process and demonstrates its significant improvement on large-scale multi-task datasets. 

With the aim of learning an even more flexible assembling strategy, evolutionary algorithms have been proposed as a training strategy in which agents are network inference routes consisting of a set of computational blocks \cite{path-net, evolutionary-architecture-search-CTR}. Similarly, to learn large-scale MTL systems that tackle \textit{catastrophic forgetting} in the I-MTL paradigm, \citet{dynamic-introduction-of-tasks-in-large-scale} adopt an evolutionary algorithm to dynamically optimise a model each time a new task is added. Moreover, motivated by even more flexible ways to share features, some work has investigated using computational operations inherent to CNN layers as modulation units. For instance, \citet{flexible-mtl-by-learning-parameter-allocation} introduce the Gumbel-SoftMax Matrix model by modulating inner components of a layer, and shows how their activation is learned to optimise tasks through logits. Alternatively, \citet{adashare} show how routing policies can be learned through the Gumbel-Softmax sampling method \cite{gumbel-softmax} taking into account computational resources. Recently, \citet{AutoMTL} use this trick to integrate the learning of such policies into its programming framework. \citet{stochastic-filter-groups} modulate networks the same way as \cite{gumbel-softmax}, however \textit{stochastic filter groups} are introduced as a  way to model the distributions, approximated via \textit{variational inference} \cite{variational-inference}, over the possible kernel groupings. More recently, Adashare \cite{adashare} introduced MTL in such architecture search-based systems to model relationships between tasks by studying the partitioned feature space. As a result, recent studies have focused on leveraging these relationships to route information through networks. For instance, \citet{fully-adaptive-feature-sharing} incrementally expand on an initially small network, at each step, grouping similar tasks based on a measure of task affinity. Similarly, \citet{branched-mtl} implement a low-resource, layer-wise sharing strategy driven by NAS, exploiting task affinity measures. In a CV context, \citet{synergistic-MTL-NAS} leverage hardware-aware NAS \cite{h-NAS-survey} together with MTL to improve the accuracy of dense-prediction tasks on edge devices.
\\
\\
\section{Optimisation Challenges}
\label{chapter:Optimisation}
MTL has underlying optimisation challenges due to it being a Multi-Objective problem. MTL is subject to two major optimization issues. First, overall performance is dependent on the relatedness of the tasks being optimised. Unrelated tasks can have conflicting gradients that will lead to a non-convergence of a multi-task solution. This phenomenon is called \textit{negative transfer}. 
Second, multi-objective performance relies on a thorough task balancing problem as respective complexities interfere during training. For example, easier tasks converge faster, resulting in larger gradients being back-propagated across all tasks. This makes the acquisition of aggregated representations for different task gradients non-trivial.
This section reviews solutions aiming to tackle the aforementioned challenges. 
First, \cref{sec:loss-based-methods} reviews how individual losses can be adjusted to balance tasks in a MTL training. Second, \cref{sec:grad-based-methods} focuses on techniques that directly operate over gradient updates during training. Third, \cref{sec:moo} reviews techniques directly inspired from Multi-Objective Optimisation to perform gradient-descent under a MTL configuration. Last, \cref{sec:other-task-balancing-techs} introduces other task balancing approaches. 


% Figure environment removed

\subsection{Loss-based Techniques}
\label{sec:loss-based-methods}
Early work in deep MTL studied a weighted average of the task-specific losses $L_{i}$. By considering $\mathit{T}$ tasks, this multi-task loss can be formulated as follows:
\begin{equation} \label{eq:basic-mtl}
L_{MTL} = \sum_{i=1}^{T} w_{i} L_{i},
\end{equation}
where $\mathit{w_{i}}$ are respective positive task weights.

Rather than setting weights manually, solutions have been proposed to incorporate the weights into the objective function to adaptively weigh tasks during training. For example, AdaLoss \cite{adaloss} suggests adaptively tweaking weights in such a way that they are inversely proportional to the average of each loss in order to project losses onto the same scale. Alternatively, \cite{uncertainty} introduces learnable scalar parameters into the minimisation objective. The authors derive their loss weighting strategy based on the \textit{Homoscedastic (or task-dependent) uncertainty} which captures the uncertainty of a model, this type of uncertainty is invariant to different inputs. The authors follow a Gaussian likelihood maximisation setting and show that the loss optimisation given two tasks can be approximated as:
\begin{equation} \label{eq:uncertainty}
L_{uncert} (W, \sigma_{1}, \sigma_{2}) = \frac{1}{2\sigma_{1}^{2}} L_{1} + \frac{1}{2\sigma_{2}^{2}} L_{2} + \log \sigma_{1} \sigma_{2}.
\end{equation}
Following the same strategy, \citet{auxiliary-tasks-in-MTL} suggest a slight difference in the $\log$ regularisation term, by changing it to $\log(1+\sigma^{2})$. This is to prevent values of $\sigma \in [0,1]$ yielding negative loss values. We refer to this method as \textit{revised uncertainty}.
However, uncertainty-based task balancing strategies have certain drawbacks and in practice, task-wise terms need to be changed in \cref{eq:uncertainty} depending on the type of task (classification or regression) and depending on the task-specific loss.  As a result, IMTL \cite{impartial-MTL} introduces a hybrid method using both gradient methods and adaptive loss tuning. The loss component IMTL-L updates task-specific parameters and learns task-wise scaling parameters $s$ by minimising a function $g$ for each task as:
\begin{equation} \label{eq:IMTL-L}
g(s) = e^{s}L(\theta) - s.
\end{equation}
\cref{eq:IMTL-L} shows that each task loss is scaled by $e^{s}$ and regularised by $s$ to avoid trivial solutions. In practice, this technique allows tasks to all have comparable scales. Moreover, as opposed to uncertainty weighting \cite{uncertainty}, IMTL-L does not bias towards any type of task such as regression or classification. 
Alternatively, \citet{geometric} introduce a Geometric Loss Strategy (GLS), using the geometric loss to weigh $n$ task-specific losses $L_{1...n}$. The geometric loss is invariant to individual loss scales which makes it an easy way to balance tasks. As a result, \citet{geometric} decide to weight respective tasks as follows:
\begin{equation} \label{eq:geometric}
L_{geometric} = \Pi_{i=1}^{n} \sqrt[n]{L_{i}}.
\end{equation}
Additionally, the authors introduce a variant to focus on $m$ ($m < n$) `more important' tasks and therefore attribute more weighting to these as demonstrated below:
\begin{equation} \label{eq:geometric-prime}
\Tilde{L}_{geometric} = \Pi_{j=1}^{m} \sqrt[m]{L'_{j}} \times \Pi_{i=1}^{n} \sqrt[n]{L''_{i}}.
\end{equation}

Alternatively, balance of tasks can be achieved through averaging task weights over time by considering the rate of change in the respective task-specific loss. \citet{MTAN} introduce \textit{Dynamic Weight Average} (DWA). DWA calculates a specific task-specific weight $\lambda_{k}$ for a task $k$ by obtaining a relative descending rate compared to other tasks with respect to the previous iteration (averaged over multiple epochs) as follows:
\begin{equation} \label{eq:DWA}
\lambda_{k}(t) = \frac{K exp(w_{k}(t-1) / T)}{\sum_{i}exp(w_{i}(t-1) / T)} , w_{k}(t-1) = \frac{L_{k}(t-1)}{L_{k}(t-2)},
\end{equation}
where $T$ is a temperature parameter controlling the stiffness of the weighting distribution and $K$ ensures that $\sum_{i}\lambda_{i}(t) = K$.

More recently, Random Loss Weighting (RLW) \cite{RLW-RLG} has drawn task-specific weights from a probability distribution at each epoch before normalising them and shows comparable results to state-of-the-art (SOTA) loss-weighting strategies. As a result, \citet{RLW-RLG} provide a more generalisable solution than the baseline (\cref{eq:basic-mtl}), due to its additional randomness. Finally, \cite{a-comparison-of-loss-weightin-strategies} provides benchmark results comparing Single Task Learning (STL) to DWA \cite{MTAN}, uncertainty \cite{uncertainty} and revised uncertainty \cite{auxiliary-tasks-in-MTL} and suggests that, given careful task selection, the revised uncertainty method \cite{auxiliary-tasks-in-MTL} generally performs best but suffers when there is lack of training samples.

\subsection{Gradient-based Techniques}
\label{sec:grad-based-methods}
Weighting losses is an indirect way of changing the model's gradients. Therefore, a line of work has investigated how to optimise MTL models by directly operating over the gradients. Throughout this section, we refer to the illustration in \cref{optimisation-techs} which provides a visualisation of the gradient update techniques introduced by the presented methods. Informally, the problem is that during multi-task optimisation, a subset of parameters $\theta$ is shared across multiple tasks, as a result, $\theta$ generally receives gradient updates to optimise all tasks at once. In practice, this is achieved by finding an aggregated representation of the vectors. However, finding such representation is not trivial as task-respective gradients might conflict. Hence, GradNorm \cite{gradnorm} proposes a method that balances training by automatically tuning the gradient magnitudes. Considering a subspace of weights of a model $W$ (generally chosen as the last shared layer for computational purposes), GradNorm \cite{gradnorm} defines the $L_{2}$ norm of the gradient for a particular weighted task loss $\textit{i}$, and similarly defines $\overline{G_{W}}(t)$ the average gradient norm across all tasks at time $t$. Additionally, GradNorm \cite{gradnorm} defines 2 training rates. The first training rate is task-specific and is defined as $\widetilde{L}_{i}(t) = L_{i}(t)/L(0)$. It is the loss ratio for a task $i$ at time $t$. The second training rate defines a relative training rate for a task $i$ as follows:
\begin{equation} \label{eq:training-rate}
r_{i}(t) = \widetilde{L}_{i}(t) / \sum_{i=1}^{n} \widetilde{L}_{i}(t),
\end{equation}
where the right term is an averaged training rate over all tasks $n$ for the given time $t$.

Subsequently, GradNorm \cite{gradnorm} calculates new task-specific gradients for the weight subspace $W$ based on the update rule below:
\begin{equation} \label{eq:gradnorm}
G_{W}^{i}(t) = \overline{G}_{W}(t) \times [r_{i}(t)]^{\alpha}, 
\end{equation}
where $\alpha$ is a hyper-parameter controlling the force of traction towards a similar training rate for all tasks. This method, by directly operating over gradients during training, adaptively tunes the speed to which tasks are being trained. 
However, solely balancing tasks does not prevent conflicting gradients (negative transfer).

GradDrop \cite{GradDrop} proposes adding a modular layer that operates during back-propagation to first select a sign (positive or negative) based on the initial distribution of gradient values. It then proposes masking out all gradient values of the opposite sign. 
Similarly, \citet{adapting-aux-losses-using-gradient-similarity} leverage auxiliary tasks in order to optimise a main task. During training, \citet{adapting-aux-losses-using-gradient-similarity} only minimise the auxiliary losses if their gradient update at epoch $t$ is non-conflicting with the main task gradient update. Specifically, \citet{adapting-aux-losses-using-gradient-similarity} use the \textit{cosine similarity} to measure the gradients relation. Conceptually, if the cosine similarity between the main and auxiliary gradients is positive, it suggests that the auxiliary loss should be minimised alongside the main loss, otherwise, it should not. \citet{regularising-DMTL-orthogonal-gradients} use a similar strategy in a more conventional MTL setting, in which multiple tasks are optimised simultaneously. \citet{regularising-DMTL-orthogonal-gradients} use the cosine similarity to ensure shared gradients are near orthogonal. The authors refer to conflicting gradients when these have a negative cosine similarity, and non-conflicting when it is positive. Unlike \cite{regularising-DMTL-orthogonal-gradients} which ensures `near orthogonal' properties of the gradients via the minimisation of the loss, PCGrad \cite{gradient-surgery} projects only conflicting gradients by projecting those of task $i$ onto the normal plane of task $j$ as shown in \cref{PCgrad-figure} (b). Formally, such projection can be defined as:
\begin{equation} \label{eq:pcgrad}
\Delta g_{i} = g_{i} - \frac{g_{i} \cdot g_{j}}{\lVert g_{j} \rVert^{2}} g_{j}.
\end{equation}
However, imposing such strong orthogonality constraint upon gradients implies that all tasks at hand should benefit from similar gradient interactions, ignoring complex relationships and destructing natural optimisation behaviour. Moreover, PCGrad \cite{gradient-surgery} stays idle when the gradients have positive cosine similarity, which still might not be optimal as a more desirable similarity (closer a positive cosine similarity) might be preferred. Hence, GradVac \cite{gradient-vaccine} leverages both directions and magnitudes in an adaptive strategy. Specifically, given two tasks $i$ and $j$, a similarity goal $\phi^{T}_{i,j}$ is fixed between two gradients $\mathbf{g_{i}}$ and $\mathbf{g_{j}}$ such that $\phi^{T}_{i,j} > \phi_{i,j}$ for which $\phi_{i,j}$ is the cosine similarity, as computed in PCGrad \cite{gradient-surgery}. To achieve this, GradVac \cite{gradient-vaccine} derives the projection equation (\cref{eq:pcgrad} by fixing the gradient of $\mathbf{g_{i}}$ and rather estimates the weight of $\mathbf{g_{j}}$ via the Law of Sines in the gradients plane. This process can be summarised as:

\begin{equation} \label{eq:vcgrad}
\Delta g_{i} = g_{i} + \frac{ \lVert g_{i} \rVert (\phi_{ij} \sqrt{1 - \phi_{ij}^{2}} - \phi_{ij} \sqrt{1 - (\phi_{ij}^{T})^{2})}}{\lVert g_{j} \rVert \sqrt{1 - (\phi_{ij}^{T})^{2}}} \cdot g_{j}.
\end{equation}

Furthermore, using an Exponential Moving Average (EMA) (similar to DWA \cite{MTAN}), $\phi^{T}_{ij}$ is estimated in an adaptive manner during training, over a subset of shared parameters $\mathbf{w}$ belonging to the same layer as:

\begin{equation} \label{eq:ema-update-vcgrad}
\Delta \phi_{ijw} = (1 - \beta) \phi^{t}_{ijw} + \beta \phi^{t-1}_{ijw}.
\end{equation}

% Figure environment removed

Similarly, \citet{impartial-MTL} suggest a hybrid method leveraging both loss and gradient tweaking, IMTL \cite{impartial-MTL} chooses, in their gradient component IMTL-G, to make all the projections from each task equal to balance the tasks. 
Recently, RotoGrad \cite{rotograd} proposed a solution to both homogenise gradients magnitude and resolve conflicting ones. To achieve this, a 2-step algorithm is implemented. The first step consists in homogenising the gradients such that the tasks that have progressed the least are encouraged to learn more. Therefore, to project the gradients $\mathbf{G_{k}}$, for a task $k$, Rotograd \cite{rotograd} assigns weights to gradients such that their weighted combination is $\mathbf{C = \sum_{k} \alpha_{k} \lVert G_{k} \rVert}$. Precisely, $\alpha$ is adaptively calculated every $i^{th}$ iteration as:

\begin{equation} \label{eq:rotograd-alpha-update}
\alpha_{k} = \frac{\lVert G_{k} \rVert / \lVert G_{k}^{0} \rVert}{\sum_{i} \lVert G_{i} \rVert / \lVert G_{i}^{0} \rVert}.
\end{equation}

In the second step, Rotograd \cite{rotograd} tunes the gradients by learning a task-specific rotation matrix $\mathbf{R_{k}}$ on the last shared representation $\mathbf{z}$. Hence, $\mathbf{R_{k}}$ aims to maximise the cosine similarity between the gradients across tasks given a batch of size $n$; or equivalently, to minimise the loss function. This process can be illustrated as:

\begin{equation} \label{eq:rotograd}
L_{rot}^{k} = - \sum \left\langle R^{T}_{k} g_{n,k} , v_{k} \right\rangle.
\end{equation}

 
\subsection{Multi-Objective Optimisation}
\label{sec:moo}
Multi-Objective Optimisation (MOO) addresses the challenge of optimising a set of possibly conflicting objectives. This section reviews gradient-based multi-objective optimisation methods applied to MTL. First, \cref{sec:pareto-optimality} formally defines Pareto optimisation and how it is relevant to MTL under gradient descent techniques. Then, \cref{sec:gradient-descent-solutions} reviews gradient-descent optimisation solutions applied to MTL.


\subsubsection{Pareto Optimality}
\label{sec:pareto-optimality}
% Here you define what is multi-objective, what is pareto optimisation, how you can do that in Multiple Gradient Descent. What are pareto stationary points, the pareto front etc... Define it formally.
As presented in \cref{sec:loss-based-methods}, tuning the task-specific weights is not trivial and usually comes at the cost of computational overhead. One way to remedy this is to reframe the MTL optimisation into a MOO problem.
Motivation to use MOO for MTL comes from the fact that global optimality for multiple tasks is unconceivable unless a pair-wise equivalence between tasks exists, which is unrealistic. For a hard-parameter sharing network as depicted in \cref{soft-hard} (top), $\theta^{sh}$ represents parameters that are shared across all tasks and $\theta^{t}$, $t \in T$, are task-specific parameters. Additionally, $\widehat{L^{t}}(\theta^{sh}, \theta^{t})$ is the empirical loss for a specific task $t \in T$. Then, a multi-objective loss function can be defined as:

\begin{equation} \label{eq:moo-minimisation}
\min_{\theta^{1}, ... ,\theta^{T}} (\widehat{L}^{1}(\theta^{sh},\theta^{1}), ... , \widehat{L}^{T}(\theta^{sh},\theta^{T})).
\end{equation}

% Figure environment removed

\noindent Minimising \cref{eq:moo-minimisation} leads to Pareto-optimal solutions. In other words, in a MTL setting, considering both shared and task-specific parameters $\theta_{i}^{sh,t}$ and $\theta_{j}^{sh,t}$ for task $i$ and $j$ respectively, a Pareto-optimal solution is one for which a change in $\theta_{i}^{sh,t}$ would damage the performance of task $j$ and vice-versa. The set of Pareto-optimal solutions can therefore be considered as a set of trade-offs between tasks \cite{navon2021learning}. This set is called the \textit{Pareto front} ($P_{\theta}$).

Pareto optimality has extensively been studied leveraging the Multiple Gradient Descent Algorithm (MGDA) \cite{MGDA_paper} which supports the Karush-Khun-Tucker (KKT) conditions that are necessary conditions for Pareto optimality. 
MGDA \cite{MGDA_paper} demonstrates that minimising \cref{eq:minimisation-mgda} supports the KKT constraints and states that the result of this minimisation is either 0 and therefore results in a multi-task solution which satisfies the KKT conditions (a point along the pareto front); otherwise, this minimisation leads to a descent direction that improves all tasks. This process can be depicted as:
\begin{equation} \label{eq:minimisation-mgda}
\min_{\alpha^{1}, ...,\alpha^{T}} 
\left\{
\mynorm{ \sum_{t=1}^{T}\alpha^{t}\nabla_{\theta^{sh}}\widehat{L}^{t}(\theta^{sh}, \theta^{t})}^{2}_{2} 
\right\},
\end{equation}
where $\alpha^{t}$ are non-negative scaling factors such that: $\sum^{T}_{t} \alpha^{t} = 1$.
%In the context of MTL, the pioneering work \cite{MTL-as-MOO} state these conditions in the context of MGDA
%for shared $\theta^{sh}$ and task-specific $\theta^{s}$ parameters for $T$ tasks.

\begin{comment}
\begin{itemize}
  \item There exist a set $\alpha^{1}, ...,\alpha^{T} \geq 0$ such that $\sum_{t=1}^{T} \alpha^{t} = 1$ and $\sum_{t=1}^{T}\alpha^{t}\nabla_{\theta^{s}}\widehat{L}^{t}(\theta^{s}, \theta^{t}) = 0$  
  \item For all tasks $t$, $\nabla_{\theta^{t}} \widehat{L}^{t} (\theta^{s}, \theta^{t}) = 0$
\end{itemize}
\end{comment}

%Such condition, in words, a convex combination of gradients at a particular point that equals 0, introduce the principle of \textit{Pareto stationarity} which is necessary for pareto optimality. Specifically, the pareto optimisation problem can be summarised as follows.

%\cite{MGDA_paper} demonstrates that minimising this objective, under the constraints discussed before, either lead to 0 or as a descent direction that improves over all tasks.

\subsubsection{Gradient Descent Solutions}
\label{sec:gradient-descent-solutions}
In a MTL context, \citet{MTL-as-MOO} show that MTL optimisation can be regarded as a MOO problem using MGDA and demonstrates that solving \cref{eq:moo-minimisation} is equivalent to finding the min-norm point in the convex hull formed by the input points. That is, finding the closest point in a convex hull to a query point. As a result, \citet{MTL-as-MOO} obtain the aggregated projection of the task-specific gradient vector updates. Subsequently, to solve \cref{eq:moo-minimisation}, \citet{MTL-as-MOO} use the Frank-Wolfe solver \cite{Frank-Wolfe} and ensures, with negligible additional training time, the convergence to a Pareto-optimal solution.    
CAGrad \cite{conflict-averse-gradient-descent-for-mtl} generalises the MGDA algorithm and chooses to ensure the convergence of the MTL objective to the equally weighted average of task-respective losses. To achieve this, CAGrad \cite{conflict-averse-gradient-descent-for-mtl} first obtains an average vector $d$ of individual task updates $g_{i}$. Then, it aims to find an update vector $g_{w}^{t}$ on a pre-defined ball around $d$, which maximises the worst local improvement between ${T}$ tasks defined as: $\max_{d\in \mathbb{R}} \min_{i \in {T}} \langle g_{i},d \rangle$. This way, CAGrad \cite{conflict-averse-gradient-descent-for-mtl} balances the different task-specific objectives. Furthermore, the authors show the dominance of CAGrad in a semi-supervised setting compared to MGDA \cite{MTL-as-MOO}.
However, this approach ensures the convergence to any point along the Pareto front which might not be representative of the desired task balance, an unbalanced solution might be preferred to enhance a target task.
Therefore, Pareto MTL \cite{Pareto-MTL} proposes generalising MGDA to generate a set of multiple Pareto optimal solutions along the Pareto front which would serve as different trade-offs to choose from. To achieve this, \citet{Pareto-MTL} take inspiration from \cite{mmo-decomposition} and decomposes the objective space into $K$ well-distributed unit preference vectors $u_{k}$ to guide solutions. Formally, this is achieved through a sub-problem to \cref{eq:moo-minimisation} where a dot-product maximisation constraint is imposed between $u_{k}$ and a given vector $v$ to guide the learning onto a targeted area of the Pareto front. A sub-region is defined as:
\begin{equation} \label{eq:partition-pareto}
\Omega_{k} = \{v \in \mathbb{R}_{+}^{m} | u_{k}^{T}v \leq u_{k}^{T}v, \forall j = 1, ..., K\}.
\end{equation}

In contrast to its predecessors, \citet{continuous-pareto-exploration-in-MTL} suggest generating continuous Pareto optimal solutions along the Pareto front. To achieve this, \citet{continuous-pareto-exploration-in-MTL} propose a 2-stage training algorithm that, in its first stage, generates a single Pareto stationary point $x_{0}$ from a network's initialisation. Then, a set of points $x_{n}$ is explored along the tangent plane direction $v_{i}$ and the points are calculated as: $x_{i} = x_{0} + sv_{i}$ where $s$ is a step size. As a result, a set of directions is obtained. Finally, \citet{continuous-pareto-exploration-in-MTL} combine the tangent vectors acquired in the previous step through linear combination to form convex hulls in which Pareto solutions are obtained, resulting in a continuous approximation of a larger Pareto front. 

All the solutions introduced thus far in this section initialise network parameters per trade-off, resulting in a large storage demand and making solutions computationally inefficient. Additionally, the generated solutions are either singular \cite{MTL-as-MOO} or subject to the practitioner's preferences \cite{Pareto-MTL, continuous-pareto-exploration-in-MTL}. To alleviate both issues, \citet{controllable-pareto-mtl} propose utilising a HyperNet \cite{HyperNetworks}, a type of neural network that learns to generate the weights of another network, rendering storage less demanding. Additionally, \citet{controllable-pareto-mtl} introduce preference-based training to perform trade-off selection in real-time. More specifically, the objective space is sampled into $K$ subspaces (similarly to \cite{continuous-pareto-exploration-in-MTL}). Specifically, given a preference vector $\mathbf{p}$, the goal is to find a local Pareto optimal solution within such subspace for which the angle is the smallest to $\mathbf{p}$. To train the network on representative trade-off preference vectors, vectors are sampled using Monte Carlo methods and are given as input to the HyperNetwork $\mathbf{G}$. \citet{controllable-pareto-mtl} use a standard hard-parameter sharing strategy and such a process is depicted in \cref{controllable-pareto-fig}. Similarly, along the lines of preference-driven Pareto optimal solution, \citet{multi-objective-multi-task-learning-framework-induced-by-pareto} choose to directly cast the MOO optimisation as a Weighted Chebyshev (WC) problem which consists of finding the Pareto front by minimising the $l_{+\infty}$-norm between the initialisation point and the Pareto front. 

Recently, Nash-MTL \cite{bargaining-game} suggests a different approach to obtain an Pareto optimal solution. Inspired by the game theory literature, the authors directly aim at obtaining the \textit{Nash Bargaining Solution} \cite{Nash-bargaining-solution} which can be found on the Pareto front and translates to a proportionally fair solution where any change to the state results in a negative update for at least one task. Specifically, let's consider $U \in \mathbb{R}^{T}$ the set of all possible trade-offs and similarly, $ D \in \mathbb{R}^{T}$ the default set of disagreements, namely, a trade-off if all tasks $T$ fail to agree on an agreement. Moreover, in order to find a task agreement, namely, find a solution for $U$ with columns $u_{i}$ such that $\forall_{i} : u_{i} > d_{i}$, the authors demonstrate that finding a Nash Bargaining Solution is equivalent to solving:
\begin{equation} \label{eq:nash-solution}
\begin{aligned}
u^{*} = \arg \max_{u \in U} \sum_{i=1}^{T} \log (u_{i} - d_{i}) \\
\textrm{s.t.} \forall_{i} : u_{i} > d_{i}
\end{aligned}
\end{equation}
Subsequently, the authors propose an iterative solution to solve \cref{eq:nash-solution} and find the aggregated update vector is equivalent to solving \cref{alg:algo}

\begin{algorithm}
\DontPrintSemicolon
\caption{Nash-MTL}\label{alg:algo}
\KwIn{$\theta^{0}$, an initial parameter vector; $\eta$, learning rate}
    \For{\texttt{t= 1,...,T}}{ 
     Computer task-specific gradients: $g^{t}$ \;
     Let $G^{t}$ be a matrix with columns $g^{t}$ \;
     Solve for $\alpha$: $(G^{t})^{\mathbf{T}}G^{t}\alpha = 1/\alpha$, to obtain $\alpha^{t}$ \;
     Update parameters: $\theta^{t} = \theta^{t} - \eta G^{t} \alpha^{t} $ \;
    }
\Return{$\theta^{T}$} \;
\end{algorithm}

where $G \in \mathbb{R}^{m \times T}$ is a multi-task gradient matrix with parameter dimension $m$. Moreover, $\alpha \in \mathbb{R}^{T}_{+}$ is a strictly positive matrix which acts as a constraint to the objective which conceptually renders gradient vectors in $G$ orthogonal when they need to be. Additionally, $t$ is a task iterator, $\theta$ represents the shared parameter networks, $\eta$ the learning rate. $G^{t}$ is a task-specific vector update matrix with columns $g^{t}_{i}, i \in T$. 
The results obtained by \cite{bargaining-game} suggest Nash-MTL achieves current state of the art weighting strategy under many MTL configurations.

However, recently, \citet{even-help} instead demonstrated that most MTL optimisation strategies \cite{MTL-as-MOO, gradnorm, gradient-surgery, GradDrop} do not improve MTL training beyond what careful choice of scalar weights in MTL weighted average (\cref{eq:basic-mtl} can achieve. Rather, \citet{even-help} identify MTL optimisation is particularly sensitive to the choice of hyper-parameters.

\subsection{Other Task Balancing Techniques}
\label{sec:other-task-balancing-techs}
\subsubsection{Stopping Criterion Techniques} 
Previous techniques balanced tasks either by finding a combination of the task weights or through gradient manipulation to prevent destructive learning. However, these techniques globally penalise some tasks over others by constraining certain parameters in the objective space. 
Therefore, \citet{task-wise-early-stopping}, as part of their solution leveraging multiple auxiliary tasks to perform facial landmark detection, propose a task-wise early stopping strategy. The intuition is that once a task starts to overfit a dataset, it will harm the main task as it will force the optimisation to be stuck in a non-global optimum. Hence, a task is stopped if its performance, measured as the product between the training error tendency, noted as $L_{tr}$ and the generalisation error \textit{w.r.t} $L_{tr}$, noted as $L_{val}$, has not exceeded a certain threshold $\epsilon$. 
Formally, a training error rate $E_{tr}$ is calculated over a patience epoch length $k$ \textit{w.r.t} a current epoch $t$. Intuitively, the smaller $E_{tr}$, the greater the signal to continue the training for the task as the training loss substantially drops over the period of time $k$ as:
\begin{equation} \label{eq:early-stopping-tr}
E_{tr} = \frac{k \cdot med_{j=t-k}^{t}L_{tr}(j)}{\sum_{j=t-k}^{t}L_{tr}(j) - k \cdot med_{j=t-k}^{t}L_{tr}(j)},
\end{equation}
where $med$ represents the median operation. 
Similarly, $E_{val}$ measures the overfitting w.r.t $L_{tr}$. \citet{task-wise-early-stopping} define $\lambda$ as an additional learnable parameter to measure the importance of the task's loss. This process is shown in \cref{eq:early-stopping-val} below: \\
\begin{equation} \label{eq:early-stopping-val}
E_{val} = \frac{L_{val}(t) - \min_{j = 1..t}L_{tr}(j)}{\lambda \cdot \min_{j = 1..t}L_{tr}(j)}. 
\end{equation}
Overall, if $E_{tr} \cdot E_{val} > \epsilon$, the stopping criterion is met.

In a MTL configuration in which all the tasks are aimed to be optimised equally, stopping a task might result in \textit{catastrophic forgetting}. Therefore, \citet{12-in-1} propose a simple dynamic Stop-and-Go procedure that continually checks for task-wise improvement and degradation during training. 
Precisely, if performance, measured as the task-wise validation loss term for a given epoch $n$, noted as $L_{t}^{n}$, has not met the performance threshold $\epsilon_{stop}$ over the patience parameter $k$ such that $L_{t}^{n \rightarrow k} < \epsilon_{stop}$. Then, task $t$ is set to \textit{STOP} mode.
If during \textit{STOP} mode, $L_{t}^{n}$ is degraded and meets the degradation threshold $\epsilon_{go}$ such that $L_{t}^{n} < \epsilon_{go}$, then task $t$ is set back within the MTL training and is set to \textit{GO} mode.
In \cite{12-in-1}, the authors set $\epsilon_{stop}$ to be 0.1\% and $\epsilon_{go}$ to be a degradation of 0.5\% of the task's best performance.

\subsubsection{Prioritisation Techniques}
An alternative to balancing the learning of multiple tasks simultaneously is to instead focus on easier or complex tasks to benefit the training for all tasks. For example, \citet{self-paced-MTL} choose to guide their MTL training by gradually incorporating both harder tasks and harder instances into the objective function. By considering a number of tasks $T$ and a number of instances per task $n$, the authors propose a regularisation $f$ over $\mathbf{W} \in \mathbb{R}^{n \times T}$ as shown below:
\begin{equation} \label{eq:self-paced}
f(W, \lambda, \gamma) = - \lambda \sum_{i=1}^{T} \lVert W \rVert_{1} + \gamma \sum_{i=1}^{T} \frac{\lVert W \rVert_{2}}{\sqrt{n_{i}}},
\end{equation}
in which the first term imposes the negative $L_{1}$-norm on the instances $n$. This term prioritise easier instances over harder ones when $\lambda$ is low. This is motivated by the fact that easy instances, for which the empirical loss will be small, have bigger gradients. This behaviour is caused by the sparsity norm defined above. On the contrary, difficult  instances have bigger empirical losses and therefore smaller gradients. As a result, as training continues, gradually increasing $\lambda$ will introduce more difficult instances by increasing the difficult task gradients. Similarly, the second term imposes the $L_{2-1}$-norm on the task-specific data instances $n_{i}$. This is motivated by the fact that harder tasks exhibit larger empirical losses and gradually reducing $\gamma$ will introduce harder tasks. This enables the training to smoothly progress whilst avoiding both inter-instance and inter-task possible conflicts.

On the other hand, some works have focused on starting with harder tasks to benefit easier tasks. For instance, \citet{dynamic-task-prio} propose a loss weighting strategy leveraging the \textit{focal loss} \cite{focal-loss} as defined below:
\begin{equation} \label{eq:original-cross-entropy}
FL(\mathbf{p},\gamma) = -(1-\mathbf{p})^{\gamma}\log(\mathbf{p}).
\end{equation}
The focal loss, described in \cref{eq:original-cross-entropy} is primarily intended for classification, \citet{dynamic-task-prio} suggest using key performance metrics (KPIs) per task $t$ (i.e. accuracy, average precision etc...) to generalise the method. Specifically, they adjust these task-specific KPIs $\kappa_{t}$ in an EMA approach as shown below: 
\begin{equation} \label{eq:FL}
\bar{\kappa}_{t}^{(\tau)} = \alpha \kappa_{t}^{(\tau)} + (1 - \alpha) \kappa_{t}^{(\tau - 1)},
\end{equation}
where $\alpha$ is a discount factor and $\tau$ is the iteration. 
Subsequently, the authors swaps original focal loss probability \textbf{p} (described in \cref{eq:original-cross-entropy}) for their KIPs $\bar{\kappa}_{t}^{(\tau)}$. As a result, the authors define a task difficulty as a combination of the task-specific loss and its respective KPI-based focal loss as:
\begin{equation} \label{eq:DTP}
L_{DTP} = \sum_{t=1}^{T} FL(\bar{\kappa_{t}};\gamma_{t}) \widehat{L}_{t}.
\end{equation}

Alternatively, \citet{learning-to-MTL-active-sampling} propose prioritising harder tasks through \textit{active sampling} (\ie choosing what data to train a model with at a particular time $t$ during training). More specifically, the model keeps track of two performance estimations: $t_{i}$ and $c_{i}$ which are a target performance and current performance, respectively, for a task $i$. The task performance is measured as follows: $m_{i} = \frac{t_{i} - c_{i}}{t_{i}}$, where a higher value of $m_{i}$ indicates the model is currently bad at task $i$. Therefore, to encourage the model to prioritise harder tasks, a task-wise sampling strategy is modeled by a distribution $p_{i}$ at every $k$ decision steps which is calculated as follows:
\begin{equation} \label{eq:MTL-active-sampling}
p_{i} = \frac{\exp^{\frac{m_{i}}{\tau}}}{\sum_{c=1}^{k} \exp^{\frac{m_{c}}{\tau}}}.
\end{equation}
Subsequently, the probability distribution is used to sample the next tasks throughout training.

\section{Task Grouping}
\label{sec:task-grouping}

As explained in \cref{chapter:Optimisation}, the overall performance of a MTL model heavily depends on the set of tasks. The optimisation space could be simplified by only processing related tasks together. This chapter focuses on how Task Relationship Learning (TRL) can be achieved through Task Grouping (TG). 

Thus far, most works relied on human judgment concerning the relatedness of the tasks. However, these assumptions can be mitigated by quantitatively measuring task relationships. Early attempts in this area aimed to model task relationships (TR) based on vectors in a shared low-dimensional subspace. For example, \citet{learning-with-whom-to-share} explicitly build upon MTFL \cite{MT-feature-learning} (introduced in \cref{sec:non-neural-MTL}) and frames the task grouping problem as a mixed integer programming problem. GO-MTL \cite{learning-task-grouping-and-overlap} learns a linear combination of task-specific vectors. 
Later, \citet{MRN} expand on previous works modelling TRs using matrix-variate normal distribution over task-specific parameters regularisation techniques to identify positive task correlations \cite{MTRL}. However, to embed this regularisation technique into DL, \citet{MRN} use the tensor normal distribution \cite{tensor-normal-distribution} as a prior over task-specific tensors and learns task relationships by learning task covariance matrices. 
Similarly, \citet{MMoE} learn gating networks in a MoE framework to implicitly model task interactions. However, these works model relationships from a high-level perspective and generally poorly describe pair-wise relatedness.
To tackle TG, a body of work focused on studying relationship based on Transfer Task Learning (TTL) by directly learning a mapping between the learned parameters for a task $a$ to a target task $b$ in a MTL setting. For instance, Taskonomy \cite{taskonomy} introduced a computational approach to perform TG based on finding transfer learning dependencies between tasks. More specifically, \citet{taskonomy}, after training task-specific networks, the encoder parts of the networks are frozen and transfer task functions and dependencies are estimated via a target task decoder. Motivated by the idea that multiple source tasks can help provide a more meaningful dependency estimation for a mutual source task, the authors include high-order transfers where a mapping function receives the five best representation as inputs (from the five best first-order source tasks mappings), as illustrated in \cref{transfer-mapping}.
% Figure environment removed
Additionally, Taskonomy \cite{taskonomy} derives a vision task clustering architecture and shows that 4 major clusters stand out, namely: 3D tasks, 2D tasks, low dimensional geometric tasks and semantic tasks. 
Calculating the affinities in such a way is extremely computationally expensive. To alleviate such demand for computation, as opposed to analysing the performance of TTL, Representation Similarity Analysis (RSA) \cite{RSA-taxonomy} directly investigates the feature maps learned by the task specific networks. The authors choose to leverage RSA to frame the task relationship problem by computing correlation through task-specific inferences on pairs of images. As a result, a dissimilarity matrix is obtained for each task-specific network and a similarity score is obtained through the Spearmans correlation. 
However, these latter works only highlight the relationships from a transfer-learning perspective and do not present performance in a multi-task setting. 
Hence, \citet{which-tasks-should-be-learned-together} propose an alternative to transfer-learning based solutions to highlight task relationships. This alternative is motivated by two findings. First, results obtained by \citet{which-tasks-should-be-learned-together} do not show any correlation in the performances between measured \textit{task affinities} and multi-task learning setting. Second, transfer-learning affinities highlight high-level semantic dependencies as only the bottleneck of the source task encoder is used for the mapping. However, MTL should benefit from clean structural dependencies in all abstraction levels of the features. 
Instead, the authors frame this TG problem as an architecture search. Specifically, given an input image, the model aims to determine the best combinations of encoder backbones and task-specific decoders and perform an exhaustive search over these components. The process is constrained by a search time budget value given a number of tasks $T$. Moreover, \citet{which-tasks-should-be-learned-together} optimise the search space using a branch-and-bound procedure and trains between $\binom{T}{2} + T$ and $2^{T} - 1$ networks given $T$ tasks before performing TG. However, this search performance is computationally expensive and as a result, \citet{efficiently-identifying-task-groupings-for-MTL} directly build upon this framework and obtains task groups in a single run only. To achieve this, the authors introduce \textit{Task Affinity Grouping} (TAG) which is a \textit{look-ahead} algorithm that tracks changes in the MTL loss (in this case, \cref{eq:basic-mtl}) under different task groupings. Therefore, the authors introduce the notion of \textbf{task affinity} between two tasks $a$ and $b$ defined by $\mathcal{\hat{Z}}_{a \rightarrow b}$ as:
\begin{equation} \label{eq:affinity-a-b}
\mathcal{\hat{Z}}^{t}_{a \rightarrow b} = 1 - \frac{L_{b}(X^{t}, \theta^{t+1}_{s|a}, \theta^{t}_{b})}{L_{b}(X^{t}, \theta^{t}_{s}, \theta^{t}_{b})},
\end{equation}
in which $t$ is the step during the estimation procedure and where the loss $L_{b}$ for task $b$ is parameterised by $X, \theta_{s}, \theta_{b}$ which represents the input, the shared parameters and task-specific parameters for task b, respectively. The look-ahead term $\theta^{t+1}_{s|a}$ represents the update of the shared parameters w.r.t. the update on task $a$. Subsequently, a network selection procedure is implemented to maximise the total inter-task affinity score. For instance, for a set of tasks $\{T\}$ the affinity scores onto a task $a$ are averaged over all the tasks.
\begin{equation} \label{eq:overall_affinity}
\mathcal{Z}_{a} = \frac{\sum_{t}^{|T|}\mathcal{\hat{Z}}_{t \rightarrow a}}{|T|} , a \in \{T\}, t \neq a.
\end{equation}
This problem is NP-Hard and can therefore be solved by a branch-and-bound algorithm. 


\section{Partially Supervised Multi-Task Learning}
\label{chapter:partial-supervision}
Methods reviewed so far have mainly focused on a fully-supervised setup which assumes that data is sufficient and all task labels are available. However, this setting is not always realistic as both acquiring data and task labels is an expensive process in certain cases. 
In practice, the diversity of the task set is limited as required data and labels generally do not co-exist within the same datasets and therefore, not in the same quantities and/or domains. Thus, there is a need to explore MTL in settings that utilise all available source of information. 
Thankfully, MTL systems can mitigate their data dependency by using available supervisory information of one task to enhance the training of the unlabelled tasks by leveraging task relationships. 
Therefore, in this chapter, \cref{sec:representation-learning} reviews how leveraging multiple auxiliary tasks in a self-supervised manner can help obtain a general representation tailored to downstream tasks. 
Then, \cref{sec:semi-supervised} studies MTL solutions in a semi-supervised settings in which all tasks are optimised.
Finally, \cref{sec:few-shot-learning} introduces how MTL can be framed in a low-data availability learning paradigm: Few-Shot Learning.
Throughout this chapter, we refer to \textit{Partial Supervision} as an umbrella term encompassing self-supervised learning, semi-supervised learning and few-shot learning.


\subsection{Self-Supervised Representation Learning}
\label{sec:representation-learning}
As seen in this review, finding a task-agnostic representation suitable for all the tasks is crucial. However, most previous work in MTL assumed high availability of data and focused on obtaining such representations without diminishing the demand for labels. To remedy this issue, an alternative way to obtain a shared representation is to exploit tasks in a self-supervised fashion. Self-supervised tasks are tasks for which labels can be created without manual annotations. Such tasks hold a strong advantage in the context of MTL as downstream tasks benefit from the representation induced by multiple tasks \cite{cross-stich}. As a result, Self-supervised Multi Task Learning (Self-MTL) can be leveraged as a pre-training strategy. 

% Multi-Task Representation
For instance, \citet{MT-self-supervised-VL} suggest leveraging 4 self-supervised vision tasks as a pre-training procedure. \textit{Relative Position} \cite{relative-position} is a task which consists of finding the relative positions of a pair of patches sampled from the same unlabeled image. \citet{relative-position} claim to perform well at this task enhances object recognition. \textit{Colorization} \cite{colorization} which requires predicting the original RGB pixel color values given a greyscale image. This task acts as a cross-channel encoder and helps pixel-level dense prediction tasks. The `\textit{Exemplar}' task \cite{examplar-task} where pseudo-classes are estimated for each sample and the network is trained to discriminate between these. This task aims to improving classification properties in the learned representation. Last, \textit{Motion Segmentation} \cite{motion-segmentation} is a task that learns, given an image $I_{t}$ at a time $t$ to recognise pixels that will move in $I_{t+1}$. This task helps refine the features necessary to both object detection and segmentation prediction through movement cues. 

\citet{MT-self-supervised-VL} identify two possible sources of conflict in a Self-MTL setting. First, there are conflicts in the task respective inputs, as for instance, the colorization tasks receive greyscale images whilst others receive RGB images. This results in an network architectural problem. To resolve this conflict, the authors suggest performing \textit{input harmonisation} by duplicating the greyscale image over the RGB channels. Second, there is conflict in whether the features being trained should generalise to the class at hand or to the specific input image. To resolve this, the authors incorporate their CNN into a lasso regularisation block where each task-specific decoder receives a layer-wise linear combination of the shared backbone convolutional blocks. Hence, a matrix $A \in \mathbb{R}^{T \times D}$ is trained to be sparse where $T$ is the number of task-specific decoders and $D$ is the number of convolutional blocks being shared. This regularisation allows the network to factorise the features to enhance the generalisation of the network. The authors present results matching fully-supervised single-task performance on diverse CV tasks such as classification, detection and depth prediction. The authors' solution is illustrated in \cref{self-supervised-visual-learning}. 

% Figure environment removed

MuST \cite{MuST} uses specialised teacher models to pseudo-label unlabeled multi-task datasets and suggests a pre-training strategy based on the following tasks: classification, detection, segmentation and depth estimation. Subsequently, a multi-task student model is trained on the pseudo-labeled dataset. Fine-tuning on downstream tasks shows that the self-supervised pre-training outperforms traditional ImageNet pre-training baseline \cite{ImageNet} and additionally, the authors identify that a large number of tasks and datasets benefit the representation for downstream tasks.

% Targeted-Task Examples
% Figure environment removed

This capacity to leverage MTL to enhance the shared representation of tasks has motivated applications in diverse areas. For instance, \citet{self-supervised-monocular-road-segmentation} pre-train a CNN encoder on stereo-paired images from the well-known road object detection dataset KITTI \cite{KITTI} to perform monocular road segmentation. To achieve this, the authors choose to learn two tasks; \textit{Drivable Space Estimation} and \textit{Surface Normal Estimation}. Given a stereo-pair of images $(I_{left}, I_{right})$, the authors obtain a pseudo disparity map $I_{disparity}$ by using semi-global matching (SGM) \cite{SGM}. Subsequently, the authors run the Stixel World algorithm \cite{Stixel-world} which, given a RGB image $I_{RGB}$ ($I_{left}$ or $I_{right}$), exploits the corresponding disparity map $I_{disparity | I_{RGB}}$ to return a semantically segmented representation. Maximum a-posteriori (MAP) estimation is then performed based on the resulting distribution of the predicted pixel labels to extract the drivable area. Subsequently, surface normals are obtained by following the method introduced by \cite{surface-normal-unsupervised}. Specifically, given camera-related information such as the baseline distance $D$ and the focal length $D_{focal}$, the previously calculated diversity map $I_{disparity}$ is converted into a depth map $I_{depth}$. This depth map is later projected onto 3D world space $W$ given $D$ and $I_{normals}$ and is obtained via calculating the least-squares plane within $W$ and allocating the planes to neighbouring set of pixels. The authors fine-tune the learned features to perform monocular road segmentation and show impressive results whilst heavily reducing the demand for data.

\citet{recycling-bounding-box-annotations} utilise Self-MTL as a way to refine preliminary Object Detection (OD) predictions. In particular, assuming bounding box labels ${A_{OD}}$ are only available for object detection, 3 auxiliary tasks recycle ${A_{OD}}$ to produce their own respective labels ${A^{t}}$. Such a strategy has two main goals: (1) to learn robust discriminatory features for OD, (2) to refine the preliminary OD prediction. These auxiliary tasks are carefully chosen as follows: First, \textit{Multi-Object Labelling} randomly produces bounding boxes over the input image, constrained by the fact that one must overlap with at least one Ground Truth (GT) bounding box. Then, labels are assigned to the sampled bounding boxes based on GT Bounding Box area it overlaps the most with. The intuition behind this task is to perform augmentation on the input image to enhance globalisation. Second, \textit{Closeness Labeling} accounts for the inherent proximity in object classes in an image. This task consists in iterating over the GT bounding box annotations to provide a one-hot encoding based on the proximity of neighbouring GT bounding boxes. Finally, \textit{Foreground labeling} encodes the foreground and background, assigning 1's to pixels within GT bounding boxes and 0's otherwise. These tasks are illustrated in \cref{recycling-auxiliary-tasks}. Information encoded by these tasks is concatenated into a representation $f$ and is used to update the original prediction $x$ via a 1-layer FC layer to obtain a final refined prediction $z$ such that: $z = f \oplus x$. 

These methods demonstrate how effectively leveraging multiple self-supervised objectives can improve a shared representation suitable for MTL. Such efficiency has motivated some works to employ Self-MTL for diverse target downstream tasks in CV. For example, \citet{self-supervised-image-aesthetic-assessment} suggest a meaningful self-supervised pre-training strategy for Image Aesthetic Assessment (IAA). IAA models, which are usually trained an aesthetic-labeled ImageNet dataset \cite{ImageNet}, do not provide much information for why an image is not aesthetically good, for example, intrinsic image characteristics (\ie brightness, blurriness, contrast etc). Therefore, the authors train a comparative network of 2 distorted images, the distortion is chosen as one of the aforementioned characteristics and the networks aim at estimating the type of distortion as well as its intensity in an unsupervised manner. The goal of the MTL system is to recognise the less distorted image. Moreover, additional tasks are added to recognise the type and intensity of the distortion operation applied to the two input images. The authors report a decrease in 47\% in the number of epochs necessary for convergence compared to a IAA network pre-trained on Imagenet \cite{ImageNet}, notwithstanding the reduced need for data.

Alternatively, self-MTL framework has shown state-of-the-art  results in real-time applications. For example, SSMTL \cite{anomaly-detection} tackles anomaly detection in videos. Acquiring anomalous labels is difficult and as a result, the authors leverage self-supervised tasks to train a 3D CNN to recognise anomaly in videos. SSTML \cite{anomaly-detection} first runs a pre-trained YOLOv3 \cite{YOLOv3} to identify objects on a set of object-level frames ${I_{n}}$. Then, the authors choose three tasks to identify anomalous objects. First, irregularity is identified through the \textit{arrow of time} task, which involves obtaining an abnormal label by training the 3D CNN on the video in reverse mode. Second, \textit{motion irregularity detection} for which abnormal events are obtained via skipping frames is used to identify irregular motions such as someone running, falling etc. Third, a \textit{middle box prediction} task is implemented to predict the middle frame. Last, the authors enhance their multi-task 3D CNN through \textit{knowledge distillation} where the object detector YOLOv3  \cite{YOLOv3} is trained to predict the last layer of a ResNet-50 \cite{resnet}, which predicts whether the middle box frame is abnormal or not. The key point is that, in the knowledge distillation head, the authors expect a high difference between the object-level predictions of the 3D CNN and the ResNet-50 predictions when an anomaly is observed. The results significantly outperform previous state-of-the-art methods. Moreover, SSMMTL++ \cite{SSMTL++} recently reviews this framework and further improves it through the introduction of different tasks such as optical flow and advanced architectures such as the ViT \cite{ViT}. 

% Multi-Modal Representation
In addition to using multiple auxiliary tasks to enhance the learned representation, multiple modalities can be utilised to provide even more useful sources of information for models to learn. Multi-modal representation learning can be achieved by pre-training on diverse datasets. For instance, \citet{12-in-1} obtain a vision-language representation by pre-training on 12 vision-linguistic datasets and shows impressive results on common multi-modal tasks such as visual question answering and caption-based image retrieval. The authors utilise multi-modal self-supervision, inspired by \cite{Vilbert}, by masking proportional amounts of both image and word tokens and also by performing \textit{multi-modal alignment}, by predicting if two instances belong together. Similarly, \citet{sound-and-visual-rep-learning} introduce Multi-Self Supervised Learning tasks (Multi-SSL), a multi-modal (sound and image) pre-training strategy aiming to provide a shared representation for both sound and image modalities that could be used for downstream tasks. 

\citet{MultiMAE} leverage the recent the success of Masked Auto-Encoders (MAEs) \cite{MAE}. MAEs \cite{MAE} are asymmetric encoder-decoder models in which the encoder only operates on a small portion (about 15 \%) of a patch-wise masked input image and  the decoder aims at regenerating the missing patches. In particular, \citet{MultiMAE} propose Multi-Task MAE (MultiMAE), a pre-training strategy reconstructing diverse image modalities. To achieve this, given a set of RGB images, image modalities are acquired solely via \textit{Pseudo-labeling}. First, the depth modality is approximated by running a pre-trained DPT-Hybrid \cite{ViT-for-dense-prediction}, a ViT-based model. Similarly, Semantic Segmentation pseudo-labels are obtained via Mask2Former \cite{Mask2Former} trained on the COCO dataset \cite{COCO}. Once these labels are obtained, similar to original MAE \cite{MAE}, the authors sample a large portion of the image modalities divided into 16x16 patches. Subsequently, a number of tokens corresponding to approximately $\frac{1}{8}$ of the entire number of tokens for the 3 modalities (RGB, depth and semantic) are kept visible. The sampling strategy follows a symmetric Dirichlet distribution, equivalent to a uniform distribution so that no modality is prioritised. Then, the authors perform a 2D-sine-cosine linear embedding on the patches which are fed as input to the multimodal ViT encoder which operates only on the visible tokens, tremendously reducing the cost of computation \cite{MAE}. For downstream tasks, the multi-modal self-trained encoder can be used to fine tune a single task whilst benefiting from geometrical cues induced by other modalities. This framework is illustrated in \cref{MultiMAE-figure}.

% Figure environment removed

%\papertomention{Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic Imagery, 2018} \\
%\papertomention{Generative Modeling for Multi-task Visual Learning, 2022}
% No CV Applications
In addition, Muli-Task Self-Supervised pre-training has been investigated in medical applications \cite{retinal-vessel-segmentation, MUSCLE, skin-lesion}, in music classification \cite{music-classification} or in NLP for multilingual reverse dictionaries \cite{multilingual-reserved-dictionary}.


\subsection{Semi-Supervised Learning Methods}
\label{sec:semi-supervised}
\subsubsection{Traditional Methods}

\citet{Semi-supervised-MTL} propose the first semi-supervised MTL framework. The framework consists of $T$ classifiers whose parameters share a joint probability distribution based on a soft variant of a Dirichlet Process. This allows for the parameters to be trained together and for the predictions to be obtained all at once. 
The probability distribution variant retains the inherent clustering property of Dirichlet Processes and as a result, the authors process unlabeled data via Parameterized Neighborhood-based Classification (PNBC). More specifically, the authors perform a Markov random walk over neighbouring data points obtained via supervised training, then, classifiers learn to assign unlabeled data to its closest point. 
Later, \citet{task-regularizations} expand on this setting by framing MTL as a clustering problem. To achieve this, after training $T$ linear classifiers, the authors improve their generalisation w.r.t. to their respective data by imposing a norm over the classification weights. Subsequently, the algorithm follows the same procedure, frames the respective classifiers into clusters via K-means clustering and assigns unlabeled points to nearby classifiers within that space. The authors also show this framework can be extended to non-linear classification through the use of kernels. 
It is worth noting that these traditional methods had a different notion of the MTL problem. In fact, the tasks are classification tasks in which `tasks' are either different datasets \cite{task-regularizations} or classes, resulting in multi-class classification \cite{Semi-supervised-MTL}. 
As a result, only one loss function is used for the optimisation which significantly differs from the contemporary definition of MTL.

\subsubsection{Self-Supervised-Semi-Supervised Methods}
% A good example for a CV target task 
The methods introduced in \cref{sec:representation-learning} highlight how multi-task learning can be used with self-supervised auxiliary tasks to minimise the overall training cost and demand for data. This characteristic has motivated numerous works to leverage both semi-supervised learning and self-supervised learning.

% Leveraging geometrical tasks for semantic
As explained in \cref{sec:task-grouping}, some tasks provide global understanding of scene geometry (\ie \textit{surface normals, depth prediction ...}) and when trained adequately, translate into low-level features tailored for dense prediction tasks. 
Therefore, there has been effort to investigate these tasks to improve an important CV task: \textit{Semantic Segmentation} (SS). For instance, \cite{urban-scene-understanding, dynamic-object-problem} use depth prediction as a proxy task for supervised urban scene understanding tasks such as car detection, road and semantic segmentation. Similarly, \citet{boostinc-semantic} use both depth estimation and colorization as a pre-training strategy for semantic segmentation in autonomous driving. 
To expand upon the idea that self-supervised depth estimation (SDE) can be effective to reduce data dependency, \citet{three-ways} introduce three ways to leverage SDE to improve semantic segmentation in a semi-supervised learning paradigm. 
%Therefore, the following description first introduces the different strategies introduced by \cite{three-ways} before presenting their semi-supervised network combining these strategies.

First, the authors suggest an active learning strategy based on depth prediction. Specifically, given a set of images of the same domain ${G}$, the authors aim to split it into two image subsets. On the one hand, ${G_{A}} \subset {G}$ will be used for pseudo-labeled annotations for SDE, whilst ${G_{U}} \subset {G}$ is the set of unlabelled images. To obtain these, the authors iteratively choose ${G_{A}}$ through diversity sampling. Precisely, diversity is obtained when the chosen images are most representative of the dataset distribution. In urban scene understanding, this could result in the most frequent types of buildings, cars, bicycles, etc being chosen. To achieve diversity, the authors first populate ${G_{A}}$ with a random image $I$ from an image set $\{I\}$ and iteratively select the farthest $L_2$ distance between two sets of features of both ${G_{A}}$ and ${G_{U}}$ as given a pre-trained network $f_{SDE}$:
\begin{equation} \label{eq:active-learning}
G_{A_{n+1}} = \underset{I_{i} \in G_{U} }{\arg \max} \underset{I_{j} \in G_{A}}{\min} \lVert f_{SDE}(\theta, I_{i}) - f_{SDE}(\theta, I_{j})\rVert_{2},
\end{equation}
where the $f_{SDE}$ outputs the post-inference features based on the same set of input features $\theta$ and the respective annotated and unlabeled image sets $G_{A}$ and $G_{U}$.

%The features correspond to the same layer $l$ of a pre-trained SDE network $f_{SDE}(\theta, I)$. Formally, the active learning procedure can be formulated as in \cref{eq:active-learning}:


Subsequently, the authors aim to incorporate another important aspect to this active sampling: \textit{Uncertainty Sampling} which consists in choosing samples that are hard to learn for the current state of the model: formally, instances in ${G_{U}}$ for which the model's decision is close to the decision boundary. To achieve this, a student model $f'_{SDE}(\theta, I)$ is trained on ${G_{A}}$. The authors then measure the disparity, on ${G_{U}}$, of both the predictions of $f_{SDE}$ and those of $f'_{SDE}$. Formally, the difference is calculated using the $L_1$ distance as:

\begin{equation} \label{eq:depthmmix}
E(i) = \lVert log(1+f_{SDE}(\theta, I)) - log(1+f'_{SDE}(\theta, I)) \rVert_{1}.
\end{equation}

The authors choose to use the $log$ regulator to avoid close-range objects dominating the disparity difference. 
Conceptually, sampling based on these two characteristics benefits from diversified, complex and representative instances which results in a decreased demand for data samples. 

Second, inspired by the success of pair-wise data augmentation in CV \cite{CutMix, ClassMix}, \citet{three-ways} introduce \textit{DepthMix} as a way to further reduce this labeling demand. In this method, considering 2 images $I_{source}$ and $I_{target}$, the goal is to learn a binary mask $M$ over $I_{source}$. Specifically, the positive values in $M$ represent regions to be copied over $I_{target}$. As a result, the augmented image $I_{augmented}$ is obtained as:
\begin{equation} \label{eq:goal-depthmix}
I_{augmented} = M \odot I_{source} + (1 - M) \odot I_{target},
\end{equation}
where $\odot$ is the element-wise product. In contrast to existing data augmentation methods, \citet{three-ways} leverage depth to avoid violating geometric semantic relationships between objects. For example, it is undesirable to have a distant object in $I_{source}$ to be copied onto the forefront of $I_{target}$, or worse, to result in geometrically implausible situations like a close-range motorbike copied on the top of a close-range car. To mitigate this problem, the authors use depth predictions for both images noted as $D_{source}$ and $D_{target}$. To achieve this, given a shared location $(x,y)$, $M$ is constrained to select only pixels for whose depth values are smaller on $I_{source}$ than on $I_{target}$. This process is demonstrated as follows:
\begin{equation} \label{eq:depthmix}
M(a,b) =\left\{
    \begin{array}{ll}
        1 & \mbox{if } D_{source}(a,b) <  D_{target}(a,b) + \epsilon\\
        0 & \mbox{otherwise}
    \end{array}
\right.
\end{equation}
where $\epsilon$ is a small noise value to avoid conflicts of objects that are the same depth plane on both images such as curb, road and sky.

The final component introduced in \cite{three-ways} is a semi-supervised MTL network to perform both Depth Estimation and Semantic Segmentation.
% Figure environment removed
The authors train their MTL network in 2 stages. The first stage is  depth pre-training. This stage consists in a self-supervised training for both depth estimation and pose estimation on an unlabeled sequence of images. As part of this procedure, a shared encoder $f_{\theta}^{E}$ is initialised with ImageNet \cite{ImageNet}. Additionally, in order not to forget the semantic features during training, the initialised features, noted as $f_{I}^{E}$, serve as a regulator for the SDE pre-training and the authors use the $L_2$-norm in order to guide the multi-task representation. The resulting loss term is formulated as:
\begin{equation} \label{eq:three-ways-regulation}
L_{SDE} = \lVert f_{\theta}^{E} - f_{I}^{E} \rVert_{2}.
\end{equation}
In the second stage, the authors introduce semantic segmentation to form a semi-supervised network. In this stage, the network is trained on depth estimation on both labeled and pseudo-labeled (using the mean teacher algorithm \cite{mean-teacher-algorithm}) instances. Their solution is illustrated in \cref{three-ways-MTL}.
As a result, the authors manage to achieve 92\% accuracy on a baseline fully-supervised model whilst using 1/30 of labeled image segmentation instances. Furthermore, whilst using 1/8 of the SS labels, it outperforms this supervised baseline by a small margin. The authors then improve their solution to perform domain adaptation \cite{semi-supervised-domain-adaptive}. 

Recently, \citet{MTL-for-image-segmentation-task} leverage both depth and surface normals estimation to improve on semantic segmentation. In addition, the authors show how Nash-MTL \cite{bargaining-game} can lead to efficient solutions.

\subsubsection{Generative Modeling}
Recent advances of general self-supervised methods such as adversarial training with Generative Adversarial Networks (GANs) \cite{GANs}, as well as the ability of generative modeling to learn useful visual representations from unlabeled images \cite{adversarial-feature-learning}, have motivated the investigation of generative modeling in MTL to lower the demand for labeled data \cite{adver-learning-semantic-semi-supervised, robust-adver-learning-for-semantic-semi}. 

For example, \citet{partly-supervised-MTL} propose a self-supervised semi-supervised MTL ($S^{4}MTL$) solution leveraging adversarial learning and semi-supervision to teach simultaneously two commonly tackled CV tasks, namely: Image Classification (for diagnostic classification) and Semantic Segmentation. By considering two datasets, one labeled $D_{A}$ and one unlabeled $D_{U}$, the authors define their respective losses as $L_{A}$ and $L_{U}$. If $\theta$ and $\upsilon$ define the parameters of network $f$ for semantic segmentation and diagnostic classification respectively, then the overall objective can be summarised as:
\begin{equation} \label{eq:partly-supervised-MTL}
\min_{\upsilon, \theta} L_{A}(D_{A},f(\upsilon, \theta)) + \alpha L_{U}(D_{U},f(\upsilon, \theta)),
\end{equation}
where $\alpha$ is a positive weight for the unsupervised loss. 
Subsequently, the authors train two networks: $G$, a mask generator for semantic segmentation and $D$ a classifier which is trained in an adversarial fashion. These two networks are divided into two branches. 
For supervised images, $G$ wants $D$ to maximise the likelihood of the segmentation masks given a regular image-label pair.
For the  unsupervised images, the model performs a transformation $t(x)$ over the input image $x$ such as rotation to enable $G$ to make predictions. Such a framework is illustrated in \cref{S4MTL}. 
% Figure environment removed
Using this framework, the model is claimed to outperform fully-supervised single task models whilst diminishing the availability of data/label up to 50\%.


\citet{semi-supervised-MTL-for-semantics-depth} extend on this framework and introduces SemiMTL. This method performs urban scene segmentation and depth estimation. However, the authors leverage multiple datasets in a heterogeneous (trained on different datasets) MTL framework and train their discriminator $D$ in a domain-aware fashion to compensate for the domain shift inherent to this environment. To do this, the authors add a inter-domain loss between the labeled dataset $A$ and unlabeled dataset $B$ for which the ground-truth value for an arbitrary task $t$ is noted as $y^{A}_{t}$ and $y^{B}_{t}$. Moreover, their respective predictions are noted $\hat{y}^{A}_{t}$ and $\hat{y}^{B}_{t}$. The authors choose to leverage the cross-entropy loss and as a result, this inter-domain loss can be expressed, over the data instances $i$, as:
\begin{equation} \label{eq:inter-domain-loss}
L_{inter}^{t} = - \sum_{i}^{T} \log (D_{t}(\hat{y}^{B}_{t})^{(i,y^{A}_{t})}),
\end{equation}
where $y^{A}_{t}$ is a 3-dimensional one-hot vector, in which
a three-way classifier is utilized in the discriminator to tell that the input is from the ground-truth from dataset $A$.
Conceptually, the loss in \cref{eq:inter-domain-loss} aligns the unlabelled task prediction $\hat{y}^{B}_{t}$ onto the labelled task ground-truth $y^{A}_{t}$ to compensate for domain shifts. 
Additionally, the authors introduce different ground-truth and prediction alignment strategies such as aligning the unlabelled prediction $\hat{y}^{B}_{t}$ onto the labelled task prediction $\hat{y}^{A}_{t}$ or aligning $\hat{y}^{B}_{t}$ onto the intersection of the labelled ground-truth $y^{A}_{t}$ and prediction $\hat{y}^{A}_{t}$.


\subsubsection{Discriminative Methods}
Discriminative methods aim at determining boundaries between image representations by directly comparing them.
This section focuses on MTL works introducing this technique under semi-supervised training paradigms. 
One type of discriminative method that has shown great success in many CV tasks is \textit{Contrastive Learning} (CL). CL was  originally introduced by \cite{original-contrastive}. It involves learning a joint-space in which similar pairs of images are close to each other and in which different pairs are far part. Momentum Contrast (MoCo) \cite{Moco} extends this concept for unsupervised visual learning and sees this framework as a dictionary look-up problem where an image $I$ is encoded by a network $f$, this is denoted as the query $q = f(I)$. Then, a queue of size $n$ of image representations ${I_{k}}$, or keys, chosen as the preceding mini-batch, which are encoded by a momentum encoder $f_{m}$ are compared $k_{n} = f_{m}({I_{k}})$. Subsequently, the matching key $k_{+}$ is noise-augmented. Finally, $f$ is updated via the InfoNCE \cite{InfoNCE} as follows:
\begin{equation} \label{eq:infoNCE}
L_{InfoNCE} = - \log \frac{\exp(q \cdot k_{+} / \tau)}{\sum_{i=1}^{N} \exp(q \cdot k_{i} / \tau)}.
\end{equation}
SimCLR \cite{SimCLR} suggests a simpler version comparing diverse augmented versions of the same image, however it requires larger batch sizes.

\begin{table*}
\centering
\caption{Single-Task vs Multi-Task Fully-Supervised Methods Comparison on NYUv2}
\label{tab.1}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllcc c c c}
    \toprule
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Method} &
    \multirow{2}{*}{MTL} &
      \multicolumn{1}{c}{Semseg} &&
      \multicolumn{1}{c}{Depth} &&
      \multicolumn{1}{c}{Normal}  
      \\
      \cline{4-4} \cline{6-6} \cline{8-8} \\
      && &  {mIoU $\uparrow$} && {RMSE $\downarrow$} && {mErr $\downarrow$} \\ 
      \midrule
    \multirow{20}{*}{NYUv2 \cite{NYUv2}}
    & \citet{8578788} & \ding{55} & 48.10 && - && - \\
    & \citet{yu2020multilayer} & \ding{55} & 50.70 && - && - \\
    & InverseForm \cite{borse2021inverseform} & \ding{55} & 53.10 && - && - \\
    & TADP \cite{unidepth} & \ding{55} & - && 0.225 && - \\
    & DepthAnything \cite{depth-anything} & \ding{55} & - && 0.206 && - \\
    & UniDepth \cite{unidepth} & \ding{55} & - && 0.201 && - \\
    & \citet{hickson2019floors} & \ding{55} & - && - && 19.7 \\
    & \citet{bae2021estimating} & \ding{55} & - && - && 14.9 \\
    & iDisc\cite{piccinelli2023idisc} & \ding{55} & - && - && 14.6 \\
    \\
    \cline{2-8}
    \\
    & Cross-Stitch \cite{cross-stich} & \ding{51} & 36.34 && 0.6290 && 20.88\\
    & PAP \cite{PAP} & \ding{51} & 36.72 && 0.6178 && 20.82\\
    & PSD \cite{PSD} & \ding{51} & 36.69 && 0.6246 && 20.87\\
    & PAD-Net \cite{PAD-net} & \ding{51} & 36.61 && 0.6270 && 20.85\\
    & MTI-Net \cite{MTI-NET} & \ding{51} & 45.97 && 0.5365 && 20.27\\
    & InvPT \cite{invPT} & \ding{51} & 53.56 && 0.5183 && 19.04\\
    & TaskPrompter \cite{taskprompter} & \ding{51} & 55.30 && 0.5152 && 18.47\\
    & DeMT \cite{DeMT} & \ding{51} & 51.50 && 0.5474 && 20.02\\
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}


Motivated by the aforementioned approaches, MTSS \cite{MTSS} suggests a simple, yet effective, semi-supervised MTL framework to optimise a discriminative self-supervised auxiliary task and a supervised main task simultaneously.
% Figure environment removed
Specifically, the authors choose to maximise the similarity between two different views of the same image. First, two augmentations on the same image are performed, these views are $x_{1}$ and $x_{2}$. Then, a shared CNN classifier process them leading to two representations $z_{1}$ and $z_{2}$. One, for example $z_{1}$, is chosen to be processed by the supervised main task. Similarly to SimCLR \cite{SimCLR}, the authors choose to attach a Multi-Layer Perceptron (MLP) in order to map representations to a similar space, let us denote the resulting representations as $p_{1}$ or $p_{2}$. Finally, the cosine similarity $D$ between $p_{1}$ and $z_{2}$ is calculated as shown below: \begin{equation} \label{eq:similarity-MTSS}
D(p_{1}, z_{2}) = - \frac{p_{1}}{\lVert p_{1} \rVert_{2}} \cdot \frac{z_{2}}{\lVert z_{2} \rVert_{2}},
\end{equation}
to minimise the cosine similarity between the representations of augmented views. The symmetric auxiliary loss, introduced by BYOL \cite{BYOL} and depicted in \cref{eq:BYOL}, is used as follows:
\begin{equation} \label{eq:BYOL}
L_{aux} = \frac{1}{2} D(p_{1}, z_{2}) + \frac{1}{2} D(p_{2}, z_{1}).
\end{equation}
This auxiliary loss is then added to the overall MTL objective. The semi-supervised  framework is depicted in \cref{MTSS-figure}.

% Figure environment removed

Another recent task discriminatory approach, Cross-Task Consistency (XTC), is introduced by \cite{X-task-consistency}. Conceptually, this notion comes from the dependency between two tasks. For instance, in the context of urban scene semantic segmentation with depth estimation, there would be inconsistency if depth estimation evaluated a flat surface where a car is detected. Therefore, \citet{X-task-consistency} aim to compute task pair-wise mapping to map the prediction from a source task to the label of the target task. 
However, each of those mapping functions are parameterised by two Deep Neural Networks (DNNs) and leverage labels from each task. To mitigate the use of labeled data, \citet{MTPSL} leverage cross-task relations in a semi-supervised framework. Specifically, \cite{MTPSL} suggests a framework to map the prediction of an unlabeled task $\hat{y}^{s}$ to the ground truth of another task $y^{t}$ through an adaptive encoder which embeds only shared parameters. Therefore, the two representations $\hat{y}^{s}$ and $y^{t}$ are mapped on to a joint space and their  cosine distance is minimised.

\citet{MTPSL} leverage XTC in their framework for semantic segmentation, depth estimation and surface normals estimation. Let us consider a partially-supervised image $I$, for which only $y^{depth}$ or $y^{semantic}$ is available. $I$ is then processed through a shared backbone network $f_{\Theta}$ to which task-specific decoders $h_{\vartheta}^{depth}$ and $h_{\vartheta'}^{semantic}$ are attached. 
The obtained predictions are noted as $\hat{y}^{depth}$ and $\hat{y}^{semantic}$. For the sake of illustration, let us consider 
$\hat{y}^{semantic}$ not to be labelled and therefore to leverage the available ground-truth from the depth estimation task. Now describing the XTC mechanism, let us consider a matrix $A$ for which entries correspond to $source \rightarrow target$, (in our example, $A[semantic,depth] = 1$) and all other entries are 0. An auxiliary network $k_{\theta}$ is used to conditionally parameterise a mapping network $m_{\psi}$. Similar to \cite{film}, $k_{\theta}$ is used to update the layers of $m_{\psi}$. This mechanism is to allow for a conditional source-to-target mapping. The two resulting representations are then projected on to the same joint-space $J$. The authors use the cosine similarity to minimise their distance. 
Additionally, to avoid trivial mappings, the features from $f_{\Theta}$ are used as a regularisation term of the distance between the mapping function's output and the encoded features $f_{\Theta}(I)$. The explained mapping is illustrated in \cref{MTPSL-fig}.


\subsection{Few-Shot Learning Methods}
\label{sec:few-shot-learning}
% Description of Few-Shot Learning
Few-Shot Learning (FSL) is a learning paradigm that aims to learn unseen classes from a few examples. This training paradigm is motivated by the fact that humans do not need hundreds or thousands of exemplar images to learn to recognise an object. Typically, FSL systems consist of two stages. First, a general feature extractor is learned from a large annotated dataset in a stage called \textit{meta-training}. Second, an adaption strategy is used to classify the new sample/class (also known as the query sample) based on a small labeled support set. This stage is called \textit{meta-testing}. A similarity function is then used on the support set to identify the matching class given the query sample. 
% How Few-Shot Learning relate to MTL, defining the problem
Traditionally, in FSL-MTL, the goal is to adapt to unseen classes for a specific task within a MTL model. In the context of MTL, cross-task interactions within a multi-task system could help enhance the generalisation to the few-shot target task. In fact, \citet{natural-language-decathlon} show that MTL models generally focus on tasks that have the least training samples, which is due to the feature sharing process across tasks. 

Recently, the FSL literature has heavily focused on the initial meta-training stage in which multiple datasets serve to train a model to obtain global representations for a target few-shot task, most commonly being \textit{image classification}. For example, \citet{improving-fsl-with-self-pretext-tasks} suggest training such a model in a MTL fashion by leveraging self-supervised tasks (similar to solutions introduced in \cref{sec:representation-learning}), on both labelled and unlabelled images. The shared encoder is regularised by the contrastive learning method: BYOL \cite{BYOL}. Subsequently, the MTL system is evaluated on traditional few-shot image classification. 

MTFormer \cite{MTFormer} suggests different dense prediction tasks as few-shot tasks and evaluates a MTL system leveraging a cross-task attention mechanism at the decoder level of a ViT \cite{ViT} on the PASCAL dataset \cite{PASCAL}. The authors evaluate three tasks, in turn, as a few-shot sampled task by randomly sampling about 1 \% of the annotated data for the few-shot task and keeping all available labels for other tasks. MTFormer \cite{MTFormer} chooses to evaluate Semantic Segmentation, Human Part Segmentation and Saliency Detection which consists of identifying interesting points in an image (points that the human eye would focus on straight away). The results, presented in \cref{tab.5}, display an impressive improvement over the single-task FSL baseline. This improvement is explained by two techniques: the feature propagation across tasks to enhance the few-shot task representation, and the use of CL in \cite{MTFormer}, in which different task representations of the same image are considered as positive samples, which further reinforces the shared representation's quality. 

\textit{Visual Token Matching} (VTM) \cite{visual-token-matching} proposes a continual few-shot learning framework for dense prediction vision tasks. In this setting, a universal few-shot learner can learn new dense prediction tasks given extremely limited labelled task images, most often only using 10 labelled examples of image-label pairs. VTM employs a encoder-decoder architecture using ViT encoders \cite{ViT} to encode both image and label. As a way to propagate features across the model hierarchies, the authors perform token matching using an attention mechanism similar to MTFormer \cite{MTFormer}. More specifically, given a target few-shot task $t$, a query image $Q_{t}$ and support set of image-label pairs of length $N$ ($(X,Y)^{1...N}_{t}$), a task-specific shared encoder $f_{t}$ is used to process both $Q_{t}$ and $X^{i}_{t}$. On the other hand, a label encoder $g$ is used to encode $Y^{i}_{t}$. Subsequently, the token matching mechanism based on attention operates on ViT blocks representations. The block-wise query label predictions are then concatenated before a classification head provides the final prediction. Finally, the results reported by \cite{visual-token-matching} suggest similar strategies should be elevated to the simultaneous MTL settings.

\begin{comment}
\begin{table*} % Test table
\caption{Self-supervised MTL}
\label{tab.1}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllllc c c c c c c c c c}
    \toprule
    \multirow{2}{*}{Method} &
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Tasks} &
    \multirow{2}{*}{Labels} &
    \multirow{2}{*}{Backbone} &
      \multicolumn{1}{c}{SS} &&
      %\multicolumn{1}{c}{Depth} &&
      \multicolumn{1}{c}{OD} &&
      \multicolumn{2}{c}{C} \\
      \cline{6-6} \cline{8-8} \cline{10-11}
      \\
      &&&&& {mIoU $\uparrow$} && {mAp $\uparrow$} && {Top-1 Acc $\uparrow$} &  {Top-5 Acc $\uparrow$}\\ 
      \midrule

    \multirow{ 2}{*}{\cite{MT-self-supervised-VL}} & ImageNet \cite{ImageNet} & C & - & Resnet-101 \cite{resnet} & - && - && - & 85.10 \\
    & Pascal VOC \cite{PASCAL} & OD & - & Resnet-101 \cite{resnet} & - && 74.17 && - & - \\
    \midrule
    
    \cite{self-supervised-monocular-road-segmentation} & KITTI \cite{KITTI} & SS (Drivable Space) & - & VGG-net \cite{resnet} & 94.23 && - && - & -\\
    \midrule
    
    \multirow{ 2}{*}{\cite{recycling-bounding-box-annotations}} & Pascal VOC \cite{PASCAL} & OD & - & Fast R-CNN \cite{Fast-R-CNN} & - && 77.5 && - & - \\
    & COCO \cite{COCO} & OD & - & Fast R-CNN \cite{Fast-R-CNN} & - && 34.6 && - & - \\
    \midrule
    
    \cite{three-ways} & Cityscapes \cite{cityscapes} & SS & 1/30 & ResNet \cite{resnet} & 62 && - && - & - \\
    \midrule

    \multirow{ 3}{*}{\cite{MultiMAE}}
    & NYUv-2 \cite{NYUv2} & SS & - & ViT \cite{ViT} (RGB-only) & 50.6 && - && - & - \\
    & NYUv-2 \cite{NYUv2} & SS & - & ViT \cite{ViT} (RGB+Depth) & 53.6 && - && - & - \\
    & ImageNet-1K \cite{ImageNet} & C & - & ViT \cite{ViT} (RGB-only) & - && - && 83.3 & -\\
    \midrule

    \multirow{ 2}{*}{\cite{MTL-for-image-segmentation-task}}
    & NYUv-2 \cite{NYUv2} & SS & - & ResNet \cite{resnet} (Nash-MTL) & 80.0 && - && - & - \\
    & NYUv-2 \cite{NYUv2} & SS & - & ResNet \cite{resnet} (Unc.W) & 74.8 && - && - & - \\
    
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}
\end{comment}

\begin{table}[h]
\centering
\caption{Fully-Supervised MTL methods on PASCAL-Context}
\label{tab.2}
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \textbf{Semseg} & \textbf{Parsing} &  \textbf{Saliency}  \\
& mIoU $\uparrow$ & mIoU $\uparrow$ & maxF $\uparrow$  \\
\midrule
Cross-Stitch \cite{cross-stich} & 63.28 & 60.21 & 65.13\\
PAD-Net \cite{PAD-net} & 60.12 & 60.70 & 67.20\\
MTI-Net \cite{MTI-NET} & 61.70 & 60.18 & 84.78\\
InvPT \cite{invPT} & 79.03 & 67.71 & 84.81\\
MTFormer \cite{MTFormer} & 74.15 & 64.89 & 67.71\\
TaskPrompter \cite{taskprompter} & 80.89 & 68.89 & 84.83\\
DeMT \cite{DeMT} & 75.33 & 63.11 & 83.42\\
\bottomrule

\end{tabular}
\label{tab:example_multirow}
\end{table}

\begin{table*}
\centering
\caption{Semi-Supervised Learning (MTPSL \cite{MTPSL}) Comparison on NYUv2 and Cityscapes}
\label{tab.3}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllcc c c c} %lllcc c c c
    \toprule
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Method} &
      \multicolumn{1}{c}{Semseg} &&
      \multicolumn{1}{c}{Depth} &&
      \multicolumn{1}{c}{Normal} & 
      \\
      \cline{3-3} \cline{5-5} \cline{7-7} \\
      && {mIoU $\uparrow$} && {aErr $\downarrow$} && {mErr $\downarrow$} \\ 
      \midrule
    \multirow{ 6}{*}{NYUv2 \cite{NYUv2}}
    & $STL_{SS}$ & 37.45 && - && - \\
    & $STL_{Depth}$ & - && 0.61 && - \\
    & $STL_{SN}$ & - && -  && 25.94 \\
    & $MTL_{CNN}$ & 36.95 && 0.55 && 29.5 \\
    \\
    \cline{2-8}
    \\
    & \cite{MTPSL} MTPSL (1/3) & 28.43 && 0.63 && 33.01 \\
    & \cite{MTPSL} MTPSL (one) & 31.00 && 0.51 && 28.58 \\
    \midrule
    \multirow{ 8}{*}{Cityscapes \cite{cityscapes}}
    & $STL_{Seg}$ & 74.19 && -  \\
    & $STL^{SegNet}_{Depth}$ &  - && 0.012 \\
    & $MTL_{CNN}$ & 73.36 && 0.016  \\
    \\
    \cline{2-8}
    \\
    & \cite{MTPSL} MTPSL (one) & 74.90 && 0.016 \\
    & \cite{MTPSL} MTPSL (1:9) & 71.89 && 0.013 \\
    & \cite{MTPSL} MTPSL (9:1) & 74.23 && 0.026 \\
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

\begin{table*}
\centering
\caption{Semi-Supervised Learning (MTPSL \cite{MTPSL}) on PASCAL-Context}
\label{tab.4}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllcc c c c c c c}
    \toprule
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Method} &
      \multicolumn{1}{c}{SemSeg} &&
      \multicolumn{1}{c}{Human Parts} &&
      \multicolumn{1}{c}{Normal} && 
      \multicolumn{1}{c}{Saliency} &&
      \multicolumn{1}{c}{Edge} 
      \\
      \cline{3-3} \cline{5-5} \cline{7-7} \cline{9-9} \cline{11-11}\\
      && {mIoU $\uparrow$} && {mIoU $\uparrow$} && {mErr $\downarrow$} && {mIoU $\downarrow$} && {odsF $\uparrow$} \\ 
      \midrule
    \multirow{ 4}{*}{Pascal-Context \cite{PASCAL}}
    & STL & 47.7 && 56.2 && 16.0 && 61.9 && 64.0  \\ 
    \\
    \cline{2-11}
    \\
    & \cite{MTPSL} MTPSL (one) & \textbf{49.5} && 55.8 && 17.0 && 61.7 && \textbf{65.1}  \\ 
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

\section{Datasets \& Tools}
%This chapter aims at providing the interested reader with a list of available tools and analysis of the current state of MTL research.

\cref{sec:datasets} refers the reader to a list of datasets commonly utilised in MTL for computer vision. Additionally, \cref{sec:discussion} provides a summary of the results achieved by partially-supervised MTL solutions. Based on these results, we discuss and analyse common trends and suggest interesting paths of exploration to further improve MTL. Last, we introduce a table summarising the different open-source MTL code. 


\subsection{Datasets}
\label{sec:datasets}
%\subsubsection{Dense Prediction Tasks}
%MTL has heavily focused on pixel-level (\textit{aka} dense) prediction tasks (\textit{semantic segmentation, depth estimation, surface normal estimation}). 
Below is a list of common multi-task CV datasets.
\begin{enumerate}
  \item \textbf{Taskonomy. }\cite{taskonomy} This dataset is the largest multi-task dataset. It contains 4.5 million indoor scene images, each labeled with 25 annotations. These images include: scene annotations, camera information, 2D/3D keypoints, surface normals and various-level object annotations. The foundational work \cite{taskonomy} on this dataset performed experiments on 26 diverse tasks.
  
  \item \textbf{NYUv2-Depth. }\cite{NYUv2} This dataset comprises 1449 labeled images drawn from indoor scene videos for which each pixel is annotated with a depth value and an object class. Additionally, there are 407,024 unlabeled images which contain RGB, depth and accelerometer data, rendering this dataset useful for real-time applications as well.  

  \item \textbf{Cityscapes. }\cite{cityscapes} This dataset consists of 5000 urban scenes. Each image is annotated with pixel-level labels for 30 classes. Additionally, the dataset includes image stereo pairs associated camera shift metadata. Therefore, \cite{cityscapes} leverages stereo-paired information to produce accurate depth labels. As a result, Cityscapes \cite{cityscapes} is typically used as a 7-class semantic segmentation class and depth estimation task. 

  \item \textbf{Pascal-Context. }\cite{PASCAL} A dataset of 1464 of regular object centered scenes. This dataset includes tasks such as  saliency estimation, depth estimation, human part segmentation as well as semantic segmentation.

  \item \textbf{KITTI. }\cite{KITTI} This dataset is one of the most popular datasets for Autonomous Driving. The images result from hours of driving in diverse traffic environments. This dataset has been utilised for 3-class \cite{3-class-kitti}, 10-class \cite{10-class-kitti} or 11-class \cite{11-class-kitti} semantic segmentation or object detection. Additionally, the dataset includes 3D labeled point clouds for 15,000 images.  
\end{enumerate}

\subsection{Results and Discussion}
\label{sec:discussion}
This section presents results for partially supervised MTL. Moreover, an attempt to derive both general performance guidelines and future areas of investigation is made. 

\cref{tab.1} provides a comparison of traditional single-task methods with a range of recent multi-task learning methods. The single-task methods covered in this table use RGB-only processing to provide a fair comparison. This table reviews three traditionally tackled tasks : semantic segmentation, monocular depth estimation and surface normal estimation. By analysing the presented methods on the NYUv2 Dataset \cite{NYUv2}, we can observe that semantic segmentation generally improves by taking advantage of the depth and surface normal features from  depth and surface. However, we can notice that, typically, MTL methods fail to perform as good as single-task methods on tasks like depth and surface estimation. We hypothesise that the reasons being (1) due to task-optimal network architectures not being the same for all the tasks, leading to a non-conceivable or overly complex MTL architecture; (2) a task-specific loss function designed generalising poorly to the MTL aggregated gradient representation and (3) the trend to design scalable and simple MTL networks with lightweight decoders which does not reflect well the difficulty of each task. 

Furthermore, we provide, in \cref{tab.2}, a summary of fully-supervised performant MTL methods on the Pascal-Context dataset \cite{PASCAL} covering commonly tackled tasks : semantic segmentation, human part parsing (which is semantic segmentation on human body parts) and saliency detection which consists of identifying interesting points in an image (points that the human eye would focus on straight away). We identify that a comparison with STL methods is complex due to the lack of STL methods covering the same split of the PASCAL dataset \cite{PASCAL}. We however notice a significant improvement brought by various MTL methods on the semantic segmentation: where the best STL method achieves 71\% mIoU \cite{hong2023minimalist}, 4 MTL methods significantly outperforms this result in \cref{tab.2} whilst performing human parsing and saliency detection. 

%\cref{tab.1} summarises some of the self-supervised methods introduced in \cref{sec:representation-learning}. The methods covered in this table leverage MTL as a pre-training strategy and subsequently fine-tune on a single target task. This table considers three common CV tasks: \textbf{Semantic Segmentation} (SS), \textbf{Object Detection} (OD) and \textbf{Classification} (C). Task metrics reported are respectively, mean Intersection over Union (mIoU), mean Average Precision (mAP) and classification accuracy on top-1 and top-5 images. 
%By analysing the aforementioned methods, we first find that studies aiming to acquire global representations almost never leverage a large number of tasks and often leverage tasks based on the authors' assumption of the relatedness of the tasks. As a result, we find that there is a lack of adaptive methods to deal with possibly unrelated tasks too. For instance, an interesting path would be to explore multi-task representations based on the task affinities (as presented in \cref{sec:task-grouping}).
%Furthermore, we identify that, although the methods in \cref{tab.1} train the representations in a self-supervised MTL setting, the fine-tuning procedure often aims at optimising a single/target task. Therefore, it would be interesting to see how a large number of tasks could enhance a representation in a MTL setting \cite{MultiMAE}.
%Last, we point out that for pseudo-labeling, authors generally use single-task pre-trained networks. An interesting path of exploration would be to explore pre-trained MTL networks to give accurate labels to further improve the accuracy of pseudo-labels.
\begin{table*} % Test table
\centering
\caption{MTFormer\cite{MTFormer} treats a \textcolor{red}{target task} annotations as few-shot samples whilst keeping two other tasks fully-supervised. \\
Results are reported on the PASCAL dataset \cite{PASCAL}.}
\label{tab.5}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{lllllc c c c c}
    \toprule
    \multirow{2}{*}{Method} &&
    \multirow{2}{*}{Few-Shot Task} &&
    \multirow{2}{*}{SS $\uparrow$} &&
    \multirow{2}{*}{Human Part Seg. $\uparrow$} &&
    \multirow{2}{*}{Saliency $\uparrow$} \\
    \\
    \cline{5-5} \cline{7-7} \cline{9-9}
    &&&& {mIoU $\uparrow$} && {mIoU $\uparrow$} && {mIoU $\uparrow$} \\
    \midrule
      STL && SS && \textcolor{red}{3.34} && 63.90 && 66.71 \\
      MTFormer \cite{MTFormer} && SS && \textcolor{red}{35.26} && 64.26 && 67.26 \\ 
    \midrule
      STL && Human Part Seg. && 71.17 && \textcolor{red}{11.27} && 66.71 \\
      MTFormer \cite{MTFormer} && Human Part Seg. && 73.36 && \textcolor{red}{51.74} && 67.74 \\ 
    \midrule
      STL && Saliency && 71.17 && 63.90 && \textcolor{red}{44.39} \\
      MTFormer \cite{MTFormer} && Saliency && 76.00 && 66.89 && \textcolor{red}{55.55} \\ 
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

\begin{table*} % Test table
\centering
\caption{MTL open-source code repositories }
\label{tab.6}
%\begin{adjustbox}{width=18.5cm, height=1cm}
  \begin{tabular}{c c p{9cm}}
    \toprule
    %\multirow{1}{*}{Type} &
    %\multirow{1}{*}{Description} &
    \multicolumn{1}{c}{Type} &
    \multicolumn{1}{c}{Link} & 
    \multicolumn{1}{c}{Description}  \\
    \midrule
    \multirow{ 2}{*}{Paper Repository} 
    & \href{https://github.com/WeiHongLee/Awesome-Multi-Task-Learning}{Awesome Multi-Task Learning 1} & This repository regroups MTL-related papers in a chronological order.\\
    & \href{https://github.com/Manchery/awesome-multi-task-learning}{Awesome Multi-Task Learning 2} & This repository gathers MTL papers and provides a categorisation. \\
    \midrule
    
    \multirow{ 2}{*}{Programming Framework} 
    & \href{https://github.com/zhanglijun95/AutoMTL}{AutoMTL} \cite{AutoMTL} & This solution performs automatic MTL model compression given an arbitrary backbone and a set of tasks. \\
    &
    \href{https://github.com/median-research-group/LibMTL}{LibMTL} \cite{LibMTL} & This is a Python library for MTL built on Pytorch. The implementation supports a large number of SOTA solutions, weighting strategies and data loaders. \\
    \midrule
    \multirow{ 3}{*}{Benchmarking} 
    & \href{https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch}{Dense Prediction Tasks} \cite{MTL-dense-predic-tasks} & This solution benchmarks a 2 MTL solutions on CV dense prediction tasks on 2 datasets. It is implemented in Pytorch.\\
    & 
    \href{http://taskonomy.stanford.edu}{Taskonomy} \cite{taskonomy} & In addition to providing web-based visualisations. Taskonomy \cite{taskonomy} introduces a API to group 25 vision tasks. Pre-trained models are available in Tensorflow and Pytorch. \\ 
    &
    \href{https://github.com/SamsungLabs/MTL}{Aligned-MTL} & A programming repository introducing a new gradient-based optimisation technique and allowing to benchmark a wide range of different MTL optimisation strategies introduced in \cref{chapter:Optimisation}. \\ 
    \midrule
    \multirow{ 1}{*}{Self/Semi-supervision} 
    & \href{https://github.com/VICO-UoE/MTPSL}{MTPSL} \cite{MTPSL} & This solution implements different cross-task mapping under balanced and imbalanced semi-supervised settings for dense prediction tasks. This solution is implemented in Pytorch and supports two datasets.\\
     & \href{https://multimae.epfl.ch}{MultiMAE} \cite{MultiMAE} & This solution implements a pre-trained strategy inspired by Masked Auto-Encoders (MAEs). In addition to visualisations, tutorials are presented. The solution is implemented in Pytorch.\\
    \bottomrule
  \end{tabular}
%\end{adjustbox}
\end{table*}

\cref{tab.3} presents results obtained by MTPSL \cite{MTPSL} on two commonly used MTL datasets: NYUv2 \cite{NYUv2} and Cityscapes \cite{cityscapes}. The results are reported on three tasks for NYUv2 \cite{NYUv2} including semantic segmentation, depth estimation and surface normals. Additionally, the results are reported on semantic segmentation and depth estimation for Cityscapes \cite{cityscapes}. 
First, MTPSL \cite{MTPSL} evaluates its cross-task consistency mapping method under two data availability settings. The first configuration consists of $\frac{1}{3}$ of the images, labelled with the three tasks, noted as MTPSL (1/3). The results reported in this setting suggest a degradation in performance compared to the single task learning (STL) baselines. However, the other setting, consisting of all images being labelled with only one of the tasks and noted as MTPSL (one) present better results closer to the STL baseline for all tasks. Although the two data settings present the same labeling demand, they showcase different performance. Therefore, this difference demonstrates that the joint space mapping is efficient \cite{MTPSL} under semi-supervised settings. 
Moreover, MTPSL \cite{MTPSL} displays, as part of their evaluation on Cityscapes \cite{cityscapes}, that some tasks are worth being shared more than others. The authors introduce an imbalanced supervision paradigm option and choose to use only 10\% of a task whilst keeping 90\% of the other task, noted as MTPSL (1:9), meaning 10\% of input images are annotated with segmentation ground truth and 90\% are labelled with depth ground truth. The results for imbalanced tasks present strong robustness, whereas the advantaged tasks outperform STL baselines. 

Similarly, \cref{tab.4} reviews results obtained by MTPSL \cite{MTPSL} on the Pascal-Context \cite{PASCAL} dataset under the 'one' data availability setting (where only one task label is available) for traditionally approached dense prediction tasks. We notice the major superiority of MTL under this setting : whilst still performing 5 tasks, \cite{MTPSL} manages to outperforms STL baselines on semantic segmentation and edge detection and still perform similarly to STL baselines on other tasks. 

\cref{tab.6} shows a range of publicly available code repositories for MTL including paper repositories, programming framework, benchmarking and partially-supervised code resources.

\section{Conclusion}
This review provided an extensive and comprehensive analysis of MTL systems in Computer Vision. 
Firstly, this work studied how architectural implications impact parameter sharing across tasks. 
Second, we analysed the concept of negative transfer and introduced MTL methods to remedy this issue through balancing the pace to which tasks learn during the training of a MTL system.
Third, this paper briefly reviewed how task relationships can be leveraged to provide new insights to task hierarchies to further improve the performance of MTL systems.
Fourth, we extensively reviewed how MTL can be utilised under partially supervised settings, for instance, as a self-supervised pre-training strategy for representation learning, or by exploiting task relationships to reduce the demand for labelled tasks in semi-supervised learning or finally by enhancing few-shot target tasks through cross-task parameter sharing.
Last, we summarised common multi-task datasets and code repositories to provide the interested reader with the necessary toolkits. 
We provide an analysis of results for partially-supervised MTL techniques. Our key insights for future work under this paradigm are: (1) MTL generally processes a small and constrained set of presumably related tasks. We identify there is a lack of adaptive methods, capable of learning relevant features from a large pool of tasks; otherwise, (2) reported results suggest partially-supervised MTL can be as performant as its fully-supervised single-task counterparts, sometimes even better whilst still providing output for multiple tasks : see \cref{tab.3}, \cref{tab.4} and \cref{tab.5} (\ie \textit{Few-Shot Learning, Semi-Supervised Learning}). There is therefore a need to explore solutions and data availability constraints under a multi-task framework. Finally, (3) we identify that MTL requires more benchmarking tools on large datasets. Taskonomy \cite{taskonomy} is the first step towards this direction and similar work could bring new insights to future research in MTL. 

\noindent \textbf{Acknowledgments.}
The authors would like to thank Prof. Tomasz Radzik for helpful discussions and  acknowledge the use of the Kings Computational Research, Engineering and Technology Environment (CREATE). Miaojing Shi was supported by the Fundamental Research Funds for the Central Universities.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% ============================================
%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here %\cite{Roberg2010}.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank D. Root for the loan of the SWAP. The SWAP that can ONLY be usefull in Boulder...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% ====== REFERENCE SECTION

%\begin{thebibliography}{1}

% IEEEabrv,

\bibliographystyle{IEEEtranN}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={((},close={))}} %Citation-related commands
%\bibliography{IEEEabrv,Bibliography}
% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{232}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Bruno et~al.(2022)Bruno, Moroni, and Martinelli]{classification-sota}
\BIBentryALTinterwordspacing
A.~Bruno, D.~Moroni, and M.~Martinelli, ``Efficient adaptive ensembling for image classification,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2206.07394}
\BIBentrySTDinterwordspacing

\bibitem[Simonyan and Zisserman(2015)]{classification-success-1}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for large-scale image recognition,'' in \emph{International Conference on Learning Representations}, 2015.

\bibitem[Huang et~al.(2018)Huang, Liu, van~der Maaten, and Weinberger]{classification-success-2}
G.~Huang, Z.~Liu, L.~van~der Maaten, and K.~Q. Weinberger, ``Densely connected convolutional networks,'' 2018.

\bibitem[Tan and Le(2020)]{classifcation-success-3}
M.~Tan and Q.~V. Le, ``Efficientnet: Rethinking model scaling for convolutional neural networks,'' 2020.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Dai, Chen, Huang, Li, Zhu, Hu, Lu, Lu, Li, Wang, and Qiao]{semantic-segmentation-sota}
\BIBentryALTinterwordspacing
W.~Wang, J.~Dai, Z.~Chen, Z.~Huang, Z.~Li, X.~Zhu, X.~Hu, T.~Lu, L.~Lu, H.~Li, X.~Wang, and Y.~Qiao, ``Internimage: Exploring large-scale vision foundation models with deformable convolutions,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2211.05778}
\BIBentrySTDinterwordspacing

\bibitem[Girshick(2015)]{Fast-R-CNN}
R.~Girshick, ``Fast r-cnn,'' 2015.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and Brox]{unet}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for biomedical image segmentation,'' 2015.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and Sun]{semantic-success1}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' 2015.

\bibitem[Shinya(2021)]{object-detection-sota}
\BIBentryALTinterwordspacing
Y.~Shinya, ``Usb: Universal-scale object detection benchmark,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.14027}
\BIBentrySTDinterwordspacing

\bibitem[Redmon and Farhadi(2018)]{YOLOv3}
\BIBentryALTinterwordspacing
J.~Redmon and A.~Farhadi, ``Yolov3: An incremental improvement,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1804.02767}
\BIBentrySTDinterwordspacing

\bibitem[Caruana(1997)]{Caruana}
R.~Caruana, ``Multitask learning,'' \emph{Machine Learning}, vol.~28, 07 1997.

\bibitem[Zeiler and Fergus(2013)]{visualizing-CNN}
\BIBentryALTinterwordspacing
M.~D. Zeiler and R.~Fergus, ``Visualizing and understanding convolutional networks,'' \emph{CoRR}, vol. abs/1311.2901, 2013. [Online]. Available: \url{http://arxiv.org/abs/1311.2901}
\BIBentrySTDinterwordspacing

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and Hebert]{cross-stich}
\BIBentryALTinterwordspacing
I.~Misra, A.~Shrivastava, A.~Gupta, and M.~Hebert, ``Cross-stitch networks for multi-task learning,'' \emph{CoRR}, vol. abs/1604.03539, 2016. [Online]. Available: \url{http://arxiv.org/abs/1604.03539}
\BIBentrySTDinterwordspacing

\bibitem[Ando and Zhang(2005)]{predictive-structures}
R.~Ando and T.~Zhang, ``A framework for learning predictive structures from multiple tasks and unlabeled data,'' \emph{Journal of Machine Learning Research}, vol.~6, pp. 1817--1853, 11 2005.

\bibitem[Bingel and Sgaard(2017)]{task-relationships}
J.~Bingel and A.~Sgaard, ``Identifying beneficial task relations for multi-task learning in deep neural networks,'' 01 2017, pp. 164--169.

\bibitem[Imran et~al.(2020)Imran, Huang, Tang, Fan, Xiao, Hao, Qian, and Terzopoulos]{partly-supervised-MTL}
\BIBentryALTinterwordspacing
A.-A.-Z. Imran, C.~Huang, H.~Tang, W.~Fan, Y.~Xiao, D.~Hao, Z.~Qian, and D.~Terzopoulos, ``Partly supervised multitask learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2005.02523}
\BIBentrySTDinterwordspacing

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Liu, and Bilen]{MTPSL}
W.-H. Li, X.~Liu, and H.~Bilen, ``Learning multiple dense prediction tasks from partially annotated data,'' in \emph{IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2022.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Tsai, Hung, Ding, Liu, and Yang]{semi-supervised-MTL-for-semantics-depth}
\BIBentryALTinterwordspacing
Y.~Wang, Y.-H. Tsai, W.-C. Hung, W.~Ding, S.~Liu, and M.-H. Yang, ``Semi-supervised multi-task learning for semantics and depth,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2110.07197}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2007)Liu, Liao, and Carin]{Semi-supervised-MTL}
\BIBentryALTinterwordspacing
Q.~Liu, X.~Liao, and L.~Carin, ``Semi-supervised multitask learning,'' in \emph{Advances in Neural Information Processing Systems}, J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, Eds., vol.~20.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2007. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2007/file/a34bacf839b923770b2c360eefa26748-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem[Khosravan and Bagci(2018)]{semi-MTL-for-lung-cancer}
N.~Khosravan and U.~Bagci, ``Semi-supervised multi-task learning for lung cancer diagnosis,'' in \emph{2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 2018, pp. 710--713.

\bibitem[Chowdhuri et~al.(2017)Chowdhuri, Pankaj, and Zipser]{multinet}
\BIBentryALTinterwordspacing
S.~Chowdhuri, T.~Pankaj, and K.~Zipser, ``Multi-modal multi-task deep learning for autonomous driving,'' \emph{CoRR}, vol. abs/1709.05581, 2017. [Online]. Available: \url{http://arxiv.org/abs/1709.05581}
\BIBentrySTDinterwordspacing

\bibitem[Ishihara et~al.(2021)Ishihara, Kanervisto, Miura, and Hautam{\"{a}}ki]{end-to-end-autonomous-driving}
\BIBentryALTinterwordspacing
K.~Ishihara, A.~Kanervisto, J.~Miura, and V.~Hautam{\"{a}}ki, ``Multi-task learning with attention for end-to-end autonomous driving,'' \emph{CoRR}, vol. abs/2104.10753, 2021. [Online]. Available: \url{https://arxiv.org/abs/2104.10753}
\BIBentrySTDinterwordspacing

\bibitem[Liang et~al.(2022)Liang, Wu, Han, Xu, Xu, and Liang]{unified-autonomous-driving}
\BIBentryALTinterwordspacing
X.~Liang, Y.~Wu, J.~Han, H.~Xu, C.~Xu, and X.~Liang, ``Effective adaptation in multi-task co-training for unified autonomous driving,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2209.08953}
\BIBentrySTDinterwordspacing

\bibitem[Karpathy(2019)]{Andrej-talk}
\BIBentryALTinterwordspacing
A.~Karpathy, ``Multi-task learning in the wilderness,'' \emph{ICML}, 2019. [Online]. Available: \url{https://slideslive.com/38917690/multitask-learning-in-the-wilderness}
\BIBentrySTDinterwordspacing

\bibitem[Islam et~al.(2020)Islam, Vibashan, and Ren]{AP-MTL}
\BIBentryALTinterwordspacing
M.~Islam, V.~S. Vibashan, and H.~Ren, ``{AP}-{MTL}: Attention pruned multi-task learning model for real-time instrument detection and segmentation in robot-assisted surgery,'' in \emph{2020 {IEEE} International Conference on Robotics and Automation ({ICRA})}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, may 2020. [Online]. Available: \url{https://doi.org/10.1109%2Ficra40945.2020.9196905}
\BIBentrySTDinterwordspacing

\bibitem[Islam et~al.(2021)Islam, VS, Lim, and Ren]{ST-MTL}
\BIBentryALTinterwordspacing
M.~Islam, V.~VS, C.~M. Lim, and H.~Ren, ``St-mtl: Spatio-temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2112.08189}
\BIBentrySTDinterwordspacing

\bibitem[Ming et~al.(2019)Ming, Xia, Luqman, Burie, and Zhao]{face_recognition-facial-expression}
\BIBentryALTinterwordspacing
Z.~Ming, J.~Xia, M.~M. Luqman, J.-C. Burie, and K.~Zhao, ``Dynamic multi-task learning for face recognition with facial expression,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1911.03281}
\BIBentrySTDinterwordspacing

\bibitem[Zheng et~al.(2022)Zheng, Deng, Zhu, Li, and Zafeiriou]{face-parsing}
Q.~Zheng, J.~Deng, Z.~Zhu, Y.~Li, and S.~Zafeiriou, ``Decoupled multi-task learning with cyclical self-regulation for face parsing,'' in \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 4146--4155.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Hu, Xie, Gong, and Yu]{privacy-preserving-face}
\BIBentryALTinterwordspacing
C.~Zhang, X.~Hu, Y.~Xie, M.~Gong, and B.~Yu, ``A privacy-preserving multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition,'' \emph{Frontiers in Neurorobotics}, vol.~13, 2020. [Online]. Available: \url{https://www.frontiersin.org/articles/10.3389/fnbot.2019.00112}
\BIBentrySTDinterwordspacing

\bibitem[Huang et~al.(2021)Huang, Zhang, and Shan]{MTLFace}
\BIBentryALTinterwordspacing
Z.~Huang, J.~Zhang, and H.~Shan, ``When age-invariant face recognition meets face age synthesis: A multi-task learning framework,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.01520}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Yu, Zheng, Lin, Huang, Liu, hu, Duan, and Si]{upper-gastro}
X.~Zhang, T.~Yu, W.~Zheng, N.~Lin, Z.~Huang, J.~Liu, W.~hu, H.~Duan, and J.~Si, ``Upper gastrointestinal anatomy detection with multi-task convolutional neural networks,'' \emph{Healthcare Technology Letters}, vol.~6, 10 2019.

\bibitem[Kong et~al.(2021)Kong, He, Luo, Xiansong, Wei, Cheng, Chen, Liang, Lu, Li, and Chen]{explicable-capsule-endoscopy}
Z.~Kong, M.~He, Q.~Luo, H.~Xiansong, P.~Wei, Y.~Cheng, L.~Chen, Y.~Liang, Y.~Lu, X.~Li, and J.~Chen, ``Multi-task classification and segmentation for explicable capsule endoscopy diagnostics,'' \emph{Frontiers in Molecular Biosciences}, vol.~8, 08 2021.

\bibitem[Yu et~al.(2022)Yu, Tang, Cheang, Yu, and Choi]{esophageal}
\BIBentryALTinterwordspacing
X.~Yu, S.~Tang, C.~F. Cheang, H.~H. Yu, and I.~C. Choi, ``Multi-task model for esophageal lesion analysis using endoscopic images: Classification with image retrieval and segmentation with attention,'' \emph{Sensors}, vol.~22, no.~1, 2022. [Online]. Available: \url{https://www.mdpi.com/1424-8220/22/1/283}
\BIBentrySTDinterwordspacing

\bibitem[Hasan and Linte(2021)]{3d-cardiac}
\BIBentryALTinterwordspacing
S.~M.~K. Hasan and C.~A. Linte, ``A multi-task cross-task learning architecture for ad-hoc uncertainty estimation in 3d cardiac mri image segmentation,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2109.07702}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2019)Liu, He, Chen, and Gao]{MTL-NLP-networks}
\BIBentryALTinterwordspacing
X.~Liu, P.~He, W.~Chen, and J.~Gao, ``Multi-task deep neural networks for natural language understanding,'' \emph{CoRR}, vol. abs/1901.11504, 2019. [Online]. Available: \url{http://arxiv.org/abs/1901.11504}
\BIBentrySTDinterwordspacing

\bibitem[Pilault et~al.(2021)Pilault, hattami, and Pal]{NLP-MTL-ADAPTIVE}
\BIBentryALTinterwordspacing
J.~Pilault, A.~E. hattami, and C.~Pal, ``Conditionally adaptive multi-task learning: Improving transfer learning in {NLP} using fewer parameters and less data,'' in \emph{International Conference on Learning Representations}, 2021. [Online]. Available: \url{https://openreview.net/forum?id=de11dbHzAMF}
\BIBentrySTDinterwordspacing

\bibitem[Aguilar et~al.(2017)Aguilar, Maharjan, L{\'o}pez-Monroy, and Solorio]{NLP-NER}
\BIBentryALTinterwordspacing
G.~Aguilar, S.~Maharjan, A.~P. L{\'o}pez-Monroy, and T.~Solorio, ``A multi-task approach for named entity recognition in social media data,'' in \emph{Proceedings of the 3rd Workshop on Noisy User-generated Text}.\hskip 1em plus 0.5em minus 0.4em\relax Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 148--153. [Online]. Available: \url{https://aclanthology.org/W17-4419}
\BIBentrySTDinterwordspacing

\bibitem[Nguyen and Nguyen(2021)]{Vietnamese-POS}
\BIBentryALTinterwordspacing
L.~T. Nguyen and D.~Q. Nguyen, ``Phonlp: A joint multi-task learning model for vietnamese part-of-speech tagging, named entity recognition and dependency parsing,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2101.01476}
\BIBentrySTDinterwordspacing

\bibitem[Changpinyo et~al.(2018)Changpinyo, Hu, and Sha]{NLP-SEQ}
\BIBentryALTinterwordspacing
S.~Changpinyo, H.~Hu, and F.~Sha, ``Multi-task learning for sequence tagging: An empirical study,'' in \emph{Proceedings of the 27th International Conference on Computational Linguistics}.\hskip 1em plus 0.5em minus 0.4em\relax Santa Fe, New Mexico, USA: Association for Computational Linguistics, Aug. 2018, pp. 2965--2977. [Online]. Available: \url{https://aclanthology.org/C18-1251}
\BIBentrySTDinterwordspacing

\bibitem[Anastasyev et~al.(2018)Anastasyev, Gusev, and Indenbom]{NLP-POS}
\BIBentryALTinterwordspacing
D.~Anastasyev, I.~Gusev, and E.~Indenbom, ``Improving part-of-speech tagging via multi-task learning and character-level word representations,'' \emph{CoRR}, vol. abs/1807.00818, 2018. [Online]. Available: \url{http://arxiv.org/abs/1807.00818}
\BIBentrySTDinterwordspacing

\bibitem[Deng et~al.(2022)Deng, Zhang, Xu, Lei, Chua, and Lam]{MTL-recommendation-systems-1}
\BIBentryALTinterwordspacing
Y.~Deng, W.~Zhang, W.~Xu, W.~Lei, T.-S. Chua, and W.~Lam, ``A unified multi-task learning framework for multi-goal conversational recommender systems,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2204.06923}
\BIBentrySTDinterwordspacing

\bibitem[Ning and Karypis(2010)]{MTL-recommendation-systems-2}
\BIBentryALTinterwordspacing
X.~Ning and G.~Karypis, ``Multi-task learning for recommender system,'' in \emph{Proceedings of 2nd Asian Conference on Machine Learning}, ser. Proceedings of Machine Learning Research, M.~Sugiyama and Q.~Yang, Eds., vol.~13.\hskip 1em plus 0.5em minus 0.4em\relax Tokyo, Japan: PMLR, 08--10 Nov 2010, pp. 269--284. [Online]. Available: \url{https://proceedings.mlr.press/v13/ning10a.html}
\BIBentrySTDinterwordspacing

\bibitem[Chen et~al.(2019)Chen, Wang, Xie, Wu, Bu, Wang, and Chen]{MTL-recommendation-systems-3}
\BIBentryALTinterwordspacing
Z.~Chen, X.~Wang, X.~Xie, T.~Wu, G.~Bu, Y.~Wang, and E.~Chen, ``Co-attentive multi-task learning for explainable recommendation,'' in \emph{Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}}.\hskip 1em plus 0.5em minus 0.4em\relax International Joint Conferences on Artificial Intelligence Organization, 7 2019, pp. 2137--2143. [Online]. Available: \url{https://doi.org/10.24963/ijcai.2019/296}
\BIBentrySTDinterwordspacing

\bibitem[Ruder(2017)]{MTL-overview-2017}
\BIBentryALTinterwordspacing
S.~Ruder, ``An overview of multi-task learning in deep neural networks,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1706.05098}
\BIBentrySTDinterwordspacing

\bibitem[Crawshaw(2020)]{MTL-in-DNN-2020}
\BIBentryALTinterwordspacing
M.~Crawshaw, ``Multi-task learning with deep neural networks: A survey,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2009.09796}
\BIBentrySTDinterwordspacing

\bibitem[Zhang and Yang(2022)]{MTL-survey-2021}
Y.~Zhang and Q.~Yang, ``A survey on multi-task learning,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~34, no.~12, pp. 5586--5609, 2022.

\bibitem[Vandenhende et~al.(2021)Vandenhende, Georgoulis, Gansbeke, Proesmans, Dai, and Gool]{MTL-dense-predic-tasks}
\BIBentryALTinterwordspacing
S.~Vandenhende, S.~Georgoulis, W.~V. Gansbeke, M.~Proesmans, D.~Dai, and L.~V. Gool, ``Multi-task learning for dense prediction tasks: A survey,'' \emph{{IEEE} Transactions on Pattern Analysis and Machine Intelligence}, pp. 1--1, 2021. [Online]. Available: \url{https://doi.org/10.1109%2Ftpami.2021.3054719}
\BIBentrySTDinterwordspacing

\bibitem[Vithayathil~Varghese and Mahmoud(2020)]{Deep-reinforcement-learning-survey}
\BIBentryALTinterwordspacing
N.~Vithayathil~Varghese and Q.~H. Mahmoud, ``A survey of multi-task deep reinforcement learning,'' \emph{Electronics}, vol.~9, no.~9, 2020. [Online]. Available: \url{https://www.mdpi.com/2079-9292/9/9/1363}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2022)Zhang, Yu, Yu, Guo, and Jiang]{MTL-NLP-survey}
\BIBentryALTinterwordspacing
Z.~Zhang, W.~Yu, M.~Yu, Z.~Guo, and M.~Jiang, ``A survey of multi-task learning in natural language processing: Regarding task relatedness and training methods,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2204.03508}
\BIBentrySTDinterwordspacing

\bibitem[Chen et~al.(2021)Chen, Zhang, and Yang]{MTL-NLP-overview}
\BIBentryALTinterwordspacing
S.~Chen, Y.~Zhang, and Q.~Yang, ``Multi-task learning in natural language processing: An overview,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2109.09138}
\BIBentrySTDinterwordspacing

\bibitem[Gong et~al.(2019)Gong, Lee, Stephenson, Renduchintala, Padhy, Ndirango, Keskin, and Elibol]{a-comparison-of-loss-weightin-strategies}
T.~Gong, T.~Lee, C.~Stephenson, V.~Renduchintala, S.~Padhy, A.~Ndirango, G.~Keskin, and O.~H. Elibol, ``A comparison of loss weighting strategies for multi task learning in deep neural networks,'' \emph{IEEE Access}, vol.~7, pp. 141\,627--141\,632, 2019.

\bibitem[Zamir et~al.(2018)Zamir, Sax, Shen, Guibas, Malik, and Savarese]{taskonomy}
\BIBentryALTinterwordspacing
A.~R. Zamir, A.~Sax, W.~B. Shen, L.~J. Guibas, J.~Malik, and S.~Savarese, ``Taskonomy: Disentangling task transfer learning,'' \emph{CoRR}, vol. abs/1804.08328, 2018. [Online]. Available: \url{http://arxiv.org/abs/1804.08328}
\BIBentrySTDinterwordspacing

\bibitem[Mensink et~al.(2021)Mensink, Uijlings, Kuznetsova, Gygli, and Ferrari]{factors-of-influence}
\BIBentryALTinterwordspacing
T.~Mensink, J.~R.~R. Uijlings, A.~Kuznetsova, M.~Gygli, and V.~Ferrari, ``Factors of influence for transfer learning across diverse appearance domains and task types,'' \emph{CoRR}, vol. abs/2103.13318, 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.13318}
\BIBentrySTDinterwordspacing

\bibitem[Argyriou et~al.(2006)Argyriou, Evgeniou, and Pontil]{MT-feature-learning}
\BIBentryALTinterwordspacing
A.~Argyriou, T.~Evgeniou, and M.~Pontil, ``Multi-task feature learning,'' in \emph{Advances in Neural Information Processing Systems}, B.~Sch\"{o}lkopf, J.~Platt, and T.~Hoffman, Eds., vol.~19.\hskip 1em plus 0.5em minus 0.4em\relax MIT Press, 2006. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2006/file/0afa92fc0f8a9cf051bf2961b06ac56b-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem[Evgeniou et~al.(2005)Evgeniou, Micchelli, and Pontil]{learning-multiple-tasks-with-kernel-methods}
\BIBentryALTinterwordspacing
T.~Evgeniou, C.~A. Micchelli, and M.~Pontil, ``Learning multiple tasks with kernel methods,'' \emph{Journal of Machine Learning Research}, vol.~6, no.~21, pp. 615--637, 2005. [Online]. Available: \url{http://jmlr.org/papers/v6/evgeniou05a.html}
\BIBentrySTDinterwordspacing

\bibitem[Lounici et~al.(2009)Lounici, Pontil, Tsybakov, and van~de Geer]{taking-advantage-of-sparsity}
\BIBentryALTinterwordspacing
K.~Lounici, M.~Pontil, A.~B. Tsybakov, and S.~van~de Geer, ``Taking advantage of sparsity in multi-task learning,'' 2009. [Online]. Available: \url{https://arxiv.org/abs/0903.1468}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2012)Liu, Ji, and Ye]{MTL-l21}
\BIBentryALTinterwordspacing
J.~Liu, S.~Ji, and J.~Ye, ``Multi-task feature learning via efficient l2,1-norm minimization,'' \emph{CoRR}, vol. abs/1205.2631, 2012. [Online]. Available: \url{http://arxiv.org/abs/1205.2631}
\BIBentrySTDinterwordspacing

\bibitem[Ji and Ye(2009)]{trace-norm-minimization}
S.~Ji and J.~Ye, ``An accelerated gradient method for trace norm minimization,'' 01 2009, p.~58.

\bibitem[Jalali et~al.(2010)Jalali, Sanghavi, Ruan, and Ravikumar]{dirty-model}
\BIBentryALTinterwordspacing
A.~Jalali, S.~Sanghavi, C.~Ruan, and P.~Ravikumar, ``A dirty model for multi-task learning,'' in \emph{Advances in Neural Information Processing Systems}, J.~Lafferty, C.~Williams, J.~Shawe-Taylor, R.~Zemel, and A.~Culotta, Eds., vol.~23.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2010. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem[Kumar and Daume(2012)]{learning-task-grouping-and-overlap}
\BIBentryALTinterwordspacing
A.~Kumar and H.~Daume, ``Learning task grouping and overlap in multi-task learning,'' 2012. [Online]. Available: \url{https://arxiv.org/abs/1206.6417}
\BIBentrySTDinterwordspacing

\bibitem[Thrun and O'Sullivan(1998)]{clustering-learning-tasks}
S.~Thrun and J.~O'Sullivan, \emph{Clustering Learning Tasks and the Selective Cross-Task Transfer of Knowledge}.\hskip 1em plus 0.5em minus 0.4em\relax USA: Kluwer Academic Publishers, 1998, p. 235257.

\bibitem[Xue et~al.(2007)Xue, Liao, Carin, and Krishnapuram]{dirichlet-process-priors}
\BIBentryALTinterwordspacing
Y.~Xue, X.~Liao, L.~Carin, and B.~Krishnapuram, ``Multi-task learning for classification with dirichlet process priors,'' \emph{Journal of Machine Learning Research}, vol.~8, no.~2, pp. 35--63, 2007. [Online]. Available: \url{http://jmlr.org/papers/v8/xue07a.html}
\BIBentrySTDinterwordspacing

\bibitem[Jacob et~al.(2008)Jacob, Bach, and Vert]{clustered-MTL}
\BIBentryALTinterwordspacing
L.~Jacob, F.~Bach, and J.-P. Vert, ``Clustered multi-task learning: A convex formulation,'' 2008. [Online]. Available: \url{https://arxiv.org/abs/0809.2085}
\BIBentrySTDinterwordspacing

\bibitem[Micchelli and Pontil(2004)]{kernels-for-MTL}
\BIBentryALTinterwordspacing
C.~Micchelli and M.~Pontil, ``Kernels for multi--task learning,'' in \emph{Advances in Neural Information Processing Systems}, L.~Saul, Y.~Weiss, and L.~Bottou, Eds., vol.~17.\hskip 1em plus 0.5em minus 0.4em\relax MIT Press, 2004. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2004/file/c4f796afbc6267501964b46427b3f6ba-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem[Gu et~al.(2011)Gu, Li, and Han]{learning-a-kernel}
\BIBentryALTinterwordspacing
Q.~Gu, Z.~Li, and J.~Han, ``Learning a kernel for multi-task clustering,'' \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~25, no.~1, pp. 368--373, Aug. 2011. [Online]. Available: \url{https://ojs.aaai.org/index.php/AAAI/article/view/7914}
\BIBentrySTDinterwordspacing

\bibitem[Zhou et~al.(2011)Zhou, Chen, and Ye]{CMTL-via-ASO}
\BIBentryALTinterwordspacing
J.~Zhou, J.~Chen, and J.~Ye, ``Clustered multi-task learning via alternating structure optimization,'' in \emph{Advances in Neural Information Processing Systems}, J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and K.~Weinberger, Eds., vol.~24.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2011. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2011/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem[Kokkinos(2016)]{Ubernet}
\BIBentryALTinterwordspacing
I.~Kokkinos, ``Ubernet: Training a 'universal' convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory,'' \emph{CoRR}, vol. abs/1609.02132, 2016. [Online]. Available: \url{http://arxiv.org/abs/1609.02132}
\BIBentrySTDinterwordspacing

\bibitem[Kendall et~al.(2017)Kendall, Gal, and Cipolla]{uncertainty}
\BIBentryALTinterwordspacing
A.~Kendall, Y.~Gal, and R.~Cipolla, ``Multi-task learning using uncertainty to weigh losses for scene geometry and semantics,'' \emph{CoRR}, vol. abs/1705.07115, 2017. [Online]. Available: \url{http://arxiv.org/abs/1705.07115}
\BIBentrySTDinterwordspacing

\bibitem[Heuer et~al.(2021)Heuer, Mantowsky, Bukhari, and Schneider]{multitask-centernet}
\BIBentryALTinterwordspacing
F.~Heuer, S.~Mantowsky, S.~S. Bukhari, and G.~Schneider, ``Multitask-centernet (mcn): Efficient and diverse multitask learning using an anchor free approach,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2108.05060}
\BIBentrySTDinterwordspacing

\bibitem[Xu et~al.(2018)Xu, Ouyang, Wang, and Sebe]{PAD-net}
\BIBentryALTinterwordspacing
D.~Xu, W.~Ouyang, X.~Wang, and N.~Sebe, ``Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1805.04409}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Cui, Xu, Yan, Sebe, and Yang]{PAP}
Z.~Zhang, Z.~Cui, C.~Xu, Y.~Yan, N.~Sebe, and J.~Yang, ``Pattern-affinitive propagation across depth, surface normal and semantic segmentation,'' 2019.

\bibitem[Vandenhende et~al.(2020)Vandenhende, Georgoulis, and Gool]{MTI-NET}
S.~Vandenhende, S.~Georgoulis, and L.~V. Gool, ``Mti-net: Multi-scale task interaction networks for multi-task learning,'' 2020.

\bibitem[Bruggemann et~al.(2021)Bruggemann, Kanakis, Obukhov, Georgoulis, and Gool]{exploring-relational-context}
D.~Bruggemann, M.~Kanakis, A.~Obukhov, S.~Georgoulis, and L.~V. Gool, ``Exploring relational context for multi-task dense prediction,'' 2021.

\bibitem[Ye and Xu(2022)]{invPT}
\BIBentryALTinterwordspacing
H.~Ye and D.~Xu, ``Invpt: Inverted pyramid multi-task transformer for dense scene understanding,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2203.07997}
\BIBentrySTDinterwordspacing

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{ViT}
\BIBentryALTinterwordspacing
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit, and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2010.11929}
\BIBentrySTDinterwordspacing

\bibitem[Ruder et~al.(2017)Ruder, Bingel, Augenstein, and Sgaard]{sluice}
\BIBentryALTinterwordspacing
S.~Ruder, J.~Bingel, I.~Augenstein, and A.~Sgaard, ``Latent multi-task architecture learning,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1705.08142}
\BIBentrySTDinterwordspacing

\bibitem[Gao et~al.(2018)Gao, She, Ma, Zhao, Liu, and Yuille]{NDDR}
\BIBentryALTinterwordspacing
Y.~Gao, Q.~She, J.~Ma, M.~Zhao, W.~Liu, and A.~L. Yuille, ``{NDDR-CNN:} layer-wise feature fusing in multi-task {CNN} by neural discriminative dimensionality reduction,'' \emph{CoRR}, vol. abs/1801.08297, 2018. [Online]. Available: \url{http://arxiv.org/abs/1801.08297}
\BIBentrySTDinterwordspacing

\bibitem[Lin et~al.(2013)Lin, Chen, and Yan]{network-in-network}
\BIBentryALTinterwordspacing
M.~Lin, Q.~Chen, and S.~Yan, ``Network in network,'' 2013. [Online]. Available: \url{https://arxiv.org/abs/1312.4400}
\BIBentrySTDinterwordspacing

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{attention-is-all-you-need}
\BIBentryALTinterwordspacing
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1706.03762}
\BIBentrySTDinterwordspacing

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and Shao]{PVT}
\BIBentryALTinterwordspacing
W.~Wang, E.~Xie, X.~Li, D.-P. Fan, K.~Song, D.~Liang, T.~Lu, P.~Luo, and L.~Shao, ``Pyramid vision transformer: A versatile backbone for dense prediction without convolutions,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2102.12122}
\BIBentrySTDinterwordspacing

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and Shao]{PVT-v2}
\BIBentryALTinterwordspacing
------, ``{PVT} v2: Improved baselines with pyramid vision transformer,'' \emph{Computational Visual Media}, vol.~8, no.~3, pp. 415--424, mar 2022. [Online]. Available: \url{https://doi.org/10.1007%2Fs41095-022-0274-8}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{swin-transformer}
\BIBentryALTinterwordspacing
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.14030}
\BIBentrySTDinterwordspacing

\bibitem[Yang et~al.(2021)Yang, Li, Zhang, Dai, Xiao, Yuan, and Gao]{focal-transformer}
J.~Yang, C.~Li, P.~Zhang, X.~Dai, B.~Xiao, L.~Yuan, and J.~Gao, ``Focal self-attention for local-global interactions in vision transformers,'' 2021.

\bibitem[Hu and Singh(2021)]{UniT}
\BIBentryALTinterwordspacing
R.~Hu and A.~Singh, ``Unit: Multimodal multitask learning with a unified transformer,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2102.10772}
\BIBentrySTDinterwordspacing

\bibitem[Bhattacharjee et~al.(2022)Bhattacharjee, Zhang, Ssstrunk, and Salzmann]{MulT}
\BIBentryALTinterwordspacing
D.~Bhattacharjee, T.~Zhang, S.~Ssstrunk, and M.~Salzmann, ``Mult: An end-to-end multitask learning transformer,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2205.08303}
\BIBentrySTDinterwordspacing

\bibitem[Xu et~al.(2022)Xu, Zhao, Vineet, Lim, and Torralba]{MTFormer}
\BIBentryALTinterwordspacing
X.~Xu, H.~Zhao, V.~Vineet, S.-N. Lim, and A.~Torralba, ``Mtformer: Multi-task learning via transformer and cross-task reasoning,'' in \emph{Computer Vision  ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXVII}.\hskip 1em plus 0.5em minus 0.4em\relax Berlin, Heidelberg: Springer-Verlag, 2022, p. 304321. [Online]. Available: \url{https://doi.org/10.1007/978-3-031-19812-0_18}
\BIBentrySTDinterwordspacing

\bibitem[Yang and Hospedales(2016{\natexlab{a}})]{deep-multi-task-representation-learning}
\BIBentryALTinterwordspacing
Y.~Yang and T.~M. Hospedales, ``Deep multi-task representation learning: {A} tensor factorisation approach,'' \emph{CoRR}, vol. abs/1605.06391, 2016. [Online]. Available: \url{http://arxiv.org/abs/1605.06391}
\BIBentrySTDinterwordspacing

\bibitem[Klema and Laub(1980)]{SVD}
V.~Klema and A.~Laub, ``The singular value decomposition: Its computation and some applications,'' \emph{IEEE Transactions on Automatic Control}, vol.~25, no.~2, pp. 164--176, 1980.

\bibitem[Tucker(1966c)]{Tuck1966c}
L.~R. Tucker, ``{S}ome mathematical notes on three-mode factor analysis,'' \emph{Psychometrika}, vol.~31, pp. 279--311, 1966c.

\bibitem[Yang and Hospedales(2016{\natexlab{b}})]{Trace-norm-DML}
\BIBentryALTinterwordspacing
Y.~Yang and T.~M. Hospedales, ``Trace norm regularised deep multi-task learning,'' \emph{CoRR}, vol. abs/1606.04038, 2016. [Online]. Available: \url{http://arxiv.org/abs/1606.04038}
\BIBentrySTDinterwordspacing

\bibitem[Rusu et~al.(2015)Rusu, Colmenarejo, Gulcehre, Desjardins, Kirkpatrick, Pascanu, Mnih, Kavukcuoglu, and Hadsell]{policy-distillation}
\BIBentryALTinterwordspacing
A.~A. Rusu, S.~G. Colmenarejo, C.~Gulcehre, G.~Desjardins, J.~Kirkpatrick, R.~Pascanu, V.~Mnih, K.~Kavukcuoglu, and R.~Hadsell, ``Policy distillation,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1511.06295}
\BIBentrySTDinterwordspacing

\bibitem[Parisotto et~al.(2015)Parisotto, Ba, and Salakhutdinov]{actor-mimic}
\BIBentryALTinterwordspacing
E.~Parisotto, J.~L. Ba, and R.~Salakhutdinov, ``Actor-mimic: Deep multitask and transfer reinforcement learning,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1511.06342}
\BIBentrySTDinterwordspacing

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell, Heess, and Pascanu]{distral}
\BIBentryALTinterwordspacing
Y.~W. Teh, V.~Bapst, W.~M. Czarnecki, J.~Quan, J.~Kirkpatrick, R.~Hadsell, N.~Heess, and R.~Pascanu, ``Distral: Robust multitask reinforcement learning,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1707.04175}
\BIBentrySTDinterwordspacing

\bibitem[Li and Bilen(2020)]{knowledge-distillation-for-mtl}
\BIBentryALTinterwordspacing
W.-H. Li and H.~Bilen, ``Knowledge distillation for multi-task learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2007.06889}
\BIBentrySTDinterwordspacing

\bibitem[Ghiasi et~al.(2021)Ghiasi, Zoph, Cubuk, Le, and Lin]{MuST}
\BIBentryALTinterwordspacing
G.~Ghiasi, B.~Zoph, E.~D. Cubuk, Q.~V. Le, and T.-Y. Lin, ``Multi-task self-training for learning general representations,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2108.11353}
\BIBentrySTDinterwordspacing

\bibitem[Yang et~al.(2022)Yang, Ye, and Wang]{factorizing-knowledge-in-NNs}
\BIBentryALTinterwordspacing
X.~Yang, J.~Ye, and X.~Wang, ``Factorizing knowledge in neural networks,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2207.03337}
\BIBentrySTDinterwordspacing

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{learning-multiple-visual-domains}
\BIBentryALTinterwordspacing
S.-A. Rebuffi, H.~Bilen, and A.~Vedaldi, ``Learning multiple visual domains with residual adapters,'' in \emph{Advances in Neural Information Processing Systems}, I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, Eds., vol.~30.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2017. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{resnet}
\BIBentryALTinterwordspacing
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1512.03385}
\BIBentrySTDinterwordspacing

\bibitem[Li and Hoiem(2016)]{learning-without-forgetting}
\BIBentryALTinterwordspacing
Z.~Li and D.~Hoiem, ``Learning without forgetting,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1606.09282}
\BIBentrySTDinterwordspacing

\bibitem[Rebuffi et~al.(2018)Rebuffi, Bilen, and Vedaldi]{efficient-parametrization-of-multi-domain-nn}
\BIBentryALTinterwordspacing
S.-A. Rebuffi, H.~Bilen, and A.~Vedaldi, ``Efficient parametrization of multi-domain deep neural networks,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1803.10082}
\BIBentrySTDinterwordspacing

\bibitem[Maninis et~al.(2019)Maninis, Radosavovic, and Kokkinos]{attentive-single-tasking}
\BIBentryALTinterwordspacing
K.-K. Maninis, I.~Radosavovic, and I.~Kokkinos, ``Attentive single-tasking of multiple tasks,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1904.08918}
\BIBentrySTDinterwordspacing

\bibitem[Hu et~al.(2017{\natexlab{a}})Hu, Shen, Albanie, Sun, and Wu]{squeeze-and-excitation}
\BIBentryALTinterwordspacing
J.~Hu, L.~Shen, S.~Albanie, G.~Sun, and E.~Wu, ``Squeeze-and-excitation networks,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1709.01507}
\BIBentrySTDinterwordspacing

\bibitem[Pfeiffer et~al.(2020)Pfeiffer, Kamath, Rckl, Cho, and Gurevych]{adapter-fusion}
\BIBentryALTinterwordspacing
J.~Pfeiffer, A.~Kamath, A.~Rckl, K.~Cho, and I.~Gurevych, ``Adapterfusion: Non-destructive task composition for transfer learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2005.00247}
\BIBentrySTDinterwordspacing

\bibitem[Stickland and Murray(2019)]{bert-and-pals}
\BIBentryALTinterwordspacing
A.~C. Stickland and I.~Murray, ``Bert and pals: Projected attention layers for efficient adaptation in multi-task learning,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1902.02671}
\BIBentrySTDinterwordspacing

\bibitem[Meyerson and Miikkulainen(2017)]{soft-layer-ordering}
\BIBentryALTinterwordspacing
E.~Meyerson and R.~Miikkulainen, ``Beyond shared hierarchies: Deep multitask learning through soft layer ordering,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1711.00108}
\BIBentrySTDinterwordspacing

\bibitem[Ma et~al.(2018)Ma, Zhao, Yi, Chen, Hong, and Chi]{MMoE}
J.~Ma, Z.~Zhao, X.~Yi, J.~Chen, L.~Hong, and E.~H. Chi, ``Modeling task relationships in multi-task learning with multi-gate mixture-of-experts,'' \emph{Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 2018.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{MoE-layer}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean, ``Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,'' 2017.

\bibitem[Hazimeh et~al.(2021)Hazimeh, Zhao, Chowdhery, Sathiamoorthy, Chen, Mazumder, Hong, and Chi]{DSelect-k}
\BIBentryALTinterwordspacing
H.~Hazimeh, Z.~Zhao, A.~Chowdhery, M.~Sathiamoorthy, Y.~Chen, R.~Mazumder, L.~Hong, and E.~H. Chi, ``Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2106.03760}
\BIBentrySTDinterwordspacing

\bibitem[Fernando et~al.(2017)Fernando, Banarse, Blundell, Zwols, Ha, Rusu, Pritzel, and Wierstra]{path-net}
\BIBentryALTinterwordspacing
C.~Fernando, D.~Banarse, C.~Blundell, Y.~Zwols, D.~Ha, A.~A. Rusu, A.~Pritzel, and D.~Wierstra, ``Pathnet: Evolution channels gradient descent in super neural networks,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1701.08734}
\BIBentrySTDinterwordspacing

\bibitem[Liang et~al.(2018)Liang, Meyerson, and Miikkulainen]{evolutionary-architecture-search-CTR}
\BIBentryALTinterwordspacing
J.~Liang, E.~Meyerson, and R.~Miikkulainen, ``Evolutionary architecture search for deep multitask networks,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1803.03745}
\BIBentrySTDinterwordspacing

\bibitem[Gesmundo and Dean(2022)]{dynamic-introduction-of-tasks-in-large-scale}
\BIBentryALTinterwordspacing
A.~Gesmundo and J.~Dean, ``An evolutionary approach to dynamic introduction of tasks in large-scale multitask learning systems,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2205.12755}
\BIBentrySTDinterwordspacing

\bibitem[Maziarz et~al.(2019)Maziarz, Kokiopoulou, Gesmundo, Sbaiz, Bartok, and Berent]{flexible-mtl-by-learning-parameter-allocation}
\BIBentryALTinterwordspacing
K.~Maziarz, E.~Kokiopoulou, A.~Gesmundo, L.~Sbaiz, G.~Bartok, and J.~Berent, ``Flexible multi-task networks by learning parameter allocation,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1910.04915}
\BIBentrySTDinterwordspacing

\bibitem[Sun et~al.(2019)Sun, Panda, and Feris]{adashare}
\BIBentryALTinterwordspacing
X.~Sun, R.~Panda, and R.~S. Feris, ``Adashare: Learning what to share for efficient deep multi-task learning,'' \emph{CoRR}, vol. abs/1911.12423, 2019. [Online]. Available: \url{http://arxiv.org/abs/1911.12423}
\BIBentrySTDinterwordspacing

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{gumbel-softmax}
\BIBentryALTinterwordspacing
E.~Jang, S.~Gu, and B.~Poole, ``Categorical reparameterization with gumbel-softmax,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1611.01144}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2021)Zhang, Liu, and Guan]{AutoMTL}
\BIBentryALTinterwordspacing
L.~Zhang, X.~Liu, and H.~Guan, ``Automtl: A programming framework for automating efficient multi-task learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2110.13076}
\BIBentrySTDinterwordspacing

\bibitem[Bragman et~al.(2019)Bragman, Tanno, Ourselin, Alexander, and Cardoso]{stochastic-filter-groups}
\BIBentryALTinterwordspacing
F.~J.~S. Bragman, R.~Tanno, S.~Ourselin, D.~C. Alexander, and M.~J. Cardoso, ``Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1908.09597}
\BIBentrySTDinterwordspacing

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{variational-inference}
\BIBentryALTinterwordspacing
D.~M. Blei, A.~Kucukelbir, and J.~D. McAuliffe, ``Variational inference: A review for statisticians,'' \emph{Journal of the American Statistical Association}, vol. 112, no. 518, pp. 859--877, apr 2017. [Online]. Available: \url{https://doi.org/10.1080%2F01621459.2017.1285773}
\BIBentrySTDinterwordspacing

\bibitem[Lu et~al.(2016)Lu, Kumar, Zhai, Cheng, Javidi, and Feris]{fully-adaptive-feature-sharing}
\BIBentryALTinterwordspacing
Y.~Lu, A.~Kumar, S.~Zhai, Y.~Cheng, T.~Javidi, and R.~Feris, ``Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1611.05377}
\BIBentrySTDinterwordspacing

\bibitem[Vandenhende et~al.(2019)Vandenhende, Brabandere, and Gool]{branched-mtl}
\BIBentryALTinterwordspacing
S.~Vandenhende, B.~D. Brabandere, and L.~V. Gool, ``Branched multi-task networks: Deciding what layers to share,'' \emph{CoRR}, vol. abs/1904.02920, 2019. [Online]. Available: \url{http://arxiv.org/abs/1904.02920}
\BIBentrySTDinterwordspacing

\bibitem[Vu et~al.(2022)Vu, Zhou, Wen, Li, and Frahm]{synergistic-MTL-NAS}
\BIBentryALTinterwordspacing
T.~Vu, Y.~Zhou, C.~Wen, Y.~Li, and J.-M. Frahm, ``Toward edge-efficient dense predictions with synergistic multi-task neural architecture search,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2210.01384}
\BIBentrySTDinterwordspacing

\bibitem[Benmeziane et~al.(2021)Benmeziane, Maghraoui, Ouarnoughi, Niar, Wistuba, and Wang]{h-NAS-survey}
\BIBentryALTinterwordspacing
H.~Benmeziane, K.~E. Maghraoui, H.~Ouarnoughi, S.~Niar, M.~Wistuba, and N.~Wang, ``A comprehensive survey on hardware-aware neural architecture search,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2101.09336}
\BIBentrySTDinterwordspacing

\bibitem[Hu et~al.(2017{\natexlab{b}})Hu, Dey, Hebert, and Bagnell]{adaloss}
\BIBentryALTinterwordspacing
H.~Hu, D.~Dey, M.~Hebert, and J.~A. Bagnell, ``Learning anytime predictions in neural networks via adaptive loss balancing,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1708.06832}
\BIBentrySTDinterwordspacing

\bibitem[Liebel and Krner(2018)]{auxiliary-tasks-in-MTL}
\BIBentryALTinterwordspacing
L.~Liebel and M.~Krner, ``Auxiliary tasks in multi-task learning,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1805.06334}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Li, Kuang, Xue, Chen, Yang, Liao, and Zhang]{impartial-MTL}
\BIBentryALTinterwordspacing
L.~Liu, Y.~Li, Z.~Kuang, J.-H. Xue, Y.~Chen, W.~Yang, Q.~Liao, and W.~Zhang, ``Towards impartial multi-task learning,'' in \emph{International Conference on Learning Representations}, 2021. [Online]. Available: \url{https://openreview.net/forum?id=IMPnRXEWpvr}
\BIBentrySTDinterwordspacing

\bibitem[Chennupati et~al.(2019)Chennupati, Sistu, Yogamani, and Rawashdeh]{geometric}
\BIBentryALTinterwordspacing
S.~Chennupati, G.~Sistu, S.~Yogamani, and S.~A. Rawashdeh, ``Multinet++: Multi-stream feature aggregation and geometric loss strategy for multi-task learning,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1904.08492}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2018)Liu, Johns, and Davison]{MTAN}
\BIBentryALTinterwordspacing
S.~Liu, E.~Johns, and A.~J. Davison, ``End-to-end multi-task learning with attention,'' \emph{CoRR}, vol. abs/1803.10704, 2018. [Online]. Available: \url{http://arxiv.org/abs/1803.10704}
\BIBentrySTDinterwordspacing

\bibitem[Lin et~al.(2021)Lin, Ye, Zhang, and Tsang]{RLW-RLG}
\BIBentryALTinterwordspacing
B.~Lin, F.~Ye, Y.~Zhang, and I.~W. Tsang, ``Reasonable effectiveness of random weighting: A litmus test for multi-task learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2111.10603}
\BIBentrySTDinterwordspacing

\bibitem[Chen et~al.(2017)Chen, Badrinarayanan, Lee, and Rabinovich]{gradnorm}
\BIBentryALTinterwordspacing
Z.~Chen, V.~Badrinarayanan, C.-Y. Lee, and A.~Rabinovich, ``Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1711.02257}
\BIBentrySTDinterwordspacing

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and Anguelov]{GradDrop}
\BIBentryALTinterwordspacing
Z.~Chen, J.~Ngiam, Y.~Huang, T.~Luong, H.~Kretzschmar, Y.~Chai, and D.~Anguelov, ``Just pick a sign: Optimizing deep multitask models with gradient sign dropout,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2010.06808}
\BIBentrySTDinterwordspacing

\bibitem[Du et~al.(2018)Du, Czarnecki, Jayakumar, Farajtabar, Pascanu, and Lakshminarayanan]{adapting-aux-losses-using-gradient-similarity}
\BIBentryALTinterwordspacing
Y.~Du, W.~M. Czarnecki, S.~M. Jayakumar, M.~Farajtabar, R.~Pascanu, and B.~Lakshminarayanan, ``Adapting auxiliary losses using gradient similarity,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1812.02224}
\BIBentrySTDinterwordspacing

\bibitem[Suteu and Guo(2019)]{regularising-DMTL-orthogonal-gradients}
\BIBentryALTinterwordspacing
M.~Suteu and Y.~Guo, ``Regularizing deep multi-task networks using orthogonal gradients,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1912.06844}
\BIBentrySTDinterwordspacing

\bibitem[Yu et~al.(2020{\natexlab{a}})Yu, Kumar, Gupta, Levine, Hausman, and Finn]{gradient-surgery}
\BIBentryALTinterwordspacing
T.~Yu, S.~Kumar, A.~Gupta, S.~Levine, K.~Hausman, and C.~Finn, ``Gradient surgery for multi-task learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2001.06782}
\BIBentrySTDinterwordspacing

\bibitem[Wang et~al.(2021{\natexlab{c}})Wang, Tsvetkov, Firat, and Cao]{gradient-vaccine}
\BIBentryALTinterwordspacing
Z.~Wang, Y.~Tsvetkov, O.~Firat, and Y.~Cao, ``Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models,'' in \emph{International Conference on Learning Representations}, 2021. [Online]. Available: \url{https://openreview.net/forum?id=F1vEjWK-lH_}
\BIBentrySTDinterwordspacing

\bibitem[Javaloy and Valera(2021)]{rotograd}
\BIBentryALTinterwordspacing
A.~Javaloy and I.~Valera, ``Rotograd: Gradient homogenization in multitask learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.02631}
\BIBentrySTDinterwordspacing

\bibitem[Lin et~al.(2020)Lin, Yang, Zhang, and Kwong]{controllable-pareto-mtl}
\BIBentryALTinterwordspacing
X.~Lin, Z.~Yang, Q.~Zhang, and S.~Kwong, ``Controllable pareto multi-task learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2010.06313}
\BIBentrySTDinterwordspacing

\bibitem[Navon et~al.(2021)Navon, Shamsian, Chechik, and Fetaya]{navon2021learning}
A.~Navon, A.~Shamsian, G.~Chechik, and E.~Fetaya, ``Learning the pareto front with hypernetworks,'' 2021.

\bibitem[Dsidri(2012)]{MGDA_paper}
J.-A. Dsidri, ``Multiple-gradient descent algorithm (mgda) for multiobjective optimization,'' \emph{Comptes Rendus Mathematique}, vol. 350, p. 313318, 03 2012.

\bibitem[Sener and Koltun(2018)]{MTL-as-MOO}
\BIBentryALTinterwordspacing
O.~Sener and V.~Koltun, ``Multi-task learning as multi-objective optimization,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1810.04650}
\BIBentrySTDinterwordspacing

\bibitem[Jaggi(2013)]{Frank-Wolfe}
\BIBentryALTinterwordspacing
M.~Jaggi, ``Revisiting {Frank-Wolfe}: Projection-free sparse convex optimization,'' in \emph{Proceedings of the 30th International Conference on Machine Learning}, ser. Proceedings of Machine Learning Research, S.~Dasgupta and D.~McAllester, Eds., vol.~28, no.~1.\hskip 1em plus 0.5em minus 0.4em\relax Atlanta, Georgia, USA: PMLR, 17--19 Jun 2013, pp. 427--435. [Online]. Available: \url{https://proceedings.mlr.press/v28/jaggi13.html}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2021{\natexlab{c}})Liu, Liu, Jin, Stone, and Liu]{conflict-averse-gradient-descent-for-mtl}
\BIBentryALTinterwordspacing
B.~Liu, X.~Liu, X.~Jin, P.~Stone, and Q.~Liu, ``Conflict-averse gradient descent for multi-task learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2110.14048}
\BIBentrySTDinterwordspacing

\bibitem[Lin et~al.(2019)Lin, Zhen, Li, Zhang, and Kwong]{Pareto-MTL}
\BIBentryALTinterwordspacing
X.~Lin, H.-L. Zhen, Z.~Li, Q.~Zhang, and S.~Kwong, ``Pareto multi-task learning,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1912.12854}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2014)Liu, Gu, and Zhang]{mmo-decomposition}
H.-L. Liu, F.~Gu, and Q.~Zhang, ``Decomposition of a multiobjective optimization problem into a number of simple multiobjective subproblems,'' \emph{IEEE Transactions on Evolutionary Computation}, vol.~18, no.~3, pp. 450--455, 2014.

\bibitem[Ma et~al.(2020)Ma, Du, and Matusik]{continuous-pareto-exploration-in-MTL}
\BIBentryALTinterwordspacing
P.~Ma, T.~Du, and W.~Matusik, ``Efficient continuous pareto exploration in multi-task learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2006.16434}
\BIBentrySTDinterwordspacing

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{HyperNetworks}
\BIBentryALTinterwordspacing
D.~Ha, A.~Dai, and Q.~V. Le, ``Hypernetworks,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1609.09106}
\BIBentrySTDinterwordspacing

\bibitem[Momma et~al.(2022)Momma, Dong, and Liu]{multi-objective-multi-task-learning-framework-induced-by-pareto}
\BIBentryALTinterwordspacing
M.~Momma, C.~Dong, and J.~Liu, ``A multi-objective / multi-task learning framework induced by pareto stationarity,'' in \emph{Proceedings of the 39th International Conference on Machine Learning}, ser. Proceedings of Machine Learning Research, K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and S.~Sabato, Eds., vol. 162.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 17--23 Jul 2022, pp. 15\,895--15\,907. [Online]. Available: \url{https://proceedings.mlr.press/v162/momma22a.html}
\BIBentrySTDinterwordspacing

\bibitem[Navon et~al.(2022)Navon, Shamsian, Achituve, Maron, Kawaguchi, Chechik, and Fetaya]{bargaining-game}
\BIBentryALTinterwordspacing
A.~Navon, A.~Shamsian, I.~Achituve, H.~Maron, K.~Kawaguchi, G.~Chechik, and E.~Fetaya, ``Multi-task learning as a bargaining game,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2202.01017}
\BIBentrySTDinterwordspacing

\bibitem[Nash(1953)]{Nash-bargaining-solution}
\BIBentryALTinterwordspacing
J.~Nash, ``Two-person cooperative games,'' \emph{Econometrica}, vol.~21, no.~1, pp. 128--140, 1953. [Online]. Available: \url{http://www.jstor.org/stable/1906951}
\BIBentrySTDinterwordspacing

\bibitem[Xin et~al.(2022)Xin, Ghorbani, Gilmer, Garg, and Firat]{even-help}
\BIBentryALTinterwordspacing
D.~Xin, B.~Ghorbani, J.~Gilmer, A.~Garg, and O.~Firat, ``Do current multi-task optimization methods in deep learning even help?'' in \emph{Advances in Neural Information Processing Systems}, A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, Eds., 2022. [Online]. Available: \url{https://openreview.net/forum?id=A2Ya5aLtyuG}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2014)Zhang, Luo, Loy, and Tang]{task-wise-early-stopping}
Z.~Zhang, P.~Luo, C.~C. Loy, and X.~Tang, ``Facial landmark detection by deep multi-task learning,'' 09 2014.

\bibitem[Lu et~al.(2019{\natexlab{a}})Lu, Goswami, Rohrbach, Parikh, and Lee]{12-in-1}
\BIBentryALTinterwordspacing
J.~Lu, V.~Goswami, M.~Rohrbach, D.~Parikh, and S.~Lee, ``12-in-1: Multi-task vision and language representation learning,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1912.02315}
\BIBentrySTDinterwordspacing

\bibitem[Li et~al.(2016)Li, Yan, Wei, Dong, Liu, and Zha]{self-paced-MTL}
\BIBentryALTinterwordspacing
C.~Li, J.~Yan, F.~Wei, W.~Dong, Q.~Liu, and H.~Zha, ``Self-paced multi-task learning,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1604.01474}
\BIBentrySTDinterwordspacing

\bibitem[Guo et~al.(2018)Guo, Haque, Huang, Yeung, and Fei-Fei]{dynamic-task-prio}
M.~Guo, A.~Haque, D.-A. Huang, S.~Yeung, and L.~Fei-Fei, ``Dynamic task prioritization for multitask learning,'' in \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, September 2018.

\bibitem[Lin et~al.(2017)Lin, Goyal, Girshick, He, and Dollr]{focal-loss}
\BIBentryALTinterwordspacing
T.-Y. Lin, P.~Goyal, R.~Girshick, K.~He, and P.~Dollr, ``Focal loss for dense object detection,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1708.02002}
\BIBentrySTDinterwordspacing

\bibitem[Sharma et~al.(2017)Sharma, Jha, Hegde, and Ravindran]{learning-to-MTL-active-sampling}
\BIBentryALTinterwordspacing
S.~Sharma, A.~Jha, P.~Hegde, and B.~Ravindran, ``Learning to multi-task by active sampling,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1702.06053}
\BIBentrySTDinterwordspacing

\bibitem[Kang et~al.(2011)Kang, Grauman, and Sha]{learning-with-whom-to-share}
Z.~Kang, K.~Grauman, and F.~Sha, ``Learning with whom to share in multi-task feature learning,'' in \emph{Proceedings of the 28th International Conference on International Conference on Machine Learning}, ser. ICML'11.\hskip 1em plus 0.5em minus 0.4em\relax Madison, WI, USA: Omnipress, 2011, p. 521528.

\bibitem[Long et~al.(2017)Long, Cao, Wang, and Yu]{MRN}
M.~Long, Z.~Cao, J.~Wang, and P.~S. Yu, ``Learning multiple tasks with multilinear relationship networks,'' in \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, ser. NIPS'17.\hskip 1em plus 0.5em minus 0.4em\relax Red Hook, NY, USA: Curran Associates Inc., 2017, p. 15931602.

\bibitem[Zhang and Yeung(2014)]{MTRL}
y.~Zhang and D.-Y. Yeung, ``A regularization approach to learning task relationships in multitask learning,'' \emph{ACM Transactions on Knowledge Discovery from Data}, vol.~8, pp. 1--31, 06 2014.

\bibitem[Ohlson et~al.(2013)Ohlson, {Rauf Ahmad}, and {von Rosen}]{tensor-normal-distribution}
\BIBentryALTinterwordspacing
M.~Ohlson, M.~{Rauf Ahmad}, and D.~{von Rosen}, ``The multilinear normal distribution: Introduction and some basic properties,'' \emph{Journal of Multivariate Analysis}, vol. 113, pp. 37--47, 2013, special Issue on Multivariate Distribution Theory in Memory of Samuel Kotz. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S0047259X11001047}
\BIBentrySTDinterwordspacing

\bibitem[Dwivedi and Roig(2019)]{RSA-taxonomy}
\BIBentryALTinterwordspacing
K.~Dwivedi and G.~Roig, ``Representation similarity analysis for efficient task taxonomy and amp; transfer learning,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1904.11740}
\BIBentrySTDinterwordspacing

\bibitem[Standley et~al.(2019)Standley, Zamir, Chen, Guibas, Malik, and Savarese]{which-tasks-should-be-learned-together}
\BIBentryALTinterwordspacing
T.~Standley, A.~R. Zamir, D.~Chen, L.~Guibas, J.~Malik, and S.~Savarese, ``Which tasks should be learned together in multi-task learning?'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1905.07553}
\BIBentrySTDinterwordspacing

\bibitem[Fifty et~al.(2021)Fifty, Amid, Zhao, Yu, Anil, and Finn]{efficiently-identifying-task-groupings-for-MTL}
\BIBentryALTinterwordspacing
C.~Fifty, E.~Amid, Z.~Zhao, T.~Yu, R.~Anil, and C.~Finn, ``Efficiently identifying task groupings for multi-task learning,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2109.04617}
\BIBentrySTDinterwordspacing

\bibitem[Doersch and Zisserman(2017)]{MT-self-supervised-VL}
\BIBentryALTinterwordspacing
C.~Doersch and A.~Zisserman, ``Multi-task self-supervised visual learning,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1708.07860}
\BIBentrySTDinterwordspacing

\bibitem[Doersch et~al.(2015)Doersch, Gupta, and Efros]{relative-position}
\BIBentryALTinterwordspacing
C.~Doersch, A.~Gupta, and A.~A. Efros, ``Unsupervised visual representation learning by context prediction,'' 2015. [Online]. Available: \url{https://arxiv.org/abs/1505.05192}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2016)Zhang, Isola, and Efros]{colorization}
\BIBentryALTinterwordspacing
R.~Zhang, P.~Isola, and A.~A. Efros, ``Colorful image colorization,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1603.08511}
\BIBentrySTDinterwordspacing

\bibitem[Dosovitskiy et~al.(2014)Dosovitskiy, Fischer, Springenberg, Riedmiller, and Brox]{examplar-task}
\BIBentryALTinterwordspacing
A.~Dosovitskiy, P.~Fischer, J.~T. Springenberg, M.~Riedmiller, and T.~Brox, ``Discriminative unsupervised feature learning with exemplar convolutional neural networks,'' 2014. [Online]. Available: \url{https://arxiv.org/abs/1406.6909}
\BIBentrySTDinterwordspacing

\bibitem[Pathak et~al.(2016{\natexlab{a}})Pathak, Girshick, Dollr, Darrell, and Hariharan]{motion-segmentation}
\BIBentryALTinterwordspacing
D.~Pathak, R.~Girshick, P.~Dollr, T.~Darrell, and B.~Hariharan, ``Learning features by watching objects move,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1612.06370}
\BIBentrySTDinterwordspacing

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{ImageNet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A large-scale hierarchical image database,'' in \emph{2009 IEEE Conference on Computer Vision and Pattern Recognition}, 2009, pp. 248--255.

\bibitem[Lee et~al.(2019)Lee, Na, and Kim]{recycling-bounding-box-annotations}
W.~Lee, J.~Na, and G.~Kim, ``Multi-task self-supervised object detection via recycling of bounding box annotations,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2019.

\bibitem[Cho et~al.(2018)Cho, Kim, Jung, Oh, Youn, and Sohn]{self-supervised-monocular-road-segmentation}
J.~Cho, Y.~Kim, H.~Jung, C.~Oh, J.~Youn, and K.~Sohn, ``Multi-task self-supervised visual representation learning for monocular road segmentation,'' in \emph{2018 IEEE International Conference on Multimedia and Expo (ICME)}, 2018, pp. 1--6.

\bibitem[Geiger et~al.(2012)Geiger, Lenz, and Urtasun]{KITTI}
A.~Geiger, P.~Lenz, and R.~Urtasun, ``Are we ready for autonomous driving? the kitti vision benchmark suite,'' in \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2012.

\bibitem[Hirschmuller(2005)]{SGM}
H.~Hirschmuller, ``Accurate and efficient stereo processing by semi-global matching and mutual information,'' in \emph{2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, vol.~2, 2005, pp. 807--814 vol. 2.

\bibitem[Hernandez-Juarez et~al.(2016)Hernandez-Juarez, Espinosa, Vzquez, Lpez, and Moure]{Stixel-world}
\BIBentryALTinterwordspacing
D.~Hernandez-Juarez, A.~Espinosa, D.~Vzquez, A.~M. Lpez, and J.~C. Moure, ``Gpu-accelerated real-time stixel computation,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1610.04124}
\BIBentrySTDinterwordspacing

\bibitem[Silberman et~al.(2012)Silberman, Hoiem, Kohli, and Fergus]{surface-normal-unsupervised}
N.~Silberman, D.~Hoiem, P.~Kohli, and R.~Fergus, ``Indoor segmentation and support inference from rgbd images,'' vol. 7576, 10 2012, pp. 746--760.

\bibitem[Pfister et~al.(2021)Pfister, Kobs, and Hotho]{self-supervised-image-aesthetic-assessment}
J.~Pfister, K.~Kobs, and A.~Hotho, ``Self-supervised multi-task pretraining improves image aesthetic assessment,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, June 2021, pp. 816--825.

\bibitem[Georgescu et~al.(2020)Georgescu, Barbalau, Ionescu, Khan, Popescu, and Shah]{anomaly-detection}
\BIBentryALTinterwordspacing
M.-I. Georgescu, A.~Barbalau, R.~T. Ionescu, F.~S. Khan, M.~Popescu, and M.~Shah, ``Anomaly detection in video via self-supervised and multi-task learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2011.07491}
\BIBentrySTDinterwordspacing

\bibitem[Barbalau et~al.(2022)Barbalau, Ionescu, Georgescu, Dueholm, Ramachandra, Nasrollahi, Khan, Moeslund, and Shah]{SSMTL++}
\BIBentryALTinterwordspacing
A.~Barbalau, R.~T. Ionescu, M.-I. Georgescu, J.~Dueholm, B.~Ramachandra, K.~Nasrollahi, F.~S. Khan, T.~B. Moeslund, and M.~Shah, ``Ssmtl++: Revisiting self-supervised multi-task learning for video anomaly detection,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2207.08003}
\BIBentrySTDinterwordspacing

\bibitem[Lu et~al.(2019{\natexlab{b}})Lu, Batra, Parikh, and Lee]{Vilbert}
\BIBentryALTinterwordspacing
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee, ``Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1908.02265}
\BIBentrySTDinterwordspacing

\bibitem[Vasudevan et~al.(2022)Vasudevan, Dai, and Van~Gool]{sound-and-visual-rep-learning}
\BIBentryALTinterwordspacing
A.~B. Vasudevan, D.~Dai, and L.~Van~Gool, ``Sound and visual representation learning with multiple pretraining tasks,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2201.01046}
\BIBentrySTDinterwordspacing

\bibitem[Bachmann et~al.(2022)Bachmann, Mizrahi, Atanov, and Zamir]{MultiMAE}
\BIBentryALTinterwordspacing
R.~Bachmann, D.~Mizrahi, A.~Atanov, and A.~Zamir, ``Multimae: Multi-modal multi-task masked autoencoders,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2204.01678}
\BIBentrySTDinterwordspacing

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Dollr, and Girshick]{MAE}
\BIBentryALTinterwordspacing
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Dollr, and R.~Girshick, ``Masked autoencoders are scalable vision learners,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2111.06377}
\BIBentrySTDinterwordspacing

\bibitem[Ranftl et~al.(2021)Ranftl, Bochkovskiy, and Koltun]{ViT-for-dense-prediction}
\BIBentryALTinterwordspacing
R.~Ranftl, A.~Bochkovskiy, and V.~Koltun, ``Vision transformers for dense prediction,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.13413}
\BIBentrySTDinterwordspacing

\bibitem[Cheng et~al.(2022)Cheng, Misra, Schwing, Kirillov, and Girdhar]{Mask2Former}
B.~Cheng, I.~Misra, A.~G. Schwing, A.~Kirillov, and R.~Girdhar, ``Masked-attention mask transformer for universal image segmentation,'' in \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 1280--1289.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Zitnick, and Dollr]{COCO}
\BIBentryALTinterwordspacing
T.-Y. Lin, M.~Maire, S.~Belongie, L.~Bourdev, R.~Girshick, J.~Hays, P.~Perona, D.~Ramanan, C.~L. Zitnick, and P.~Dollr, ``Microsoft coco: Common objects in context,'' 2014. [Online]. Available: \url{https://arxiv.org/abs/1405.0312}
\BIBentrySTDinterwordspacing

\bibitem[Tu et~al.(2022)Tu, Zhou, Zou, and Zhang]{retinal-vessel-segmentation}
\BIBentryALTinterwordspacing
Z.~Tu, Q.~Zhou, H.~Zou, and X.~Zhang, ``A multi-task dense network with self-supervised learning for retinal vessel segmentation,'' \emph{Electronics}, vol.~11, no.~21, 2022. [Online]. Available: \url{https://www.mdpi.com/2079-9292/11/21/3538}
\BIBentrySTDinterwordspacing

\bibitem[Liao et~al.(2022)Liao, Xiong, Wang, Mo, Li, Liu, Chen, Huang, and Dou]{MUSCLE}
W.~Liao, H.~Xiong, Q.~Wang, Y.~Mo, X.~Li, Y.~Liu, Z.~Chen, S.~Huang, and D.~Dou, ``Muscle: Multi-task self-supervised continual learning topre-train deep models forx-ray images ofmultiple body parts,'' in \emph{Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022}, L.~Wang, Q.~Dou, P.~T. Fletcher, S.~Speidel, and S.~Li, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Cham: Springer Nature Switzerland, 2022, pp. 151--161.

\bibitem[Chaves et~al.(2021)Chaves, Bissoto, Valle, and Avila]{skin-lesion}
\BIBentryALTinterwordspacing
L.~Chaves, A.~Bissoto, E.~Valle, and S.~Avila, ``An evaluation of self-supervised pre-training for skin-lesion analysis,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2106.09229}
\BIBentrySTDinterwordspacing

\bibitem[Wu et~al.(2021)Wu, Kao, Tang, Sun, McFee, Bello, and Wang]{music-classification}
\BIBentryALTinterwordspacing
H.-H. Wu, C.-C. Kao, Q.~Tang, M.~Sun, B.~McFee, J.~P. Bello, and C.~Wang, ``Multi-task self-supervised pre-training for music classification,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2102.03229}
\BIBentrySTDinterwordspacing

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Weng, Xia, He, Sun, and Li]{multilingual-reserved-dictionary}
\BIBentryALTinterwordspacing
B.~Li, Y.~Weng, F.~Xia, S.~He, B.~Sun, and S.~Li, ``{L}ing{J}ing at {S}em{E}val-2022 task 1: Multi-task self-supervised pre-training for multilingual reverse dictionary,'' in \emph{Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)}.\hskip 1em plus 0.5em minus 0.4em\relax Seattle, United States: Association for Computational Linguistics, Jul. 2022, pp. 29--35. [Online]. Available: \url{https://aclanthology.org/2022.semeval-1.4}
\BIBentrySTDinterwordspacing

\bibitem[Wang et~al.(2009)Wang, Wang, and Li]{task-regularizations}
F.~Wang, X.~Wang, and T.~Li, ``Semi-supervised multi-task learning with task regularizations,'' in \emph{2009 Ninth IEEE International Conference on Data Mining}, 2009, pp. 562--568.

\bibitem[Jiang et~al.(2017)Jiang, Learned-Miller, Larsson, Maire, and Shakhnarovich]{urban-scene-understanding}
\BIBentryALTinterwordspacing
H.~Jiang, E.~Learned-Miller, G.~Larsson, M.~Maire, and G.~Shakhnarovich, ``Self-supervised relative depth learning for urban scene understanding,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1712.04850}
\BIBentrySTDinterwordspacing

\bibitem[Klingner et~al.(2020)Klingner, Termhlen, Mikolajczyk, and Fingscheidt]{dynamic-object-problem}
\BIBentryALTinterwordspacing
M.~Klingner, J.-A. Termhlen, J.~Mikolajczyk, and T.~Fingscheidt, ``Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2007.06936}
\BIBentrySTDinterwordspacing

\bibitem[Novosel(2019)]{boostinc-semantic}
J.~Novosel, ``Boosting semantic segmentation with multi-task self-supervised learning for autonomous driving applications,'' 2019.

\bibitem[Hoyer et~al.(2020)Hoyer, Dai, Chen, Kring, Saha, and Van~Gool]{three-ways}
\BIBentryALTinterwordspacing
L.~Hoyer, D.~Dai, Y.~Chen, A.~Kring, S.~Saha, and L.~Van~Gool, ``Three ways to improve semantic segmentation with self-supervised depth estimation,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2012.10782}
\BIBentrySTDinterwordspacing

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{CutMix}
\BIBentryALTinterwordspacing
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo, ``Cutmix: Regularization strategy to train strong classifiers with localizable features,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1905.04899}
\BIBentrySTDinterwordspacing

\bibitem[Olsson et~al.(2020)Olsson, Tranheden, Pinto, and Svensson]{ClassMix}
\BIBentryALTinterwordspacing
V.~Olsson, W.~Tranheden, J.~Pinto, and L.~Svensson, ``Classmix: Segmentation-based data augmentation for semi-supervised learning,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2007.07936}
\BIBentrySTDinterwordspacing

\bibitem[Tarvainen and Valpola(2017)]{mean-teacher-algorithm}
\BIBentryALTinterwordspacing
A.~Tarvainen and H.~Valpola, ``Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1703.01780}
\BIBentrySTDinterwordspacing

\bibitem[Hoyer et~al.(2021)Hoyer, Dai, Wang, Chen, and Van~Gool]{semi-supervised-domain-adaptive}
\BIBentryALTinterwordspacing
L.~Hoyer, D.~Dai, Q.~Wang, Y.~Chen, and L.~Van~Gool, ``Improving semi-supervised and domain-adaptive semantic segmentation with self-supervised depth estimation,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2108.12545}
\BIBentrySTDinterwordspacing

\bibitem[Gao et~al.(2023)Gao, Khamesra, Kumbhar, and Aglawe]{MTL-for-image-segmentation-task}
\BIBentryALTinterwordspacing
L.~Gao, C.~Khamesra, U.~Kumbhar, and A.~Aglawe, ``Multi-task self-supervised learning for image segmentation task,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2302.02483}
\BIBentrySTDinterwordspacing

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{GANs}
\BIBentryALTinterwordspacing
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair, A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' 2014. [Online]. Available: \url{https://arxiv.org/abs/1406.2661}
\BIBentrySTDinterwordspacing

\bibitem[Donahue et~al.(2016)Donahue, Krhenbhl, and Darrell]{adversarial-feature-learning}
\BIBentryALTinterwordspacing
J.~Donahue, P.~Krhenbhl, and T.~Darrell, ``Adversarial feature learning,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1605.09782}
\BIBentrySTDinterwordspacing

\bibitem[Hung et~al.(2018)Hung, Tsai, Liou, Lin, and Yang]{adver-learning-semantic-semi-supervised}
\BIBentryALTinterwordspacing
W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H. Yang, ``Adversarial learning for semi-supervised semantic segmentation,'' 2018. [Online]. Available: \url{https://arxiv.org/abs/1802.07934}
\BIBentrySTDinterwordspacing

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Li, Zhang, and Ma]{robust-adver-learning-for-semantic-semi}
J.~Zhang, Z.~Li, C.~Zhang, and H.~Ma, ``Robust adversarial learning for semi-supervised semantic segmentation,'' in \emph{2020 IEEE International Conference on Image Processing (ICIP)}, 2020, pp. 728--732.

\bibitem[Hadsell et~al.(2006)Hadsell, Chopra, and LeCun]{original-contrastive}
R.~Hadsell, S.~Chopra, and Y.~LeCun, ``Dimensionality reduction by learning an invariant mapping,'' in \emph{2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, vol.~2, 2006, pp. 1735--1742.

\bibitem[He et~al.(2019)He, Fan, Wu, Xie, and Girshick]{Moco}
\BIBentryALTinterwordspacing
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for unsupervised visual representation learning,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1911.05722}
\BIBentrySTDinterwordspacing

\bibitem[Pathak et~al.(2016{\natexlab{b}})Pathak, Krahenbuhl, Donahue, Darrell, and Efros]{InfoNCE}
\BIBentryALTinterwordspacing
D.~Pathak, P.~Krahenbuhl, J.~Donahue, T.~Darrell, and A.~A. Efros, ``Context encoders: Feature learning by inpainting,'' 2016. [Online]. Available: \url{https://arxiv.org/abs/1604.07379}
\BIBentrySTDinterwordspacing

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and Hinton]{SimCLR}
\BIBentryALTinterwordspacing
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for contrastive learning of visual representations,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2002.05709}
\BIBentrySTDinterwordspacing

\bibitem[Nathan~Silberman and Fergus(2012)]{NYUv2}
P.~K. Nathan~Silberman, Derek~Hoiem and R.~Fergus, ``Indoor segmentation and support inference from rgbd images,'' in \emph{ECCV}, 2012.

\bibitem[Bilinski and Prisacariu(2018)]{8578788}
P.~Bilinski and V.~Prisacariu, ``Dense decoder shortcut connections for single-pass semantic segmentation,'' in \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2018, pp. 6596--6605.

\bibitem[Yu et~al.(2020{\natexlab{b}})Yu, Gao, Zhou, Zhang, and Wu]{yu2020multilayer}
L.~Yu, Y.~Gao, J.~Zhou, J.~Zhang, and Q.~Wu, ``Multi-layer feature aggregation for deep scene parsing models,'' 2020.

\bibitem[Borse et~al.(2021)Borse, Wang, Zhang, and Porikli]{borse2021inverseform}
S.~Borse, Y.~Wang, Y.~Zhang, and F.~Porikli, ``Inverseform: A loss function for structured boundary-aware segmentation,'' 2021.

\bibitem[Piccinelli et~al.(2024)Piccinelli, Yang, Sakaridis, Segu, Li, Gool, and Yu]{unidepth}
L.~Piccinelli, Y.-H. Yang, C.~Sakaridis, M.~Segu, S.~Li, L.~V. Gool, and F.~Yu, ``Unidepth: Universal monocular metric depth estimation,'' 2024.

\bibitem[Yang et~al.(2024)Yang, Kang, Huang, Xu, Feng, and Zhao]{depth-anything}
L.~Yang, B.~Kang, Z.~Huang, X.~Xu, J.~Feng, and H.~Zhao, ``Depth anything: Unleashing the power of large-scale unlabeled data,'' 2024.

\bibitem[Hickson et~al.(2019)Hickson, Raveendran, Fathi, Murphy, and Essa]{hickson2019floors}
S.~Hickson, K.~Raveendran, A.~Fathi, K.~Murphy, and I.~Essa, ``Floors are flat: Leveraging semantics for real-time surface normal prediction,'' 2019.

\bibitem[Bae et~al.(2021)Bae, Budvytis, and Cipolla]{bae2021estimating}
G.~Bae, I.~Budvytis, and R.~Cipolla, ``Estimating and exploiting the aleatoric uncertainty in surface normal estimation,'' 2021.

\bibitem[Piccinelli et~al.(2023)Piccinelli, Sakaridis, and Yu]{piccinelli2023idisc}
L.~Piccinelli, C.~Sakaridis, and F.~Yu, ``idisc: Internal discretization for monocular depth estimation,'' 2023.

\bibitem[Zhou et~al.(2020)Zhou, Cui, Xu, Zhang, Wang, Zhang, and Yang]{PSD}
\BIBentryALTinterwordspacing
L.~Zhou, Z.~Cui, C.~Xu, Z.~Zhang, C.~Wang, T.~Zhang, and J.~Yang, ``Pattern-structure diffusion for multi-task learning,'' in \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}.\hskip 1em plus 0.5em minus 0.4em\relax Los Alamitos, CA, USA: IEEE Computer Society, jun 2020, pp. 4513--4522. [Online]. Available: \url{https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00457}
\BIBentrySTDinterwordspacing

\bibitem[Ye and Xu(2023)]{taskprompter}
\BIBentryALTinterwordspacing
H.~Ye and D.~Xu, ``Taskprompter: Spatial-channel multi-task prompting for dense scene understanding,'' in \emph{The Eleventh International Conference on Learning Representations}, 2023. [Online]. Available: \url{https://openreview.net/forum?id=-CwPopPJda}
\BIBentrySTDinterwordspacing

\bibitem[Xu et~al.(2023)Xu, Yang, and Zhang]{DeMT}
Y.~Xu, Y.~Yang, and L.~Zhang, ``Demt: Deformable mixer transformer for multi-task learning of dense prediction,'' 2023.

\bibitem[Li et~al.(2021)Li, Hu, Sun, Zhao, Zhang, and Lin]{MTSS}
Y.~Li, J.~Hu, J.~Sun, S.~Zhao, Q.~Zhang, and Y.~Lin, ``A novel multi-task self-supervised representation learning paradigm,'' in \emph{2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID)}, 2021, pp. 94--99.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and Valko]{BYOL}
J.-B. Grill, F.~Strub, F.~Altch, C.~Tallec, P.~H. Richemond, E.~Buchatskaya, C.~Doersch, B.~A. Pires, Z.~D. Guo, M.~G. Azar, B.~Piot, K.~Kavukcuoglu, R.~Munos, and M.~Valko, ``Bootstrap your own latent: A new approach to self-supervised learning,'' 2020.

\bibitem[Zamir et~al.(2020)Zamir, Sax, Yeo, Kar, Cheerla, Suri, Cao, Malik, and Guibas]{X-task-consistency}
\BIBentryALTinterwordspacing
A.~Zamir, A.~Sax, T.~Yeo, O.~Kar, N.~Cheerla, R.~Suri, Z.~Cao, J.~Malik, and L.~Guibas, ``Robust learning through cross-task consistency,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2006.04096}
\BIBentrySTDinterwordspacing

\bibitem[Perez et~al.(2017)Perez, Strub, de~Vries, Dumoulin, and Courville]{film}
\BIBentryALTinterwordspacing
E.~Perez, F.~Strub, H.~de~Vries, V.~Dumoulin, and A.~Courville, ``Film: Visual reasoning with a general conditioning layer,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1709.07871}
\BIBentrySTDinterwordspacing

\bibitem[McCann et~al.(2018)McCann, Keskar, Xiong, and Socher]{natural-language-decathlon}
B.~McCann, N.~S. Keskar, C.~Xiong, and R.~Socher, ``The natural language decathlon: Multitask learning as question answering,'' 2018.

\bibitem[Simard and Lagrange(2021)]{improving-fsl-with-self-pretext-tasks}
N.~Simard and G.~Lagrange, ``Improving few-shot learning with auxiliary self-supervised pretext tasks,'' 2021.

\bibitem[Everingham et~al.(2010)Everingham, Van~Gool, Williams, Winn, and Zisserman]{PASCAL}
M.~Everingham, L.~Van~Gool, C.~K.~I. Williams, J.~Winn, and A.~Zisserman, ``The pascal visual object classes (voc) challenge,'' \emph{International Journal of Computer Vision}, vol.~88, no.~2, pp. 303--338, Jun. 2010.

\bibitem[Kim et~al.(2023)Kim, Kim, Cho, Luo, and Hong]{visual-token-matching}
\BIBentryALTinterwordspacing
D.~Kim, J.~Kim, S.~Cho, C.~Luo, and S.~Hong, ``Universal few-shot learning of dense prediction tasks with visual token matching,'' in \emph{The Eleventh International Conference on Learning Representations}, 2023. [Online]. Available: \url{https://openreview.net/forum?id=88nT0j5jAn}
\BIBentrySTDinterwordspacing

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson, Franke, Roth, and Schiele]{cityscapes}
M.~Cordts, M.~Omran, S.~Ramos, T.~Rehfeld, M.~Enzweiler, R.~Benenson, U.~Franke, S.~Roth, and B.~Schiele, ``The cityscapes dataset for semantic urban scene understanding,'' 2016.

\bibitem[Alvarez et~al.(2012)Alvarez, Gevers, Lecun, and Lpez]{3-class-kitti}
J.~M. Alvarez, T.~Gevers, Y.~Lecun, and A.~Lpez, ``Road scene segmentation from a single image,'' 10 2012.

\bibitem[Zhang et~al.(2015)Zhang, Candra, Vetter, and Zakhor]{10-class-kitti}
R.~Zhang, S.~A. Candra, K.~Vetter, and A.~Zakhor, ``Sensor fusion for semantic segmentation of urban scenes,'' in \emph{2015 IEEE International Conference on Robotics and Automation (ICRA)}, 2015, pp. 1850--1857.

\bibitem[Ros et~al.(2015)Ros, Ramos, Granados, Bakhtiary, Vazquez, and Lopez]{11-class-kitti}
G.~Ros, S.~Ramos, M.~Granados, A.~Bakhtiary, D.~Vazquez, and A.~M. Lopez, ``Vision-based offline-online perception paradigm for autonomous driving,'' in \emph{2015 IEEE Winter Conference on Applications of Computer Vision}, 2015, pp. 231--238.

\bibitem[Hong et~al.(2023)Hong, Wang, Sun, and Pan]{hong2023minimalist}
Y.~Hong, J.~Wang, W.~Sun, and H.~Pan, ``Minimalist and high-performance semantic segmentation with plain vision transformers,'' 2023.

\bibitem[Lin and Zhang(2022)]{LibMTL}
B.~Lin and Y.~Zhang, ``Libmtl: A python library for multi-task learning,'' 2022.

\end{thebibliography}




%\end{thebibliography}
% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{% Figure removed}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission
\begin{IEEEbiography}[{% Figure removed}]{Maxime Fontana}
received his BSc and his MSc in Computer Science from the University of Sheffield. He is currently a Ph.D. candidate from King's College London, United Kingdom. His research interests include computer vision, machine learning, real-time rendering, scene understanding and autonomous driving. His current research involves the development of innovative, more data-efficient multi-task learning systems.
\end{IEEEbiography}
\begin{IEEEbiography}[{% Figure removed}]{Michael Spratling}'s research is concerned with understanding the computational and neural mechanisms underlying visual perception, and developing biologically-inspired neural networks to solve problems in computer vision and machine learning. He has a multidisciplinary background having trained and held posts in engineering, psychology, and computer science at various universities (Loughborough, Edinburgh, St Andrews, Cambridge, Birkbeck, and King's College London). He is currently a researcher in the Department of Behavioural and Cognitive Sciences at the University of Luxembourg.
\end{IEEEbiography}
\begin{IEEEbiography}[{% Figure removed}]{Miaojing Shi}
(Senior Member, IEEE) received the Ph.D. degree from Peking University in 2015. He also engaged with a joint Ph.D. program with the University of Oxford and INRIA Rennes for a year. He held a postdoctoral position at the University of Edinburgh and was a Research Scientist at INRIA Rennes. Between 2020 and 2022, he has been a Lecturer/Senior Lectuer with the Department of Informatics, King's College London. Since 2023, he becomes a Full Professor at Tongji University and a visiting Senior Lecturer at King's. He has authored or co-authored over 70 papers in prestigious journals such as IEEE Transactions on Pattern Analysis and Machine Intelligence and Proceedings of the IEEE, as well as top AI conferences including CVPR, ICCV, NeurIPS, among others. His current research focus is on visual learning with few data, vision-language learning and medical imaging analysis.
\end{IEEEbiography}

%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}
%\end{IEEEbiographynophoto}

%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\end{document}


