% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
reprint,
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose,
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
% amsmath,amssymb,
aps,
pre,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}
% \usepackage{dblfloatfix}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{amsfonts}
\usepackage{float}
% \usepackage{lipsum}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{subcaption}
% \usepackage{caption}
\usepackage{comment}
\usepackage{acronym}
% for shapes:
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}

\usepackage{color}

\newacro{PE}{participation entropy}
\newacro{PC}{participation coefficient}

% Define macros for figures and equations:
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\eq}[1]{Eq.~(\ref{eq:#1})}


%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}

% \preprint{APS/123-QED}

%\title{Participation Entropy: An information theoretic formulation of the participation coefficient.}
\title{On the information-theoretic formulation of network participation}

    \author{Pavle Cajic}
    \affiliation{School of Physics, Faculty of Science, The University of Sydney, Sydney NSW 2006, Australia}

    \author{Dominic Agius}
    \affiliation{School of Physics, Faculty of Science, The University of Sydney, Sydney NSW 2006, Australia}

    \author{Oliver M. Cliff}
    \affiliation{School of Physics, Faculty of Science, The University of Sydney, Sydney NSW 2006, Australia}
    \affiliation{Centre for Complex Systems, The University of Sydney, Sydney NSW 2006, Australia}

    \author{James M. Shine}
    \affiliation{Centre for Complex Systems, The University of Sydney, Sydney NSW 2006, Australia}
    \affiliation{Brain and Mind Centre, Faculty of Medicine, The University of Sydney, Sydney NSW 2006, Australia}

    \author{Joseph T. Lizier$^\ast$}
    \affiliation{Centre for Complex Systems, The University of Sydney, Sydney NSW 2006, Australia}
    \affiliation{School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney NSW 2006, Australia}

    \author{Ben D. Fulcher$^\ast$}
    \affiliation{School of Physics, Faculty of Science, The University of Sydney, Sydney NSW 2006, Australia}
    \affiliation{Centre for Complex Systems, The University of Sydney, Sydney NSW 2006, Australia}

% \altaffiliation[Also at ]{Physics Department, XYZ University.}%Lines break automatically or can be forced with \\
%\author{Second Author}%
% \email{Second.Author@institution.edu}
%\affiliation{%
% Authors' institution and/or address\\
% This line break forced with \textbackslash\textbackslash
%}%
%
%\collaboration{MUSO Collaboration}%\noaffiliation
%
%\author{Charlie Author}
% \homepage{http://www.Second.institution.edu/~Charlie.Author}
%\affiliation{
% Second institution and/or address\\
% This line break forced% with \\
%}%
%\affiliation{
% Third institution, the second for Charlie Author
%}%
%\author{Delta Author}
%\affiliation{%
% Authors' institution and/or address\\
% This line break forced with \textbackslash\textbackslash
%}%
%
%\collaboration{CLEO Collaboration}%\noaffiliation

%\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
The participation coefficient is a widely used metric of the diversity of a node's connections with respect to a modular partition of a network.
An information-theoretic formulation of this concept of connection diversity, referred to here as participation entropy, has been introduced as the Shannon entropy of the distribution of module labels across a node's connected neighbors.
While diversity metrics have been studied theoretically in other literatures, including to index species diversity in ecology, many of these results have not previously been applied to networks.
Here we show that the participation coefficient is a first-order approximation to participation entropy and use the desirable additive properties of entropy to develop new metrics of connection diversity with respect to multiple labelings of nodes in a network, as joint and conditional participation entropies.
% thus placing this commonly used heuristic in information-theoretic terms.
% extensions of the concept of uncertainty in connection patterns across modules.
% To demonstrate this, we introduce the conditional participation entropy, which can be used to quantify the diversity of connection patterns in the context of additional information about partner nodes such as multiple network partitions, e.g., for hierarchical or higher-order network structures.
The information-theoretic formalism developed here allows new and more subtle types of nodal connection patterns in complex networks to be studied.
\end{abstract}

%\keywords{Suggested keywords}
%Use showkeys class option if keyword display desired
\maketitle

% \section{Introduction}

Many real-world networks exhibit modular structure, in which nodes form densely interconnected modules with relatively sparse connectivity between modules.
Such modularity is observed in social networks, food webs, metabolic networks, protein--protein interaction networks, air-traffic networks, and brain networks \cite{guimera2005cartography}.
% Such examples occur from the scale of neuronal circuits \cite{} to that of macroscopic brain areas \cite{}.
Within such modular networks, individual nodes can vary substantially in their degree of within- versus across-module connectivity.
% are more likely to connect to other nodes within their own module than between modules,
% For example, some nodes may primarily connect within a single module, while others may connect equally across all modules.
These differences can provide important insights into a node's functional role within a network, such as facilitating local information processing (consistent with strong within-module connectivity) versus distributed/integrative communication (strong cross-module connectivity).

To measure the extent to which a given node's connections are distributed within or across modules, the participation coefficient was introduced by Guimer\`a and Amaral~\cite{Guimera2005, guimera2005cartography}.
% , $\mathcal{P}_i$, of a given node, $i$,
% It has been widely used to quantify the .
% $\mathcal{P}$ takes a minimal value ($0$) for a node with connections entirely within a single module, and a maximal value (near $1$) for a node that connects equally across all modules.
It has been used widely to analyze networks across domains, including the Internet, metabolic, air transportation, protein-interaction, and neural networks \cite{guimera2007classes, Sporns2007}.
For example, the participation coefficient of nodes in macroscopic brain networks has been used to distinguish levels of consciousness caused by brain injury \cite{Rizkallah2019:DecreasedIntegrationEEG} and to identify emerging new research directions from scientific publication citation networks \cite{Shibata2008:DetectingEmergingResearch}.
This concept of nodal connection diversity across modules was also formulated as a Shannon entropy by \citet{RubinovSporns}.
% In the context of brain networks, an alternative measure of connection diversity has been introduced as the `diversity coefficient' in .
% This is defined as the Shannon entropy of a node's distribution of connections into each module.
Quantifying diversity is a general problem studied across many fields, with a prominent application to species diversity in ecology for which the Shannon entropy and Gini--Simpson index (the measure underlying the participation coefficient \cite{RubinovSporns}) formulations have been used for decades, among a host of alternative indices \cite{Peet2003, Daly2018}.
% To quantify species diversity In addition, ecology literature has been using the Shannon entropy, as well as the participation coefficient (which is also known as the `Gini--Simpson index') and a host of other diversity indices to  for decades \cite{Peet2003,Daly2018}.
Mathematical relationships between different formulations of diversity indices have been uncovered.
For example, the Gini--Simpson index and Shannon entropy have each been shown to be special cases of `generalised entropies' \cite{Havrda1967, Keylock2005, Vajda2007}.
\citet{Zhang2014} have further shown that the Gini--Simpson index can be expressed as a first-order Taylor approximation to the Shannon entropy formulation of diversity.
% and a wide variety of similar diversity measures
% Zhang and Grabchak \cite{Zhang2014} have described.

Despite the wide variety of diversity indices used in ecology, the participation coefficient has remained the dominant measure of node participation in network theory since it was introduced in 2005 \cite{Guimera2005, guimera2005cartography}.
Here we connect the problem of quantifying nodal connection diversity in networks with a large and existing literature on diversity indices, and in particular explain the relationship between the participation coefficient and the corresponding Shannon entropy-based formulation of connection diversity \cite{RubinovSporns}, which we call `participation entropy' here.
We argue that participation entropy is a better-motivated measure of node participation diversity, primarily due to its additive behaviour with respect to chaining probability distributions, an operation which can arise naturally when the nodes in a network have labels in multiple module sets.
Taking advantage of this behavior, we define novel measures of connection diversity---`joint' and `conditional' participation entropy---for quantifying more nuanced types of connection patterns in complex networks.

\subsection*{Participation Coefficient and Participation Entropy}

We consider a binary, undirected network partitioned into $M$ non-overlapping modules, with each node labeled as belonging to a module, from the set $\mathcal{M} = \{m_1,m_2,...,m_M\}$.
Note that this modular partition is most commonly obtained as the result of a community-detection algorithm operating on the network \cite{fortunato2016community}, but could in general represent any assignment of categorical labels to nodes in a network.
% Viewed in this way, we note that the distribution $p_i(m)$ need not be over connections into modules $m \in \mathcal{M}$, but could be for connections into nodes with any arbitrary labelling scheme, $\mathcal{M}$.
% Such labels may represent node properties, such as gender of a scientist in a citation network, that may or may not not relate to the modular structure of those nodes.
Given $\mathcal{M}$, the participation coefficient, $\mathcal{P}_i$, of node $i$ is defined as
\begin{equation}\label{eqn:PC}
    \mathcal{P}_i(\mathcal{M}) = 1 - \sum_{j = 1}^{M} \left( \frac{\kappa_{ij}}{k_i} \right)^2 \,,
    % \sum_{m\in\mathcal{M}} \left( \frac{\kappa_{im}}{k_i} \right)^2 \,,
\end{equation}
where $\kappa_{ij}$ is the number of edges between node $i$ and a node in module $m_j$, and $k_i$ is the degree of node $i$ (the total number of connections made to all other nodes in the network) \cite{Guimera2005, guimera2005cartography}.
For simplicity, we focus on undirected networks here, but note that this formulation extends straightforwardly to weighted networks (substituting $\kappa_{ij}$ and $k_i$ for weighted versions that sum edge weights) and directed networks (e.g., by defining $\kappa_{ij}$ and $k_i$ as counting connections outward from, or arriving to node $i$, as the in-degree or out-degree).
% , by appropriate substitutions to in-degree or out-degree).
% As per $\mathcal{P}_i$, we can study the participation entropy regarding the diversity of connections coming out from node $i$, into node $i$, or else undirected edges in an undirected network.
% Of course, one can study the participation of node $i$ with respect to edges that it is the source of, or edges that it is the target of, or else for undirected edges in an undirected network.
% Without loss of generality, for the remainder of the paper we will consider participation with respect to edges out from node $i$.
Equation~\eqref{eqn:PC} exhibits the desired behavior of a connection diversity metric, taking a minimal value for a node with connections entirely within a single module ($\mathcal{P}_i = 0$) and a maximal value for a node that connects equally across all $M$ modules ($\mathcal{P}_i = 1 - 1/M$).
%  as $M \rightarrow \infty$
% ($\mathcal{P}_i \rightarrow 1$)
%
% At the end of the PC section (see comment)

\paragraph{A probabilistic formulation.}
An alternative interpretation of Eq.~\eqref{eqn:PC} can be considered by identifying $\kappa_{ij}/k_i$ as the probability, $p_i(m_j)$, that a randomly selected connected neighbor of node $i$ is assigned to module $m_j$.
An example is depicted in Fig.~\ref{fig:schematic}a, which depicts the connected neighbors of node $i$ across each of three modules, $\mathcal{M} = \{m_1, m_2, m_3\}$.
This, or any other pattern of connectivity, can be represented as a probability distribution, $\{p_i(m)\}_{m\in\mathcal{M}}$, plotted for this simple example in Fig.~\ref{fig:schematic}b.
In this probabilistic formulation, $\mathcal{P}_i$ can be expressed as a function of $p_i(m)$ by rewriting Eq.~\eqref{eqn:PC} as $\mathcal{P}_i(\mathcal{M}) = 1 - \sum_{m \in \mathcal{M}} p_{i}(m)^2$.
% That is, $\mathcal{P}_i$ is 1 minus the sum of squares over $p_{i}(m)$.

This formulation allows us to clearly see that the participation coefficient is an implementation of the Gini--Simpson index of diversity \cite{Simpson1949:MeasurementDiversity}, as observed previously \cite{RubinovSporns}.
This is an important measure used in many other contexts, including quantifying biodiversity \cite{Peet2003, Daly2018}.
Following the interpretation that motivated Simpson's original formulation \cite{Simpson1949:MeasurementDiversity}, $\mathcal{P}_i$ can be interpreted as the probability that two randomly selected nodes connected to node $i$ (with replacement) lie in different modules.

% The participation coefficient would then measure the diversity of connections from node $i$ over these labels instead of module labels.

% Figure environment removed

% Using this notation, we obtain the equivalent formulation of the participation coefficient as .
% In this formulation, we see that $\mathcal{P}$ computes the  .
% formulate the problem of how connections of a given node, $i$, are distributed across modules, by considering the probabilities that its edges connect to nodes in each module.
% This probability is expressed as:
% \begin{equation} \label{eq:prob}
%     p_{im} = \frac{k_{im}}{k_i},
% \end{equation}{i}

\paragraph{Participation Entropy}

The Shannon entropy \cite{Shannon1948, Cover2005} of $p_{i}(m)$ is a natural measure of the connection diversity of node $i$ across the label set, $\mathcal{M}$:
\begin{equation} \label{eq:participation-entropy}
    \mathcal{E}_i(\mathcal{M}) = H[p_i(m)] = -\sum_{m \in \mathcal{M}} p_{i}(m) \log p_{i}(m)\,.
\end{equation}
We term $\mathcal{E}_i(\mathcal{M})$ the `participation entropy' of node $i$, which measures the uncertainty (or average surprise) in the module labels (from $\mathcal{M}$) of its connected neighbors.
This matches a previous formulation of nodal connection diversity introduced by \citet{RubinovSporns} (named the `diversity coefficient' in its implementation in code in the \textit{Brain Connectivity Toolbox} \cite{Rubinov2010:ComplexNetworkMeasures}).
Participation entropy exhibits the same desired qualitative behavior as the participation coefficient, $\mathcal{P}_i$; that is, $\mathcal{E}_i = 0$ is minimal when all connected neighbors of node $i$ are in the same module (minimum uncertainty about the module label of node $i$'s neighbors) and $\mathcal{E}_i = \log(M)$ is maximal when connected neighbors are equally distributed across all of the modules (maximum uncertainty about the module label of node $i$'s neighbors).
Note that both $\mathcal{E}_i$ and $\mathcal{P}_i$ may be normalised by dividing by their maximum value for a given number of modules $M$, if desired (as `normalized connection diversity' \cite{RubinovSporns}, which has the effect of setting its range to the unit interval).
% \citet{RubinovSporns} also proposed a method to handle signed weights of the connections.

% \citet{RubinovSporns} argue in favor of (the normalized version of) $\mathcal{E}_i$ over $\mathcal{P}_i$ in terms of having a more consistent upper bound; however we suggest its advantages are deeper.
Compared to $\mathcal{P}_i$, quantifying connection diversity as an entropy, $\mathcal{E}_i$, is the unique formulation that satisfies three key advantageous properties.
First, it is continuous with respect to changes in $p_{i}(m)$.
Second, it increases monotonically with the number of modules, $M$, when $p_{i}(m) = 1/M$, $\forall m$.
Third, and most importantly, $\mathcal{E}_i$ can be chained consistently across multiple labeling sets for nodes \cite{Shannon1948, Ash1965}, opening new ways of quantifying and interpreting nodal connection patterns in networks, as we develop later.
As per the original formulation of $\mathcal{P}_i$, it also generalizes straightforwardly to weighted and directed networks.
% We finally note that both participation entropy and participation coefficient may be normalized to lie between 0 and 1 by dividing through by their maximum possible value given $M$ modules.
% Thus, the measures may not be distinguished on the basis of normalizability, as was claimed in \cite{RubinovSporns}.
% networks (e.g., using a reweighted $p_i(m_j) = \sum_{k \in m_j}{w_{ki}}/\sum_k{w_{ki}}$) and directed networks (by reformulating in terms of in- or out- connections), as per the original formulation.
% For example, to extend $\mathcal{E}_i$ to a weighted version (for non-negatively weighted edges) by defining $p_i(m_j) = \frac{\sum_{k \in m_j}{w_{ki}}}{\sum_k{w_{ki}}}$ as the proportion of total weights coming from module $m_j$.
% of the same concept quantified by the .

\paragraph{Connecting the two formulations}

The mathematical relationship between the Gini--Simpson index and Shannon entropy is well-known \cite{Havrda1967, Keylock2005, Vajda2007} and has been demonstrated in the context of species diversity indices \cite{Zhang2014}.
But the connection has not been reported for the corresponding measures of nodal connection diversity in networks, $\mathcal{P}$ and $\mathcal{E}$.
% , where these measures are known respectively as the participation coefficient \cite{Guimera2005} and the diversity coefficient \cite{RubinovSporns}.
The relationship can be seen through the series expansion of participation entropy via the logarithm in Eq.~\eqref{eq:participation-entropy}:
\begin{equation}
\label{eqn:series_expansion}
    \mathcal{E}_i(\mathcal{M}) = -\sum_{m\in\mathcal{M}} p_{i}(m) \sum_{n=1}^\infty \frac{-[1 - p_{i}(m)]^{n}}{n}\,.
\end{equation}
This quantity converges for $0 < p_{i}(m) \leq 1$, and we take $0 \log 0 \rightarrow 0$ by convention, so there is no contribution from any $p_{i}(m) = 0$.
Limiting the expansion to the leading term, $n = 1$, yields
\begin{align}
    \mathcal{E}_i(\mathcal{M}) & \approx \sum_{m\in\mathcal{M}} p_i(m) - p_{i}(m)^2\,, \nonumber \\
            &= 1 - \sum_{m\in\mathcal{M}} p_{i}(m)^2\,,\\
            &= \mathcal{P}_i(\mathcal{M})\,.
\end{align}
We thus recapitulate the participation coefficient as a first-order approximation to participation entropy (as per the Gini--Simpson index and Shannon entropy \cite{Zhang2014}).
% &\approx \sum_m p_{im} \frac{(-1)^1(-1 + p_{im})^{1}}{1}\,, \nonumber \\
% since $\sum_m p_{im} = 1$ by definition.
% Therefore, the participation coefficient, $\mathcal{P}_i$, is a first-order approximation to the participation entropy:
% \begin{equation}
%     H_{p_i} \approx \mathcal{P}_i.
% \end{equation}
% \paragraph{Taylor expansion of logarithm **about 1** ($\log(p)$ at $p=1$ must be zero; natural point to expand to ensure this constraint for a sensible definition). Show the first few expansions and make the connection.}
% We thus recognise the original formulation of the participation coefficient, $\mathcal{P}_i$ \cite{Guimera2005, guimera2005cartography}, as a first-order approximation to participation entropy, $\mathcal{E}_i$.
% Note that the maximal $\mathcal{P}_i$ is $1 - 1/M$, while the maximal $\mathcal{E}_i$ is $\log(M)$.
% , the entropy of $p_{i}(m)$.
% As stated earlier, the information-theoretic formulation of the participation entropy affords unique properties, such as consistency across multiple labelling sets; with $\mathcal{P}_i$ being just an approximation, it cannot share such properties.
% We will now explore these two concepts, beginning with the relationship between the coefficient and the entropy.

% Treating $\mathcal{E}_i$ as the `true' quantity we wish to compute from $p_{i}(m)$, and $\mathcal{P}_i$ as a first-order approximation of it, we can compute the error, $\mathcal{E}_i - \mathcal{P}_i$, using the remainder formula for a Taylor expansion:
% % For a given probability distribution, $p_{i}(m)$, the error of using $\mathcal{P}_i$, a first-order Taylor expansion of the entropy is
% \begin{align}
% R_2(\mathcal{M}) &= \mathcal{E}_i - \mathcal{P}_i\,, \nonumber \\
%             &= \sum_{m\in\mathcal{M}} p_{i}(m) \sum_{n=2}^\infty \frac{(-1)^n (-1 + p_{i}(m))^n}{n}\,. \nonumber \\
% \end{align}

% To demonstrate the relationship between $\mathcal{P}_i$ and $\mathcal{E}_i$ for a simple two-module partition $\mathcal{M} = \{m_1,m_2\}$, we consider the modular connectivity distribution for a node $i$: $\{p_i(m_1), p_i(m_2)\} = \{p_i(m_1),1 - p_i(m_1)\}$.
% This allows us to specify all possible connectivity distributions for a node in terms of a single variable, $p_i(m_1)$.
% The participation entropy, $\mathcal{E}_i$, and the first three orders of approximations to it as a function of $p_i(m_1)$ for this two-module case is shown in Fig.~\ref{fig:twoModuleApproximation} (with the first-order approximation corresponding to the participation coefficient, $\mathcal{P}_i$).
% Recovering the expected qualitative behavior of an appropriate statistic of connection diversity, all approximations yield zero when all connections are in a single module, $p_i(m_1) = 0$ or $p_i(m_1) = 1$, and are maximal when connections are split equally between the two modules, $p_i(m_1) = p_i(m_2) = 0.5$.
% In particular, for this two-module case, $\mathcal{P}_i$ varies monotonically with $\mathcal{E}_i$ such that in any real network, sorting all nodes by either $\mathcal{P}_i$ or $\mathcal{E}_i$ would yield the same node ordering.

% % Figure environment removed

In order to investigate the discrepancy between $\mathcal{E}_i$ and its first-order approximation, $\mathcal{P}_i$, we sampled from possible distributions, $p_i(m)$ for $M = 2, ..., 5$, and plotted the resulting accessible regions of $\mathcal{P}_i$--$\mathcal{E}_i$ space in Fig.~\ref{fig:multipleModulesRegions}.
Our numerical results match analytic expressions for these regions for the underlying measures on $p_i(m)$ derived by \citet{Vajda2007}.
% as a function of the number of modules, $M$.
% Results are shown in Fig.~\ref{fig:multipleModulesRegions}.
We find that $\mathcal{P}_i$ varies monotonically with $\mathcal{E}_i$ for $M = 2$, but for $M > 2$, allowed values of $\mathcal{P}_i$ and $\mathcal{E}_i$ are constrained to specific regions of the space.
This accessible region expands with the addition of each new module; Fig.~\ref{fig:multipleModulesRegions} annotates the additional accessible region with each increment of $M$.
% $(\mathcal{P}_i, \mathcal{E}_i)$ regions, with the accessible region expanding with each addition of a new module.
% Results for $M = 2, ..., 5$ modules are shown .
% across a uniformly sampled set of allowed probability distributions, $p_i(m)$.
% however, as shown in Fig.~\ref{fig:multipleModulesRegions}.
% , sampling discretely and uniformly.
% For example, for 3 modules, we sampled probability distributions $\{p,q,1-p-q\}$, with $p \in [0,1]$ and $q \in [0,1-p]$.
% When sampling, we used a discrete spacing increment of 0.01.
% We find that, when there are three or more modules, there is .
% For every additional module, the accessible region of $(\mathcal{P}_i, \mathcal{E}_i)$ space is expanded.
% Where node $i$ has no connections to the newly added module, then $(\mathcal{P}_i, \mathcal{E}_i)$ remain constrained in the region that we had before we added the new module.
% Parts of the region with the greatest height or width correspond to regions in probability space where the two \ac{PC} measures can diverge the most.
% The results indicate that ranking nodes by $\mathcal{P}_i$ and $\mathcal{E}_i$
% would yield the same ordering
%  but for $M > 2$, the ranking of nodes by $\mathcal{P}_i$ can diverge from that obtained from $\mathcal{E}_i$.
The results indicate that there can be a substantial discrepancy between an analysis using $\mathcal{P}_i$ versus $\mathcal{E}_i$, with greater potential for differences at moderate-to-high values of $\mathcal{P}_i$ and with increasing $M$.
% where values can  the allowed ranges are larger.
% With three modules for example, a measurement of $\mathcal{P}_i \approx 0.5$ could correspond to $\mathcal{E}_i$ in the relatively large (approximate) range 0.6--0.8, or conversely a true value of $\mathcal{E}_i \approx 0.7$\,nats could correspond to measurements of $\mathcal{P}_i$ in the large approximate range 0.35--0.5, depending on the higher order subtleties of the distribution across modules.
% Viewing $\mathcal{P}_i$ as a first-order approximation of $\mathcal{E}_i$,
Published results using $\mathcal{P}_i$ to quantify nodal diversity (or extract a list of `high-participation nodes' \cite{Sporns2007, Pedersen2020}), may thus obtain different results when using $\mathcal{E}_i$ instead of its first-order approximation, $\mathcal{P}_i$.
% We draw particular attention to y nodespublished work that has used $\mathcal{P}_i$ to extract a small number of high-participation nodes (e.g., \cite{}); this result may differ when using $\mathcal{E}_i$ instead of its first-order approximation, $\mathcal{P}_i$.
% this discrepancy suggests that such results may vary when substituting the first-order approximation ($\mathcal{P}_i$) with the full quantity, $\mathcal{E}_i$.
% In particular, these regions are where the original \ac{PC} may make the most `error' if interpreted as a first-order approximation of the entropic PC, as in Eq.~\eqref{eqn:PCent_1storder}.
% The  the height and width of the region grow as we add modules, implying that the approximation has more potential for discrepancies in networks with a greater number of modules.
% The scope for such discrepancies is minimal at low values of original \ac{PC} (where the allowed region is highly constrained), and greatest near the mid-range of \ac{PC} values (in the approximate range 0.4--0.6).
% This is important, because it is often the nodes with higher \ac{PC} values that are of interest to researchers, as these nodes play an integrative role communicating across modules.
% Hence, the maximal value of original \ac{PC} is constrained to be less than 1, whereas the maximal value of the entropic \ac{PC} will grow without bound as $|M|\rightarrow \infty$.
% This implies that, as we add modules, the approximation's potential for error also grows \textit{without bound}.
% This result is hugely significant, and implies that there are cases for which our entropic \ac{PC} is required to get values that are anything close to accurate.

% Figure environment removed

% \paragraph{With higher numbers of modules how much can PE and \ac{PC} differ?
% Shaded area figure.
% }

% we aim to compute the approximation error of $\mathcal{P}_i$ using the remainder formula for a Taylor expansion.
% between the participation coefficient and entropy, we can use the remainder formula for Taylor expansion to get the approximation error.
% \omc{Unsure if this is helpful? Sets up Fig. 2 maybe?}
% \paragraph{Characterization}
% \paragraph{;
% 2--Approximations converge and maintain qualitative behavior of $p\log p$ when expanding log around 1.
% [Combine Figs 1/2 as panels or inset]
% }
% \paragraph{Is there a role for the stochastic simulations of generating networks using probabilities based on module labels?}
% \omc{Oliver's stuff thrown in here:}


% \begin{comment}
% The current formulation of PC, first proposed in 2005 by \citet{Guimera2005} (which we call `original PC') is a heuristic metric. It defines the participation of a given node $i$ as
% \begin{equation} \label{eqn:PC}
%     PC_{\text{orig},i}=1-\sum_{m \in M}\left(\frac{k_{i}(m)}{k_{i}}\right)^{2} \text{ ,}
% \end{equation}
% where $M$ is the set of network modules, $k_i(m)$ is the degree (number of connections) between node $i$ and all of the nodes in module $m$, and $k_i$ is the degree between node $i$ and all other nodes in the entire network.
% This formulation gives 0 for a node which is only connected to a single module (i.e., its own module), and a maximum of $1-1/|M|$ when a node's connections are equally distributed amongst all $|M|$ modules.
% In the limit of many modules ($|M| \rightarrow \infty$), the maximum \ac{PC} value thus approaches 1.
% However, beyond these sensible upper and lower bounds, its use is not grounded in well-established theory; it was developed as a heuristic measure which captured the intuitive notion of node participation.

% By contrast to this heuristic framing of participation, information theory encompasses a deep quantitative literature \cite{Cover2005}.
% Here we aim to:
% (i) formulate an analogue of \ac{PC} in information theoretic terms;
% and (ii) investigate its relationship to the popular heuristic.
% % We address this issue by providing a formulation of the participation coefficient which has foundations in information theory.
% In particular, we use the concept of entropy to define the \ac{PC} of each node within a network.
% In this information-theoretic context, we make use of the entropy of a discrete random variable $X$, with possible outcomes $x_{1}, \ldots, x_{n}$, which occur with probability $p(x_{1}), \ldots, p(x_{n})$.
% The entropy of $X$ is defined to be
% \begin{equation}
%     \label{eqn:entropy}
%     \mathrm{H}(X) = -\sum_{i=1}^{n} p(x_{i}) \log p(x_{i}),
% \end{equation}
% where the logarithm can be taken to any desired base; here we use natural logarithms.

% Entropy can be understood as a measure of the average information gained upon the discovery of the value of $X$.
% Put another way, it is a measure of the inherent uncertainty about the value of $X$, given its probability distribution.
% If $X$ has a uniform probability distribution, there is maximal uncertainty about what value it will take on a particular trial, hence the entropy is maximal.
% Conversely, if the probability distribution of $X$ is zero everywhere except for a single value, then there is no uncertainty about its outcome, and its entropy is zero.

% We use this information-theoretic framework to define a new PC, which we refer to as `entropic PC'.
% This approach gives a metric which is well-justified from an information theoretic standpoint, meaning it can be well interpreted and extended within the preexisting, rich framework of information theory.
% Moreover, this new metric reproduces the original \ac{PC} as a first-order approximation.

% \section{Proposal}
% \subsection{An information-theoretic \ac{PC} formulation}
% We are proposing a new measure of participation, which we call `entropic PC', grounded in the formalism of information theory; specifically, using the concept of entropy (cf. Eq.~\eqref{entropy}). To define our probability distribution, we use the quantities defined in the original \ac{PC} formulation, Eq.~\eqref{eqn:PC}:
% \begin{equation}
%     p_{i,m} = \frac{k_i(m)}{k_i}.
% \end{equation}
% We then define our entropic \ac{PC} as the entropy of this distribution (in the following equation, node index $i$ has been suppressed for simplicity):
% \begin{equation}
%     \label{eqn:entropic_PC}
%     PE = - \sum_{m \in M} p_m \log p_m.
% \end{equation}
% The intuition underlying this formulation is that if a node's connections are equally distributed amongst all modules, there is maximal uncertainty about which module a given link will lie in, and its entropy achieves its maximum value of $\log(|M|)$. Conversely, if the node only connects within a single module, there is no uncertainty about where its connections lie, and its entropy is 0. Hence, entropy provides a very natural way to define the PC.

% We next aim to investigate the relationship between the entropic \ac{PC} and original PC.
% To do so, we expand the entropic \ac{PC} out to first and second order.
% We can approximate the logarithm in Eq.~\eqref{eqn:entropic_PC} about 1 by writing $p = 1-(1-p)$ and performing a Taylor expansion (using natural logarithms to avoid a scaling constant):
% \begin{equation}
% PE =-\sum_m p_{m}\left[-(1-p_m) - \frac{1}{2}(1-p_m)^2 - ...\right].
% \end{equation}
% This gives a sequence of approximations which converge to the true value of the entropy. Expanding the logarithm to first and second-order, respectively, yields the following approximations to the entropy:
% \begin{align}
% \label{eqn:PCent_1storder}
% PE &\approx 1-\sum_m p_m^2,\\
% \label{eqn:PCent_2ndorder}
% PE &\approx \frac{3}{2}-2\sum_m p_m^2+\frac{1}{2}\sum_m p_m^3.
% \end{align}
% Remarkably, the first-order approximation of \ac{PE}, Eq.~\eqref{eqn:PCent_1storder} coincides exactly with the original formulation, Eq.~\eqref{eqn:PC}, revealing a deep connection between these two quantities.
% \end{comment}

% \begin{comment}
% % Figure environment removed
% \end{comment}

% However, this picture drastically changes when we consider three or more modules.
% In this more realistic case, \ac{PE} and \ac{PC} are no longer perfectly correlated, but can diverge quite substantially.

\subsection*{Joint and conditional participation entropy}

A major advantage of formulating $\mathcal{E}_i$ as an entropy is the ability to capture more subtle types of connection-pattern diversity in networks.
Here we demonstrate this capability by developing entropy-based network participation measures for the case that each node is annotated with \textit{multiple} labels.
Specifically, we consider $L$ different module sets, $\mathcal{M}_1, \mathcal{M}_2, ..., \mathcal{M}_L$, that each define a labeling of network nodes.
% Each module set $\mathcal{M}^s$ provides a module label $m^s$ for each node in the network $j$.
In a social network, this could correspond to individuals being labeled by both gender, $\mathcal{M}_g$, and friendship group, $\mathcal{M}_f$.
Or, in a brain network, it could correspond brain regions being labeled by both their hemisphere, $\mathcal{M}_h$ (left or right) and their functional network module, $\mathcal{M}_f$ (e.g., auditory, visual, association, etc.).
There is no clear way of extending $\mathcal{P}_i$ to such a setting, but it can be incorporated naturally in the information-theoretic formulation of $\mathcal{E}_i$.

% (derived from applying community detection to the structural connectome).
% As above, for a given module set, $\mathcal{M}_s$, we can compute the participation entropy, $\mathcal{E}_{i}(\mathcal{M}_s)$, to quantify the connection diversity of node $i$ across the modules in $\mathcal{M}_s$ using the probability distribution of the module assignments of the node's connected neighbors, $\{ p_i(m^{(s)}) \}_{m^{(s)} \in \mathcal{M}_s}$ via \eq{participation-entropy}.
% Now go into joint distribution
Extending participation entropy with respect to any single labeling of nodes, $\mathcal{M}$, we now consider the diversity of connections involving node $i$ across multiple label sets jointly.
Writing the $L$ sets as $\underline{\mathcal{M}} = (\mathcal{M}_1, \mathcal{M}_2, ..., \mathcal{M}_L)$, and a combination of labels from $\underline{\mathcal{M}}$ for a given node as $\underline{m} = (m^{(1)}, m^{(2)}, ..., m^{(L)})$, we define the joint probability distribution $p_i(\underline{m})$ for the connected neighbors of node $i$.
%, for $m^k \in \mathcal{M}^k, \forall k$. % I don't think we need that part
% Writing a combination of labels as $\underline{m} = (m^{(1)}, m^{(2)}, ..., m^{(L)})$ from the sets $\underline{\mathcal{M}}$,
We can then define the \textit{joint participation entropy} of node $i$ as:
\begin{align} \label{eq:joint-participation-entropy}
    \mathcal{E}_i(\underline{\mathcal{M}}) & = H[p_i(\underline{m})] \nonumber \\
     & = - \sum_{\underline{m}} p_{i}(\underline{m}) \log p_{i}(\underline{m})\,.
% Writing it out in full:
%    \mathcal{E}_i(\underline{\mathcal{M}}) & = H[p_i(m^1, ..., m^L)] \nonumber \\
%     & = -\sum_{\substack{m^1, ..., m^L \in \\
%                \mathcal{M}^1, ..., \mathcal{M}^L}}
%                p_{i}(m^1, ..., m^L) \log p_{i}(m^1, ..., m^L)\,.
\end{align}
This tells us the total diversity of connections across these multiple module sets, $\underline{\mathcal{M}}$.

Similarly, we can define the \textit{conditional participation entropy} as the entropy of modular assignments $\underline{m}$ from sets $\underline{\mathcal{M}}$ of the connected neighbors of node $i$, given knowledge of the modular assignments $\underline{n}$ from other sets $\underline{\mathcal{N}}$:
\begin{align}% \label{eq:joint-participation-entropy}
    \mathcal{E}_i(\underline{\mathcal{M}} | \underline{\mathcal{N}}) & = H[p_i(\underline{m} | \underline{n})]\,, \nonumber \\
     & = \mathcal{E}_i(\underline{\mathcal{M}}, \underline{\mathcal{N}}) - \mathcal{E}_i(\underline{\mathcal{N}})\,.
\label{eq:chainRulePE}
\end{align}
This quantifies the remaining uncertainty in the distributions of connections across the modules of sets $\underline{\mathcal{M}}$, given that we already know their distributions across sets $\underline{\mathcal{N}}$.
% This is closely related to a measure of mutual information between different modular partitions of a network that was introduced in \cite{RubinovSporns}. Specifically, conditional participation entropy $\mathcal{E}_i(\underline{\mathcal{M}} | \underline{\mathcal{N}})$ is inversely related to the mutual information between the module labellings $\underline{\mathcal{M}}$, $\underline{\mathcal{N}}$ of a given node's connections.

The joint participation entropy, $\mathcal{E}_i(\underline{\mathcal{M}})$, and conditional participation entropy, $\mathcal{E}_i(\underline{\mathcal{M}} | \underline{\mathcal{N}})$, are related via the chain rule for entropies \cite{Cover2005} (vis-\`a-vis \eq{chainRulePE}), which means that we can consistently decompose and re-compose the diversity of connections over multiple module sets, regardless of which order we chain our knowledge of the module labelings.
This property is unique to the information-theoretic formulation \cite{Ash1965}.
% ---the participation coefficient, $\mathcal{P}_i$ cannot be chained across multiple module labels in this consistent way.

%The conditional PE defined above thus tells us how much uncertainty there is about a node's $M_n$ label, given that we know its $M_{n-1},...,M_1$ labels. For example, if the $M_{n-1},...,M_1$ labels completely determined the node's $M_n$ label, the conditional PE above would be 0. On the other hand, if knowledge of the other labels gave us no additional information about the $M_1$ label, the conditional PE would achieve its maximum possible value of $\text{PE}_i(M_n)$, that is, it would simply be equal to the participation entropy of module set $M_n$ prior to conditioning.\\
% We won't bother to mention this, it's implicit from the equations:
%Note that the conditional PE could have been defined directly (in an equivalent way) using the conditional probability distribution of a node's connections to different module sets, as in Eq.~\eqref{condentropy}.

To illustrate the calculation of conditional participation entropy, we show some illustrative examples in \fig{schematic_conditional} for the simple case of two node labelings: $\mathcal{M} = \{m_1, m_2, m_3\}$ and $\mathcal{S} = \{$\FiveStarOpen, $\bigcirc$, $\hexagon$\}.
The three cases shown in Fig.~\ref{fig:schematic_conditional} correspond to distinct types of connection patterns of node $i$ with respect to $\mathcal{M}$ and $\mathcal{S}$.
% Three cases are depicted in Fig.~\ref{fig:schematic_conditional}, corresponding to
% are fully dependent (Fig.~\ref{fig:schematic_conditional}a), are independent (Fig.~\ref{fig:schematic_conditional}c), and non-trivially dependent (Fig.~\ref{fig:schematic_conditional}e).
In \fig{schematic_conditional}a, the labels assigned to node $i$'s connected neighbors are \textit{redundant} with respect to $\mathcal{M}$ and $\mathcal{S}$.
That is, for a given connected neighbor, knowledge of the label $s$ leaves no uncertainty about the label $m$ (and vice-versa), resulting in the symmetric $p(m_i|s_j)$ matrix shown in \fig{schematic_conditional}b.
For this case, the conditional participation entropy of node $i$, $\mathcal{E}_i(\mathcal{M} | \mathcal{S}) = 0$.

% Figure environment removed

For the connection pattern shown in \fig{schematic_conditional}c, the labelings $m$ and $s$ are statistically independent.
That is, for a given connected neighbor, knowledge of the label $s$ does not reduce our uncertainty about the label $m$, as reflected in the $p(m_i|s_j)$ matrix in \fig{schematic_conditional}d.
In this case, $\mathcal{E}_i(\mathcal{M} | \mathcal{S}) = \mathcal{E}_i(\mathcal{M})$.

In general, a node's connection pattern will involve non-trivial statistical dependencies between the combinations of labels.
Such a case is shown in \fig{schematic_conditional}e, where knowledge of the label $s$ reduces our uncertainty about $m$.
% (somewhat but not completely).
% for a  given connected neighbor of node $i$
For example, as depicted in \fig{schematic_conditional}f, if we learn that a node is labeled $s = \bigcirc$, then our uncertainty about its label, $m$, is reduced, from $\{p(m_1),p(m_2),p(m_3)\} = \{0.25, 0.5, 0.25\}$ to $\{0, 0.5, 0.5\}$.
As such, $0 < \mathcal{E}_i(\mathcal{M} | \mathcal{S}) < \mathcal{E}_i(\mathcal{M})$ here.
% the labelings $m$ and $s$ of the partner nodes $j$ on edges from $i$ have a  \textit{non-trivial dependence}

% Notice that the label set $\mathcal{S} = \{$\FiveStarOpen,$\bigcirc$,$\hexagon$\} underlines how our labels need not be specifically a module assignment.
% We emphasize that the measures introduced above for quantifying the connection diversity of node $i$ across any labellings of nodes, whether it be derived from a community detection algorithm, independently assigned metadata, or otherwise.
% in the context of additional information about the targets (being a separate module assignment or otherwise).
The conditional participation entropy thus provides a new way to quantify a node's connection diversity across multiple labelings of network nodes.
For example, in a structural brain network in which brain areas (nodes) are annotated by both by a functional annotation, $\mathcal{M}_f$ (e.g., visual, auditory, motor, etc.) and their hemisphere, $\mathcal{M}_h$ (left or right), $\mathcal{E}(\mathcal{M}_h|\mathcal{M}_f)$ could be used to highlight nodes whose diversity of connectivity between left and right hemispheres depends on which functional module they connect to.
% , given information about structural brain connectivity, nodes could be labeled both by a functional annotation, $\mathcal{M}_f$ (e.g., visual, auditory, motor, etc.) as well as their hemisphere, $\mathcal{M}_h$ (left or right).
% Or in a scientific collaboration network in which scientists (nodes) are labeled by their gender, $\mathcal{M}_g$ and their research area $\mathcal{M}_a$, the quantity $\mathcal{E}(\mathcal{M}_g|\mathcal{M}_a)$ captures the extent to which a scientist's collaboration with other scientists by gender depends on the field of research.
% An example with node labellings referring to node properties rather than modular structure would be in a citation network, where $\mathcal{M}$ labels a scientist's area of research, and $\mathcal{S}$ their gender.
% Here, conditional participation entropy could be used to highlight areas of research where citation is more dependent on gender than others.
% An example where the node labellings refer to separate module assignments would be in a brain connectivity context, where $\mathcal{M}$ labels brain modules associated with specific functions (e.g. visual, auditory or motor regions) and $\mathcal{S}$ labels left or right hemisphere.

% Finally, we note that these new measures are distinct from the joint entropy and mutual information between two node labellings introduced in \cite{RubinovSporns}.
% Those are measures of the diversity of the labellings or partitions themselves, in contrast to the family of participation entropies above being measures on the diversity of connections from one particular node.

\subsection*{Conclusion}

% \paragraph{
% Properties of the new beast (range/bounds/...) characterize PE relative to original \ac{PC} and help outline interpretations.
% What does a change in base do to the expansion? (e.g., nats for units?).
% Limits as number of modules, $M \rightarrow \infty$.
% Information theory has interpretation that makes sense to grow with $M$ (the idea to keep a unit range is a source of confusion because they are not comparable and there are no units and it misleads with regard to an interpretable measure of uncertainty that ought to grow).
% }
We have introduced an information-theoretic formulation of nodal connection diversity in complex networks, incorporating results from the broader literature on quantitative diversity indices that builds on a prior introduction of the Shannon entropy formulation of participation coefficient \cite{RubinovSporns}.
Quantifying connection diversity as the average uncertainty in the module label of a connected neighboring node, termed participation entropy, $\mathcal{E}_i$, has mathematically favourable properties over the more commonly used participation coefficient.
Using a probabilistic formulation of the two measures, we show that the participation coefficient is a first-order approximation to the participation entropy (as per the relationship of the underlying measures of diversity \cite{Zhang2014}).
% (also known as the Gini-Simpson index \cite{RubinovSporns})
Using the additivity of participation entropy with respect to chaining probability distributions for multiple module sets, we introduce new ways of measuring connection diversity for cases where nodes are labeled from multiple label sets, defining joint and conditional participation entropy.
% , thus providing an information-theoretic interpretation of this popular existing metric of nodal connection diversity.
% from social network analysis \cite{Wasserman1994} to transport and internet network analysis \cite{JUNHOH.2006}
% We used this property to develop new ways of measuring connection diversity, including for cases involving multiple module sets, as `conditional participation entropy'.

Future work may build on the theoretical foundations laid here, including applying the new measures to data.
This will require developing statistical significance tests against appropriate null distributions.
For example, analysis on the conditional participation entropy of a node, $\mathcal{E}_i(\mathcal{M} | \mathcal{N})$ (i.e., the diversity of connectivity across modules $\mathcal{M}$ given the labeling $\mathcal{N}$) requires comparison to an appropriate null hypothesis.
One choice of null hypothesis is that node $i$ connects randomly with respect to $\mathcal{M}$, while preserving the distribution of connections over $\mathcal{N}$ (which could be sampled from numerically).
% One choice of null hypothesis is that the module labels $\mathcal{M}$ and $\mathcal{N}$ are independent, such that node $i$ connects randomly with respect to $\mathcal{M}$, while preserving the distribution of connections over $\mathcal{N}$ (which could be sampled from numerically).
% (while retaining their marginal distributions for node $i$), which could be sampled from numerically.
% within each module in the given set $\mathcal{M}$,
% requires sampling from the null distribution corresponding to the null hypothesis that the module labels $\mathcal{M}$ and $\mathcal{N}$ are independent (while retaining their marginal distributions for node $i$), akin to significance tests for mutual information \cite{Chavez2003, Lindner2011, Vicente2011, Lizier2011}.
% a null distribution of $\mathcal{E}_i(\underline{\mathcal{M}}|\underline{\mathcal{N}})$ resampled under the null hypothesis that
% PAVLE:
% Furthermore, the conditional entropy $\mathcal{E}_i(\mathcal{M}|\mathcal{N})$ could be appended with a statistical significance test under the null-hypothesis that node $i$ connects randomly within each module in the given set $\mathcal{M}$, while preserving the distribution of connections over $\mathcal{N}$.
% This could be used to distinguish nodes which favor connectivity with respect to node labels $\mathcal{M}$, given $\mathcal{N}$.
% ``unbiased' nodes, whose connection patterns are consistent with the underlying population statistics of modules $\mathcal{M}$ within modules $\mathcal{N}$, from `biased' nodes, which favour certain connections over others.
% The above sentence is a way of laying out how we would do the following:
%and developing methods for statistical null-hypothesis testing on network-wide participation entropy values.
% We demonstrate this by developing new metrics that extend the concept of nodal connection diversity to cases where there are multiple node labelings using conditional entropies.
% including the ability to work on a rigorous theoretical foundation in, e.g., constructing new derivative measures.
% which is based on information theory, providing a solid theoretical backing for its use as a measure of node participation in a network.
Future work could also explore alternative probabilistic formulations of connection diversity that may differently account for module size \cite{Pedersen2020, RuizVargas2014:GatewayCoefficientNovel}.
In summary, the new theory introduced here enables practical new ways of understanding and quantifying more subtle types of nodal connection patterns in complex networks.
% The ability to consistently handle multiple node labelings is unique to this information-theoretic perspective, and cannot be performed with the original participation coefficient.
% We expect the more rigorous theoretical foundation provided by the participation entropy to enable many new insights into characteristics of interactions between modules (or other labelings of nodes) in networks in the future.

% Furthermore, we have shown that the old formulation shows discrepancies when compared to our new formulation, leading us to new interpretations of node participation in complex systems.
% We have demonstrated this directly through an example of an application to a functional connectivity brain network, and found significant differences between the two \ac{PC} formulations for specific brain regions.
% Further work in this area is likely to reveal even more discrepancies with the original \ac{PC} and hence enhance our understanding of brain structure and function.
% Moreover, given the widespread use of the old \ac{PC} measure in over 3000 publications in diverse subject areas, we foresee many interdisciplinary applications of our novel method, from social network analysis \cite{Wasserman1994} to transport and internet network analysis \cite{JUNHOH.2006}.

% Two-module case to show the $p\log(p)$ curve with original versus full; 1--this quadratic form *may be* common in ways that *could* bridge to other fields because it seems like it might be a common heuristic formulation [to discussion].

%\nocite{*}
\bibliography{references}

\end{document}
% ****** End of file apssamp.tex ******
