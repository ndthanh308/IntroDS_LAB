\section{Our Algorithm}

We now introduce our two-phase spectral algorithm, FNM, for the problem of normalized-cut minimization with range-based proportional fairness constraints.
Next, we will present our range-based fair spectral embedding and rounding methods in Sections~\ref{subsec-spectral} and~\ref{subsec-rounding}.

\subsection{Range-based Fair Spectral Embedding}
\label{subsec-spectral}

\noindent\textbf{(Unconstrained) Spectral Normalized-Cut Minimization.}
We begin with a review of (unconstrained) spectral normalized-cut minimization.
According to \cite{YuS03}, the Ncut value can be expressed in terms of the graph Laplacian $\bm{L}:= \bm{D} - \bm{W}$ and a cluster membership indicator matrix $\bm{H} \in \mathbb{R}^{n \times k}$ as $ \mathsf{Ncut}(\mathcal{C}) = \mathsf{trace}(\bm{H}^\top \bm{L} \bm{H})$, where
\begin{equation}\label{H_matrix}
  \bm{H}_{i,l} =
  \begin{cases}
    \frac{1}{\sqrt{\mathsf{vol}(C_l)}}, & \text{if } \ i \in C_l; \\
    0, & \text{otherwise};
  \end{cases}
  \quad
  \forall i \in [n],\forall l \in [k].
\end{equation}
As such, the Ncut minimization problem is equivalent to minimizing $\mathsf{trace}(\bm{H}^\top \bm{L} \bm{H})$ over all possible $\bm{H}$ in the form of Eq.~\ref{H_matrix}.
However, the transformed problem is still NP-hard due to its combinatorial nature.
Therefore, the (normalized) spectral method \cite{YuS03} solves the following relaxed continuous optimization problem by allowing fractional assignments of nodes to clusters:
\begin{equation}\label{NM}
  \min_{\bm{H} \in \mathbb{R}^{n \times k}} \mathsf{trace}(\bm{H}^\top \bm{L} \bm{H})\;\;\text{s.t.}\;\;\bm{H}^\top \bm{D} \bm{H} = \bm{I}_k,
\end{equation}
where $\bm{I}_k$ is an identity matrix of size $k \times k$.
Note that $\bm{H}$ in the form of Eq.~\ref{H_matrix} must satisfy $\bm{H}^\top \bm{D} \bm{H} = \bm{I}_k$.
To solve the problem in Eq.~\ref{NM}, under an assumption that $d_{i}>0, \forall i \in [n]$ (i.e., $G$ has no isolated node), we substitute $\bm{H}$ with $\bm{D}^{-\frac{1}{2}}\bm{T}$ as follows:
\begin{equation}\label{NM1}
  \min_{\bm{T} \in \mathbb{R}^{n \times k}} \mathsf{trace}(\bm{T}^\top \bm{D}^{-\frac{1}{2}} \bm{L} \bm{D}^{-\frac{1}{2}} \bm{T}) \;\;\text{s.t.}\;\; \bm{T}^\top \bm{T} = \bm{I}_k.
\end{equation}
By Rayleigh-Ritz theorem \cite[\S~5.2.2]{Lutkepohl97}, an optimal solution to the problem in Eq.~\ref{NM1} is the matrix $\bm{T}$ which has the eigenvectors of $\bm{D}^{-\frac{1}{2}} \bm{L} \bm{D}^{-\frac{1}{2}}$ 
with respect to its $k$ smallest eigenvalues as columns.

\smallskip\noindent\textbf{Range-based Fair Spectral Ncut Minimization.}
We incorporate range-based proportional fairness into the problem in Eq.~\ref{NM1}.
Let us define two matrices $\bm{A} = [\bm{\alpha}, \ldots, \bm{\alpha}]^\top, \bm{B} = [\bm{\beta}, \ldots, \bm{\beta}]^\top \in \mathbb{R}^{n \times m}$ with the two fairness vectors $\bm{\alpha}, \bm{\beta} \in [0,1]^{m}$ as their rows.
By definition, a partitioning $ \mathcal{C} = \{C_1, \ldots, C_k\}$ denoted as $\bm{H}$ in the form of Eq.~\ref{H_matrix} is fair if and only if $(\bm{A} - \bm{M})^\top \bm{H} \geq \bm{0}$ and $(\bm{M} - \bm{B})^\top \bm{H} \geq \bm{0}$, where $\bm{0}$ is the zero matrix of size $m \times k$, because $\beta_c \cdot \lvert C_l \rvert \leq \lvert V_c \cap C_l \rvert \leq \alpha_c \cdot \lvert C_l \rvert$ if and only if the $(c,j)$-th entries of $(\bm{M} - \bm{B})^\top \bm{H}$ and $(\bm{A} - \bm{M})^\top \bm{H}$ are nonnegative for any $c \in [m]$ and $l \in [k]$.
In this way, the fairness constraints are expressed equivalently as \emph{linear constraints} in matrix form.

Given the above result, the problem in Definition~\ref{def-fnm} is transformed to minimizing $\mathsf{trace}(\bm{H}^\top \bm{L} \bm{H})$ over all $\bm{H}$ in the form of Eq.~\ref{H_matrix} with two additional linear constraints of $(\bm{A} - \bm{M})^\top \bm{H} \geq \bm{0}$ and $(\bm{M} - \bm{B})^\top \bm{H} \geq \bm{0}$.
By applying the same relaxation procedure as for the unconstrained problem, we obtain the following relaxed problem for range-based fair spectral Ncut minimization:
\begin{subequations}
\label{FNM}
\begin{align}
  \min_{\bm{T} \in \mathbb{R}^{n \times k}} & \quad \mathsf{trace}( \bm{T}^\top \bm{D}^{-\frac{1}{2}} \bm{L} \bm{D}^{-\frac{1}{2}} \bm{T}) \label{contiNM} \\
  \text{subject to} & \quad \bm{T}^\top \bm{T} = \bm{I}_k \label{Stiefel} \\
  & \quad (\bm{A} - \bm{M})^\top \bm{D}^{-\frac{1}{2}} \bm{T} \geq \bm{0} \label{alpha} \\
  & \quad (\bm{M} - \bm{B})^\top \bm{D}^{-\frac{1}{2}} \bm{T} \geq \bm{0} \label{beta}
\end{align}
\end{subequations}
Unlike Eq.~\ref{NM1}, the problem in Eq.~\ref{FNM} cannot be directly solved by eigendecomposition due to two additional constraints in Eqs.~\ref{alpha} and~\ref{beta}.

\smallskip\noindent\textbf{Range-based Fair Spectral Embedding with Augmented Lagrangian Method and OptStiefelGBB.}
To resolve the problem in Eq.~\ref{FNM}, we propose a novel algorithm based on the augmented Lagrangian method \cite{Wright99} for constrained optimization and OptStiefelGBB \cite{WenY13} for optimization with orthogonal constraints, as presented in Algorithm~\ref{alg1}, to find a solution $\bm{T}$ and a matrix $\bm{H} = \bm{D}^{-\frac{1}{2}} \bm{T}$ denoting a fractional assignment of each node in $V$ to $k$ clusters.

\input{algorithm_fair_embedding}

The basic idea of the augmented Lagrangian method is to solve a constrained optimization problem by converting the constraints into penalty and Lagrange multiplier terms in the objective function.
In our problem, the violation of fairness constraints in Eqs.~\ref{alpha} and~\ref{beta} by a matrix $\bm{T}$ is denoted as the following penalty matrix:
\begin{displaymath}
  \bm{P}(\bm{T}) := [(\bm{A} - \bm{M})^\top \bm{D}^{-\frac{1}{2}}\bm{T}, (\bm{M} - \bm{B})^\top \bm{D}^{-\frac{1}{2}}\bm{T}] \in \mathbb{R}^{m \times 2k}.
\end{displaymath}
Based on \cite[\S~17.4]{Wright99}, the objective function in Eq.~\ref{contiNM} with penalty and Lagrange multiplier terms is as follows:
\begin{equation}\label{obj-Lagrange}
  L_{\mu}(\bm{T}, \bm{\Lambda}) := \mathsf{trace}( \bm{T}^\top \bm{D}^{-\frac{1}{2}} \bm{L} \bm{D}^{-\frac{1}{2}} \bm{T}) + \sum_{c=1}^m \sum_{l=1}^{2k} \rho(\bm{P}_{c,l}(\bm{T}), \bm{\Lambda}_{c,l}, \mu),
\end{equation}
where $\mu > 0$ is the penalty parameter, $\bm{\Lambda}$ is the estimation of Lagrange multipliers, and $\rho$ is defined as:
\begin{displaymath}
  \rho(p,\lambda,\mu) :=
  \begin{cases}
    -\lambda p + \frac{1}{2} \mu p^2, & \text{if}\; p - \frac{\lambda}{\mu} \leq 0; \\
    -\frac{1}{2\mu} \lambda^2, & \text{otherwise}.
\end{cases}
\end{displaymath}
The augmented Lagrangian method starts from the initial parameters $\mu_0 > 0$, $\bm{\Lambda}_0 = \bm{0}_{m \times 2k}$ and a solution $\bm{T}_0 \in \mathbb{R}^{n \times k}$ with $\bm{T}_0^{\top} \bm{T}_0 = \bm{I}_k$.
Then, it solves a sub-problem and iteratively updates the parameters.
Here, the $t$-th sub-problem ($t = 0, 1, \ldots$) to solve is:
\begin{equation}\label{subproblem}
  \min_{\bm{T} \in R^{n \times k}} L_{\mu_t}(\bm{T}, \bm{\Lambda}_t)\;\; \text{s.t.}\;\; \bm{T}^\top \bm{T} = \bm{I}_k.
\end{equation}
By solving Eq.~\ref{subproblem}, it obtains a new solution $\bm{T}_{t+1}$ and then updates $\mu$ and $\bm{\Lambda}$ as follows:
\begin{equation}\label{params}
    \bm{\Lambda}_{t+1} = \max\{\bm{\Lambda}_t - \mu_t \bm{P}(\bm{T}_{t+1}), \bm{0}\},
    \;\;
    \mu_{t+1} = \xi \mu_t,
\end{equation}
where $\xi > 1$ is an amplification parameter.
The augmented Lagrangian method terminates when the fairness violation of $\bm{T}_{t+1}$, defined as $\lVert \min \{\bm{P}(\bm{T}_{t+1}), \bm{0}\} \rVert_{F}$, is below an error parameter $\varepsilon_1 \geq 0$.
After that, we finally obtain a ``nearly fair'' embedding matrix $\bm{H} = \bm{D}^{-\frac{1}{2}} \bm{T}_{t+1}$ w.r.t.~$\bm{T}_{t+1}$.

Then, we consider how to solve the sub-problem in Eq.~\ref{subproblem} with OptStiefelGBB \cite{WenY13}, a general method for optimization under orthogonality constraints.
Its core idea is to model the feasible region as a $(n,k)$-Stiefel manifold \cite{manifold}, i.e., $\{\bm{T} \in \mathbb{R}^{n\times k}: \bm{T}^\top \bm{T} = \bm{I}_k\}$, and to apply Cayley transformation \cite{cayley} to update the solution at each iteration.
For the problem in Eq.~\ref{subproblem}, given a matrix $\bm{T} \in \mathbb{R}^{n \times k}$ with $\bm{T}^{\top} \bm{T} = \bm{I}_k$ and the gradients $\bm{\nabla_T} L_{\mu}(\bm{T}, \bm{\Lambda})$ ($\bm{\nabla_T}$ for short) of the augmented objective function $L_{\mu}(\bm{T}, \bm{\Lambda})$ w.r.t.~$\bm{T}$, the updated matrix $\bm{T}'$ is expressed as $\bm{T} - \frac{\tau}{2} \bm{\Omega} (\bm{T} + \bm{T}')$, where $\tau > 0$ is the step size and $\bm{\Omega} = \bm{\nabla_T} \bm{T}^{\top} - \bm{T} \bm{\nabla_T}^{\top}$.
By applying Cayley transformation \cite{cayley} on $\bm{\Omega}$, $\bm{T}'$ has a closed form expression as:
\begin{equation}\label{Cayley}
  \bm{T}' = (\bm{I}_n +\frac{\tau}{2} \bm{\Omega})^{-1}(\bm{I}_n - \frac{\tau}{2} \bm{\Omega}) \bm{T}.
\end{equation}
Then, we compute the gradients using the chain rule as follows:
\begin{equation}
\bm{\nabla_T} = 2\bm{D}^{-\frac{1}{2}} \bm{L} \bm{D}^{-\frac{1}{2}} \bm{T} + \bm{A}'{\bm{\nabla\rho}^{[0, k]}} + \bm{B}'{\bm{\nabla\rho}^{[k, 2k]}},
\end{equation}
where $\bm{A}' = ((\bm{A} - \bm{M})^\top \bm{D}^{-\frac{1}{2}})^\top$, $\bm{B}' = ((\bm{M} - \bm{B})^\top \bm{D}^{-\frac{1}{2}})^\top$, and
${\bm{\nabla\rho}^{[0, k]}}$, ${\bm{\nabla \rho}^{[k, 2k]}}$ are the first and last $k$ columns of $\bm{\nabla\rho} \in \mathbb{R}^{m \times 2k}$, respectively.
The $(c,l)$-entry of $\bm{\nabla\rho}$ is expressed as
\begin{equation}
  {(\bm{\nabla\rho})}_{c,l} =
  \begin{cases}
    -\bm{\Lambda}_{c,l} + \mu \bm{P}_{c,l}(\bm{T}), & \text{if } \bm{P}_{c,l} - \frac{\bm{\Lambda}_{c,l}}{\mu} \leq 0; \\
    0, & \text{otherwise.}
  \end{cases}
\end{equation}

Since computing the inversion of the matrix $\bm{I}_n +\frac{\tau}{2}\bm{\Omega} \in \mathbb{R}^{n \times n}$ is time-consuming, the Sherman-Morrison-Woodbury formula \cite[Appendix~A]{Wright99} is further applied to devise a much more efficient update scheme when $k \ll n$ that only computes the inversion of a much smaller matrix $\bm{I}_{2k} +\frac{\tau}{2} \bm{Y}^\top \bm{X} \in \mathbb{R}^{2k \times 2k}$ as follows:
\begin{equation}\label{SMW}
  \bm{T}' = \bm{T} - \tau \bm{X}(\bm{I}_{2k} + \frac{\tau}{2} \bm{Y}^\top \bm{X})^{-1} \bm{Y}^{\top} \bm{T}
\end{equation}
where $\bm{X} = [\bm{\nabla_T}, \bm{T}] \in \mathbb{R}^{n \times 2k}$ and $\bm{Y} = [\bm{T}, -\bm{\nabla_T}] \in \mathbb{R}^{n \times 2k}$.
According to \cite{WenY13}, $\bm{T}'$ by Eqs.~\ref{Cayley} and~\ref{SMW} has two important properties:
(\emph{i}) $\bm{T}'^{\top} \bm{T}' = \bm{I}_k$ if $\bm{T}^{\top} \bm{T} = \bm{I}_k$;
(\emph{ii}) $L_{\mu}(\bm{T}', \bm{\Lambda}) \leq L_{\mu}(\bm{T}, \bm{\Lambda})$.
Thus, with a proper step size $\tau$, OptStiefelGBB always converges to a feasible stationary point after sufficient iterations.
Following \cite{WenY13}, we use the Barzilai-Borwein method \cite{Barzilai88} to adaptively adjust the step size $\tau$ at each iteration.

\smallskip\noindent\textbf{Time Complexity.}
Let $T_1$ and $T_2$ denote the maximum numbers of iterations in the augmented Lagrangian method and OptStiefelGBB, respectively.
Computing the gradients $\bm{\nabla_T}$ for $L_{\mu}(\bm{T}, \bm{\Lambda})$ w.r.t.~$\bm{T}$ takes $O\left((|E| + n m) k\right)$ time.
Updating $\bm{T}$ with Eq.~\ref{Cayley} or~\ref{SMW} needs $O(n^3)$ or $O(n k^2)$ time, respectively.
As $m, k \leq n$ and $|E| \leq n^2$, the time complexity of Algorithm~\ref{alg1} is $O(T_1 T_2 n^3)$.
When $k \ll n$, $m = O(k)$, and $|E| = O(n)$, the time complexity of Algorithm~\ref{alg1} is reduced to $O(T_1 T_2 n k^2)$.

\subsection{Range-based Fair Rounding}
\label{subsec-rounding}
    
Like vanilla spectral methods, the output $\bm{H}$ of Algorithm~\ref{alg1} is a $k$-dimensional node embedding matrix where the $i$-th row vector $\bm{h}_i$ represents a fractional assignment of node $i \in [n]$ to $k$ clusters.
Thus, we must round the fractional solution into an integral one for partitioning.
However, a $k$-means clustering on embedding vectors, the standard rounding technique for spectral methods, is infeasible for the fair variant because the produced clusters may not be fair.

Next, we propose a novel rounding algorithm to produce a strictly fair partitioning scheme in Algorithm~\ref{alg2}.
Generally, it follows the same procedure as Lloyd's $k$-means clustering algorithm \cite{Lloyd82}, which initializes $k$ cluster centers, assigns each vector to one of the $k$ centers to generate the clusters, and updates each center to the median of each generated cluster iteratively until the stopping condition is met.
The difference from Lloyd's algorithm is that it requires the generated clusters at every iteration to be $(\bm{\alpha}, \bm{\beta})$-proportionally fair.

\input{algorithm_fair_rounding}

\smallskip\noindent\textbf{Nearly-Fair Initial Assignment via LP.}
Our rounding algorithm begins with running $k$-means++ \cite{ArthurV07} on the set $H = \{ \bm{h}_i \in \mathbb{R}^k: i \in [n] \}$ of embedding vectors\footnote{The nodes in $V$ and vectors in $H$, as well as the groups defined on $V$ and $H$, will be used interchangeably in this subsection.} to obtain an initial set $Q = \{ \bm{q}_1, \ldots, \bm{q}_k \}$ of centers at the first iteration.
Then, we formulate the following \emph{fair assignment} problem \cite{BeraCFN19} to assign the vectors in $H$ to $Q$ to minimize the $l_2$-loss while ensuring that the cluster around each center is $(\bm{\alpha}, \bm{\beta})$-proportionally fair:
\begin{definition}\label{fair-assi}
  Given a point set $H$ with $m$ disjoint groups $H_1, \ldots,$ $H_m$, a set $Q$ of $k$ centers, and two fairness vectors $ \bm{\alpha}, \bm{\beta} \in [0,1]^m$, find an assignment $\varphi: H \to Q$ that minimizes $\sum_{\bm{h} \in H} \lVert \bm{h} - \varphi(\bm{h}) \rVert_2$ and ensures that $\beta_{c} \cdot \lvert C_{l} \rvert \leq \lvert H_c \cap C_l \rvert \leq \alpha_c \cdot \lvert C_l \rvert, \forall c \in [m], l \in [k]$, where $C_l = \{ \bm{h} \in H : \varphi(\bm{h}) = \bm{q}_l \}$.
\end{definition}

By denoting an assignment as an indicator matrix $\bm{S} \in \{0, 1\}^{n \times k}$, where $\bm{S}_{i,l} = 1$ if $\varphi(\bm{h}_i) = \bm{q}_l$ and $0$ otherwise, the problem in Definition~\ref{fair-assi} is represented as the following integer program ($\mathrm{IP1}$):
\begin{subequations}
\label{assi}
\begin{align}
  \mathrm{IP1} := \min \;\; & \mathsf{trace}(\bm{C}^{\top} \bm{S}) \label{assi-obj} \\
  \text{subject to} \;\; & (\bm{A} - \bm{M})^{\top} \bm{S} \geq \bm{0} \label{assi-alpha} \\
  & (\bm{M} - \bm{B})^{\top} \bm{S} \geq \bm{0} \label{assi-beta} \\
  & \bm{S} \bm{1}_k = \bm{1}_n,\, \bm{1}_{n}^{\top} \bm{S} \geq \bm{1}^{\top}_k \label{assi-not-empty} \\
  & \bm{S} \in \{0,1\}^{n \times k}, \label{int}
\end{align}
\end{subequations}
where Eq.~\ref{assi-obj} denotes the minimization of the $l_2$-loss by setting the cost matrix $\bm{C} \in \mathbb{R}^{n \times k}$ with $\bm{C}_{i,l} = \lVert \bm{h}_{i} - \bm{q}_{l} \rVert_2$,
Eqs.~\ref{assi-alpha} and~\ref{assi-beta} represent the fairness conditions,
and the two constraints in Eq.~\ref{assi-not-empty} mean that each vector must be assigned to exactly one center, and each center must be assigned with at least one vector.
Since $\mathrm{IP1}$ is NP-hard \cite{ShmoysT93}, we relax it to a linear program ($\mathrm{LP1}$) by substituting the condition of Eq.~\ref{int} with $\bm{S} \in [0,1]^{n \times k}$.
After obtaining the optimal solution $\bm{S}^{*}$ to $\mathrm{LP1}$ using any LP solver, we assign each vector $\bm{h}_i$ to the center $\varphi(\bm{h}_i) = \bm{q}_{l^*}$ with $l^* = \argmax_{l \in [k]} \bm{S}^{*}_{i, l}$.

\smallskip\noindent\textbf{Reassignments to Generate Strictly Fair Clusters.}
Although the above assignment scheme can produce fairer clusters than standard $k$-means without fairness constraints, it may still violate the fairness conditions after rounding a fractional solution by $\mathrm{LP1}$ to an integral one.
Therefore, we must reassign some vectors (nodes) to other centers to produce a strictly fair partitioning scheme.
To reduce the quality loss led by reassignments, we should (\emph{i}) move as few nodes as possible and (\emph{ii}) find the node leading to the smallest Ncut growth at each reassignment.
For (\emph{i}), we should seek a fair assignment $\varphi'$ closest to the current assignment $\varphi$.
Given an assignment $\varphi$, we use a matrix $\bm{N} = (n_{cl})_{c \in [m], l \in [k]} \in \mathbb{R}^{m \times k}$, where $n_{cl} = \lvert H_c \cap C_l \rvert$, to denote the number of nodes from each of the $m$ groups in $k$ clusters.
Then, we define the problem of computing an optimal scheme with the least number of reassigned nodes in the following integer program ($\mathrm{IP2}$):
\begin{subequations}
\begin{align}
  \mathrm{IP2} := \min \;\; & \sum_{c = 1}^m \sum_{l=1}^k \vert n'_{cl} - n_{cl} \vert \\
  \text{subject to} \;\; & \beta_c n'_{l} \leq n'_{cl} \leq \alpha_c n'_{l}, \forall c \in [m], l \in [k] \label{N-fair}\\
  & \sum_{l=1}^k n'_{cl} - n_{cl} = 0, \forall c \in [m] \\
  & n'_{cl} \in \mathbb{Z}^+, \forall c \in [m], l \in [k]
\end{align}
\end{subequations}
where $n'_{cl}$ is the number of nodes in $C_l$ from $H_c$ after reassignments, $n'_{l} = \sum_{c=1}^{m} n'_{cl}$, and the objective value is twice as many as the number of reassignments from $\varphi$ to fair $\varphi'$.
To solve $\mathrm{IP2}$, we can call an exact IP solver to find its optimal solution, which may take exponential time in the worst case but is still efficient in practice because the values of $k$ and $m$ are pretty small or run a heuristic search method, e.g., hill-climbing, to obtain a near-optimal solution in polynomial time.
Using either method, we can find a reassignment scheme denoted as $\bm{\Delta} = \bm{N}' - \bm{N}$, where $\Delta_{cl} = n'_{cl} - n_{cl}$ is greater than $0$ if $\Delta_{cl}$ nodes from $V_c$ should be moved to $C_l$, is smaller than $0$ if $-\Delta_{cl}$ nodes from $V_c$ should be moved from $C_l$, or is equal to $0$ if no reassignment is needed.
For (\emph{ii}), we first select a pair of clusters $C_{l}$ and $C_{l'}$ with $\Delta_{cl} < 0$ and $\Delta_{cl'} > 0$ for a specific $c$, which corresponds to the movement of a node in $V_c$ from $C_l$ to $C_{l'}$.
As reassignments can reduce partition quality, we want to find the node leading to as small Ncut growth as possible.
Since computing the Ncut value of a given partitioning from scratch is time-consuming and we only need to recalculate the changed parts (i.e., one node and two clusters) for a reassignment, we obtain the following equation to update the Ncut value incrementally by taking the difference between the Ncut values after and before reassigning a node $i$ from $C_l$ to $C_{l'}$ and eliminating all the unchanged terms:
\begin{equation}\label{eq-delta}
  \delta_{l,l'}(i) = \frac{\mathsf{cut}(C_l) - d_i + 2 z_{il}}{\mathsf{vol}(C_l) - d_i} - \frac{\mathsf{cut}(C_l)}{\mathsf{vol}(C_l)} + \frac{\mathsf{cut}(C_{l'}) + d_i - 2 z_{il'}}{\mathsf{vol}(C_{l'}) + d_i} - \frac{\mathsf{cut}(C_{l'})}{\mathsf{vol}(C_{l'})},
\end{equation}
where $z_{il} = \sum_{j \in C_l} w_{ij}$.
For each reassignment, we compute $\delta_{l,l'}(i)$ based on Eq.~\ref{eq-delta} for each eligible node $i$ (i.e., $i \in V_c \cap C_l$) and pick the node with the smallest $\delta_{l,l'}(i)$ accordingly.
After that, we update $\bm{N}, \bm{\Delta}$ and select the next group and pair of clusters for reassignment.
The above process terminates when $\bm{N} = \bm{N}'$ and all the clusters have been fair.
We obtain a fair partitioning $\mathcal{C} = \{C_1, \ldots, C_k\}$ from the final assignment, based on which we obtain an updated set $Q = \{\bm{q}_1, \ldots, \bm{q}_k\}$ of centers where $\bm{q}_l = \frac{1}{|C_l|} \sum_{i \in C_l} \bm{h}_i$.
After $Q$ is updated, the above procedures, i.e., solving $\mathrm{LP1}$ \& $\mathrm{IP2}$ and reassigning nodes for fairness, will be executed again to acquire a new fair partitioning.
This iterative procedure will terminate until the set $Q$ of centers does not change significantly between two iterations or the total number of iterations reaches a predefined threshold $T_3$.
Finally, a fair partitioning with the smallest Ncut value among all iterations will be returned as the final solution $\mathcal{C}^*$.

\smallskip\noindent\textbf{Time Complexity.}
The $k$-means++ algorithm takes $O(T_0 n k^2)$ time, where $T_0$ is the number of iterations.
At each iteration of Algorithm~\ref{alg2}, computing $\bm{C}$ takes $O(n k^2)$ time.
Using the interior point method \cite{Karmarkar84}, solving $\mathrm{LP1}$ takes $O(n^{4.5} k^{4.5} m)$ time in the worst case.
The hill-climbing search takes $O(n m k)$ time to solve $\mathrm{IP2}$.
Furthermore, the time to perform one reassignment is $O(n)$, and there are at most $O(n)$ reassignments.
But unlike Lloyd's $k$-means clustering algorithm, Algorithm~\ref{alg2} does not guarantee convergence to a local optimum after sufficient iterations.
If the convergence is not reached, it will stop and return the best partitioning found after $T_3$ iterations.
Suppose that $T_0 = O(T_3)$, the overall time complexity of Algorithm~\ref{alg2} is $O(T_3 n^{4.5} k^{4.5} m)$.
