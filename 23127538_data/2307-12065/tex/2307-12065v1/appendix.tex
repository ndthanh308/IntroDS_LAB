\appendix

\section{Dataset Preparation}
\label{app:dataset}

Our experiments use eight public real-world datasets with sensitive attributes in \cite{10097603, snapnets} and one synthetic dataset.
Here, we provide detailed information on each dataset as follows.
\begin{itemize}
  \item \textbf{Facebook} \cite{Mastrandrea15} is a social network representing the friendships between students in a high school in Marseilles, France. We remove the nodes whose \textit{gender} attributes are unknown and extract the largest connected component with $155$ nodes and $1,412$ edges.
  \item \textbf{German}\footnote{\url{https://github.com/yushundong/Graph-Mining-Fairness-Data/tree/main/dataset/german}} is a similarity graph created from an i.i.d. dataset where each record contains both categorical and numerical attributes to predict a person's credit risk. It consists of $1,000$ nodes and $21,742$ edges with \textit{gender} as the sensitive attribute.
  \item \textbf{SBM} is a synthetic stochastic block model \cite{HOLLAND1983109} graph comprising $5$ clusters and $5$ groups with intra-cluster and inter-cluster probabilities of 0.25 and 0.05, respectively. For cluster sizes, we set $\vert C_1 \vert = 500$, $\vert C_2 \vert = 200$, $\vert C_3 \vert = 100$, $\vert C_4 \vert = 100$, $\vert C_5 \vert = 100$.
  For each node in cluster $C_l$, we assign it to group $l$ with probability $0.6$ or to any other group $l' \neq l$ with probability $0.1$.
  According to our generation procedure, the five ground-truth partitions all have balance values of around $0.2$. When $\sigma = 0.8$, the best partitioning scheme will be very close to the ground truth, and when $\sigma < 0.8$ (e.g., $0.2$), reassignments will be required to ensure fairness.
  Using the SBM graph, we aim to evaluate the performance of each algorithm in controllable settings. On the one hand, can it recover the ground-truth partitions when the fairness constraint is relatively loose? On the other hand, how much does it spend on modifying the ground-truth partitions to be fair (as measured by the Ncut increase) with relatively tighter constraints?
  \item \textbf{DBLP}\footnote{\url{https://github.com/yushundong/Graph-Mining-Fairness-Data/tree/main/dataset/dblp}} is a coauthor network, where there is an edge between two authors if they have ever collaborated. We merge the five original groups \{ `Asia', `Oceania', `North America', `South America', `Europe'\} by \emph{continent} into three new groups \{`Asia-Oceania', `America', `Europe'\}. We also extract the largest connected component with $1,061$ nodes and $2,576$ edges for evaluation.
  \item \textbf{LastFM}\footnote{\url{https://snap.stanford.edu/data/feather-lastfm-social.html}} is a social network with $7,624$ nodes and $27,806$ edges representing the friendships between users on the last.fm website. The nodes are divided into four groups by \emph{country}.
  \item \textbf{Deezer}\footnote{\url{https://snap.stanford.edu/data/feather-deezer-social.html}} is a social network with $28,281$ nodes and $92,752$ edges representing the friendships between users on Deezer, where \textit{gender} is used as the sensitive attribute.
  \item \textbf{Credit}\footnote{\url{https://github.com/yushundong/Graph-Mining-Fairness-Data/tree/main/dataset/credit}} is also a similarity graph created from an i.i.d. dataset for credit risk analysis. We extract the largest connected component with $29,460$ nodes and $136,196$ edges and use \emph{education} as the sensitive attribute.
  \item \textbf{Pokec}\footnote{\url{https://snap.stanford.edu/data/soc-Pokec.html}} is an online social network in Slovakia. The directed edges denoting the follower-followee relationships in the original graph are transformed into undirected edges. For sensitive attribute \textit{age}, we remove the nodes whose age information is not available and transform it into a category attribute as `1': [0, 18], `2': [19, 25], `3': [26,35], `4': 36+. Then, we obtain an undirected graph with $1,097,077$ nodes and $10,792,894$ edges, referred to as \textbf{Pokec-A} in the experiments. Using sensitive attribute \textit{gender}, we obtain an undirected graph with $1,632,803$ nodes and $22,301,964$ edges, referred to as \textbf{Pokec-G} in the experiments.
\end{itemize}

\section{Parameter Tuning}
\label{app:parameter}

The parameters in FNM include (1) $T_1$, $T_2$, $\varepsilon_1$, $\varepsilon_2$, $\mu_0$, $\xi$, and $\tau$ in Algorithm 1 as well as $T_0$, $T_3$, and $\varepsilon_3$ in Algorithm 2.
Among these parameters, the effects of $T_1$, $T_2$, $\varepsilon_1$, $\varepsilon_2$, $\tau$, $T_0$, $T_3$, and $\varepsilon_3$ on FNM are apparent: larger numbers of maximum iterations ($T_0$, $T_1$, $T_2$, $T_3$), smaller error parameters ($\varepsilon_1$, $\varepsilon_2$, $\varepsilon_3$), and smaller initial step size ($\tau$) lead to higher-quality solutions but lower time efficiency.
In practice, we keep the default values $T_0 = 100$ and $\tau = 10^{-3}$ that have been widely used in existing implementations.
For $T_1$, $T_2$, and $T_3$, we attempt to gradually increase their values until the performance of FNM cannot be improved anymore.
Then, these ``large enough'' values are used for FNM in all the remaining experiments.
The values of $\varepsilon_1$, $\varepsilon_2$, and $\varepsilon_3$ are determined similarly.

The procedure of tuning parameters $\mu_0$ and $\xi$ for FNM is a bit more complex because too large or too small values may lead to poor performance.
There are two issues to consider in the parameter tuning:
(\emph{i}) the value of $\mu_0$ should not be too large or too small to avoid the first sub-problem being ill-conditioned or the same as the unconstrained problem;
(\emph{ii}) the value of $\mu_t$ should not increase too fast or too slow over $t$ so that the number of sub-problems to solve is appropriate.
We note that several studies (e.g., \cite{curtis2015adaptive}) consider adaptively adjusting $\mu_t$ in the augmented Lagrangian method.
Nevertheless, we adopt a simple yet effective scheme that chooses an appropriate value of $\mu_0$ and uses a fixed amplification parameter $\xi > 1$ to adjust it by setting $\mu_{t+1} = \xi \mu_t$ over iterations.
In each experiment, we perform a grid search on $\xi \in \{2, 4, 6, 8, 10\}$ and $\mu_0 \in \{10^{-4}, 10^{-2}, 10^0, 10^2\}$ and select the combination of $\xi$ and $\mu_0$ with the smallest Ncut value.
The grid search procedure on five datasets when $k = 5$ and $\sigma = 0.2$ are shown in Figure~\ref{fig-grid}.
From Figure~\ref{fig-grid}, we observe that, unlike other parameters, no single combination of $\xi$ and $\mu_0$ achieves good performance across all datasets.
Therefore, we run the grid search to find appropriate values of $\xi$ and $\mu_0$ individually for each experiment.

% Figure environment removed

\section{More Results on the Effects of \texorpdfstring{$\sigma$}{Sigma} and \texorpdfstring{$k$}{k}}
\label{app:additional}

The first rows of Figures~\ref{fig-sigma-others} and~\ref{fig-k-others} present the results for the effects of $\sigma$ and $k$ on different algorithms in the remaining four datasets not included in Figures~\ref{fig-k} and~\ref{fig-sigma}. We observe similar trends for Ncut and balance values to those analyzed in Section~\ref{sec-exp}.

Next, we illustrate the running time of different algorithms with varying $\sigma$ and $k$ on all the nine datasets in Figures~\ref{fig-sigma-time} and~\ref{fig-k-time} as well as the second rows of Figures~\ref{fig-sigma-others} and~\ref{fig-k-others}.
In terms of time efficiency, we find that FNM generally runs slower when $\sigma$ is smaller or $k$ is larger.
A larger $k$ naturally leads to higher computational costs per iteration in both the embedding and rounding phases.
However, the running time of FNM is sometimes shorter when $k$ is larger because the number of iterations in Algorithm 1 is also affected by parameter settings w.r.t.~different $k$'s.
Then, a smaller $\sigma$ corresponds to a tighter fairness constraint and a more different solution from the unconstrained one, thus requiring more iterations in Algorithm 1 for convergence.
But this trend may not strictly always follow also because of various parameter settings for different values of $\sigma$.
In addition, we also observe that FNM runs slower than SC as it is more time-consuming in both the embedding and rounding phases.
But FNM runs faster than FSC in most cases since it does not require eigendecomposition.
Compared to sFSC, which improves the scalability of FSC by avoiding eigendecomposition on dense matrices, FNM runs slower on smaller graphs due to a longer time for fair rounding but becomes faster on larger graphs owing to the efficiency improvements for fair embedding.

% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed
