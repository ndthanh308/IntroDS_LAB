\section{Experiments}
\label{sec-exp}

In this section, we perform extensive empirical evaluations of our FNM algorithm.
We introduce our experimental setup in Section~\ref{setup} and describe our results in Section~\ref{results}.

\subsection{Experimental Setup}
\label{setup}

\noindent\textbf{Datasets.}
We use eight public real datasets with sensitive attributes and one synthetic dataset in the experiments.
\emph{Facebook}, \emph{LastFM}, \textit{Deezer}, \textit{Pokec-A}, \textit{Pokec-G} are all social networks;
\emph{DBLP} is a coauthor network;
\emph{German} and \emph{Credit} are similarity graphs created from i.i.d.~data;
and \emph{SBM} is generated from a stochastic block model with random groups.
If a graph is disconnected, we will extract and use its largest connected component.
Table~\ref{stats} summarizes the statistics of all processed datasets.
Detailed descriptions of the above datasets are provided in Appendix~\ref{app:dataset}.

\input{table_dataset_statistics}

\smallskip\noindent\textbf{Baselines.}
We compare our FNM algorithm with the following three baseline methods for graph partitioning:
\emph{(i)} spectral clustering (SC) \cite{YuS03}, \emph{(ii)} fair spectral clustering (FSC) \cite{KleindessnerSAM19}, and \emph{(iii)} scalable fair spectral clustering (sFSC) \cite{pmlr-v206-wang23h}.
In the ablation study, we compare our range-based fair spectral embedding (rFSE) in Algorithm~\ref{alg1} with the following six node embeddings, i.e.,
spectral embedding (SE) \cite{YuS03}, fair spectral embedding (FSE) \cite{KleindessnerSAM19}, scalable fair spectral embedding (sFSE) \cite{pmlr-v206-wang23h}, DeepWalk (DW) \cite{PerozziAS14}, Node2Vec (N2V) \cite{GroverL16}, and FairWalk (FW) \cite{RahmanSBZ19};
and our range-based fair rounding (FR) in Algorithm~\ref{alg2} with four alternatives, i.e., $k$-means++ (KM+) \cite{ArthurV07}, $k$-means++ with reassignments (K+R), fair $k$-means (FK) \cite{BeraCFN19}, and solving $\mathrm{IP1}$ directly (IP).
Since the IP/LP solver fails to provide solutions in a reasonable time, FR, FK, and IP do not work on \textit{Pokec-A} and \textit{Pokec-G}, and we alternatively use K+R together with rFSE to obtain the results of FNM on both datasets.

\smallskip\noindent\textbf{Parameter Settings.}
For FNM,  $\bm{\alpha}, \bm{\beta}$ are parameterized by $\sigma \in [0,1]$ as per Section~\ref{sec-def}.
By default, we set $\sigma = 0.2$ and $0.8$ (resp.~the common 80\%-rule) to generate tight and loose fairness constraints.
For Algorithm~\ref{alg1}, we set $T_1 = 100$, $\varepsilon_1 = 10^{-6}$, $T_2 = 2,000$, $\tau = 10^{-3}$, and $\varepsilon_2 = 10^{-3}$.
We perform a grid search on $\xi \in \{2, 4, \ldots, 10\}$ and $\mu_0 \in \{10^{-4}, 10^{-2}, 10^0, 10^2\}$ and select the combination of $\xi, \mu_0$ achieving the lowest objective value for each experiment.
For Algorithm~\ref{alg2}, we set $T_0 = 100$, $T_3 = 10$, and $\varepsilon_3 = 10^{-4}$ for all experiments.
Further details of our parameter-tuning procedure are provided in Appendix~\ref{app:parameter}.
For the baselines, we follow the default parameters or use the recommended methods for parameter tuning as given in their original papers.

\smallskip\noindent\textbf{Evaluation Metrics.}
Each method is evaluated in three aspects.
First, we measure partition quality by the Ncut value in Eq.~\ref{eq-Ncut}.
Second, we adopt the notion of \emph{balance} in \cite{Chierichetti0LV17, BeraCFN19} as the metric for fairness.
Given a set $\mathcal{C} = \{C_1, \ldots, C_k\}$ of $k$ clusters and a set $\{V_1, \ldots, V_m\}$ of $m$ groups, the proportion of group $c$ in cluster $C_l$ is defined as $r_{cl} = \lvert C_l \cap V_c \rvert / \lvert C_l \rvert$.
Then, the \emph{balance} of $\mathcal{C}$ is defined by $\mathsf{balance}(\mathcal{C}) := \min_{c \in [m], l \in [k]} \min \big\{ r_c/r_{cl}, r_{cl}/r_c \big\}$, where $r_c = |V_c| / n$.
Higher \emph{balance} implies that the partitioning scheme is closer to being proportionally fair.
Balance also serves as an indicator of whether the fairness constraints parameterized by $\sigma$ are satisfied because $\mathcal{C}$ is $(\bm{\alpha}, \bm{\beta})$-proportionally fair iff $\mathsf{balance}(\mathcal{C}) \geq 1 - \sigma$.
Third, we use \emph{CPU time} to evaluate the efficiency of each method.

\smallskip\noindent\textbf{Implementation.}
We implement FNM in Python 3 and use Gurobi Optimizer to solve LP and IPs.
For each baseline, we either use a standard implementation in the SciPy library or the implementation published by the original authors.
The experiments were conducted on a desktop with an Intel Core i5-9500 processor @3.0GHz and 32GB RAM running Ubuntu 20.04.
Our code and data are published at \url{https://github.com/JiaLi2000/FNM}.

\subsection{Experimental Results}
\label{results}

\noindent\textbf{Overview.}
Table~\ref{default} presents the performance of different algorithms for normalized-cut graph partitioning with two fairness constraints parameterized by $\sigma = 0.8$ and $0.2$ when $k = 5$ on all nine datasets.

In terms of partition quality and fairness, the (unconstrained) SC mostly achieves the lowest Ncut values but fails to provide a fair partitioning when $\sigma = 0.8$ on four datasets while never meeting tighter fairness constraints when $\sigma = 0.2$.
Although FSC and sFSC provide more balanced partitions than SC in some cases, they still cannot guarantee the satisfaction of fairness constraints.
In addition, FSC does not return any results on medium and large graphs with over $10$k nodes due to huge memory consumption for eigendecomposition on dense matrices.
Next, we observe that FNM always provides fair partitioning schemes in all cases.
If unconstrained SC returns fair solutions when $\sigma = 0.8$, FNM will achieve nearly the same Ncut values.
Otherwise, the Ncut values of FNM will increase slightly to ensure fairness.
Moreover, the Ncut values of FNM for $\sigma = 0.2$ are significantly higher than those for $\sigma = 0.8$, which can be regarded as the \emph{price of fairness}.

In terms of time efficiency, FNM runs slower than SC as it is more time-consuming in embedding and rounding.
But FNM runs faster than FSC in most cases since it does not require eigendecomposition.
Compared to sFSC, which improves the scalability of FSC by avoiding eigendecomposition on dense matrices, FNM runs slower on smaller graphs due to a longer time for fair rounding but becomes faster on larger graphs owing to the efficiency improvements for fair embedding.
Finally, FNM is more efficient when $\sigma = 0.8$ than $\sigma = 0.2$ because of fewer iterations for convergence.

\input{table_algorithm_performance}

\smallskip\noindent\textbf{Trade-off between Quality and Fairness.}
We present the performance of four algorithms with different fairness constraints parameterized by $\sigma = 0.1, 0.2, \ldots, 1$ in Figure~\ref{fig-sigma}.
We ignore $\sigma = 0$ since no solution may exist for indivisibility.
Since the results of SC, FSC, and sFSC are independent of $\sigma$, they are drawn as horizontal lines in the figure.
For FNM, as the value of $\sigma$ decreases (when the fairness constraints become looser), the Ncut and balance values also decrease.
When $\sigma = 1$ (no fairness constraint), FNM returns partitions of similar quality to SC.
To our best knowledge, FNM is the only known algorithm that achieves different trade-offs between partition quality (i.e., \emph{Ncut}) and fairness (i.e., \emph{balance}) w.r.t.~$\sigma$.

We illustrate the performance of four algorithms as a function of the numbers of clusters $k$ in Figure~\ref{fig-k}.
We vary $k$ from $2$ to $10$ on three smaller datasets and from $5$ to $50$ on two larger datasets.
For each algorithm, the Ncut value increases with $k$.
Meanwhile, the balance of each algorithm generally drops with increasing $k$, and FNM is the only algorithm that consistently achieves a balance of at least $1 - \sigma$, i.e., guaranteeing the fairness constraints.
Furthermore, FNM has comparable Ncut values to SC, FSC, and sFSC and higher balances on most datasets when $\sigma = 0.8$.
But on the \emph{LastFM} dataset, FNM has higher Ncut values than other algorithms when $\sigma = 0.8$ because it performs reassignments to ensure a balance of at least $0.2$.
When $\sigma = 0.2$, as FNM assigns more nodes to non-closest clusters for fairness than when $\sigma = 0.8$, it often has inferior partition quality, especially when $k$ is large.

Note that the results examining the partition quality by varying $\sigma$ and $k$ on the remaining four datasets, as well as the time efficiency by varying $\sigma$ and $k$ on all the nine datasets, are deferred to Appendix~\ref{app:additional}.

\input{figure_sigma}
\input{figure_k}

\smallskip\noindent\textbf{Ablation Study.}
In the ablation study, we first run each embedding method to obtain a $k$-dimensional node embedding (for $k = 5, 20$) on each graph and use the same fair rounding in Algorithm~\ref{alg2} to produce two fair partitioning schemes with $\sigma = 0.8, 0.2$ from node vectors.
SC, FSC, sFSC, and FNM are thus renamed SE, FSE, sFSE, and rFSE since we only test their spectral embedding performance.
We report the Ncut values on five small datasets (since FSE and FW cannot provide any result on four large datasets) in Table~\ref{tbl-ablation-embed}.
Our method, rFSE in Algorithm~\ref{alg1}, achieves the best or second-best partition quality among all methods in almost all cases.
Here, SE and FSE/sFSE are special cases of rFSE when $\sigma = 1$ (i.e., w/o fairness) and $0$ (i.e., with strict proportionality), respectively.
As such, SE performs closely to rFSE when $\sigma = 0.8$, and FSE/sFSE provides similar embeddings to rFSE when $\sigma = 0.2$.
Especially, sFSE slightly outperforms rFSE when $\sigma = 0.2$. Since sFSE and rFSE both provide embeddings with fractional fairness constraints, which are looser than the original integral ones, using a tighter (fractional) constraint than required (i.e., equal to $1 - \sigma$) in the embedding phase might help improve the overall performance after rounding.
The partition quality of deep learning-based node embeddings (with or without fairness) is much inferior to that of rFSE and other spectral embeddings since they are not designed for graph partitioning.

Then, we evaluate the performance of each rounding method on node vectors provided by rFSE on five small datasets for $k = 5, 20$ and $\sigma = 0.8, 0.2$ and present the Ncut values in Table~\ref{tbl-ablation-round}.
Our method, FR in Algorithm~\ref{alg2}, performs best among the four fair methods we compare.
We find that fair $k$-means for i.i.d.~data in \cite{BeraCFN19} may not be appropriate to round embedding vectors though it adopts the same fairness notion as ours.
Despite having the lowest Ncut values, $k$-means++ cannot produce fair partitions.
When $\sigma = 0.8$, FR provides partitions closer to $k$-means++ than when $\sigma = 0.2$ since fewer or no reassignments are required as fairness constraints are looser.

\input{table_ablation_embedding}
\input{table_ablation_rounding}
