\vspace{-0.25em}
\section{Related Work}\label{sec:relatedwork}
\vspace{-0.25em}


SASTs have been adapted for finding security vulnerabilities~\cite{CM04}, resource leaks~\cite{2013_FlowDroidPreciseContext_arzt,2018_AmandroidPreciseGeneral_wei, OMJ+13, CGM16, AB16, KFSZ18, BBD+16}, enforcing policies~\cite{ARLB14}, and crypto-API misuse~\cite{RXA+19,cognicrypteclipse, KNR+17,FHM+12, EBFK13, BD16, NWA+17, ZCD+19}.
Our work studies the perspectives and beliefs of practitioners regarding SASTs through 20 in-depth interviews, and is closely related to work in three areas, namely prior studies on the usability of static analysis tools, research on evaluating SASTs, and the study of general security practices in industry.



\myparagraphnew{Usability of Static Analysis Tools}
Researchers have studied how practitioners use static analysis tools and their perspectives on improving the output of static analysis tools~\cite{APM+07,APH+08,AP08,EN08,BBC+10,DFLO19}.
In particular, Johnson \etal~\cite{JSMB13} found that poorly presented output, including false positives, is one of the main problems from developers' perspective.
This has been confirmed by later studies~\cite{CB16}, where they suggested that the false positive rate should be around or below 20\%.
In a similar vein, Distefano \etal~\cite{DFLO19} recognized that while false negatives matter, it is difficult to quantify false negative rate compared to false positive rate, and thus, it is prudent to focus on optimizing the latter.
Our work complements existing literature by detailing how practitioners across the industry choose and perceive \sasts{}~(\fnumber{4}, \fnumber{5}, \fnumber{7}).
However, our qualitative findings deviate from prior work (in the context of security), \ie we find that developers prefer low false negatives, and are willing to tolerate high false positive rates (\fnumber{10}, \fnumber{11}) provided the tool detects vulnerabilities.


Further, recent work~\cite{APH+08,AP08, SVJ+15a} proposes allowing subjective interpretations of defect warnings for productivity, \ie the notion of ``effective false positives'', which has been adapted in the context of security by Wickert \etal~\cite{WBS+22}.
We study whether this concept works in the context of security, \ie whether practitioners see merit in letting developers decide what constitutes a vulnerability, and find that it does not, drawing attention to the risks of letting developers become the arbiters of false positives (\fnumber{12}).

Finally, prior work has focused on particular usability aspects of static analysis tools, such as the use cases and their contexts~\cite{NWA20, VPP+20a,NSB22}, and filtering warning messages~\cite{IRFW19}.
Our analysis of the pain points experienced by developers echoes some of these concerns (\fnumber{17}), highlights unique challenges practitioners face (\fnumber{4}, \fnumber{5}, \fnumber{15}), and culminates in our discussion of a path forward for both researchers and practitioners towards better and more useful SASTs.




\myparagraphnew{Evaluating \sasts} Historically, security researchers have focused on creating benchmarks for evaluating \sasts with a focus on precision, recall, and efficiency, with the help of benchmarks, such as
ICC-Bench~\cite{2018_AmandroidPreciseGeneral_wei}, DroidBench~\cite{FAR+13}, CryptoAPI-Bench~\cite{ARY19}, ApacheCryptoAPIBench~\cite{AXR+22}, OWASP Benchmark~\cite{owasp-benchmark}, Parametric Crypto Misuse Benchmark~\cite{WRE+19}, Ghera~\cite{MR17}, and CamBench~\cite{SWK+22}.
Moreover, prior work has proposed automated evaluation using benchmarks~\cite{PBW18,LPP+22}.
Our study reveals that practitioners, in general, do not trust third-party benchmarks, due to them being basic (\ie not representative of real vulnerabilities), or worse, biased in favor of a specific SAST tool.
Since SAST tools often accompany custom benchmarks claimed to be general (as is the case with many of the benchmarks above), we cannot deny this perception.

A recent body of research considers the limitations of existing benchmarks and leverages mutation testing to uncover flaws in the detection capabilities of SASTs~\cite{BKM+18,AKM+21, ACK+22, AKM+21-Demo, AAR+23-demo}.
As we discuss in Section~\ref{sec:discussion}, enabling better benchmarks or automated evaluation of SASTs using such evolving approaches may be a path forward towards providing developers with what they care most, \ie SASTs that can detect real, valuable, vulnerabilities.



\myparagraphnew{Study of Security Practices in Industry}
Finally, prior work has studied the relationship between developers' security expertise, and the actual implementation of secure software.
For example, researchers have identified that practitioners need security-specific support in the form of developer-friendly APIs and supporting tools~\cite{ASW+17a, GIW+18, GALF20, NKMB16, SJM+15} to make better security choices, recognizing that practitioners may not know enough about security~\cite{GS16,Wv08,CB16}.
Furthermore, researchers have explored how developers address security-specific tasks and the challenges they face~\cite{ASW+17, GKB+22, HGK+19, SKDM22, JFB+22}.
Our work complements previous studies by exploring how developers choose \sasts (\fnumber{4}, \fnumber{5}).
Moreover, we explore how developers depend on \sasts to cover their knowledge gaps (\fnumber{6}) and the challenges, beliefs, and perceptions associated with implementing security in software with the help of \sasts~(\fnumber{8}--\fnumber{17}).






























