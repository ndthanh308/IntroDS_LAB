\vspace{-0.5em}
\subsection{\interviewSectionLimitExpectSast}\label{sec:interviewSectionLimitExpectSast}
\vspace{-0.5em}
Developers shared that while they expect \sast tools to detect all vulnerabilities as long as they are within scope, they generally do not expect \sasts to detect all types of security vulnerabilities, \eg \myquoteinline{If it is in the scope, then it can detect, but my expectation is not like static analysis is the final solution...(P10)}.
When asked whether this assertion was based on ``belief'' or ``evidence'', P18 explained that it was \myquoteinline{based on belief}, further explaining that \myquoteinline{We have the user ratings of our tools and there are many stars in the repos. So we think that it is reliable, and many developers use that, so it must be good}.


\finding{Although expressing that no tool can find everything, participants {\em believe} that SASTs (should or do) detect {\em all vulnerabilities considered \underline{within scope}} (\ie which a SAST tool claims to detect).}
When asked to give examples of vulnerabilities that developers do not expect \sasts to detect, runtime (Input/Output), external component, and software goal based issues frequently came up, \eg \myquoteinline{These tools are pretty agnostic of the goals that we have put forward in the first place. They can only really seem to process errors in code and not errors in software taken as a whole (P03)}.


Interestingly, when we asked developers if they consider a \sast{} to be acceptable to use even if it misses some more difficult issues, they generally expressed that they do, \eg \myquoteinline{I don't expect that there will ever be a tool that will
look at a piece of code as complex as \angled{product} and find all the security issues. \dots But any issue fixed is an issue fixed and that's a good thing} (P02).
When asked to elaborate, participants shared different reasons for finding such \sasts acceptable, such as lack of alternatives, \myquoteinline{If there is no other accessible alternative, then I would go and accept whatever it offers} (P10) and additional techniques being used to cover for (issues in) \sasts \eg \myquoteinline{\dots for our team, the manual review part is actually the biggest deal for us. \dots So, for our team, I think that should not be a big issue} (P07).

\finding{Participants consider SASTs valuable even if they miss certain vulnerabilities, as {\em finding something would be better than nothing.}}


\myparagraphnew{Reducing False Positives vs False Negatives}
In the context of program analysis, increasing analysis sensitivity decreases false negatives, while increasing false positives, and vice versa.
Contemporary literature asserts that false positives are a major reason for practitioners to avoid using SASTs~\cite{JSMB13, IRFW19} since \textit{"Developer Happiness is Key"}~\cite{SAE+18}, and argues that it is necessary to reduce false positives, \textit{in general}.
That is, conventional wisdom dictates that developers want lower false positives even at the cost of false negatives, which has led to a significant focus on increasing the precision of SASTs in academia and industry in pursuit of practicality~\cite{RXA+19, 2018_AmandroidPreciseGeneral_wei, 2013_FlowDroidPreciseContext_arzt, FAR+13, KFSZ18, 2015_ScalablePreciseTaint_huang, AB16}.


However, we found considerable evidence that contradicts this understanding of the developers' perspective on the soundness-precision tradeoff, with participants strongly favoring lower false negatives, even at the cost of increased false positives. As P04 and P06 state,

\myquote{False negative for sure. I just told you the amount of the price of the bug (in millions), so I don't care if there are 10 false positives. False negative - that one is going to kill you.}{P04\textsubscript{Automobile Sensors}}
\myquote{From my understanding it is actually more threatening that we aren't even aware of the vulnerability...So to me, false negative should be bigger concern...it (false positives) wastes time of developers, but it is not harmful in the whole picture}{P06\textsubscript{Software Service}}
P14 even argued that false positives indicate a working \sast, and when it comes to security, no stone should be left unturned, \myquote{If you're getting a bunch of false positives, then that typically means your static code analysis tool is doing its job. \dots I'd rather my security tool be annoying and tell me about every single possible issue over it not telling me anything and just letting security things slide through.}{P14\textsubscript{Law Enforcement }}
\finding{Nearly all the practitioners expressed a preference for {\em fewer false negatives}, \ie as long as the \sast is able to find valid security vulnerabilities, they would tolerate and even prefer few false negatives at the cost of many false positives.}
Since existing literature argues that false positive rate for program analysis should not exceed 20\%~\cite{JSMB13, SFZ11, CB16, BBC+10 }, we requested our participants to approximately quantify their preference (or experience) regarding the acceptable proportion of false positives to true positives.
For most participants, this preference was far higher than 20\% as long as the tool detected some valid vulnerabilities (i.e., had true positives), as indicated in \fnumber{10} as well.
For instance, P02 admitted to dropping a tool in favor of manual analysis due to overwhelming false positives without a single valid vulnerability:
\myquote{I wouldn't mind wading through $100$ false positives, if I thought there were actually going to be genuine issues there}{P02\textsubscript{OSS - Java App Server}}
Some participants expressed tolerance for 80\% or more false positives, although not to the extreme extent as P02.
\myquote{The acceptable range is for (reducing) one false negative, that there could be five false positive}{P10\textsubscript{Healthcare}}
Further, some, \eg P09, stated that 80\% false positives were common in a tool they were currently using; although they were dismayed by the low number of serious, real vulnerabilities found:
\myquote{(At present) 80\% of them are actually false positives and 20\% of them are actually something we can fix. Even those 20, you don't generally find serious problems.}{P09\textsubscript{Fintech}}
Finally, P01 expressed a lower tolerance for FPs than most other practitioners, stating that \myquote{We ended up with $20\%$ real issues. $80\%$ just false positives. And one of my last actions in that company before leaving was saying, `Hey, look, this tool is a waste of time'}{P01\textsubscript{Program Analysis for Security}}
\finding{Practitioners are generally more tolerant of false positives than the 20\% upper bound proposed in literature, given their preferences and the tools they currently use, with some finding even 80\% or more false positives practical.}




\myparagraphnew{Effective False Positives and \sast}
Due to the perceived notoriety of false positives affecting adoption of general static analysis tools, the notion of \textit{effective false positives}, defined as \textit{"any report from the tool where a user chooses not
to take action to resolve the report"}~\cite{SVJ+15a}, or in other words - letting the developer determine whether {\em any} reported defect is a false positive, is gaining attention.
Effective false positives have further been contextualized in \sasts in the form of letting a developer determine whether a reported vulnerability should be considered as within the scope of security context~\cite{WBS+22}.
However, several participants cautioned that in their experience, developers may not make the right call when it comes to identifying an effective false positive issue \eg when an insecure code segment is considered "\textit{inactive}".

P14 and P15 expressed something similar to "\textit{The Developer is the enemy}" threat model~\cite{Wv08}. P15 argues that \myquoteinline{Junior developers don't understand what is the impact}, and P14 states (on effective false positives):
\myquoteinline{From a security standpoint, you can't really trust, you shouldn't trust other devs, and users to always know that something could be potentially insecure. So you need to make sure that it's not possible for it to happen or reduce the possibility of it happening as much as possible (by removing insecure code)}.
Furthermore, P14 cautioned that developers may habitually mark an actual issue as false positives erroneously, \myquote{It seems familiar, but it may be new. And then you're just going to ignore it because it's close enough to something you've seen in the past, and you just say that it's OK. So we do need to be vigilant on those false positives to make sure that they are truly false positives}{P14\textsubscript{Law Enforcement}}
P04 made a similar remark about making mistakes in deciding whether to run \sast or not on code patches, sharing that they had a vulnerability that could've been detected using static analysis, but was not due to the deliberate decision of  not using \sast, costing millions:
\myquote{The undisclosed amount is in a couple of millions. \dots  We had two static analysis tools which should be used, but the decision from the management was because it was a minor fix that they did not use them}{P04\textsubscript{Automobile Sensors}}
\finding{Practitioners are generally against letting developers define ``effective'' false positives, or letting them decide when to run SASTs. This reservation stems from their prior experience of the adverse cost of leaving a vulnerability in the code, and/or from their knowledge of developers (1) lacking an understanding of the impact of vulnerabilities, (2) being prone to incorrectly marking actual issues as false positives, (3) being untrustworthy/biased towards marking issues as effective false positives.}








