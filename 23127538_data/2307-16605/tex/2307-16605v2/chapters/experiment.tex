\section{Experiments}\label{sec:experiment}
\input{fig/text_conditional}
\subsection{Conditional 3D Generation}
\paragraph{Text-conditioned Generation}
Following~\cite{pointe22,clipforge22,clipsculptor23}, we use Accuracy (ACC), Fr√©chet Inception Distance (FID)~\citep{FID17}, and Inception Score (IS)~\citep{ISS16} as metrics to assess the generation quality and diversity. 
We follow~\cite{clipforge22} to define 234 predetermined text queries and apply a classifier for evaluation. 
For a fair comparison, all methods use ``a/an'' as the prefix for text prompts. 
Similar to~\citep{pointe22}, we employ a double-width PointNet++ model~\citep{PointNet++} to extract features and predict class probabilities for point clouds in ShapetNet~\citep{ShapeNet15}. 
\cref{tab:fid} shows the comparison between VPP and other methods.
It can be observed that our VPP outperforms all other methods substantially in all metrics. 
Notably, the FID metric of ours is much lower than other methods, demonstrating that our method generates higher quality and richer diversity of 3D shapes. 

By employing a \textit{Shape As Points}~\citep{sap21, slide23} model pre-trained on multi-category ShapeNet data, VPP is capable of performing smooth surface reconstruction on generated point clouds. In Blender rendering, we employ varnish and automatic smoothing techniques to improve the display quality of the generated mesh.
We qualitatively show in \cref{fig:textconditional} that our method can generate higher-quality shapes and remain faithful to text-shape correspondence. 
It is noteworthy that VPP can generate smoother 3D shapes, \eg, the truck. Besides, our generation results are visually pleasing across different object categories with complex text descriptions. Therefore, the above advantages enable our method to be much more useful in practical applications.

\paragraph{Image-Conditioned Generation} By leveraging the CLIP features of 2D images as conditions in the Voxel Semantic Generator, VPP also possesses the capability of single-view reconstruction. \cref{fig:imageconditional}  illustrates image-conditional generated samples of VPP. It can be observed that the generated point clouds are of detailed quality and visually consistent with the input image. 
For example, the flat wings of an airplane and the curved backrest of a chair.

\subsection{Text-Conditioned 3D Editing and Upsampling}
Text-conditioned 3D editing and upsample completion tasks are more challenging than most conditional 3D generation tasks, which most of previous methods can not realize due to the inflexible model nature. 
In contrast, our VPP can be used for a variety of shape editing and upsample completion without extra training or model fine-tuning.
\paragraph{Text-Conditioned 3D Editing}
Since we use the masked generative Transformer in VPP, our model can be conditioned on any set of point tokens: we first obtain a set of input point tokens, then mask the tokens corresponding to a local area, and decode the masked tokens with unmasked tokens and text prompts as conditions. We show the examples in \cref{fig:edit} (a). The figure provides examples of VPP being capable of changing part attributes (view) and modifying local details (view) of the shape, which correctly corresponds to the text prompts.
\paragraph{Point Cloud Upsampling}
Except for editing, we also present our conditional generative model for the point cloud upsample completion, where the sparse point cloud inputs are completed as a dense output. 
We use the second stage Point Upsampler to achieve this, and
the results are illustrated in~\cref{fig:edit} (b). It is observed that VPP generates completed and rich shapes of high fidelity while being consistent with the input sparse geometry.
\input{fig/image_conditional}
\input{tabs/fid}

\subsection{Transfer Learning on Downstream Tasks}
After training, the Mask Transformer has learned powerful geometric knowledge through generative reconstruction~\citep{MAE}, which enables VPP to serve as a self-supervised learning method for downstream representation transferring. Following previous works~\citep{PointMAE,ACT23,recon23}, we fine-tune the Mask Point Transformer encoder, \ie, the second-stage \textit{Point Upsampler} for 3D shape recognition.

ScanObjectNN~\citep{ScanObjectNN19} and ModelNet~\citep{ModelNet15} are currently the two most challenging 3D object datasets, which are obtained through real-world sampling and synthesis, respectively.
We show the evaluation of 3D shape classification in \cref{tab:scanobjectnn}. 
It can be seen that: (i) Compared to any supervised or self-supervised method that only uses point clouds for training, VPP achieves the best generalization performance. (ii) Notably, VPP outperforms Point-MAE by +4.0\% and +0.3\% accuracy on the most challenging PB\_T50\_RS and ModelNet40 benchmark.

\input{fig/edit}
\input{tabs/scanobjectnn}

\subsection{Diversity \& Specific Text-Conditional Generation}
\input{fig/text_conditional_sup}

We show the diverse qualitative results of VPP on text-conditional 3D generation in \cref{fig:textconditionalsup} (a). It can be observed that VPP can generate a broad category of shapes with rich diversity while remaining faithful to the provided text descriptions. Meanwhile, we present the qualitative results of VPP on more specific text-conditional 3D generation in \cref{fig:textconditionalsup} (b). Notably, VPP can generate high-fidelity shapes that react well to very specific text descriptions, like \texttt{a cup chair}, \texttt{a round table with four legs} and \texttt{an aircraft during World War II}, \etc.

\subsection{Partial Completion}
\input{fig/part_generation}

We present the point cloud partial completion experiments in \cref{fig:part}. By employing a block mask on the original point clouds, VPP can generate partial samples, which illustrates the partial completion capability of VPP. Besides, the generated samples exhibit diversity, further demonstrating the diverse performance of VPP.

\input{fig/retrieval}
\subsection{ShapeNet Retrieval Experiment}
\vspace{-5pt}
We conduct the retrieval evaluation on samples generated by VPP from the ShapeNet dataset. The results are shown in \cref{fig:retrieval}. It can be observed that there are no completely identical samples, proving the great generation ability of VPP is not caused by overfitting the ShapeNet dataset. The VPP results more likely come from the understanding and integration of the learned shape knowledge.
\vspace{-4pt}