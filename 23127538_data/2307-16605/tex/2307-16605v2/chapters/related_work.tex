\vspace{-2pt}
\section{Related Works}\label{sec:related work}
\vspace{-5pt}
Conditional 3D generation with 2D images or text has witnessed rapid development in recent years. 
One line of works focuses on solving the ill-posed image-conditioned 3D generation (\ie, single-view reconstruction), where impressive results have been achieved~\citep{R2N216,ChamferDistance17,Pixel2Mesh,2022autosdf}.
By introducing a text-shape paired dataset based on ShapeNet~\citep{ShapeNet15} that includes chair and table categories, \cite{2019text2shape} pioneered another line of single-category text-conditioned shape generation.
Facilitated with this human-crawled 3D data, ShapeCrafter~\citep{2022shapecrafter} further enables text-conditioned shape edition in a recursive fashion. 
To leverage easily rendered 2D images rather than human-crafted languages, several works propose to train rendered image-to-shape generation~\citep{clipforge22,clipsculptor23,iss22,sdfusion22} which enables text-conditioned generation through pretrained large vision-language (VL) models, \ie, CLIP~\citep{CLIP}. 
Besides rendered images, Point-E~\citep{pointe22} and Shape-E~\citep{shapee23} propose to use real-world text-image-shape triplets from a large-scale in-house dataset, where images are used as the representation bridge.

However, 3D data is significantly lacking and expensive to collect~\citep{ACT23}.
DreamFields~\citep{dreamfields22} first achieves text-only training by using NeRF~\citep{nerf21}, where rendered 2D images are weakly supervised to align with text inputs using CLIP.
Following this direction, 
DreamFusion~\citep{dreamfusion22} incorporates the distillation loss of a diffusion model~\citep{StableDiffusion22}. Dream3D~\citep{dream3d22} improves the knowledge prior by providing the prompt shapes initialization. 
Though impressive results are obtained, these approaches also introduce significant time costs because of the case-by-case NeRF training during inference. 
More recently, TAPS3D aligns rendered images and text prompts based on DMTet~\citep{dmtet} to enable text-conditional generation, but its generalizability is limited to only four categories. 3DGen~\citep{3dgen23} achieves high-quality generation through the utilization of Triplane-based diffusion models~\citep{EG3D22,TriplaneDiffusion22} and the Objaverse dataset~\citep{objaverse23}.
Zero-1-to-3~\citep{zero123} employs relative camera viewpoints as conditions for diffusion modeling.
Based on this, One-2-3-45~\citep{one2345} achieves rapid open-vocabulary mesh generation through multi-view synthesis.
Fantasia3D~\citep{Fantasia3D} leverages DMTet to employ differentiable rendering of SDF as a substitute for NeRF, enabling a more direct mesh generation.
SJC~\citep{sjc22} and ProlificDreamer~\citep{ProlificDreamer23} achieve astonishing results by proposing the variational diffusion distillation.