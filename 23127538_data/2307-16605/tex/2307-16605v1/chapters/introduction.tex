\section{Introduction}\label{sec:introduction}
In the last few years, text-conditional image generation~\citep{ramesh2022, 2021styleclip, 2021zero, 2022high} has made significant progress in terms of generation quality and diversity and has been successfully applied in games~\citep{2019text2shape}, content creation~\citep{2022opal,20223dall} and human-AI collaborative design~\citep{2021generative}. The success of text-to-image generation is largely owed to the large-scale image-text paired datasets~\citep{2021laion}. Recently, there has been an increasing trend to boost conditional generation from 2D images to 3D shapes.
Many works attempt to use existing text-3D shape paired datasets~\citep{2019text2shape, 2022shapecrafter, 2022towards, sdfusion22} to train the models, but the insufficient pairs limit these methods to generate multi-category, diverse, and high-res 3D shapes.

To tackle this challenge, some works~\citep{dreamfields22, dreamfusion22, dream3d22} use NeRF~\citep{nerf21} and weak supervision of large scale vision-language models~\citep{CLIP} to generate 3D shapes.
Although creative quality is achieved, the optimization costs of differentiable rendering are quite expensive, which is impractical. 
In addition, some methods use images as bridges~\citep{pointe22, shapee23} for the text-conditional 3D generation, which improves the quality-time trade-off. However, the complex multi-stage model leads to long inference latency and cumulative bias.
Moreover, most methods~\citep{clipforge22, 2022clipmesh} are task-limited due to specific model design that can not be implemented to more downstream tasks, \eg, editing, leading to narrow application scope.
To sum up, current 3D generation methods are faced with three challenges: \textit{low inference efficiency}, \textit{limited generation category}, and \textit{restricted downstream tasks}, which have not been solved by a unified method.


One of the most crucial components in 3D generation is the \textit{geometry representation}, and mainstream methods are typically built with voxel, 3D point cloud, or Signed Distance Function (SDF)~\citep{SDF96,DeepSDF19}.
However, the aforementioned issues in 3D generation may come from the different properties of these representations, which are summarized as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Voxel} is a structured and explicit representation. It shares a similar form with 2D pixels, facilitating the adoption of various image generation methods. However, the dense nature of voxels results in increased computational resources and time requirements when generating high-resolution shapes.
    \item \textbf{3D Point Cloud} is the unordered collection of points that explicitly represents positional information with sparse semantics, enabling a flexible and efficient shape representation. However, 3D point clouds typically dedicatedly designed architectures, \eg, PointNet~\citep{PointNet,PointNet++}.
    \item \textbf{SDF} describes shapes through an implicit distance function. It is a continuous representation method and is capable of generating shapes with more intricate details than points or voxels. But it requires substantial computation for high-resolution sampling and is sensitive to initialization.
\end{itemize}

Due to significant shape differences across various categories, the structured and explicit positional information provides spatial cues, thereby aiding the generalization of multiple categories. It coincides with previous multi-category generation methods~\citep{clipforge22, clipsculptor23}. 
In contrast, single-category generation may require less variation. Hence an implicit and continuous representation would contribute to the precise expression of local details~\citep{get3d22,sdfusion22}.

Motivated by the analysis above, we aim to leverage the advantage of different forms of representation. Therefore, a novel \textbf{V}oxel-\textbf{P}oint \textbf{P}rogressive (\textbf{VPP}% Figure removed) Representation method is proposed to achieve efficient and universal 3D generation. 
We use voxel and point as representations for the coarse to fine stages to adapt to the characteristics of 3D.
To further enhance the inference speed, we employ mask generative Transformers for parallel inference~\citep{MaskGiT22} in both stages, which can concurrently obtain broad applicability scope~\citep{mage23}.
Based on voxel VQGAN~\citep{vqgan21}, we utilize the discrete semantic codebooks as the reconstruction targets  to obtain authentic and rich semantics. Given the substantial computational resources required by high-resolution voxels, we employ sparse points as representations in the second stage. Inspired by the masked point modeling approaches based on position cues~\citep{PointBERT, PointMAE}, we utilize the voxels generated in the first stage as the positional encoding for generative modeling. We reconstruct the points and predict semantic tokens to obtain sparse representations. \cref{Table:comparison-table} compares the efficiency and universality of text-conditional 3D generation methods.

\input{tabs/comparison}

In summary, our contributions are: 
(1) We propose a novel voxel-point progressive generation method that shares the merits of different geometric representations, enabling efficient and multi-category 3D generation. Notably, VPP is capable of generating high-quality 8K point clouds \textit{\textbf{within 0.2 seconds}} on a single RTX 2080Ti.
(2) To accommodate 3D generation, we implement unique module designs, \eg~3D VQGAN, and propose a Central Radiative Temperature Schedule strategy in inference.
(3) As a universal method, VPP can complete multiple tasks such as editing, completion, and even pre-training. To the best of our knowledge, VPP is the first work that unifies various 3D tasks.
