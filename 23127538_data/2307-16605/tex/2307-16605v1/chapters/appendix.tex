\newpage
\appendix
\section{Additional Related Work}
3D Representation Learning includes point-based~\citep{PointNet, PointNet++}, voxel-based~\citep{voxelnet15}, and multiview-based methods~\citep{mvcnn15,MVTN}, \etc. Due to the sparse but geometry-informative representation, point-based methods~\citep{PointNext, PointTrans21} have become mainstream approaches in object classification~\citep{ModelNet15, ScanObjectNN19}. Voxel-based CNN methods~\citep{SyncSpecCNN17,voxelrcnn21} provide dense representation and translation invariance, achieving outstanding performance in object detection~\citep{ScanNet17} and segmentation~\citep{ShapeNetPart16,S3DIS16}. Furthermore, due to the vigorous development of attention mechanisms~\citep{AttentionIsAllYouNeed}, 3D Transformers~\citep{PointTrans21,groupfree21,voxeltransformer21} have also brought about effective representations for downstream tasks. Recently, 3D self-supervised representation learning has been widely studied. PointContrast~\citep{PointContrast20} leverages contrastive learning across different views to acquire discriminative 3D scene representations. Point-BERT~\citep{PointBERT} and Point-MAE~\citep{PointMAE} first introduce masked modeling pretraining into 3D. ACT~\citep{ACT23} pioneers cross-modal geometry understanding via 2D/language foundation models. {\scshape ReCon}~\citep{recon23} further proposes to unify generative and contrastive learning.
Facilitated by foundation vision-language models like CLIP~\citep{CLIP}, another line of works are proposed towards open-world 3D representation learning~\citep{ULIP22,OpenScene23,CLIPFO3D23,LAS3D23,PLA23, PointGCC23}.

\section{Implementation Details}\label{app:impl_detail}
\subsection{Experimental Details}
\textbf{Training Details}~
We use ShapeNetCore from ShapeNet~\citep{ShapeNet15} as the training dataset. ShapeNet is a clean set of 3D CAD object models with rich annotations, including ~51K unique 3D models from 55 common object categories.
For the acquisition of multi-modal data, we follow ReCon~\citep{recon23} for multi-view rendering and utilize BLIP~\citep{blip} based on the rendered images to obtain textual data.
In \cref{tab:details}, we show the training hyperparameters and model architecture information of each part of our \vpp.
\input{tabs/deatils}

\newpage
\textbf{Downstream Tasks Details}~
Following Point-E~\citep{pointe22}, we use pre-trained PointNet++ as a classifier in all ACC, FID, and IS evaluations to extract the features and calculate the accuracy of generated point clouds. 
In point cloud generation and editing, we employ 8 or 4 steps for a parallel generation. The generation task utilizes initial voxel codebooks composed entirely of \texttt{[MASK]} tokens. In editing, we extract VQGAN features from the original point cloud to initialize the voxel codebooks.
As for the transfer classification task on ScanObjectNN~\citep{ScanObjectNN19} and ModelNet40~\citep{ModelNet15}, we fully follow the previous work~\citep{PointMAE, ACT23} configuration and trained 300 epochs with the AdamW optimizer, and used the voting strategy in testing.

\subsection{Implementation Details of 3D VQGAN}\label{app:vqgan}
\input{fig/vqgan}
\input{fig/vqgan_vis}
We show the detailed training overview of 3D VQGAN in \cref{fig:vqgan}. Following the training recipe of VQGAN~\citep{vqgan21}, we use L1 loss to supervise the reconstruction of the voxel and feed the reconstructed voxel into the discriminator to ensure the generated authenticity by GAN loss. Besides the L1 loss and GAN loss, we also introduce the occupancy rate loss to make the occupancy rate of the reconstructed voxel grid similar to the ground truth so as to obtain a better reconstruction of the voxel. \cref{fig:vqgan_vis} illustrates the reconstructed results of 3D VQGAN, showcasing its impressive capabilities in the domain of 3D voxel reconstruction. As depicted in the figure, the model exhibits remarkable noise rectification capabilities. It not only reconstructs the voxels faithfully but also manages to rectify certain imperfections and artifacts present in the input data.

\section{Additional Experiments}\label{app:add_exp}
We conduct more experiments to further demonstrate the generation quality and universality of \vpp. Including diversity \& specific text-conditional generation, few-shot transfer classification, and ablation studies.
\subsection{Diversity \& Specific Text-Conditional Generation}
\input{fig/text_conditional_sup}

We show the diverse qualitative results of VPP on text-conditional 3D generation in \cref{fig:textconditionalsup} (a). It can observe that VPP can generate a broad category of shapes with rich diversity while remaining faithful to the provided text descriptions. Meanwhile, we present the qualitative results of VPP on more specific text-conditional 3D generation in \cref{fig:textconditionalsup} (b). Notably, VPP can generate high-fidelity shapes that react well to very specific text descriptions, like \texttt{a cup chair}, \texttt{a round table with four legs} and \texttt{an aircraft during World War II}, \etc.

\subsection{Few-Shot Transfer Classification}
We conduct few-shot experiments on the ModelNet40~\citep{ModelNet15} dataset, and the results are shown in \cref{tab:few-shot}. In the self-supervised benchmark without the use of additional modality data, \vpp\ achieves excellent performance compared to previous works.
\input{tabs/fewshot}

\input{fig/ablation}
\subsection{Ablation Study}

\textbf{Training Hyper Parameter}~
\cref{fig:ablation} (a-b) shows the ablation study of the image-text features ratio and Gaussian noise strength. It can be observed that either too large or too small text-image feature ratio and noise are not conducive to the quality and diversity of 3D generation.

\input{fig/clip_ablation}
\textbf{Inference Parameters}~
To explore the parameter dependencies of the Mask Voxel Transformer in the inference process, we study the effects of temperature and iteration steps. The results are shown in~\cref{fig:ablation} (c-d). It can be seen that moderate temperature achieves optimal generation classification accuracy. Higher temperature promotes model diversity, while with the increase of iteration steps, the model's FID initially decreases and then increases.

\textbf{Backbone Choice}~
\cref{fig:ablation_clip} shows the selection of the CLIP model backbones during VPP training. Both ViT-B/32 and ViT/L14 achieve excellent accuracy and diversity.

\section{Limitations \& Future Works}
Despite the substantial advantages of our model in terms of generation speed and applicability, there exists a considerable gap in generation quality compared to NeRF-based models~\citep{dreamfields22,dreamfusion22,sjc22}. To address this limitation, we intend to explore the utilization of larger models and more extensive datasets for training, \eg, Objaverse~\citep{objaverse23}.