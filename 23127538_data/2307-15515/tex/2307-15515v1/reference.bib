
@article{lucambio_perez_nonlinear_2018,
  title={Nonlinear conjugate gradient methods for vector optimization},
  author={Lucambio P{\'e}rez, LR and Prudente, LF},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={3},
  pages={2690--2720},
  year={2018},
  publisher={SIAM}
}

@article{goncalves_2020,
  author = {Gon\c{c}alves, M. L. N. and Prudente, L. F.},
  year = {2020},
  title = {On the extension of the Hager--Zhang conjugate gradient method for vector optimization},
  journal = {Computational Optimization and Applications},
  volume = {76},
  pages = {889--916},
  year = {2020},
}


@article{goncalves_study_nodate,
  title={A study of Liu-Storey conjugate gradient methods for vector optimization},
  author={Gon{\c{c}}alves, Max LN and Lima, FS and Prudente, LF},
  journal={Applied Mathematics and Computation},
  volume={425},
  pages={127099},
  year={2022},
  publisher={Elsevier}
}


@article{gilbert_global_1992,
  title={Global convergence properties of conjugate gradient methods for optimization},
  author={Gilbert, Jean Charles and Nocedal, Jorge},
  journal={SIAM Journal on optimization},
  volume={2},
  number={1},
  pages={21--42},
  year={1992},
  publisher={SIAM}
}


@article{zhu_riemannian_2017,
	title = {A Riemannian conjugate gradient method for optimization on the Stiefel manifold},
	volume = {67},
	abstract = {In this paper we propose a new Riemannian conjugate gradient method for optimization on the Stiefel manifold. We introduce two novel vector transports associated with the retraction constructed by the Cayley transform. Both of them satisfy the Ring-Wirth nonexpansive condition, which is fundamental for convergence analysis of Riemannian conjugate gradient methods, and one of them is also isometric. It is known that the {RingWirth} nonexpansive condition does not hold for traditional vector transports as the diﬀerentiated retractions of {QR} and polar decompositions. Practical formulae of the new vector transports for low-rank matrices are obtained. Dai’s nonmonotone conjugate gradient method is generalized to the Riemannian case and global convergence of the new algorithm is established under standard assumptions. Numerical results on a variety of low-rank test problems demonstrate the eﬀectiveness of the new method.},
	pages = {73--110},
	number = {1},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Comput Optim Appl},
	author = {Zhu, Xiaojing},
	date = {2017-05},
	langid = {english},
}

@article{bento_unconstrained_2012,
  title={Unconstrained steepest descent method for multicriteria optimization on Riemannian manifolds},
  author={Bento, GC and Ferreira, Orizon Pereira and Oliveira, P Roberto},
  journal={Journal of Optimization Theory and Applications},
  volume={154},
  pages={88--107},
  year={2012},
  publisher={Springer}
}

@misc{sakai_sufficient_2021,
	title = {Sufficient Descent Riemannian Conjugate Gradient Method},
	url = {http://arxiv.org/abs/2009.01451},
	abstract = {This paper considers suﬃcient descent Riemannian conjugate gradient methods with line search algorithms. We propose two kinds of suﬃcient descent nonlinear conjugate gradient method and prove that these methods satisfy the suﬃcient descent condition on Riemannian manifolds. One is a hybrid method combining a Fletcher–Reeves-type method with a Polak–Ribi`ere–Polyak–type method, and the other is a Hager–Zhangtype method, both of which are generalizations of those used in Euclidean space. Moreover, we prove that the hybrid method has a global convergence property under the strong Wolfe conditions and the Hager–Zhangtype method has the suﬃcient descent property regardless of whether a line search is used or not. Further, we review two kinds of line search algorithm on Riemannian manifolds and numerically compare our generalized methods by solving several Riemannian optimization problems. The results show that the performance of the proposed hybrid methods greatly depends on the type of line search used. Meanwhile, the Hager–Zhang-type method has the fast convergence property regardless of the type of line search used.},
	number = {{arXiv}:2009.01451},
	publisher = {{arXiv}},
	author = {Sakai, Hiroyuki and Iiduka, Hideaki},
	date = {2021-04-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2009.01451 [math]},
	keywords = {65K05, 90C26, 57R35, Mathematics - Optimization and Control},
	file = {Sakai 和 Iiduka - 2021 - Sufficient Descent Riemannian Conjugate Gradient M.pdf:/Users/chenkm/Zotero/storage/UG45JSWB/Sakai 和 Iiduka - 2021 - Sufficient Descent Riemannian Conjugate Gradient M.pdf:application/pdf},
}


@article{sato_riemannian_2021_article,
  title={Riemannian conjugate gradient methods: General framework and specific algorithms with convergence analyses},
  author={Sato, Hiroyuki},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={4},
  pages={2690--2717},
  year={2022},
  publisher={SIAM}
}



@article{grana_drummond_steepest_2005,
	title = {A steepest descent method for vector optimization},
	volume = {175},
	pages = {395--414},
	number = {2},
	journaltitle = {Journal of Computational and Applied Mathematics},
	shortjournal = {Journal of Computational and Applied Mathematics},
	author = {Graña Drummond, L.M. and Svaiter, B.F.},
	date = {2005-03},
	langid = {english},
	file = {全文:/Users/chenkm/Zotero/storage/5U8JICTP/Graña Drummond 和 Svaiter - 2005 - A steepest descent method for vector optimization.pdf:application/pdf},
}


@book{sato_riemannian_2021,
	location = {Cham, Switzerland},
	title = {Riemannian Optimization and Its Applications},
	isbn = {978-3-030-62391-3},
	abstract = {This brief describes the basics of Riemannian optimization--optimization on Riemannian manifolds--introduces algorithms for Riemannian optimization problems, discusses the theoretical properties of these algorithms, and suggests possible applications of Riemannian optimization to problems in other fields. To provide the reader with a smooth introduction to Riemannian optimization, brief reviews of mathematical optimization in Euclidean spaces and Riemannian geometry are included. Riemannian optimization is then introduced by merging these concepts. In particular, the Euclidean and Riemannian conjugate gradient methods are discussed in detail. A brief review of recent developments in Riemannian optimization is also provided. Riemannian optimization methods are applicable to many problems in various fields. This brief discusses some important applications including the eigenvalue and singular value decompositions in numerical linear algebra, optimal model reduction in control engineering, and canonical correlation analysis in statistics},
	publisher = {Springer},
	author = {Sato, Hiroyuki},
	date = {2021},
	note = {{OCLC}: 1237966923},
	keywords = {Riemannian},
}



@BOOK{AbsMahSep2008,
 author = "P.-A. Absil and R. Mahony and R. Sepulchre",
 title = "Optimization Algorithms on Matrix Manifolds",
 publisher = "Princeton University Press",
 address = "Princeton, NJ",
 year = 2008,
 pages = "xvi+224",
 isbn = "978-0-691-13298-3",
 keywords = "optimization on manifolds, Riemannian optimization, retraction, vector transport",
}


@Book{boumal2023intromanifolds,
  title     = {An Introduction to Optimization on Smooth Manifolds},
  author    = {Boumal, Nicolas},
  publisher = {Cambridge University Press},
  year      = {2023}
}

@article{JMLR:v17:16-177,
    author = {James Townsend and Niklas Koep and Sebastian Weichwald},
    journal = {Journal of Machine Learning Research},
    number = {137},
    pages = {1-5},
    title = {Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation},
    url = {http://jmlr.org/papers/v17/16-177.html},
    volume = {17},
    year = {2016}
}



@article{wei_global_2008,
  title        = {Global convergence of the Polak-Ribière-Polyak conjugate gradient method with an Armijo-type inexact line search for nonconvex unconstrained optimization problems},
  volume       = {77},
  issn         = {0025-5718},
  abstract     = {We propose two algorithms for nonconvex unconstrained optimization problems that employ Polak-Ribi`ere-Polyak conjugate gradient formula and new inexact line search techniques. We show that the new algorithms converge globally if the function to be minimized has Lipschitz continuous gradients. Preliminary numerical results show that the proposed methods for particularly chosen line search conditions are very promising.},
  pages        = {2173--2193},
  number       = {264},
  journaltitle = {Mathematics of Computation},
  shortjournal = {Math. Comp.},
  author       = {Wei, Zeng Xin and Li, Guo Yin and Qi, Li Qun},
  date         = {2008-06-11},
  langid       = {english},
  file         = {Wei - 2008 - Global convergence of the Polak-Ribière-Polyak con.pdf:/Users/chenkm/Zotero/storage/V6YH9SU7/Wei 等 - 2008 - Global convergence of the Polak-Ribière-Polyak con.pdf:application/pdf}
}


@article{huang_riemannian_2022,
  title        = {Riemannian proximal gradient methods},
  volume       = {194},
  issn         = {0025-5610, 1436-4646},
  abstract     = {In the Euclidean setting the proximal gradient method and its accelerated variants are a class of efﬁcient algorithms for optimization problems with decomposable objective. In this paper, we develop a Riemannian proximal gradient method ({RPG}) and its accelerated variant ({ARPG}) for similar problems but constrained on a manifold. The global convergence of {RPG} is established under mild assumptions, and the O(1/k) is also derived for {RPG} based on the notion of retraction convexity. If assuming the objective function obeys the Rimannian Kurdyka–Łojasiewicz ({KL}) property, it is further shown that the sequence generated by {RPG} converges to a single stationary point. As in the Euclidean setting, local convergence rate can be established if the objective function satisﬁes the Riemannian {KL} property with an exponent. Moreover, we show that the restriction of a semialgebraic function onto the Stiefel manifold satisﬁes the Riemannian {KL} property, which covers for example the well-known sparse {PCA} problem. Numerical experiments on random and synthetic data are conducted to test the performance of the proposed {RPG} and {ARPG}.},
  pages        = {371--413},
  number       = {1},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  author       = {Huang, Wen and Wei, Ke},
  urldate      = {2023-04-13},
  date         = {2022-07},
  langid       = {english},
  keywords     = {Riemannian manifolds},
  file         = {Huang 和 Wei - 2022 - Riemannian proximal gradient methods.pdf:/Users/chenkm/Zotero/storage/M6IDA9ES/Huang 和 Wei - 2022 - Riemannian proximal gradient methods.pdf:application/pdf}
}




@article{najafi_multiobjective_2023,
	title = {Multiobjective conjugate gradient methods on Riemannian manifolds},
	volume = {197},
	abstract = {In this paper, we present the multiobjective optimization methods of conjugate gradient on Riemannian manifolds. The concepts of optimality and Wolfe conditions, as well as Zoutendijk’s theorem, are redefined in this setting. We show that under some standard assumptions, a sequence generated by these algorithms converges to a critical Pareto point. This is when the step sizes satisfy the multiobjective Wolfe conditions. In particular, we propose the Fletcher–Reeves, Dai–Yuan, Polak–Ribière–Polyak, and Hestenes–Stiefel parameters and further analyze the convergence behavior of the first two methods and test their performance against the steepest descent method.},
	pages = {1229--1248},
	number = {3},
	journaltitle = {Journal of Optimization Theory and Applications},
	shortjournal = {J Optim Theory Appl},
	author = {Najafi, Shahabeddin and Hajarian, Masoud},
	date = {2023-06-01},
	langid = {english},
	keywords = {Conjugate gradient methods, Multicriteria optimization, Multiobjective optimization, Pareto optimality, Riemannian manifolds, Wolfe conditions},
	file = {Full Text PDF:/Users/chenkm/Zotero/storage/8XM5CI3K/Najafi 和 Hajarian - 2023 - Multiobjective Conjugate Gradient Methods on Riema.pdf:application/pdf},
}


@article{absil_trust-region_2007,
  title={Trust-region methods on Riemannian manifolds},
  author={Absil, P-A and Baker, Christopher G and Gallivan, Kyle A},
  journal={Foundations of Computational Mathematics},
  volume={7},
  pages={303--330},
  year={2007},
  publisher={Springer}
}

@article{bento_subgradient_2013,
  title={A subgradient method for multiobjective optimization on Riemannian manifolds},
  author={Bento, GC and Cruz Neto, JX},
  journal={Journal of Optimization Theory and Applications},
  volume={159},
  pages={125--137},
  year={2013},
  publisher={Springer}
}

@article{bento_proximal_2018,
	title = {Proximal point method for vector optimization on Hadamard manifolds},
	volume = {46},
	issn = {0167-6377},
	abstract = {In this paper, the proximal point method for vector optimization and its inexact version are extended from Euclidean space to the Riemannian context. Under suitable assumptions on the objective function, the well-definedness of the methods is established. In addition, the convergence of any generated sequence to a weak efficient point is obtained.},
	pages = {13--18},
	number = {1},
	journaltitle = {Operations Research Letters},
	shortjournal = {Operations Research Letters},
	author = {Bento, Glaydston de C. and Ferreira, Orizon P. and Pereira, Yuri R. L.},
	date = {2018-01-01},
	langid = {english},
	keywords = {Continuous optimization, Fejér convergence, Hadamard manifolds, Proximal method, Vector optimization},
	file = {ScienceDirect Snapshot:/Users/chenkm/Zotero/storage/27X4AV7X/S016763771730202X.html:text/html},
}


@article{bento_inexact_2013,
  title={An inexact steepest descent method for multicriteria optimization on Riemannian manifolds},
  author={Bento, GC and da Cruz Neto, Jo{\~a}o Xavier and Santos, PSM},
  journal={Journal of Optimization Theory and Applications},
  volume={159},
  pages={108--124},
  year={2013},
  publisher={Springer}
}


@article{quiroz2008steepest,
  title={Steepest descent method with a generalized Armijo search for quasiconvex functions on Riemannian manifolds},
  author={Quiroz, EA Papa and Quispe, EM and Oliveira, P Roberto},
  journal={Journal of mathematical analysis and applications},
  volume={341},
  number={1},
  pages={467--477},
  year={2008},
  publisher={Elsevier}
}


@article{fliege_steepest_2000,
	title = {Steepest descent methods for multicriteria optimization},
	volume = {51},
	abstract = {We propose a steepest descent method for unconstrained multicriteria optimization and a “feasible descent direction” method for the constrained case. In the unconstrained case, the objective functions are assumed to be continuously differentiable. In the constrained case, objective and constraint functions are assumed to be Lipshitz-continuously differentiable and a constraint qualification is assumed. Under these conditions, it is shown that these methods converge to a point satisfying certain first-order necessary conditions for Pareto optimality. Both methods do not scalarize the original vector optimization problem. Neither ordering information nor weighting factors for the different objective functions are assumed to be known. In the single objective case, we retrieve the Steepest descent method and Zoutendijk's method of feasible directions, respectively.},
	pages = {479--494},
	number = {3},
	journaltitle = {Mathematical Methods of Operations Research},
	shortjournal = {Mathematical Methods of {OR}},
	author = {Fliege, Jörg and Svaiter, Benar Fux},
	date = {2000-08-01},
	langid = {english},
	keywords = {Key words: Multicriteria optimization, multi-objective programming, vector optimization, Pareto points, steepest descent},
	file = {Full Text PDF:/Users/chenkm/Zotero/storage/4E7M6I73/Fliege 和 Svaiter - 2000 - Steepest descent methods for multicriteria optimiz.pdf:application/pdf},
}


@article{fukuda_inexact_2013,
	title = {Inexact projected gradient method for vector optimization},
	volume = {54},
	abstract = {In this work, we propose an inexact projected gradient-like method for solving smooth constrained vector optimization problems. In the unconstrained case, we retrieve the steepest descent method introduced by Graña Drummond and Svaiter. In the constrained setting, the method we present extends the exact one proposed by Graña Drummond and Iusem, since it admits relative errors on the search directions. At each iteration, a decrease of the objective value is obtained by means of an Armijo-like rule. The convergence results of this new method extend those obtained by Fukuda and Graña Drummond for the exact version. For partial orders induced by both pointed and nonpointed cones, under some reasonable hypotheses, global convergence to weakly efficient points of all sequences generated by the inexact projected gradient method is established for convex (respect to the ordering cone) objective functions. In the convergence analysis we also establish a connection between the so-called weighting method and the one we propose.},
	pages = {473--493},
	number = {3},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Comput Optim Appl},
	author = {Fukuda, Ellen H. and Graña Drummond, L. M.},
	date = {2013-04-01},
	langid = {english},
	keywords = {Multiobjective optimization, Projected gradient method, Vector optimization, Weak efficiency},
}


@article{tanabe_proximal_2019,
	title = {Proximal gradient methods for multiobjective optimization and their applications},
	volume = {72},
	abstract = {We propose new descent methods for unconstrained multiobjective optimization problems, where each objective function can be written as the sum of a continuously differentiable function and a proper convex but not necessarily differentiable one. The methods extend the well-known proximal gradient algorithms for scalar-valued nonlinear optimization, which are shown to be efﬁcient for particular problems. Here, we consider two types of algorithms: with and without line searches. Under mild assumptions, we prove that each accumulation point of the sequence generated by these algorithms, if exists, is Pareto stationary. Moreover, we present their applications in constrained multiobjective optimization and robust multiobjective optimization, which is a problem that considers uncertainties. In particular, for the robust case, we show that the subproblems of the proximal gradient algorithms can be seen as quadratic programming, second-order cone programming, or semideﬁnite programming problems. Considering these cases, we also carry out some numerical experiments, showing the validity of the proposed methods.},
	pages = {339--361},
	number = {2},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Comput Optim Appl},
	author = {Tanabe, Hiroki and Fukuda, Ellen H. and Yamashita, Nobuo},
	date = {2019-03},
	langid = {english},
}


@article{chen_conditional_2023,
	title = {Conditional gradient method for vector optimization},
	issn = {1573-2894},
	abstract = {In this paper, we propose a conditional gradient method for solving constrained vector optimization problems with respect to a partial order induced by a closed, convex and pointed cone with nonempty interior. When the partial order under consideration is the one induced by the non-negative orthant, we regain the method for multiobjective optimization recently proposed by Assunção et al. (Comput Optim Appl 78(3):741–768, 2021). In our method, the construction of the auxiliary subproblem is based on the well-known oriented distance function. Three different types of step size strategies (Armijo, adaptative and nonmonotone) are considered. Without convexity assumption related to the objective function, we obtain the stationarity of accumulation points of the sequences produced by the proposed method equipped with the Armijo or the nonmonotone step size rule. To obtain the convergence result of the method with the adaptative step size strategy, we introduce a useful cone convexity condition which allows us to circumvent the intricate question of the Lipschitz continuity of Jocabian for the objective function. This condition helps us to generalize the classical descent lemma to the vector optimization case. Under convexity assumption for the objective function, it is proved that all accumulation points of any generated sequences obtained by our method are weakly efficient solutions. Numerical experiments illustrating the practical behavior of the methods are presented.},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Comput Optim Appl},
	author = {Chen, Wang and Yang, Xinmin and Zhao, Yong},
	date = {2023-04-27},
	langid = {english},
}


@article{drummond_projected_2004,
	title = {A Projected Gradient Method for Vector Optimization Problems},
	volume = {28},
	issn = {1573-2894},
	abstract = {Vector optimization problems are a significant extension of multiobjective optimization, which has a large number of real life applications. In vector optimization the preference order is related to an arbitrary closed and convex cone, rather than the nonnegative orthant. We consider extensions of the projected gradient gradient method to vector optimization, which work directly with vector-valued functions, without using scalar-valued objectives. We provide a direction which adequately substitutes for the projected gradient, and establish results which mirror those available for the scalar-valued case, namely stationarity of the cluster points (if any) without convexity assumptions, and convergence of the full sequence generated by the algorithm to a weakly efficient optimum in the convex case, under mild assumptions. We also prove that our results still hold when the search direction is only approximately computed.},
	pages = {5--29},
	number = {1},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Computational Optimization and Applications},
	author = {Drummond, L.M. Graña and Iusem, A.N.},
	date = {2004-04-01},
	langid = {english},
}


@article{drummond2005steepest,
  title={A steepest descent method for vector optimization},
  author={Drummond, LM Grana and Svaiter, Benar F},
  journal={Journal of computational and applied mathematics},
  volume={175},
  number={2},
  pages={395--414},
  year={2005},
  publisher={Elsevier}
}


@book{luc_theory_1989,
	location = {Berlin, Heidelberg},
	title = {Theory of Vector Optimization},
	volume = {319},
	isbn = {978-3-540-50541-9 978-3-642-50280-4},
	series = {Lecture Notes in Economics and Mathematical Systems},
	publisher = {Springer},
	author = {Luc, Dinh The},
	editorb = {Beckmann, M. and Krelle, W.},
	editorbtype = {redactor},
	date = {1989},
	keywords = {convex analysis, derivatives, duality, Optimality Conditions, optimization, research, science and technology, sets, stability},
}


@article{flecher1964function,
  title={Function minimization by conjugate gradient},
  author={Flecher, R and Reeves, CM},
  journal={Computer Journal},
  volume={7},
  pages={149--154},
  year={1964}
}

@article{fletcher1980unconstrained,
  title={Unconstrained optimization},
  author={Fletcher, Roger},
  journal={Practical Methods of Optimization},
  volume={1},
  year={1980},
  publisher={John Wiley and Sons, New York}
}

@article{dai2000convergence,
  title={Convergence properties of nonlinear conjugate gradient methods},
  author={Dai, Yuhong and Han, Jiye and Liu, Guanghui and Sun, Defeng and Yin, Hongxia and Yuan, Ya-xiang},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={2},
  pages={345--358},
  year={2000},
  publisher={SIAM}
}

@article{polak1969note,
  title={Note sur la convergence de m{\'e}thodes de directions conjugu{\'e}es},
  author={Polak, Elijah and Ribiere, Gerard},
  journal={Revue fran{\c{c}}aise d'informatique et de recherche op{\'e}rationnelle. S{\'e}rie rouge},
  volume={3},
  number={16},
  pages={35--43},
  year={1969},
  publisher={EDP Sciences}
}

@article{stiefel1952methods,
  title={Methods of conjugate gradients for solving linear systems},
  author={Stiefel, Eduard},
  journal={J. Res. Nat. Bur. Standards},
  volume={49},
  pages={409--435},
  year={1952}
}

@article{liu1991efficient,
  title={Efficient generalized conjugate gradient algorithms, part 1: theory},
  author={Liu, Y and Storey, C},
  journal={Journal of Optimization Theory and Applications},
  volume={69},
  pages={129--137},
  year={1991},
  publisher={Springer}
}

@article{sakai2021sufficient,
  title={Sufficient descent Riemannian conjugate gradient methods},
  author={Sakai, Hiroyuki and Iiduka, Hideaki},
  journal={Journal of Optimization Theory and Applications},
  volume={190},
  number={1},
  pages={130--150},
  year={2021},
  publisher={Springer}
}

@article{zhu2017riemannian,
  title={A Riemannian conjugate gradient method for optimization on the Stiefel manifold},
  author={Zhu, Xiaojing},
  journal={Computational Optimization and Applications},
  volume={67},
  pages={73--110},
  year={2017},
  publisher={Springer}
}

@book{VectorOptimization,
author = {Jahn, Johannes},
year = {2004},
month = {01},
pages = {},
title = {Vector Optimization: Theory, Applications, and Extensions},
isbn = {978-3-540-20615-6},
journal = {Vector Optimization: Theory, Applications, and Extensions}
}


@inproceedings{powell1984nonconvex,
  title={Nonconvex minimization calculations and the conjugate gradient method},
  author={Powell, Michael JD},
  booktitle={Numerical Analysis: Proceedings of the 10th Biennial Conference held at Dundee, Scotland, June 28--July 1, 1983},
  pages={122--141},
  year={1984},
  organization={Springer}
}

@book{andrei2020nonlinear,
  title={Nonlinear Conjugate Gradient Methods for Unconstrained Optimization},
  author={Andrei, Neculai and others},
  year={2020},
  publisher={Springer}
}

@misc{ferreira_unpublished,
  author = {Ferreira, O. P. and Lucambio P\'erez, L. R. and Prudente, L. F.},
  date = {2019},
  howpublished = {Personal communication},
}