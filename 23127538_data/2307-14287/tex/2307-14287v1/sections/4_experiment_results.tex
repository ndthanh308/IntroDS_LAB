\section{Experiment Results}
\label{sec:experiment_results}

This section presents our experiments and associated results, and discusses our key findings.
To obtain a meaningful evaluation, each experiment was performed three times. 
The plots of the results always include the data from all three executions, whereas the mean of the three data points is highlighted. 

\subsection{Fraud Detection Use Case}

The fraud detection use case uses the default configuration of the \texttt{HashMapStateBackend} with a checkpointing interval of 10 minutes to avoid affecting performance as would be the case when using a performance-intensive state backend such as the \texttt{EmbeddedRocksDBStateBackend}. A window size of 10 seconds and a slide size of 5 seconds were chosen for the sliding window operation, balancing the detection of fraud and performance requirements. The chosen parameters ensure that no large state is required during execution, which could affect latency and distort results.

\subsubsection{Datasource Clients}

% Figure environment removed

The purpose of the first experiment was to compare the latency and throughput of synchronous and asynchronous Cassandra clients under different conditions. 
The experiment was conducted using throughput rates ranging from 1,000 to 2,200 events per second in increments of 100 events per second. 
The results, illustrated in~\autoref{fig:sync_async_latency}, showed that asynchronous enrichment had a fairly constant latency that was always lower than the latency of enrichment by synchronous queries. 
Synchronous enrichment showed a slight increase in latency from 1,600 events/s, and a significant increase from 1,900 events/s, reaching a latency of approximately 50 seconds at 2,200 events/s. 
This rapid increase in latency is due to the maximum throughput being reached, leading to the back pressure mechanism taking effect, delaying event processing. 
The comparison of the consumed rate of the two streaming jobs was measured using the Kafka metric \textit{records-consumed-rate} and is depicted in~\autoref{fig:sync_async_consumedrate}. 
The data indicated that synchronous enrichment consumes slightly fewer events per second than asynchronous enrichment. 
Synchronous enrichment had a maximum throughput of approximately 1,900 events/s and reached a load of 100\% after 50 minutes (~\autoref{fig:sync_busy_rates}). 
On the other hand, asynchronous enrichment had a lower load, and the busy values of the enrichment tasks only increased slightly and remained below 100ms per second (~\autoref{fig:async_busy_rates}). 
In the streaming job with synchronous enrichment, the enrichment task had the highest load and reached 100\% at 1,900 events/s.
In conclusion, the results showcase the limitations of synchronous enrichment, and while asynchronous enrichment comes out superior, it will as well face problems in light of substantially higher throughput rates due to its technical implications, underlining the need for caching strategies, as discussed next.

% Figure environment removed

\subsubsection{Caching Methods}

In the previous experiments, it has become clear that asynchronous enrichment allows both better latency and higher throughput than enrichment using synchronous database queries.
We consequently now evaluate enrichment methods that use a cache to store database records combined with asynchronous Cassandra queries. 
The evaluation was conducted under two factors - the generation of events and the size of caches. 
To maintain the uniformity of the evaluation, each transaction event was required to be unique and generated uniformly. 
Also, the number of cache entries had to be equal to the amount of entries of the external cache. 
A total maximum number of cache entries was defined, with the sum of all cache entries being equal to 24,000. 
A fixed throughput of 4,000 events per second was selected for the latency evaluation, with each execution running for 70 minutes. 
The results, depicted in~\autoref{fig:caches_latency}, indicate that caching using a preceding custom partitioner has the best latency and cache hit rate of 100\% (~\autoref{fig:caches_cachehit_rates}). 
With our particular experiment design, local caching without a preceding custom partitioner achieved a cache hit rate of only 50\% and lower latency than asynchronous enrichment without cache but worse latency than caching with a preceding custom partitioner.
The local cache size for each local cache was 3,000. 
In contrast, asynchronous enrichment without cache showed a more volatile latency, ranging approximately between 3.3s and 3.5s. 
Also, enrichment with an external Redis cache performed comparably bad with a volatile latency and, oftentimes, being slower than asynchronous enrichment without cached database entries. 
The reason for the highly fluctuating latency may be the overhead incurred by the additional asynchronous operator and associated network connections, along with the implementation of the asynchronous Redis client.
Although latency is poor compared to the other enrichment methods in this evaluation, an external cache has the advantage over local caches of not being affected when \textit{TaskManagers} fail and does not need to be refilled. 
In order to show how a streaming job with a local cache behaves in the case of a failure of all \textit{TaskManagers}, we conducted an experiment in which all \textit{TaskManager} processes were deleted every few minutes. 
We chose the streaming job with the enrichment method with local cache and preceding custom partitioner for the experiment. 
~\autoref{fig:cache_partition_failures_latency} shows how the latency behaves in the case of a failure of all \textit{TaskManagers} at once. 
It can be seen that in the beginning, after restarting the streaming job at minutes 4 and 10, the volatility is comparatively high and then decreases after a short time and the latency becomes constant again. 
This is due to the fact that the cache must first be filled again by database accesses. 
~\autoref{fig:cache_partition_failures_cachehit} shows the corresponding cache hit rates. 
It can be seen that after all \textit{TaskManagers} fail, the cache hit is back at 100\% after a short time. 
This is because a single local cache contains only 3.000 entries and with a throughput of 4.000 events/s it is filled again very timely.
Note that our chosen latency measurement does not account for waiting time of events at the streaming platform, which is why we do not see a spike of latency in~\autoref{fig:cache_partition_failures_latency} after complete failure even though real consumer lag is experienced.

% Figure environment removed

% Figure environment removed

\subsubsection{State}

As an alternative to enriching events using a cache, we also evaluated data stream enrichment using Flinkâ€™s HashMapStateBackend.
Latency is measured over a period of 70 minutes, with a fixed throughput of 4.000 events/s.
We evaluated the enrichment method using two different amounts of historical transaction events, 2,000 and 200,000, since the amount of data managed within the Flink cluster has an impact on latency.
This means that depending on the execution, the available amount of enrichment data is written to Kafka and is then read by Flink as another data stream to enrich the current events.
The results, illustrated in~\autoref{fig:stream_enrichment}, showed that as the amount of enrichment data increased, the latency also increased. 
However, both methods of enrichment using the state backend had lower latency and minor fluctuations compared to enrichment using asynchronous database queries. 
Increasing the amount of data could potentially result in increased latency, and a geographically distributed Flink cluster could further increase network overhead and latency.

\subsection{Log Analytics Use Case}

% Figure environment removed

For the log analytics use case, we evaluated the performance of embedded ML models in Flink with a focus on the number and size of models. 
The evaluation used pre-trained ResNet models, trained on the ImageNet dataset and provided by the ONNX community. 
ResNet models from the image classification domain were selected since different models are provided in terms of memory capacity. The context of the models does not fit into the area of log analytics, but the goal of this evaluation is to achieve a realistic performance analysis of embedded models.
The selected models were ResNet-18 with 18 layers and 44.7 MB memory size and ResNet-101 with 101 layers and 170.6 MB memory size. 
These models receive mini-batches of 3-channel RGB images of the form (N x 3 x H x W), where N is the batch size and H and W must be at least 224.
During the evaluation, the same image with a batch size of 1 and 224 for H and W was always selected as input. 
The models were stored in a local cache in Flink, which was flushed every four minutes to control the number of cache misses and create the same conditions for all executions. 
The throughput chosen for the evaluation was 1000 events/s, and the streaming job was run for 25 minutes with only 12 different keys that were normally distributed. 
For each key, the same model was used, which meant that when all caches were filled, the same model was cached 12 times.

The evaluation revealed that the Java Virtual Machine (JVM) Garbage Collector (GC) was slow in freeing the memory occupied by the ONNX session, which has a similar memory size as the models. 
As a result, the memory resources of a \textit{TaskManager} were quickly used up if new ONNX sessions were added several times per second, while removing old ones from the cache. 
This led to the \textit{TaskManager} crashing, and the entire streaming job had to be restarted. 
It was assumed that the amount of memory of the models was larger than the memory capacity of the \textit{TaskManagers}. 
To prevent the GC from being overwhelmed, the caches were flushed every four minutes, and the number of cache misses was controlled. 
The latencies of the embedded executions of the models were affected by the loading of the models from Google Cloud Storage and the subsequent creation of the ONNX session. 
The latency was highest at the beginning of the execution due to these processes.
The evaluation showed that the ONNX session creation time for the ResNet-18 model was almost up to half a second, while for the ResNet-101 model, it could take over a second. 
The prediction duration for the ResNet-101 model was also more than twice as long as that of the ResNet-18 model. 
The maximum execution times of these two processes for both models are shown in Table \ref{table:model_time_details}. 
The latencies of the ResNet-101 model, as depicted in~\autoref{fig:model_time_latency}, were higher than those of the ResNet-18 model, particularly during the periods when the cache was cleared every four minutes, increasing up to 3.6 seconds during these periods after the latencies normalized in the first few minutes.
Generally, we conclude that while good latencies can be obtained using embedded ML models, established frameworks such as Flink are not yet optimized for such usage, as indicated by our previous findings.

\subsection{Discussion}

\begin{table}
\centering
\caption{Model Details for Log Analytics Use Case}
\begin{tabular}{ccccc} 
 \toprule
 Model & Size & \shortstack{GCS\\Fetch Time} & \shortstack{Session\\Creation Time} & \shortstack{Prediction\\Time} \\
 \midrule
 resnet-18 & 44.7Mb & 871ms & 461ms & 90.8ms \\
 resnet-101 & 170.6Mb & 1663ms & 1098ms & 216.75ms \\
 \bottomrule
\end{tabular}
\label{table:model_time_details}
\end{table}

We presented the evaluation results of different enrichment methods executed in a real data infrastructure in GCP using Flink as a representative DSP system. 
These enrichment methods were to varying extents tested with two representative use cases, where the fraud detection use case can be associated with our defined categories of \emph{simple} and \emph{complex queries}, and the log analytics use case sheds light on a special case from category \emph{finite data sources}.
The experiments conducted to assess synchronous and asynchronous Cassandra queries showed that streaming jobs with asynchronous queries have better performance and are more resource-efficient. 
The evaluation of caching enrichment methods revealed that, for the investigated workloads, using a local cache rather than an external cache leads to better performance, but places a greater load on the resources of the respective DSP system. 
It should be noted that outsourcing data to a cache is not always possible due to the complexity of the database queries or the data structure. 
The use of state backend for enrichment demonstrated reliable enrichment of stream events, but the performance decreases with an increasing amount of enrichment data. 
Furthermore, running embedded ML models in Flink is not suitable for performance-heavy workloads in a single task, and multiple memory-intensive workloads tend to consume too much of available resources, causing the JVM Garbage Collector to free memory slowly.
All in all, and regardless of the details of the implementations, the results presented allow an assessment of in which case and at which approximate throughput rates certain methods of data enrichment are appropriate and what advantages and disadvantages they bring.