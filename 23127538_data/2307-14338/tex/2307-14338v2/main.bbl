\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akiba et~al.(2019)Akiba, Sano, Yanase, Ohta, and Koyama]{akiba2019optuna}
T.~Akiba, S.~Sano, T.~Yanase, T.~Ohta, and M.~Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In \emph{KDD}, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv}, 1607.06450v1, 2016.

\bibitem[Baldi et~al.(2014)Baldi, Sadowski, and Whiteson]{higgs}
P.~Baldi, P.~Sadowski, and D.~Whiteson.
\newblock Searching for exotic particles in high-energy physics with deep learning.
\newblock \emph{Nature Communications}, 5, 2014.

\bibitem[Blackard and Dean.(2000)]{covertype}
J.~A. Blackard and D.~J. Dean.
\newblock Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables.
\newblock \emph{Computers and Electronics in Agriculture}, 24\penalty0 (3):\penalty0 131--151, 2000.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, van~den Driessche, Lespiau, Damoc, Clark, de~Las~Casas, Guy, Menick, Ring, Hennigan, Huang, Maggiore, Jones, Cassirer, Brock, Paganini, Irving, Vinyals, Osindero, Simonyan, Rae, Elsen, and Sifre]{borgeaud2022improving}
S.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican, G.~van~den Driessche, J.~Lespiau, B.~Damoc, A.~Clark, D.~de~Las~Casas, A.~Guy, J.~Menick, R.~Ring, T.~Hennigan, S.~Huang, L.~Maggiore, C.~Jones, A.~Cassirer, A.~Brock, M.~Paganini, G.~Irving, O.~Vinyals, S.~Osindero, K.~Simonyan, J.~W. Rae, E.~Elsen, and L.~Sifre.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{ICML}, 2022.

\bibitem[Bottou and Vapnik(1992)]{bottou1992local}
L.~Bottou and V.~Vapnik.
\newblock Local learning algorithms.
\newblock \emph{Neural Computation}, 4, 1992.

\bibitem[Chen and Guestrin(2016)]{chen2016xgboost}
T.~Chen and C.~Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{SIGKDD}, 2016.

\bibitem[Das et~al.(2021)Das, Zaheer, Thai, Godbole, Perez, Lee, Tan, Polymenakos, and McCallum]{das2021case}
R.~Das, M.~Zaheer, D.~Thai, A.~Godbole, E.~Perez, J.~Y. Lee, L.~Tan, L.~Polymenakos, and A.~McCallum.
\newblock Case-based reasoning for natural language queries over knowledge bases.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Du et~al.(2022)Du, Zhang, Zhou, Wang, Zhao, Jin, Gan, Zhang, and Wipf]{kounianhua2022learning}
K.~Du, W.~Zhang, R.~Zhou, Y.~Wang, X.~Zhao, J.~Jin, Q.~Gan, Z.~Zhang, and D.~P. Wipf.
\newblock Learning enhanced representation for tabular data via neighborhood propagation.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Bindel, Weinberger, and Wilson]{gardner2018gpytorch}
J.~R. Gardner, G.~Pleiss, D.~Bindel, K.~Q. Weinberger, and A.~G. Wilson.
\newblock Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Gorishniy et~al.(2021)Gorishniy, Rubachev, Khrulkov, and Babenko]{gorishniy2021revisiting}
Y.~Gorishniy, I.~Rubachev, V.~Khrulkov, and A.~Babenko.
\newblock Revisiting deep learning models for tabular data.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Gorishniy et~al.(2022)Gorishniy, Rubachev, and Babenko]{gorishniy2022embeddings}
Y.~Gorishniy, I.~Rubachev, and A.~Babenko.
\newblock On embeddings for numerical features in tabular deep learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Grinsztajn et~al.(2022)Grinsztajn, Oyallon, and Varoquaux]{grinsztajn2022why}
L.~Grinsztajn, E.~Oyallon, and G.~Varoquaux.
\newblock Why do tree-based models still outperform deep learning on typical tabular data?
\newblock In \emph{NeurIPS, the "Datasets and Benchmarks" track}, 2022.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020retrieval}
K.~Guu, K.~Lee, Z.~Tung, P.~Pasupat, and M.~Chang.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{ICML}, 2020.

\bibitem[Hazimeh et~al.(2020)Hazimeh, Ponomareva, Mol, Tan, and Mazumder]{hazimeh2020tree}
H.~Hazimeh, N.~Ponomareva, P.~Mol, Z.~Tan, and R.~Mazumder.
\newblock The tree ensemble layer: Differentiability meets conditional computation.
\newblock In \emph{ICML}, 2020.

\bibitem[Huang et~al.(2020)Huang, Khetan, Cvitkovic, and Karnin]{huang2020tabtransformer}
X.~Huang, A.~Khetan, M.~Cvitkovic, and Z.~Karnin.
\newblock Tabtransformer: Tabular data modeling using contextual embeddings.
\newblock \emph{arXiv}, 2012.06678v1, 2020.

\bibitem[Iscen et~al.(2022)Iscen, Bird, Caron, Fathi, and Schmid]{iscen2022memory}
A.~Iscen, T.~Bird, M.~Caron, A.~Fathi, and C.~Schmid.
\newblock A memory transformer network for incremental learning.
\newblock \emph{arXiv}, abs/2210.04485v1, 2022.

\bibitem[Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi{-}Yu, Joulin, Riedel, and Grave]{izacard2022few}
G.~Izacard, P.~S.~H. Lewis, M.~Lomeli, L.~Hosseini, F.~Petroni, T.~Schick, J.~Dwivedi{-}Yu, A.~Joulin, S.~Riedel, and E.~Grave.
\newblock Few-shot learning with retrieval augmented language models.
\newblock \emph{arXiv}, abs/2208.03299v3, 2022.

\bibitem[James et~al.(2013)James, Witten, Hastie, and Tibshirani]{james2013introduction}
G.~James, D.~Witten, T.~Hastie, and R.~Tibshirani.
\newblock \emph{An Introduction to Statistical Learning}.
\newblock Springer, 2013.
\newblock \url{https://www.statlearning.com/}.

\bibitem[Jia et~al.(2021)Jia, Chen, Wu, Cardie, Belongie, and Lim]{jia2021rethinking}
M.~Jia, B.-C. Chen, Z.~Wu, C.~Cardie, S.~Belongie, and S.-N. Lim.
\newblock Rethinking nearest neighbors for visual classification.
\newblock \emph{arXiv preprint arXiv:2112.08459}, 2021.

\bibitem[Kadra et~al.(2021)Kadra, Lindauer, Hutter, and Grabocka]{kadra2021well}
A.~Kadra, M.~Lindauer, F.~Hutter, and J.~Grabocka.
\newblock Well-tuned simple nets excel on tabular datasets.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ke et~al.(2017)Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu]{ke2017lightgbm}
G.~Ke, Q.~Meng, T.~Finley, T.~Wang, W.~Chen, W.~Ma, Q.~Ye, and T.-Y. Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock \emph{Advances in neural information processing systems}, 30:\penalty0 3146--3154, 2017.

\bibitem[{Kelley Pace} and Barry(1997)]{california}
R.~{Kelley Pace} and R.~Barry.
\newblock Sparse spatial autoregressions.
\newblock \emph{Statistics \& Probability Letters}, 33\penalty0 (3):\penalty0 291--297, 1997.

\bibitem[Khandelwal et~al.(2020)Khandelwal, Levy, Jurafsky, Zettlemoyer, and Lewis]{khandelwal2020generalization}
U.~Khandelwal, O.~Levy, D.~Jurafsky, L.~Zettlemoyer, and M.~Lewis.
\newblock Generalization through memorization: Nearest neighbor language models.
\newblock In \emph{ICLR}, 2020.

\bibitem[Kim et~al.(2019)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum, Vinyals, and Teh]{kim2019attentive}
H.~Kim, A.~Mnih, J.~Schwarz, M.~Garnelo, S.~M.~A. Eslami, D.~Rosenbaum, O.~Vinyals, and Y.~W. Teh.
\newblock Attentive neural processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Klambauer et~al.(2017)Klambauer, Unterthiner, Mayr, and Hochreiter]{klambauer2017self}
G.~Klambauer, T.~Unterthiner, A.~Mayr, and S.~Hochreiter.
\newblock Self-normalizing neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Kohavi(1996)]{adult}
R.~Kohavi.
\newblock Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid.
\newblock In \emph{KDD}, 1996.

\bibitem[Kossen et~al.(2021)Kossen, Band, Lyle, Gomez, Rainforth, and Gal]{kossen2021self}
J.~Kossen, N.~Band, C.~Lyle, A.~N. Gomez, T.~Rainforth, and Y.~Gal.
\newblock Self-attention between datapoints: Going beyond individual input-output pairs in deep learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"{u}}ttler, Lewis, Yih, Rockt{\"{a}}schel, Riedel, and Kiela]{lewis2020retrieval}
P.~S.~H. Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal, H.~K{\"{u}}ttler, M.~Lewis, W.~Yih, T.~Rockt{\"{a}}schel, S.~Riedel, and D.~Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Long et~al.(2022)Long, Yin, Ajanthan, Nguyen, Purkait, Garg, Blair, Shen, and van~den Hengel]{long2022retrieval}
A.~Long, W.~Yin, T.~Ajanthan, V.~Nguyen, P.~Purkait, R.~Garg, A.~Blair, C.~Shen, and A.~van~den Hengel.
\newblock Retrieval augmented classification for long-tail visual recognition.
\newblock In \emph{CVPR}, 2022.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2019decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Malinin et~al.(2021)Malinin, Band, Chesnokov, Gal, Gales, Noskov, Ploskonosov, Prokhorenkova, Provilkov, Raina, Raina, Shmatova, Tigas, and Yangel]{malinin2021weather}
A.~Malinin, N.~Band, G.~Chesnokov, Y.~Gal, M.~J.~F. Gales, A.~Noskov, A.~Ploskonosov, L.~Prokhorenkova, I.~Provilkov, V.~Raina, V.~Raina, M.~Shmatova, P.~Tigas, and B.~Yangel.
\newblock Shifts: A dataset of real distributional shift across multiple large-scale tasks.
\newblock \emph{ArXiv}, abs/2107.07455v3, 2021.

\bibitem[Nader et~al.(2022)Nader, Sixt, and Landgraf]{nader2022dnnr}
Y.~Nader, L.~Sixt, and T.~Landgraf.
\newblock Dnnr: Differential nearest neighbors regression.
\newblock In \emph{ICML}, 2022.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay]{pedregosa2011scikit}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830, 2011.

\bibitem[Popov et~al.(2020)Popov, Morozov, and Babenko]{popov2020neural}
S.~Popov, S.~Morozov, and A.~Babenko.
\newblock Neural oblivious decision ensembles for deep learning on tabular data.
\newblock In \emph{ICLR}, 2020.

\bibitem[Prokhorenkova et~al.(2018)Prokhorenkova, Gusev, Vorobev, Dorogush, and Gulin]{prokhorenkova2018catboost}
L.~Prokhorenkova, G.~Gusev, A.~Vorobev, A.~V. Dorogush, and A.~Gulin.
\newblock Catboost: unbiased boosting with categorical features.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Qin et~al.(2020)Qin, Zhang, Wu, Jin, Fang, and Yu]{qin2020user}
J.~Qin, W.~Zhang, X.~Wu, J.~Jin, Y.~Fang, and Y.~Yu.
\newblock User behavior retrieval for click-through rate prediction.
\newblock In \emph{SIGIR}, 2020.

\bibitem[Qin et~al.(2021)Qin, Zhang, Su, Liu, Liu, Tang, He, and Yu]{qin2021retrieval}
J.~Qin, W.~Zhang, R.~Su, Z.~Liu, W.~Liu, R.~Tang, X.~He, and Y.~Yu.
\newblock Retrieval {\&} interaction machine for tabular data prediction.
\newblock In \emph{KDD}, 2021.

\bibitem[Qin and Liu(2013)]{microsoft}
T.~Qin and T.~Liu.
\newblock Introducing {LETOR} 4.0 datasets.
\newblock \emph{arXiv}, 1306.2597v1, 2013.

\bibitem[Ramsauer et~al.(2021)Ramsauer, Sch{\"{a}}fl, Lehner, Seidl, Widrich, Gruber, Holzleitner, Adler, Kreil, Kopp, Klambauer, Brandstetter, and Hochreiter]{ramsauer2021hopfield}
H.~Ramsauer, B.~Sch{\"{a}}fl, J.~Lehner, P.~Seidl, M.~Widrich, L.~Gruber, M.~Holzleitner, T.~Adler, D.~P. Kreil, M.~K. Kopp, G.~Klambauer, J.~Brandstetter, and S.~Hochreiter.
\newblock Hopfield networks is all you need.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Sch{\"{a}}fl et~al.(2022)Sch{\"{a}}fl, Gruber, Bitto{-}Nemling, and Hochreiter]{schafl2022hopular}
B.~Sch{\"{a}}fl, L.~Gruber, A.~Bitto{-}Nemling, and S.~Hochreiter.
\newblock Hopular: Modern hopfield networks for tabular data.
\newblock \emph{arXiv}, abs/2206.00664, 2022.

\bibitem[Somepalli et~al.(2021)Somepalli, Goldblum, Schwarzschild, Bruss, and Goldstein]{somepalli2021saint}
G.~Somepalli, M.~Goldblum, A.~Schwarzschild, C.~B. Bruss, and T.~Goldstein.
\newblock {SAINT:} improved neural networks for tabular data via row attention and contrastive pre-training.
\newblock \emph{arXiv}, 2106.01342v1, 2021.

\bibitem[Vanschoren et~al.(2014)Vanschoren, van Rijn, Bischl, and Torgo]{openml}
J.~Vanschoren, J.~N. van Rijn, B.~Bischl, and L.~Torgo.
\newblock Openml: networked science in machine learning.
\newblock \emph{arXiv}, 1407.7722v1, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Wang and Sabuncu(2023)]{wang2023flexible}
A.~Q. Wang and M.~R. Sabuncu.
\newblock A flexible nadaraya-watson head can offer explainable and calibrated classification.
\newblock In \emph{TMLR}, 2023.

\bibitem[Wang et~al.(2020)Wang, Shivanna, Cheng, Jain, Lin, Hong, and Chi]{wang2020dcn2}
R.~Wang, R.~Shivanna, D.~Z. Cheng, S.~Jain, D.~Lin, L.~Hong, and E.~H. Chi.
\newblock Dcn v2: Improved deep \& cross network and practical lessons for web-scale learning to rank systems.
\newblock \emph{arXiv}, 2008.13535v2, 2020.

\bibitem[Wang et~al.(2022)Wang, Xu, Fang, Liu, Sun, Xu, Zhu, and Zeng]{wang2022training}
S.~Wang, Y.~Xu, Y.~Fang, Y.~Liu, S.~Sun, R.~Xu, C.~Zhu, and M.~Zeng.
\newblock Training data is more valuable than you think: A simple and effective method by retrieving from training data.
\newblock \emph{arXiv preprint arXiv:2203.08773}, 2022.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and Xing]{wilson2016deep}
A.~G. Wilson, Z.~Hu, R.~Salakhutdinov, and E.~P. Xing.
\newblock Deep kernel learning.
\newblock In \emph{AISTATS}, 2016.

\bibitem[Zhao and Cho(2018)]{zhao2018retrieval}
J.~Zhao and K.~Cho.
\newblock Retrieval-augmented convolutional neural networks for improved robustness against adversarial examples.
\newblock \emph{arXiv preprint arXiv:1802.09502}, 2018.

\end{thebibliography}
