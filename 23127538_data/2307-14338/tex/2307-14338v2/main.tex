\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

% >>> Suggested packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\definecolor{citecolor}{rgb}{0.0, 0.15, 0.5}
\definecolor{linkcolor}{rgb}{0.55, 0.0, 0.25}
\definecolor{urlcolor}{rgb}{0.0, 0.2, 0.55}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=citecolor,linkcolor=linkcolor,urlcolor=urlcolor,bookmarks=false]{hyperref}

\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{nicefrac}
\usepackage{lipsum}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{fontawesome5}
\usepackage{pgf}
\usepackage[binary-units]{siunitx}

% >>> Other packages.
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{anyfontsize}
\usepackage{caption}
\usepackage{color}
\usepackage{enumitem}
\usepackage{float}
\usepackage{hyperref}
\usepackage{nicefrac}
\usepackage{rotating}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{transparent}
\usepackage{wrapfig}
\graphicspath{ {./data/} }

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{lib}

% >>> Paper.
\title{TabR: Tabular Deep Learning Meets \\ Nearest Neighbors in 2023}

\author{%
    Yury Gorishniy\thanks{
        The first author: \texttt{firstnamelastname@gmail.com}
        \qquad
        \qquad \textsuperscript{$\dag$}Yandex
        \qquad \textsuperscript{$\ddag$}HSE
    }\textsuperscript{\hspace{0.5em}$\dag$}
    \qquad \qquad Ivan Rubachev\textsuperscript{$\ddag \dag$}
    \qquad \qquad Nikolay Kartashev\textsuperscript{$\ddag \dag$}
    \\
    \textbf{
    \quad \qquad \qquad Daniil Shlenskii\textsuperscript{$\dag$}
    \qquad \qquad Akim Kotelnikov\textsuperscript{$\ddag \dag$}
    \qquad \qquad Artem Babenko\textsuperscript{$\dag \ddag$}
    }
}

\iclrfinalcopy
\begin{document}

\maketitle

\begin{abstract}
Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers.
However, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems.
One of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models.
For a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction.

In this work, we present \model\ -- essentially, a feed-forward network with a custom k-Nearest-Neighbors-like component in the middle.
On a set of public benchmarks with datasets up to several million objects, \model\ marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see \autoref{fig:timeline}).
Among the important findings and technical details powering \model, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them.
In addition to the much higher performance, \model\ is simple and significantly more efficient compared to prior retrieval-based tabular DL models.
The source code is published: \href{https://github.com/yandex-research/tabular-dl-tabr}{link}.
\end{abstract}

% Figure environment removed

\section{Introduction}
\label{sec:introduction}

Machine learning (ML) problems on tabular data, where objects are described by a set of heterogeneous features, are ubiquitous in industrial applications in medicine, finance, manufacturing, and other fields.
Historically, for these tasks, the models based on gradient-boosted decision trees (GBDT) have been a go-to solution for a long time.
However, lately, tabular deep learning (DL) models have been receiving increasingly more attention, and they are becoming more competitive \citep{klambauer2017self, popov2020neural, wang2020dcn2, hazimeh2020tree, huang2020tabtransformer, gorishniy2021revisiting, somepalli2021saint, kossen2021self, gorishniy2022embeddings}.

In particular, several attempts to design a retrieval-augmented tabular DL model have been recently made \citep{somepalli2021saint,qin2021retrieval,kossen2021self}.
For a target object, a retrieval-augmented model retrieves additional objects from the training set (e.g. the target object's nearest neighbors, or even the whole training set) and uses them to improve the prediction for the target object.
In fact, the retrieval technique is widely popular in other domains, including natural language processing \citep{das2021case, wang2022training, izacard2022few}, computer vision \citep{jia2021rethinking, iscen2022memory, long2022retrieval}, CTR prediction \citep{qin2020user,qin2021retrieval,kounianhua2022learning} and others.
Compared to purely parametric (i.e. retrieval-free) models, the retrieval-based ones can achieve higher performance and also exhibit several practically important properties, such as the ability for incremental learning and better robustness \citep{das2021case, jia2021rethinking}.

While multiple retrieval-augmented models for tabular data problems exist, in our experiments, we show that they provide if only minor benefits over the properly tuned multilayer perceptron (MLP; the simplest parametric model), while being significantly more complex and costly.
Nevertheless, in this work, we show that, with certain previously overlooked design aspects in mind, it is possible to obtain a retrieval-based tabular architecture that is powerful, simple and substantially more efficient than prior retrieval-based models.
We summarize our main contributions as follows:
\begin{enumerate}[nosep, leftmargin=2em]
    \item We design \model\ -- a simple retrieval-augmented tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among DL models, achieves the new state-of-the-art on several datasets and is significantly more efficient than prior deep retrieval-based tabular models.
    \item In particular, \model\ achieves a notable milestone for tabular DL by outperforming GBDT on the recently proposed benchmark with middle-scale tasks \citep{grinsztajn2022why}, which was originally used to illustrate the superiority of decision-tree-based models over DL models. Tree-based models, in turn, remain a cheaper solution.
    \item We highlight the important degrees of freedom of the attention mechanism (the often used module in retrieval-based models) that allow designing better retrieval-based tabular models.
\end{enumerate}

\section{Related work}
\textbf{Gradient boosted decision trees (GBDT)}.
GBDT-based ML models are non-DL solutions for supervised problems on tabular data that are popular within the community due to their strong performance and high efficiency.
By employing the modern DL building blocks and, in particular, the retrieval technique, our new model successfully competes with GBDT and, in particular, demonstrates that DL models can be superior on non-big data by outperforming GBDT on the recently proposed benchmark with small-to-middle scale tasks \citep{grinsztajn2022why}.

\textbf{Parametric deep learning models.}
Parametric tabular DL is a rapidly developing research direction aimed at bringing the benefits of deep learning to the world of tabular data while achieving competitive performance \citep{klambauer2017self, popov2020neural, wang2020dcn2, hazimeh2020tree, huang2020tabtransformer, gorishniy2021revisiting, gorishniy2022embeddings}.
The recent studies reveal that MLP-like backbones are still competitive \citep{gorishniy2021revisiting,kadra2021well,gorishniy2022embeddings}, and that embeddings for continuous features \citep{gorishniy2022embeddings} significantly reduce the gap between tabular DL and GBDT.
In this work, we show that a properly designed retrieval component can boost the performance of tabular DL even further.

\textbf{Retrieval-augmented models in general.}
Usually, the retrieval-based models are designed as follows.
For an input object, first, they retrieve relevant samples from available (training) data.
Then, they process the input object together with the retrieved instances to produce the final prediction for the input object.
One of the common motivations for designing retrieval-based schemes is the local learning paradigm \citep{bottou1992local}, and the simplest possible example of such a model is the $k$-nearest neighbors (kNN) algorithm \citep{james2013introduction}.
The promise of retrieval-based approaches was demonstrated across various domains, such as natural language processing \citep{lewis2020retrieval,guu2020retrieval,khandelwal2020generalization,izacard2022few,borgeaud2022improving}, computer vision \citep{iscen2022memory, long2022retrieval}, CTR prediction \citep{qin2020user,qin2021retrieval,kounianhua2022learning} and others.
Additionally, retrieval-augmented models often have useful properties such as better interpretability \citep{wang2023flexible}, robustness \citep{zhao2018retrieval} and others.

\textbf{Retrieval-augmented models for tabular data problems.}
The classic example of non-deep retrieval-based tabular models are the neighbor-based and kernel methods \citep{james2013introduction, nader2022dnnr}.
There are also deep retrieval-based models applicable to (or directly designed for) tabular data problems \citep{wilson2016deep,kim2019attentive,ramsauer2021hopfield,kossen2021self,somepalli2021saint}.
Notably, some of them omit the retrieval step and use \textit{all} training data points as the ``retrieved'' instances \citep{somepalli2021saint,kossen2021self,schafl2022hopular}.
However, we show that the existing retrieval-based tabular DL models are only marginally better than simple parametric DL models, and that often comes with a cost of using heavy Transformer-like architectures.
Compared to prior work, where several layers with multi-head attention between objects and features are often used \citep{ramsauer2021hopfield,kossen2021self,somepalli2021saint}, our model \model\ implements its retrieval component with just \textit{one} single-head attention-like module.
Importantly, the single attention-like module of \model\ is customized in a way that makes it better suited for tabular data problems.
As a result, \model\ substantially outperforms the existing retrieval-based DL models while being significantly more efficient.

\section{\model}
\label{sec:model}

In this section, we design a new retrieval-augmented deep learning model for tabular data problems.

\subsection{Preliminaries}
\label{sec:model-preliminaries}

\textbf{Notation.}
For a given supervised learning problem on tabular data, we denote the dataset as $\left\{\left( x_i,y_i \right)\right\}_{i=1}^n$ where $x_i \in \X$ represents the $i$-th object's features and $y_i \in \Y$ represents the $i$-th  object's label.
Depending on the context, the $i$ index can be omitted.
We consider three types of tasks: binary classification $\Y = \{0, 1\}$,
multiclass classification $\Y = \{1, . . . , C\}$ and regression $\Y = \R$.
For simplicity, in most places, we will assume that $x_i$ contains only continuous (i.e., continuous) features, and we will give additional comments on binary and categorical features when necessary.
The dataset is split into three disjoint parts: $\overline{1,n} = I_{train} \cup I_{val} \cup I_{test}$, where the ``train'' part is used for training, the ``validation'' part is used for early stopping and hyperparameter tuning, and the ``test'' part is used for the final evaluation.
An input object for which a given model makes a prediction is referred to as ``input object'' or ``target object''.

When the retrieval technique is used for a given target object, the retrieval is performed within the set of ``context candidates'' or simply ``candidates'': $I_{cand} \subseteq I_{train}$.
The retrieved objects, in turn, are called ``context objects'' or simply ``context''.
Optionally, the target object can be included in its own context.
In this work, we use the same set of candidates for all input objects.

\textbf{Experiment setup.}
We extensively describe our tuning and evaluation protocols in \autoref{A:sec:experiment-setup}.
The most important points are that, for any given algorithm, on each dataset, following \citet{gorishniy2022embeddings}, (1) we perform hyperparameter tuning and early stopping using the \textit{validation} set; (2) for the best hyperparameters, in the main text, we report the metric on the \textit{test} set averaged over 15 random seeds, and provide standard deviations in \autoref{A:sec:extended-results}; (3) when comparing any two algorithms, we take the standard deviations into account as described in \autoref{A:sec:experiment-setup}; (4) to obtain ensembles of models of the same type, we split the 15 random seeds into three disjoint groups (i.e., into three ensembles) each consisting of five models, average predictions within each group, and report the average performance of the obtained three ensembles.

In this work, we mostly use the datasets from prior literature and provide their summary in \autoref{tab:datasets} (sometimes, we refer to this set of datasets as ``the default benchmark'').
Additionally, in \autoref{sec:main}, we use the recently introduced benchmark with middle-scale tasks ($\le 50K$ objects) \citep{grinsztajn2022why} where GBDT was reported to be superior to DL solutions.
\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Dataset properties. ``RMSE'' denotes root-mean-square error, ``Acc.'' denotes accuracy.}
    \label{tab:datasets}
    \scalebox{0.95}{\input{data/tab-datasest}}
\end{table*}

\subsection{Architecture}
\label{sec:model-architecture}

To build a retrieval-based tabular DL model, we choose an incremental approach, where we start from a simple retrieval-free architecture, and, step by step, add and improve a retrieval component.

Let's consider a generic feed-forward retrieval-free network $f(x) = P(E(x))$ informally partitioned into two parts: encoder $E: \X \rightarrow \R^d$ and predictor $P: \R^d \rightarrow \hat{\Y}$.
To incrementally make it retrieval-based, we add retrieval module $R$ in a residual branch after $E$ as illustrated in \autoref{fig:architecture-overview}, where $\tilde{x} \in \R^d$ is the intermediate representation of the target object, $\{\tilde{x}_i\}_{i \in I_{cand}} \subset \R^d$ are the intermediate representations of the candidates and $\{y_i\}_{i \in I_{cand}} \subset \Y$ are the labels of the candidates.

% Figure environment removed

\textbf{Encoder and predictor}.
The encoder $E$ and predictor $P$ modules (\autoref{fig:architecture-overview}) are not the focus of this work, so we keep them simple as illustrated in \autoref{fig:architecture-encoder-predictor}.

% Figure environment removed

% Figure environment removed

\textbf{Retrieval module}.
We define the retrieval module $R$ in the spirit of $k$-nearest neighbors as illustrated in \autoref{fig:architecture-retrieval}.
In the figure, the following formal details are omitted for clarity:
\begin{enumerate}[nosep, leftmargin=2em]
    \item If the encoder $E$ contains at least one \texttt{Block} (i.e. $N_E > 0$), then, before being passed to $R$, $\tilde{x}$ and all $\tilde{x}_i$ are normalized with a shared layer normalization \citep{ba2016layer}.
    \item Optionally, the target object itself can be unconditionally (i.e. ignoring the top-$m$ operation) added as the $(m+1)$-th object to its set of context objects with the similarity score $\mathcal{S}(\tilde{x}, \tilde{x})$.
    \item Dropout is applied to the weights produced by the softmax function.
    \item Throughout the paper, we use $m=96$ and $I_{cand} = I_{train}$.
\end{enumerate}

Now, we iterate over possible designs of the similarity module $\mathcal{S}$ and the value module $\mathcal{V}$ (introduced in \autoref{fig:architecture-retrieval}).
During this process, we do not use embeddings for numerical features \citep{gorishniy2022embeddings} in the \mbox{\texttt{Input Module}} of the encoder $E$ and set $N_E$ = 0, $N_P$ = 1 (see \autoref{fig:architecture-encoder-predictor}).

\textbf{Step-0. The vanilla-attention-like baseline}.
The self-attention operation \citep{vaswani2017attention} was often used in prior work to model the interaction between a target object and candidate/context objects \citep{somepalli2021saint,kossen2021self,schafl2022hopular}.
Then, instantiating retrieval module $R$ as the vanilla self-attention (modulo the top-$m$ operation) is a reasonable baseline:
\begin{align}
\label{eq:step-0}
\begin{split}
    \mathcal{S}(\tilde{x}, \tilde{x}_i) = W_Q (\tilde{x})^T W_K (\tilde{x_i}) \cdot d^{-\nicefrac{1}{2}} \qquad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_V (\tilde{x}_i)
\end{split}
\end{align}
where $W_Q$, $W_K$, and $W_V$ are linear layers, and the target object is added as the $(m + 1)$-th object to its own context (i.e., ignoring the top-$m$ operation).
As reported in \autoref{tab:design}, the Step-0 configuration performs similarly to MLP, which means that using the vanilla self-attention is a suboptimal strategy.

\textbf{Step-1. Adding context labels}.
A natural attempt to improve the Step-0 configuration is to utilize labels of the context objects, for example, by incorporating them into the value module as follows:
\begin{align}
\label{eq:step-1}
\begin{split}
    \mathcal{S}(\tilde{x}, \tilde{x}_i) = W_Q (\tilde{x})^T W_K (\tilde{x_i}) \cdot d^{-\nicefrac{1}{2}} \qquad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = \underline{W_Y (y_i) +} W_V (\tilde{x}_i)
\end{split}
\end{align}
where the difference with \autoref{eq:step-0} is the \underline{underlined} addition of $W_Y: \Y \rightarrow \R^d$, which is an embedding table for classification tasks and a linear layer for regression tasks.
\autoref{tab:design} shows no improvements from using labels, which is counter-intuitive.
Perhaps, the similarity module $\mathcal{S}$ taken from the vanilla attention does not allow benefiting from such a valuable signal as labels.

\textbf{Step-2. Improving the similarity module $\mathcal{S}$}.
Empirically, we observed that removing the notion of queries (i.e. removing $W_Q$) and using the $L_2$ distance instead of the dot product significantly improves performance on several datasets in \autoref{tab:design}:
\begin{align}
\label{eq:step-2}
\begin{split}
    \mathcal{S}(\tilde{x}, \tilde{x}_i) = \underline{-\| W_K(\tilde{x}) -  W_K(\tilde{x}_i) \|^2} \cdot d^{-\nicefrac{1}{2}} \qquad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_Y (y_i) + W_V (\tilde{x}_i)
\end{split}
\end{align}
where the difference with \autoref{eq:step-1} is \underline{underlined}.
This change is a turning point in our story, which was overlooked in prior work.
Crucially, in \autoref{A:sec:ablation}, we show that removing any of the three ingredients (context labels, key-only representation, $L_2$ distance) results in a performance drop back to the level of MLP.
While the $L_2$ distance is unlikely to be the universally best choice for problems (even within the tabular domain), it seems to be a reasonable default choice for tabular data problems.

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The performance of the implementations of the retrieval module $R$, described in \autoref{sec:model-architecture}.
        If a number is underlined, then it is better than the corresponding number from the previous step at least by the standard deviation.
        Noticeable improvements over MLP start at Step-2.
        Notation: \textdownarrow\ corresponds to RMSE, \textuparrow\ corresponds to accuracy.
    }
    \label{tab:design}
    \scalebox{0.9}{\input{data/tab-design}}
\end{table*}

\textbf{Step-3. Improving the value module $\mathcal{V}$}.
Now, we take inspiration from DNNR \citep{nader2022dnnr} -- the recently proposed generalization of the kNN algorithm for regression problems.
Namely, we make the value module $\mathcal{V}$ more expressive by taking the target object's representation $\tilde{x}$ into account:
\begin{align}
\label{eq:step-3}
\begin{split}
    & \mathcal{S}(\tilde{x}, \tilde{x}_i) = -\| W_K(\tilde{x}) -  W_K(\tilde{x}_i) \|^2 \cdot d^{-\nicefrac{1}{2}}\ \ \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_Y (y_i) + \underline{T(W_K(\tilde{x}) - W_K(\tilde{x}_i))} \\
    & T(\cdot) = \texttt{LinearWithoutBias}(\texttt{Dropout}(\texttt{ReLU}(\texttt{Linear}(\cdot))))
\end{split}
\end{align}
where the difference with \autoref{eq:step-2} is \underline{underlined}.
\autoref{tab:design} shows that the new value module further improves the performance on several datasets.
Intuitively, the term $W_Y(y_i)$ (the embedding of the context object's label) can be seen as the ``raw'' contribution of the $i$-th context object.
The term $T(W_K(\tilde{x}) - W_K(\tilde{x}_i))$ can be seen as the ``correction'' term, where the module $T$ translates the differences in the key space into the differences in the label embedding space.

\textbf{Step-4. \model}.
Finally, empirically, we observed that omitting the scaling term $d^{-\nicefrac{1}{2}}$ in the similarity module and not including the target object to its own context leads to better results on average as reported in \autoref{tab:design}.
Both aspects can be considered hyperparameters, and the above notes can be seen as our default recommendations.
We call the obtained model ``\model'' (Tab $\sim$ tabular, R $\sim$ retrieval).
The formal complete description of how \model\ implements the retrieval module $R$ is as follows:
\begin{align}
\label{eq:model}
\begin{split}
    k = W_K(\tilde{x}),\ k_i = W_K(\tilde{x}_i) \quad \mathcal{S}(\tilde{x}, \tilde{x}_i) = -\| k - k_i \|^2 \quad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_Y (y_i) + T(k - k_i)
\end{split}
\end{align}
where $W_K$ is a linear layer, $W_Y$ is an embedding table for classification tasks and a linear layer for regression tasks, (by default) a target object is not included in its own context, (by default) the similarity scores are not scaled, and $T(\cdot) = \texttt{LinearWithoutBias}(\texttt{Dropout}(\texttt{ReLU}(\texttt{Linear}(\cdot))))$.

\textbf{Limitations.}
\model\ has standard limitations of retrieval-augmented models, which we describe in \autoref{A:sec:limitations}.
We encourage practitioners to review the limitations before using \model\ in practice.

\section{Experiments on public benchmarks}
\label{sec:experiments}

In this section, we compare \model\ (introduced in \autoref{sec:model}) with existing retrieval-based solutions and state-of-the-art parametric models.
In addition to the fully-fledged configuration of \model\ (with all degrees of freedom available for $E$ and $P$ as described in \autoref{fig:architecture-encoder-predictor}), we also use \textbf{\modelsimple} (``S'' stands for ``simple'') -- a simple configuration, which does not use feature embeddings \citep{gorishniy2022embeddings}, has a linear encoder ($N_E = 0$) and a one-block predictor ($N_P = 1$).
We specify when \modelsimple\ is used only in tables, figures, and captions but not in the text.
For other details on \model, including hyperparameter tuning, see \autoref{A:sec:impl-model}.

\subsection{Evaluating retrieval-augmented deep learning models for tabular data}
\label{sec:revisiting}

In this section, we compare \model\ (\autoref{sec:model}) and the existing retrieval-augmented solutions with fully parametric DL models (see \autoref{A:sec:implementation-details} for implementation details for all algorithms). \autoref{tab:exp-revisiting} indicates that \model\ is the only retrieval-based model that provides a significant performance boost over MLP on many datasets.
In particular, the full variation of \model\ outperforms MLP-PLR (the modern parametric DL model with the highest average rank from \citet{gorishniy2022embeddings}) on several datasets (CA, OT, BL, WE, CO), and performs on par with it on the rest except for the MI dataset.
Regarding the prior retrieval-based solutions, we faced various technical limitations, such as incompatibility with classification problems and scaling issues (e.g., as we show in \autoref{A:sec:training-time}, it takes dramatically less time to train \model\ than NPT \citep{kossen2021self} -- the closest retrieval-based competitor from \autoref{tab:exp-revisiting}).
Notably, the retrieval component is not universally beneficial for all datasets.

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Comparing \model\ with existing retrieval-augmented tabular models and parametric DL models. The notation follows \autoref{tab:design}. The bold entries are the best-performing algorithms, which are defined with standard deviations taken into account as described in \autoref{A:sec:experiment-setup}.
    }
    \label{tab:exp-revisiting}
    \scalebox{0.75}{\input{data/tab-exp-revisiting}}
\end{table*}

The obtained results highlight the retrieval technique and embeddings for numerical features \citep{gorishniy2022embeddings} (used in MLP-PLR and \model) as two powerful architectural elements that improve the optimization properties of tabular DL models.
Interestingly, the two techniques are not fully orthogonal, but none of them can recover the full power of the other, and it depends on a given dataset whether one should prefer the retrieval, the embeddings, or a combination of both.

\textbf{The main takeaway.}
\model\ becomes a new strong deep learning solution for tabular data problems and demonstrates a good potential of the retrieval-based approach.
\model\ demonstrates strong average performance and achieves the new state-of-the-art on several datasets.

\subsection{Comparing \model\ with gradient-boosted decision trees}
\label{sec:main}

\begin{table*}[b]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Comparing ensembles of \model\ with ensembles of GBDT models.
        See \autoref{A:sec:impl-model} to learn how the ``default'' \modelsimple\ was obtained.
        The notation follows \autoref{tab:exp-revisiting}.
    }
    \label{tab:exp-gbdt-ours}
    \scalebox{0.85}{\input{data/tab-exp-gbdt-ours}}
\end{table*}

In this section, we compare \model\ with models based on gradient-boosted decision trees (GBDT): XGBoost \citep{chen2016xgboost}, LightGBM \citep{ke2017lightgbm} and CatBoost \citep{prokhorenkova2018catboost}.
Specifically, we compare ensembles (e.g. an ensemble of TabRs vs. an ensemble of XGBoosts) for a fair comparison since gradient boosting is already an ensembling technique.

\textbf{The default benchmark}.
\autoref{tab:exp-gbdt-ours} shows that, on the default benchmark, the tuned \model\ provides noticeable improvements over tuned GBDT on several datasets (CH, CA, HO, HI, WE, CO), while being competitive on the rest, except for the MI dataset.
The table also demonstrates that \model\ has a competitive default configuration (defined in \autoref{A:sec:impl-model}).

\textbf{The benchmark from} \citet{grinsztajn2022why}.
Now, we go further and use the recently proposed benchmark with small-to-middle-scale tasks \citet{grinsztajn2022why}.
Importantly, this benchmark was originally used to illustrate the superiority of GBDT over parametric DL models on datasets with $\leq 50K$ objects, which makes it an interesting challenge for \model.
We adjust the benchmark to our tuning and evaluation protocols (see \autoref{A:sec:benchmark-why} for details) and report the results in \autoref{tab:exp-gbdt-why}.
While MLP-PLR (one of the best parametric DL models) indeed is slightly inferior to GBDT on this set of tasks, \model\ makes a significant step forward and outperforms GBDT on average.

In the appendix, we provide more analysis: in \autoref{A:sec:xgboost-with-neighbors}, we try augmenting XGBoost with a retrieval component; in \autoref{A:sec:training-time}, we compare training times of \model\ and GBDT models.

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Comparing ensembles of DL models with ensembles of GBDT models on the benchmark from \citet{grinsztajn2022why} (e.g., an ensemble of MLPs vs ensemble of XGBoosts; note that in \autoref{fig:timeline}, we compare \textit{single} models, hence the different numbers).
        See \autoref{A:sec:impl-model} for the details on the ``default'' \modelsimple.
        The default configuration of \modelsimple\ is compared against the default configurations of GBDT models.
        The comparison is performed in a pairwise manner with standard deviations taken into account as described in \autoref{A:sec:experiment-setup}.
    }
    \label{tab:exp-gbdt-why}
    \scalebox{0.95}{\input{data/tab-exp-gbdt-why}}
\end{table*}

\textbf{The main takeaway}. After the comparison with GBDT, \model\ confirms its status of a new strong solution for tabular data problems: it provides strong average performance and can provide a noticeable improvement over GBDT on some datasets.

\section{Analysis}

\subsection{Freezing contexts for faster training of \model}
\label{sec:context-freeze}

In the vanilla formulation of \model\ (\autoref{sec:model}), for each training batch, the most up-to-date contexts are mined by encoding all the candidates and computing similarities with all of them, which can be prohibitively slow on large datasets.
For example, it takes more than 18 hours to train a single \model\ on the full ``Weather prediction'' dataset  \citep{malinin2021weather} (3M+ objects; with the default hyperparameters from \autoref{tab:exp-gbdt-ours}).
However, as we show in \autoref{fig:context-freeze}, for an average training object, its context (i.e. the top-$m$ candidates and the distribution over them according to the similarity module $\mathcal{S}$) gradually ``stabilizes'' during the course of training, which gives an opportunity for simple optimization.
Namely, after a fixed number of epochs, we can perform ``context freeze'': i.e., compute the up-to-date contexts for all training (but not validation and test) objects for the one last time and then reuse these contexts for the rest of the training.
\autoref{tab:context-freeze} indicates that, on some datasets, this simple technique allows accelerating training of \model\ without much loss in metrics, with more noticeable speedups on larger datasets.
In particular, on the full ``Weather prediction'' dataset, we achieve nearly sevenfold speedup (from 18h9min to 3h15min) while maintaining competitive RMSE.
See \autoref{A:sec:impl-context-freeze} for implementation details.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The performance of \modelsimple\ with the ``context freeze'' as described in \autoref{sec:context-freeze}.
        \modelsimple\ (CF-$N$) denotes \modelsimple\ with the context freeze applied after $N$ epochs.
        In parentheses, we provide the fraction of time spent on training compared to the training without freezing (the last row).
    }
    \label{tab:context-freeze}
    \scalebox{0.85}{\input{data/tab-context-freeze}}
\end{table*}

\subsection{Updating \model\ with new training data without retraining}
\label{sec:new-candidates}

Getting access to new unseen training data \textit{after} training a machine learning model (e.g., after collecting yet another portion of daily logs of an application) is a common practical scenario.
Technically, \model\ allows utilizing the new data \textit{without retraining} by adding the new data to the set of candidates for retrieval.
We test this approach on the full ``Weather prediction'' dataset \citep{malinin2021weather} (3M+ objects).
\autoref{fig:new-candidates} indicates that such ``online updates'' may be a viable solution for incorporating new data into an already trained \model.
Additionally, this approach can be used to scale \model\ to large datasets by training the model on a subset of data and retrieving from the full data.
Overall, we consider the conducted experiment as a preliminary exploration and leave a systematic study of continual updates for future work.
See \autoref{A:sec:impl-new-candidates} for implementation details.

\begin{minipage}[t]{0.48\textwidth}
    \centering
    % Figure removed
    \captionof{figure}{ 
        \mbox{$\Delta$-context} (explained below) averaged over training objects until the early stopping while training \modelsimple.
        On a given epoch, for a given object, \mbox{$\Delta$-context} shows the portion of its context (the top-$m$ candidates and their weights) changed compared to the previous epoch (i.e., the lower the value, the smaller the change; see \autoref{A:sec:impl-context-freeze} for formal details).
        The plot shows that context updates become less intensive during the course of training, which motivates the optimization described in \autoref{sec:context-freeze}.
    }
    \label{fig:context-freeze}
\end{minipage}
\hspace{\fill}
\begin{minipage}[t]{0.48\textwidth}
    \centering
    % Figure removed
    \captionof{figure}{
        Training \modelsimple\ on various portions of the training data of the full ``Weather prediction'' dataset and gradually adding the remaining unseen training data to the set of candidates \textit{without retraining} as described in \autoref{sec:new-candidates}.
        For each curve, the leftmost point corresponds to not adding any new data to the set of candidates after the training, and the rightmost point corresponds to adding all unseen training data to the set of candidates.
    }
    \label{fig:new-candidates}
\end{minipage}

\subsection{Further analysis}

In the appendix, we provide a more insightful analysis. A non-exhaustive list of examples:
\begin{itemize}[nosep,leftmargin=2em]
    \item in \autoref{A:sec:similarity-module}, we analyze the key-only $L_2$-based similarity module $\mathcal{S}$ introduced on Step-2 of \autoref{sec:model-architecture}, which was a turning point in our story.
    We provide intuition behind this specific implementation of $\mathcal{S}$ and perform an in-depth comparison with the similarity module of the vanilla attention (the dot product between queries and keys).

    \item in \autoref{A:sec:value-module}, we analyze the value module $\mathcal{V}$ introduced on Step-3 of \autoref{sec:model-architecture}.
    On regression problems, we confirm the correction semantics of the module $T$ from \autoref{eq:step-3}.

    \item in \autoref{A:sec:training-time}, we compare training times of \model\ with training times of all the baselines.
    We show that compared to prior retrieval-based tabular models, \model\ makes a big step forward in terms of efficiency.
    While \model\ is relatively slower than simple retrieval-free models, within the considered scope of dataset sizes, the absolute training times of \model\ are affordable for most practical scenarios.

    \item in \autoref{A:sec:technical-notes}, we highlight additional technical properties of \model.
\end{itemize}

\section{Conclusion \& Future work}

In this work, we have demonstrated that retrieval-based deep learning models have great potential in supervised machine learning problems on tabular data.
Namely, we have designed \model\ -- a retrieval-augmented tabular DL architecture that provides strong average performance and achieves the new state-of-the-art on several datasets.
Importantly, we have highlighted similarity and value modules as the important details of the attention mechanism which have a significant impact on the performance of attention-based retrieval components.

An important direction for future work is improving the efficiency of retrieval-augmented models to make them faster in general and in particular applicable to tens and hundreds of millions of data points.
Also, in this paper, we focused more on the aspect of task performance, so some other properties of \model\ remain underexplored.
For example, the retrieval nature of \model\ provides new opportunities for interpreting the model's predictions through the influence of context objects.
Also, \model\ may enable better support for continual learning (we scratched the surface of this direction in \autoref{sec:new-candidates}).
Regarding architecture details, possible directions are improving similarity and value modules, as well as performing multiple rounds of retrieval and interactions with the retrieved instances.

\section*{Reproducibility Statement}

To make the results and models reproducible and verifiable, \textbf{we provide our full codebase, all the results, and step-by-step usage instructions}: \href{https://github.com/yandex-research/tabular-dl-tabr}{link}.
In particular, (1) \textit{the results and hyperparameters reported the paper is just a summary of the results available at the provided URL} (with minor exceptions); (2) implementations of \model\ and all the baselines (except for NPT) are available; (3) the hyperparameter tuning, training and evaluation pipelines are available; (4) the hyperparameters are available; (5) the used datasets and splits are available; (6) hyperparameter tuning and training times are available; (7) the used hardware is available; (8) \textit{within a fixed environment (i.e. fixed hardware and software versions), most of the results are bitwise reproducible}.

\medskip
\bibliographystyle{abbrvnat}
\bibliography{references}

\newpage
\appendix

\section*{Supplementary material}

\section{Additional analysis}

\subsection{Similarity module of \model}
\label{A:sec:similarity-module}

\subsubsection{Intuitive motivation}

Recall that in Step-2 of \autoref{sec:model-architecture}, the change from the similarity module of the vanilla attention to the new key-only $L_2$-driven similarity module was a turning point in our story, where a retrieval-based model started showing noticeable improvements over MLP on several datasets.
In fact, in addition to the empirical results (in \autoref{tab:design} and \autoref{A:sec:ablation}), this specific similarity module has a reasonable intuitive motivation, which we now provide.

\begin{itemize}[nosep,leftmargin=2em]
    \item \textbf{First}, aligning two (query and key) representations of target and candidate objects is an additional challenge for the optimization process, and there is no clear motivation for introducing this challenge in our case.
    And, as demonstrated in \autoref{A:sec:ablation}, avoiding this challenge is not just beneficial, but rather necessary.
    \item \textbf{Second}, during the design process in \autoref{sec:model-architecture}, the similarity module $\mathcal{S}$ operates over \textit{linear transformations} of the input (because, at that point, encoder $E$ is just a linear layer, since we fixed $N_E=0$).
    Then, a reasonable similarity measure in the \textit{original} feature space may remain reasonable in the \textit{transformed} feature space.
    And, for tabular data, $L_2$ is usually a better similarity measure than the dot product in the \textit{original} feature space.
    Note that the case of shallow/linear encoder is a specific, but very important case: since $E$ is applied to many candidates on each training step, $E$ is better to be lightweight to maintain adequate efficiency.
\end{itemize}

Combined, the above two points motivate removing query representations and switching to the $L_2$ distance, which leads to the similarity module introduced in Step-2 of \autoref{sec:model-architecture}.

\subsubsection{Analyzing attention patterns over candidates}
\label{A:sec:similarity-module-patterns}

In this section, we analyze the similarity module $\mathcal{S}$ introduced in Step-2 of \autoref{sec:model-architecture}, which greatly improved the performance on several datasets in \autoref{tab:design}.

Formally, for a given input object, the similarity module defines a distribution over candidates (``weights'' in \autoref{fig:architecture-retrieval}) with exactly $m + 1$ non-zero entries ($m$ is the context size; $+1$ comes from adding the target object to its own context in Step-2).
Intuitively, the less diverse such distributions are on average, the more frequently \textit{different input objects} are augmented with \textit{similar contexts}.
In \autoref{A:tab:similarity-module}, we demonstrate that such distributions are more diverse on average with the new similarity module compared to the one from the vanilla attention.
The implementation details are provided in \autoref{A:sec:impl-similarity-module}.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Entropy of the average distribution over candidates (the averaging is performed over individual distributions for test objects).
        The distributions are produced by the similarity module as explained in \autoref{A:sec:similarity-module-patterns}.
        The trained Step-1 and Step-2 models are taken directly from \autoref{tab:design}.
        The similarity module introduced at Step-2 of \autoref{sec:model-architecture} produces more diverse contexts.
    }
    \label{A:tab:similarity-module}
    \scalebox{0.9}{\input{data/A-tab-similarity-module}}
\end{table*}

\subsubsection{Case studies}

In this section, we consider three datasets where the transition to the key-only $L_2$ similarity module from the vanilla dot-product-between-queries-and-keys demonstrated the most impressive performance.
Formally, this is the transition from ``Step-1'' to ``Step-2'' in \autoref{tab:design}.
For each of the three datasets, first, we notice that, for a given input object, there is a domain-specific notion of ``good neighbors'', i.e., such neighbors that, from a human perspective, are very relevant to the input object and provide strong hints for making a better prediction for the input object.
Then, we show that the new similarity module allows finding and exploiting those natural hints.

\textbf{California housing (CA)}.
On this dataset, the transition from the ``vanilla'' dot-product-between-queries-and-keys similarity module to the key-only $L_2$ similarity module resulted in a substantial performance boost, as indicated by the difference between ``Step-1'' and ``Step-2'' in \autoref{tab:design}.
On this dataset, the task is to estimate the prices of houses in California.
Intuitively, for a given house from the test set, the prices of the training houses in the geographical neighborhood should be a strong hint for solving the task.
Moreover, there are coordinates (longitude and latitude) among the features, which should simplify finding good neighbors.
And the ``Step-2'' model successfully does that, which is not true for the ``Step-1'' model.
Specifically, for an average test object, the ``Step-2'' model concentrates approximately 7\% of the attention mass on the object itself (recall that ``Step-2'' includes the target object in the context objects) and approximately 77\% on the context objects within the 10km radius. The corresponding numbers of the ``Step-1'' model are 0.07\% and 1\%.

\textbf{Weather prediction (WE)}.
Here, the story is seemingly similar to the one with the CA dataset analyzed in the previous paragraph, but in fact has a major difference.
Again, here, for a given test data point, the dataset contains natural hints in the form of geographical neighbors from the training set which allow making a better weather forecast for a test query; and the ``Step-2'' model (\autoref{tab:design}) successfully exploits that, while the ``Step-1'' model cannot pay any meaningful attention to those hints.
Specifically, for an average object, the ``Step-2'' model concentrates approximately 29\% of the attention mass on the object itself (recall that ``Step-2'' includes the target object in the context objects) and approximately 25\% on the context objects within the 200km radius. The corresponding numbers of the ``Step-1'' model are 0.25\% and 0.5\%.
\textbf{However}, there is a crucial distinction from the CA case: in the version of the dataset WE that we used, \textit{the features did not contain the coordinates}.
In other words, to perform the analysis, \textit{after} the training, we restored the original coordinates for each row from the original dataset and observed that the model \textit{learned} the ``correct'' notion of ``good neighbors'' from other features.

\textbf{Facebook comments volume (FB)}.
In this paper, this is the first time when we mention this dataset, which was used in prior work \citep{gorishniy2022embeddings} and which we also used for some time in this project.
Notably, on this dataset, \model\ was demonstrating unthinkable improvements over competitors (including GBDT and the best-in-class parametric DL models).
Then we noticed a strange pattern: often, for a given input, \model\ concentrated an abnormally high percentage of its attention mass on just one context object (a different one for each input object).
This is how we discovered that the dataset split that we inherited from \citet{gorishniy2022embeddings} contained a ``leak'': roughly speaking, for many objects, it was possible to find their almost exact copies in the training set, and the task was dramatically simpler with this kind of hint.
In practice, it was dramatically simpler for the \model, but not for other models.
Specifically, for an average object, the ``Step-2'' model concentrates approximately 20\% of the attention mass on the object itself (recall that ``Step-2'' includes the target object in the context objects) and approximately 35\% on its leaked almost-copies. The corresponding numbers of the ``Step-1'' model are 0.5\% and 0.09\%.

\subsection{Analyzing the value module of \model}
\label{A:sec:value-module}

In this section, we analyze the value module $\mathcal{V}$ of \model\ (see \autoref{eq:model}):
\begin{align}
\label{A:eq:step-3-module-reminder}
    \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)
    = W_Y (y_i) + T(k - k_i)
    = W_Y (y_i) + T(\Delta k_i)
\end{align}
Intuitively, for a given context object, its label $y_i$ can be an important part of its contribution to the prediction.
Let's consider regression problems, where, in \autoref{A:eq:step-3-module-reminder}, $y_i \in \R$ is embedded by $W_Y$ to $\tilde{\mathbb{Y}} \subset \R^d$.
Since $W_Y$ is a linear layer, $\tilde{\mathbb{Y}}$ is just a line, and each point on this line can be mapped back to the corresponding label from $\R$.
Then, the projection of the correction term $T(\Delta k_i)$ on $\tilde{\mathbb{Y}}$ can be translated to the correction of the context label $y_i$:
\begin{align}
\label{A:eq:step-3-module}
    \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)
    = W_Y (y_i) + \underline{\text{proj}_{\tilde{\mathbb{Y}}} T(\Delta k_i)} + \text{proj}_{\tilde{\mathbb{Y}}^\perp} T(\Delta k_i)
    = W_Y (y_i + \underline{\Delta y_i}) + \text{proj}_{\tilde{\mathbb{Y}}^\perp} T(\Delta k_i)
\end{align}
To check whether the underlined correction term $\text{proj}_{\tilde{\mathbb{Y}}} T(\Delta k_i)$ (or $\Delta y_i$) is important, we take a \textit{trained} \model, and reevaluate it \textit{without retraining} while ignoring this projection (which is equivalent to setting $\Delta y_i = 0$).
As a baseline, we also try ignoring the projection of $T(\Delta k_i)$ on a random one-dimensional subspace instead of $\tilde{\mathbb{Y}}$.
\autoref{A:tab:value-module} indicates that the correction along $\tilde{\mathbb{Y}}$ plays a vital role for the model.
The implementation details are provided in \autoref{A:sec:impl-value-module}.


\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Evaluating RMSE of trained \modelsimple\ while ignoring projections of $T(\Delta k_i)$ on different one-dimensional subspaces as described in \autoref{A:sec:value-module}.
        The first column shows the projection on which one-dimensional subspace is removed from $T(\Delta k_i)$.
        The first row corresponds to not removing any projections (i.e., the unmodified \modelsimple).
        Ignoring the projection on $\tilde{\Y}$ (the label embedding space) breaks the model while ignoring a random projection does not have much effect.
    }
    \label{A:tab:value-module}
    \scalebox{0.9}{\input{data/A-tab-value-module}}
\end{table*}


For classification problems, we tested similar hypotheses but did not obtain any interesting results.
Perhaps, the value module $\mathcal{V}$ and specifically the $T$ module should be designed differently to better model the nature of classification problems.

\subsection{Ablation study}
\label{A:sec:ablation}

Recall that on Step-2 of \autoref{sec:model-architecture}, we mentioned that it was crucial that \textit{all} changes from Step-2 compared to Step-0 (using labels + not using queries + using the $L_2$ distance instead of the dot product) are important to provide noticeable improvements over MLP on several datasets.
Note that not using queries is equivalent to sharing weights of $W_Q$ and $W_K$: $W_Q = W_K$.
\autoref{A:tab:ablation} contains the results of the corresponding experiment and indeed demonstrates that the Step-2 configuration cannot be trivially simplified without loss in metrics (see the CH, CA, BL, WE datasets).

Overall, we hypothesize that both things are important: how valuable the additional signal is (Step-1) and how well we measure the distance from the target object to the source of that valuable signal (Step-2).


\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The ablation study as described in \autoref{A:sec:ablation}.
        $W_Q = W_K$ means using only keys and not using queries.
        Step-2 is the only variation providing noticeable improvements over MLP on the CH, CA, BL, WE datasets.
    }
    \label{A:tab:ablation}
    \scalebox{0.9}{\input{data/A-tab-ablation}}
\end{table*}

% \clearpage
\subsection{Comparing training times}
\label{A:sec:training-time}

While \model\ demonstrates strong performance, these benefits do not come for free, since, as with all retrieval-augmented models, the retrieval component of \model\ brings additional overhead.
In this section, we aim to quantify this overhead by comparing the training times of \model\ with those of all the baselines.
\autoref{A:tab:training-time-all} shows two important things:

\begin{itemize}[nosep,leftmargin=2em]
    \item first, \model\ is significantly more efficient (i.e. provides a significantly better trade-off between the downstream performance and training times) than prior retrieval-augmented tabular models.
    In particular, \model\ is significantly (and, sometimes, dramatically) more efficient than NPT \citep{kossen2021self} -- the closest retrieval-based competitor according to \autoref{tab:exp-revisiting}.
    \newpage
    \item second, within the considered scope of dataset sizes, the absolute training times of \model\ will be affordable in practice.
    Moreover, the reported execution times are achieved with our naive implementation which lacks even some of the basic optimizations.
\end{itemize}

To sum up, compared to prior work on retrieval-based tabular DL, \model\ makes a big step forward in terms of efficiency.
\model\ is relatively slower than simple models (GBDT, parametric DL models), and improving its efficiency is an important research direction.
However, given the room for technical optimizations and techniques similar to context freeze (\autoref{sec:context-freeze}), the future of retrieval-based tabular DL looks positive.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Training times of the tuned models (from \autoref{tab:exp-revisiting}, \autoref{tab:exp-gbdt-ours} and \autoref{tab:context-freeze}) averaged over the random seeds.
        The format is \texttt{hh:mm:ss}.
        \modelsimple\ (CF-4) is \modelsimple\ with the context freeze (\autoref{sec:context-freeze}) applied after four epochs.
        Colors describe to the following informal tiers:
        \\
        {\color[HTML]{c1e57b} $\blacksquare$} <5 minutes \qquad
        {\color[HTML]{63bc62} $\blacksquare$} <30 minutes \qquad
        {\color[HTML]{fee491} $\blacksquare$} <2 hours \qquad
        {\color[HTML]{f57748} $\blacksquare$} <10 hours \qquad
        {\color[HTML]{a50026} $\blacksquare$} >10 hours
    }
    \label{A:tab:training-time-all}
    \scalebox{0.85}{\input{data/A-tab-training-time-all-color}}    
\end{table*}


\subsection{Augmenting XGBoost with a retrieval component}
\label{A:sec:xgboost-with-neighbors}

After the successful results of \model\ reported in \autoref{sec:main}, we tried augmenting XGBoost with a simple retrieval component to ensure that we do not miss this opportunity to improve the baselines.
Namely, for a given input object, we find $m=96$ (equal to the context size of \model) nearest training objects in the original feature space, average their features and labels (the label as-is for regression problems, the one-hot encoding representations for classification problems), concatenate the target object's features with the ``average neighbor's'' features and label, and the obtained vector is used as the input for XGBoost.
The results in \autoref{A:tab:xgboost-with-neighbors} indicate that this strategy does not lead to any noticeable profit for XGBoost.
We tried to vary the number of neighbors but did not achieve any significant improvements.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Results for ensembles of tuned models. ``XGBoost + retrieval'' stands for XGBoost augmented with the ``average neighbor's'' features and label as described in \autoref{A:sec:xgboost-with-neighbors}.}
    \label{A:tab:xgboost-with-neighbors}
    \scalebox{0.8}{\input{data/A-tab-xgboost-with-neighbors}}
\end{table*}

\clearpage
\subsection{Additional results for the ``context freeze'' technique}
\label{A:sec:context-freeze-extended}

We report the extended results for \autoref{sec:context-freeze} in \autoref{A:fig:context-freeze-extended}, \autoref{A:tab:context-freeze-extended-scores} and \autoref{A:tab:context-freeze-extended-time}.
For the formal definition of the $\Delta$-context metric, see \autoref{A:sec:impl-context-freeze}.

% Figure environment removed

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The extended version of \autoref{tab:context-freeze}.
        Freezing after 0 epochs means freezing with a randomly initialized model.
        The speedups are provided in \autoref{A:tab:context-freeze-extended-time}
    }
    \label{A:tab:context-freeze-extended-scores}
    \scalebox{0.8}{\input{data/A-tab-context-freeze-extended-scores}}
\end{table*}

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Fraction of time spent on training in \autoref{A:tab:context-freeze-extended-scores}, relative to the training time without the context freeze (the last row; the format is hours:minutes:seconds).}
    \label{A:tab:context-freeze-extended-time}
    \scalebox{0.8}{\input{data/A-tab-context-freeze-extended-time}}
\end{table*}

\subsection{Additional technical notes on \model}
\label{A:sec:technical-notes}

We highlight the following technical aspects of \model:
\begin{enumerate}[nosep,leftmargin=2em]
    \item Because of the changes introduced in the Step-3 in \autoref{sec:model-architecture}, the value representations $\mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)$ of the candidates cannot be precomputed for a trained model, since they depend on the target object.
    This implies roughly twice less memory usage when deploying the model to production (since only the key representations and labels have to be deployed for training objects), but $\mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)$ has to be computed in runtime.
    \item Despite the attention-like nature of the retrieval module $R$, contrary to prior work, \model\ does not suffer from the quadratic complexity w.r.t. the number of candidates, because it computes attention only for the target object, but not for the context objects.
\end{enumerate}

\section{Limitations \& Practical considerations}
\label{A:sec:limitations}

The following limitations and practical considerations are applicable to retrieval-augmented models in general.
\model\ itself does not add anything new to this list.

\textbf{First}, for a given application, one should carefully evaluate from various perspectives (business logic, legal considerations, ethical aspects, etc.) whether using real training objects for making predictions is reasonable.

\textbf{Second}, depending on an application, for a given target object, one may want to retrieve only from a subset of the available data, where the subset is dynamically formed for the target object based on application-specific filters.
In terms of \autoref{sec:model-preliminaries}, it means $I_{cand} = I_{cand}(x) \subset I_{train}$.

\textbf{Third}, ideally, retrieval during training should simulate retrieval during deployment, otherwise, a retrieval-based model can lead to (highly) suboptimal performance.
Examples:

\begin{itemize}[nosep, leftmargin=2em]
    \item For time series, during training, \model\ must be allowed to retrieve only from the past.
    Moreover, perhaps, this ``past'' should also be limited to prevent the retrieval from too old data and too recent data.
    The decision should be made based on the domain expertise and business logic.

    \item Let's consider a task where, among all training objects, there are some ``related objects''.
    For example, when solving a ranking problem as a point-wise regression, such ``related objects'' can be obtained as query-document pairs corresponding to the same query, but different documents.
    In some cases, during training, for a given target object, retrieving from ``related objects'' can be unfair, because the same will not be possible in production for new objects that do not have ``related objects'' in the available data.
    Again, this design decision should be made based on the domain expertise and business logic.
\end{itemize}

\textbf{Lastly}, while \model\ is significantly more efficient than prior retrieval-based tabular DL models, the retrieval module $R$ still causes overhead compared to purely parametric models, so \model\ may not scale to truly large datasets as-is.
We showcase a simple trick to scale \model\ to larger datasets in \autoref{sec:context-freeze}.
We discuss the efficiency aspect in more detail in \autoref{A:sec:training-time}.

\section{Benchmarks}

\subsection{The default benchmark}
\label{A:sec:benchmark-ours}

In \autoref{A:tab:benchmark-ours}, we provide more information on the datasets from \autoref{tab:datasets}.
The datasets include:
\begin{itemize}[nosep,leftmargin=2em]
    \item Churn Modeling\footnote{https://www.kaggle.com/shrutimechlearn/churn-modelling}
    \item California Housing (real estate data, \citep{california})
    \item House 16H\footnote{https://www.openml.org/d/574}
    \item Adult (income estimation, \citep{adult})
    \item Diamond\footnote{https://www.openml.org/d/42225}
    \item Otto Group Product Classification\footnote{https://www.kaggle.com/c/otto-group-product-classification-challenge/data}
    \item Higgs (simulated physical particles, \citep{higgs}; we use the version with 98K samples available in the OpenML repository \citep{openml})
    \item Black Friday\footnote{https://www.openml.org/d/41540}
    \item Weather (temperature, \citep{malinin2021weather}). We take 10\% of the dataset for our experiments due to its large size.
    \item Weather (full) (temperature, \citep{malinin2021weather}). Original splits from the paper.
    \item Covertype (forest characteristics, \citep{covertype})
    \item Microsoft (search queries, \citep{microsoft}). We follow the pointwise approach to learning to rank and treat this ranking problem as a regression problem.
\end{itemize}

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Details on datasets from the main benchmark. ``\# Num'', ``\# Bin'', and ``\# Cat'' denote the number of numerical, binary, and categorical features, respectively. The ``Batch size'' is the default batch size used to train DL-based models.}
    \label{A:tab:benchmark-ours}
    \scalebox{0.9}{\input{data/A-tab-benchmark-ours}}
\end{table*}

\subsection{The benchmark from \citet{grinsztajn2022why}}
\label{A:sec:benchmark-why}

In this section, we describe how exactly we used the benchmark proposed in \citet{grinsztajn2022why}.
\begin{itemize}[nosep,leftmargin=2em]
    \item We use the same train-val-test splits.
    \item When there are several splits for one dataset (i.e., when the n-fold-cross-validation was performed in \citet{grinsztajn2022why}), we first treat each of them as separate datasets while tuning and evaluating algorithms as described in \autoref{A:sec:implementation-details}, but then, we average the metrics over the splits to obtain the final numbers for the dataset. For example, if there are five splits for a given dataset, then we tune and evaluate a given algorithm five times, each of the five tuned configurations is evaluated under 15 random seeds on the corresponding splits, and the reported metric value is the average over \mbox{$5 * 15 = 75$} runs.
    \item When there are \textit{multiple} versions of \textit{one} dataset (e.g., the original regression task and the same dataset but converted to the binary classification task or the same dataset, but with the categorical features removed, etc.), we keep only one \textit{original} dataset.
    \item We removed the ``Eye movements'' dataset because there is a leak in that dataset.
    \item We use the tuning and evaluation protocols as described in \autoref{A:sec:implementation-details}, which was also used in prior works on tabular DL \citep{gorishniy2021revisiting,gorishniy2022embeddings}. Crucially, we tune hyperparameters of the GBDT models more extensively than most (if not all) prior work in terms of both budget (20 warmup iterations of random sampling followed by 180 iterations of the tree-structured Parzen estimator algorithm) and hyperparameter spaces (see the corresponding sections in \autoref{A:sec:implementation-details}).
\end{itemize}

\section{Implementation details}
\label{A:sec:implementation-details}

\subsection{Hardware}
\label{A:sec:impl-harware}

We report the used hardware in the results published along with the source code.
In a nutshell, the vast majority of experiments on GPU were performed on one NVidia A100 GPU, the remaining small part of GPU experiments was performed on one Nvidia 2080 Ti GPU, and there was also a small portion of runs performed on CPU (e.g. all the experiments on LightGBM).

\subsection{Implementation details of \autoref{sec:context-freeze}}
\label{A:sec:impl-context-freeze}

In \autoref{sec:context-freeze}, we used \modelsimple\ with the default hyperparameters (see \autoref{A:sec:impl-model}).
To compute \mbox{$\Delta$-context}, we collect context distributions for training objects \textit{between training epochs}.
That is, after the $i$-th training epoch, we pause the training, collect the context distributions for all training objects, and then start the next $(i+1)$-th training epoch.

\textbf{$\Delta$-context.}
Intuitively, this heuristic metric describes in a single number how much, for a given input object, the context attention mass was updated compared to the \textit{previous} epoch.
Namely, it is a sum of two terms:
\begin{enumerate}[nosep,leftmargin=2em]
    \item the \texttt{novel} attention mass, i.e. the attention mass coming from the context objects presented on the current epoch, but not presented on the previous epoch
    \item the \texttt{increased} attention mass, i.e. we take the intersection of the current and the previous context objects and compute the increase of their total attention mass. We set it to 0.0 if actually decreased.
\end{enumerate}
Now, we formally define this metric.
For a given input object, let $a \in \R^{|I_{train}|}$ and $b \in \R^{|I_{train}|}$ denote the two distributions over the candidates from the previous and the current epochs, respectively.
Let denote the sets of non-zero entries as $A = \{i: a_i > 0\}$ and $B = \{i: a_i > 0\}$.
Note that $|A| = |B| = m = 96$.
In other words, $A$ and $B$ are the contexts from the two epochs.
Then:
\begin{align}
    \Delta\text{-context} &= \texttt{novel} + \texttt{increased} \\
    \texttt{novel} &= \sum_{i \in B \setminus A} b_i \\
    \texttt{increased} &=  \max \left( \sum_{i \in B \cap A} b_i - \sum_{i \in B \cap A} a_i, 0.0 \right)
\end{align}

\subsection{Implementation details of \autoref{sec:new-candidates}}
\label{A:sec:impl-new-candidates}

In \autoref{sec:new-candidates}, we used \modelsimple\ with the default hyperparameters (see \autoref{A:sec:impl-model}).

\subsection{Implementation details of \autoref{A:sec:similarity-module-patterns}}
\label{A:sec:impl-similarity-module}

In \autoref{A:sec:similarity-module-patterns}, we performed the analysis over exactly the same model checkpoints that we used to assemble the rows ``Step-1'' and ``Step-2'' in \autoref{tab:design}.

To reiterate, this is how the entropy in \autoref{A:tab:similarity-module} is computed:
\begin{enumerate}
    \item First, we obtain individual distributions over candidates for all test objects. One such distribution contains exactly ($m + 1$) non-zero entries.
    \item Then, we average all individual distributions and obtain the average distribution.
    \item \autoref{A:tab:similarity-module} reports the entropy of the average distribution.
\end{enumerate}

Note that, when obtaining the distribution over candidates, the top-$m$ operation is taken into account.
Without that, if the distribution is always uniform regardless of the input object, then the average distribution will also be uniform and with the highest possible entropy, which would be misleading in the context of the story in \autoref{A:sec:similarity-module-patterns}.

Lastly, recall that in the Step-1 and Step-2 models, an input object is added to its own context.
Then, the edge case when all input objects pay 100\% attention only to themselves would lead to the highest possible entropy, which would be misleading for the story in \autoref{A:sec:similarity-module-patterns}.
In other words, for the story in \autoref{A:sec:similarity-module-patterns}, we should treat the ``paying attention to self'' behavior similarly for all objects.
To achieve that, on the first step of the above recipe, we reassign the attention mass from ``self'' to a new virtual context object, which is \textit{the same} for all input objects.

\subsection{Implementation details of \autoref{A:sec:value-module}}
\label{A:sec:impl-value-module}

To build \autoref{A:tab:value-module}, we used \modelsimple\ with the default hyperparameters (see \autoref{A:sec:impl-model}).

\subsection{Experiment setup}
\label{A:sec:experiment-setup}

For the most part, we simply follow \citet{gorishniy2022embeddings}, but we provide all the details for completeness.
Note that some of the prior work may differ from the common protocol that we describe below, but we provide the algorithm-specific implementation details further in this section.

\textbf{Data preprocessing.}
For each dataset, for all DL-based solutions, the same preprocessing was used for fair comparison.
For numerical features, by default, we used the quantile normalization from the Scikit-learn package \citep{pedregosa2011scikit}, with rare exceptions when it turned out to be detrimental (for such datasets, we used the standard normalization or no normalization).
For categorical features, we used one-hot encoding.
Binary features (i.e. the ones that take only two distinct values) are mapped to $\{0,1\}$ without any further preprocessing.

\textbf{Training neural networks.}
For DL-based algorithms, we minimize cross-entropy for classification problems and mean squared error for regression problems.
We use the AdamW optimizer \citep{loshchilov2019decoupled}.
We do not apply learning rate schedules.
We do not use data augmentations.
For each dataset, we used a predefined dataset-specific batch size.
We continue training until there are $\texttt{patience} + 1$ consecutive epochs without improvements on the validation set; we set $\texttt{patience} = 16$ for the DL models.

\textbf{How we compare algorithms.}
For a given dataset, first, we define the ``preliminary best'' algorithm as the algorithm with the best mean score.
Then, we define a set of the best algorithms (i.e. their results are in bold in tables) as follows: a given algorithm is included in the best algorithms if its mean score differs from the mean score of the preliminary best algorithm by no more than the standard deviation of the preliminary best algorithm.

\subsection{Embeddings for numerical features}
\label{A:sec:embeddings}

\begin{minipage}{0.39\textwidth}
    \centering
    % Figure removed
    \captionof{figure}{(Copied from \citet{gorishniy2022embeddings}) The vanilla MLP. The model takes two numerical features as input.}
    \label{A:fig:mlp-without-embeddings}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}{0.51\textwidth}
    \vspace{-1em}
    \centering
    % Figure removed
    \captionof{figure}{(Copied from \citet{gorishniy2022embeddings}) The same MLP as in \autoref{A:fig:mlp-without-embeddings}, but now with embeddings for numerical features.}
    \label{A:fig:mlp-with-embeddings}
\end{minipage}

In this work, we actively used embeddings for numerical features from \citep{gorishniy2022embeddings} (see \autoref{A:fig:mlp-without-embeddings} and \autoref{A:fig:mlp-with-embeddings}), the technique which was reported to universally improve DL models.
In a nutshell, for a given scalar numerical feature, an embedding module is a trainable module that maps this scalar feature to a vector.
Then, the embeddings of all numerical features are concatenated into one flat vector which is passed to further layers.
Following the original paper, when we use embeddings for numerical features, the same embedding architecture is used for all numerical features.

In this work, we used the \texttt{LR} (the combination of a linear layer and ReLU) and \texttt{PLR} (the combination of periodic embeddings, a linear layer, and ReLU) embeddings from the original paper.
Also, we introduce the \texttt{PLR(lite)} embedding, a simplified version of the \texttt{PLR} embedding where the linear layer is shared across all features.
We observed it to be significantly more lightweight without critical performance loss.

\textbf{Hyperparameters tuning.}
For the \texttt{LR} embeddings, we tune the embedding dimension in $\mathrm{Uniform}[16,96]$.
For the \texttt{PLR} and \texttt{PLR(lite)} embeddings, we tune the number of frequencies in $\mathrm{Uniform}[16,96]$ (in $\mathrm{Uniform}[8,96]$ for \model\ on the datasets from \citet{grinsztajn2022why}), the frequency initialization scale in $\mathrm{LogUniform}[0.01,100.0]$ and the embedding dimension in $\mathrm{Uniform}[16,64]$  (in $\mathrm{Uniform}[4,64]$ for \model\ on the datasets from \citet{grinsztajn2022why}).

\subsection{\model}
\label{A:sec:impl-model}

\implnote

\textbf{Embeddings for numerical features.} (see \autoref{A:sec:embeddings})
For the non-simple configurations of \model, on datasets CH, CA, HO, AD, DI, OT, HI, BL, and on all the datasets from \citet{grinsztajn2022why}, we used the \texttt{PLR(lite)} embeddings as defined in \autoref{A:sec:embeddings}.
For other datasets, we used the \texttt{LR} embeddings.

\textbf{Other details.}
We observed that initializing the $W_Y$ module properly may be important for good performance.
Please, see the source code.

\textbf{Default \modelsimple.}
The default hyperparameters for \modelsimple\ were obtained at some point in the project by literally averaging the tuned hyperparameters over multiple datasets.
The specific set of datasets for averaging included all datasets from \autoref{A:tab:benchmark-ours} plus two datasets that used to be a part of the default benchmark, but were excluded later.
So, in total, 13 datasets contributed to the default hyperparameters.

Formally, this is not 100\% fair to evaluate the obtained default \modelsimple\ on the datasets which contributed to this default hyperparameters as in \autoref{tab:exp-gbdt-ours}.
However, we tested the fair leave-one-out approach as well (i.e. for a given dataset, averaging tuned hyperparameters over all datasets except for this one dataset) and did not observe any meaningful changes, so we decided to keep things simple and to have one common set of default hyperparameters for all datasets.
Plus, the obtained default \modelsimple\ demonstrates decent performance in \autoref{tab:exp-gbdt-why} as well, which illustrates that the obtained default configuration is not strongly ``overfitted'' to the datasets from \autoref{A:tab:benchmark-ours}.
The specific default hyperparameter values of \modelsimple\ are as follows:

\begin{itemize}[nosep,leftmargin=2em]
    \item $d = 265$
    \item Attention dropout rate $ = 0.38920071545944357$
    \item Dropout rate in \texttt{FFN} $= 0.38852797479169876$
    \item Learning rate $ = 0.0003121273641315169$
    \item Weight decay $ = 0.0000012260352006404615$
\end{itemize}

\textbf{Hyperparameters.}
The output size of the first linear layer of \texttt{FFN} and of $T$ is $2d$.
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.
The same protocol and hyperparameter spaces were used when tuning models in \autoref{tab:design} and \autoref{A:tab:ablation}.

\begin{table}[H]
\centering
\caption{
    The hyperparameter tuning space for \model.
    Here (A) = \{CH, CA, HO, AD, DI, OT, HI, BL\}, (B) = \{WE, CO, MI\}.
    For the datasets from \citet{grinsztajn2022why}, the tuning space is identical to (A) with the only difference that $d$ is tuned in $\mathrm{UniformInt}[16,384]$.
}
\label{tab:S-transformer-space}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter                     & (Datasets) Distribution & Comment \\
    \midrule
    Width $d$                     & (A,B) $\mathrm{UniformInt}[96,384]$ \\
    Attention dropout rate        & (A,B) $\mathrm{Uniform}[0.0,0.6]$ \\
    Dropout rate in \texttt{FFN}  & (A,B) $\mathrm{Uniform}[0.0, 0.6]$  \\
    Learning rate                 & (A,B) $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay                  & (A,B) $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]\}$ \\
    $N_E$                         & (A,B) $\mathrm{UniformInt}[0,1]$ & $\mathrm{Const}[0]$ for \modelsimple \\
    $N_P$                         & (A,B) $\mathrm{UniformInt}[1,2]$ & $\mathrm{Const}[1]$ for \modelsimple \\
    \midrule
    \# Tuning iterations                 & (A) 100 (B) 50 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{MLP}
\implnote

We used the implementation from \citet{gorishniy2022embeddings}.

\textbf{Hyperparameters.}
We use the same hidden dimension throughout the whole network.
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for MLP}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \# layers           & $\mathrm{UniformInt}[1,6]$ \\
    Width (hidden size) & $\mathrm{UniformInt}[64,1024]$ \\
    Dropout rate        & $\{0.0, \mathrm{Uniform}[0.0,0.5]\}$ \\
    Learning rate       & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]\}$ \\
    \midrule
    \# Tuning iterations & 100 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{FT-Transformer}
\implnote

We used the implementation from the "\texttt{rtdl}" Python package (version 0.0.13).

\textbf{Hyperparameters}.
We use the \mbox{\texttt{rtdl.FTTransformer.make\_baseline}} method to create \mbox{FT-Transformer}, so most of hyperparameters is inherited from this method's signature, and the rest is tuned as shown in the corresponding table.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for FT-Transformer}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \# blocks           & $\mathrm{UniformInt}[1,4]$ \\
    $d_{token}$         & $\mathrm{UniformInt}[16,384]$ \\
    Attention dropout rate        & $\mathrm{Uniform}[0.0,0.5]$ \\
    FFN hidden dimension expansion rate       & $\mathrm{Uniform}[\nicefrac{2}{3},\nicefrac{8}{3}]$ \\
    FFN dropout rate & $\mathrm{Uniform}[0.0,0.5]$ \\
    Residual dropout rate & $\{0.0, \mathrm{Uniform}[0.0,0.2] \}$ \\
    Learning rate       & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4]\}$ \\
    \midrule
    \# Tuning iterations & 100 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{kNN}

\implnote

The features are preprocessed in the same way as for DL models.
The only hyperparameter is the number of neighbors which we tune in $\mathrm{UniformInt}[1, 128]$.

\subsection{DNNR}

\implnote

We've used the official implementation \footnote{\url{https://github.com/younader/dnnr}}, but to evaluate DNNR on larger datasets with greater hyperparameters variability, we have rewritten parts of the source code to make it more efficient: enabling GPU usage, batched data processing, multiprocessing, where possible.
Crucially, we leave the underlying method unchanged.
We provide our efficiency-improved DNNR in the source code.
There is no support for classification problems, so we evaluate DNNR only on regression problems.

\textbf{Hyperparameters.} We performed a grid-search over the main DNNR hyperparameters on all datasets, falling back to defaults (suggested by the authors) due to scaling issues on WE and MI.

\begin{table}[H]
\centering
\caption{The hyperparameter grid used for DNNR. Here (A) = \{CA, HO\}; (B) = \{DI, BL, WE, MI\}. Notation: $N_f$ -- number of features for the dataset.}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter                     & (Datasets) Parameter grid & Comment \\
    \midrule
    \# neighbors $k$                 & (A,B) $[1,2,3, \ldots, 128]$ \\
    Learned scaling               & (A,B) [No scaling, Trained scaling] \\
    \# neighbors used in scaling & (A,B) [$8 \cdot N_f, 2, 3, 4, 8, 16, 32, 64, 128$] & $8 \cdot N_f$ on WE, MI \\
    \# epochs used in scaling & $10$ \\
    Cat. feature encoding & [one-hot, leave-one-out] \\
    \midrule
    \# neighbors for derivative $k'$        & (A) $\mathrm{LinSpace}[2\cdot N_f, 18 \cdot N_f, 20]$ \\
    & (B) $\mathrm{LinSpace}[2\cdot N_f, 12 \cdot N_f, 14]$ \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{DKL}

\implnote

We used DKL implementation from GPyTorch \citep{gardner2018gpytorch}. We do not evaluate DKL on WE and MI datasets due to scaling issues (tuning alone takes 1 day and 17 hours, compared to 3 hours for \model\ on the medium DI dataset, for example). There is no support for classification problems, thus we evaluate DKL only on regression problems.

\textbf{Hyperparameters.} As with MLP we use the same hidden dimension throughout the whole network. And perform tuning using the tree-structured Parzen Estimator algorithm from the Akiba et al. [1] library. 
\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for DKL}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    Kernel              & $\{\mathrm{rbf}, \mathrm{sm}\}$ \\
    \# layers           & $\mathrm{UniformInt}[1,4]$ \\
    Width (hidden size) & $\mathrm{UniformInt}[64,768]$ \\
    Dropout rate        & $\{0.0, \mathrm{Uniform}[0.0,0.5]\}$ \\
    Learning rate       & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}2]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]\}$ \\
    \midrule
    \# Tuning iterations & 100 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{ANP}

While the original paper introducing ANP did not focus on the tabular data, conceptually, it is very relevant to prior work on retrieval-based tabular DL, so we consider it as one of the baselines.

\implnote

We used the Pytorch implementation from an unofficial repository\footnote{\url{https://github.com/soobinseo/Attentive-Neural-Process}} and modified it with respect to the official implementation from \cite{kim2019attentive}. Specifically, we reimplemented \texttt{Decoder} class exactly as it was done in \cite{kim2019attentive} and changed a binary cross-entropy loss with a Gaussian negative log-likelihood loss in \texttt{LatentModel} class since it matches with the official implementation.

We do not evaluate ANP on the MI dataset due to scaling issues. Tuning alone on the smaller WE dataset took more than four days for 20(!) iterations (instead of 50-100 used for other algorithms). Also, there is no support for classification problems, thus we evaluate ANP only on regression problems.

We used 100 tuning iterations on CA and HO, 50 on DI, and 20 on BL and WE. 

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for ANP}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \# decoder layers           & $\mathrm{UniformInt}[1,3]$ \\
    \# cross-attention layers           & $\mathrm{UniformInt}[1,2]$ \\
    \# self-attention layers           & $\mathrm{UniformInt}[1,2]$ \\
    Width (hidden size) & $\mathrm{UniformInt}[64,384]$ \\
    Learning rate       & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4]\}$ \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{NPT}

We use the official NPT \citep{kossen2021self} implementation \footnote{\url{https://github.com/OATML/non-parametric-transformers}}.
We leave the model and training code unchanged and only adjust the datasets and their preprocessing according to our protocols.

We evaluate the \texttt{NPT-Base} configuration of the model and follow both \texttt{NPT-Base} architecture and optimization hyperparameters.
We train NPT for $2000$ epochs on CH, CA, AD, HO, $10000$ epochs on OT, WE, MI, $15000$ epochs on DI, BL, HI and $30000$ epochs on CO.
For all datasets that don't fit into the A100 80GB GPU, we use batch size $4096$ (as suggested in the NPT paper).
We also decrease the hidden dim to $32$ on WE and MI to avoid the OOM error.

Note that NPT is conceptually equivalent to other transformer-based non-parametric tabular DL solutions: \citep{somepalli2021saint, schafl2022hopular}.
All three methods use dot-product-based self-attention modules alternating between self-attention between object features and self-attention between objects (for the whole training dataset or its random subset).

\subsection{SAINT}
\implnote

We use the official implementation of SAINT \footnote{\url{https://github.com/somepago/saint}} \textbf{with one important fix}.
Recall that, in SAINT, a target object interacts with its context objects with intersample attention.
In the official implementation of SAINT, \textit{context objects are taken from the same dataset part, as a target object}: for training objects, context objects are taken from the training set, for validation objects -- from the validation set, for test objects -- from the test set.
This is different from the approach described in this paper, where \textit{context objects are always taken from the training set}.
Taking context objects from different dataset parts, as in the official implementation of SAINT, may be unwanted because of the following reasons:
\begin{enumerate}[nosep,leftmargin=2em]
    \item model can have suboptimal validation and test performance because it is trained to operate when context objects are taken from the training set, but evaluated when context objects are taken from other dataset parts.
    \item for a given validation/test object, \textit{the prediction depends on other validation/test objects}. This is not in line with other retrieval-based models, which may result in inconsistent comparisons. Also, in many real-world scenarios, during deployment/test time, input objects should be processed independently, which is not the case for the official implementation of SAINT.
\end{enumerate}
For the above reasons, we slightly modify SAINT such that each individual sample attends only to itself and to context samples from the training set, both during training and evaluation.
See the source code for details.

On small datasets (CH, CA, HO, AD, DI, OT, HI, BL) we fix the number of attention heads at $8$ and performed hyperparameter tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for SAINT}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    Depth  & $\mathrm{UniformInt}[1,4]$ \\
    Width & $\mathrm{UniformInt}[4,32,4]$ \\
    Feed forward multiplier & $\mathrm{Uniform}[\nicefrac{2}{3}, \nicefrac{8}{3}]$ \\
    Attention dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    Feed forward dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    Learning rate       & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4]\}$ \\
    \bottomrule
\end{tabular}}
\end{table}

On larger datasets (WE, CO, MI) we use slightly modified (for optimizing memory consumption) default configuration from the paper with following fixed hyperparameters:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{depth} = 4
    \item \texttt{n\_heads} = 8
    \item \texttt{dim} = 32
    \item \texttt{ffn\_mult} = 4
    \item \texttt{attn\_head\_dim} = 48
    \item \texttt{attn\_dropout} = 0.1
    \item \texttt{ff\_dropout} = 0.8
    \item \texttt{learning\_rate} = 0.0001
    \item \texttt{weight\_decay} = 0.01
\end{itemize}

\subsection{XGBoost}

\implnote

\implgbdt

The following hyperparameters are fixed and not tuned:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{booster} = ``gbtree''
    \item \texttt{n\_estimators} = 4000
    \item \texttt{tree\_method} = ``gpu\_hist''
    \item \texttt{early\_stopping\_rounds} = 200
\end{itemize}
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for XGBoost}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \texttt{colsample\_bytree}  & $\mathrm{Uniform}[0.5,1.0]$ \\
    \texttt{gamma}              & $\{0.0, \mathrm{LogUniform}[0.001,100.0]\}$ \\
    \texttt{lambda}             & $\{0.0, \mathrm{LogUniform}[0.1,10.0]\}$ \\
    \texttt{learning\_rate}     & $\mathrm{LogUniform}[0.001,1.0]$ \\
    \texttt{max\_depth}         & $\mathrm{UniformInt}[3,14]$ \\
    \texttt{min\_child\_weight} & $\mathrm{LogUniform}[0.0001,100.0]$ \\
    \texttt{subsample}          & $\mathrm{Uniform}[0.5,1.0$ \\
    \midrule
    \# Tuning iterations & 200 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{LightGBM}

\implnote

\implgbdt

The following hyperparameters are fixed and not tuned:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{n\_estimators} = 4000
    \item \texttt{early\_stopping\_rounds} = 200
\end{itemize}
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for LightGBM}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \texttt{feature\_fraction}  & $\mathrm{Uniform}[0.5,1.0]$ \\
    \texttt{lambda\_l2}             & $\{0.0, \mathrm{LogUniform}[0.1,10.0]\}$ \\
    \texttt{learning\_rate}     & $\mathrm{LogUniform}[0.001,1.0]$ \\
    \texttt{num\_leaves}         & $\mathrm{UniformInt}[4,768]$ \\
    \texttt{min\_sum\_hessian\_in\_leaf} & $\mathrm{LogUniform}[0.0001,100.0]$ \\
    \texttt{bagging\_fraction}          & $\mathrm{Uniform}[0.5,1.0]$ \\
    \midrule
    \# Tuning iterations & 200 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{CatBoost}

\implnote

\implgbdt

The following hyperparameters are fixed and not tuned:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{n\_estimators} = 4000
    \item \texttt{early\_stopping\_rounds} = 200
    \item \texttt{od\_pval} = 0.001
\end{itemize}
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for CatBoost}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \texttt{bagging\_temperature}  & $\mathrm{Uniform}[0.0,1.0]$ \\
    \texttt{depth}             & $\mathrm{UniformInt}[3,14]$ \\
    \texttt{l2\_leaf\_reg}             & $\mathrm{Uniform}[0.1,10.0]$ \\
    \texttt{leaf\_estimation\_iterations}             & $\mathrm{Uniform}[1,10]$ \\
    \texttt{learning\_rate}             & $\mathrm{LogUniform}[0.001,1.0]$ \\
    \midrule
    \# Tuning iterations & 200 \\
    \bottomrule
\end{tabular}}
\end{table}

\section{Extended results with standard deviations}
\label{A:sec:extended-results}

In this section, we provide the extended results with standard deviations for the main results reported in the main text.
The results for the default benchmark are in the \autoref{A:tab:extended-results-ours}.
The results for the benchmark from \citet{grinsztajn2022why} are in the \autoref{A:tab:extended-results-why}.

\newcommand{\topalign}[1]{%
\vtop{\vskip 0pt #1}}

\begin{longtable}{p{0.5\textwidth}p{0.5\textwidth}}
\caption{Extended results for the default benchmark. Results are grouped by datasets and span multiple pages below. Notation: \textdownarrow\ corresponds to RMSE, \textuparrow\ corresponds to accuracy.}\\
\label{A:tab:extended-results-ours}
\input{data/A-tab-extended-results-ours}
\end{longtable}

\begin{longtable}{p{0.5\textwidth}p{0.5\textwidth}}
\caption{Extended results for \citet{grinsztajn2022why} benchmark. Results are grouped by datasets and span multiple pages below. Notation: \textdownarrow\ corresponds to RMSE, \textuparrow\ corresponds to accuracy.}\\
\label{A:tab:extended-results-why}
\input{data/A-tab-extended-results-why}
\end{longtable}

\end{document}
