\documentclass{article}

% \usepackage{neurips_2023}
\usepackage[preprint,nonatbib]{neurips_2023}
\usepackage[square,sort,comma,numbers]{natbib}

% >>> Suggested packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\definecolor{blueish}{rgb}{0.0, 0.3, .6}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=blueish,linkcolor=blueish,urlcolor=blueish,bookmarks=false]{hyperref}

\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{nicefrac}
\usepackage{lipsum}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{fontawesome5}
\usepackage{pgf}
\usepackage[binary-units]{siunitx}

% >>> Other packages.
\usepackage{amsmath}
\usepackage{anyfontsize}
\usepackage{caption}
\usepackage{color}
\usepackage{enumitem}
\usepackage{float}
\usepackage{nicefrac}
\usepackage{rotating}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{wrapfig}
\graphicspath{ {./data/} }

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{lib}

% >>> Paper.
\title{\model: Unlocking the Power of \\Retrieval-Augmented Tabular Deep Learning}

\author{%
    Yury Gorishniy \\
    Yandex
    \And
    Ivan Rubachev \\
    HSE, Yandex
    \And
    Nikolay Kartashev \\
    HSE, Yandex
    \AND
    Daniil Shlenskii \\
    Yandex
    \And
    Akim Kotelnikov \\
    HSE, Yandex
    \And
    Artem Babenko \\
    Yandex, HSE
}

\begin{document}

\maketitle

\begin{abstract}
Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution.
Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed.
For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction.
However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines.
Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.

In this work, we give a strong positive answer to this question.
We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models.
Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work.
As a result, we design \model\ -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (\autoref{fig:timeline}).
The source code is available at \url{\repository}.
\end{abstract}

% Figure environment removed

\newpage
\section{Introduction}
\label{sec:introduction}

Machine learning (ML) problems on tabular data, where objects are described by a set of heterogeneous features, are ubiquitous in industrial applications in medicine, finance, manufacturing, and other fields.
Historically, for these tasks, the models based on gradient-boosted decision trees (GBDT) have been a go-to solution for a long time.
However, lately, tabular deep learning (DL) models have been receiving increasingly more attention, and the tabular DL models are becoming more competitive \citep{klambauer2017self, popov2020neural, wang2020dcn2, hazimeh2020tree, huang2020tabtransformer, gorishniy2021revisiting, somepalli2021saint, kossen2021self, gorishniy2022embeddings}.

In particular, several attempts to design a retrieval-augmented tabular DL model have been recently made \citep{somepalli2021saint,qin2021retrieval,kossen2021self}.
In these models, for a given input object, one first retrieves relevant data points, such as the object's nearest neighbors, from a database, and then a machine learning model processes the retrieved objects together with the target object to produce a better prediction for this target object.
In fact, the retrieval technique is widely popular in other mainstream domains, including natural language processing \citep{das2021case, wang2022training, izacard2022few}, computer vision \citep{jia2021rethinking, iscen2022memory, long2022retrieval}, CTR prediction \citep{qin2020user,qin2021retrieval,kounianhua2022learning}, protein structure prediction \citep{cramer2021alphafold2} and others.
Compared to purely parametric (i.e. retrieval-free) models, the retrieval-based ones can achieve higher performance and also exhibit several practically important properties, such as an ability for incremental learning and better robustness \citep{das2021case, jia2021rethinking}.

While multiple retrieval-augmented models applicable to (or directly designed for) tabular data problems exist, in our experiments, we show that they provide only minor benefits at best over the properly tuned multilayer perceptron (MLP) -- the simplest parametric model.
This is a critical observation, since retrieval components are typically associated with a noticeable overhead, and marginal metric improvements may not be enough to justify the extra costs.
Nevertheless, in this work, we show that it is possible to design a retrieval-augmented model for tabular data problems that (1) provides strong average performance with significant improvements over parametric models at least on some datasets (2) while being a mainstream accessible model, that scales to datasets with at least up to several million data points.
We summarize our main contributions as follows:
\begin{enumerate}[nosep, leftmargin=2em]
    \item We revisit the existing retrieval-augmented solutions for tabular data problems and reveal that they provide only minor benefits over a tuned MLP while introducing significant overhead.
    \item We design \model\ -- a simple retrieval-augmented tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among DL models and achieves the new state-of-the-art on several datasets.
    \item In particular, \model\ outperforms GBDT on the recently proposed benchmark with middle-scale tasks \citep{grinsztajn2022why}, which was originally used to illustrate the superiority of decision-tree-based models over DL models. Tree-based models, in turn, remain a more efficient solution.
    \item We highlight the important degrees of freedom of the attention mechanism (the often used block in retrieval-based models) that allow designing better retrieval-based tabular models.
\end{enumerate}

\section{Related work}

\textbf{Gradient boosted decision trees (GBDT)}.
ML models based on gradient-boosted decision trees (GBDT) are non-DL solutions for supervised problems on tabular data, which are popular within the community due to their strong performance and high efficiency.
By employing the modern DL building blocks and, in particular, the retrieval technique, our new model successfully competes with GBDT and, in particular, demonstrates that DL models can be superior on non-big data by outperforming GBDT on the recently proposed benchmark with small-to-middle scale tasks \citep{grinsztajn2022why}.

\textbf{Parametric deep learning models.}
Parametric tabular DL is a rapidly developing research direction aimed at bringing the benefits of deep learning to the world of tabular data while achieving competitive performance \citep{klambauer2017self, popov2020neural, wang2020dcn2, hazimeh2020tree, huang2020tabtransformer, gorishniy2021revisiting, gorishniy2022embeddings}.
The recent studies reveal that MLP-like backbones are still competitive \citep{gorishniy2021revisiting,kadra2021well,gorishniy2022embeddings}, and that embeddings for numerical features \citep{gorishniy2022embeddings} significantly reduce the gap between tabular DL and GBDT.
In this work, we show that a properly designed retrieval component can boost the performance of tabular DL even further.

\textbf{Retrieval-augmented models in general.}
Usually, the retrieval-based models are designed as follows.
For a given input object, first, they retrieve relevant samples from available (training) data.
Then, they process the input object augmented with features (and potentially labels) of the retrieved instances to produce the final prediction.
In some retrieval-based models, the retrieval step is omitted and all training data points serve as the ``retrieved'' instances \citep{somepalli2021saint,kossen2021self,schafl2022hopular}.
One of the common motivations for designing retrieval-based schemes is the local learning paradigm \citep{bottou1992local}, and the simplest possible example of such a model is the $k$ nearest neighbors (kNN) algorithm \citep{james2013introduction}.
The promise of retrieval-based approaches was demonstrated across various domains, ranging from natural language processing \citep{lewis2020retrieval,guu2020retrieval,khandelwal2020generalization,das2021case,wang2022training,izacard2022few,borgeaud2022improving}, computer vision \citep{iscen2022memory, long2022retrieval}, CTR prediction \citep{qin2020user,qin2021retrieval,kounianhua2022learning}, reinforcement learning \citep{ritter2020rapid}, protein structure prediction \citep{cramer2021alphafold2}, point cloud processing \citep{zhang2023nearest,zhang2023parameter}, and others.
In addition to better performance, the retrieval-augmented models often have useful properties such as better interpretability \citep{wang2023flexible} and robustness \citep{zhao2018retrieval}.

\textbf{Retrieval-augmented models for tabular data problems.}
The classic examples of non-deep retrieval-based tabular models are the ``shallow'' neighbor-based and kernel methods \citep{james2013introduction, nader2022dnnr}.
There are also deep retrieval-based models applicable to (or directly designed for) tabular data problems \citep{wilson2016deep,kim2019attentive,ramsauer2021hopfield,kossen2021self,somepalli2021saint}.
However, we show that they are only marginally better than simple parametric DL models, and that often comes with a cost of using heavy Transformer-like architectures.
Compared to prior work, where several layers with multi-head attention between objects and features are often used \citep{ramsauer2021hopfield,kossen2021self,somepalli2021saint}, our model \model\ implements its retrieval component with just \textit{one} single-head attention-like module.
However, the single attention-like module of \model\ is customized in a way that makes it better suited for tabular data problems.
As a result, \model\ substantially outperforms the existing retrieval-based DL models while being significantly more efficient.
In particular, \model\ avoids the infamous quadratic complexity of the attention mechanism.

\section{\model}
\label{sec:model}

In this section, we design a new retrieval-augmented deep learning model for tabular data problems.

\subsection{Preliminaries}
\label{sec:model-preliminaries}

\textbf{Notation.}
For a given supervised learning problem on tabular data, we denote the dataset as $\left\{\left( x_i,y_i \right)\right\}_{i=1}^n$ where $x_i \in \X$ represents the $i$-th object's features and $y_i \in \Y$ represents the $i$-th  object's label.
Depending on the context, the $i$ index can be omitted.
We consider three types of tasks: binary classification $\Y = \{0, 1\}$,
multiclass classification $\Y = \{1, . . . , C\}$ and regression $\Y = \R$.
For simplicity, in most places, we will assume that $x_i$ contains only numerical (i.e., continuous) features, and we will give additional comments on binary and categorical features when necessary.
The dataset is split into three disjoint parts: $\overline{1,n} = I_{train} \cup I_{val} \cup I_{test}$, where the ``train'' part is used for training, the ``validation'' part is used for early stopping and hyperparameter tuning, and the ``test'' part is used for the final evaluation.
An input object for which a given model makes a prediction is referred to as ``input object'' or ``target object''.

When the retrieval technique is used for a given target object, the retrieval is performed within the set of ``context candidates'' or simply ``candidates'': $I_{cand} \subseteq I_{train}$.
The retrieved objects, in turn, are called ``context objects'' or simply ``context''.
Optionally, the target object can be included in its own context.
In this work, we use the same set of candidates for all input objects.

\textbf{Experiment setup.}
We extensively describe our tuning and evaluation protocols in \autoref{A:sec:experiment-setup}.
The most important points are that, for any given algorithm, on each dataset, following \citet{gorishniy2022embeddings}, (1) we perform hyperparameter tuning and early stopping using the \textit{validation} set; (2) for the best hyperparameters, in the main text, we report the metric on the \textit{test} set averaged over 15 random seeds, and provide standard deviations in \autoref{A:sec:extended-results}; (3) when comparing any two algorithms, we take the standard deviations into account as described in \autoref{A:sec:experiment-setup}; (4) to obtain ensembles of models of the same type, we split the 15 random seeds into three disjoint groups (i.e., into three ensembles) each consisting of five models, average predictions within each group, and report the average performance of the obtained three ensembles.

\textbf{Datasets.} In this work, we mostly use the datasets from prior literature and provide their summary in \autoref{tab:datasets} (sometimes, we refer to this set of datasets as ``our benchmark'').
Additionally, in \autoref{sec:main}, we use the recently introduced benchmark with middle-scale tasks ($\le 50K$ objects) \citep{grinsztajn2022why} where GBDT was reported to be superior to DL solutions.
\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Dataset properties. ``RMSE'' denotes root-mean-square error, ``Acc.'' denotes accuracy.}
    \label{tab:datasets}
    \scalebox{0.95}{\input{data/tab-datasest}}
\end{table*}

\newpage
\subsection{Overview of the architecture}
\label{sec:architecture}

Since there are no established architectural blocks for building deep retrieval-based models for tabular data problems, we have to make a subjective choice of the architecture.
We choose to keep things simple and to incrementally integrate the retrieval functionality in a conventional feed-forward network as illustrated in \autoref{fig:architecture-overview}.
In a nutshell, a target object and its context candidates are first passed through an encoder $E: \X \rightarrow \R^d$, then the retrieval component $R$ does its job in the residual branch and enriches the target object's representation, and finally, the predictor $P: \R^d \rightarrow \hat{\Y}$ makes the prediction.
The modules $E$, $P$, and $R$ are defined later in the text.
Unless otherwise noted, we use the whole training set as the fixed set of candidates for all objects (i.e., $I_{cand} = I_{train}$).

% Figure environment removed

\textbf{Encoder and predictor}.
The encoder $E$ and predictor $P$ modules (\autoref{fig:architecture-overview}) are not the focus of this work, so we keep them simple as illustrated in \autoref{fig:architecture-encoder-predictor}.

% Figure environment removed

% Figure environment removed

% \textbf{Retrieval module}. In \autoref{fig:architecture-retrieval}, we provide a more detailed illustration for the retrieval module $R$ introduced in \autoref{fig:architecture-overview}, which operates over the representation $\tilde{x} \in \R^d$ of the target object and the representations $\{\tilde{x}_i\}_{i \in I_{cand}} \subset \R^d$ and the labels $\{y_i\}_{i \in I_{cand}} \subset \Y$ of the candidates.
\textbf{Retrieval module}. In \autoref{fig:architecture-retrieval}, we provide a \textit{template} of the retrieval module $R$ introduced in \autoref{fig:architecture-overview}, which operates over the representation $\tilde{x} \in \R^d$ of the target object and the representations $\{\tilde{x}_i\}_{i \in I_{cand}} \subset \R^d$ and the labels $\{y_i\}_{i \in I_{cand}} \subset \Y$ of the candidates.
The specific instances of this template will be explored in \autoref{sec:retrieval}.
% \newpage
% Formally, the retrieval module $R$ performs the following steps (the notation follows all the above text and illustrations):
% The retrieval module $R$ (\autoref{fig:architecture-retrieval}) can be seen as a generalized version of the attention mechanism.
Overall, the retrieval module $R$ can be seen as a generalized version of the attention mechanism, which is computed only for the target object.
Formally, it performs the following steps (the notation follows all the above text and illustrations):
\begin{enumerate}[nosep, leftmargin=2em]
    \item If the encoder $E$ contains at least one \texttt{Block} (i.e. $N_E > 0$), then $\tilde{x}$ and all $\tilde{x}_i$ are first normalized with a shared layer normalization \citep{ba2016layer} (this is omitted in \autoref{fig:architecture-retrieval} for clarity).
    \item The context objects are defined as the $m$ candidates most similar to the target object according to the similarity module $\mathcal{S}: (\R^d, \R^d) \rightarrow \R$.
    Optionally, the target object itself is added to the set of context objects with the similarity score $\mathcal{S}(\tilde{x}, \tilde{x})$ (this is omitted in \autoref{fig:architecture-retrieval} for clarity).
    \item The weights of the context objects are defined as the output of the \texttt{softmax} function over the similarities with the context objects obtained in the previous steps.
    Then, dropout is applied to the obtained weights (this is omitted in \autoref{fig:architecture-retrieval} for clarity).
    \item The values of the context objects are defined as the outputs of the value module \mbox{$\mathcal{V}: (\R^d, \R^d, \Y) \rightarrow \R^d$}.
    \item Output the weighted aggregation using the values and the weights obtained in the previous steps.
\end{enumerate}
From now on, we set the context size to a relatively large value $m=96$ and allow the softmax function to select the ``effective'' context size automatically.
In the next section, we explore different implementations of the $\mathcal{S}$ and $\mathcal{V}$ modules and arrive at the final model.

\subsection{Implementing the retrieval module $R$}
\label{sec:retrieval}

After the high-level introduction of the architecture in \autoref{sec:architecture}, in this section, we explore different implementations of the retrieval module $R$ (\autoref{fig:architecture-retrieval})), and in particular of the similarity module $\mathcal{S}$ and the value module $\mathcal{V}$.
During this exploration phase, we do not use embeddings for numerical features \citep{gorishniy2022embeddings} in the \mbox{\texttt{Input Module}} of the encoder $E$ and fix $N_E$ = 0, $N_P$ = 1 (see \autoref{fig:architecture-encoder-predictor}).
We obtain the final model in several steps, which we describe in detail in the following paragraphs.

\textbf{Step-0. Evaluating the similarity and value modules of the vanilla attention}.
In fact, modulo the top-$m$ operation (see \autoref{fig:architecture-retrieval}), the retrieval module $R$ can be instantiated as the vanilla self-attention \citep{vaswani2017attention} as follows:
\begin{align}
\label{eq:step-0}
\begin{split}
    \mathcal{S}(\tilde{x}, \tilde{x}_i) = W_Q (\tilde{x})^T W_K (\tilde{x_i}) \cdot d^{-\nicefrac{1}{2}} \qquad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_V (\tilde{x}_i)
\end{split}
\end{align}
where $W_Q$, $W_K$, and $W_V$ are linear layers, and the target object is added as the $(m + 1)$-th object to its own context (i.e., ignoring the top-$m$ operation).
We follow the original terminology and call the results of $W_Q$ and $W_K$ queries and keys, respectively.
Given the frequent usage of the vanilla attention mechanism in prior work on retrieval-based tabular DL, we see the above configuration as a reasonable baseline implementation of $R$.
As reported in \autoref{tab:design}, the Step-0 configuration performs similarly to MLP, which clearly does not justify the usage of the retrieval component at this point.

\textbf{Step-1. Adding context labels}.
A natural attempt to improve the Step-0 configuration is to utilize labels of the context objects, for example, by incorporating them into the value module as follows:
\begin{align}
\label{eq:step-1}
\begin{split}
    \mathcal{S}(\tilde{x}, \tilde{x}_i) = W_Q (\tilde{x})^T W_K (\tilde{x_i}) \cdot d^{-\nicefrac{1}{2}} \qquad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = \underline{W_Y (y_i) +} W_V (\tilde{x}_i)
\end{split}
\end{align}
where the difference with \autoref{eq:step-0} is the \underline{underlined} addition of $W_Y: \Y \rightarrow \R^d$, which is an embedding table for classification tasks and a linear layer for regression tasks.
For the target object, a trainable placeholder vector from $\R^d$ is used instead of the label embedding $W_Y (y)$, since $y$ is not available.
\autoref{tab:design} shows no improvements from using context labels, which is counter-intuitive to say the least.
This makes us suspect that the similarity module $\mathcal{S}$ of the vanilla attention is the bottleneck which does not allow benefiting from such a valuable signal as labels.

\textbf{Step-2. Improving the similarity module $\mathcal{S}$}.
In terms of the local learning paradigm \citep{bottou1992local}, the similarity module $\mathcal{S}$ (\autoref{fig:architecture-retrieval}) defines the notion of locality.
Currently, however, the $\mathcal{S}$ module taken from the vanilla attention is rather an arbitrary choice inherited from other fields where the attention mechanism is used for completely different purposes.
First, the motivation for using different representations (query and key) for the target and candidate objects is unclear.
Second, in this section, the encoder $E$ is shallow (a single linear layer, in fact), so a similarity measure reasonable in the original features space may remain reasonable after applying the encoder as-well.
These two observations motivate us to remove the notion of queries and replace the dot product with the $L_2$ distance since the latter one is a somewhat reasonable similarity measure in the original feature space of a typical tabular data problem.
The obtained ``Step-2'' configuration is defined as follows:
\begin{align}
\label{eq:step-2}
\begin{split}
    \mathcal{S}(\tilde{x}, \tilde{x}_i) = \underline{-\| W_K(\tilde{x}) -  W_K(\tilde{x}_i) \|^2} \cdot d^{-\nicefrac{1}{2}} \qquad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_Y (y_i) + W_V (\tilde{x}_i)
\end{split}
\end{align}
where the difference with \autoref{eq:step-1} is \underline{underlined}.
The results in \autoref{tab:design} show that this is a turning point in our story, which was overlooked in prior work, and the retrieval-based model makes a significant jump in performance on several datasets.
Crucially, in \autoref{A:sec:ablation}, we show that removing any of the three ingredients (context labels, key-only representation, $L_2$ distance) results in a performance drop back to the level of MLP.
So we hypothesize that both things are important: how valuable the additional signal is (Step-1) and how well we measure the distance from the target object to the source of that valuable signal (Step-2).
We provide further analysis on the new similarity module in \autoref{sec:similarity-module}.
Note that the $L_2$ distance as such is unlikely to be the universally best choice for all domains and problems (even within the tabular domain), but it seems to be a reasonable default choice for tabular data problems.

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The performance of the implementations of the retrieval module $R$, described in \autoref{sec:retrieval}.
        If a number is underlined, then it is better than the corresponding number from the previous step, at least by the standard deviation.
        Noticeable improvements over MLP start at Step-2.
        Notation: \textdownarrow\ corresponds to RMSE, \textuparrow\ corresponds to accuracy.
    }
    \label{tab:design}
    \scalebox{0.9}{\input{data/tab-design}}
\end{table*}

\textbf{Step-3. Improving the value module $\mathcal{V}$}.
Now, we take inspiration from DNNR \citep{nader2022dnnr} -- the recently proposed generalization of the kNN algorithm for regression problems.
Conceptually, while kNN captures only local label distributions, DNNR also captures local trends (formally, derivatives).
Technically, contrary to kNN, in DNNR, a neighbor contributes to the prediction not only its label but also an additional correction term that depends on the difference between the target object and the neighbor in the original feature space.
Inspired by DNNR, we make the value module $\mathcal{V}$ more expressive by taking the target object's representation $\tilde{x}$ into account as follows:
\begin{align}
\label{eq:step-3}
\begin{split}
    & \mathcal{S}(\tilde{x}, \tilde{x}_i) = -\| W_K(\tilde{x}) -  W_K(\tilde{x}_i) \|^2 \cdot d^{-\nicefrac{1}{2}}\ \ \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_Y (y_i) + \underline{T(W_K(\tilde{x}) - W_K(\tilde{x}_i))} \\
    & T(\cdot) = \texttt{LinearWithoutBias}(\texttt{Dropout}(\texttt{ReLU}(\texttt{Linear}(\cdot))))
\end{split}
\end{align}
where the difference with \autoref{eq:step-2} is \underline{underlined}.
\autoref{tab:design} shows that the new value module further improves the performance on several datasets.
Intuitively, the term $W_Y(y_i)$ (the embedding of the context object's label) can be seen as the ``raw'' contribution of the $i$-th context object.
The term $T(W_K(\tilde{x}) - W_K(\tilde{x}_i))$ can be seen as the ``correction'' term, where the module $T$ translates the differences in the key space into the differences in the label embedding space.
We provide further analysis on the new value module in \autoref{sec:value-module}.

\textbf{Step-4. \model}.
Finally, empirically, we observed that omitting the scaling term $d^{-\nicefrac{1}{2}}$ in the similarity module and not including the target object to its own context (i.e., using cross-attention) leads to better results on average as reported in \autoref{tab:design}.
Both aspects can be considered hyperparameters, and the above notes can be seen as our default recommendations.
We call the obtained model ``\model'' (Tab $\sim$ tabular, R $\sim$ retrieval).
The formal complete description of how \model\ implements the retrieval module $R$ is as follows:
\begin{align}
\label{eq:model}
\begin{split}
    k = W_K(\tilde{x}),\ k_i = W_K(\tilde{x}_i) \quad \mathcal{S}(\tilde{x}, \tilde{x}_i) = -\| k - k_i \|^2 \quad \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i) = W_Y (y_i) + T(k - k_i)
\end{split}
\end{align}
Where the notation follows \autoref{sec:architecture}, $W_K$ is a linear layer, $W_Y$ is an embedding table for classification tasks and a linear layer for regression tasks, (by default) cross-attention is used, (by default) the similarity scores are not scaled, and $T(\cdot) = \texttt{LinearWithoutBias}(\texttt{Dropout}(\texttt{ReLU}(\texttt{Linear}(\cdot))))$.

\subsection{Technical notes}

We highlight the following technical aspects of \model:
\begin{enumerate}[nosep,leftmargin=2em]
    \item Because of the changes introduced in the Step-3 configuration, the value representations $\mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)$ of the candidates cannot be precomputed for a trained model, since they depend on the target object.
    This implies roughly twice less memory usage when deploying the model to production (since only the key representations and labels have to be deployed for training objects), but $\mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)$ has to be computed in runtime.
    \item Despite the attention-like nature of the retrieval module $R$, contrary to prior work, \model\ does not suffer from the quadratic complexity, because it computes attention only for the target object, but not for the context objects.
\end{enumerate}

\subsection{Limitations}

We highlight two main limitations of \model.
First, as for all retrieval-augmented models, one should evaluate whether the usage of real training objects for making predictions is reasonable and allowed from the application perspective (for example, privacy and ethical aspects must be considered).
Second, specifically for \model, we note that the retrieval component $R$, while being significantly more efficient than the analogous elements in prior work, still causes noticeable overhead compared to fully parametric models, and may not scale to truly large datasets as-is.
We showcase a simple trick to scale \model\ to larger datasets in \autoref{sec:context-freeze}.
We discuss the efficiency aspect in more detail in \autoref{A:sec:training-time}.

\section{Experiments on public benchmarks}
\label{sec:experiments}

In this section, we compare \model\ (introduced in \autoref{sec:model}) with existing retrieval-augmented solutions and state-of-the-art parametric models.
In addition to the fully-fledged configuration of \model\ (with all degrees of freedom available for $E$ and $P$ as described in \autoref{fig:architecture-encoder-predictor}), we also use \textbf{\modelsimple} (``S'' stands for ``simple'') -- a simple configuration, which does not use feature embeddings \citep{gorishniy2022embeddings}, has a linear encoder ($N_E = 0$) and a one-block predictor ($N_P = 1$) (in fact, in \autoref{sec:model}, it was TabR-S all the time).
We specify when \modelsimple\ is used only in tables, figures, and captions but not in the text.
For other details on \model, including hyperparameter tuning, see \autoref{A:sec:impl-model}.

\subsection{Evaluating retrieval-augmented deep learning models for tabular data}
\label{sec:revisiting}

In this section, we compare \model\ (\autoref{sec:model}) and the existing retrieval-augmented solutions with fully parametric DL models (see \autoref{A:sec:implementation-details} for implementation details for all algorithms). \autoref{tab:exp-revisiting} indicates that \model\ is the only retrieval-based model that provides a significant performance boost over MLP on many datasets.
In particular, the full variation of \model\ outperforms MLP-PLR (the modern parametric DL model with the highest average rank from \citet{gorishniy2022embeddings}) on several datasets (CA, OT, BL, WE, CO), and performs on par with it on the rest except for the MI dataset.
Regarding the prior retrieval-based solutions, we faced various technical limitations, such as incompatibility with classification problems and scaling issues (e.g., as we show in \autoref{A:sec:training-time}, it takes dramatically less time to train \model\ than NPT \citep{kossen2021self} -- the closest retrieval-based competitor from \autoref{tab:exp-revisiting}).
Note that the retrieval component is not universally beneficial for all datasets; however, even in those cases, \model\ remains competitive with the parametric solutions.

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Comparing \model\ with existing retrieval-augmented tabular models and parametric DL models. The notation follows \autoref{tab:design}. The bold entries are the best-performing algorithms, which are defined with standard deviations taken into account as described in \autoref{A:sec:experiment-setup}.
    }
    \label{tab:exp-revisiting}
    \scalebox{0.85}{\input{data/tab-exp-revisiting}}
\end{table*}

The obtained results highlight the retrieval technique and embeddings for numerical features \citep{gorishniy2022embeddings} (used in MLP-PLR and \model) as two powerful architectural elements that improve the optimization properties of tabular DL models.
Interestingly, the two techniques are not fully orthogonal, but none of them can recover the full power of the other, and it depends on a given dataset whether one should prefer the retrieval, the embeddings, or a combination of both.

\textbf{The main takeaway.}
\model\ becomes a new strong deep learning solution for tabular data problems and demonstrates a good potential of the retrieval-based approach.
\model\ demonstrates strong average performance and achieves the new state-of-the-art on several datasets.

\subsection{Comparing \model\ with gradient-boosted decision trees}
\label{sec:main}

\begin{table*}[b]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Comparing ensembles of \model\ with ensembles of GBDT models.
        See \autoref{A:sec:impl-model} to learn how the ``default'' \modelsimple\ was obtained.
        The notation follows \autoref{tab:exp-revisiting}.
    }
    \label{tab:exp-gbdt-ours}
    \scalebox{0.85}{\input{data/tab-exp-gbdt-ours}}
\end{table*}

In this section, we compare \model\ with models based on gradient-boosted decision trees (GBDT): XGBoost \citep{chen2016xgboost}, LightGBM \citep{ke2017lightgbm} and CatBoost \citep{prokhorenkova2018catboost}.
Specifically, we compare ensembles of \model\ with ensembles of GBDT (e.g., an ensemble of MLPs vs. an ensemble of XGBoosts) for a fair comparison since gradient boosting is already an ensembling technique.

\textbf{Our benchmark}.
\autoref{tab:exp-gbdt-ours} shows that, on our benchmark, tuned \model\ provides noticeable improvements over tuned GBDT on several datasets (CH, CA, HO, HI, WE, CO), while being competitive on the rest, except for the MI dataset.
The table also demonstrates that \model\ has a competitive default configuration (defined in \autoref{A:sec:impl-model}).

\textbf{The benchmark from \citet{grinsztajn2022why}}.
Now, we go further and use the benchmark with middle-scale tasks, which has been recently introduced in \citet{grinsztajn2022why}.
Importantly, this benchmark was originally used to illustrate the superiority of GBDT over parametric DL models on datasets with $\leq 50K$ objects, which makes it an interesting challenge for \model.
We adjust the benchmark to our tuning and evaluation protocols (see \autoref{A:sec:benchmark-why} for details) and report the results in \autoref{tab:exp-gbdt-why}.
\\While MLP-PLR (one of the best parametric DL models) indeed is slightly inferior to GBDT on this set of tasks, \model\ makes a significant step forward and outperforms GBDT on average.

In the appendix, we provide more analysis: in \autoref{A:sec:xgboost-with-neighbors}, we try augmenting XGBoost with a retrieval component; in \autoref{A:sec:training-time}, we compare training times of \model\ and GBDT models.

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Comparing ensembles of DL models with ensembles of GBDT models on the benchmark from \citet{grinsztajn2022why} (e.g., an ensemble of MLPs vs ensemble of XGBoosts; note that in \autoref{fig:timeline}, we compare \textit{single} models, hence the different numbers).
        See \autoref{A:sec:impl-model} for the details on the ``default'' \modelsimple.
        The default configuration of \modelsimple\ is compared against the default configurations of GBDT models.
        The comparison is performed in a pairwise manner with standard deviations taken into account as described in \autoref{A:sec:experiment-setup}.
    }
    \label{tab:exp-gbdt-why}
    \scalebox{0.95}{\input{data/tab-exp-gbdt-why}}
\end{table*}

\textbf{The main takeaway}. After the comparison with GBDT, \model\ confirms its status of a new strong solution for tabular data problems: it provides strong average performance and can provide a noticeable improvement over GBDT on some datasets.

\section{Analysis}

\subsection{Freezing contexts for faster training of \model}
\label{sec:context-freeze}

In the vanilla formulation of \model\ (\autoref{sec:model}), for each training batch, the most up-to-date contexts are mined by encoding all the candidates and computing similarities with all of them, which can be prohibitively slow on large datasets.
For example, it takes more than 18 hours to train a single \model\ on the full ``Weather prediction'' dataset  \citep{malinin2021weather} (3M+ objects; with the default hyperparameters from \autoref{tab:exp-gbdt-ours}).
However, as we show in \autoref{fig:context-freeze}, for an average training object, its context (i.e. the top-$m$ candidates and the distribution over them according to the similarity module $\mathcal{S}$) gradually ``stabilizes'' during the course of training, which gives an opportunity for simple optimization.
Namely, after a fixed number of epochs, we can perform ``context freeze'': i.e., compute the up-to-date contexts for all training (but not validation and test) objects for the one last time and then reuse these contexts for the rest of the training.
\autoref{tab:context-freeze} indicates that, on some datasets, this simple technique allows accelerating training of \model\ without much loss in metrics, with more noticeable speedups on larger datasets.
In particular, on the full ``Weather prediction'' dataset, we achieve nearly sevenfold speedup (from 18h9min to 3h15min) while maintaining competitive RMSE.
See \autoref{A:sec:impl-context-freeze} for implementation details.

\subsection{Updating \model\ with new training data without retraining (preliminary exploration)}
\label{sec:new-candidates}

Getting access to new unseen training data \textit{after} training a machine learning model (e.g., after collecting yet another portion of daily logs of an application) is a common practical scenario.
Technically, \model\ allows utilizing the new data \textit{without retraining} by adding the new data to the set of candidates for retrieval.
We test this approach on the full ``Weather prediction'' dataset \citep{malinin2021weather} (3M+ objects).
\autoref{fig:new-candidates} indicates that such ``online updates'' may be a viable solution for incorporating new data into an already trained \model.
Additionally, this approach can be used to scale \model\ to large datasets by training the model on a subset of data and retrieving from the full data.
Overall, we consider the conducted experiment as a preliminary exploration, and leave a systematic study of continual updates for future work.
See \autoref{A:sec:impl-new-candidates} for implementation details.

\begin{minipage}[t]{0.48\textwidth}
    \centering
    % Figure removed
    \captionof{figure}{ 
        \mbox{$\Delta$-context} (explained below) averaged over training objects until the early stopping while training \modelsimple.
        On a given epoch, for a given object, \mbox{$\Delta$-context} shows the portion of its context (the top-$m$ candidates and their weights) changed compared to the previous epoch (i.e., the lower the value, the smaller the change; see \autoref{A:sec:impl-context-freeze} for formal details).
        The plot shows that context updates become less intensive during the course of training, which motivates the optimization described in \autoref{sec:context-freeze}.
    }
    \label{fig:context-freeze}
\end{minipage}
\hspace{\fill}
\begin{minipage}[t]{0.48\textwidth}
    \centering
    % Figure removed
    \captionof{figure}{
        Training \modelsimple\ on various portions of the training data of the full ``Weather prediction'' dataset and gradually adding the remaining unseen training data to the set of candidates \textit{without retraining} as described in \autoref{sec:new-candidates}.
        For each curve, the leftmost point corresponds to not adding any new data to the set of candidates after the training, and the rightmost point corresponds to adding all unseen training data to the set of candidates.
    }
    \label{fig:new-candidates}
\end{minipage}

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The performance of \modelsimple\ with the ``context freeze'' as described in \autoref{sec:context-freeze}.
        The first column shows after how many epochs the contexts are frozen.
        In parentheses, we provide the fraction of time spent on training compared to the training without freezing (the last row).
    }
    \label{tab:context-freeze}
    \scalebox{0.85}{\input{data/tab-context-freeze}}
\end{table*}

\subsection{Analyzing the similarity module of \model}
\label{sec:similarity-module}

In this section, we analyze the similarity module $\mathcal{S}$ introduced in Step-2 of \autoref{sec:retrieval}, which greatly improved the performance on several datasets in \autoref{tab:design} and which is used in \model.

\textbf{Diversity of attention patterns.} Formally, for a given input object, the similarity module defines a distribution over candidates (``weights'' in \autoref{fig:architecture-retrieval}) with exactly $m + 1$ non-zero entries ($m$ is the context size; $+1$ comes from adding the target object to its own context in Step-2).
Intuitively, the less diverse such distributions are on average, the more frequently \textit{different input objects} are augmented with \textit{similar contexts}.
In \autoref{tab:similarity-module}, we demonstrate that such distributions are more diverse on average with the new similarity module compared to the one from the vanilla attention.
The implementation details are provided in \autoref{A:sec:impl-similarity-module}.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Entropy of the average distribution over candidates (the averaging is performed over individual distributions for test objects).
        The distributions are produced by the similarity module as explained in \autoref{sec:similarity-module}.
        The trained Step-1 and Step-2 models are taken directly from \autoref{tab:design}.
        The similarity module introduced at Step-2 of \autoref{sec:retrieval} produces more diverse contexts.
    }
    \label{tab:similarity-module}
    \scalebox{0.9}{\input{data/tab-similarity-module}}
\end{table*}

\textbf{Domain analysis.}
In \autoref{A:sec:similarity-module}, we perform additional analysis, which we now briefly describe in this paragraph.
First, we select three datasets where it is possible to define ``good neighbors'' from human expert intuition (e.g., to forecast weather in a given region, it is useful to pay attention to geographical neighbors from the training set).
Then, we show that the similarity module of \model\ allows the model to find and exploit those good neighbors, which is not the case for the vanilla attention.
The best part of this story is that one of the three datasets contains a leak (i.e., for a target object, the ``good neighbor'' is its almost precise copy available in the training data!), and that dataset was used in prior work, but \textit{none} of the considered ML models (including DL and GBDT), except for \model, could exploit that.

\subsection{Analyzing the value module of \model}
\label{sec:value-module}

In this section, we analyze the value module $\mathcal{V}$ of \model\ (see \autoref{eq:model}):
\begin{align}
\label{eq:step-3-module-reminder}
    \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)
    = W_Y (y_i) + T(k - k_i)
    = W_Y (y_i) + T(\Delta k_i)
\end{align}
Intuitively, for a given context object, its label $y_i$ can be an important part of its contribution to the prediction.
Let's consider regression problems, where, in \autoref{eq:step-3-module-reminder}, $y_i \in \R$ is embedded by $W_Y$ to $\tilde{\mathbb{Y}} \subset \R^d$.
Since $W_Y$ is a linear layer, $\tilde{\mathbb{Y}}$ is just a line, and each point on this line can be mapped back to the corresponding label from $\R$.
Then, the projection of the correction term $T(\Delta k_i)$ on $\tilde{\mathbb{Y}}$ can be translated to the correction of the context label $y_i$:
\begin{align}
\label{eq:step-3-module}
    \mathcal{V}(\tilde{x}, \tilde{x}_i, y_i)
    = W_Y (y_i) + \underline{\text{proj}_{\tilde{\mathbb{Y}}} T(\Delta k_i)} + \text{proj}_{\tilde{\mathbb{Y}}^\perp} T(\Delta k_i)
    = W_Y (y_i + \underline{\Delta y_i}) + \text{proj}_{\tilde{\mathbb{Y}}^\perp} T(\Delta k_i)
\end{align}
To check whether the underlined correction term $\text{proj}_{\tilde{\mathbb{Y}}} T(\Delta k_i)$ (or $\Delta y_i$) is important, we take a \textit{trained} \model, and reevaluate it \textit{without retraining} while ignoring this projection (which is equivalent to setting $\Delta y_i = 0$).
As a baseline, we also try ignoring the projection of $T(\Delta k_i)$ on a random one-dimensional subspace instead of $\tilde{\mathbb{Y}}$.
\autoref{tab:value-module} indicates that the correction along $\tilde{\mathbb{Y}}$ plays a vital role for the model.
The implementation details are provided in \autoref{A:sec:impl-value-module}.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        Evaluating RMSE of trained \modelsimple\ while ignoring projections of $T(\Delta k_i)$ on different one-dimensional subspaces as described in \autoref{sec:value-module}.
        The first column shows the projection on which one-dimensional subspace is removed from $T(\Delta k_i)$.
        The first row corresponds to not removing any projections (i.e., the unmodified \modelsimple).
        Ignoring the projection on $\tilde{\Y}$ (the label embedding space) breaks the model while ignoring a random projection does not have much effect.
    }
    \label{tab:value-module}
    \scalebox{0.9}{\input{data/tab-value-module}}
\end{table*}

For classification problems, we tested similar hypotheses but did not obtain any interesting results.
Perhaps, the value module $\mathcal{V}$ and specifically the $T$ module should be designed differently to better model the nature of classification problems.

\section{Conclusion \& Future work}

In this work, we have demonstrated that retrieval-based deep learning models have great potential in supervised machine learning problems on tabular data.
Namely, we have designed \model\ -- a retrieval-augmented tabular DL architecture that provides strong average performance and achieves the new state-of-the-art on several datasets.
Importantly, we have highlighted similarity and value modules as the important details of the attention mechanism which have a significant impact on the performance of attention-based retrieval components.

An important direction for future work is improving the efficiency of retrieval-augmented models to make them faster in general and in particular applicable to tens and hundreds of millions of data points.
Also, in this paper, we focused more on the aspect of task performance, so some other properties of \model\ remain underexplored.
For example, the retrieval nature of \model\ provides new opportunities for interpreting the model's predictions through the influence of context objects.
Also, \model\ may enable better support for continual learning (we scratched the surface of this direction in \autoref{sec:new-candidates}).
Regarding architecture details, possible directions are improving similarity and value modules, as well as performing multiple rounds of retrieval and interactions with the retrieved instances.

\medskip
\bibliographystyle{abbrvnat}
\bibliography{references}

\newpage
\appendix

\section*{Supplementary material}

\section{Source code}

The source code is available at: \\\url{\repository}

\section{Additional analysis}

\subsection{Similarity module of \model}
\label{A:sec:similarity-module}

In this section, we consider three datasets where the transition to the key-only $L_2$ similarity module from the ``vanilla'' dot-product-between-queries-and-keys demonstrated the most impressive performance.
Formally, this is the transition from ``Step-1'' to ``Step-2'' in \autoref{tab:design}.
For each of the three datasets, first, we notice that, for a given input object, there is a domain-specific notion of ``good neighbors'', i.e., such neighbors that, from a human perspective, are very relevant to the input object and provide strong hints for making a better prediction for the input object.
Then, we show that the new similarity module allows finding and exploiting those natural hints.

\textbf{California housing (CA)}.
On this dataset, the transition from the ``vanilla'' dot-product-between-queries-and-keys similarity module to the key-only $L_2$ similarity module resulted in a substantial performance boost, as indicated by the difference between ``Step-1'' and ``Step-2'' in \autoref{tab:design}.
On this dataset, the task is to estimate the prices of houses in California.
Intuitively, for a given house from the test set, the prices of the training houses in the geographical neighborhood should be a strong hint for solving the task.
Moreover, there are coordinates (longitude and latitude) among the features, which should simplify finding good neighbors.
And the ``Step-2'' model successfully does that, which is not true for the ``Step-1'' model.
Specifically, for an average test object, the ``Step-2'' model concentrates approximately 7\% of the attention mass on the object itself (recall that ``Step-2'' includes the target object in the context objects) and approximately 77\% on the context objects within the 10km radius. The corresponding numbers of the ``Step-1'' model are 0.07\% and 1\%.

\textbf{Weather prediction (WE)}.
Here, the story is seemingly similar to the one with the CA dataset analyzed in the previous paragraph, but in fact has a major difference.
Again, here, for a given test data point, the dataset contains natural hints in the form of geographical neighbors from the training set which allow making a better weather forecast for a test query; and the ``Step-2'' model (\autoref{tab:design}) successfully exploits that, while the ``Step-1'' model cannot pay any meaningful attention to those hints.
Specifically, for an average object, the ``Step-2'' model concentrates approximately 29\% of the attention mass on the object itself (recall that ``Step-2'' includes the target object in the context objects) and approximately 25\% on the context objects within the 200km radius. The corresponding numbers of the ``Step-1'' model are 0.25\% and 0.5\%.
\textbf{However}, there is a crucial distinction from the CA case: in the version of the dataset WE that we used, \textit{the features did not contain the coordinates}.
In other words, to perform the analysis, \textit{after} the training, we restored the original coordinates for each row from the original dataset and observed that the model \textit{learned} the ``correct'' notion of ``good neighbors'' from other features.

\textbf{Facebook comments volume (FB)}.
In this paper, this is the first time when we mention this dataset, which was used in prior work \citep{gorishniy2022embeddings} and which we also used for some time in this project.
Notably, on this dataset, \model\ was demonstrating unthinkable improvements over competitors (including GBDT and the best-in-class parametric DL models).
Then we noticed a strange pattern: often, for a given input, \model\ concentrated an abnormally high percentage of its attention mass on just one context object (a different one for each input object).
This is how we discovered that the dataset split that we inherited from \citet{gorishniy2022embeddings} contained a ``leak'': roughly speaking, for many objects, it was possible to find their almost exact copies in the training set, and the task was dramatically simpler with this kind of hint.
In practice, it was dramatically simpler for the \model, but not for other models.
Specifically, for an average object, the ``Step-2'' model concentrates approximately 20\% of the attention mass on the object itself (recall that ``Step-2'' includes the target object in the context objects) and approximately 35\% on its leaked almost-copies. The corresponding numbers of the ``Step-1'' model are 0.5\% and 0.09\%.

\subsection{Ablation study}
\label{A:sec:ablation}

Recall that on Step-2 of \autoref{sec:retrieval}, we mentioned that it was crucial that \textit{all} changes from Step-2 compared to Step-0 (using labels + not using queries + using the $L_2$ distance instead of the dot product) are important to provide noticeable improvements over MLP on several datasets.
Note that not using queries is equivalent to sharing weights of $W_Q$ and $W_K$: $W_Q = W_K$.
\autoref{A:tab:ablation} contains the results of the corresponding experiment and indeed demonstrates that the Step-2 configuration cannot be trivially simplified without loss in metrics (see the CH, CA, BL, WE datasets).

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The ablation study as described in \autoref{A:sec:ablation}.
        $W_Q = W_K$ means using only keys and not using queries.
        Step-2 is the only variation providing noticeable improvements over MLP on the CH, CA, BL, WE datasets.
    }
    \label{A:tab:ablation}
    \scalebox{0.9}{\input{data/A-tab-ablation}}
\end{table*}

\subsection{Additional results for the ``context freeze'' technique}
\label{A:sec:context-freeze-extended}

We report the extended results for \autoref{sec:context-freeze} in \autoref{A:fig:context-freeze-extended}, \autoref{A:tab:context-freeze-extended-scores} and \autoref{A:tab:context-freeze-extended-time}.
For the formal definition of the $\Delta$-context metric, see \autoref{A:sec:impl-context-freeze}.

% Figure environment removed

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{
        The extended version of \autoref{tab:context-freeze}.
        Freezing after 0 epochs means freezing with a randomly initialized model.
        The speedups are provided in \autoref{A:tab:context-freeze-extended-time}
    }
    \label{A:tab:context-freeze-extended-scores}
    \scalebox{0.85}{\input{data/A-tab-context-freeze-extended-scores}}
\end{table*}

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Fraction of time spent on training in \autoref{A:tab:context-freeze-extended-scores}, relative to the training time without the context freeze (the last row; the format is hours:minutes:seconds).}
    \label{A:tab:context-freeze-extended-time}
    \scalebox{0.9}{\input{data/A-tab-context-freeze-extended-time}}
\end{table*}

\newpage
\subsection{Augmenting XGBoost with a retrieval component}
\label{A:sec:xgboost-with-neighbors}

After the successful results of \model\ reported in \autoref{sec:main}, we tried augmenting XGBoost with a simple retrieval component to ensure that we do not miss this opportunity to improve the baselines.
Namely, for a given input object, we find $m=96$ (equal to the context size of \model) nearest training objects in the original feature space, average their features and labels (the label as-is for regression problems, the one-hot encoding representations for classification problems), concatenate the target object's features with the ``average neighbor's'' features and label, and the obtained vector is used as the input for XGBoost.
The results in \autoref{A:tab:xgboost-with-neighbors} indicate that this strategy does not lead to any noticeable profit for XGBoost.
We tried to vary the number of neighbors but did not achieve any significant improvements.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Results for ensembles of tuned models. ``XGBoost + retrieval'' stands for XGBoost augmented with the ``average neighbor's'' features and label as described in \autoref{A:sec:xgboost-with-neighbors}.}
    \label{A:tab:xgboost-with-neighbors}
    \scalebox{0.9}{\input{data/A-tab-xgboost-with-neighbors}}
\end{table*}

\subsection{Comparing training times}
\label{A:sec:training-time}

Among the results published along with the source code, we report tuning and training times for most, if not all, experiments.
In this section, for convenience, we summarize some of the most interesting and important figures.

\textbf{Comparing \model\ with retrieval-based tabular DL.}
In \autoref{A:tab:training-time-npt}, we report training times of \model\ and NPT \cite{kossen2021self} (the second best retrieval-based model, after \model, according to \autoref{tab:exp-revisiting}).
The table indicates that, in addition to much better performance, \model\ is also significantly faster.

\begin{table*}[h!]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Training times of NPT \cite{kossen2021self} and \modelsimple\, averaged over multiple runs. The format is \texttt{hh:mm:ss}.}
    \label{A:tab:training-time-npt}
    \scalebox{0.9}{\input{data/A-tab-training-time-npt}}    
\end{table*}

\textbf{Comparing \model\ with parametric DL and GBDT models.}
In \autoref{A:tab:training-time-dl-gbdt}, we report training times of the tuned configurations of \model, XGBoost \citep{chen2016xgboost} (trained on NVidia A100), LightGBM (trained on 4 threads on CPU), and ``MLP-EMB'' (MLP with the same embeddings for numerical features \citep{gorishniy2022embeddings} as the corresponding \model\ on a given dataset).
The results are rather expected, with GBDT being the fastest solution.
In practice, however, absolute times can be of more importance than relative overhead.

\begin{table*}[h]
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Training times of the tuned configurations of several models. ``MLP-EMB'' is an MLP with the same embeddings for numerical features as the corresponding \model\ on a given dataset. The format is \texttt{hh:mm:ss}.}
    \label{A:tab:training-time-dl-gbdt}
    \scalebox{0.9}{\input{data/A-tab-training-time-dl-gbdt}}    
\end{table*}

\section{Benchmarks}

\subsection{Our benchmark}
\label{A:sec:benchmark-ours}

In \autoref{A:tab:benchmark-ours}, we provide more information on the datasets from \autoref{tab:datasets}.
The datasets include:
\begin{itemize}[nosep,leftmargin=2em]
    \item Churn Modeling\footnote{https://www.kaggle.com/shrutimechlearn/churn-modelling}
    \item California Housing (real estate data, \citep{california})
    \item House 16H\footnote{https://www.openml.org/d/574}
    \item Adult (income estimation, \citep{adult})
    \item Diamond\footnote{https://www.openml.org/d/42225}
    \item Otto Group Product Classification\footnote{https://www.kaggle.com/c/otto-group-product-classification-challenge/data}
    \item Higgs (simulated physical particles, \citep{higgs}; we use the version with 98K samples available in the OpenML repository \citep{openml})
    \item Black Friday\footnote{https://www.openml.org/d/41540}
    \item Weather (temperature, \citep{malinin2021weather}). We take 10\% of the dataset for our experiments due to its large size.
    \item Weather (full) (temperature, \citep{malinin2021weather}). Original splits from the paper.
    \item Covertype (forest characteristics, \citep{covertype})
    \item Microsoft (search queries, \citep{microsoft}). We follow the pointwise approach to learning to rank and treat this ranking problem as a regression problem.
\end{itemize}

\begin{table*}
    \setlength\tabcolsep{2.2pt}
    \centering
    \caption{Details on datasets from the main benchmark. ``\# Num'', ``\# Bin'', and ``\# Cat'' denote the number of numerical, binary, and categorical features, respectively. The ``Batch size'' is the default batch size used to train DL-based models.}
    \label{A:tab:benchmark-ours}
    \scalebox{0.9}{\input{data/A-tab-benchmark-ours}}
\end{table*}

\subsection{The benchmark from \citet{grinsztajn2022why}}
\label{A:sec:benchmark-why}

In this section, we describe how exactly we used the benchmark proposed in \citet{grinsztajn2022why}.
\begin{itemize}[nosep,leftmargin=2em]
    \item We use the same train-val-test splits.
    \item When there are several splits for one dataset (i.e., when the n-fold-cross-validation was performed in \citet{grinsztajn2022why}), we first treat each of them as separate datasets while tuning and evaluating algorithms as described in \autoref{A:sec:implementation-details}, but then, we average the metrics over the splits to obtain the final numbers for the dataset. For example, if there are five splits for a given dataset, then we tune and evaluate a given algorithm five times, each of the five tuned configurations is evaluated under 15 random seeds on the corresponding splits, and the reported metric value is the average over \mbox{$5 * 15 = 75$} runs.
    \item When there are \textit{multiple} versions of \textit{one} dataset (e.g., the original regression task and the same dataset but converted to the binary classification task or the same dataset, but with the categorical features removed, etc.), we keep only one \textit{original} dataset.
    \item We removed the ``Eye movements'' dataset because there is a leak in that dataset.
    \item We use the tuning and evaluation protocols as described in \autoref{A:sec:implementation-details}, which was also used in prior works on tabular DL \citep{gorishniy2021revisiting,gorishniy2022embeddings}. Crucially, we tune hyperparameters of the GBDT models more extensively than most (if not all) prior work in terms of both budget (20 warmup iterations of random sampling followed by 180 iterations of the tree-structured Parzen estimator algorithm) and hyperparameter spaces (see the corresponding sections in \autoref{A:sec:implementation-details}).
\end{itemize}

\section{Implementation details}
\label{A:sec:implementation-details}

\subsection{Hardware}
\label{A:sec:impl-harware}

We report the used hardware in the results published along with the source code.
In a nutshell, the vast majority of experiments on GPU were performed on one NVidia A100 GPU, the remaining small part of GPU experiments was performed on one Nvidia 2080 Ti GPU, and there was also a small portion of runs performed on CPU (e.g. all the experiments on LightGBM).

\subsection{Implementation details of \autoref{sec:context-freeze}}
\label{A:sec:impl-context-freeze}

In \autoref{sec:context-freeze}, we used \modelsimple\ with the default hyperparameters (see \autoref{A:sec:impl-model}).
To compute \mbox{$\Delta$-context}, we collect context distributions for training objects \textit{between training epochs}.
That is, after the $i$-th training epoch, we pause the training, collect the context distributions for all training objects, and then start the next $(i+1)$-th training epoch.

\textbf{$\Delta$-context.}
Intuitively, this heuristic metric describes in a single number how much, for a given input object, the context attention mass was updated compared to the \textit{previous} epoch.
Namely, it is a sum of two terms:
\begin{enumerate}[nosep,leftmargin=2em]
    \item the \texttt{novel} attention mass, i.e. the attention mass coming from the context objects presented on the current epoch, but not presented on the previous epoch
    \item the \texttt{increased} attention mass, i.e. we take the intersection of the current and the previous context objects and compute the increase of their total attention mass. We set it to 0.0 if actually decreased.
\end{enumerate}
Now, we formally define this metric.
For a given input object, let $a \in \R^{|I_{train}|}$ and $b \in \R^{|I_{train}|}$ denote the two distributions over the candidates from the previous and the current epochs, respectively.
Let denote the sets of non-zero entries as $A = \{i: a_i > 0\}$ and $B = \{i: a_i > 0\}$.
Note that $|A| = |B| = m = 96$.
In other words, $A$ and $B$ are the contexts from the two epochs.
Then:
\begin{align}
    \Delta\text{-context} &= \texttt{novel} + \texttt{increased} \\
    \texttt{novel} &= \sum_{i \in B \setminus A} b_i \\
    \texttt{increased} &=  \max \left( \sum_{i \in B \cap A} b_i - \sum_{i \in B \cap A} a_i, 0.0 \right)
\end{align}

\subsection{Implementation details of \autoref{sec:new-candidates}}
\label{A:sec:impl-new-candidates}

In \autoref{sec:new-candidates}, we used \modelsimple\ with the default hyperparameters (see \autoref{A:sec:impl-model}).

\subsection{Implementation details of \autoref{sec:similarity-module}}
\label{A:sec:impl-similarity-module}

In \autoref{sec:similarity-module}, we performed the analysis over exactly the same model checkpoints that we used to assemble the lines ``Step-1'' and ``Step-2'' in \autoref{sec:model}.

To reiterate, this is how the entropy in \autoref{tab:similarity-module} is computed:
\begin{enumerate}
    \item First, we obtain individual distributions over candidates for all test objects. One such distribution contains exactly ($m + 1$) non-zero entries.
    \item Then, we average all individual distributions and obtain the average distribution.
    \item \autoref{tab:similarity-module} reports the entropy of the average distribution.
\end{enumerate}

Note that, when obtaining the distribution over candidates, the top-$m$ operation is taken into account.
Without that, if the distribution is always uniform regardless of the input object, then the average distribution will also be uniform and with the highest possible entropy, which would be misleading in the context of the story in \autoref{sec:similarity-module}.

Lastly, recall that in the Step-1 and Step-2 models, an input object is added to its own context.
Then, the edge case when all input objects pay 100\% attention only to themselves would lead to the highest possible entropy, which would be misleading for the story in \autoref{sec:similarity-module}.
In other words, for the story in \autoref{sec:similarity-module}, we should treat the ``paying attention to self'' behavior similarly for all objects.
To achieve that, on the first step of the above recipe, we reassign the attention mass from ``self'' to a new virtual context object, which is \textit{the same} for all input objects.

\subsection{Implementation details of \autoref{sec:value-module}}
\label{A:sec:impl-value-module}

To build \autoref{tab:value-module}, we used \modelsimple\ with the default hyperparameters (see \autoref{A:sec:impl-model}).

\subsection{Experiment setup}
\label{A:sec:experiment-setup}

For the most part, we simply follow \citet{gorishniy2022embeddings}, but we provide all the details for completeness.
Note that some of the prior work may differ from the common protocol that we describe below, but we provide the algorithm-specific implementation details further in this section.

\textbf{Data preprocessing.}
For each dataset, for all DL-based solutions, the same preprocessing was used for fair comparison.
For numerical features, by default, we used the quantile normalization from the Scikit-learn package \citep{pedregosa2011scikit}, with rare exceptions when it turned out to be detrimental (for such datasets, we used the standard normalization or no normalization).
For categorical features, we used one-hot encoding.
Binary features (i.e. the ones that take only two distinct values) are mapped to $\{0,1\}$ without any further preprocessing.

\textbf{Training neural networks.}
For DL-based algorithms, we minimize cross-entropy for classification problems and mean squared error for regression problems.
We use the AdamW optimizer \citep{loshchilov2019decoupled}.
We do not apply learning rate schedules.
We do not use data augmentations.
For each dataset, we used a predefined dataset-specific batch size.
We continue training until there are $\texttt{patience} + 1$ consecutive epochs without improvements on the validation set; we set $\texttt{patience} = 16$ for the DL models.

\textbf{How we compare algorithms.}
For a given dataset, first, we define the ``preliminary best'' algorithm as the algorithm with the best mean score.
Then, we define a set of the best algorithms (i.e. their results are in bold in tables) as follows: a given algorithm is included in the best algorithms if its mean score differs from the mean score of the preliminary best algorithm by no more than the standard deviation of the preliminary best algorithm.

\subsection{Embeddings for numerical features}
\label{A:sec:embeddings}

\begin{minipage}{0.39\textwidth}
    \centering
    % Figure removed
    \captionof{figure}{(Copied from \citet{gorishniy2022embeddings}) The vanilla MLP. The model takes two numerical features as input.}
    \label{A:fig:mlp-without-embeddings}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}{0.51\textwidth}
    \vspace{-1em}
    \centering
    % Figure removed
    \captionof{figure}{(Copied from \citet{gorishniy2022embeddings}) The same MLP as in \autoref{A:fig:mlp-without-embeddings}, but now with embeddings for numerical features.}
    \label{A:fig:mlp-with-embeddings}
\end{minipage}

In this work, we actively used embeddings for numerical features from \citep{gorishniy2022embeddings} (see \autoref{A:fig:mlp-without-embeddings} and \autoref{A:fig:mlp-with-embeddings}), the technique which was reported to universally improve DL models.
In a nutshell, for a given scalar numerical feature, an embedding module is a trainable module that maps this scalar feature to a vector.
Then, the embeddings of all numerical features are concatenated into one flat vector which is passed to further layers.
Following the original paper, when we use embeddings for numerical features, the same embedding architecture is used for all numerical features.

In this work, we used the \texttt{LR} (the combination of a linear layer and ReLU) and \texttt{PLR} (the combination of periodic embeddings, a linear layer, and ReLU) embeddings from the original paper.
Also, we introduce the \texttt{PLR(lite)} embedding, a simplified version of the \texttt{PLR} embedding where the linear layer is shared across all features.
We observed it to be significantly more lightweight without critical performance loss.

\textbf{Hyperparameters tuning.}
For the \texttt{LR} embeddings, we tune the embedding dimension in $\mathrm{Uniform}[16,96]$.
For the \texttt{PLR} and \texttt{PLR(lite)} embeddings, we tune the number of frequencies in $\mathrm{Uniform}[16,96]$ (in $\mathrm{Uniform}[8,96]$ for \model\ on the datasets from \citet{grinsztajn2022why}), the frequency initialization scale in $\mathrm{LogUniform}[0.01,100.0]$ and the embedding dimension in $\mathrm{Uniform}[16,64]$  (in $\mathrm{Uniform}[4,64]$ for \model\ on the datasets from \citet{grinsztajn2022why}).

\subsection{\model}
\label{A:sec:impl-model}

\implnote

\textbf{Embeddings for numerical features.} (see \autoref{A:sec:embeddings})
For the non-simple configurations of \model, on datasets CH, CA, HO, AD, DI, OT, HI, BL, and on all the datasets from \citet{grinsztajn2022why}, we used the \texttt{PLR(lite)} embeddings as defined in \autoref{A:sec:embeddings}.
For other datasets, we used the \texttt{LR} embeddings.

\textbf{Other details.}
We observed that initializing the $W_Y$ module properly may be important for good performance.
Please, see the source code.

\textbf{Default \modelsimple.}
The default hyperparameters for \modelsimple\ were obtained at some point in the project by literally averaging the tuned hyperparameters over multiple datasets.
The specific set of datasets for averaging included all datasets from \autoref{A:tab:benchmark-ours} plus two datasets that used to be a part of our benchmark, but were excluded later.
So, in total, 13 datasets contributed to the default hyperparameters.

Formally, this is not 100\% fair to evaluate the obtained default \modelsimple\ on the datasets which contributed to this default hyperparameters as in \autoref{tab:exp-gbdt-ours}.
However, we tested the fair leave-one-out approach as well (i.e. for a given dataset, averaging tuned hyperparameters over all datasets except for this one dataset) and did not observe any meaningful changes, so we decided to keep things simple and to have one common set of default hyperparameters for all datasets.
Plus, the obtained default \modelsimple\ demonstrates decent performance in \autoref{tab:exp-gbdt-why} as well, which illustrates that the obtained default configuration is not strongly ``overfitted'' to the datasets from \autoref{A:tab:benchmark-ours}.
The specific default hyperparameter values of \modelsimple\ are as follows:

\begin{itemize}[nosep,leftmargin=2em]
    \item $d = 265$
    \item Attention dropout rate $ = 0.38920071545944357$
    \item Dropout rate in \texttt{FFN} $= 0.38852797479169876$
    \item Learning rate $ = 0.0003121273641315169$
    \item Weight decay $ = 0.0000012260352006404615$
\end{itemize}

\textbf{Hyperparameters.}
The output size of the first linear layer of \texttt{FFN} and of $T$ is $2d$.
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.
The same protocol and hyperparameter spaces were used when tuning models in \autoref{tab:design} and \autoref{A:tab:ablation}.

\begin{table}[H]
\centering
\caption{
    The hyperparameter tuning space for \model.
    Here (A) = \{CH, CA, HO, AD, DI, OT, HI, BL\}, (B) = \{WE, CO, MI\}.
    For the datasets from \citet{grinsztajn2022why}, the tuning space is identical to (A) with the only difference that $d$ is tuned in $\mathrm{UniformInt}[16,384]$.
}
\label{tab:S-transformer-space}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter                     & (Datasets) Distribution & Comment \\
    \midrule
    Width $d$                     & (A,B) $\mathrm{UniformInt}[96,384]$ \\
    Attention dropout rate        & (A,B) $\mathrm{Uniform}[0.0,0.6]$ \\
    Dropout rate in \texttt{FFN}  & (A,B) $\mathrm{Uniform}[0.0, 0.6]$  \\
    Learning rate                 & (A,B) $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay                  & (A,B) $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]\}$ \\
    $N_E$                         & (A,B) $\mathrm{UniformInt}[0,1]$ & $\mathrm{Const}[0]$ for \modelsimple \\
    $N_P$                         & (A,B) $\mathrm{UniformInt}[1,2]$ & $\mathrm{Const}[1]$ for \modelsimple \\
    \midrule
    \# Tuning iterations                 & (A) 100 (B) 50 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{MLP}
\implnote

We used the implementation from \citet{gorishniy2022embeddings}.

\textbf{Hyperparameters.}
We use the same hidden dimension throughout the whole network.
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for MLP}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \# layers           & $\mathrm{UniformInt}[1,6]$ \\
    Width (hidden size) & $\mathrm{UniformInt}[64,1024]$ \\
    Dropout rate        & $\{0.0, \mathrm{Uniform}[0.0,0.5]\}$ \\
    Learning rate       & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]\}$ \\
    \midrule
    \# Tuning iterations & 100 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{FT-Transformer}
\implnote

We used the implementation from the "\texttt{rtdl}" Python package (version 0.0.13).

\textbf{Hyperparameters}.
We use the \mbox{\texttt{rtdl.FTTransformer.make\_baseline}} method to create \mbox{FT-Transformer}, so most of hyperparameters is inherited from this method's signature, and the rest is tuned as shown in the corresponding table.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for FT-Transformer}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \# blocks           & $\mathrm{UniformInt}[1,4]$ \\
    $d_{token}$         & $\mathrm{UniformInt}[16,384]$ \\
    Attention dropout rate        & $\mathrm{Uniform}[0.0,0.5]$ \\
    FFN hidden dimension expansion rate       & $\mathrm{Uniform}[\nicefrac{2}{3},\nicefrac{8}{3}]$ \\
    FFN dropout rate & $\mathrm{Uniform}[0.0,0.5]$ \\
    Residual dropout rate & $\{0.0, \mathrm{Uniform}[0.0,0.2] \}$ \\
    Learning rate       & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4]\}$ \\
    \midrule
    \# Tuning iterations & 100 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{kNN}

\implnote

The features are preprocessed in the same way as for DL models.
The only hyperparameter is the number of neighbors which we tune in $\mathrm{UniformInt}[1, 128]$.

\subsection{DNNR}

\implnote

We've used the official implementation \footnote{\url{https://github.com/younader/dnnr}}, but to evaluate DNNR on larger datasets with greater hyperparameters variability, we have rewritten parts of the source code to make it more efficient: enabling GPU usage, batched data processing, multiprocessing, where possible.
Crucially, we leave the underlying method unchanged.
We provide our efficiency-improved DNNR in the source code.
There is no support for classification problems, so we evaluate DNNR only on regression problems.

\textbf{Hyperparameters.} We performed a grid-search over the main DNNR hyperparameters on all datasets, falling back to defaults (suggested by the authors) due to scaling issues on WE and MI.

\begin{table}[H]
\centering
\caption{The hyperparameter grid used for DNNR. Here (A) = \{CA, HO\}; (B) = \{DI, BL, WE, MI\}. Notation: $N_f$ -- number of features for the dataset.}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter                     & (Datasets) Parameter grid & Comment \\
    \midrule
    \# neighbors $k$                 & (A,B) $[1,2,3, \ldots, 128]$ \\
    Learned scaling               & (A,B) [No scaling, Trained scaling] \\
    \# neighbors used in scaling & (A,B) [$8 \cdot N_f, 2, 3, 4, 8, 16, 32, 64, 128$] & $8 \cdot N_f$ on WE, MI \\
    \# epochs used in scaling & $10$ \\
    Cat. feature encoding & [one-hot, leave-one-out] \\
    \midrule
    \# neighbors for derivative $k'$        & (A) $\mathrm{LinSpace}[2\cdot N_f, 18 \cdot N_f, 20]$ \\
    & (B) $\mathrm{LinSpace}[2\cdot N_f, 12 \cdot N_f, 14]$ \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{DKL}

\implnote

We used DKL implementation from GPyTorch \citep{gardner2018gpytorch}. We do not evaluate DKL on WE and MI datasets due to scaling issues (tuning alone takes 1 day and 17 hours, compared to 3 hours for \model\ on the medium DI dataset, for example). There is no support for classification problems, thus we evaluate DKL only on regression problems.

\textbf{Hyperparameters.} As with MLP we use the same hidden dimension throughout the whole network. And perform tuning using the tree-structured Parzen Estimator algorithm from the Akiba et al. [1] library. 
\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for DKL}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    Kernel              & $\{\mathrm{rbf}, \mathrm{sm}\}$ \\
    \# layers           & $\mathrm{UniformInt}[1,4]$ \\
    Width (hidden size) & $\mathrm{UniformInt}[64,768]$ \\
    Dropout rate        & $\{0.0, \mathrm{Uniform}[0.0,0.5]\}$ \\
    Learning rate       & $\mathrm{LogUniform}[1e\text{-}5, 1e\text{-}2]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}3]\}$ \\
    \midrule
    \# Tuning iterations & 100 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{ANP}

While the original paper introducing ANP did not focus on the tabular data, conceptually, it is very relevant to prior work on retrieval-based tabular DL, so we consider it as one of the baselines.

\implnote

We used the Pytorch implementation from an unofficial repository\footnote{\url{https://github.com/soobinseo/Attentive-Neural-Process}} and modified it with respect to the official implementation from \cite{kim2019attentive}. Specifically, we reimplemented \texttt{Decoder} class exactly as it was done in \cite{kim2019attentive} and changed a binary cross-entropy loss with a Gaussian negative log-likelihood loss in \texttt{LatentModel} class since it matches with the official implementation.

We do not evaluate ANP on the MI dataset due to scaling issues. Tuning alone on the smaller WE dataset took more than four days for 20(!) iterations (instead of 50-100 used for other algorithms). Also, there is no support for classification problems, thus we evaluate ANP only on regression problems.

We used 100 tuning iterations on CA and HO, 50 on DI, and 20 on BL and WE. 

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for ANP}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \# decoder layers           & $\mathrm{UniformInt}[1,3]$ \\
    \# cross-attention layers           & $\mathrm{UniformInt}[1,2]$ \\
    \# self-attention layers           & $\mathrm{UniformInt}[1,2]$ \\
    Width (hidden size) & $\mathrm{UniformInt}[64,384]$ \\
    Learning rate       & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4]\}$ \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{NPT}

We use the official NPT \citep{kossen2021self} implementation \footnote{\url{https://github.com/OATML/non-parametric-transformers}}.
We leave the model and training code unchanged and only adjust the datasets and their preprocessing according to our protocols.

We evaluate the \texttt{NPT-Base} configuration of the model and follow both \texttt{NPT-Base} architecture and optimization hyperparameters.
We train NPT for $2000$ epochs on CH, CA, AD, HO, $10000$ epochs on OT, WE, MI, $15000$ epochs on DI, BL, HI and $30000$ epochs on CO.
For all datasets that don't fit into the A100 80GB GPU, we use batch size $4096$ (as suggested in the NPT paper).
We also decrease the hidden dim to $32$ on WE and MI to avoid the OOM error.

Note that NPT is conceptually equivalent to other transformer-based non-parametric tabular DL solutions: \citep{somepalli2021saint, schafl2022hopular}.
All three methods use dot-product-based self-attention modules alternating between self-attention between object features and self-attention between objects (for the whole training dataset or its random subset).

\subsection{SAINT}
\implnote

We use the official implementation of SAINT \footnote{\url{https://github.com/somepago/saint}} \textbf{with one important fix}.
Recall that, in SAINT, a target object interacts with its context objects with intersample attention.
In the official implementation of SAINT, \textit{context objects are taken from the same dataset part, as a target object}: for training objects, context objects are taken from the training set, for validation objects -- from the validation set, for test objects -- from the test set.
This is different from the approach described in this paper, where \textit{context objects are always taken from the training set}.
Taking context objects from different dataset parts, as in the official implementation of SAINT, may be unwanted because of the following reasons:
\begin{enumerate}[nosep,leftmargin=2em]
    \item model can have suboptimal validation and test performance because it is trained to operate when context objects are taken from the training set, but evaluated when context objects are taken from other dataset parts.
    \item for a given validation/test object, \textit{the prediction depends on other validation/test objects}. This is not in line with other retrieval-based models, which may result in inconsistent comparisons. Also, in many real-world scenarios, during deployment/test time, input objects should be processed independently, which is not the case for the official implementation of SAINT.
\end{enumerate}
For the above reasons, we slightly modify SAINT such that each individual sample attends only to itself and to context samples from the training set, both during training and evaluation.
See the source code for details.

On small datasets (CH, CA, HO, AD, DI, OT, HI, BL) we fix the number of attention heads at $8$ and performed hyperparameter tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for SAINT}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    Depth  & $\mathrm{UniformInt}[1,4]$ \\
    Width & $\mathrm{UniformInt}[4,32,4]$ \\
    Feed forward multiplier & $\mathrm{Uniform}[\nicefrac{2}{3}, \nicefrac{8}{3}]$ \\
    Attention dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    Feed forward dropout & $\mathrm{Uniform}[0, 0.5]$ \\
    Learning rate       & $\mathrm{LogUniform}[3e\text{-}5, 1e\text{-}3]$ \\
    Weight decay        & $\{0, \mathrm{LogUniform}[1e\text{-}6, 1e\text{-}4]\}$ \\
    \bottomrule
\end{tabular}}
\end{table}

On larger datasets (WE, CO, MI) we use slightly modified (for optimizing memory consumption) default configuration from the paper with following fixed hyperparameters:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{depth} = 4
    \item \texttt{n\_heads} = 8
    \item \texttt{dim} = 32
    \item \texttt{ffn\_mult} = 4
    \item \texttt{attn\_head\_dim} = 48
    \item \texttt{attn\_dropout} = 0.1
    \item \texttt{ff\_dropout} = 0.8
    \item \texttt{learning\_rate} = 0.0001
    \item \texttt{weight\_decay} = 0.01
\end{itemize}

\subsection{XGBoost}

\implnote

\implgbdt

The following hyperparameters are fixed and not tuned:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{booster} = ``gbtree''
    \item \texttt{n\_estimators} = 4000
    \item \texttt{tree\_method} = ``gpu\_hist''
    \item \texttt{early\_stopping\_rounds} = 200
\end{itemize}
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for XGBoost}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \texttt{colsample\_bytree}  & $\mathrm{Uniform}[0.5,1.0]$ \\
    \texttt{gamma}              & $\{0.0, \mathrm{LogUniform}[0.001,100.0]\}$ \\
    \texttt{lambda}             & $\{0.0, \mathrm{LogUniform}[0.1,10.0]\}$ \\
    \texttt{learning\_rate}     & $\mathrm{LogUniform}[0.001,1.0]$ \\
    \texttt{max\_depth}         & $\mathrm{UniformInt}[3,14]$ \\
    \texttt{min\_child\_weight} & $\mathrm{LogUniform}[0.0001,100.0]$ \\
    \texttt{subsample}          & $\mathrm{Uniform}[0.5,1.0$ \\
    \midrule
    \# Tuning iterations & 200 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{LightGBM}

\implnote

\implgbdt

The following hyperparameters are fixed and not tuned:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{n\_estimators} = 4000
    \item \texttt{early\_stopping\_rounds} = 200
\end{itemize}
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for LightGBM}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \texttt{feature\_fraction}  & $\mathrm{Uniform}[0.5,1.0]$ \\
    \texttt{lambda\_l2}             & $\{0.0, \mathrm{LogUniform}[0.1,10.0]\}$ \\
    \texttt{learning\_rate}     & $\mathrm{LogUniform}[0.001,1.0]$ \\
    \texttt{num\_leaves}         & $\mathrm{UniformInt}[4,768]$ \\
    \texttt{min\_sum\_hessian\_in\_leaf} & $\mathrm{LogUniform}[0.0001,100.0]$ \\
    \texttt{bagging\_fraction}          & $\mathrm{Uniform}[0.5,1.0]$ \\
    \midrule
    \# Tuning iterations & 200 \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{CatBoost}

\implnote

\implgbdt

The following hyperparameters are fixed and not tuned:
\begin{itemize}[nosep,leftmargin=2em]
    \item \texttt{n\_estimators} = 4000
    \item \texttt{early\_stopping\_rounds} = 200
    \item \texttt{od\_pval} = 0.001
\end{itemize}
We performed tuning using the tree-structured Parzen Estimator algorithm from the \citet{akiba2019optuna} library.

\begin{table}[H]
\centering
\caption{The hyperparameter tuning space for CatBoost}
\vspace{1em}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lll}
    \toprule
    Parameter           & Distribution \\
    \midrule
    \texttt{bagging\_temperature}  & $\mathrm{Uniform}[0.0,1.0]$ \\
    \texttt{depth}             & $\mathrm{UniformInt}[3,14]$ \\
    \texttt{l2\_leaf\_reg}             & $\mathrm{Uniform}[0.1,10.0]$ \\
    \texttt{leaf\_estimation\_iterations}             & $\mathrm{Uniform}[1,10]$ \\
    \texttt{learning\_rate}             & $\mathrm{LogUniform}[0.001,1.0]$ \\
    \midrule
    \# Tuning iterations & 200 \\
    \bottomrule
\end{tabular}}
\end{table}

\section{Extended results with standard deviations}
\label{A:sec:extended-results}

In this section, we provide the extended results with standard deviations for the main results reported in the main text.
The results for our benchmark are in the \autoref{A:tab:extended-results-ours}.
The results for the benchmark from \citet{grinsztajn2022why} are in the \autoref{A:tab:extended-results-why}.

\newcommand{\topalign}[1]{%
\vtop{\vskip 0pt #1}}

\begin{longtable}{p{0.5\textwidth}p{0.5\textwidth}}
\caption{Extended results for our benchmark. Results are grouped by datasets and span multiple pages below. Notation: \textdownarrow\ corresponds to RMSE, \textuparrow\ corresponds to accuracy.}\\
\label{A:tab:extended-results-ours}
\input{data/A-tab-extended-results-ours}
\end{longtable}

\begin{longtable}{p{0.5\textwidth}p{0.5\textwidth}}
\caption{Extended results for \citet{grinsztajn2022why} benchmark. Results are grouped by datasets and span multiple pages below. Notation: \textdownarrow\ corresponds to RMSE, \textuparrow\ corresponds to accuracy.}\\
\label{A:tab:extended-results-why}
\input{data/A-tab-extended-results-why}
\end{longtable}

% \newpage
% \bibliographystyle{abbrvnat}
% \bibliography{references_supplementary}

\end{document}