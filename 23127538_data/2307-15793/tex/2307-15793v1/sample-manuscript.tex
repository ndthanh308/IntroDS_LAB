%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%\documentclass[acmsmall,screen,review,anonymous]{acmart}
\documentclass[acmsmall,screen]{acmart}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{xcolor}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
\setcopyright{none}
\copyrightyear{}
\acmYear{}
\acmDOI{none}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\newcommand{\sumit}[1]{\textcolor{red}{\textbf{(SA:} #1)}}
\newcommand{\ah}[1]{\textcolor{violet}{\textbf{(AH:} #1)}}
\newcommand\leadin[1]{%
    \vskip 5pt \noindent\textbf{#1.} %
}
\renewcommand{\mkbegdispquote}[2]{\itshape}
\newcommand{\change}[1]{\textcolor{black}{#1}}
\newcommand{\colortbl}{\color{black}}



%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%% Can we write this down? - A system for generating automatic meeting recap and a study of its effectiveness
%% Designing effective and adaptive meeting recap experiences with LLMs: Can the AI write this down for us? 
%% A system for automatic meeting recap: Can the AI write this down for us? 
%% That meeting could have been an email -- and now it is!
%% All meetings are now emails. 
%% Learning to summarize meetings
%% Leveraging cognitive science and LLMs to summarize meetings
%% Summaries, Highlights, and Action items: LLM-powered meeting summarization
%% Summaries, Highlights, and Action items: LLM-powered meeting summarization and a study of its effectiveness
%% Meeting recaps should be hierarchical and other insights from an LLM-powered summarization system
%% Meeting recaps should be hierarchical and other insights from the lab study of an LLM-powered summarization system
%% Summaries, Highlights, and Action items: An LLM-powered meeting recap system and a study of its effectiveness
\title{Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Sumit Asthana}
\email{asumit@umich.edu}
\affiliation{%
  \institution{University of Michigan, Ann Arbor}
  \city{Ann Arbor}
  \state{MI}
  \country{USA}
}

\author{Sagih Hilleli}
\email{sagih@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Tel Aviv}
  \state{}
  \country{Israel}
}

\author{Pengcheng He}
\email{pengcheng.h@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \state{Washington}
  \country{USA}
}

\author{Aaron Halfaker}
\email{aaron.halfaker@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \state{Washington}
  \country{USA}
}

% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{none}

\begin{abstract}
% Meetings are important to organizations
%Meetings play a critical infrastructural role in the coordination of work.  In recent years, the nature of meetings have been changing with the shift to hybrid and remote work -- meetings have moved into computer mediated spaces in new ways that have lead to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g., automated transcription/captioning and recap support).  Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs.  Yet it is unclear how to best apply these technologies to support cooperative work.
%In this study we describe the design, implementation, and evaluation a meeting summarization system. We evaluate the effectiveness of the system in two different experiences that deliver complimentary insights - important ``highlights'', and a structured, ``hierarchical'' minutes view informed by insights from the cognitive science literature on event perception and recall. We evaluate the effectiveness of the system with seven users in the context of their work meetings. Our findings suggest that while LLM-based approaches still lack an understanding of whats personally relevant to participants and sometimes miss important details, our experimental recap experiences have the potential to dramatically improve post-meeting recap experience, save workers time, and provide a space for asynchronous collaboration. We also report on implications for designing AI systems to partner with users to learn and improve from \emph{natural interactions} to overcome the limitations related to personal relevance and summarization quality. 
Meetings play a critical infrastructural role in the coordination of work.  In recent years, the nature of meetings have been changing with the shift to hybrid and remote work -- meetings have moved into computer mediated spaces in new ways that have lead to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. \change{Despite this potential, they exhibit significant issues if directly applied to summarize meeting long transcripts. Moreover, prior studies
of recap highlight varying recap needs based on userâ€™s context that no one design addresses, highlighting the need for in-context evaluations.}
To address these gaps, we describe the design, implementation, and in-context evaluation a meeting recap system. We first conceptualize two salient recap representations -- important ``highlights'', and a structured, ``hierarchical'' minutes view and provide supporting rationales from cognitive science and discourse theories on perception and recall. \change{We develop a system to operationalize the representations with dialogue summarization as its building blocks.} Finally, we evaluate the effectiveness of the system with seven users in the context of their work meetings. \change{Our findings show promise in using LLM-based dialogue summarization for meeting recap and the need for both representations in different contexts. However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can  miss important details, and mis-attributions can be detrimental to group dynamics. We identify collaboration opportunities such as a shared recap document that a high quality recap enables.} We report on implications for designing AI systems to partner with users to learn and improve from \emph{natural interactions} to overcome the limitations related to personal relevance and summarization quality. We synthesize these findings as design implications to advance the space of meeting recap in supporting group work in organizations.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% Meetings are important to organizations
``Communication is the lifeblood of organizations''\cite{sethi2009interpersonal} and meetings are ``window into the soul of a business''\cite{karen2009effective} are truisms that describe the reality of modern work in organizational settings.  Meetings serve an important organizational purpose for people to discuss ideas, share information, build consensus, and make decisions. They help get key points, actions items, questions and answers distributed so people get the information on time~\cite{whittaker2008design}. They also help to reduce the uncertainty in the organization by bringing the participants together, to discuss and resolve issues~\cite{allen2015cambridge}. However, key details of the meetings could be missed or forgotten due to participants being oversubscribed to meetings or due to distractions by emails, IMs and other communication~\cite{nathan2012}. Further, time conflicts and increasing cross-timezone collaborations cause participants to need to miss meetings altogether~\cite{nurmi2011coping}.

% Work is changing an meetings are moving online
With the rise in geographically dispersed teams, the COVID-19 pandemic, and shifts towards remote work, organizations are increasingly adopting online web conferencing software to hold meetings\cite{NFW22, yankelovich2004}.  With the wide adoption of technically mediated meetings comes an opportunity for designers and researchers to apply technical approaches to support these new ways of working.

% Meeting recap was always important but it's becoming even more important recently.
In this paper, we consider technological support for \emph{meeting recap} -- systems that aid in the capture of important information, decisions, and action items conveyed in a meeting for asynchronous review and engagement. Meeting recap serves an important role in preserving meeting content both for attendees and non-attendees alike~\cite{whittaker2008design}. Past practices for capturing meeting content such as note-taking are often informal and their manual nature places a high burden on meeting participants~\cite{kalnikaitundefined2012}. While current meeting technologies allow for effective participation within meetings, like seamless high quality videos and raising hands for participation~\cite{sarkar2021, koh2022}, tools for supporting meeting recap either rely on participants to engage in distracted note-taking during the meeting or require the user to sort through dense meeting content (e.g. recordings or transcript) by inefficient scanning patterns~\cite{moran1997ll}. 

% When people 

Prior approaches have attempted to ease recap by presenting a mix of audio, video, or transcripts generated using automated speech recognition (ASR) in navigable browsers~\cite{whittaker2008design}. While such browsers provide a record to reference, raw utterances and recordings still leave sense-making to participants, making it cumbersome to digest information~\cite{arons1992techniques, moran1997ll}.
%but they are too long and dense to be consumed efficiently~\cite{arons1992techniques, moran1997ll} and 
This also limits asynchronous participation -- opportunities for non-attendees to engage with the meeting content and with attendees after the meeting has taken place -- due to the lack of context in generated summaries~\cite{kalnikaitundefined2012}. 
\change{Recent dialogue summarization models~\cite{lewis2020} have been shown to generate contextual summaries for short chit-chat style dialogues~\cite{gliwa-etal-2019-samsum} and such summaries could improve over raw transcripts. However, meeting dialogues are long discussions between multiple participants for disseminating information, achieving consensus, negotiating deals and deliberations~\cite{tuggener-etal-2021-summarizing,allen2015cambridge}. Directly applying dialogue summarization on meeting length transcripts has been challenging due to scattered information, multiple and changing topics and unclear topic boundaries~\cite{jia2022taxonomy,prasad-etal-2023-meetingqa}. Moreover, research shows that user needs vary based on their role, whether they attended the meeting, and their work expectations~\cite{banerjee2005necessity, whittaker2008design}. A single recap representation is unlikely to capture all such needs~\cite{tuggener-etal-2021-summarizing}. Thus, automated metrics, or crowd-sourced evaluations are un-reliable indicators of useful meeting recap~\cite{whittaker2008design,kalnikaitundefined2008}. Prior research in CSCW indicates that studying how users apply technology in their work (e.g., ML assisted Wikipedia editing or ML assisted medical note-taking)~\cite{knoll-etal-2022-user} is essential to identify technology's \emph{alignment} with contextual user needs that automated metrics or crowd-sourced evaluations cannot measure~\cite{smith2020, star1994,schaekermann2018}. 
%Studying how people apply new technology for their work (e.g. using ML decisions to judge vandalism edits)~\cite{star1994, yang2019} yields better measures of relevance, and close \emph{alignment} between the technology and its context of use. 
Yet, evaluations in people's context of use is especially challenging because AI systems for complex work do not lend themselves well to traditional low-cost evaluation methods in HCI like heuristic evaluations, or wizard-of-oz ~\cite{yang2019}.}

\change{To address this gap of how to apply dialogue summarization for meeting recap, and evaluate how well recap representations serve user needs
%of how dialogue summarization can benefit end user's recap needs in their organizational context, and identify interactions that empower users to indicate relevance to improve recap models, 
we 1) Develop an LLM-based summarization system~\cite{lewis2020,cohen-etal-2021-automatic,he2020deberta} for long meeting dialogues using transformers and text segmentation, 2) Use theories from cognitive science and discourses~\cite{zacks2001perceiving} to conceptualize two diverse \emph{experimental representations}~\cite{yang2019} "highlights" and "hierarchical" that capture the need for quick takeaways, and chronological topic focused discussions respectively~\cite{kalnikaitundefined2008, nathan2012}, 3) Perform preliminary evaluations with seven information workers in a large software company to explore the benefits and challenges when attendees use the representations to support their work and collaborations. Since the models we develop are trained on crowd-sourced data, that do \emph{not align} with user needs in high context applications like meeting recap~\cite{yang2017role}, we also explore what interactions can enable users to provide high quality feedback to improve and align model behavior to their needs. In summary, our context of study is organizational work which is a core tenet of cscw~\cite{schaekermann2018}. We leverage the concept of \emph{imperfect prototype experiences} from HCI~\cite{yang2019} to explore the design space of meeting summarization for recap support as well as ways in which user interactions with a meeting recap experience could be turned into useful training data to improve the \emph{alignment} of the summarization models.}

%along three dimensions of CSCW group work 1) Help participants make sense of their meetings, 2) Provides interactions that closely reflect user's intentions as they engage with the notes for sensemaking, and can be used to generate good quality contextual training data for aligning the models. 

\change{
Our in-context evaluations highlight user needs that existing linguistic evaluations of dialogue summarization do not capture. %As hypothesized, we observed the need for a gist of discussion, or key decisions, depending on the role and context. 
 We find that dialogue summarization generates summaries that are understandable by users, allowing them to recall their meeting, plan action items, and share, improving over the distractions observed from transcript based browsers~\cite{kalnikaitundefined2008}. 
 While all participants found the summaries understandable, pronoun issues and mis-attributions are mildly distracting for individual recap, but could be detrimental to group dynamics. We find strong support for a hierarchical approach to meeting summary presentation that is implied by cognitive theory on task structure understanding~\cite{zacks2001perceiving} and discourse analysis~\cite{grosz-sidner-1986-attention}. With both "highlights" and "hierarchical", we found the need to incorporate personal relevance in recap (e.g., priority ordering of summary items). Additionally, we found strong inclination to collaborate using the recap, by sharing summaries, or using it as a collaborative document indicating the recap's role in persisting meeting outcomes as permanent documents. %To address the issue of relevance, we study interactions that are (e.g., additions, edits, selecting highlights, sharing) and are not (e.g., deletions) likely to carry a strong signal for bringing models into alignment with users' expectations and needs. 
 To improve the relevance of generated summaries, we found that ML models can learn from users adding or editing summaries but deleting summaries may not provide high quality training data as participants reported multiple reasons for deletions.
 These implications are grounded in existing literature of increasing alignment of Human-AI models~\cite{yang2019clinic}. Our work follows the lineage of CSCW \textit{Design and Evaluation systems} category of work that uses existing technological support to design  systems for supporting group work~\cite{resnick1994, wallace2017} and evaluate the system's usefulness in supporting the organizational processes through a preliminary in-context study~\cite{star1994}.}

% We employ an \emph{imperfect prototype experience}~\cite{yang2019} powered by existing state of the art dialog summarization algorithms to explore different strategies for constructing a meeting recap experience. 

\section{Related work}
%\ah{alignment and problem of getting in context data from users}
%\ah{past work meeting cscw - if we can give people a really simple taxonomy - that helps people as better participants, and another that helps people async communication, like a threaded video}
%Time travel proxy: using lightweight video recordings to create asynchronous, interactive meetings
\subsection{Speech and text summarization}
% Shorten corpus based summarization, spend more time on dialogue summarization
% allude to extractive and abstractive summarization and what are the contrasts - extractive for documents is very good not for dialogues, abstractive is good for dialogues, 
Speech and text summarization is the backbone of generating intelligent meeting recap experiences. The most straightforward approach is to directly detect important utterances using automatic speech recognition (ASR). However, natural language summarization methods~\cite{zhang-etal-2021-exploratory-study} are designed for text and cannot be directly applied to speech signals
%ASR technology while mature suffers from the challenges like miscategorizations, lack of context, and poor generalizability across vocabularies~\cite{mcgregor2017}. 
% We should say more plainly that there are no studies of dialog summarization applied in context.
Therefore, it is more effective to translate the speech to text and then apply text summarization~\cite{allahyari2017text, moratanch2017}. Early text summarization approaches simply extracted important sentences from the corpus to represent in the summary. However, a summary consisting of important sentences verbatim does not capture enough context due to unresolved pronouns and references. With advent of neural based summarization approaches~\cite{dong2018survey}, models can generate an abstracted summary of corpus with context. While corpus based abstractive summarization provides more context, it does not work well in the dialogue setting due to the inability to map dialogues to speakers and resolve the references in the multiparty dialogues~\cite{khalifa-etal-2021-bag}. Dialogue datasets based on informal SMS exchanges~\cite{chen2017, chen-etal-2021-dialogsum} led to progress on capturing context resulting in summaries that are close to human written notes. However, meetings can typically span 30 minutes or more and involve sensemaking in a shared grounded environment, and even state of the art transformer models cannot be directly applied to long discussions because of limited text input length~\cite{zhang-etal-2021-exploratory-study}, and inability to capture long range context dependencies. To overcome these limitations, Cohen et al~\cite{cohen-etal-2021-automatic} explored a two stage process 1) Identify the important meeting segments using transcripts where important utterances are annotated for relevance, 2) Paraphrase the utterances identified in step (1) to generate abstractive notes and add them to the summary. Current evaluations in summarization explore the faithfulness of summaries to reference text dialogues~\cite{liu2021}, %interactive alignment with a user relevant concept~\cite{dash2019, ghodratnama2020adaptive} such as factuality~\cite{tang-etal-2022-confit} 
or automated metrics like ROGUE~\cite{lin-2004-rouge}. \change{While ROGUE supports evaluating summaries for text corpus, or short dialogues~\cite{gliwa-etal-2019-samsum}, metrics and crowd-sourced evaluations are not a good choice for evaluating meeting recap as it requires high contextual knowledge to assess relevance~\cite{star1994}. Factors like user's background, organizational contexts like established recap practices and cognitive load can all have an impact on what recap fits user's needs and requires in-context studies to evaluate~\cite{whittaker2008design}.} 

\subsection{Meeting recap tools}
% I'll get that off the audio - a case study of salvaging
% Capturing the capture concepts: a case study in the design of computer-supported meeting environments
Most contemporary meeting tools allow participants to record meetings and generate automatic transcription, for later recap. However, raw audio and video recordings of meetings have been found to be poor means of recap~\cite{moran1997ll, arons1992techniques} because they are long with redundant information. Prior approaches to ease recap have focused on building tools to combine audio, video and other artefacts (e.g., slides, documents, whiteboard annotations, transcripts marked with discourse labels)~\cite{whittaker2008design}. Such methods employed various strategies to help participants consume the raw information like helping take notes or mark timestamps in meetings, or automatically extract phrases with keywords that participants can use and review later. They could also provide index into meetings~\cite{geyer2005towards, richter2003, whittaker1994, nathan2012}, generate short annotations~\cite{banerjee2005necessity}, or record notes on electronic devices~\cite{Davis:CSD-98-997}. For example, the Lite minutes system~\cite{chiu2001liteminutes} allowed participants to create and share text, audio and slide based notes with automatically generated smart links, and post meeting note correction over email. Such annotations when linked with video have been shown to provide value in education environments as well~\cite{siangliulue2015, nathan2012} acting as digital bookmarks. To further ease the cognitive load, research into automatic indexing tools developed indexes based on the transcript content~\cite{topkara2010tag}, speech recognition on audio, semantic grouping of concepts represented in the transcript~\cite{geyer2005towards, kazman1996, ehlen2008, manteidynamic,purver-etal-2006-unsupervised}, or removing redundant utterances from recordings~\cite{tucker2010}. Segmentation, action item detection and note extraction can then be combined in a meeting browser~\cite{ehlen2007meeting} for easy meeting recap. Despite the rich modalities of recap browsers, \change{Whittaker et al~\cite{whittaker2008design} in their evaluation of meeting browsers found that participants found audio/video distracting, and resorted to scanning transcripts due to relative ease of skimming~\cite{whittaker2008design}. Yet, due to digesting raw utterances, they repeatedly missed the high level picture of the meeting, while being overconfident in their recall. Both Whittaker et al and Banerjee et al's recap studies ~\cite{whittaker2008design, banerjee2005necessity} highlight the need to consider different recap forms for different needs, one for quick catch-up and another for detailed topic focused structured discussions. With all prior recap systems~\cite{whittaker1994, banerjee2005necessity, chiu2001liteminutes,ehlen2007meeting} participants could identify disjoint independent nuggets of information, but struggled to draw a high level picture of the meeting. This was due to limitations of text processing at the time to rephrase transcripts in formats that are akin to manual notes and summaries.}
%Thus, participants still prefer manually created notes either by themselves, or others~\cite{nathan2012} that captures a more refined gist of discussions. Manual note-taking also helps participants provide cues for later retrieval~\cite{kalnikaitundefined2008,kalnikaitundefined2012, reilly2011shared} and improve subject matter understanding in lectures~\cite{costley2021collaborative, mik2019effects} due to more cognitive involvement. Thus, we study the potential of recent transformer models~\cite{lewis2020} in generating notes that help participants recap easily. 
Recap in online discussions is similarly complex and has similar opportunities.  Structured presentation of discussion in a hierarchical format has been found to be helpful for recap sense-making activities~\cite{zhang2017, nam2007}. \change{Moreover, Zhang et al found that simple information augmentation on messages like attaching discourse labels (e.g. action, answer, decision) to slack discussions~\cite{zhang2018} helped participants skim and filter relevant discussions on asynchronous discussion forums. However, their augmentation still required participants to add tags manually, which is not feasible when attending meetings online~\cite{marshall2002}.}

\subsection{Meeting recap and sensemaking}
Meetings play a crucial role in organizations in terms of reducing the uncertainty by building mutual understanding, as well as helping participants brainstorm, and create new ideas~\cite{allen2015cambridge, acai2018getting}. Meeting recap helps to sustain these benefits after a meeting by documenting the discussions and outputs. Documented discussions can foster collaboration and dissemination of ideas~\cite{bryant2005, kittur2007, andr2014, siangliulue2015}. Beyond collaboration, meeting discussions are a rich source of resources whose documentation can add to existing resources in the organization for newcomers, and experts alike. Besides acting as memory aids, meeting recap afford participants retrospective sensemaking that may lead to more insights, or clarifications of concepts\cite{zhang2018}. 

\change{
Due to such diverse uses of meeting recap, there is considerable debate on what constitutes a good recap~\cite{whittaker2008design}. Including everything in the recap can lead to increased cognitive burden on participants, no better than video, and fixation on ideas~\cite{moran1997ll, jansson1991design}. For attendees, meetings serve as reminders, acting as "cues" into their memory~\cite{kalnikaitundefined2008}, for which a meeting recap with the most important discussions and decisions suffices. However, for non-attendees and attendees after a sufficient time since the meeting, a "minutes" like view that provides a chronological meeting record helps understand the purpose, discussions and decisions of the meeting. In-context prior evaluations with meeting attendees surface similar needs~\cite{whittaker2008design} According to cognitive fit theory~\cite{speier2006influence}, the structure of the meeting recap can determine the kind of tasks, and sensemaking that it can support. Recap composed of a list of key discussions and decisions is suitable to aid recall when participants already have meeting details in their memory. "Minutes" like chronological structure is supported by theories in discourses and cognitive science. Zacks et al.\cite{zacks2001perceiving} show through experiments asking people to record, recall, and communicate about video recordings that people tend to put together events in hierarchical structure where the object of reference (e.g. a meeting agenda item) is at the top of the hierarchy and details discussing and referencing that object appear below.  %At the very bottom, Zacks refers to 
They refer to details at the very bottom as ``behavioral primitives'' -- details that are so low level that they are often ignored in summary and communication.  This structure roughly reflects the practice of producing meeting ``minutes'', a summary record of what took pace at a meeting that is intended for general recap use cases. Theories in discourses find similar evidence of structures in written work documents. 
Intentional theory of discourse~\cite{grosz-sidner-1986-attention} posits that discourses follow a hierarchical structure with intentions guiding the discourse and discourse segments form individual sub-topics within the discourse. Mann et al.~\cite{mann1987rhetorical} studied letters, memos, newspapers, scientific articles and found that similar hierarchical relations hold between parts of written text. Moreover, hierarchical structure also emerges in our sensemaking of temporal events condensed in Rhetorical Structure Theory of text. Thus, there is compelling evidence in cognitive science about hierarchical structures in sense-making both during perception, and during expression of information.}

\section{System Design}
\subsection{Design Rationale}
\change{Meeting recap supports complex organizational workflows that requires participants to scan through summaries, judge each summary block's relevance to their own work, and decide how to act on it~\cite{allen2015cambridge}. Naturalistic sense-making under  constraints (e.g., limited time, multi-tasking) for such complex tasks has been shown to be impacted both by limitations of the technology, and interactions of the technology with the user's context (e.g., how much context the user has about the meeting, or their time availability). Since an "ideal" summary can have many different representations for different users (e.g., most important points, full minutes, action items), using dialogue summarization to generate actual recap representations from people's meetings and evaluating with them is necessary help bridge the gap between user expectations, and technology affordances~\cite{star1994, resnick1994, whittaker2008design}. However conceptualizing good representations is hard without well defined measures of quality~\cite{yang2020}. Thus, we leverage the cognitive fit theory to conceptualize representations whose structure aligns with potential user needs~\cite{speier2006influence} and represent them using "design rationales"~\cite{lee1991s}. Design Rationales (DR) are used in HCI to evaluate computational support for complex tasks in user's context. Design Rationales explicitly define representations, their supporting reasoning, and a criteria for their evaluation against reference tasks~\cite{lee1991s}. %Explicitly defining rationales and then evaluating can lead to a better understanding of HCI issues in the socio-technical context, and uncover metrics for subsequent quantitative studies to evaluate the representation at a larger scale.
Since prior work has not explored the in-context usefulness of AI generated meeting recap to participants, we take an interpretivist approach to evaluate the design rationales using interviews. Our goal is to discover themes that capture relevant aspects of meeting recap in relation to usefulness for end user needs.}

Inspired by design rationales, we ask -- what shape should a meeting recap take?  On one hand, we try to mimic the way that people take personal notes -- \change{\textit{focusing on only the most important information (e.g., decisions and action items~\cite{allen2015cambridge})}}. On the other hand, some use cases for meeting recap may prioritize \change{\textit{knowing how a decision was made as well the details of the decision itself}~\cite{whittaker2008design, moran1997ll}.}

Both of these approaches have potential advantages and drawbacks.  Since each represents a design rationale about what will be most useful to meeting participants, we formalize them below and then describe their operationalization as different user experiences to evaluate. 
\begin{itemize}
    \item \textbf{\change{DR1}: Personal note-taking:} a meeting recap should be as short as possible and focus on outcomes to serve the users' needs efficiently.
    \item \textbf{\change{DR2}: Meeting minutes:} a meeting recap should summarize the entire meeting including discussions and outcomes within a hierarchical structure to enable broad use cases and contextual navigation.
\end{itemize}

\change{By focusing only on the most important outcomes of the meeting, the recap experience can be as brief as possible and thus save the user the time and energy of reviewing information that is less relevant after the meeting. Meeting minutes tend to employ a hierarchical structure that is supported by Zack et al's~\cite{zacks2001perceiving} study that people perceive and understand events in hierarchy, where agenda items represent the high-level objects of discussion and details about that agenda item's discussion are recorded beneath. Moreover, hierarchical structure also shows up in discourse analysis of people's communication, and written documents of work, both of which are crucial elements of organizational collaborations~\cite{mann1987rhetorical, grosz-sidner-1986-attention}.}

We designed two experiences that exemplify each design rationale (see Fig~\ref{fig:experiences_ux}) respectively. The \emph{highlights experience} (DR1) focuses on pulling out key points and action items from meetings as these help participants decide on the consensus on the decisions, as well as upcoming action items. The experience provides users a brief set of key points and action items from their meetings, each represented by one to two sentence summaries. %One to two sentence summaries of \emph{key points} are shown under "AI Notes" and summaries of action items are shown under "AI Tasks".  A user is able to see the raw meeting transcript relevant to selected notes and tasks by selecting "show context" from a menu.  

In contrast, the \emph{hierarchical experience} (DR2) leverages a ``chapterization'' strategy to break the meeting into parts and to give each part a representative title.  Beneath, each chapter, a user can explore lower level summaries.  And beneath those summaries is the raw transcript.  Unlike the \emph{highlights experiences}, the \emph{hierarchical experience} captures the entirety of the meeting in a structure that should be intuitive for users. 

We also designed these experiences to provide opportunities for users to correct, direct, and use what they saw in front of them in ways that we hoped would provide signals our models could learn from in order to become more aligned with users' expectations~\cite{christian2020alignment}.  In order to achieve this, these interactions need to have relatively consistent meaning when users apply them (alignment), they need to be performed in cases where the model was wrong (informative).  Further, the users themselves need to understand what they are doing well enough to teach the model (situatedness), and they need to perform these actions frequently enough that we could gather a reasonable amount of training data over time (quantity). In our interviews, in addition to focusing on the proposed design's usefulness in providing recap support, we also focused on what users said and did in the UX to improve the summaries. We argue that these interactions provide early indicators of the type and quality of training data we would likely get in a deployment at scale to support meeting recap and improving the meeting recap AI through feedback. We provide full description of the two user experiences with specific details about these interface components in Section~\ref{sec:prototype}.

\change{While prior recap studies~\cite{whittaker1994} have highlighted needs for both quick takeaways and detailed hierarchical discussions, we are the first to 1) Formalize the needs as design rationales to structure exploration, and provide supporting reasoning from cognitive science, 2) Design recap systems to operationalize these rationales to evaluate their fit in addressing target recap needs.}

% Figure environment removed

\subsection{Modeling}
\subsubsection{Highlights model}
Highlights experience is generated from four sequential transformer models~\cite{vaswani2017attention}, two for the key points, and two for the action items in the meeting. For each note or action item, the first model is an extractive model that takes an utterance with its surrounding context as an input, and classifies it as a key point or action item. The second is an abstractive model that gets the utterances identified as a key point or action item from the previous model and rewrites them using the surrounding utterances as their context~\cite{cohen-etal-2021-automatic}. Key points and action items in the ``highlights'' experience both use fine-tuned deBERTa with 12 transformer layers~\cite{he2020deberta} for their extractive part. They were trained on ICSI and AMI labeled datasets~\cite{janin2003}. Their input size is 100 tokens which are extracted from the relevant utterance and enough context from previous and next utterances to fill the 100 tokens input size. 

For the abstractive part, both key points and action items models use fine-tuned BART~\cite{lewis2020}. This was trained on ICSI and AMI labeled datasets as well~\cite{janin2003}. The models get the input utterance identified as a key point or action item by the extractive model and a surrounding context 500 tokens. The output of the model is the rephrased action item or note in third person. Thus, the abstractive model relies on a supportive extractive model which identifies the relevant context from the surrounding utterances in order to make BART~\cite{lewis2020} focus on the important information in the context. e.g., if the extractive model identified the following as an action item - ``Serena: I will finish this by Friday'', the abstractive model will rephrase it as ``Serena will finish the slides by Friday''. In this example, the abstractive model replaces ``this'' with ``slides'' that Serena referenced earlier. Writing the note in 3rd person and with context makes it easy to understand for a broader audience~\cite{whittaker2008design}. Figure~\ref{fig:pipeline} (left) illustrates this pipeline. Please refer to Cohen et al.~\cite{cohen-etal-2021-automatic} for more details on the models.

% % Figure environment removed
%We use a combination of extractive and abstractive summarization[] to generate meeting recap experiences. We use a pre-trained transformer to identify the relevant meeting utterances. The transformer model is fine-tuned on a  dataset of meeting utterances labeled with relevant and non-relevant utterances. The dataset has XX meeting utterances, labeled by XX annotators, with an inter-annotator score of YY. Once the extractive model identifies the relevant meeting utterances, we use another pre-trained transformer to rephrase the extracted utterances, into a notes format that captures the notes and action items as well as the relevant context. Figure XX shows an abstracted action item from a meeting utterance.

\subsubsection{Hierarchical (``Chapters'') model}
%\ah{segmentation and lexical similarity is an old technique, which is now recently applied using transformer models. If RW is close to what they have done, we can write less in Chapters}
% Figure environment removed
%annotators are asked to annotate the start token of each segment, based on the annotations we train a classification model to predict if a token is start of a new segment. the annotation is at the beginning of each utterance. To handle long input, we split the document into chunks, with overlaps. rough length 500 tokens. cls is a position predicting start of a segment.
%special token prepended to first token of each utterance. Window is just for training, workaround for the long sequence issue.
%need to confirm: 1) how big windows, how much they overlap
%2) how do we combine the predictions

We generate the hierarchical (``chapters'') experience in two steps -- 1) Segment the entire meeting transcript into parts where each part corresponds to a sub-topic or set of topics (Figure~\ref{fig:segmentation-pipeline}), 2) Synthesize a title and a set of notes representing each part (Figure~\ref{fig:pipeline} right half). %The chapters segmentation process is based on the idea of document segmentation using lexical cohesion~\cite{hearst-1997-text}. The segmentation boundary is determined so that distribution of representative words within the segment are similar and distribution of representative words across segments are dissimilar~\cite{galley-etal-2003-discourse, purver-etal-2006-unsupervised}.

For step (1), we follow the text-tiling approach for segmenting meetings into chapters (topics)~\cite{hearst-1997-text, tur2011spoken}. Segmentation is based on the idea of document segmentation using lexical cohesion~\cite{hearst-1997-text}. The segmentation boundary is determined so that distribution of representative words within the segment are similar and distribution of representative words across segments are dissimilar~\cite{galley-etal-2003-discourse, purver-etal-2006-unsupervised}. Text-tiling is a method for transcript segmentation that breaks a long input sequence into smaller windows of overlapping sequences. Each window is then labeled with the classifier for topic boundaries and the final boundary is identified with max-voting of the boundaries of the individual windows. (see Figure~\ref{fig:segmentation-pipeline} for an illustration). To train the segmentation model, we annotate a dataset of meetings with chapter boundaries. We recruit annotators on a the UHRS crowd-sourced platform\footnote{\url{https://prod.uhrs.playmsn.com/uhrs/}} and ask them to mark utterances in the transcript that mark the end of one topic of a meeting and beginning of the next as chapter boundaries. This dataset has 12,600 meetings with 126,872 segmentation blocks. The train, dev, test split was 70\%, 15\%, 15\% respectively. We compensated the annotators \$10 per transcript for their time. 
%We are unable to make this dataset publicly available due to the sensitive nature of the discussions in organizational meetings. 
Using the annotated dataset we train a BART~\cite{lewis2020} classification model to predict if an utterance is the start of a new segment. For prediction, we split the transcript into overlapped windows of 30 utterances and stride of 10 utterances. We then apply the classifier to each utterance in the window. We then combine predictions by maximum pooling to arrive at segment boundaries. We adopt the sliding window approach for prediction because average meeting transcripts are much longer than the input length of transformer models (token limit of 512)~\cite{lewis2020}. The result of this step is the meeting transcript segmented into blocks with each block corresponding to one topic or a set of coherent topics (see Figure~\ref{fig:segmentation-pipeline}). 

In step (2), we generate chapter heading and notes for each of these blocks using deBERTa with 12 transformer layers~\cite{he2020deberta}, same as the highlights model. This step is similar to the rephrasing in step (2) of the highlights model, where the model takes in sequence of utterances as input, and rephrases them in 3rd person. We train a transformer model on a dataset of short (eight) dialogue utterances and their corresponding summary. This dataset has 1M meeting utterance-summary pairs. The model generates notes, one for each sequential chunk of eight utterances in the meeting utterance blocks. The chapter headings are generated by a third deBERTa with 12 transformer layers~\cite{he2020deberta} model that is trained on a dataset of meeting utterances, and their corresponding topic assignment. This dataset has 1M meeting utterance topic assignment pairs and was also obtained through annotation on the UHRS crowd-work platform. The final output from the three models is a set of topics marked with topic headings, and a set of notes under each of those topics that represent the entire meeting summary. This result is very similar to a set of meeting minutes. Figure~\ref{fig:pipeline} (right) illustrates this pipeline.

% note abstraction is common across chapters and action items model. How do we explain the two?

% % Figure environment removed

% Figure environment removed

\subsection{Prototype User Experience}
\label{sec:prototype}
We refer to ``key points'' as ``AI notes'' and ``action items'' as ``AI tasks'' in the UX for ease of understanding of participants. We use the shorthand terminology ``notes'' and ``tasks'' in the context of UX and results. To refer to both ``notes'' and ``tasks'' together, we use the term ``summary''.

We prototype the recap experiences on an HTML webapp. The users start with a text area where they can copy-paste a transcript of a recorded meeting. Clicking ``Process'' sends the transcript to the backend for generating the recap insights for the two experiences The backend uses the two summarization pipelines described above to generate the recap data which is sent back to the webapp frontend. Once the recap data is received from the two summarization pipelines, we populate the two tabs ``Highlights'' and ``Hierarchical'' with the data, formatting it as shown (Figure~\ref{fig:experiences}).

The \emph{highlights view} displays a sequence of notes selected by the \textit{key points} extractive model, and rephrased by the \textit{key points} abstractive model. The action items displays a sequence of action items selected by the \textit{action items} extractive model, and rephrased by the \textit{action items} abstractive model. Against each action item, the UI displays an ``assigned to'' field and ``date'' field which is initially empty, and can be filled by participants to reflect the assignee and the deadline for the action item. Users can see more context for both notes and action items by clicking on the three dots at the end of each item line and selecting ``show context''. This opens up a tooltip displaying upto three transcript utterances before and after the utterance where the algorithm detected the notes or action item.  

The \emph{hierarchical view} displays a summary of the entire meeting segmented into topics.  Each topic is made up of a topic heading that summarizes the topic name, followed by a one line summary of the topic, and the timespan (in minutes) of the meeting that the topic covers. Clicking the timespan opens up the topic into a list of summaries that provide a rolling summary of that section of the meeting. Each summary is represented by a summary text, followed by a timestamp that corresponds to the first transcript utterance included in the summary. Clicking each summary item further opens up the constituent transcript utterances that make up the summary. If the summarization algorithm detects one or more important ``key points'' in the topic, stars are displayed to the left of the summary and also next to the chapter title that contains the key point summary. Similarly, If the summarization algorithm detects one or more action items in the topic, checkboxes are displayed to the left of the summary and also next to the chapter title that contains the action item summary.  If the user clicks on the star or checkbox, the the chapter is expanded into individual summaries with the summary containing the key point or action item emphasized by the respective star or checkbox.  

In order to explore interactions that could be used as training data to align~\cite{christian2020alignment} the model behavior with user expectations, we implemented various features that would allow the user to correct and direct the model.  In the \emph{highlights view}, A user can directly edit any suggested note or action item.  They can also add or remove notes and action items from the lists that are automatically generated.  Similarly in the \emph{hierarchical view}, a user can directly edit any summary or even the chapter titles.  Users can also remove an action item, but they cannot remove a summary because doing so would render some of the raw meeting transcript inaccessible.  However users can add or remove stars/checkboxes from various low-level summaries to flag moments of the meeting as key points/action items or remove the same, respectively.  We also provide basic sharing functionality by providing a ``share to chat'' option at the top of the experience.

\section{Methods}
\subsection{Participants}
We conducted our evaluation of the two recap experiences through semi-structured interviews with seven participants. We recruited the participants through internal lists and emails. When the participants agreed for the study, we setup a time of 90 minutes over video conferencing for the interview. Since we recruited employees within the organization as participants, we did not compensate them. 
\begin{table}[h]
    \centering
\begin{tabular}{ |l|l|l| }
\hline
\multicolumn{3}{|c|}{Demographic information} \\
\hline
\multirow{2}{*}{Gender} & Male & 50\% \\
 & Female & 50\% \\ \hline
 \multirow{5}{*}{Age} & 21-25 & 17\% \\
 & 26-30 & 50\% \\ 
 & 31-35 & -- \\ 
 & 36-40 & 17\% \\ 
 & 46-50 & 17\% \\ \hline
 \multirow{4}{*}{Region} & North America & 66\% \\
 & Greater China & 33\% \\ 
 & India & 0\% \\ 
 & Middle-east & 0\% \\  \hline
 \multirow{5}{*}{Profession} & Research & 58.3\% \\
 & Software engineer & 16.7\% \\ 
 & User engineer & 8.3\% \\ 
 & Design/creative & 8.3\% \\ 
 & Other & 8.3\% \\\hline
\end{tabular}
    \caption{Demographic details of interview participants}
    \label{tab:participant-demographics}
\end{table}

\subsection{Tasks and procedures}
Each interview session was conducted by two authors of the paper, with one leading the interview, and the other taking notes. At the start of each session, the interview lead explained the interview process to the participants and solicited their consent to record the session and use transcript of one of their recent meetings to generate meeting recap and study its usefulness. We had informed participants in advance about using one of their meetings to generate the recap and study its contextual usefulness so that they could record a meeting prior to the study after seeking consent from the participants of the respective meeting. All the participants' recorded meetings that they brought to the study were between one to two weeks old. We started the interview by asking the participants to copy-paste the transcript of the meeting that they had decided to discuss, and hit process to start generating the recap experiences. We did this in the beginning because the system took several minutes to generate the recap and we used that time to ask preliminary questions. The prototype did not log any user data, and we did not have access to the contents of the meeting transcripts that participants chose to use for the study. We also asked participants to share their screen during the study so we could observe their interactions with the recap experiences as we proceeded through the interview. 

\change{We designed the interview questions so that we could 1) Understand people's prior notetaking practices, 2) Capture the opinions of the participants when making sense of the recap, 3) Observe how participants interacted with the recap to validate their understanding of the meeting, and fix any issues they came across. Observing participants perform tasks instead of simply eliciting problems about the recap from them evokes higher order cognitive capabilities~\cite{cotton2006reflecting} and is expected to give data that is closer in alignment to their actual understanding of the recap.} Thus, we divided the interview questions into three sections -- 1) General participant background and meeting habits, 2) Questions and tasks for the \emph{highlights view}, 3) Questions and tasks for the \emph{hierarchical view}. For both highlights and hierarchical sections we ask participants questions about the i) general usefulness of the experience, ii) missing summaries and the reasons, iii) inaccurate and irrelevant summaries and the reasons, iv) sharing the summaries. For missing, inaccurate and irrelevant summaries, we also ask participants to interact with the UX and add, edit or remove the summaries respectively and think aloud through the process so that we get insights on the reasons for participant's actions. \change{To get natural insights, we made sure to let the participants explore the UX on their own, and only direct them when absolutely necessary (e.g., they get stuck with an interaction).} Think aloud~\cite{duncker1945problem} is a popular HCI method to elicit user expectations, and capture the thought process of participants as they interact with the output of a system. We provide the complete question set that we used for the interviews in the supplementary material. \change{The study was reviewed and approved by the institution's review board.}

%\ah{we can talk about the limited perspective from the utility of these experiences - we're getting at something more general, but somewhat a limitation of our work. majority were researchers or research adjacent, some were mnanagers, some were individual contributors}

%See the this paper's supplementary materials for a full list of interview questions. 

\subsection{Analysis}
After each interview, one of the authors went through the recordings and transcripts and added any missing notes to those taken during the interview. \change{At the end of all the interviews, we had about 7 hours of recordings, associated transcripts, and notes taken during the interviews.} We analyzed the interviews using thematic analysis~\cite{braun2006using}. One author went through the transcripts line by line and assigned open codes, additionally using the notes taken during the interviews. \change{Following established coding practices, we inductively assigned codes to reflect what participants said, and did during the interview, and avoided codes that reflect prior understanding of the coder~\cite{linneberg2019coding, mihas2019qualitative}}. Our codes captured the general opinion of the participants with respect to the usefulness of the experiences (e.g., ``this is useful''), how they imagined using the recap (e.g., ``planning upcoming tasks''), and their challenges associated with using it (e.g., ``ambiguous note''). We also made sure to code participants' interactions with the UX during the study to understand how they modified the summaries to align~\cite{christian2020alignment} it better with their own preferences and what UX interactions enabled them to do so (e.g., ``looks up context'', ``edits summary'').

\change{After coding each interview session, the primary author refined the codes through discussions with another author to resolve any disagreements. At the end of the coding session, we had about 143 codes that reflected various dimensions of recap that we wanted to study. We grouped the final set of codes into themes using affinity diagramming~\cite{holtzblatt2009contextual}. We performed the affinity diagramming step using Microsoft Excel.} 
% https://arxiv.org/ftp/arxiv/papers/2205/2205.05661.pdf
% familiarization phase - listen to recordings and transcripts
% assigned open codes staying close to the data. Iteratively revised codes with another author resolving disagreements through discussion

\section{Results}
%\ah{sitting down and writing down the discussion of the results will help us}
%\ah{move the results introduction, to discussion, and make it more about what we learn from it, rather than presentation of facts}
%All participants agreed that both the experiences were generally useful. While the participants found issues (e.g. 4 found issues with pronouns being wrong and nearly everyone (6) pointed out issues in the wording of some chapter headings) participants did not report these issues to be severe.  Four of our participants struggled to add action items and notes in the \emph{highlights view} without going back to the source material (transcript).  Many participants also reflected how the \emph{hierarchical view} helped them remember the meeting and what happened (6) (situatedness), so deploying this type of experience might help users show us what our AIs missed more easily â€“ thus increasing recall (informativeness).  Most found the pattern of selecting highlights from the \emph{hierarchical view} to be intuitive (4) but others were confused by the meaning of the â€œstarsâ€ so other interaction patterns for selecting highlights would be worth exploring.

\begin{table}[h]
    \centering
    \colortbl \begin{tabular}{|p{1.4cm}|p{11cm}|}
        \toprule
        Participant & Meeting description\\
        \midrule
        1 & A 60 minute long research project meeting. The attendee was a research intern and the project lead. \\
        2 & A 45 minute long talk on a topic of Computer Architecture, with a 15 minute Q\&A.\\
        3 & A 60 minute long research update between a research mentor and a research intern.\\
        4 & A research group meeting with the agenda of "paper submission" amongst two research mentors, and a research intern.\\
        5 & Kickoff meeting for a project between two teams, one research team and one product team. The agenda was to introduce collaboration between the research and the product team. The attendee was from the research team and attended the meeting while driving a car. Was only partially able to pay attention to the meeting.\\
        6 & A 60 minute long weekly research update between a research mentor and a research intern. \\
        7 & Discussion of analyses, discussing survey results, atleast 3 people in the meeting. Atleast 2 interns and atleast one mentor. \\
        \bottomrule
    \end{tabular}
    \caption{Overview of meetings}
    \label{tab:meeting_details}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|p{3.7cm}|p{1.6cm}|p{4.3cm}|p{1.6cm}|}
        \toprule
        Construct & Number of participants & Construct & Number of participants\\
        \midrule
        Takes notes & 7 & Should take more notes& 2\\
        Keyword notes & 1 & Notes focus on ToDos& 4\\
        Digital notes & 7 & Recordings too long& 2\\
        Physical notes & 2 & Transcripts too long& 1\\
        Notes in chat & 2 & Notes help plan next meeting& 3\\
        Notes in Gdoc & 2 & Recaps with collaborator& 3\\
        Records meetings& 4 & Recaps with transcript& 3\\
        Shares notes& 6 & Recaps with recordings& 5\\
        Shares tasks& 2 & Recaps with slides& 1\\
        Shares slides& 0 & Recaps with chat& 2\\
        Cleanup before share& 1 & ASR issues& 1\\
        Collaborative notes& 4 & Task tracking& 4\\
        Agenda driven meeting& 1 & Recap at end of meeting& 1 \\
        Ask to record& 1 & & \\
        \bottomrule
    \end{tabular}
    \caption{Overview of meeting recap habits}
    \label{tab:general-meeting-habits}
\end{table}

\subsection{General meeting habits \change{and meetings overview}}
\change{Figure~\ref{tab:meeting_details} provides an overview of the meetings of our participants.} Generally, all participants reported that they take notes in one way or another during meetings.  All took some version of digital notes while \change{two} participants also took physical notes with pen and paper. \change{Four} preferred putting notes into the chat or using a collaborative document format like google docs. \change{Six} participants also shared notes with others sometimes. \change{Four} participants suggested that their note-taking generally focused on To-Doâ€™s and task tracking. 

Participants reported several different strategies for recapping meetings: 1) \change{Three} asked someone who attended, 2) \change{Three} Used the transcript, 3) \change{Five} Watched recording 4) \change{One} indicated reviewing presented slides and 5) \change{Two} reviewed chat. \change{These numbers are not mutually exclusive as each participant indicated multiple strategies for recap depending on their needs and time availability.} Table~\ref{tab:general-meeting-habits} provides a summary of general meeting habits across all participants.

\subsection{Highlights view}
\subsubsection{General benefits and issues} \hfill

\leadin{Action items and reminders} Four participants pointed out that suggested summaries served as reminders of what happened in the meeting.  Participants indicated using the summaries to decide their action items for the next week's meeting if it was a recurring one. They also said it helps them plan out their action items for the upcoming week if they had many action items, and prioritize them. Some participants already had a good understanding of what they needed to work on, but still found the summaries helpful as \textit{reminders} to go back to when unsure. 
\texttt{P05} said
\begin{displayquote}
It is helpful to remember.  They are accurate.  Jogging my memory about the content of the meeting.  The task is helpful.  It reminds me that I can go look at the email about the meeting to see something I missed.
\end{displayquote}

\leadin{Relevant notes \& action items} %The highlights model was designed to be brief and focus on the most important needs of the user. Due to brevity, we found that sometimes the included summaries were not relevant to participants, and it missed out on the relevant aspects of the meeting. 
\change{Six} participants indicated that the generated higlights were not relevant to them. The model either did not capture notes from the relevant section of the meeting, or it captured action items that were low priority for the participant. e.g., in status update meeting, each participant discusses their own status sequentially and the status of one participant is not always relevant to others. To easily see discussions of relevant people, \texttt{P02} also suggested to \textit{group the summaries by people} so that they can pick the people relevant to them and see their discussions. Further, participants indicated that tasks assigned by their manager were high priority action items that they needed to address urgently, and termed it as ``main task'' (P03)--other deliverables could be addressed later. 

\texttt{P03} reported
\begin{displayquote}
Itâ€™s not that good because the AI tells you to do two things â€“ one tiny task. A detail. Like, this detail is not correct, you have to fix after the meeting. It is not the main task. If these kind of tasks are extracted, there will be plenty of to-do list and I might miss the important ones.
\end{displayquote}

On finding less relevant summaries at the top, \change{four} participants also expressed the need for reordering them based on \textit{how much the feedback is useful for them} (\texttt{P02}) for their workflow. They requested having the most relevant notes \& action items right at the top so that they can get a quick overview of the most relevant items when in a hurry.

\leadin{Process oriented discussions around summaries} %One interesting drawback that we identified with 
\change{Two participants} indicated that the \emph{highlights} experience is that discussion process is not well captured. This is because this experience focuses on highlighting key moments (high precision, low recall), and does not have an affordance for including discussion that lead up to that moment. For example, \texttt{P04} mentioned not seeing a discussion around a paper submission.
\begin{displayquote}
\textit{We had a lot of discussion about what needed to be done. [...] If I were taking notes, I would have written down something to that effect. [...] Ultimately, we decided to work with the smaller dataset. No notes about any of that discussion. [...] It would have been nice to see notes around that.}
\end{displayquote}

\leadin{Context of summaries} %In certain cases, the generated summary did not capture the entire context of the discussion, and participants looked for more information to understand the summary.
\change{Two participants indicated the need for more context to understand the summary}.
This was most prominent when the system generated summaries for a recorded presentation, and the context is a combination of utterances, as well as visual information on the slides. Current models only used transcripts to generate summaries making it difficult to summarize discussions that referred to presentation visuals.\texttt{P04} noted 
\begin{displayquote}
If there was a way to link to the meeting for each of these notes I could watch.  I canâ€™t scroll back in the context, so it would be great if I had the whole transcript in the context and link to the video.  I may want some more detail.  If I can jump to the transcript or video, I can get that detail.  
\end{displayquote}

To get more context for ambiguous summaries, \change{two} participants explored the ``show context'' feature of the UX. However, three other participants still requested a link to the original video or the full transcript to get more context. They mentioned that the limited context \emph{did not help them understand completely and it would be helpful to look at the original discussion for clarity}.  \texttt{P01} notes
\begin{displayquote}
I'm not sure what it is referring to and what it means in the context we were talking. [...] (Participant expands context)  Oh! I remember what happened. A teams bug and the team couldn't see the slide deck. [...] So maybe 1st part got confused with 2nd part. Even for a human it looks strange.
\end{displayquote}

Additionally, while examining the context of an action that mentioned someone's name \texttt{P04} wanted to know why the name was mentioned:
\begin{displayquote}
I donâ€™t see anything in the context about sending out spreadsheets!  I see where the ``<name>'' came from.  We referred to his project and named the survey after their name.  That does help in understanding how the name got in there.  [...] 
This is really cool.  I love that I can see the context of where it was drawn from.  
\end{displayquote}

\leadin{Pronoun issues} Four participants indicated that the model generated summaries with their names associated with the wrong pronoun.  Participants had varying feeling towards such errors, ranging from attributing it as a \textit{minor point} (\texttt{P01}) to noticing and point them out, or even feeling uncomfortable about it. This was more frequent with non-Western names, in which case participants felt bad that the model behavior was less inclusive. % TODO: cultural context discussion 

% P01 said: 
% \begin{displayquote}
% [...] pronouns are not perfect but thatâ€™s a minor point. 
% \end{displayquote}
\texttt{P06} reflected on the value of the summaries as well as the pronoun issues they saw
\begin{displayquote}
Even just looking at the headers, I remember much more about the meeting at a glance.  The sentences have errors and the pronouns are wrong.  
\end{displayquote}


%\leadin{Priority ordering of notes} 
%[discussion] Systems can learn user's preference from re-ordering of notes, and personalized summarization models can use this preference to tailer to the notes experience to each individual meeting attendee. E.g., the learned relevance can be used to present notes in priority order, as well as a high recall experience, showing only the most relevant notes. 
% P02 notes
% \begin{displayquote}
% Can it prioritize the notes?   
% [...]
%  It would be valuable in that it is ordered by how much the feedback is useful for me. 
% \end{displayquote}

%\ah{call highlights instead of ai insights}



\subsubsection{Add, edit, delete} \hfill
\leadin{Adding and editing notes holds consistent meaning} All participants agreed that the intention behind adding and editing notes \& tasks was consistent (high alignment) in meaning. The notes that participants added were personally important to them~(4), a discussion to remember~(6), capture general topic/hierarchy~(4), or to add details to another note~(5). \texttt{P03} explained while adding a note from memory
\begin{displayquote}
For the notes, I added them according to the timeline in my memory.  In the beginning of the meeting, we discussed the result/trend.
\end{displayquote}

\texttt{P03} also reflected on adding notes that are important to them
\begin{displayquote}
I remember these because they are important parts I needed to do.  
[...]
Iâ€™m writing down the mistakes I made, the things I need to correct, and the main task.  
\end{displayquote}

While adding a missing note \texttt{P04} remembered of an task but was unsure if it was worth adding
\begin{displayquote}
As Iâ€™m writing, I think there could be a task there.  Itâ€™s our path forward, but Iâ€™m not sure it needs to be a task. 
\end{displayquote}

Regarding editing notes, two participants raised concerns about the AIâ€™s ability to learn from their edits because they used external knowledge to make the edits. Five edited to add relevant context~, three fixed grammar issues, one fixed a pronoun issue and two included external information. Six participants were comfortable adding or editing summaries even if they were unsure or did not clearly remember the meeting details.  For the likelihood to edit (quantity), two participants indicated that they would be more likely to edit if the summaries were collaborative, two were more likely to edit if the AI would learn from their edits~(2) or if the summaries were of decent quality. Four participants also referenced non-audio meeting content such as presentations when making edits. 

\change{Three} participants made minor grammar edits like \textit{inserting verbs} (\texttt{P01}) into the summaries, or major edits like rephrasing the note or task to make it \textit{more actionable} (\texttt{P03}). \change{Four} participants made edits that changed the order of summaries without changing the content, like \textit{moving less relevant notes to the end} (\texttt{P01}) to help them review the summaries better. Participants made considerable use of the context when editing notes, added external context like ``\textit{<show on screen>}'' (\texttt{P01}) to refer to non-audio content and indicating that the likelihood of editing notes depends on the quality. e.g., \texttt{P04} stated that they would edit the notes if they satisfied a quality threshold
\begin{displayquote}
Itâ€™s easier to fix or clarify these mistakes than to start from scratch. If it takes less time for me to fix and correct stuff, then Iâ€™m going to do it. If its so bad that it takes more time, Iâ€™ll probably stop using it.
\end{displayquote}

% P01 learn words that are related to concepts in the meeting
% P02 - AI should learn how to get the sentence right.Â  It can learn from the new sentence.
% P02 - i would only edit it once after the meeting
% P03 -  I remember these because they are important parts I needed to do. Iâ€™m writing down the mistakes I made, the things I need to correct, and the main task.Â Â 
% P03 - The AI should learn that a note I added represented a very long part of the discussion in the meeting.Â Â 
% P03 - If I edit something, the AI did a good job identifying where the action item was but the summary was wrong.  So it should use my edit to become more accurate.
% P04 - It would be great if it could learn how to summarize this discussion.Â  This discussion took place over 20 minutes.
% P04 - Iâ€™d do that pretty often.Â  If I just need to add to what it populates, it should take less than 5 minutes.Â  If it is helping me, and it starts getting better, Iâ€™m happy to do it.Â  It would be easier right after the meeting.Â  It would be easy to do this.Â  At the end of the meeting we have a summary, what was discussed what are the next steps, so it would be easy to do it.Â 
% P04 - Even if I donâ€™t remember perfectly, I would add it.Â  Ideally I would do this right after the meeting.Â  If Iâ€™m not positive, I might indicate that in the notes like â€œI think we decided this and suggest someone can correct me if Iâ€™m wrong. â€œ
% P04 - Easier to fix notes than it is to write notes.Â  You donâ€™t need to remember what happened.Â Because with adding you have to first recall what happened in the meeting, then write it, with fixingÂ  its already there.
% TODO: adaptive interactions based on their likelihood to edit

% \texttt{P03} explained while editing an action item
% \begin{displayquote}
% It needed context to make the item more actionable. Now I can delete the second one that I added. 
% \end{displayquote}

% While making edits, P01 notes
% \begin{displayquote}
% The ordering of notes suggests priority, but it looks like it is chronological.  I might move the less relevant notes to the end.
% [...]
% Moving a note that is low priority to an action item helped me see that it is actually high priority. 
% \end{displayquote}

%<quote add reason: include decision>
% P06 reflects while adding context to a task
% \begin{displayquote}
% When I look at the context, Iâ€™m able to fix.  It is accurate but it didn't have enough context. 
% \end{displayquote}
% P01 said while reflecting on reasons to edit (fix grammar):
% \begin{displayquote}
% But it only needed minor edits. <inserts the verb â€œincludeâ€ to a note> What was happening here was unclear.  This is fairly understandable.  Maybe Iâ€™m used to AI generated stuff. 
% \end{displayquote}
% P01 also added ``<shown on screen>'' to a note to reference non-audio meeting content. 

\leadin{Deleting has inconsistent meaning} Deleting notes and tasks was more complicated. \change{Two} participants reported that they might delete a task when it is done, \change{three} reported they might delete a task if it is redundant, and \change{two} others reported they might delete the task if it is inaccurate. Two said they would not delete any notes or tasks without a very high level of confidence. \change{Five} participants generally found it straightforward to edit notes or tasks but only one participant deleted a task. One possible explanation for this is that deleting makes the summaries go away, so participants are more wary to delete, than adding or editing, where the content is still preserved. e.g., \texttt{P03} reflected that completed, wrong or redundant action could be misleading.
\begin{displayquote}
If I see an inaccurate note or something I have already done, I will remove it.  The wrong actionable could be misleading. I might delete redundant actions.  I might insert a new task with the right meeting and delete the task that was summarized wrongly.  
\end{displayquote}

\texttt{P07}'s comment brings out the contrast between lack of a consistent meaning for deleting notes and a consistent meaning for adding or editing summaries 
\begin{displayquote}
For editing, I wonâ€™t wait until Iâ€™m super confident.  

For delete, Iâ€™d be more careful.  I might delete something that is important to others.  If it learns from my deletions, Iâ€™m worried about that. 
\end{displayquote}

\leadin{Recall difficulty when adding or editing notes and tasks} \change{Four} participants were able to add a note or a task from memory or using the context in UX but \change{three of these four} needed to reference the raw transcript (that they had just copy-pasted into the prototype) in order to think of something they might add. %P04 got confused by the name of someone unexpected showing up in the summary, and used the context to understand that the name of the person in the discussion, and that they themselves had referred to a survey as â€œ<name> surveyâ€ once in the discussion. 
%[discussion] Prior work has also found that meeting attendees find it difficult to accurately recall context, so this has implications for systems learning from user feedback in notes, such as incorporating uncertainty or confidence of feedback. 


%\ah{people report recalling the meeting was way easier for hierarchical-}

\subsubsection{Sharing} \hfill
%All participants agreed that sharing usually means quality in that they would fix/edit before sharing, but not always.  E.g., some participants pointed out that they would be willing to share imperfect notes/tasks with teammates and peers while they would spend time making sure the notes/tasks are accurate before sharing to managers or sharing with people they were not close to. 
%Most of the participants specifically asked to share to a collaborative canvas (many referred to gdocs or a fluid component) (5).  Most wanted to share via meeting chat (5) while some wanted to share via email (3) and others mentioned sharing directly to a task management tool (3).  Many pointed out that they might want to only share some tasks/notes â€“ not the whole set (4).  Further, others mentioned that they wanted to leave comments directly on notes/tasks like commenting on a word document (4).
%Only one participant immediately found the share button in the upper right.  Many tried to copy and paste what was on the screen or take a screenshot to paste it into an email.  
%Participants mentioned that they would ensure quality when sharing notes, but this behavior is dependent on the hierarchy of the person they share with. Participants were okay sharing imperfect notes with colleagues at the same level or below, but expressed that they would edit the notes to fix quality if sharing with their managers. \sumit{should this go at the end of the section as a summary?}


\leadin{Collaborative notes and sharing behavior} \change{Five} participants expressed the desire to collaboratively edit the notes and have their changes reflected for others to see. They viewed collaboration on the notes as a way to build consensus around high value tasks, transparency, and also help identify dependencies between tasks. e.g., \texttt{P03} noted that collaboratively available notes are helpful to transparently identify if their task requires someone else to finish theirs first.
\begin{displayquote}
In meetings that involve more than two people, there are dependencies. e.g., someone else needs to address a task before I can do my task. It would be nice to have this dependency.
\end{displayquote}

Moreover, participants indicated more willingness to contribute to collaborative notes. They indicated \textit{adding and editing them if it was shared and attached to the meeting} (\texttt{P06}) and \textit{putting more effort into them if it was needed to be shared with [Product team] collaborators} (\texttt{P05}). For more accountability, (\texttt{P06}) further suggested \textit{to assign people in notes, and notify via email to include people who might miss it otherwise}. %[discussion] Participants mentioned that they would ensure quality when sharing notes, but this behavior is dependent on the hierarchy of the person they share with. Participants were okay sharing imperfect notes with colleagues at the same level or below, but expressed that they would edit the notes to fix quality if sharing with their managers. This suggests that shared notes may yield a higher quality, given participants may engage with them more due to transparency. This is similar to open source platforms like Wikipedia, where visibility of edits can lead to different outcomes.
Another interesting sharing behavior we observed was the tendency to ensure a \textit{higher quality of the TODO list} if they are going to share (\texttt{P07}), and a higher likelihood to edit the notes if they are part of a shared meeting board
% \texttt{P07} reflects
% \begin{displayquote}
% I would be careful to make sure it is the right TODO list and we have consensus before sharing. 
% \end{displayquote}
\begin{displayquote}
If it was shared?  Yes.  If it was shared and attached to the meeting, I would consider it.
[...]
If it was shared or persistent and I was the meeting owner. I might consider doing it as a resource for other people. (\texttt{P05})
\end{displayquote} 

Further \change{two} participants indicated performing a more thorough oversight on the summaries before sharing if they shared it with their superiors \begin{displayquote}
Good enough but depends on who Iâ€™m sharing with.  If closest collaborator who was in the meeting...  If I wanted to share with <product team> collaborators, Iâ€™d put more effort into [editing the notes]. (\texttt{P07})
\end{displayquote} 
% \texttt{P05} said:
% \begin{displayquote}
% If I wanted to share with [Product team] collaborators, Iâ€™d put more effort into them.  
% \end{displayquote}
% \texttt{P06} said:
% \begin{displayquote}
% If it was shared?  Yes.  If it was shared and attached to the meeting, I would consider [editing and adding notes].  
% \end{displayquote}
% \texttt{P06} also said
% \begin{displayquote}
% In [collaborative editor] you can assign people and it goes out through email.  This might be more effective.  It could drag people into the notes that they might otherwise ignore. 
% \end{displayquote}

\leadin{Sharing modalities}
Only one participant noticed the ``share'' button on the top. Participants had different and strong expectations of sharing the notes with attendees and others. Five wanted to share via meeting chat while three wanted to share via email and three mentioned sharing directly to a task management tool. Four pointed out that they might want to only share some tasks/notes â€“ not the whole set. Four mentioned that they wanted to leave comments directly on notes/tasks like commenting on a word document. 

%Sharing behavior expressed by participant's also indicates interesting implications. Four asked for basic functionality for starting a conversation around a specific point raised in the meeting--sometimes to ask for clarity and sometimes to continue the discussion beyond the bounds of the meeting.  Two participants imagined the \emph{hierarchical} view as a document that could have comments added to it like a Word document or a Google document by highlighting a section of the text.  One participant imagined sending parts of the recap to a chat or email thread for a follow-up discussion. We hypothesize such a high interest in asynchronous collaboration with recap due to its familiarity with handwritten notes, and documents that are easily shareable without needing any additional technological affordance.

% \begin{displayquote}
% I would even share it if it was not 100\% right if they had the ability to edit as well.  I would look at it, edit stuff, and send it out to people.  I'd like a URL where someone else can make an edit and we can all see it â€“ better than sending an email.  
% \end{displayquote} 

\texttt{P05} imagined the \textit{shared summary document connected to people's organizational workflows} such as part of the meeting calendars so that it is easy for people to keep track of the summaries. Along similar lines, \texttt{P01} suggested that \textit{shared notes can also act as planning boards for teams, capturing team dependencies, and next tasks for the team}.
% \begin{displayquote}
% It could be valuable if it is tightly connected to people's note-taking flows/organizational workflows for projects.  It would be especially valuable if it could link out.  If it was integrated into how people prepare for and participate in meetings, I would do it.  
% \end{displayquote}

%[discussion] The need to tag people in collaborative notes, send out email notifications, and use them as interactive evolving documents suggests that meeting attendees want to use notes simply beyond meeting recap as a meeting artifact that can do X, Y Z.

%With regards to quality, participants considered ensuring notes are high quality when sharing with their managers, or for more formal important upcoming meetings (revise). When we asked \texttt{P07} about if we could assume that shared notes were all high quality:

% or \textit{a way to move this to a collaborative notes tool} (\texttt{P07}).

% \texttt{P01} suggested shared notes can also act as planning boards for teams, capturing team dependencies. 
\begin{displayquote}
In meetings that involve more than two people, there are dependencies. e.g., someone else needs to address a task before I can do my task. It would be nice to have this dependency.
\end{displayquote}
% \begin{displayquote}
% It would be great if it could just go into [collaborative notes tool].  
% \end{displayquote}

\subsection{Hierarchical view}
%All participants pointed out various imperfections in the chapter names, but all pointed out that they were still useful.   Most participants called out how the experience helped them remember the meeting better than the Highlights View (6).   The chapter headings helped them drill down into a relevant section of the meeting (6).  Some participants pointed out that the experience helped them understand the context of the notes/tasks extracted in the Highlights View.  Most immediately understood the meaning of checkboxes and stars (5) and some were able to add stars (4) and tasks (1).  Some expressed that this would be a good way to teach the AI what it missed (3). 

%Most participants expressed that the Hierarchical View captured how they imagined the structure of a meeting (5).  A majority also saw complimentary value in both the Highlights and Hierarchical views (5) (with three calling out specifically the value of Hierarchy for notes vs. Highlights for tasks).  One participant said they preferred only the Highlights View while four participants expressed a preference for the Hierarchical View. Two participants encountered non-rephrased action items in the Hierarchical View and were able to quickly and easily rewrite them without prompting by the interviewers (2).

%\textit{Topic segmentation and chapter headings useful}
\leadin{Chapters help get a quick meeting overview} One of the major reasons participants appreciated the hierarchical view was its ability to provide quick overview of the meeting. \change{Six} participants indicated this overview useful for quick recap. \change{This overview helped} a participant who \textit{missed the meeting, and wants to understand in five minutes } (\texttt{P04}). Topic segmentation and progressive details specifically helped participants easily navigate different parts of the meeting. It helped \texttt{P06} \textit{skip the poster session discussion, and scan the analysis discussion} that they cared about. 
% \texttt{P04} said:
% \begin{displayquote}
% We had an attendee who missed a meeting recently.  I think they could understand this in 5 minutes.  I love the progressive detail.  I donâ€™t care about the poster session so Iâ€™ll skip that.  But I do care about the analysis so Iâ€™ll scan that point.
% \end{displayquote}
Hierarchical view was more intuitive and \textit{looked more like a meeting to participants} (\texttt{P01}). \texttt{P05} was able to explore and understand the hierarchy on their own
\begin{displayquote}
This looks like a navigation tool which is more valuable.  [...]  This is two levels.  It seems right to me.  I see some duplication.  You have a main topic, a secondary topic, and then transcript. [...] This seems way more useful to me.  Even though there are errors, I can try to make sense of this.
\end{displayquote}

\change{Three participants indicated errors in the chapters, but it did not affect their recall when they looked at the chapter headings and skimmed the chapter contents. This suggests that chapters helped to aid participant's recall of meeting attendance beyond simply providing a record of what happened in the meeting} [disc].
%, which was even more useful if they \textit{missed something and wanted to get a quick overview} (\texttt{P06}). 
\texttt{P07} said
\begin{displayquote}
I like the chapters or headers.  Even just looking at the headers, I remember much more about the meeting at a glance. [...]
I like this more than [the highlights experience]. It gives more topics of what we discussed. It is more aligned with what we discussed.[...]
[The highlights experience] is good for giving a rough understanding â€“ a glance of the TODOs.   
I'd glance at [the highlights experience] and AI Tasks, then Iâ€™d go to the hierarchical and go into the sections we discussed.  It is easy for me to focus on the sections I care about. 
\end{displayquote}
%TODO: experience with error still holds value

%\textit{Hierarchical view better for missed meetings.}
% \texttt{P06} said:
% \begin{displayquote}
% If I missed a meeting, I like hierarchical the best.  Either I missed the meeting and I want to know all about it and I get the overview quickly.  Or it helps me narrow down. 
% \end{displayquote}

%The chapter titles provided participants an easy way to grasp the themes of the meeting~\cite{zacks2001perceiving}.
\change{We also observed that participant's recall of the meeting was impacted if the chapters contained more errors. Two participants faced difficulty in getting a quick overview because they observed many titles that did not make grammatical sense or were out of context}. \texttt{P06} noticed these errors as they had a number of poorly worded titles in their meeting recap
\begin{displayquote}
We have the accuracy problem.  Some are better than the others.  <Points out an accurate and important chapter>  The idea of these headers is very useful.
\end{displayquote}

\leadin{People think of meetings in hierarchy} %When  participants looked at the hierarchical experience, they understood it without any difficulty. 
% In some cases, participants even directly mentioned that they think of meetings in hierarchy. \texttt{P01} said 
% \begin{displayquote}
%  I can remember and understand [the hierarchical view] conceptually. [...] Hierarchical looks like a meeting.
% \end{displayquote}
\change{All} participant's manner of exploration of the hierarchical experience suggested a progressive, breadth-first exploration strategy. They would consider a relevant part of the meeting, locate its chapter heading (breadth scan), and explore the chapter by expanding it (depth scan), and looking at the notes that are part of the chapter.  If necessary, they would then expand a low level summary to see the raw transcript that was being summarized. [disc - organization and sensemaking - how error and sensemaking interact]

\texttt{P02} and \texttt{P04}'s words exemplify this approach
\begin{displayquote}
This (the hierarchical view) will definitely help me find whatâ€™s most helpful to me.  It reasonably aggregates by topic.  I might just want to see... <selects a section>.  With this [chapter], I can look at the context. (\texttt{P02})
\end{displayquote}
%\texttt{P04} said
\begin{displayquote}
Iâ€™m getting a summary flow of the meeting. [...] We had an attendee who missed a meeting recently.  I think they could understand this in 5 minutes.  I love the progressive detail. [...] I can understand this really quickly.  (\texttt{P04})
\end{displayquote}

\leadin{Participants could easily expand chapters to get more context} 
%(7) (drills down into chapter) 
The navigation provided by the hierarchical view also allowed participants to get more context if they needed to delve deeper into the subject (see Section~\ref{sec:prototype}). \change{Five participants used this context to expand discussions}, understanding who said what, and locating important notes and tasks. It helped \texttt{P02} understand the explanation in the Q\&A section of their meeting better
\begin{displayquote}
\textit{(Picks a star) It explains the main difference between one project and another. Compared to other things, Iâ€™d say this is the main difference. Oh this is important. (clicks the star bullet next to the other item) This is a very good question and answer, I donâ€™t know why this wasn't flagged at all.}
\end{displayquote}
\texttt{P04} first glanced at the headings, then drilled down on the second chapter to open the notes, then further into the notes to look at the context of the note
\begin{displayquote}
I like that it segments the meeting into topics of discussion.  I can hone into what I want.  If Iâ€™m trying to recall or share, they get a high level view in less than a minute.  Then you can drill down.  It turns a transcript from a sequential into a random access.  I can jump anywhere to get what I want.
\end{displayquote}


\leadin{Easy to locate important notes and tasks in chapters} Most participants did not face difficulty in inferring checkboxes and star as ``key points'' and ``action items'' respectively. \change{As participants navigated the chapters experience we} found that it allowed them to quickly favor navigation to chapters that contained more tasks and assign it to \change{relevant people}.
\begin{displayquote}
Iâ€™m choosing [this chapter] because it has a lot of action items.  [Picks an action item] This one Iâ€™d actually assign to [other person].  Itâ€™s accurate.  He said it himself. (\texttt{P03})
\end{displayquote}

Not just navigation, flagging important notes and tasks in the hierarchical experience allowed participants to quickly lookup the context by reading the prior and following summaries, helping them decide the relevance of the task more easily than the highlights view. \texttt{P05} opened the section with most tasks and mentioned 
\begin{displayquote}
Iâ€™m choosing [this chapter] because it has a lot of action items.  <Picks an action item> This one Iâ€™d actually assign to [other person].  Itâ€™s accurate.  He said it himself.
\end{displayquote}
% \texttt{P05} said
% \begin{displayquote}
% [Check boxes] looks like tasks. These look like main points. But why do some not have stars?  [Stars] must be main points.
% \end{displayquote}

Overall, we found that participants considered highlights view and hierarchical view as complimentary. While highlights helped participants quickly look at their tasks, they did not find the notes very useful due to their limited coverage. On the other hand, participants preferred the hierarchical experience for its ability to quickly provide a meeting timeline like experience (revisit).
P02 said
\begin{displayquote}
If I want to look at tasks, I would prefer the [highlights view].  Itâ€™s easy to look at it and add more tasks. 
[...]
If I want to read the summary of the meeting or identify some part of the notes to share, I would look at the hierarchical version. 
\end{displayquote}

%\ah{discourse acts are helpful, and highlighting them helped people [discussion]-}
 
%\textit{Full chapter based summary helpful for rewriting notes and action items}

\section{Discussion}
Our user studies indicate that all participants find both recap experiences useful for different reasons. Most of our participants found complimentary value between experiences for different use cases. For example, it was a common theme that the \emph{highlights view} was most useful for recap for meetings one attended, and \emph{hierarchical view} was most useful for meetings one missed entirely. \change{This confirms prior findings of varying recap needs~\cite{whittaker2008design} and} also aligns with Nathan et al.'s~\cite{nathan2012} finding that annotations regardless of whether it was their own or others helped recall. 
%Participants found the highlights experience useful for getting a list of action items, while participants found the hierarchical experience useful for getting a quick overview of the meeting, despite the inaccuracies. The hierarchical view was more favored for recall as it helped participants quickly glance over the sequence of chapters, and notes within chapters to recall what was being discussed. Most (4) found the pattern of selecting highlights from the Hierarchical view to be intuitive but others were confused by the meaning of the "stars", so other interaction patterns might be worth exploring.%\ah{cite from acm magazine-}
%\sumit{connect it with prior work - what did other papers find in the literature with respect to meeting recap}
\subsection{\change{DR1}: Personal note-taking / Highlights view} 
When describing our design rationale for the \emph{highlights view}, we argued that a meeting recap should be as short as possible and focus on outcomes to serve the users' needs efficiently -- mimicking personal note-taking.  Participants generally agreed that they preferred this approach for recap when they were reviewing a meeting that they had attended--specifically noting the value of being able to see tasks at a quick glance.  

\change{However, determining relevance was a big issue for ML models when picking and choosing what to represent in a summary.} Some participants noted that main points of discussion were not included in the suggested notes while for some participants, the model generated no notes when it couldn't detect any utterance as important with high enough confidence. Some participants also reflected on their expectation that notes and tasks would be presented in priority order and their desire to re-order the notes that we suggested to them. \change{To show the most relevant summaries when aiming for the most important items, models need to personalize to individual user needs to provide a good recap experience.}

The interactions we provided for users show promise in gathering high quality training data to further align~\cite{christian2020alignment} the models with users expectations.  Users found editing notes and tasks straightforward--especially when using the ``show context'' option to see the raw dialog, and users reported that the meaning our models should learn from their work was consistent (high alignment).  However, users struggled to add notes that were missing without doing the difficult work of going back to the transcript to remember what happened in the meeting (situatedness).  This suggests that they would be less likely to add notes and tasks that our models missed, thus their interactions would be less \emph{informative} for improving the recall of \emph{key point} and \emph{action item} detection.  The concerns raised about deletion of notes and tasks suggest that more work is needed in order to find a consistent way for users to express when an item should not have been included in the highlights.

\subsection{\change{DR2}: Meeting minutes / Hierarchical view}
When describing our design rationale for the \emph{hierarchical view}, we argued that a meeting recap should summarize the entire meeting including discussions and outcomes within a hierarchical structure (drawing from Zacks et al.\cite{zacks2001perceiving} and the practice of generating \emph{meeting minutes}~\cite{chiu2001liteminutes}) to enable broad use cases and contextual navigation.  Participants were generally positive about the hierarchical view and some even specifically said that the experience reflected how they imagined a meeting--reflecting the insights from Zacks et al.\cite{zacks2001perceiving}.

Participants found the process of drilling down into the meeting summary to be intuitive.  Despite the limitations in the model's ability to generate accurate titles for each section that made sense to participants, they found them to be close enough to support their information seeking needs--allowing them to identify and expand low level summaries of the meeting that were relevant to their recap needs.  It was apparent that the complete summary also helped participants put the ``key points'' and ``action items'' that the system flagged into the context of the discussion from which they arose. 

The interactions we provided for users in this situation also show promise for gathering useful training data.  Users found editing of summaries to be intuitive.  They were able better understand the tasks we summarized for them in the context of nearby low-level summaries.  In this case, adding a note was not relevant, but selecting or unselecting highlights from the meeting mostly intuitive.  Again participants reflected on being more likely to do so if their work would benefit others, but many also reflected that they would correct the which summaries were highlighted if they felt that the AI behind the highlights would learn quickly from their corrections. 

\subsection{Design implications}
Our evaluation points to several important considerations for better meeting recap systems and designing scalable evaluations.

% Past work called for a system that does this, we implemented a system that does this, this is from a long line of cscw work.
% Dialogue summarization systems enable a level of automation in recap systems that were previously impossible and yet were called 
% Hybrid approach - complementary user needs are better satisfied if we were to 
% Fact that dialogue summarization makes a document, enables a lot of async collaboration.
% Recall difficulty proble,
% Pronoun problem.
\change{
\leadin{Dialogue summarization improves recap sense-making} Dialogue summarization enables a level of automation in recap support that was previously impossible but yet called for~\cite{whittaker2008design} -- generation of contextual summaries that resemble human note-taking. Prior meeting recap approaches that relied on raw transcripts were difficult for meeting attendees to use due to information overload. In contrast, all our participants found the generated summaries to be understandable, despite imperfections and found them helpful when recalling parts of their meeting. This allowed our participants to focus on higher order needs with the recap, such as how they would use the recap to plan their upcoming tasks, or the possible ways in which they might share the summary to collaborate. Such observations were limited in prior studies where the cognitively demanding task of sensemaking from transcripts prevented participants from thinking how the recap could apply to their work~\cite{whittaker2008design, kalnikaitundefined2008}.}

\leadin{A hybrid approach}
One of our key insights from comparing the \emph{highlights} and \emph{hierarchical} experiences was that they were both valuable in different ways complimenting each other.  As mentioned earlier, many participants reflected that the quick access to tasks was valuable to them when recapping a meeting they attended while seeing the whole meeting summary was useful for meeting that they did not attend. \change{Highlights requires models to understand personal relevance to generate the most important summaries, while hierarchical generates minutes from the full meeting, so it serves as a common document for all attendees. Nathan et al~\cite{nathan2012} and Whittaker et al~\cite{whittaker2008design} both hypothesized that a personal summary could act as a personal todo list, while a group summary could be useful for collaborations and public contractual obligations. This exploratory finding also aligns with cognitive fit theory~\cite{speier2006influence} that the most useful representation of the summary will be the one that matches the structure of the task that the participants hope to achieve from the recap.}

\leadin{Access to source dialog and audio/video supports recall}
All participants used the ability to expand the ``context'' of a summary into the original dialog in order to make sense of a suggested note, suggested task, or a low-level summary (see Figure~\ref{fig:experiences_ux} for the context affordance).  Many participant also requested deep links to video recordings to quickly review the actual discussion. These observations agree with prior findings on \change{audio indexes explored by Geyer et al and Moran et al~\cite{geyer2005towards, moran1998} that enabled participants to make better sense of their notes when they were associated with timestamped audio recordings. Access to original meeting artefacts is also helpful to enable participants to correct any issues with the model generated summaries. Whittaker et al~\cite{whittaker1994} showed that access to verbatim speech in the meeting browser was found to boost participant's confidence in their own recall, as well as helped them correct any mistakes in their manual notes.} %So meeting recap systems should include easily accessible  deep links to transcripts and video recordings (e.g. \cite{topkara2010tag}) from any summarized content. 

\leadin{Non-text artefacts like visual content enhances sense-making}
Dialogue summarization does not capture other meeting artifacts like presentation slides, hand gestures, whiteboard illustrations--\change{all of which have been shown to contribute to sense-making~\cite{moran1997ll} in the very early meeting recap browsers explored previously.} We saw at least one participant intentionally reference non-speech meeting content in their explorations of our system. \change{Combining dialogue summaries with additional artefacts like slides, attached files can provide additional value that early meeting browsers could not identify because of the distractions produced by using utterances as recap units~\cite{topkara2010tag}.}

%\leadin{Opportunities for asynchronous collaboration}
\change{\leadin{Meeting recap as a natural artefact for collaboration}
Prior meeting recap systems~\cite{ehlen2007meeting, banerjee2005necessity} focused on effective individual recap, but their collaboration aspect was under-explored. This is because hybrid interfaces comprising text, audio and video elements or summaries represented as a collection of important utterances from the meeting do not lend themselves well to sense-making and collaboration. As an exception, Geyer et al~\cite{geyer2005towards} designed a customized system to associate domain specific artefacts with indexes in meetings for later collaboration. However, their evaluations highlight the difficulty of using the recap system's artefacts outside of the system due to recap being part of a highly customized interface.}

\change{
For the first time, through our system, we were able to generate recap in the form of a document, that resembled how a person would take notes, and very similar to text documents authored in organizations~\cite{samuli2003}. Thus, all our participants expressed their desire to engage with meeting content after the meeting took place. Their expressed sharing behaviors has implications for design such as starting a conversation around a specific point raised in the meeting--to get clarifications and extend discussions.  Two participants imagined the \emph{hierarchical} view as a document that could have comments added to it like a Word document or a Google document by highlighting a section of the text.  One participant imagined sending parts of the recap to a chat or email thread for a follow-up discussion. We hypothesize such a high interest in asynchronous collaboration with recap due to its familiarity with handwritten notes, and documents that are easily shareable without needing any additional technological affordance.}

\change{
Using meeting recap as a collaborative document facilitates the use of the meeting in the context of increasing remote work and cross-timezone collaboration.}  With such functionality, it could be possible to both miss a meeting and be an active participant in the discussion started by that meeting through consuming the recap and continuing the discussions asynchronously~\cite{richter2001}.  

\change{
\leadin{Privacy and transparency implications} Meeting recap supports an inherently collaborative workspace. Transparency imposed by meeting recap can change existing work practices~\cite{smith2020}, a dimension previously overlooked in recap studies. E.g., while a mis-attributed action item is not bad for individual recap, wrongly attributed action items could impact group dynamics and obligations. Moreover, our participants indicated disappointment at being referred by the wrong pronoun, which when made transparent to the group in a recap can be even more detrimental, especially for culturally sensitive groups~\cite{branham2014co}. Two of our participants indicated concerns with recording meetings to generate recap. Since meeting recap makes meeting discussions explicit, permanent and easily shareable, it has the potential to change work obligations similar to how algorithmic support to Wikipedia processes changed Wikipedia work practices~\cite{halfaker2016ores}. Anyone can easily look at the recap document to assess work and recap documents as collaborative artifacts implies that people's group discussions could be more broadly accessible~\cite{kling1992cscwprivacy}. Future work should understand how meeting recap technologies can consentfully integrate in work practices. }


\begin{table}[h]
\centering
\colortbl \begin{tabular}{|p{6cm}|p{7cm}|}
\toprule
User behavior & Implication for system \\
\midrule
User edits summary & Summary item after the edit is better quality \\
User shares summary & Summary item is important to user \\
User opens a section in hierarchical view & Section is relevant to the user \\
User looks up source dialogues for summary & Summary item is relevant to the user, and possibly lacks full context\\
User deletes summary & Summary could be non-relevant, wrong or poorly written\\
\bottomrule
\end{tabular}
\caption{User interactions with the UX that can be used to provide feedback signals to the model on summary quality.}
\label{tab:d2l}
\end{table}

\leadin{Designing to learn}
  The models we used to construct these recap experiences were all trained on crowd-sourced data--data gathered by asking crowd workers to summarize and label ``key points''/``action items'' from meetings that they did not attend.  There will always be a gap between the understanding of someone who attends a meeting and is part of the social context in which the meeting was planned and executed and someone who is merely reading the transcript. Understanding meetings through transcripts is akin to the problem of grounding and overhearers~\cite{schober1989understanding} which suggests that labelers will never perfectly capture what participants understand because conversations utilize implications and grounding.  Thus models built on the labelers they produce are rarely well aligned~\cite{christian2020alignment} with the intended use~\cite{yang2019clinic}.  At best, current AI systems trained from a crowd-level understanding of relevance yields generalist models, \emph{i.e.}, they work well on instances of high consensus, but their performance will go down if users disagree on instances~\cite{yang2017role}.

Systems can re-train and get better from in-context data if the feedback is of high quality~\cite{ehlen2008}. %Designing with a feedback loop that engages a user in-context affords a way for models to improve from contextual data from real users who are interpreting a meeting through a relevant social context. 
%Asking users for explicit feedback demands more of their time, and is infeasible given the demands of the amount of feedback.
ML models can learn from in-context interactions but the interactions may not always have consistent meaning (alignment). We found this in our interviews with participant's indicating varying degrees of consistency in meaning for adding, editing, and deleting suggested notes and tasks. All participants agreed that adding notes or tasks had consistent meaning (alignment), i.e, they will add notes/tasks or tasks that is relevant to them and that the AI should learn to produce.  Editing notes/tasks also had consistent meaning for them, implying that participants would edit notes to more accurately reflect the discussion and that the AI could learn from their cues to do better as well.  Most participants suggested they would be more likely to edit the notes if their edits benefited others or if they were planning to share them out with the group (quantity).
 
Deleting notes was more nuanced for participants, as participants expressed various reasons for deleting the notes -- not relevant to their task, unclear to make sense, or low relevance. Therefore, when designing an AI to learn from user's actions it is useful to first elicit the reasons behind user's actions. Feedback from addition, and edits to the summaries could be directly used to re-train models to improve precision, recall, and the accuracy of summarized dialog. But feedback related to items that should not have been suggested as note/highlights or should not have been called out as tasks should be gathered in a way that makes the user intention less ambiguous.  Table~\ref{tab:d2l} summarizes possible user behaviors and their implications for improving the quality of the system from feedback.
% Talk about delete in table
% disc - multiple representations when designing for complex problems


%For example, feedback on whether the task was low relevance, or incorrectly worded can suggest which model in the pipeline should the example be used during retraining %(extractive or abstractive, elaborate more on this...).

%From a design to learn perspective, we have some clear conclusions.  All participants agreed that adding and editing notes/tasks had consistent meaning (alignment).  They also agreed that situations where notes were shared or otherwise collaborative would motivate them to fix them (quantity).  However, deletion was much more complicated with varied meanings that might be hard for an AI to differentiate, so it seems that providing the user with a way to clarify their intent might improve our ability to learn from these interactions (alignment).  Generally, participants seemed open to the idea of doing extra work to help the AI learn â€“ so long improvements due to their work were seen relatively quickly.  So, asking â€œwhyâ€ for a deletion might be well received.

% Collaboration
%\sumit{discussion on sharing}
%A set of participants also expressed the need to have document like commenting and collaboration on the notes and tasks. This suggests the potential for meeting notes to act as collaboration artifacts to foster ideation, and achieve consensus on to-do tasks. Shared meeting notes may even be a an effective means of collaboration, as participants have fresh context right after the meeting.

%The language models~\cite{lewis2020} that generated the summaries point to several directions of improvement. Participants found the headings in hierarchical experience to be sometimes inaccurate which is a result of limited training data, and training with crowdworkers[]. Others, such as misattributions or wrong pronouns are common issues that current Language Models suffer from, and need a more fundamental approach to solve~\cite{limisiewicz2022don}. Generating the most precise notes is related to the problem of grounding and overhearers~\cite{schober1989understanding} where the AI attempting to summarize a meeting lacks the relevant grounding to resolve ambiguities. Future work can explore how to add additional organizational level information to the models so that models can get more context about participant's discussions.



% Our findings on how participants explore the two recap experience point to interesting directions for building effective recaps.
% 1) Large language models are still not perfect at generating accurate notes, but participants are able to make sense from the context,
% 2) easy way to access the raw data always helpful for more context,
% 3) notes and action items have different relevance for different people, so personalized priority makes sense,
% 4) Participants like collaborative aspects to the notes
%5) They are more willing to add/edit/delete notes if it helps them later.
%Schober, M. F., and Clark, H. H. 1989. Understanding by
%addressees and overhearers. Cognitive Psychology 21:211â€“
%232. - rich discussion on subjectivity of meeting discussion relevance.

\section{Conclusion, Limitations and Future work}
%\ah{engage with the limitations in conclusion - most of the interviews are people from research, however alignment between what we learnt and saw suggests that it might generalize, we also didn't do quantitative work, to justify if what people say matches with what they do-}
With more meetings happening in organizations and an increasing proportion of them turning to hybrid or online, effective meeting recap can provide participants a way to strategize their meeting workload more effectively. We propose design of a meeting recap system with two different experiences that target different but complementary use cases -- 1) quick highlights recap, 2) Full meeting minutes segmented into chapters. \change{We perform preliminary evaluation} of the experiences with users in a within subjects design. Our findings suggest that participants find value in both highlights, as well as the hierarchical experience. \change{Future work can build on these findings to design and evaluate recaps at a larger scale}.

We recruited participants from the research organization where we conducted the research to perform preliminary evaluations \change{and many of our meetings were progress update meetings that involved reporting of analyses, and next steps. All our participants had a basic minimum technical background due to employment at the organization which limited our ability to explore how users from diverse backgrounds will use recap. We observed the difficulty in studying in-context usefulness of meeting recap. Our study required participants to record their meetings and share it with the system to generate recap and discuss the same in the interviews. Recording is not a commonly accepted practice in the organization, and many participants were not comfortable either discussing their personal meetings or seeking consent from other meeting participants to record and share their meeting. These concerns are likely to come up in other contexts as well~\cite{ackerman2000intellectual}. To get meetings from diverse backgrounds, such as finance, human resources, scientific collaborations, and health future research should carefully think of how to manage the privacy aspect of studying recap~\cite{kling1992cscwprivacy}.} All the participants in our study attended their respective meetings and had fresh context of their meetings. While prior work suggests non-attendees benefit as much as attendees from notes created during meetings~\cite{nathan2012}, understanding the perspectives of attendees weeks after the meeting and non-attendees could uncover additional insights (e.g., need for richer context). This is because, loss of context with time, or no-context when not attending the meeting impacts recall~\cite{kalnikaitundefined2012}. Our study provides early stage insights into the usefulness of a meeting recap system that generates high quality personal style notes but future research should study recap needs in a longitudinal setup, both qualitatively and quantitatively. \change{We evaluated both are design rationales in the same sequence of highlights then hierarchical. While the sequence can have an effect on the findings, it would be minimal due to the exploratory nature of our study. Future work aiming to draw statistical conclusions should consider randomized sequence for evaluating the representations. }

%\change{this is not intended to be generalizable. We focused on progress update meetings that involved some reporting of analyses and next steps. Diff affordances necessary for other meeting types.}

Meeting recap is a challenging domain where ML models trained through crowdwork are less aligned with the needs of target audience (e.g., organization workers)~\cite{yang2019, star1994}. This creates the need for designing interfaces that generate useful training data so that models can be improved as part of ``natural'' use~\cite{yang2017role}. Our findings also provide preliminary insights for designs that allow participants to easily interact with the notes that reflects their preferences, so it can also be used to generate useful training data. Quick edit suggestions, or memory aids to recall missing notes are examples of such design ideas. Future work can setup quantitative experiments to validate the quality of training data gathered from user interactions in a complex meeting recap like application.

From a collaboration perspective, our findings suggest the use of notes for more than just meeting recap. Future work can explore the ways meeting participants can potentially engage in the discussions around notes, and its impact on generated ideas, consensus or information exchange between participants. 


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix


%\section{Research Methods}


%\subsection{Part One}

%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
%malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
%sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
%vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
%lacinia dolor. Integer ultricies commodo sem nec semper.

%\subsection{Part Two}

%Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
%ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
%ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
%eros. Vivamus non purus placerat, scelerisque diam eu, cursus
%ante. Etiam aliquam tortor auctor efficitur mattis.

%\section{Online Resources}

%Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
%pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
%enim maximus. Vestibulum gravida massa ut felis suscipit
%congue. Quisque mattis elit a risus ultrices commodo venenatis eget
%dui. Etiam sagittis eleifend elementum.

%Nam interdum magna at lectus dignissim, ac dignissim lorem
%rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
%massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
