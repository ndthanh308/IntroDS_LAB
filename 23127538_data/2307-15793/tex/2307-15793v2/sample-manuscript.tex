%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall,screen]{acmart}
% \documentclass[acmsmall,screen]{acmart}
% \settopmatter{printacmref=false}
% \setcopyright{none}
% \renewcommand\footnotetextcopyrightpermission[1]{}
% \pagestyle{plain}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage{array}
\usepackage{longtable}
\usepackage{tabularx}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
\setcopyright{none}
\copyrightyear{}
\acmYear{}
\acmDOI{none}
\renewcommand\footnotetextcopyrightpermission[1]{}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\newcommand{\sumit}[1]{\textcolor{red}{\textbf{(SA:} #1)}}
\newcommand{\ah}[1]{\textcolor{violet}{\textbf{(AH:} #1)}}

\newcommand{\kp}[1]{``key-point#1''}
\newcommand{\ai}[1]{``action-item#1''}
\newcommand{\high}[1]{\textit{highlights}}
\newcommand{\hier}[1]{\textit{hierarchical}}
\newcommand{\chapter}[1]{\textit{chapter#1}}

\newcommand\leadin[1]{%
    \vskip 5pt \noindent\textbf{#1.} %
}
\renewcommand{\mkbegdispquote}[2]{\itshape}
\newcommand{\change}[1]{\textcolor{black}{#1}}
\newcommand{\colortbl}{\color{black}}



%%
%% end of the preamble, start of the body of the document source.
\begin{document}
\title{Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system}

\author{Sumit Asthana}
\email{asumit@umich.edu}
\affiliation{%
  \institution{University of Michigan, Ann Arbor}
  \city{Ann Arbor}
  \state{MI}
  \country{USA}
}

\author{Sagih Hilleli}
\email{sagih@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Tel Aviv}
  \state{}
  \country{Israel}
}

\author{Pengcheng He}
\email{pengcheng.h@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \state{Washington}
  \country{USA}
}

\author{Aaron Halfaker}
\email{aaron.halfaker@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \state{Washington}
  \country{USA}
}

% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{none}
\renewcommand{\shortauthors}{Asthana et. al}

\begin{abstract}
Meetings play a critical infrastructural role in coordinating work. The recent surge of hybrid and remote meetings in computer-mediated spaces has led to new problems (e.g., more time spent in less engaging meetings) and new opportunities (e.g., automated transcription/captioning and recap support).
%transcription/captioning and recap support).
Recent advances in language understanding for dialog summarization can potentially generate helpful recaps to improve the post-meeting experience for personal and group work.
% reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. 
Despite this potential, a single fixed-length summary of the entire meeting may not serve diverse needs, such as quick takeaways or their use for an in-depth understanding of meeting discussions.
To address these gaps, we use cognitive science and discourse theories to conceptualize two recap designs: important highlights and a structured, hierarchical minutes view, targeting complementary recap needs. We operationalize these representations into high-fidelity prototypes using dialogue summarization. Finally, we evaluate the representations' effectiveness with seven users in the context of their work meetings. Our findings indicate the need for both representations in different contexts. We identify collaboration opportunities by sharing a recap for discussions or consensus. Exploring the meaning of users adding, editing, and deleting from recaps suggests varying alignment for using these actions to improve AI-recap. 
% We report on implications for designing AI systems to partner with users to learn and improve from \emph{natural interactions} to personalize recap. 
Our design implications, such as incorporating organizational artifacts (e.g., linking presentations) in recaps and personalizing context, advance the discourse of effective recap designs for organizational work and support past results from cognition studies.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% Meetings are important to organizations
``Communication is the lifeblood of organizations''\cite{sethi2009interpersonal} and meetings are ``window into the soul of a business''\cite{karen2009effective} are truisms that describe the reality of modern work in organizational settings.  Meetings serve an important organizational purpose for people to discuss ideas, share information, build consensus, and make decisions. They help distribute key information, coordinate action, answer questions, and help people align their work efforts~\cite{whittaker2008design}. They also help reduce the organization's uncertainty by bringing the participants together to discuss and resolve issues~\cite{allen2015cambridge}. However, key details of the meetings could be missed or forgotten due to participants being oversubscribed to meetings or distractions by emails, IMs, and other communication~\cite{nathan2012}. Further, time conflicts and increasing cross-timezone collaborations cause participants to need to miss meetings altogether~\cite{nurmi2011coping}.

With the rise in geographically dispersed teams, the COVID-19 pandemic, and shifts towards remote work, organizations are increasingly adopting online web conferencing software to hold meetings\cite{NFW22, yankelovich2004}. The wide adoption of technically mediated meetings offers an opportunity for designers and researchers to apply technical approaches to support these new ways of working.

In this paper, we consider technological support for \emph{meeting recap} -- systems that aid in the capture of knowledge, decisions, and action items conveyed in a meeting for asynchronous review and engagement. A meeting recap is essential for preserving meeting content for attendees and non-attendees alike~\cite{whittaker2008design}. Traditional practices for capturing meeting content, such as paper and electronic note-taking, are often informal and burdensome on meeting participants~\cite{kalnikaitundefined2012}. 
Meeting content browsers provide a mix of audio, video, and ASR-generated transcripts with bookmark-style annotations~\cite{whittaker2008design,geyer2005towards} speed users in referencing information from recordings but do not eliminate the manual effort of searching and interpreting meeting content~\cite{arons1992techniques, moran1997ll}. This also limits asynchronous participation -- opportunities for non-attendees to engage with the meeting content and with attendees post-meeting -- due to the lack of context~\cite{kalnikaitundefined2012}. 

Recent advances in dialogue summarization in NLP~\cite{lewis2020} are capable of generating contextual summaries from transcripts that can better support meeting recap goals. 
% On the other hand, dialogue summarization in NLP~\cite{lewis2020} has progressed considerably to enable the generation of readable summaries of text ~\cite{gliwa-etal-2019-samsum}. However, dialogue summarization alone is not sufficient to capture the diverse information needs of meeting participants. 
However, summaries alone are not sufficient.  People use meetings for diverse goals such as sharing knowledge, achieving consensus, negotiating deals, and deliberations~\cite{tuggener-etal-2021-summarizing,allen2015cambridge}. Directly applying dialogue summarization on meeting transcripts to generate a fixed-length summary may not have the right structure and information to support these various goals. For example, the same summary may be too verbose for participants needing quick highlights and too concise for those requiring in-depth context. Thus, an effective recap requires deciding what and how much should be summarized and what structure the recap should take to cater to these often diverging meeting goals. Unlike summaries of short dialogues, meetings have rich organizational context~\cite{whittaker2008design,kalnikaitundefined2008}. Hence, potential recap designs require expensive in-context evaluations of real meetings instead of inexpensive third-party crowdsourced evaluations~\cite{smith2020, star1994,schaekermann2018}. 

To address this gap of identifying effective recap designs and how they can support organizational work, we pose the following research questions.

\begin{itemize}
    \item RQ1: What \textit{key structures and information items} should be captured in a meeting recap to serve users' needs?
    \item RQ2: What \textit{benefits and challenges} do meeting attendees get for their \textit{personal and group work} from these key structures and information items in recap?
    \item RQ3: What do meeting attendees' post-meeting interactions with the recap mean for aligning AI-generated recap with participants' needs?
    % How might we leverage interactions with the recap experience to better align AI behavior to 
\end{itemize}

Building on CSCW scholarship of exploring technological support for complex work~\cite{knoll-etal-2022-user} through in-context evaluations of experimental high-fidelity designs, to answer our research questions, we 1) Use design rationales supported by theories from cognitive science and discourses~\cite{zacks2001perceiving} to conceptualize the design of two \emph{recap designs}~\cite{yang2019} that serve diverse recap needs -- "highlights", that represent key moments in meetings for quick takeaways, and "hierarchical", that represents a meeting as chronological topic-focused discussions for sharing knowledge~\cite{kalnikaitundefined2008, nathan2012}, 2) Leverage the concept of \emph{experimental prototype experiences} from HCI~\cite{yang2019} to create high-fidelity operationalization our proposed recap designs using dialogue summarization, and 3) Perform preliminary task-based evaluations with seven information workers in a large software company to explore the benefits and challenges when attendees use the representations to support their work and collaborations. To address the scarcity of available meeting datasets for training~\cite{rennard-etal-2023-abstractive} and the need for personalization, we also seek to understand how user interactions could be used to improve the \emph{alignment}~\cite{yang2017role} of AI-generated recap models.

Our in-context evaluations indicate that ``highlights'' and ``hierarchical'' representations serve differing, complimentary recap needs. While both recaps provided comprehensible summaries, participants preferred highlights for quick overviews and sharing key decisions, while they preferred hierarchical for understanding discussions and sharing knowledge~\cite{geyer2005towards}. Participants preferred some form of personalization in selecting appropriate notes in the \high{s} recap or personalized markers to support navigation in the \hier{} recap. 
Beyond personal work, participants' desire to use the recap as a collaborative document for consensus and knowledge work indicates the potential for meeting recap as an organizational artifact supporting articulation work~\cite{schmidt1992taking}. To improve the relevance of generated summaries, additions, and edits to summaries provide consistent signals of relevance, but many divergent reasons to delete summaries suggest summary deletions may not offer high-quality feedback signals. These implications are grounded in the existing literature of increasing alignment of Human-AI models~\cite{yang2019clinic}. 

We contribute a (theoretically backed\cite{zacks2001perceiving, mann1987rhetorical,grosz-sidner-1986-attention}) foundation for \emph{designing meeting recap experiences} using automated summarization technologies that serve the practical needs of meeting attendees and non-attendees (examined in the context of work) and its role in supporting organizational work. Our work also discusses specific implications of using UX interactions with recap designs for aligning AI-recap with user's \emph{personal} recap needs.

\section{Related work}
\subsection{Meeting recap and sensemaking}
Meetings reduce uncertainty, build mutual understanding, and help participants brainstorm new ideas~\cite{allen2015cambridge, acai2018getting}. A meeting recap sustains these benefits by documenting discussions and outputs, fostering collaboration and idea dissemination~\cite{bryant2005, kittur2007, andr2014, siangliulue2015}. Documented discussions serve as valuable resources for newcomers and experts, aiding memory and facilitating retrospective sensemaking that can lead to further insights or clarifications~\cite{zhang2018}.

Meetings both shape and are shaped by organizational culture~\cite{scott2023toward}. The value participants derive from meeting recaps depends on organizational expectations. Decision-driven meetings prioritize action items and key discussions, while collaboration-driven meetings focus on process discussions~\cite{scott2024}. Additionally, meeting discussions may not be linear, with participants using non-meeting tools for back-channel communication to resolve issues, seek clarifications, or improve decision-making~\cite{stephens2012multiple, dennis2010invisible}. Less structured meetings could also be non-linear where participants visit previously discussed topics to seek clarifications or discuss further~\cite{park2024}

Due to diverse roles and expectations from meetings, there is debate on what constitutes a good recap~\cite{whittaker2008design}. Including too much can overwhelm participants, leading to cognitive burden and fixation on ideas~\cite{moran1997ll, jansson1991design}. We propose two complementary meeting recaps: 1) focusing on key decisions and discussions to serve as "cues" for driving meeting agenda~\cite{kalnikaitundefined2008}, and 2) a "minutes" view providing a chronological record for knowledge work~\cite{whittaker2008design}. These approaches leverage cognitive fit theory~\cite{speier2006influence} and discourse interpretation theory~\cite{zacks2001perceiving,grosz-sidner-1986-attention,mann1987rhetorical} to align recap structure with these meeting goals.

\subsection{Meeting recap tools}
%Artifacts that represent the organization of work (e.g., design documents, action items, decisions) are popularly referred to as articulation work in CSCW~\cite{schmidt1992taking}. 
Mediating work artifacts (e.g., design documents, action items, decisions) play a critical role in the coordination of work activities~\cite{schmidt1992taking}.
People participate in meetings to create and modify these artifacts. Meeting recap tools can help record this activity for later reference through automatic transcription. However, due to high redundancy, raw audio and video recordings of meetings do not support sensemaking~\cite{moran1997ll, arons1992techniques}. Effective recap centers around making it easy to recall and modify these artifacts by marking timestamps in meetings, automatically extracting keyphrases for later edits, and combining meeting artifacts (e.g., slides, documents, whiteboard annotations, transcripts marked with discourse labels)~\cite{whittaker2008design}. Indexes into meetings or supporting them with short annotations are another popular way to help participants recall important moments~\cite{geyer2005towards, richter2003, whittaker1994, nathan2012,banerjee2005necessity}. For example, the Lite Minutes system~\cite{chiu2001liteminutes} allowed participants to create and share text, audio, and slide-based notes with automatically generated smart links and post-meeting note correction over email. When linked with video, such annotations provide recap value in learning environments as well~\cite{siangliulue2015, nathan2012} acting as digital bookmarks. Systems like Tivoli and Teamspace~\cite{geyer2005towards} directly support the organization and modification of artifacts (e.g., whiteboard diagrams, pen-written notes) in the meeting browser. To ease the cognitive load, meeting browsers also experimented with applying machine intelligence on transcripts, such as grouping semantically similar concepts to reduce redundancy~\cite{topkara2010tag, geyer2005towards, kazman1996, ehlen2008, manteidynamic, purver-etal-2006-unsupervised}, or automatically detecting action items~\cite{ehlen2007meeting}. Despite the rich modalities of recap browsers, Whittaker et al. ~\cite{whittaker2008design} found that participants found audio/video distracting and preferred scanning transcripts for their ease of skimming. However, this led to missing the high-level picture of meetings while being overconfident in their recall. Whittaker et al. and Banerjee et al.'s studies \cite{whittaker2008design, banerjee2005necessity} highlight the need for different recap forms: quick catch-ups and detailed, structured discussions. Previous recap systems\cite{whittaker1994, banerjee2005necessity, chiu2001liteminutes,ehlen2007meeting} allowed identification of low-level information but failed to provide a high-level narrative due to the limitations of text processing at the time. Modern recap tools also face challenges with ASR when multiple participants speak through a single microphone, complicating role attribution in the recap. Technological support for meetings has also been applied to make the intentionality of participants pre-meeting more explicit~\cite{scott2024} and adaptive interfaces that support efficient agenda planning during meetings~\cite{park2024}. Our work on recap support is complementary to support for planning meeting goals. Better articulation of meeting goals also helps develop better recap experiences, which can inform the goal of future meetings. 

\subsection{Speech and text summarization}
Speech and text summarization is the backbone of generating intelligent meeting recap experiences. Early meeting browsers used automatic speech recognition (ASR) to detect important meeting utterances directly. However, to leverage advances in natural language summarization methods~\cite{zhang-etal-2021-exploratory-study}, which are designed for text, it is more effective to use the ASR transcript of the meeting for recap generation~\cite{allahyari2017text, moratanch2017}. Recent neural-based summarization approaches~\cite{dong2018survey} can generate an abstract summary of the corpus with context. While corpus-based abstractive summarization provides more context and is useful for summarizing news articles or short dialogues, it does not work well for meetings due to long exchanges between multiple participants and topic shifts~\cite{khalifa-etal-2021-bag, chen2017, chen-etal-2021-dialogsum}. Besides, ASR transcripts may introduce limitations such as the inability to detect multiple-speakers~\cite{kanda2021investigation}. Even for large models like GPT-4, summarizing long-form meeting style dialogues leads to hallucinations~\cite{tang2024tofueval}. To overcome these limitations, recent approaches have explored reducing the dialogue complexity by segmenting transcripts into topically focused areas and then summarizing the smaller segments~\cite{cohen-etal-2021-automatic,qi-etal-2021-improving-abstractive}. Another recent approach by ~\citet{hua-etal-2023-improving} explored representing the transcript in its abstract form using semantic relationships between concepts. The exploration of ideal designs for meeting summarization is complicated by the limitation of automated dialogue summarization metrics like ROGUE~\cite{lin-2004-rouge} to evaluate meeting recap goals reliably. This is because factors like the user's background, organizational contexts like established recap practices, and cognitive load can decide the user's contextual recap needs~\cite{whittaker2008design}. ~\citet{rennard-etal-2023-abstractive}'s survey of abstraction summarization specifically calls out the need for new metrics and datasets for evaluating meeting recap.

Thus, we hypothesized designs that reduce processing complexity for individual models by having each model focus on a specific aspect of processing (identifying \kp{s}, segmentation, summarization of segments)~\cite{cohen-etal-2021-automatic}. Our in-context evaluations by meeting participants themselves inform the design of effective recap and how participants use them for their work.

\section{System Design}
\subsection{Design Rationale (RQ1)}
\label{sec:design_rationales}
Meeting recaps can support group work by allowing participants to recall important decisions, discussions, and knowledge from meetings and build on them~\cite{allen2015cambridge}. However, technology limitations and user factors (e.g., limited time availability) often constrain effective sense-making of raw meeting artifacts like transcripts and videos. Identifying key structures and information items that support sensemaking for relevant recap needs (e.g., quick takeaways, a summary overview, and topics discussed) can reduce the cognitive burden for meeting attendees. We use cognitive fit theory to instrument design rationales\footnote{``Design rationales'' are the documented explanations of why specific design decisions were made during a product, system, or process development and how they were evaluated~\cite{lee1991s}.} that capture these key structures and information items relevant to the user's contextual recap needs~\cite{speier2006influence, lee1991s}.  How attendees will use and benefit from the key structures and information items in the recap depends on their contextual needs, organizational culture, and their prior experience with technology~\cite{star1994, resnick1994, whittaker2008design}. Thus, we evaluate the effectiveness of our design rationales through task-based interviews about real meetings in the context of organizational work.  
% An ideal recap varies based on an individual's needs , so generating and evaluating recaps based on actual meetings is crucial to bridging user expectations and technological capabilities~\cite{star1994, resnick1994, whittaker2008design} \sumit{revisit this line}. 
% Because relevant summary depends on the user's context, defining quality measures for these summaries is also challenging~\cite{yang2020}. % These rationales define representations, their reasoning, and evaluation criteria. 

% By focusing on most important info we can reduce information overload, save people's time. Here's our DR - personal note-taking. Can have a table or a separate paragraph.

% Inspired by design rationales, we ask -- what shape should a meeting recap take?  On the one hand, we try to mimic the way that people take personal notes -- \textit{focusing on only the most important information (e.g., decisions and action items~\cite{allen2015cambridge})}. On the other hand, some use cases for meeting recap may prioritize \textit{knowing how a decision was made as well the details of the decision itself}~\cite{whittaker2008design, moran1997ll}. Analysis of people's communication and written documents of work using discourse theory suggests that hierarchy is a crucial part of discussions, and any summarized representation is most useful if it preserves such hierarchies~\cite{mann1987rhetorical, grosz-sidner-1986-attention}.

% Both of these approaches have potential advantages and drawbacks. We formalize them below and then describe their operationalization as different user experiences to evaluate, answering 

% \textit{RQ1: What key structures and information items should be captured in a meeting recap?}
% \begin{itemize}
%     \item \textbf{DR1: Personal note-taking:} A meeting recap should be as short as possible and focus on outcomes to efficiently serve users' needs.
%     \item \textbf{DR2: Meeting minutes:} A meeting recap should summarize the entire meeting, including discussions and outcomes within a hierarchical structure to enable broad use cases and contextual navigation.
% \end{itemize}

To answer \textit{RQ1: What key structures and information items should be captured in a meeting recap to serve users' needs?} We conceptualize the following two design rationales (DR1 and DR2).

\leadin{\textbf{DR1 - Highlights: A meeting recap should be concise and focus on outcomes to support planning and coordination}} This rationale represents people's purpose for using meetings to coordinate their work, achieve consensus, and plan their upcoming tasks~\cite{allen2015cambridge}. The \emph{highlights recap} (DR1) focuses on pulling out key points and action items from meetings and representing them with one to two-sentence summaries. These are expected to help participants focus on the most important aspects of the meeting for decision-making without getting overwhelmed with meeting discussions. 

% we can get sucked into the details.
    
\leadin{\textbf{DR2 - Hierarchical: A meeting recap should summarize the entire meeting, including discussions and outcomes within a hierarchical structure to support detailed context and knowledge sharing.}} This rationale represents people's needs to understand discussions in meetings from a knowledge perspective and \emph{how} key decisions were taken. The \emph{hierarchical recap} (DR2) leverages a ``chapterization'' strategy to break the meeting into ``chapters'' with a relevant topic.  Beneath each chapter, a user can explore lower-level summaries.  And under those summaries is the raw transcript.  Unlike the \emph{highlights recap}, the \emph{hierarchical recap} captures the entirety of the meeting in a recursive structure that resembles how people organize and interpret discourses~\cite{mann1987rhetorical}. 

% By focusing only on the most important outcomes of the meeting, the recap experience can be as brief as possible and thus save the user the time and energy of reviewing less relevant information after the meeting. Meeting minutes tend to employ a hierarchical structure that is supported by Zack et al.' s~\cite{zacks2001perceiving} study that people perceive and understand events in the hierarchy, where agenda items represent the high-level objects of discussion and details about that agenda item's discussion are recorded beneath. Moreover, hierarchical structure also shows up in discourse analysis of people's communication and written documents of work, both of which are crucial elements of organizational collaborations~\cite{mann1987rhetorical, grosz-sidner-1986-attention}.

% We designed two experiences that exemplify each design rationale (see Fig~\ref{fig:experiences_ux}) respectively. The \emph{highlights experience} (DR1) focuses on pulling out key points and action items from meetings and representing them with one to two-sentence summaries. These help participants reach a consensus on the decisions and upcoming action items.

%The experience provides users a brief set of key points and action items from their meetings, each  %One to two sentence summaries of \emph{key points} are shown under "AI Notes" and summaries of action items are shown under "AI Tasks".  A user is able to see the raw meeting transcript relevant to selected notes and tasks by selecting "show context" from a menu.  

% In contrast, the \emph{hierarchical experience} (DR2) leverages a ``chapterization'' strategy to break the meeting into ``chapters'' with a relevant topic.  Beneath each chapter, a user can explore lower-level summaries.  And beneath those summaries is the raw transcript.  Unlike the \emph{highlights experiences}, the \emph{hierarchical experience} captures the entirety of the meeting in a recursive structure that resembles how people organize and interpret discourses~\cite{mann1987rhetorical}. 

% We also designed these experiences to provide opportunities for users to correct, direct, and use what they saw in front of them in ways that we hoped would provide signals our models could learn from in order to become more aligned with users' expectations~\cite{christian2020alignment}.  In order to achieve this, these interactions need to have relatively consistent meaning when users apply them (alignment), and they need to be performed in cases where the model is wrong (informative).  Further, the users themselves need to understand what they are doing well enough to teach the model (situatedness), and they need to perform these actions frequently enough that we can gather a reasonable amount of training data over time (quantity). In our interviews, in addition to focusing on the proposed design's usefulness in providing recap support, we also focused on what users said and did in the UX to improve the summaries. We argue that these interactions provide early indicators of the type and quality of training data we would likely get in a deployment at scale to support meeting recap and improving the meeting recap AI through feedback. We provide a full description of the two user experiences with specific details about these interface components in Section~\ref{sec:prototype}.

In each recap, we also provided users affordances to add to, edit, and copy parts of the recap to share with others to understand how their feedback can help align the AI-recap with their expectations~\cite{christian2020alignment}. In the \high{} recap, we allowed users to add notes and action items and edit and delete existing ones to personalize their recap. To support planning and coordination, we also offered users the ability to assign tasks and due dates to users. In the \hier{} recap, we allowed users to add new content to existing chapters to personalize the chapters with their additional background knowledge. We also offered users the ability to mark new notes within chapters as \kp{s} or \ai{s} to increase the personal relevance of the recap and provide training data for models to learn about their relevance. These interactions were motivated by prior work that allowed participants to modify recap to improve their personal usefulness~\cite{geyer2005towards,gross2000towards} and team planning. We believe these interactions offer early indicators of the type and quality of training data we could gather at scale to enhance meeting recaps and AI through feedback. Detailed descriptions of the user experiences and interface components are in Section~\ref{sec:prototype}.

% Evaluating meeting recaps requires in-context evaluations with meeting participants. Thus, we interviewed seven information workers in a large software organization. Our interviews focused on 1) the proposed design rationales' usefulness for users' personal recaps and collaborations and 2) how users interacted with the recaps to improve their recaps and what it may mean for model alignment.
% For this to work, user interactions must be consistent (alignment), informative (occurring when the model errs), understandable (situatedness), and frequent (quantity).
 
% \textbf{User interactions in the \high{} and \hier{} recaps} 

While prior recap studies~\cite{whittaker1994} have highlighted needs for both quick takeaways and detailed hierarchical discussions, we are the first to 1) Formalize the needs as design rationales to structure exploration, and provide supporting reasoning from cognitive science, and discourse theories, 2) Design high-fidelity recap experiences based on the rationale to evaluate how well they meet recap needs of real meeting attendees in practice.

% Figure environment removed

\subsection{Modeling}
We generate each recap using a set of transformer models. For each recap, we begin by processing the transcript utterances in the ASR text representation of the meeting discussion. We define an utterance as a sentence or a statement in the ASR representation of the transcript spoken as a continuous piece of speech without a pause~\footnote{https://en.wikipedia.org/wiki/Utterance}. 

\subsubsection{Highlights model}
% Highlights experience is generated from four sequential transformer models~\cite{vaswani2017attention}, two for the key points and two for the action items in the meeting. For each note or action item, the first model is an extractive model (\emph{highlights\_extractive}) that takes an utterance with its surrounding context as input and classifies it as a key point or action item. The second is an abstractive model (\emph{highlights\_abstractive}) that gets the utterances identified as a key point or action item from the (\emph{highlights\_extractive}) model and rewrites them using the surrounding utterances as their context~\cite{cohen-etal-2021-automatic}. \emph{Highlights\_extractive} is a fine-tuned deBERTa with 12 transformer layers~\cite{he2020deberta} for their extractive part. They were trained on ICSI and AMI labeled datasets~\cite{janin2003}. Their input size is 100 tokens, which are extracted from the relevant utterance and have enough context from previous and next utterances to fill the 100 tokens' input size. 

% \emph{Highlights\_abstractive} is a fine-tuned BART~\cite{lewis2020} model. This was trained on ICSI and AMI labeled datasets as well~\cite{janin2003}. The models get the input utterance identified as a key point or action item by the \emph{highlights\_extractive} and a surrounding context of 500 tokens. The output of the model is the rephrased action item or note in the third person. Thus, the \emph{highlights\_abstractive} model relies on a supportive \emph{highlights\_extractive} model, which identifies the relevant context from the surrounding utterances in order to make BART~\cite{lewis2020} focus on the important information in the context. e.g., if the \emph{highlights\_extractive} model identified the following as an action item - ``Serena: I will finish this by Friday'', the \emph{highlights\_abstractive} model will rephrase it as ``Serena will finish the slides by Friday''. In this example, the abstractive model replaces ``this'' with ``slides'' that Serena referenced earlier. Writing the note in 3rd person and with context makes it easy to understand for a broader audience~\cite{whittaker2008design}. Figure~\ref{fig:pipeline} (left) illustrates this pipeline. Please refer to Cohen et al.~\cite{cohen-etal-2021-automatic} for more details on the models.

We generate the highlights recap using four sequential transformer models~\cite{vaswani2017attention}: two for key points and two for action items. Each model pair consists of an extractive model (\emph{highlights\_extractive}) and an abstractive model (\emph{highlights\_abstractive})~\cite{cohen-etal-2021-automatic}. For each note or action item, the \emph{highlights\_extractive} model takes an utterance with its surrounding context as input and classifies it as a \kp{} or \ai{}. The \emph{highlights\_abstractive} model gets the utterances identified as a \kp{} or \ai{} from the \emph{highlights\_extractive} model and creates an abstractive summary of the utterance along with its surrounding context~\cite{cohen-etal-2021-automatic}.

The \emph{highlights\_extractive} model is a fine-tuned deBERTa with 12 transformer layers~\cite{he2020deberta}. It classifies utterances as key points or action items using a 106-token input size with context from surrounding utterances. This model was trained on ICSI and AMI labeled datasets~\cite{janin2003}. The \emph{highlights\_abstractive} model is a fine-tuned BART~\cite{lewis2020} model. It takes the output from the \emph{highlights\_extractive} model along with 512 context tokens and rephrases them in the third person. For example, if \emph{highlights\_extractive} identifies "Serena: I will finish this by Friday" as an action item, \emph{highlights\_abstractive} rewrites it as "Serena will finish the slides by Friday." This approach makes the notes context-independent for independent review and sharing~\cite{whittaker2008design}.

Figure~\ref{fig:pipeline} (left) illustrates this pipeline. Please refer to~\citet{cohen-etal-2021-automatic}

\subsubsection{Hierarchical (``Chapters'') model}
% Figure environment removed

We generate the hierarchical (``chapters'') recap in two steps -- 1) Segment the entire meeting transcript into segments or \chapter{s} where each \chapter{} corresponds to a sub-topic or set of topics (Figure~\ref{fig:segmentation-pipeline}), 2) Synthesize a title and a set of notes representing each \chapter{} (Figure~\ref{fig:pipeline} right half).

\textbf{Segmenting the transcript} We divide the meeting transcript into \chapter{s} using the \emph{hierarchical\_segment}. \emph{Hierarchical\_segment} is a BART model that uses the text-tiling approach~\cite{hearst-1997-text, tur2011spoken}, which leverages lexical cohesion to determine segmentation boundaries where word distributions within segments are similar and across segments are dissimilar~\cite{galley-etal-2003-discourse, purver-etal-2006-unsupervised}. For prediction, we use a sliding window approach. In this approach, we break long input sequences into smaller windows of overlapping sequences. We then labeled each window with the classifier for topic boundaries, and the final boundary was identified with max-voting of the boundaries of the individual windows, resulting in segmented transcript blocks.

To train the \emph{hierarchical\_segment} model, we annotated a dataset of 12,600 meetings with chapter boundaries using crowd-sourced annotators from the UHRS platform\footnote{\url{https://prod.uhrs.playmsn.com/uhrs/}}, who marked transitions between topics. We split the dataset into 70\% training, 15\% validation, and 15\% testing. Annotators were compensated \$10 per transcript. We trained a BART~\cite{lewis2020} classification model on this annotated data to predict new segment starts. We split the transcripts into overlapping windows of 30 utterances with a stride of 10. We then applied the classifier to each utterance within these windows, and predictions were combined via maximum pooling to determine segment boundaries. This approach was necessary due to the token limit of 512 in transformer models~\cite{lewis2020}. The result was a transcript segmented into blocks, each representing a coherent topic.

\textbf{Generating Titles and Notes} For each of the blocks identified in the meeting transcript by the \emph{hierarchical\_segment} model, we generate its abstractive summary and title using the \emph{hierarchical\_abstractive} model. The \emph{hierarchical\_abstractive} model is a fine-tuned deBERTa model~\cite{he2020deberta}, trained on a dataset of 1M short (eight) dialogue utterances and their corresponding summary and rephrases them in the third person. The model generates notes, one for each sequential chunk of eight utterances in the meeting utterance blocks. We generate the chapter headings using the \emph{hierarchical\_title} model. \emph{Hierarchical\_title} is a deBERTa model based on meeting utterances and topic assignments from another annotated dataset of 1M pairs.  This dataset has 1M meeting utterance topic assignment pairs and was also obtained through annotations on the UHRS crowd-work platform. 

The final output is a structured set of topics with headings and notes representing the meeting summary, like meeting minutes. Figure~\ref{fig:pipeline} (right) illustrates this pipeline. Table~\ref{tab:model_summary} describes all the models we used in our system for both recaps.

% For step (1), we follow the text-tiling approach for segmenting meetings into chapters (topics)~\cite{hearst-1997-text, tur2011spoken}. Segmentation is based on the idea of document segmentation using lexical cohesion~\cite{hearst-1997-text}. The segmentation boundary is determined so that the distribution of representative words within the segment is similar and the distribution of representative words across segments is dissimilar~\cite{galley-etal-2003-discourse, purver-etal-2006-unsupervised}. Text-tiling is a method for transcript segmentation that breaks a long input sequence into smaller windows of overlapping sequences. Each window is then labeled with the classifier for topic boundaries, and the final boundary is identified with max-voting of the boundaries of the individual windows. (see Figure~\ref{fig:segmentation-pipeline} for an illustration). To train the segmentation model, we annotate a dataset of meetings with chapter boundaries. We recruit annotators on the UHRS crowd-sourced platform\footnote{\url{https://prod.uhrs.playmsn.com/uhrs/}} and ask them to mark utterances in the transcript that mark the end of one topic of a meeting and the beginning of the next as chapter boundaries. This dataset has 12,600 meetings with 126,872 segmentation blocks. The train, dev, and test split was 70\%, 15\%, and 15\%, respectively. We compensated the annotators \$10 per transcript for their time. Using the annotated dataset, we train a BART~\cite{lewis2020} classification model to predict if an utterance is the start of a new segment. For prediction, we split the transcript into overlapped windows of 30 utterances and stride of 10 utterances. We then apply the classifier to each utterance in the window. We then combine predictions by maximum pooling to arrive at segment boundaries. We adopt the sliding window approach for prediction because average meeting transcripts are much longer than the input length of transformer models (token limit of 512)~\cite{lewis2020}. The result of this step is the meeting transcript segmented into blocks, with each block corresponding to one topic or a set of coherent topics (see Figure~\ref{fig:segmentation-pipeline}). 

% In step (2), we generate chapter headings and notes for each of these blocks using deBERTa with 12 transformer layers~\cite{he2020deberta}, same as the highlights model. This step is similar to the rephrasing in step (2) of the highlights model, where the model takes a sequence of utterances as input and rephrases them in 3rd person. We train a transformer model on a dataset of short (eight) dialogue utterances and their corresponding summary. This dataset has 1M meeting utterance-summary pairs. The model generates notes, one for each sequential chunk of eight utterances in the meeting utterance blocks. The chapter headings are generated by a third deBERTa with 12 transformer layers~\cite{he2020deberta} model that is trained on a dataset of meeting utterances and their corresponding topic assignment. This dataset has 1M meeting utterance topic assignment pairs and was also obtained through annotations on the UHRS crowd-work platform. The final output from the three models is a set of topics marked with topic headings and a set of notes under each of those topics that represent the entire meeting summary. This result is very similar to a set of meeting minutes. Figure~\ref{fig:pipeline} (right) illustrates this pipeline.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|p{9.5cm}|}
        \toprule
        Model name & Description \\
        \midrule
        \textit{highlights\_extractive} & BART model that identifies the important utterances in the meeting around which the discourse needs to be captured in the highlights model (Figure~\ref{fig:pipeline} left).\\
        \textit{highlights\_abstractive} & deBERTa model that does an abstraction summarization of the context of the utterances identified by the highlights model (Figure~\ref{fig:pipeline} left).\\
        \textit{hierarchical\_segment} & BART model that identifies the segment boundary of transcripts that mark topic shifts (Figure~\ref{fig:pipeline} right)\\
        \textit{hierarchical\_abstractive} & deBERTa model that summarizes blocks of 8-utterances in each topic, and represents them as a rolling summary of the meeting (Figure~\ref{fig:pipeline} right).\\
        \textit{hierarchical\_title} & deBERTa model that generates chapter titles (Figure~\ref{fig:pipeline} right).\\
        \bottomrule
    \end{tabular}
    \caption{Summary of models used in meeting recaps system design.}
    \label{tab:model_summary}
\end{table}

% note abstraction is common across chapters and action items model. How do we explain the two?

% % Figure environment removed

% Figure environment removed

\subsection{Practical system design decisions}
Below, we summarize the rationales for the practical design decisions for our system implementation. We rationalize our choices using prior work and chose numbers that allowed for a reasonable system implementation. We use \citet{banerjee2005necessity}'s finding of 8 words per utterance and the widely accepted standard of 1 token = 0.75 words (or 1 word = 1.33 tokens) in NLP to explain the choices below.

\textbf{Input size of 106 tokens for the \textit{highlights\_extractive} model.}~\citet{cohen-etal-2021-automatic} identified that about ten utterance context is reasonable for detecting important highlights in the meeting. Thus, considering an average of 8 words per utterance~\cite{banerjee2005necessity}, our context window comprised about 106 tokens ($10*8*1.33$).

\textbf{Windows of 30 utterances and stride of 10 utterances for \textit{hierarchical\_segment} model} With 8 words per utterance~\cite{banerjee2005necessity}, a stride of 10 translated to a stride of about 133 tokens ($10*10*1.33$) and a window of 30 utterances translated to about 225 tokens ($10*30*1.33$). We leveraged ~\citet{koay-etal-2021-sliding}'s finding that a stride of 128 tokens and a window of 256 tokens yield the best performance.

\textbf{Surrounding context of 512 tokens for the \textit{highlights\_abstractive} and \textit{hierarchical-\_abstractive} models.}~\citet{he2022z} used a context window of 512 tokens to generate abstractive summaries for meeting related datasets in their experiments. They also identified that a similar context window is reasonable for identifying segment boundaries for topical segmentation of meetings. 

\textbf{Linearity of meetings} Our proposed design rationales work well for linear meetings but do not explicitly support non-linear meetings. Still, representing the meeting as chronological chapters in the \hier{} recap can support sensemaking for non-linear meetings as revisited topics will appear as chapters. Future work can augment the hierarchical approach by linking chapters across the meeting that pertain to the same topic, making it easier for participants to understand topic shifts to previous parts of the meeting.

\subsection{Prototype User Experience}
\label{sec:prototype}
We refer to \kp{s} as ``AI notes'' and \ai{s} as ``AI tasks'' in the UX for ease of understanding of participants. We use the shorthand terminology ``notes'' and ``tasks'' in the context of UX. To refer to both ``notes'' and ``tasks'' together, we use the term ``summary''.

We prototype the recap experiences on an HTML web app. The users start with a text area where they can copy-paste a transcript of a recorded meeting. Clicking ``Process'' sends the transcript to the back-end to generate the recap insights for the two recaps. The back-end uses the two summarization pipelines described above to create the recap data to send back to the web app front-end. Once the recap data is received from the two summarization pipelines, we populate the two tabs ``Highlights'' and ``Hierarchical'' with the data, formatting it as shown (Figure~\ref{fig:experiences}).

% The \emph{highlights view} displays a sequence of notes selected by the \textit{key points} extractive model and rephrased by the \textit{key points} abstractive model. The action items display a sequence of action items selected by the \textit{action items} extractive model and rephrased by the \textit{action items} abstractive model. Against each action item, the UI displays an ``assigned to'' field and a ``date'' field, which is initially empty and can be filled by participants to reflect the assignee and the deadline for the action item. Users can see more context for both notes and action items by clicking on the three dots at the end of each item line and selecting ``show context''. This opens up a tooltip displaying up to three transcript utterances before and after the utterance where the algorithm detected the notes or action item.  

% The \emph{hierarchical view} displays a summary of the entire meeting segmented into topics.  Each topic is made up of a topic heading that summarizes the topic name, followed by a one-line summary of the topic and the timespan (in minutes) of the meeting that the topic covers. Clicking the timespan opens up the topic into a list of summaries that provide a rolling summary of that section of the meeting. Each summary is represented by a summary text, followed by a timestamp that corresponds to the first transcript utterance included in the summary. Clicking each summary item further opens up the constituent transcript utterances that make up the summary. If the summarization algorithm detects one or more important ``key points'' in the topic, stars are displayed to the left of the summary and also next to the chapter title that contains the key point summary. Similarly, If the summarization algorithm detects one or more action items in the topic, checkboxes are displayed to the left of the summary and also next to the chapter title that contains the action item summary.  If the user clicks on the star or checkbox, the chapter is expanded into individual summaries, with the summary containing the key point or action item emphasized by the respective star or checkbox.  

% In order to explore interactions that could be used as training data to align~\cite{christian2020alignment} the model behavior with user expectations, we implemented various features that would allow the user to correct and direct the model.  In the \emph{highlights view}, A user can directly edit any suggested note or action item.  They can also add or remove notes and action items from the lists that are automatically generated. Similarly, in the \emph{hierarchical view}, a user can directly edit any summary or even the chapter titles.  Users can also remove an action item, but they cannot remove a summary because doing so would render some of the raw meeting transcript inaccessible. However, users can add or remove stars/checkboxes from various low-level summaries to flag moments of the meeting as key points/action items or remove the same, respectively.  We also provide basic sharing functionality by providing a ``share to chat'' option at the top of the experience.

The highlights view shows a list of important \kp{s} (``notes'') followed by a list of important \ai{s} (``tasks''). Key points are selected by the \emph{highlights extractive} model and rephrased by the \emph{highlights abstractive} model. Against each action item, the UI displays an ``assigned to'' field and a ``date'' field, which is initially empty and can be filled by participants to reflect the assignee and the deadline for the task. Users can see more context for notes and action items by clicking on the three dots at the end of each item line and selecting ``show context''. This opens up a tooltip displaying up to three transcript utterances before and after the utterance where the algorithm detected the notes or action item. Figure~\ref{fig:highlights} shows a screenshot of this UI.

The hierarchical view summarizes the meeting by topics, each with a heading, one-line summary, and timespan. Clicking the timespan opens up the topic into a list of summaries that provide a rolling summary of that meeting section. Each summary is represented by a summary text, followed by a timestamp that corresponds to the first transcript utterance included in the summary. Clicking each summary item further opens up the constituent transcript utterances that make up the summary. If the summarization algorithm detects one or more important ``key points'' in the topic, stars are displayed to the left of the summary and also next to the chapter title that contains the key point summary. Similarly, if the summarization algorithm detects one or more action items in the topic, checkboxes are displayed to the left of the summary and also next to the chapter title that contains the action item summary.  If the user clicks on the star or checkbox, the chapter is expanded into individual summaries, with the summary containing the key point or action item emphasized by the respective star or checkbox.  Figure~\ref{fig:chapters} shows a screenshot of this UI.

To explore interactions that could yield useful training data to align~\cite{christian2020alignment} the model behavior with user expectations, we implemented user interactions (add, edit, delete, mark-important) to make the summaries more useful. Users could edit, add, or remove notes and action items in the highlights view. In the hierarchical view, users could edit chapter summaries and chapter titles and add or remove stars and checkboxes but could not remove summaries. A "share to chat" option allows for basic sharing functionality. We note that we provide these affordances to the users to explore how they interact with the summaries to make them more relevant and draw implications for use as model feedback. Evaluating actual model improvements using the feedback is not the goal of our work.

\section{User study method}
\subsection{Participants}
We evaluated the two recaps through semi-structured interviews with seven participants. We recruited the participants through internal lists and emails. When the participants agreed to the study, we set a 90-minute video conference interview time. Since we recruited employees within the organization as participants, we did not compensate them. 
\begin{table}[h]
    \centering
\begin{tabular}{ |l|l|l| }
\hline
\multicolumn{3}{|c|}{Demographic information} \\
\hline
\multirow{2}{*}{Gender} & Male & 50\% \\
 & Female & 50\% \\ \hline
 \multirow{5}{*}{Age} & 21-25 & 17\% \\
 & 26-30 & 50\% \\ 
 & 31-35 & -- \\ 
 & 36-40 & 17\% \\ 
 & 46-50 & 17\% \\ \hline
 \multirow{4}{*}{Region} & North America & 66\% \\
 & Greater China & 33\% \\ 
 & India & 0\% \\ 
 & Middle-east & 0\% \\  \hline
 \multirow{5}{*}{Profession} & Research & 58.3\% \\
 & Software engineer & 16.7\% \\ 
 & User engineer & 8.3\% \\ 
 & Design/creative & 8.3\% \\ 
 & Other & 8.3\% \\\hline
\end{tabular}
    \caption{Demographic details of interview participants}
    \label{tab:participant-demographics}
\end{table}

\subsection{Tasks and procedures}
% Each interview session was conducted by two authors of the paper, one leading the interview and the other taking notes. The lead interviewer explained the interview process to the participants at the start of each session. The interviewer then solicited the participant's consent to record the session and use a transcript of one of their recent meetings to generate the meeting recap and discuss its usefulness. We had informed participants in advance about using one of their meetings to create the recap and study its contextual usefulness so that they could record a meeting before the study when informing them about the research and scheduling the interview. All the participants' recorded meetings that they brought to the study were between one and two weeks old. We started the interview by asking the participants to copy-paste the meeting transcript they had decided to discuss and hit ``process'' to begin generating the recap experiences. We did this initially because the system took several minutes to create the recap, and we used that time to ask preliminary questions. The prototype did not log any user data, and we did not have access to the contents of the meeting transcripts that participants chose to use for the study. We also asked participants to share their screens during the study to observe their interactions with the recap experiences as we proceeded through the interview. 
Two authors conducted each interview session: one leading and one taking notes. The lead interviewer explained the process, obtained consent to record, and used a recent meeting transcript to generate and discuss the meeting recap. We informed participants in advance to prepare a meeting recording, ensuring it was one to two weeks old. We began the sessions by asking participants to paste their meeting transcript into the prototype and start the recap generation. The system took some minutes to generate the entire recap. We started the interview by asking the participants preliminary questions about their meeting habits during this time. The prototype logged no user data, and meeting transcript contents remained private. During the interview, we asked the participants to share their screens to allow observation of their interactions with the recap.

We designed the interview questions to 1) Understand people's prior note-taking practices, 2) Understand the participants' \textit{actions and opinions} when attempting to use the recap for their personal and group work, and 3) Observe how participants interacted with the recap to understand how they would improve the recap and its implications for model development. Observing participants perform tasks instead of simply eliciting problems about the recap from them evokes higher-order cognitive capabilities~\cite{cotton2006reflecting} and is expected to provide data closer to their actual understanding of the recap. 

Thus, we divided the interview questions into three sections -- 1) General participant background and meeting habits, 2) \textit{Tasks and questions} for the \high{} recap, and 3) \textit{Tasks and questions} for the \hier{} recap. For both \high{} and \hier{} recaps, we asked participants to perform the following tasks -- i) Identify important aspects of the meeting from the recap for personal work, ii) Identify important aspects of the meeting from the reacp for sharing (group work), iii) Identifying relevant missing summaries and adding them, iv) Identifying inaccurate summaries and fixing them, v) Identifying irrelevant summaries and deleting them (if applicable). We observed how participants performed the tasks and asked them why they chose a summary for personal work and why they chose it for sharing. After the participants performed the tasks, we also asked them their reasons. For missing, inaccurate, and irrelevant summaries, we also asked participants to interact with the UX to add, edit, or remove the summaries and think aloud through the process to get insights on the reasons for the participant's actions. To get natural insights, we let the participants explore the UX independently and only direct them when absolutely necessary (e.g., when they get stuck with an interaction). Think aloud~\cite{duncker1945problem} is a popular HCI method to elicit user expectations and capture participants' thought processes as they interact with the output of a system. The supplementary material provides the complete question set used for the interviews. The study was reviewed and approved by the institution's review board. We provide the full set of questions in supplementary materials.

\subsection{Analysis}
After each interview, one of the authors reviewed the recordings and transcripts and added any notes missed during the interviews. At the end of all the interviews, we had about 7 hours of recordings, associated transcripts, and notes taken during the interviews. We analyzed the interviews using thematic analysis~\cite{braun2006using}. 

\begin{table}[ht]
    \centering
    \colortbl \begin{tabular}{|p{0.8cm}|p{12.3cm}|}
        \toprule
        Parti-cipant & Meeting description\\
        \midrule
        1 & A 60-minute long research project meeting. The attendee was a research intern and the project lead. \\
        2 & A 45-minute long talk on a topic of Computer Architecture, with a 15 minute Q\&A. Attendees were one speaker from the domain of Computer Architecture and about 20 researchers in the same area.\\
        3 & A 60-minute long research update between a research mentor and a research intern.\\
        4 & A 60-minute research group meeting with the agenda of "paper submission" amongst two research mentors and a research intern.\\
        5 & A 60-minute kickoff meeting for a project between two teams, one research team and one product team. The agenda was to introduce collaboration between the research and the product team. The attendee was from the research team and attended the meeting while driving a car and could only partially pay attention to the meeting.\\
        6 & A 60-minute long weekly research update between a research mentor and a research intern. \\
        7 & A 60-minute long weekly research update. The discussion was on analyses and survey results. Two interns and their mentor participated in the meeting. \\
        \bottomrule
    \end{tabular}
    \caption{Overview of meetings}
    \label{tab:meeting_details}
\end{table}

\subsubsection{Coding}
We conducted a thematic analysis of the data. We used our design rationales (Section~\ref{sec:design_rationales}), research questions, and prior work~\cite{geyer2005towards} to define an initial set of codes focused on the strengths and weaknesses of the two recaps for supporting personal and group work. Thus, our initial codes captured the benefits and challenges of the recap designs, such as "quick takeaways," "detailed overview," "supports decision-making," "supports understanding discussions," "helpful context," and "lack of context."

We started by applying these codes to the participants' responses about their recaps. Wherever applicable, we inductively refined our codebook with new open codes for flexible data exploration. For example, we observed participants indicate the need for "personalization" in both recaps and "quick overview" for \hier{} recap, which we had not identified before analysis. We also refined our initial codes, such as refining "lack of context" to "lack of spoken context due to limited summaries" or "lack of external context" such as videos or slides. We also coded participants interactions with the UX to better understand how they adjusted the summaries to suit their preferences. Given the novelty of the LLM-powered meeting recap tool, we did not have initial codes for these interactions.

At the end of the initial open coding process, we had about 143 codes that reflected various dimensions of recap that we wanted to study. We grouped the final set of codes into themes using affinity diagramming~\cite{holtzblatt2009contextual}. We categorized the codes based on their relevance to key aspects of meeting recaps, such as "access to reminders," "understanding discussions," "access to context," "personalization," "achieving consensus," and "sharing knowledge." Finally, we clustered related codes into high-level themes, such as "benefits/challenges for personal work", "benefits/challenges for group work" and "user interaction with AI," representing our prominent findings in the paper.

Our team comprised of a research group and a product group where the research group preferred the \hier{} recap and the product group preferred the \high{} recap. The balanced preferences for both recap designs acted as a check on individual biases. Further, in weekly meetings, we challenged our assumptions about the designs to reduce bias in coding and aggregation. Our final results don't pick a favorite recap design but highlight the benefits and challenges of both recaps in different contexts, indicating our lack of bias towards any specific recap. 

\begin{table}[h]
    \centering
    \begin{tabular}{|p{3.7cm}|p{1.6cm}|p{4.3cm}|p{1.6cm}|}
        \toprule
        Construct & \#participants & Construct & \#participants\\
        \midrule
        Takes notes & 7 & Should take more notes& 2\\
        Keyword notes & 1 & Notes focus on ToDos& 4\\
        Digital notes & 7 & Recordings too long& 2\\
        Physical notes & 2 & Transcripts too long& 1\\
        Notes in chat & 2 & Notes help plan next meeting& 3\\
        Notes in Gdoc & 2 & Recaps with collaborator& 3\\
        Records meetings& 4 & Recaps with transcript& 3\\
        Shares notes& 6 & Recaps with recordings& 5\\
        Shares tasks& 2 & Recaps with slides& 1\\
        Shares slides& 0 & Recaps with chat& 2\\
        Cleanup before share& 1 & ASR issues& 1\\
        Collaborative notes& 4 & Task tracking& 4\\
        Agenda driven meeting& 1 & Recap at end of meeting& 1 \\
        Ask to record& 1 & & \\
        \bottomrule
    \end{tabular}
    \caption{Overview of meeting recap habits}
    \label{tab:general-meeting-habits}
\end{table}

% We open-coded these interactions to allow for an inductive and flexible exploration of the data to effectively uncover emergent themes and patterns without being constrained by predefined categories.

% We center our analysis around 1) People's existing recap practices and 2) the Usefulness of the two design rationales identified in RQ1: ``What meeting recap designs could capture salient user needs for supporting their work and collaborations?'' in serving people's recap needs. Thus, we started with an initial set of seed codes that capture the salient aspects of the ``highlights'' and the ``hierarchical'' view. For highlights, our initial codes centered around its strengths - "quick takeaways", "quick recap", and weaknesses - "lack of process details", "less recall". Similarly, for the  "hierarchical" view, our initial codes centered around its strengths - "high recall", "contains process details", and weaknesses - "very detailed", "high redundancy". \sumit{revisit. TODO: code aggregate and mitigation of biases.}. One author went through the transcripts line by line and assigned open codes, additionally using the notes taken during the interviews. Following established coding practices, we inductively assigned codes to reflect what participants said and did during the interview and avoided codes that reflected any prior understanding of the coder~\cite{linneberg2019coding, mihas2019qualitative}. Our codes captured the general opinion of the participants with respect to the usefulness of the recap experiences (e.g., ``this is useful''), how they imagined using the recap (e.g., ``planning upcoming tasks''), and their challenges associated with using it (e.g., ``ambiguous note''). We also made sure to code participants' interactions with the UX during the study to understand how they modified the summaries to align~\cite{christian2020alignment} it better with their own preferences and what UX interactions enabled them to do so (e.g., ``looks up context'', ``edits summary'').

% After coding each interview session, the primary author refined the codes through discussions with another author to resolve any disagreements. At the end of the coding session, we had about 143 codes that reflected various dimensions of recap that we wanted to study. We grouped the final set of codes into themes using affinity diagramming~\cite{holtzblatt2009contextual}. We performed the affinity diagramming step using Microsoft Excel.
% https://arxiv.org/ftp/arxiv/papers/2205/2205.05661.pdf
% familiarization phase - listen to recordings and transcripts
% assigned open codes staying close to the data. Iteratively revised codes with another author resolving disagreements through discussion

\section{Insights from User Study}
We now describe the findings from our user study of the \high{} and \hier{} recaps. These findings answer our second and third research questions.

RQ2: What \textit{benefits and challenges} do meeting attendees get for their \textit{personal and group work} from these key structures and information items in recap?

RQ3: What do meeting attendees' post-meeting interactions with the recap mean for aligning AI-generated recap with participants' needs?
%User study results (Previous Heading)

%\ah{sitting down and writing down the discussion of the results will help us}
%\ah{move the results introduction, to discussion, and make it more about what we learn from it, rather than presentation of facts}
%All participants agreed that both the experiences were generally useful. While the participants found issues (e.g. 4 found issues with pronouns being wrong and nearly everyone (6) pointed out issues in the wording of some chapter headings) participants did not report these issues to be severe.  Four of our participants struggled to add action items and notes in the \emph{highlights view} without going back to the source material (transcript).  Many participants also reflected how the \emph{hierarchical view} helped them remember the meeting and what happened (6) (situatedness), so deploying this type of experience might help users show us what our AIs missed more easily  thus increasing recall (informativeness).  Most found the pattern of selecting highlights from the \emph{hierarchical view} to be intuitive (4) but others were confused by the meaning of the stars so other interaction patterns for selecting highlights would be worth exploring.

\renewcommand{\arraystretch}{1.3}
\begin{table}[ht]
\centering
\begin{tabular}{p{4cm}|p{4.7cm}|p{4.7cm}}
\toprule
 & \textbf{Highlights recap} & \textbf{Hierarchical recap}\\
\hline
\multicolumn{3}{l}{RQ2: Benefits and challenges for personal work} \\
\hline
\hline
\textbf{Access to quick reminders} & Provides quick reminders of key moments and action-items for planning weekly tasks. & Easy to locate key moments and action-items when catching up the entire meeting but difficult when time is limited. \\
% \hline
% \textbf{Non-text artifacts to support comprehension} & \multicolumn{2}{p{8cm}}{Key highlights aid decisions but lack of non-text artifacts (e.g., images, presentations) affects context.} \\
\textbf{Personalization of summaries} & Personalized selection of highlights could improve relevance. & Personalized relevance markers in chapters could support efficient navigation.\\
% \hline
\textbf{Understanding discussions} & Summarizes key points efficiently but lacked depth in capturing different viewpoints. & Provides a comprehensive understanding of discussion nuances. \\
% \hline
\textbf{Access to meeting context} & Provides key decisions but lacks context to understand their reasoning. & Requires effort to locate key decisions but helps understand the discussions' reasoning.\\
& & \\
\hline
\multicolumn{3}{l}{RQ2: Benefits and challenges for group work} \\
\hline
\hline
\textbf{Sharing decisions for consensus} & Quickly sending key decisions out in email/group chat helps ensure consensus and accountability. & Sharing chapters is less preferred for quick consensus and accountability due to verbosity.\\
% \hline
\textbf{Sharing recap for brainstorming and knowledge sharing} & Sharing a list of key outcomes is not helpful for brainstorming. & Sharing chapters as collaborative notes could help brainstorm and provide clarifications.% and follow-up QnA could resolve confusions.
\\
\hline
\multicolumn{3}{l}{RQ3: User interactions for model improvement} \\
\hline
\hline
\textbf{Consistent meaning for adding, editing summaries} & \multicolumn{2}{p{9.5cm}}{Additions or edits to summaries consistently meant that the resulting summary is more relevant to the participant.} \\
% \hline
\textbf{Inconsistent meaning for deleting summaries} & \multicolumn{2}{p{9.5cm}}{Deleting summaries could mean less personal relevance, difficult to comprehend, or trivial item.}\\
% \hline
\textbf{Ease of editing summaries} & Easy UX to edit summaries due to its clear and brief structure but limited context affected recall  & Full context around a decision or meeting point enabled easy edits.\\
% \textbf{Likelihood to edit shared notes} & wdf w & w wdv\\
\hline
% \textbf{Quality of recap if shared} & Recap can be generally considered high quality if shared (especially with seniors). & Recap can be generally considered high quality if shared (especially with seniors). & Sharing recap indicates a minimum level of quality on AI generated recap. \\
% \hline

\end{tabular}
\caption{Comparative summary of findings for highlights and hierarchical recaps for personal, and group work and implications of user-interactions for aligning AI recap with user's needs.}
\label{tab:summary_findings}
\end{table}

\subsection{Meeting Practices and Overview}
% General meeting habits and meetings overview

Table~\ref{tab:meeting_details} provides an overview of the meetings of our participants. Generally, all participants reported that they take notes in one way or another during meetings.  All took some version of digital notes, while two participants also took physical notes with pen and paper. Four preferred putting notes into the chat or using a collaborative document format like Google Docs. Six participants also shared notes with others sometimes. Four participants suggested that their note-taking generally focused on To-Do's and task tracking. 

Participants reported several different strategies for recapping meetings: 1) Three asked someone who attended, 2) Three Used the transcript, 3) Five Watched recording, 4) One indicated reviewing presented slides and 5) Two reviewed chat. These numbers are not mutually exclusive as each participant indicated multiple strategies for recap depending on their needs and time availability. Table~\ref{tab:general-meeting-habits} provides a summary of general these meeting habits across all participants.

We describe below the findings related to each of the recap designs. Table~\ref{tab:summary_findings} summarizes all of the findings described below across our two recap designs.
% https://www.overleaf.com/learn/latex/Questions/How_do_I_change_column_or_row_separation_in_LaTeX_tables%3F


% Access and recall?

\subsection{Highlights Recap}
\subsubsection{Benefits and Challenges for Personal Use} \hfill
%Benefits and challenges of highlights for personal work

\leadin{Utilizing Highlights Recap as Quick Reminders} %Access to quick reminders
Four participants pointed out that suggested summaries served as reminders of what happened in the meeting.  Participants indicated using the summaries to decide their action items for the next week's meeting if it was a recurring one. They also said it helps them plan out their action items for the upcoming week if they have many action items and prioritize them. Some participants already understood what they needed to work on but still found the summaries helpful as \textit{reminders} (\texttt{P03}) to go back to when unsure. 

\begin{displayquote}
It is helpful to remember. They are accurate. Jogging my memory about the content of the meeting.  The task is helpful.  It reminds me that I can go look at the email about the meeting to see something I missed. -- \texttt{P03}
\end{displayquote}

\leadin{Inadequate Personalization in Note Prioritization} % Lack of personalization in ordering and selecting notes \& action items 
%The highlights model was designed to be brief and focus on the most important needs of the user. Due to brevity, we found that sometimes the included summaries were not relevant to participants, and it missed out on the relevant aspects of the meeting. 
Six participants indicated that some of the generated highlights were not relevant to them. The model either did not capture notes from the relevant section of the meeting, or it captured action items that were low priority for the participant. For example, in a status update meeting, each participant discussed their own status sequentially, and the update given by others was not always relevant to the participant. To easily see discussions of relevant people, \texttt{P02} also suggested to \textit{group the summaries by people} so that they can pick the people relevant to them and see their discussions. Further, participants indicated that tasks assigned by their manager were high-priority action items that they needed to address urgently and termed it as ``main task'' other deliverables could be addressed later (\texttt{P03}). 

\begin{displayquote}
It's not that good because the AI tells you to do two things  one tiny task. A detail. Like, this detail is not correct, you have to fix after the meeting. It is not the main task. If these kinds of tasks are extracted, there will be plenty of to-do items, and I might miss the important ones. -- \texttt{P03}
\end{displayquote}

On finding less relevant summaries at the top, four participants also expressed the need to reorder them to have the most relevant items at the top for reviewing when in a hurry.

% on \textit{how much the feedback is useful for them} (\texttt{P02}) for their workflow. They requested having the most relevant notes \& action items right at the top so they can get a quick overview of the most relevant items when in a hurry.

\leadin{Inadequacy in Capturing Comprehensive Discussions} % Understanding Discussions %One interesting drawback that we identified with 
Two participants indicated that discussions in the \emph{highlights} recap were not properly captured. We designed this recap to highlight key moments. % While the recap provided an affordance for exploring additional context around key moments by surfacing, it was limited, mainly to serve the user's memory. 
For example, \texttt{P04} used ``show-context'' on an action-item to explore how the decision was made. The provided context of 3-utterances in this recap (Section~\ref{sec:prototype}) was not sufficient for them to understand the discussion around a paper submission.
\begin{displayquote}
We had a lot of discussion about what needed to be done. [...] If I were taking notes, I would have written down something to that effect. [...] Ultimately, we decided to work with the smaller dataset. No notes about any of that discussion. [...] It would have been nice to see notes around that. -- \texttt{P04}
\end{displayquote}

\leadin{Seeking Comprehensive Context in Summaries} %Access to summary context
Participants looked for more context to understand summaries better. In the specific instance of \texttt{P04}, part of their recap was a recorded presentation, and the context was a combination of utterances and visual information on the slides. After looking at the text-only summaries, \texttt{P04} remarked about access to the original video from the summaries.
\begin{displayquote}
If there was a way to link to the meeting for each of these notes, I could watch the relevant parts of the video based on the notes.  I can't scroll back in the context, so it would be great if I had the whole transcript in the context and a link to the video.  I may want some more detail.  If I can jump to the transcript or video, I can get that detail. -- \texttt{P04}
\end{displayquote} 

Participants also looked for context to understand who made the decision or why it was made. To get more context for ambiguous summaries, two participants explored the ``show context'' feature of the UX. However, three other participants still requested a link to the original video or the full transcript for more context. They mentioned that the limited context \emph{did not help them understand completely, and it would be helpful to look at the original discussion for clarity} (\texttt{P05}).  \texttt{P01} notes
\begin{displayquote}
I'm not sure what it is referring to and what it means in the context we were talking about. [...] (Participant expands context)  Oh! I remember what happened. A teams bug and the team couldn't see the slide deck. [...] So maybe 1st part got confused with 2nd part. Even for a human, it looks strange. -- \texttt{P01}
\end{displayquote}

% Additionally, while examining the context of an action that mentioned someone's name \texttt{P04} wanted to know why the name was mentioned:
% \begin{displayquote}
% I don't see anything in the context about sending out spreadsheets!  I see where the ``<name>'' came from.  We referred to his project and named the survey after their name.  That does help in understanding how the name got in there.  [...] 
% This is really cool.  I love that I can see the context of where it was drawn from.  -- \texttt{P04}
% \end{displayquote}

\leadin{Addressing Pronoun Misassignments} %Pronoun issues 
Four participants indicated that the model generated summaries with their names associated with the wrong pronoun.  Participants had varying feelings towards such errors, ranging from attributing it as a \textit{minor point} (\texttt{P01}) to noticing and pointing them out or even feeling uncomfortable about it. This was more frequent with non-Western names, in which case technology's behavior induced feelings of non-inclusivity. % TODO: cultural context discussion 
% P01 said: 
% \begin{displayquote}
% [...] pronouns are not perfect but that's a minor point. 
% \end{displayquote}
\texttt{P06} reflected on the value of the summaries as well as the pronoun issues they saw.
\begin{displayquote}
Even just looking at the headers, I remember much more about the meeting at a glance.  The sentences have errors, and the pronouns are wrong. -- \texttt{P06} 
\end{displayquote}

%\leadin{Priority ordering of notes} 
%[discussion] Systems can learn user's preference from re-ordering of notes, and personalized summarization models can use this preference to tailer to the notes experience to each individual meeting attendee. E.g., the learned relevance can be used to present notes in priority order, as well as a high recall experience, showing only the most relevant notes. 
% P02 notes
% \begin{displayquote}
% Can it prioritize the notes?   
% [...]
%  It would be valuable in that it is ordered by how much the feedback is useful for me. 
% \end{displayquote}

%\ah{call highlights instead of ai insights}

\subsubsection{Benefits and Challenges for Group Work} \hfill
%for group work

\leadin{Limited Utility for Collaborative Knowledge Sharing and Brainstorming} 
%Sharing recap for knowledge and brainstorming
Due to the limited context in the \high{} recap, there was limited indication for use of the recap for brainstorming or sharing knowledge. Only one participant (\texttt{P07}) indicated using the \high{} recap to start a discussion around a \kp{} that the recap contained.

%Moreover, participants indicated more willingness to contribute to collaborative notes. They indicated \textit{adding and editing them if it was shared and attached to the meeting} (\texttt{P06}) and \textit{putting more effort into them if it was needed to be shared with [Product team] collaborators} (\texttt{P05}).  
%[discussion] Participants mentioned that they would ensure quality when sharing notes, but this behavior is dependent on the hierarchy of the person they share with. Participants were okay sharing imperfect notes with colleagues at the same level or below, but expressed that they would edit the notes to fix quality if sharing with their managers. This suggests that shared notes may yield a higher quality, given participants may engage with them more due to transparency. This is similar to open source platforms like Wikipedia, where visibility of edits can lead to different outcomes.

% Another interesting sharing behavior we observed was the tendency to ensure a \textit{higher quality of the TODO list} if they are going to share (\texttt{P07}), and a higher likelihood to edit the notes if they are part of a shared meeting board.

% \begin{displayquote}
% If it was shared? Yes. If it was shared and attached to the meeting, I would consider it.
% [...]
% If it was shared or persistent and I was the meeting owner. I might consider doing it as a resource for other people. -- \texttt{P05}
% \end{displayquote} 

% Further, two participants indicated performing a more thorough oversight on the summaries before sharing if they shared it with their superiors \begin{displayquote}
% Good enough but depends on who I'm sharing with.  If closest collaborator who was in the meeting...  If I wanted to share with <product team> collaborators, I'd put more effort into [editing the notes]. -- \texttt{P07}
% \end{displayquote} 


% \texttt{P05} said:
% \begin{displayquote}
% If I wanted to share with [Product team] collaborators, I'd put more effort into them.  
% \end{displayquote}
% \texttt{P06} said:
% \begin{displayquote}
% If it was shared?  Yes.  If it was shared and attached to the meeting, I would consider [editing and adding notes].  
% \end{displayquote}
% \texttt{P06} also said
% \begin{displayquote}
% In [collaborative editor] you can assign people and it goes out through email.  This might be more effective.  It could drag people into the notes that they might otherwise ignore. 
% \end{displayquote}

\leadin{Ensuring Consensus Through Collaborative Note Sharing}
%Sharing recap for consensus
% Only one participant noticed the ``share'' button at the top. Participants had different and strong expectations of sharing the notes with attendees and others. Five wanted to share via meeting chat, three wanted to share via email, and three mentioned sharing directly with a task management tool. Four pointed out that they might want to share only some tasks/notesnot the whole set. Four mentioned they wanted to leave comments directly on notes/tasks, such as on a Word document. 
Five participants wanted to edit the notes collaboratively for common visibility of changes. They viewed collaboration on the notes as a way to build consensus around high-value tasks and transparency and help identify dependencies between tasks. E.g., \texttt{P03} noted that collaboratively available notes are helpful to transparently identify if their task requires someone else to finish theirs first.

\begin{displayquote}
In meetings that involve more than two people, there are dependencies. e.g., someone else needs to address a task before I can do my task. It would be nice to have this dependency -- \texttt{P03}.
\end{displayquote}

\texttt{P05} imagined the \textit{shared summary document connected to people's organizational workflows} such as part of the meeting calendars so that it is easy for people to keep track of the summaries. Along similar lines, \texttt{P01} suggested that \textit{shared notes can also act as planning boards for teams, capturing team dependencies and next tasks for the team} (\texttt{P05}).
% \begin{displayquote}
% It could be valuable if it is tightly connected to people's note-taking flows/organizational workflows for projects.  It would be especially valuable if it could link out.  If it was integrated into how people prepare for and participate in meetings, I would do it.  
% \end{displayquote}

%[discussion] The need to tag people in collaborative notes, send out email notifications, and use them as interactive evolving documents suggests that meeting attendees want to use notes simply beyond meeting recap as a meeting artifact that can do X, Y Z.

%With regards to quality, participants considered ensuring notes are high quality when sharing with their managers, or for more formal important upcoming meetings (revise). When we asked \texttt{P07} about if we could assume that shared notes were all high quality:

% or \textit{a way to move this to a collaborative notes tool} (\texttt{P07}).

% \texttt{P01} suggested shared notes can also act as planning boards for teams, capturing team dependencies. 
\begin{displayquote}
In meetings that involve more than two people, there are dependencies. e.g., someone else needs to address a task before I can do my task. It would be nice to have this dependency. -- \texttt{P05}
\end{displayquote}
% \begin{displayquote}
% It would be great if it could just go into [collaborative notes tool].  
% \end{displayquote}

For more accountability, (\texttt{P06}) further suggested \textit{to assign people in notes and notify via email to include people who might miss it otherwise}.

\subsubsection{Implications of User Interactions for Model Improvement} \hfill
% Implications of user interactions with highlights recap for model improvement

\leadin{Consistency in Adding and Editing Highlights} %Adding and editing to highlights has consistent meaning
All participants agreed that the intention behind adding and editing notes \& action-items was consistent (high alignment) in meaning. The notes that participants added were personally important to them~(4), a discussion to remember~(6), capture general topic/hierarchy~(4), or to add details to another note~(5). \texttt{P03} explained while adding a note from memory.
\begin{displayquote}
For the notes, I added them according to the timeline in my memory. In the beginning of the meeting, we discussed the result/trend. -- \texttt{P03}
\end{displayquote}

\texttt{P03} also reflected on adding notes that are important to them
\begin{displayquote}
I remember these because they are important parts I needed to do.  
[...]
I'm writing down the mistakes I made, the things I need to correct, and the main task. -- \texttt{P03}
\end{displayquote}

% While adding a missing note \texttt{P04} remembered of a task but was unsure if it was worth adding
% \begin{displayquote}
% As I'm writing, I think there could be a task there.  It's our path forward, but I'm not sure it needs to be a task. -- \texttt{P04}
% \end{displayquote}

Regarding reasons for editing notes, two participants raised concerns about the AI's ability to learn from their edits because they used external knowledge to make the edits. Five participants edited the notes to add relevant context like ``\textit{<show on screen>}'' (\texttt{P01}) to refer to non-audio content. Three made major edits like rephrasing the note or task to make it \textit{more actionable} (\texttt{P03}). Four participants only changed the order of summaries like \textit{moving less relevant notes to the end} (\texttt{P01}) for easier review (\texttt{P01}). Participants indicated that the likelihood of editing notes depends on the quality. e.g., \texttt{P04} stated that they would edit the notes if the notes were of good quality.

\begin{displayquote}
It's easier to fix or clarify these mistakes than to start from scratch. If it takes less time for me to fix and correct stuff, then I'm going to do it. If its so bad that it takes more time, I'll probably stop using it. -- \texttt{P04}
\end{displayquote} %, three fixed grammar issues, one fixed a pronoun issue, and two included external information. 
% Six participants were comfortable adding or editing summaries even if they were unsure or did not clearly remember the meeting details. For the likelihood to edit (quantity), two participants indicated that they would be more likely to edit if the summaries were collaborative, and two were more likely to edit if the AI would learn from their edits or if the summaries were of decent quality. Four participants also referenced non-audio meeting content, such as presentations, when making edits. 

% Three participants made minor grammar edits like \textit{inserting verbs} (\texttt{P01}) into the summaries, or major edits like rephrasing the note or task to make it \textit{more actionable} (\texttt{P03}). Four participants made edits that changed the order of summaries without changing the content, like \textit{moving less relevant notes to the end} (\texttt{P01}) to help them review the summaries better. Participants made considerable use of the context when editing notes, added external context like ``\textit{<show on screen>}'' (\texttt{P01}) to refer to non-audio content, and indicated that the likelihood of editing notes depends on the quality. e.g., \texttt{P04} stated that they would edit the notes if they satisfied a quality threshold
% \begin{displayquote}
% It's easier to fix or clarify these mistakes than to start from scratch. If it takes less time for me to fix and correct stuff, then I'm going to do it. If its so bad that it takes more time, I'll probably stop using it.
% \end{displayquote}

\leadin{Inconsistencies in Deleting Highlights} %Deleting from highlights has inconsistent meaning
Deleting notes and tasks was more complicated. Two participants reported that they might delete a task when it is done, three reported they might delete a task if it is redundant, and two others reported they might delete the task if it is inaccurate. 
%Two said they would not delete any notes or tasks without a very high level of confidence. Five participants generally found it straightforward to edit notes or tasks, but only one participant deleted a task. 
One possible explanation for this is that it is not possible to recover deleted summaries, so participants were more wary of deleting than adding or editing, where the content can still be modified or reverted back. For example, \texttt{P03} reflected that \ai{s} which were already completed, wrong or redundant could be misleading.
\begin{displayquote}
If I see an inaccurate note or something I have already done, I will remove it. The wrong actionable could be misleading. I might delete redundant actions, insert a new task with the right meeting, and delete the task that was summarized wrongly. -- \texttt{P03}
\end{displayquote}

\texttt{P07}'s comment brings out the contrast between lack of a consistent meaning for deleting notes and a consistent meaning for adding or editing summaries 
\begin{displayquote}
For editing, I won't wait until I'm super confident.  

For delete, I'd be more careful.  I might delete something that is important to others.  If it learns from my deletions, I'm worried about that. -- \texttt{P07}
\end{displayquote}

\leadin{Difficulty in Editing Highlights Due to Incomplete Context} %Editing highlights was difficult due to lack of complete context
Four participants were able to add a note or a task from memory or using the context in UX, but three of these four needed to reference the raw transcript (that they had just copy-pasted into the prototype) in order to think of something they might add. Six participants were comfortable adding or editing summaries even if they were unsure or did not clearly remember the meeting details. %P04 got confused by the name of someone unexpected showing up in the summary, and used the context to understand that the name of the person in the discussion, and that they themselves had referred to a survey as <name> survey once in the discussion. 
%[discussion] Prior work has also found that meeting attendees find it difficult to accurately recall context, so this has implications for systems learning from user feedback in notes, such as incorporating uncertainty or confidence of feedback. 


%\ah{people report recalling the meeting was way easier for hierarchical-}

\subsection{Hierarchical Recap}
\subsubsection{Benefits and Challenges for Personal Use}
\hfill
\leadin{Leveraging Hierarchical Recap for Comprehensive Overviews} 
 %Access to quick reminders
%\leadin{Chapters help get a quick meeting overview} 
One of the major reasons participants appreciated the hierarchical view was its ability to provide a quick overview of the meeting. While the \high{} recap offered fast access to reminders by only displaying the important meeting highlights, star icons in the \hier{} recap against chapters containing \kp{s} and \ai{s} also helped in easy access to reminders. Six participants indicated that a quick overview was useful to them. This overview helped a participant who \textit{missed the meeting and wants to understand in five minutes } (\texttt{P04}). Topic segmentation and progressive details helped participants easily navigate different parts of the meeting. It helped \texttt{P06} \textit{skip the poster session discussion and scan the analysis discussion} that they cared about. 
% \texttt{P04} said:
% \begin{displayquote}
% We had an attendee who missed a meeting recently.  I think they could understand this in 5 minutes.  I love the progressive detail.  I don't care about the poster session so I'll skip that.  But I do care about the analysis so I'll scan that point.
% \end{displayquote}
The hierarchical view was more intuitive, and \textit{looked more like a meeting to participants} (\texttt{P01}). \texttt{P05} was able to explore and understand the hierarchy on their own
\begin{displayquote}
This looks like a navigation tool which is more valuable.  [...]  This is two levels.  It seems right to me.  I see some duplication.  You have a main topic, a secondary topic, and then transcript. [...] This seems way more useful to me.  Even though there are errors, I can try to make sense of this. -- \texttt{P05}
\end{displayquote}

Three participants indicated errors in the chapters, but it did not affect their recall when they looked at the chapter headings and skimmed the chapter contents. This suggests that chapters helped participants recall meeting attendance beyond simply providing a record of what happened in the meeting.
%, which was even more useful if they \textit{missed something and wanted to get a quick overview} (\texttt{P06}). 
\texttt{P07} said
\begin{displayquote}
I like the chapters or headers. Even just looking at the headers, I remember much more about the meeting at a glance. [...]
I like this more than [the highlights recap]. It gives more topics of what we discussed. [...]
[The highlights recap] is good for giving a rough understanding [...].   
I'd glance at [the highlights recap] and AI Tasks, then I'd go to the hierarchical and go into the sections we discussed.  It is easy for me to focus on the sections I care about. -- \texttt{P07}
\end{displayquote}

However, in alignment with the above quote, four participants indicated that glancing at \ai{s} in the \hier{} recap was still slower than the \high{} recap as the \hier{} recap required going over the entire meeting minutes.

\leadin{Moderate Personalization in Note Organization} %personalization of summaries
Most participants did not have difficulty inferring checkboxes and stars as ``key points'' and ``action items'' respectively in the chapters. As participants navigated the \hier{} recap, it allowed them to favor navigation to chapters that contained more tasks quickly and assigned them to relevant people.
\begin{displayquote}
I'm choosing [this chapter] because it has a lot of action items.  [Picks an action item] This one I'd actually assign to [other person].  It's accurate.  He said it himself. -- \texttt{P03}
\end{displayquote}

However, similar to \high{}, \hier{} recap's \kp{s} and \ai{s} markers were not personalized. Participants indicated that personalized markers of relevant chapters could help them \textit{favor exploration of chapters} (\texttt{P05}) that were important for them, and skim other chapters. It is important to note that personalization in \high{s} recap related to selection of more relevant \high{} but personalization in \hier{} meant markers that favored personalization navigation.

% Not just navigation, flagging important notes and tasks in the \hier{} recap allowed participants to quickly look up the context by reading the prior and following summaries, helping them decide the relevance of the task more easily than the highlights view. \texttt{P05} opened the section with most tasks and mentioned 
% \begin{displayquote}
% I'm choosing [this chapter] because it has a lot of action items.  <Picks an action item> This one I'd actually assign to [other person].  It's accurate.  He said it himself.
% \end{displayquote}

% While the \hier{} recap favored personalization by providing more context around important items, three participants still missed personally relevant markers of importance, similar to the highlights experience.

\leadin{Comprehensive Discussion Understanding} %Understanding discussions
All participants' manner of exploration of the hierarchical recap suggested a progressive, breadth-first exploration strategy. They would consider a relevant part of the meeting, locate its chapter heading (breadth scan), and explore the chapter by expanding it (depth scan), and looking at the notes that are part of the chapter.  If necessary, they would then expand a low-level summary to see the raw transcript that was being summarized. %[disc - organization and sensemaking - how error and sensemaking interact]

\texttt{P04}'s words exemplify this approach
% \begin{displayquote}
% This (the hierarchical view) will definitely help me find what's most helpful to me.  It reasonably aggregates by topic.  I might just want to see... <selects a section>.  With this [chapter], I can look at the context. (\texttt{P02})
% \end{displayquote}
%\texttt{P04} said
\begin{displayquote}
I'm getting a summary flow of the meeting. [...] We had an attendee who missed a meeting recently.  I think they could understand this in 5 minutes.  I love the progressive detail. [...] I can understand this really quickly. -- \texttt{P04}
\end{displayquote}

Access to the full meeting context in a chapterized format allowed participants to explore the meeting in enough details that supported their recall. This contrasts with the \high{} recap, where participants found it difficult to understand discussions related to meeting highlights and decisions.

\leadin{Enhanced Comprehension through (Hierarchical) Meeting Context} %Access to meeting context
%(7) (drills down into chapter) 
The navigation provided by the \hier{} recap also allowed participants to get more context if they needed to delve deeper into the subject (see Section~\ref{sec:prototype}). Five participants used this context to expand discussions, understand who said what, and locate important notes and tasks. It helped \texttt{P02} understand the explanation in the Q\&A section of their meeting better.
\begin{displayquote}
\textit{(Picks a star) It explains the main difference between one project and another. Compared to other things, I'd say this is the main difference. Oh, this is important. (clicks the star bullet next to the other item) This is a very good question and answer, I don't know why this wasn't flagged at all.} -- \texttt{P02}
\end{displayquote}

However, similar to \high{}, absence of video access, or presentation materials limited sensemaking in scenarios when meeting discussions involved non-text artifacts (e.g., slides). \texttt{P02}'s attempt to understand a question related the Q\&A, where the speaker referred to a slide for part of an answer highlights the need for such context.

% \texttt{P04} first glanced at the headings, then drilled down on the second chapter to open the notes, then further into the notes to look at the context of the note
% \begin{displayquote}
% I like that it segments the meeting into topics of discussion.  I can hone into what I want.  If I'm trying to recall or share, they get a high level view in less than a minute.  Then you can drill down.  It turns a transcript from a sequential into a random access.  I can jump anywhere to get what I want.
% \end{displayquote}

% \texttt{P05} said
% \begin{displayquote}
% [Check boxes] looks like tasks. These look like main points. But why do some not have stars?  [Stars] must be main points.
% \end{displayquote}

% Overall, we found that participants considered the highlights view and hierarchical view complementary. While highlights helped participants quickly review their tasks, they did not find the notes very useful due to their limited coverage. On the other hand, participants preferred the hierarchical experience for its ability to quickly provide a meeting timeline-like experience (revisit).
% P02 said
% \begin{displayquote}
% If I want to look at tasks, I would prefer the [highlights view].  It's easy to look at it and add more tasks. 
% [...]
% If I want to read the summary of the meeting or identify some part of the notes to share, I would look at the hierarchical version. -- \texttt{P02}
% \end{displayquote}

\subsubsection{Benefits and challenges of hierarchical recap for group work} \hfill
\leadin{Effective Collaboration through Detailed Recaps} %Sharing recap for brainstorming
Five participants remarked that it was very easy to simply ``copy-paste'' a relevant meeting discussion into an email or on workplace chat to discuss with colleagues. This affordance was possible due to the context provided by chapters, where participants could open chapters to get more detailed summaries and choose the right level of details to share. \texttt{P06} remarked on how they could choose the right granularity to share the discussions.

\begin{displayquote}
For the first part of the meeting, I just wanted to confirm with my coworkers about the decision, but for the second part it was helpful to open up the details of the discussion and share it for further clarifications. -- \texttt{P06}
\end{displayquote}

Two participants also expressed the desire for follow-up discussions around the discussion minutes, similar to comments in a Google document, indicating the usefulness of recap for knowledge sharing and consensus building. Like the highlights recap, participants were more likely to contribute to collaboratively shared notes.

\leadin{Limited Utility for Consensus and Accountability} %Sharing recap for consensus
While the hierarchical recap helped participants get better meeting overviews, none indicated that they would use it to share their action items and decisions for consensus or accountability, as they did for the highlights experience. One possible reason is that the hierarchical recap contains meeting highlights as part of rolling summaries, and sharing discussions is too verbose for quick consensus and accountability. 

\subsubsection{Implications of User Interactions for Model Improvement} \hfill %Implications of user interactions with the hierarchical recap for model improvement.
Participants indicated opinions similar to those in the highlights recap regarding adding and deleting notes in the hierarchical recap. However, they found editing notes relatively easy due to their ease of recall.

\leadin{Consistency in Adding and Editing Highlights} % Adding and editing chapters had consistent meaning 
Five participants indicated that adding summaries to chapters or editing them held consistent meaning, suggesting they made changes to make the summaries more relevant. These participants were confident in their ability to edit the recap to fix any issues or add relevant context because of access to the full meeting context as chapters.% XX added more context to notes in chapters, XX marked a chapter note as important, and XX edited a chapter to fix an error.

\texttt{P02} added more context from their background knowledge to a chapter note.

\begin{displayquote}
I am making this chapter summary more relevant for myself by taking note of when we started this discussion a month back. -- \texttt{P02}
\end{displayquote}

\leadin{Inconsistencies in Hiding Chapters} %Hiding chapters (deleting) had inconsistent meaning
We did not allow participants to delete chapters as they represent the entire meeting. Instead, participants could adjust the depth of chapters they preferred in their recap by opening chapters they wanted more context on and hiding (collapsing) chapters they preferred less context on. We observed that participants reduced the depth of chapters for many reasonserrors in chapters and chapters that were less relevant to their work. This behavior is similar to participants' interactions with deleting highlights, which they did not prefer to be in their recap.

%Four participants indicated that deleting chapters had varying meanings. Participants deleted chapters to \textit{remove irrelevant meeting segments} (\texttt{P01}), poorly written chapters, or \textit{when the chapters contained too much detail} (\texttt{P02}). \texttt{P06} indicated deleting an entire chapter because of errors.

\begin{displayquote}
This second chapter has a lot of errors, and it is distracting to look at when trying to make sense of the meeting. -- \texttt{P06}
\end{displayquote}

\leadin{Ease of Editing Chapters Due to Summary Context} %Editing chapters was easy due to access to summary context
Four participants indicated that editing summaries in chapters was relatively easier than highlights because they had access to surrounding context in chronological order to aid their memory. Three even edited the summaries without difficulty or referring to the transcript, which participants had to resort to in the highlights recap because of insufficient context. 

\begin{displayquote}
I easily recalled what happened in this part of the meeting by looking at this chapter, so fixing this note was fairly easy! -- \texttt{P05}
\end{displayquote}

%\ah{discourse acts are helpful, and highlighting them helped people [discussion]-}
 
%\textit{Full chapter based summary helpful for rewriting notes and action items}

% Overall, we found that participants considered the highlights and hierarchical views complementary. While highlights helped participants quickly review their tasks, they did not see the notes as useful due to their limited coverage. On the other hand, participants preferred the hierarchical recap for its ability to quickly provide a meeting timeline-like recap \sumit{revisit}.

\section{Discussion}
% 3 column table
In our design rationales, we hypothesized that recap designs that contain key structures and information items relevant to users' contextual recap needs would reduce the cognitive burden and facilitate recap. We identified two recap designs: one that focuses on key outcomes to support planning and coordination (\high{}) and another that creates an overview of the entire meeting within a hierarchical structure to enable knowledge sharing (\hier{}). Our user study indicates evidence that both recap designs reduce cognitive burden for different recap needs.

% Our user studies indicate that all participants find both recap experiences useful for different reasons. Most of our participants found complimentary value between recaps for different use cases. For example, it was a common theme that the \high{} recap was most useful for quick catchups, especially when the participant attended the meeting, and the \hier{} recap was most useful for getting knowledge about discussions, especially on missing meetings. This confirms prior findings of varying recap needs~\cite{whittaker2008design} and aligns with Nathan et al.'s~\cite{nathan2012} finding that annotations, regardless of whether it was their own or others, helped recall. 
%Participants found the highlights experience useful for getting a list of action items, while participants found the hierarchical experience useful for getting a quick overview of the meeting, despite the inaccuracies. The hierarchical view was more favored for recall as it helped participants quickly glance over the sequence of chapters, and notes within chapters to recall what was being discussed. Most (4) found the pattern of selecting highlights from the Hierarchical view to be intuitive but others were confused by the meaning of the "stars", so other interaction patterns might be worth exploring.%\ah{cite from acm magazine-}
%\sumit{connect it with prior work - what did other papers find in the literature with respect to meeting recap}
\subsection{DR1 - Highlights: Meeting recap should focus on key outcomes to support planning and coordination.} 
When describing our design rationale for the \emph{highlights view}, we argued that a meeting recap should be as short as possible and focus on outcomes to support planning and coordination~\cite{moran1997}. Participants generally agreed that they preferred the highlights recap for reviewing a meeting they had attended--specifically noting the value of seeing tasks at a glance. However, beyond key decisions and outcomes, participants found the recap less helpful for getting more context about the decisions and outcomes, such as identifying each participant's viewpoints or how decisions were taken. 

Determining the most relevant outcome to include in the highlights recap was a big issue for ML models. Some participants noted that the main points of discussion were not included in the suggested notes, while for some participants, the model generated no notes when it couldn't identify any utterance as important with high enough confidence. Some participants also suggested personalizing the order of the notes and tasks in their recap could help them process meeting takeaways faster. 

The interactions we provided for users show promise in gathering high-quality training data to further align~\cite{christian2020alignment} the models with users' expectations. Users found editing notes and tasks straightforward- especially when using the ``show context'' option to see the raw dialog. They reported that the meaning our models should learn from their edits was consistent (high alignment). However, users struggled to add missing notes without doing the difficult work of returning to the transcript to remember what happened in the meeting (situatedness). This suggests they would be less likely to add notes and tasks that our models missed. Thus, their interactions would be less \emph{informative} for improving the recall of \emph{key point} and \emph{action item} detection.  The concerns raised about the deletion of notes and tasks suggest that more work is needed to find a consistent way for users to express when an item should not have been included in the highlights.

\subsection{DR2 - Hierarchical: Meeting recap should summarize the entire meeting in a hierarchical format with outcomes to support detailed context and knowledge sharing.}
When describing our design rationale for the \emph{hierarchical view}, we argued that a meeting recap should summarize the entire meeting, including discussions and outcomes within a hierarchical structure (drawing from Zacks et al.\cite{zacks2001perceiving} and discourse structures~\cite{mann1987rhetorical}) to enable knowledge sharing and more context on outcomes. Participants were generally positive about the recap's role in helping them get a quick overview and the recap structure's alignment with their view of the meeting~\cite{zacks2001perceiving}. This alignment facilitated understanding discussions and using the recap to share knowledge.  Discourse-like view of the meeting also supported participant's context~\cite{grosz-sidner-1986-attention,mann1987rhetorical}; participants found it intuitive to drill down into individual meeting chapters as much needed. Despite inaccuracies in chapter titles, chapter summaries supported their recall to make them aware of such inaccuracies. It was apparent that the complete summary also helped participants put the \kp{s} and \ai{s} that the system flagged into the context of the discussion from which they arose. 

Recall support also made users confident when editing summaries to improve the summaries' alignment with their needs. They were able to understand better the tasks we summarized for them in the context of nearby low-level summaries.  In this case, adding a note was irrelevant, but selecting or deselecting highlights from the meeting was primarily intuitive.  Again, participants reflected on being more likely to do so if their work would benefit others, but many also reflected that they would correct the which summaries were highlighted if they felt that the AI behind the highlights would learn quickly from their corrections. 

\subsection{Design implications}
We now describe the design considerations for better meeting recap systems and designing scalable evaluations that applied to both our recap designs. Please refer to Table~\ref{tab:summary_findings} for a summary of the findings that inform these implications.

\leadin{Dialogue summarization supports recap sense-making} Contrary to Whittaker et al.~\cite{whittaker2008design} where participants found the summaries to be distracting and prevented high-level takeaways, all our participants found most summaries to be understandable despite imperfections and helpful when recalling parts of their meetings. This allowed our participants to focus on higher-order needs with the recap, such as how they would use the recap to plan their upcoming tasks or the possible ways to share the summary to collaborate. Such observations were limited in prior studies where the cognitively demanding task of sensemaking from transcripts prevented participants from thinking about how the recap could apply to their work~\cite{whittaker2008design, kalnikaitundefined2008}.

\leadin{Different meeting recaps for quick takeaways versus detailed discussions.}
One of our key insights from comparing the \emph{highlights} and \emph{hierarchical} recaps was that they were both valuable in different contexts, complementing each other. 
While quick access to tasks was valuable to participants for planning after a meeting they attended, a rolling summary of the meeting helped get an overview if they missed the meeting or for a presentation. 
%Highlights require models to understand personal relevance to generate the most important summaries, while hierarchical generates minutes from the full meeting, so it serves as a common document for all attendees.
Nathan et al. ~\cite{nathan2012} and Whittaker et al. ~\cite{whittaker2008design} both hypothesized that a personal summary could act as a personal todo list, while a group summary could be helpful for collaborations and public contractual obligations. Recent work on meeting intentions by Scott et al.~\cite{scott2024} also indicates varying meeting needs that require different recap experiences. This exploratory finding also aligns with cognitive fit theory~\cite{speier2006influence} that the most useful representation of the summary will be the one that matches the structure of the task that the participants hope to achieve from the recap.

\leadin{Better context improves meeting recall}
All participants used the ability to expand the ``context'' of a summary into the original dialog to make sense of a suggested note, suggested task, or a low-level summary (see Figure~\ref{fig:experiences_ux} for context affordance).  Many participants also requested deep links to video recordings to review the discussion quickly. These observations agree with prior findings on audio indexes explored by Geyer et al. and Moran et al. ~\cite{geyer2005towards, moran1998} that enabled participants to make better sense of their notes when they were associated with timestamped audio recordings. Access to original meeting artifacts is also helpful in allowing participants to correct any issues with the model-generated summaries. Whittaker et al. ~\cite{whittaker1994} showed that access to verbatim speech in the meeting browser boosted participants' confidence in their recall and helped them correct any mistakes in their manual notes. We also observed at least one participant intentionally reference non-speech meeting content in their explorations of our system when referring to a presentation. Combining dialogue summaries with additional artifacts like slides, illustrations, and attached files could provide a more holistic picture of the meeting~\cite{topkara2010tag}. This relates to previous meeting browsers that provided index into multimedia records~\cite{cruz1994capturing} with voice and text annotations or summaries for audio-video presentations based on how users browsed the recap~\cite{he1999}.

%\sumit{talk about explicit and derived indices} Streams [11] is an example of a system that allows reviewers to index multimedia records of presentations or meetings with voice and text annotations. For instance, he et al~\cite{he1999} create summaries for audio/video presentations based on users' access patterns. Gross et al. [17] recognize action items from speech and create summaries based on the meeting record. %So meeting recap systems should include easily accessible  deep links to transcripts and video recordings (e.g. \cite{topkara2010tag}) from any summarized content. 

% \leadin{Non-text artifacts like visual content needed to enhance sense-making}
% Dialogue summarization does not capture other meeting artifacts like presentation slides, hand gestures, and whiteboard illustrations -- all of which have been shown to contribute to sense-making~\cite{moran1997ll} in the very early meeting recap browsers explored previously. 


%\leadin{Opportunities for asynchronous collaboration}
\leadin{Meeting recap as a natural artifact for collaboration}
Prior meeting recap systems~\cite{ehlen2007meeting, banerjee2005necessity} focused on the effective individual recap, but their collaboration aspect was under-explored due to a lack of mature collaborative editing technologies. As an exception, Geyer et al.~\cite{geyer2005towards} designed a customized system to associate domain-specific artifacts with indexes in meetings for later collaboration. However, their evaluations highlight the difficulty of using the recap system's artifacts outside the system because the recap is part of a highly customized interface.

For the first time, through our system, we could generate a recap as a document resembling manual note-taking and similar in form to documents in organizations where people collaborate~\cite{samuli2003} (e.g., Google documents, Microsoft Word). Thus, all our participants expressed their desire to engage with the meeting content afterward. Seeking clarifications and follow-up discussions around a specific point raised in the meeting, sending key decisions over email, or using the entire hierarchical recap as a document to comment on indicates the desire to use the recap to support post-meeting work.
%Participants need to start a conversation around a specific point raised in the meeting--to get clarifications and extend discussions suggest designs to enable such collaborations. Two participants imagined the \emph{hierarchical} view as a document that could have comments added to it, like a Word document or a Google document, by highlighting a section of the text.  
% One participant imagined sending parts of the recap to a chat or email thread for a follow-up discussion. We hypothesize that there is such a high interest in asynchronous collaboration with recap due to its familiarity with handwritten notes and the maturity of collaborative editing technologies without needing any additional technological affordance.

Using meeting recap as a collaborative document facilitates the use of the meeting to increase remote work and cross-timezone collaboration.  With such functionality, it could be possible to both miss a meeting and actively participate in the discussion started by that meeting by consuming the recap and continuing the discussions asynchronously~\cite{richter2001}.  

\leadin{Permanence and transparency of recap can change work practices} Meeting recap supports an inherently collaborative workspace. Transparency imposed by meeting recap can change existing work practices~\cite{smith2020}, a dimension previously overlooked in recap studies. Without any support, prior work suggests that there is limited post-meeting agreement over meeting discussions and decisions~\cite{mantei1988}, important discussions could be forgotten, and new "fictitious" ideas could be added~\cite{olson1992small}. Participants found value in the \high{} recap for consensus and transparency, and it could act as a documented reminder even for non-attendees for accountability, and reference. Since meeting recap makes meeting discussions explicit, permanent, and easily shareable, it can potentially change work obligations similar to how algorithmic support to Wikipedia processes changed Wikipedia work practices~\cite{halfaker2016ores}. Visibility and permanence of meeting discussions can change what organizational workers share and how they share it in meetings~\cite{kling1992cscwprivacy,scott2023toward}. Future work should understand how meeting recap technologies can consentfully integrate into work practices.

% Olson Carte and Storrsten [15] found that the groups using shared tools generated a better meeting report and better design ideas - we also find that shared docs are a good way to improve quality \sumit{revisit} Given the mandate to relay these meeting notes to a missing team member, large amounts of what took place was either forgotten or intentionally omitted. In addition, new ideas were added that had never been discussed, 9\% for the unsupported groups and 15\% for the supported groups (dynamic meeting annotation and indexing).


% Related to AI errors, participants' reports of misattributed pronouns could be detrimental to group dynamics, given that a recap is visible to everyone~\cite{branham2014co}. In our discussion about how participants would use recaps for collaborations, they indicated more care in editing the recap if it was like a collaborative document. 


\begin{table}[h]
\centering
\colortbl \begin{tabular}{|p{6cm}|p{7cm}|}
\toprule
User behavior & Implication for system \\
\midrule
User edits summary & Summary item after the edit is better quality \\
User shares summary & Summary item is important to user \\
User opens a section in hierarchical view & Section is relevant to the user \\
User looks up source dialogues for summary & Summary item is relevant to the user, and possibly lacks full context\\
User deletes summary & Summary could be non-relevant, wrong or poorly written\\
\bottomrule
\end{tabular}
\caption{User interactions with the UX that can be used to provide feedback signals to the model on summary quality.}
\label{tab:d2l}
\end{table}

\leadin{Designing to learn}
  The models we used to construct these recap experiences were trained on crowd-sourced data--data gathered by asking crowd workers to summarize and label the \kp{s} and \ai{s} from the meetings they did not attend. 
  There will always be a gap between someone's understanding of the meeting who attended it and is part of the social context of the meeting and someone who is merely reading the transcript. Understanding meetings through transcripts is akin to the problem of grounding and overhearers~\cite{schober1989understanding}, which suggests that labelers will never perfectly capture what participants understand because conversations utilize implications and grounding.  Thus, models built on the labelers they produce are rarely well aligned~\cite{christian2020alignment} with the intended use~\cite{yang2019clinic}.  At best, current AI systems trained from a crowd-level understanding of relevance yield generalist models, \emph{i.e.}. They work well on instances of high consensus, but their performance will decrease if users disagree on instances~\cite{yang2017role}. Moreover, due to privacy considerations and the need for context, very few publicly available meeting datasets exist~\cite{rennard-etal-2023-abstractive}.

  Thus, for such applications, it is important to identify methods to improve systems through in-context user interactions~\cite{ehlen2008}. %Designing with a feedback loop that engages a user in-context affords a way for models to improve from contextual data from real users who are interpreting a meeting through a relevant social context. 
%Asking users for explicit feedback demands more of their time, and is infeasible given the demands of the amount of feedback.
However, the interactions may not always have consistent meaning (alignment). In our study, participants indicated varying degrees of consistency in meaning for adding, editing, and deleting notes and tasks. All participants agreed that adding notes or tasks had consistent meaning (alignment), i.e., they will add notes/tasks or tasks that are relevant to them and that the AI should learn to produce.  Editing notes/tasks also had consistent meaning, implying that participants would edit notes to reflect the discussion more accurately and that the AI could learn from their cues to improve. Most participants suggested they would be more likely to edit the notes if their edits benefited others or if they were planning to share them with the group (quantity). Deleting notes was more nuanced for participants, as participants expressed various reasons for deleting them -- not relevant to their task, unclear to make sense, or low relevance.

Therefore, when designing an AI to learn from a user's actions, it is useful first to elicit the reasons behind the user's actions. Feedback from additions and edits to the summaries could be directly used to re-train models to improve precision, recall, and the accuracy of summarized dialog. However, feedback related to items that should not have been suggested as notes/highlights or should not have been called out as tasks should be gathered in a way that makes the user intention less ambiguous. Table~\ref{tab:d2l} summarizes possible user behaviors and their implications for improving the system's quality from feedback.

\section{Limitations} 
\textbf{Study limitations.}
Our participants had a basic technical background, limiting the exploration of diverse user contexts. Recording meetingsa requirement for this studywas not commonly accepted and posed privacy concerns and discomfort, affecting participation. Future research should consider how to manage privacy aspects when studying meeting recaps. All participants attended their respective meetings and had fresh context. While prior studies suggest non-attendees benefit from meeting notes, our study focused on immediate recap needs, which may not capture the full spectrum of insights. Longitudinal studies with diverse and randomized participant sequences are needed to draw more comprehensive conclusions. Participants familiarity with technology and AI might have influenced their tolerance of AI errors and their perceptions of the recap tool's usefulness. Different organizations may find novel uses for meeting recaps that we did not cover and encourage to explore in future studies. 

\textbf{System design limitations.} 
Our design focuses on identifying key meeting moments (\high{}) or providing a rolling summary (\hier{}) as meeting minutes. Some meetings involve revisiting earlier topics~\cite{park2024}. \hier{} recap does not break the pattern of non-linearity, as revisiting prior topics after new topics are introduced will still be captured as chapters. To enhance this, future work can explore additions of explicit text or visual references that link related discussions, helping participants understand the non-linear structure. Our system uses ASR transcript as input for summarization which could attribute the same name to multiple speakers if they are attending from conference room. In such scenarios, additional intelligence is needed on the ASR side to distinguish speakers based on voice characteristics (e.g., tone).
% Our design decisions involve identifying important meeting moments (\high{}) or providing a rolling summary of meetings (\hier{}) in the form of minutes. There may be meetings where participants jump back to previous topics discussed early in the meetings. Our proposed \hier{} recap does not break the pattern of non-linearity, as revisiting prior topics after new topics are introduced will still be captured as chapters. However, to highlight this structure, the recap can be augmented with more explicit affordances, such as providing explicit text or visual references that link related discussions and help participants understand this non-linearity.

We do not employ state-of-the-art LLMs for our work; instead, we use a sequence of low-resource transformer models that we trained on summarization datasets. The sensitivity of participants' real organizational meeting data motivated this choice. Because our system generates meeting recap in a two-step process, instead of a single model processing the entire transcript, our system divides the complexity of the task across several transformer models (e.g., identifying \kp{s} and \ai{s}, abstract summarization of small dialogue segments)~\cite{rennard-etal-2023-abstractive}. Larger models like GPT4 also introduce errors in abstraction summarization~\cite{ramprasad2024analyzing}. However, such models may provide additional benefits such as "circumstantial inference" where the summary is not directly stated in the meeting, but inferred from the dialogues~\cite{ramprasad2024analyzing}. Future work could explore how meeting recaps can benefit from such inferred summaries. %Moreover, the sensitivity of participants' real organizational meeting data made it impossible to use third-party LLM services for summarization. Thus, locally trained and hosted low-resource BART-based models struck a reasonable balance of user data privacy and monetary and environmental costs against task performance. We provide an example for future work to weigh such costs against the benefits of LLM-based systems carefully.



% One important consideration for adopting low-resource technology was the privacy of participant's meeting data. Meetings for organizational work constitute sensitive personal and organizational data, and it was not possible for us to send this data to LLM services for processing. Thus, we trained all our models locally. We hope our approach can also serve as a reference for future work when the privacy of user data or the environmental or monetary costs of using LLM services outweigh the gains in performance improvements against low-cost models. Nevertheless, LLM-based recap systems may generate better-quality summaries at the cost of significantly more computational resources.

% Our technical approach can be applied to any meeting context. However, training data might limit generalization, as our models were trained on open ICSI and AMI meeting datasets. Meeting contexts that diverge significantly from such contexts may need additional fine-tuning of the models for effective use. LLMs may afford some additional benefits such as inferences through dialogues to capture in summaries~\cite{ramprasad2024analyzing}. However, LLMs have also been shown to be prone to hallucinations as the context window of input increases~\cite{}.

\section{Conclusion and Future work}
%\ah{engage with the limitations in conclusion - most of the interviews are people from research, however alignment between what we learnt and saw suggests that it might generalize, we also didn't do quantitative work, to justify if what people say matches with what they do-}
With more meetings happening in organizations and an increasing proportion of them turning to hybrid or online, effective meeting recap can provide participants a way to strategize their meeting workload more effectively. We propose designing a meeting recap system with two different recaps that target distinct but complementary use cases -- 1) quick highlights recap and 2) Full meeting minutes segmented into chapters. We perform a preliminary evaluation of the recaps with users in a within-subjects design. Our findings suggest that participants value both highlights and the hierarchical recap. Future work can build on these findings to design and evaluate recaps at a larger scale.

%\change{this is not intended to be generalizable. We focused on progress update meetings that involved some reporting of analyses and next steps. Diff affordances necessary for other meeting types.}

Meeting recap is a challenging domain where ML models trained through crowdwork are less aligned with the target audience's needs (e.g., organization workers)~\cite{yang2019, star1994}. This creates the need for designing interfaces that generate useful training data so that models can be improved as part of ``natural'' use~\cite{yang2017role}. Our findings also provide preliminary insights for designs that allow participants to easily interact with the notes that reflect their preferences, so they can also be used to generate useful training data. Quick edit suggestions or memory aids to recall missing notes are examples of such design ideas. Future work can set up quantitative experiments to validate the quality of training data gathered from user interactions in a complex meeting recap-like application.

From a collaboration perspective, our findings suggest using notes for more than just meeting recaps. Future work can explore the ways meeting participants can potentially engage in discussions around notes and their impact on generated ideas, consensus, or information exchange between participants. 


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix


%\section{Research Methods}


%\subsection{Part One}

%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
%malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
%sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
%vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
%lacinia dolor. Integer ultricies commodo sem nec semper.

%\subsection{Part Two}

%Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
%ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
%ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
%eros. Vivamus non purus placerat, scelerisque diam eu, cursus
%ante. Etiam aliquam tortor auctor efficitur mattis.

%\section{Online Resources}

%Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
%pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
%enim maximus. Vestibulum gravida massa ut felis suscipit
%congue. Quisque mattis elit a risus ultrices commodo venenatis eget
%dui. Etiam sagittis eleifend elementum.

%Nam interdum magna at lectus dignissim, ac dignissim lorem
%rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
%massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
