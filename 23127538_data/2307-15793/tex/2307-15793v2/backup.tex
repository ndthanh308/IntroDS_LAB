%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%% Can we write this down? - A system for generating automatic meeting recap and a study of its effectiveness
%% Designing effective and adaptive meeting recap experiences with LLMs: Can the AI write this down for us? 
%% A system for automatic meeting recap: Can the AI write this down for us? 
%% That meeting could have been an email -- and now it is!
%% All meetings are now emails. 
%% Learning to summarize meetings
%% Leveraging cognitive science and LLMs to summarize meetings
%% Summaries, Highlights, and Action items: LLM-powered meeting summarization
%% Summaries, Highlights, and Action items: LLM-powered meeting summarization and a study of its effectiveness
%% Meeting recaps should be hierarchical and other insights from an LLM-powered summarization system
%% Meeting recaps should be hierarchical and other insights from the lab study of an LLM-powered summarization system
%% Summaries, Highlights, and Action items: An LLM-powered meeting recap system and a study of its effectiveness

%Thus, dialogue summarization metrics like ROGUE, or crowd-sourced evaluations, cannot be directly used to evaluate useful meeting recap~\cite{whittaker2008design,kalnikaitundefined2008}. 
%Participants seeking brief highlights to swiftly grasp key decisions and action items find such summaries overly verbose and cumbersome to navigate. Conversely, those needing an in-depth understanding of discussions to comprehend the context behind decisions are left with incomplete information due to the imposed length constraints. Customizable summaries that adapt to the specific needs of users, whether they prefer concise overviews or comprehensive minutes, are essential to effectively support diverse meeting goals and enhance the utility of meeting recaps.
%Studying how people apply new technology for their work (e.g., using ML decisions to judge vandalism edits)~\cite{star1994, yang2019} yields better measures of relevance and close \emph{alignment} between the technology and its context of use. 

%along three dimensions of CSCW group work: 1) Help participants make sense of their meetings, 2) Provide interactions that closely reflect users' intentions as they engage with the notes for sensemaking, and can be used to generate good quality contextual training data for aligning the models. 

% As hypothesized, we observed the need for a gist of discussion or key decisions, depending on the role and context. 
% We find that dialogue summarization generates summaries that are understandable by users, allowing them to recall their meeting, plan action items, and share, improving over the distractions observed from transcript-based browsers~\cite{kalnikaitundefined2008}. 

% Meetings are important to organizations
%Meetings play a critical infrastructural role in the coordination of work.  In recent years, the nature of meetings have been changing with the shift to hybrid and remote work -- meetings have moved into computer mediated spaces in new ways that have lead to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g., automated transcription/captioning and recap support).  Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs.  Yet it is unclear how to best apply these technologies to support cooperative work.
%In this study we describe the design, implementation, and evaluation a meeting summarization system. We evaluate the effectiveness of the system in two different experiences that deliver complimentary insights - important ``highlights'', and a structured, ``hierarchical'' minutes view informed by insights from the cognitive science literature on event perception and recall. We evaluate the effectiveness of the system with seven users in the context of their work meetings. Our findings suggest that while LLM-based approaches still lack an understanding of whats personally relevant to participants and sometimes miss important details, our experimental recap experiences have the potential to dramatically improve post-meeting recap experience, save workers time, and provide a space for asynchronous collaboration. We also report on implications for designing AI systems to partner with users to learn and improve from \emph{natural interactions} to overcome the limitations related to personal relevance and summarization quality. 

%Meeting browsers have attempted to ease recap by presenting a mix of audio, video, or transcripts generated using automated speech recognition (ASR) and containing some bookmark-style annotations for navigation~\cite{whittaker2008design}. While such browsers provide a record to reference, raw utterances and recordings require participants to manually search for and re-interpret relevant content from the meeting~\cite{arons1992techniques, moran1997ll}.

% While current meeting technologies allow for effective participation within meetings, like seamless high quality videos and raising hands for participation~\cite{sarkar2021, koh2022}, tools for supporting meeting recap either rely on participants to engage in distracted note-taking during the meeting or require the user to sort through dense meeting content (e.g. recordings or transcript) by inefficient scanning patterns~\cite{moran1997ll}. 
%but they are too long and dense to be consumed efficiently~\cite{arons1992techniques, moran1997ll} and 
% This also limits asynchronous participation -- opportunities for non-attendees to engage with the meeting content and with attendees after the meeting has taken place -- due to the lack of context in generated summaries~\cite{kalnikaitundefined2012}. 

% Directly applying dialogue summarization to yield a fixed-length meeting summary may not support participants' diverse meeting goals~\cite{tuggener-etal-2021-summarizing} \sumit{discuss highlights and hierarchical}. Effective recap requires deciding what should be summarized, how much should be summarized, and how the summaries should be presented. These choices depend on the nature of the meeting and the participant's personal and group work goals~\cite{jia2022taxonomy,prasad-etal-2023-meetingqa,banerjee2005necessity, whittaker2008design}.
%However, meeting dialogues are long discussions between multiple participants for disseminating information, achieving consensus, negotiating deals, and deliberations~\cite{tuggener-etal-2021-summarizing,allen2015cambridge}. Directly applying dialogue summarization to yield a fixed-length meeting summary has been challenging due to scattered information, multiple and changing topics, unclear topic boundaries, and diverging expectations of meeting participants from the recap~\cite{jia2022taxonomy,prasad-etal-2023-meetingqa,banerjee2005necessity, whittaker2008design}. 
% A single recap representation is unlikely to capture all such needs~\cite{tuggener-etal-2021-summarizing}. 


% how to design a meeting recap using dialogue summarization, and evaluate how  salient recap designs address user needs
%of how dialogue summarization can benefit end user's recap needs in their organizational context, and identify interactions that empower users to indicate relevance to improve recap models, 


% Highlights

%All participants agreed that sharing usually means quality in that they would fix/edit before sharing, but not always.  E.g., some participants pointed out that they would be willing to share imperfect notes/tasks with teammates and peers while they would spend time making sure the notes/tasks are accurate before sharing to managers or sharing with people they were not close to. 
%Most of the participants specifically asked to share to a collaborative canvas (many referred to gdocs or a fluid component) (5).  Most wanted to share via meeting chat (5) while some wanted to share via email (3) and others mentioned sharing directly to a task management tool (3).  Many pointed out that they might want to only share some tasks/notes – not the whole set (4).  Further, others mentioned that they wanted to leave comments directly on notes/tasks like commenting on a word document (4).
%Only one participant immediately found the share button in the upper right.  Many tried to copy and paste what was on the screen or take a screenshot to paste it into an email.  
%Participants mentioned that they would ensure quality when sharing notes, but this behavior is dependent on the hierarchy of the person they share with. Participants were okay sharing imperfect notes with colleagues at the same level or below, but expressed that they would edit the notes to fix quality if sharing with their managers. \sumit{should this go at the end of the section as a summary?}

%Sharing behavior expressed by participant's also indicates interesting implications. Four asked for basic functionality for starting a conversation around a specific point raised in the meeting--sometimes to ask for clarity and sometimes to continue the discussion beyond the bounds of the meeting.  Two participants imagined the \emph{hierarchical} view as a document that could have comments added to it like a Word document or a Google document by highlighting a section of the text.  One participant imagined sending parts of the recap to a chat or email thread for a follow-up discussion. We hypothesize such a high interest in asynchronous collaboration with recap due to its familiarity with handwritten notes, and documents that are easily shareable without needing any additional technological affordance.

% \begin{displayquote}
% I would even share it if it was not 100\% right if they had the ability to edit as well.  I would look at it, edit stuff, and send it out to people.  I'd like a URL where someone else can make an edit and we can all see it – better than sending an email.  
% \end{displayquote} 

% P01 learn words that are related to concepts in the meeting
% P02 - AI should learn how to get the sentence right.  It can learn from the new sentence.
% P02 - i would only edit it once after the meeting
% P03 -  I remember these because they are important parts I needed to do. I’m writing down the mistakes I made, the things I need to correct, and the main task.  
% P03 - The AI should learn that a note I added represented a very long part of the discussion in the meeting.  
% P03 - If I edit something, the AI did a good job identifying where the action item was but the summary was wrong.  So it should use my edit to become more accurate.
% P04 - It would be great if it could learn how to summarize this discussion.  This discussion took place over 20 minutes.
% P04 - I’d do that pretty often.  If I just need to add to what it populates, it should take less than 5 minutes.  If it is helping me, and it starts getting better, I’m happy to do it.  It would be easier right after the meeting.  It would be easy to do this.  At the end of the meeting we have a summary, what was discussed what are the next steps, so it would be easy to do it. 
% P04 - Even if I don’t remember perfectly, I would add it.  Ideally I would do this right after the meeting.  If I’m not positive, I might indicate that in the notes like “I think we decided this and suggest someone can correct me if I’m wrong. “
% P04 - Easier to fix notes than it is to write notes.  You don’t need to remember what happened. Because with adding you have to first recall what happened in the meeting, then write it, with fixing  its already there.
% TODO: adaptive interactions based on their likelihood to edit

% \texttt{P03} explained while editing an action item
% \begin{displayquote}
% It needed context to make the item more actionable. Now I can delete the second one that I added. 
% \end{displayquote}

% While making edits, P01 notes
% \begin{displayquote}
% The ordering of notes suggests priority, but it looks like it is chronological.  I might move the less relevant notes to the end.
% [...]
% Moving a note that is low priority to an action item helped me see that it is actually high priority. 
% \end{displayquote}

%<quote add reason: include decision>
% P06 reflects while adding context to a task
% \begin{displayquote}
% When I look at the context, I’m able to fix.  It is accurate but it didn't have enough context. 
% \end{displayquote}
% P01 said while reflecting on reasons to edit (fix grammar):
% \begin{displayquote}
% But it only needed minor edits. <inserts the verb “include” to a note> What was happening here was unclear.  This is fairly understandable.  Maybe I’m used to AI generated stuff. 
% \end{displayquote}
% P01 also added ``<shown on screen>'' to a note to reference non-audio meeting content. 

% Hierarchical
%All participants pointed out various imperfections in the chapter names, but all pointed out that they were still useful.   Most participants called out how the experience helped them remember the meeting better than the Highlights View (6).   The chapter headings helped them drill down into a relevant section of the meeting (6).  Some participants pointed out that the experience helped them understand the context of the notes/tasks extracted in the Highlights View.  Most immediately understood the meaning of checkboxes and stars (5) and some were able to add stars (4) and tasks (1).  Some expressed that this would be a good way to teach the AI what it missed (3). 

%Most participants expressed that the Hierarchical View captured how they imagined the structure of a meeting (5).  A majority also saw complimentary value in both the Highlights and Hierarchical views (5) (with three calling out specifically the value of Hierarchy for notes vs. Highlights for tasks).  One participant said they preferred only the Highlights View while four participants expressed a preference for the Hierarchical View. Two participants encountered non-rephrased action items in the Hierarchical View and were able to quickly and easily rewrite them without prompting by the interviewers (2).

%\textit{Topic segmentation and chapter headings useful}

%TODO: experience with error still holds value

%\textit{Hierarchical view better for missed meetings.}
% \texttt{P06} said:
% \begin{displayquote}
% If I missed a meeting, I like hierarchical the best.  Either I missed the meeting and I want to know all about it and I get the overview quickly.  Or it helps me narrow down. 
% \end{displayquote}

%The chapter titles provided participants an easy way to grasp the themes of the meeting~\cite{zacks2001perceiving}.
% We also observed that participants' recall of the meeting was impacted if the chapters contained more errors. Two participants faced difficulty in getting a quick overview because they observed many titles that did not make grammatical sense or were out of context. \texttt{P06} noticed these errors as they had a number of poorly worded titles in their meeting recap
% \begin{displayquote}
% We have the accuracy problem.  Some are better than the others.  <Points out an accurate and important chapter>  The idea of these headers is very useful.
% \end{displayquote}

% \leadin{People think of meetings in hierarchy} %When  participants looked at the hierarchical experience, they understood it without any difficulty. 
% In some cases, participants even directly mentioned that they think of meetings in hierarchy. \texttt{P01} said 
% \begin{displayquote}
%  I can remember and understand [the hierarchical view] conceptually. [...] Hierarchical looks like a meeting.
% \end{displayquote}

% However, unlike dialogue summarization, it is challenging to design and evaluate meeting recap representations on offline datasets using automated metrics or crowd-sourced evaluations due to the richness and diversity in meeting goals and user's context~\cite{whittaker2008design,kalnikaitundefined2008}. Prior research in CSCW indicates that studying how users apply technology in their work (e.g., ML-assisted Wikipedia editing or ML-assisted medical note-taking)~\cite{knoll-etal-2022-user} is essential to identify technology's \emph{alignment} with contextual user needs that automated metrics or crowd-sourced evaluations cannot measure~\cite{smith2020, star1994,schaekermann2018}. Yet, evaluations in people's context of use are especially challenging because AI systems for complex work do not lend themselves well to traditional low-cost evaluation methods in HCI like heuristic evaluations or wizard-of-oz ~\cite{yang2019}.

% To address these gaps, we describe the implementation and in-context evaluation of two meeting recap designs. Using cognitive science and discourse theories, we first identify recap representations -- important ``highlights'', and a structured, ``hierarchical'' minutes view that serve important and different recap needs. We develop a system to operationalize the representations with dialogue summarization as its building blocks. Finally, we evaluate the system's effectiveness with seven users in the context of their work meetings. Our findings show promise in using LLM-based dialogue summarization for meeting recap and the need for both representations in different contexts. However, meeting recaps still lack an understanding of personal relevance.
%we find that LLM-based recap still lacks an understanding of what is personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics.
% We identify collaboration opportunities such as a shared recap document that a high-quality recap enables. We report on implications for designing AI systems to partner with users to learn and improve from \emph{natural interactions} to overcome the limitations related to personal relevance and summarization quality. We synthesize these findings as design implications for advancing the space of meeting recap in supporting group work in organizations.

%they exhibit significant issues if directly applied to summarize meeting long transcripts. Moreover, prior studies
%of recap highlight varying recap needs based on the user's context that no one design addresses, highlighting the need for in-context evaluations.

%In recent years, the nature of meetings has been changing with the shift to hybrid and remote work -- meetings have moved into computer-mediated spaces in new ways that have led to new problems (e.g., more time spent in less engaging meetings) and new opportunities (e.g., automated 

% Since crowd-sourced trained models are unlikely to \emph{align} with user needs in high-context applications like meeting recap~\cite{yang2017role}, we also explore what interactions can enable users to provide high-quality feedback to improve and align model behavior to their needs. 
%Our study context is organizational work, a core tenet of cscw~\cite{schaekermann2018}. 
%Building on CSCW scholarship on technological support for complex work~\cite{knoll-etal-2022-user}, we leverage the concept of \emph{experimental prototype experiences} from HCI~\cite{yang2019} to explore the design space of meeting summarization for recap support as well as ways in which user interactions with a meeting recap experience could be turned into useful training data to improve the \emph{alignment} of the summarization models. 

%We find strong support for a hierarchical approach to meeting summary presentation that is implied by cognitive theory on task structure understanding~\cite{zacks2001perceiving} and discourse analysis~\cite{grosz-sidner-1986-attention}. 
% With both "highlights" and "hierarchical," we found the need to personalize the recap (e.g., priority ordering of summary items). 

% situates the role of high-quality recap in supporting organizational work
% We also provide insights for thinking about UX interactions to improve summaries 
% designing interactions with recap systems that will produce log data effective for aligning AI behavior to \emph{personal} user needs.


% Making post-meeting experience more efficient by aligning recaps with the needs of the individuals as well as organizational practices. 

% What we learn about how people use this and make sense of it, and how it improves cscw organizational work in future work.


%Our work follows the lineage of CSCW \textit{Design and Evaluation systems} category of work that uses existing technological support to design systems for supporting group work~\cite{resnick1994, wallace2017} and evaluate the system's usefulness in supporting the organizational processes through a preliminary in-context study~\cite{star1994}.

% We employ an \emph{imperfect prototype experience}~\cite{yang2019} powered by existing state of the art dialog summarization algorithms to explore different strategies for constructing a meeting recap experience. 

%\ah{alignment and problem of getting in context data from users}
%\ah{past work meeting cscw - if we can give people a really simple taxonomy - that helps people as better participants, and another that helps people async communication, like a threaded video}
%Time travel proxy: using lightweight video recordings to create asynchronous, interactive meetings

% Shorten corpus based summarization, spend more time on dialogue summarization
% allude to extractive and abstractive summarization and what are the contrasts - extractive for documents is very good not for dialogues, abstractive is good for dialogues, 

%ASR technology while mature suffers from the challenges like miscategorizations, lack of context, and poor generalizability across vocabularies~\cite{mcgregor2017}. 
% We should say more plainly that there are no studies of dialog summarization applied in context.

% We build our system on Cohen et al.'s two-stage process but evaluate the relevance of meeting recap for the participant's work context beyond simply evaluating the faithfulness of generated summaries to original meeting content. 


% Current evaluations in summarization explore the faithfulness of summaries to reference text dialogues~\cite{liu2021}, %interactive alignment with a user relevant concept~\cite{dash2019, ghodratnama2020adaptive} such as factuality~\cite{tang-etal-2022-confit} 
% or automated metrics like ROGUE~\cite{lin-2004-rouge}. While ROGUE supports evaluating summaries for text corpus or short dialogues~\cite{gliwa-etal-2019-samsum}, metrics and crowd-sourced evaluations are not a good choice for evaluating meeting recap as they require high contextual knowledge to assess relevance~\cite{star1994}. 
% Factors like the user's background, organizational contexts like established recap practices, and cognitive load can decide the user's contextual recap needs~\cite{whittaker2008design}. ~\citet{rennard-etal-2023-abstractive}'s survey of abstraction summarization specifically calls out the need for new metrics and datasets for evaluating meeting recap. We build our system on Cohen et al.'s two-stage process but evaluate the relevance of meeting recap for the participant's work context beyond simply evaluating the faithfulness of generated summaries to original meeting content. 

% Cohen et al. ~\cite{cohen-etal-2021-automatic} explored a two-stage process: 1) Identify the important meeting segments in transcripts by annotating important utterances for relevance, 2) Paraphrase the utterances identified in step (1) using dialogue summarization to generate abstractive notes and add them to the summary. 

%Thus, participants still prefer manually created notes either by themselves, or others~\cite{nathan2012} that captures a more refined gist of discussions. Manual note-taking also helps participants provide cues for later retrieval~\cite{kalnikaitundefined2008,kalnikaitundefined2012, reilly2011shared} and improve subject matter understanding in lectures~\cite{costley2021collaborative, mik2019effects} due to more cognitive involvement. Thus, we study the potential of recent transformer models~\cite{lewis2020} in generating notes that help participants recap easily. 
% Recap in online discussions is similarly complex and has similar opportunities.  Structured presentation of discussion in a hierarchical format has been found to be helpful for recap sense-making activities~\cite{zhang2017, nam2007}. Moreover, Zhang et al. found that simple information augmentation on messages like attaching discourse labels (e.g., action, answer, decision) to slack discussions~\cite{zhang2018} helped participants skim and filter relevant discussions on asynchronous discussion forums. However, their augmentation still required participants to add tags manually, which is not feasible when attending meetings online~\cite{marshall2002}.

% Due to such diverse roles and expectations from meetings, there is considerable debate on what constitutes a good recap~\cite{whittaker2008design}. Including everything in the recap can lead to increased cognitive burden on participants, no better than video, and fixation on ideas~\cite{moran1997ll, jansson1991design}. Acknowledging this diversity, we conceptualize two complementary meeting representations -- 1) focusing on important decisions and discussions to act as "cues" into the memory of attendees~\cite{kalnikaitundefined2008} and serve agenda-driven meetings, 2) a "minutes" like view that provides a chronological meeting record helps understand the purpose, discussions, and decisions of the meeting to support knowledge-driven meetings~\cite{whittaker2008design}. We leverage cognitive fit-theory~\cite{speier2006influence} and theory of discourse interpretation~\cite{zacks2001perceiving,grosz-sidner-1986-attention,mann1987rhetorical} to argue that these two representations serve different meeting goals, as the structure of each recap is in alignment with specific meeting structures. 

% For attendees, meetings serve as reminders, acting as "cues" into their memory~\cite{kalnikaitundefined2008}, for which a meeting recap with the most important discussions and decisions suffices. However, for non-attendees and attendees after a sufficient time since the meeting, a "minutes" like view that provides a chronological meeting record helps understand the purpose, discussions, and decisions of the meeting. In-context prior evaluations with meeting attendees surface similar needs~\cite{whittaker2008design}. According to cognitive fit theory~\cite{speier2006influence}, the structure of the meeting recap can determine the kind of tasks and sensemaking that it can support. A recap composed of a list of key discussions and decisions is suitable to aid recall when participants already have meeting details in their memory. "Minutes," like chronological structure is supported by theories in discourse and cognitive science. Zacks et al.\cite{zacks2001perceiving} show through experiments asking people to record, recall, and communicate about video recordings that people tend to put together events in a hierarchical structure where the object of reference (e.g. a meeting agenda item) is at the top of the hierarchy and details discussing and referencing that object appear below.  %At the very bottom, Zacks refers to 
% They refer to details at the very bottom as ``behavioral primitives'' -- details that are so low level that they are often ignored in summary and communication.  This structure roughly reflects the practice of producing meeting ``minutes'', a summary record of what took place at a meeting that is intended for general recap use cases. Theories in discourses find similar evidence of structures in written work documents. 
% Intentional theory of discourse~\cite{grosz-sidner-1986-attention} posits that discourses follow a hierarchical structure with intentions guiding the discourse and discourse segments form individual sub-topics within the discourse. Mann et al.~\cite{mann1987rhetorical} studied letters, memos, newspapers, and scientific articles and found that similar hierarchical relations hold between parts of written text. Moreover, the hierarchical structure also emerges in our sensemaking of temporal events condensed in the Rhetorical Structure Theory of text. Thus, there is compelling evidence in cognitive science about hierarchical structures in sense-making both during perception and during the expression of information.

% Meeting recap supports complex organizational workflows that require participants to identify relevant meeting discussions, and decide how to act on them~\cite{allen2015cambridge}. Naturalistic sense-making under constraints (e.g., limited time, multi-tasking) for such complex tasks has been shown to be impacted both by limitations of the technology and interactions of the technology with the user's context (e.g., how much context the user has about the meeting, or their time availability). Since an "ideal" summary can have many different representations for different users (e.g., most important points, full minutes, action items), using dialogue summarization to generate actual recap representations from people's meetings and evaluating with them is necessary to help bridge the gap between user expectations, and technology affordances~\cite{star1994, resnick1994, whittaker2008design}. However, conceptualizing good representations is hard without well-defined measures of quality~\cite{yang2020}. Thus, we leverage the cognitive fit theory to conceptualize representations whose structure aligns with potential user needs~\cite{speier2006influence} and represent them using "design rationales"~\cite{lee1991s}. Design Rationales (DR) are used in HCI to evaluate computational support for complex tasks in user's context. Design Rationales explicitly define representations, their supporting reasoning, and criteria for their evaluation against reference tasks~\cite{lee1991s}. Since prior work has not explored the in-context usefulness of AI-generated meeting recap to participants, we take an interpretive approach to evaluate the design rationales using interviews. Our goal is to discover themes that capture relevant aspects of meeting recap in relation to usefulness for end-user needs.

% Past work called for a system that does this, we implemented a system that does this, this is from a long line of cscw work.
% Dialogue summarization systems enable a level of automation in recap systems that were previously impossible and yet were called 
% Hybrid approach - complementary user needs are better satisfied if we were to 
% Fact that dialogue summarization makes a document, enables a lot of async collaboration.
% Recall difficulty proble,
% Pronoun problem.

% Talk about delete in table
% disc - multiple representations when designing for complex problems


%For example, feedback on whether the task was low relevance, or incorrectly worded can suggest which model in the pipeline should the example be used during retraining %(extractive or abstractive, elaborate more on this...).

%From a design to learn perspective, we have some clear conclusions.  All participants agreed that adding and editing notes/tasks had consistent meaning (alignment).  They also agreed that situations where notes were shared or otherwise collaborative would motivate them to fix them (quantity).  However, deletion was much more complicated with varied meanings that might be hard for an AI to differentiate, so it seems that providing the user with a way to clarify their intent might improve our ability to learn from these interactions (alignment).  Generally, participants seemed open to the idea of doing extra work to help the AI learn – so long improvements due to their work were seen relatively quickly.  So, asking “why” for a deletion might be well received.

% Collaboration
%\sumit{discussion on sharing}
%A set of participants also expressed the need to have document like commenting and collaboration on the notes and tasks. This suggests the potential for meeting notes to act as collaboration artifacts to foster ideation, and achieve consensus on to-do tasks. Shared meeting notes may even be a an effective means of collaboration, as participants have fresh context right after the meeting.

%The language models~\cite{lewis2020} that generated the summaries point to several directions of improvement. Participants found the headings in hierarchical experience to be sometimes inaccurate which is a result of limited training data, and training with crowdworkers[]. Others, such as misattributions or wrong pronouns are common issues that current Language Models suffer from, and need a more fundamental approach to solve~\cite{limisiewicz2022don}. Generating the most precise notes is related to the problem of grounding and overhearers~\cite{schober1989understanding} where the AI attempting to summarize a meeting lacks the relevant grounding to resolve ambiguities. Future work can explore how to add additional organizational level information to the models so that models can get more context about participant's discussions.



% Our findings on how participants explore the two recap experience point to interesting directions for building effective recaps.
% 1) Large language models are still not perfect at generating accurate notes, but participants are able to make sense from the context,
% 2) easy way to access the raw data always helpful for more context,
% 3) notes and action items have different relevance for different people, so personalized priority makes sense,
% 4) Participants like collaborative aspects to the notes
%5) They are more willing to add/edit/delete notes if it helps them later.
%Schober, M. F., and Clark, H. H. 1989. Understanding by
%addressees and overhearers. Cognitive Psychology 21:211–
%232. - rich discussion on subjectivity of meeting discussion relevance.

%% Coding
%Codes denoting the strengths of the recap focused on their ability for "quick takeaways," "quick overview," "effective for group decision-making," and "supporting discussions and knowledge sharing." Codes that captured weaknesses of the recaps focused on "difficult to recap", "lack of context," "limited recall," "ineffective for group decision-making," and "ineffective to support knowledge sharing." 

% We took special care to code responses in context of their recap browsing activity. For example, if a participant was glanced at a recap, and indicated its usefulness
%We began with open coding to capture the key aspects of the \high{} and the \hier{} recaps. Open coding was chosen to allow for an inductive and flexible exploration of the data, effectively uncovering emergent themes and patterns without being constrained by predefined categories. This approach was crucial given the novelty of the LLM-powered meeting recap tool and the diverse interactions of participants with it. For the highlights, initial codes focused on strengths such as "quick takeaways" and "quick recap," and weaknesses like "lack of process details" and "less recall." Similarly, for the hierarchical view, codes centered around strengths like "high recall" and "contains process details," and weaknesses like "very detailed" and "high redundancy." 

% We recruited participants from the research organization where we conducted the research to perform preliminary evaluations. Many of our meetings were progress update meetings that involved reporting analyses and next steps. All our participants had a basic minimum technical background due to employment at the organization, which limited our ability to explore how users from diverse backgrounds use recaps, such as universities, scientific labs, health, and legal domains. However, we note that our participants indicated a considerable range of recap use cases for their recap needs.
% Our study required participants to record their meetings and share them with the system to generate a recap and discuss them in the interviews. Recording is not a commonly accepted practice in the organization, and many participants were uncomfortable discussing their personal meetings or seeking consent from other meeting participants to record and share their meetings. These concerns are likely to come up in other contexts as well~\cite{ackerman2000intellectual}. To get meetings from diverse backgrounds, such as finance, human resources, scientific collaborations, and health, future research should carefully consider managing the privacy aspect of studying recap~\cite{kling1992cscwprivacy}. All the participants in our study attended their respective meetings and had a fresh context of their meetings. Attendees tend to lose meeting context as time progresses~\cite{nathan2012}. Understanding the perspectives of meeting participants weeks after the meeting could uncover additional insights (e.g., the need for richer context). 
% We evaluated both design rationales in the same sequence of highlights and then hierarchical. While the sequence can have an effect on the findings, it would be minimal due to the exploratory nature of our study. Future work aiming to draw statistical conclusions should consider randomized sequences for evaluating the representations. 
% We recruited participants from the same organization where we developed this system. All participants in our study are information workers who have a considerable awareness of technology and AI. It is possible that participants in our study were more forgiving of AI's mistakes in generating recap with relatively limited effect on their tendency to use the system. It's also possible that they may find different relevance for recap depending on their nature of work. However, we note that our participants indicated a considerable range of recap use-cases for their recap needs. However, other organizations such as universities, scientific labs, health, legal, and economic organizations may find more novel use cases of recap that we did not cover. 