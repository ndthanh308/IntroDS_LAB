\newcommand{\Img}{I}
\newcommand{\Cam}{\Phi}
\newcommand{\iI}{q}

% per ray, per shading point j, sub t
\newcommand{\Color}{c}
\newcommand{\ShadX}{x}
\newcommand{\iS}{q}
\newcommand{\iSS}{t}
\newcommand{\ShadNum}{Q}

\newcommand{\Dir}{\mathbf{d}}
\newcommand{\Rad}{c}
\newcommand{\Trans}{T}
\newcommand{\Dens}{\sigma}
\newcommand{\Step}{\delta}
\newcommand{\Contr}{\alpha}

%per Tensor
\newcommand{\TensorThreeD}{\tau}
\newcommand{\TensorCloud}{\mathcal{T}}

\newcommand{\TensorNum}{N}
\newcommand{\TensorX}{p}
\newcommand{\TensorL}{l}

\newcommand{\iT}{n}
\newcommand{\iLT}{m}
\newcommand{\TensorF}{f}
\newcommand{\TensorV}{\mathbf{v}}
\newcommand{\TensorWeight}{w}
\newcommand{\TensorSpace}{\omega}
\newcommand{\TensorGrid}{\mathcal{G}}
\newcommand{\TensorCompNumber}{R}
\newcommand{\TensorComp}{\mathcal{A}}

% location:
\newcommand{\CoveredSpace}{\Omega}
\newcommand{\ShadingX}{\chi}
\newcommand{\QueryNum}{M}

% multiscale
\newcommand{\ScaleNum}{S}
\newcommand{\iScale}{s}


% shading & features:
\newcommand{\Densf}{f^\Dens}
\newcommand{\Radf}{f^\Rad}
\newcommand{\AggF}{A}
\newcommand{\AppearanceM}{\mathbf{B}}
\newcommand{\AppearanceMC}{\AppearanceM^\Rad}
\newcommand{\AppearanceV}{\mathbf{b}}
\newcommand{\DensShift}{\epsilon}

% Net
\newcommand{\AppearanceFunc}{\psi}
% Loss
\newcommand{\GTColor}{\Tilde{C}}



%others
\newcommand{\ThresContr}{T_{\text{opacity}}}
\newcommand{\ThresDist}{T_{\text{dist}}}


% Figure environment removed
    
    % As illustrated in Fig.~\ref{fig:overview}, our full pipeline uses a pre-trained feed-forward network to reconstruct a point-based radiance field from captured images and achieves realistic rendering via differentiable ray marching.
    % In this section, we first introduce our Point-NeRF representation in Sec.~\ref{sec:pcl_render}. 
    % We then present our cross-scene network that can reconstruct our Point-NeRF representation via network inference, in Sec.~\ref{sec:pcl_gen}.  
    % This reconstruction network outputs an initial radiance field that reasonably approximates scene geometry and appearance. 
    % In Sec.~\ref{sec:prune_grow}, we describe our optimization process, including our point grow and removal techniques, that finetunes our model on a specific scene and quickly surpasses the visual quality of previous methods.
    
\section{Sparse Tri-Vector Field Representation}
We now present our novel radiance field representation.
In essence, our model consists of a cloud of small local tri-vector tensors at multiple scales, designed to leverage both sparsity and multi-scale spatial coherence (see Fig. \ref{fig:overview}).

Let $\TensorCloud = \{\TensorThreeD_\iT|\iT=1,...,\TensorNum\}$ denote a cloud of tri-vector tensors. 
Each local tensor $\TensorThreeD$ is located at $\TensorX$, covering a local cuboid space $\TensorSpace$ with an edge length of $\TensorL$. 
This cloud of tri-vector tensors represents a radiance field for the 3D space:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        \CoveredSpace = \bigcup_{\iT=1}^{N}\TensorSpace_{\iT}.
    \end{align}
% \end{adjustwidth}

Here, each tensor $\TensorThreeD$ encodes a local multi-channel feature grid that includes a (single-channel) density grid $\AggF_\Dens$ and a (multi-channel) appearance grid $\AggF_\Rad$, similar to the tensor grid in TensoRF \cite{chen2022tensorf}.
In contrast to using a single global tensor in TensoRF \cite{chen2022tensorf}, we model the volume density and view-dependent colors with multiple local tensors.
In particular, for an arbitrary location $\ShadingX \in \CoveredSpace$, we select $\QueryNum$ nearest tensors that cover $\ShadingX$. 
% Each tensor can extract the density features and appearance features at the location, based on the local scene factorization. 
Across the selected tensors, we aggregate the extracted density and appearance features recovered by their tri-vector factors for radiance field property regression, where the volume density $\Dens$ is directly obtained after the aggregation and the view-dependent color $\Rad$ is regressed by a small MLP $\AppearanceFunc$ along with the viewing direction $\Dir$.
The continuous radiance field can be expressed as: 
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        \Dens_\ShadingX, \Rad_\ShadingX = \AggF_\Dens(\{\TensorGrid^\Dens(\ShadingX)\}), \AppearanceFunc(\AggF_\Rad(\{\TensorGrid^\Rad(\ShadingX)\}), \Dir).
    \end{align}
% \end{adjustwidth}
% where $\TensorGrid^\Dens$ and $\TensorGrid^\Rad$ are the geometry grid and the appearance grid of a single tri-vector tensor. Features will be extracted from these grids. $\AggF_\Dens$ and $\AggF_\Rad$ are the aggregation functions. We follow \cite{chen2022tensorf} and model $\AppearanceFunc$ as a small MLP. 

\subsection{Local tri-vector tensors.}
\label{sec:localtri}
We apply the classic Canonical polyadic (CP) decomposition~\cite{carroll1970analysis} to model our local tensors with tri-vector components.

\paragraph{CP decomposition.} 
CP decomposition factorizes a $M$ dimension tensor $\mathcal{\TensorThreeD} \in \mathbb{R}^{I_1 \times I_2 \times ... \times I_M}$ into a linear combination of $\TensorCompNumber$ rank-1 tensors:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{equation}
    \begin{aligned}
        \mathcal{\TensorThreeD} &= \sum^\TensorCompNumber_{r=1}{\lambda_r \TensorV^0_r \otimes \TensorV^1_r \otimes ..., \otimes \TensorV^M_r},
    \label{eq:cp}
    \end{aligned}
    \end{equation}
% \end{adjustwidth}
where $\otimes$ denotes outer product; the weighting factor $\lambda_r$ can be absorbed into vectors $\{\TensorV_r^0, ..., \TensorV_r^M\}$.

\paragraph{Density and appearance tensors.}
In our case of modeling a 3D radiance field, we set the geometry grid $\TensorGrid^\Dens \in \mathbb{R}^{I \times J \times K}$ as a 3D tensor. And the multi-channel appearance grid $\TensorGrid^\Rad \in \mathbb{R}^{I \times J \times K \times P}$ corresponds to a 4D tensor. The fourth appearance mode is of lower dimension (compared with the spatial modes), representing the final dimension of the features sent to the MLP decoder network.

According to Eqn.\ref{eq:cp}, we factorize each tensor's feature grids, $\TensorGrid^\Dens$ and $\TensorGrid^\Rad$, by CP decomposition:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        \TensorGrid^\Dens &= \sum^{\TensorCompNumber_\Dens}_{r=1}{\TensorComp_{\Dens,r}} = \sum^{\TensorCompNumber_\Dens}_{r=1}{\TensorV^X_{\Dens,r} \otimes \TensorV^Y_{\Dens,r} \otimes \TensorV^Z_{\Dens,r}},
        \label{eq:denfact} \\
        \TensorGrid^\Rad &= \sum^{\TensorCompNumber_\Rad}_{r=1}{\TensorComp_{\Rad,r} \otimes \AppearanceV_r} = \sum^{\TensorCompNumber_\Rad}_{r=1}{\TensorV^X_{\Rad,r} \otimes \TensorV^Y_{\Rad,r} \otimes \TensorV^Z_{\Rad,r} \otimes \AppearanceV_r},
         \label{eq:appfact} 
    \end{align}
% \end{adjustwidth}
Here $R_\Dens$ and $R_\Rad$ denote numbers of component; $\TensorComp_{\Dens,r}$ and $\TensorComp_{\Rad,r}$ are the component tensors that are factorized spatially; $\TensorV^X_{\Dens,r}, ..., \TensorV^X_{\Rad,r}, ...$ are the 1D vectors with resolution $I,J,K$, modeling scene geometry and appearance along $X,Y,Z$ axis; $\TensorCompNumber_\Dens$ and $\TensorCompNumber_\Rad$ are the component numbers;  $\AppearanceV_r$ expresses the feature dimension.

As done in TensoRF \cite{chen2022tensorf}, we stack all feature-mode vectors $\AppearanceV_r$ as columns together, which ends up a $P \times \TensorCompNumber_\Rad$ appearance matrix $\AppearanceM$. 
This matrix models the appearance feature variations of the tensor and functions like a appearance dictionary. 
Note that naively following CP decomposition like TensoRF will assign a different appearance matrix for every local tensor.
Instead, we propose to utilize a global appearance matrix $\AppearanceMC$ shared across the entire cloud of local tensors, leading to a global appearance dictionary that explains the color correlations across scene. This further improves both the computational efficiency and model compactness of our model.

Therefore, each of our local tensors is represented by their unique local tri-vector factors $\TensorV^X_{r}$, $\TensorV^Y_{r}$, $\TensorV^Z_{r}$.


\paragraph{Feature evaluation.}
To achieve a continuous field, we consider trilinear interpolation when evaluating the tensor grid features. 
For a location $\ShadingX$, we first compute its relative position $\Tilde{\ShadingX}$ to the selected tensor located at $\TensorX$:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        \Tilde{x},\Tilde{y},\Tilde{z} = x - \TensorX_x, y - \TensorX_y, z - \TensorX_z.
        \label{eq:relapos} 
    \end{align}
% \end{adjustwidth}
Then, for example, to get $\TensorComp_{\Dens,r}$ at ($\Tilde{x},\Tilde{y},\Tilde{z}$), we can compute and trilinearly interpolate eight $\TensorComp_{\Dens,r}$ on the corners. As mentioned in \cite{chen2022tensorf}, applying linear interpolation on each vector first is mathematically equivalent and can reduce the computation cost. 
Under the rule of outer product, we have $\TensorComp_{r,i,j,k} = \TensorV_{r,i}^X \TensorV_{r,j}^Y \TensorV_{r,k}^Z$, then the interpolated density features at location $\ShadingX$ are:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
         \TensorGrid^\Dens(\ShadingX) & = \sum_r{\TensorV_{\Dens,r}^X(\Tilde{x}) \TensorV_{\Dens,r}^Y(\Tilde{y}) \TensorV_{\Dens,r}^Z(\Tilde{z})} = \sum_r{\TensorComp_{\Dens,r}(\Tilde{\ShadingX})},
         \label{eq:interp} 
    \end{align}
% \end{adjustwidth}
where $\TensorV_{\Dens,r}^X(\Tilde{x})$ is $\TensorV_{\Dens,r}^X$'s linearly interpolated value at $(\Tilde{x})$ along its $X$ axis. Here, $\TensorGrid^\Dens(\ShadingX)$ is a scalar.

Similarly, the interpolated appearance features can be computed as:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
         \TensorGrid^\Rad(\ShadingX) &= \sum_r{\TensorV_{\Rad,r}^X(\Tilde{x}) \TensorV_{\Rad,r}^Y(\Tilde{y}) \TensorV_{\Rad,r}^Z(\Tilde{z}) \AppearanceV_r} \\
                                      &= \sum_r{\TensorComp_{\Rad,r}(\Tilde{\ShadingX})\AppearanceV_r} \\
                                      &= \AppearanceM \cdot (\oplus [\TensorComp_{\Rad,r}]_r), 
         \label{eq:interp}
    \end{align}
% \end{adjustwidth}
where ``$\oplus$'' denotes concatenation, ``$\cdot$'' denotes dot product. The appearance feature $\TensorGrid^\Rad(\ShadingX) \in \mathbb{R}^P $ is a vector.

\subsection{Feature aggregation.}
\label{sec:feaAgg}
We propose to aggregate the features from $\QueryNum$ neighboring tensors to jointly model the volume density and appearance for each 3D location $\ShadingX$. 
In particular, inspired by Point-NeRF, we leverage an inverse distance-based weighting function to directly aggregate the multi-tensor features.
Specifically, this weight can be expressed by
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        \TensorWeight_\iLT=\frac{1}{\lVert \TensorX_\iLT-\ShadingX \rVert}.
    \end{align}
% \end{adjustwidth}

% DVGO \cite{sun2022direct} demonstrate that directly regressing density from features is the most computationally efficient way to model high-fidelity scene geometry.
With this weight function, we directly obtain the density feature via the weighted sum:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        f^\Dens(\ShadingX) = \frac{1}{\sum \TensorWeight_\iLT} \sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT \TensorGrid_\iLT^\Dens(\ShadingX).
    \end{align}
% \end{adjustwidth}
Similarly, the appearance feature aggregation can be expressed in a similar way, while using the shared appearance matrix (as described in Sec.~\ref{sec:localtri}) across local tensors:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        f^\Rad(\ShadingX) &= \frac{1}{\sum \TensorWeight_\iLT} \sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT \TensorGrid_\iLT^\Rad(\ShadingX) \\
               &= \frac{1}{\sum \TensorWeight_\iLT} \sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT \AppearanceMC \cdot (\oplus [\TensorComp_{\Rad,r}]_r) \label{eq:btsorf} \\
               &= \frac{1}{\sum \TensorWeight_\iLT}\AppearanceMC \cdot (\sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT (\oplus [\TensorComp_{\Rad,r}]_r)) \label{eq:bours}.
    \end{align}
% \end{adjustwidth}
Note that owing to sharing the appearance matrix across tensors, we reduce the computational complexity from $O(\QueryNum \cdot P \cdot \TensorCompNumber_\Rad)$ in Eqn.\ref{eq:btsorf}, to $O( (\QueryNum + P) \cdot \TensorCompNumber_\Rad )$ in Eqn.\ref{eq:bours}.

\subsection{Multi-scale tri-vector fields.}
\label{sec:scaleAgg}
Complex 3D scenes often contain multi-frequency geometry and appearance details. 
This motivates us to build multi-scale tensor clouds to discover the local geometry and appearance commonalities at multiple scales.
Our final radiance field is modeled by multiple tri-vector tensor clouds at $\ScaleNum$ different scales. Different clouds consist of tensors with different resolutions. 

To regress the density and appearance at a location $\ShadingX$, we gather the density and appearance features from a set of tensor clouds that cover $\ShadingX$, $\{\TensorCloud_\iScale | 1 \leq \iScale \leq \ScaleNum, \ShadingX \in \CoveredSpace_\iScale\}$. 
Please note that tensor clouds of certain scales might not cover the location, so that $ \lVert\{\TensorCloud_\iScale\}\rVert \leq \ScaleNum$. We simply compute the mean features across these scales:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{align}
        f^\Dens(\ShadingX) = \frac{1}{\lVert\{\TensorCloud_\iScale\}\rVert}\sum_sf^\Dens_\iScale(\ShadingX), \\
        f^\Rad(\ShadingX) = \frac{1}{\lVert\{\TensorCloud_\iScale\}\rVert}\sum_sf^\Rad_\iScale(\ShadingX).
    \end{align}
% \end{adjustwidth}

Note that $f^\Dens(\ShadingX)$ and $f^\Rad(\ShadingX)$ are the final density and appearance features we aggregate across multiple scales and multiple neighboring tensors.
\subsection{Decoding.}
\label{sec:decoding}
We apply softplus activation on the density feature $f^\Dens(\ShadingX)$ to obtain the final volume density and regress the view-dependent color by sending the appearance feature $f^\Rad(\ShadingX)$ and the viewing direction $\Dir$ to the MLP decoder $\AppearanceFunc$.


















% \section{Representation}
% \label{sec:pcl_render}
% We present our novel tri-vector tensor-based radiance field representation.
% In essence, our model consists a cloud of small tri-vector tensors at multiple scales, designed to leverage both sparsity and multi-scale spatial coherence (see Fig. \ref{fig:overview}). We start with some preliminaries.

% \subsection{Preliminaries: Tensor Factorization}
% \paragraph{Canonical polyadic (CP) decomposition} A popular method of tensor factorization is CP decomposition. CP decomposition factorizes a $M$ dimension tensor $\mathcal{\TensorThreeD} \in \mathbb{R}^{I_1 \times I_2 \times ... \times I_M}$ into a linear combination of $\TensorCompNumber$ rank-1 tensors:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{equation}
%     \begin{aligned}
%         \mathcal{\TensorThreeD} &= \sum^\TensorCompNumber_{r=1}{\lambda_r \TensorV^0_r \otimes \TensorV^1_r \otimes ..., \otimes \TensorV^M_r},
%     \label{eq:cp}
%     \end{aligned}
%     \end{equation}
% \end{adjustwidth}
% where $\otimes$ denotes outer product; the weighting factor $\lambda_r$ can be absorbed into vectors $\{\TensorV_r^0, ..., \TensorV_r^M\}$.
% \paragraph{Factorize 3D scenes} 
% A scalar field in a 3D scene can be naturally represented by a third-order tensor $\mathcal{\TensorThreeD} \in \mathbb{R}^{I \times J \times K}$ and its CP decomposition can be expressed in Eqn.\ref{eq:cp3}, while each tensor element $\mathcal{\TensorThreeD}_{i,j,k}$ is the sum of the products of the corresponding elements in the rank-1 vectors Eqn.\ref{eq:cpe}:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \mathcal{\TensorThreeD} = \sum^\TensorCompNumber_{r=1}{\TensorV^1_r \otimes \TensorV^2_r \otimes \TensorV^3_r},
%         \label{eq:cp3} \\
%         \mathcal{\TensorThreeD}_{i,j,k} = \sum^\TensorCompNumber_{r=1}{\TensorV^1_{r,i} \TensorV^2_{r,j} \TensorV^3_{r,k}}.
%         \label{eq:cpe}
%     \end{align}
% \end{adjustwidth}

% \subsection{Sparse Tri-Vector Radiance Fields}
% Let $\TensorCloud = \{\TensorThreeD_\iT|\iT=1,...,\TensorNum\}$ denote a cloud of tri-vector tensors. Each local tensor $\TensorThreeD$ is located at $\TensorX$, covering a local cubic space $\TensorSpace$ with an edge length of $\TensorL$. The tensor cloud is designed to represent a radiance field for the 3D space:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \CoveredSpace = \bigcup_{\iT=1}^{N}\TensorSpace_{\iT}.
%     \end{align}
% \end{adjustwidth}
% For an arbitrary location $\ShadingX \in \CoveredSpace$, we select $\QueryNum$ nearest tensors that cover $\ShadingX$. Each tensor can extract the density features and appearance features at the location, based on the local scene factorization. Across the selected tensors, we aggregate these extracted features. The density $\Dens$ can be directly obtained after the aggregation. For the view-dependent color, we first aggregate the appearance features. Then, along with a viewing direction $\Dir$, it is sent to a function $\AppearanceFunc$ to regress $\Rad$. The continuous radiance field can be expressed as: 
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \Dens_\ShadingX, \Rad_\ShadingX = \AggF_\Dens(\{\TensorGrid^\Dens(\ShadingX)\}), \AppearanceFunc(\AggF_\Rad(\{\TensorGrid^\Rad(\ShadingX)\}), \Dir),
%     \end{align}
% \end{adjustwidth}
% where $\TensorGrid^\Dens$ and $\TensorGrid^\Rad$ are the geometry grid and the appearance grid of a single tri-vector tensor. Features will be extracted from these grids. $\AggF_\Dens$ and $\AggF_\Rad$ are the aggregation functions. We follow \cite{chen2022tensorf} and model $S$ as a small MLP. 

% In the following paragraphs, we will walk through the computation in a single tensor, the feature aggregation among nearby tensors, the way we decode features to infer the radiance field, and the design of multiscale tensor cloud. 

% \paragraph{Per-tensor factorization.} 
% Following \cite{chen2022tensorf}, we set the geometry grid $\TensorGrid^\Dens \in \mathbb{R}^{I \times J \times K}$ as a 3D tensor. For appearance grid,  $\TensorGrid^\Rad \in \mathbb{R}^{I \times J \times K \times P}$ is set as a 4D tensor. The fourth appearance mode is often of low dimension \cite{chen2022tensorf}. 

% According to Eqn.\ref{eq:cp3}, we factorize each tensor's feature grids, $\TensorGrid^\Dens$ and $\TensorGrid^\Rad$, by CP decomposition:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \TensorGrid^\Dens &= \sum^{\TensorCompNumber_\Dens}_{r=1}{\TensorComp_{\Dens,r}} = \sum^{\TensorCompNumber_\Dens}_{r=1}{\TensorV^X_{\Dens,r} \otimes \TensorV^Y_{\Dens,r} \otimes \TensorV^Z_{\Dens,r}},
%         \label{eq:denfact} \\
%         \TensorGrid^\Rad &= \sum^{\TensorCompNumber_\Rad}_{r=1}{\TensorComp_{\Rad,r} \otimes \AppearanceV_r} = \sum^{\TensorCompNumber_\Rad}_{r=1}{\TensorV^X_{\Rad,r} \otimes \TensorV^Y_{\Rad,r} \otimes \TensorV^Z_{\Rad,r} \otimes \AppearanceV_r},
%          \label{eq:appfact} 
%     \end{align}
% \end{adjustwidth}
% where $R_\Dens$ and $R_\Rad$ denote numbers of component; $\TensorComp_{\Dens,r}$ and $\TensorComp_{\Rad,r}$ are the component tensors that are factorized spatially; $\TensorV^X_{\Dens,r}, ..., \TensorV^X_{\Rad,r}, ...$ are the 1D vectors with resolution $I,J,K$ along $X,Y,Z$ axis; $\TensorCompNumber_\Dens$ and $\TensorCompNumber_\Rad$ are the component numbers. 

% To achieve a continuous field, we model the tensor grid by using trilinear interpolation. For a location $\ShadingX$, we first compute its relative position $\Tilde{\ShadingX}$ to the selected tensor located at $\TensorX$:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \Tilde{x},\Tilde{y},\Tilde{z} = x - \TensorX_x, y - \TensorX_y, z - \TensorX_z.
%         \label{eq:relapos} 
%     \end{align}
% \end{adjustwidth}
% Then, for example, to get $\TensorComp_{\Dens,r}$ at ($\Tilde{x},\Tilde{y},\Tilde{z}$), we can compute and trilinearly interpolate eight $\TensorComp_{\Dens,r}$ on the corners. As mentioned in \cite{chen2022tensorf}, applying linear interpolation on each vector first is mathematically equivalent and can reduce the computation cost. Under the rule of outer product, we have $\TensorComp_{r,i,j,k} = \TensorV_{r,i}^X \TensorV_{r,j}^Y \TensorV_{r,k}^Z$, then the interpolated density features at location $\ShadingX$ are:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%          \TensorGrid^\Dens(\ShadingX) & = \sum_r{\TensorV_{\Dens,r}^X(\Tilde{x}) \TensorV_{\Dens,r}^Y(\Tilde{y}) \TensorV_{\Dens,r}^Z(\Tilde{z})} = \sum_r{\TensorComp_{\Dens,r}(\Tilde{\ShadingX})},
%          \label{eq:interp} 
%     \end{align}
% \end{adjustwidth}
% where $\TensorV_{\Dens,r}^X(\Tilde{x})$ is $\TensorV_{\Dens,r}^X$'s linearly interpolated value at $(\Tilde{x})$ along its $X$ axis. Here, $\TensorGrid^\Dens(\ShadingX)$ is a scalar.

% Since the appearance feature-mode vectors $\AppearanceV_r$ express the appearance correlations, \cite{chen2022tensorf} proposes to stack all $\AppearanceV_r$ as columns together, which ends up to a $P \times \TensorCompNumber_\Rad$ matrix $\AppearanceM$; Then the interpolated appearance features can be calculated as:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%          \TensorGrid^\Rad(\ShadingX) &= \sum_r{\TensorV_{\Rad,r}^X(\Tilde{x}) \TensorV_{\Rad,r}^Y(\Tilde{y}) \TensorV_{\Rad,r}^Z(\Tilde{z}) \AppearanceV_r} \\
%                                       &= \sum_r{\TensorComp_{\Rad,r}(\Tilde{\ShadingX})\AppearanceV_r} \\
%                                       &= \AppearanceM \cdot (\oplus [\TensorComp_{\Rad,r}]_r), 
%          \label{eq:interp}
%     \end{align}
% \end{adjustwidth}
% where ``$\oplus$'' denotes concatenation, ``$\cdot$'' denotes dot product. The appearance feature $\TensorGrid^\Rad(\ShadingX) \in \mathbb{R}^P $ is a vector.

% \paragraph{Feature aggregation.}
% Since we queried $\QueryNum$ neighboring tensors for each 3D location $\ShadingX$, these tensors can jointly model the $\Dens_\ShadingX$ and $\Rad_\ShadingX$. Following Point-NeRF \cite{xu2022point}, the feature importance of a tensor is weighted by the standard inverse distance between its center and the 3D location:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \TensorWeight_\iLT=\frac{1}{\lVert \TensorX_\iLT-\ShadingX \rVert}.
%     \end{align}
% \end{adjustwidth}

% DVGO \cite{sun2022direct} demonstrate that directly regressing density from features is the most computationally efficient way to model high-fidelity scene geometry. Therefore, we can efficiently obtain the density feature by the weighted sum:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         f^\Dens(\ShadingX) = \frac{1}{\sum \TensorWeight_\iLT} \sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT \TensorGrid_\iLT^\Dens(\ShadingX).
%     \end{align}
% \end{adjustwidth}
% Similarly, the appearance feature can be aggregated with the same way. In addition, each matrix $\AppearanceM_\iLT$ can be seen as an appearance dictionary that abstracts the appearance commonalities within each local tensor. We propose to utilize a cloud-wise appearance matrix $\AppearanceMC$ that share across tensors and thereby further improved both the computational efficiency and model compactness. The appearance features are aggregated as:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         f^\Rad(\ShadingX) &= \frac{1}{\sum \TensorWeight_\iLT} \sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT \TensorGrid_\iLT^\Rad(\ShadingX) \\
%                &= \frac{1}{\sum \TensorWeight_\iLT} \sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT \AppearanceMC \cdot (\oplus [\TensorComp_{\Rad,r}]_r) \label{eq:btsorf} \\
%                &= \frac{1}{\sum \TensorWeight_\iLT}\AppearanceMC \cdot (\sum_{\iLT=1}^\QueryNum \TensorWeight_\iLT (\oplus [\TensorComp_{\Rad,r}]_r)) \label{eq:bours}.
%     \end{align}
% \end{adjustwidth}
% This formulation reduces the computational complexity from $O(\QueryNum \cdot P \cdot \TensorCompNumber_\Rad)$ in Eqn.\ref{eq:btsorf}, to $O( (\QueryNum + P) \cdot \TensorCompNumber_\Rad )$ in Eqn.\ref{eq:bours}.

% \paragraph{Multiscale tri-vector radiance fields.}
% The smoothness in natural scenes motivates the exploration of multiscale decomposition. A multiscale tri-vector field is modeled by tensor clouds of $\ScaleNum$ scales. Different tensor clouds consist of tensors with different resolutions. 

% To regress the density and appearance at a location $\ShadingX$, we gather the density and appearance features from a set of tensor clouds that cover $\ShadingX$, $\{\TensorCloud_\iScale | 1 \leq \iScale \leq \ScaleNum, \ShadingX \in \CoveredSpace_\iScale\}$. Please note that tensor clouds of certain scales might not cover the location, so that $ \lVert\{\TensorCloud_\iScale\}\rVert \leq \ScaleNum$. We simply compute the mean features across these scales:
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         f^\Dens(\ShadingX) = \frac{1}{\lVert\{\TensorCloud_\iScale\}\rVert}\sum_sf^\Dens_\iScale(\ShadingX), \\
%         f^\Rad(\ShadingX) = \frac{1}{\lVert\{\TensorCloud_\iScale\}\rVert}\sum_sf^\Rad_\iScale(\ShadingX).
%     \end{align}
% \end{adjustwidth}

% \paragraph{Radiance fields regression.}
% It is discovered \cite{chen2022tensorf,sun2022direct} that local features are capable to directly regress high-fidelity density values without networks. We follow DVGO \cite{sun2022direct} to directly apply the softplus activation \cite{barron2021mip} on the density feature. For appearance, we follow \cite{chen2022tensorf,sun2022improved,xu2022point} to simply concatenate the positional encoding of the view direction $\Dir$ with the appearance features and send them to a MLP $\AppearanceFunc$ to generate the color :
% \begin{adjustwidth}{0pt}{0pt}
%     % \setlength{\abovedisplayskip}{-5pt}%
%     % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
%     % \setlength{\belowdisplayshortskip}{-5pt}%
%     \begin{align}
%         \Dens_\ShadingX &= log(1 + exp(f^\Dens(\ShadingX) + \DensShift)), \\
%         \Rad_\ShadingX &= \AppearanceFunc(f^\Rad(\ShadingX), \Dir),
%     \end{align}
% \end{adjustwidth}
% where $\DensShift$ is a shift hyperparameter. 


\begin{table*}[!hbt]
    \begin{adjustwidth}{0pt}{0pt}  
    \centering
    % \setlength\tabcolsep{5pt}
        \small{
            \begin{tabular}{ccccccccc}
            \hline %\\[-1.05em]
            Method & BatchSize & Steps & Time$\downarrow$ & \# Param.(M)$\downarrow$  & PSNR$\uparrow$ & SSIM$\uparrow$& LPIPS$_{Vgg}$ $\downarrow$ & LPIPS$_{Alex}$ $\downarrow$\\ \hline
            NeRF\cite{martin2021nerf} & 4096 & 300k & ~35.0h  & ~~~~~~~1.25 \second & 31.01 & 0.947 & 0.081 & -\\
            %NSVF\cite{liu2020neural} & 4096 & 150k & 0.80-4.00 & - & 31.75 & 0.953 & - & 0.047\\
            Plenoxels\cite{yu2021plenoxels} & 5000 & 128k & 11.4m  & 194.50 & 31.71 & 0.958 & 0.049 & - \\
            DVGO\cite{sun2022direct} & 5000 & 30k & 15.0m  &  153.00 & 31.95 & 0.960 & 0.053 & 0.035 \\
            % Point-NeRF$_{20k}$\cite{xu2022point} & 4096 & 20k & 27.74 & 33.0m & 30.71 & 0.967 & 0.081& 0.050\\ 
            Point-NeRF$_{200k}$\cite{xu2022point} & 4096 & 200k & ~~5.5h & ~~27.74 & ~~~~33.31 \second & ~~~0.962~*  & 0.049& 0.027\\ 
            InstantNGP\cite{muller2022instant}& 10k-85k & 30k & ~~3.9m  &  ~~11.64 & 32.59 & 0.960 & - & - \\
            TensoRF-CP\cite{chen2022tensorf}& 4096 & 30k & 25.2m & ~~~~~~~0.98 \first & 31.56 & 0.949 & 0.076 & 0.041 \\
            TensoRF-VM\cite{chen2022tensorf}& 4096 & 30k &  17.4m & ~~17.95 & 33.14 & ~~~~0.963 \second & 0.047 & ~~~~0.027 \third \\ \hline
            %Ours(SmallSize)& 4096 & 30k & 10.40 & 33.42 & 0.964 & 0.043 &  0.026 \\
            Ours-24& 4096 & 30k & 34.3m & ~~~~~~~7.07 \third & ~~~~33.24 \third & ~~~~0.963 \second & 0.046 \second &  ~~~~0.026 \second   \\
            Ours-48& 4096 & 30k & 35.7m & ~~~13.52 & ~~~~33.55 \first  & ~~~~0.965 \first & 0.044 \first & ~~~~0.025 \first   \\ \hline
            \end{tabular}
        }
        \captionsetup{aboveskip=5pt}
        \captionsetup{belowskip=-0pt}
        \caption {Comparisons of our method with other radiance-based models \cite{martin2021nerf,ibrnet,liu2020neural,xu2022point, chen2022tensorf,muller2022instant} on the Synthetic-NeRF dataset \cite{martin2021nerf}. Ours-24 is the one with 24 components while Ours-48 is the one with 48 components. We report the corresponding rendering quality (PSNR, SSIM, and LPIPS), model capacity (number of parameters), and training time, batch size and steps. Our model achieves the best rendering quality with a compact model size. We report PointNeRF's updated SSIM.}
        % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
        \label{tb:nerfsynth} 
    \end{adjustwidth}
\end{table*}
    
\section{Rendering and Reconstruction}
\label{sec:renrecon}
\paragraph{Volume Rendering}
We evaluates each pixel's color with physically-based volume rendering via differentiable ray marching. Following NeRF \cite{mildenhall2020nerf}, we sample $\ShadNum$ shading points at $\{\ShadingX_\iS\;|\;\iS=1,...,\ShadNum\}$ along the ray, and accumulate radiance by density:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-5pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-5pt}%
    \begin{equation}
    \begin{aligned}
        \Color &=  \sum^\ShadNum_{\iS=1} \Trans_\iS (1-\exp (-\Dens_\iS \Step_\iS)) \Rad_\iS, \\
        \Trans_\iS &= \exp (-\sum_{\iSS=1}^{\iS-1} \Dens_\iSS \Step_\iSS ).
    \label{eq:raymarching}
    \end{aligned}
    \end{equation}
% \end{adjustwidth}
$\Dens_\iS$ and $\Rad_\iS$ are the density and radiance at shading points; $\Step_\iSS$ is the marching distance per step; $\Trans$ denotes transmittance.


% We introduce the pipeline for optimizing the sparse tri-vector radiance fields for scene reconstruction. 
\paragraph{Distributing local tensors.} First of all, to better leverage the sparsity of a scene, we first obtain a geometric prior that roughly covers the scene geometry. 
The geometric prior can be in any commonly-used form, e.g., point cloud, occupancy grid, octree, or mesh vertices. Then we can uniformly distribute tensors in the spatial neighborhood of the geometry. For a multi-scale model, each of the scale will be distributed independently. 
For most of our results, we quickly optimize a coarse RGBA volume from the multi-view images and use the optimized occupancy grid as the geometry prior, as done in DVGO~\cite{sun2022direct}, which finishes in seconds. 

To maintain training stability and speed, each tensor $\TensorThreeD$'s position $\TensorX$ and coverage $\TensorSpace$ is fixed once determined. We also initialize the $3(\TensorCompNumber_\Dens + \TensorCompNumber_\Rad)$ vectors ($\TensorV_{\Dens,r}^X,...,\TensorV_{\Rad,r}^X,...$) of each tensor by normal distribution. For each scale $\iScale$, a $P \times \TensorCompNumber_\Rad$ appearance matrix $\AppearanceMC_\iScale$ is shared by all tri-vector tensors of that scale. Specifically, ``$\AppearanceMC \cdot ()$'' in Eqn.\ref{eq:bours} can be efficiently implemented as a fully-connected neural layer. Therefore, $\AppearanceMC$ for each scale and a global appearance MLP $\AppearanceFunc$ will be implemented as neural networks and initialized by default methods \cite{he2015delving}.

\paragraph{Optimization and objectives.}
Given a set of multi-view RGB images with camera poses, the sparse tri-vector radiance field is per-scene optimized to reconstruct the radiance fields, under the supervision of  the ground truth pixel colors. Following the volume rendering equation \ref{eq:raymarching}, the L2 rendering loss can be past back to the global MLP and  aggregated features, then, all the way to the the appearance matrices and the feature vectors of local tensors. 

We apply a rendering loss to supervise the reconstruction and also apply an $L1$ regularization loss on density feature vectors $\TensorV_{\Dens,r}$ to promote geometry sparsity and to avoid overfitting as done in TensoRF \cite{chen2022tensorf}:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-10pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-15pt}%
        \begin{align}
            \mathcal{L}_{r} = \lVert C - \GTColor \rVert^2_2,
            \label{eq:lr} \\
            \mathcal{L}_{L1} = \cfrac{1}{N} \sum_{r}^{\TensorCompNumber_\Dens}{\lVert \TensorV_{\Dens,r} \rVert},
            \label{eq:l1}
        \end{align}
    % \vspace{-15pt}
% \end{adjustwidth}
where $\GTColor$ is the ground truth pixel color, $\lVert \TensorV_{\Dens,r} \rVert$ is the sum of absolute values of elements on density vectors, and $N$ is the total number of elements.
The total loss is:
% \begin{adjustwidth}{0pt}{0pt}
    % \setlength{\abovedisplayskip}{-10pt}%
    % \setlength{\abovedisplayshortskip}{\abovedisplayskip}%
    % \setlength{\belowdisplayshortskip}{-15pt}%
        \begin{align}
            \mathcal{L} = \mathcal{L}_{r} + \alpha \mathcal{L}_{L1}.
            \label{eq:ltotal}
        \end{align}
    % \vspace{-15pt}
% \end{adjustwidth}
We set the weight of the sparsity term $\alpha$ as $1e^{-5}$ by default. %We set $\alpha = 1e^{-5}$ by default.