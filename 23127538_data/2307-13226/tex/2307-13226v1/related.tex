\section{Related Work}
\paragraph{Scene representations.}
To represent a 3D scene, traditional and learning-based methods have studied various representations, such as depth map \cite{huang2018deepmvs,liu2015learning}, mesh \cite{kanazawa2018learning,wang2018pixel2mesh,shewchuk1998tetrahedral}, point cloud \cite{qi2017pointnet,achlioptas2018learning,wang2018mvpnet} and implicit function \cite{chen2018learning,mescheder2018occupancy,niemeyer2020differentiable,yariv2020multiview}. 
In recent years, continuous neural field representations stand out in various 3D tasks such as single-view 3D reconstruction \cite{xu2019disn,genova2020local}, surface completion \cite{chibane2020implicit, park2019deepsdf}, multi-view reconstruction \cite{niemeyer2020differentiable} and novel view synthesis \cite{mildenhall2020nerf,martin2021nerf}. 
Compared with traditional discrete representations, a continuous field have no limitation on spatial resolution, e.g., volume resolution or the number of points. 
It can also naturally be represented by neural networks, such as an MLP, which are known for approximating complex functions well. 

\paragraph{Neural field representations.}
Specifically, NeRF \cite{mildenhall2020nerf} represents a 3D scene as a radiance field with a global coordinate MLP, which models geometry, lighting and texture information jointly, leading to photo-realistic rendering quality in novel view synthesis. 
Apart from its advantage, purely MLP-based NeRF models \cite{barron2021mip,verbin2022ref} in general suffer from inefficiency \cite{arandjelovic2021nerf} when modeling highly complex or large-scale scenes, due to limited model capacity, slow optimization speed, and the cost of modeling empty space. 


To model radiance fields more efficiently, recent works have explored combining neural fields with various traditional 3D representations, including voxels \cite{liu2020neural,yu2021plenoxels,sun2022direct,zhang2022nerfusion} and points \cite{xu2022point}. Low-rank representations such as triplane~\cite{chan2022efficient,fridovich2023k} and tensor decomposition~\cite{chen2022tensorf,obukhov2022tt} have also been studied.
In particular, DVGO \cite{sun2022direct} and Plenoxels \cite{yu2021plenoxels} respectively use dense and sparse voxels with neural features for radiance field modeling.
While being efficient to optimize, these localized feature grid-based representations lead to a large model size and can face overfitting issues when the features are of very high resolution.
Consequently, DVGO can also work with a low-resolution grid and Plenoxels requires additional spatial regularization terms.
On the other hand, recent works have adopted global feature encoding to express a high-resolution feature grid, including Instant-NGP \cite{muller2022instant} that hashes spatial features into multi-scale hash tables and TensoRF \cite{chen2022tensorf} that factorizes a feature grid into vector and matrix factors.
These global feature encoding methods exploit the spatial correlation across the entire scene space, leading to fast and compact reconstruction and surpassing previous MLP-based or grid-based representations on rendering quality.
However, similar to NeRF, such global representation can also be limited by its model capacity when representing highly complex or large-scale content.

Our approach instead combines local and global representations.
Our tri-vector fields are sparsely distributed in the scene, similar to local representations (like plenoxels and Point-NeRF); meanwhile, features in each field are represented by tri-vector components shared across the local region as done in TensoRF, exploiting spatial feature commonalities.
Our model leverages both spatial sparsity and coherence, leading to much higher compactness and better reconstruction quality than previous local and global representations (see Tab.~\ref{tb:nerfsynth}).

Relevant to our work, previous methods, such as KiloNeRF \cite{reiser2021kilonerf} and BlockNeRF \cite{tancik2022block} have also utilized multiple local MLPs to represent a scene.
Specifically, KiloNeRF focuses and speeding up NeRF and their rendering quality is sacrificed; BlockNeRF essentially uses multiple NeRFs to increase the total model capacity.
Instead of pure MLPs, our work is built upon tensor factorization-based feature encoding as done in TensoRF \cite{chen2022tensorf}, and we in fact achieve superior rendering quality while decreasing the model capacity.






% To model radiance fields more efficiently, recent works have explored combining neural fields with various traditional 3D representations, leading to localized field representations. 
% NSVF \cite{liu2020neural}, Plenoxel \cite{yu2021plenoxels}, DVGO \cite{sun2022direct} and NeRFusion \cite{zhang2022nerfusion} combine sparse voxels  volume with implicit functions. The grid occupancy enables efficient shading point sampling and the implicit function networks conditioned on the optimizable features on voxel grids so that the representation power can scale with the scene. KiloNeRF \cite{reiser2021kilonerf} also decomposes the scene by voxel grids and the radiance field in each voxel is represented by an independent implicit function. Instant-NGP \cite{muller2022instant} also voxelizes the scene but use multi-scale hash map to store features, leading to compact and fast reconstruction. Point-NeRF \cite{xu2022point} leverages existing point cloud to efficiently limit sampling locations. Its implicit function is conditioned on the local features on nearby points. TensoRF \cite{chen2022tensorf} uses tensor factorization techniques \cite{harshman1970foundations,carroll1970analysis} to decompose the scene with axis-aligned tensors. Similarly, EG3D \cite{chan2022efficient} also use tri-planes as the low rank tensors to represent the scene and achieve stunning generalizability. 

% Natural scenes exhibit both sparsity and smoothness \cite{muller2022instant}. Either voxel-based \cite{liu2020neural,yu2021plenoxels,sun2022direct,zhang2022nerfusion} or point-based methods \cite{xu2022point,ost2022neural} represent local radiance information by sparsely distributed local features. Yet, both tensor-based \cite{chen2022tensorf,chan2022efficient} and hashing-based \cite{muller2022instant} methods use shared features for different locations to achieve efficient encoding and compactness. The proposed TensoRF Cloud hits a sweet spot to leverage both spatial sparsity and compactness. Our method can achieve outstanding synthesis quality with compact model size.





% Recently, various neural scene representations have been presented \cite{zhou2018stereo,sitzmann2019deepvoxels,lombardi2019neural,bi2020deep}, advancing the state of the art in novel view synthesis and realistic rendering, with volumetric neural radiance fields (NeRFs) \cite{mildenhall2020nerf} producing high fidelity results. 
% NeRFs are often reconstructed as global MLPs \cite{mildenhall2020nerf,zhang2020nerf++,park2021nerfies} that encode the entire scene space; this can be inefficient and expensive when reconstructing complex and large-scale scenes.
% Instead, Point-NeRF is a localized neural representation, combining volumetric radiance fields with point clouds that are classically used to approximate scene geometry.
% We distribute fine-grained neural points to model complex local scene geometry and appearance, leading to better rendering quality than NeRF 
% (see Fig.~\ref{fig:dtu},~\ref{fig:nerfsynth}).
% % (see \KS{Tab.~\ref{tb:cm}}).  

% Voxel grids with per-voxel neural features \cite{liu2020neural,chen2021mvsnerf,hedman2021baking} are also a local neural radiance representation. However, our point-based representation adapts better to actual surfaces, leading to better quality. Also, we directly predict good initial neural point features, bypassing the per-scene optimization that is required by most voxel-based methods \cite{liu2020neural,hedman2021baking}.
% , thus achieving highly efficient radiance field reconstruction.

% \boldstartspace{Multi-view reconstruction and rendering.}
% Multi-view 3D reconstruction has been extensively studied and addressed with a number of structure-from-motion \cite{schoenberger2016sfm,vijayanarasimhan2017sfm,tang2018ba} and multi-view stereo techniques \cite{furukawa2009accurate,kutulakos2000theory,schoenberger2016mvs,yao2018mvsnet,cheng2020deep}.
% Point clouds are often the direct output from MVS or depth sensor, though they are usually converted to meshes \cite{lorensen1987marching,kazhdan2006poisson} for rendering and visualization.
% Meshing can introduce errors and may require image-based rendering \cite{debevec1998efficient,buehler2001unstructured,zhou2014color} for high-quality rendering.
% We instead directly use point clouds from deep MVS to achieve realistic rendering.

% Point clouds have been widely used in rendering, often via rasterization-based point splatting, and even differentiable rasterization modules \cite{wiles2020synsin,lassner2021pulsar}.
% However, reconstructed point clouds often have holes and outliers that lead to artifacts in rendering. Point-based neural rendering methods address this by splatting neural features and using 2D CNNs to render them \cite{aliev2020neural,kopanas2021point,meshry2019neural}.
% In contrast, our point-based approach utilizes 3D volume rendering, 
% leading to significantly better results than previous point-based methods. 

% \boldstartspace{Neural radiance fields scene decomposition.} NeRFs \cite{mildenhall2020nerf} have demonstrated remarkably high-quality results for novel view synthesis. %and quickly been used in various problems.
% They have been extended to achieve dynamic scene capture \cite{li2021neural,park2021hypernerf}, relighting \cite{bi2020neural,boss2021nerd}, appearance editing \cite{xiang2021neutex}, fast rendering \cite{hedman2021baking,yu2021plenoctrees}, and generative models \cite{chan2021pi,schwarz2020graf,niemeyer2021giraffe}.  
% However, most methods \cite{li2021neural,park2021hypernerf,xiang2021neutex,bi2020neural} still follow the original NeRF framework and train per-scene MLPs to represent radiance fields.
% We make use of neural points with spatially varying neural features in a scene to encode its radiance field.
% This localized representation can model more complex scene content than pure MLPs that have limited network capacity. More importantly, we show that our point-based neural field can be efficiently initialized via a pre-trained deep neural network that generalizes across scenes and leads to highly efficient radiance field reconstruction.

% Prior works also present generalizable radiance field-based methods.
% PixelNeRF \cite{yu2020pixelnerf} and IBRNet \cite{ibrnet} aggregate multi-view 2D image features at every sampled ray point to regress volume rendering properties for radiance field rendering.
% In contrast, we leverage features in 3D neural points around the scene surface to model radiance fields. This avoids sampling points in the vast empty space and leads to higher rendering quality and faster radiance field reconstruction than PixelNeRF and IBRNet.
% MVSNeRF \cite{chen2021mvsnerf} can achieve very fast voxel-based radiance field reconstruction. However, its prediction network requires a fixed number of three small-baseline images as input and thus can only efficiently reconstruct local radiance fields.
% Our approach can fuse neural points from an arbitrary number of views and achieve fast reconstruction of complete 360 radiance fields which MVSNeRF cannot support. 
%\KS{We might want to discuss IBRNet and MVSNeRF a bit more in the intro, especially since the numbers are actually quite similar.}
