   
    % \begin{table*}[ht]
    %     \begin{adjustwidth}{0pt}{0pt}  
    %     \centering
    %     % \setlength\tabcolsep{5pt}
    %         \small{
    %             \begin{tabular}{l|ccccccc}
    %             \hline %\\[-1.05em]
    %             & NeRF \cite{martin2021nerf} & IBRNet \cite{ibrnet} & NSVF \cite{liu2020neural}  & Point-NeRF$_{200K}\cite{xu2022point}$ & TensoRF-VM_{30k}\cite{chen2022tensorf} & InstantNGP\cite{muller2022instant} & Ours_{30k}\\ \hline
    %             PSNR $\uparrow$  & \bthird 31.01 & 28.14 & \btwo 31.75   & 33.31 & 33.14 & 33.17 & \bone \textbf{33.56}\\
    %             SSIM $\uparrow$  & 0.947 & 0.942 & \bthird \bthird 0.953 & \bone \textbf{0.978} & 0.963 & - & 0.965\\
    %             LPIPS$_{Vgg}$ $\downarrow$ & 0.081 & \bthird 0.072  & -  & 0.049 & 0.047& - & \bone \textbf{0.044}\\
    %             LPIPS$_{Alex}$ $\downarrow$ & - & -  & \bthird 0.047 & 0.027 & 0.027& - & \bone \textbf{0.025} \\ \hline
    %             \end{tabular}
    %         }
    %          \captionsetup{aboveskip=3pt}
    %         \captionsetup{belowskip=-10pt}
    %         \caption {Comparisons of our Cloud TensoRF with other radiance-based models \cite{martin2021nerf,ibrnet,liu2020neural,xu2022point, chen2022tensorf,muller2022instant} on the Synthetic-NeRF dataset \cite{martin2021nerf}. The subscripts indicate the number of iterations during training.}
    %         % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
    %         \label{tb:nerfsynth2} 
    %     \end{adjustwidth}
    % \end{table*}

  


    % Figure environment removed



    
    % \vspace{-10pt}
    \section{Experiments}
    
    \subsection{Evaluation on the NeRF Synthetic Dataset.}
    % Our full pipeline is trained on the DTU dataset \cite{dtu}. 
    We first evaluate our method on the Synthetic-NeRF dataset \cite{martin2021nerf} and the quantitative results compared with other methods are reported in Tab.\ref{tb:nerfsynth}, including NeRF \cite{mildenhall2020nerf}, Plenoxels \cite{yu2021plenoxels}, DVGO \cite{sun2022direct}, Point-NeRF \cite{xu2022point}, iNGP \cite{muller2022instant}, and TensoRF \cite{chen2022tensorf}. 
    % Our approach not only outperforms all the compared methods in terms of PSNR, LPIPS$_{Vgg}$ and LPIPS$_{Alex}$ but also has a relatively small model size. 
    We report our models of two different model sizes with different numbers of components; both settings are with the same 3 scales of local tensors. 
    
    Our approach achieves the best averaged PSNR, LPIPS$_{Vgg}$ and LPIPS$_{Alex}$ in all the methods, leading to superior visual quality as shown in Fig.~\ref{fig:syn_comp}
    Meanwhile, our high rendering quality is achieved with a compact model size. 
    When compared with local voxel-based representations, such as Plenoxels and DVGO, our approach are significantly better.
    
    On the other hand, global featuring encoding-based methods, like iNGP and TensoRF, are known for their high compactness and can achieve higher rendering quality than local voxel-based methods. 
    Nonetheless, our method can still outperform them. 
    Note that, even our smaller model (Ours-24) leads to better rendering quality than both iNGP and TensoRF that leverage global feature encoding, while our model uses significantly fewer parameters (about 60\% and 40\% of the size of iNGP and TensoRF).
    This clearly demonstrates the high visual quality and high compatness of our model with our sparsely distributed tri-vector tensors.

    In all the baseline methods, Point-NeRF is able to achieve relatively higher rendering quality than others. However, this is enabled by optimizing their model for 300k steps with a long period of 5.5 hours. In contrast, our method achieves higher quality with significantly fewer optimization steps (only 30k) and optimization time (about 36 min).
    As expected, our model is slower to optimize than TensoRF due to the additional costs of multi-tensor aggregation.
    However, though speed is not our focus in this work, our model can still converge quickly and lead to significantly faster reconstruction than MLP-based methods, such as NeRF, as well as Point-NeRF that is point-based.
    

    \boldstartspace{Performance w.r.t. rotation.} 
    We observe that tensor factorization-based methods can be sensitive to the orientation of the coordinate frames, since axis-aligned features are used; in essence, this is because the rank of a sparse tensor is sensitive to rotation.
    Especially, this can benefit reconstruction on synthetic synthetic scenes where the objects are perfectly aligned with the axes, e.g. the lego scene in the NeRF synthetic data.
    However, we find that our method based on local tensors are more robust against the orientation of the axes than a global TensoRF.
    In particular, we compare our method with TensoRF in Tab.\ref{tb:rot} with different degrees of rotation around the Z axis on two scenes, lego (which is strongly aligned with axes) and chair (which is less aligned and thus less affected ) . 
    As shown in the table, while both methods are affected by the rotation, our method has much smaller drops of PSNRs. 
    

    % Especially, this can benefit reconstruction on synthetic synthetic scenes where the objects are  perfectly aligned with the axis.
    % However, we find that our method based on local tensors are more robust against the orientation of the axes than a global TensoRF.
    % In particular, we compare our method with TensoRF in Table.\ref{tb:rot} with different degrees of axis rotation. 
    % As shown in the table, while both methods are affected by the rotation, our method has smaller drops of PSNRs. And our method after rotation can still lead high PSNRs (33.27 and 33.18), outperforming iNGP and many other baselines shown in Tab.~\ref{tb:nerfsynth}.
    
    % shows that our Strivec is more robust than TensoRF under different rotation settings. Because of its tensor decomposition nature of TensoRF, global representation is hard to maintain row rank when the scene is no longer axis aligned.

    % \boldstartspace{Point pruning and growing}
 % \begin{table}[hbt]
 %        \begin{adjustwidth}{0pt}{0pt}  
 %        \centering
 %        % \setlength\tabcolsep{5pt}
 %            \small{
 %                \begin{tabular}{l|ccc}
 %                \hline %\\[-1.05em]
 %                & rot 0$^{\circ}$ & rot 5$^{\circ}$ & rot 45$^{\circ}$ \\ \hline
 %                TensoRF-CP & 31.56 & 30.73 & 29.91\\ 
 %                TensoRF-VM & 33.14 & 32.06 & 31.96 \\
 %                Ours-48&  33.55 & 33.27 & 33.18 \\
 %                \hline
 %                \end{tabular}
 %            }
 %             \captionsetup{aboveskip=3pt}
 %            \captionsetup{belowskip=-10pt}
 %            \caption {Comparisons of our Strivec with TensoRF \cite{ chen2022tensorf} on the Synthetic-NeRF dataset \cite{martin2021nerf} when considering rotation of different angles around $z$-axis.}
 %            % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
 %            \label{tb:rot} 
 %        \end{adjustwidth}
 %    \end{table}


 % \begin{table}[hbt]
 %        \begin{adjustwidth}{0pt}{0pt}  
 %        \centering
 %        % \setlength\tabcolsep{5pt}
 %            \small{
 %                \begin{tabular}{l|ccc}
 %                %\hline %\\[-1.05em]
 %                %& \multicolumn{3}{c}{\textbf{Chair}}  \\
 %                \hline 
 %                 \multicolumn{1}{l|}{}& rot 0$^{\circ}$ & rot 5$^{\circ}$ & rot 45$^{\circ}$ \\ \hline
 %                \multicolumn{1}{l|}{TensoRF-CP} & 33.60 & 32.90 & 32.50\\ 
 %                \multicolumn{1}{l|}{TensoRF-VM} & 35.76 & 34.91 & 34.55 \\
 %                \multicolumn{1}{l|}{Ours-48} &  35.88 & 35.72 & 35.64 \\
 %                \hline
 %                \end{tabular}
 %            }
 %             \captionsetup{aboveskip=3pt}
 %            \captionsetup{belowskip=-10pt}
 %            \caption {Comparisons of our Strivec with TensoRF \cite{ chen2022tensorf} on the Synthetic-NeRF dataset \cite{martin2021nerf} when considering rotation of different angles around $z$-axis.}
 %            % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
 %            \label{tb:rot} 
 %        \end{adjustwidth}
 %    \end{table}

 \begin{table}[hbt]
        \begin{adjustwidth}{0pt}{0pt}  
        \centering
        % \setlength\tabcolsep{5pt}
            \small{
                \begin{tabular}{l|ccc}
                \hline %\\[-1.05em]
                chair~/~lego & rot 0$^{\circ}$ & rot 5$^{\circ}$ & rot 45$^{\circ}$ \\ \hline
                TensoRF-CP &  33.60~/~34.50 & 32.90~/~29.79 & 32.50~/~28.57 \\ 
                TensoRF-VM & 35.76~/~36.46 & 34.91~/32.53 & 34.55~/~32.31 \\
                Ours-48&  35.88~/~36.52 & 35.72~/~35.37 & 35.64~/~34.97 \\
                \hline
                \end{tabular}
            }
             \captionsetup{aboveskip=3pt}
            \captionsetup{belowskip=-5pt}
            \caption {Comparisons of our method with TensoRF \cite{ chen2022tensorf} on the chair and lego scenes of Synthetic-NeRF dataset \cite{martin2021nerf} when considering rotation of different angles around $z$-axis.}
            % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
            \label{tb:rot} 
        \end{adjustwidth}
    \end{table}





    
    % Figure environment removed

\subsection{Evaluation on the real datasets.}

    \begin{table}%[hbt]
        \begin{adjustwidth}{0pt}{0pt}  
        \centering
        \setlength\tabcolsep{0.5pt}
            \small{
                \begin{tabular}{lcccc}
                & \multicolumn{4}{c}{\textbf{Average over Scene 101 and Scene 241}}  \\
                \hline %\\[-1.05em]
                \multicolumn{1}{l|}{} & PSNR~$\uparrow$ & SSIM~$\uparrow$ & LPIPS$_{Alex}$~$\downarrow$  & \# Param.(M)~$\downarrow$  \\ \hline
                 \multicolumn{1}{l|}{SRN~\cite{sitzmann2019scene}}  & 18.25 & 0.592 & 0.586 & -\\
                 \multicolumn{1}{l|}{NeRF~\cite{martin2021nerf}}& 22.99 & 0.620 & 0.369 & - \\
                 \multicolumn{1}{l|}{NSVF~\cite{liu2020neural}} & 25.48 & 0.688 & 0.301 & -\\
                 \multicolumn{1}{l|}{Point-NeRF\cite{xu2022point}} &  25.92 & ~~~~~0.891 \first & ~~~~~0.273 \second & 159.01 \\
                 \multicolumn{1}{l|}{TensoRF-CP\cite{chen2022tensorf}} &  ~~~~~27.54 \third & 0.751 & 0.328 & ~~~~~0.97 \first  \\ 
                 \multicolumn{1}{l|}{TensoRF-VM\cite{chen2022tensorf}} & ~~~~~28.61 \second & ~~~~~0.787 \third & ~~~~~0.290 \third & ~~~~~49.92 \third \\ 
                %TensoRF-VM(L) & 27.68 &  88.01 & 29.23 &  & 28.87 & 201.74\\
                 \multicolumn{1}{l|}{Ours-48} & ~~~~~29.05 \first & ~~~~~0.792 \second & ~~~~~0.243 \first & ~~~~~12.82 \second \\ \hline
                \end{tabular}
            }
             \captionsetup{aboveskip=3pt}
            \captionsetup{belowskip=-5pt}
            \caption {Quantitative comparison on two scenes in the ScanNet dataset \cite{dai2017scannet}. Point-NeRF, TensoRF-CP, TensoRF-VM and Ours-48 are optimized for 100k steps.}
            % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
            \label{tb:scannet2} 
        \end{adjustwidth}
    \end{table}
    
\paragraph{The ScanNet dataset.} We evaluate our method on the real dataset, ScanNet \cite{dai2017scannet} with the two scenes selected by NSVF \cite{liu2020neural}, and compare with other methods.
We follow the same experiment setting as done in NSVF \cite{liu2020neural}, using the provided mesh to distribute our tensors, and optimize our model, TensoRF for the same 100k steps for fair comparisons. Please note that Point-NeRF uses all scanned depth images as initial geometry instead of meshes. Therefore, we also obtain the results of Point-NeRF 100k steps from the authors, using the provided mesh for fairness.
We find the Scannet data has many holes in the provided mesh geometry, while methods, such as NSVF and Point-NeRF, require accurate initial geometry; though Point-NeRF can potentially fix them with it point growing technique as shown in their original paper, it is not able to fully address them in 100k step optimization and lead to holes in their final rendering.
Our approach instead does not require very accurate coarse geometry, since our local tensors cover relatively large regions.
We show the quantitative results in Tab.~\ref{tb:scannet2} and qualitative results in Fig.~\ref{fig:scan_vis}.
Note that our method can also perform well on real scenes, achieving the highest performance in terms of PSNR and LPIPS$_{Alex}$, while using the second smallest model size (the smallest one is TensoRF-CP$_{100k}$). 
Our visual quality is also higher than the comparison methods.  


\paragraph{The Tanks and Temples dataset.} We also evaluate our method on another real dataset, Tanks and Temples \cite{dai2017scannet} with the 5 object scenes selected by NSVF \cite{liu2020neural}. We using the very coarse geometries estimated by DVGO\cite{sun2022direct} to distribute our tensors. We follow the same experiment setting as done in TensoRF \cite{chen2022tensorf}, optimizing our model for the same 30k steps for fair comparisons. As is shown in Tab.~\ref{tb:scannet2}, our method outperforms other methods in terms of PSNR, SSIM and LPIPS$_{Alex}$, while using the second smallest model size. 


    % We compare our method with TensoRF and Point-NeRF on ScanNet dataset with 100k training iterations in Table.\ref{tb:scannet2}. The qualitative comparisons are in Fig.\ref{fig:scan_vis}. We use the same mesh as Point-NeRF from CLOMAP~\cite{schonberger2016pixelwise} to place our local tensors. Here we use 4 different scales of local tensors: 2.5, 1.6, 0.8 and 0.4. TensoRF-CP and TensoRF-VM are with resolutions (number of voxels along each dimension) from 150$^3$ to 500$^3$. Note that our method achieves the highest performance in terms of PSNR, LPIPS$_{Vgg}$ and LPIPS$_{Alex}$, while with the second smallest model size (the smallest one is TensoRF-CP$_{100k}$). The reason why Point-NeRF$_{100k}$ achieves the highest SSIM is that it highly relies on input geometry and there is almost no aliasing in the empty space. However, Point-NeRF got bad results in terms of other metrics because the input mesh is far from perfect and has many holes.  
    





%%%%%%%
% % Figure environment removed 
%%%%%%%%%%%%%    
\begin{table}[hbt]
        \begin{adjustwidth}{0pt}{0pt}  
        \centering
        \setlength\tabcolsep{3.0pt}
            \small{
                \begin{tabular}{l|cccc}
                %& \multicolumn{5}{c}{\textbf{Average over Scene 101 and Scene 241}} & \\
                \hline %\\[-1.05em]
                 Scale & PSNR~$\uparrow$ & SSIM~$\uparrow$  &  \# Param.(M)~$\downarrow$ & Time~$\downarrow$ \\ \hline
                 Single(0.6)  & 32.22 & 0.957 & 1.75 & 18.22m \\
                 Single(0.3) &  32.73 & 0.961 & 4.15 & 21.31m\\
                 Single(0.15) & 31.96 & 0.952 & 10.20 & 28.55m\\
                 Multi(0.6, 0.3) & 33.11 & 0.963 & 6.20 & 30.12m\\
                 Multi(0.6, 0.3, 0.15) & 33.55 & 0.965 & 13.52 & 35.70m
                 \\ \hline
                \end{tabular}
            }
             \captionsetup{aboveskip=3pt}
            \captionsetup{belowskip=-5pt}
            \caption {Ablation under different scale settings on Synthetic-NeRF dataset. We select 3 scales of tensors with cube sizes of 0.6, 0.3, and 0.15.}
            % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
            \label{tb:scale_ablation} 
        \end{adjustwidth}
    \end{table}
\subsection{Ablation study}
    We analyze our model in terms of different scales in Table.\ref{tb:scale_ablation}, while keeping the number of components the same (here we use 48). 
    The scale here is the size of our local tensors of each axis. We considered 3 different scales, i.e., 0.6, 0.3, and 0.15 respectively as single-scale settings and some of their combinations as multi-scale settings. Note that even with a single scale (0.3), the performance of our method can be comparable with some other methods such as iNGP~\cite{muller2022instant} while ours have less than half of the model size. 
    When increasing the number of scales or decreasing the size of local tensors, our model size will also increase. 
    In general, there is a trade-off of our method between scales and computational consumption (time and size). 
    
    Usually, a smaller scale can lead to better performance, though our method with a scale of 0.15 does not strictly follow because we don't have high-quality input geometry to place these local tensors with a very small size. 
    In fact, according to our per-scene breakdown results on the Synthetic-NeRF dataset (please refer to our supplemental material), single-scale(0.075) can achieve higher performance than single-scale(0.3) and single-scale(0.15) on most scenes, except for ship because it has many thin structures that our coarse reconstruction does not cover. 
    
   
    
    % % Figure environment removed




    %   \begin{table*}[ht]
    %     \begin{adjustwidth}{0pt}{0pt}  
    %     \centering
    %     % \setlength\tabcolsep{5pt}
    %         \small{
    %             \begin{tabular}{lcccccccc}
    %             & \multicolumn{3}{c}{\textbf{Scene 101}} & \multicolumn{3}{c}{\textbf{Scene 241}} &  \multicolumn{2}{c}{\textbf{Avg.}}\\
    %             \hline %\\[-1.05em]
    %             \multicolumn{1}{l|}{} & PSNR~$\uparrow$ & SSIM~$\uparrow$ & \multicolumn{1}{c|}{Size(MB)~$\downarrow$} & PSNR~$\uparrow$ & SSIM~$\uparrow$ & \multicolumn{1}{c|}{Size(MB)~$\downarrow$} & PSNR~$\uparrow$ & SSIM~$\uparrow$ \\ \hline
    %              \multicolumn{1}{l|}{SRN~\cite{sitzmann2019scene}}  & - & -&  \multicolumn{1}{c|}{-} & - & - &  \multicolumn{1}{c|}{-} & 18.25 & 0.592\\
    %              \multicolumn{1}{l|}{NeRF~\cite{martin2021nerf}}& - & -&  \multicolumn{1}{c|}{-} & - & - &  \multicolumn{1}{c|}{-} & 22.99 & 0.620 \\
    %              \multicolumn{1}{l|}{NSVF~\cite{liu2020neural}} & - & -&  \multicolumn{1}{c|}{-} & - & - &  \multicolumn{1}{c|}{-} & 25.48 & 0.688\\
    %              \multicolumn{1}{l|}{Point-NeRF\cite{xu2022point}} & 21.98 & \textbf{0.882} &  \multicolumn{1}{c|}{591.10} & 29.86 & \textbf{0.900} &  \multicolumn{1}{c|}{681.00} & 25.92 & \textbf{0.891} \\
    %              \multicolumn{1}{l|}{TensoRF-CP\cite{chen2022tensorf}} & 26.49 & 0.749 &  \multicolumn{1}{c|}{4.01} & 28.03 & 0.753 &  \multicolumn{1}{c|}{3.78} & 27.54 & 0.751  \\ 
    %              \multicolumn{1}{l|}{TensoRF-VM\cite{chen2022tensorf}} & 27.71 & 0.783 &  \multicolumn{1}{c|}{212.33} & 29.52 & 0.791 &  \multicolumn{1}{c|}{186.99} & 28.61 & 0.787 \\ 
    %             %TensoRF-VM(L) & 27.68 &  88.01 & 29.23 &  & 28.87 & 201.74\\
    %              \multicolumn{1}{l|}{Ours} & \textbf{28.29} & 0.789 &  \multicolumn{1}{c|}{53.32} & \textbf{29.81} & 0.794 &  \multicolumn{1}{c|}{49.22} & \textbf{29.05} & 0.792 \\ \hline
    %             \end{tabular}
    %         }
    %          \captionsetup{aboveskip=3pt}
    %         \captionsetup{belowskip=-10pt}
    %         \caption {Quantity comparison on two scenes in the ScanNet dataset \cite{dai2017scannet} selected in NSVF \cite{liu2020neural}. Point-NeRF\cite{xu2022point}, TensoRF-CP\cite{chen2022tensorf}, TensoRF-VM\cite{chen2022tensorf} and our TensoRF Cloud are reported after 100k iterations.}
    %         \label{tb:scannet} 
    %     \end{adjustwidth}
    % \end{table*}




    % \begin{table*}[ht]
    %     \begin{adjustwidth}{0pt}{0pt}  
    %     \centering
    %     % \setlength\tabcolsep{5pt}
    %         \small{
    %             \begin{tabular}{lccccc}
    %             & \multicolumn{5}{c}{\textbf{Average over Scene 101 and Scene 241}}  \\
    %             \hline %\\[-1.05em]
    %             \multicolumn{1}{l|}{} & PSNR~$\uparrow$ & SSIM~$\uparrow$ & LPIPS$_{Vgg}$ ~$\downarrow$ & LPIPS$_{Alex}$~$\downarrow$  &  Size(M)~$\downarrow$  \\ \hline
    %              \multicolumn{1}{l|}{SRN~\cite{sitzmann2019scene}}  & 18.25 & 0.592 & - & 0.586 & -\\
    %              \multicolumn{1}{l|}{NeRF~\cite{martin2021nerf}}& 22.99 & 0.620 & - &  0.369 & - \\
    %              \multicolumn{1}{l|}{NSVF~\cite{liu2020neural}} & 25.48 & 0.688 & - & 0.301 & -\\
    %              \multicolumn{1}{l|}{Point-NeRF\cite{xu2022point}} &  25.92 & \textbf{0.891} & 0.335 & 0.273 & 159.01 \\
    %              \multicolumn{1}{l|}{TensoRF-CP\cite{chen2022tensorf}} &  27.54 & 0.751 & 0.379 & 0.328 & \textbf{0.973} \\ 
    %              \multicolumn{1}{l|}{TensoRF-VM\cite{chen2022tensorf}} & 28.61 & 0.787 & 0.371 & 0.290 & 49.92 \\ 
    %             %TensoRF-VM(L) & 27.68 &  88.01 & 29.23 &  & 28.87 & 201.74\\
    %              \multicolumn{1}{l|}{Ours-48} &\textbf{29.05} & 0.792  & \textbf{0.319} & \textbf{0.243} & 12.82\\ \hline
    %             \end{tabular}
    %         }
    %          \captionsetup{aboveskip=3pt}
    %         \captionsetup{belowskip=-10pt}
    %         \caption {Quantity comparison on two scenes in the ScanNet dataset \cite{dai2017scannet} selected in NSVF \cite{liu2020neural}.}
    %         % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
    %         \label{tb:scannet2} 
    %     \end{adjustwidth}
    % \end{table*}

\begin{table}[hbt]
    \begin{adjustwidth}{0pt}{0pt}  
    \centering
    \setlength\tabcolsep{1.5pt}
        \small{
            \begin{tabular}{lcccc}
            \hline %\\[-1.05em]
            \multicolumn{1}{l|}{} & PSNR~$\uparrow$ & SSIM~$\uparrow$ & LPIPS$_{Alex}$~$\downarrow$  & \# Param.(M)~$\downarrow$  \\ \hline
             \multicolumn{1}{l|}{NeRF~\cite{mildenhall2020nerf}}  & 25.78 & 0.864 & 0.198 & - \\
             \multicolumn{1}{l|}{NSVF~\cite{liu2020neural}} & 28.40 & 0.900 & 0.153 & - \\
             \multicolumn{1}{l|}{TensoRF-CP$_{30k}$\cite{chen2022tensorf}} &  27.59 & 0.897 & 0.144 & \textbf{0.97} \\ 
             \multicolumn{1}{l|}{TensoRF-VM$_{30k}$\cite{chen2022tensorf}} & 28.56 & 0.920 & 0.125 & 49.92 \\ 
             \multicolumn{1}{l|}{Ours-48$_{30k}$} &\textbf{28.70} & \textbf{0.922}  & \textbf{0.113} & 14.11\\ \hline
            \end{tabular}
        }
         \captionsetup{aboveskip=3pt}
        \captionsetup{belowskip=-10pt}
        \caption {Quantitative comparison on scenes in the Tanks and Temples dataset \cite{Knapitsch2017} selected in NSVF \cite{liu2020neural}.  TensoRF-CP, TensoRF-VM and Ours-48 are optimized for 30k steps.}
        % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
        \label{tb:tt} 
    \end{adjustwidth}
\end{table}

We also compare our method with a variant that uses vector-matrix (VM) decomposition~\cite{chen2022tensorf} in each local tensor instead of CP decomposition. Please refer to Appendix.A. for more details. Also, we can achieve a higher training and inference speed without a significant performance drop, which we refer to Appendix.E.
% \begin{table*}[hbt]
%         \begin{adjustwidth}{0pt}{0pt}  
%         \centering
%         % \setlength\tabcolsep{5pt}
%             \small{
%                 \begin{tabular}{l|cccccc}
%                 %& \multicolumn{5}{c}{\textbf{Average over Scene 101 and Scene 241}} & \\
%                 \hline %\\[-1.05em]
%                  & PSNR~$\uparrow$ & SSIM~$\uparrow$ & LPIPS$_{Vgg}$ ~$\downarrow$ & LPIPS$_{Alex}$~$\downarrow$  &  Size(M)~$\downarrow$ & Time~$\downarrow$ \\ \hline
%                  Single-Scale(0.6)  & 32.22 & 0.957 & 0.057 & 0.032 & 1.75 & 18.22m \\
%                  Single-Scale(0.3) &  32.73 & 0.961 & 0.049 & 0.028 & 4.15 & 21.31m\\
%                  Single-Scale(0.15) & 31.96 & 0.952 & 0.037 & 0.034 & 10.20 & 28.55m\\
%                  Multi-Scale(0.6, 0.3) & 33.11 & 0.963 & 0.047 & 0.026 & 6.20 & 30.12m\\
%                  Multi-Scale(0.6, 0.3, 0.15) & 33.55 & 0.965 & 0.044 & 0.025 & 13.52 & 35.70m
%                  \\ \hline
%                 \end{tabular}
%             }
%              \captionsetup{aboveskip=3pt}
%             \captionsetup{belowskip=-10pt}
%             \caption {Ablation under different scale settings on Synthetic-NeRF dataset. We select 3 scales of tensors with cube sizes of 0.6, 0.3, and 0.15.}
%             % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
%             \label{tb:scale_ablation} 
%         \end{adjustwidth}
%     \end{table*}
    

    %  \begin{table*}[ht]
    %     \begin{adjustwidth}{0pt}{0pt}  
    %     \centering
    %     % \setlength\tabcolsep{5pt}
    %         \small{
    %             \begin{tabular}{l|ccccccc}
    %             \hline %\\[-1.05em]
    %             Scene 101 & SRN~\cite{sitzmann2019scene} & NeRF~\cite{martin2021nerf} &  NSVF~\cite{liu2020neural} & Point-NeRF\cite{xu2022point} & TensoRF-CP\cite{chen2022tensorf} &  TensoRF-VM\cite{chen2022tensorf} & Ours \\ \hline
    %             PSNR &  \\
    %             NeRF~\cite{martin2021nerf}& - & -& - & - & 22.99 & - \\
    %             NSVF~\cite{liu2020neural} & - & -& - & - & 25.48 & - \\
    %             Point-NeRF\cite{xu2022point} & 21.98 & 591.10 & 29.86 & 681.00 & 25.92 & 636.05\\
    %             TensoRF-CP & 26.49 & 0.749 & 4.01 & 28.03 & 0.753 & 3.78 & 27.54 & 0.751 & 3.89 \\ 
    %             TensoRF-VM & 27.71 & 0.783 & 212.33 & 29.52 & 0.791 & 186.99 & 28.61 & 0.787 & 199.66\\ 
    %             %TensoRF-VM(L) & 27.68 &  88.01 & 29.23 &  & 28.87 & 201.74\\
    %             Ours & 28.29 & 0.789 & 53.32 & 29.813 &  &  &  & & \\ \hline
    %             \end{tabular}
    %         }
    %          \captionsetup{aboveskip=3pt}
    %         \captionsetup{belowskip=-10pt}
    %         \caption {Quantity comparison on two scenes in the ScanNet dataset \cite{dai2017scannet} selected in NSVF \cite{liu2020neural}.}
    %         % \KS{Add time/iterations for IBRNet. What are the different variants of Point-NeRF discussed here?}}
    %         \label{tb:scannet} 
    %     \end{adjustwidth}
    % \end{table*}