
% shading & features:
% \newcommand{\Densf}{f^\Dens}
% \newcommand{\Radf}{f^\Rad}
% \newcommand{\AggF}{A}
% \newcommand{\AppearanceM}{\mathbf{B}}
% \newcommand{\AppearanceMC}{\AppearanceM^\Rad}
% \newcommand{\AppearanceV}{\mathbf{b}}
% \newcommand{\AppearanceFunc}{\psi}
% \newcommand{\DensShift}{\epsilon}
% \newcommand{\QueryNum}{M}


\newcommand{\DVGORes}{100^3}
\newcommand{\DVGODuration}{30}

\section{Implementation}

To obtain coarse scene geometry, we modify the coarse density estimation introduced in \cite{sun2022direct} and get a $\DVGORes$ occupancy volume in  $\DVGODuration$ seconds. We can skip this step if there exists available geometric data, e.g., the meshes in ScanNet \cite{dai2017scannet}, or point clouds from multiview stereo. According to the experiments, our method is not very sensitive to the initial geometry. Please refer to Appendix.B. for more details. We set the default number of scales to 3. In a scene box of [-1,1], we rasterize the scene geometry (occuppied voxels centers or points) onto 3 grids with different voxel sizes, e.g., $0.4^3,0.2^3,0.1^3$. For each grid, we distribute tri-vector tensors at the center of its occupied voxels. The tensor spatial coverage of these 3 scales is $0.6^3, 0.3^3, 0.15^3$, respectively. For each scale, we query $\QueryNum = 4$ nearby tensors. Following \cite{sun2022direct}, our feature decoding network $\AppearanceFunc$ is a 2-layer MLP with 128 channels. For each scale, its appearance matrix $\AppearanceMC$ is implemented by a single linear layer of 27 channels. 

We implement the framework in PyTorch \cite{imambi2021pytorch} with customized CUDA operations. During optimization, we adopt the coarse to fine strategy in \cite{chen2022tensorf}, linearly up-sample the vectors' dimension ($I,J,K$) from 29 to 121 for scale one, 15 to 61 for scale two, and 7 to 31 for scale three. The up-sampling happens at step 2000, 3000, 4000, 5500, and 7000. We use the Adam optimizer \cite{kingma2014adam} with initial learning rates of 0.02 for vectors and 0.001 for networks. On a single 3090 RTX GPU, we train each model for 30000 steps while each batch contains 4096 rays. Please find more details in the supplemental materials.


 