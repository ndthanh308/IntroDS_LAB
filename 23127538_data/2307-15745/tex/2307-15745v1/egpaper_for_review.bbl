\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
  C.~Lawrence Zitnick, and Devi Parikh.
\newblock {VQA: Visual Question Answering}.
\newblock {\em IEEE International Conference on Computer Vision},
  abs/1505.00468, 2015.

\bibitem{vqa_understanding_vs_accessibility}
Yang~Trista Cao, Kyle Seelman, Kyungjun Lee, and Hal~III Daumé.
\newblock {What’s Different between Visual Question Answering for Machine
  “Understanding” Versus for Accessibility?}
\newblock In {\em The 2nd Conference of the Asia-Pacific Chapter of the
  Association for Computational Linguistics and the 12th International Joint
  Conference on Natural Language Processing}, 2022.

\bibitem{visual_dialog}
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José
  M.~F. Moura, Devi Parikh, and Dhruv Batra.
\newblock {Visual Dialog}, 2016.

\bibitem{dognin2022image}
Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit Padhi, Mattia Rigotti, Jarret
  Ross, Yair Schiff, Richard~A Young, and Brian Belgodere.
\newblock {Image Captioning as an Assistive Technology: Lessons Learned from
  VizWiz 2020 Challenge}.
\newblock {\em Journal of Artificial Intelligence Research}, 73:437--459, 2022.

\bibitem{vqa_v2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock {Making the V in VQA Matter: Elevating the Role of Image
  Understanding in Visual Question Answering}.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, 2017.

\bibitem{image_search}
Darren Guinness, Edward Cutrell, and Meredith~Ringel Morris.
\newblock {Caption Crawler: Enabling Reusable Alternative Text Descriptions
  using Reverse Image Search}.
\newblock In {\em Proceedings of the 2018 CHI Conference on Human Factors in
  Computing Systems}, 2018.

\bibitem{vizwiz}
Danna Gurari, Qing Li, Abigale~J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
  Jiebo Luo, and Jeffrey~P Bigham.
\newblock {VizWiz Grand Challenge: Answering Visual Questions from Blind
  People}.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3608--3617, 2018.

\bibitem{gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock {GQA: A new dataset for real-world visual reasoning and compositional
  question answering}.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6700--6709, 2019.

\bibitem{clevr}
Justin Johnson, Bharath Hariharan, Laurens van~der Maaten, Li Fei-Fei,
  C~Lawrence Zitnick, and Ross Girshick.
\newblock {CLEVR: A Diagnostic Dataset for Compositional Language and
  Elementary Visual Reasoning}.
\newblock In {\em 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 1988--1997. IEEE, 2017.

\bibitem{refless_metrics}
Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith~Ringel
  Morris, and Christopher Potts.
\newblock Context matters for image descriptions for accessibility: Challenges
  for referenceless evaluation metrics.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 4685--4697, 2022.

\bibitem{concadia}
Elisa Kreiss, Fei Fang, Noah Goodman, and Christopher Potts.
\newblock Concadia: Towards image-based text generation with a purpose.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 4667--4684, 2022.

\bibitem{blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models}.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{ok_vqa}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock {OK-VQA: A Visual Question Answering Benchmark Requiring External
  Knowledge}.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3195--3204, 2019.

\bibitem{muehlbradt2022s}
Annika Muehlbradt and Shaun~K Kane.
\newblock {What's in an ALT Tag? Exploring Caption Content Priorities through
  Collaborative Captioning}.
\newblock {\em ACM Transactions on Accessible Computing (TACCESS)},
  15(1):1--32, 2022.

\bibitem{openai2023gpt4}
OpenAI.
\newblock {GPT-4 Technical Report}, 2023.

\bibitem{describing_images_survey}
Helen Petrie, Chandra Harrison, and Sundeep Dev.
\newblock {Describing images on the web: A survey of current practice and
  prospects for the future}.
\newblock In {\em Proceedings of Human Computer Interaction International},
  2005.

\bibitem{person_shoes_tree}
Abigale Stangl, Meredith~Ringel Morris, and Danna Gurari.
\newblock {Person, Shoes, Tree. Is the Person Naked? What People with Vision
  Impairments Want in Image Descriptions}.
\newblock In {\em Conferences on Human-Computer Interaction}, 2020.

\bibitem{beyondonesize}
Abigale Stangl, Nitin Verma, Kenneth Fleischmann, Meredith~Ringel Morris, and
  Danna Gurari.
\newblock {Going Beyond One-Size-Fits-All Image Descriptions to Satisfy the
  Information Wants of People Who are Blind or Have Low Vision}.
\newblock In {\em 23rd International ACM SIGACCESS Conference on Computers and
  Accessibility (ASSETS ’21)}, 2021.

\bibitem{visual7w}
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.
\newblock Visual7w: Grounded question answering in images.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4995--5004, 2016.

\end{thebibliography}
