\documentclass{ecai}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{optidef}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{adjustbox}
\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.

                  
\newtheorem{theo}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\begin{document}

\begin{frontmatter}

\title{A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs}

%\author[A]{\fnms{First}~\snm{Author}\orcid{....-....-....-....}\thanks{Corresponding Author. Email: somename@university.edu.}}
%\author[B]{\fnms{Second}~\snm{Author}\orcid{....-....-....-....}}
%\author[B]{\fnms{Third}~\snm{Author}\orcid{....-....-....-....}} % use of \orcid{} is optional


%\address[A]{Short Affiliation of First Author}
%\address[B]{Short Affiliation of Second Author and Third Author}


\author[A]{\fnms{C.}~\snm{Coelho}\orcid{0009-0009-4502-937X}\thanks{Corresponding Author. Email: cmartins@cmat.uminho.pt}}
\author[A]{\fnms{M. Fernanda}~\snm{P. Costa}\orcid{0000-0001-6235-286X}}
\author[A;B]{\fnms{L.L.}~\snm{Ferrás}\orcid{0000-0001-5477-3226}} % use of \orcid{} is optional


\address[A]{Centre of Mathematics (CMAT), University of Minho, Braga, Portugal}
\address[B]{Department of Mechanical Engineering, FEUP - University of Porto, Porto, Portugal}

\begin{abstract}
% The continuous dynamics exhibited by natural systems have been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems.
% In this work, we propose a self-adaptive penalty algorithm for Neural ODEs, to allow modelling constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge constraints contributes to increasing the interpretability of Neural ODE-based models.
% We validate the proposed approach by modelling three constrained natural systems: population growth, chemical reaction evolution, and damped harmonic oscillator motion.
% The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, show the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems. Moreover, the self-adaptive penalty approach provides more accurate and robust models, with reliable and meaningful predictions. 


%(Luís) fiz apenas pequenas alterações:
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems.
In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models.
We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion.
The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems. Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.

%The code to replicate the experiments presented in this work is available at [LINK].



\end{abstract}

\end{frontmatter}

\section{Introduction}

Accurately predicting the behaviour of natural systems requires mathematical equations that describe the relationships between their variables over time. Due to its continuous-time dynamics, these systems are often formulated by Ordinary Differential Equations (ODEs). ODEs provide a robust framework for understanding and analysing the continuous changes that occur in natural systems.

However, traditional methods of modelling natural systems using ODEs can be challenging due to the complexity of the interactions between variables. In recent years, Neural Networks (NNs) have been used to model the dynamics of natural systems based on experimental data. However, the resulting models are typically discrete and may not be suitable for modelling continuous-time dynamics. Specifically, the discrete nature of NN models limits their ability to capture the complex interactions between variables in natural systems, which are often continuous \cite{chenNeuralOrdinaryDifferential2019}. 

Neural ODEs \cite{chenNeuralOrdinaryDifferential2019} are a NN architecture that introduces the concept of adjusting the solution curve of a continuous-time function (ODE) $\boldsymbol{f_\theta}$ to the data. The function $\boldsymbol{f_\theta}$ is defined by a NN with parameters $\boldsymbol{\theta}$, and the result of training a Neural ODE is an ODE. To make predictions, an Initial Value Problem (IVP) is solved \cite{chenNeuralOrdinaryDifferential2019}. Some natural systems modelled with Neural ODEs are reported in the literature \cite{suKineticsParameterOptimization2022, xingContinuousGlucoseMonitoring2022}.

In \cite{suKineticsParameterOptimization2022} the authors use a Neural ODE to optimise the kinetic parameters of chemical reactions, demonstrating the effectiveness of this approach in modelling complex combustion phenomena.

In \cite{xingContinuousGlucoseMonitoring2022} the authors propose using a Neural ODE to model glucose levels based on sparse training data, which exhibits high precision in predicting blood glucose trends, demonstrating its potential for practical applications in patient care.

% An important characteristic often found in natural systems is the presence of governing laws mathematically designated by constraints. However, when building the mathematical models of these systems, the underlying dynamics may exhibit complex and nonlinear dependencies among variables. This may lead to poor mathematical modelling and consequently unmeaningful predictions.

Natural systems often have governing laws that are mathematically expressed as constraints. However, when we create mathematical models for these systems, we may encounter complex and nonlinear dependencies among variables, which can make the modelling process difficult and lead to inaccurate predictions.

% NNs are commonly referred to as "black-box" models, complex and opaque, since it is difficult to understand what information the model has extracted from the training data, making it challenging to interpret its predictions. This lack of interpretability raises concerns and distrust in the scientific community, since there is no guarantee that the NN-based models satisfy the underlying governing laws of these natural systems.

Neural networks (NNs) are commonly referred to as "black-box" models because they are complex and opaque. It can be challenging to understand how the model uses the training data to make predictions, which raises concerns about their interpretability and reliability. This lack of transparency can lead to distrust in the scientific community, as there is no guarantee that NN-based models satisfy to the underlying governing laws of these natural systems.

Incorporating prior knowledge into NNs can reduce the need for large amounts of training data, improve generalisation, predictive performance and reduce the risk of overfitting.

In the literature, several strategies have been proposed to incorporate prior knowledge constraints into NNs \cite{vonruedenInformedMachineLearning2023}. A popular and well-established constraint optimisation approach is the use of penalty methods. Penalty methods have already been used in NNs and Neural ODEs \cite{daiNovelEstimationMethod2019,tuorConstrainedNeuralOrdinary2020}.

For instance in \cite{daiNovelEstimationMethod2019} the state of health of a lithium-ion battery is predicted by introducing constraints related to the charging profile of the battery. 

In \cite{tuorConstrainedNeuralOrdinary2020} the authors use a penalty method to introduce prior knowledge to guarantee safety and performance when modelling industrial systems.

Penalty methods are very straightforward to implement and easy to incorporate into standard NN training algorithms. However, a major challenge is the choice of the penalty parameter $\mu$. The penalty parameter plays a crucial role in controlling the trade-off between the original loss function and the penalty term, influencing the overall training process and the model's final performance. Thus, selecting an appropriate value for $\mu$ is not a trivial task, as it requires a careful balance.
To address the issue of selecting an appropriate $\mu$, adaptive penalty functions have been proposed. These functions are designed to dynamically adjust penalty parameters during the optimisation process \cite{aliPenaltyFunctionbasedDifferential2013, costaTheoreticalPracticalConvergence2017}. 

% The contributions of this work are a new self-adaptive penalty function and a self-adaptive penalty algorithm for Neural ODEs, for modelling constrained natural systems. The proposed method adjusts the penalty parameters $\mu$ by inferring and updating it using the information provided by the constraints violation at each iteration. 
% The penalty parameters $\mu$ are updated such that the constraints are penalised more strongly if they are heavily violated, and less if they are slightly violated.
% The proposed self-adaptive penalty method balances the minimisation of the objective function with the satisfaction of the constraints. Additionally, by adapting $\mu$ over time, the method is able to handle possible changes in the input data distribution, being more robust and adaptable.

This work presents a new approach to modelling constrained natural systems using Neural ODEs. We introduce a new self-adaptive penalty function and algorithm that adjusts the penalty parameters $\mu$ based on constraints violation (at each iteration). The proposed method applies stronger penalties to heavily violated constraints and weaker penalties to slightly violated ones, balancing the objective function minimisation with constraint satisfaction. By adapting $\mu$ over time, our approach is more robust and adaptable to possible changes in input data distribution. Our contributions are significant steps towards more effective and flexible modelling of constrained natural systems using Neural ODEs.

This paper is organised as follows. Section \ref{sec:background} provides a brief introduction to Neural ODEs and penalty methods. Section \ref{sec:method} presents the new self-adaptive penalty function and Section \ref{sec:algo} outlines the self-adaptive penalty algorithm for Neural ODEs. Section \ref{sec:experiments} presents the results obtained for some numerical experiments. The conclusions and future work are presented in Section \ref{sec:conclusion}.


\section{Background} \label{sec:background}

Consider a time series describing a natural system $\boldsymbol{X}=(\boldsymbol{x}_1, \boldsymbol{x}_2, \dots, \boldsymbol{x}_N)$, with $\boldsymbol{x}_n \in \mathbb{R}^d$ at time step $t_n$ ($n=1,\dots,N$). Let $\boldsymbol{Y}=(\boldsymbol{y}_1, \boldsymbol{y}_2, \dots, \boldsymbol{y}_N)$ be the corresponding ground-truth output time series, with $\boldsymbol{y}_n \in \mathbb{R}^{d^*}$ and $\boldsymbol{\hat{Y}}=(\boldsymbol{\hat{y}}_1, \boldsymbol{\hat{y}}_2, \dots, \boldsymbol{\hat{y}}_N)$ is the prediction, with $\boldsymbol{\hat{y}}_n \in \mathbb{R}^{d^*}$, at time step $t_n $.

\subsection{Neural ODEs}

Chen et al. proposed Neural ODEs, a NN architecture that adjusts a continuous-time function (ODE) to the dynamics of the training data \cite{chenNeuralOrdinaryDifferential2019}. 

A Neural ODE is composed of two components, a NN that builds the ODE dynamics, $\boldsymbol{f}_{\boldsymbol{\theta}}$, and a numerical ODE solver. During training, the parameters $\boldsymbol{\theta}$ of the NN are adjusted by comparing the ground-truth values with the predictions made by using the ODE solver to solve an IVP over a time interval, $(t_0, t_N)$:

\begin{equation*}
   \{\boldsymbol{\hat{y}}_{n}\}_{n= 1 \dots N} = ODESolve(\boldsymbol{f_\theta}, \boldsymbol{x}_0 , (t_0, t_N)), 
\end{equation*}

\noindent where $(\boldsymbol{x}_0, t_0)$ is the initial condition of the IVP, $x(t_0)=x_0$, \, $\boldsymbol{\hat{y}}_n$ is the solution at an arbitrary time step $t_n$ with $n=1, \dots, N$. 

Therefore, the solver produces predictions that are adjusted by the NN to be as close as possible to the ground-truth data $\boldsymbol{Y}$.  

\subsection{Approaches for Handling Constrained Systems}

In general, a constrained natural system can be formulated as the following constrained optimisation problem:

\begin{mini}|l|[0]
    {\boldsymbol{\theta} \in \mathbb{R}^{n_\theta}}{l(\boldsymbol{\theta})}
    {\label{eq:constrainedMinProblem}}
    {}
    \addConstraint{\boldsymbol{c}_t^i(\boldsymbol{\theta)}}{= 0}, \,\,\, i \in \varepsilon  ,\,\,\, t=t_1, \dots, t_N
    \addConstraint{\boldsymbol{c}_t^j(\boldsymbol{\theta)}}{ \le 0}, \,\, j \in \mathcal{I} ,\,\,\,  t=t_1, \dots, t_N,
\end{mini}

\noindent where $l: \mathbb{R}^{n_\theta} \rightarrow \mathbb{R}$ is the objective function, $\boldsymbol{c}_t^i, \boldsymbol{c}_t^j: \mathbb{R}^{n_\theta} \rightarrow \mathbb{R}$ are the equality and inequality constraint functions, respectively, with $\varepsilon$ the equality and $\mathcal{I}$ inequality index sets of constraints. The set of points that satisfy all the constraints defines the feasible set $\mathcal{S}=\{ \boldsymbol{\theta}  \in \mathbb{R}^{n_\theta} : \boldsymbol{c}_t^i(\boldsymbol{\theta})=0, \,  i \in \varepsilon ;\,\, \boldsymbol{c}_t^j(\boldsymbol{\theta}) \leq 0, \, j \in \mathcal{I}, t=t_1,\dots,t_N \}$.

 The constrained optimisation problem \eqref{eq:constrainedMinProblem}, can be rewritten and solved as an unconstrained problem using penalty methods. Penalty methods combine the objective function $l$ and the constraints into a penalty function \cite{fletcherBriefHistoryFilter}. For instance, using a quadratic penalty function,   %being solved by a sequence of unconstrained problems \cite{fletcherBriefHistoryFilter}.

\begin{mini}|l|[0]
    {\boldsymbol{\theta}\in \mathbb{R}^{n_\theta}}{l(\boldsymbol{\theta}) + \dfrac{\mu}{2} P(\boldsymbol{\theta})}
    {\label{eq:penalty}}
    {} \vspace{-0.5cm}
\end{mini}

\noindent with 

$$P(\boldsymbol{\theta})=\sum_{i \in \varepsilon} \dfrac{1}{N} \sum_{n=1}^N |c_{t_n}^i(\boldsymbol{\theta})|^2 + \sum_{j \in \mathcal{I}} \dfrac{1}{N} \sum_{n=1}^N \left([c_{t_n}^j(\boldsymbol{\theta})]^+\right)^2$$

\noindent where $\mu > 0$ is the penalty parameter and $[z]^+$ denotes $ \max(z, 0)$. 

Note that, to generalise penalty methods for time series data, we compute penalty terms as the average of the quadratic constraints violation for all time steps $t_n$ with $n=1,\dots,N$.

Generally, the penalty function is minimised for a sequence of increasingly larger values of $\mu$ until a solution is found \cite{nocedalNumericalOptimization2006}.
% Choosing an appropriate initial value of $\mu$ is a difficult task. Large values of $\mu$ enforces the constraints harder but can lead to slower convergence, whereas smaller values penalises the violation lighter leading to infeasible solutions
Selecting an appropriate initial value of $\mu$ can be challenging. Large values of $\mu$ enforce constraints more strictly, but can result in slower convergence. Conversely, smaller values of $\mu$ penalize constraint violations more lightly, potentially leading to infeasible solutions \cite{aliPenaltyFunctionbasedDifferential2013}.
Thus, finding the right balance for $\mu$ is crucial to achieve both constraint satisfaction and efficient training. However, this process often involves a tedious trial and error approach due to the trade-off between constraint adherence and convergence speed. 

When applied to NNs, penalty methods incorporate constraints by adding a penalty term into the loss function, resulting in a penalty loss function. The penalty function is defined by a measure of the distance between the ground-truth and the predicted values, $l$, and a measure of the violation of the constraints, $\mu P$. In practice, it is common to use a fixed value of $\mu$ during all the NN training process \cite{caiPhysicsinformedNeuralNetworks2021, daiNovelEstimationMethod2019}. However, as mentioned previously, selecting an appropriate value of $\mu$ is challenging, and using a fixed value that remains unchanged during the optimisation process can lead to some problems. One of the primary issues is that a fixed $\mu$ may not be optimal for all stages of training. In the early stages, when the NN is far from the optimal solution, using a large $\mu$ could lead to over-penalisation of constraints, impeding the model's ability to explore the solution space effectively. On the other hand, in later stages when the NN is close to convergence, a fixed $\mu$ might not provide enough constraint enforcement, leading to sub-optimal solutions.

\section{Self-Adaptive Penalty Function} \label{sec:method}

The aim of this study is to develop a self-adaptive penalty function, $\phi(\boldsymbol{\theta})$,
%Unlike traditional penalty methods where $\mu$, is predetermined by the user, our approach eliminates the need for such manual specification.
which dynamically adjusts the penalty parameters $\mu$ taking into account the constraints violation during the training process of the NN. The proposed self-adaptive penalty function adapts the penalty parameters to the degree of constraint violation observed at the current iteration.

Problem \eqref{eq:constrainedMinProblem} is rewritten as an unconstrained problem,

\begin{mini}|l|[0]
    {\boldsymbol{\theta}\in \mathbb{R}^{n_\theta}}{\phi (\boldsymbol{\theta})}
    {\label{eq:adaptive}}
    {},
\end{mini}

\noindent where the self-adaptive penalty function $\phi$ relates the objective function $l$ and the constraints $\boldsymbol{c}_t^i, \boldsymbol{c}_t^j$.

To define $\phi$, the objective function $l$ and the constraint violation values at each point $\theta$, are normalised using a bounded function $\psi$, to ensure that all values have the same order of magnitude. In this work, we propose the following normalisation function \vspace{-0.2cm}$$\psi(x) = 1 - \dfrac{1}{1+x}$$ 
where $0\leq \psi(x) \leq 1$, for all $x \in \mathbb{R}_0^+$.

The objective function value $l$ at each point $\boldsymbol{\theta}$ is normalised using $\psi$ resulting in the new objective function $F_{\boldsymbol{\theta}}$:

\begin{equation}
\label{eq:F}
F_{\boldsymbol{\theta}} = \psi(l(\boldsymbol{\theta})).
\end{equation}

For each equality constraint $c^i_t$, the constraint violation vector $\boldsymbol{v}^i \in \mathbb{R}^\varepsilon$, at each point $\boldsymbol{\theta}$, is given by, 
$$\boldsymbol{v}^i = |c_t^i(\boldsymbol{\hat{y}}_t)|^2_{t=t_1\dots t_N}.$$ 
Then, the normalisation step $\psi$ is applied and the total violation $P_i$ is given by the average of the violations $v_t^i$, with $t=t_1,\dots,t_N$:

\begin{equation}
\label{eq:Pi}
P_i =  \dfrac{1}{N}  \sum_{n=1}^N \psi(v^i_{t_n}).
\end{equation}

Likewise, for each inequality constraint $c^j_t$ the constraint violation vector $\boldsymbol{v}^j \in \mathbb{R}^{\mathcal{I}}$, at each point $\boldsymbol{\theta}$, is defined by,
$$\boldsymbol{v}^j = ([c_t^j(\boldsymbol{\hat{y}}_t)]^+)^2_{t=t_1,\dots, t_N}.$$ 
Then, the normalisation step $\psi$ is applied and the total violation $P_j$ is given by the average of the violations $v^j_t$, with $t=t_1,\dots,t_N$:

\begin{equation}
\label{eq:Pj}
P_j = \dfrac{1}{N} \sum_{n=1}^N \psi(v^j_{t_n}).
\end{equation}

For each equality and inequality constraints, penalty parameters $\mu_i$ and $\mu_j$, respectively, are defined by the proportion of predictions that violate the constraint at all time steps $t=t_1,\dots,t_N$, at the current iteration:

\begin{equation}
\label{eq:mu}
\mu_i = \dfrac{\#\{t: v_t^i \neq 0\} }{N}, \,\,\, \mu_j = \dfrac{\#\{t: v_t^j \neq 0\}}{N},
\end{equation}

\noindent where $\#\{z\}$ denotes the cardinality of set $z$.

One of the advantages of computing self-adaptive penalty parameters is that it does not require to be provided by the user. Instead, they are computed using the information, gathered at each iteration, on how many predictions violate a constraint. A constraint that is violated in a higher number of time steps (than any other constraint) will have a larger penalty parameter. Additionally, the normalisation step $\psi$ prevents numerical instability and improves the accuracy of the training process. 

Finally, the self-adaptive penalty function $\phi$ is dynamically defined at each iteration as follows. The $\phi$ function is given by the $F_{\boldsymbol{\theta}}$ function, if the current point $\boldsymbol{\theta}$ is a feasible point. Otherwise, $\phi$ is defined by $F_{\boldsymbol{\theta}}$ plus the penalty terms $P_i$ and $P_j$ multiplied by the self-adaptive penalty parameters $\mu_i$ and $\mu_j$. Specifically, a penalty term per constraint violation, given by the normalised sum of the constraint violation vector $\boldsymbol{v}^i, \boldsymbol{v}^j$ multiplied by its corresponding penalty parameter $\mu_i, \mu_j$ for equality and inequality constraints, respectively.

\begin{equation}
\label{eq:phi}
    \phi(\boldsymbol{\theta}) =
    \begin{cases}
        F_{\boldsymbol{\theta}}, & \text{if } \boldsymbol{\theta} \in \mathcal{S}, \cr
        F_{\boldsymbol{\theta}} + \dfrac{1}{\#\{\varepsilon\}} \sum_{i \in \varepsilon} \mu_i P_i + \dfrac{1}{\#\{\mathcal{I}\}} \sum_{j \in \mathcal{I}} \mu_j P_j , & \text{if } \boldsymbol{\theta} \notin \mathcal{S},
    \end{cases}
\end{equation}

\noindent with $F_\theta$, $P_i$, $P_j$, $\mu_i$ and $\mu_i$ given by \eqref{eq:F}-\eqref{eq:mu}, respectively.

We demonstrate the equivalence of problems \eqref{eq:constrainedMinProblem} and \eqref{eq:adaptive}, thereby establishing that they possess identical global minimizers.

\begin{theo}
\label{theo:1}
    Let $\boldsymbol{\theta^*}$ be a global solution of \eqref{eq:constrainedMinProblem} and let $\boldsymbol{\omega} \in \mathcal{S}$ be such that $l(\boldsymbol{\omega}) \geq l(\boldsymbol{\theta}^*)$. Then, $\boldsymbol{\theta^*}$ is a global solution to \eqref{eq:adaptive}, where $\phi$ is the penalty function defined in \eqref{eq:phi}.
\end{theo}

\begin{proof}
    Let $\boldsymbol{\theta^*} \in \mathcal{S}$ be a global solution of \eqref{eq:constrainedMinProblem}. By definition, we have $l(\boldsymbol{\theta^*}) \leq l(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \mathcal{S}$. Therefore, for all $\boldsymbol{\theta} \in \mathcal{S}$, we have:
    
    $$
    \phi(\boldsymbol{\theta^*}) = 1 - \dfrac{1}{1 + l(\boldsymbol{\theta^*})} \leq  1 - \dfrac{1}{1 + l(\boldsymbol{\theta})} = \phi(\boldsymbol{\theta}).
    $$
    
    Now, consider the case when $\boldsymbol{\theta} \in \mathbb{R}^{n_\theta}{\setminus}\mathcal{S}$. Assuming that $l(\boldsymbol{\theta}) \leq l(\boldsymbol{\omega})$, we have,
    
    \begin{multline}
    \phi(\boldsymbol{\theta}^*) = \psi(l(\boldsymbol{\theta^*})) \leq \psi(l(\boldsymbol{\omega})) \leq \psi(l(\boldsymbol{\theta})) \leq \\ \psi(l(\boldsymbol{\theta})) + \dfrac{1}{\#\{\varepsilon\}} \sum_{i \in \varepsilon} \mu_i P_i + \dfrac{1}{\#\{\mathcal{I}\}} \sum_{j \in \mathcal{I}} \mu_j P_j = \phi(\boldsymbol{\theta}),
    \end{multline}
    
    which implies $\phi(\boldsymbol{\theta^*}) \leq \phi(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \mathbb{R}^{n_\theta}$, \textit{i.e.}, $\boldsymbol{\theta^*}$ is a global solution of \eqref{eq:adaptive}.
\end{proof}

\begin{lemma}
\label{lem:1}
    If $\boldsymbol{\theta^*}$ is a global solution of \eqref{eq:adaptive}, where $\phi$ is the penalty function defined in \eqref{eq:phi}, then $\boldsymbol{\theta^*}$ is a feasible point of \eqref{eq:constrainedMinProblem}.
\end{lemma}

\begin{proof}
    By contradiction, we assume that $\boldsymbol{\theta^*} \in \mathbb{R}^{n_\theta}{\setminus}\mathcal{S}$. When $l(\boldsymbol{\theta^*}) \leq l(\boldsymbol{\omega})$ and $\boldsymbol{\omega} \in \mathcal{S}$ we get, from \eqref{eq:phi},

    \begin{multline}
        \phi(\boldsymbol{\theta^*}) = \psi(l(\boldsymbol{\theta^*})) + \dfrac{1}{\#\{\varepsilon\}} \sum_{i \in \varepsilon} \mu_i P_i + \dfrac{1}{\#\{\mathcal{I}\}} \sum_{j \in \mathcal{I}} \mu_j P_j \\ 
        > \psi(l(\boldsymbol{\theta^*})) > \psi(l(\boldsymbol{\omega})) = \phi(\boldsymbol{\omega}),
    \end{multline}

    which contradict the definition of a global solution of \eqref{eq:adaptive}. Therefore, $\boldsymbol{\theta^*} \in \mathcal{S}$.
\end{proof}

We are now able to establish the reciprocal of Theorem \ref{theo:1}.

\begin{theo}
    Let $\boldsymbol{\theta^*} \in \mathcal{R}^{n_\theta}$ be a global solution of \eqref{eq:adaptive}, where $\phi$ is the penalty function defined by \eqref{eq:phi}. Then, $\boldsymbol{\theta^*}$ is a global solution of \eqref{eq:constrainedMinProblem}.
\end{theo}

\begin{proof}
    By Lemma \ref{lem:1} $\boldsymbol{\theta^*} \in \mathcal{S} \subset \mathbb{R}^{n_\theta}$. We have  $\psi(l(\boldsymbol{\theta^*})) = \phi(\boldsymbol{\theta^*}) \leq \phi(\boldsymbol{\theta})$ for all $\boldsymbol{\theta} \in \mathbb{R}^{n_\theta}$, and in particular, for all $\boldsymbol{\theta} \in \mathcal{S}$, we have $\psi(l(\boldsymbol{\theta^*})) \leq \psi(l(\boldsymbol{\theta}))$, which implies $l(\boldsymbol{\theta^*}) \leq l(\boldsymbol{\theta})$. Therefore, $\boldsymbol{\theta^*}$ is a global solution of \eqref{eq:constrainedMinProblem}.
\end{proof}


\section{Self-adaptive Penalty Algorithm for Neural ODEs} \label{sec:algo}

% Herein, we present the algorithm that implements the self-adaptive penalty algorithm for Neural ODEs, for modelling natural systems with prior knowledge constraints.

In this section, we introduce an algorithm that implements the self-adaptive penalty method for Neural ODEs to model natural systems with prior knowledge constraints. The algorithm is straightforward and requires minimal modification to the traditional Neural ODE training algorithm presented in \cite{chenNeuralOrdinaryDifferential2019}.

% During the whole NN optimisation process, the point $\boldsymbol{\theta}$ that produces the smallest value of $\phi$, $\phi_{\text{best}}$, is kept, $\boldsymbol{\theta}_{\text{best}}$. Otherwise, it is discarded, and the optimisation step is done with $\boldsymbol{\theta}_{\text{best}}$.

Throughout the process of optimising the neural network, we keep track of the point $\boldsymbol{\theta}$ that produces the smallest value of $\phi$ (denoted as $\phi_{\text{best}}$) and store it as $\boldsymbol{\theta}_{\text{best}}$. This serves as a guide during the optimization process to prevent the solution from deteriorating. If a new point does not improve upon $\boldsymbol{\theta}_{\text{best}}$, we discard it and continue the optimisation process using $\boldsymbol{\theta}_{\text{best}}$.

The stopping criteria is the maximum number of iterations, $k_{\max}$. When $k_{\max}$ is reached, the final parameters of the NN that build the ODE dynamics are $\boldsymbol{\theta}_{\text{best}}$.

The proposed self-adaptive penalty algorithm for Neural ODEs is presented in Algorithm \ref{alg:selfAdaptive}.

\begin{algorithm*}[]
\caption{: The self-adaptive penalty algorithm for Neural ODEs.}
\label{alg:selfAdaptive}
\begin{algorithmic}
\State \textbf{Input:} Initial condition $(\boldsymbol{y}_0,t_0)$, start time $t_0$, end time $t_{N}$, maximum number of iterations $k_{max}$;
\State $tol \leftarrow 1e-4$;
\State $\psi(x) = 1 - \dfrac{1}{1+x}$; 
\State $\boldsymbol{f_\theta} = DynamicsNN()$;
\State Initialise parameters $\boldsymbol{\theta}$;
\State $\phi_{\text{best}} \leftarrow +\infty$; 

\For {$k=1:k_{max}$}
    \State \{$\boldsymbol{\hat{y}}_n\}_{n=1, \dots, N} = ODESolve(\boldsymbol{f_\theta}, \boldsymbol{y}_0, (t_0, t_N))$;
    \State Evaluate normalised objective function $F_{\boldsymbol{\theta}}$;
    
   \State $P_\theta \leftarrow 0$; 
    \For {$i \in \varepsilon$}
        \State Compute penalty term $P_i$ using \eqref{eq:Pi};
        \State Compute self-adaptive penalty parameter $\mu_i$ using \eqref{eq:mu};
        \State $P_{\boldsymbol{\theta}} \leftarrow P_{\boldsymbol{\theta}} + P_i$;
    \EndFor
    \For{$j \in \mathcal{I}$}
        \State Compute penalty term $P_j$ using \eqref{eq:Pj};
        \State Compute self-adaptive penalty parameter $\mu_j$ using \eqref{eq:mu};
        \State $P_{\boldsymbol{\theta}} \leftarrow P_{\boldsymbol{\theta}} + P_j$;
    \EndFor

    \If{$P_{\boldsymbol{\theta}} \leq tol$}
        \State $\phi \leftarrow F_{\boldsymbol{\theta}}$;
    \Else
        \State $\phi \leftarrow F_{\boldsymbol{\theta}} + \dfrac{1}{\#\{\varepsilon\}} \sum_{i \in \varepsilon} \mu_i P_i + \dfrac{1}{\#\{\mathcal{I}\}} \sum_{j \in \mathcal{I}} \mu_j P_j$;
    \EndIf 
    \If{$\phi_{\theta} < \phi_{\text{best}}$}
        \State $\boldsymbol{\theta}_{\text{best}} \leftarrow \boldsymbol{\theta}$;
        \State $\phi_{\text{best}} \leftarrow \phi_{\boldsymbol{\theta}}$;
    \EndIf

    
    \State $\nabla \phi_{\text{best}} \leftarrow Optimiser.BackpropCall(\phi_{\text{best}})$;
    \State $\boldsymbol{\theta} \leftarrow Optimiser.Step(\nabla \phi_{\text{best}}, \boldsymbol{\theta}_{\text{best}})$;
\EndFor

\State \textbf{return} $\boldsymbol{\theta}_{\text{best}}$;

\end{algorithmic}
\end{algorithm*}


\section{Numerical Experiments} \label{sec:experiments}

To assess the effectiveness of using the self-adaptive penalty algorithm for Neural ODEs, three synthetic datasets describing natural systems with prior knowledge constraints were used. Namely, the World Population Growth (WPG) \cite{coelho_population_2023}, Chemical Reaction (CR) \cite{coelho_chemical_2023}, and Damped Harmonic Oscillator (DHO) \cite{coelho_oscillator_2023}. Three distinct experiments were carried out to test and validate the models trained using the proposed approach: 

\begin{itemize}
    \item \textbf{reconstruction:} where the same dataset was used for both training and testing. The objective was to assess the model's ability to accurately reproduce the input patterns. By using the same dataset for testing, it allowed for a direct comparison between the original data and the reconstructed data generated

    \item \textbf{extrapolation:} where the model was tested on a larger time interval. At this experiment the trained models were put to the test by extrapolating their predictions to a larger time interval than what was encountered during the training phase. This task aimed to assess the models' generalisation capabilities beyond the temporal scope of the original dataset. By evaluating the accuracy and stability of predictions in this extended time horizon, we could determine the models' robustness in forecasting data beyond their training time interval.

    \item \textbf{completion:} where the test set had a different sampling frequency in the same time interval. This experiment closely resembles real-world scenarios where data is irregularly or sparsely sampled. By assessing the models' ability to handle varying data densities and still provide accurate predictions, we could ascertain their adaptability to practical data collection settings.
\end{itemize}


As a form of comparison, we consider a \emph{vanilla} Neural ODE (a traditional Neural ODE without incorporating constraints) and a Neural ODE with a quadratic penalty function, using three different $\mu$ values ($\mu=1, 10, 100$) to demonstrate the influence of $\mu$ on the model's performance, which serve as our baselines.
The assessment of the models was based on the Mean Squared Error (MSE) and mean total constraints violation $P$.

\textbf{Remark:} The selected datasets were chosen because they contained valuable information regarding the constraints of the system, which is typically not readily available.

Tables \ref{tab:pop}-\ref{tab:visc} show the average values of MSE and $P$, and their respective standard deviation (std), after three independent runs, for self-adaptive penalty algorithm for Neural ODEs and two baselines, \emph{vanilla} Neural ODE and a Neural ODE with quadratic penalty function. The best MSE and $P$ values at each task are written in bold.
Figures \ref{fig:pop}-\ref{fig:visc} show the predicted and real curves for the best run at each task.

In the experiments, the objective function $l$ is defined by,

$$l(\boldsymbol{\theta}) = \sum_{n=1}^N \left(\boldsymbol{\hat{y}}_{n} - \boldsymbol{y}_{n} \right)^2.$$

\subsection{World Population Growth}

The WPG dataset is publicly available on \emph{Kaggle} \cite{coelho_population_2023}. It is a time series composed of two features (time step $t$, population $p(t)$) which describes a system with an inequality constraint defined by the carrying capacity at each time step $t$. Ideally, all data points of the adjusted model must satisfy this constraint,

$$\{p(t) \leq 12\}_{t=t_1,\dots,t_N}.$$

The trained NNs have $4$ hidden layers: linear with $50$ neurons; hyperbolic tangent (tanh); linear with $50$ neurons; Exponential Linear Unit (ELU). The input and output layers have $1$ neuron. The Adam optimiser was used with a learning rate of $1e-5$. Training was done for $10000$ iterations ($k_{\max}$).
The reconstruction task uses the same set of $200$ data points in the time interval $[0,300]$ for both training and testing. In the extrapolation task, the training set comprises $200$ data points in the time interval $[0,300]$, while the testing set consists of $200$ data points in the interval $[0,400]$. The completion task involves $200$ training points and $300$ testing points in the interval $[0,300]$.

From Table \ref{tab:pop}, as expected, the vanilla Neural ODEs show higher $P$ and MSE values than the quadratic penalty ($\mu=1$) and self-adaptive versions. Hence, incorporating constraints into Neural ODEs not only ensures they are satisfied, but also contribute to better data fitting.

In general, in Neural ODEs with a quadratic penalty function, a larger $\mu$ results in models with smaller constraints violation value $P$ but with a higher MSE. Inversely, a smaller value of $\mu$ produces models with lower MSE values but a higher $P$ value. 

Neural ODEs with self-adaptive penalty algorithm exhibit the best performance in all experiments, with the smallest MSE and $P$ values. Thus, effectively balancing the minimisation of the objective function (fit to the training data) and the constraints violation.

% Figure \ref{fig:pop} (Appendix \ref{app:figures}) shows the predicted and real curves of the testing set for the best run at each task. As can be seen, the \emph{vanilla} Neural ODE (top row) was unable to extract the inequality constraint from the training data, as evidenced by the exponential increase of the population value over time. On the other hand, the Neural ODEs with quadratic penalty function (middle row) and with the self-adaptive penalty algorithm (bottom row, bold) do not exhibit this behaviour. Moreover, the predicted curve of the Neural ODE with a self-adaptive penalty algorithm closely fits the real curve, demonstrating its superiority.

Figure \ref{fig:pop} displays the predicted and real curves of the testing set for each task's best run. The top row shows the results for the \emph{vanilla} Neural ODE, which was unable to capture the inequality constraint from the training data. This is evident from the exponential increase in population values over time. In contrast, the middle row shows the results for Neural ODEs with a quadratic penalty function, which do not exhibit this behaviour. The bottom row, displays the results for the Neural ODE with a self-adaptive penalty algorithm, which closely fits the real curve and demonstrates its superiority.

\begin{table*}[!]
\Large
\begin{center}
\caption{Performance on WPG dataset of \emph{vanilla} Neural ODE, Neural ODE with a quadratic penalty function ($\mu=1, 10, 100$), and self-adaptive penalty algorithm for Neural ODE.}
\label{tab:pop}
\adjustbox{max width=\textwidth}{% <- Use adjustbox to resize the table
\begin{tabular}{lccccclclcc}
\hline
\rule{0pt}{12pt}               & \multicolumn{2}{c}{\multirow{2}{*}{\emph{Vanilla} Neural ODE}}     & \multicolumn{6}{c}{Quadratic Penalty Function}                                                                                                                               & \multicolumn{2}{c}{\multirow{2}{*}{Self-adaptive Penalty Algorithm}}  \\
                               & \multicolumn{2}{c}{}                                & \multicolumn{2}{c}{$\mu=1$}                         & \multicolumn{2}{c}{$\mu=10$}                     & \multicolumn{2}{c}{$\mu=100$}                    & \multicolumn{2}{c}{}                                \\ \cline{2-11} 
\rule{0pt}{12pt}Experiment     & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$    & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$    & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$    \\ \hline
\rule{0pt}{12pt}Reconstruction & 1.9e-1 $\pm$ 1.5e-1        & 7.8e-2 $\pm$ 5.1e-2 & 4.7e-2 $\pm$ 5e-2       & 9.1e-3 $\pm$ 1.5e-2 &     4.6e-2 $\pm$ 4e-2                        &   1.6e-4 $\pm$ 5e-05                 &       4.1e-2 $\pm$ 1.9e-2                      &      \textbf{2e-7 $\pm$ 1e-7}              & \textbf{1.3e-3 $\pm$ 4.3e-4}       & 6.8e-4 $\pm$ 2.3e-4 \\
Extrapolation                  & 1.3e-1 $\pm$ 7.2e-2       & 1.5e-1 $\pm$ 5.3e-2 & 2.6e-2 $\pm$ 1.9e-2       & 3.2e-2 $\pm$ 1.5e-2  &    1.4e-1 $\pm$ 1.3e-1                         &    4.5e-2 $\pm$ 4.e-2                &     8.8e-2 $\pm$ 4.3e-2                        &   2.5e-2 $\pm$ 7.3e-3                 & \textbf{1.4e-3 $\pm$ 1.8e-3}       & \textbf{1e-2 $\pm$ 3.9e-4} \\
Completion                     & 4.3e-2 $\pm$ 2.9e-3       & 2.8e-2 $\pm$ 1.2e-2 & 2.6e-2 $\pm$ 1.8e-2       & 2.8e-3 $\pm$ 2.5e-3 &    3.1e-2 $\pm$ 1e-2                         &      6e-5 $\pm$ 7e-5              &     2.5e-1 $\pm$ 1e-1                        &   \textbf{2e-5 $\pm$ 3e-5}                 &  \textbf{8.9e-4 $\pm$ 5e-5}         & 4.4e-4 $\pm$ 5e-5   \\ \hline
\end{tabular}
}
\end{center}
\end{table*}


% Figure environment removed





\subsection{Chemical Reaction}

The CR dataset is publicly available on \emph{Kaggle}, which describes the evolution of the species in a chemical reaction \cite{coelho_chemical_2023}. It is a time-series composed of five features (time step $t$, and the masses of species A, $m_A$, B, $m_B$, C, $m_C$, and D, $m_D$). At each time step $t$ ($t=t_1,\dots,t_N$), this system has an equality constraint defined by the conservation of mass: 

$$\{m_A(t) + m_B(t) + m_C(t) + m_D(t) = m_{\text{total}}\}_{t=t_1,\dots,t_N},$$

\noindent with $m_{\text{total}}$ the total constant mass of the system. Ideally, all data points of the adjusted model must satisfy this constraint,

The NNs were trained for $10000$ iterations ($k_{\max}$) and have $6$ hidden layers: linear with $50$ neurons; tanh; linear with $64$ neurons; ELU; linear with $50$ neurons; tanh. The input and output layers have $4$ neurons. The Adam optimiser was used with a learning rate of $1e-5$.
For the reconstruction task, both the training and testing sets consist of $100$ data points in the time interval $[0,100]$. The extrapolation task involves training with $100$ data points in the time interval $[0,100]$ and testing with $100$ data points in the interval $[0,200]$. The completion task uses $100$ training points and $200$ testing points in the interval $[0,100]$.

% From Table \ref{tab:react}, the \emph{vanilla} Neural ODE shows similar performance to Neural ODE with a quadratic penalty function with $\mu=1$. As we can be seen, increasing the $\mu$ value, to $10$ and $100$, results in a slightly increase of the MSE values but an improvement in the constraints violation, with the exception for the extrapolation task for $\mu=100$. 

Table \ref{tab:react} indicates that the performance of the \emph{vanilla} Neural ODE is comparable to that of Neural ODE with a quadratic penalty function and $\mu=1$. Increasing $\mu$ to $10$ and $100$ slightly raises MSE values but also improves constraint violation, except for the extrapolation task with $\mu=100$.

% The CR dataset was best modelled by the Neural ODE with the self-adaptive penalty algorithm, showing very low values of both MSE and violation of the constraints, for the three tasks. In general, these results show that the equality constraint is effectively incorporated while also achieving the best fit to the data. This is also corroborated by comparing the plots in Figure \ref{fig:react} (Appendix \ref{app:figures}). As it can be seen, for the harder extrapolation task (since it involves predicting data from a previous unseen time horizon) the Neural ODE with a self-adaptive penalty algorithm \ref{fig:react}-(h) effectively models the data dynamics while the other two \ref{fig:react}-(b) and \ref{fig:react}-(e) deviate from the true dynamics for a time horizon larger than the one seen during training ($100s$).

The Neural ODE with self-adaptive penalty algorithm was found to be the best model for the CR dataset, with very low values of both mean squared error (MSE) and constraint violation across all three tasks. These results suggest that the equality constraint is effectively incorporated into the model while also providing the best fit to the data. This is supported by the comparison of plots in Figure \ref{fig:react}, which demonstrate that the Neural ODE with self-adaptive penalty algorithm \ref{fig:react}-(h) performs well even for the more challenging extrapolation task, accurately modelling the dynamics of the data. In contrast, the other two models \ref{fig:react}-(b) and \ref{fig:react}-(e) deviate from the true dynamics for a time horizon beyond that seen during training ($100s$).

Figure \ref{fig:react} shows the difficulty of \emph{vanilla} Neural ODE (top row) to fit the real curves, achieving a better result for the completion task. The plots of the predicted curves of Neural ODE with a self-adaptive penalty algorithm (bottom row) show the best modelling performance for the three tasks.




\begin{table*}[!]
\Large
\begin{center}
\caption{Performance on CR dataset of \emph{vanilla} Neural ODE, Neural ODE with a quadratic penalty function ($\mu=1, 10, 100$), and self-adaptive penalty algorithm for Neural ODE.}
\label{tab:react}
\adjustbox{max width=\textwidth}{% <- Use adjustbox to resize the table
\begin{tabular}{lccccclclcc}
\hline
\rule{0pt}{12pt}               & \multicolumn{2}{c}{\multirow{2}{*}{Vanilla Neural ODE}}     & \multicolumn{6}{c}{Quadratic Penalty Function}                                                                                                                               & \multicolumn{2}{c}{\multirow{2}{*}{Self-adaptive Penalty Algorithm}}   \\
                               & \multicolumn{2}{c}{}                                & \multicolumn{2}{c}{$\mu=1$}                         & \multicolumn{2}{c}{$\mu=10$}                     & \multicolumn{2}{c}{$\mu=100$}                    & \multicolumn{2}{c}{}                                 \\ \cline{2-11} 
\rule{0pt}{12pt}Experiment     & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$    & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$    & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$     \\ \hline
\rule{0pt}{12pt}Reconstruction & 2.1e-3 $\pm$ 1.4e-3       & 7.8e-3 $\pm$ 6.7e-3 & 1.2e-2 $\pm$ 2.1e-2       & 5.3e-3 $\pm$ 1.2e-2 &   2.9e-2 $\pm$ 3.5e-2                          &  3.1e-4 $\pm$ 4e-4                  &           5.7e-2 $\pm$ 3.8e-2                  &   8e-5 $\pm$ 1.1e-4                 & \textbf{2e-4 $\pm$ 2e-4}         & \textbf{2.2e-6 $\pm$ 1.4e-6} \\
Extrapolation                  & 4.1e-2 $\pm$ 5.7e-2        & 1.6e-2 $\pm$ 1.6e-2 & 2.3e-1 $\pm$ 2.7e-1       & 2e-1 $\pm$ 2.6e-1 &   2.1e-3 $\pm$ 1.8e-3                          &   1.8e-3 $\pm$ 1.6e-3                 &         7.9e-1 $\pm$ 1.1                    &   6.7 $\pm$ 9.3                 &          \textbf{8e-5 $\pm$ 2e-5}                   &     \textbf{2e-5 $\pm$ 2e-5}                   \\
Completion                     & \textbf{9e-4 $\pm$ 5.9e-4}        & 2.6e-3 $\pm$ 3.1e-3 & 1.7e-2 $\pm$ 2.3e-2       & 1.e-3 $\pm$ 1.6e-3 &   2.5e-2 $\pm$ 3.5e-2                          &  3e-4 $\pm$ 4.2e-4                    &       1.2e-1 $\pm$ 9.7e-2                      &  2.9e-4 $\pm$ 3.8e-4                  & 1e-3 $\pm$ 1.4e-3       & \textbf{1.4e-4 $\pm$ 2e-4}   \\ \hline
\end{tabular}
}
\end{center}
\end{table*}

% Figure environment removed




\subsection{Damped Harmonic Oscillator}

% The DHO dataset is publicly available on \emph{Kaggle} and describes the motion of a mass connected to a spring subjected to a dissipative force given by a second-order ODE \cite{coelho_oscillator_2023}.
% It is a time series composed of four features (time step $t$, displacement $x$, velocity $v$, and acceleration $a$). 
% This is a system with one inequality constraint and one equality constraint. The inequality constraint is given by the decreasing of the system's energy due to damping: $E(t) > E(t+1)$ with $E(t) = \dfrac{1}{2} m v^2 + \dfrac{1}{2} k x^2$ and $k$ the spring constant. The equality constraint is given by the conservation of the rate at which energy is dissipated due to damping: $\Delta P(t) = 0$ with $P(t) = -c v x$ and $c$ the damping coefficient. Ideally, all data points of the adjusted model must satisfy these constraints.

The DHO dataset, available on \emph{Kaggle}, describes the motion of a mass attached to a spring under the influence of a dissipative force described by a second-order ODE \cite{coelho_oscillator_2023}. The dataset consists of a time series with four features: time step $t$, displacement $x$, velocity $v$, and acceleration $a$. 
At each time step $t$, the system has one inequality constraint and one equality constraint. The inequality constraint requires the system's energy to decrease due to damping: 

$$\{E(t) > E(t+1)\}_{t=t_1,\dots,t_N},$$

\noindent where $E(t) = \dfrac{1}{2} m\ v(t)^2 + \dfrac{1}{2} k x(t)^2$, and $k$ is the spring constant. 

The equality constraint requires the conservation of the rate at which energy is dissipated due to damping: 

$$\{\Delta P(t) = 0\}_{t=t_1,t_N},$$

\noindent where $P(t) = -c v(t) x(t)$, and $c$ is the damping coefficient. To obtain meaningful results, all data points of the adjusted model should satisfy these constraints.

% The DHO dataset stands out from the other datasets since it describes a system with two constraints. As seen from Figure \ref{fig:visc} (Appendix \ref{app:figures}), this system has proven to be a challenge to model with both \emph{vanilla} Neural ODE and Neural ODE with a quadratic penalty function, being the predicted curves far from the real curves in all three runs. On the other hand, the Neural ODE with the self-adaptive penalty algorithm was able to adjust the dynamics of the data accurately. Take special attention to the extrapolation task, Figures \ref{fig:visc}-(b)(e)(h), in which it can be easily verified that the constraints are effectively incorporated into the model. For instance when predicting for a longer time horizon, not seen during training, the model correctly captures the physical dynamics, which eventually comes to rest as time elapses. This behaviour was not observed in either \emph{vanilla} Neural ODE or Neural ODE with a quadratic penalty function. 

The DHO dataset is unique among the datasets used in this study because it involves a system with two constraints. As shown in Figure \ref{fig:visc}, both the \emph{vanilla} Neural ODE and the Neural ODE with a quadratic penalty function struggled to model this system, with predicted curves significantly deviating from the actual curves in all three runs. In contrast, the Neural ODE with the self-adaptive penalty algorithm accurately adjusted to the data dynamics, particularly in the extrapolation task (Figures \ref{fig:visc}-(b)(e)(h)). The self-adaptive penalty algorithm effectively incorporates the constraints into the model, as demonstrated by the model's ability to capture the physical dynamics of the system even when predicting for longer time horizons not seen during training. This behaviour was not observed with either the \emph{vanilla} Neural ODE or the Neural ODE with a quadratic penalty function.

% The numerical results in Table \ref{tab:visc} further corroborate the findings shown in Figure \ref{fig:visc}. The use of a Neural ODE with a quadratic penalty function results in the worst performances, completely failing at the extrapolation task. Although when comparing with \emph{vanilla} Neural ODE, the constraints violation values are smaller for the reconstruction and completion tasks.

% The Neural ODE with the self-adaptive penalty algorithm stands out, with a significant difference in magnitude in both the MSE and the constraints violation $P$ when compared with the baselines. Furthermore, as evident from the std values, it yields the most stable performing models across all three runs.

The trained NNs have $4$ hidden layers: linear with $50$ neurons; hyperbolic tangent (tanh); linear with $50$ neurons; Exponential Linear Unit (ELU). The input and output layers have $2$ neurons. The Adam optimiser was used with a learning rate of $1e-5$ and the optimisation process was done until $10000$ iterations ($k_{\max}$) were done.
In the reconstruction task, the training and testing sets contain the same $400$ data points in the time interval $[0,50]$. The extrapolation task involves training with $400$ data points in the time interval $[0,50]$ and testing with $400$ data points in the interval $[0,400]$. For the completion task, there are $400$ training points and $600$ testing points in the interval $[0,50]$.

The numerical results presented in Table \ref{tab:visc} support the findings shown in Figure \ref{fig:visc}. The use of a Neural ODE with a quadratic penalty function performed the worst, failing completely at the extrapolation task. However, when compared to a \emph{vanilla} Neural ODE, constraint violation values were lower for the reconstruction and completion tasks.

In contrast, the Neural ODE with the self-adaptive penalty algorithm performed significantly better, exhibiting a notable difference in magnitude for both MSE and constraint violation $P$ when compared to the baselines. Moreover, the standard deviation values indicate that this method produced the most stable models across all three runs.


\begin{table*}[!]
\Large
\begin{center}
\caption{Performance on DHO dataset of \emph{vanilla} Neural ODE, Neural ODE with a quadratic penalty function ($\mu=1, 10, 100$), and self-adaptive penalty algorithm for Neural ODE.}
\label{tab:visc}
\adjustbox{max width=\textwidth}{% <- Use adjustbox to resize the table
\begin{tabular}{lccccclclcc}
\hline
\rule{0pt}{12pt}               & \multicolumn{2}{c}{\multirow{2}{*}{Vanilla Neural ODE}}  & \multicolumn{6}{c}{Quadratic Penalty Function}                                                                                                                            & \multicolumn{2}{c}{\multirow{2}{*}{Self-adaptive Penalty Algorithm}} \\
                               & \multicolumn{2}{c}{}                             & \multicolumn{2}{c}{$\mu = 1$}                    & \multicolumn{2}{c}{$\mu = 10$}                   & \multicolumn{2}{c}{$\mu = 100$}                  & \multicolumn{2}{c}{}                               \\ \cline{2-11} 
\rule{0pt}{12pt}Experiment     & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$ & $P \pm \text{std}$ & $\text{MSE} \pm \text{std}$  & $P \pm \text{std}$  \\ \hline
\rule{0pt}{12pt}Reconstruction &          1.2e-1 $\pm$ 2.9e-2                   &   1.2e-3 $\pm$ 1.5e-3                 &    2.7e-1 $\pm$ 2.3e-1                         &  \textbf{5e-6 $\pm$ 7.1e-6}                  &   2.2e-1 $\pm$ 8.3e-2                          & 1.3e-4 $\pm$ 1.8e-4                   &             2.3e-1 $\pm$ 1.1e-1                & 1e-5 $\pm$ 1e-5                   & \textbf{8e-5 $\pm$ 1e-4}           & 6.4e-3 $\pm$ 3e-5 \\
Extrapolation                  &          8.5 $\pm$ 11.9                   &  13.9 $\pm$ 19.7                  &    220.5 $\pm$ 250.2                         &    78.7 $\pm$ 85.1                &       331.4 $\pm$ 442.9                      &  79.5 $\pm$ 111.2                  &        24.6 $\pm$ 34.6                     &    3.1 $\pm$ 4.4                & \textbf{1e-1 $\pm$ 2e-5}          & \textbf{8e-4 $\pm$ 1e-8}   \\
Completion                     &         1e-1 $\pm$ 9.3e-3                    &       1.4e-4 $\pm$ 6e-5             &     1.2e-1 $\pm$ 1.6e-2                        &      \textbf{1e-8 $\pm$ 1e-8}              &  15e-1 $\pm$ 7.2e-3                           &   2e-8 $\pm$ 4e-8                 &     4.6e-1 $\pm$ 1.9e-1                        &     1e-7 $\pm$ 3e-8               & \textbf{5e-5 $\pm$ 3e-5}            & 6.3e-3 $\pm$ 1e-5 \\ \hline
\end{tabular}
}
\end{center}
\end{table*}

% Figure environment removed




\section{Conclusion} \label{sec:conclusion}

% In this work, we propose a self-adaptive penalty function and a self-adaptive penalty algorithm for Neural ODEs to incorporate prior knowledge constraints when modelling natural systems.

% Our approach represents a significant improvement over quadratic penalty functions that require manual tuning of the penalty parameters $\mu$, which is problem dependent and can be a difficult and time-consuming task, not always providing optimal solutions. In contrast, our self-adaptive penalty function can dynamically adjust the penalty parameters resulting in models with more efficient and accurate predictions, particularly for complex systems with constraints.
% Furthermore, by adding a normalisation step to the objective function and constraint violation values we ensure the stability of the training process. This is particularly important when dealing with complex systems where numerical instability can be a major issue, and the normalisation technique provides a robust and effective solution to this problem.

% In order to assess the effectiveness of the self-adaptive penalty algorithm for Neural ODEs, we applied it to the modelling of three constrained natural systems and evaluated the performance on three distinct tasks: reconstruction, extrapolation, and completion. Our analysis included two baselines: a \emph{vanilla} Neural ODE and three Neural ODEs with quadratic penalty functions that used fixed penalty parameter values ($\mu=1,10,100$). The performance was measured in terms of mean squared error (MSE) and total constraints violation ($P$).
% The results demonstrate that the Neural ODE with our proposed self-adaptive penalty algorithm achieves remarkably higher performance, exhibiting lower MSE and reduced total constraints violation values. This indicates that the resulting models not only fit the data but also satisfy the governing laws of the constrained systems. Additionally, our approach shows the ability to extrapolate and generalise well.

% Our proposed self-adaptive penalty function and algorithm has the potential to be applied to any NN architecture and represents a promising approach for incorporating prior knowledge constraints into NNs while enhancing their interpretability.

% In the future, more complex real-world scenarios featuring multiple constraints will be modelled using Neural ODEs with the proposed self-adaptive penalty method. Additionally, theoretical studies on the convergence of our approach will be conducted.

This work proposes a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters


 Our approach is an improvement over traditional penalty methods which require  an appropriate initialisation and tuning of penalty parameters $\mu$ during the optimization process. This selection is challenging and time-consuming. In general, in the context of NNs, a fixed penalty parameter is used, and consequently an optimal solution is not guaranteed to be found.
 
 The proposed self-adaptive penalty function dynamically adjusts penalty parameters taking into account the degree of constraints violation, resulting in more efficient and accurate predictions, especially for complex systems with constraints. The self-adaptive penalty function employs a normalisation step to ensure all values (objective and constraints violations) have the same order of magnitude, improving training stability, critical for complex systems prone to numerical instability.

To evaluate the self-adaptive penalty algorithm for Neural ODEs, we used three constrained natural systems and tested the performance on three tasks: reconstruction, extrapolation, and completion. We compared the results obtained by our algorithm with two baselines, a \emph{vanilla} Neural ODE and three Neural ODEs with quadratic penalty functions with the penalty parameter values $\mu=1,10,100$, respectively. We measured performance in terms of mean squared error (MSE) and total constraints violation ($P$). Our algorithm achieved remarkably higher performance, exhibiting lower MSE and reduced total constraint violation values, indicating that the models not only fit data but also satisfy the governing laws of constrained systems. Additionally, our algorithm showed the ability to extrapolate and generalise well.

The proposed self-adaptive penalty function and self-adaptive penalty algorithm for Neural ODEs can be applied to any NN architecture and represents a promising approach for incorporating prior knowledge constraints into NNs while enhancing interpretability. Future work will focus on modelling more complex scenarios with more complex constraints using Neural ODEs with our self-adaptive penalty method, as well as conducting theoretical studies on the convergence of our approach.




\ack The authors acknowledge the funding by Fundação para a Ciência e Tecnologia (FCT) (Portuguese Foundation for Science
and Technology) through CMAT projects UIDB/00013/2020 and UIDP/00013/2020.
C. Coelho would like to thank FCT for the funding through the scholarship with reference 2021.05201.BD and the computing facilities provided by Project Search-ON2: Revitalization of HPC infrastructure of UMinho” (NORTE-07-0162-FEDER-000086), co-funded by the North Portugal Regional Operational Programme (ON.2 – O Novo Norte), under the National Strategic Reference Framework (NSRF), through the European Regional Development Fund (ERDF). 

\newpage

\bibliography{ecai.bib}

%\appendix

%\section{Experimental Conditions} \label{app:experiments}
%
%\paragraph{Population growth.} The trained NNs have $4$ hidden layers: linear with $50$ neurons; hyperbolic tangent (tanh); linear with $50$ neurons; Exponential Linear Unit (ELU). The input and output layers have $1$ neuron. The Adam optimiser was used with a learning rate of $1e-5$. Training was done for $10000$ iterations.
%The reconstruction task uses the same set of $200$ data points in the time interval $[0,300]$ for both training and testing. In the extrapolation task, the training set comprises $200$ data points in the time interval $[0,300]$, while the testing set consists of $200$ data points in the interval $[0,400]$. The completion task involves $200$ training points and $300$ testing points in the interval $[0,300]$.
%
%\paragraph{Chemical reaction.} The NNs were trained for $10000$ iterations and have $6$ hidden layers: linear with $50$ neurons; tanh; linear with $64$ neurons; ELU; linear with $50$ neurons; tanh. The input and output layers have $4$ neurons. The Adam optimiser was used with a learning rate of $1e-5$.
%For the reconstruction task, both the training and testing sets consist of $100$ data points in the time interval $[0,100]$. The extrapolation task involves training with $100$ data points in the time interval $[0,100]$ and testing with $100$ data points in the interval $[0,200]$. The completion task uses $100$ training points and $200$ testing points in the interval $[0,100]$.
%
%\paragraph{Damped harmonic oscillator.} The trained NNs have $4$ hidden layers: linear with $50$ neurons; hyperbolic tangent (tanh); linear with $50$ neurons; Exponential Linear Unit (ELU). The input and output layers have $2$ neurons. The Adam optimiser was used with a learning rate of $1e-5$ and the optimisation process was done until $10000$ iterations were done.
%In the reconstruction task, the training and testing sets contain the same $400$ data points in the time interval $[0,50]$. The extrapolation task involves training with $400$ data points in the time interval $[0,50]$ and testing with $400$ data points in the interval $[0,400]$. For the completion task, there are $400$ training points and $600$ testing points in the interval $[0,50]$.

%\section{Figures from Experiments} \label{app:figures}
%
%Figures \ref{fig:pop}-\ref{fig:visc} show the predicted (solid lines) and real (dashed lines) curves for the best run at the tasks of reconstruction (first column), extrapolation (second column) and completion (third column), for the WPG, CR and DHO datasets. The top row of each Figure depicts \emph{vanilla} Neural ODE, middle row the Neural ODE with a quadratic penalty function and $\mu=1$ and the bottom row, the Neural ODE with the self-adaptive penalty algorithm.

%% Figure environment removed
%
%% Figure environment removed
%
%% Figure environment removed


%\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#AUTHOR RESPONSE\#\#\#\#\#\#\#\#\#\#\#\#\#\#
%
%Dear reviewers, we appreciate the valuable comments and insights you gave us.
%
%\#REVIEWER \#2
%
%1-We selected three real-life systems from different fields to demonstrate the versatility of our approach to deal with different systems from various fields (biology, chemistry and physics) and with several types of constraints. These systems reflect the increasing complexity of the datasets. The population growth system has an inequality constraint per time step. The chemical reaction system, as observed from the plots, has higher dimensionality compared to population growth and must satisfy an equality constraint per time step (mass conservation). The damped harmonic oscillator serves as a test case with equality and inequality constraints per time step, representing a more challenging dynamics for the neural network to learn.
%
%2-For the population and chemical systems, the number of constraints is equal to the number of time-steps. In the oscillator dataset the number of constraints is twice the number of time-steps. 
%In respect to the "hardness regarding their satisfaction", we considered a tolerance of 1e-4 for feasibility (see Algorithm 1).
%
%3-The answers to this question is the same as the one provided in 2.
%
%4-In Appendix A, the value of $k_{max}$ is specified as 10000 for each dataset. To ensure clarity, we could have used the notation $k_{max}$ in that section.
%
%5-Actually we are currently working on that. The submitted paper represents an initial approach to the proposed method, and we are actively addressing its limitations and exploring additional avenues for improvement.
%
%\#REVIEWER \#3
%
%1. Please explain the experimental setting of the ODE learning in more detail. Is the setting where the ODE's are known and you want to learn a simpler ODE with enforced physics? Or is it a situation where the ODEs are partially know and you want to learn the full ODE, enforcing the partially known properties of the ODE? Or a situation where the ODEs are well known and you want to see if the method can recover essentially the same ODE? Or something else?
%
%
%1-Given a dataset with unknown dynamics, Neural ODEs are used to build an ODE dynamics ($f_theta$).
%The experimental setting of our study involves working with fully unknown ODE dynamics.
%Additionally for the systems analysed its governing laws that must be satisfied (the algebraic equations/inequations) are known. By using our proposed method, the Neural ODE identifies an ODE that not only captures the underlying patterns in the training data but also adheres to the imposed constraints.
%Due to the limitations of a 7-page limit, we only present a very brief introduction of Neural ODEs but a detailed description can be found in reference [3]. The main contribution of our work is to incorporate the constraints into the Neural ODEs architecture.
%
%\#REVIEWER \#4
%Q:"The only minor variation introduced here is the consideration of adaptive penalties."
%A- In this paper, we introduce a novel adaptive penalty function that provides 3 main contributions: the definition of the adaptive penalty function, a normalization function to be applied to the objective function and the constraint violation values to ensure that all values have the same order of magnitude and a strategy to dynamically compute the penalty parameters.
%
%
%Q:"However, a cursory search reveals that there is a significant body of literature on adaptive penalty approaches for neural network training, which the authors have failed to acknowledge in their paper."
%A- We note that the main goal of this work is to propose an adaptive penalty function for Neural ODEs. To the best of our knowledge there is no contribution of adaptive penalties in this field. 
%
%
%Q:"There is a lack of discussion of convergence of the algorithm. Is it guaranteed to converge? where does it converge, i.e.., what are the fixed points?"
%A-As it can be seen by the results presented in the Tables, the algorithm produces feasible solutions and these solutions are approximately optimal as can be seen by seen by the lower MSE values.
%
%Q:"The paper violates the page limit...The paper is 9 pages + 1 page references and there are no supplementary material."
%A-We note that the guidelines state that the paper cannot exceed 7 pages, excluding references and supplementary material. Our paper has 6 pages and the references start in the middle of the 6th page. Then the next 4 pages are supplementary material, namely, "A Experimental Conditions" and "B Figures from Experiments".
%
%Q:"The algorithm is not properly described.The algorithm is discussed in section 4. But section 4 is 4 sentence, where each its own paragraph." 
%A-We note that Section 3 describes the process of the adaptive penalty function in detail and the Neural ODE algorithm is described in detail in reference [3]. So, in Section 4 we present our algorithm and provide a description to select the best parameters during the iterative process. 
%
%Q:"Essential, the considered problem is a constrained optimization. This paper considers an old idea, which is penalty methods. However, the typical adaptive approaches to penalties are often based on primal-dual methods or a combination of primal-dual and penalty techniques, such as ADMM. The algorithm employed in this paper seems to lack a comprehensive understanding of the existing literature on constrained optimization and how it relates to the chosen approach, which raises concerns."
%
%A-Due to the page-limit in this conference we could not do an extensive review on penalty methods. ADMM can only be applied in the context of convex problems so it cannot be applied to non-convex ones. As it can be seen in the literature penalty methods are the most used in context of AI and are still a favourite technique, for instance references [8,12,13].
%
%
%\#META-REVIEWER
%Dear Meta Reviewer and Area Chair,
%
%We are writing to express our concerns regarding the review provided by reviewer #4. While we appreciate the feedback provided by all reviewers, we believe that the comments from reviewer #4 exhibit low knowledge in the domain of our paper and lack constructive criticism. We respectfully request that you consider these concerns in the evaluation of our manuscript.
%
%The reviewer claims that our paper fails to acknowledge the extensive exploration of the area in recent studies related to penalty techniques, but do not provide references to support his/her claims. We note that the main goal of our work is to propose an adaptive penalty function for Neural ODEs. To the best of our knowledge there is no contribution of adaptive penalties in this field.
%
%Reviewer #4 asserts their confidence and knowledge (points 15 and 16) in their evaluation, yet they mention ADMM as a means to criticize our paper. However, ADMM is only suitable for convex problems not being possible to be applied to non-convex ones directly. Furthermore there exists other methods in the literature as competitive as ADMM.
%
%We kindly request that you address our concerns regarding the quality of the review by reviewer #4. Their lack of references to support the claims, dismissal of our proposed method to extend Neural ODEs to model constrained natural systems, and the inappropriate suggestion of ADMM in our work undermine the validity of their critique.
%
%In addition to our concerns regarding reviewer #4's evaluation, we would like to draw attention to the contrasting perspectives provided by reviewers #2 and #3. Both reviewers acknowledge the relevance and significance of our contribution in the field of AI and machine learning.
%
%Thank you for your attention to this matter.


\end{document}



