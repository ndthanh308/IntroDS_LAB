
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed


\section{Dataset}\label{sec:dataset}


To evaluate the proposed heterogeneous multi-agent collaboration model, we create the novel dataset for the heterogeneous multi-agent tidying-up tasks based on the ProcTHOR-10K \cite{procthor} which provides a large number of houses with multiple rooms. In each task, several objects are placed in unreasonable locations, and multiple heterogeneous agents are required to find these misplaced objects and put them to the reasonable locations. The newly built dataset contains the \textit{Single-Room} (\textit{Single}) and \textit{Cross-Room} (\textit{Cross}) tasks to train and evaluate the proposed model.

%Since we focus on heterogeneous agents collaboration, we selects 120 scenes (the number is the same as AI2-THOR) from ProcTHOR-10k to train and test the proposed model.


% Figure environment removed

%We consider three heterogeneous agents in our task. They have the same visual perception ability but have different action abilities and morphological characteristics. Specifically, $A^{(1)}$ has only the navigation ability with the low height, that is, $Nav^{(1)}=1,Mani^{(1)}=0,Hei^{(1)}=0$. $A^{(2)}$ only has the navigation ability and its height is high, that is, $Nav^{(2)}=1,Mani^{(2)}=0,Hei^{(2)}=1$. $A^{(3)}$ has both the navigation and manipulation abilities and its height is high, that is, $Nav^{(3)}=1,Mani^{(3)}=1,Hei^{(3)}=1$. These three agents can solve different situations of the tidying-up task. $A^{(3)}$ can pick up objects which are pickupable from the correct interaction locations. It is easier for $A^{(1)}$ with the lower field of view to find the objects thrown on the floor, and it can share its detected information to $A^{(3)}$ to help it pick up thrown objects. $A^{(2)}$ with the higher field of view can observe some areas that $A^{(1)}$ cannot, and its perspective can complement that of $A^{(1)}$ to assisting $A^{(3)}$ complete the tidying-up task.

% Figure environment removed



ProcTHOR-10K is a generated dataset of houses with multiple rooms \cite{procthor} built based on AI2-THOR \cite{ai2thor}, including 10,000 training houses, 1000 validation houses, and 1000 testing houses, and each room belongs to one type of \textit{Kitchen}, \textit{LivingRoom}, \textit{Bedroom} and \textit{Bathroom}. We propose a method to generate multi-agent tidying-up tasks in ProcTHOR-10K. Since every single room in ProcTHOR-10K is built based on AI2-THOR, we regard the object placement relationships in AI2-THOR as the reasonable placement constraints in ProcTHOR-10K. We obtain the properties of objects and their reasonable locations from ProcTHOR-10K. Specifically, we obtain the objects which have the properties \textit{Pickupable} and \textit{Receptacle} to form the set $\mathcal{O}_{pick}$ and $\mathcal{O}_{recep}$ respectively. For each object type in $\mathcal{O}_{pick}$, we generate the triples of reasonable candidate locations $Re_i = \{(o_i, p_j, r_j)\}$ from the metadata, where $o_i \in \mathcal{O}_{pick}$, $p_j \in \mathcal{O}_{recep}$, $r_j \in \{Kitchen, LivingRoom, Bedroom, Bathroom\}$, $o_i$ can be put on $p_j$ in room $r_j$. We generate 10 meta-tasks for each house in ProcTHOR-10K, and the process is shown in Fig.\ref{fig:process}. For each tidying-up meta-task, we randomly select $k$ objects to change their current location $(o_i, p_i^{cur}, r_i^{cur})$ to an unreasonable location $(o_i, p_i^{new}, r_i^{new})$, and $k$ is randomly chosen from $\{1,2,3,4,5\}$. We use the action $PutObject$ in ProcTHOR-10K to place the misplaced object to an unreasonable receptacle or use the action $DropHandObject$, $ThrowObject$ to drop the object to a random location. For each misplaced object, $p_i^{new}$ and $r_i^{cur}$ may be the same or different. If $r_i^{new} = r_i^{cur}$, the misplaced object may be placed in an unreasonable location in the same room. If $r_i^{new} \neq r_i^{cur}$, the misplaced object may be placed to the location in another unreasonable room. %There are a total of 100,000 training meta-tasks, 10,000 validation meta-tasks, and 10,000 testing meta-tasks. 
All meta-tasks are divided into \textit{Single} and \textit{Cross} tasks. In \textit{Single} tasks, all $k$ misplaced objects are placed in the same room as their initial positions, and in \textit{Cross} tasks, at least one misplaced object is placed in a different room from its initial position. The statistics of the dataset are shown in Fig. \ref{fig:statics}. The proportion of the selected number of misplaced objects in the generated dataset is shown in Fig. \ref{fig2a}, and Fig. \ref{fig2b} illustrated the proportion of \textit{Single-Room} tasks and \textit{Cross-Room} tasks. 


For each meta-task, we randomly generate 5 initial positions for multiple heterogeneous agents. We also generate the expert demonstration with heuristic shortest path methods for imitation learning. Specifically, we obtain the metadata and scene information of each task, and collect labels for the sub-task and sub-goal predictors of the action strategy module. We generate oracle sub-task lists for each agent from task information and find viewpoints that agents can interact with misplaced objects as target locations for manipulation. We generate the sub-task allocation results for each agent at each step according to the distance between each agent and misplaced objects. Then, we generate the heuristic shortest path in the sub-goal level, generate 5 reasonable trajectories per training task, and obtain pairs of state features and sub-goal actions ($\Delta x_t, \Delta y_t, \Delta rot_t, ope_t, stop_t$) from them, which are mentioned in Section \ref{sec:method}. Then we use sub-goal actions as labels for imitation learning.


%Since every single room in ProcTHOR-10k is built based on AI2-THOR, the object placement relationships between objects, receptacles, and room types are extracted from AI2-THOR. The examples of object placement relationships are demonstrated in Table \ref{table_obj_relation}, which lists the reasonable receptacles and room types for corresponding objects. 


We notice that our dataset is different from that in \cite{sarch2022tidee}. The tasks in our dataset are more difficult than \cite{sarch2022tidee}, and our tasks require deeper scene understanding and reasoning. Firstly, the tidying-up tasks in our dataset are built on large houses with multi-rooms and misplaced objects are likely to be placed in another unreasonable room, while the dataset in \cite{sarch2022tidee} are only built in the single room in AI2-THOR. Secondly, we consider the relationship among pickupable objects, receptacles and rooms, which means that the misplaced objects may be placed in an unreasonable receptacle, or in a reasonable receptacle but an unreasonable room. But in \cite{sarch2022tidee}, misplaced objects are generated only by being thrown away on the ground in the same room, and it does not need to consider the relationship between objects and rooms. It does not consider the relationship between objects and receptacles either. Besides, our dataset is used to solve the multi-agent tasks and each task has initial positions for multiple agents, while the dataset in \cite{sarch2022tidee} is designed for the single-agent task. Furthermore, the major differences between our dataset and \textit{Housekeep} in \cite{kant2022housekeep} are that our dataset has a larger scale, can be automatically generated through the proposed program, and is designed for heterogeneous multi-agent collaboration. We build a benchmark dataset based on ProcTHOR-10K which contains 10K houses with multiple rooms, while dataset in \cite{kant2022housekeep} only has 14 scenes. We also provide the program to generate task data and our dataset can be automatically generated through the program, while the generation of \textit{Housekeep} needs a large number of efforts from human annotations. Our program can also be easily extended to any other indoor simulation platform with a large number of houses to generate the corresponding task data. Meanwhile, our dataset is designed for heterogeneous multi-agent task, while \textit{Housekeep} is designed for the single-agent task. Misplaced objects in our dataset are likely to be placed in the receptacle or thrown on the floor, which are easier to be observed by agents with high or low height respectively, making it more suitable for the study of heterogeneous multi-agent collaboration.



%Specifically, to train and verify the proposed heterogeneous collaboration strategy, we choose 120 houses from ProcTHOR-10K, the number is the same as AI2-THOR which is widely used for embodied tasks, while the dataset in \cite{kant2022housekeep} only has 14 scenes. We also provide the program to generate task data and our dataset can be automatically generated through the program, while the generation of \textit{Housekeep} needs a large number of efforts from human annotations. Our program can be directly applied to full ProcTHOR-10K containing 10K houses for larger-scale experiments, but \textit{Housekeep} is built on iGibson with only 15 scenes and cannot be extended to more scenes. Meanwhile, misplaced objects in our dataset are likely to be placed in the receptacle or thrown on the floor, which are easier to be observed by agents with high or low height respectively, making it more suitable for the study of heterogeneous multi-agent collaboration, while \textit{Housekeep} is designed for the single-agent task.


%Meanwhile, \textit{Housekeep} is designed for the single-agent task. And misplaced objects in \textit{Housekeep} are only put in incorrect receptacles, while misplaced objects in our dataset are likely to be thrown on the floor, which are easier to be observed by agents with low height, so our dataset is more suitable for the study of heterogeneous multi-agent collaboration. 



\section{Methodology}\label{sec:method}

The proposed model that solves the multi-agent tidying-up task consists of four main modules: the misplaced object detector, the reasonable receptacle predictor, the communication module, and the hierarchical decision. The overview of this model is shown in Fig. \ref{fig:model}. The misplaced object detector judges whether there exists an object placed in an unreasonable location. The reasonable receptacle predictor generates a reasonable receptacle and room type to place the misplaced objects. The communication module transmits the communication information to other heterogeneous agents. The hierarchical decision module predicts the next sub-task, sub-goal, and next actions for each agent to execute.


\subsection{Misplaced Object Detector}

Each agent builds a top-down semantic map with the input RGB and depth using the method similar to the semantic mapping in \cite{liu2022multi}. At each sub-goal step, the agent obtains a local semantic map of size $G \times G \times (K_{total} + 2)$, where $G$ indicates the size of the local map, and $K_{total}$ is the number of object categories. Then agent $A^{(i)}$ generates the map embedding $sm_t^{(i)}$ with a pre-trained scene encoder similar to that in \cite{liuembodied} consisting of multiple convolutional layers.

%Then each agent merges its local semantic map with its existing map based on the pose to get its global semantic map of size $L \times W \times (K_{total} + 2)$, where $L \times W$ represents the size of the whole map.

%We extract the common spatial relationships among objects as well as the relationships between objects and room types from Visual Genome \cite{krishna2017visual}. 


We filter out spatial relationships among objects, receptacles and room types existing in our dataset from Visual Genome \cite{krishna2017visual} to form a graph structure. We regard these relationships as the commonsense prior knowledge. We use the Glove embedding \cite{pennington2014glove} to encode the class label, concatenate with the visual embedding as node features, and use GCN \cite{KipfW2017semi} to obtain the embedding of the commonsense $sq^{(i)}_t$. ResNet \cite{he2016deep} is utilized to extract the visual embedding $sv^{(i)}_t$ of the visual observation. Meanwhile, the pre-trained Mask-RCNN \cite{he2017mask} is used to detect the existing objects and obtain the feature $sl^{(i)}_t$. We construct a room classifier to predict the room type where the agent is currently located based on detected objects and their placement relationships. The classifier extracts the room embedding $sr_t^{(i)}$ of the current state. The misplaced object detector $F_d$, a binary classifier consisting of two linear layers, is utilized to fuse embeddings and detect whether the object is in a reasonable location. $det^{(i)}_t = F_d(sq^{(i)}_t, sv^{(i)}_t, sl^{(i)}_t, sr^{(i)}_t)$. $det^{(i)}_t=1$ denotes the object is misplaced in an unreasonable location and  $det^{(i)}_t=0$ denotes the object is in a reasonable location.

\subsection{Reasonable Receptacle Predictor}

The detection feature $sl^{(i)}_t$ and the commonsense embedding $sq^{(i)}_t$ are fed into an attention layer to obtain the object-commonsense attention $attl_t^{(i)}$. The room embedding $sr^{(i)}_t$ and $sq^{(i)}_t$ are also fed into an attention layer to generate the room-commonsense attention $attr_t^{(i)}$. The reasonable receptacle predictor $F_p$ including linear layers is built to fuse the visual embedding $sv^{(i)}_t$ with the attention embedding and predicts the reasonable location including receptacle $p_t$ and room type $r_t$ for the misplaced object $o_t$, then we have $(p_t, r_t) = F_p(sv^{(i)}_t, attl_t^{(i)}, attr_t^{(i)})$.



\subsection{Communication}

Before communication, each agent generates a characteristic vector $va^{(i)}$ based on their capability and property, $va^{(i)} = (Nav^{(i)}, Mani^{(i)}, Hei^{(i)})$, and obtains a pose vector $pose_t^{(i)} = (x_t^{(i)}, y_t^{(i)}, rot_t^{(i)})$. The state feature extractor $F_s$ including linear layers and the LSTM \cite{hochreiter1997long} layer is utilized to obtain the fused state feature $sf_t^{(i)} = F_s(va^{(i)}, pose_t^{(i)}, fd_t^{(i)}, fp_t^{(i)})$, where $fd_t^{(i)}, fp_t^{(i)}$ are the extracted features from the second-from-last layers of $F_d$ and $F_p$ (the layer before the last classification layer) respectively. To solve the collaboration among heterogeneous agents, we propose the \textit{Handshake-based Group Communication (HanGrCom.)}. In this module, the agent generates the query vector $qry_t^{(i)}$, key vector $key_t^{(i)}$, value vector $val_t^{(i)}$ and inter-group information vector $inv_t^{(i)}$ based on its state feature $[qry_t^{(i)}, key_t^{(i)}, val_t^{(i)}, inv_t^{(i)}] = [\theta^q, \theta^k, \theta^v, \theta^e] (sf_t^{(i)})$, where $\theta^q$, $\theta^k$, $\theta^v$, $\theta^e$ are corresponding vector generators consisting of linear layers respectively. At the $t$-th sub-goal step, each agent sends $qry_t^{(i)}$ to others and calculates the scaled inner product attention of received query vectors and its key vector $attc_{ij} = \frac{qry_t^{(j)} key_t^{(i)}}{\sqrt{d}}$, where $d$ is the dimension of $qry_t^{(j)}$ and $key_t^{(i)}$. Then a communication matrix $T_t = \sigma([attc_{ij}]_{N\times N})$ is obtained, where $attc_{ij}$ indicates the effectiveness of the information send from $A^{(j)}$ to $A^{(i)}$, and $\sigma$ denotes row-wise softmax function. When communicating, we set two thresholds $\delta$ and $\mu$. If $attc_{ii} < \delta$, $A^{(i)}$ needs to receive messages from other agents, since its own information is not efficient. If $attc_{ij} > \mu (i \neq j)$, $A^{(i)}$ can receive messages from $A^{(j)}$. Based on $T_t$, agents can be implicitly divided into several groups. Intra-group communication exchanges the state information from agents in the same group to complete short-term sub-tasks, and $A^{(i)}$ obtain the intra-group communication information $inn_t^{(i)} = \sum_{(j \neq i,att_{ij} > \mu)} att_{ij} \cdot val_t^{(j)}$. Inter-group communication conveys higher-level information on the task allocation across different groups which is beneficial for agents to make future decisions in the long-term task completion, and $A^{(i)}$ obtains the inter-group communication information $int_t^{(i)} = inv_t^{(j)}, j=argmax(avg(att_{* j}))$, where $avg$ is the average operation and $*$ denotes the ids of agents that belong to the same group as $A^{(i)}$. $inn_t^{(i)}$ and $int_t^{(i)}$ are utilized to generate the sub-task. An example of this mechanism is shown in Fig. \ref{fig:group_com}. 

In the training process, in order to ensure the generation of communication information is differentiable, it is assumed that agents can obtain state features of other agents. At each sub-goal step, $A^{(i)}$ obtains the intra-group communication information $inn_t^{(i)} = \sum_{j=1}^{N} att_{ij} \cdot val_t^{(j)}$ and inter-group information $int_t^{(i)} = \sum_{j=1}^{N} (1 - att_{ij}) \cdot inv_t^{(j)}$. In the inference process after training, agents can only obtain their own state features and complete the communication process according to the latent divided communication groups under the threshold $\delta, \mu$ as mentioned above. We set $\mu = 0.2, \delta = 0.8$ in our experiments.



%Before communication, each agent generates a characteristic vector $va^{(i)}$ based on their capability and property, $va^{(i)} = (Nav^{(i)}, Mani^{(i)}, Hei^{(i)})$, and obtains a pose vector $pose^{(i)} = (x_t^{(i)}, y_t^{(i)}, rot_t^{(i)})$. To solve the collaboration among heterogeneous agents, we propose the \textit{Handshake-based Communication (HandComm.)}. In this module, the agent generate the query vector $query_t^{(i)}=(va^{(i)}, pose_t^{(i)})$, and send it to other agents. Each agent considers whether the received query vectors from other agents $query_t^{(j)}$ are matched with its own query vector. If matched, it can receive the information from the corresponding agent. Specifically, in this task, if $Mani^{(i)} \oplus Mani^{(j)}=1$ or $Dis(pose_t^{(i)},pose_t^{(j)})>thres$, then $query_t^{(i)}$ and $query_t^{(j)}$ are matched, and $A^{(i)}$ shares the detection and prediction information as well as map information with $A^{(j)}$, where $\oplus$ is XOR operation, $Dis$ represents the distance between two poses and $thres$ indicates a threshold of the distance.


%We also consider other different ways that can save communication bandwidth. In \textit{Conditional Communication (CondComm.)}, only when an agent detects the misplaced object in the current view, it would broadcast its state and map to others. \textit{Compressed Communication (CmprComm.)} utilizes an encoder-decoder structure to compress the state and map information, and decode the received message from other agents, similar to the communication method in \cite{liu2022multi}. In \textit{Intention Inference with Communication (IntenComm.)}, agents exchange the pose information and infer other agent's next sub-goal, and then decide their next sub-task, which is referred to the idea in \cite{wang2021tom2c}.


%Besides, we also pay attention to the central and broadcast mechanisms, which require a large communication amount. \textit{Central Communication (CentralComm.)} adopts a central node to obtain the state and map information from all agents, then the central node decides the next sub-tasks for each agent. In \textit{Broadcast Communication (BroadComm.)}, each agent broadcasts its state and map information to all agents, then each agent determines the next actions with its state and received messages from others. All these communication methods are demonstrated in Fig. \ref{fig:comm}.


\begin{comment}
\begin{itemize} [leftmargin=0pt, itemindent=2em, itemsep=-0.15cm, topsep=0cm]
% \setlength{\itemsep}{-0.15cm}
\item \textit{Conditional Communication (CondComm.)}: Only when an agent detects the misplaced object in the current view, will it broadcasts its state and map to others. 

\item \textit{Compressed Communication (CmprComm.)}: It utilizes an encoder-decoder structure to compress the state and map information, and decode the received message from other agents, similar to the communication method in \cite{liu2022multi}.

\item \textit{Little Communication \& Intention Inference (IntenComm.)}: Agents exchange the pose information and infer other agent's next sub-goal, and then decide their next sub-task, which is referred to the idea in \cite{wang2021tom2c}.  
\end{itemize}
\end{comment}


\begin{comment}
\begin{itemize} [leftmargin=0pt, itemindent=2em, itemsep=-0.15cm, topsep=0cm]
% \setlength{\itemsep}{-0.15cm}
\item \textit{Central Communication (CentralComm.)}: It adopts a virtual central node to obtain and merge the state and map information from all agents. Then the central node decide the next sub-tasks for each agent.

\item \textit{Broadcast Communication (BroadComm.)}: Each agent broadcasts its state and map information to all agents. Then the subsequent actions of each agent are determined by its state feature and received messages from others.
\end{itemize}
\end{comment}


% Figure environment removed


\begin{table*}[!t] 
	\setlength{\abovecaptionskip}{0.05cm}
	%\setlength{\tabcolsep}{0.50mm}
	\small
	%\setlength{\belowcaptionskip}{-1cm}
	\caption{The Details of the Model Structure}
	
	\label{model}
	\centering
	
	\begin{tabular}{c|c}
		%\toprule[0.8pt]
		\Xhline{1pt}
		
		\textbf{Module} & \textbf{Layers} \\ 
		%\cline{2-16}
		
		
		% \midrule [0.7pt] 
		\Xhline{0.7pt}
		
		Room Classifier  & \textit{Linear} (256, 256) $\rightarrow$ \textit{Linear} (256, 4) \quad [$r_t^{(i)}$]\\
		
		\Xhline{0.7pt}
		
		Misplaced Object Detector  & \textit{Linear} (256, 256) $\rightarrow$ \textit{Linear} (256, 2) \quad [$det_t^{(i)}$]   \\ 
		
		\Xhline{0.7pt}
		
		Receptacle Predictor  & \makecell{\textit{Linear} (256, 256) $\rightarrow$ \textit{Linear} (256, 43) \quad  [$p_t^{(i)}$] \\ \qquad \qquad \qquad \quad $\searrow$  \textit{Linear} (256, 4) \qquad [$r_t^{(i)}$]} \\ 
		
		\Xhline{0.7pt}
		
		Sub-Task Predictor  & \makecell{\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad $\nearrow$ \textit{Linear} (256, 2) \quad [$Explore$ or $Place$] \\ \qquad \qquad \qquad \qquad \qquad \qquad \qquad $\nearrow$ \textit{Linear} (256, 118) \quad [$o_t^{(i)}$] \\ \textit{LSTM} (512) $\rightarrow$ \textit{Linear} (512, 256) $\rightarrow$ \textit{Linear} (256, 43) \qquad  [$p_t^{(i)}$] \\ \qquad \qquad \qquad \qquad \qquad \qquad \qquad $\searrow$ \textit{Linear} (256, 4)  \qquad \, [$r_t^{(i)}$]}  \\ 
		
		\Xhline{0.7pt}
		
		Sub-Goal Predictor  & \makecell{\qquad \qquad \qquad \qquad \qquad \qquad \quad $\nearrow$ \textit{Linear} (256, 9) \quad [$\Delta x_t^{(i)}$] \\ \qquad \qquad \qquad \qquad \qquad \qquad \quad  $\nearrow$ \textit{Linear} (256, 9) \quad [$\Delta y_t^{(i)}$] \\ \textit{LSTM} (512) $\rightarrow$ \textit{Linear} (512, 256) $\rightarrow$ \textit{Linear} (256, 5) \quad [$\Delta rot_t^{(i)}$] \\ \qquad \qquad \qquad \qquad \qquad \qquad \quad $\searrow$ \textit{Linear} (256, 4) \quad [$ope_t^{(i)}$] \\ \qquad \qquad \qquad \qquad \qquad \qquad \quad $\searrow$ \textit{Linear} (256, 2) \quad [$stop_t^{(i)}$]} \\ 
		
		
		\Xhline{1pt}
	\end{tabular}
\end{table*}


\subsection{Hierarchical Decision}

The hierarchical decision module generates the sub-task for each agent, predicts the sub-goal for the corresponding sub-task, and chooses the low-level actions to execute.


% Figure environment removed


\subsubsection{Sub-Task Planning}

The sub-task planning part generates the sub-task for each agent to complete. In this tidying-up task, two types of sub-tasks can be executed, \textit{Explore} and \textit{Place}. In the sub-task \textit{Explore}, the agent needs to explore the scene from its current position and search for misplaced objects. The sub-task \textit{Place} contains three parameters, which are denoted as $(Place, o_t, p_t, r_t)$, meaning to put the misplaced object $o_t$ to the receptacle $p_t$ in the room type $r_t$. A sub-task predictor $F_a$ including the LSTM layer and linear layers is utilized to predict the type and parameters of the executed sub-task with $sf_i^{(i)}$, $inn_i^{(i)}$ and $int_i^{(i)}$. We use the output of the layer before the last prediction layer to obtain the embedding $se_t^{(i)}$ of the predicted sub-task $se_t^{(i)} = F_a(sf_t^{(i)}, inn_t^{(i)}, int_t^{(i)})$.


%The sub-task planning part generates the sub-task for each agent to complete. In this tidying-up task, two types of sub-tasks can be executed, \textit{Explore} and \textit{Place}. In the sub-task \textit{Explore}, the agent needs to explore the scene from its current position and search for misplaced objects. The sub-task \textit{Place} contains three parameters, which are denoted as $(Place, o_t, p_t, r_t)$, meaning to put the misplaced object $o_t$ to the receptacle $p_t$ in the room type $r_t$. At $t$-th step, each agent $A^{(i)}$ has a candidate sub-task list $C^{(i)}_t$. $(Explore)$ is always one of the candidate sub-tasks for all agents. When $A^{(i)}$ detects the misplaced object, if $Mani^{(i)}=1$, $(Place, o_t, p_t, r_t)$ would be a candidate sub-task for its next step, and if $Mani^{(i)}=0$, the obtained sub-task information would be delivered to other agents through communication. $C^{{i}}_t$ also contains the sub-task information received from other agents. If $Mani^{(i)}=1$, $C^{(i)}_t = [(Explore), (Place, o^{(1)}_t, p^{(1)}_t, r^{(1)}_t),\cdots, (Place, o^{(d)}_t, p^{(d)}_t, \; \allowbreak r^{(d)}_t)]$, where $d$ is the number of detected misplaced objects. If $Mani^{(i)}=0$, $C^{(i)}_t = [(Explore)]$. Based on the predicted distance to misplaced objects and the reasoned receptacle, $A^{(i)}$ sorts the sub-tasks in the candidate list and selects the top sub-task $sk^{(i)}_t$ to execute. 



\subsubsection{Action Decision}

Each agent encodes the $pose_t^{(i)}$ to get the pose embedding $sp_t^{(i)}$. A sub-goal predictor $F_g$ including the LSTM layer and linear layers is utilized to predict the next sub-goal $sg^{(i)}_t$ with $se_t^{(i)}$, $sm_t^{(i)}$, $sp_t^{(i)}$ and the visual embedding $sv_t^{(i)}$. $sg^{(i)}_t = (\Delta x^{(i)}_t, \Delta y^{(i)}_t, \Delta rot^{(i)}_t, ope^{(i)}_t, stop^{(i)}_t) = F_g(se_t^{(i)}, sm_t^{(i)}, sp_t^{(i)}, sv_t^{(i)})$, where $\Delta x^{(i)}_t$, $\Delta y^{(i)}_t$, $\Delta rot^{(i)}_t$ are the movement in egocentric x and y axis and the rotation angle of the agent respectively. $ope^{(i)}_t$ denotes whether performing action \textit{PickUp}, \textit{PutDown}, \textit{Drop} or \textit{NoAction} after reaching the sub-goal. $stop^{(i)}_t$ predicts whether the agent stops or not after this sub-goal step. Then, a low-level action strategy based on the shortest path algorithm is utilized to generate specific actions from $I^{(i)}$ to reach the current sub-goal.

%After communication, each agent can fuse the map information received from other agents to get the whole semantic map, and obtain the map embedding $sm_t^{(i)}$ with a pre-trained scene encoder which is similar to that in \cite{liuembodied} consisting of multiple convolutional layers. We encode $sk^{(i)}_t$ and $pose_t^{(i)}$ to get the sub-task embedding $se_t^{(i)}$ and the pose embedding $sp_t^{(i)}$ respectively. A sub-goal predictor $F_g$ including the LSTM layer and multiple MLPs is utilized to predict the next sub-goal $sg^{(i)}_t$ with $se_t^{(i)}$, $sm_t^{(i)}$, $sp_t^{(i)}$ and the visual embedding $sv_t^{(i)}$, $sg^{(i)}_t = (\Delta x^{(i)}_t, \Delta y^{(i)}_t, \Delta rot^{(i)}_t, ope^{(i)}_t) = F_g(se_t^{(i)}, sm_t^{(i)}, sp_t^{(i)}, sv_t^{(i)})$, where $\Delta x^{(i)}_t$, $\Delta y^{(i)}_t$, $\Delta rot^{(i)}_t$ are the movement in egocentric x and y axis and the rotation angle of the agent respectively. $ope^{(i)}_t$ denotes whether performing action \textit{PickUp}, \textit{PutDown} or \textit{Drop} after reaching the sub-goal. Then, a low-level action strategy based on the shortest path algorithm is utilized to generate specific actions from $I^{(i)}$ to reach the current sub-goal. %The details of model training is presented in the Appendix. 


\subsection{Model Training}


We demonstrate the details of our model structure in Table \ref{model}. $Linear(d_{in},d_{out})$ indicates the linear layer, and $d_{in}$, $d_{out}$ denote the dimensions of input and output of the layer respectively. $LSTM(d_{hid})$ indicates the LSTM layer, and $d_{hid}$ denotes the hidden size. $[*]$ represents the predicted contents. Specifically, $r_t^{(i)}$ indicates the room types which include 4 categories, $det_t^{(i)}$ indicates whether the object is misplaced or not, $p_t^{(i)}$ denotes the receptacle types which include 43 categories, $o_t^{(i)}$ denotes the object types which include 118 categories. The agent would predict the egocentric sub-goal within 1 meter in the x and y axis respectively, and the agent moves 0.25 meters each time corresponding to one grid after the discretization of the environment. The values of $\Delta x_t^{(i)}$ and $\Delta y_t^{(i)}$ are $\{-4, -3, -2, -1, 0, 1, 2, 3, 4\}$ respectively, where the negative values indicate moving in the negative direction of x and y axis, the positive values indicate moving in the positive direction of x and y axis. $rot^{(i)}_t$ can take the value of 0, 90, 180, or 270.


Since the function of each module and their training process are relatively independent, we train them separately. The misplaced object detector and reasonable receptacle predictor are trained with supervised learning, and the action decision part of the hierarchical decision module is trained with imitation learning.


We first train the room classifier in the misplaced object detection module with the true labels of the rooms where the specific object is located obtained from the houses in ProcTHOR-10K. During the training process, the parameters of the pre-trained ResNet and Mask-RCNN are frozen. After the training of the room classifier, we fix its parameters and utilize it to extract the room embedding. Then we select the pickupable objects in a number of training houses in ProcTHOR-10K and place them to reasonable or unreasonable locations in the houses to get the training samples for the misplaced object detector and the reasonable receptacle predictor. We generate the true labels of whether the corresponding objects are misplaced and extract the reasonable locations of misplaced objects based on the metadata of ProcTHOR-10K to obtain the ground truth of training samples for the misplaced object detector and the reasonable receptacle predictor respectively. We train the misplaced object detector and the reasonable receptacle predictor with the parameters of the room classifier fixed. The loss function of this process is defined as follows,
\begin{equation}\label{supervised}
	\begin{split}
		Loss_{cla} &= Loss_{type} \\ 
		Loss_{det} &= \alpha Loss_{mis} + \beta Loss_{rec1} + \lambda Loss_{room1} \\ 
	\end{split}
\end{equation}
where $Loss_{cla}$ denotes the loss function of the room classifier, and $Loss_{type}$ indicates the cross entropy loss between the predicted and the true type of the room where the specific object is located. $Loss_{det}$ denotes the loss function of the misplaced object detector and the reasonable receptacle predictor. $Loss_{mis}$ indicates the binary cross entropy of whether the object is misplaced. $Loss_{rec1}$, $Loss_{room1}$ denotes the multi-label classification loss between the predicted and true reasonable types of the receptacle and the room where the object can be placed. $\alpha$, $\beta$, $\lambda$ are the hyper-parameters. In our implementation, we set $\alpha$, $\beta$, $\lambda$ to 1.


The scene encoder consists of convolutional layers and is pre-trained to extract the map embedding of the semantic map. We design the query to predict whether a type of object exists in a sub-region of the top-down map. We generate a series of queries, which consist of a sub-area and a specific type of object, and extract the true answer of the query from the metadata of the houses. Then we pre-train the scene encoder with supervised learning, and the loss function is denoted as the entropy loss between the predicted answers and the true answers. 


The hierarchical decision module predicts the next sub-task and sub-goal for each agent. The sub-task planning part predicts the type of sub-task ($Explore$ or $Place$) and the parameters of the sub-task, which contain the misplaced object type $o_t^{(i)}$, the reasonable receptacle type $p_t^{(i)}$ and the reasonable room type $r_t^{(i)}$. The action decision part predicts the next sub-goal specified by the grid of 0.25 meters. It independently predicts the number of movement grids of the sub-goal within 1 meter in the x and y axis $\Delta x^{(i)}_t$, $\Delta y^{(i)}_t$ respectively, the rotation angle $\Delta rot^{(i)}_t$, the operation action $ope^{(i)}_t$ and the probability of performing the action $Stop^{(i)}_t$. $rot^{(i)}_t$ takes the value of 0, 90, 180, or 270. $ope^{(i)}_t$ takes the value of \textit{PickUp}, \textit{PutDown}, \textit{Drop} or \textit{NoAction}. Since agents are heterogeneous and their abilities are different, the hierarchical decision modules of each agent are trained separately, and they do not share weights. We use behavior cloning to train the hierarchical decision module with the generated expert demonstrations of allocated sub-task and sub-goal actions. The loss function of training each sub-task planning part and action decision part with imitation learning is defined as follows,
\begin{equation}\label{subgoal}
	\begin{split}
		Loss_{subtask} &= Loss_{task} + \gamma_1 Loss_{obj} + \delta_1 Loss_{rec2} \\ 
		&+ \theta_1 Loss_{room2} \\
		Loss_{subgoal} &= Loss_{x_{loc}} + Loss_{y_{loc}} + \gamma_2 Loss_{rot} \\ 
		&+ \delta_2 Loss_{ope} + \theta_2 Loss_{stop} 
	\end{split}
\end{equation}
where $Loss_{task}$ indicates the binary cross entropy of the sub-task type. $Loss_{obj}$, $Loss_{rec2}$, $Loss_{room2}$ denotes the cross entropy loss between the predicted $o_t^{(i)}$, $p_t^{(i)}$, $r_t^{(i)}$ and those in demonstrations respectively. $Loss_{x_{loc}}$, $Loss_{y_{loc}}$, $Loss_{rot}$,  $Loss_{ope}$ denotes the cross entropy loss between the predicted results and those in demonstrations respectively, $Loss_{stop}$ denotes the binary cross entropy of whether the agent stops or not. $\gamma_1$, $\gamma_2$, $\delta_1$, $\delta_2$, $\theta_1$, $\theta_2$ are the hyper-parameters that can control the training process. In our implementation, we set $\gamma_1$, $\gamma_2$, $\delta_1$, $\delta_2$, $\theta_1$, $\theta_2$ to be 1.




\renewcommand{\arraystretch}{1.1}
\begin{table*}[!t] 
	\setlength{\abovecaptionskip}{0.05cm}
	\setlength{\tabcolsep}{0.55mm}
	\small
	%\setlength{\belowcaptionskip}{-1cm}
	\caption{Quantitative Results in Heterogeneous Multi-agent Tidying-up Task in \textit{Setting I}}
	
	\label{quan1}
	\centering
	
	\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
		%\toprule[0.8pt]
		\Xhline{1pt}
		
		\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{$Success(\uparrow)$} & \multicolumn{3}{c|}{$\%PS(\uparrow)$}& \multicolumn{3}{c|}{$\%FM(\uparrow)$}  & \multicolumn{3}{c|}{$\#PL(\downarrow)$} &\multicolumn{3}{c|}{$ACm(\downarrow)$} & \multicolumn{3}{c}{$CES(\uparrow)$} \\ 
		%\cline{2-16}
		
		& Single  & Cross & All  & Single  & Cross & All  & Single  & Cross & All   & Single  & Cross & All & Single  & Cross & All & Single  & Cross & All \\
		
		% \midrule [0.7pt] 
		\Xhline{0.7pt}
		SA  & 0.045  & 0.030  & 0.038  &  0.096  & 0.081  & 0.089  &  0.525  &  0.483 & 0.507  & 278.6  &  294.5  &  285.5 & - & - & - & - & - & - \\ 
		
		SA(Oracle)  & 0.090  & 0.068  & 0.080  &  0.181  & 0.163  & 0.173  & 0.703  & 0.681  & 0.693  & 184.8  & 197.1   & 190.1 & - & - & - & - & - & -\\ 
		
		\Xhline{0.7pt}
		
		Random  & 0  & 0  &  0  &  0  & 0  &  0  &  0.003  &  0  &  0.002  &  300.0  &  300.0  &  300.0 & 0 & 0 & 0 & - & - & -\\ 
		
		
		QMIX  & 0.032  & 0.015  &  0.025  &  0.082 &  0.069  & 0.076  & 0.472  & 0.421  & 0.450  &  293.1 & 300.0  & 296.1 &  512.0  &  512.0 &  512.0 & 0 & 0 & 0\\ 
		
		
		\Xhline{0.7pt}
		
		
		CondComm.  & 0.081  & 0.066  & 0.075  &  0.176  & 0.151  & 0.165  &  0.712  &  0.709 & 0.710  & 192.3  &  263.1  &  223.1 & 523.1 & 501.3 & 513.6 & 0.7 & 0.7 & 0.7 \\ 
		
		CmprComm.  & 0.067  & 0.053  & 0.061  &  0.167  & 0.131  & 0.151  &  0.695  &  0.677 & 0.687  & 188.3  &  253.3  &  216.6 & 220.0 & 220.0 & 220.0 & 1.0 & 1.0 & 1.0\\ 
		
		IntenComm.  & 0.047  & 0.032  & 0.040  &  0.136  & 0.101  & 0.121  &  0.645  &  0.609 & 0.629  & 214.6  &  267.5  &  237.6 & \textbf{20.0} & \textbf{20.0} & \textbf{20.0} & 1.0 & 1.0 & 1.0 \\ 
		
		\textbf{Ours}  & \textbf{0.121}  & \textbf{0.079}  & \textbf{0.103}  &  \textbf{0.194} & \textbf{0.160}  & \textbf{0.179}  &  \textbf{0.739}  &  \textbf{0.726} & \textbf{0.733}  & \textbf{186.1}  &  \textbf{252.6}  &  \textbf{215.0} & 363.1 & 366.5 & 364.6 & \textbf{2.1} & \textbf{1.4} & \textbf{1.8}\\ 
		
		%\Xhline{0.7pt}
		
		\rowcolor{lightgray!30} BroadComm. & 0.130  & 0.089  & 0.112  &  0.201  & 0.184  & 0.194  & 0.742 & 0.731 & 0.737  & 183.3  &  245.9  &  210.5 & 820.0 & 820.0 & 820.0 & 1.0 & 0.7 & 0.9\\
		
		\rowcolor{lightgray!30} CentralComm.  & {0.131}  & {0.092}  & {0.114}  & {0.205}  & {0.166}  & {0.188}  &  {0.755}  & {0.739} & {0.748}  & {183.2}  &  {243.1} & {209.3} & 820.0 & 820.0 & 820.0 & 1.0 & 0.8 & 0.9 \\ 
		
		\Xhline{1pt}
	\end{tabular}
\end{table*}


\renewcommand{\arraystretch}{1.1}
\begin{table*}[!t] 
	\setlength{\abovecaptionskip}{0.05cm}
	\setlength{\tabcolsep}{0.55mm}
	\small
	%\setlength{\belowcaptionskip}{-1cm}
	\caption{Quantitative Results in Heterogeneous Multi-agent Tidying-up Task in \textit{Setting II}}
	
	\label{quan2}
	\centering
	
	\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
		%\toprule[0.8pt]
		\Xhline{1pt}
		
		\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{$Success(\uparrow)$} & \multicolumn{3}{c|}{$\%PS(\uparrow)$}& \multicolumn{3}{c|}{$\%FM(\uparrow)$}  & \multicolumn{3}{c|}{$\#PL(\downarrow)$} &\multicolumn{3}{c|}{$ACm(\downarrow)$} & \multicolumn{3}{c}{$CES(\uparrow)$} \\ 
		%\cline{2-16}
		
		& Single  & Cross & All  & Single  & Cross & All  & Single  & Cross & All   & Single  & Cross & All & Single  & Cross & All & Single  & Cross & All \\
		
		% \midrule [0.7pt] 
		\Xhline{0.7pt}
		SA  & 0.045  & 0.030  & 0.038  &  0.096  & 0.081  & 0.089  &  0.525  &  0.483 & 0.507  & 278.6  &  294.5  &  285.5 & - & - & - & - & - & - \\ 
		
		SA(Oracle)  & 0.090  & 0.068  & 0.080  &  0.181  & 0.163  & 0.173  & 0.703  & 0.681  & 0.693  & 184.8  & 197.1   & 190.1 & - & - & - & - & - & -\\ 
		
		\Xhline{0.7pt}
		
		Random  & 0  & 0  &  0  &  0  & 0  &  0  &  0.005  &  0  &  0.003  &  300.0  &  300.0  &  300.0 & 0 & 0 & 0 & - & - & -\\ 
		
		
		QMIX  & 0.035  & 0.021  &  0.029  &  0.091 &  0.076  & 0.084  & 0.483  & 0.425  & 0.458  &  290.5 & 296.3  & 292.9 &  512.0  &  512.0 &  512.0 & 0 & 0 & 0\\ 
		
		
		\Xhline{0.7pt}
		
		
		CondComm.  & 0.090  & 0.072  & 0.082  &  0.179  & 0.156  & 0.169  &  0.720  &  0.711 & 0.716  & 188.5  &  230.7  &  206.9 & 620.6 & 613.1 & 617.3 & 0.7 & 0.7 & 0.7 \\ 
		
		CmprComm.  & 0.073  & 0.059  & 0.067  &  0.173  & 0.139 & 0.158  &  0.703  &  0.683 & 0.694  & 183.1  &  226.9  &  202.1 & 330.0 & 330.0 & 330.0 & 0.8 & 0.9 & 0.8\\ 
		
		IntenComm.  & 0.048  & 0.033  & 0.041  &  0.139  & 0.112  & 0.127  &  0.661  &  0.620 & 0.643  & 210.1  &  255.3  &  229.8 & \textbf{30.0} & \textbf{30.0} & \textbf{30.0} & 1.0 & 1.0 & 1.0 \\ 
		
		\textbf{Ours}  & \textbf{0.129}  & \textbf{0.090}  & \textbf{0.112}  &  \textbf{0.203} & \textbf{0.169}  & \textbf{0.188}  &  \textbf{0.745}  &  \textbf{0.739} & \textbf{0.743}  & \textbf{166.9}  &  \textbf{221.3}  &  \textbf{190.6} & 472.5 & 478.3 & 475.0 & \textbf{1.8} & \textbf{1.3} & \textbf{1.6}\\ 
		
		%\Xhline{0.7pt}
		
		\rowcolor{lightgray!30} BroadComm. & 0.136  & 0.094  & 0.118  &  0.209  & 0.189  & 0.200  & 0.748 & 0.741 & 0.745  & 161.1  &  213.3  &  183.8 & 1230.0 & 1230.0 & 1230.0 & 0.7 & 0.5 & 0.6\\
		
		\rowcolor{lightgray!30} CentralComm.  & {0.138}  & {0.097}  & {0.120}  & {0.211}  & {0.173}  & {0.194}  &  {0.760}  & {0.745} & {0.753}  & {160.6}  &  {213.1} & {183.4} & 1230.0 & 1230.0 & 1230.0 & 0.8 & 0.5 & 0.7 \\ 
		
		\Xhline{1pt}
	\end{tabular}
\end{table*}



\section{Experiments}\label{sec:experiment}

\subsection{Experiment Setting}

We focus on heterogeneous multi-agent collaboration, and we evaluate the proposed collaboration strategy in the tidying-up task. To effectively and efficiently train and verify our model, we select 120 scenes from the newly built dataset, 80 houses for training, 20 for validation, and 20 for testing. The number of scenes in our experiment is the same as that in other embodied tasks in AI2-THOR, which is widely used in different embodied tasks. Each house contains 10 different meta-tasks, and each meta-tasks contains 5 different initial positions for heterogeneous agents. 


We consider two different heterogeneous settings with different numbers of heterogeneous agents and different ability settings. In \textit{Setting I}, there are three agents with the same visual perception ability but different action abilities and morphological characteristics. Specifically, $A^{(1)}$ only has the navigation ability with the low height, $Nav^{(1)}=1,Mani^{(1)}=0,Hei^{(1)}=0$. $A^{(2)}$ only has the navigation ability and its height is high, $Nav^{(2)}=1,Mani^{(2)}=0,Hei^{(2)}=1$. $A^{(3)}$ has both the navigation and manipulation abilities and its height is high, $Nav^{(3)}=1,Mani^{(3)}=1,Hei^{(3)}=1$. In \textit{Setting II}, there are four heterogeneous agents. The settings of $A^{(1)}$, $A^{(2)}$ and $A^{(3)}$ are the same as \textit{Setting I}, and $A^{(4)}$ has the same abilities with $A^{(3)}$. The reasons that we select the \textit{Setting I} and \textit{Setting II} are that these two settings can represent the general heterogeneous collaboration scenarios, and the experimental results with these two settings can demonstrate the generalization of our model across different numbers and heterogeneous settings of agents. In \textit{Setting I}, $A^{(3)}$ can pick up objects which are pickupable from the correct interaction locations. It is easier for $A^{(1)}$ with the lower field of view to find the objects thrown on the floor, and it can share its detected information to $A^{(3)}$ to help it pick up thrown objects. $A^{(2)}$ with the higher field of view can observe some areas that $A^{(1)}$ cannot, and its perspective can complement that of $A^{(1)}$ to assisting $A^{(3)}$ complete the tidying-up task. The visual perception of $A^{(1)}$ and $A^{(2)}$ with perspectives at different heights can complement each other during exploration, and $A^{(3)}$ can operate the misplaced objects with the assistance of $A^{(1)}$ and $A^{(2)}$ at different heights to complete the task. In \textit{Setting II}, both $A^{(3)}$ and $A^{(4)}$ can pick up misplaced objects, and they can also cooperate with each other to improve the efficiency of putting objects to the reasonable locations. These two settings can solve different situations of this task. $A^{(3)}$ ($A^{(4)}$) can pick up misplaced objects detected but cannot be picked up by $A^{(1)}$ and $A^{(2)}$. The different perspectives of $A^{(1)}, A^{(2)}$ can complement each other to easier find objects thrown on the floor. For each setting, there are a total of 4000 training tasks ($80 \times 10 \times 5$), 1000 validation tasks ($20 \times 10 \times 5$), and 1000 testing tasks. In testing tasks, there are 565 \textit{Single} tasks and 435 \textit{Cross} tasks.



%We consider three heterogeneous agents in our task. They have the same visual perception ability but have different action abilities and morphological characteristics. Specifically, $A^{(1)}$ has only the navigation ability with the low height, that is, $Nav^{(1)}=1,Mani^{(1)}=0,Hei^{(1)}=0$. $A^{(2)}$ only has the navigation ability and its height is high, that is, $Nav^{(2)}=1,Mani^{(2)}=0,Hei^{(2)}=1$. $A^{(3)}$ has both the navigation and manipulation abilities and its height is high, that is, $Nav^{(3)}=1,Mani^{(3)}=1,Hei^{(3)}=1$. These three agents can solve different situations of the tidying-up task. $A^{(3)}$ can pick up objects which are pickupable from the correct interaction locations. It is easier for $A^{(1)}$ with the lower field of view to find the objects thrown on the floor, and it can share its detected information to $A^{(3)}$ to help it pick up thrown objects. $A^{(2)}$ with the higher field of view can observe some areas that $A^{(1)}$ cannot, and its perspective can complement that of $A^{(1)}$ to assisting $A^{(3)}$ complete the tidying-up task.

%For each task, we randomly generated three reachable initial positions for three heterogeneous agents in our experiment.



\renewcommand{\arraystretch}{1.1}
\begin{table*}[!t] 
	\setlength{\abovecaptionskip}{0.05cm}
	\setlength{\belowcaptionskip}{-0.2cm}
	\setlength{\tabcolsep}{0.45mm}
	\small
	
	\caption{Ablation Experiments Results in Heterogeneous Multi-agent Tidying-up Task in \textit{Setting I}}
	
	\label{abla1}
	\centering
	
	\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
		%\toprule[0.8pt]
		\Xhline{1pt}
		
		\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{$ Success (\uparrow)$} & \multicolumn{3}{c|}{$\%PS (\uparrow)$}& \multicolumn{3}{c|}{$\%FM (\uparrow)$}  & \multicolumn{3}{c|}{$\#PL (\downarrow)$} &\multicolumn{3}{c|}{$ACm(\downarrow)$} & \multicolumn{3}{c}{$CES(\uparrow)$} \\ 
		
		& Single  & Cross & All  & Single  & Cross & All  & Single  & Cross & All   & Single  & Cross & All  & Single  & Cross & All  & Single  & Cross & All\\
		
		\Xhline{0.7pt}
		
		Ours w/o Know.  & 0.042  & 0.030  &  0.037  &  0.087  & 0.080  & 0.084  &  0.425  &  0.366 & 0.399  & 235.1  &  267.3  &  249.1 & 363.5 & 366.6 & 364.8 & 0 & 0 & 0 \\ 
		
		Ours w/o MisObjDec.  & 0.051  & 0.046  &  0.049  &  0.109  & 0.091  & 0.101  &  0.515  &  0.471 & 0.496  & 234.6  &  264.5  &  247.6 & 363.3 & 366.6 & 364.7 & 0.2 & 0.4 & 0.3 \\ 
		
		Ours w/o ReaRecPre.  & 0.063  & 0.058  &  0.061  &  0.120 & 0.098  & 0.110  &  0.551  &  0.509 & 0.533  & 231.1  &  260.3  &  243.8 & 363.3 & 366.9 & 364.9 & 0.5 & 0.8 & 0.6 \\ 
		
		Ours w/o Comm.  & 0.045  & 0.030  & 0.038  &  0.096  & 0.081  & 0.089  &  0.525  &  0.483 & 0.507  & 278.6  &  294.5  &  285.5 & 0 & 0 & 0 & - & - & - \\ 
		
		Ours w/o HierDec.  & 0.055  & 0.049  & 0.052  &  0.106 & 0.093  & 0.100  &  0.573  &  0.532 & 0.555  & 266.3  &  284.5  &  274.2 & 366.1 & 367.2 & 366.6 & 0.3 & 0.5 & 0.4 \\ 
		
		Ours  & {0.121}  & {0.079}  & {0.103}  &  {0.194} & {0.160}  & {0.179}  &  {0.739}  &  {0.726} & {0.733} & {186.1}  &  {252.6}  &  {215.0} & 363.1 & 366.5 & 364.6 & {2.1} & {1.4} & {1.8} \\
		
		%\midrule [0.8pt]
		%\Xhline{0.8pt}
		%\bottomrule[0.8pt]
		\Xhline{1pt}
	\end{tabular}
\end{table*}



\renewcommand{\arraystretch}{1.1}
\begin{table*}[!t] 
	\setlength{\abovecaptionskip}{0.05cm}
	\setlength{\belowcaptionskip}{-0.2cm}
	\setlength{\tabcolsep}{0.45mm}
	\small
	
	\caption{Ablation Experiments Results in Heterogeneous Multi-agent Tidying-up Task in \textit{Setting II}}
	
	\label{abla2}
	\centering
	
	\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
		%\toprule[0.8pt]
		\Xhline{1pt}
		
		\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{$ Success (\uparrow)$} & \multicolumn{3}{c|}{$\%PS (\uparrow)$}& \multicolumn{3}{c|}{$\%FM (\uparrow)$}  & \multicolumn{3}{c|}{$\#PL (\downarrow)$} &\multicolumn{3}{c|}{$ACm(\downarrow)$} & \multicolumn{3}{c}{$CES(\uparrow)$} \\ 
		
		& Single  & Cross & All  & Single  & Cross & All  & Single  & Cross & All   & Single  & Cross & All  & Single  & Cross & All  & Single  & Cross & All\\
		
		\Xhline{0.7pt}
		
		Ours w/o Know.  & 0.049  & 0.033  &  0.042  &  0.103  & 0.096  & 0.010  &  0.452  &  0.375 & 0.419  & 227.0  &  258.5  &  240.7 & 473.3 & 479.2 & 475.9 & 0.09 & 0.06 & 0.08 \\ 
		
		Ours w/o MisObjDec.  & 0.058  & 0.052  &  0.055  &  0.118  & 0.103  & 0.111  &  0.523  &  0.480 & 0.504  & 225.5  &  256.1  &  238.8 & 473.1 & 479.0 & 475.7 & 0.3 & 0.5 & 0.4 \\ 
		
		Ours w/o ReaRecPre.  & 0.075  & 0.069  &  0.072  &  0.128 & 0.105  & 0.118  &  0.560  &  0.515 & 0.540  & 220.6  &  251.9  &  234.2 & 473.3 & 478.9 & 475.8 & 0.6 & 0.8 & 0.7 \\ 
		
		Ours w/o Comm.  & 0.049  & 0.033  & 0.042  &  0.105  & 0.091  & 0.099  &  0.535  &  0.496 & 0.518  & 273.1  &  280.6  &  276.4 & 0 & 0 & 0 & - & - & - \\ 
		
		Ours w/o HierDec.  & 0.063  & 0.055  & 0.060  &  0.111 & 0.99  & 0.106  &  0.581  &  0.537 & 0.562  & 260.1  &  273.6  &  266.2 & 475.3 & 478.8 & 477.3 & 0.4 & 0.5 & 0.4 \\ 
		
		Ours  & {0.129}  & {0.090}  & {0.112}  &  {0.203} & {0.169}  & {0.188}  &  {0.745}  &  {0.739} & {0.743}  & {166.9}  &  {221.3}  & {190.6} & 472.5 & 478.3 & 475.0 & {1.8} & {1.3} & {1.6} \\
		
		%\midrule [0.8pt]
		%\Xhline{0.8pt}
		%\bottomrule[0.8pt]
		\Xhline{1pt}
	\end{tabular}
\end{table*}



\subsection{Evaluation Metrics}

We propose six metrics to evaluate the performance of the model, and $N_{epi}$ denotes the number of tasks: 
%\textit{Success}, \textit{\%PatialSuccess}, \textit{\%FindMisObj}, and \textit{\#PathLength}, which are shown as follows, $N_{epi}$ denotes the number of episodes.

\textit{1) Success (Suc)}: $Suc=\frac{1}{N_{epi}}\sum_{i=1}^{N_{epi}}R_i$, where $R_i=1$ if all misplaced objects are re-placed in reasonable locations, otherwise $R_i=0$. It denotes the strict success rate.

\textit{2) \%PatialSuccess (\%PS)}: $\%PS=\frac{1}{N_{epi}}\sum_{i=1}^{N_{epi}}\frac{K^{suc}_i}{K_i}$, where ${K_i}$ denotes the number of misplaced objects in the $i$-th task, and $K^{suc}_i$ is the number of misplaced objects that are re-placed to reasonable locations by agents in this task. It can reflect the proportion of misplaced objects that can be successfully tidied in each task.

\textit{3) \%FindMisObj (\%FM)}: $\%FM=\frac{1}{N_{epi}}\sum_{i=1}^{N_{epi}}\frac{K^{det}_i}{K_i}$, where $K^{det}_i$ is the number of misplaced objects that are detected and picked up by agents in this task. It can reflect the effect of misplaced objects detector to some extent.

\textit{4) \#PathLength (\#PL)}: $\#PL=\frac{1}{N_{epi}}\sum_{i=1}^{N_{epi}}Len_i$, where $Len_i$ is the number of steps in the multi-agent trajectory when synchronously completing the $i$-th task, that is, the maximum number of steps of three agents. It evaluates the efficiency of completing the task. The fewer steps, the higher efficiency of the multi-agent collaboration.


\textit{5) AvgCom (ACm)}: $ACm=\frac{1}{N_{epi}}\sum_{i=1}^{N_{epi}} \frac{Total_i}{Len_i \cdot N}$, where $Total_i$ is the total dimensions of communication messages among agents. It evaluates the average communication amounts per agent per step.


\textit{6) CommunicationEfficiencyScore (CES)}: $CES=max(0, \frac{10000 \cdot (Suc - SucSA)}{ACm})$, where $SucSA$ is the success rate of the single-agent model. It evaluates the proportion of efficiency improvement of different communication methods. The higher $CES$, the higher efficiency of the communication mechanism.


\subsection{Quantitative Analysis}

We evaluate the proposed model with different action decision strategies and communication methods in testing tasks. For different action strategies, we compare the results with \textit{SA}, \textit{SA(Oracle)}, \textit{Random}, and \textit{QMIX} model. \textit{SA} only utilizes one agent with both navigation and manipulation abilities to complete the task, and the other parts are the same as the proposed model. In \textit{SA(Oracle)}, the objects' locations are known to the agent, and the other parts are the same as \textit{SA}. \textit{Random} model randomly generates actions for each agent from their action spaces. \textit{QMIX} is a MARL method based on \cite{rashid2020monotonic}, in which agents obtain all state features from others, fuse with their own state features, and directly predict the next low-level actions based on fused state features. For different communication methods, we consider several methods that save bandwidth. In \textit{CondComm.}, only when an agent detects the misplaced object in the current view, it would broadcast its state and map to others. \textit{CmprComm.} utilizes an encoder-decoder structure to compress the communication messages, and decode received messages from other agents, similar to that in \cite{liu2022multi}. In \textit{IntenComm.}, agents exchange state and sub-task information, infer other agent's next sub-goal, and then decide their next sub-task, which is referred to as the idea in \cite{wang2021tom2c}. We also pay attention to the central and broadcast mechanisms, which require large communication amounts. \textit{CentralComm.} adopts a central node to obtain the messages from all agents and generates the sub-task for each agent. In \textit{BroadComm.}, each agent broadcasts its messages to all agents and determines the next actions with its state and received messages from others. All these communication baselines are demonstrated in Fig. \ref{fig:comm}.



% Figure environment removed

\begin{comment}
% Figure environment removed
\end{comment}


% Figure environment removed


We show the quantitative results of \textit{Single} and \textit{Cross} tasks in \textit{Setting I}, \textit{Setting II} in Table \ref{quan1} and Table \ref{quan2} respectively. The best performance except for central and broadcast methods are bolded. The maximum number of steps per task is 300. The results show that the heterogeneous multi-agent tidying-up task is challenging. $\%FM$ is relatively high, but the success rate of the whole task is relatively low, which indicates that moving the detected misplaced objects to the reasonable location is challenging, since agents may not reason the proper receptacle or not generate the valid manipulation actions to re-place the misplaced object to predicted locations. The results in \textit{Setting I} and \textit{Setting II} indicate that our model can generalize to different numbers and heterogeneous settings of agents. Compared with multi-agent models, the success rate of task completion of \textit{SA} and \textit{SA(Oracle)} is lower, which demonstrates that collaboration and communication among heterogeneous multi-agents can help agents improve the accuracy and efficiency of tasks. Although agents in \textit{QMIX} can obtain state features of other agents, its performance is relatively low, indicating that this MARL method is not effective enough to deal with visual information, and it is difficult to directly predict low-level actions. 


As for different communication methods, the performances of \textit{Ours}, \textit{CondComm.}, \textit{CmprComm.} and \textit{IntenComm.} in task completion ($Suc$,$\%PS$,$\%FM$,$\#PL$) are lower than \textit{CentralComm.} and \textit{BroadComm.}, because agents share partial information with others for decision-making. Since \textit{CentralComm.} utilizes a central node to process information, its task completion performance can be regarded as the upper bound of multi-agent models. Our model performs better than other communication methods that share partial messages with agents in task completion. In our model, the agent with manipulation ability can obtain the state and map information from other agents with only the navigation ability, which is beneficial for the agent to put the object to a reasonable location, and its performance is better than the other three models. In \textit{CondComm.}, agents exchange their state only when a misplaced object is detected, and the incomplete shared map information makes its performance lower than our model. The performance of \textit{CmprComm.} indicates that decoding the compressed communication message would lose some effective information though the communication bandwidth can be reduced. The performance of \textit{IntenComm.} is relatively low due to the improper reasoning results of other agents' decisions and incomplete exchanging of information. Furthermore, in \textit{QMIX}, each agent receives the fused 512-dimensional state features from other agents, so $ACm$ of \textit{QMIX} is always 512. The size of local map embedding $sm_t^{(i)}$ is $20 \times 20$. The transmitted state information contains the 3-dimensional $va^{(i)}$, 3-dimensional pose $pose_t^{(i)}$ and 4-dimensional sub-task information $(Place/Explore, o_t, p_t, r_t)$, so the number of total dimensions of the state information from one agent is 10. Since \textit{IntenComm.} only transmits the state information from other agents to each agent, so \textit{ACm} in \textit{Setting I} is 20 and \textit{ACm} in \textit{Setting II} is 30. In \textit{CmprComm.}, each agent compresses the dimensions of the map embedding to be 100, and transmits the compressed map embedding as well as the state information to other agents, so \textit{ACm} in \textit{Setting I} is $2 \times (100 + 10) = 220$, \textit{ACm} in \textit{Setting II} is $3 \times (100 + 10) = 330$. In \textit{BroadComm.} and \textit{CentralComm.}, each agent transmits their state information and map embeddings to all other agents, so \textit{ACm} in \textit{Setting I} is $2 \times (400 + 10) = 820$ and \textit{ACm} in \textit{Setting II} is $3 \times (400 + 10) = 1230$. $ACm$ of \textit{CentralComm.} and \textit{BroadComm.} are the largest, and with the increased number of heterogeneous agents, their $ACm$ have increased greatly, but the declines in $SCE$ are relatively larger, which indicates that they are not cost-effective enough. Although \textit{IntenComm.} uses the smallest $ACm$, its $CES$ is not high, since its $Suc$ is not good. Our method can save communication amounts, achieve good performance of task completion, and obtain the highest communication efficiency $CES$, demonstrating the effectiveness of the proposed communication mechanism.


\subsection{Qualitative Analysis}

We demonstrate successful samples with three agents in \textit{Setting I} and four agents in \textit{Setting II} respectively in Fig. \ref{fig:succ}. The tidying-up tasks in \ref{a} and \ref{b} are the same. Agents complete the same task more quickly in \textit{Setting II}, since \textit{Agent 3} and \textit{Agent 4} with both the navigation and manipulation abilities can collaborate with each other to pick up misplaced objects and put them to reasonable locations, which demonstrates that the proposed model can make effective use of different abilities of heterogeneous to improve efficiency. We also show two failed cases in Fig. \ref{fig:failcase}, where the agent places the misplaced object in an unreasonable location or agents fail to detect all the misplaced objects.


\subsection{Ablation Experiments}
To evaluate the role and effectiveness of different modules of our model, we conduct ablation experiments by removing the specific part of the model and comparing the performance. \textit{Ours w/o Know.} removes the commonsense prior knowledge, which judge the misplaced objects and infer the reasonable locations directly from visual features. \textit{Ours w/o MisObjDec.} removes the misplaced object detector, which extracts visual features and relationship features from the current observation as the subsequent input of the model. \textit{Ours w/o ReaRecPre.} removes the reasonable receptacle predictor and directly extracts the receptacle features with the linear layer to input to the subsequent model to generate next actions. We remove the communication module to obtain \textit{Ours w/o Comm.}, in which agents do not communicate with others and each agent performs actions independently. \textit{Ours w/o HierDec.} removes the hierarchical decision, and agents do not predict the executing sub-tasks and sub-goals, but directly generate low-level actions with their state features. The results of the ablation experiments for \textit{Single} and \textit{Cross} tasks in \textit{Setting I}, \textit{Setting II} in Table \ref{abla1} and Table \ref{abla2} respectively. 


%Since \textit{Ours w/o MisObjDec.} does not explicitly detect whether the object is in the unreasonable location, $\%FM$ drops. \textit{Ours w/o ReaRecPre.} does not explicitly generate the reasoned receptacle, so the success rate decreases. Because the misplaced object detector is retained in this model, its $\%FM$ is higher than \textit{Ours w/o MisObjDec.}. 

The performance of \textit{Ours w/o Know.} drops obviously, indicating that the commonsense knowledge is quite important for scene reasoning in this task. Since the misplaced object detector and the reasonable receptacle predictor can provide a clear target object to be picked up and a clear receptacle to put the object for subsequent sub-tasks generating, the performance of \textit{Ours w/o MisObjDec.} and \textit{Ours w/o ReaRecPre.} would decrease. In \textit{Ours w/o Comm.}, each agent has no communication with others, since in \textit{Setting I} only one agent has the manipulation capability, the performance is the same as \textit{SA} in \textit{Setting I}. \textit{Ours w/o HierDec.} directly generates the low-level actions with the detected misplaced object, the reasoned receptacle, and the state features. Because it is difficult to directly learn the relations between state features and executed low-level actions, this model would generate some invalid actions for the executing task, which leads to worse performance. Since the misplaced object detector and reasonable receptacle predictor are retained in \textit{Ours w/o HierDec.}, its $\%FM$ is still higher than that of \textit{Ours w/o MisObjDec.} and \textit{Ours w/o ReaRecPre.}. The results demonstrate that each module of the proposed model is effective for heterogeneous multi-agent task completion.


\section{Conclusion}\label{sec:conclusion}

In this paper, we propose the heterogeneous multi-agent collaborative framework based on the handshake-based group communication strategy and hierarchical decision model. To evaluate the effectiveness of the framework, we propose the heterogeneous multi-agent tidying-up task and generate a benchmark dataset for this task in houses with multiple rooms. The results demonstrate the effectiveness of each module of our model. In the future, we will continue to study the communication mechanism in more complex tasks and larger multi-agent systems with both homogeneous and heterogeneous agents.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%% Figure environment removed

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%% Figure environment removed
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

This work was supported in part by the National Natural Science Foundation of China under Grants 62025304.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


%{\small
%  \bibliographystyle{IEEEtran}
%  \bibliography{TPami_heterogeneous_collaboration}
%}

\bibliographystyle{IEEEtran}
\bibliography{main}


% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{comment}
\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

\end{comment}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


