\documentclass[10pt]{article}

% Required for cross references (careful when changing the name of the file)
% \usepackage{xr-hyper}
\usepackage[colorlinks,citecolor=blue]{hyperref}
% \externaldocument{pairs_supplement1}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage[pdftex]{graphicx}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage{natbib,setspace,enumerate,mathtools}
\allowdisplaybreaks
\usepackage[toc,title,titletoc,header]{appendix}
\usepackage{etoolbox} % This package goes here to add \qed to remarks automatically.
\usepackage{booktabs}
\usepackage{threeparttable}
% \usepackage{footnote} % for footnote
\usepackage{adjustbox}
% \usepackage{showlabels} \usepackage[T1]{fontenc}
\usepackage{multirow}
\bibliographystyle{ims}

\def\qed{\rule{2mm}{2mm}}
\def\indep{\perp \!\!\! \perp}
    
\parskip = 1.5ex plus 0.5 ex minus0.2 ex

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{algorithm}{Algorithm}[section]

\AtEndEnvironment{remark}{~\qed}
\AtEndEnvironment{example}{~\qed}

\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\cov}{Cov}
\DeclareMathOperator*{\diag}{diag}

\renewcommand{\baselinestretch}{1.35}

\renewcommand*{\proofname}{\textsc{Proof}}
\newcommand{\cas}{\stackrel{a.s.}{\rightarrow}}
\newcommand{\cp}{\stackrel{P}{\rightarrow}}
\newcommand{\cd}{\stackrel{d}{\rightarrow}}
\newcommand{\cld}{\stackrel{L_d}{\rightarrow}}

\begin{document}

\author{
Yuehao Bai \\
Department of Economics\\
University of Southern California \\
\url{yuehao.bai@usc.edu}
\and
Hongchang Guo \\
Department of Economics\\
Northwestern University\\
\url{hongchangguo2028@u.northwestern.edu}
\and
Azeem M.\ Shaikh\\
Department of Economics\\
University of Chicago \\
\url{amshaikh@uchicago.edu}
\and
Max Tabord-Meehan\\
Department of Economics\\
University of Chicago \\
\url{maxtm@uchicago.edu}
}

\bigskip

\title{Inference in Experiments with Matched Pairs \\ and Imperfect Compliance\thanks{We thank Alex Torgovitsky for helpful discussions. The fourth author acknowledges support from NSF grant SES-2149408.}}

\maketitle

\vspace{-0.3in}

\begin{spacing}{1.2}
\begin{abstract}
This paper studies inference for the local average treatment effect in randomized controlled trials with imperfect compliance where treatment status is determined according to ``matched pairs.''   By ``matched pairs,'' we mean that units are sampled i.i.d.\ from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment.  Under weak assumptions governing the quality of the pairings, we first derive the limiting behavior of the usual Wald (i.e., two-stage least squares) estimator of the local average treatment effect.  We show further that the conventional heteroskedasticity-robust estimator of its limiting variance is generally conservative in that its limit in probability is (typically strictly) larger than the limiting variance.  We therefore provide an alternative estimator of the limiting variance that is consistent for the desired quantity.  Finally, we consider the use of additional observed, baseline covariates not used in pairing units to increase the precision with which we can estimate the local average treatment effect.  To this end, we derive the limiting behavior of a two-stage least squares estimator of the local average treatment effect which includes both the additional covariates in addition to pair fixed effects, and show that the limiting variance is always less than or equal to that of the Wald estimator.  To complete our analysis, we provide a consistent estimator of this limiting variance.  A simulation study confirms the practical relevance of our theoretical results.  We use our results to revisit a prominent experiment studying the effect of macroinsurance on microenterprise in Egypt.


% It follows immediately that conventional tests about the local average treatment effect based on these quantities are conservative in that the limiting rejection probability is less than the nominal level.  We provide an alternative estimator of the limiting variance that is consistent and therefore leads to tests that are exact in the sense that the limiting rejection probability under the null hypothesis equals the nominal level.  Finally, we show how 
\end{abstract}
\end{spacing}

\noindent KEYWORDS: Matched pairs, randomized controlled trial, experiments, noncompliance, imperfect compliance

\noindent JEL classification codes: C12, C14

\thispagestyle{empty} 
\newpage
\setcounter{page}{1}

\section{Introduction}

This paper studies inference for the local average treatment effect in randomized controlled trials with imperfect compliance where treatment status is determined according to  ``matched pairs.'' By ``matched pairs,'' we mean that units are sampled i.i.d.\ from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. This method is used routinely in all parts of sciences. Indeed, commands to facilitate its implementation are included in popular software packages, such as \texttt{sampsi} in Stata. References to a variety of specific examples can be found, for instance, in the following surveys of various field experiments: \cite{glennerster2014running}, \cite{riach2002field}, \cite{rosenberger2015randomization}. See also \cite{bruhn2009pursuit}, who, based on a survey of selected development economists, report that 56\% of researchers have used such a design at some point.  Furthermore, in many such experiments, compliance may be imperfect:  some recent examples of experiments featuring both mathched pairs and imperfect compliance are  \cite{groh2016macroinsurance} and \cite{resnjanskij2021can}.  Under weak assumptions that ensure pairs are formed so that units within pairs are suitably ``close'' in terms of observed, baseline covariates, we derive a variety of results pertaining to inference about the local average treatment effect in such experiments.  % \textcolor{magenta}{Maybe not needed, but could work in ref to athey and imbens above for saying that imperfect compliance is common}

We first study the behavior of the usual Wald (i.e., two-stage least squares) estimator of the local average treatment effect.  When all observed, baseline covariates are used in forming pairs, we find that the estimator is efficient among all estimators for the local average treatment effect in the sense that it achieves the lower bound on the limiting variance developed in \cite{bai2023efficient} over a broad class of treatment assignment schemes that hold the marginal probability of treatment assignment equal to one half, thereby including matched pairs, in particular, as a special case.  On the other hand, we find that the conventional heteroskedasticity-robust estimator of its limiting variance is conservative in that its limit in probability is always weakly larger than the limiting variance and strictly larger unless heterogeneity is constrained in a particular fashion.  We provide an alternative estimator of the limiting variance and show that it is consistent for the desired quantity.  In a simulation study, we find that tests using the Wald estimator together with the conventional heteroskedasticity-robust estimator of its limiting variance may as a consequence have very poor power when compared to tests using the Wald estimator together with our estimator of its limiting variance. 

Next, we consider situations in which there are additional, observed, baseline covariates that are not used when pairing units.  We first derive the limiting behavior for a class of covariate-adjusted estimators indexed by different ``working models'' for conditional expectations.  Importantly, these working models need not be correctly specified for the true conditional expectations in order for this estimator to remain consistent for the local average treatment effect, but we show that the limiting variance of the estimator is minimized, in particular, when they are correctly specified.  These results are derived under a high-level assumption governing the quality of the estimators of these working models.  We therefore specialize our results for the case of linear working models and characterize the optimal linear working model in the sense of minimizing the limiting variance of the resulting covariate-adjusted estimator among all such working models.  We find that this limiting variance can be obtained by a familiar estimator: the two-stage least squares estimator using treatment assignment as an instrument of the coefficient on treatment status in a linear regression of the outcome on the following quantities: treatment status; (functions of) the observed baseline covariates; and pair fixed effects. We complete our analysis by providing an estimator of the limiting variance and show that it is consistent for the desired quantity.

Our paper builds upon the analysis of \cite{bai2022mp}, who analyzed the behavior of the difference-in-means estimator of the average treatment effect in context of experiments with matched pairs and perfect compliance.  Our covariate-adjusted estimator is inspired by the analysis in \cite{bai2023covariate}, who studied the use of additional, observed, baseline covariates in experiments with matched pairs and perfect compliance to improve the precision with which we can estimate the average treatment effect \citep[similar results have also been obtained by][]{cytrynbaum2023covariate}.  We emphasize, however, that none of the aforementioned papers permit imperfect compliance, which, as argued by \cite{athey2017econometrics}, is one of the most common complications in even the most well designed experiments.  We note, however, that imperfect compliance has been studied in the context of randomized controlled trials with other treatment assignment schemes, such as stratified block randomization: see, for example, \cite{ansel2018ols}, \cite{bugni2021inference}, and \cite{jiang2022improving}. We also emphasize that all of these papers, like ours, carry out their analysis in a superpopulation sampling framework.  In this way, our analysis differs from the analysis of experiments with a finite population sampling framework.  

%\textcolor{magenta}{Should we say most of this literature is not about matched pairs and list the few exceptions?}

The remainder of the paper is organized as follows.  In Section \ref{sec:setup} we describe our setup and notation.  In particular, there we describe the precise sense in which we require that units in each pair are ``close'' in terms of their baseline covariates. Our main results concerning the Wald estimator are contained in Section \ref{sec:main}.  In Section \ref{sec:adjustment}, we develop our results pertaining to our covariate-adjusted estimator that exploits additional observed, baseline covariates not used in pairing units to increase the precision with which we can estimate the local average treatment effect.  In Section \ref{sec:sims}, we examine the practical relevance of our theoretical results via a small simulation study. In Section \ref{sec:empirical-application}, we provide a brief empirical illustration of our proposed tests using data from an experiment in \cite{groh2016macroinsurance}.  Finally, we conclude in Section \ref{sec:recs} with some recommendations for empirical practice guided by both our theoretical results and our simulation study.  As explained further in that section, we do not recommend the use of the Wald estimator with the conventional heteroskedasticity-robust estimator of its limiting variance because it is conservative in the sense described above; we instead encourage the use of the Wald estimator with our consistent estimator of its limiting variance because it is asymptotically exact, and, as a result, considerably more powerful.  When there are additional, observed, baseline covariates that are not used when forming pairs, we recommend the use of our covariate-adjusted Wald estimator with our consistent estimator of its limiting variance. Proofs of all results are provided in the  Appendix.


% \newpage

% \cite{bai2022mp} solved fundamental questions about the inference of the average treatment effects under the main assumption that pairs are formed such that units within pairs are ``close'' to each other in terms of the baseline covariates.

%Imperfect compliance is a common issue in RCT, see \cite{angrist2009effects}, \cite{beuermann2015one}, \cite{romero2020outsourcing}, among many others. When there is imperfect compliance issue, two parameters of interests are discussed: intent-to-treat effects (ITT) and local average treatment effects (LATE). For ITT, since we only care about the effect of treatment assignment rather than the effect of treatment status, estimation and inference can be done as if there was perfect compliance. For LATE, which is the average treatment effects for individuals who comply with the treatment treatment, identification and estimation of LATE are solved in \cite{angristimbens1994late}, and inference can be done using robust standard errors. However, for individual-level ``matched pairs'' design, when there is noncompliance issue, how to do inference on LATE is unsolved, and existing empirical papers used robust standard errors for hypothesis testing and construction of confidence interval, see \cite{groh2016macroinsurance} and \cite{resnjanskij2021can}. In this paper, we derived the limiting variance of IV estimator without covariates under ``matched pairs'' design with noncompliance issue, and showed that it achieved the efficiency bound provided by \cite{frolich2007nonparametric} and \cite{hong2010semiparametric}. We also argued that under ``matched pairs'' design with noncompliance issue, traditional robust standard errors are conservative, and we provide a standard error that leads to asymptotically exact tests in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. We refer to this standard error as the ``matched-pair IV'' standard error.


% The remainder of the paper is organized as follows. In Section \ref{sec:setup}, we describe our setup and notation. Our main results concerning the ``matched-pair IV'' standard error are contained in Section \ref{sec:main}. In Section \ref{sec:sims}, we examine the finite-sample behavior of these tests via a small simulation study. In Section \ref{sec:empirical-application}, we provide a brief empirical application of our proposed standard error using data from an experiment in \cite{groh2016macroinsurance}.



\section{Setup and Notation} \label{sec:setup}

Let $Y_{i} \in \mathbf{R}$ denote the (observed) outcome of the $i$th unit, $A_i \in \{0, 1\}$ be an indicator for whether or not unit $i$ is assigned to treatment, $D_i \in \{0, 1 \}$ be an indicator for whether or not unit $i$ decides to take up treatment, $X_i \in \mathbf{R}^{k_x}$ denote observed, baseline covariates for the $i$th unit which are used for matching, and $W_i \in \mathbf{R}^{k_w}$ denote observed, baseline covariates for the $i$th unit which will be used when we consider covariate adjustment in Section \ref{sec:adjustment}. In contrast to the setting considered in \cite{bai2022mp}, we allow for imperfect compliance, i.e. for $D_i \ne A_i$. Further denote by $Y_i(d)$ the potential outcome of the $i$th unit if they make treatment decision $d \in \{0, 1\}$, and by $D_i(a)$ the potential treatment decision of the $i$th unit if assigned to treatment $a \in \{0, 1\}$. The observed treatment decision and potential treatment decision are related to treatment assignment via the usual relationship 
\begin{equation}\label{eq:obsD}
    D_i = D_i(1)A_i + D_i(0)(1 - A_i)~,
\end{equation}
and the observed outcome and potential outcome are related to treatment decision via the relationship
\begin{equation}\label{eq:obsY}
    Y_i = Y_i(1)D_i + Y_i(0)(1 - D_i)~.
\end{equation}

\noindent We will also often make use of the following alternative representation for the observed outcome, which is numerically equivalent to (\ref{eq:obsY}):
\begin{equation}\label{eq:obsY'}
    Y_i = \tilde Y_i(1)A_i + \tilde Y_i(0)(1 - A_i)~,
\end{equation}
where
\begin{equation}\label{eq:TildeY}
    \tilde Y_i(a) = Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a))
\end{equation}
for $a \in \{0, 1\}$. In words, $\tilde Y_i(a)$ represents the ``intent-to-treat" potential outcome for unit $i$ when assigned to treatment $a \in \{0, 1\}$. 

Following \cite{angristimbens1994late}, each participant in the experiment can be categorized into one of four types: units for which $D_i(1) = 1$ and $D_i(0) = 0$ are referred to as compliers, units for which $D_i(1) = 1$ and $D_i(0) = 1$ are referred to as always takers, units for which $D_i(1) = 0$ and $D_i(0) = 0$ are referred to as never takers, and finally units for which $D_i(1) = 0$ and $D_i(0) = 1$ are referred to as defiers.  We use the notation
\begin{equation}\label{eq:compliers}
    C_i = I\left\{ D_i(1) = 1, D_i(0) = 0 \right\}
\end{equation}
below to indicate whether or not unit $i$ is a complier.

For a random variable indexed by $i$, say for example $A_i$, it will be useful to denote by $A^{(n)}$ the random vector $(A_1, ..., A_{2n})$. Denote by $P_n$ the distribution of the observed data $(Y^{(n)}, D^{(n)}, A^{(n)}, X^{(n)}, W^{(n)})$, and by $Q_n$ the distribution of $(Y^{(n)}(1), Y^{(n)}(0), D^{(n)}(1), D^{(n)}(0), X^{(n)}, W^{(n)})$. Note that $P_n$ is jointly determined by (\ref{eq:obsD}), (\ref{eq:obsY}), $Q_n$, and the mechanism for determining treatment assignment. We assume throughout that our sample consists of $2n$ i.i.d.\ observations, so that $Q_n = Q^{2n}$, where $Q$ is the marginal distribution of $(Y_i(1), Y_i(0), D_i(1), D_i(0), X_i, W_i)$. We therefore state our assumptions below in terms of assumptions on $Q$ and the mechanism for determining treatment assignment. Indeed, we will not make reference to $P_n$ in the sequel and all operations are understood to be under $Q$ and the mechanism for determining treatment assignment.

Our object of interest is the local average treatment effect, which may be expressed in our notation as
\begin{equation} \label{eq:LATE}
    \Delta(Q) = E\left[ Y_i(1) - Y_i(0) | C_i = 1 \right].
\end{equation}

For a pre-specified choice of $\Delta_0$, the testing problem of interest is 
\begin{equation}\label{eq:H0}
    H_0: \Delta(Q) = \Delta_0\  \text{versus}\  H_1: \Delta(Q) \neq \Delta_0
\end{equation}
at level $\alpha \in (0, 1)$.

We begin by describing our primary assumptions on the data generating process.
% \begin{assumption}\label{ass:Q'}
%     The distribution $Q$ is such that \vspace{-.25cm}
%     \begin{itemize}
%         %\item[(a)] $\{(Y_i(1), Y_i(0), D_i(1), D_i(0), X_i)\}_{i=1}^{2n}$ are i.i.d. observations from distribution $Q$. 
%         \item[(a)] $E\left[ Y_i(d)^2 \right] < \infty$ for $d = 0, 1$.
%         \item[(b)] $E[ Y_i(1)^r D_i(a) | X_i = x ]$ and $E[ Y_i(0)^r (1 - D_i(a)) | X_i = x]$ are Lipschitz for $a = 0, 1$ and $r = 0, 1, 2$.
%         \item[(c)] $P\left\{ D_i(1) \geq D_i(0) \right\} = 1$.
%         % (d) impleies this (d)': \item[(d)'] $E[ Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a)) | X_i ]$, $\var( Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a)) | X_i )$, $\var( D_i(a) | X_i )$, and $\cov( Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a)), D_i(a) | X_i )$ are Lipschitz for $a = 0, 1$.
%         \item[(d)] $P \{C_i = 1\} > 0$.
%         %\item[(f)] $Y_i(d, a) = Y_i(d)$ (Exclusion Restriction) \textcolor{red}{[check the proof later]}
%     \end{itemize}
% \end{assumption}

\begin{assumption}\label{ass:Q}
    The distribution $Q$ is such that \vspace{-.25cm}
    \begin{enumerate}[\rm (a)]
        \item $0 < E\left[ \var\left[ \tilde{Y}_i(a) - \Delta(Q)D_i(a) | X_i \right] \right]$ for $a \in \{0, 1  \}$.
        \item $E\left[ Y_i(a)^2 \right] < \infty$ for $a \in \{0, 1 \}$. 
        \item $E[ Y_i(1)^r D_i(a) | X_i = x ]$ and $E[ Y_i(0)^r (1 - D_i(a)) | X_i = x]$ are Lipschitz for $a = 0, 1$ and $r = 0, 1, 2$.
        \item $P\{D_i(1) \geq D_i(0)\} = 1$.
        % (d) impleies this (d)': \item[(d)'] $E[ Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a)) | X_i ]$, $\var( Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a)) | X_i )$, $\var( D_i(a) | X_i )$, and $\cov( Y_i(1)D_i(a) + Y_i(0)(1 - D_i(a)), D_i(a) | X_i )$ are Lipschitz for $a = 0, 1$.
        \item $P \{C_i = 1\} > 0$.
        %\item[(f)] $Y_i(d, a) = Y_i(d)$ (Exclusion Restriction) \textcolor{red}{[check the proof later]}
    \end{enumerate}
\end{assumption}
%\textcolor{red}{[Assumption \ref{ass:Q}-(b) assumes that compliance status are similar on average for units with ``close'' covariates for constructing pairs. Will that help infer which units are compliers? Confidence set for compliers?]}

 Assumption \ref{ass:Q}(a)--(b) are mild restrictions imposed to rule out degenerate situations and to permit the application of suitable laws of large numbers and central limit theorems. Assumption \ref{ass:Q}(c) is a smoothness requirement that ensures that units that are ``close" in terms of their baseline covariates are suitably comparable. Similar smoothness requirements are also considered in \cite{bai2022mp}, \cite{cytrynbaum2023covariate}, and generally play a key role in establishing the asymptotic exactness of our proposed tests. Assumptions \ref{ass:Q}(d)--(e) are the standard ``monotonicity" and ``relevance" conditions of \cite{angristimbens1994late} which ensure that the probability limit of the Wald estimator which we define in Section \ref{sec:main} can be interpreted as the local average treatment effect $\Delta(Q)$.

Next, we describe our assumptions on the mechanism determining treatment assignment. Following the notation in \cite{bai2022mp}, the $n$ pairs can be represented by the sets
\begin{align*}
    \{ \pi(2j - 1), \pi(2j)\} \ \text{for}\ j = 1, ..., n~,
\end{align*}
where $\pi = \pi_n\left(X^{(n)}\right)$ is a permutation of $2n$ elements. Given such a $\pi$, we assume that treatment status is assigned as described in the following assumption:
\begin{assumption}\label{ass:assignment}
    Treatment status is assigned so that $\left( Y^{(n)}(1), Y^{(n)}(0), D^{(n)}(1), D^{(n)}(0), W^{(n)} \right) \indep A^{(n)}|X^{(n)}$, and conditional on $X^{(n)}$, $(A_{\pi(2j-1)}, A_{\pi(2j)})$, $j = 1, ..., n$ are i.i.d.\ and each uniformly distributed over the values in $\{(0, 1), (1, 0)\}$.
\end{assumption}

We further require that the unts in each pair be ``close" in terms of their baseline covariates in the following sense:
% \textcolor{red}{[This assumption borrows directly from \cite{bai2022mp}. One question: does the part ``conditional on $X^{(n)}$, $(A_{\pi(2j-1)}, A_{\pi(2j)})$, $j = 1, ..., n$ are i.i.d. and each uniformly distributed over the values in $\{(0, 1), (1, 0)\}$'' implies the part ``treatment status is assigned so that $\left( Y^{(n)}(1), Y^{(n)}(0), D^{(n)}(1), D^{(n)}(0) \right) \indep A^{(n)}|X^{(n)}$''? If so, the first part could be deleted.] //[Answer: No! Because the first part only assumes the dependence of $A^{(n)}$, and said nothing about the relationship among $Y(a)^{(n)}$, $D(a)^{(n)}$, and $A^{(n)}$.]}

% \textcolor{red}{[It seems that ``conditional on $X^{(n)}$'' could be replaced by ``conditional on $\pi = \pi_n(X^{(n)})$'', because given an ordering $\pi$, the treatment assignment of pairs is just iid of uniformly distributed $\{(0, 1), (1, 0) \}$. Not a big issue, because ``conditional on $X^{(n)}$'' implies ``conditional on $\pi = \pi_n(X^{(n)})$''.]}

\begin{assumption}\label{ass:pairs}
    The pairs used in determining treatment status satisfy 
    \begin{align*}
        \frac{1}{n} \sum_{1\leq j\leq n} \| X_{\pi(2j)} - X_{\pi(2j-1)} \|_2^r \cp 0~,
    \end{align*}
    for $r\in\{1, 2\}$.
\end{assumption}

We will also sometimes require that the distances between units in adjacent pairs be ``close" in terms of their baseline covariates:
\begin{assumption}\label{ass:pairsofpairs}
    The pairs used in determining treatment status satisfy
    \begin{align*}
        \frac{1}{n}\sum_{1\leq j\leq \lfloor \frac{n}{2} \rfloor} \| X_{\pi(4j-k)} - X_{\pi(4j-l)} \|_2^2 \cp 0~,
    \end{align*}
    for $k\in\{ 2,3\}$ and $l\in \{ 0,1\}$.
\end{assumption}

\cite{bai2022mp} and \cite{bai2023inference} provide several examples of pairing algorithms which satisfy Assumptions \ref{ass:pairs}--\ref{ass:pairsofpairs}. The simplest such example is when $X_i \in \mathbf{R}$, in which case we can order units from smallest to largest according to $X_i$ and pair adjacent units. It then follows from Theorem 4.1 in \cite{bai2022mp} that Assumptions \ref{ass:pairs}--\ref{ass:pairsofpairs} are satisfied as long as $E[X_i^2] < \infty$.


\section{Main Results}\label{sec:main}

\subsection{Asymptotic Behavior of the Wald Estimator}\label{subsec:MPIV}
In this section, we study the asymptotic properties of the standard Wald estimator (i.e., the two-stage least squares estimator of $Y_i$ on $D_i$ using $A_i$ as an instrument) of $\Delta(Q)$ under a matched pairs design. In order to introduce this estimator, define
\begin{equation}\label{eq:meanYa}
    \begin{split}
        \hat{\psi}_n(a) = \frac{1}{n} \sum_{1\leq i\leq 2n: A_i = a} Y_i~,
    \end{split}
\end{equation}
\begin{equation}\label{eq:meanDa}
    \begin{split}
        \hat{\phi}_n(a) = \frac{1}{n} \sum_{1\leq i\leq 2n: A_i = a} D_i~.
    \end{split}
\end{equation}
Using this notation, the Wald estimator is defined as
\begin{equation}\label{eq:hatDelta}
    \begin{split}
        \hat{\Delta}_n = \frac{\hat{\psi}_n(1) - \hat{\psi}_n(0)}{\hat{\phi}_n(1) - \hat{\phi}_n(0)}~.
    \end{split}
\end{equation}
Note that this estimator may be obtained as the ratio of the estimator of the coefficient of $A_i$ in an ordinary least squares regression of $Y_i$ on a constant and $A_i$ (the ``reduced-form") to the estimator of the coefficient of $A_i$ in an ordinary least squares regression of $D_i$ on a constant and $A_i$ (the ``first-stage"). Theorem \ref{theorem:main} establishes the limiting distribution of $\hat{\Delta}_n$ under a matched pairs design. 
\begin{theorem}\label{theorem:main}
Suppose $Q$ satisfies Assumption \ref{ass:Q} and the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairs}.  Then,
\begin{align*}
    \sqrt{n} \left( 
    \hat{\Delta}_n
    -
    \Delta(Q)
    \right) \cd 
    N\left(0, \nu^2   \right),
\end{align*}
where
\begin{multline*}
    \nu^2 = \frac{1}{P \{C_i = 1\}^2}\Biggr (\var[Y^\ast_i(1)] + \var[Y^\ast_i(0)] \\
    - \frac{1}{2}E\left[((E[Y^\ast_i(1)|X_i] - E[Y^\ast_i(1)]) + (E[Y^\ast_i(0)|X_i] - E[Y^\ast_i(0)]))^2\right]\Biggr )~,
\end{multline*}
with
\begin{equation}\label{eq:Ystar-a}
    Y^\ast_i(a) = \tilde Y_i(a) - \Delta(Q)D_i(a)~,
\end{equation}
for $a \in \{0, 1\}$.

% \begin{align*}
%    \nu^2  &= 
%    \frac{1}{P \{C_i = 1\}^2} \left( \Sigma_{11} - 2 \Delta(Q)\Sigma_{21} + \Delta(Q)^2\Sigma_{22} \right),  \\
%    \Sigma_{11} &= 
%    E\left[ \var\left[ \tilde Y_i(1) |X_i \right] \right] + E\left[ \var\left[ \tilde Y_i(0) |X_i \right]  \right] + \frac{1}{2} \var\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i \right] \right], \\
%    \Sigma_{22} &= E[\var[D_i(1)|X_i]] + E[\var[D_i(0)|X_i]] + \frac{1}{2} \var\left[ E\left[ D_i(1) - D_i(0) | X_i \right]  \right], \\
%    \Sigma_{21} 
%     &= 
%     E\left[ E\left[Y_i(1)D_i(1)|X_i\right] - E\left[\tilde Y_i(1)|X_i\right] E\left[D_i(1)|X_i\right]\right] + E\left[ E\left[Y_i(1)D_i(0)|X_i\right] - E\left[\tilde Y_i(0)|X_i\right] E\left[D_i(0)|X_i\right]\right] + \\
%     &\ \ \ \ \frac{1}{2} \left( E\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i \right] E\left[ D_i(1) - D_i(0) | X_i \right] \right] - E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right] E\left[ D_i(1) - D_i(0) \right] \right) \\
%     &= E\left[ \cov\left( \tilde Y_i(1), D_i(1) \Bigg| X_i \right) \right] + E\left[ \cov\left( \tilde Y_i(0), D_i(0) \Bigg| X_i \right) \right] + \\
%     &\ \ \ \ \frac{1}{2} E\left[ \cov\left( \tilde Y_i(1) - \tilde Y_i(0), D_i(1) - D_i(0) \Bigg| X_i \right) \right] \\
%    &= \Sigma_{12}. \\
%    &= E\left[ \tilde Y_i(1) D_i(1) \right] + E\left[ \tilde Y_i(0) D_i(0) \right] - \frac{1}{2} E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right] E\left[ D_i(1) - D_i(0) \right]  \\
%    &\ \ \ \ \ 
%    - \frac{1}{2} 
%    E\left[ E\left[ \tilde Y_i(1) | X_i \right] E\left[ D_i(1) | X_i \right] \right]- 
%    \frac{1}{2} 
%    E\left[ E\left[ \tilde Y_i(0) | X_i \right] E\left[ D_i(0) | X_i \right] \right]  \\
%    &\ \ \ \ \ 
%    - \frac{1}{2} 
%    E\left[ E\left[ \tilde Y_i(1) | X_i \right] E\left[ D_i(0) | X_i \right] \right] - 
%    \frac{1}{2} 
%    E\left[ E\left[ \tilde Y_i(0) | X_i \right] E\left[ D_i(1) | X_i \right] \right]. 
%     &\textcolor{red}{\text{[Have checked that it degenerates to $\nu^2$ for adjusted t-test in \cite{bai2022mp} when there is perfect compliance]}} 
%     \\
%     &= E\left[ Y_i(1) D_i(1) \right] + E\left[ Y_i(1) D_i(0) \right] - \frac{1}{2} E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right] E\left[ D_i(1) - D_i(0) \right]
%     \\
%     &\ \ \ \ \ 
%     - \frac{1}{2} E\left[ 
%     \left( E\left[ \tilde Y_i(1) +  \tilde Y_i(0) | X_i \right] \right)
%     \left( E\left[ D_i(1) +  D_i(0) | X_i \right] \right)
%     \right]
% \end{align*}
\end{theorem}

Note that the expression for $\nu^2$ corresponds exactly to the limiting variance obtained in \cite{bai2022mp} with the usual potential outcomes $Y_i(a)$ replaced with the transformed outcomes $Y_i^\ast(a)$.  In particular, when there is perfect compliance, so that $D_i = A_i$, $P \{C_i = 1\} = 1$, and $D_i(a) = a$, the limiting variance we obtain in Theorem \ref{theorem:main} corresponds exactly to the limiting variance derived in \cite{bai2022mp}.  Moreover, it can be shown that our expression for $\nu^2$ attains the efficiency bound derived in \cite{bai2023efficient} over a broad class of treatment assignments which include matched pairs as a special case \citep[it can also be shown that this bound coincides with the bounds derived in][in settings  with observational data with i.i.d assignment when $P\{A_i = a|X_i\} = 1/2$, where the local average treatment effect is estimated non-parametrically using the covariates $X_i$: see Lemma \ref{lemma:efficiency-bound} for details.]{frolich2007nonparametric, hong2010semiparametric} 

\begin{remark}\label{rem:TSLS}
It can be shown that under our matched pairs design, the two-stage least squares estimator of $\beta_1$ in the linear regression
\[Y_i = \beta_0 + \beta_1D_i + \gamma'X_i + \epsilon_i~,\]
with $A_i$ used as an instrument for $D_i$, is asymptotically equivalent to the Wald estimator $\hat{\Delta}_n$. This mirrors similar observations made in Remark 3.8 in \cite{bai2022mp} for the case of perfect compliance.
\end{remark}


%\begin{remark}
%    The limiting variance $\nu^2$ provided in Theorem \ref{theorem:main} achieves the efficiency bound provided in \cite{frolich2007nonparametric} and \cite{hong2010semiparametric}. See Lemma \ref{lemma:efficiency-bound} for the proof.
%\end{remark}

\subsection{Variance Estimation}

% \textcolor{magenta}{Not important, but we sometimes say limiting and sometimes say asymptotic.  Should we try to be consistent?}

In this section, we construct a consistent variance estimator for the limiting variance $\nu^2$, and then contrast this to the asymptotic behavior of standard regression-based variance estimators. As noted in the discussion following Theorem \ref{theorem:main}, the expression for $\nu^2$ corresponds exactly to the limiting variance obtained in \cite{bai2022mp} with the usual potential outcomes $Y_i(a)$ replaced with the transformed outcomes $Y_i^\ast(a)$. We thus follow the variance construction from \cite{bai2022mp}, but with a feasible version of $Y_i^\ast(a)$ defined as
\[\hat{Y}_i = Y_i - \hat{\Delta}_nD_i~.\]
This strategy leads to the following variance estimator:
\begin{equation} \label{eq:mpiv-varest}
\hat{\nu}^2_n = \frac{\hat{\tau}^2_n - \frac{1}{2}(\hat{\lambda}^2_n + \hat{\Gamma}^2_n)}{\left(\hat{\phi}_n(1) - \hat{\phi}_n(0)\right)^2}~,   
\end{equation}
where
\begin{align*}
    \hat{\tau}^2_n & = \frac{1}{n}\sum_{1 \le j \le n}(\hat{Y}_{\pi(2j)} - \hat{Y}_{\pi(2j-1)})^2 \\
    \hat{\lambda}^2_n & = \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor}\left(\hat{Y}_{\pi(4j-3)} - \hat{Y}_{\pi(4j-2)}\right)\left(\hat{Y}_{\pi(4j-1)} - \hat{Y}_{\pi(4j)}\right)\left(A_{\pi(4j-3)} - A_{\pi(4j-2)}\right)\left(A_{\pi(4j-1)} - A_{\pi(4j)}\right) \\
    \hat{\Gamma}_n & = \frac{1}{n}\sum_{1 \le i \le 2n: A_i = 1}\hat{Y}_i - \frac{1}{n}\sum_{1 \le i \le 2n: A_i = 0}\hat{Y}_i~.
\end{align*}
% \[\hat{\tau}^2_n = \frac{1}{n}\sum_{1 \le j \le n}(\hat{Y}_{\pi(2j)} - \hat{Y}_{\pi(2j-1)})^2~,\]
% \[\hat{\lambda}^2_n = \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor}\left(\hat{Y}_{\pi(4j-3)} - \hat{Y}_{\pi(4j-2)}\right)\left(\hat{Y}_{\pi(4j-1)} - \hat{Y}_{\pi(4j)}\right)\left(A_{\pi(4j-3)} - A_{\pi(4j-2)}\right)\left(A_{\pi(4j-1)} - A_{\pi(4j)}\right)~,\]
% \[\hat{\Gamma}_n = \frac{1}{n}\sum_{1 \le i \le n: A_i = 1}\hat{Y}_i - \frac{1}{n}\sum_{1 \le i \le n: A_i = 0}\hat{Y}_i~.\]
Note that the construction of the numerator of $\hat{\nu}^2_n$ can be motivated using a similar intuition to what has been previously discussed in \cite{bai2022mp}: to consistently estimate
\[ E[(E[Y^\ast_i(1)|X_i] - E[Y^\ast_i(0)|X_i])^2]~, \]
ideally we would like access to four different units with similar values of $X_i$, of which two are treated. However, because each pair only contains two units, we need to average across ``pairs of pairs" of units, where two pairs are grouped together so that they are ``close'' in terms of $X_i$. We establish the following consistency result for $\hat{\nu}^2_n$:

\begin{theorem}\label{theorem:varianceestimation}
Suppose $Q$ satisfies Assumption \ref{ass:Q} and the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairsofpairs}. Then,
    \begin{align*}
        \hat{\nu}_n^2 \cp \nu^2.
    \end{align*}
\end{theorem}

%\begin{remark}
%    Following similar arguments to \cite{bai2022mp}, $\hat \nu_n^2$ is non-negative.
%\end{remark}

We thus immediately obtain the following corollary which establishes the asymptotic exactness of a $t$-test for the null hypothesis \eqref{eq:H0} constructed using the variance estimator $\hat{\nu}^2_n$:
\begin{corollary}\label{cor:exact_test}
Suppose $Q$ satisfies Assumption \ref{ass:Q} and the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairsofpairs}. Then,
     \begin{align*}
        \frac{\sqrt{n} \left( \hat{\Delta}_n - \Delta(Q) \right)}{\hat \nu_n} \cd N\left( 0, 1\right)~.
    \end{align*}
\end{corollary}


%\subsection{Comparison with Robust Standard Error}\label{subsec:robust}
Next, we consider the limiting behavior of the usual heteroskedasticity-robust variance estimator from a standard two-stage least squares regression, which is commonly used in practice: see \cite{groh2016macroinsurance}, and \cite{resnjanskij2021can} for examples. Let $\hat U_i$ denote the $i$th residual generated by the two-stage least squares  estimator in a linear regression of $Y_i$ on a constant and $D_i$ using $A_i$ as an instrument for $D_i$. The heteroskedasticity-robust variance estimator is defined as
\begin{equation} \label{eq:robust}
\hat \omega_n^2 = \left ( n \left ( \sum_{1 \leq i \leq 2n} \begin{pmatrix}
    1 \\ A_i
\end{pmatrix}
\begin{pmatrix}
    1 & D_i
\end{pmatrix} \right )^{-1} \sum_{1 \leq i \leq 2n} \hat U_i^2 \begin{pmatrix}
    1 \\ A_i
\end{pmatrix}
\begin{pmatrix}
    1 & A_i
\end{pmatrix} 
\left ( \sum_{1 \leq i \leq 2n} \begin{pmatrix}
    1 \\ D_i
\end{pmatrix}
\begin{pmatrix}
    1 & A_i
\end{pmatrix} \right )^{-1}
\right )_{2, 2}~,
\end{equation}
% \begin{align*}
%     \hat{\nu}_{r} = \sqrt{\left[ n \left( \mathbb{A}' \mathbb{D} \right)^{-1} \mathbb{A}' \diag\left( \hat{U_i}^2 \right) \mathbb{A} \left( \left( \mathbb{A}' \mathbb{D} \right)^{-1}\right)' \right]_{2, 2}}, 
% \end{align*}
where the notation $(\cdot)_{2,2}$ denotes the $(2,2)$-element of its (matrix) argument.
% i.e. $\hat \omega^2_n$ is the second diagonal component of the corresponding $2\times2$ matrix. 
Theorem \ref{theorem:robust} derives the limit in probability of $\hat \omega_n^2$.

\begin{theorem}\label{theorem:robust}
    Suppose $Q$ satisfies Assumption \ref{ass:Q} and the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairs}.  Then,
    \begin{align*}
        \hat{\omega}_{n}^2 \cp \omega^2~,
    \end{align*}
    where 
    \[\omega^2 = \frac{1}{P \{C_i = 1\}^2} (\var[Y_i^\ast(1)] + \var[Y_i^\ast(0)])~.\]
\end{theorem}

We thus immediately obtain the following corollary which establishes the \emph{conservativness} of a $t$-test constructed using the variance estimator $\hat\omega_n^2$ for the null hypothesis \eqref{eq:H0}.

\begin{corollary}
     Suppose $Q$ satisfies Assumption \ref{ass:Q} and the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairs}.  Then,
     \begin{align*}
        \frac{\sqrt{n} \left( \hat{\Delta}_n - \Delta(Q) \right)}{\hat \omega_n} \cd N\left( 0, \xi^2 \right), 
    \end{align*}
    where $\xi^2 \leq 1$ and the inequality is strict unless
    \[E[\tilde Y_i(1) - \Delta(Q)D_i(1) + \tilde Y_i(0) - \Delta(Q)D_i(0)|X_i] = E[\tilde Y_i(1) - \Delta(Q)D_i(1) + \tilde Y_i(0) - \Delta(Q)D_i(0)]\] with probability one under $Q$.
\end{corollary}

%e conclude that a $t$-test constructed using the robust variance estimator from a standard two-stage least squares regression is generally conservative for testing the null hypothesis \eqref{eq:H0}, unless the covariates are irrelevant in the sense that 


% \subsection{``Matched Pairs'' t-Test}

% \textcolor{red}{[What is the ``Matched Pairs'' $t$-Test with noncompliance analogous to that in \cite{bai2022mp}? What did empirical papers do other than ``robust standard error''? It would be more meaningful for theoretical discussion of ``Matched Pairs'' $t$-Test with noncompliance if there are empirical papers using it. Keep searching for empirical papers!]}

%\subsection{Variance Estimation}\label{subsec:varianceestimation}
%\textcolor{red}{[In simulations, the estimated variance could be negative.... Not desirable!]}

%\begin{theorem}\label{theorem:varianceestimation}
%    Suppose Assumptions \ref{ass:Q}, \ref{ass:assignment}, \ref{ass:pairs}, and \ref{ass:pairsofpairs} are %satisfied, then
%    \begin{align*}
%        \hat{\nu}_n^2 \cp \nu^2,
%    \end{align*}
%    where
%    \begin{align*}
%        \hat{\nu}_n^2 &= \frac{\hat{\Sigma}_{11} - 2 \hat{\Delta}_n \hat{\Sigma}_{21} + \hat{\Delta}_n^2  %\hat{\Sigma}_{22}}{\left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right)^2}, \\
%        \hat{\Sigma}_{11} &= \hat{\tau}_{11n}^2 - \frac{1}{2} \left( \hat{\lambda}_{11n}^2  + \left( \hat{\psi}_n(1) - \hat{\psi}_n(0) \right)^2 \right), \\
%        \hat{\Sigma}_{22} &= \hat{\tau}_{22n}^2 - \frac{1}{2} \left( \hat{\lambda}_{22n}^2  + \left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right)^2 \right), \\ 
%        \hat{\Sigma}_{21} &= \hat{\zeta}_n - \frac{1}{2} \left( \hat{\psi}_n(1) - \hat{\psi}_n(0) \right) \left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right) - \frac{1}{2}\hat{\eta}_{11n} - \frac{1}{2}\hat{\eta}_{10n} - \frac{1}{2}\hat{\eta}_{01n} - \frac{1}{2}\hat{\eta}_{00n},\\
%        \hat{\tau}_{11n}^2 &= \frac{1}{n} \sum_{1\leq j\leq n} \left( Y_{\pi(2j)} - Y_{\pi(2j-1)} \right)^2, \\
 %       \hat{\lambda}_{11n}^2 &= \frac{2}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2}\rfloor}  (Y_{\pi(4j-3)} - Y_{\pi(4j-2)}) (Y_{\pi(4j-1)} - Y_{\pi(4j)}) (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}), \\
%        \hat{\tau}_{22n}^2 &= \frac{1}{n} \sum_{1\leq j\leq n} \left( D_{\pi(2j)} - D_{\pi(2j-1)} \right)^2, \\
        %\hat{\lambda}_{22n}^2 &= \frac{2}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2}\rfloor} (D_{\pi(4j-3)} - %D_{\pi(4j-2)}) (D_{\pi(4j-1)} - D_{\pi(4j)}) (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - %A_{\pi(4j)}), \\
        %\hat{\eta}_{10n} &= \frac{1}{n} \sum_{1\leq j\leq n} \left( Y_{\pi(2j-1)} A_{\pi(2j-1)} + Y_{\pi(2j)} %A_{\pi(2j)} \right) \left( D_{\pi(2j-1)} \left(1 - A_{\pi(2j-1)}\right) + D_{\pi(2j)} \left( 1 - %A_{\pi(2j)} \right) \right), \\
        %\hat{\eta}_{01n} &= \frac{1}{n} \sum_{1\leq j\leq n} \left( Y_{\pi(2j-1)} \left(1 - A_{\pi(2j-%1)}\right) + Y_{\pi(2j)} \left(1 - A_{\pi(2j)}\right) \right) \left(D_{\pi(2j-1)} A_{\pi(2j-1)} + %D_{\pi(2j)} A_{\pi(2j)}\right), \\
        %\hat{\eta}_{11n} &= 
        %\frac{1}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2} \rfloor} \left[ \left( Y_{\pi(4j-3)}A_{\pi(4j-3)} + %Y_{\pi(4j-2)} A_{\pi(4j-2)} \right) \left( D_{\pi(4j-1)} A_{\pi(4j-1)} + D_{\pi(4j)} A_{\pi(4j)} \right) + \right. \\
        %&\ \ \ \ \ \ \ \ \ \ \ \ \ \left. \left( Y_{\pi(4j-1)} A_{\pi(4j-1)} + Y_{\pi(4j)} A_{\pi(4j)} \right) \left( D_{\pi(4j-3)}A_{\pi(4j-3)} + D_{\pi(4j-2)} A_{\pi(4j-2)} \right) \right], \\
        %\hat{\eta}_{00n} &=
        %\frac{1}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2} \rfloor} \left[ \left( Y_{\pi(4j-3)} (1- A_{\pi(4j-3)}) + Y_{\pi(4j-2)} (1 - A_{\pi(4j-2)}) \right) \left( D_{\pi(4j-1)} (1 - A_{\pi(4j-1)}) + D_{\pi(4j)} (1 - A_{\pi(4j)}) \right) + \right. \\
        %&\ \ \ \ \ \ \ \ \ \ \ \left. \left( Y_{\pi(4j-1)} (1 - A_{\pi(4j-1)}) + Y_{\pi(4j)} (1 - A_{\pi(4j)}) \right) \left( D_{\pi(4j-3)} (1 - A_{\pi(4j-3)}) + D_{\pi(4j-2)} (1 - A_{\pi(4j-2)}) \right) \right], \\
        %\hat{\zeta}_n &= \frac{1}{n} \sum_{1\leq i \leq 2n} Y_i D_i.
    %\end{align*}
%\end{theorem}

%\begin{theorem}\label{theorem:varianceestimation'}
%    Suppose Assumptions \ref{ass:Q}, \ref{ass:assignment}, \ref{ass:pairs}, and \ref{ass:pairsofpairs} are satisfied, then
%    \begin{align*}
%        \hat{\nu}_n^2 \cp \nu^2,
%    \end{align*}
%    where
%    \begin{align*}
 %       \hat{\nu}_n^2 &= \frac{\hat{\Sigma}_{11} - 2 \hat{\Delta}_n \hat{\Sigma}_{21} + \hat{\Delta}_n^2  \hat{\Sigma}_{22}}{\left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right)^2}, \\
 %       \hat{\Sigma}_{11} &= \hat{\tau}_{11n}^2 - \frac{1}{2} \left( \hat{\lambda}_{11n}^2  + \left( \hat{\psi}_n(1) - \hat{\psi}_n(0) \right)^2 \right), \\
 %       \hat{\Sigma}_{22} &= \hat{\tau}_{22n}^2 - \frac{1}{2} \left( \hat{\lambda}_{22n}^2  + \left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right)^2 \right), \\ 
 %       \hat{\Sigma}_{21} &= \hat{\zeta}_n - \frac{1}{2} \left( \hat{\psi}_n(1) - \hat{\psi}_n(0) \right) \left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right) - \frac{1}{2}\hat{\eta}_n,\\
 %       \hat{\tau}_{11n}^2 &= \frac{1}{n} \sum_{1\leq j\leq n} \left( Y_{\pi(2j)} - Y_{\pi(2j-1)} \right)^2, \\
 %       \hat{\lambda}_{11n}^2 &= \frac{2}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2}\rfloor}  (Y_{\pi(4j-3)} - Y_{\pi(4j-2)}) (Y_{\pi(4j-1)} - Y_{\pi(4j)}) (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}), \\
 %       \hat{\tau}_{22n}^2 &= \frac{1}{n} \sum_{1\leq j\leq n} \left( D_{\pi(2j)} - D_{\pi(2j-1)} \right)^2, \\
 %       \hat{\lambda}_{22n}^2 &= \frac{2}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2}\rfloor} (D_{\pi(4j-3)} - D_{\pi(4j-2)}) (D_{\pi(4j-1)} - D_{\pi(4j)}) (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}), \\
 %       \hat{\eta}_n &= \frac{1}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2} \rfloor} \left[ \left( Y_{\pi(4j-3)} + Y_{\pi(4j-2)} \right) \left( D_{\pi(4j-1)} + D_{\pi(4j)} \right) + \left( Y_{\pi(4j-1)} + Y_{\pi(4j)} \right) \left( D_{\pi(4j-3)} + D_{\pi(4j-2)} \right) \right], \\
 %       \hat{\zeta}_n &= \frac{1}{n} \sum_{1\leq i \leq 2n} Y_i D_i.
 %   \end{align*}
%\end{theorem}


% \section{Randomization Test}
% Finite sample results is not obtained. 

% What is the relationship between finite sample results and large sample results? Does impossibility of finite sample results imply impossibility of large sample results?


\section{Covariate Adjustment} \label{sec:adjustment}
In this section, we consider a generalization of the estimator $\hat \Delta_n$ defined in Section \ref{subsec:MPIV} that allows for covariate adjustment using the additional, observed, baseline covariates $W^{(n)}$ that are not used when forming pairs. We then show how a careful application of this covariate-adjusted estimator can ensure an improvement over the unadjusted Wald estimator $\hat{\Delta}_n$ in terms of precision. 

Following \cite{bai2023covariate}, note that it can be shown under Assumption \ref{ass:assignment} that for any $a \in \{0, 1 \}$, $m_{a, \tilde Y}: \mathbb{R}^{k_x} \times \mathbb{R}^{k_w} \to \mathbb{R}$, and $m_{a, D}: \mathbb{R}^{k_x} \times \mathbb{R}^{k_w} \to \mathbb{R}$ such that $E[|m_{a, \tilde Y}(X_i, W_i)|] < \infty$, $E[|m_{a, D}(X_i, W_i)|] < \infty$,
\begin{equation}\label{eq:moment-tildeY}
    E[ 2 I\left\{ A_i = a \right\} ( Y_i - m_{a, \tilde Y}( X_i, W_i ) ) + m_{a, \tilde Y}\left( X_i, W_i \right) ] = E[ \tilde Y_i(a) ],
\end{equation}
\begin{equation}\label{eq:moment-D}
    E\left[ 2 I\left\{ A_i = a \right\} \left( D_i - m_{a, D}\left( X_i, W_i \right) \right) + m_{a, D}\left( X_i, W_i \right) \right] = E\left[ D_i(a) \right].
\end{equation}
Here, we view $m_{a, D}$ and $m_{a, \tilde Y}$ as ``working models'' of $E[\tilde{Y}_i(a)|X_i,W_i]$ and $E[D_i(a)|X_i,W_i]$, respectively, but we emphasize that \eqref{eq:moment-tildeY}--\eqref{eq:moment-D} hold even if they are incorrectly specified.
% Let $\hat{\psi}_n^{\text{adj}}(a)$ denote the sample analog of the left hand side of (\ref{eq:moment-tildeY}), and $\hat{\phi}_n^{\text{adj}}(a)$ denote the sample analog of the left hand side of (\ref{eq:moment-D}).
This observation suggests a covariate-adjusted estimator defined as
\[ \hat \Delta_n^{\rm adj} = \frac{\hat \psi_n^{\rm adj}(1) - \hat \psi_n^{\rm adj}(0)}{\hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0)}~, \]
where
\begin{align*}
    \hat \psi_n^{\rm adj}(a) & = \frac{1}{2n} \sum_{1 \leq i \leq 2n} (2 I \{A_i = a\} (Y_i - \hat m_{a, \tilde Y}(X_i, W_i)) + \hat m_{a, \tilde Y}(X_i, W_i))~, \\
    \hat \phi_n^{\rm adj}(a) & = \frac{1}{2n} \sum_{1 \leq i \leq 2n} (2 I \{A_i = a\} (D_i - \hat m_{a, D}(X_i, W_i)) + \hat m_{a, D}(X_i, W_i))~,
\end{align*}
and $\hat m_{a, \tilde Y}$ and $\hat m_{a, D}$ are suitable estimators of $m_{a, \tilde Y}$ and $m_{a, D}$. Note that if we set $\hat m_{a,\tilde{Y}} = \hat m_{a, D} = 0$, then $\hat{\Delta}_n^{\rm adj}$ simplifies to $\hat{\Delta}_n$. Theorem \ref{theorem:adjustment} below establishes the limiting distribution of $\hat{\Delta}_n^{\rm adj}$ for a matched-pairs design under the following high-level assumption on the working models:

% \begin{assumption}\label{ass:assignment-adj}
%     Treatment status is assigned so that $\left( Y^{(n)}(1), Y^{(n)}(0), D^{(n)}(1), D^{(n)}(0), W^{(n)} \right) \indep A^{(n)}|X^{(n)}$, and conditional on $X^{(n)}$, $(A_{\pi(2j-1)}, A_{\pi(2j)})$, $j = 1, ..., n$ are i.i.d. and each uniformly distributed over the values in $\{(0, 1), (1, 0)\}$.
% \end{assumption}

\begin{assumption}\label{ass:m}
    Let $m_{a, \tilde Y D} = m_{a, \tilde Y} - \Delta(Q) m_{a, D}$. The functions $m_{a, \tilde Y D}$ for $a \in \{0, 1 \}$ satisfy \vspace{-.25cm}
    \begin{enumerate}[\rm (a)]
        % \item[(a)] For $a \in \{0, 1 \}$,
        % \begin{align*}
        %     \liminf_{n\to \infty} E\left[ \var\left[ Y_i^\ast(a) - \frac{1}{2} (m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) ) \Bigg| X_i \right] \right] > 0~.
        % \end{align*}

        \item For $a \in \{0, 1\}$,
        \begin{align*}
            \lim_{\lambda\to \infty} E\left[m_{a, \tilde Y D}(X_i, W_i)^2 I\left\{ |m_{a, \tilde Y D}(X_i, W_i)| > \lambda \right\} \right] = 0~.
        \end{align*}

        \item $E[m_{a, \tilde Y D}(X_i, W_i) | X_i = x]$, $E[(m_{a, \tilde Y D}(X_i, W_i))^2 | X_i = x]$, and $E[m_{a, \tilde Y D}(X_i, W_i) Y_i^\ast(a) | X_i = x]$ for $d \in \{0, 1 \}$, and $E[m_{1, \tilde Y D}(X_i, W_i) m_{0, \tilde Y D}(X_i, W_i) | X_i = x]$ are Lipschitz.
    \end{enumerate}
\end{assumption}
\noindent Before proceeding, we note that we later provide low-level sufficient conditions for this high-level assumption for the special case in which $m_{a,\tilde{Y}}$ and $m_{a,D}$ correspond to linear working models that are optimal in the sense of minimizing the limiting variance of $\hat \Delta_n^{\rm adj}$ among all linear working models.

\begin{theorem}\label{theorem:adjustment}
    Suppose $Q$ satisfies Assumption \ref{ass:Q}, the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairs}, and $m_{a, \tilde Y}$ and $m_{a, D}$ for $a\in \{0, 1 \}$ satisfy Assumption \ref{ass:m}. 
    % Let $\hat m^\ast_{a, \tilde Y D} = \hat m_{a, \tilde Y} - \Delta(Q) \hat m_{a, D}$. Further suppose that $\hat m^\ast_{a, \tilde Y D}$ satisfies
    % \begin{equation}\label{eq:m-hat-close}
    %     \frac{1}{\sqrt{2n}} \sum_{1\leq i\leq 2n} (2A_i - 1) (\hat m^\ast_{a, \tilde Y D}(X_i, W_i) - m_{a, \tilde Y D}(X_i, W_i)) \cp 0~.
    % \end{equation}
    Further suppose $\hat m_{a, \tilde Y}$ and $\hat m_{a, D}$ satisfy that 
    \begin{equation}\label{eq:m-Y-hat-close}
        \frac{1}{\sqrt{2n}} \sum_{1\leq i\leq 2n} (2A_i - 1) (\hat m_{a, \tilde Y}(X_i, W_i) - m_{a, \tilde Y}(X_i, W_i)) \cp 0~,
    \end{equation}
    \begin{equation}\label{eq:m-D-close}
        \frac{1}{\sqrt{2n}} \sum_{1\leq i\leq 2n} (2A_i - 1) (\hat m_{a, D}(X_i, W_i) - m_{a, D}(X_i, W_i)) \cp 0~.
    \end{equation}
    Then, $\hat \Delta_n^{\rm adj}$ satisfies
\begin{align*}
    \sqrt{n} \left( \hat \Delta_n^{\rm adj}
    -
    \Delta(Q)
    \right) \cd 
    N\left(0, \nu_{\rm adj}^2  \right),
\end{align*}
where $\nu_{\rm adj}^2 = \frac{1}{P \{C_i = 1\}^2} ( \nu_{1, \rm adj}^2 + \nu_{2, \rm adj}^2 + \nu_{3, \rm adj}^2 )$ with
\begin{align*}
    \nu_{1, \rm adj}^2 
    &= \frac{1}{2} E[ \var[ E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] 
    - ( m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) ) | X_i ] ]~, \\
    \nu_{2, \rm adj}^2 
    &= \frac{1}{2} \var[ E[ Y^\ast_i(1) - Y^\ast_i(0) | X_i, W_i] ]~, \\
    \nu_{3, \rm adj}^2
    &= E[ \var[ Y^\ast_i(1) | X_i, W_i ] + \var[ Y^\ast_i(0) | X_i, W_i ] ]~.
\end{align*}
\end{theorem}

Note that the expression for $\nu_{\rm adj}^2$  corresponds exactly to the limiting variance obtained in \cite{bai2023covariate} with the usual potential outcomes $Y_i(a)$ replaced with the transformed outcomes $Y_i^\ast(a)$ and the working models in \cite{bai2023covariate} replaced with $m_{a, \tilde Y}(X_i, W_i) - \Delta(Q) m_{a, D}(X_i, W_i)$. In general, $\nu^2_{\rm adj}$ is not guaranteed to be weakly smaller than $\nu^2$ for all choices of working models, but we note that $\nu_{\rm adj}^2$ is minimized when $\nu_{1, \rm adj}^2 = 0$, i.e., when the working models satisfy
\begin{multline*}
    E[Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i] - E[Y_i^\ast(1) + Y_i^\ast(0) | X_i] \\
     = m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) - E[m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) | X_i]
\end{multline*}
with probability one. This property holds, in particular, when $m_{a,\tilde{Y}}$ and $m_{a,D}$ are correctly specified.

We now specialize Theorem \ref{theorem:adjustment} to the case in which $m_{a,\tilde{Y}}$ and $m_{a,D}$ are the optimal linear working models in a sense to be described below. To that end, let $\zeta_i = \zeta(X_i, W_i)$ be a user-specified transformation of the baseline characteristics $(X_i, W_i)$ given by some function $\zeta: \mathbb R^{k_x} \times \mathbb R^{k_w} \to \mathbb R^p$.  Denote by $\hat \beta_n^Y$ and $\hat \beta_n^D$ the ordinary least squares estimators of $\beta^Y$ and $\beta^D$ in the following two linear regressions with pair fixed effects:
\begin{align}
    \label{eq:pfe-Y} Y_i & = \alpha^Y A_i + \zeta_i' \beta^Y + \sum_{1 \leq j \leq n} \theta_j^Y I \{i \in \{\pi(2j - 1), \pi(2j)\}\} + \epsilon_i^Y \\
    \label{eq:pfe-D} D_i & = \alpha^D A_i + \zeta_i' \beta^D + \sum_{1 \leq j \leq n} \theta_j^D I \{i \in \{\pi(2j - 1), \pi(2j)\}\} + \epsilon_i^D
\end{align}
Using this notation, consider $\hat{\Delta}^{\rm adj}_n$ with $\hat m_{a, \tilde Y}(X_i, W_i) = \zeta_i' \hat \beta_n^Y$ and $\hat m_{a, D}(X_i, W_i) = \zeta_i' \hat \beta_n^D$ for $a \in \{0, 1\}$.  In order to analyze the large-sample behavior of this estimator, we introduce the following assumption:
\begin{assumption} \label{ass:adjustment}
    The function $\zeta(\cdot)$ is such that
    \begin{enumerate}[\rm (a)]
        \item No component of $\zeta_i$ is a constant and $E[\var[\zeta_i | X_i]]$ is nonsingular.
        \item $\var[\zeta_i] < \infty$.
        \item $E[\zeta_i | X_i = x]$, $E[\zeta_i \zeta_i' | X_i = x]$, $E[\zeta_i \tilde Y_i(a) | X_i = x]$, $E[\zeta_i D_i(a) | X_i = x]$ are Lipschitz.
    \end{enumerate}
\end{assumption}
Theorem \ref{thm:zeta} shows that Assumption \ref{ass:adjustment} provides low-level sufficient conditions for Assumption \ref{ass:m}.  In this way, it establishes the large-sample behavior of $\hat{\Delta}_n^{\rm adj}$ for the special case of these working models and further verifies their optimality among all linear working models:

\begin{theorem} \label{thm:zeta}
Suppose $Q$ satisfies Assumption \ref{ass:Q}, the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairs}, and in addition Assumption \ref{ass:adjustment} is satisfied. Then, 
    \begin{align*}
        \hat \beta_n^Y & \stackrel{P}{\to} \beta^{\tilde Y} = (2 E[\var[\zeta_i | X_i]])^{-1} E[\cov[\zeta_i, \tilde Y_i(1) + \tilde Y_i(0) | X_i]] \\
        \hat \beta_n^D & \stackrel{P}{\to} \beta^D = (2 E[\var[\zeta_i | X_i]])^{-1} E[\cov[\zeta_i, D_i(1) + D_i(0) | X_i]]~.
    \end{align*}
    In addition, \eqref{eq:m-Y-hat-close}--\eqref{eq:m-D-close} and Assumption \ref{ass:m} are satisfied for $\hat m_{a, \tilde Y}(X_i, W_i) = \zeta_i' \hat \beta_n^Y$, $\hat m_{a, D}(X_i, W_i) = \zeta_i' \hat \beta_n^D$, $m_{1, \tilde Y}(X_i, W_i) = m_{0, \tilde Y}(X_i, W_i) = \zeta_i' \beta^{\tilde Y}$, and $m_{1, \tilde D}(X_i, W_i) = m_{0, \tilde D}(X_i, W_i) = \zeta_i' \beta^D$. Moreover, $\hat \Delta_n^{\rm adj}$ is optimal in the sense of minimizing $\nu_{\rm adj}^2$ among all choices of $m_{a,D}$ and $m_{a,\tilde Y}$ that are linear in $\zeta_i$.
\end{theorem}

%\begin{remark}
%    Note that when we include covariates linearly in addition to pair fixed effects, the proposed estimator $\hat \Delta_n^{\rm adj}$ may be obtained as the ratio of the estimator of the coefficient of $A_i$ in a OLS regression of $Y_i$ on a constant, $A_i$, covariates used in pairing, linear additional covariates, and pair fixed effects, divided by the estimator of the coefficient of $A_i$ in a OLS regression of $Y_i$ on a constant, $A_i$, covariates used in pairing, linear additional covariates, and pair fixed effects. 
%\end{remark}

\begin{remark}\label{rem:FE_TSLS}
Here, we present two equivalent expressions for $\hat{\Delta}^{\rm adj}_n$ when $\hat m_{a, \tilde Y}(X_i, W_i) = \zeta_i' \hat \beta_n^Y$ and $\hat m_{a, D}(X_i, W_i) = \zeta_i' \hat \beta_n^D$.  Denote by $\hat \alpha_n^Y$ and $\hat \alpha_n^D$ the ordinary least squares estimators of $\alpha^Y$ and $\alpha^D$ in \eqref{eq:pfe-Y}--\eqref{eq:pfe-D}. The Frisch-Waugh-Lovell theorem implies that 
\begin{eqnarray*}
\hat \alpha_n^Y &=&  \hat \psi_n^{\rm adj}(1) - \hat \psi_n^{\rm adj}(0) \\
\hat \alpha_n^D &=& \hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0)~.
\end{eqnarray*}
It thus follows immediately that
\[ \hat \Delta_n^{\rm adj} = \frac{\hat \alpha_n^Y}{\hat \alpha_n^D}~. \]
Next, denote by $\hat{\alpha}_n^{\rm IV}$ estimator of $\alpha$ in the following linear regression
\begin{equation} \label{eq:pfe-iv}
    Y_i = \alpha D_i + \zeta_i' \beta + \sum_{1 \leq j \leq n} \theta_j I \{i \in \{\pi(2j - 1), \pi(2j)\}\} + \epsilon_i~.
\end{equation}
obtained by applying two-stage least squares with $A_i$ as an instrument for $D_i$.  It can be shown that $$\hat \Delta_n^{\rm adj} = \hat{\alpha}_n^{\rm IV}~.$$  See Appendix \ref{sec:FE_TSLS} for details. We emphasize, however, that the usual heteroskedasticity-robust estimator of the variance of $\hat{\alpha}_n^{\rm IV}$ is not consistent for the limiting variance derived in Theorem \ref{theorem:adjustment}; \cite{bai2023inference} provide a related discussion of when such an estimator may even be invalid. We therefore construct a consistent estimator of the required variance in Theorem \ref{theorem:varianceestimation-adj} below.
\end{remark}








% \begin{align*}
%     \tilde \psi_n^{\rm adj}(1) - \tilde \psi_n^{\rm adj}(0) &= \frac{1}{n} \sum_{1 \leq i \leq 2n} A_i \phi_{1, n, i}^{\rm adj} - \frac{1}{n} \sum_{1 \leq i \leq 2n} (1 - A_i) \phi_{0, n, i}^{\rm adj} + o_P(n^{-1/2}) \\
%     \phi_{a, n, i}^{\rm adj} &= Y_i^\ast(a) - \frac{1}{2} ( (m_1^{\tilde Y}(X_i, W_i) - \Delta m_{1, D}(X_i, W_i) ) + (m_0^{\tilde Y}(X_i, W_i) - \Delta m_{0, D}(X_i, W_i) ) )
% \end{align*}

% \begin{align*}
%     \sigma_{n, \rm adj}^2(Q) &= \sigma_{1, n, \rm adj}^2(Q) + \sigma_{2, \rm adj}^2(Q) + \sigma_{3, \rm adj}^2(Q)  \\
%     \sigma_{1, n, \rm adj}^2(Q) &= \frac{1}{2} E\left[ \var\left[ E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] - ( (m_1^{\tilde Y}(X_i, W_i) - \Delta m_{1, D}(X_i, W_i) ) + (m_0^{\tilde Y}(X_i, W_i) - \Delta m_{0, D}(X_i, W_i) ) ) \Bigg| X_i \right] \right]
% \end{align*}

% For linear adjustments, let $m_{a, \tilde Y}(X_i, W_i) = \psi_i' \beta^{\tilde Y}(a)$, $m_{a, D}(X_i, W_i) = \psi_i' \beta^{D}(a)$, then we have
% \begin{align*}
%     \sigma_{n, \rm adj}^2(Q) &= \sigma_{1, n, \rm adj}^2(Q) + \sigma_{2, \rm adj}^2(Q) + \sigma_{3, \rm adj}^2(Q)  \\
%     \sigma_{1, n, \rm adj}^2(Q) &= \frac{1}{2} E\left[ \var\left[ E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] -  \psi_i' ( (\beta^{\tilde Y}(1) + \beta^{\tilde Y}(0) ) - \Delta (\beta^D(1) + \beta^D(0) ) )  \Bigg| X_i \right] \right]
% \end{align*}

% Minimizing $\sigma_{1, n, \rm adj}^2(Q)$:
% \begin{align*}
%     (\beta^{\tilde Y}(1) + \beta^{\tilde Y}(0) ) - \Delta (\beta^D(1) + \beta^D(0) ) = (E[\var[\psi_i | X_i]])^{-1} E[ \cov(Y^\ast_i(1) + Y^\ast_i(0), \psi_i | X_i) ] 
%     = 2 (\beta^{\rm pfe}_{\tilde Y} + \beta^{\rm pfe}_{D})
% \end{align*}



% Then by Theorem 3.1 in \cite{bai2023covariate}, we have
% \begin{align*}
%     \frac{\sqrt{n}\left( \hat{\psi}_n^{\text{adj}}(1) - \hat{\psi}_n^{\text{adj}}(0) - E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right]\right)}{\sigma_{\tilde Y}(Q)} &\cd N(0, 1), \\
%     \frac{\sqrt{n}\left( \hat{\phi}_n^{\text{adj}}(1) - \hat{\phi}_n^{\text{adj}}(0) - E\left[ D_i(1) - D_i(0) \right]\right)}{\sigma_{D}(Q)} &\cd N(0, 1),
% \end{align*}
% where $\sigma_{\tilde Y}(Q)$ and $\sigma_{D}(Q)$ are of the form as in Theorem 3.1 in \cite{bai2023covariate}.

% All we need to do is to do the multivariate version of Theorem 3.1 in \cite{bai2023covariate} and use Delta method to get the limiting variance.


% \begin{theorem}\label{theorem:adjustment}
%     Suppose Assumptions ??? are satisfied \textcolor{red}{[Assumptions can be adopted from \cite{bai2023covariate} with minor revisions. Will write these up if we have decided to include this part]}, then
% \begin{align*}
%     \sqrt{n} \left( 
%     \frac{\hat{\psi}_n^{\text{adj}}(1) - \hat{\psi}_n^{\text{adj}}(0)}{\hat{\phi}_n^{\text{adj}}(1) - \hat{\phi}_n^{\text{adj}}(0)}
%     -
%     \Delta(Q)
%     \right) \cd 
%     N\left(0, \nu_{\text{adj}}^2   \right),
% \end{align*}
% where
% \begin{align*}
%     \nu_{\text{adj}}^2  &= 
%     \frac{1}{P \{C_i = 1\}^2} \left( \Sigma_{11}^{\text{adj}} - 2 \Delta(Q)\Sigma_{21}^{\text{adj}} + \Delta(Q)^2\Sigma_{22}^{\text{adj}} \right),  \\
%     \Sigma_{11}^{adj} 
%     % &= 
%     % E\left[ \var\left( \psi_{1, n, i} |X_i \right)  \right] + E\left[ \var\left( \psi_{0, n, i} |X_i \right)  \right] + \frac{1}{2} \var\left( E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i \right] \right) \\
%     &= \frac{1}{2} E\left[ \var\left[ E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i, W_i \right] - \left( m_1^{\tilde Y}(X_i, W_i) +  m_0^{\tilde Y}(X_i, W_i) \right) \Bigg| X_i\right] \right] + \\
%     &\ \ \ \ \frac{1}{2} \var\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i, W_i  \right] \right] + E\left[ \var\left( \tilde Y_i(1) | X_i, W_i  \right) + \var\left( \tilde Y_i(0) | X_i, W_i  \right)  \right] \\
%     &= \frac{1}{2} E\left[ \var\left[ E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i, W_i  \right] \Bigg| X_i \right] \right] + \frac{1}{2} E\left[ \var\left[ \left( m_1^{\tilde Y}(X_i, W_i) +  m_0^{\tilde Y}(X_i, W_i) \right) \Bigg| X_i \right] \right] - \\
%     &\ \ \ \ E\left[ \cov\left( E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i, W_i  \right], \left( m_1^{\tilde Y}(X_i, W_i) +  m_0^{\tilde Y}(X_i, W_i) \right) \Bigg| X_i\right) \right] + \\
%     &\ \ \ \ \frac{1}{2} \var\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i, W_i  \right] \right] + E\left[ \var\left( \tilde Y_i(1) | X_i, W_i  \right) + \var\left( \tilde Y_i(0) | X_i, W_i  \right)  \right], \\
%     \Sigma_{22}^{adj} 
%     % &= E[\var(\phi_{1, n, i}|X_i)] + E[\var(\phi_{0, n, i}|X_i)] + \frac{1}{2} \var\left( E\left[ D_i(1) - D_i(0) | X_i \right]  \right) \\
%     &= \frac{1}{2} E\left[ \var\left[ E\left[ D_i(1) + D_i(0) | X_i, W_i \right] - \left( m_1^{D}(X_i, W_i) +  m_0^{D}(X_i, W_i) \right) \Bigg| X_i\right] \right] + \\
%     &\ \ \ \ \frac{1}{2} \var\left[ E\left[ D_i(1) - D_i(0) | X_i, W_i  \right] \right] + E\left[ \var\left[ D_i(1) | X_i, W_i  \right] + \var\left[ D_i(0) | X_i, W_i  \right]  \right] \\
%     &= \frac{1}{2} E\left[ \var\left[ E\left[ D_i(1) + D_i(0) | X_i, W_i  \right] \Bigg| X_i \right] \right] + \frac{1}{2} E\left[ \var\left[ \left( m_1^{D}(X_i, W_i) +  m_0^{D}(X_i, W_i) \right) \Bigg| X_i \right] \right] - \\
%     &\ \ \ \ E\left[ \cov\left( E\left[ D_i(1) + D_i(0) | X_i, W_i  \right], \left( m_1^{D}(X_i, W_i) +  m_0^{D}(X_i, W_i) \right) \Bigg| X_i\right) \right] + \\
%     &\ \ \ \ \frac{1}{2} \var\left[ E\left[ D_i(1) - D_i(0) | X_i, W_i  \right] \right] + E\left[ \var\left( D_i(1) | X_i, W_i  \right) + \var\left( D_i(0) | X_i, W_i  \right)  \right], \\
%     \Sigma_{21}^{adj} 
%     % &= 
%     % E\left[ E\left[\psi_{1, n, i}\phi_{1, n, i}|X_i\right] - E\left[\psi_{1, n, i}|X_i\right] E\left[\phi_{1, n, i}|X_i\right]\right] + E\left[ E\left[\psi_{0, n, i}\phi_{0, n, i}|X_i\right] - E\left[\psi_{0, n, i}|X_i\right] E\left[\phi_{0, n, i}|X_i\right]\right] + \\
%     % &\ \ \ \ \frac{1}{2} \left( E\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i \right] E\left[ D_i(1) - D_i(0) | X_i \right] \right] - E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right] E\left[ D_i(1) - D_i(0) \right] \right) \\
%     &= \frac{1}{2} E\left[ \left( m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) \right) \left( m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) \right) - \right. \\
%     &\ \ \ \ \ \ \ \ \ \left. \left( \tilde Y_i(1) + \tilde Y_i(0) \right) \left( m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) \right) - \right. \\
%     &\ \ \ \ \ \ \ \ \ \left. \left( D_i(1) + D_i(0) \right) \left( m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) \right) - \right. \\
%     &\ \ \ \ \ \ \ \ \ \left. E\left[ m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) | X_i \right] E\left[ m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) | X_i \right]  + \right. \\
%     &\ \ \ \ \ \ \ \ \ \left. E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i \right] E\left[ m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) | X_i \right] +  \right. \\
%     &\ \ \ \ \ \ \ \ \ \left. E\left[ D_i(1) + D_i(0) | X_i \right] E\left[ m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) | X_i \right] - \right.\\
%     &\ \ \ \ \ \ \ \ \ \left. E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i \right] E\left[ D_i(1) + D_i(0) | X_i \right] + \right. \\
%     &\ \ \ \ \ \ \ \ \ \left. E\left[ \tilde Y_i(1) + \tilde Y_i(0)  \right] E\left[ D_i(1) + D_i(0)  \right] \right] \\
%     &= \frac{1}{2} E\left[ \cov\left( \left( m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) \right), \left( m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) \right) \Bigg| X_i \right) \right] - \\
%     &\ \ \ \ \frac{1}{2} E\left[ \cov\left( \left( \tilde Y_i(1) + \tilde Y_i(0) \right), \left( m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) \right) \Bigg| X_i \right) \right] - \\
%     &\ \ \ \ \frac{1}{2} E\left[ \cov\left( \left( D_i(1) + D_i(0) \right), \left( m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) \right) \Bigg| X_i \right) \right] - \\
%     &\ \ \ \ \frac{1}{2}  \cov\left( E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i \right],  E\left[ D_i(1) + D_i(0) | X_i \right] \right) 
%     % \psi_{a, n, i} &= \tilde Y_i(a) - \frac{1}{2} \left( m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) \right), \\
%     % \phi_{a, n, i} &= D_i(a) - \frac{1}{2} \left( m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) \right).
% \end{align*}
% \end{theorem}

% \begin{remark}
%     If we suppose that
%     \begin{align*}
%         m_1^{\tilde Y}(X_i, W_i) + m_0^{\tilde Y}(X_i, W_i) &= E\left[ \tilde Y_i(1) + \tilde Y_i(0) | X_i, W_i  \right], \\
%         m_1^{D}(X_i, W_i) + m_0^{D}(X_i, W_i) &= E\left[ D_i(1) + D_i(0) | X_i, W_i  \right], 
%     \end{align*}
%     and both $X_i$ and $W_i$ are used in determining the pairs, we have $\nu_{\text{adj}}^2 = \nu^2$.
% \end{remark}

% \begin{remark}
%     I've tried to mimic the linear adjustment with pair fixed effects part in \cite{bai2023covariate}, but did not follow Remark 4.3 in \cite{bai2023covariate}. More specifically, ``$\sigma_n^2(Q)$ in (7) is minimized when $\beta(1) + \beta(0) = 2\beta^{\text{pfe}}$'' seems not to hold. Here is the derivation: The first order condition for minimizing $\sigma_1^2(Q)$ is 
%     \begin{align*}
%         E\left[ \left( \psi_i - E[\psi_i | X_i] \right) \left( E[Y_i(1) + Y_i(0) | X_i, W_i] - E[Y_i(1) + Y_i(0) | X_i] - (\psi_i - E[\psi_i | X_i])' (\beta(1) + \beta_0)) \right) \right] = 0,
%     \end{align*}
%     which implies that
%     \begin{align*}
%         \beta(1) + \beta(0) &= \left( E[\var[\psi_i | X_i]] \right)^{-1} E\left[ \cov[\psi_i, E[Y_i(1) + Y_i(0) | X_i, W_i] | X_i] \right] \\
%         &\neq \left( E[\var[\psi_i | X_i]] \right)^{-1} E\left[ \cov[\psi_i, Y_i(1) + Y_i(0)  | X_i] \right] = 2\beta^{\text{pfe}}.
%     \end{align*} 
% It turns out that they are equivalent.
% \end{remark}

%\begin{remark}
%    High-dimensional adjustments can be discussed mimicking \cite{bai2023covariate}. This can be done if necessary.
%\end{remark}

% \begin{remark}
%     We want to write $\nu_{\text{adj}}^2$ using the notation $Y^\ast(a)$ as
%     \begin{align*}
%         \nu_{\text{adj}, Y^\ast}^2 
%         &= \frac{1}{2} E\left[ \var\left[ E\left[ Y^\ast_i(1) + Y^\ast_i(0) | X_i, W_i \right] - \left( m_1^{Y^\ast}(X_i, W_i) +  m_0^{Y^\ast}(X_i, W_i) \right) \Bigg| X_i\right] \right] + \\
%         &\ \ \ \ \frac{1}{2} \var\left[ E\left[ Y^\ast_i(1) - Y^\ast_i(0) | X_i, W_i  \right] \right] + E\left[ \var\left( Y^\ast_i(1) | X_i, W_i  \right) + \var\left( Y^\ast_i(0) | X_i, W_i  \right)  \right],
%     \end{align*}
%     and see what $m_a^{Y^\ast}(X_i, W_i)$ for $a \in \{ 0, 1 \}$ should be to make $\nu_{\text{adj}, Y^\ast}^2 = \nu_{\text{adj}}^2 $ hold. 
%     % Then we can use sample analogs of $m_a^{Y^\ast}(X_i, W_i)$ for $a \in \{ 0, 1 \}$ to do backward engineering to get the estimator [Just my guess]. The moment condition becomes 
%     % \begin{align*}
%     %     E\left[ 2 I\left\{ A_i = a \right\} \left( (Y_i - \Delta(Q)D_i) - m_a^{Y^\ast}\left( X_i, W_i \right) \right) + m_a^{Y^\ast}\left( X_i, W_i \right) \right] = E\left[ Y^\ast_i(a) \right].
%     % \end{align*}
% \end{remark}

Finally, we construct a consistent variance estimator for the limiting variance $\nu_{\rm adj}^2$. As noted in the discussion following Theorem \ref{theorem:adjustment}, the expression for $\nu_{\rm adj}^2$ corresponds exactly to the limiting variance obtained in \cite{bai2023covariate} with the usual potential outcomes $Y_i(a)$ replaced with the transformed outcomes $Y_i^\ast(a)$ and working models in  \cite{bai2023covariate} replaced with $m_{a, \tilde Y}(X_i, W_i) - \Delta(Q) m_{a, D}(X_i, W_i)$. We thus follow the variance construction from \cite{bai2023covariate}, but with a feasible version of $Y_i^\ast(a)$ defined as
\[\hat{Y}_i = Y_i - \hat{\Delta}_nD_i~,\]
and a feasible version of $m_{a, \tilde Y}(X_i, W_i) - \Delta(Q) m_{a, D}(X_i, W_i)$ defined as 
\[
\hat m_{a, \tilde Y D}(X_i, W_i) = \hat m_{a, \tilde Y}(X_i, W_i) - \hat{\Delta}_n \hat m_{a, D}(X_i, W_i)~.
\]
This strategy leads to the following variance estimator:
\begin{equation} \label{eq:adj-varest}
\hat \nu_{n, \rm adj}^2 = \frac{\hat \tau_{n, \rm adj}^2 - \frac{1}{2} ( \hat \lambda_{n, \rm adj} +  \hat \Gamma_{n, \rm adj}^2 )}{(\hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0))^2}~,
\end{equation}
where
\begin{align*}
    \hat \tau_{n, \rm adj}^{2} &= \frac{1}{n} \sum_{1 \leq j \leq n} ( \hat Y_{\pi(2j-1), \rm adj} - \hat Y_{\pi(2j), \rm adj} )^2~, \\
    \hat \lambda_{n, \rm adj} &= \frac{2}{n} \sum_{1 \leq j \leq \lfloor \frac{n}{2} \rfloor} ( \hat Y_{\pi(4j-3), \rm adj} - \hat Y_{\pi(4j-2), \rm adj} ) \\
    & \hspace{3em} \times ( \hat Y_{\pi(4j-1), \rm adj} - \hat Y_{\pi(4j), \rm adj} ) ( A_{\pi(4j-3)} - A_{\pi(4j-2)} ) ( A_{\pi(4j-1)} - A_{\pi(4j)} )~, \\
    \hat \Gamma_{n, \rm adj} &= \frac{1}{n}\sum_{1 \le i \le n: A_i = 1} \hat Y_{i, \rm adj} - \frac{1}{n}\sum_{1 \le i \le n: A_i = 0} \hat Y_{i, \rm adj}~, \\
    \hat Y_{i, \rm adj} &= \hat Y_i - \frac{1}{2} ( \hat m_{1, \tilde Y D}(X_i, W_i) + \hat m_{0, \tilde Y D}(X_i, W_i)  )~.
\end{align*}
The following theorem establishes the consistency of $\hat{\nu}_{n, \rm adj}^2$ for $ \nu_{\rm adj}^2$:
\begin{theorem}\label{theorem:varianceestimation-adj}
    Suppose $Q$ satisfies Assumption \ref{ass:Q}, the treatment assignment mechanism satisfies Assumptions \ref{ass:assignment}--\ref{ass:pairsofpairs}, and $m_{a, \tilde Y}$ and $m_{a, D}$ for $a\in \{0, 1 \}$ satisfy Assumption \ref{ass:m}. Further suppose $\hat m_{a, \tilde Y}$ and $\hat m_{a, D}$ satisfy (\ref{eq:m-Y-hat-close})--(\ref{eq:m-D-close}) and 
    \begin{equation}\label{eq:m-Y-hat-close-adj}
        \frac{1}{2n} \sum_{1\leq i\leq 2n} ( \hat m_{a, \tilde Y}(X_i, W_i) - m_{a, \tilde Y}(X_i, W_i) )^2 \cp 0~,
    \end{equation}
    \begin{equation}\label{eq:m-D-close-adj}
        \frac{1}{2n} \sum_{1\leq i\leq 2n} ( \hat m_{a, D}(X_i, W_i) - m_{a, D}(X_i, W_i) )^2 \cp 0~.
    \end{equation}
    Then,
    \begin{align*}
        \hat{\nu}_{n, \rm adj}^2 \cp \nu_{\rm adj}^2~.
    \end{align*}
\end{theorem}

%\textcolor{magenta}{Maybe add a corollary?  The corollary (or a theorem?) could be for the special case of linear models?}

\section{Simulations}\label{sec:sims}
In this section, we examine the finite-sample behavior of the estimation and inference procedures introduced in Section \ref{sec:main}. Following the simulation design in \cite{bai2022mp}, the potential outcomes are generated according to the equation:
\begin{align*}
    Y_i(d) = \mu_d + m_d(X_i) + \sigma_d(X_i) \epsilon_{d, i},
\end{align*}
where $\mu_d$, $m_d(X_i)$, and $\epsilon_{d, i}$ are specified in each model below. In Models 1--3, we model compliance status as being independent of potential outcomes and covariates, with each of the following three compliance types being equally likely: compliers, always takers, and never takers. In Models 4--6, we allow the compliance status of an individual to depend on $X_i$, and define $C_i = 1$ if individual $i$ is a complier, $AT_i = 1$ if individual $i$ is an always taker, and $NT_i = 1$ if individual $i$ is a never taker.
We consider the following model specifications:
\begin{itemize}
    \item []
    \begin{itemize}
        \item []
        \begin{itemize}
            \item [\textbf{\textbf{Model 1}:}] $X_i \sim \text{Unif}[0, 1]$; $m_1(X_i) = m_0(X_i) = \gamma(X_i - \frac{1}{2})$; $\epsilon_{d, i} \sim N(0, 1)$ for $d\in\{0, 1\}$; $\sigma_0(X_i) = \sigma_0 = 1$, and $\sigma_1(X_i) = \sigma_1$. $\gamma = 1$, $\sigma_1 = 1$, $\mu_0 = 0$ and $\mu_1 = \Delta$, where $\Delta = 0$ is to study the behavior of the tests under the null hypothesis and $\Delta = 1$ is to study the behavior of the tests under the alternative hypothesis.
            \item [\textbf{Model 2}:] As in Model 1, but $m_0(X_i) = 0$, and $m_1(X_i) = 10 \left( X_i^2 - \frac{1}{3}\right)$.
            \item [\textbf{Model 3}:] As in Model 2, but $\sigma_0(X_i) = X_i^2$ and $\sigma_1(X_i) = \sigma_1 X_i^2$.
            \item [\textbf{Model 4}:] As in Model 1, but $P\{C_i = 1| X_i\} = X_i$, and $P\{AT_i = 1 | X_i\} = P\{NT_i = 1 | X_i\} = \frac{1 - X_i}{2}$.
            \item [\textbf{Model 5}:] As in Model 4, but $m_0(X_i) = 0$, and $m_1(X_i) = 10 \left( X_i^2 - \frac{1}{3} - \frac{1}{6}\right)$.
            \item [\textbf{Model 6}:] As in Model 5, but $\sigma_0(X_i) = X_i^2$ and $\sigma_1(X_i) = \sigma_1 X_i^2$.
        \end{itemize}
    \end{itemize}
\end{itemize}

Since $\dim(X_i) = 1$, we construct pairs by sorting units according to $X_i$ and matching adjacent units. By Theorem 4.1 in \cite{bai2022mp}, this pairing algorithm ensures that both Assumptions \ref{ass:pairs} and \ref{ass:pairsofpairs} are satisfied.
Table \ref{table:rej-prob-models123456} reports the rejection probabilities from $5000$ Monte Carlo replications, for testing the null hypothesis \eqref{eq:H0} using $t$-tests constructed using either the regression-based variance $\hat{\omega}_n^2$ (Robust and Robust-S, where Robust-S employs a standard small sample correction) or the consistent estimator $\hat{\nu}^2_n$ (MPIV). As expected given our theoretical results, the regression-based estimator $\hat \omega_n^2$ is conservative, which leads to a loss of power relative to our consistent estimator $\hat \nu^2_n$. Figure \ref{figure:power-curve} displays the power curves for Models 1--6 with $2n = 1000$.
%We propose some DGPs and compare the behavior of tests using our proposed standard error and robust standard error (which is used in empirical papers) under these DGPs. For each simulation design, we show the average results of 5000 independent
%replications (as in \cite{bugni2021inference}) of the proposed DGP for different sample sizes, $2n = 200, 800, 1600, 3200$.


%The results of our simulations are presented in Table \ref{table:rej-prob-models146} and Figure \ref{figure:power-curve-model146} below. For Table \ref{table:rej-prob-models146}, columns are labeled in the following way:
%\begin{itemize}
%    \item []
%    \begin{itemize}
%        \item []
%        \begin{itemize}
%            \item [\textbf{Robust}:] Using robust standard error in TSLS regression.
%            \item [\textbf{Robust-S}:] Using robust standard error in TSLS regression corrected for small sample (robust standard error times $\sqrt{\frac{N}{N - K}}$, where $N$ is the sample size, and $K$ is the number of variables in TSLS regression including constant). 
%            \item [\textbf{MPIV}:] Using the standard error proposed in this paper.
%        \end{itemize}
%    \end{itemize}
%\end{itemize}

% \begin{table}[ht!]
%    \centering
%    \begin{tabular}{ c c  c c c c c c  }
%         \toprule
%         \multicolumn{1}{c}{} & 
%         \multicolumn{1}{c}{} &
%         \multicolumn{3}{c}{$H_0: \Delta = 0$} & \multicolumn{3}{c}{$H_1: \Delta = 1$}  \\
%         \cmidrule(lr){3-5} \cmidrule(lr){6-8}
%         Model & Sample Size & Robust & Robust-S & MPIV & Robust & Robust-S & MPIV \\
%         \midrule
%         \multirow{4}*{1} & 200& 2.72& 2.64& 3.42& 60.76& 60.46& 63.22\\
%          & 800& 4.10& 4.06& 5.06& 99.14& 99.14& 99.34\\
%          & 1600& 4.22& 4.20& 5.02& 100.00& 100.00& 100.00\\
%          & 3200& 4.42& 4.38& 5.14& 100.00& 100.00& 100.00\\
%         \addlinespace
%         \multirow{4}*{2} & 200& 0.48& 0.46& 3.40& 10.60& 10.34& 25.90\\
%          & 800& 0.78& 0.78& 4.38& 53.28& 53.20& 73.54\\
%          & 1600& 1.30& 1.30& 5.02& 87.20& 87.16& 95.12\\
%          & 3200& 1.14& 1.14& 5.18& 99.52& 99.52& 99.96\\
%         \addlinespace
%         \multirow{4}*{3} & 200& 0.36& 0.34& 2.96& 11.46& 11.14& 33.50\\
%          & 800& 0.34& 0.34& 4.40& 61.84& 61.72& 85.20\\
%          & 1600& 0.76& 0.74& 5.02& 93.24& 93.20& 98.60\\
%          & 3200& 0.72& 0.72& 5.14& 99.94& 99.94& 100.00\\
%         \bottomrule
%        \end{tabular}
%    \caption{Rejection probabilities for Models 1, 2, and 3}
%    \label{table:rej-prob-models146}
% \end{table}

% \begin{table}[ht!]
%     \centering
%     \begin{tabular}{ c c  c c c c c c  }
%          \toprule
%          \multicolumn{1}{c}{} & 
%          \multicolumn{1}{c}{} &
%          \multicolumn{3}{c}{$H_0: \Delta = 0$} & \multicolumn{3}{c}{$H_1: \Delta = 1$}  \\
%          \cmidrule(lr){3-5} \cmidrule(lr){6-8}
%          Model & Sample Size & Robust & Robust-S & MPIV & Robust & Robust-S & MPIV \\
%          \midrule
%          \multirow{4}*{1} & 200 & 2.72 & 2.64 & 3.36 & 60.76 & 60.46 & 63.36\\
%          & 800 & 4.10 & 4.06 & 5.10 & 99.14 & 99.14 & 99.38\\
%          & 1600 & 4.22 & 4.20 & 4.98 & 100.00 & 100.00 & 100.00\\
%          & 3200 & 4.42 & 4.38 & 5.10 & 100.00 & 100.00 & 100.00\\
%         \addlinespace
%         \multirow{4}*{2} & 200 & 0.48 & 0.46 & 3.50 & 10.60 & 10.34 & 25.80\\
%          & 800 & 0.78 & 0.78 & 4.50 & 53.28 & 53.20 & 73.70\\
%          & 1600 & 1.30 & 1.30 & 5.00 & 87.20 & 87.16 & 95.10\\
%          & 3200 & 1.14 & 1.14 & 5.14 & 99.52 & 99.52 & 99.96\\
%         \addlinespace
%         \multirow{4}*{3} & 200 & 0.36 & 0.34 & 3.14 & 11.46 & 11.14 & 33.50\\
%          & 800 & 0.34 & 0.34 & 4.44 & 61.84 & 61.72 & 85.04\\
%          & 1600 & 0.76 & 0.74 & 5.04 & 93.24 & 93.20 & 98.62\\
%          & 3200 & 0.72 & 0.72 & 5.16 & 99.94 & 99.94 & 100.00\\
%          \bottomrule
%         \end{tabular}
%     \caption{Rejection probabilities for Models 1, 2, and 3} 
%     \label{table:rej-prob-models146}
% \end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{ c c  c c c c c c  }
         \toprule
         \multicolumn{1}{c}{} & 
         \multicolumn{1}{c}{} &
         \multicolumn{3}{c}{$H_0: \Delta = 0$} & \multicolumn{3}{c}{$H_1: \Delta = 1$}  \\
         \cmidrule(lr){3-5} \cmidrule(lr){6-8}
         Model & Sample Size & Robust & Robust-S & MPIV & Robust & Robust-S & MPIV \\
         \midrule
        \multirow{4}*{1} & 200 & 2.72 & 2.64 & 3.36 & 60.76 & 60.46 & 63.36\\
         & 800 & 4.10 & 4.06 & 5.10 & 99.14 & 99.14 & 99.38\\
         & 1600 & 4.22 & 4.20 & 4.98 & 100.00 & 100.00 & 100.00\\
         & 3200 & 4.42 & 4.38 & 5.10 & 100.00 & 100.00 & 100.00\\
         \addlinespace 
        \multirow{4}*{2} & 200 & 0.48 & 0.46 & 3.50 & 10.60 & 10.34 & 25.80\\
         & 800 & 0.78 & 0.78 & 4.50 & 53.28 & 53.20 & 73.70\\
         & 1600 & 1.30 & 1.30 & 5.00 & 87.20 & 87.16 & 95.10\\
         & 3200 & 1.14 & 1.14 & 5.14 & 99.52 & 99.52 & 99.96\\
         \addlinespace
        \multirow{4}*{3} & 200 & 0.36 & 0.34 & 3.14 & 11.46 & 11.14 & 33.50\\
         & 800 & 0.34 & 0.34 & 4.44 & 61.84 & 61.72 & 85.04\\
         & 1600 & 0.76 & 0.74 & 5.04 & 93.24 & 93.20 & 98.62\\
         & 3200 & 0.72 & 0.72 & 5.16 & 99.94 & 99.94 & 100.00\\
         \addlinespace
        \multirow{4}*{4} & 200 & 3.58 & 3.46 & 4.56 & 91.64 & 91.52 & 92.58\\
         & 800 & 4.10 & 4.08 & 4.96 & 100.00 & 100.00 & 100.00\\
         & 1600 & 4.06 & 4.06 & 5.04 & 100.00 & 100.00 & 100.00\\ & 3200 & 4.00 & 4.00 & 4.84 & 100.00 & 100.00 & 100.00\\
         \addlinespace
        \multirow{4}*{5} & 200 & 1.36 & 1.30 & 4.80 & 20.80 & 20.42 & 41.58\\
         & 800 & 1.36 & 1.36 & 4.60 & 90.24 & 90.16 & 96.90\\
         & 1600 & 1.58 & 1.56 & 4.72 & 99.80 & 99.80 & 99.98\\
         & 3200 & 1.32 & 1.32 & 4.82 & 100.00 & 100.00 & 100.00\\
         \addlinespace
        \multirow{4}*{6} & 200 & 1.02 & 0.98 & 4.86 & 23.64 & 23.20 & 52.04\\
         & 800 & 0.94 & 0.92 & 4.94 & 95.70 & 95.70 & 99.48\\
         & 1600 & 1.02 & 1.02 & 5.20 & 100.00 & 100.00 & 100.00\\
         & 3200 & 0.78 & 0.78 & 4.38 & 100.00 & 100.00 & 100.00\\
        \bottomrule
        \end{tabular}
    \caption{Rejection rates for Models 1, 2, 3, 4, 5, and 6} 
    \label{table:rej-prob-models123456}
\end{table}


% Figure environment removed


% \section{Random Thoughts}

% Check Federico's paper for simulation.

% Choose DGPs from Yuehao and Federico's papers!

% \textcolor{red}{Start with Model 1, 4, 6 in Yuehao.}

% mathced pair t-test version?

% Then use the data in empirical papers for empirical application. (question: is it obvious to construct pair of pairs?)

% Do we want covariate adjustment? \textcolor{red}{Should try anyway to learn!}

% Optimal matched-pair design under noncompliance? \textcolor{red}{[Run some pilot survey to get some information about the distribution of compliance status (or the relationship between compliance status and covariates). Then use this information to assign treatments to optimize MSE/welfare function, etc.]}

% policy learning under noncompliance?

\section{Empirical Application}\label{sec:empirical-application}

% Stata command ``areg'' automatically drops missing values when forming estimates. (from Stata manual)
In this section, we illustrate our findings by revisiting the empirical application in \cite{groh2016macroinsurance}. \cite{groh2016macroinsurance} designed a matched-pair experiment in Egypt to study the effect on microenterprises of acquiring insurance against macroeconomic shocks.\footnote{To most closely align the dataset with our theoretical results, we made the following modifications to the dataset: (1) for each outcome variable, we drop pairs if at least one of the individuals in that pair has a missing outcome variable, (2) we drop pairs if at least one of the individuals in that pair is missing treatment assignment (the eligibility of purchasing the insurance), treatment status (whether a company actually purchased the insurance), or any baseline covariates, (3) we keep only pairs with exactly two individuals (there were 39 pairs with only one individual and one ``block" with 16 individuals), 
% (2) dropped the pairs with defiers (there are 5 individuals in the control group who purchased the insurance), 
%(3) we drop any pairs with covariate values {\bf [is this correct?]},  
(4) if necessary, we drop one pair from the end of the resulting dataset to ensure that the sample size is divisible by 4. (5) to construct the pairs of pairs when computing $\hat{\nu}_{n}$ and $\hat{\nu}_{n,adj}$, we use the \texttt{R} package \texttt{nbpMatching} to match pairs of pairs such that the conditions in Theorem 4.3 of \cite{bai2022mp} are satisfied. Modifications (1)-(5) result in an average sample size reduction of 96 observations (3.29\% of total sample size) across outcomes.} The eligibility to purchase macroinsurance was offered to companies in the treatment group.
% while companies in the control group were not able to {\bf [so this is one-sided non-compliance?]} \textcolor{red}{[HG: There are 5 individuals in the control group purchasing insurance. I dropped pairs with those individuals in our empirical application.]} purchase macroinsurance. 
The take-up rate of purchasing in the treatment group was 37\%: among 1481 companies in the treatment group, 548 of them purchased insurance. We also note among 1480 companies in the control group, 5 of them purchased insurance as well.
%\textcolor{red}{[HG: 36.7\% = (548-5)/1481 in \cite{groh2016macroinsurance}]}. 

%In \cite{groh2016macroinsurance}, both ITT (intention-to-treat) and LATE estimates were reported. In this section we only consider the estimation and inference for LATE in this section.

%In most empirical papers with matched-pair design under imperfect compliance, inferences for LATE were implemented using robust standard errors, see \cite{groh2016macroinsurance} and \cite{resnjanskij2021can}. However, as shown in Theorem \ref{theorem:robust}, tests obtained by traditional robust standard error are conservative for matched-pair design with noncompliance issues, and using the standard error proposed in Theorem \ref{theorem:varianceestimation} will provide us
%asymptotically exact test in the sense that its limiting rejection probability under the null hypothesis equals the
%nominal level. To illustrate this point, we compare robust standard error with the standard error proposed in Theorem \ref{theorem:varianceestimation} using the data in \cite{groh2016macroinsurance}.


%We reproduce the LATE estimates and inferences from Table 7 of \cite{groh2016macroinsurance}, which presents estimated LATE on profits, revenues, employees and household consumption.
% \textcolor{red}{[Caveat: Whether the variance estimator proposed in Theorem \ref{theorem:varianceestimation} can be applied to the data in \cite{groh2016macroinsurance} depends on whether Assumptions \ref{ass:pairs} and \ref{ass:pairsofpairs} are satisfied. Experiment in \cite{groh2016macroinsurance} used ``an optimal greedy algorithm to minimize the Mahalanobis distance between the values of 13 variables that we hypothesized may determine loan take-up and investment decisions''. Since I cannot get access to their codes implementing pairing and randomization procedures, I do not know whether their data is sorted such that Assumption \ref{ass:pairsofpairs} (pairs of pairs) is satisfied. I sorted their data by variable ``pair'', which is the pair ID, before I implemented each regression, and assumed that their pair ID sorting ensures conditions in Theorem 4.3 of \cite{bai2022mp} are satisfied.]}


Table \ref{table:empirical-application} reports the estimated LATEs for a collection of outcomes, using both the unadjusted Wald estimator $\hat{\Delta}_n$ as well as the linearly adjusted estimator $\hat{\Delta}_n^{\rm adj}$ which uses the same covariates as those considered in the analysis in \cite{groh2016macroinsurance}.\footnote{We note however that we exclude the female dummy and branchid dummies, which were used in the original regression specifications in \cite{groh2016macroinsurance}, since these are  perfectly collinear with our pair fixed effects.} We also report the standard errors obtained from the regression-based variance estimator $\hat{\omega}_n^2$ as well as the standard errors obtained from our consistent variance estimators $\hat{\nu}^2_n$ and $\hat{\nu}^{2}_{n, \rm adj}$ (note that we do not report the standard error obtained from the standard regression-based variance estimator when considering covariate adjustment, since as mentioned in Remark \ref{rem:FE_TSLS} this is not guaranteed to be valid). Our findings are consistent with the theoretical results presented in Sections \ref{sec:main} and \ref{sec:adjustment}: for the unadjusted estimates, standard errors constructed from $\hat{\nu}^2_n$ are smaller than those constructed from $\hat{\omega}^2_n$ across all outcomes, and in fact result in significance tests for the effect on profits and the aggregate index which reject at the $10\%$ level. For the adjusted estimates, we find that the standard errors constructed from $\hat{\nu}^2_{n, \rm adj}$ are smaller than those constructed from $\hat{\nu}^2_n$ across all outcomes. However, point estimates for profits and the aggregate index change in such a way that these are no longer significant at the $10\%$ level.

%both robust standard errors and MPIV standard errors proposed by our paper for each of the regressions. For all specifications, MPIV standard error is smaller than the robust standard error, which provides evidence that favors the test we proposed.

\begin{table}[ht!]
\caption{Summary of Estimates Obtained from Empirical Application \cite{groh2016macroinsurance}}
\begin{adjustbox}{max width=\linewidth,center}
\begin{tabular}[t]{llllllllll}
\toprule
& & High& &High &Number &Any &Owner's &Monthly & Aggregate\\
  & Profits & Profit & Revenue & Revenue & Employees & Worker & Hours & Consumption & Index\\
\midrule
$\hat \Delta_n$ & -241.602 & -0.029 & -2561.536 & -0.063 & -0.085 & 0.009 & -1.324 & -14.197 & -0.104\\
\addlinespace
SE from $\hat \omega^2_n$ & (149.696) & (0.022) & (937.455)$^{\ast\ast\ast}$ & (0.021)$^{\ast\ast\ast}$ & (0.144) & (0.050) & (2.506) & (91.237) & (0.069)\\
\addlinespace
SE from $\hat \nu^2_n$ & (130.946)$^{\ast}$ & (0.021) & (841.562)$^{\ast\ast\ast}$ & (0.020)$^{\ast\ast\ast}$ & (0.138) & (0.049) & (2.179) & (79.273) & (0.061)$^{\ast}$\\
\addlinespace[1em]
$\hat \Delta_n^{\rm adj}$ & -151.345 & -0.020 & -1802.829 & -0.052 & -0.023 & 0.024 & -0.401 & 6.541 & -0.069\\
\addlinespace
SE from $\hat \nu^2_{n, \rm adj}$ & (120.969) & (0.020) & (751.651)$^{\ast\ast}$ & (0.018)$^{\ast\ast\ast}$ & (0.133) & (0.047) & (2.116) & (74.388) & (0.057)\\
\addlinespace[1em]
Sample Size & 2804 & 2804 & 2800 & 2800 & 2824 & 2824 & 2796 & 2880 & 2880\\
\bottomrule
\end{tabular}
\end{adjustbox}
\begin{tablenotes}
   \small \item Notes: $^\ast$: significant at 10\% level. $^{\ast\ast}$: significant at 5\% level. $^{\ast\ast\ast}$: significant at 1\% level. For each outcome listed in Table 7 of \cite{groh2016macroinsurance}, we report (a) the Wald estimates $\hat \Delta_n$ in \eqref{eq:hatDelta}, (b) the robust standard error obtained from $\hat \omega^2_n$ in \eqref{eq:robust}, (c) the MPIV standard error obtained from $\hat \nu^2_n$ in \eqref{eq:mpiv-varest}, (d) the covariate-adjusted estimates $\hat \Delta_n^{\rm adj}$ with pair fixed effects based on \eqref{eq:pfe-Y} and \eqref{eq:pfe-D}, (e) the standard error obtained from $\hat \nu^2_{n, \rm adj}$ in \eqref{eq:adj-varest}, and (f) the sample sizes for the regression of each outcome variable. 
\end{tablenotes}
\label{table:empirical-application}
\end{table}

\section{Recommendations for Empirical Practice} \label{sec:recs}
Based on our theoretical results as well as the simulation study above, we conclude with some recommendations for practitioners when conducting inference about the local average treatment effect in matched-pairs experiments. Our findings are that the standard Wald estimator is generally consistent and asymptotically normal under matched-pair designs, but its limiting variance is typically smaller than what would be obtained under i.i.d.\ assignment.  It follows that inferences using the usual heteroskedasticty-robust estimator of the variance will typically be conservative. We therefore recommend that practitioners use our consistent variance estimator $\hat{\nu}^2_n$ instead. When considering covariate adjustment, our findings are that the two-stage least squares estimator with pair fixed effects leads to an estimator that is optimal in the sense of having smallest limiting variance among all linearly-adjusted estimators. An important caveat, however, is that the usual heteroskedasticty-robust estimator of the variance is not consistent for its variance. As a result, we recommend that practitioners use our consistent variance estimator $\hat{\nu}^2_{n, \rm adj}$ instead.

\clearpage
\appendix

\section{Proofs of Main Results}
\subsection{Proof of Theorem \ref{theorem:main}}
\begin{proof}
    \begin{align*}
        \sqrt{n} \left( \hat \Delta_n - \Delta(Q) \right) &= \sqrt{n} \left( \frac{\hat{\psi}_n(1) - \hat{\psi}_n(0)}{\hat{\phi}_n(1) - \hat{\phi}_n(0)} - \Delta(Q) \right) \\
        &= \frac{\sqrt{n} \left( \hat{\psi}_n(1) - \hat{\psi}_n(0) - \Delta(Q) \left( \hat{\phi}_n(1) - \hat{\phi}_n(0)  \right) \right)}{\hat{\phi}_n(1) - \hat{\phi}_n(0) }~.
    \end{align*}
    Following similar arguments to the proof of Lemma S.1.5 in \cite{bai2022mp}, we have
    \begin{equation}\label{eq:phi-hat-cp-pc}
        \hat \phi_n(1) - \hat \phi_n(0) \cp P \{C_i = 1\}~.
    \end{equation}
    Therefore, to understand the limiting distribution of $\hat \Delta_n$, it suffices to show that
    \begin{equation}\label{eq:psi-tilde-clt}
        \sqrt n(\tilde \psi_n(1) - \tilde \psi_n(0)) \cd N(0, \nu^2 P \{C_i = 1\}^2)~,
    \end{equation}
    where
    \[ \tilde \psi_n(a) = \frac{1}{n} \sum_{1\leq i\leq 2n: A_i = a} Y_i^\ast(a)~. \]
    We may write 
    \begin{align*}
        \sqrt{n} \left( \tilde \psi_n(1) - \tilde \psi_n(0) \right) = A_n - B_n + C_n - D_n~,
    \end{align*}
    where
    \begin{align*}
        A_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} \left( Y_i^\ast(1) A_i - E[Y_i^\ast(1) A_i | X^{(n)}, A^{(n)}] \right)~,\\
        B_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} \left( Y_i^\ast(0) (1 - A_i) - E[Y_i^\ast(0) (1 - A_i) | X^{(n)}, A^{(n)}] \right)~,\\
        C_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} \left( E[Y_i^\ast(1) A_i | X^{(n)}, A^{(n)}] - A_i E[Y_i^\ast(1)] \right)~,\\
        D_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} \left( E[Y_i^\ast(0) (1 - A_i) | X^{(n)}, A^{(n)}] - (1 - A_i) E[Y_i^\ast(0)] \right)~.
    \end{align*}
    The decomposition holds because
    \begin{equation}\label{eq:Ystar1-Ystar0}
        \begin{split}
            E[Y_i^\ast(1)] - E[Y_i^\ast(0)] 
            &= E[\tilde Y_i(1) - \Delta(Q)D_i(1)] - E[\tilde Y_i(0) - \Delta(Q)D_i(0)] \\
            &= E[\tilde Y_i(1) - \tilde Y_i(0)] - \Delta(Q) E[D_i(1) - D_i(0)] \\
            &= E[(Y_i(1) - Y_i(0)) (D_i(1) - D_i(0))] - \Delta(Q) E[D_i(1) - D_i(0)] \\
            &= 0~,
        \end{split}
    \end{equation}
    where the first equation follows by (\ref{eq:Ystar-potential}), the second equation follows by inspection, the third equation follows by (\ref{eq:TildeY}) two equations follow by inspection, and the last equation follows by Assumption \ref{ass:Q}-(d). Then the the proof of (\ref{eq:psi-tilde-clt}) follows similarly to the proof of Lemma S.1.4 in \cite{bai2022mp}. Therefore, by Slutsky's theorem, the desired conclusion follows by (\ref{eq:phi-hat-cp-pc}) and (\ref{eq:psi-tilde-clt}) under Assumption \ref{ass:Q}-(e).
\end{proof}
% \begin{proof}
% We have that 
% \begin{equation*}
%     \begin{split}
%         \hat{\Delta}_n = \frac{\hat{\psi}_n(1) - \hat{\psi}_n(0)}{\hat{\phi}_n(1) - \hat{\phi}_n(0)}~.
%     \end{split}
% \end{equation*}
% In particular, for $h(x,y) = x/y$, observe that $\hat{\Delta}_n = h(\hat{\psi}_n(1) - \hat{\psi}_n(0), \hat{\phi}_n(1) - \hat{\phi}_n(0))$, and the Jacobian of $h(\cdot)$ is given by
% \[D_h(x,y) = \left(\frac{1}{y}, -\frac{x}{y^2}\right)~.\]
% %Let
% %\begin{align*}
% %    f(x) &= \frac{x_1}{x_2} \\
% %    \mu &= 
% %    \begin{pmatrix}
% %        \Delta(Q) P \{C_i = 1\} \\
% %        P \{C_i = 1\}
% %    \end{pmatrix}.
% %\end{align*}
% %By Assumption \ref{ass:Q}-(e), we have
% %\begin{align*}
% %    Df(\mu) &= 
% %    \begin{pmatrix}
% %        \frac{1}{x_2} & -\frac{x_1}{x_2^2}
% %    \end{pmatrix}\Bigg|_{(x_1, x_2)' = \mu} \\
% %    &= 
% %    \begin{pmatrix}
% %        \frac{1}{P \{C_i = 1\}} & -\frac{\Delta(Q)}{P \{C_i = 1\}}
% %    \end{pmatrix}.
% %\end{align*}
% Let $\mu = (\Delta(Q)P \{C_i = 1\}, P \{C_i = 1\})$, and recall by Assumption \ref{ass:Q}(d) that $P \{C_i = 1\} > 0$. By the Delta method and Lemma \ref{lemma:vectorCLT} we thus obtain that
% \begin{align*}
%     \sqrt{n} \left( 
%     \hat{\Delta}_n
%     -
%     \Delta(Q)
%     \right) \cd 
%     N\left(0, D_h(\mu) \Sigma D_h(\mu)'   \right),
% \end{align*}
% where
% \begin{align*}
%     &D_h(\mu) \Sigma D_h(\mu)' \\
%     =&
%     \begin{pmatrix}
%         \frac{1}{P \{C_i = 1\}} & -\frac{\Delta(Q)}{P \{C_i = 1\}}
%     \end{pmatrix}
%     \begin{pmatrix}
%         \Sigma_{11} & \Sigma_{12} \\
%         \Sigma_{21} & \Sigma_{22}
%     \end{pmatrix}
%     \begin{pmatrix}
%         \frac{1}{P \{C_i = 1\}} \\
%         -\frac{\Delta(Q)}{P \{C_i = 1\}}
%     \end{pmatrix} \\
%     =& 
%     \begin{pmatrix}
%         \frac{1}{P \{C_i = 1\}} \Sigma_{11}  - \frac{\Delta(Q)}{P \{C_i = 1\}}\Sigma_{21} &
%         \frac{1}{P \{C_i = 1\}} \Sigma_{12}  - \frac{\Delta(Q)}{P \{C_i = 1\}}\Sigma_{22}
%     \end{pmatrix}
%     \begin{pmatrix}
%         \frac{1}{P \{C_i = 1\}} \\
%         -\frac{\Delta(Q)}{P \{C_i = 1\}}
%     \end{pmatrix} \\
%     =&
%     \frac{1}{P \{C_i = 1\}^2} \left( \Sigma_{11} - 2 \Delta(Q)\Sigma_{21} + \Delta(Q)^2\Sigma_{22} \right),
% \end{align*}
% where
% \begin{align*}
%     \Sigma_{11} &= 
%     E\left[ \var\left[ \tilde Y_i(1) |X_i \right]  \right] + E\left[ \var\left[ \tilde Y_i(0) |X_i \right]  \right] + \frac{1}{2} \var\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i \right] \right], \\
%     \Sigma_{22} &= E[\var(D_i(1)|X_i)] + E[\var[D_i(0)|X_i]] + \frac{1}{2} \var\left[ E\left[ D_i(1) - D_i(0) | X_i \right]  \right], \\
%     \Sigma_{21} 
%     % &= 
%     % E\left[ E\left[Y_i(1)D_i(1)|X_i\right] - E\left[\tilde Y_i(1)|X_i\right] E\left[D_i(1)|X_i\right]\right] + E\left[ E\left[Y_i(1)D_i(0)|X_i\right] - E\left[\tilde Y_i(0)|X_i\right] E\left[D_i(0)|X_i\right]\right] + \\
%     % &\ \ \ \ \frac{1}{2} \left( E\left[ E\left[ \tilde Y_i(1) - \tilde Y_i(0) | X_i \right] E\left[ D_i(1) - D_i(0) | X_i \right] \right] - E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right] E\left[ D_i(1) - D_i(0) \right] \right) \\
%     % &= \Sigma_{12}. \\
%     &= E\left[ Y_i(1) D_i(1) \right] + E\left[ Y_i(1) D_i(0) \right] - \frac{1}{2} E\left[ \tilde Y_i(1) - \tilde Y_i(0) \right] E\left[ D_i(1) - D_i(0) \right]  \\
%     &\ \ \ \ \ 
%     - \frac{1}{2} 
%     E\left[ E\left[ \tilde Y_i(1) | X_i \right] E\left[ D_i(1) | X_i \right] \right]- 
%     \frac{1}{2} 
%     E\left[ E\left[ \tilde Y_i(0) | X_i \right] E\left[ D_i(0) | X_i \right] \right]  \\
%     &\ \ \ \ \ 
%     - \frac{1}{2} 
%     E\left[ E\left[ \tilde Y_i(1) | X_i \right] E\left[ D_i(0) | X_i \right] \right] - 
%     \frac{1}{2} 
%     E\left[ E\left[ \tilde Y_i(0) | X_i \right] E\left[ D_i(1) | X_i \right] \right].
% \end{align*}
% The final expression for the variance then follows by algebraic verification. 
% \end{proof}

\subsection{Proof of Theorem \ref{theorem:varianceestimation}}
\begin{proof}
Recall that for $a\in \{0, 1\}$, we denote the adjusted potential outcome as
\begin{equation}\label{eq:Ystar-potential}
    Y^\ast_i(a) = \tilde Y_i(a) - \Delta(Q)D_i(a)~,
\end{equation}
and denote the (infeasible) adjusted observed outcome as
\begin{equation}\label{eq:Ystar}
    Y^\ast_i = Y_i - \Delta(Q) D_i~.
\end{equation}
With this notation, the observed adjusted outcome can be written as
\begin{equation}\label{eq:obs-adj-Y}
    Y^\ast = Y_i^\ast(1) A_i + Y_i^\ast(0) (1 - A_i)~.
\end{equation}
First note that as a consequence of Lemma S.1.5 in \cite{bai2022mp} and the continuous mapping theorem, 
\[\left(\hat{\phi}_n(1) - \hat{\phi}_n(0)\right)^2 \cp P \{C_i = 1\}^2~.\]
It thus suffices to show that the numerator converges to the desired quantity.

Consider the following infeasible version of the numerator, given by 
\[\tilde{\tau}^2_n - \frac{1}{2}(\tilde{\lambda}^2_n + \tilde{\Gamma}^2_n)~,\]
where 
\[\tilde{\tau}^2_n = \frac{1}{n}\sum_{1 \le j \le n}(Y^\ast_{\pi(2j)} - Y^\ast_{\pi(2j-1)})^2~,\]
\[\tilde{\lambda}^2_n = \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor}\left(Y^\ast_{\pi(4j-3)} - Y^\ast_{\pi(4j-2)}\right)\left(Y^\ast_{\pi(4j-1)} - Y^\ast_{\pi(4j)}\right)\left(A_{\pi(4j-3)} - A_{\pi(4j-2)}\right)\left(A_{\pi(4j-1)} - A_{\pi(4j)}\right)~,\]
\[\tilde{\Gamma}_n = \frac{1}{n}\sum_{1 \le i \le 2n: A_i = 1}Y_i^\ast - \frac{1}{n}\sum_{1 \le i \le 2n: A_i = 0}Y_i^\ast~.\]
It follows immediately from Assumption \ref{ass:Q} that (b)--(c) from \cite{bai2022mp} are satisfied for the transformed outcomes $Y_i^\ast(a)$, and thus it follows from Lemmas S.1.5, S.1.6, and S.1.7 in \cite{bai2022mp} that this infeasible numerator converges to the desired quantity. It thus remains to show that 
\begin{align}
\label{eq:tau} \hat{\tau}^2_n & = \tilde{\tau}^2_n + o_P(1) \\
\label{eq:lambda-P} \hat{\lambda}^2_n & = \tilde{\lambda}^2_n + o_P(1) \\
\label{eq:gamma-P} \hat{\Gamma}_n & = \tilde{\Gamma}_n + o_P(1)~.
\end{align}
We begin with \eqref{eq:tau}. To see this, note that 
\begin{multline*}
\frac{1}{n}\sum_{1 \le j \le n}\left\{(Y^\ast_{\pi(2j)} - Y^\ast_{\pi(2j-1)})^2 - (\hat{Y}_{\pi(2j)} - \hat{Y}_{\pi(2j-1)})^2\right\} \\
= \frac{1}{n}\sum_{1 \le j \le n}\left\{(Y^\ast_{\pi(2j)} - Y^\ast_{\pi(2j-1)})^2 - (\hat{Y}_{\pi(2j)} - \hat{Y}_{\pi(2j-1)})^2\right\}A_{\pi(2j)} \\
+ \frac{1}{n}\sum_{1 \le j \le n}\left\{(Y^\ast_{\pi(2j)} - Y^\ast_{\pi(2j-1)})^2 - (\hat{Y}_{\pi(2j)} - \hat{Y}_{\pi(2j-1)})^2\right\}A_{\pi(2j-1)}~.
\end{multline*}
It thus suffices to show that each component on the RHS converges in probability to zero. We only show the first since the second follows symmetrically. From the definitions of $Y^\ast_i$ and $\hat{Y}_i$ we obtain that 
\begin{align*}
& \frac{1}{n}\sum_{1 \le j \le n}\left\{(Y^\ast_{\pi(2j)} - Y^\ast_{\pi(2j-1)})^2 - (\hat{Y}_{\pi(2j)} - \hat{Y}_{\pi(2j-1)})^2\right\}A_{\pi(2j)} \\
& = \frac{1}{n}\sum_{1 \le j \le n}\Big\{(\tilde Y_{\pi(2j)}(1) - \tilde Y_{\pi(2j-1)}(0) - \Delta(Q)(D_{\pi(2j)}(1) - D_{\pi(2j-1)}(0)))^2 \\
& \hspace{3em} - (\tilde Y_{\pi(2j)}(1) - \tilde Y_{\pi(2j-1)}(0) - \hat{\Delta}_n(D_{\pi(2j)}(1) - D_{\pi(2j-1)}(0)))^2 \Big\}A_{\pi(2j)} \\
& = -2(\Delta(Q) - \hat{\Delta}_n)\frac{1}{n}\sum_{1 \le j \le n}\left\{(\tilde Y_{\pi(2j)}(1) - \tilde Y_{\pi(2j-1)}(0))((D_{\pi(2j)}(1) - D_{\pi(2j-1)}(0))\right\}A_{\pi(2j)} \\
& \hspace{3em} + (\Delta(Q)^2 - \hat{\Delta}_n^2)\frac{1}{n}\sum_{1 \le j \le n}\left\{(D_{\pi(2j)}(1) - D_{\pi(2j-1)}(0))^2\right\}A_{\pi(2j)}
\end{align*}
Next, note that by the triangle inequality, Assumption \ref{ass:Q}(b) and the weak law of large numbers, 
\begin{multline*}
  \left|\frac{1}{n}\sum_{1 \le j \le n}\left\{(\tilde Y_{\pi(2j)}(1) - \tilde Y_{\pi(2j-1)}(0))((D_{\pi(2j)}(1) - D_{\pi(2j-1)}(0))\right\}A_{\pi(2j)}\right| \\
  \le \frac{1}{n}\sum_{1 \le i \le 2n}|\tilde Y_i(1)| + \frac{1}{n}\sum_{1 \le i \le 2n}|\tilde Y_i(0)| = O_P(1)~,  
\end{multline*}
and since $D$ and $A$ are binary,
\[\left|\frac{1}{n}\sum_{1 \le j \le n}\left\{(D_{\pi(2j)}(1) - D_{\pi(2j-1)}(0))^2\right\}A_{\pi(2j)}\right| \le 1~,\]
and hence the result follows from the fact that $\hat{\Delta}_n \cp \Delta(Q)$ by Theorem \ref{theorem:main}.
To show \eqref{eq:lambda-P}, note that
\begin{align*}
& \hat \lambda_n^2 - \tilde \lambda_n^2 \\
& = (\hat \Delta_n - \Delta(Q))^2 \frac{2}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2} \rfloor} ( D_{\pi(4j-3)} - D_{\pi(4j-2)} ) ( D_{\pi(4j-1)} - D_{\pi(4j)} ) \\
& \hspace{5em} \times (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}) \\
&\hspace{0.5em} - (\hat \Delta_n - \Delta(Q)) \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor} (D_{\pi(4j-3)} - D_{\pi(4j-2)}) (Y^\ast_{\pi(4j-1)} - Y^\ast_{\pi(4j)}) \\
& \hspace{5em} \times (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}) \\
& \hspace{0.5em} - (\hat \Delta_n - \Delta(Q)) \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor} (D_{\pi(4j-1)} - D_{\pi(4j)}) (Y^\ast_{\pi(4j-3)} - Y^\ast_{\pi(4j-2)}) \\
& \hspace{5em} \times (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)})~.
\end{align*}
Note $\hat \Delta_n \cp \Delta(Q)$ because of Theorem \ref{theorem:main}. On the other hand,
\begin{align*}
    & \Big | \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor} (D_{\pi(4j-3)} - D_{\pi(4j-2)}) (Y^\ast_{\pi(4j-1)} - Y^\ast_{\pi(4j)}) (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}) \Big | \\
    & \leq \frac{2}{n} \sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor} (|Y^\ast_{\pi(4j-1)}| + |Y^\ast_{\pi(4j)}|) \\
    & \leq \frac{2}{n} \sum_{1 \leq i \leq 2n} |Y_i^\ast(1)| + \frac{2}{n} \sum_{1 \leq i \leq 2n} |Y_i^\ast(0)|~,
\end{align*}
where the first inequality follows from the triangle inequality and the fact that $D$ and $A$ are binary, and the last inequality follows trivially. And since $D$ and $A$ are binary,
\[ \left| \frac{2}{n} \sum_{1\leq j\leq \lfloor \frac{n}{2} \rfloor} ( D_{\pi(4j-3)} - D_{\pi(4j-2)} ) ( D_{\pi(4j-1)} - D_{\pi(4j)} ) (A_{\pi(4j-3)} - A_{\pi(4j-2)}) (A_{\pi(4j-1)} - A_{\pi(4j)}) \right| \leq 1~. \]
\eqref{eq:lambda-P} then follows because
\[ \frac{2}{n} \sum_{1 \leq i \leq 2n} |Y_i^\ast(a)| = O_P(1) \]
for $a \in \{0, 1\}$ because of Assumption \ref{ass:Q}(b) and the weak law of large numbers. Finally, \eqref{eq:gamma-P} follows immediately from $\hat \Delta_n \cp \Delta(Q)$.
\end{proof}



%The desired results follow by Lemmas \ref{lemma:Sigma11}-\ref{lemma:WaDa}. Assumption \ref{ass:Q}-(e) does not appear in the proofs for Lemmas \ref{lemma:Sigma11}-\ref{lemma:WaDa}, but it's required for Theorem \ref{theorem:varianceestimation} because the consistent estimator of $P \{C_i = 1\}$, which is $\left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right)$, appeared in the denominator, whose limit should be not equal to zero.

\subsection{Proof of Theorem \ref{theorem:robust}}
\begin{proof}
Following arguments similar to those used in the proof of Lemma S.1.5 in \cite{bai2022mp}, it can be shown that 
\begin{align*}
    \frac{1}{n} \sum_{1 \leq i \leq 2n} I\{ A_i = a\} D_i & \cp E[D_i(a)] \\
    \frac{1}{n} \sum_{1 \leq i \leq 2n} I\{ A_i = a\} Y_i^r & \cp E[\tilde Y_i^r(a)] 
\end{align*}
for $r = 1, 2$. Note in addition that
\[ \hat U_i = Y_i - \frac{1}{2n} \sum_{1 \leq i \leq 2n} Y_i - \bigg ( D_i - \frac{1}{2n} \sum_{1 \leq i \leq 2n} D_i \bigg ) \hat \Delta_n~. \]
It follows from direct calculation that
\[ \hat \omega_n^2 = \frac{\displaystyle \frac{1}{n} \sum_{1 \leq i \leq 2n} \hat U_i^2}{\displaystyle \bigg ( \frac{2}{n} \sum_{1 \leq i \leq 2n} A_i D_i - \frac{1}{n} \sum_{1 \leq i \leq 2n} D_i \bigg )^2}~. \]
The conclusion then follows from the above derivations, the continuous mapping theorem, and additional direct calculations.
\end{proof}

%\subsection{Proof of Theorem \ref{theorem:varianceestimation'}}
%The desired results follows by Lemmas \ref{lemma:Sigma11}-\ref{lemma:PCLATEPC} and \ref{lemma:TildeY1+Y0D_i+D0}. Assumption \ref{ass:Q}-(e) does not appear in the proofs for Lemmas \ref{lemma:Sigma11}-\ref{lemma:WaDa}, but it's required for Theorem \ref{theorem:varianceestimation} because the consistent estimator of $P \{C_i = 1\}$, which is $\left( \hat{\phi}_n(1) - \hat{\phi}_n(0) \right)$, appeared in the denominator, whose limit should be not equal to zero.

\subsection{Proof of Theorem \ref{theorem:adjustment}}
% Consider the covariate-adjusted estimator defined by
%     \[ \hat \Delta_n^{\rm adj} = \frac{\hat \psi_n^{\rm adj}(1) - \hat \psi_n^{\rm adj}(0)}{\hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0)}~, \]
%     where
%     \begin{align*}
%         \hat \psi_n^{\rm adj}(a) & = \frac{1}{2n} \sum_{1 \leq i \leq 2n} (2 I \{A_i = a\} (Y_i - \hat m_{a, \tilde Y}(X_i, W_i)) + \hat m_{a, \tilde Y}(X_i, W_i)) \\
%         \hat \phi_n^{\rm adj}(a) & = \frac{1}{2n} \sum_{1 \leq i \leq 2n} (2 I \{A_i = a\} (D_i - \hat m_{a, D}(X_i, W_i)) + \hat m_{a, D}(X_i, W_i))~.
%     \end{align*}
\begin{proof}
Note
    \[ \sqrt n (\hat \Delta_n^{\rm adj} - \Delta(Q)) = \frac{\sqrt n(\hat \psi_n^{\rm adj}(1) - \hat \psi_n^{\rm adj}(0) - \Delta(Q)(\hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0)))}{\hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0)}~. \]
    Note by similar arguments to the proof of Theorem 3.1 in \cite{bai2023covariate} and \eqref{eq:m-D-close},
    \begin{equation}\label{eq:adj-phi-hat-cp-pc}
        \hat \phi_n^{\rm adj}(1) - \hat \phi_n^{\rm adj}(0) \cp P \{C_i = 1\}~.
    \end{equation}
    Therefore, to understand the limiting distribution of $\hat \Delta_n^{\rm adj}$, it suffices to show that
    \begin{equation}\label{eq:adj-clt}
        \sqrt n(\tilde \psi_n^{\rm adj}(1) - \tilde \psi_n^{\rm adj}(0)) \cd N(0, \nu_{1, \rm adj}^2 + \nu_{2, \rm adj}^2 + \nu_{3, \rm adj}^2)~,
    \end{equation}
    where
    \begin{align*}
        \tilde \psi_n^{\rm adj}(a) &= \frac{1}{2n} \sum_{1 \leq i \leq 2n} (2 I \{A_i = a\} (Y_i^\ast - \hat m^\ast_{a, \tilde Y D}(X_i, W_i)) 
        + \hat m^\ast_{a, \tilde Y D}(X_i, W_i))~, \\
        \hat m^\ast_{a, \tilde Y D}(X_i, W_i) &= \hat m_{a, \tilde Y}(X_i, W_i) - \Delta(Q) \hat m_{a, D}(X_i, W_i)~.
    \end{align*}
    % Note $Y_i - \Delta D_i$ is indeed the adjusted outcome we defined early, so proof of effiency and variance estimator can be based on this adjusted outcome too. Then the proof follows similarly to that of Theorem 3.1 in \cite{bai2023covariate}.
    First note that \eqref{eq:m-Y-hat-close} and \eqref{eq:m-D-close} imply that
    \begin{equation}\label{eq:m-star-hat-close}
        \frac{1}{\sqrt{2n}} \sum_{1\leq i\leq 2n} (2A_i - 1) (\hat m^\ast_{a, \tilde Y D}(X_i, W_i) - m_{a, \tilde Y D}(X_i, W_i)) \cp 0~,
    \end{equation}
    then we have
    \begin{align*}
        \tilde \psi_n^{\rm adj}(1) &= \frac{1}{2n} \sum_{1\leq i\leq 2n} (2 A_i (Y_i^\ast(1) - \hat m^\ast_{1, \tilde Y D}(X_i, W_i)) + \hat m^\ast_{1, \tilde Y D}(X_i, W_i) ) \\
        &= \frac{1}{2n} \sum_{1\leq i\leq 2n} (2 A_i Y_i^\ast(1) - (2A_i - 1) \hat m^\ast_{1, \tilde Y D}(X_i, W_i) ) \\
        &= \frac{1}{2n} \sum_{1\leq i\leq 2n} (2 A_i Y_i^\ast(1) - (2A_i - 1) m_{1, \tilde Y D}(X_i, W_i) ) + o_P(n^{-1/2}) \\
        &= \frac{1}{2n} \sum_{1\leq i\leq 2n} (2 A_i Y_i^\ast(1) - A_i m_{1, \tilde Y D}(X_i, W_i) - (1 - A_i) m_{1, \tilde Y D}(X_i, W_i) ) + o_P(n^{-1/2})~,
    \end{align*}
    where the third equality follows from (\ref{eq:m-star-hat-close}). Similarly,
    \begin{align*}
        \tilde \psi_n^{\rm adj}(0) = \frac{1}{2n} \sum_{1\leq i\leq 2n} ( 2(1 - A_i) Y_i^\ast(0) - A_i m_{0, \tilde Y D}(X_i, W_i) - (1 - A_i) m_{0, \tilde Y D}(X_i, W_i) ) + o_P(n^{-1/2})~.
    \end{align*}
    Then
    \begin{align*}
        \tilde \psi_n^{\rm adj}(1) - \tilde \psi_n^{\rm adj}(0) &= \frac{1}{n} \sum_{1\leq i\leq 2n} A_i \phi^\ast_{1, i} - \frac{1}{n} \sum_{1\leq i\leq 2n} (1 - A_i) \phi^\ast_{0, i} + o_P(n^{-1/2})~,
    \end{align*}
    where
    \begin{align*}
        \phi^\ast_{1, i} &= Y_i^\ast(1) - \frac{1}{2} (m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) )~, \\
        \phi^\ast_{0, i} &= Y_i^\ast(0) - \frac{1}{2} (m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) )~.
    \end{align*}
    Then we have the decomposition
    \begin{align*}
        \sqrt{n} (\tilde \psi_n^{\rm adj}(1) - \tilde \psi_n^{\rm adj}(0)) = A^{\rm adj}_n - B^{\rm adj}_n + C^{\rm adj}_n - D^{\rm adj}_n + o_P(n^{-1/2})~,
    \end{align*}
    where
    \begin{align*}
        A^{\rm adj}_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} (A_i \phi^\ast_{1, i} - E[A_i \phi^\ast_{1, i} | X^{(n)}, A^{(n)}])~, \\
        B^{\rm adj}_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} ( (1 - A_i) \phi^\ast_{1, i} - E[ (1 - A_i) \phi^\ast_{1, i} | X^{(n)}, A^{(n)}])~, \\
        C^{\rm adj}_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} A_i (E[Y^\ast_i(1)|X_i] - E[Y^\ast_i(1)] )~, \\
        D^{\rm adj}_n &= \frac{1}{\sqrt{n}} \sum_{1\leq i\leq 2n} (1 - A_i) (E[Y^\ast_i(0)|X_i] - E[Y^\ast_i(0)] )~.
    \end{align*}
    This decomposition holds because of (\ref{eq:Ystar1-Ystar0}). Then the the proof of (\ref{eq:adj-clt}) follows similarly to the proof of Theorem 3.1 in \cite{bai2023covariate}. Therefore, by Slutsky's theorem, the desired conclusion follows by (\ref{eq:adj-phi-hat-cp-pc}) and (\ref{eq:adj-clt}) under Assumption \ref{ass:Q}-(e).

  Next, in order to see that $\hat \Delta_n^{\rm adj}$ in Theorem \ref{thm:zeta} is the optimal linear adjustment, note that $\nu_{\rm adj}^2$ only depends on $m_{a, \tilde Y}(X_i, W_i)$ and $m_{a, D}(X_i, W_i)$ through $\nu_{1, \rm adj}^2$. Then for arbitrary linear adjustments $m_{a, \tilde Y}(X_i, W_i) = \zeta_i' \tilde{\beta}^{\tilde Y}(a)$ and $m_{a, D}(X_i, W_i) = \zeta_i' \tilde{\beta}^{D}(a)$ for $a \in \{0, 1\}$, $\nu_{1, \rm adj}^2$ can be re-written as
  \begin{align*}
    \nu_{1, \rm adj}^2
    &= \frac{1}{2} E[ \var[ E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] 
    - ( m_{1, \tilde Y D}(X_i, W_i) + m_{0, \tilde Y D}(X_i, W_i) ) | X_i ] ] \\
    &= \frac{1}{2} E[ \var[ E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] -  \zeta_i' ( (\tilde \beta^{\tilde Y}(1) + \tilde \beta^{\tilde Y}(0) ) - \Delta(Q) (\tilde \beta^D(1) + \tilde \beta^D(0) )) | X_i ] ] \\
    &= \frac{1}{2} E[ E[ (E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] - E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i\right] -  \\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ (\zeta_i - E[\zeta_i|X_i])' ( (\tilde \beta^{\tilde Y}(1) + \tilde \beta^{\tilde Y}(0) ) - \Delta(Q) (\tilde \beta^D(1) + \tilde \beta^D(0) )) )^2 | X_i ] ] \\
    &= \frac{1}{2} E[ (E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i, W_i \right] - E\left[ Y_i^\ast(1) + Y_i^\ast(0) | X_i\right] -  \\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ (\zeta_i - E[\zeta_i|X_i])' ( (\tilde \beta^{\tilde Y}(1) + \tilde \beta^{\tilde Y}(0) ) - \Delta(Q) (\tilde \beta^D(1) + \tilde \beta^D(0) )) )^2  ]~,
\end{align*}
  which minimized when
   \begin{align}\label{eq:var_min}
    (\beta^{\tilde Y}(1) + \beta^{\tilde Y}(0) ) - \Delta(Q) (\beta^D(1) + \beta^D(0) ) &= (E[\var[\zeta_i|X_i]])^{-1} E[\cov[E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i], \zeta_i | X_i]] \\
    &= (E[\var[\zeta_i | X_i]])^{-1} E[ \cov[Y^\ast_i(1) + Y^\ast_i(0), \zeta_i | X_i] ]~,
\end{align}
where the first equality follows from the first order condition of minimizing $\nu_{1, \rm adj}^2$ and the second equality follows by the fact that
\begin{align*}
    &E[\cov[E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i], \zeta_i | X_i] - \cov[Y^\ast_i(1) + Y^\ast_i(0), \zeta_i | X_i]] \\
    =&E[\cov[E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i] - (Y^\ast_i(1) + Y^\ast_i(0)), \zeta_i | X_i]] \\
    =&E[ (E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i] - (Y^\ast_i(1) + Y^\ast_i(0))) \zeta_i ] \\
    =& E[E[ (E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i] - (Y^\ast_i(1) + Y^\ast_i(0))) \zeta_i |X_i, W_i ]] \\
    =& E[(E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i] - E[Y_i^\ast(1) + Y_i^\ast(0)|X_i, W_i]) \zeta_i  ]\\
    =&0~,
\end{align*}
where the fourth equality follows by the fact that $\zeta_i$ depends only on $X_i$ and $W_i$, the rest of the equalities follows by inspection.

Finally, note that \eqref{eq:var_min} holds when $\tilde{\beta}^{\tilde Y}(a) = \beta^{\tilde Y}$ and $\tilde{\beta}^{D}(a) = \beta^D$ for $a \in \{0, 1\}$, as desired.
\end{proof}

\subsection{Proof of Theorem \ref{thm:zeta}}
\begin{proof}
The result follows from applying the arguments in the proof of Theorem 4.2 in \cite{bai2023covariate} to $\hat \beta_n^Y$ and $\hat \beta_n^D$.
\end{proof}

\subsection{Proof of Theorem \ref{theorem:varianceestimation-adj}}
\begin{proof}
First note that (\ref{eq:adj-phi-hat-cp-pc}) and continuous mapping theorem implies 
    \[\left(\hat{\phi}^{\rm adj}_n(1) - \hat{\phi}^{\rm adj}_n(0)\right)^2 \cp P \{C_i = 1\}^2~.\]
    It thus suffices to show that the numerator converges to the desired quantity.
    
    Consider the following infeasible version of the numerator, given by 
    \[\tilde{\tau}^2_{n, \rm adj} - \frac{1}{2}(\tilde{\lambda}^2_{n, \rm adj} + \tilde{\Gamma}^2_{n, \rm adj})~,\]
    where 
    \[\tilde{\tau}^2_{n, \rm adj} = \frac{1}{n}\sum_{1 \le j \le n}(\hat Y^\ast_{\pi(2j), \rm adj} - \hat Y^\ast_{\pi(2j-1), \rm adj})^2~,\]
    \[\tilde{\lambda}^2_{n, \rm adj} = \frac{2}{n}\sum_{1 \le j \le \lfloor \frac{n}{2} \rfloor}\left(\hat Y^\ast_{\pi(4j-3), \rm adj} - \hat Y^\ast_{\pi(4j-2), \rm adj}\right)\left(\hat Y^\ast_{\pi(4j-1), \rm adj} - \hat Y^\ast_{\pi(4j), \rm adj}\right)\left(A_{\pi(4j-3)} - A_{\pi(4j-2)}\right)\left(A_{\pi(4j-1)} - A_{\pi(4j)}\right)~,\]
    \[\tilde{\Gamma}_{n, \rm adj} = \frac{1}{n}\sum_{1 \le i \le 2n: A_i = 1}\hat Y_{i, \rm adj}^\ast - \frac{1}{n}\sum_{1 \le i \le 2n: A_i = 0}\hat Y_{i, \rm adj}^\ast~,\]
    \[\hat Y_{i, \rm adj}^\ast = Y_i^\ast - \frac{1}{2} (\hat m^\ast_{1, \tilde Y D}(X_i, W_i) + \hat m^\ast_{0, \tilde Y D}(X_i, W_i))~.\]
    It then follows from the arguments in the proof of Theorem 3.2 in \cite{bai2023covariate} that this infeasible numerator converges to the desired quantity. It thus remains to show that 
    \begin{align}
    \label{eq:tau-adj} \hat{\tau}^2_{n, \rm adj} & = \tilde{\tau}^2_{n, \rm adj} + o_P(1) \\
    \label{eq:lambda-P-adj} \hat{\lambda}^2_{n, \rm adj} & = \tilde{\lambda}^2_{n, \rm adj} + o_P(1) \\
    \label{eq:gamma-P-adj} \hat{\Gamma}_{n, \rm adj} & = \tilde{\Gamma}_{n, \rm adj} + o_P(1)~.
    \end{align}
    The rest of the proof is similar to that of Theorem \ref{theorem:varianceestimation} and is omitted.
\end{proof}
\subsection{Auxiliary Results}

\begin{lemma}\label{lemma:efficiency-bound}
    $\nu^2$ in Theorem \ref{theorem:main} matches the expression of the efficiency bound in Theorem 2 of \cite{frolich2007nonparametric}.
\end{lemma}

\noindent {\sc Proof}: 
The efficiency bound in Theorem 2 of \cite{frolich2007nonparametric} is
\begin{align*}
    V &= \frac{1}{P \{C_i = 1\}^2} E\left[ \frac{\var[Y_i|X_i, A_i=1] - 2 \Delta(Q) \cov[Y_i, D_i | X_i, A_i = 1] + \Delta(Q)^2 \var[D_i|X_i, A_i = 1]}{P\{A_i = 1| X_i\}} \right. \\
    &\hspace{1.5em} \left. + \frac{\var[Y_i|X_i, A_i=0] - 2 \Delta(Q) \cov[Y_i, D_i | X_i, A_i = 0] + \Delta(Q)^2 \var[D_i|X_i, A_i = 0]}{P\{A_i = 0| X_i\}} \right] \\
    & \hspace{1.5em} + \frac{1}{P \{C_i = 1\}^2} E[  ( E[Y_i | X_i, A_i = 1] - E[Y_i | X_i, A_i = 0] \\
    & \hspace{5em} - \Delta(Q) E[D_i | X_i, A_i = 1] + \Delta(Q) E[D_i | X_i, A_i = 0] )^2 ] \\
    &= \frac{2}{P \{C_i = 1\}^2} E\left[ \var[\tilde Y_i(1)|X_i] - 2 \Delta(Q) \cov[\tilde Y_i(1), D_i(1) | X_i] + \Delta(Q)^2 \var[D_i(1)|X_i] \right. \\
    & \hspace{1.5em} \left. + \var[\tilde Y_i(0)|X_i] - 2 \Delta(Q) \cov[\tilde Y_i(0), D_i(0) | X_i] + \Delta(Q)^2 \var[D_i(0)|X_i] \right] \\
    & \hspace{1.5em} + \frac{1}{P \{C_i = 1\}^2} E\left[  \left( E[\tilde Y_i(1) | X_i] - E[\tilde Y_i(0) | X_i] - \Delta(Q) E[D_i(1) | X_i] + \Delta(Q) E[D_i(0) | X_i] \right)^2 \right] \\
    & = \frac{2}{P \{C_i = 1\}^2} E\left[ 
    \var[\tilde Y_i(1) | X_i] + \var[\tilde Y_i(0) | X_i] + \frac{1}{2} E^2 [\tilde Y_i(1) - \tilde Y_i(0) | X_i] \right. \\
    & \hspace{1.5em} \left. - 2\Delta(Q) \left( \cov[\tilde Y_i(1), D_i(1) | X_i] +  \cov[\tilde Y_i(0), D_i(0) | X_i] + \frac{1}{2} E[\tilde Y_i(1) - \tilde Y_i(0) | X_i] E[D_i(1) - D_i(0) | X_i ]\right) \right.\\
    & \hspace{1.5em} \left. + \Delta(Q)^2 
    \left(   \var[D_i(1) | X_i] + \var[D_i(0) | X_i] + \frac{1}{2} E^2 [D_i(1) - D_i(0) | X_i]\right)
    \right] \\
    &= \frac{2}{P \{C_i = 1\}^2} E\left[ \var[Y_i^\ast(1)|X_i] + \var[Y_i^\ast(0)|X_i] + \frac{1}{2} E^2[Y_i^\ast(1) - Y_i^\ast(0)|X_i] \right]~,
\end{align*}
where the first equality follows by Theorem 2 in \cite{frolich2007nonparametric}, the second equality follows by (\ref{eq:TildeY}) and Assumption \ref{ass:assignment}, the third equality follows by direct calculation, and the fourth equality follows by (\ref{eq:Ystar-a}). Then we have 
\begin{align*}
    & \frac{V}{2} - \nu^2 \\
    &= \frac{1}{ P \{C_i = 1\}^2} \left( E\left[ \var[Y_i^\ast(1)|X_i] + \var[Y_i^\ast(0)|X_i] + \frac{1}{2} E^2[Y_i^\ast(1) - Y_i^\ast(0)|X_i] \right] \right. \\ 
    & \hspace{7.5em} \left. - \var[Y_i^\ast(1)] - \var[Y_i^\ast(0)] + \frac{1}{2} \var[E[Y_i^\ast(1) + Y_i^\ast(0) | X_i]] \right) \\
    &= \frac{1}{ 2 P \{C_i = 1\}^2} \left( - 2\var[E[Y_i^\ast(1)|X_i]] - 2\var[E[Y_i^\ast(0)|X_i]] + E [E^2[Y_i^\ast(1) - Y_i^\ast(0)|X_i]]  + \var[E[Y_i^\ast(1) + Y_i^\ast(0) | X_i]]\right) \\
    &= \frac{1}{ 2 P \{C_i = 1\}^2} \left( E [E^2[Y_i^\ast(1) - Y_i^\ast(0)|X_i]] -  \var[E[Y_i^\ast(1) - Y_i^\ast(0) | X_i]] \right) \\
    &= \frac{1}{ 2 P \{C_i = 1\}^2} E^2[Y_i^\ast(1) - Y_i^\ast(0)] \\
    &= 0~,
\end{align*}
where the first three equalities follow by inspection, and the fourth equation follows Assumption \ref{ass:Q}(e), and the fact that Assumption \ref{ass:Q}(d) implies $E[Y_i^\ast(1) - Y_i^\ast(0)] = 0$.
\qed

\section{Details for Remark \ref{rem:FE_TSLS}} \label{sec:FE_TSLS}
The subvector formula for IV says that
\[ \hat \alpha^{\rm IV}_n = \left ( \frac{1}{n} \sum_{1 \leq i \leq 2n} \tilde A_i D_i \right )^{-1} \left ( \frac{1}{n} \sum_{1 \leq i \leq 2n} \tilde A_i Y_i \right )~, \]
where $\tilde A_i$ is the residual in the projection of $A$ on $\zeta$ and fixed effects. Formally, consider the projection
\[ A_i = \zeta_i' \gamma + \sum_{1 \leq j \leq n} \tau_j I \{i \in \{\pi(2j - 1), \pi(2j)\}\} + U_i~. \]
Let $\hat \gamma_n$ and $\hat \tau_{j, n}$ denote the OLS estimators of $\gamma$ and $\tau_j$. We first calculate $\hat \gamma_n$. To do so, use the subvector formula and note the residual of projection of $\zeta_{\pi(2j - 1)}$ on the fixed effects is $(\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)}) / 2$ and similarly for $2j$. It then follows that
\[ \hat \gamma_n = \left ( \sum_{1 \leq j \leq n} (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)}) (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)})' \right )^{-1} \sum_{1 \leq j \leq n} (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)}) (A_{\pi(2j - 1)} - A_{\pi(2j)})~. \]
Given this, it follows from the orthogonality condition
\[ \sum_{1 \leq i \leq 2n} \left ( A_i - \sum_{1 \leq i \leq 2n} \zeta_i' \hat \gamma_n - \sum_{1 \leq j \leq n} \hat \tau_{j, n} I \{i \in \{\pi(2j - 1), \pi(2j)\}\}) \right ) I \{i \in \{\pi(2j - 1), \pi(2j)\}\} = 0 \]
that
\[ \hat \tau_{j, n} = \frac{1}{2} - \frac{1}{2} (\zeta_{\pi(2j - 1)} + \zeta_{\pi(2j)})' \hat \gamma_n~. \]
Therefore,
\[ \tilde A_{\pi(2j - 1)} = \left ( A_{\pi(2j - 1)} - \frac{1}{2} \right ) - \frac{1}{2} (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)})' \hat \gamma_n \]
and similarly for $\tilde A_{\pi(2j)}$. We then have
\begin{align*}
     & \sum_{1 \leq i \leq 2n} \tilde A_i D_i \\
     & = \frac{1}{2} \sum_{1 \leq i \leq 2n}(D_{\pi(2j - 1)} - D_{\pi(2j)}) (A_{\pi(2j - 1)} - A_{\pi(2j)}) - \frac{1}{2} \sum_{1 \leq j \leq n} (D_{\pi(2j - 1)} - D_{\pi(2j)}) (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)})' \hat \gamma_n \\
     & = \frac{1}{2} \sum_{1 \leq i \leq 2n} (D_i - \zeta_i' \hat \beta_n^D) (2 A_i - 1)
\end{align*}
where the last step follows because
\[ \sum_{1 \leq j \leq n} (D_{\pi(2j - 1)} - D_{\pi(2j)}) (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)})' \hat \gamma_n = \hat \beta_n^{D'} \sum_{1 \leq j \leq n} (\zeta_{\pi(2j - 1)} - \zeta_{\pi(2j)}) (A_{\pi(2j - 1)} - A_{\pi(2j)})~, \]
where $\hat \beta_n^D$ is the OLS estimator for $\beta^D$ in \eqref{eq:pfe-D} and equivalently that in the regression of pairwise difference of $D$ on pairwise difference in $\zeta$ (see Section 4.2 in \cite{bai2023covariate}). We therefore have $\hat \alpha_n^D = \sum_{1 \leq i \leq 2n} \tilde A_i D_i$ by direct calculation and similarly for $\hat \alpha_n^Y$.

\newpage
\bibliography{bibliography}

\end{document}