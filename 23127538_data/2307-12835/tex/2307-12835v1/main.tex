
\section{Introduction}
\label{section1}
Although Neural Machine Translation~(NMT) has made remarkable advances~\citep{VaswaniSPUJGKP17}, %SutskeverVL14,
%with the advent of neural networks
%. Similar to other tasks, Neural Machine Translation~(NMT)
it still requires large amounts of data to induce correct generalizations that characterize human intelligence~\citep{lake_ullman_tenenbaum_gershman_2017}.
%that characterize human intelligence
%This crucial behaviour can guarantee that the model behaves robustly in the predictions, specifically when data distribution at inference time is different from the data that the model were trained on. 
%The higher generalizability eventually results in a good performance, while a good performance can not guarantee a fair generalization. 
%One handy approach to increase the generalizability of NMT systems is to add more data when training the model.
%When trained on sufficient amounts of data, modern NMT systems can generalize beyond their training data and achieve impressive performance \citep{VaswaniSPUJGKP17}. %However, when NMT systems are exposed to limited imperfect training data, they are not able to generalize as well, which also leads to performance drops~\citep{KoehnK17}.
However, such a vast amount of data to make robust, reliable, and fair predictions is not available for low-resource NMT~\citep{KoehnK17}.
%always available and as a consequence the imperfect generalization leads to performance drop in low-resource data regime~\citep{KoehnK17}. 
%Even with millions of sentence pair there could be always inputs that the model is not able to induce the correct generalization and behave in a human-like way. 

%Generalization can be seen from different angles.
The generalizability of NMT has been extensively studied in prior research, revealing the volatile behaviour of translation outputs when even a single token in the source sentence is modified~\citep{BelinkovB18, FadaeeM20, LiYCZ20}. For instance, in the sentence ``\emph{smallpox killed billions of people on this planet}'' from our IWSLT test set, when replacing the noun ``\emph{smallpox}'' with another acute disease like ``\emph{tuberculosis}'', the model should ideally generate a correct translation by only modifying the relevant part while keeping the rest of the sentence unchanged. However, in many instances, such a small perturbation adversely affects the translation of the entire sentence, highlighting the limited generalization and robustness of existing NMT models \citep{FadaeeM20}.

Compositionality is regarded as the most prominent form of generalization that embodies the ability of human intelligence to generalize to new data, tasks, and domains~\citep{schmidhuber1990towards,LakeB18}, while other types mostly focus on the practical considerations across domains, tasks, and languages, model robustness, and structural generalization~\citep{abs-2210-03050}. 
%Among these, compositionality is often considered as the backbone of the human's ability to generalize over new data, tasks, and domains~\citep{schmidhuber1990towards,LakeB18}.
%, which is defined as the ability to systematically recombine previously learned atoms to map new inputs made up from these atoms to the correct output~\citep{schmidhuber1990towards}.
Research in compositional generalization has two main aspects: evaluating the current models' compositional abilities as well as improving them. 

In terms of evaluation, some studies use artificially created test sets that mimic arithmetic-like compositionality~\citep{LakeB18}, while others evaluate compositionality in a more natural way~\citep{KeysersSSBFKMSS20,KimL20,DankersBH22}. 
In terms of improvement, earlier work aimed to enhance the models' compositional abilities on tasks such as semantic parsing datasets~\citep{QiuSPNLST22}, math word problem solving~\citep{abs-2209-01352}, data-to-text generation~\citep{MehtaRTKPS22}, and classification~\citep{KimRAO20}.
As for NMT, previous work has shown shortcomings in systematic compositional skills~\cite{LakeB18,LiYCZ20}, particularly for low-resource languages~\cite{DankersBH22}, yet no direct improvements have been proposed.

We aim to improve compositionality in NMT,
%which in this task the meaning of the output represents the input. We mainly 
with a focus on low-resource scenarios that necessitate more robustness to form new combinations of previously seen smaller units.
%that have not been seen during training. 
%and allow us to reduce the effect of memorization obtainable by massive training data.
 %To the best of our knowledge, our work is the first attempt to improve compositional generalization for NMT.
%\AA{}{explain why whe choose robustness, domain shift and compositionality}
%Previous approaches investigate the generalizability of NMT demonstrating its volatile behaviour in translation after modifying only one token in the source sentence \citep{BelinkovB18,FadaeeM20,LiYCZ20}. For example, consider the sentence ``\emph{smallpox killed billions of people on this planet}'' from our IWSLT test set.  When the noun ``\emph{smallpox}'' is substituted with another acute disease e.g.,``\emph{tuberculosis}'', the model should be able to translate it correctly by only replacing the modified part in the output and keep the rest of the sentence intact.
%However, in many cases the translation of the whole sentence is negatively influenced by the small perturbation, reflecting the weak generalization and robustness of current NMT models~\citep{FadaeeM20}.
%This issue becomes more severe when the training parallel corpora are insufficient and the systems easily over-fit. 
%There are two main approaches to improve generalization in low-resource NMT: (1) exploiting external data, such as monolingual data~\citep{GulcehreFXCBLBS15,SennrichHB16} and parallel data from other language pairs~\citep{ZophYMK16}, and (2) making the most of the model capacity by tailoring the model architecture to the low-resource setting~\citep{SennrichZ19, AraabiM20}, e.g., using different types of regularization techniques such as dropout.
%To the best of our knowledge, robustness of low-resource NMT against input perturbation is still not well understood.
To achieve this, we introduce Joint Dropout (JD), a simple and effective method that jointly replaces translation-equivalent phrase pairs in the source and target sentences with variables, encouraging the model to maintain the translation of the remaining sentence, regardless of the dropped phrases.
%JD is inspired by dropout-like generalization and also mainly rooted in hierarchical phrase-based statistical machine translation~\citep{Chiang05}, where variables are used to learn phrase reorderings to improve the generalization in Phrase-Based Statistical Machine Translation~(PBSMT).
%Unlike the first category of generalization approaches, JD  does not require additional data sources. Similar to  the second category, JD draws inspiration from previous dropout-like regularization methods.
%Also, different from prior work on improving the robustness of NMT models in high-resource settings~\citep{ChengJM19,XuADWJ21}, we only focus on extremely low-resource languages, where training data is limited and often available only for a specific domain, so more generalization and robustness is
%which suffers more from the lack of robustness due to limited data.
%JD is simple and easy to apply as it only needs a word aligner to create a phrase translation table. Our approach is rooted in hierarchical phrase-based statistical machine translation~\citep{Chiang05}, where variables are used to learn phrase reorderings to improve the generalization in Phrase-Based Statistical Machine Translation~(PBSMT). While \citet{Chiang05} exploits variables to capture the rules of a synchronous context-free grammar between source and target phrases, we use pairs of variables in the source and target sentences as placeholders, which guide the model to preserve the translation of the rest of the sentence, regardless of the dropped phrases.
%\VN{Not fully clear. Instead of ``enable [...] to translate'' maybe ``guide the model to preserve the translation of the rest of the sentence''. Because models do translate the rest of the sentence anyway, they just might do it wrong.}{}
JD is orthogonal to and compatible with other methods for improving NMT performance. Specifically, it is designed to be data-centric and model-agnostic, allowing it to be easily combined with existing techniques that focus on different aspects of the NMT pipeline.


Our analysis on simulated and real low-resource data demonstrates JD's ability to significantly improve compositional generalization and translation quality.
%that JD improves the performance of Transformer-based models up to $+3.1$ BLEU. Our analysis shows that models trained with JD are much more robust to sentence perturbation. Finally, our evaluation on a test set with a different domain distribution demonstrates the effectiveness of JD in a more realistic low-resource scenario, with domain mismatch.
%where only limited data exists in a few domains while the model has to perform well on other domains. 
%\VN{simplify the syntax of this last sentence}{}

\section{Methodology}
\label{section2}
\if0
In this section, before introducing Joint Dropout, we describe the basic SMT preliminaries from the literature.

\header{SMT preliminaries}

%As one of the outstanding approaches  The systematic word order difference between two languages was one of the crucial problems of SMT systems, where the system had to decide in which order to translate the given source words and it was not able to generalize the word orders.
The problem of generalization has been long discussed in the context of machine translation. 
One of the ideas to address this problem within statistical translation models, is PBSMT~\citep{GalleyM08,FengMN10}, which is based on the translation of phrases as atomic units~\citep{ZensON02}. These phrases are mapped one-to-one between two languages according to a phrase translation table that is already learned through a word alignment model.
% that maps the words in the source and target sentence into each other.
A phrase pair $N$ that is consistent with the word alignment is added to the phrase table:
\begin{equation}
    N\rightarrow  \mbox{\emph{$f_1$ ... $f_m$} \:|\: \emph{$e_1$ ... $e_n$}} ,
    \label{eq1}
\end{equation}
where $f_i$ and $e_j$ are terminal symbols~(words) in source and target languages.
Thus, translating word groups instead of single words helps to resolve translation ambiguities and address local reordering, resulting in more generalization.
%\VN{Don't leave empty line above and below equation. It causes the text to be indented. The ``where'' should not be indented.}{}
However, PBSMT models only handle local reorderings. For example, they are able to translate the French phrase \emph{maison bleu} to English \emph{blue house} by swapping the order of the adjective and the noun. However, these models do not have an explicit representation of how to order phrases.
Building upon the insights of PBSMT, \citet{Chiang05} presents a probabilistic synchronous context-free grammar~(PSCFG) translation model that uses the bilingual phrase pairs of PBSMT as starting point to learn hierarchical rules. To this end, we can generalize each already obtained rule \ref{eq1},
%phrase pair extracted from a sentence pair is assigned a generic nonterminal as left-hand-side, making it an initial rule. Then, we can generalize each already obtained rule
%\begin{equation}
%    N\rightarrow  \mbox{\emph{$f_1$ ... $f_m$} \:|\: \emph{$e_1$ ... $e_m$}}
%\end{equation}
for which there is an initial rule
\begin{equation}
    M\rightarrow  \mbox{\emph{$f_i$ ... $f_u$} \:|\: \emph{$e_j$ ... $e_v$}} ,
\end{equation}
where $1 \leq i<u\leq m$ and $1\leq j< v\leq n$, to obtain a new rule

\begin{equation}
    N_{new}\rightarrow  \mbox{\emph{$f_1^{i-1}$ $X_k$ $f_{u+1}^m$} \:|\: \emph{$e_1^{j-1}$ $Y_k$ $e_{v+1}^n$}} ,
\end{equation}
where $f_1^{i-1}$ denotes $f_1 ... f_{i-1}$, $k$ is an index for nonterminals and X and Y indicate the one-to-one correspondence between the tokens on either side. The recursive form of this generalization method generates more rules with multiple nonterminal symbols. Thus, PSCFG methods attempt to generalize beyond the purely lexical knowledge represented in PBSMT models, allowing reordering decisions to be explicitly encoded in each rule.
\fi
%The issue of generalization in the field of machine translation has been a topic of discussion for a long time. Back in Statistical MT, one of the prominent attempts to increase generalization was to use phrases as atomic translation units in order to take contextual information into account~\cite[Phrase-Based Statistical Machine Translation~(PBSMT)]{ZensON02}. \citet{Chiang05} presents Hierarchical PBSMT that uses bilingual phrase pairs of PBSMT as a starting point to learn hierarchical rules in order to obtain more generalization by capturing discontinuous translation equivalences.
Generalization has been a longstanding concern in the field of machine translation. In the past, Statistical MT utilized phrases as the fundamental translation units in order to consider contextual information, such as in Phrase-Based Statistical Machine Translation~\cite[PBSMT]{ZensON02}. To increase generalization, Hierarchical PBSMT proposed by \citet{Chiang05} builds upon the bilingual phrase pairs of PBSMT to learn hierarchical rules, capturing discontinuous translation equivalences and therefore allowing for better generalization.
%\header{Joint Dropout}
%\CM{Expand this subsection. Include: why you can't use PSCFG directrly and how this approximates this. Also provide a concrete example showing what the variables are supposed to do. Also discuss some limitations.}{}

Similarly, JD leverages bilingual phrases to make the rest of the translation not dependent on a specific phrase pair. However, the main idea behind JD originates from compositionality:
%One of the most well-known definitions of compositionality is
the meaning of a %compound expression
sentence is a function of the meanings of its
known atoms and how they are systematically and syntactically combined~\citep{partee1984compositionality}. 
By substituting \emph{meaning} with \emph{translation} in this definition, we come up with a rule of compositionality for translation systems:
\begin{equation}
    \tau (P \:\circ \: Q)=\tau (P)\; \circ \; \tau (Q)
\label{eq1}
\end{equation}
in which $\tau$ is the translation function, $P$ and $Q$ are the constituents of the sentence, and $\circ$ is a combiner. %Based on this rule of compositionality, the translation of a sentence is a function of the translations of its constituent parts. 
%and the way they are syntactically combined. 
%Therefore, if we replace one word or phrase with another in the input sentence, ideally
%the model should be able to translate the whole sentence by only replacing the modified part and the rest of translation should not be negatively influenced by the change. However, currently used NMT systems are not able to generalize well on these kind of changes in the input sentence and they show an unrobust or volatile behaviour toward these changes. 
JD aims to transfer the principle of compositionality to the translation model in order to improve generalization and robustness of NMT by replacing  joint phrases with variables.
%Given the sentence ``smallpox killed billions of people on this planet'' from our IWSLT test set, if we take out the noun phrases in the sentence, what remains will be the pseudo-sentence, ``$X_1$ killed $X_2$ on $X_3$'', which describes the connection between $X_1$, $X_2$, and $X_3$.
To exemplify, given the De-En sentence pair $\langle$\emph{Sie hat Rom besucht, She visited Rome}$\rangle$, we replace nouns with variables: $\langle$\emph{$X_1$ hat $X_2$ besucht, $Y_1$ visited $Y_2$}$\rangle$. 
%According to Eq.\ref{eq1}:
Per Equation\ref{eq1}:
\begin{equation}
\small
    \begin{aligned}
        &\quad\;\tau(\textrm{Sie hat Rom besucht})\\
         &=\tau(((X_1 \:\textrm{hat}\: X_2 \:\textrm{besucht})\circ_{X_1}\:\textrm{Sie})\:\circ_{X_2}\textrm{Rom})\\
          &=\tau((X_1 \:\textrm{hat}\: X_2 \:\textrm{besucht})\circ_{X_1}\textrm{Sie})\circ_{\tau(X_2)}\tau(\textrm{Rom})\\
         &= (\tau(X_1 \:\textrm{hat}\: X_2 \:\textrm{besucht})\circ_{\tau(X_1)}\tau(\textrm{Sie}))\circ_{\tau(X_2)}\tau(\textrm{Rom})\\
         &=((Y_1 \: \textrm{visited} \: Y_2)\circ_{Y_1}\:\textrm{She})\circ_{Y_2}\textrm{Rome}\\
         &=\textrm{She visited Rome}
    \end{aligned}
\end{equation}
%Given the De-En sentence pair <Sie hat Mailand besucht, Peter visited Berlin>, if we take out the nouns, what remains will be the pseudo-sentence pair <$X_1$ hat $X_2$ besucht, $Y_1$ visited $Y_2$> which describes the connection between variables.
%``smallpox killed billions of people on this planet'' from our IWSLT test set, if we take out the noun phrases in the sentence, what remains will be the pseudo-sentence, ``$X_1$ killed $X_2$ on $X_3$'', which describes the connection between $X_1$, $X_2$, and $X_3$.
where $\tau(X_i)=Y_i$, and $\sigma \circ_{X_i} \gamma = \sigma[X_i \backslash \gamma]$, i.e., $\circ_{X_i}$ performs the replacement of $\gamma$ in the position $X_i$ in the sentence $\sigma$.
%$\circ_{X_i}$ denotes the place of its following element.
In the above sketch, we disregard any potential dependencies within the sentence. However, the variables are independent of the rest of the sentence in any manner.
Therefore, our goal is to enable the model to translate the entire sentence without being affected by the specific words or phrases at position $X_i$.
Hence, if the model learns the rules of composition properly, changing one or more lexical units will not hurt the rest of the translation.  
%Recently, many works have benefited from masking spans of the text and letting the representations learn, no matter what has been masked~\citep{DevlinCLT19,LiuGGLEGLZ20}. However, such a technique is mostly used to pre-train a general model of language understanding before being fine-tuned on down-stream tasks and there are only few attempts to use masking for NMT purposes~\citep{GuoXC20}.
To this end, inspired by hierarchical PBSMT, 
%which uses the bilingual phrase pairs to improve generalization in SMT, 
we make use of bilingual phrases to improve generalization in low-resource NMT. However, since NMT has a strong capability to learn ordering through the cross-attention mechanism~\citep{ToralS17}, our aim is not to directly apply hierarchical PBSMT to NMT, but to propose an approximation as a lightweight and efficient regularization method. %Therefore, JD does not involve any hierarchies and only takes advantage of replacing parallel phrases with variables. Thereby, in contrast to common masking strategy, when we mask a phrase by replacing it with a variable~($X_1$) in the source side, the model is also aware of the position of its corresponding translation in the target side~($Y_1$). 

First, using Eflomal~\citep{OstlingT16},
%~\footnote{github.com/robertostling/eflomal}
an efficient word alignment tool, we generate symmetrized word alignments for the parallel training corpus to find the correspondences between source and target words in each pair of training sentences. Then, we use alignments as the input to generate the phrase translation table
%. This phrase translation table is created 
by decomposing the source and target sentences into a set of dozens of bilingual phrase pairs that are consistent with the word alignment~\citep{KoehnOM03}.
%We adopt this generalization method from SMT to see to what extent this approach can benefit NMT. 
In the next step, we select phrase pairs from the phrase table for each pair of training sentences and replace them with joint variables of~($X_i$,$Y_i$). More specifically, given a pair of sentences $S=\{w_1,w_2,...w_n\}$ and $T=\{w'_1,w'_2,...w'_m\}$, after substitution the sentences are $S=\{w_1,...,X_i,...,w_l,...,X_j,...,w_n\}$ and $T=\{w'_1,...,Y_i,...,w'_k,...,Y_j,...,w'_m\}$,
\if0
as follows:
\begin{description}
\centering
    \small \item $S=\{s_1,...,X_i,...,s_l,...,X_j,...,s_n\}$,
    \small \item $T=\{t_1,...,Y_i,...,t_k,...,Y_j,...,t_m\}$,
\end{description}
\fi
where $X$ and $Y$ are variables corresponding to the source and target phrases, respectively. 
%As an example, consider the following German example and its English translation in TED data from IWSLT $2014$:
\if0
 \hspace{0.6cm}De: der ganze Prozess reagiert sehr empfindlich auf Temperatur und Luftfeuchtigkeit.
 
 \hspace{0.6cm}En: the whole process is very sensitive to temperature and humidity.
 
 \noindent After extracting phrase translation table, we substitute some phrases as follows:
 
 \hspace{0.6cm}De: $X_1$ reagiert sehr empfindlich auf $X_4$ und Luftfeuchtigkeit.
 
 \hspace{0.6cm}En: $Y_1$ is very sensitive to $Y_4$ and humidity.
\noindent In the above example, regardless of the phrase that occurs at position $X_1$, the model learn to place its translation at position $Y_1$. For instance, substitution of  \emph{Das neue Gerät} [the new device], \emph{Mein Körper} [my body], or \emph{Diese Art von Batterie} [this kind of battery] at this position, should not negatively influence the rest of the translation.
\fi
We discuss different criteria to replace phrases with variables in \S\ref{section3.2}.\footnote{The code is available at \url{https://github.com/aliaraabi/Joint_Dropout}}
%We consider different criteria to replace the phrases with variables, i.e., the maximum number of variables in each sentence, lengths of the phrases, phrase types, and maximum number of variables in the training corpus. We discuss these parameters in \S\ref{section3.2}.
Finally, we add the variable-induced corpus to the original training set, effectively doubling its size.\footnote{We ensure all models undergo the same maximum number of updates during training, allowing a fair evaluation.}




\section{Experiments}
In this section, we present a comprehensive overview of our experiments. We begin by providing details regarding the datasets used and the training systems employed. Next, we delve into the specific criteria we considered when replacing phrases with variables. Subsequently, we discuss the significant improvements achieved by our proposed method, JD, across various aspects, including compositional generalization, translation performance, robustness, and the ability to generalize across domains.

\subsection{Experimental setup}
\header{Data} For the preliminary experiments, we use the TED data from the IWSLT 2014 German-English~(De-En) shared translation task~\citep{CettoloNSBF14} and randomly sample 
 %5k, 10k, 20k, and 40k sentence pairs 
from the training data to represent various low-resource settings. In order to evaluate the models trained on IWSLT subsets, we use the concatenation of the IWLST $2014$ dev sets (tst$2010$–$2012$, dev$2010$, dev$2012$) as our test set, which consists of 6,750 sentence pairs.

%Similar to~\citet{AraabiM20}, 
We further evaluate JD on multiple actual low-resource language pairs: Belarusian~(Be), Galician~(Gl), and Slovak~(Sk) TED talks~\citep{QiSFPN18} and Slovenian~(Sl) from IWSLT2014~\citep{CettoloNSBF14} with training sets ranging from 4.5k to 55k sentence pairs. 

In order to evaluate the compositional ability of JD, following~\citet{DankersBH22}, we use an English-Dutch~(En-Nl) training set from OPUS~\footnote{Available on \url{https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/data/README-v2020-07-28.md}}~\citep{TiedemannT20} and randomly sample to create low-resource sets. To evaluate these models, we use both the ‘dev’ and the ‘devtest’ sets from  FLORES-101~\citep{GoyalGCCWJKRGF22} as the validation and test data.

\begin{table}[]
  \centering
  \begin{minipage}[t]{0.4\linewidth}
        \centering
  	\begin{tabular}{ll rr}
		\toprule 
		 \multicolumn{1}{l}{Setup} &
		 \multicolumn{1}{c}{\#Phrases} & 
		 \multicolumn{1}{c}{BLEU}  \\
		\midrule
		T-base &	$ 0 $ &	$ 12.2 $  \\
		T-opt. &	$0$ &	$18.0$\\
		\midrule
		 T-opt. + JD\_PP & $8013$ & $18.6$ \\
		 T-opt. + JD\_VP & $8013$ & $18.8$ \\
		 T-opt. + JD\_NP & $8013$ & $18.7$ \\
		 T-opt. + JD\_Mix & $8013$ & $18.8$ \\
		\bottomrule			
	\end{tabular}
    \captionof{table}{Results of Transformer-base, Transformer-optimized and Joint Dropout with various phrase types on 10K De-En training samples. Noun Phrases~(NP), Prepositional Phrases~(PP), Verb Phrases~(VP), and mixture (Mix) of all the above.}
  \label{tab6}
    \end{minipage}
    \hspace{0.1\linewidth}
    \begin{minipage}[t]{0.4\linewidth}
        \centering
  	\begin{tabular}{ll r}
		\toprule 
		Setup &
		BLEU  \\
		\midrule
		T-opt. &		$18.0$  \\
		T-opt. + JD &		$19.9$\\
		\midrule
		T-opt. + target variables only &  $15.5$ \\
		T-opt. + source variables only &  $17.3$ \\
		T-opt. + not aligned variables &  $17.8$ \\
		\bottomrule		
	\end{tabular}
	\caption{Importance of jointly dropping aligned phrases for model trained on 10K De-En samples.}
	\label{tab7}
    \end{minipage}
  
\end{table}
\header{Pre-processing} We apply punctuation normalization, tokenization, data cleaning, and true-casing using the Moses scripts~\citep{KoehnHBCFBCSMZDBCH07}. The sentence length is limited to a maximum of 175 tokens during training. After replacing phrases with variables, we also apply BPE segmentation~\citep{SennrichHB16a} with the parameter tailored to the low-resource training data~\citep{AraabiM20}. We ensure that variables are not split into smaller segments.

\header{Data annotation} To generate a realistic test set for evaluating robustness against sentence perturbation, we first randomly select 300 translation outputs from the inference stage of baseline systems trained using optimized parameters on 20k samples. These outputs are then ranked using the Direct Assessment~(DA) method by engaging native annotators. The top 100 outputs are then selected and the corresponding outputs from the model trained with JD are extracted and ranked using DA. Next, the input sentences are modified by replacing specific phrases or words while ensuring their syntactic and semantic accuracy. After obtaining the outputs for both the baseline and JD systems on the perturbed sentences, we conduct a DA on them.

\header{Training system} To conduct our experiments, we employ two different models: Transformer-optimized~\citep{AraabiM20}, specifically tailored to low-resource NMT and Transformer-base with its default hyper-parameters~\citep{VaswaniSPUJGKP17}. This choice allows us to demonstrate that the improvements achieved are consistent and independent of the specific model settings.
We use the Fairseq library~\citep{OttEBFGNGA19} for our experiments and average sacreBLEU\footnote{sacreBLEU parameters:
nrefs:1$|$case:mixed$|$eff:no$|$tok:13a$|$smooth:exp$|$version:2.0.0}~\citep{Post18} over three runs as the evaluation metric. All of the models are trained on a single GPU for a few hours with the model parameters ranging from 28M to 47M.



\subsection{Joint Dropout parameters}
\label{section3.2}
The following conditions are considered in replacing phrases with variables. First, we do not allow two adjacent phrases to be replaced with variables.
Although phrases can vary in length, we consider all phrases as potential candidates for substitution with variables, irrespective of their length. 
%We limit the number of variables in a single sentence. 
After conducting initial experiments, we have determined that setting the maximum number of variables allowed in each sentence to $10$ yields satisfactory results.


Since noun phrases are the most cross-linguistically common phrases, we hypothesize that they are good candidates to be replaced.
Therefore, in a set of experiments we investigate the choice of phrase types. We consider four different scenarios: replacing 1)~only Noun Phrases~(NP), 2)~only Prepositional Phrases~(PP), 3)~only Verb Phrases~(VP), and 4)~mixtures of all the above.
We train four systems on $10$k samples from the TED talks dataset with four different substitution scenarios yet the same number of variables~(8013).\footnote{8013 is the number of all possible substitutions for PPs.}
We use the constituency parser from Stanford CoreNLP~\citep{manning-EtAl:2014:P14-5}.

It is important to note that our selection of phrase pairs in both languages is solely based on English constituency parse trees. We do not rely on the use of a constituency parser, which is often not available for many low-resource languages. The results presented in Table~\ref{tab6} demonstrate that the choice of different phrase types does not lead to significant differences in our method. Therefore, our approach eliminates the need for a constituency parser, making it applicable to a wider range of low-resource languages.
% Figure environment removed
%As shown in App.~\ref{appendixB}, there is no significant difference between selecting different phrase types. Thus, our method does not require a constituency parser which is not available for many languages. 
For the rest of the experiments, we substitute phrases regardless of their types.

To make JD independent of a phrase translation table, we consider not-aligned phrases in both or either translation sides. The importance of using aligned phrases is demonstrated in Table~\ref{tab7}, where it is observed that utilizing not-aligned phrases results in a degradation of performance by $2.5$ BLEU points. This finding highlights the significance of incorporating aligned phrases in the JD method.
%For the rest of the experiments, we substitute the phrases regardless of their types. 

%In order to control the number of variables throughout the entire training corpus, we propose the use of a \emph{Joint Dropout rate}, which is calculated as the proportion of dropped tokens (from within phrases) with respect to the total length of the source and target sentences.
To maintain control over the number of variables across the entire training corpus, we introduce a concept called the \emph{Joint Dropout rate}. This rate is determined by calculating the proportion of dropped tokens, specifically from within phrases, in relation to the total length of both the source and target sentences. By utilizing this Joint Dropout rate, we can effectively regulate and manage the presence of variables throughout the training process.
%in the whole training corpus, we introduce \emph{Joint Dropout rate}, the ratio of dropped tokens~(within phrases) to the summation of source and target sentence lengths.
%For a given sentence pair, we first randomly select a set of at most $10$ phrases with no overlapping between them as replacement candidates.
%We randomly replace a subset of candidates according to the Joint Dropout rate.
%The details of the procedure are provided in~App.~\ref{appendixA}.
%Figure~\ref{fig1} illustrates the improvements obtained by two different systems with increasing Joint Dropout rate.JD consistently improves Transformer-base and Transformer optimized by $+2.4$ and $+1.8$ BLEU points on 10k samples, respectively, and also by $+3.1$ and $+1.4$ BLEU points on 20k samples.
Figure~\ref{fig1} illustrates the improvements achieved by two distinct systems as the Joint Dropout rate increases. Notably, JD consistently improves the performance of both the Transformer-base and Transformer optimized models. Specifically, on a dataset of 110k samples, JD yields a notable increase of $+2.4$ BLEU points for the Transformer-base model and $+1.8$ BLEU points for the Transformer-optimized model. Moreover, when evaluating a larger dataset of 20k samples, JD further improves translation quality by $+3.1$ BLEU points for the Transformer-base model and $+1.4$ BLEU points for the Transformer-optimized model.





%VN: not really therefore, it doesnt follow
%Therefore,
We see that
%if we substitute all candidates~(maximum of $10$ variables per sentence), the performance slightly drops, showing that too much noise in the training set hurts the performance.
the Joint Dropout rate of $0.3$ 
%VN: Not optimal mathematically
%seems to be the optimal value
is a good choice%
, while more noise in the training set hurts performance.
We use this rate for the remainder of the experiments. 

\begin{table}[t]
	\centering
	\begin{tabular}{r rrrr}
		\toprule
            & \multicolumn{2}{c}{BLEU} & \multicolumn{2}{c}{Consistency} \\
            \cmidrule(lr){2-3}
            \cmidrule(lr){4-5}
		%& \multicolumn{3}{c}{Newstest2020} \\ 
		%\cmidrule{2-4}
		\multicolumn{1}{l}{\#Samples} &
		\multicolumn{1}{l}{T-opt.} &
            \multicolumn{1}{l}{T.opt.+JD} &
		\multicolumn{1}{c}{T-opt.}   & \multicolumn{1}{c}{T.opt.+JD }\\
		\midrule
		$5k$ & $4.2$ & $\mathbf{6.1}$ &$2.0$& $\mathbf{4.0^\ast}$\\
		  $20k$ & $10.4$ & $\mathbf{10.7}$ &$8.1$ & $\mathbf{11.0^\ast}$\\
            $40k$ & $12.8$ & $\mathbf{13.4}$ &$13.1$ & $\mathbf{15.6^\ast}$\\
		$80k$& $\mathbf{16.4}$ & $\mathbf{16.4}$ &$37.1$ & $\mathbf{43.8^\ast}$\\
		  $200k$ & $\mathbf{19.2}$ & $18.7$ & $58.2$ & $\mathbf{65.4^\ast}$\\
		%\midrule
		%$1m$ & $21.2$ & $21.2$ &$59.1$ & $\mathbf{64.3^\ast}$\\
		%$8m$ &$23.2$ & $23.0$ &$70.0$ & $\mathbf{72.1}$\\
		\bottomrule			
	\end{tabular}
	\caption{BLEU and consistency scores (En~$\rightarrow$ Nl) when replacing a noun %from the NP
 in the subject position with a different noun.
 %while preserving agreement with the VP. 
 Significant improvements on compositionality of JD over the strong baseline are marked with $\ast$ (approximate randomization, p $<$ 0.01).}
	\label{tab9}
\end{table}

\subsection{Compositional generalization}
%In order to acquire human intelligence, 
%the NMT systems should be able to understand and combine the meaning from different pieces of a sentence. Principally, an NMT system is a function that maps the surface forms from the input sentence to the meaning and then maps this meaning to the surface forms in a sentence in another language. Therefore, 
%the model should be able to build up the meaning of an expression by combining the meaning of its constituents~\citep{partee1984compositionality}.
%This concept is conventionally defined as compositionality
%VN: "cases" is a very vague word here
%Unlike a few cases, 
Unlike phenomena
such as idioms,
%the meaning of the whole expression can not be directly understood from its part and it 
which require a more global understanding,
%However, since 
JD concentrates on improving compositionality at the local level. In this section, we aim to evaluate our method on local compositionality. Here, we take advantage of the most relevant theoretically grounded test from~\citet{HupkesDMB20} which is \emph{systematicity}, a notion frequently used in the context of compositionality. This attribute of the model concerns the recombination of known parts and rules, ensuring that the model's ability to grasp novel inputs is systematically tied to their aptitude to comprehend related inputs. For instance, understanding ``smallpox killed billions of people on this planet'' and ``tuberculosis'', also implies understanding ``tuberculosis killed billions of people on this planet''. 

Given that there are an infinite number of potential novel combinations that can be derived from known parts in natural data, we concentrate on a sentence-level, context-free rule: S~$\rightarrow$ NP VP, as proposed by~\citet{DankersBH22}, where a noun from the NP in the subject position is replaced with a different noun, while maintaining number agreement with the VP. 
Additionally, they highlight that a systematic system necessitates consistency. We assess this systematicity of translations based on their consistency across various contexts when presenting words or phrases. Consistency is measured by evaluating the equality between two translations while taking into account anticipated modifications. In S~$\rightarrow$~NP~VP setup, after replacement, translations are deemed consistent if there is only one word difference between them. Table~\ref{tab9} illustrates that JD consistently enhances the consistency scores for various low-resource data conditions. 




\subsection{Translation performance}
In this section, we conduct a comprehensive evaluation of translation quality across multiple language pairs to assess the effectiveness of JD. The results presented in Table~\ref{tab1} highlight the significant improvements in translation quality achieved by JD for actual low-resource language pairs.
Importantly, these improvements also hold true for the reverse language direction.
%using aligned phrases is necessary, and using not-aligned phrases degrades the performance $2.5$ BLEU scores.

Furthermore, we compare JD to three comparable methods for dropping tokens: Zero-Out, where the embedding of a token is set to zero~\citep{Sennrich16}, Token Drop, which replaces tokens with the \texttt{<dropped>} tag~\cite{ZhangQDZ20}, and SwitchOut, where words are replaced with random words from their corresponding vocabularies~\cite{WangPDN18}.
%We compare JD with three closely related strategies in dropping tokens: Zero-Out sets the embedding of a token to zero~\citep{Sennrich16},Token Drop replaces tokens with a 
%% VN edit: the $<$ in math mode renders with too much space around it, hard to read.
%$<dropped>$ 
%\texttt{<dropped>}
%tag~\cite{ZhangQDZ20}, and SwitchOut randomly replaces words with other random words from their corresponding vocabularies~\cite{WangPDN18}.
%and tries to detect them with Replaced Token Detection~(RTD) and predict them with Dropped Token Prediction~(DTP) as two self-supervised objectives~\cite{ZhangQDZ20}. 
%Table~\ref{tab2} shows that while Zero-Out results in improvements, Token Drop and SwichOut
%even with its two self-supervised objectives 
%are not useful in extremely low-resource scenarios; JD is the best performing approach in both settings.
%\VN{I thought you found that TD was better than JD, no? Is this without the adversarial terms?}{}
%and even its combination with full Token Drop strategy
The results in Table~\ref{tab2-a} demonstrate that Zero-Out only provides marginal improvements. Moreover, both Token Drop and SwitchOut methods prove to be ineffective in low-resource scenarios. In contrast, JD consistently outperforms these methods, particularly in extreme low-resource cases. 
As shown in Table~\ref{tab2-a}, Zero-Out only provides marginal improvements. In addition, while Token Drop and SwitchOut methods prove to be ineffective in low-resource situations, JD consistently yields the largest improvements, especially for extreme low-resource cases. In addition, Table~\ref{tab2-b} provides additional evidence supporting the superiority of JD over similar methods, even when optimized parameters for the Transformer model are not specifically chosen.

\begin{table}[]
	\centering
	%\vspace{-8pt}
	\begin{tabular}{l rrrr | rrrr}
		\toprule 
		 \multicolumn{1}{l}{Method} &
		 \multicolumn{1}{c}{Be-En} & 
		 \multicolumn{1}{c}{Gl-En} & 
		 \multicolumn{1}{c}{Sl-En} &
		 \multicolumn{1}{c}{Sk-En} &
          \multicolumn{1}{c}{En-Be} & 
		 \multicolumn{1}{c}{En-Gl} & 
		 \multicolumn{1}{c}{En-Sl} &
		 \multicolumn{1}{c}{En-Sk}\\
		\midrule
		T-base &	$ 4.6 $ &	$13.4$  & $8.9$   & $24.0$ & $ 3.5 $ &	$10.1$  & $6.8$   & $19.0$ \\
		T-base + JD & $\mathbf{6.5}$ &   $\mathbf{15.8}$  & $\mathbf{10.2}$  & $\mathbf{25.0}$  & $\mathbf{4.5}$ &   $\mathbf{12.9}$  & $\mathbf{7.8}$  & $\mathbf{19.2}$ \\
		\midrule
		T-opt. &	$8.0$   &	$21.8$  & $15.2$  & $28.9$ & $5.5$   &	$18.3$  & $12.3$  & $23.1$ \\
		T-opt. + JD & $\mathbf{9.9}$ &   $\mathbf{22.8}$  & $\mathbf{16.1}$  & $\mathbf{29.8}$  & $\mathbf{7.3}$ &   $\mathbf{18.9}$  & $\mathbf{12.7}$  & $\mathbf{23.5}$\\

		\bottomrule			
	\end{tabular}
	\caption{BLEU scores for actual extremely low-resource languages: Be, Gl, Sl, and Sk with 4.5k, 10k, 13k, and 55k training samples, respectively. %
%\VN{Is there a risk that ``extremely low-resource'' is too strong for En-Sk 55k and could upset readers who work on even more low-resource languages?}{} 
}
	\label{tab1}
\end{table}
\begin{table}[t]
\small
	\centering
        \begin{minipage}[b]{0.4\linewidth}
            \centering
            \begin{tabular}{l rrr}
            
		\toprule 
		\multicolumn{1}{l}{Method} &
		\multicolumn{1}{c}{5k} & 
		\multicolumn{1}{c}{10k} & 
		\multicolumn{1}{c}{20k} \\
		\midrule
		T-opt.     &$13.4$ &$18.0$   &	$23.0$ \\
		T-opt. + ZO & $13.6$ &$ 18.3 $ &	$22.8$  \\
		T-opt. + TD & $9.5$	&$ 16.8 $ &	$23.9$  \\
		T-opt. + SW & $13.4$	&$ 18.4 $ &	$24.0$  \\
		T-opt. + JD & $\mathbf{15.2}$ &$\mathbf{19.9}$ &   $\mathbf{24.4}$\\
		\bottomrule			
	\end{tabular}
        \subcaption{Transformer-optimized}
	\label{tab2-a}    
    \end{minipage}
    \hspace{0.1\linewidth}
    \begin{minipage}[b]{0.4\linewidth}
            \centering
            \begin{tabular}{l rrr}
            
		\toprule 
		\multicolumn{1}{l}{Method} &
		\multicolumn{1}{c}{5k} & 
		\multicolumn{1}{c}{10k} & 
		\multicolumn{1}{c}{20k} \\
		\midrule
		T-base      & $8.6$	&$ 12.1 $ &	$16.6$  \\
		T-base + ZO & $8.9$	&$ 13.3 $ &	$18.3$  \\
		T-base + TD & $5.3$	&$ 8.9 $ &	$14.6$  \\
		T-base + SW & $5.5$	& $9.8$ &	$14.5$  \\
		T-base + JD & $\mathbf{9.8}$ &$\mathbf{14.5}$ &   $\mathbf{19.1}$  \\
		\bottomrule			
	\end{tabular}
        \subcaption{Transformer-base}
	\label{tab2-b} 
    \end{minipage}
\caption{Comparing BLEU scores for Joint Dropout~(JD) and the reimplementations of Token Drop~(TD), Zero Out~(ZO), and SwitchOut~(SW) on 5k, 10k and 20k training samples from IWSLT De-En.}	
\label{tab2} 
\end{table}





\subsection{NMT Robustness}
Recent work has shown that trivial modifications to the source sentence can cause unexpected changes in the translation~\citep{FadaeeM20}. Furthermore, models with stronger compositional abilities are anticipated to generate more robust translations~\cite{DankersBH22}. To evaluate the robustness of JD against such modifications, we differ from previous methods that automatically introduce noise to the test set~\citep{MichelN18, ChengJM19} which is prone to creating semantic and syntactic errors in the input. Instead, we manually develop a more realistic test set. 

\begin{table}[t]
\small
	\centering
	\begin{tabular}{ll rrr}
		\toprule 
		Method &
		Metric &
		Orig.  & 
		Per. &
		$\Delta$\\
		\midrule
		T-base  & \makecell[l]{\small{DA}\\\small{BLEU}} &	\makecell[r]{$62.1$ \\ $28.5$ } &	\makecell[r]{$49.3$\\$26.0$} & \makecell[r]{$-12.8$\\$-2.5$}    \\
		
		\cmidrule{2-5}
		
		T-base + JD & \makecell[l]{\small{DA}\\\small{BLEU}} &\makecell[r]{$69.8$\\$30.7$} &	\makecell[r]{$59.3$\\$30.4$}  & \makecell[r]{$\mathbf{-10.5}$\\$\mathbf{-0.3}$}    \\
		
		\midrule
		
		T-opt. &   \makecell[l]{\small{DA}\\\small{BLEU}}    &	\makecell[r]{$79.9$\\$37.4$}   &	\makecell[r]{$56.6$\\$31.8$}  & \makecell[r]{$-23.3$\\$-5.6$}   \\
		
		\cmidrule{2-5}
		
		T-opt. + JD & \makecell[l]{\small{DA}\\\small{BLEU}} & \makecell[r]{$83.7$\\$41.8$}   &   \makecell[r]{$77.4$\\$39.9$}  & \makecell[r]{$\mathbf{-6.3}$\\$\mathbf{-1.9}$} \\
		
		\bottomrule			
	\end{tabular}
	
	\caption{Direct assessment and BLEU scores, pre and post input perturbation on random samples from De-En test set.}
	
	\label{tab4}
\end{table}

\begin{table*}[t]
	\centering
	% vlad comment: here is how to remove spacing between columns:
	% @{} gives no space at all
	% @{~} adds a small space
	% in fact you can put any character there, eg @{hi} will print the word hi on every row
	\begin{tabular}{l@{~}l@{~}l}
		\toprule 
		 \multicolumn{1}{l}{} &
		 \multicolumn{1}{l}{\small{Original test sentence}} & 
		 \multicolumn{1}{l}{\small{Test sentence after perturbation}}  \\
		\midrule
		\small{Src} &	\makecell[l]{\small{[\textbf{ein Kind in Indien}] sagt:}\\ \small{``heute habe ich einen Affen gesehen''.}} & 
        \makecell[l]{\small{\{\textbf{meine Oma in China}\} sagt:} \\ \small{``heute habe ich einen Affen gesehen''.}} \\[2.5ex]
	\small{Ref.} & \makecell[l]{\small{[a child in India] says} ,\\ \small{``\uline{I saw a monkey} today .''}}	  &	
        \makecell[l]{\small{\{my grandmother in China\} says,} \\
        \small{``\uline{I saw a monkey} today .''}}   \\
		\midrule
		
		\small{T-opt.} &	\makecell[l]{\small{[a child in India] says,} \\ \small{``today \uline{I've seen a monkey}s.''}} &	\makecell[l]{\small{\{my grandmother's mother in China\}}\\ \small{ says, \small{``\uwave{\textcolor{red}{Look}} today.''}}}\\[2.5ex]
		
		\small{T-opt. + JD} & \makecell[l]{\small{[a kid in India] says,} \\  \small{``\uline{I've seen a monkey}s today.''}} &
        \makecell[l]{\small{\{my grandmother in China\} says,} \\ \small{ ``today \uline{\textcolor{OliveGreen}{I've seen a monkey}}s.''}} \\
		\bottomrule			
	\end{tabular}
	\vspace{-2pt}
	\caption{By replacing the German noun phrase \textbf{\emph{ein Kind in Indien}} [a child in India] with \textbf{\emph{meine Oma in China}} [my grandmother in China], there is no undesirable behavior in the rest of the translation when using Joint Dropout. Underlined text means the rest of the translation is approximately the same with the reference, while the wavy underline means it has changed. Bracket shows the phrase that we perturb, while the curly bracket is the perturbed phrase}
	\label{tab3}
\end{table*}
First, based on Direct Assessment~(DA) on a $100$-point scale~\citep{GrahamBMZ13}, we select the top $100$ sentences out of randomly selected $300$ translation outputs generated by a Transformer-optimized model trained on 20k samples. We then alter the input sentences by replacing a specific phrase or word, while ensuring that they remain syntactically and semantically accurate. Table~\ref{tab4} illustrates that perturbing the original sentences results in a smaller performance decrease for the model trained with JD, when compared to the baseline.
This means that our proposed method significantly decreases the volatile behavior of low-resource NMT.

Table~\ref{tab3} shows an example of perturbing a sentence. After replacing ``\emph{ein Kind in Indien}'' in the source sentence with ``\emph{meine Oma in China}'',  while the rest of the translation is negatively affected using the baseline model, the JD shows more robustness against the input perturbation and does not exhibit any negative behavior. 


\subsection{Generalization across domains}
In low-resource language settings, NMT systems frequently encounter challenges when it comes to achieving effective translation across distinct domains. This is primarily attributed to their tendency to prioritize the idiosyncrasies of the training domain, rather than capturing the broader linguistic characteristics shared by the language pairs.
Therefore, in addition to evaluating generalization in terms of compositionality and robustness, it is also crucial to assess generalization concerning distributional shift and uncertainty estimation~\citep{abs-2210-03050}. While the definition of a domain is not precisely defined~\citep{WeesBWM15},for our evaluation, we consider TED talks and news as belonging to different domains.

Table~\ref{tab5} provides insights into the behavior of JD when there is a domain shift between the training domain (TED talks) and the test domain (news from WMT). The results demonstrate that JD exhibits greater robustness in such scenarios, showcasing its ability to better handle distributional shifts and improve translation quality across different domains. This highlights the effectiveness of JD in mitigating the negative effects of domain-specific training and enhancing the generalizability of NMT systems in low-resource language pairs.
%In NMT, translating text that diverges from the canonical training domain is a key challenge~\citep{KoehnK17}.
%and it is a the common way to assess how well a model performs in terms of generalization. %Thus, the model generalization is evaluated on training and testing samples which are independent and identically distributed~(i.i.d).
%Generalization can also be evaluated in relation to distributional shift and uncertainty estimation. 
%The inability to translate a distinct domain is due to NMT systems tend to focus on idiosyncrasies of the training domain rather than general features of the language pairs, especially for less-resource languages. While the definition of what constitutes a domain is not clear~\citep{WeesBWM15}, we consider TED talks and news to be from  different domains.
%Table~\ref{tab5} shows that when the training domain consists of TED talks and the test domain is news from WMT, JD behaves more robustly.
%is more robust to changes in the input distribution.
%Also, the target domain my be unknown when a system is built. The generalization of models to unseen text domains (a.k.a.  domain robustness) is modest for both statistical and neural machine translation. Such failure to translate out-of-domain text is mostly due to the fact that NMT systems focus on and learn idiosyncrasies of the domain and over-fit on the training set rather than learning general features of the language pairs. This issue becomes more severe in low-resource scenario where there is a high probability of over-fitting and the model suffers from lack of a moderate generalization. In particular, for many low-resource language pairs, the training set is limited to one or very few domains. Accordingly, it is worthwhile to develop approaches that enhance domain robustness in low-resource NMT. As shown in Table~\ref{tab2}, when using joint dropout, while the extremely low-resource training corpus is TED talks, our proposed approach improves up to $+1$ BLEU point over the  baseline.

\begin{table}[t]
	\centering
	\begin{tabular}{l rrr}
		\toprule
		%& \multicolumn{3}{c}{Newstest2020} \\ 
		%\cmidrule{2-4}
		\multicolumn{1}{l}{Method} & \multicolumn{1}{c}{10k}   & \multicolumn{1}{c}{20k} & \multicolumn{1}{c}{40k}\\
		\midrule
		T-base          & $2.4$& $3.9$  & $7.1$   \\
		T-base + JD &$\mathbf{3.2}$ & $\mathbf{5.4}$  & $\mathbf{9.8}$   \\
		\midrule
		T-opt.          &$6.2$ & $8.7$  & $13.9$  \\
		T-opt. + JD & $\mathbf{7.5}$ & $\mathbf{10.9}$ & $\mathbf{14.6}$ \\
		\bottomrule			
	\end{tabular}
	\caption{Results of training on different subsamples of TED talks and testing on a domain with different distribution~(Newstest2020).}
	\label{tab5}
\end{table}
\section{Conclusion}
Despite the fact that NMT's success is closely tied to having large amounts of training data, it is still beneficial to explore methods that can help improve generalization when working with limited data. In this paper, we introduce Joint Dropout as a straightforward yet effective approach to enhancing the compositional generalization and translation quality  of low-resource NMT. Specifically, we demonstrate that jointly replacing phrases with variables has a regularizing effect that mitigates overfitting by enabling the system to translate sentences regardless of the specific phrases present at the variable positions.

\section{Future work}
We only focus on improving generalizability of low-resource NMT, while higher-resource settings might also gain from joint variables. Additionally, we demonstrate the effectiveness of our proposed method using multiple low-resource language pairs, whereas there are many other language pairs with limited data. Furthermore, since JD tries to capture the rules of compositionality in translation, we expect more benefit to the language pairs with less similarity. Additionally, our approach is data-centric and model-agnostic, applicable to various models and tasks beyond the methods evaluated in this paper. Therefore, it has the potential to improve existing pre-trained models such as mBART~\citep{LiuGGLEGLZ20}, when fine-tuning on low-resource languages, but further experimentation is needed to confirm its effectiveness.
We leave these investigations to future work.


\section{Broader Impact}
The implementation of NMT has brought about significant progress in the translation field, however, it also poses potential challenges such as liability for mistakes made by using NMT and mistranslation, which could be more of a concern when dealing with limited data. Furthermore, the high ability of NMT to generalize well presents a potential risk of difficulty in identifying errors, specifically those related to compositionality. This can be a concern in safety-critical domains where a single error can have severe consequences. Moreover, the ability of NMT to produce more coherent and fluent translations may impede the identification of where the system is malfunctioning, thus hindering the correction of errors or biases in the model.  %Additionally, the reliance on GPUs for training and inference in NMT systems, like many machine learning models, consume energy and have an impact on the environment, particularly when the models are running on a large scale or for extended periods of time. Nevertheless, our low-resource setup requires only one GPU for a few hours, resulting in a highly energy-efficient approach.
%Our proposed method aims to enhance low-resource NMT by incorporating phrase pairs, resulting in an improvement in generalization. However, it may also result in a decrease in the model's ability to memorize the data.

%*In this paper,  
%we focused on generalization of low resource NMT. 
%8we propose Joint Dropout, a new generalization approach for low-resource NMT which is an approximation of hierarchical PBSMT, as a lightweight efficient regularization technique. In particular, we show jointly replacing phrases with variables has a regularization effect that alleviates over-fitting by translating sentences regardless of what occurs at the positions of variables.
%a generalization approach for SMT.
%This method is expected to improve translation quality because jointly dropping words has a regularization effect that alleviates over-fitting by translating sentences regardless of what occurs in the positions of variables.
%Here, we only apply an approximation of this method on NMT as a lightweight efficient regularization technique.
%In particular, we extract phrase translation table for each training sentence pair and replace a number of the phrase pairs with joint variables.
%Our experiments 

%*Furthermore, we show that JD can significantly improve the translation quality of low-resource language pairs in terms of BLEU score.

%Moreover, our analyzes confirm the effectiveness of Joint Dropout in boosting the robustness and generalizability of NMT against sentence perturbation, measured by both direct assessment and BLEU score.

%*Our experiments confirm that developing generalization methods is necessary for NMT models trained on extremely low-resource languages which may lack memorization obtainable by large training data. 
%*In the future, we aim to study generalization versus memorization in low-resource NMT to gain more insights for improving NMT generalization.
%In the future, we explore the ability of Joint Dropout in improving volatile behavior of high-resource settings.


%\CM{Include a discussion of the limitations. Also repeat the motivation coming from PSCFG and how this is approximated in this paper.}{}

\bibliographystyle{apalike}
\bibliography{main}


\iffalse
\newpage
\appendix
\begin{appendices}


\section{Choice of phrase types}
\label{appendixB}




\section{Translation quality}

\label{AppendixD}
Table~\ref{tab1.5} shows the translation quality improvements also holds in from-English directions.
\begin{table}[H]
	\centering
	%\vspace{-8pt}
	\begin{tabular}{l rrrr }
		\toprule 
		 \multicolumn{1}{l}{Method} &
		 \multicolumn{1}{c}{En-Be} & 
		 \multicolumn{1}{c}{En-Gl} & 
		 \multicolumn{1}{c}{En-Sl} &
		 \multicolumn{1}{c}{En-Sk} \\
		\midrule
		T-base & $ 3.5 $ &	$10.1$  & $6.8$   & $19.0$ \\
		T-base + JD & $\mathbf{4.5}$ &   $\mathbf{12.9}$  & $\mathbf{7.8}$  & $\mathbf{19.2}$ \\
		\midrule
		T-opt. & $5.5$   &	$18.3$  & $12.3$  & $23.1$ \\
		T-opt. + JD & $\mathbf{7.3}$ &   $\mathbf{18.9}$  & $\mathbf{12.7}$  & $\mathbf{23.5}$\\

		\bottomrule			
	\end{tabular}
	\caption{BLEU scores for actual extremely low-resource languages: Be, Gl, Sl, and Sk with 4.5k,
     10k, 13k, and 55k training samples, respectively.%
%\VN{Is there a risk that ``extremely low-resource'' is too strong for En-Sk 55k and could upset readers who work on even more low-resource languages?}{} 
     }
	\label{tab1.5}
\end{table}



\section{Ablation study}
\label{AppendixC}
To make JD independent of a phrase translation table, we consider not-aligned phrases in both or either translation side.
As shown in Table~\ref{tab7}, jointly dropping aligned phrases in the source and target side is essential and losing each of these constraints significantly hurts the performance of the baseline by $2.5$ BLEU points.
\begin{table}[H]
    \small
	\centering
	\begin{tabular}{ll r}
		\toprule 
		Setup &
		BLEU  \\
		\midrule
		T-opt. &		$18.0$  \\
		T-opt. + JD &		$19.9$\\
		\midrule
		T-opt. + target variables only &  $15.5$ \\
		T-opt. + source variables only &  $17.3$ \\
		T-opt. + not aligned variables &  $17.8$ \\
		\bottomrule		
	\end{tabular}
	\caption{Importance of jointly dropping aligned phrases for model trained on 10K De-En samples.}
	\label{tab7}
\end{table}



\section{An example for sentence perturbation}
\label{appendixE}
Table~\ref{tab3} shows an example of perturbing a sentence. After replacing ``\emph{ein Kind in Indien}'' in the source sentence with ``\emph{meine Oma in China}'',  while the rest of the translation is negatively affected with the baseline model, the JD shows more robustness against the input perturbation.

\section{Comparisons with Transformer-base settings}
\label{AppendixF}
Table~\ref{tab10} shows the superiority of JD over similar methods also holds without choosing the optimized Transformer parameters.
%\subsection{Translation quality}
\begin{table}[H]
\small
	\centering
	\begin{tabular}{l rrr}
		\toprule 
		\multicolumn{1}{l}{Method} &
		\multicolumn{1}{c}{5k} & 
		\multicolumn{1}{c}{10k} & 
		\multicolumn{1}{c}{20k} \\
		\midrule
		T-base      & $8.6$	&$ 12.1 $ &	$16.6$  \\
		T-base + ZO & $8.9$	&$ 13.3 $ &	$18.3$  \\
		T-base + TD & $5.3$	&$ 8.9 $ &	$14.6$  \\
		T-base + SW & $5.5$	& $9.8$ &	$14.5$  \\
		T-base + JD & $\mathbf{9.8}$ &$\mathbf{14.5}$ &   $\mathbf{19.1}$  \\
		
		\midrule
		T-opt.     &$13.4$ &$18.0$   &	$23.0$ \\
		T-opt. + ZO & $13.6$ &$ 18.3 $ &	$22.8$  \\
		T-opt. + TD & $9.5$	&$ 16.8 $ &	$23.9$  \\
		T-opt + SW & $13.4$	&$ 18.4 $ &	$24.0$  \\
		T-opt. + JD & $\mathbf{15.2}$ &$\mathbf{19.9}$ &   $\mathbf{24.4}$\\
		\bottomrule			
	\end{tabular}
	\caption{BLEU scores for Joint Dropout~(JD), Token Drop~(TD), Zero Out~(ZO), and SwitchOut~(SW) on 5k, 10k and 20k training samples from IWSLT De-En.}
	\label{tab10}
\end{table}




\section{Reproducibility details}
\label{AppendixG}
We use Transformer implemented in Fairseq library~\citep{OttEBFGNGA19} to conduct our experiments  and all of the models are trained on a single GPU for a few hours with the model parameters ranging from 28M to 47M.

\header{Data} In the preliminary experiments, in order to evaluate the models trained on IWSLT subsets, we use the concatenation of the IWLST $2014$ dev sets (tst$2010$–$2012$, dev$2010$, dev$2012$) as our test set, which consists of 6,750 sentence pairs.
For the compositionality experiments, we use subsets of English-Dutch MT corpus OPUS~\citep{TiedemannT20} available on~\url{https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/data/README-v2020-07-28.md}. To evaluate these models, we use both the ‘dev’ and the ‘devtest’ sets from  FLORES-101~\citep{GoyalGCCWJKRGF22} as the validation and test data.

\header{Pre-processing} We apply punctuation normalization, tokenization, data cleaning, and true-casing using the Moses scripts~\citep{KoehnHBCFBCSMZDBCH07}. The sentence length is limited to a maximum of 175 tokens during training. After replacing phrases with variables, we also apply BPE segmentation~\citep{SennrichHB16a} with the parameter tailored to the low-resource training data~\citep{AraabiM20}.

\header{Data annotation} To generate a realistic test set for evaluating robustness against sentence perturbation, we first randomly select 300 translation outputs from the inference stage of baseline systems trained using optimized parameters on 20k samples. These outputs are then ranked using the Direct Assessment~(DA) method by engaging native annotators. The top 100 outputs are then selected and the corresponding outputs from the model trained with JD are extracted and ranked using DA. Next, the input sentences are modified by replacing specific phrases or words while ensuring their syntactic and semantic accuracy. After obtaining the outputs for both the baseline and JD systems on the perturbed sentences, we conduct a DA on them.
\end{appendices}
\fi
