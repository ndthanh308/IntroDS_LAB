\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abu~Farha et~al.(2018)Abu~Farha, Richard, and Gall]{abu2018will}
Yazan Abu~Farha, Alexander Richard, and Juergen Gall.
\newblock When will you do what?-anticipating temporal occurrences of activities.
\newblock In \emph{CVPR}, 2018.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Ashutosh et~al.(2023)Ashutosh, Girdhar, Torresani, and Grauman]{hierVL}
Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and Kristen Grauman.
\newblock Hiervl: Learning hierarchical video-language embeddings.
\newblock \emph{CVPR}, 2023.

\bibitem[Bansal et~al.(2022)Bansal, Arora, and Jawahar]{bansal2022my}
Siddhant Bansal, Chetan Arora, and CV~Jawahar.
\newblock My view is the best view: Procedure learning from egocentric videos.
\newblock In \emph{ECCV}, 2022.

\bibitem[Bi et~al.(2021)Bi, Luo, and Xu]{bi2021procedure}
Jing Bi, Jiebo Luo, and Chenliang Xu.
\newblock Procedure planning in instructional videos via contextual modeling and model-based policy learning.
\newblock In \emph{ICCV}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Camporese et~al.(2021)Camporese, Coscia, Furnari, Farinella, and Ballan]{camporese2021knowledge}
Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni~Maria Farinella, and Lamberto Ballan.
\newblock Knowledge distillation for action anticipation via label smoothing.
\newblock In \emph{ICPR}, 2021.

\bibitem[Carreira \& Zisserman(2017)Carreira and Zisserman]{i3d_cvpr17}
J.~Carreira and A.~Zisserman.
\newblock Quo vadis, action recognition? {A} new model and the {K}inetics dataset.
\newblock In \emph{CVPR}, 2017.

\bibitem[Chang et~al.(2020)Chang, Huang, Xu, Adeli, Fei-Fei, and Niebles]{chang2020procedure}
Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li~Fei-Fei, and Juan~Carlos Niebles.
\newblock Procedure planning in instructional videos.
\newblock In \emph{ECCV}, 2020.

\bibitem[Chen et~al.(2023)Chen, Zheng, Wang, Xu, Huang, Pan, Wang, Wang, Qiao, Lu, et~al.]{chen2023videollm}
Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi~Wang, Yali Wang, Yu~Qiao, Tong Lu, et~al.
\newblock Videollm: Modeling video sequence with large language models.
\newblock \emph{arXiv preprint arXiv:2305.13292}, 2023.

\bibitem[Damen et~al.(2020)Damen, Doughty, Farinella, Fidler, Furnari, Kazakos, Moltisanti, Munro, Perrett, Price, et~al.]{Epic-Kitchen}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et~al.
\newblock The epic-kitchens dataset: Collection, challenges and baselines.
\newblock \emph{TPAMI}, 2020.

\bibitem[Das \& Ryoo(2022)Das and Ryoo]{das2022video+}
Srijan Das and Michael~S Ryoo.
\newblock Video+ clip baseline for ego4d long-term action anticipation.
\newblock \emph{arXiv preprint arXiv:2207.00579}, 2022.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem[Epstein et~al.(2021)Epstein, Wu, Schmid, and Sun]{epstein2021learning}
Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun.
\newblock Learning temporal dynamics from cycles in narrated video.
\newblock \emph{ICCV}, 2021.

\bibitem[Farha et~al.(2020)Farha, Ke, Schiele, and Gall]{farha2020long}
Yazan~Abu Farha, Qiuhong Ke, Bernt Schiele, and Juergen Gall.
\newblock Long-term anticipation of activities with cycle consistency.
\newblock \emph{GCPR}, 2020.

\bibitem[Gammulle et~al.(2019{\natexlab{a}})Gammulle, Denman, Sridharan, and Fookes]{gammulle2019forecasting}
Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.
\newblock Forecasting future action sequences with neural memory networks.
\newblock \emph{BMVC}, 2019{\natexlab{a}}.

\bibitem[Gammulle et~al.(2019{\natexlab{b}})Gammulle, Denman, Sridharan, and Fookes]{gammulle2019predicting}
Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.
\newblock Predicting the future: A jointly learnt model for action anticipation.
\newblock In \emph{ICCV}, 2019{\natexlab{b}}.

\bibitem[Gao et~al.(2020)Gao, Sun, Zhao, Shen, Anguelov, Li, and Schmid]{gao2020vectornet}
Jiyang Gao, Chen Sun, Hang Zhao, Yi~Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid.
\newblock Vectornet: Encoding hd maps and agent dynamics from vectorized representation.
\newblock In \emph{ICCV}, 2020.

\bibitem[Ghosh et~al.(2023)Ghosh, Aggarwal, Hoai, and Balasubramanian]{ghosh2023text}
Sayontan Ghosh, Tanvi Aggarwal, Minh Hoai, and Niranjan Balasubramanian.
\newblock Text-derived knowledge helps vision: A simple cross-modal distillation for video-based action anticipation.
\newblock In \emph{EACL}, 2023.

\bibitem[Girase et~al.(2023)Girase, Agarwal, Choi, and Mangalam]{girase2023latency}
Harshayu Girase, Nakul Agarwal, Chiho Choi, and Karttikeya Mangalam.
\newblock Latency matters: Real-time action forecasting transformer.
\newblock In \emph{CVPR}, 2023.

\bibitem[Girdhar \& Grauman(2021)Girdhar and Grauman]{girdhar2021anticipative}
Rohit Girdhar and Kristen Grauman.
\newblock Anticipative video transformer.
\newblock In \emph{ICCV}, 2021.

\bibitem[Girdhar et~al.(2017)Girdhar, Ramanan, Gupta, Sivic, and Russell]{girdhar2017actionvlad}
Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, and Bryan Russell.
\newblock Actionvlad: Learning spatio-temporal aggregation for action classification.
\newblock \emph{CVPR}, 2017.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari, Girdhar, Hamburger, Jiang, Liu, Liu, et~al.]{ego4d}
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In \emph{CVPR}, 2022.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Huang et~al.(2023)Huang, Hilliges, Van~Gool, and Wang]{huang2023palm}
Daoji Huang, Otmar Hilliges, Luc Van~Gool, and Xi~Wang.
\newblock Palm: Predicting actions through language models@ ego4d long-term action anticipation challenge 2023.
\newblock \emph{arXiv preprint arXiv:2306.16545}, 2023.

\bibitem[Hussein et~al.(2019{\natexlab{a}})Hussein, Gavves, and Smeulders]{hussein2019timeception}
Noureldien Hussein, Efstratios Gavves, and Arnold W.~M. Smeulders.
\newblock Timeception for complex action recognition.
\newblock \emph{CVPR}, 2019{\natexlab{a}}.

\bibitem[Hussein et~al.(2019{\natexlab{b}})Hussein, Gavves, and Smeulders]{hussein2019videograph}
Noureldien Hussein, Efstratios Gavves, and Arnold W.~M. Smeulders.
\newblock Videograph: Recognizing minutes-long human activities in videos.
\newblock \emph{arXiv preprint arXiv:1905.05143}, 2019{\natexlab{b}}.

\bibitem[Ishibashi et~al.(2023)Ishibashi, Ono, Kugo, and Sato]{ishibashi2023technical}
Tatsuya Ishibashi, Kosuke Ono, Noriyuki Kugo, and Yuji Sato.
\newblock Technical report for ego4d long term action anticipation challenge 2023.
\newblock \emph{arXiv preprint arXiv:2307.01467}, 2023.

\bibitem[Ke et~al.(2019)Ke, Fritz, and Schiele]{ke2019time}
Qiuhong Ke, Mario Fritz, and Bernt Schiele.
\newblock Time-conditioned action anticipation in one shot.
\newblock In \emph{CVPR}, 2019.

\bibitem[Kruglanski \& Szumowska(2020)Kruglanski and Szumowska]{kruglanski2020habitual}
Arie~W Kruglanski and Ewa Szumowska.
\newblock Habitual behavior is goal-driven.
\newblock \emph{Perspectives on Psychological Science}, 15\penalty0 (5):\penalty0 1256--1271, 2020.

\bibitem[Lan et~al.(2014)Lan, Chen, and Savarese]{lan2014hierarchical}
Tian Lan, Tsung-Chuan Chen, and Silvio Savarese.
\newblock A hierarchical representation for future action prediction.
\newblock In \emph{ECCV}, 2014.

\bibitem[Li et~al.(2018)Li, Liu, and Rehg]{li2018eye}
Yin Li, Miao Liu, and James~M Rehg.
\newblock In the eye of beholder: Joint learning of gaze and actions in first person video.
\newblock In \emph{ECCV}, 2018.

\bibitem[Lu et~al.(2023)Lu, Feng, Zhu, Xu, Wang, Eckstein, and Wang]{lu2022neuro}
Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin~Eric Wang, Miguel Eckstein, and William~Yang Wang.
\newblock Neuro-symbolic procedural planning with commonsense prompting.
\newblock \emph{ICLR}, 2023.

\bibitem[Mascaro et~al.()Mascaro, Ahn, and Lee]{icvae}
Esteve~Valls Mascaro, Hyemin Ahn, and Dongheui Lee.
\newblock Intention-conditioned long-term human egocentric action forecasting @ ego4d challenge 2022.
\newblock URL \url{https://arxiv.org/abs/2207.12080}.

\bibitem[Merullo et~al.(2023)Merullo, Castricato, Eickhoff, and Pavlick]{merullo2022linearly}
Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick.
\newblock Linearly mapping from image to text space.
\newblock \emph{ICLR}, 2023.

\bibitem[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Ichter, Driess, Arenas, Rao, Sadigh, and Zeng]{mirchandani2023large}
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat~Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng.
\newblock Large language models as general pattern machines.
\newblock \emph{arXiv preprint arXiv:2307.04721}, 2023.

\bibitem[Nagarajan et~al.(2020)Nagarajan, Li, Feichtenhofer, and Grauman]{ego-topo}
Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, and Kristen Grauman.
\newblock Ego-topo: Environment affordances from egocentric video.
\newblock In \emph{CVPR}, 2020.

\bibitem[Narasimhan et~al.(2023)Narasimhan, Yu, Bell, Zhang, and Darrell]{narasimhan2023learning}
Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, and Trevor Darrell.
\newblock Learning and verification of task structure in instructional videos.
\newblock \emph{arXiv preprint arXiv:2303.13519}, 2023.

\bibitem[Nawhal et~al.(2022)Nawhal, Jyothi, and Mori]{anticipator}
Megha Nawhal, Akash~Abdu Jyothi, and Greg Mori.
\newblock Rethinking learning approaches for long-term action anticipation.
\newblock In \emph{ECCV}, 2022.

\bibitem[Pastra \& Aloimonos(2012)Pastra and Aloimonos]{pastra2012minimalist}
Katerina Pastra and Yiannis Aloimonos.
\newblock The minimalist grammar of action.
\newblock \emph{Philosophical Transactions of the Royal Society B: Biological Sciences}, 367\penalty0 (1585):\penalty0 103--117, 2012.

\bibitem[Payne \& Green(1986)Payne and Green]{payne1986task}
Stephen~J Payne and Thomas~RG Green.
\newblock Task-action grammars: A model of the mental representation of task languages.
\newblock \emph{Human-computer interaction}, 2\penalty0 (2):\penalty0 93--133, 1986.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Roy \& Fernando(2022)Roy and Fernando]{roy2022predicting}
Debaditya Roy and Basura Fernando.
\newblock Predicting the next action by modeling the abstract goal.
\newblock \emph{arXiv preprint arXiv:2209.05044}, 2022.

\bibitem[Sener \& Yao(2019)Sener and Yao]{sener2019zero}
Fadime Sener and Angela Yao.
\newblock Zero-shot anticipation for instructional activities.
\newblock In \emph{ICCV}, 2019.

\bibitem[Sener et~al.(2020)Sener, Singhania, and Yao]{sener2020temporal}
Fadime Sener, Dipika Singhania, and Angela Yao.
\newblock Temporal aggregate representations for long-range video understanding.
\newblock In \emph{ECCV}, 2020.

\bibitem[Sou{\v{c}}ek et~al.(2022)Sou{\v{c}}ek, Alayrac, Miech, Laptev, and Sivic]{souvcek2022look}
Tom{\'a}{\v{s}} Sou{\v{c}}ek, Jean-Baptiste Alayrac, Antoine Miech, Ivan Laptev, and Josef Sivic.
\newblock Look for the change: Learning object states and state-modifying actions from untrimmed web videos.
\newblock In \emph{CVPR}, 2022.

\bibitem[Sun et~al.(2019{\natexlab{a}})Sun, Myers, Vondrick, Murphy, and Schmid]{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: A joint model for video and language representation learning.
\newblock In \emph{ICCV}, 2019{\natexlab{a}}.

\bibitem[Sun et~al.(2019{\natexlab{b}})Sun, Shrivastava, Vondrick, Sukthankar, Murphy, and Schmid]{sun2019relational}
Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Sukthankar, Kevin Murphy, and Cordelia Schmid.
\newblock Relational action forecasting.
\newblock In \emph{CVPR}, 2019{\natexlab{b}}.

\bibitem[Sun et~al.(2022)Sun, Huang, Lu, Liu, Zhou, and Garg]{sun2022plate}
Jiankai Sun, De-An Huang, Bo~Lu, Yun-Hui Liu, Bolei Zhou, and Animesh Garg.
\newblock Plate: Visually-grounded planning with transformers in procedural tasks.
\newblock \emph{IEEE Robotics and Automation Letters}, 7\penalty0 (2):\penalty0 4924--4930, 2022.

\bibitem[Sur{\'\i}s et~al.(2023)Sur{\'\i}s, Menon, and Vondrick]{suris2023vipergpt}
D{\'\i}dac Sur{\'\i}s, Sachit Menon, and Carl Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock \emph{ICCV}, 2023.

\bibitem[Tran et~al.(2021)Tran, Le, and Tran]{tran2021goal}
Hung Tran, Vuong Le, and Truyen Tran.
\newblock Goal-driven long-term trajectory prediction.
\newblock In \emph{WACV}, 2021.

\bibitem[Vondrick et~al.(2016)Vondrick, Pirsiavash, and Torralba]{vondrick2016anticipating}
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
\newblock Anticipating visual representations from unlabeled video.
\newblock In \emph{CVPR}, 2016.

\bibitem[Wang et~al.(2022)Wang, Chen, Wu, Chen, Dai, Liu, Jiang, Zhou, and Yuan]{wang2022bevt}
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu~Yuan.
\newblock Bevt: Bert pretraining of video transformers.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker, Tombari, Purohit, Ryoo, Sindhwani, Lee, Vanhoucke, and Florence]{zeng2022socratic}
Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.
\newblock Socratic models: Composing zero-shot multimodal reasoning with language.
\newblock \emph{arXiv preprint arXiv:2204.00598}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Hadji, Dvornik, Derpanis, Wildes, and Jepson]{zhao2022p3iv}
He~Zhao, Isma Hadji, Nikita Dvornik, Konstantinos~G Derpanis, Richard~P Wildes, and Allan~D Jepson.
\newblock P3iv: Probabilistic procedure planning from instructional videos with weak supervision.
\newblock In \emph{CVPR}, pp.\  2938--2948, 2022.

\end{thebibliography}
