\section{Method}

%In this section, we first give a formalized introduction of long-term action anticipation task and the definition of bottom-up and top-down LTA paradigm. Then, we introduce how to represent videos for LTA task. Finally, we introduce our proposed method \textbf{AntGPT} and its variants.
%including the visual module and action anticipation with LLM in both top-down and bottom-up paradigm. %\frank{This sentence is a little bit confused when initial read.}
\vspace{-.5em}
We introduce our proposed \textbf{AntGPT} framework for LTA. An overview is shown in Figure~\ref{fig:model}.
\vspace{-.5em}
%In this section, we will describe our proposed two-stage framework AntGPT. We first provide background information on the long-term action anticipation task definition. We then describe the visual perception module which converts video observations into discretized text sequences. By changing this module's target outputs from observed actions to future actions, it serves as a competitive baseline for the LTA task. In the last section, we discuss how to leverage LLMs to generate action predictions and infer the goals. An overview of the proposed approach can be found in Figure~\ref{fig:model}.%


%\chen{Define mathematically the latent goal. Procedure planning is conditioned on the future as well.}
%\kwon{This paragraph is copied from ICCV submission. Needs rewrite.}
%\kevin{Rewritten. Please see if you like it.}
% Better models are possible, but not the main focus of our paper.


%Many video benchmarks have been proposed for action anticipation due to the importance of this task. While most benchmarks~\citep{DARKO,activityforecasting,ego-topo,Epic-Kitchen,RescalingEgocentricVision,sun2019relational} focus on predicting the short-term future in terms of the next action to be performed, our focus is on the long-term action anticipation task. We follow the task definition and evaluation setup as proposed by the Ego4D~\citep{ego4d} LTA benchmark due to its large dataset size and diverse domains of videos.

%In order to evaluate how accurate is the prediction of future action sequences, researchers have proposed many benchmarks~\citep{DARKO,activityforecasting,ego-topo,Epic-Kitchen,RescalingEgocentricVision,sun2019relational}. In this paper, we choose to focus on the Long-term action anticipation (\textbf{LTA}) task, introduced by the Ego4D benchmark~\citep{ego4d}, which is largest existing dataset on action anticipation.\kevin{Do we need to say why we focus on Ego4d? If so, please help me elaborate.}
\subsection{Long-term Action Anticipation} 
The long-term action anticipation (LTA) task requires predicting a sequence of $Z$ actions in a long future time horizon based on a video observation. In the LTA task, a long video $V$ is split into an ordered set of $N$ annotated short segments $\{S^j, a^j\}_{j=1}^N$, where $S^j$ denotes the $j$-th segment in video $V$ and $a^j$ denotes the corresponding action label in the form of noun-verb pair $(n^j, v^j)$. The video is also specified with a stop time $T$, which is represented as the index of the last observed segment. In this way, a video is split into the observed segments $V_{o}$ and the future segments of the video $V_{f}$ whose labels $\{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\}$ are to be predicted. 
%\kwon{This is different from WACV submission. I believe WACV version is correct. You're mixing segment index with time index. I think the correct one should be j+1 instead of T+1.} 
A hyper-parameter $N_\text{seg} $ controls how many segments the model can observe. Concretely, we take the observable video segments $\{S^j\}_{j=T-N_\text{seg}+1 }^T$ from $V_{o}$ as input and output action sequence $\{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\}$ as predictions. Alternatively, Ego-Topo~\citep{ego-topo} takes a simplified approach, which only requires predicting the set of future actions, but not their ordering.

% Figure environment removed


%Another setup of the LTA task is given the same input but to output all actions that will occur in the future, instead of the sequence. The labels of the observable video segments $\{a^j\}_{j=T-N_\text{seg}+1 }^T$ are available during training time but not during inference. \chen{How the two are connected? How about ``Alternatively, \citep{ego-topo} takes a simplified approach, which only requires predicting the set of future actions, but not their ordering.''}

%In the long-term action anticipation task, a long video $V$ is split into an ordered set of annotated short segments $\{S_V^{(j)}\}$, where $S_V^{(j)}$ is the $j$-th segment. Each video segment is annotated with its starting time, end time and action label. An action label consists of one verb label and one noun label from pre-defined vocabularies, forming an an action pair $(n^{(j)}, v^{(j)})$. Then by specifying a stop time $t^{(j)}$, which is the end time of the last observable video segment of all observable video segments $V_{obs}$,  \chen{hard to understand the connection between $t^{(j)}$, $V_{obs}$, $S_V^{(j)}$, and $n^{(j)}$.} parameterized by $N_\text{seg} $ indicating the number of segments in the observable window, a LTA task is defined as after observing any video frames before $t^{(j)}$, make predictions on future action sequence $\{(n^{(j+1)}, v^{(j+1)}), ..., (n^{(j+Z)}, v^{(j+Z)})\}$, where $Z$ is the number of future steps to predict. An example would be after observing the video of a person frying an egg, predict the actions this person would take sequentially such as wash hand, drink water, cut onions, etc. In corresponding to the uncertainty of human behaviors, the model can make up to $K$ action sequences and the best is considered. We follow the standard setup and use $Z=20$, $K=5$. 

\noindent\textbf{Bottom-up and Top-down LTA.} We categorize action anticipation models into bottom-up and top-down. 
The bottom-up approach directly models the temporal dynamics from the history observations and predicts future actions autoregressively or in parallel. The top-down framework first explicitly infers the longer-term goal from the history actions, then plans the procedure according to both history and the goal. We define the prediction procedure of bottom-up model $\mathcal{F}_\text{bu}$ as $\{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{F}_\text{bu}(V_o)$. Here $a^j$ denotes the $j$-th video segment's action label, and $T$ is the index of the last observed segment.
%\begin{equation}
%    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{F}_\text{bu}(V_o)
%\end{equation}
For the top-down model $\mathcal{F}_\text{td}$, we formulate the prediction procedure into two steps: First, infer the goal by $g = \mathcal{G}_\text{td}(V_o)$, then perform goal-conditioned planning as $    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{F}_\text{td}(V_o, g)$,
%\begin{align}
%       g &= \mathcal{G}_\text{td}(V_o) \\
%    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} &= \mathcal{F}_\text{td}(V_o, g)  
%\end{align}
where $g$ corresponds to the long-term goal inferred by the top-down model.

%\nate{I thought this subsection was VERY well explained. I wish all AI papers had notation conventions this good :)}

%\chen{Where does the goal come from?}
%\chen{Mathematically define what we mean by top-down and bottom-up.}
%\shijie{formal definition of Top-down and bottom-up}
%\subsection{Video Recognition and Sequence Modeling for Action Anticipation}
\subsection{Video Representation}

To understand the benefits of LLMs for video-based LTA, an important design choice is the interface~\citep{zeng2022socratic,suris2023vipergpt} between visual inputs and the language model. 
%However, since we are given only visual inputs, it remains unclear what may be an effective and compact representation to leverage LLMs.
We are interested in investigating how to represent long-form videos in a compact, text-only bottleneck, while being helpful for goal inference and procedure planning with LLMs. The video data often contains complex and dynamic scenarios, with multiple characters, actions, and interactions occurring over an extended period. While such rich information can be potentially captured by (pretrained) visual embeddings or even ``video tokens''~\citep{sun2019videobert,wang2022bevt}, it remains unclear what visual representation would be sufficient to compress the long observed video context, while being friendly to the LLMs. 
%\chen{Need to rewrite as we use CLIP embeddings for EK and Gaze.}
%\chen{Rewrite based on latest experiments.} \kevin{I have changed a little bit. How is this now?} 

We first consider the standard approach to represent video frames as distributed embedding representations, computed with pre-trained vision backbone models, such as the CLIP visual encoder~\citep{radford2021clip}. For each video segment $S^j$ in $V_o$, the backbone extracts the representations of $n$ uniformly sampled frames from this segment to obtain $E^j = \{e_1, e_2, \ldots, e_n\}$. A neural network can then take the embedding representation and predict action labels for the observed frames (action recognition), or the future timesteps (action anticipation).

Our action recognition network $\mathcal{E}$ is implemented as a Transformer encoder. It takes in the visual embeddings and one learnable query token as the input. We then apply two separate MLP heads to decode the verb and noun from the encoded query token. For each observed video segment $S^j$, the recognition model $\mathcal{E}$ takes in randomly sampled image features $ E^j_{s} = \{e_a, e_b, \ldots, e_k\}, E^j_{s} \subseteq E^j$,  and outputs the corresponding action $\hat{a}^{(j)}$ for $S^j$. This process is repeated for every labeled segment in $V_{o}$, which results in $N_\text{seg} $ actions $\{\hat{a}^{(T-N_\text{seg} )}, ..., \hat{a}^{(T)}\}$, in the format of noun-verb pairs. The recognition model $\mathcal{E}$ is trained on the training set to minimize the Cross Entropy Loss between the predictions and the ground-truth action labels.

%\begin{equation}
%    \{\hat{a}^{(T-N_\text{seg} )}, ..., \hat{a}^{(T)}\} =\{\mathcal{E}(E^{(T-N_\text{seg} )}_{s}), ..., \mathcal{E}(E^{(T)}_{s})\}
%    \label{eq:recog}
%\end{equation}

\noindent\textbf{How to Represent Videos for the LLMs?} We consider a simple approach to extract video representations for a large language model. We first compute the embedding representation of $V_o$, and then apply the action recognition model $\mathcal{E}$ to convert the distributed representation into discrete action labels, which can be directly consumed by an off-the-shelf LLM. Despite its simplicity, we observe that this representation is strong enough for the LLM to extract meaningful high-level goals for top-down LTA (see Section~\ref{sec:antgpt}), and can even be applied directly to perform both bottom-up and top-down LTA with the LLMs. Alternative approaches, such as discretizing the videos via video captioning or object detection, or projecting the visual embedding via parameter-efficient fine-tuning~\citep{hu2021lora,merullo2022linearly}, can also be applied under our framework.
%We leave these explorations as interesting future work.


%To interact with large language models for the LTA task, we first need to design a discrete representation to summarize the visual video inputs $V_o$, and can be interpreted by the language model. Here, we choose to use the observed action sequence as the discrete representation of the input videos. We use an action recognition model $\mathcal{E}$ as the vision module to generate action sequence from observable video $V_{o}$.  \chen{Explain why we pick the action labels, and alternatives such as PEFT.}

\subsection{\textbf{AntGPT}: Long-term Action Anticipation with LLMs}
\label{sec:antgpt}

We now describe \textbf{AntGPT} (Action \textbf{Ant}icipation \textbf{GPT}), a framework that incorporates LLMs for the LTA task. An LLM serves both as an few-shot high-level goal predictor via in-context learning, and also as a temporal dynamics model which predicts the future actions conditioned on the observed actions. It hence benefits top-down and bottom-up LTA, in full-shot and few-shot scenarios.

% for goal generation with in-context learning and propose two model variants for action anticipation: a transformer model taking in visual observations and a LLM-based model using action prompts for tempral reasoning and future prediciton.

\noindent\textbf{Few-shot Goal Inference.} In order to perform top-down long-term action anticipation, we conduct in-context learning on LLMs to infer the goals by taking the recognized action labels as inputs, as illustrated in Figure~\ref{fig:model} (b) and (c). The ICL prompts $q_\text{goal}$ is formulated with examples in the format of \texttt{"<observed actions> => <goal>"} and the final query in the format of \texttt{"<observed actions> =>"}. The observed actions for the in-context examples are based on ground-truth annotations, and the observed actions in the final query are generated by recognition models.
Since no ground truth goals are available, we either use the video metadata as \textit{pseudo goals} when it is available, or design the goals manually. Figure~\ref{fig:goals} shows several examples for in-context goal inference with the LLM. We treat the raw output of the LLM $T_\text{goal} = \mathcal{\pi}(q_\text{goal})$ as the high-level goal.
%\minhquan{Just double checking regarding ICL prompts. I don't think this matters too much but for our Llama prompts we use "next 20 actions" instead of "=>".}

\noindent\textbf{Bottom-up and Top-down LTA.} We now describe a unified framework to perform bottom-up and top-down LTA. The framework largely resembles the action recognition network $\mathcal{E}$ which takes visual embeddings as inputs, but has a few important distinctions. Let's first consider the bottom-up model $\mathcal{B}$. Its transformer encoder takes sub-sampled visual embeddings $E^j_{s}$ from each segment $S^j$ of $V_o$. The embeddings from different segments are concatenated together along the time axis to form the input tokens to the transformer encoder. To perform action anticipation, we append additional learnable query tokens to the input sequence of the Transformer encoder, each of which corresponds to a future step to predict. Each encoded query token is decoded into verb and noun predictions with two separate MLP heads. We minimize the Cross Entropy Loss for all future actions to be predicted with equal weights. Note that one can choose to use either bidirectional or causal attention masks for the query tokens, resulting in parallel or autoregressive action prediction. We observe that this design choice has marginal impact on performance, and use parallel decoding unless otherwise mentioned. %\kwon{Consider adding architecture diagram of this one in the supplementary sec A.3, if time permits.}


%We first design a bottom-up model using transformer encoder that takes in visual features of frames sampled from the observable segments and then appended it with decoders to output future predictions. We first consider this bottom-up model $\mathcal{F}_\text{bu}$ with visual input only. Essentially, our transformer encoder takes in image features $E^j_{s} = \{e_a, e_b, \ldots, e_k\}, E^j_{s} \subseteq E^j$ from each segment $S^j$ of the observable segments $V_o$. The features $\{E^{(T-N_\text{seg} )}_{s}, ..., E^{(T)}_{s}\}$ are concatenated together to form a joint representation. Then we feed the joint representation to our transformer aggregator in order to generate future actions. When our desired output is in the form of action sequences, we optimize the Multi-class Cross Entropy Loss for all positions in the future. Essentially, this is formulated as Equation \ref{eq:btransformer}. When our desired output is all the actions to occur in the future, we optimize the Binary Cross Entropy Loss for all the actions.

%\begin{equation}
%    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{B}(\{E^{(T-N_\text{seg} )}_{s}, ..., E^{(T)}_{s}\})
%    \label{eq:btransformer}
%\end{equation}

Thanks to few-shot goal inference with in-context learning, implementing the top-down model $\mathcal{F}_\text{td}$ is straightforward: We first embed the inferred goals $T_\text{goal}$ with a pre-trained CLIP text encoder. The goal token is then prepended at the beginning of the visual embedding tokens to perform goal-conditioned action anticipation. During training, we use ground-truth action labels to infer the goals via in-context learning. During evaluation, we use the recognized action labels to infer the goals.

% Now that we want to introduce our top-down model $\mathcal{T}$, in which we perform top-down action anticipation by including the LLM-generated goals as an additional modality to feed into the transformer encoder. In details, texts of goal obtained from ICL $T_{goal}$ are encoded into textual embeddings $E_g$ using CLIP's text encoder. Then the text embeddings are concatenated at the end of the visual embeddings in forming the joint representation to feed into the transformer aggregator. The remaining part is the same with the vision-only framework. Essentially, this is formulated as Equation \ref{eq:ttransformer}. We expect CLIP's text encoder will be helpful than other text encoders because it is trained together with CLIP's vision encoder and thus makes their features  more compatible. 

%\begin{equation}
%    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{T}(\{E^{(T-N_\text{seg} )}_{s}, ..., E^{(T)}_{s}, E_g\})
%    \label{eq:ttransformer}
%\end{equation}

\noindent\textbf{Modeling Temporal Dynamics with LLMs.}
%The traditional bottom-up approach uses a transformer encoder to aggregate visual features and decode the future preduction. Now we want to 
We further investigate if LLMs are able to model temporal dynamics via recognized action labels and perform action anticipation via autoregressive sequence completion. We first study the fully supervised scenario, where we perform parameter-efficient (optionally) fine-tuning on LLMs on the training set of an LTA benchmark. Both the input prompt and the target sequence are constructed by concatenating the action labels separated with commas. During training, the input sequences are formed either via teacher forcing (ground truth actions), or the (noisy) recognized actions. The LLM is optimized with the standard sequence completion objective. During inference, we use the action recognition model $\mathcal{E}$ to form input prompts from the recognized action labels. We perform postprocessing to convert the output sequence into action labels. Details of the postprocessing can be found in Section~\ref{sec:postprocessing}. To perform top-down LTA, we simply prepend an inferred goal at the beginning of each input prompt. The goals are again inferred from ground-truth actions during training, and recognized actions during evaluation.

%Essentially, how do LLMs work as the ``reasoning module''? This approach allows the LLM to work with in-domain knowledge from the datasets. To achieve that, we fine-tune OpenAI's GPT-3-curie model $\pi$ on the training set of Ego4d, as shown in Figure~\ref{fig:model} (c). Specifically, we model the LTA task as an auto-regressive language sequence completion task, aiming for generating future action sequence based on prompts $q_\text{act}$ formed by concatenating observed actions we obtained with our recognition model into sequences. During training, we use teacher-forcing by utilizing the ground-truth actions from the training set's video segments as prompts and perform supervised learning with the future prediction as the completion labels. During the two-stage inference, we first use action recognition model $\mathcal{E}$ to generate actions and construct prompts $q_\text{act}$, then get prediction sequence in the form of text from the fine-tuned LLM $\pi'$, formulated as Equation \ref{eq:bllm}.

%\begin{equation}
%    \{\hat{a}^{(T+1 )}, ..., \hat{a}^{(T+Z)}\} = \mathcal{\pi'}(q_{act})
%    \label{eq:bllm}
%\end{equation}

%Similarly, LLM can also perform top-down prediction by adding the LLM-generated goal into prompts in the form of Equation \ref{eq:tllm}.

%\begin{equation}
%    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{\pi'}(T_{goal}, q_{act})
%    \label{eq:tllm}
%\end{equation}

%\noindent\textbf{Few-Shot Prediction with LLM}

\noindent\textbf{Knowledge Distillation}~\citep{hinton2015distilling} is applied to understand if the knowledge encoded by LLMs about temporal dynamics can be condensed into a much more compact neural network for efficient inference. For sequence models such as LLMs, the distillation loss is calculated as the sum of per-token losses between the encoded feature (e.g. logits) sequences by the teacher and the student. Formally, during distillation, given the input sequence $x$ of length $N$, a well-trained  LLM as the teacher model $\pi_{t}$, the student model $\pi_{s}$ is optimized to minimize the language modeling loss $\mathcal{L}_\text{lm}$ and distillation loss $\mathcal{L}_\text{dist} = \sum_{i=1}^ND_{KL}(\hat{y}_t^{(i)} || \hat{y}_s^{(i)})$, where $\hat{y}_t=\pi_t(x)$ and $\hat{y}_s=\pi_s(x)$ are the feature sequence encoded by $\pi_t$ and $\pi_s$ respectively, $i$ is the token index of the target sequence, and $D_{KL}$ is the Kullback-Leibler divergence between the teacher and student distribution. The teacher model $\pi_t$ is frozen during training. An illustration is shown in Figure~\ref{fig:model} (d).

\noindent\textbf{Few-shot Learning with LLMs.} Beyond fine-tuning, we are also interested in understanding if LLM's in-context learning capability generalizes to the LTA task. Compared with fine-tuning model with the whole training set, in-context learning avoids updating the weights of a pre-trained LLM.
%We use the same GPT-3.5-turbo model as for few-shot goal generation, which exhibits competitive in-context learning performance after instruction tuning~\citep{ouyang2022training}.
%have shown large language models usually have the ability of in-context learning (ICL): learning from few examples in the format of input-output pairs during inference without updating any weights. Compared with fine-tuning model with the whole training set, using ICL for bottom-up prediction does not require further training the LLM, and is a convenient yet effective approach for few-shot learning. The success of ChatGPT and GPT4~\citep{openai2023gpt4} shows that LLM after instruction tuning~\citep{ouyang2022training} behave better on following instruction from context examples. Thus we choose to conduct in-context learning predictions using the GPT-3.5-turbo model which is further trained on additional instruction tuning data as shown in Figure~\ref{fig:model} (d).
As illustrated in Figure~\ref{fig:model} (e), an ICL prompt consists of three parts: First, an instruction that specifies the anticipating action task, the output format, and the verb and noun vocabulary. Second, the in-context examples randomly sampled from the training set. They are in the format of \texttt{"<observed actions> => <future actions>"} with ground-truth actions. Finally, the query in the format \texttt{"<observed actions> => "} with recognized actions. An example of the model's input and output is shown in Figure~\ref{fig:vis2} (b). Alternatively, we also attempt to leverage chain-of-thoughts prompts~\citep{wei2022chain} (CoT) to ask the LLM first infer the goal, then perform LTA conditioned on the inferred goal. An example of CoT LTA is shown in Figure~\ref{fig:vis2} (c).


%While few-shot top-down LTA can be performed in two stages, by first inferring the goals and then forming the in-context prompt, a more elegant approach is based on chain-of-thoughts prompts~\citep{wei2022chain} (CoT). As shown in Figure~\ref{fig:model} (e), a CoT prompt also consists of three parts: task definition and rule specification, example CoT demonstrations, and the CoT queries. We construct two CoT questions: \texttt{Q1.What's the goal according to previous actions? Q2.What are the future actions based on the goal and previous actions?} To generate the CoT examples, we choose samples from training set and then use the LLM-generated goal as the answers of \texttt{Q1}, the ground-truth future actions as the answer of \texttt{Q2}. Finally, we formulate CoT queries in the format of  \texttt{"<observed actions> Q1 Q2 => "}. An example of CoT LTA is shown in Figure~\ref{fig:vis2} (c). \shijie{we might need to shorten few-short part and move some details to appendix}
%\noindent\textbf{Post-processing.} We observe that despite the fine-tuning or in-context learning, the outputs generated by the LLMs sometimes do not strictly follow the demonstrations in the training examples. Common error patterns include different output sequence length than expected, or out of vocabulary predictions.

%To handle these edge-cases, we perform post-processing by first examining if each action in the output sequence is valid, and can be parsed into one verb and one noun. We then remove the invalid actions and pad the sequence to the desired length with the last valid action. To handle synonyms, for all the verbs and nouns, we find their nearest neighbors in the action vocabulary using the Levenshtein distance between two strings. We also expand the vocabulary to include common synonyms of the verbs and the nouns. 


%\noindent\textbf{Chain-of-Thoughts}.
%A different way to apply top-down few-shot prediction with LLM is chain-of-thoughts prompts~\citep{wei2022chain} (CoT). This is a recently developed prompt method that encourages language models to decompose a composite task into several intermediate steps. Top-down LTA can be viewed as a special case of CoT: It first predicts the goal conditioned on the history actions, and then predicts the future actions conditioned on both the history and the inferred goal.

%We use GPT-3.5-turbo for CoT as for ICL. As shown in Figure~\ref{fig:model} (e), CoT prompts also consist of three parts: task definition and rule specification, example CoT demonstrations, and the CoT queries. Inspired by the idea of procedure planning for goal longer-term based future prediction, we construct two CoT questions: \texttt{Q1.What's the goal according to previous actions? Q2.What are the future actions based on the goal and previous actions?} To generate the CoT examples, we choose samples from training set and then use the LLM-generated goal as the answers of \texttt{Q1}, the ground-truth future actions as the answer of \texttt{Q2}. Finally we formulate CoT queries in the format of  \texttt{"<observed actions> Q1 Q2 => "}. An demonstration of CoT LTA is shown in Figure~\ref{fig:vis2} (c). By CoT prompting, LLM will generate both predicted goal and future actions following the top-down fashion.

%\noindent\textbf{Goal-conditioned Multimodal Reasoning}.

%Similar to our CoT experiments, we first get the goal information by running the vision baseline to perform action recognition, and then prompt the LLM to infer the goal using the observed action sequence as context. The inferred goal is then encoded into a text embedding with CLIP's pre-trained text encoder. After that, we incorporate the goal information to the vision pipeline by treating it as another modality input to the transformer encoder. To adapt our vision-only model to perform the LTA task setup of EPIC-Kitchens-55 and Gaze, we change its training objective into a multi-class classification one, specifically the mean of binary cross entropy loss of each prediction class. The inputs are CLIP-features of frames sampled from the observable segments. They fuse into a joint representation through a Transformer Encoder to reproduce a representation from which the decoder output a vector, indicating the likelihood of this verb-prediction appear in the future. When adding the text embedding of the goal information, we append the text embedding to the CLIP representations to form a join embedding before we produce the fused representation.

%\subsubsection{Post-processing.} The output of LLM is not always well-structured and strictly following the demonstration in training examples. Frequent errors include output sequence failing to predict action sequence of the given length or format, or predicting verbs or nouns not in the label space, and synonyms. To handle these edge-cases, we examine whether the output sequence can be broken down into the desired number of action pairs, each of which contain a verb and a noun. If action pairs are invalid, then we remove those invalid action pairs and pad the sequence to our desired length with the last valid action pair. To handle synonyms, for all the verbs and nouns, we find its nearest neighbor in the updated dictionary that contains all words and their synonyms, using Levenshtein distance between two strings. After post-processing, we parse the output language sequence to a valid action sequence.
