\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

%hugely influence LLM's prediction and that LLM can generate plausible predictions when given the alternative goal. This also shows a potential weakness of top-down prediction as incorrect goals may mislead models to predict a different future. \chen{Future work, improve goal quality}

\section{Ablation Study}
\label{sec:ablation}
In this section, we conduct ablation study on model designs for the Transformer model and LLM-based model on Ego4D v1 and v2 to show how we choose the final settings. In the table, rows labeled as gray denote the final setting we use to report in Table~\ref{tab:sota_ego4d}.

\subsection{Transformers: Number of layers}
\label{sec:n_layers}
We want to compare fine-tuned LLM with auto-regressive transformers trained from-scratch. However, our transformer model and the large language model we used do not have comparable parameter size. Here, we want to explore how parameter size affect the performance of the transformer and to investigate whether the performance gap we demonstrated in 4.4 is due to model size. We ran the transformer model with varying number of layers from 1 to 6 and the corresponding results are in Table~\ref{tab:layers}. We observed that adding more layers to the auto-regressive transformer model does not help performance. This further substantiate our conclusion that large language models are better in modeling action temporal dynamics.

\begin{table}[h]
\centering
\scalebox{1.0}{
\begin{tabular}{c cc}
\toprule
\# layers & Verb $\downarrow$ & Noun $\downarrow$ \\ 
\midrule
1 & \textbf{0.739} & 0.758 \\
2 & 0.741 & 0.756 \\
\rowcolor{baselinecolor}3 & 0.743 & 0.756 \\
4 & 0.742 & \textbf{0.755} \\
5 & 0.743 & 0.757 \\
6 & 0.747 & 0.759 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Ablations on the number of layers to auto-regressive transformers.} Transformers are trained with actions labels from-scratch. Results on Ego4d v1 validation set. Layer number of Transformer model barely influence its performance.}
\label{tab:layers}
\vspace{-0.15in}
\end{table}

\subsection{Teacher forcing}
\label{sec:teacher_forcing}
We developed a data augmentation technique to further boost the performance of our approach. When reporting on test set, we can augment the training set with recognized action tokens which comes from a recognition model that is trained on the validation set and makes inference on the training set. In this way, we double our training set with more noisy data, the distribution of which is closer to the recognized action tokens that we input into the fine-tuned model when making inference on the test set. Table\ref{tab:teacher_forcing} shows the comparison between three training paradigms.

\subsection{Recognition Model}
To study the impact of vision backbones, we altered the vision features from CLIP to EgoVLP and trained on the same architecture. The result is shown in Table\ref{tab:recognition_ablation} We observed that recognition models based EgoVLP features 
outperformed CLIP features. This is expected since EgoVLP is pretrained on Ego4D datasets. This finding suggests that our approach is robust to different recognition models and can be further improved with better recognition models.

\subsection{Large Language Models}
\label{sec:llms}
We compared different LLMs for their performance on fine-tuning. Table\ref{tab:llm_ablation} shows the result on Ego4D v2 test set. We observe that Llama2-7B with PEFT performs better than fine-tuned GPT-3.

\subsection{Number of input segments}
Table \ref{tab:n_seg} shows the influence of the number of input segments to LLM. A higher number of input segments allows the LLM to observe longer temporal context. We choose the input segments from \{1, 3, 5, 8\}. We did not go beyond 8 because that was the maximum segments allowed by the Ego4d benchmark. 
%\chen{Why? This is just an implementation choice, which can be solved by padding, no?} \kevin{Yes. We can do more than 8. I actually have Nseg=10, shall we include it?}.
In general, \textbf{AntGPT}'s performances on both verb and noun increase as we increase $N_\text{seg}$. This is different from the vision baseline in Table 1, whose performance saturates at $N_\text{seg}=3$.


\begin{table}[h]
\begin{minipage}[c]{0.45\linewidth}
\centering
\scalebox{0.9}{
\begin{tabular}{ccccc}
\toprule
Recog Model  & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
\midrule
CLIP &0.6794 &0.6757 &0.8912 \\
\rowcolor{baselinecolor}EgoVLP  &\textbf{0.6677} &\textbf{0.6607} &\textbf{0.8854}  \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Ablation on recognition model.} Results on Ego4D v2 test set. EgoVLP as recognition model brings better performance.} 
\label{tab:recognition_ablation}
\end{minipage}
\hfill
\begin{minipage}[c]{0.5\linewidth}
\centering
\scalebox{0.9}{
    \begin{tabular}{ccccc}
    \toprule
    Temporal Model & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
    \midrule
    % v1 &GPT-3 &0.7023 &0.7029 &0.9120 \\
    % v1 &Llama2 &\textbf{}\cellcolor{baselinecolor} & \textbf{}\cellcolor{baselinecolor} & \textbf{}\cellcolor{baselinecolor} \\
    GPT-3 curie &0.6969 &0.6813 &0.9060 \\
    \rowcolor{baselinecolor}Llama2-7B &\textbf{0.6794} & \textbf{0.6757} & \textbf{0.8912} \\
    \bottomrule
    \end{tabular}
}
\caption{\textbf{Ablation on LLM as temporal model.} Results on Ego4D v2 test set. Llama2-7B perform better than GPT-3 curie as tempral model.}
\label{tab:llm_ablation}
\end{minipage}
\end{table}

\begin{table}[h]
\begin{minipage}[c]{0.48\linewidth}
\centering
\scalebox{0.9}{
\begin{tabular}{ccccc}
\toprule
Input & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
\midrule
gt &0.6794 &0.6757 &0.8912 \\
recog &0.6618 &0.6721 &0.8839 \\
\rowcolor{baselinecolor}gt+recog &\textbf{0.6611} &\textbf{0.6506} &\textbf{0.8778}  \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Ablation on teacher forcing.} Results on Ego4D v2 test set. Using ground-truth and recognized actions as training samples bring best performance.} 
\label{tab:teacher_forcing}
\end{minipage}
\hfill
\begin{minipage}[c]{0.48\linewidth}
\centering
\scalebox{0.85}{
\begin{tabular}{c cc}
\toprule
\# seg & Verb $\downarrow$ & Noun $\downarrow$ \\ 
\midrule
1 & 0.734 & 0.748 \\
3 & 0.717 & 0.726 \\
5 & 0.723 & 0.722 \\
\rowcolor{baselinecolor} 8 & \textbf{0.707} & \textbf{0.719}\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Ablations on input segments number to LLM.} Results on Ego4d v1 validation set. Segment number of 8 brings best performance.}
\label{tab:n_seg}
\end{minipage}
\end{table}

\section{Few-shot Action Anticipation}
%\noindent\textbf{Qualitative Analysis.} 
%\noindent\textbf{Quantitative Experiments.}
\begin{table}[h]
\centering
\begin{tabular}{ccccccc}
\toprule
Model & Learning Method & With Goal & Verb $\downarrow$ & Noun $\downarrow$ \\ 
\midrule
Transformer &SGD & No &0.770  &0.968  \\
Llama2-chat-13B &ICL & No & 0.761 & 0.803  \\
GPT-3.5 &ICL & No & 0.758 & 0.728 \\
GPT-3.5 &ICL & Yes & 0.775 & 0.728 \\
GPT-3.5 &CoT & Yes &  \textbf{0.756} & \textbf{0.725} \\
%Llama2-13B &ICL & Yes &  &  \\
%Llama2-13B &CoT & Yes &  &  \\
%\\\midrule
%GT actions & LLM &ICL & No & 0.648 & 0.581 \\
%GT actions & LLM &ICL & Yes & 0.663 & 0.586 \\
%GT actions & LLM &CoT & Yes &  0.660 & 0.584 \\

\bottomrule
\end{tabular}
\vspace{0.1in}
\caption{\textbf{Few-shot results with LLM on Ego4D v1 validation set.} The transformer baseline is trained with the same 12 examples from training set as the in-context examples for LLM.}
\vspace{-1em}
\label{tab:icl}
\end{table}
We conduct quantitative few-shot learning experiments on the Ego4D v1 validation set, and compare the transformer-based model and LLM. The transformer baseline is trained on the same 12 examples sampled from the training set of Ego4D as the in-context examples used by LLM. The transformer is optimized by gradient descent. For top-down LTA with LLM, we compare the two-stage approach to infer goals and predict actions separately, and the one-stage approach based on chain-of-thoughts. Results are shown in Table~\ref{tab:icl}. We observe that all LLM-based methods perform much better than the transformer baseline for few-shot LTA. Among all the LLM-based methods, top-down prediction with CoT prompts achieves the best performance on both verb and noun. However, the gain of explicitly using goal conditioning is marginal, similar to what we have observed when training on the full set. In Figure~\ref{fig:vis2} (b) and (c), we illustrate the example input and output for ICL and CoT, respectively. More examples can be found in Section~\ref{sec:viz}.

% Figure environment removed

% Figure environment removed

\subsection{counterfactual prediction}
To better understand the impact of goals for action anticipation, we design a ``counterfactual'' prediction experiment: We first use GPT-3.5-Turbo to infer goals for examples as we did above and treat the outputs as the originally inferred goals, and then manually provide ``altered goals‚Äù which are different than the original goals but are equally reasonable. We then perform in-context learning using both sets of the goals and with the same sequences of recognized actions as inputs to the LLM. Figure~\ref{fig:vis3} illustrates our workflow and two examples. In the second example, we can see that although the recognized actions are the same, the output action predictions are all different. Despite the differences, using "fix machine" as the goal generates actions highly related to fixing such as "\texttt{tighten nut}" and "\texttt{turn screw}" while switching to "\texttt{examine machine}" leads to actions like "\texttt{test machine}" and "\texttt{record data}". More examples can be found in Section~\ref{sec:viz}.

Our qualitative analysis with the counterfactual prediction shows that the choice of goal can have large impacts on the action anticipation outputs. This indicates another future direction to improve the top-down action anticipation performance is to improve the accurate and quality of the predicted goals, and to consider multiple plausible goals.

\section{Additional Experimental Details}
\label{sec:exp_details}

In this section, we provide additional details for the fine-tuning language model experiments on Ego4D v1 and v2, as well as goal inference via in-context learning. We also provide additional details for all the transformer models involved in experiments on all the datasets. Finally, we describe the details of the different setups in Ego4d datasets versus EK-55 and Gaze. All experiments are accompolished on NVIDIA A6000 GPUs.

\subsection{Preprocessing and Postprocessing} 
\label{sec:postprocessing}
During preprocessing for fine-tuning, we empirically observe that using a single token to represent each verb or noun helps the performance of the fine-tuned model. Since the tokenization is handled by OpenAI's API, some verbs or nouns in the Ego4D's word taxonomy may span over multiple tokens. For example, the verbs ``turn-on'' and ``turn-off'' are two-token long, and the noun ``coconut'' is broken into three tokens. We attempt to minimize the token length of each Ego4D label without losing semantic information of the labels, which are important to leverage prior knowledge in the LLMs. As a result, we choose the first unique word to describe the verb and noun labels in the Ego4D label vocabulary. This preprocessing is performed for both the input action sequences, and the target action sequences for prediction. No additional task information is provided as input during fine-tuning, as we observe that the fine-tuning training examples themselves clearly specify the task.

We observe that LLM models demonstrate strong capability on following the instructions as specified by fine-tuning or in-context learning examples. However, the output sequences generated by the LLM are sometimes invalid, and post-processing is needed before the predictions can be evaluated by the edit distance metrics. Specifically, the standard Ego4D LTA evaluation protocol requires generating 5 sequences of long-term action predictions, each in the form of 20 verb and noun pairs.

We consider the following scenarios to trigger postprocessing on the LLM's predictions: (1) the output sequence length is different than expected (i.e. 20); (2) an action in the output sequence cannot be parsed into a pair of one verb and one noun; (3) a predicted verb or noun is outside the vocabulary. Table \ref{tab:incident} shows the statistics on how often each type of incident would occur.

Instead of repeatedly sampling new responses from the LLM until it is valid, we choose to use a simple post-processing strategy: First, inspect each action in the predicted sequence, and remove the invalid ones. Second, truncate the sequence, or pad the sequence with the last predicted valid action until the sequence length is as desired. We then map the predicted words into the fixed noun and verb label space. This is achieved by retrieving the nearest neighbor of each word in our label vocabulary based on Levenshtein distance.

\begin{table}[h]
\begin{minipage}[c]{\linewidth}
\centering
\scalebox{1}{
\begin{tabular}{c c}
\toprule
Incident & \% over all instances \\ 
\midrule
Short Seq (1) & 2.74\%  \\
Long Seq (1) & 12.93\%  \\
Invalid Seq (2) & 10.44\%  \\
Invalid Verb (3) & 1.93\% \\
Invalid Noun (3)  & 2.62\% \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Statistics on scenarios which require post-processing.} Invalid sequence refers to the scenario where the output sequence contains invalid strings that cannot be parsed into pairs of a verb and a noun. Invalid verb/noun refers to the scenario where the word is outside of the vocabulary. Both scenarios often imply wrong sequence length.}
\label{tab:incident}
\end{minipage}
\end{table}

\subsection{Bottom-up and Top-down Transformer Models Implementation Details.}

For the bottom-up model that establishes our vision baseline, we use frozen CLIP~\cite{radford2021clip} image encoder (ViT-L/14@336px) as vision backbone. Each frame is encoded as a 768D representation. For Ego4D, we use 3 layers, 8 heads of a vanilla Transformer Encoder with a hidden representation dimension of 2048. We use Nesterov Momentum SGD + CosineAnnealing Scheduler with learning rate 5e-4. We train the model for 30 epochs with the first 4 as warm-up epochs. All hyper-parameters are chosen by minimizing the loss on the validation set. In our top-down models, we used the compatible text encoder of the ViT-L/14@336px CLIP model as the text encoder. We used Adam optimizer with learning rate 2e-4.

For EK-55, we extracted features of all provided frames from the dataset. For the transformer, we use 1 layers, 8 heads with a hidden representation dimension of 2048. We use Adam optimizer with learning rate 5e-5. All hyper-parameters are chosen by minimizing the loss on the validation set. In our top-down models, we used the compatible text encoder of the ViT-L/14@336px CLIP model as the text encoder. We used Adam optimizer with learning rate 5e-5.

For Gaze, we use 1 layers, 8 heads of a vanilla Transformer Encoder with a hidden representation dimension of 2048. We use Nesterov Momentum SGD + CosineAnnealing Scheduler with learning rate 2e-2. All hyper-parameters are chosen by minimizing the loss on the validation set. In our top-down models, we used the compatible text encoder of the ViT-L/14@336px CLIP model as the text encoder. We used Nesterov Momentum SGD + CosineAnnealing with learning rate 15e-3.

\subsection{Recognition Model Implementation Details.}
For the recognition model, we use frozen CLIP~\cite{radford2021clip} image encoder (ViT-L/14@336px) as vision backbone. For all the datasets, we randomly sample 4 frames from each segments. Each frame is encoded as a 768D representation. 
We use 3 layers, 8 heads of a vanilla Transformer Encoder with a hidden representation dimension of 2048. We use Nesterov Momentum SGD + CosineAnnealing Scheduler with learning rate 1e-3. We train the model for 40 epochs with 4 warm-up epochs. All hyper-parameters are chosen by minimizing the loss on the validation set. 

\eat{
\begin{table}[h]
    \centering
    \scalebox{1}{
    % \setlength{\tabcolsep}{1pt}
        \begin{tabular}{ccccc}
        \toprule
        Model &Input action &Verb $\downarrow$ & Noun $\downarrow$ \\ 
        \midrule
        GPT-3 &GT &\textbf{0.679} &\textbf{0.617} \\
        GPT-3 &recog &0.707&0.719\\
        Llama2 &GT &\textbf{0.625} &\textbf{0.565}\\
        Llama2 &recog &0.704 &0.705 \\
        \bottomrule
        \end{tabular}
    }
\caption{Ground-truth actions as model input}
\label{tab:comp_to_vision}
\end{table}
}


\subsection{Inferring goal descriptors using LLMs}

In Section 4.2, we describe an ablation study where goal descriptor is used for in-context learning. More specifically, we use GPT-3.5 Turbo and Llama2-chat-13B to infer the goal by giving recognized history actions and few samples as demonstrations with ICL as goal descriptors. An illustraion of the goals can be found in Figure~\ref{fig:vis2}. For the experiments in Table~\ref{tab:trend}, the goal descriptors are added as LLM prompts in the form of "\texttt{Goal:<goal> Observed actions:<observed actions> => }". We note that this ablation study poses an interesting contrast to the CoT experiments: Instead of asking the LLM to jointly infer the goal and predict the action sequences, the ablation experiment conducts goal inference and action prediction separately, and achieves slightly worse performance than CoT.

\subsection{Additional details on EPIC-Kitchens-55 and EGTEA Gaze + experiments}

In the experiment section, we describe two setups of the LTA task on different datasets. For the Ego4D datasets, we measure the edit distance of future action sequence predictions while for EK-55 and Gaze, we measure the mean average precision for all actions to occur in the future. The distinction arises in how ``action'' is defined among these different datasets and setups. In Ego4D benchmarks, we follow the official definition of action, which a verb-prediction and a noun-prediction combining together to form an action. For EK-55 and Gaze, we adopt the task setup of Ego-Topo \cite{ego-topo} in order to compare with previous works. In their setup they defined ``action'' as only the verb-prediction, excluding the noun-prediction in the final   ``action'' prediction.

\section{Additional Visualization and Analysis}
\label{sec:viz}
\subsection{Examples of AntGPT Prediction}
We provide four examples of bottom-up fine-tuned \textbf{AntGPT} in Figure~\ref{fig:sup_vis1}, two of which are positive, and the other two negative. Correctly recognized actions from the video observations (outputs of the vision module, and inputs to the LLM) and future predictions (outputs of the LLM) are marked in green while wrong observations or predictions are marked red. We observe that \textbf{AntGPT} is able to predict reasonable and correct future actions even when the recognized actions are not perfect. However, when most of the recognized actions are incorrect (Figure~\ref{fig:sup_vis1}c), the predictions would also fail as expected. Finally, we observe that the bottom-up approach sometimes fail to implicitly infer the long-term goal based on the observed actions. In Figure~\ref{fig:sup_vis1}d, the actor keeps on mixing the ice while the LLM predicts curry-cooking related activities.


% Figure environment removed

\subsection{Additional Examples of ICL and CoT} Similarly with Figure 2, Figure~\ref{fig_sup:icl_cot} shows two additional examples of bottom-up prediction with ICL and top-down prediction with CoT. We can further observe the influence of implicitly prediction future goal from the two examples. In the first example, due to the low quality of recognized actions from the video observation (repeating "\texttt{play card}"), the bottom-up prediction follows the observation pattern to repeat "\texttt{play card}" while in the prediction of top-down prediction using CoT to consider the goal, the model succeed to predict "\texttt{take card}" and "\texttt{put card}" alternatively which both matches the ground-truth future actions and common sense in the scene of card playing better, with the verb edit distance significantly goes down from 0.95 to 0.45. The second example shows a failure example of using CoT. In this video of ``playing tablet'', the top-down model predicts the goal as ``using electronic devices'', thus influencing the following future action prediction to hallucinate actions like "\texttt{operate computer}". This behavior is intuitive, as an incorrect goal sampled by the LLM is likely to mislead the predicted actions that conditioned on it. We believe adding additional cues for goal inference, and explicitly model the different ``modes'' of the future to capture the goal diversity would mitigate this behavior.

% Figure environment removed


\subsection{Additional Examples of Counterfactual Prediction}
In Figure~\ref{fig:sub_counter}, we demonstrate four additional counterfactual prediction examples. In order to predicting with explicit goal using ICL, we construct the prompts in the form of: \texttt{"Goal:<inferred/altered goal> Observed actions:<observed actions> => <future actions>"} We observe that switching the goals from the ``inferred goal'' to the ``altered goal'' indeed has huge impact on the predicted actions. This confirms that the goals are utilized by the LLM to predict (or plan) the future actions to take. This behavior is consistent with our observations in Figure~\ref{fig_sup:icl_cot}.

% Figure environment removed


