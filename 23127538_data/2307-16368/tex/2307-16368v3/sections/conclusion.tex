\section{Conclusion and Future Work}

% We have presented a top-down approach for long-term action anticipation from video inputs. We view the task as inferring latent goals based on observed human actions, and plan the steps to take in order to accomplish the goal. We have demonstrated via quantitative evaluations and qualitative visualizations that large language models (LLMs) encode useful information for latent goal inference and procedure planning, with a simple text-based interface between the action recognition module and the LLMs. Our proposed method significantly outperforms previously published results, and achieves competitive new state-of-the-art performance on the Ego4D LTA v1/v2 benchmarks.
%In this paper, we strives to study how large language models can help the video-based long-term action anticipation task in both “bottom-up” and “top-down” paradigm by experimenting on the Ego4d Dataset. We proposed AntGPT, a two-stage bottom up approach which achieves the state-of-the art performance in Ego4d by leveraging both learned and built-in action temporal dynamics. We have also investigate both quantitatively and qualitatively how LLMs can help LTA in a “top-down” fashion by making goal-conditioned predictions. We found that LLM is particularly effective in leveraging temporal dynamics and can also effectively provide latent goal information for the LTA task. We further study the benefits and limitations of applying LLM on vision-based action anticipation, therefore sets ground for further investigations into this direction.
%\chen{Limitations, we can explore better representation of the video segments and more recent prompt tuning and instruct tuning strategies. Task is ambiguous and proper evaluation can be hard.}
%\shijie{should we summarize the 4 questions proposed in the introduction part here?}\kevin{I did not add more stuff here since we are already out of space.}

In this paper, we propose \textbf{AntGPT} to investigate if large language models encode useful prior knowledge on bottom-up and top-down long-term action anticipation. Thorough experiments with two LLM variants demonstrate that LLMs are capable of inferring goals helpful for top-down LTA and also modeling the temporal dynamics of actions. Moreover, the useful encoded prior knowledge from LLMs can be distilled into very compact neural networks for efficient practical use. Our proposed method sets new state-of-the-art performances on the Ego4D LTA, EPIC-Kitchens-55, and EGTEA GAZE+ benchmarks. We further study the advantages and limitations of applying LLM on video-based action anticipation, thereby laying the groundwork for future research in this field.


\noindent\textbf{Limitations.} Although our approach provides a promising new perspective in tackling the LTA task, there are limitations that are worth pointing out. The choice of representing videos with fixed-length actions is both efficient and effective for LTA task. However, the lack of visual details may pose constraints on other tasks. Another limitation is the prompt designs of ICL and CoT are still empirical, and varying the prompt strategy may cause significant performance differences. Finally, as studied in our counterfactual experiments, the goal accuracy would have significant impact on the action recognition outputs, and an important future direction is to improve the inferred goal accuracy, and also take multiple plausible goals into account.

\noindent\textbf{Acknowledgements.} We would like to thank Nate Gillman for valuable feedback. This work is in part supported by Honda Research Institute, Meta AI, and Samsung Advanced Institute of Technology.

%Besides, complicated post-processing is also inconvenient compared with traditional label prediction paradigm.  \chen{We don't account for the uncertain of recognized actions or inferred goals.}

%Compressing video information in textual representations is efficient and effective. However, there are information loss due to the limited size of textual representations, which have the same length for videos of any length. Therefore, there might exist other form of bottleneck representations for video segments that are better than texts for sequence-to-sequence modeling and predictions. These problems imply many future works to be done. An limitation with respect to our in-context learning is that our approach could benefit from more delicate prompt designing strategies and instruct tuning strategies. 

%\section{Reproducibility}
%We provide details about model designs and experiment details that help reproduction in Section~\ref{sec:exp} and Section~\ref{sec:ablation}, ~\ref{sec:exp_details} in Appendix, including model implementation details, pre-processing/post-processing, prompt design and experiment settings. 