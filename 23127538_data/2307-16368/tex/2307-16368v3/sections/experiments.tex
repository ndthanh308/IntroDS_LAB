\vspace{-1em}
\section{Experiments}
\vspace{-1em}

We now present quantitative results and qualitative analysis on the Ego4D~\citep{ego4d}, EPIC-Kitchens~\citep{Epic-Kitchen}, and EGTEA Gaze+~\citep{li2018eye} benchmarks.  

\vspace{-1em}
\subsection{Experimental Setup}
\label{sec:exp}
\noindent\textbf{Ego4D} \cite{ego4d} The \textit{v1} dataset contains 3,670 hours of egocentric video of daily life activity spanning hundreds of scenarios. We focus on the videos in the \textit{Forecasting} subset which contains 1723 clips with 53 scenarios. The total duration is around 116 hours. There are 115 verbs and 478 nouns in total. The \textit{v2} dataset contains 3472 annotated clips with total duration of around 243 hours. There are 117 verbs and 521 nouns. We follow the datasets' standard splits.
%\noindent\textbf{Ego4D v1} \citep{ego4d} contains 3,670 hours of egocentric video of daily life activity spanning hundreds of scenarios. We focus on the videos in the \textit{Forecasting} subset which contains 1723 clips with 53 scenarios. The total duration is around 116 hours. There are 115 verbs and 478 nouns in total. We follow the standard train, validation, and test splits from ~\citet{ego4d}.

%\noindent\textbf{Ego4D v2} extends Ego4d v1. It contains 3472 annotated clips with total duration of around 243 hours. There are 117 verbs and 521 nouns. We follow the standard train, validation, and test splits.

\noindent\textbf{EPIC-Kitchens-55}~\citep{Epic-Kitchen} (EK-55) contains 55 hours egocentric videos of cooking activities of  different video takers. Each video is densely annotated with action labels, spanning over 125 verbs and 352 nouns. We adopt the train and test splits from~\citet{ego-topo}.

\noindent\textbf{EGTEA Gaze+}~\citep{li2018eye} (EGTEA) contains 86 densely labeled egocentric cooking videos over 26 hours. There are 19 verbs and 53 nouns. We adopt the splits from~\citet{ego-topo}.

\noindent\textbf{Evaluation Metrics.} For Ego4D, we use the edit distance (ED) metric. It is computed as the Damerau-Levenshtein distance over sequences of predictions of verbs, nouns or actions. We follow the standard practice in~\citet{ego4d} and report the minimum edit distance between each of the top $K=5$ predicted sequences and the ground-truth. We report Edit Distance at $Z = 20$ (ED@20) on the validation set and the test set.
%For the $K$ possible sequences the model predicts, we choose the smallest edit distance between the ground truth and each of the $K$ sequences. Following \citep{ego4d}, we set $K=5$ in our experiments. We report Edit Distance at $Z = 20$ (ED@20) on the validation set and the test set.
For EK-55 and EGTEA, we follow the evaluation metric described in~\citet{ego-topo}. The first K\% of each video is given as input, and the goal is to predict the set of actions happening in the remaining (100-K)\% of the video as multi-class classification. We sweep values of K = [25\%, 50\%, 75\%] representing different anticipation horizons and report mean average precision (mAP) on the validation sets. We report the performances on all target actions (All), the frequently appeared actions (Freq), and the rarely appeared actions (Rare) as in~\citet{ego-topo}. A number of previous work reported performance on these two datasets. The order agnostic LTA setup in these two datasets complements the Ego4D evaluation.

\noindent\textbf{Implementation Details.} We use the frozen CLIP~\citep{radford2021clip} ViT-L/14 for image features, and a transformer encoder with 8 attention heads, and 2048 hidden size for the recognition model. To study the impact of vision backbones, we also include EgoVLP, a video backbone pre-trained on Ego4D datasets. For the large language models, we adopt open-source Llama2-13B for in-context learning and 7B model for fine-tuning. For comparison, we also use OpenAI's GPT-3.5 Turbo for in-context learning and GPT-3 curie for fine-tuning. More details and ablation study on recognition model, teacher forcing, LLMs and other design choices are described in appendix.

% \chen{Either discuss the important experimental details here, or refer to the appendix. CLIP, how do we sample examples from Ego4D/EK, transformer number of layers,}
% \vspace{-0.1in}
% Figure environment removed

\subsection{Can LLMs Infer Goals to Assist Top-down LTA?}

%\chen{We need to explain why we use image features, we also need to offer sufficient information that we are using a transformer encoder here. Then we need to explain why such setup is sufficient to answer the question we asked in the section title.}\kevin{addressed with changes in beginning of second paragraph, please take a look!}

%\chen{This subsection is about (1) can LLM infer goals for LTA? (2) Should we use bottom-up or top-down?} \kevin{Do you feel we succeeded in answering those two questions now after the change?}

% \noindent\textbf {Can LLM Predict Goal?} For top-down future anticipation methods, we hope the model to infer the goal before making predictions. In order to achieve this, we design qualitative experiment to see if the model could output reasonable goals given previous actions. By manually checking the predicted goal, we find LLM could generate reasonable and related long-term goal based on the previous observations in most of time. Figure~\ref{fig:vis2} shows an example video. For this video, the LLM manages to predict the goal ``painting a room'' given the action recognition results despite the incorrectly recognized actions, such as "\texttt{paintbrush}" to "\texttt{stick}" and "\texttt{container}" to "\texttt{string}". 

We compare two LLMs, GPT-3.5 Turbo and Llama2-chat-13B, on goal inference: To obtain the pseudo ground-truth goals for constructing the in-context examples, we use the video titles for EGTEA, and the video descriptions for EK-55. We manually annotate the goals for Ego4D. We use 12 in-context examples to infer the goals. For EK-55 and EGTEA, we always use the recognized actions in the first 25\% of each video to infer the goals. For Ego4D, we set $N_\text{seg} = 8$.

We first use the Transformer encoder model described in Section~\ref{sec:antgpt} as the temporal model: It allows us to study the standalone impact of goal conditioning by comparing the bottom-up and the top-down LTA performances. The Transformer encoder takes in the same visual features as used for action recognition. The text embeddings of the inferred goals are provided for the top-down variant.
Table~\ref{tab:trend} shows results on Ego4D v1, EK-55, and EGTEA. We notice a clear trend that using the inferred goals leads to consistent improvements for the top-down approach, especially for the \textit{rare} actions of EK-55 and EGTEA. We also noticed that both LLMs are able to infer helpful goals for top-down LTA and GPT-3.5 Turbo generates goals slightly better than the ones from Llama2-chat-13B. We also construct ``oracle goals'' using the video metadata provided by EK-55 and EGTEA datasets. We observe that using the oracle goals leads to slight improvements, indicating that the inferred goals already offer competitive performance improvements. Figure~\ref{fig:goals} provides some examples of the helpful and unhelpful goals inferred by Llama2.

\eat{
\begin{table}[h]
\setlength\tabcolsep{3pt}
\small
\centering
    \begin{tabular}{lccccccccc}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Goal Generator} &\multicolumn{2}{c}{Ego4d v1 (ED)} & \multicolumn{3}{c}{EK-55 (mAP)} & \multicolumn{3}{c}{EGTEA (mAP)} \\
    \cmidrule(lr){3-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
     & & Verb $\downarrow$ & Noun $\downarrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ \\
    \midrule
    image features & - &0.735 & 0.753 & 38.2 & \textbf{59.3} & 29.0 & 78.7 & 84.7 & 68.3\\
    image features + ICL goals & GPT-3.5 &\textbf{0.724} & \textbf{0.744} & \textbf{40.1} & 58.8 & \textbf{31.9} & \textbf{80.2} & \textbf{84.8} & \textbf{72.9} \\
    image features + ICL goals & Llama2 & 0.728 & 0.747 & \textbf{40.1} & 58.1 & 32.1 & 80.0 & 84.6 & 70.0 \\
    \midrule
    image features + oracle goals\textsuperscript{$\ast$} & - & - & - & 40.9 & 58.7 & 32.9 & 81.6 & 86.8 & 69.3 \\
    \bottomrule
    \end{tabular}
\vspace{0.1in}
\caption{\textbf{Impact of the goals for LTA.} Goal-conditioned (top-down) models outperforms the bottom-up model in all three datasets. We report edit distance for Ego4D, mAP for EK-55 and EGTEA. All results are reported on the validation set.} 
\label{tab:trend}
\vspace{-0.1in}
\end{table}
}

\begin{table}[h]
\setlength\tabcolsep{3pt}
\small
\centering
% \vspace{-0.1in}
    \begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}{*}{Method} &\multicolumn{2}{c}{Ego4d v1 (ED)} & \multicolumn{3}{c}{EK-55 (mAP)} & \multicolumn{3}{c}{EGTEA (mAP)} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
     & Verb $\downarrow$ & Noun $\downarrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ \\
    \midrule
    image features &0.735 & 0.753 & 38.2 & \textbf{59.3} & 29.0 & 78.7 & 84.7 & 68.3\\
    image features + Llama2 inferred goals & 0.728 & 0.747 & \textbf{40.1} & 58.1 & \textbf{32.1} & 80.0 & 84.6 & 70.0 \\
    image features + GPT-3.5 inferred goals &\textbf{0.724} & \textbf{0.744} & \textbf{40.1} & 58.8 & 31.9 & \textbf{80.2} & \textbf{84.8} & \textbf{72.9} \\
    \midrule
    image features + oracle goals\textsuperscript{$\ast$} & - & - & 40.9 & 58.7 & 32.9 & 81.6 & 86.8 & 69.3 \\
    \bottomrule
    \end{tabular}
\vspace{-0.1in}
\caption{\textbf{Impact of goal conditioning on LTA performance.} Goal-conditioned (top-down) models outperforms the bottom-up model in all three datasets. We report edit distance for Ego4D, mAP for EK-55 and EGTEA. All results are reported on the validation set.} 
\label{tab:trend}
\vspace{-0.2in}
\end{table}
%For Gaze, we use the video titles as the pseudo goals and for EK-55 we use the description of each video as the pseudo goals. For Ego4D, we did not find a suitable pseudo goals so we manually design them. We demonstrate examples in prompt using ground truth action inputs and the pseudo goals from the training set. Then we prompt the LLM to predict the goals for videos in the validation set based on recognized action inputs. In order to prevent information leakage, we only use the recognized action inputs under the smallest K value to predict goals and them for other data points where K is larger.

%\noindent\textbf{Goal-Conditioned Multimodal LTA.} Now that we have predicted goals from LLM, we explore this novel approach to incorporate the goals inferred by the LLMs into a vision-based framework. Conceptually, this approach combines the benefits of LLMs being able to infer the goals ``zero-shot'', with the flexibility of a vision model that does not perform hard quantization on the video observations. We conduct experiments on EK-55, Gaze, and Ego4D v1 to show that adding goal information can improve a vision-only framework. Results are in Table \ref{tab:trend}.


% We also consider using \textit{oracle} metadata information as the goals for EK-55 and Gaze. 

% For EK-55 and GAZE, we always use the recognized actions in the first 25\% of each video to infer the goals. We observe that our vision-only baseline is able to achieve 38.2\% mAP the highest for all target actions in EK-55, and 78.7\% mAP in GAZE. When augmenting our vision model with goal information, we first consider the realistic scenario where we obtain the goals from the LLM by providing recognized actions. With goal information inferred with recognized actions, the LLM-augmented vision model achieves 40.2\% mAP in EK-55 and 80.1\% mAP in GAZE. We further observe that when providing although imperfect goal information, the performance on rare actions improve greatly, signaling the top-down goal information is complimentary to vision information for the LTA task. Furthermore, we consider an ideal scenario where we are able to obtain the pseudo goal information, which indicates an upper bound of our approach. \chen{todo.}

% For Ego4D, we compare goal-conditioned vision model to a vision-only model at $N_\text{seg} = 8$. The vision-only model is not the best but the performance is comparable. However, using longer observation segments allows us to generate better goals. We can observe the same trend that when introducing goals from recognized input, our new model improves the vision-only model on both verb from 0.735 to 0.724 and noun from 0.753 to 0.744. 

% We can clearly see that: 1) LLMs is capable of using prior knowledge to infer meaningful goals few-shot under ICL with discrete action labels as inputs; 2) our proposed AntGPT top-down approach for LTA is successful in incorporating goal information to improve vision models as our top-down models empowered by LLMs consistently outperform their bottom-up counterparts.



\subsection{Do LLMs Model Temporal Dynamics?}

%During training, the $N_\text{seg}$ ground-truth action labels are concatenated as the input sequence, and $Z$ ground-truth action labels are concatenated as the target sequence. During evaluation, we concatenate $N_\text{seg}$ recognized actions as input, and postprocess the output sequence into $Z$ anticipated actions (see Section~\ref{sec:postprocessing}).

\noindent
\begin{minipage}{0.55\textwidth}
  \centering
        \scalebox{0.82}{
            \begin{tabular}{lcccccc}
            \toprule
            Model &Goal &Input &Verb $\downarrow$ & Noun $\downarrow$ \\ 
            \midrule
            Transformer & GPT-3.5 &image features &0.724 &0.744 \\
            GPT-3-curie & GPT-3.5 & recog actions &0.709 &0.729 \\
            Transformer & Llama2-13B &image features &0.728 &0.747 \\
            Llama2-7B & Llama2-13B & recog actions &\textbf{0.700} &\textbf{0.717} \\
            \bottomrule
            \end{tabular}
        }
    \vspace{-0.07in}
    \captionsetup[table]{font={small}}
    \captionof{table}{\textbf{Comparison of temporal models for top-down LTA.} Results on Ego4D v1 val set. }%Goals are generated by Llama2-Chat-13B.}
    \label{tab:temporal}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    %\hspace{0.4in}
  \centering
    \scalebox{0.82}{
        \begin{tabular}{cccccc}
        \toprule
        Model &Goal &Verb $\downarrow$ & Noun $\downarrow$ \\ 
        \midrule
        GPT-3-curie &No &\textbf{0.707} &\textbf{0.719} \\
        GPT-3-curie &Yes &0.709 &0.729 \\
        Llama2-7B &No &0.704 &\textbf{0.705}\\
        Llama2-7B &Yes &\textbf{0.700} &0.717 \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.07in}
    \captionsetup[table]{font={small},width=0.8\textwidth}
    %\captionsetup[table]{font={small}}
    \captionof{table}{\textbf{Top-down vs Bottom-up for LLM-based LTA.} Results on v1 val set.}
    \label{tab:llm-top}
\end{minipage}

We further explore if LLMs can be directly applied to model temporal dynamics. We focus on the Ego4D benchmark as it measures the ordering of the anticipated actions.

%on Ego4D dataset, as it explicitly measures the ordering of the anticipated actions. 

%First main point: Does action representation also works for LTA? Compare image features + goals and action labels + goals on ED v1 (Table a). Demonstrate that action labels + goals achieve better performance. Cite Appendix Table A3 to explain that although the Transformer + CLIP embedding is a shallower model, but we have confirmed that adding more layers would not help.

\noindent\textbf{LLMs are able to model temporal dynamics.} 
To utilize an LLM to predict future actions, we adopt the same video representation as used for in-context goal inference but fine-tune the LLM on the training set. For bottom-up LTA, we by default perform teacher forcing during training, and concatenate the $N_\text{seg}$ ground-truth action labels as the input sequence. $Z$ ground-truth action labels are concatenated as the target sequence. During evaluation, we concatenate $N_\text{seg}$ recognized actions as input, and postprocess the output sequence into $Z$ anticipated actions. For top-down LTA, we prepend the inferred goals to the input sequence.

We conduct top-down LTA with the open-sourced Llama2-7B LLM. During training, we adopt parameter-efficient fine-tuning (PEFT) with LoRA~\citep{hu2021lora} and 8-bit quantization. We compare with the transformer baseline with image features, and report results on Ego4D v1 validation set in Table~\ref{tab:temporal}. We observe that leveraging the LLM as the temporal dynamics model leads to significant improvement, especially for nouns. Additionally, we validate that simply adding more layers (and hence increasing the model size) does not improve the performance of the image feature baseline (see Table~\ref{tab:layers} in appendix), confirming that the improvement comes from the action representation and better temporal dynamics modeling. The results demonstrate the effectiveness of action-based representation, when an LLM is used for temporal dynamics modeling.

%\chen{Add a paragraph here to briefly talk about few-shot experiments, this demonstrates that a frozen LLM can also model temporal dynamics.} We consider two 

\noindent\textbf{LLMs can perform few-shot temporal modeling.} We further tested LLMs' ability to model temporal dynamics when only shown a few examples. We consider both in-context learning (ICL) and chain-of-thoughts (CoT) and compare them with a transformer model trained from-scratch with the same examples. The results are illustrated in Table\ref{tab:icl}. We observed that LLMs can model temporal dynamics competitively in a few-shot setting. As expected, chain-of-thoughts outperforms regular in-context learning, but both significantly outperform fine-tuning the Transformer model. 

\noindent\textbf{LLM-based temporal model performs \textit{implicit} goal inference.} We have shown that LLMs can assist LTA by providing the inferred goals, and serving as the temporal dynamics model, respectively. Does combining the two lead to further improved performance? Table~\ref{tab:llm-top} aims to answer this question. We report results with fine-tuned Llama2-7B and GPT-3-curie as the temporal model, which use Llama2-Chat-13B and GPT-3.5 Turbo for goal inference, respectively. We empirically observe that the bigger models lead to better inferred goals, while the smaller models are sufficient for temporal modeling. 
%For Llama2-7B, the goals are from Llama2-Chat-13B and GPT-3-curie takes goals from GPT-3.5 Turbo. 
We observe that the bottom-up performance without explicitly inferred goals are on par (marginally better) with the top-down models for both LLMs. This indicates the LLM may implicitly inferred the goals when asked to predict the future actions, and performing explicit goal inference is not necessary. In the following experiments, we stick with this \textit{implicit} goal inference setup.

%when LLM is used as the temporal dynamics model, it's unnecessary to explicitly infer the goals with additional computational cost. Thus in the following experiments, we follow the bottom-up setting for LLM-based temporal model.

\begin{minipage}{.45\textwidth}
  \centering
        \scalebox{0.9}{
        % \setlength{\tabcolsep}{1pt}
        \begin{tabular}{ccccc}
        \toprule
        Seq Type & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
        \midrule
        Action Labels & \textbf{0.6794} & \textbf{0.6757} & \textbf{0.8912} \\
        Shuffled Labels &0.6993 &0.6972 &0.9040 \\
        Label Indices &0.7249 & 0.6805 & 0.9070 \\
        \bottomrule
        \end{tabular}
        }
        \captionsetup[table]{font={small}}
        \captionof{table}{\textbf{Benefit of language prior.} Results on Ego4D v2 test set. We replace original action sequences to semantically nonsensical sequences.}
        \label{tab:lang_prior}
\end{minipage}
\hfill
\begin{minipage}{0.53\textwidth}
    \centering
        \scalebox{0.9}{
        % \setlength{\tabcolsep}{1pt}
            \begin{tabular}{cccccc}
            \toprule
            Model & Setting & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
            \midrule
            7B &Pre-trained &0.6794 & 0.6757& 0.8912\\
            91M &From-scratch &0.7176 &0.7191 &0.9117  \\
            %91M &Yes &\textbf{0.6684} &0.6795 &0.8924  \\
            91M &Distilled &\textbf{0.6649} &\textbf{0.6752} &\textbf{0.8826}  \\
            \bottomrule
            \end{tabular}
        }
    \captionsetup[table]{font={small}}
    \captionof{table}{\textbf{LLM as temporal model.} Results on Ego4D v2 test set. Llama2-7B model is fine-tuned on Ego4D v2 training set. 91M models are randomly initialized.}
    \label{tab:distill}
\end{minipage}

\noindent\textbf{Language prior encoded by LLMs benefit LTA.} We further investigate if the \textit{language} (e.g. goals and action labels) used for our video representation is actually helpful to utilize the language priors encoded by the LLMs.
%After validating LLM's capability to model temporal dynamics, we further ask the question: Why does LLM perform well on LTA task? Does the language prior and encoded knowledge from large-scale pre-training benefit LTA? In order to investigate how much does LTA task benefit from the prior knowledge encoded in language,
We first conduct experiments by replacing the action label representation with two representations that we assume the pretrained LLMs are unfamiliar with: (1) \textbf{Shuffled Labels.} We randomly generate a mapping of verbs and nouns so that the original verbs/nouns are 1-to-1 projected to randomly sampled words in the dictionary to construct semantically nonsensical language sequence (e.g ``open window” to ``eat monitor”). (2) \textbf{Label Indices.} Instead of using words to represent actions in the format of verb-noun pairs, we can also use the index of the verb/noun in the dictionary to map the words to digits to form the input and output action sequence.

We fine-tune the Llama2-7B model on the three types of action representations on the Ego4D v2 dataset and report results on the test set. As shown in Table~\ref{tab:lang_prior}, the performance drops severely when shuffled action labels or label indices are used, especially for verb. The performance gap indicates that even LLMs have strong capability to model patterns beyond natural language~\citep{mirchandani2023large}, the encoded language prior from large-scale pre-training still significantly benefits long-term video action anticipation.

%For comparison, we also designed a baseline model Llama2-91M, which is a 6-layer transformer decoder with the similar structure as Llama2-7B but with a much smaller size and without any pre-training. The 91M model takes in the same input during training and evaluation and follows the same post-processing. Results on the test set are reported in Table~\ref{tab:distill} in the top two rows. From the results, we can see the pre-trained 7B model achieves excellent performance on all three metrics and largely outperforms the 91M baselines (5.3\% and 6.4\% lower ED for verb and noun), indicating LLMs' strong ability to model temporal dynamics and predict actions well when given action sequence as video representation.

%For comparison, we also designed a baseline model Llama2-91M, which is a 6-layer transformer decoder with the similar structure as Llama2-7B but with a much smaller size and without any pre-training. The 91M model takes in the same input during training and evaluation and follows the same post-processing. Results on the test set are reported in Table~\ref{tab:distill} in the top two rows. From the results, we can see the pre-trained 7B model achieves excellent performance on all three metrics and largely outperforms the 91M baselines (5.3\% and 6.4\% lower ED for verb and noun), indicating LLMs' strong ability to model temporal dynamics and predict actions well when given action sequence as video representation.

\noindent\textbf{LLM-encoded knowledge can be condensed into a compact model.} We first introduce the baseline model Llama2-91M, which is a 6-layer randomly initialized transformer decoder model with the similar structure as Llama2-7B. The 91M model takes in the same input during training and evaluation and follows the same post-processing. 
We then conduct model distillation to use the Llama2-7B model tuned on Ego4D v2 training set as the teacher model and the same randomly initialized Llama2-91M as the student model. Results on test set are shown in Table~\ref{tab:distill}. We observe that the distilled model achieves significant improvement comparing with model trained without distillation in the second row (7.3\% and 6.1\% for verb and noun). It's also worth noting that the distilled 91M model even outperforms the 7B teacher model on all three metrics, while using 1.3\% of the model size. The results confirm that LLM-encoded knowledge on \textit{implicit} goal inference and \textit{explicit} temporal modeling can be condensed into a compact neural network.

%show that in addition to sequence modeling, LLMs can also benefit LTA by distilling knowledge and language priors to compact models that requiring much less resources in practical utilization.

% We then explore if LLMs can be directly applied to model temporal dynamics, in addition to few-shot goal inference. We focus our ablation on the Ego4D benchmark as its evaluation criteria explicitly measures the ordering of the anticipated actions. We adopt the same video representation as for in-context goal inference, but now fine-tunes the LLM on the Ego4D training set. We used the fine-tuning endpoint provided by OpenAI to fine-tune the GPT-3-curie model and also the open-source Llama2 models. We picked the hyper-parameters based on the sequence completion loss on the validation set. To understand the effectiveness of the fine-tuned LLM for temporal dynamics modeling, we consider three research questions and ablations: (1) \textbf{The impact of LLM} versus transformer-based models trained from-scratch given the same recognized action inputs and does LLM leverage language priors to better model temporal dynamics; (2) \textbf{The effect on LLM model sizes} and can knowledge from large language models distilled into small models? (3) \textbf{The impact of video representation} given the similar transformer-based temporal model. \kevin{I need to reword these questions}

% \noindent\textbf{Are LLMs better at modeling temporal dynamics and does Language Prior Benefits?} First we explore the impact of LLMs and report the comparisons of LLMs and transformer models in Table~\ref{tab:lang_prior}. We observe that: \textbf{LLM models temporal dynamics better} than the transformer baseline regardless of the number of input segments. The transformer baseline with recognized actions has 4 layers and 8 attention heads, and adopt the causal attention mask for the query tokens to perform autoregressive action anticipation. \kevin{could consider move a table up or add a line to table 2 here if we have space}

% We are now curious about why LLMs are better at modeling temporal dynamics? We hypothesize they are leveraging the language priors picked up during pertraining. In order to investigate how much does LTA task benefit from the prior knowledge encoded in language, we conduct two experiments to replace the natural language of action sequences to two non-symbolic representations \shijie{not sure how to describe them...}\kevin{does this work look good?} that language models are unfamiliar with. (1) \textbf{Action index Sequence.} Instead of using text to describe action in the format of verb-noun pair, we can also use the index of the verb/noun in the dictionary to represent the verb/noun to form the input and output action sequence. (2) \textbf{Semantically nonsensical sequence.} To further understand the influence of language prior of pre-trained LLM, we shuffle the noun/verb name of the input and output action sequence. By replacing the original words to unrelated words to construct semantic-meaningless action sequence (e.g `open window” to ``eat monitor”), we observe the performance degradation to observe how much does prior knowledge encoded in semantic prior influence LLM on the LTA task. We conduct PEFT on Llama2-7B following the same setting and report results on Ego4D v1 validation set. Results are shown in table~\ref{tab:lang_prior}.



\subsection{Comparison With State-of-the-art}
Finally, we compare AntGPT with the previous state-of-the-art methods. We choose the model design settings such as recognition models and input segments number based on ablation study discussed in appendix Section~\ref{sec:ablation}. For Ego4d v1 and v2, we train the action recognition and fine-tune the LLM temporal models with their corresponding training set. Table~\ref{tab:sota_ego4d} shows performance comparisons on Ego4D v1 and v2 benchmarks. We observe that AntGPT achieves best performance on both datasets and largely outperforms other SoTA baselines. Since Ego4d v1 and v2 share the same test set, it is also worth mentioning that our model trained solely on v1 data is able to outperform any other models trained on the v2 data, which indicates the data efficiency and the promise of our approach.

For EK-55 and EGTEA, we compare the goal-conditioned AntGPT with the previous state-of-the-art results in Table~\ref{tab:sota_gaze_ek}. AntGPT achieves the overall best performance on both datasets. We observe that our proposed model performs particularly well on rare actions. 

\begin{table}
\vspace{-0.3in}
\centering
\scalebox{0.8}{
% \setlength{\tabcolsep}{1pt}
    \begin{tabular}{ccccc}
    \toprule
    Method & Version & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
    \midrule
    HierVL~\citep{hierVL} & v1 & 0.7239 & 0.7350 & 0.9276 \\
    ICVAE\citep{icvae} & v1 & 0.7410 & 0.7396 & 0.9304 \\
    VCLIP ~\citep{das2022video+} & v1 & 0.7389 & 0.7688 & 0.9412 \\
    Slowfast ~\citep{ego4d} & v1 & 0.7389 & 0.7800 & 0.9432 \\
    % \textbf{AntGPT} (ours) & v1 & 0.7023 & 0.7029 & 0.9120 \\
    % \textbf{AntGPT} (ours) & v1 & \textbf{0.6684} & \textbf{0.6787} & \textbf{0.8892} \\
    \textbf{AntGPT} (ours) & v1 & \textbf{0.6584}\scriptsize{$\pm$7.9e-3} & \textbf{0.6546}\scriptsize{$\pm$3.8e-3} & \textbf{0.8814}\scriptsize{$\pm$3.1e-3} \\

    \midrule
    Slowfast~\citep{ego4d} & v2 & 0.7169	& 0.7359 & 0.9253 \\
    VideoLLM~\citep{chen2023videollm} & v2 & 0.721 & 0.725 & 0.921\\
    PaMsEgoAI~\citep{ishibashi2023technical} &v2 &0.6838 &0.6785 &0.8933 \\
    Palm~\citep{huang2023palm} & v2 &0.6956 &0.6506 &0.8856 \\
    %\textbf{AntGPT-Distill} (ours) & v2 & 0.6553 & 0.6706 & 0.8846\\
    \textbf{AntGPT} (ours) & v2 & \textbf{0.6503}\scriptsize{$\pm$3.6e-3} & \textbf{0.6498}\scriptsize{$\pm$3.4e-3} & \textbf{0.8770}\scriptsize{$\pm$1.2e-3} \\
    \bottomrule
    \end{tabular}
}
\vspace{-0.1in}
\caption{\textbf{Comparison with SOTA methods on the Ego4D v1 and v2 test sets in ED@20.} Ego4d v1 and v2 share the same test set. V2 contains more training and validation examples than v1.
}
%\vspace{-0.2in}
\label{tab:sota_ego4d}
\end{table}

\begin{table}
\centering
% \setlength{\tabcolsep}{1pt}
\scalebox{0.8}{
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{EK-55}} & \multicolumn{3}{c}{\textbf{EGTEA}} \\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    & ALL & FREQ & RARE & ALL & FREQ & RARE \\
    \midrule
    I3D~\citep{i3d_cvpr17} & 32.7 & 53.3 & 23.0 & 72.1 & 79.3 & 53.3 \\
    ActionVLAD~\citep{girdhar2017actionvlad} & 29.8 & 53.5 & 18.6 & 73.3 & 79.0 & 58.6\\
    Timeception~\citep{hussein2019timeception} & 35.6 & 55.9 & 26.1 & 74.1 & 79.7 & 59.7 \\
    VideoGraph~\citep{hussein2019videograph} & 22.5 & 49.4 & 14.0 & 67.7 & 77.1 & 47.2 \\
    EGO-TOPO~\citep{ego-topo} & 38.0 & 56.9 & 29.2 & 73.5 & 80.7 & 54.7 \\
    Anticipatr~\citep{anticipator} & 39.1 & 58.1 & 29.1 & 76.8 &  83.3 & 55.1 \\
    \textbf{AntGPT} (ours) & \textbf{40.1}\scriptsize{$\pm$2e-2} & \textbf{58.8}\scriptsize{$\pm$2e-1} & \textbf{31.9}\scriptsize{$\pm$5e-2} & \textbf{80.2}\scriptsize{$\pm$2e-1} & \textbf{84.8}\scriptsize{$\pm$2e-1} & \textbf{72.9}\scriptsize{$\pm$1.2} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.1in}
\caption{\textbf{Comparison with SOTA methods on the EK-55 and EGTEA Dataset in mAP.} ALL, FREQ and RARE represent the performances on all, frequent, and rare target actions respectively.}
\vspace{-1em}
\label{tab:sota_gaze_ek}
\end{table}

