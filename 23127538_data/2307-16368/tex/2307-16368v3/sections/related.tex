\section{Related Work}

\noindent\textbf{Action anticipation} 
%algorithms are crucial for building intelligent agents, as they provide important signals for planning in interactive environments. The action anticipation task 
can be mainly categorized into next action prediction (NAP)~\citep{Epic-Kitchen, li2018eye} and long-term anticipation (LTA)~\citep{ego4d}. Our work focuses on the LTA task, where modeling the (latent) goals of the actors is intuitively helpful. 
% A reliable action anticipation algorithm is crucial for building intelligent agents, as it provides important signals for planning in interactive environments. Action anticipation task can be divided into next action prediction (NAP) \citep{damen2020epic, li2018eye} and long-term anticipation (LTA) \citep{ego4d} which is the particular interest lies in this paper based on the time horizon and action number to be predicted. 
Most prior works on action anticipation aim at modeling the temporal dynamics directly from visual cues, such as by utilizing hierarchical representations~\citep{lan2014hierarchical}, modeling the temporal dynamics of discrete action labels~\citep{sun2019relational}, predicting future latent representations~\citep{vondrick2016anticipating,gammulle2019predicting}, or jointly predicting future labels and features~\citep{girdhar2021anticipative, girase2023latency}. As the duration of each action is unknown, some prior work proposed to discover object state changes~\citep{epstein2021learning,souvcek2022look} as a proxy task for action anticipation. The temporal dynamics of labels or latent representations are modeled by neural networks, and are often jointly trained with the visual observation encoder in an end-to-end fashion. To predict longer sequences into the future for LTA, existing work either build autoregressive generative models~\citep{abu2018will, gammulle2019forecasting, sener2020temporal, farha2020long} or use timestep as a conditional parameter and predict in one shot based on provided timestep~\citep{ke2019time}. We consider these approaches as bottom-up as they model the shorter-term temporal transitions of human activities. 

% For the long-term anticipation task, existing works follow the paradigm of generating video-level representations and predicts actions based on the representations recursively over each future timestep

\noindent\textbf{Visual procedure planning} is closely related to long-term action anticipation, but assumes that both source state and the goal state are explicitly specified. For example, \citet{chang2020procedure} proposed to learn both forward and conjugate dynamics models in the latent space, and plans the actions to take accordingly. 
%Intuitively, knowing the goal state allows ``planning'' for the future, and can thus reduce the ambiguity in LTA. Existing procedure planning benchmarks include cooking domain~~\citep{youcook2}, daily household tasks~\citep{zhukov2019cross,Tang2019}, and even toy assembly~\citep{sener2022assembly101}.
Procedure planning algorithms can be trained and evaluated with video observations~\citep{chang2020procedure,bi2021procedure,sun2022plate,zhao2022p3iv,narasimhan2023learning,bansal2022my}, they can also be applied to visual navigation and object manipulation~\citep{driess2023palm,ahn2022can,lu2022neuro}. 
Unlike procedure planning, our top-down LTA approach does not assume access to the goal information. Our explicit inference of the high-level goals (with LLMs) also differs from prior attempts to model the goal as a latent variable, which is optimized via weakly-supervised learning~\citep{roy2022predicting,icvae}.

%Prior approaches attempt to model the goal as a latent variant and perform weakly-supervised learning based on next action prediction~\citep{roy2022predicting} and LTA~\citep{icvae} supervision. We take an alternative approach and propose to leverage the goal inference and planning capabilities of LLMs.


%discussed instructional videos procedure planning which take an observation and final goal as input and plan to anticipate the reasonable actions to reach the goal. However, these works only use vision modality and perform temporal modeling solely from pixel-level input. In this paper, we design to utilize both vision frames and text information from action labels as two views of the video observation.

% Mention how this can be adapted to LTA, but existing approaches formulate the goals as latent.
%Recipe cooking~\citep{youcook2} and instructional videos~\citep{zhukov2019cross,Tang2019}, assemble toy vehicles \citep{sener2022assembly101}.

% Connection with LTA? Latent goals!!

%Given start and end states~\citep{chang2020procedure,bi2021procedure,sun2022plate}, also for robotics applications, such as Palm-E~\citep{driess2023palm}, SayCan~\citep{ahn2022can}. Also for ~\citep{lu2022neuro}. We don't have the plans and also the goals.



%Use latent goal for next action prediction~\citep{roy2022predicting}, and LTA~\citep{icvae}. 

%Structure of the task~\citep{narasimhan2023learning}, incorrect ordering, future step predictions, etc. We are interested in longer-term prediction.

%Procedure learning~\citep{bansal2022my,lin2022learning,batra2022closer}


\noindent\textbf{Multimodal learning}, such as joint vision and language modeling, have also been applied to the action anticipation tasks. One approach is to treat the action labels as the language modality, and to distill the text-derived knowledge into vision-based models. For example, ~\citet{camporese2021knowledge} models label semantics with hand-engineered label prior based on statistics information from the training action labels. ~\citet{ghosh2023text} trains a teacher model with text input from the training set and distills the text-derived knowledge to a vision-based student model. ~\citet{sener2019zero} transfers knowledge from a text-to-text encoder-decoder by projecting vision and language representations in a shared space. Compared to these prior work, our focus is on investigating the benefits of large language models for modeling the temporal dynamics of human activities. 

%Our proposed framework decouples the vision module and the temporal dynamics module (in language), which allows us to explore the recent advances of LLMs, such as in-context learning and chain-of-thought prompting. \chen{Let's make this paragraph more compact and add a few more relevant papers}

\vspace{-1em}