\section{Introduction}

% Paper story outline
% is LTA even a well defined task?
% What are the main contributions and insights?

% Main related work:
% AVA
% Long-term feature bank
% Can Wikipedia Help Offline Reinforcement Learning?
% Socratic Models
% SayCan

% We need to limit the framing a bit, currently it is too ambitious. Instead of saying we have a general solution to represent long-videos for understanding, we claim that existing methods for LTA focus on end-to-end learning, but actually they can be safely de-coupled into two tasks.
% Main observations:
% The main challenge of LTA is still on recognition, not on perception
% Temporal dynamics can be captured by LLMs.
% Leveraging LLMs give human-machine interaction possibilities, and naturally support goal-conditioned queries.

% Do we need the pixels for video understanding? The answer seems obvious for recognition tasks, such as object detection or action recognition.

% Long-term anticipation as planning in language space. Avoid the need to learn to infer the latent intents from annotated videos.

% 1: Define what is LTA, why it is important, why we want to formulate as procedure planning.
% 2: Recent success on saycan and other robotics applications. Clearly LLMs are promising.
% 3: How to represent the visual inputs remains challenging. We have the videobert approach and other approaches, but a natural interface is obvious. We explore simpler interfaces in this work.
% 4: Our overall approach has two steps: action recognition, action prediction. Action prediction can be improved by advances of LLMs. We can close the loop by feeding back the outputs of LLMs into our model.
% 5: Summary of empirical results and our main contributions.

Our work addresses the long-term action anticipation (LTA) task from video observations. Its desired outputs are sequences of \textit{verb} and \textit{noun} predictions over a typically long-term time horizon for an actor of interest. LTA is a crucial component for human-machine interaction. A machine agent could leverage LTA to assist humans in scenarios such as daily household tasks~\citep{ego-topo} and autonomous driving~\citep{gao2020vectornet}. The LTA task is challenging due to noisy perception (e.g. action recognition), and the inherent ambiguity and uncertainty that reside in human behaviors.

A common approach for LTA is \textit{bottom-up}, which directly models the temporal dynamics of human behavior either in terms of the discrete action labels~\citep{sun2019relational}, or the latent visual representations~\citep{vondrick2016anticipating}. 
%Most of the recent approaches for bottom-up LTA are implemented as neural networks that are end-to-end trained from visual inputs. 
Meanwhile, human behaviors, especially in daily household scenarios, are often ``purposive''~\citep{kruglanski2020habitual}, and knowing an actor's longer-term goal can potentially help action anticipation~\citep{tran2021goal}. As such, we consider an alternative \textit{top-down} framework: It first explicitly infers the longer-term goal of the human actor, and then \textit{plans} the procedure needed to accomplish the goal. However, the goal information is often left unlabeled and thus latent in existing LTA benchmarks, making it infeasible to directly apply goal-conditioned procedure planning for action anticipation. %\chen{Add reference on how humans use language to communicate actions and procedures.}

%a main challenge of this top-down approach is the lack of supervised data mapping visual observations to goals in existing datasets, hence existing work often models the goal as a latent variable~\citep{icvae}.

Our paper seeks to address these challenges in modeling long-term temporal dynamics of human behaviors. Our research is inspired by prior work on the mental representations of tasks as \textit{action grammars}~\citep{payne1986task,pastra2012minimalist} in cognitive science, and by large language models' (LLMs) empirical success on procedure planning~\citep{ahn2022can,driess2023palm}. We hypothesize that the LLMs, which use procedure text data during pretraining, encode useful prior knowledge for the long-term action anticipation task. Ideally, the prior knowledge can help both bottom-up and top-down LTA approaches, as they can not only answer questions such as ``what are the most likely actions following this current action?'', but also ``what is the actor trying to achieve, and what are the remaining steps to achieve the goal?''


%infer the latent goals and perform procedure planning in natural language. More generally, our work seeks to answer the question: Can LLMs help long-term action anticipation from video observations?



%Our question is inspired by the recent advances on large language models' effectiveness of robotic planning via natural language (e.g. SayCan~\citep{ahn2022can} and PaLM-E~\citep{driess2023palm}), or visual question answering via computer programs (e.g. ToolFormer~\citep{schick2023toolformer} and ViperGPT~\citep{suris2023vipergpt}).


%This work focuses on long-term action anticipation (LTA) from video observations. The LTA task is an important component to enable human-machine interactions for daily household tasks~\citep{ego-topo}, autonomous driving~\citep{gao2020vectornet}, etc. A common approach for LTA is ``bottom-up'', by modeling temporal dynamics in discrete labels~\citep{sun_cvpr19_forcasting} or the latent space~\citep{vondrick2016anticipating} of visual inputs. We propose an alternative, ``top-down'' framework that formulates action anticipation as \textit{procedure planning}~\citep{chang2020procedure}, by first inferring the (latent) goal of the actor based on the visual observations, and then generate the steps necessary to accompolish the goal. However, a main challenge of this top-down approach is the lack of supervised data mapping visual observations to goals. We hypothesize that large language models (LLMs)~\citep{gpt3}, which use procedure data such as recipes during pretraining, can potentially infer the latent goals and perform procedure planning in natural language. We then ask the questions, can LLMs help long-term action anticipation from video observations?

%Our approach is inspired by the recent advances on large language models' effectiveness of robotic planning via natural language (e.g. SayCan~\citep{ahn2022can} and PaLM-E~\citep{driess2023palm}), or visual question answering via computer programs (e.g. ToolFormer~\citep{schick2023toolformer} and ViperGPT~\citep{suris2023vipergpt}). 

Concretely, our paper strives to answer four research questions on modeling human behaviors for long-term action anticipation: (1) Does top-down (i.e. goal-conditioned) LTA outperform the bottom-up approach? (2) Can LLMs infer the goals useful for top-down LTA, with minimal additional supervision? (3) Do LLMs capture prior knowledge useful for modeling the temporal dynamics of human actions? If so, what would be a good interface between the videos and an LLM? And (4) Can we condense LLMs' prior knowledge into a compact neural network for efficient inference? 

%\chen{Answers: (1) Actions are effective for both goal inference and temporal dynamics; (2) Top-down LTA is helpful for three datasets; (3) LLMs can implicitly perform top-down LTA while modeling temporal dynamics; (4) Such information can be distilled into a much smaller model. Downplay few-shot.}

To perform quantitative and qualitative evaluations necessary to answer these questions, we propose \textbf{AntGPT}, which constructs an action-based video representation, and leverages an LLM to perform goal inference and model the temporal dynamics. We conduct experiments on multiple LTA benchmarks, including Ego4D~\citep{ego4d}, EPIC-Kitchens-55~\citep{Epic-Kitchen}, and EGTEA GAZE+~\citep{li2018eye}. Our evaluations reveal the following observations: First, we find that our video representation, based on sequences of noisy action labels from action recognition algorithms, serves as an effective interface for an LLM to infer longer-term goals, both qualitatively from visualization, and quantitatively as the goals enable a top-down LTA pipeline to outperform its bottom-up counterpart. The goal inference is achieved via in-context learning~\citep{gpt3}, which requires few human-provided examples of action sequence and goal pairs. Second, we observe that the same video representation allows effective temporal dynamics modeling with an LLM, by formulating LTA as (action) sequence completion. Interestingly, we observe that the LLM-based temporal dynamics model appears to perform implicit goal-conditioned LTA, and achieves competitive performance without relying on explicitly inferred goals. These observations enable us to answer the final research question by distilling the bottom-up LLM to a compact student model 1.3\% of the original model size, while achieving similar or even better LTA performance.


% Summarize the significance of our observations, and indicate future research directions
% LLMs encode useful prior information about modeling human temporal dynamics


%which first recognizes human actions based on supervised action recognition algorithms~\citep{feichtenhofer2019slowfast,radford2021clip}. \textbf{AntGPT} feeds the recognized actions as discretized video representation to large language models in order to get the goal of such actions or the future actions, which can be optionally post-processed into the final predictions. For bottom-up LTA, the temporal model autoregressively predict the future action sequences, either via fine-tuning or in-context learning~\citep{gpt3}. To achieve top-down LTA, we first ask LLMs to predict the goal of the actor before generating the actors' actions. Then, LLM-generated goals are fed into the temporal model with actions to make goal-conditioned predictions. Additionally, we distill the trained LLM into a compact student model, shrinking it to 1.3\% of the teacher model's size. Finally, we examine \textbf{AntGPT}'s capability to perform few-shot bottom-up LTA via in-context learning~\citep{gpt3}, and top-down LTA via chain-of-thought~\citep{wei2022chain}, respectively. 
%The overall \textbf{AntGPT} framework and different methods of applying LLMs are illustrated in Figure~\ref{fig:model}. \chen{Paraphrase to match the four questions asked in the previous paragraph. Adjust the flow based on ``implicit procedure planning'' (when fine-tuning) and justify distillation.}

%We conduct experiments on several LTA benchmarks, including Ego4D~\citep{ego4d}, EPIC-Kitchens-55~\citep{Epic-Kitchen}, and EGTEA GAZE+~\citep{li2018eye}. The quantitative and qualitative experiments demonstrate that LLMs can not only infer the high-level goals of the actors that benefit top-down action anticipation but also be applied directly to LTA task for temporal dynamics modeling. We also designed experiments that replacing action sequences in natural language to semantically nonsensical sequences, validating the language prior from large-scaled pre-training is indeed beneficial, and further knowledge distillation experiments demonstrate that the language prior from LLMs could be well distilled into more compact models. Our paper makes the following contributions:
%We also observe that the LLMs are able to perform counterfactual action anticipation given different goals as inputs. 
To summarize, our paper makes the following contributions: 
%\chen{Briefly discuss the new experiments validating language prior is indeed helpful, and that the prior can be effectively distilled into a more compact model.}

1. We propose to investigate if large language models encode useful prior knowledge on modeling the temporal dynamics of human behaviors, in the context of bottom-up and top-down action anticipation.
%1. We formulate long-term action anticipation as bottom-up and top-down approaches, and propose to leverage large language models for goal generation and temporal dynamics modeling.

2. We propose the \textbf{AntGPT} framework, which naturally bridges the LLMs with computer vision algorithms for video understanding, and achieves state-of-the-art long-term action anticipation performance on the Ego4D LTA v1 and v2 benchmarks, EPIC-Kitchens-55, and EGTEA GAZE+.

3. We perform thorough experiments with two LLM variants and demonstrate that LLMs are indeed helpful for both goal inference and temporal dynamics modeling. We further demonstrate that the useful prior knowledge encoded by LLMs can be distilled into a very compact neural network (1.3\% of the original LLM model size), which enables efficient inference.
\vspace{-1.2em}

%3. Thorough experiments with two LLMs demonstrate that LLMs help LTA from three aspects: 1. Goal generation for top-down LTA. 2. Temporal dynamic modeling. 3. Knowledge distillation to small models, and also validate the significance of language prior from large-scale pre-training.

% evaluations to understand the important design choices, promises and limitations of LLMs when applied to the LTA task.

% 3. We conduct thorough quantitative and qualitative evaluations to understand the important design choices, promises and limitations of LLMs when applied to the LTA task.
%1. We propose to model long-term action anticipation task as a language sequence modeling task and use LLM for temporal modeling, with the benefits discussed above.\\
%2. Our proposed method achieves pl performance, outperforming (sota/baseline model) without vision features during temporal modeling.\\%\shijie{we only use vision features during pre-processing.}\\
%3. We conduct further research on prompt engineering and Chain-of-Thoughts which shows further improvement, unveiling the potentiality for combining latest NLP development with vision tasks.


%\shijie{(1) What is the main advantage of treating LTA as sequence modeling rather than prediction? (2) Does experiment support our claim and is there any corner case that traditional methods hard to achieve but we can. (3) Since this is a very first work, what do we think are the main disadvantages and does our methods have the potential to be further extended, combined with traditional methods?}

%\chen{ (1) it offers one nice way to represent and understand long videos (although we only demonstrate on LTA task), note that we don't argue it's the best way or the only way, but a possible way with nice properties; (2) and the nice properties would be it allow us to build on the success of LLMs, such as action priors from text-only pre-training data, CoT, etc. (3) we should try to have Ce's PTE to incorporate Kevin's GPT output text, and see if it's helpful or not, helpful but not must have.} 

%\nate{(Feedback on the intro section: I love the motivation, I think it's super strong and well written. I want to suggest potentially rewriting the start of the paragraph that begins "our paper strives to answer four questions...", so that you actually list the four questions, and so that there is some parallel structure between what "our paper strives to answer" and "our paper makes the following contributions". As a reader, it would be nice to see that you strived to answer those four questions, and then your contributions, one by one, answered those four questions... definitely not necessary, just a thought, might strengthen the argument a little. Let me know if you want me to take a crack at re-wording the beginning of that paragraph.)}