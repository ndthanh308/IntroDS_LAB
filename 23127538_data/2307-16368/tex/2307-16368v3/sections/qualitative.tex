\section{Visualization and Analysis}

\subsection{Analysis on In-context Learning}
Previous quantitative results and analysis have answered our question that prior knowledge about temporal modeling from LLM greatly benefit bottom-up future prediction by finetuning LLM. In this part we want to further exploring the following two questions: Can LLM infers goal based on observed actions and how would predicting goal influence top-down future prediction? What will happen if explicitly knowing the goal and how would the given goal affect LLM's prediction. To answer these first questions, we designed three qualitative experiments and analysis the results. For the second question, we designed counterfactual prediction experiment.

% We wish to study qualitatively HOW in-context learning with optionally Chain-of-Thought would make LLM prediction the future action sequences. We demonstrate a few examples from our training data to the LLM and query for the action sequence of interest. We design three type of prompt corresponding to three questions we want to know about. Concretely, we wish to study the LLM's capability in 1)predicting the goal, 2)predicting the future actions, and 3)predicting the future actions given the predicted goal of the actor. Figure 2 is an illustrative examples of our in-context learning approach. 

\noindent\textbf {ICL: Goal Prediction}
For top-down future anticipation methods, we hope the model to "think about future" before making predictions. In order to achieve this, we design qualitative experiment to see if the model could output reasonable goal given previous actions. By manually checking the predicted goal, we find LLM is able to generate reasonable and related long-term goal according to the previous observations. Figure~\ref{fig:vis2} shows an example video. For this video, LLM succeed predicting the goal as 'painting a room' given the action recognition results even there are few obvious mistakes like recognizing 'paintbrush' to 'stick' and 'container' to 'string'. Although successfully predicting 'painting' as the goal, LLM wrongly predict the goal as a room while the video happens outdoor. This is probably due to our discrete representations lost the detailed visual context.


\noindent\textbf{ICL: Future Action Prediction}
After verifying that LLM could make reasonable predictions of potential goal given past actions, we design the second experiment to use ICL to predict future actions. Follwing the ICL prompt design, we provide LLM several examples and history actions. We demonstrate the prompts and answers from LLM in the yellow block in Figure~\ref{fig:vis2}. In this example, the prediction edit distance for verb and noun are 0.55 and 0.55. For the first action, the model predict 'clean brush' while the answer is 'dip paintbrush', even though it failed in both verb and noun during metric calculating, but 'brush' and 'paintbrush' are actually synonyms so that the noun prediction also makes sense. This reveals that LLM results have further improvement space by using better post-processing methods to deal with complex linguistic phenomenon and the ambiguity of the task itself.

\noindent\textbf{CoT: Goal and Future Action Prediction}
The previous two experiments show that LLM can infer goals based on previous actions and do bottom-up future prediction using ICL. We further design experiment to see if LLM can predict both goal and future in a top-down fashion. In the green block in Figure~\ref{fig:vis2}, we demonstrate the procedure of top-down prediction using CoT prompts. The model succeed to predict the goal and get a significant improvement on verb from 0.55 to 0.10. For the first action, the model predicts 'dip brush' which actually match the ground-truth 'dip paintbrush'. Compared with bottom-up prediction 'clean brush', it's also more reasonable since it's not common to wash the brush during painting. Besides, bottom-up prediction shows the pattern of simply repeating the action 'paint wall' while top-down model successfully predict to 'paint wall' and 'dip brush' alternatively, which is not only closer to ground-truth labels but also better match common sense in the scene of 'painting a wall'. From this example, we can see predicting the goal before anticipating future can help generate examples more congenial with reason and common sense.

\noindent\textbf{Counterfactual: What If Given an Alternative Goal?}
To better understand the how would providing goal explicitly during prediction, we designed counterfactual prediction experiments: We first use GPT-3.5-Turbo to generate goal for the given example as we did above and treat this as the 'oracle goal'. We then manually design an 'alternative goal' which also makes sense to human when given the previous actions but different from the oracle. We then perform ICL prediction based on two different goals and the same observations. Figure~\ref{fig:vis3} illustrates the overall flow and two examples. Taking the second sample as example, even though sharing the same observations, the goal of 'fix machine' generates future actions more highly related with fixing such as 'tighten nut' and 'turn screw' while the goal of 'examine machine' generates totally different but highly goal-related actions like 'test machine' and 'record data'.

The counterfactual prediction experiment shows that explicit goal hugely influence LLM's prediction and LLM can generate reasonable prediction (although not correct) when giving the alternative goal. This could also explain the performance drop after explicitly adding LLM-generated goal into prompts hurts the performance shown in Table~\ref{tab:input_types} in some way: explicitly adding suboptimal goal will distract LLM from effectively reasoning from history actions.

%To observe how does the latent goal impact the LLLM's prediction, we come up with this setting where we switch the correct latent goal output by the model earlier with a perturbed but relative latent goal designed by ourselves and observe the change in predictions. Concretely, we want to know if the LLM rely on the explicitly expressed latent goal in the context to make predictions which is crucial in understanding the effectiveness of our top-down approach. As illustrated in Figure 4, we observed that even small changes in the latent goal would make the language model output quite different predictions. We also found that the predictions made in the counterfactual setting are consistent with the new goal. For example, when we switch the scene descriptor from "fixing machine" to "examining machine", the language model outputs many things only related to "examining machine", such as "examine circuits" or "record data". This shows that explicit latent goals have a high impact on large language model's prediction.


% Figure environment removed