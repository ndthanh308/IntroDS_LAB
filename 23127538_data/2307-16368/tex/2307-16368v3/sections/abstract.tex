\begin{abstract}

Can we better anticipate an actor's future actions (e.g. \textit{mix eggs}) by knowing what commonly happens after the current action (e.g. \textit{crack eggs})? What if the actor also shares the goal (e.g. \textit{make fried rice}) with us? The long-term action anticipation (LTA) task aims to predict an actor's future behavior from video observations in the form of verb and noun sequences, and it is crucial for human-machine interaction. We propose to formulate the LTA task from two perspectives: a \textit{bottom-up} approach that predicts the next actions autoregressively by modeling temporal dynamics; and a \textit{top-down} approach that infers the goal of the actor and \textit{plans} the needed procedure to accomplish the goal. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos), have the potential to help LTA from both perspectives. It can help provide the prior knowledge on the possible next actions, and infer the goal given the observed part of a procedure, respectively. We propose \textbf{AntGPT}, which represents video observations as sequences of human actions, and uses the action representation for an LLM to infer the goals and model temporal dynamics.
%It first recognizes the actions already performed in the observed videos and then asks an LLM to predict the future actions via conditioned generation, or to infer the goal and plan the whole procedure by chain-of-thought prompting. 
%Empirical results on the Ego4D LTA v1 and v2 benchmarks, EPIC-Kitchens-55, as well as EGTEA GAZE+ demonstrate the effectiveness of our proposed approach. 
\textbf{AntGPT} achieves state-of-the-art performance on Ego4D LTA v1 and v2, EPIC-Kitchens-55, as well as EGTEA GAZE+, thanks to LLMs' goal inference and temporal dynamics modeling capabilities. We further demonstrate that these capabilities can be effectively distilled into a compact neural network 1.3\% of the original LLM model size. Code and model are available at \href{https://brown-palm.github.io/AntGPT}{brown-palm.github.io/AntGPT}. %\href{https://brown-palm.github.io/AntGPT}{brown-palm.github.io/AntGPT}.

%\shijie{we actually have two kind of models now, one uses LLM as main model for future prediction, another use LLM to generate goals only and provide text-based goal as multimodal feature to Vision Model, should think if we want to call both of them AntGPT} \kevin{Let's keep AntGPT, one name is stronger and more memorable, in both setup we use GPT models to do anticipation so this is a good name}
%We propose to formulate the long-term action anticipation (LTA) task as procedure planning: Instead of predicting the upcoming actions based on visual observations, we ask a model to infer the latent goals and the steps necessary to accomplish them. We hypothesize that large language models (LLMs) pretrained on procedure data (e.g. recipes, how-tos) can potentially infer the latent goals and perform procedure planning in natural language, and propose a LTA framework that decouples action recognition and anticipation. We empirically observe that a simple approach to provide recognized actions as input sequences to the LLM already leads to competitive performance, and improved designs on the prompt templates can further improve the performance. Our proposed framework outperforms end-to-end approaches trained with video inputs, and a decoupled approach where the ``language model'' is trained from scratch. We demonstrate that our framework can perform ``counterfactual'' prediction by providing different goals as inputs. Our approach achieves competitive performance on the Ego4D LTA and the EGTEA Gaze+ benchmarks.


% The common held belief is that multimodal learning is the key for long form video understanding. In this paper, we present a completely different approach that decouples visual recognition and temporal dynamics modeling for long-term action anticipation (LTA) tasks, the former is solved by vision algorithms, the latter is solved by LLMs. Our approach is motivated by the idea that we can represent video observations as discrete action sequences and leverage the power of Large Language Models for forecasting by reasoning in predicting the future actions.
%We show that our methods outperform vision-only models on three datasets[numbers]. To study whether the built-in action prior of LLM helps forecasting, we demonstrate that the our LLM based model performs significantly better than from-scratch models with the same inputs. Furthermore, we have investigated the effect of Chain of Thought(CoT) in boosting the forecasting performance of LLMs through explicit reasoning.    \chen{Finish this.}
\end{abstract}


