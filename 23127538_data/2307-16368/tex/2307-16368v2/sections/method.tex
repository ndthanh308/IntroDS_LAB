\section{Method}

We introduce our proposed \textbf{AntGPT} framework for LTA. An overview is shown in Figure~\ref{fig:model}.

\subsection{Long-term Action Anticipation} 
The long-term action anticipation (LTA) task requires predicting a sequence of $Z$ actions in a long future time horizon based on a video observation. In the LTA task, a long video $V$ is split into an ordered set of $N$ annotated short segments $\{S^j, a^j\}_{j=1}^N$, where $S^j$ denotes the $j$-th segment in video $V$ and $a^j$ denotes the corresponding action label in the form of noun-verb pair $(n^j, v^j)$. The video is also specified with a stop time $T$, which is represented as the index of the last observed segment. In this way, a video is split into the observed segments $V_{o}$ and the future segments of the video $V_{f}$ whose labels $\{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\}$ are to be predicted. 
A hyper-parameter $N_\text{seg} $ controls how many segments the model can observe. Concretely, we take the observable video segments $\{S^j\}_{j=T-N_\text{seg}+1 }^T$ from $V_{o}$ as input and output action sequence $\{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\}$ as predictions. Alternatively, Ego-Topo~\cite{ego-topo} takes a simplified approach, which only requires predicting the set of future actions, but not their ordering.

% Figure environment removed

\noindent\textbf{Bottom-up and Top-down LTA.} We categorize action anticipation models into bottom-up and top-down. 
The bottom-up approach directly models the temporal dynamics from the history observations and predicts future actions autoregressively or in parallel. The top-down framework first explicitly infers the longer-term goal from the history actions, then plans the procedure according to both history and the goal. We define the prediction procedure of bottom-up model $\mathcal{F}_\text{bu}$ as $\{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{F}_\text{bu}(V_o)$. Here $a^j$ denotes the $j$-th video segment's action label, and $T$ is the index of the last observed segment. For the top-down model $\mathcal{F}_\text{td}$, we formulate the prediction procedure into two steps: First, infer the goal by $g = \mathcal{G}_\text{td}(V_o)$, then perform goal-conditioned planning as $    \{\hat{a}^{(T+1)}, ..., \hat{a}^{(T+Z)}\} = \mathcal{F}_\text{td}(V_o, g)$, where $g$ corresponds to the long-term goal inferred by the top-down model.

\subsection{Video Representation}
To understand the benefits of LLMs for video-based LTA, an important design choice is the interface~\cite{zeng2022socratic,suris2023vipergpt} between visual inputs and the language model. 
We are interested in investigating how to represent long-form videos in a compact, text-only bottleneck, while being helpful for goal inference and procedure planning with LLMs. The video data often contains complex and dynamic scenarios, with multiple characters, actions, and interactions occurring over an extended period. While such rich information can be potentially captured by (pretrained) visual embeddings or even ``video tokens''~\cite{sun2019videobert,wang2022bevt}, it remains unclear what visual representation would be sufficient to compress the long observed video context, while being friendly to the LLMs. 

We first consider the standard approach to represent video frames as distributed embedding representations, computed with pre-trained vision backbone models, such as the CLIP visual encoder~\cite{radford2021clip}. For each video segment $S^j$ in $V_o$, the backbone extracts the representations of $n$ uniformly sampled frames from this segment to obtain $E^j = \{e_1, e_2, \ldots, e_n\}$. A neural network can then take the embedding representation and predict action labels for the observed frames (action recognition), or the future timesteps (action anticipation).

Our action recognition network $\mathcal{E}$ is implemented as a Transformer encoder. It takes in the visual embeddings and one learnable query token as the input. We then apply two separate MLP heads to decode the verb and noun from the encoded query token. For each observed video segment $S^j$, the recognition model $\mathcal{E}$ takes in randomly sampled image features $ E^j_{s} = \{e_a, e_b, \ldots, e_k\}, E^j_{s} \subseteq E^j$,  and outputs the corresponding action $\hat{a}^{(j)}$ for $S^j$. This process is repeated for every labeled segment in $V_{o}$, which results in $N_\text{seg} $ actions $\{\hat{a}^{(T-N_\text{seg} )}, ..., \hat{a}^{(T)}\}$, in the format of noun-verb pairs. The recognition model $\mathcal{E}$ is trained on the training set to minimize the Cross Entropy Loss between the predictions and the ground-truth action labels.

\noindent\textbf{How to Represent Videos for the LLMs?} We consider a simple approach to extract video representations for a large language model. We first compute the embedding representation of $V_o$, and then apply the action recognition model $\mathcal{E}$ to convert the distributed representation into discrete action labels, which can be directly consumed by an off-the-shelf LLM. Despite its simplicity, we observe that this representation is strong enough for the LLM to extract meaningful high-level goals for top-down LTA (see Section~\ref{sec:antgpt}), and can even be applied directly to perform both bottom-up and top-down LTA with the LLMs. Alternative approaches, such as discretizing the videos via video captioning or object detection, or projecting the visual embedding via parameter-efficient fine-tuning~\cite{hu2021lora,merullo2022linearly}, can also be applied under our proposed framework. We leave these explorations as interesting future work.

\subsection{\textbf{AntGPT}: Long-term Action Anticipation with LLMs}\label{sec:antgpt}
We now describe \textbf{AntGPT} (Action \textbf{Ant}icipation \textbf{GPT}), a framework that incorporates LLMs for the LTA task. An LLM serves both as an few-shot high-level goal predictor via in-context learning, and also as a temporal dynamics model which predicts the future actions conditioned on the observed actions. It hence benefits top-down and bottom-up LTA, in full-shot and few-shot scenarios.

\noindent\textbf{Few-shot Goal Inference.} In order to perform top-down long-term action anticipation, we conduct in-context learning on LLMs to infer the goals by taking the recognized action labels as inputs, as illustrated in Figure~\ref{fig:model} (b) and (c). The ICL prompts $q_\text{goal}$ is formulated with examples in the format of \texttt{"<observed actions> => <goal>"} and the final query in the format of \texttt{"<observed actions> =>"}. The observed actions for the in-context examples are based on ground-truth annotations, and the observed actions in the final query are generated by recognition models.
Since no ground truth goals are available, we either use the video metadata as \textit{pseudo goals} when it is available, or design the goals manually. Figure~\ref{fig:goals} shows several examples for in-context goal inference with the LLM. We treat the raw output of the LLM $T_\text{goal} = \mathcal{\pi}(q_\text{goal})$ as the high-level goal.

\noindent\textbf{Bottom-up and Top-down LTA.} We now describe a unified framework to perform bottom-up and top-down LTA. The framework largely resembles the action recognition network $\mathcal{E}$ which takes visual embeddings as inputs, but has a few important distinctions. Let's first consider the bottom-up model $\mathcal{B}$. Its transformer encoder takes sub-sampled visual embeddings $E^j_{s}$ from each segment $S^j$ of $V_o$. The embeddings from different segments are concatenated together along the time axis to form the input tokens to the transformer encoder. To perform action anticipation, we append additional learnable query tokens to the input sequence of the Transformer encoder, each of which corresponds to a future step to predict. Each encoded query token is decoded into verb and noun predictions with two separate MLP heads. We minimize the Cross Entropy Loss for all future actions to be predicted with equal weights. Note that one can choose to use either bidirectional or causal attention masks for the query tokens, resulting in parallel or autoregressive action prediction. We observe that this design choice has marginal impact on performance, and use parallel decoding unless otherwise mentioned. 

Thanks to few-shot goal inference with in-context learning, implementing the top-down model $\mathcal{F}_\text{td}$ is straightforward: We first embed the inferred goals $T_\text{goal}$ with a pre-trained CLIP text encoder. The goal token is then prepended at the beginning of the visual embedding tokens to perform goal-conditioned action anticipation. During training, we use ground-truth action labels to infer the goals via in-context learning. During evaluation, we use the recognized action labels to infer the goals.

\noindent\textbf{Modeling Temporal Dynamics with LLMs.}
We further investigate if LLMs are able to model temporal dynamics via recognized action labels and perform action anticipation via autoregressive sequence completion. We first study the fully supervised scenario, where we perform parameter-efficient (optionally) fine-tuning on LLMs on the training set of an LTA benchmark. Both the input prompt and the target sequence are constructed by concatenating the action labels separated with commas. During training, the input sequences are formed either via teacher forcing (ground truth actions), or the (noisy) recognized actions. The LLM is optimized with the standard sequence completion objective. During inference, we use the action recognition model $\mathcal{E}$ to form input prompts from the recognized action labels. We perform postprocessing to convert the output sequence into action labels. Details of the postprocessing can be found in Section~\ref{sec:postprocessing}. To perform top-down LTA, we simply prepend an inferred goal at the beginning of each input prompt. The goals are again inferred from ground-truth actions during training, and recognized actions during evaluation.

\noindent\textbf{Knowledge Distillation}~\cite{hinton2015distilling} is applied to understand if the knowledge encoded by LLMs about temporal dynamics can be condensed into a much more compact neural network for efficient inference. For sequence models such as LLMs, the distillation loss is calculated as the sum of per-token losses between the encoded feature (e.g. logits) sequences by the teacher and the student. Formally, during distillation, given the input sequence $x$ of length $N$, a well-trained  LLM as the teacher model $\pi_{t}$, the student model $\pi_{s}$ is optimized to minimize the language modeling loss $\mathcal{L}_\text{lm}$ and distillation loss $\mathcal{L}_\text{dist} = \sum_{i=1}^ND_{KL}(\hat{y}_t^{(i)} || \hat{y}_s^{(i)})$, where $\hat{y}_t=\pi_t(x)$ and $\hat{y}_s=\pi_s(x)$ are the feature sequence encoded by $\pi_t$ and $\pi_s$ respectively, $i$ is the token index of the target sequence, and $D_{KL}$ is the Kullback-Leibler divergence between the teacher and student distribution. The teacher model $\pi_t$ is frozen during training. An illustration is shown in Figure~\ref{fig:model} (d).

\noindent\textbf{Few-shot Learning with LLMs.} Beyond fine-tuning, we are also interested in understanding if LLM's in-context learning capability generalizes to the LTA task. Compared with fine-tuning model with the whole training set, in-context learning avoids updating the weights of a pre-trained LLM.
As illustrated in Figure~\ref{fig:model} (e), an ICL prompt consists of three parts: First, an instruction that specifies the anticipating action task, the output format, and the verb and noun vocabulary. Second, the in-context examples randomly sampled from the training set. They are in the format of \texttt{"<observed actions> => <future actions>"} with ground-truth actions. Finally, the query in the format \texttt{"<observed actions> => "} with recognized actions. An example of the model's input and output is shown in Figure~\ref{fig:vis2} (b). Alternatively, we also attempt to leverage chain-of-thoughts prompts~\cite{wei2022chain} (CoT) to ask the LLM first infer the goal, then perform LTA conditioned on the inferred goal. An example of CoT LTA is shown in Figure~\ref{fig:vis2} (c).