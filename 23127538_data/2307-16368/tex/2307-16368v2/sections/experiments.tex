\vspace{-1em}
\section{Experiments}
\vspace{-1em}

% Figure environment removed


We now present quantitative results and qualitative analysis on the Ego4D~\cite{ego4d}, EPIC-Kitchens~\cite{Epic-Kitchen}, and EGTEA Gaze+~\cite{li2018eye} benchmarks.  

\subsection{Experimental Setup}
\label{sec:exp}
\noindent\textbf{Ego4D v1} \cite{ego4d} contains 3,670 hours of egocentric video of daily life activity spanning hundreds of scenarios. We focus on the videos in the \textit{Forecasting} subset which contains 1723 clips with 53 scenarios. The total duration is around 116 hours. There are 115 verbs and 478 nouns in total. We follow the standard train, validation, and test splits from ~\cite{ego4d}.

\noindent\textbf{Ego4D v2} extends Ego4d v1. It contains 3472 annotated clips with total duration of around 243 hours. There are 117 verbs and 521 nouns. We follow the standard train, validation, and test splits.

\noindent\textbf{EPIC-Kitchens-55}~\cite{Epic-Kitchen} (EK-55) contains 55 hours egocentric videos of cooking activities of  different video takers. Each video is densely annotated with action labels, spanning over 125 verbs and 352 nouns. We adopt the train and test splits from~\cite{ego-topo}.

\noindent\textbf{EGTEA Gaze+}~\cite{li2018eye} (EGTEA) contains 86 densely labeled egocentric cooking videos over 26 hours. There are 19 verbs and 53 nouns. We adopt the splits from~\cite{ego-topo}.

\noindent\textbf{Evaluation Metrics.} For Ego4D, we use the edit distance (ED) metric. It is computed as the Damerau-Levenshtein distance over sequences of predictions of verbs, nouns or actions. We follow the standard practice in~\cite{ego4d} and report the minimum edit distance between each of the top $K=5$ predicted sequences and the ground-truth. We report Edit Distance at $Z = 20$ (ED@20) on the validation set and the test set.
For EK-55 and EGTEA, we follow the evaluation metric described in~\cite{ego-topo}. The first K\% of each video is given as input, and the goal is to predict the set of actions happening in the remaining (100-K)\% of the video as multi-class classification. We sweep values of K = [25\%, 50\%, 75\%] representing different anticipation horizons and report mean average precision (mAP) on the validation sets. We report the performances on all target actions (All), the frequently appeared actions (Freq), and the rarely appeared actions (Rare) as in~\cite{ego-topo}. A number of previous work reported performance on these two datasets. The order agnostic LTA setup in these two datasets complements the Ego4D evaluation.


\noindent\textbf{Implementation Details.} We use the frozen CLIP~\cite{radford2021clip} ViT-L/14 for image features, and a transformer encoder with 8 attention heads, and 2048 hidden size for the recognition model. To study the impact of vision backbones, we also include EgoVLP, a video backbone pre-trained on Ego4D datasets. For the large language models, we adopt open-source Llama2-13B for in-context learning and 7B model for fine-tuning. For comparison, we also use OpenAI's GPT-3.5 Turbo for in-context learning and GPT-3 curie for fine-tuning. More details and ablation study on recognition model, teacher forcing, LLMs and other design choices are described in appendix.

% \vspace{-0.1in}
% Figure environment removed

\subsection{Can LLMs Infer Goals to Assist Top-down LTA?}
We compare two LLMs, GPT-3.5 Turbo and Llama2-chat-13B, on goal inference: To obtain the pseudo ground-truth goals for constructing the in-context examples, we use the video titles for EGTEA, and the video descriptions for EK-55. We manually annotate the goals for Ego4D. We use 12 in-context examples to infer the goals. For EK-55 and EGTEA, we always use the recognized actions in the first 25\% of each video to infer the goals. For Ego4D, we set $N_\text{seg} = 8$.

We first use the Transformer encoder model described in Section~\ref{sec:antgpt} as the temporal model: It allows us to study the standalone impact of goal conditioning by comparing the bottom-up and the top-down LTA performances. The Transformer encoder takes in the same visual features as used for action recognition. The text embeddings of the inferred goals are provided for the top-down variant.
Table~\ref{tab:trend} shows results on Ego4D v1, EK-55, and EGTEA. We notice a clear trend that using the inferred goals leads to consistent improvements for the top-down approach, especially for the \textit{rare} actions of EK-55 and EGTEA. We also noticed that both LLMs are able to infer helpful goals for top-down LTA and GPT-3.5 Turbo generates goals slightly better than the ones from Llama2-chat-13B. We also construct ``oracle goals'' using the video metadata provided by EK-55 and EGTEA datasets. We observe that using the oracle goals leads to slight improvements, indicating that the inferred goals already offer competitive performance improvements. Figure~\ref{fig:goals} provides some examples of the helpful and unhelpful goals inferred by Llama2.

\begin{table}[h]
\setlength\tabcolsep{3pt}
\small
\centering
% \vspace{-0.1in}
    \begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}{*}{Method} &\multicolumn{2}{c}{Ego4d v1 (ED)} & \multicolumn{3}{c}{EK-55 (mAP)} & \multicolumn{3}{c}{EGTEA (mAP)} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
     & Verb $\downarrow$ & Noun $\downarrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ \\
    \midrule
    image features &0.735 & 0.753 & 38.2 & \textbf{59.3} & 29.0 & 78.7 & 84.7 & 68.3\\
    image features + Llama2 inferred goals & 0.728 & 0.747 & \textbf{40.1} & 58.1 & \textbf{32.1} & 80.0 & 84.6 & 70.0 \\
    image features + GPT-3.5 inferred goals &\textbf{0.724} & \textbf{0.744} & \textbf{40.1} & 58.8 & 31.9 & \textbf{80.2} & \textbf{84.8} & \textbf{72.9} \\
    \midrule
    image features + oracle goals\textsuperscript{$\ast$} & - & - & 40.9 & 58.7 & 32.9 & 81.6 & 86.8 & 69.3 \\
    \bottomrule
    \end{tabular}
\vspace{0.08in}
\caption{\textbf{Impact of goal conditioning on LTA performance.} Goal-conditioned (top-down) models outperforms the bottom-up model in all three datasets. We report edit distance for Ego4D, mAP for EK-55 and EGTEA. All results are reported on the validation set.} 
\label{tab:trend}
\vspace{-0.2in}
\end{table}

\subsection{Do LLMs Model Temporal Dynamics?}

\noindent
\begin{minipage}{0.55\textwidth}
  \centering
        \scalebox{0.82}{
            \begin{tabular}{lcccccc}
            \toprule
            Model &Goal &Input &Verb $\downarrow$ & Noun $\downarrow$ \\ 
            \midrule
            Transformer & GPT-3.5 &image features &0.724 &0.744 \\
            GPT-3-curie & GPT-3.5 & recog actions &\textbf{0.709} &\textbf{0.729} \\
            Transformer & Llama2-13B &image features &0.728 &0.747 \\
            Llama2-7B & Llama2-13B & recog actions &\textbf{0.700} &\textbf{0.717} \\
            \bottomrule
            \end{tabular}
        }
    \vspace{-0.07in}
    \captionsetup[table]{font={small}}
    \captionof{table}{\textbf{Comparison of temporal models for top-down LTA.} Results on Ego4D v1 val set. }%Goals are generated by Llama2-Chat-13B.}
    \label{tab:temporal}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    %\hspace{0.4in}
  \centering
    \scalebox{0.82}{
        \begin{tabular}{cccccc}
        \toprule
        Model &Goal &Verb $\downarrow$ & Noun $\downarrow$ \\ 
        \midrule
        GPT-3-curie &No &\textbf{0.707} &\textbf{0.719} \\
        GPT-3-curie &Yes &0.709 &0.729 \\
        Llama2-7B &No &0.704 &\textbf{0.705}\\
        Llama2-7B &Yes &\textbf{0.700} &0.717 \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.07in}
    \captionsetup[table]{font={small},width=0.8\textwidth}
    %\captionsetup[table]{font={small}}
    \captionof{table}{\textbf{Top-down vs Bottom-up for LLM-based LTA.} Results on v1 val set.}
    \label{tab:llm-top}
\end{minipage}
\vspace{0.05in}

We further explore if LLMs can be directly applied to model temporal dynamics. We focus on the Ego4D benchmark as it measures the ordering of the anticipated actions.

\noindent\textbf{LLMs are able to model temporal dynamics.} 
To utilize an LLM to predict future actions, we adopt the same video representation as used for in-context goal inference but fine-tune the LLM on the training set. For bottom-up LTA, we by default perform teacher forcing during training, and concatenate the $N_\text{seg}$ ground-truth action labels as the input sequence. $Z$ ground-truth action labels are concatenated as the target sequence. During evaluation, we concatenate $N_\text{seg}$ recognized actions as input, and postprocess the output sequence into $Z$ anticipated actions. For top-down LTA, we prepend the inferred goals to the input sequence.

We conduct top-down LTA with the open-sourced Llama2-7B LLM. During training, we adopt parameter-efficient fine-tuning (PEFT) with LoRA~\cite{hu2021lora} and 8-bit quantization. We compare with the transformer baseline with image features, and report results on Ego4D v1 validation set in Table~\ref{tab:temporal}. We observe that leveraging the LLM as the temporal dynamics model leads to significant improvement, especially for nouns. Additionally, we validate that simply adding more layers (and hence increasing the model size) does not improve the performance of the image feature baseline (see Table~\ref{tab:layers} in ablation study), confirming that the improvement comes from the action representation and better temporal dynamics modeling. The results demonstrate the effectiveness of action-based representation, when an LLM is used for temporal dynamics modeling.

\noindent\textbf{LLMs can perform few-shot temporal modeling.} We further tested LLMs' ability to model temporal dynamics when only shown a few examples. We consider both in-context learning (ICL) and chain-of-thoughts (CoT) and compare them with a transformer model trained from-scratch with the same examples. More detailed experiment settings are in Section~\ref{sec:fewshot} and the results are illustrated in Table\ref{tab:icl} in appendix. We observed that LLMs can model temporal dynamics competitively in a few-shot setting. As expected, chain-of-thoughts outperforms regular in-context learning, but both significantly outperform fine-tuning the Transformer model. 

\noindent\textbf{LLM-based temporal model performs \textit{implicit} goal inference.} We have shown that LLMs can assist LTA by providing the inferred goals, and serving as the temporal dynamics model, respectively. Does combining the two lead to further improved performance? Table~\ref{tab:llm-top} aims to answer this question. We report results with fine-tuned Llama2-7B and GPT-3-curie as the temporal model, which use Llama2-Chat-13B and GPT-3.5 Turbo for goal inference, respectively. We empirically observe that the bigger models lead to better inferred goals, while the smaller models are sufficient for temporal modeling. 
We observe that the bottom-up performance without explicitly inferred goals are on par (marginally better) with the top-down models for both LLMs. This indicates the LLM may implicitly inferred the goals when asked to predict the future actions, and performing explicit goal inference is not necessary. In the following experiments, we stick with this \textit{implicit} goal inference setup.

\begin{minipage}{.45\textwidth}
  \centering
        \scalebox{0.9}{
        % \setlength{\tabcolsep}{1pt}
        \begin{tabular}{ccccc}
        \toprule
        Seq Type & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
        \midrule
        Action Labels & \textbf{0.6794} & \textbf{0.6757} & \textbf{0.8912} \\
        Shuffled Labels &0.6993 &0.6972 &0.9040 \\
        Label Indices &0.7249 & 0.6805 & 0.9070 \\
        \bottomrule
        \end{tabular}
        }
        \captionsetup[table]{font={small}}
        \captionof{table}{\textbf{Benefit of language prior.} Results on Ego4D v2 test set. We replace original action sequences to semantically nonsensical sequences.}
        \label{tab:lang_prior}
\end{minipage}
\hfill
\begin{minipage}{0.53\textwidth}
    \centering
        \scalebox{0.9}{
        % \setlength{\tabcolsep}{1pt}
            \begin{tabular}{cccccc}
            \toprule
            Model & Setting & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
            \midrule
            7B &Pre-trained &0.6794 & 0.6757& 0.8912\\
            91M &From-scratch &0.7176 &0.7191 &0.9117  \\
            %91M &Yes &\textbf{0.6684} &0.6795 &0.8924  \\
            91M &Distilled &\textbf{0.6649} &\textbf{0.6752} &\textbf{0.8826}  \\
            \bottomrule
            \end{tabular}
        }
    \captionsetup[table]{font={small}}
    \captionof{table}{\textbf{LLM as temporal model.} Results on Ego4D v2 test set. Llama2-7B model is fine-tuned on Ego4D v2 training set. 91M models are randomly initialized.}
    \label{tab:distill}
\end{minipage}
\vspace{0.1in}

\noindent\textbf{Language prior encoded by LLMs benefit LTA.} We further investigate if the \textit{language} (e.g. goals and action labels) used for our video representation is actually helpful to utilize the language priors encoded by the LLMs. We first conduct experiments by replacing the action label representation with two representations that we assume the pretrained LLMs are unfamiliar with: (1) \textbf{Shuffled Labels.} We randomly generate a mapping of verbs and nouns so that the original verbs/nouns are 1-to-1 projected to randomly sampled words in the dictionary to construct semantically nonsensical language sequence (e.g ``open window” to ``eat monitor”). (2) \textbf{Label Indices.} Instead of using words to represent actions in the format of verb-noun pairs, we can also use the index of the verb/noun in the dictionary to map the words to digits to form the input and output action sequence.

We fine-tune the Llama2-7B model on the three types of action representations on the Ego4D v2 dataset and report results on the test set. As shown in Table~\ref{tab:lang_prior}, the performance drops severely when shuffled action labels or label indices are used, especially for verb. The performance gap indicates that even LLMs have strong capability to model patterns beyond natural language~\cite{mirchandani2023large}, the encoded language prior from large-scale pre-training still significantly benefits long-term video action anticipation.

\noindent\textbf{LLM-encoded knowledge can be condensed into a compact model.} We first introduce the baseline model Llama2-91M, which is a 6-layer randomly initialized transformer decoder model with the similar structure as Llama2-7B. The 91M model takes in the same input during training and evaluation and follows the same post-processing. 
We then conduct model distillation to use the Llama2-7B model tuned on Ego4D v2 training set as the teacher model and the same randomly initialized Llama2-91M as the student model. Results on test set are shown in Table~\ref{tab:distill}. We observe that the distilled model achieves significant improvement comparing with model trained without distillation in the second row (7.3\% and 6.1\% for verb and noun). It's also worth noting that the distilled 91M model even outperforms the 7B teacher model on all three metrics, while using 1.3\% of the model size. The results confirm that LLM-encoded knowledge on \textit{implicit} goal inference and \textit{explicit} temporal modeling can be condensed into a compact neural network.

\begin{table}[ht]
%\vspace{-0.3in}
\centering
\scalebox{0.95}{
% \setlength{\tabcolsep}{1pt}
    \begin{tabular}{ccccc}
    \toprule
    Method & Version & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
    \midrule
    HierVL~\cite{hierVL} & v1 & 0.7239 & 0.7350 & 0.9276 \\
    ICVAE\cite{icvae} & v1 & 0.7410 & 0.7396 & 0.9304 \\
    VCLIP ~\cite{das2022video+} & v1 & 0.7389 & 0.7688 & 0.9412 \\
    Slowfast ~\cite{ego4d} & v1 & 0.7389 & 0.7800 & 0.9432 \\
    \textbf{AntGPT} (ours) & v1 & \textbf{0.6584}\scriptsize{$\pm$7.9e-3} & \textbf{0.6546}\scriptsize{$\pm$3.8e-3} & \textbf{0.8814}\scriptsize{$\pm$3.1e-3} \\

    \midrule
    Slowfast~\cite{ego4d} & v2 & 0.7169	& 0.7359 & 0.9253 \\
    VideoLLM~\cite{chen2023videollm} & v2 & 0.721 & 0.725 & 0.921\\
    PaMsEgoAI~\cite{ishibashi2023technical} &v2 &0.6838 &0.6785 &0.8933 \\
    Palm~\cite{huang2023palm} & v2 &0.6956 &0.6506 &0.8856 \\
    \textbf{AntGPT} (ours) & v2 & \textbf{0.6503}\scriptsize{$\pm$3.6e-3} & \textbf{0.6498}\scriptsize{$\pm$3.4e-3} & \textbf{0.8770}\scriptsize{$\pm$1.2e-3} \\
    \bottomrule
    \end{tabular}
}
\vspace{0.08in}
\caption{\textbf{Comparison with SOTA methods on the Ego4D v1 and v2 test sets in ED@20.} Ego4d v1 and v2 share the same test set. V2 contains more training and validation examples than v1.
}
\vspace{-0.2in}
\label{tab:sota_ego4d}
\end{table}

\subsection{Comparison With State-of-the-art}
Finally, we compare AntGPT with the previous state-of-the-art methods. We choose the model design settings such as recognition models and input segments number based on ablation study discussed in appendix Section~\ref{sec:ablation}. For Ego4d v1 and v2, we train the action recognition and fine-tune the LLM temporal models with their corresponding training set. Table~\ref{tab:sota_ego4d} shows performance comparisons on Ego4D v1 and v2 benchmarks. We observe that AntGPT achieves best performance on both datasets and largely outperforms other SOTA baselines. Since Ego4d v1 and v2 share the same test set, it is also worth mentioning that our model trained solely on v1 data is able to outperform any other models trained on the v2 data, which indicates the data efficiency and the promise of our approach.

For EK-55 and EGTEA, we compare the goal-conditioned AntGPT with the previous state-of-the-art results in Table~\ref{tab:sota_gaze_ek}. AntGPT achieves the overall best performance on both datasets. We observe that our proposed model performs particularly well on rare actions.

\begin{table}
\centering
% \setlength{\tabcolsep}{1pt}
\scalebox{0.95}{
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{EK-55}} & \multicolumn{3}{c}{\textbf{EGTEA}} \\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    & ALL & FREQ & RARE & ALL & FREQ & RARE \\
    \midrule
    I3D~\cite{i3d_cvpr17} & 32.7 & 53.3 & 23.0 & 72.1 & 79.3 & 53.3 \\
    ActionVLAD~\cite{girdhar2017actionvlad} & 29.8 & 53.5 & 18.6 & 73.3 & 79.0 & 58.6\\
    Timeception~\cite{hussein2019timeception} & 35.6 & 55.9 & 26.1 & 74.1 & 79.7 & 59.7 \\
    VideoGraph~\cite{hussein2019videograph} & 22.5 & 49.4 & 14.0 & 67.7 & 77.1 & 47.2 \\
    EGO-TOPO~\cite{ego-topo} & 38.0 & 56.9 & 29.2 & 73.5 & 80.7 & 54.7 \\
    Anticipatr~\cite{anticipator} & 39.1 & 58.1 & 29.1 & 76.8 &  83.3 & 55.1 \\
    \textbf{AntGPT} (ours) & \textbf{40.1}\scriptsize{$\pm$2e-2} & \textbf{58.8}\scriptsize{$\pm$2e-1} & \textbf{31.9}\scriptsize{$\pm$5e-2} & \textbf{80.2}\scriptsize{$\pm$2e-1} & \textbf{84.8}\scriptsize{$\pm$2e-1} & \textbf{72.9}\scriptsize{$\pm$1.2} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{0.08in}
\caption{\textbf{Comparison with SOTA methods on the EK-55 and EGTEA Dataset in mAP.} ALL, FREQ and RARE represent the performances on all, frequent, and rare target actions respectively.}
\label{tab:sota_gaze_ek}
\end{table}
