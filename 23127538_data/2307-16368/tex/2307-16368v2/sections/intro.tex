\section{Introduction}
Our work addresses the long-term action anticipation (LTA) task from video observations. Its desired outputs are sequences of \textit{verb} and \textit{noun} predictions over a typically long-term time horizon for an actor of interest. LTA is a crucial component for human-machine interaction. A machine agent could leverage LTA to assist humans in scenarios such as daily household tasks~\cite{ego-topo} and autonomous driving~\cite{gao2020vectornet}. The LTA task is challenging due to noisy perception (e.g. action recognition), and the inherent ambiguity and uncertainty that reside in human behaviors.

A common approach for LTA is \textit{bottom-up}, which directly models the temporal dynamics of human behavior either in terms of the discrete action labels~\cite{sun2019relational}, or the latent visual representations~\cite{vondrick2016anticipating}. 
Meanwhile, human behaviors, especially in daily household scenarios, are often ``purposive''~\cite{kruglanski2020habitual}, and knowing an actor's longer-term goal can potentially help action anticipation~\cite{tran2021goal}. As such, we consider an alternative \textit{top-down} framework: It first explicitly infers the longer-term goal of the human actor, and then \textit{plans} the procedure needed to accomplish the goal. However, the goal information is often left unlabeled and thus latent in existing LTA benchmarks, making it infeasible to directly apply goal-conditioned procedure planning for action anticipation.

Our paper seeks to address these challenges in modeling long-term temporal dynamics of human behaviors. Our research is inspired by prior work on the mental representations of tasks as \textit{action grammars}~\cite{payne1986task,pastra2012minimalist} in cognitive science, and by large language models' (LLMs) empirical success on procedure planning~\cite{ahn2022can,driess2023palm}. We hypothesize that the LLMs, which use procedure text data during pretraining, encode useful prior knowledge for the long-term action anticipation task. Ideally, the prior knowledge can help both bottom-up and top-down LTA approaches, as they can not only answer questions such as ``what are the most likely actions following this current action?'', but also ``what is the actor trying to achieve, and what are the remaining steps to achieve the goal?''

Concretely, our paper strives to answer four research questions on modeling human behaviors for long-term action anticipation: (1) Does top-down (i.e. goal-conditioned) LTA outperform the bottom-up approach? (2) Can LLMs infer the goals useful for top-down LTA, with minimal additional supervision? (3) Do LLMs capture prior knowledge useful for modeling the temporal dynamics of human actions? If so, what would be a good interface between the videos and an LLM? And (4) Can we condense LLMs' prior knowledge into a compact neural network for efficient inference? 

To perform quantitative and qualitative evaluations necessary to answer these questions, we propose \textbf{AntGPT}, which constructs an action-based video representation, and leverages an LLM to perform goal inference and model the temporal dynamics. We conduct experiments on multiple LTA benchmarks, including Ego4D~\cite{ego4d}, EPIC-Kitchens-55~\cite{Epic-Kitchen}, and EGTEA GAZE+~\cite{li2018eye}. Our evaluations reveal the following observations to answer the research questions: First, we find that our video representation, based on sequences of noisy action labels from action recognition algorithms, serves as an effective interface for an LLM to infer longer-term goals, both qualitatively from visualization, and quantitatively as the goals enable a top-down LTA pipeline to outperform its bottom-up counterpart. The goal inference is achieved via in-context learning~\cite{gpt3}, which requires few human-provided examples of action sequence and goal pairs. Second, we observe that the same video representation allows effective temporal dynamics modeling with an LLM, by formulating LTA as (action) sequence completion. Interestingly, we observe that the LLM-based temporal dynamics model appears to perform implicit goal-conditioned LTA, and achieves competitive performance without relying on explicitly inferred goals. These observations enable us to answer the final research question by distilling the bottom-up LLM to a compact student model 1.3\% of the original model size, while achieving similar or even better LTA performance.

To summarize, our paper makes the following contributions: 

1. We propose to investigate if large language models encode useful prior knowledge on modeling the temporal dynamics of human behaviors, in the context of bottom-up and top-down action anticipation.

2. We propose the \textbf{AntGPT} framework, which naturally bridges the LLMs with computer vision algorithms for video understanding, and achieves state-of-the-art long-term action anticipation performance on the Ego4D LTA v1 and v2 benchmarks, EPIC-Kitchens-55, and EGTEA GAZE+.

3. We perform thorough experiments with two LLM variants and demonstrate that LLMs are indeed helpful for both goal inference and temporal dynamics modeling. We further demonstrate that the useful prior knowledge encoded by LLMs can be distilled into a very compact neural network (1.3\% of the original LLM model size), which enables efficient inference.