\begin{abstract}

Can we better anticipate an actor's future actions (e.g. \textit{mix eggs}) by knowing what commonly happens after the current action (e.g. \textit{crack eggs})? What if the actor also shares the goal (e.g. \textit{make fried rice}) with us? The long-term action anticipation (LTA) task aims to predict an actor's future behavior from video observations in the form of verb and noun sequences, and it is crucial for human-machine interaction. We propose to formulate the LTA task from two perspectives: a \textit{bottom-up} approach that predicts the next actions autoregressively by modeling temporal dynamics; and a \textit{top-down} approach that infers the goal of the actor and \textit{plans} the needed procedure to accomplish the goal. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos), have the potential to help LTA from both perspectives. It can help provide the prior knowledge on the possible next actions, and infer the goal given the observed part of a procedure, respectively. We propose \textbf{AntGPT}, which represents video observations as sequences of human actions, and uses the action representation for an LLM to infer the goals and model temporal dynamics.
\textbf{AntGPT} achieves state-of-the-art performance on Ego4D LTA v1 and v2, EPIC-Kitchens-55, as well as EGTEA GAZE+, thanks to LLMs' goal inference and temporal dynamics modeling capabilities. We further demonstrate that these capabilities can be effectively distilled into a compact neural network 1.3\% of the original LLM model size. Code and model will be released at \href{https://brown-palm.github.io/AntGPT}{brown-palm.github.io/AntGPT}.

\end{abstract}


