\section{Related Work}
\noindent\textbf{Action anticipation} 
can be mainly categorized into next action prediction (NAP)~\cite{Epic-Kitchen, li2018eye} and long-term anticipation (LTA)~\cite{ego4d}. Our work focuses on the LTA task, where modeling the (latent) goals of the actors is intuitively helpful. 
Most prior works on action anticipation aim at modeling the temporal dynamics directly from visual cues, such as by utilizing hierarchical representations~\cite{lan2014hierarchical}, modeling the temporal dynamics of discrete action labels~\cite{sun2019relational}, predicting future latent representations~\cite{vondrick2016anticipating,gammulle2019predicting}, or jointly predicting future labels and features~\cite{girdhar2021anticipative, girase2023latency}. As the duration of each action is unknown, some prior work proposed to discover object state changes~\cite{epstein2021learning,souvcek2022look} as a proxy task for action anticipation. The temporal dynamics of labels or latent representations are modeled by neural networks, and are often jointly trained with the visual observation encoder in an end-to-end fashion. To predict longer sequences into the future for LTA, existing work either build autoregressive generative models~\cite{abu2018will, gammulle2019forecasting, sener2020temporal, farha2020long} or use timestep as a conditional parameter and predict in one shot based on provided timestep~\cite{ke2019time}. We consider these approaches as bottom-up as they model the shorter-term temporal transitions of human activities. 

\noindent\textbf{Visual procedure planning} is closely related to long-term action anticipation, but assumes that both source state and the goal state are explicitly specified. For example, \cite{chang2020procedure} proposed to learn both forward and conjugate dynamics models in the latent space, and plans the actions to take accordingly. 
Procedure planning algorithms can be trained and evaluated with video observations~\cite{chang2020procedure,bi2021procedure,sun2022plate,zhao2022p3iv,narasimhan2023learning,bansal2022my}, they can also be applied to visual navigation and object manipulation~\cite{driess2023palm,ahn2022can,lu2022neuro}. 
Unlike procedure planning, our top-down LTA approach does not assume access to the goal information. Our explicit inference of the high-level goals (with LLMs) also differs from prior attempts to model the goal as a latent variable, which is optimized via weakly-supervised learning~\cite{roy2022predicting,icvae}.

\noindent\textbf{Multimodal learning}, such as joint vision and language modeling, have also been applied to the action anticipation tasks. One approach is to treat the action labels as the language modality, and to ``distill'' the text-derived knowledge into vision-based models. For example, ~\cite{camporese2021knowledge} models label semantics with hand-engineered label prior based on statistics information from the training action labels. ~\cite{ghosh2023text} trains a teacher model with text input from the training set and distills the text-derived knowledge to a vision-based student model. ~\cite{sener2019zero} transfers knowledge from a text-to-text encoder-decoder by projecting vision and language representations in a shared space. Compared to these prior work, our focus is on investigating the benefits of large language models for modeling the temporal dynamics of human activities. 
