\begin{thebibliography}{10}

\bibitem{abu2018will}
Yazan Abu~Farha, Alexander Richard, and Juergen Gall.
\newblock When will you do what?-anticipating temporal occurrences of activities.
\newblock In {\em CVPR}, 2018.

\bibitem{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock {\em arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{hierVL}
Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and Kristen Grauman.
\newblock Hiervl: Learning hierarchical video-language embeddings.
\newblock {\em arXiv preprint arXiv:2301.02311}, 2023.

\bibitem{bansal2022my}
Siddhant Bansal, Chetan Arora, and CV~Jawahar.
\newblock My view is the best view: Procedure learning from egocentric videos.
\newblock In {\em ECCV}, 2022.

\bibitem{bi2021procedure}
Jing Bi, Jiebo Luo, and Chenliang Xu.
\newblock Procedure planning in instructional videos via contextual modeling and model-based policy learning.
\newblock In {\em ICCV}, 2021.

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, 2020.

\bibitem{camporese2021knowledge}
Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni~Maria Farinella, and Lamberto Ballan.
\newblock Knowledge distillation for action anticipation via label smoothing.
\newblock In {\em ICPR}, 2021.

\bibitem{i3d_cvpr17}
J.~Carreira and A.~Zisserman.
\newblock Quo vadis, action recognition? {A} new model and the {K}inetics dataset.
\newblock In {\em CVPR}, 2017.

\bibitem{chang2020procedure}
Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li~Fei-Fei, and Juan~Carlos Niebles.
\newblock Procedure planning in instructional videos.
\newblock In {\em ECCV}, 2020.

\bibitem{chen2023videollm}
Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi~Wang, Yali Wang, Yu~Qiao, Tong Lu, et~al.
\newblock Videollm: Modeling video sequence with large language models.
\newblock {\em arXiv preprint arXiv:2305.13292}, 2023.

\bibitem{Epic-Kitchen}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et~al.
\newblock The epic-kitchens dataset: Collection, challenges and baselines.
\newblock {\em TPAMI}, 2020.

\bibitem{das2022video+}
Srijan Das and Michael~S Ryoo.
\newblock Video+ clip baseline for ego4d long-term action anticipation.
\newblock {\em arXiv preprint arXiv:2207.00579}, 2022.

\bibitem{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{epstein2021learning}
Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun.
\newblock Learning temporal dynamics from cycles in narrated video.
\newblock {\em arXiv preprint arXiv:2101.02337}, 2021.

\bibitem{farha2020long}
Yazan~Abu Farha, Qiuhong Ke, Bernt Schiele, and Juergen Gall.
\newblock Long-term anticipation of activities with cycle consistency.
\newblock {\em arXiv preprint arXiv:2009.01142}, 2020.

\bibitem{gammulle2019forecasting}
Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.
\newblock Forecasting future action sequences with neural memory networks.
\newblock {\em arXiv preprint arXiv:1909.09278}, 2019.

\bibitem{gammulle2019predicting}
Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.
\newblock Predicting the future: A jointly learnt model for action anticipation.
\newblock In {\em ICCV}, 2019.

\bibitem{gao2020vectornet}
Jiyang Gao, Chen Sun, Hang Zhao, Yi~Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid.
\newblock Vectornet: Encoding hd maps and agent dynamics from vectorized representation.
\newblock In {\em ICCV}, 2020.

\bibitem{ghosh2023text}
Sayontan Ghosh, Tanvi Aggarwal, Minh Hoai, and Niranjan Balasubramanian.
\newblock Text-derived knowledge helps vision: A simple cross-modal distillation for video-based action anticipation.
\newblock In {\em EACL}, 2023.

\bibitem{girase2023latency}
Harshayu Girase, Nakul Agarwal, Chiho Choi, and Karttikeya Mangalam.
\newblock Latency matters: Real-time action forecasting transformer.
\newblock In {\em CVPR}, 2023.

\bibitem{girdhar2021anticipative}
Rohit Girdhar and Kristen Grauman.
\newblock Anticipative video transformer.
\newblock In {\em ICCV}, 2021.

\bibitem{girdhar2017actionvlad}
Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, and Bryan Russell.
\newblock Actionvlad: Learning spatio-temporal aggregation for action classification.
\newblock {\em arXiv preprint arXiv:1704.02895}, 2017.

\bibitem{ego4d}
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh~Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric~Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola~Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni~Maria
  Farinella, Christian Fuegen, Bernard Ghanem, Vamsi~Krishna Ithapu, C.~V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun~Soo Park, James~M. Rehg, Yoichi Sato, Jianbo Shi, Mike~Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video, 2021.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{huang2023palm}
Daoji Huang, Otmar Hilliges, Luc Van~Gool, and Xi~Wang.
\newblock Palm: Predicting actions through language models@ ego4d long-term action anticipation challenge 2023.
\newblock {\em arXiv preprint arXiv:2306.16545}, 2023.

\bibitem{hussein2019timeception}
Noureldien Hussein, Efstratios Gavves, and Arnold W.~M. Smeulders.
\newblock Timeception for complex action recognition.
\newblock {\em arXiv preprint arXiv:1812.01289}, 2019.

\bibitem{hussein2019videograph}
Noureldien Hussein, Efstratios Gavves, and Arnold W.~M. Smeulders.
\newblock Videograph: Recognizing minutes-long human activities in videos.
\newblock {\em arXiv preprint arXiv:1905.05143}, 2019.

\bibitem{ishibashi2023technical}
Tatsuya Ishibashi, Kosuke Ono, Noriyuki Kugo, and Yuji Sato.
\newblock Technical report for ego4d long term action anticipation challenge 2023.
\newblock {\em arXiv preprint arXiv:2307.01467}, 2023.

\bibitem{ke2019time}
Qiuhong Ke, Mario Fritz, and Bernt Schiele.
\newblock Time-conditioned action anticipation in one shot.
\newblock In {\em CVPR}, 2019.

\bibitem{kruglanski2020habitual}
Arie~W Kruglanski and Ewa Szumowska.
\newblock Habitual behavior is goal-driven.
\newblock {\em Perspectives on Psychological Science}, 15(5):1256--1271, 2020.

\bibitem{lan2014hierarchical}
Tian Lan, Tsung-Chuan Chen, and Silvio Savarese.
\newblock A hierarchical representation for future action prediction.
\newblock In {\em ECCV}, 2014.

\bibitem{li2018eye}
Yin Li, Miao Liu, and James~M Rehg.
\newblock In the eye of beholder: Joint learning of gaze and actions in first person video.
\newblock In {\em ECCV}, 2018.

\bibitem{lu2022neuro}
Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin~Eric Wang, Miguel Eckstein, and William~Yang Wang.
\newblock Neuro-symbolic procedural planning with commonsense prompting.
\newblock {\em arXiv preprint arXiv:2206.02928}, 2022.

\bibitem{icvae}
Esteve~Valls Mascaro, Hyemin Ahn, and Dongheui Lee.
\newblock Intention-conditioned long-term human egocentric action forecasting @ ego4d challenge 2022.

\bibitem{merullo2022linearly}
Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick.
\newblock Linearly mapping from image to text space.
\newblock {\em arXiv preprint arXiv:2209.15162}, 2022.

\bibitem{mirchandani2023large}
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat~Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng.
\newblock Large language models as general pattern machines.
\newblock {\em arXiv preprint arXiv:2307.04721}, 2023.

\bibitem{ego-topo}
Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, and Kristen Grauman.
\newblock Ego-topo: Environment affordances from egocentric video.
\newblock In {\em CVPR}, 2020.

\bibitem{narasimhan2023learning}
Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, and Trevor Darrell.
\newblock Learning and verification of task structure in instructional videos.
\newblock {\em arXiv preprint arXiv:2303.13519}, 2023.

\bibitem{anticipator}
Megha Nawhal, Akash~Abdu Jyothi, and Greg Mori.
\newblock Rethinking learning approaches for long-term action anticipation.
\newblock In {\em ECCV}, 2022.

\bibitem{pastra2012minimalist}
Katerina Pastra and Yiannis Aloimonos.
\newblock The minimalist grammar of action.
\newblock {\em Philosophical Transactions of the Royal Society B: Biological Sciences}, 367(1585):103--117, 2012.

\bibitem{payne1986task}
Stephen~J Payne and Thomas~RG Green.
\newblock Task-action grammars: A model of the mental representation of task languages.
\newblock {\em Human-computer interaction}, 2(2):93--133, 1986.

\bibitem{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language supervision.
\newblock {\em arXiv preprint arXiv:2103.00020}, 2021.

\bibitem{roy2022predicting}
Debaditya Roy and Basura Fernando.
\newblock Predicting the next action by modeling the abstract goal.
\newblock {\em arXiv preprint arXiv:2209.05044}, 2022.

\bibitem{sener2020temporal}
Fadime Sener, Dipika Singhania, and Angela Yao.
\newblock Temporal aggregate representations for long-range video understanding.
\newblock In {\em ECCV}, 2020.

\bibitem{sener2019zero}
Fadime Sener and Angela Yao.
\newblock Zero-shot anticipation for instructional activities.
\newblock In {\em ICCV}, 2019.

\bibitem{souvcek2022look}
Tom{\'a}{\v{s}} Sou{\v{c}}ek, Jean-Baptiste Alayrac, Antoine Miech, Ivan Laptev, and Josef Sivic.
\newblock Look for the change: Learning object states and state-modifying actions from untrimmed web videos.
\newblock In {\em CVPR}, 2022.

\bibitem{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: A joint model for video and language representation learning.
\newblock In {\em ICCV}, 2019.

\bibitem{sun2019relational}
Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Sukthankar, Kevin Murphy, and Cordelia Schmid.
\newblock Relational action forecasting.
\newblock In {\em CVPR}, 2019.

\bibitem{sun2022plate}
Jiankai Sun, De-An Huang, Bo~Lu, Yun-Hui Liu, Bolei Zhou, and Animesh Garg.
\newblock Plate: Visually-grounded planning with transformers in procedural tasks.
\newblock {\em IEEE Robotics and Automation Letters}, 7(2):4924--4930, 2022.

\bibitem{suris2023vipergpt}
D{\'\i}dac Sur{\'\i}s, Sachit Menon, and Carl Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock {\em arXiv preprint arXiv:2303.08128}, 2023.

\bibitem{tran2021goal}
Hung Tran, Vuong Le, and Truyen Tran.
\newblock Goal-driven long-term trajectory prediction.
\newblock In {\em WACV}, 2021.

\bibitem{vondrick2016anticipating}
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
\newblock Anticipating visual representations from unlabeled video.
\newblock In {\em CVPR}, 2016.

\bibitem{wang2022bevt}
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu~Yuan.
\newblock Bevt: Bert pretraining of video transformers.
\newblock In {\em CVPR}, 2022.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock {\em arXiv preprint arXiv:2201.11903}, 2022.

\bibitem{zeng2022socratic}
Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.
\newblock Socratic models: Composing zero-shot multimodal reasoning with language.
\newblock {\em arXiv preprint arXiv:2204.00598}, 2022.

\bibitem{zhao2022p3iv}
He~Zhao, Isma Hadji, Nikita Dvornik, Konstantinos~G Derpanis, Richard~P Wildes, and Allan~D Jepson.
\newblock P3iv: Probabilistic procedure planning from instructional videos with weak supervision.
\newblock In {\em CVPR}, pages 2938--2948, 2022.

\end{thebibliography}
