\section{Related Work}
\vspace{-0.1in}
% Figure environment removed

\noindent\textbf{Action anticipation} algorithms are crucial for building intelligent agents, as they provide important signals for planning in interactive environments. The action anticipation task can be categorized into next action prediction (NAP)~\cite{Epic-Kitchen, li2018eye} and long-term anticipation (LTA)~\cite{ego4d}. Our work mainly focuses on the LTA task, where modeling the (latent) goals of the actors are intuitively helpful. 


Most prior works on action anticipation aim at modeling the temporal dynamics directly from visual cues, such as by utilizing hierarchical representations~\cite{lan2014hierarchical}, modeling the temporal dynamics of discrete action labels~\cite{sun2019relational}, predicting future latent representations~\cite{vondrick2016anticipating,gammulle2019predicting}, or jointly predicting future labels and features~\cite{girdhar2021anticipative, girase2023latency}. As the duration of each action is unknown, some prior work proposed to discover object state changes~\cite{epstein2021learning,souvcek2022look} as a proxy task for action anticipation. The temporal dynamics of labels or latent representations are modeled by neural networks, and are often jointly trained with the visual observation encoder in an end-to-end fashion. To predict longer sequences into the future for LTA, existing work either build autoregressive generative models~\cite{abu2018will, gammulle2019forecasting, sener2020temporal, farha2020long} or use timestep as a conditional parameter and predict in one shot based on provided timestep~\cite{ke2019time}. We consider these approaches as bottom-up as they model the shorter-term temporal transitions of human activities. 


\noindent\textbf{Visual procedure planning} is an area of research closely related to long-term action anticipation, and also has important applications in robotics. A main difference is that procedure planning assumes that both source state and the goal state are specified in the form of images, while LTA does not provide the goal state as input. For example, Chang et al.~\cite{chang2020procedure} proposed to learn both forward and conjugate dynamics models in the latent space, and to generate states and actions necessary to reach the goal state from the source. Intuitively, knowing the goal state allows ``planning'' for the future, and can thus reduce the ambiguity in LTA. Existing procedure planning benchmarks include cooking domain~~\cite{youcook2}, daily household tasks~\cite{zhukov2019cross,Tang2019}, and even toy assembly~\cite{sener2022assembly101}.

Procedure planning algorithms can be trained and evaluated with video observations~\cite{chang2020procedure,bi2021procedure,sun2022plate,zhao2022p3iv,narasimhan2023learning,bansal2022my,lin2022learning,batra2022closer}, they can also be applied to embodied agents for visual navigation and object manipulation~\cite{driess2023palm,ahn2022can,lu2022neuro}. Our approach is particularly inspired by the recent line of approach that leverages prior knowledge encoded by large language models for procedure planning in robots~\cite{driess2023palm,ahn2022can}.

Unlike procedure planning, our top-down approach of LTA does not have the goal information as inputs. Prior approaches attempt to model the goal as a latent variant and perform weakly-supervised learning based on next action prediction~\cite{roy2022predicting} and LTA~\cite{icvae} supervision. We take an alternative approach and propose to leverage the goal inference and planning capabilities of LLMs.












\noindent\textbf{Multimodal learning}, such as joint vision and language modeling, have also been applied to the action anticipation tasks. One approach is to treat the action labels as the language modality, and to ``distill'' the text-derived knowledge into vision-based models. For example, Camporese et al.~\cite{camporese2021knowledge} model label semantics with hand-engineered label prior based on statistics information from the training action labels. Ghosh et al.~\cite{ghosh2023text} train a teacher model with text input from training set and distill the text-derived knowledge to a vision-based student model. Sener et al.~\cite{sener2019zero} transfer knowledge from a text-to-text encoder-decoder by projecting vision and language representations in a shared space and replacing the text encoder with a video encoder and generate future action in the form of natural language. Compared to these prior work, our focus is on investigating the benefits of large language models for capturing autoregressive action priors (for bottom-up LTA), and for jointly inferring the goals and performing procedure planning (for top-down LTA). Our proposed framework decouples the vision module and the temporal dynamics module (in language), which allows us to explore the recent advances of LLMs, such as in-context learning and chain-of-thought prompting.
