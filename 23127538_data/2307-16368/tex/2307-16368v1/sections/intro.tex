\section{Introduction}







Our work addresses the long-term action anticipation (LTA) task from video observations. Its desired outputs are sequences of \textit{verb} and \textit{noun} predictions over a typically long-term time horizon for an actor of interest. LTA is a crucial component for human-machine interaction. A machine agent could leverage LTA to assist humans in scenarios such as daily household tasks~\cite{ego-topo} and autonomous driving~\cite{gao2020vectornet}. In addition, it is extremely challenging even with perfect perception for video action recognition, due to the inherent ambiguity and uncertainty that reside in human behaviors.

A common approach for LTA is \textit{bottom-up}, which directly models the temporal dynamics of human behavior either in terms of the discrete action labels~\cite{sun2019relational}, or the latent visual representations~\cite{vondrick2016anticipating}. Most of the recent approaches for bottom-up LTA are implemented as neural networks that are end-to-end trained from visual inputs. Intuitively, human behaviors, especially in daily household scenarios, are often ``purposive''~\cite{kruglanski2020habitual}, and knowing an actor's intent can potentially help action anticipation~\cite{tran2021goal}. As such, we consider a \textit{top-down} framework in addition to the commonly adopted bottom-up approach. The top-down framework first explicitly infers the longer-term goal of the human actor, and then \textit{plans} the procedure needed to accomplish the goal. However, the goal information is often left unlabeled and thus latent in existing LTA benchmarks, making it infeasible to directly apply goal-conditioned procedure planning for action anticipation. 


Our paper seeks to address these challenges in bottom-up and top-down LTA.
Inspired by large language models' (LLMs) success on robotic planning~\cite{ahn2022can,driess2023palm}, and program-based visual question answering~\cite{gupta2022visual,suris2023vipergpt,schick2023toolformer}, we propose to investigate if they can also benefit LTA from videos. We hypothesize that the LLMs, which use procedure text data such as recipes during pretraining, encode useful prior knowledge for the long-term action anticipation task. Ideally, the prior knowledge encoded in LLMs can help both bottom-up and top-down LTA approaches, as they can not only answer questions such as ``what are the most likely actions following this current action?'', but also ``what is the actor trying to achieve, and what are the remaining steps to achieve the goal?''









Concretely, our paper strives to answer four questions on leveraging LLMs for long-term action anticipation: First, what is a good interface between videos and LLMs for the LTA task? Second, can LLMs infer the goals and are they helpful for top-down LTA? Third, do LLMs capture prior knowledge about temporal dynamics helpful for action anticipation? Finally, can we leverage LLMs' in-context learning capability to perform few-shot LTA?

To perform quantitative and qualitative evaluations necessary to answer these questions, we propose a two-stage framework, \textbf{AntGPT}, which first recognizes human actions based on supervised action recognition algorithms~\cite{feichtenhofer2019slowfast,radford2021clip}. \textbf{AntGPT} feeds the recognized actions as discretized video representation to the OpenAI GPT models in order to get the goal of such actions or the future actions, which can be optionally post-processed into the final predictions. For bottom-up LTA, we directly ask the GPT model to autoregressively predict the future action sequences, either via fine-tuning or in-context learning~\cite{gpt3}. To achieve top-down LTA, we first ask GPT to predict the goal of the actor before generating the actors' actions. Then, we feed the goal information to make goal-conditioned predictions. Additionally, we examine \textbf{AntGPT}'s capability to perform few-shot bottom-up LTA via in-context learning~\cite{gpt3}, and top-down LTA via chain-of-thought~\cite{wei2022chain}, respectively. The overall \textbf{AntGPT} framework and different methods of applying LLMs are illustrated in Figure~\ref{fig:model}.

We conduct experiments on several LTA benchmarks, including Ego4D~\cite{ego4d}, EPIC-Kitchens-55~\cite{Epic-Kitchen}, and EGTEA GAZE+~\cite{li2018eye}. The quantitative experiments show the effectiveness of our proposed \textbf{AntGPT}. Further quantitative and qualitative experiments demonstrate that LLMs are able to infer the high-level goals of the actors, given discretized action labels from the video observations. We also observe that the LLMs are able to perform counterfactual action anticipation given different goals as inputs. Our paper makes the following contributions:

1. We formulate long-term action anticipation as bottom-up and top-down approaches, and propose to leverage large language models for inferring goals and modeling temporal dynamics.

2. We propose the \textbf{AntGPT} framework, which naturally bridges the LLMs with computer vision algorithms for video understanding, and achieves state-of-the-art long-term action anticipation performance on the Ego4D LTA v1 and v2 benchmarks, EPIC-Kitchens-55, and EGTEA GAZE+. 

3. We conduct thorough quantitative and qualitative evaluations to understand the important design choices, promises and limitations of LLMs when applied to the LTA task.




