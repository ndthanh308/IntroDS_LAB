\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\section{Additional Experimental Details}
\label{sec:exp_details}

In this section, we provide additional details for the fine-tuning language model experiments on Ego4D v1 and v2, as well as goal generation via in-context learning. We also provide additional details for all the transformer models involved in experiments on all the datasets. We then present the ablation studies we conduct on fine-tuning LLMs and also the auto-regressive transformer. Finally, we describe the details of the different setups in Ego4d datasets versus EK-55 and Gaze.

\subsection{Preprocessing and Postprocessing} 
\label{sec:postprocessing}
During preprocessing for fine-tuning, we empirically observe that using a single token to represent each verb or noun helps the performance of the fine-tuned model. Since the tokenization is handled by OpenAI's API, some verbs or nouns in the Ego4D's word taxonomy may span over multiple tokens. For example, the verbs ``turn-on'' and ``turn-off'' are two-token long, and the noun ``coconut'' is broken into three tokens. We attempt to minimize the token length of each Ego4D label without losing semantic information of the labels, which are important to leverage prior knowledge in the LLMs. As a result, we choose the first unique word to describe the verb and noun labels in the Ego4D label vocabulary. This preprocessing is performed for both the input action sequences, and the target action sequences for prediction. No additional task information is provided as input during fine-tuning, as we observe that the fine-tuning training examples themselves clearly specify the task.

We observe that LLM models demonstrate strong capability on following the instructions as specified by fine-tuning or in-context learning examples. However, the output sequences generated by the LLM are sometimes invalid, and post-processing is needed before the predictions can be evaluated by the edit distance metrics. Specifically, the standard Ego4D LTA evaluation protocol requires generating 5 sequences of long-term action predictions, each in the form of 20 verb and noun pairs.

We consider the following scenarios to trigger postprocessing on the LLM's predictions: (1) the output sequence length is different than expected (i.e. 20); (2) an action in the output sequence cannot be parsed into a pair of one verb and one noun; (3) a predicted verb or noun is outside the vocabulary. Table \ref{tab:incident} shows the statistics on how often each type of incident would occur.

Instead of repeatedly sampling new responses from the LLM until it is valid, we choose to use a simple post-processing strategy: First, inspect each action in the predicted sequence, and remove the invalid ones. Second, truncate the sequence, or pad the sequence with the last predicted valid action until the sequence length is as desired. We then map the predicted words into the fixed noun and verb label space. This is achieved by retrieving the nearest neighbor of each word in our label vocabulary based on Levenshtein distance.


\subsection{Recognition Model Implementation Details.}

For the recognition model, we use frozen CLIP~\cite{radford2021clip} image encoder (ViT-L/14@336px) as vision backbone. For all the datasets, we randomly sample 4 frames from each segments. Each frame is encoded as a 768D representation. We use 3 layers, 8 heads of a vanilla Transformer Encoder with a hidden representation dimension of 2048. We use Nesterov Momentum SGD + CosineAnnealing Scheduler with learning rate 1e-3. We train the model for 40 epochs with the first 4 as warm-up epochs. All hyper-parameters are chosen by minimizing the loss on the validation set.

\subsection{Bottom-up and Top-down Models Implementation Details.}

For the bottom-up model that establishes our vision baseline, we use frozen CLIP~\cite{radford2021clip} image encoder (ViT-L/14@336px) as vision backbone. Each frame is encoded as a 768D representation. For Ego4D, we use 3 layers, 8 heads of a vanilla Transformer Encoder with a hidden representation dimension of 2048. We use Nesterov Momentum SGD + CosineAnnealing Scheduler with learning rate 5e-4. We train the model for 30 epochs with the first 4 as warm-up epochs. All hyper-parameters are chosen by minimizing the loss on the validation set. In our top-down models, we used the compatible text encoder of the ViT-L/14@336px CLIP model as the text encoder. We used Adam optimizer with learning rate 2e-4.

For EK-55, we extracted features of all provided frames from the dataset. For the transformer, we use 1 layers, 8 heads with a hidden representation dimension of 2048. We use Adam optimizer with learning rate 5e-5. All hyper-parameters are chosen by minimizing the loss on the validation set. In our top-down models, we used the compatible text encoder of the ViT-L/14@336px CLIP model as the text encoder. We used Adam optimizer with learning rate 5e-5.

For Gaze, we use 1 layers, 8 heads of a vanilla Transformer Encoder with a hidden representation dimension of 2048. We use Nesterov Momentum SGD + CosineAnnealing Scheduler with learning rate 2e-2. All hyper-parameters are chosen by minimizing the loss on the validation set. In our top-down models, we used the compatible text encoder of the ViT-L/14@336px CLIP model as the text encoder. We used Nesterov Momentum SGD + CosineAnnealing with learning rate 15e-3.

\subsection{Number of layers for auto-regressive transformers}
\label{sec:n_layers}
We want to compare fine-tuned LLM with auto-regressive transformers trained from-scratch. However, our transformer model and the large language model we used do not have comparable parameter size. Here, we want to explore how parameter size affect the performance of the transformer and to investigate whether the performance gap we demonstrated in 4.4 is due to model size. We ran the transformer model with varying number of layers from 1 to 6 and the corresponding results are in table\ref{tab:layers}. We observed that adding more layers to the auto-regressive transformer model does not help performance. This further substantiate our conclusion that large language models are better in modeling action temporal dynamics.

\vspace{-0.05in}
\begin{table}[h]
\centering
\scalebox{1.0}{
\begin{tabular}{c cc}
\toprule
\# layers & Verb $\downarrow$ & Noun $\downarrow$ \\ 
\midrule
1 & \textbf{0.739} & 0.758 \\
2 & 0.741 & 0.756 \\
3 & 0.743 & 0.756 \\
4 & 0.742 & \textbf{0.755} \\
5 & 0.743 & 0.757 \\
6 & 0.747 & 0.759 \\
\bottomrule
\end{tabular}
}
\vspace{0.1in}
\caption{\textbf{Ablations on the number of layers to auto-regressive transformers.} Transformers are trained with actions labels from-scratch. Results are from the validation set of Ego4d v1. All transformers use 8 input segments.}
\label{tab:layers}

\hfill
\vspace{-0.15in}
\end{table}


\subsection{Number of input segments for fine-tuned LLM}
Table \ref{tab:n_seg} shows the influence of the number of input segments to LLM. A higher number of input segments allows the LLM to observe longer temporal context. We choose the input segments from \{1, 3, 5, 8\}. We did not go beyond 8 because that was the maximum segments allowed by the Ego4d benchmark. 
In general, \textbf{AntGPT}'s performances on both verb and noun increase as we increase $N_\text{seg}$. This is different from the vision baseline in Table 1, whose performance saturates at $N_\text{seg}=3$.

\vspace{-0.05in}
\begin{table}[h]
\begin{minipage}[c]{0.6\linewidth}
\centering
\scalebox{0.88}{
\begin{tabular}{c c}
\toprule
Incident & \% over all instances \\ 
\midrule
Short Seq (1) & 2.74\%  \\
Long Seq (1) & 12.93\%  \\
Invalid Seq (2) & 10.44\%  \\
Invalid Verb (3) & 1.93\% \\
Invalid Noun (3)  & 2.62\% \\
\bottomrule
\end{tabular}
}
\vspace{0.1in}
\caption{\textbf{Statistics on scenarios which require post-processing.} Invalid sequence refers to the scenario where the output sequence contains invalid strings that cannot be parsed into pairs of a verb and a noun. Invalid verb/noun refers to the scenario where the word is outside of the vocabulary. Both scenarios often imply wrong sequence length.}
\label{tab:incident}
\end{minipage}
\hfill
\begin{minipage}[c]{0.36\linewidth}
\centering
\vspace{0.02in}
\scalebox{1.05}{
\begin{tabular}{c cc}
\toprule
\# seg & Verb $\downarrow$ & Noun $\downarrow$ \\ 
\midrule
1 & 0.734 & 0.748 \\
3 & 0.717 & 0.726 \\
5 & 0.723 & 0.722 \\
8 & \textbf{0.707} & \textbf{0.719} \\
\bottomrule
\end{tabular}
}
\vspace{0.1in}
\caption{\textbf{Ablations on the number of input segments to LLM.} Results are reported the on the validation set of Ego4d v1 from fine-tuned model. Segment number of 8 brings best performance.}
\label{tab:n_seg}
\end{minipage}
\vspace{-0.15in}
\end{table}

\subsection{Generating goal descriptors using LLMs}

In Section 4.4, we describe an ablation study where goal descriptor is used for in-context learning. More specifically, we use GPT-3.5-Turbo API to generate the goal by giving recognized history actions and few samples as demonstrations with ICL as goal descriptors. An illustraion of the goals can be found in Figure~\ref{fig_sup:icl_cot}. For the experiments in Table 4, the goal descriptors are added as LLM prompts in the form of "\texttt{Goal:<goal> Observed actions:<observed actions> => }". We note that this ablation study poses an interesting contrast to the CoT experiments: Instead of asking the LLM to jointly infer the goal and predict the action sequences, the ablation experiment conducts goal inference and action prediction separately, and achieves slightly worse performance than CoT.

\subsection{Additional details on EPIC-Kitchens-55 and EGTEA Gaze + experiments}

In the experiment section, we describe two setups of the LTA task on different datasets. For the Ego4D datasets, we measure the edit distance of future action sequence predictions while for EK-55 and Gaze, we measure the mean average precision for all actions to occur in the future. The distinction arises in how ``action'' is defined among these different datasets and setups. In Ego4D benchmarks, we follow the official definition of action, which a verb-prediction and a noun-prediction combining together to form an action. For EK-55 and Gaze, we adopt the task setup of Ego-Topo \cite{ego-topo} in order to compare with previous works. In their setup they defined ``action'' as only the verb-prediction, excluding the noun-prediction in the final   ``action'' prediction.

\section{Visualization and Analysis}
\label{sec:viz}
\subsection{Examples of AntGPT Prediction}
We provide four examples of bottom-up fine-tuned \textbf{AntGPT} in Figure~\ref{fig:sup_vis1}, two of which are positive, and the other two negative. Correctly recognized actions from the video observations (outputs of the vision module, and inputs to the LLM) and future predictions (outputs of the LLM) are marked in green while wrong observations or predictions are marked red. We observe that \textbf{AntGPT} is able to predict reasonable and correct future actions even when the recognized actions are not perfect. However, when most of the recognized actions are incorrect (Figure~\ref{fig:sup_vis1}c), the predictions would also fail as expected. Finally, we observe that the bottom-up approach sometimes fail to implicitly infer the long-term goal based on the observed actions. In Figure~\ref{fig:sup_vis1}d, the actor keeps on mixing the ice while the LLM predicts curry-cooking related activities.


% Figure environment removed

\subsection{Additional Examples of ICL and CoT} Similarly with Figure 2, Figure~\ref{fig_sup:icl_cot} shows two additional examples of bottom-up prediction with ICL and top-down prediction with CoT. We can further observe the influence of implicitly prediction future goal from the two examples. In the first example, due to the low quality of recognized actions from the video observation (repeating "\texttt{play card}"), the bottom-up prediction follows the observation pattern to repeat "\texttt{play card}" while in the prediction of top-down prediction using CoT to consider the goal, the model succeed to predict "\texttt{take card}" and "\texttt{put card}" alternatively which both matches the ground-truth future actions and common sense in the scene of card playing better, with the verb edit distance significantly goes down from 0.95 to 0.45. The second example shows a failure example of using CoT. In this video of ``playing tablet'', the top-down model predicts the goal as ``using electronic devices'', thus influencing the following future action prediction to hallucinate actions like "\texttt{operate computer}". This behavior is intuitive, as an incorrect goal sampled by the LLM is likely to mislead the predicted actions that conditioned on it. We believe adding additional cues for goal inference, and explicitly model the different ``modes'' of the future to capture the goal diversity would mitigate this behavior.

% Figure environment removed


\subsection{Additional Examples of Counterfactual Prediction}
In Figure~\ref{fig:sub_counter}, we demonstrate four additional counterfactual prediction examples. In order to predicting with explicit goal using ICL, we construct the prompts in the form of: \texttt{"Goal:<inferred/altered goal> Observed actions:<observed actions> => <future actions>"} We observe that switching the goals from the ``inferred goal'' to the ``altered goal'' indeed has huge impact on the predicted actions. This confirms that the goals are utilized by the LLM to predict (or plan) the future actions to take. This behavior is consistent with our observations in Figure~\ref{fig_sup:icl_cot}.

% Figure environment removed

\section{Contribution Statement}
\textbf{Qi Zhao} proposed and implemented LLM finetuning for temporal dynamics modeling, designed and conducted the supervised learning experiments on Ego4D, Epic-Kitchen, and Gaze+.

\textbf{Ce Zhang} implemented the base transformer models for action recognition and prediction, and created the overall training infrastructure.

\textbf{Shijie Wang} designed and implemented the few-shot learning framework for goal generation and LTA, performed ablation studies on in-context learning and chain-of-thoughts, and optimized the LLM prompting strategies. 

\textbf{Changcheng Fu} created the base visual feature extraction framework, and performed qualitative analysis and visualization of the results with Shijie.

\textbf{Nakul Agarwal} and \textbf{Kwonjoon Lee} provided comments and feedback on experiments and paper.

\textbf{Chen Sun} designed and led the overall project.

\textbf{Chen Sun}, \textbf{Shijie Wang}, \textbf{Qi Zhao}, \textbf{Ce Zhang} and \textbf{Changcheng Fu} wrote the paper.



