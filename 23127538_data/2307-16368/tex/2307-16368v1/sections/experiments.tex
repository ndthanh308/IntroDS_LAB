\vspace{-1em}
\section{Experiments}
\vspace{-1em}

We now present quantitative results and qualitative analysis on the Ego4D~\cite{ego4d}, EPIC-Kitchens~\cite{Epic-Kitchen}, and EGTEA Gaze+~\cite{li2018eye} benchmarks.

\vspace{-1em}
\subsection{Experimental Setup}
\noindent\textbf{Ego4D v1} \cite{ego4d} contains 3,670 hours of egocentric video of daily life activity spanning hundreds of scenarios. We focus on the videos in the \textit{Forecasting} subset which contains 1723 clips with 53 scenarios. The total duration is around 116 hours. There are 115 verbs and 478 nouns in total. We follow the standard train, validation, and test splits from Ego4D~\cite{ego4d}.

\noindent\textbf{Ego4D v2} extends Ego4d v1. It contains 3472 annotated clips with total duration of around 243 hours. There are 117 verbs and 521 nouns. We follow the standard train, validation, and test splits.

\noindent\textbf{EPIC-Kitchens-55}~\cite{Epic-Kitchen} (EK-55) contains 55 hours egocentric videos of cooking activities of  different video takers. Each video is densely annotated with action labels, spanning over 125 verbs and 352 nouns. We adopt the train and test splits from~\cite{ego-topo}.

\noindent\textbf{EGTEA Gaze+}~\cite{li2018eye} (GAZE) contains 86 egocentric cooking videos over 26 hours. Each video is densely annotated with action labels of 19 verbs and 53 nouns. We adopt the splits from~\cite{ego-topo}.

\noindent\textbf{Evaluation Metrics.} For Ego4D, we use the edit distance (ED) metric. It is computed as the Damerau-Levenshtein distance over sequences of predictions of verbs, nouns or actions. We follow the standard practice in~\cite{ego4d} and report the minimum edit distance between each of the top $K=5$ predicted sequences and the ground-truth. We report Edit Distance at $Z = 20$ (ED@20) on the validation set and the test set.
For EK-55 and Gaze, we follow the evaluation metric described in~\cite{ego-topo}. The first K\% of each video is given as input, and the goal is to predict the set of actions happening in the remaining (100-K)\% of the video as multi-class classification.
We sweep values of K = [25\%, 50\%, 75\%] representing different anticipation horizons and report mean average precision (mAP) on the validation sets. We report the performances on all target actions (All), the frequently appeared actions (Freq), and the rarely appeared actions (Rare) as in~\cite{ego-topo}.

\noindent\textbf{Implementation Details.} We use the frozen CLIP~\cite{radford2021clip} ViT-L/14 for image features, and a transformer encoder with attention 8 heads, and 2048 hidden size. Additional details are described in Section~\ref{sec:exp_details}.



\subsection{Can LLMs Help Top-down LTA?}



We generate goals using the recognized actions for all three datasets: To obtain the ground-truth goals for constructing the in-context examples, we use the video titles for Gaze, and the video descriptions for EK-55. We manually inspect the Ego4D videos and annotate the goals ourselves. We use 12 in-context examples to infer the goals. For EK-55 and GAZE, we always use the recognized actions in the first 25\% of each video to infer the goals. For Ego4D, we set $N_\text{seg} = 8$.

\begin{table}[h]
\setlength\tabcolsep{3pt}
\small
\centering
    \begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Ego4d v1 (ED)} & \multicolumn{3}{c}{EK-55 (mAP)} & \multicolumn{3}{c}{Gaze (mAP)} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
     & Verb $\downarrow$ & Noun $\downarrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ & ALL $\uparrow$ & Freq $\uparrow$ & Rare $\uparrow$ \\
    \midrule
    image features & 0.735 & 0.753 & 38.2 & \textbf{59.3} & 29.0 & 78.7 & \textbf{84.7} & 68.3\\
    image features + ICL goals & \textbf{0.724} & \textbf{0.744} & \textbf{40.2} & 58.8 & \textbf{32.0} & \textbf{80.2} & 84.5 & \textbf{74.0} \\
    \midrule
    image features + oracle goals\textsuperscript{$\ast$} & - & - & 40.9 & 58.7 & 32.9 & 81.6 & 86.8 & 69.3 \\
    \bottomrule
    \end{tabular}
\vspace{0.1in}
\caption{\textbf{Impact of the goals for LTA.} Goal-conditioned (top-down) models outperforms the bottom-up model in all three datasets. We report edit distance for Ego4D, mAP for EK-55 and Gaze. All results are reported on the validation set.} %
\label{tab:trend}
\vspace{-0.1in}
\end{table}




Table~\ref{tab:trend} shows results on Ego4D v1, EK-55, and Gaze.
We first compare a bottom-up approach based on the visual embedding features and a top-down approach that additionally takes inferred goals as its inputs. We notice a clear trend that the inferred goals lead to consistent improvements for the top-down approach, especially for the rare actions of EK-55 and Gaze. We also consider using the video metadata to form oracle goals, they are available for all training and validation examples of EK-55 and Gaze. We observe that the oracle goals lead to some improvements, indicating that the inferred goals already offer competitive performance improvements, and the performance can be further boosted with more accurate goals.








\subsection{Can LLMs Model Temporal Dynamics?}

We then explore if LLMs can be directly applied to model temporal dynamics, in addition to few-shot goal inference. We focus our ablation on the Ego4D benchmark as its evaluation criteria explicitly measures the ordering of the anticipated actions.
We adopt the same video representation as for in-context goal inference, but now fine-tunes the LLM on the Ego4D training set. During training, the $N_\text{seg}$ ground-truth action labels are concatenated as the input sequence, and $Z$ ground-truth action labels are concatenated as the target sequence. During evaluation, we concatenate $N_\text{seg}$ recognized actions as input, and postprocess the output sequence into $Z$ anticipated actions (see Section~\ref{sec:postprocessing}). We directly use the fine-tuning endpoint provided by OpenAI to fine-tune the GPT-3-curie model. We picked the hyper-parameters based on the sequence completion loss on the validation set.

\begin{table}[t]
\setlength\tabcolsep{3pt}
\small
\centering
    \begin{tabular}{ll|c|c|cc|ccc}
    \toprule
    \multirow{2}{*}{Input} & Temporal & With & $N_\text{seg}$ & \multicolumn{2}{c}{Val} & \multicolumn{3}{c}{Test} \\
    & Model & Goal & & Verb $\downarrow$ & Noun $\downarrow$ & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\
    \midrule
    GT actions & LLM & No & 8 & 0.679 & 0.617 & - & - & - \\
    \midrule
    \multirow{5}{*}{recognized actions} & LLM & No & 8 & \textbf{0.707\scriptsize{$\pm$4e-4}} & \textbf{0.719\scriptsize{$\pm$4e-4}} & \textbf{0.702} & \textbf{0.703} & \textbf{0.912} \\
    & LLM & No & 3 & 0.717 & 0.726 & 0.713 & 0.708 & 0.915 \\
    & Transformer & No & 8 & 0.742 & 0.755 & - & - & - \\
    & Transformer & No & 3 & 0.781 & 0.763 & - & - & - \\
    & LLM & Yes & 8 & 0.709 & 0.729 & - & - & - \\
    \midrule
    image features & Transformer & No & 3 & 0.731\scriptsize{$\pm$8e-3} & 0.743\scriptsize{$\pm$8e-3} & 0.726 & 0.739 & 0.929 \\
    image features & Transformer & No & 8 & 0.735 & 0.753 & - & - & - \\
    image features & Transformer & Yes & 8 & 0.724 & 0.744 & - & - & - \\
    \bottomrule
    \end{tabular}
\vspace{0.1in}
\caption{\textbf{LLM as the Temporal Model on the Ego4D v1 Dataset.} Using recognized action labels as video representation and LLM as the temporal model consistently outperforms Transformer-based temporal model and the visual embedding representation. We report ED@20 and compute the standard deviations over three runs. Test set results are computed by the Ego4D evaluation server.}
\label{tab:comp_to_vision}
\vspace{-2em}
\end{table}


To understand the effectiveness of the fine-tuned LLM for temporal dynamics modeling, we consider two groups of ablations: (1) \textbf{The impact of video representation} given the same transformer-based temporal model; (2) \textbf{The impact of LLM} versus transformer-based temporal model given the same recognized action inputs. We report the comparisons in Table~\ref{tab:comp_to_vision}, and observe that: (1) \textbf{Recognized action labels make a good video representations}. When the transformer model is used and $N_\text{seg} = 8$, the edit distance of recognized actions is similar to that of image features. We notice a reverse trend on the optimal $N_\text{seg}$ for the two video representations, indicating action labels work better when $N_\text{seg}$ is large; (2) \textbf{LLM models temporal dynamics better} than the transformer baseline. When $N_\text{seg}=3$, LLM achieves 8.2\% lower ED for verb and 4.8\% for noun. When $N_\text{seg}=8$, LLM achieves 4.7\% lower ED for verb and 4.8\% for noun. The transformer baseline with recognized actions has 4 layers and 8 attention heads, and adopt the causal attention mask for the query tokens to perform autoregressive action anticipation. In Section~\ref{sec:n_layers}, we perform an ablation on the number of layers for the transformer baseline, and find that adding more layers has negligible impact on its performance. 

Table~\ref{tab:comp_to_vision} also reveals additional insights: For example, we observe the LTA performance for the LLM temporal model can be significantly improved when ground-truth action labels are used, especially for the noun edit distance. This indicates that one performance bottleneck of our proposed approach is the action recognition accuracy. Additionally, we observe that while goal conditioning is helpful for image feature + transformer, it is not helpful for recognized action + LLM. We conjecture that it might be due to the LLM already implicitly encodes the goal information.








\subsection{Few-shot Action Anticipation}
\begin{table}[h]
\centering
\begin{tabular}{ccccccc}
\toprule
Model & Learning Method & With Goal & Verb $\downarrow$ & Noun $\downarrow$ \\ 
\midrule
Transformer &SGD & No &0.770  &0.968  \\
LLM &ICL & No & 0.758 & 0.728 \\
LLM &ICL & Yes & 0.775 & 0.728 \\
LLM &CoT & Yes &  \textbf{0.756} & \textbf{0.725} \\%\\\midrule
\bottomrule
\end{tabular}
\vspace{0.1in}
\caption{\textbf{Few-shot results with LLM on Ego4D v1 validation set.} The transformer baseline is trained with the same 12 examples from training set as the in-context examples for LLM.} %
\vspace{-1em}
\label{tab:icl}
\end{table}
We conduct quantitative few-shot learning experiments on the Ego4D v1 validation set, and compare the transformer-based model and LLM. The transformer baseline is trained on the same 12 examples sampled from the training set of Ego4D as the in-context examples used by LLM. The transformer is optimized by gradient descent. For top-down LTA with LLM, we compare the two-stage approach to infer goals and predict actions separately, and the one-stage approach based on chain-of-thoughts. Results are shown in Table~\ref{tab:icl}. We observe that all LLM-based methods perform much better than the transformer baseline for few-shot LTA. Among all the LLM-based methods, top-down prediction with CoT prompts achieves the best performance on both verb and noun. However, the gain of explicitly using goal conditioning is marginal, similar to what we have observed when training on the full set. In Figure~\ref{fig:vis2} (b) and (c), we illustrate the example input and output for ICL and CoT, respectively. More examples can be found in Section~\ref{sec:viz}.




% Figure environment removed

To better understand the impact of goals for action anticipation, we design a ``counterfactual'' prediction experiment: We first use GPT-3.5-Turbo to generate goals for examples as we did above and treat the outputs as the originally inferred goals, and then manually provide ``altered goals‚Äù which are different than the original goals but are equally reasonable. We then perform in-context learning using both sets of the goals and with the same sequences of recognized actions as inputs to the LLM. Figure~\ref{fig:vis3} illustrates our workflow and two examples. In the second example, we can see that although the recognized actions are the same, the output action predictions are all different. Despite the differences, using "fix machine" as the goal generates actions highly related to fixing such as "\texttt{tighten nut}" and "\texttt{turn screw}" while switching to "\texttt{examine machine}" leads to actions like "\texttt{test machine}" and "\texttt{record data}". More examples can be found in Section~\ref{sec:viz}.

Our qualitative analysis with the counterfactual prediction shows that the choice of goal can have large impacts on the action anticipation outputs. This indicates another future direction to improve the top-down action anticipation performance is to improve the accurate and quality of the predicted goals, and to consider multiple plausible goals.



\subsection{Comparison With State-of-the-art}
Finally, we compare AntGPT with the previous state-of-the-art methods. Table~\ref{tab:sota_ego4d} shows performance comparisons on Ego4D v1 and v2 benchmarks, we observe that AntGPT achieves the overall best performance on both datasets. For Ego4d v1 and v2, we train the action recognition and fine-tune the LLM temporal models with their corresponding training set.
On v1, our approach improves the previous best performance by 2.98\% in verb, 4.37\% in noun and 1.68\% in action. On v2, our approach improves the previous best performance by 2.78\% in verb, 7.42\% in noun and 2.09\% in action. Since Ego4d v1 and v2 share the same test set, it is also worth mentioning that our model trained solely on v1 data is able to outperform the best vision-only model~\cite{chen2023videollm} trained on the v2 data, which indicates the data efficiency and the promise of our approach.

For EK-55 and GAZE, we compare the goal-conditioned AntGPT with the previous state-of-the-art results in Table~\ref{tab:sota_gaze_ek}. AntGPT achieves the overall best performance on both datasets. %
We observe that our proposed model performs particularly well on rare actions. 

\begin{table}
\vspace{-0.1in}
\centering
\scalebox{1}{
    \begin{tabular}{ccccc}
    \toprule
    Method & Version & Verb $\downarrow$ & Noun $\downarrow$ & Action $\downarrow$ \\ 
    \midrule
    HierVL~\cite{hierVL} & v1 & 0.7239 & 0.7350 & 0.9276 \\
    ICVAE\cite{icvae} & v1 & 0.7410 & 0.7396 & 0.9304 \\
    VCLIP ~\cite{das2022video+} & v1 & 0.7389 & 0.7688 & 0.9412 \\
    Slowfast ~\cite{ego4d} & v1 & 0.7389 & 0.7800 & 0.9432 \\
    \textbf{AntGPT} (ours) & v1 & \textbf{0.7023} & \textbf{0.7029} & \textbf{0.9120} \\
    \midrule
    Slowfast~\cite{ego4d} & v2 & 0.7169	& 0.7359 & 0.9253 \\
    VideoLLM~\cite{chen2023videollm} & v2 & 0.721 & 0.725 & 0.921\\
    \textbf{AntGPT} (ours) & v2 & \textbf{0.6969} & \textbf{0.6813} & \textbf{0.9060}\\
    \bottomrule
    \end{tabular}
}
\vspace{0.1in}
\caption{\textbf{Comparison with SOTA methods on the Ego4D v1 and v2 test sets in ED@20.} Ego4d v1 and v2 share the same test set. V2 contains more training and validation examples than v1. 
}
\vspace{-0.1in}
\label{tab:sota_ego4d}
\end{table}

\begin{table}
\centering
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{EK-55}} & \multicolumn{3}{c}{\textbf{GAZE}} \\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    & ALL & FREQ & RARE & ALL & FREQ & RARE \\
    \midrule
    I3D~\cite{i3d_cvpr17} & 32.7 & 53.3 & 23.0 & 72.1 & 79.3 & 53.3 \\
    ActionVLAD~\cite{girdhar2017actionvlad} & 29.8 & 53.5 & 18.6 & 73.3 & 79.0 & 58.6\\
    Timeception~\cite{hussein2019timeception} & 35.6 & 55.9 & 26.1 & 74.1 & 79.7 & 59.7 \\
    VideoGraph~\cite{hussein2019videograph} & 22.5 & 49.4 & 14.0 & 67.7 & 77.1 & 47.2 \\
    EGO-TOPO~\cite{ego-topo} & 38.0 & 56.9 & 29.2 & 73.5 & 80.7 & 54.7 \\
    Anticipatr~\cite{anticipator} & 39.1 & 58.1 & 29.1 & 76.8 &  83.3 & 55.1 \\
    \textbf{AntGPT} (ours) & \textbf{40.2} & \textbf{58.8} & \textbf{32.0} & \textbf{80.2} & \textbf{84.5} & \textbf{74.0} \\
    \bottomrule
    \end{tabular}
\vspace{0.1in}
\caption{\textbf{Comparison with SOTA methods on the EK-55 and GAZE Dataset in mAP.} ALL, FREQ and RARE represent the highest performances on all, frequent, and rare target actions respectively. }
\vspace{-1em}
\label{tab:sota_gaze_ek}
\end{table}


