\section{Conclusion and Future Work}


In this paper, we propose \textbf{AntGPT} a framework to incorporate large language models for the video-based long-term action anticipation task. By representing videos as discrete action labels, \textbf{AntGPT} is capable of inferring goals helpful for top-down LTA and also modeling the temporal dynamics of actions helpful for LTA in general. We have demonstrated the effectiveness of large language model and compared various strategies (i.e. fine-tuning, in-context learning, and chain-of-thought) via extensive quantitative and qualitative evaluations. Our proposed method sets new state-of-the-art performances on the Ego4D LTA, EPIC-Kitchens-55, and EGTEA GAZE+ benchmarks. We further study the advantages and limitations of applying LLM on video-based action anticipation, thereby laying the groundwork for future research in this field.


\noindent\textbf{Limitations.} Although our approach provides a promising new perspective in tackling the LTA task, there are limitations that worth pointing out. The choices of representing videos with fixed-length actions is both efficient and effective for LTA task. However, the lack of visual details may pose constraints on other tasks. Another limitation is the prompt designs of ICL and CoT are still empirical, and varying the prompt strategy may cause significant performance differences. Finally, as studied in our counterfactual experiments, the goal accuracy would have significant impact on the action recognition outputs, and an important future direction is to improve the inferred goal accuracy, and also take multiple plausible goals into account.



\noindent\textbf{Acknowledgements.} We would like to thank Nate Gillman and Minh Quan Do for their feedback and insights. This work is in part supported by Honda Research Institute, Meta AI, and Samsung Advanced Institute of Technology.
