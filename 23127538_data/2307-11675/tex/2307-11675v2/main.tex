\documentclass[man]{apa7}
%\PassOptionsToPackage{showframe}{geometry}
\usepackage{enumitem}
\usepackage{newfloat}
\DeclareFloatingEnvironment[name={Supplementary Figure}]{suppfigure}
\newcommand{\beginsupplement}{%
    \setcounter{table}{0}
    \setcounter{section}{0}
    \renewcommand{\thesection}{S\arabic{section}}
    \setcounter{equation}{0}
    \renewcommand{\theequation}{S\arabic{equation}}
    \renewcommand{\thetable}{S\arabic{table}}%
    \setcounter{figure}{0}
    \renewcommand{\thefigure}{S\arabic{figure}}%
}
\renewcommand\appendixname{Supplimentary Information}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{url}
\DeclareGraphicsExtensions{.png,.pdf}
\usepackage{algorithm2e}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{appendix}
\usepackage{ragged2e}

%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\addbibresource{sample.bib}
%\usepackage[margin=1.25in]{geometry}

%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.

%%%%%% Title %%%%%%
% Full titles can be a maximum of 15 words. 
% Title Format: Use title case, capitalizing the first letter of each word, except for certain small words, such as articles and short prepositions

%%%%%% Authors %%%%%%
% Authors should be listed in order of contribution to the paper, by first name, then middle initial (if any), followed by last name.
% Authors should be listed in the order in which they will appear in the published version if the manuscript is accepted. 
% Use an asterisk (*) to identify the corresponding author, and be sure to include that person’s e-mail address. Use symbols (in this order: †, ‡, §, ||, ¶, #, ††, ‡‡, etc.) for author notes, such as present addresses, “These authors contributed equally to this work” notations, and similar information.
% You can include group authors, but please include a list of the actual authors (the group members) in the Supplementary Materials.
\title{Analyzing time series of unequal durations using Multidimensional Recurrence Quantification Analysis (MdRQA): validation and implementation using Python}
\shorttitle{Sliding window MdRQA validation}
\affiliation{Swarag Thaikkandi\textsuperscript{1, 2$\dag$}, K. M. Sharika\textsuperscript{2*} \\ \textsuperscript{1}Department of Biology, Indian Institute of Science Education and Research, Pune, Maharashtra, India \\ \textsuperscript{2}Department of Cognitive Science, Indian Institute of Technology Kanpur, Kanpur, Uttar Pradesh, India}
\authornote{
   $\dag$ Current affiliation \\
   * Address correspondence to }

\abstract{In recent years, recurrent quantification analysis (RQA) and its multi-dimensional version (MdRQA) have emerged as a popular tool for assessing interpersonal behavioral or physiological synchrony in groups of two or more individuals. While experimental data in such studies are typically collected for a fixed, pre-determined duration, naturally occurring phenomena may often reach a state of transition after an unpredictable or varying duration of time. The resulting recurrence plots (RPs) across samples cannot be compared directly via linear scaling because the sensitivity of RQA variables to local dynamics would vary. We propose to address this by using the sliding window technique on individual RPs and using the summary statistics of the different RQA variable distributions computed across the sliding windows to differentiate the dynamics of the original time series of unequal durations. We tested our approach in two models: 1) the Rossler attractor and  2) the Kuramoto model. We compared the ability of different summary statistics of RQA variable distributions to accurately predict the dynamic states of the system across varying levels of noise, unequal lengths of time series, and, in the case of the Kuramoto model, different numbers of oscillators across samples. We found that while the mean, compared to other measures of central tendency, was a more accurate predictor of the underlying dynamic state of the system at high noise conditions, the mode was the most robust to the degree of noise in the signals. To our knowledge, this is the first systematic attempt to validate the use of MdRQA in computing and comparing synchrony between systems of non-uniform composition and unequal time series data, paving the way for future work that examines interpersonal synchrony in more naturalistic, ecologically valid contexts. }

\begin{document}
\maketitle
%%%%%% Main Text %%%%%%
\section{Introduction}
\justifying \par Recent years have witnessed a steady rise in the number of studies examining behavioral and/ or physiological synchrony between individuals in relation to their interpersonal outcomes including cohesion, cooperation and trust (\citeauthor{guastello2006electrodermal}, \citeyear{guastello2006electrodermal}; \citeauthor{richardson2007art}, \citeyear{richardson2007art}; \citeauthor{shockley2003mutual}, \citeyear{shockley2003mutual}; \citeauthor{schippers2010mapping}, \citeyear{schippers2010mapping}; \citeauthor{konvalinka2011synchronized}, \citeyear{konvalinka2011synchronized}; \citeauthor{yun2012interpersonal}, \citeyear{yun2012interpersonal}; \citeauthor{hasson2012brain}, \citeyear{hasson2012brain}; \citeauthor{spiegelhalder2014interindividual}, \citeyear{spiegelhalder2014interindividual}; \citeauthor{golland2015mere}, \citeyear{golland2015mere}; \citeauthor{mitkidis2015building}, \citeyear{mitkidis2015building}; \citeauthor{wallot2016multidimensional}, \citeyear{wallot2016multidimensional}; \citeauthor{bevilacqua2019brain}, \citeyear{bevilacqua2019brain}; \citeauthor{bevilacqua2019brain}, \citeyear{bevilacqua2019brain}; \citeauthor{pan2020instructor}, \citeyear{pan2020instructor}; \citeauthor{thorson2021oxytocin}, \citeyear{thorson2021oxytocin}; \citeauthor{gordon2021group}, \citeyear{gordon2021group}; \citeauthor{tomashin2022interpersonal},\citeyear{tomashin2022interpersonal}). Recurrent quantification analysis (RQA) has emerged as a popular tool for assessing interpersonal synchrony in such studies primarily since it makes very few assumptions about the constituent components of the system under study and because of its general robustness to dynamic variability in the signal (also known as non-stationarity) (\citeauthor{marwan2007recurrence}, \citeyear{marwan2007recurrence}). RQA is based on the principle that in a complex system governed by many inter-dependent, interacting components, any measured variable or dimension (and  its time delayed copies) could be used to retrieve the dynamical behavior  of the system or in other words, reconstruct what is known as its \emph{phase space} or \emph{state space} (\citeauthor{takens1981dynamical}, \citeyear{takens1981dynamical}). Phase space is a graphical representation of all possible states of the system over time and is composed of all the coordinates (or dimensions) required to define a given state of the system (\citeauthor{huffaker2017nonlinear}, \citeyear{huffaker2017nonlinear}). Quantifying \emph{recurrence} or how often a trajectory revisits a point in the phase space (typically represented as black dots against a white background in a recurrence plot, Fig.\ref{fig:4}) would indicate whether and how different components of the multi-variate system interact and converge on the same state (or in other words, are \emph{in sync}) across time (\citeauthor{webber1994dynamical}, \citeyear{webber1994dynamical}; \citeauthor{marwan2002recurrence}, \citeyear{marwan2002recurrence}). MdRQA or multidimensional RQA is an extension of RQA that allows the time series data of more than two components (or dimensions) of the system to be considered simultaneously for computing recurrence at the group level (\citeauthor{wallot2016multidimensional}, \citeyear{wallot2016multidimensional}; \citeauthor{gordon2021group}, \citeyear{gordon2021group}; \citeauthor{tomashin2022interpersonal}, \citeyear{tomashin2022interpersonal}). 
\par Recent years have witnessed a steady rise in the number of studies examining behavioral and/or physiological synchrony between individuals in relation to their interpersonal outcomes including cohesion, cooperation and trust (\citeauthor{guastello2006electrodermal}, \citeyear{guastello2006electrodermal}; \citeauthor{richardson2007art}, \citeyear{richardson2007art}; \citeauthor{shockley2003mutual}, \citeyear{shockley2003mutual}; \citeauthor{schippers2010mapping}, \citeyear{schippers2010mapping}; \citeauthor{konvalinka2011synchronized}, \citeyear{konvalinka2011synchronized}; \citeauthor{yun2012interpersonal}, \citeyear{yun2012interpersonal}; \citeauthor{hasson2012brain}, \citeyear{hasson2012brain}; \citeauthor{spiegelhalder2014interindividual}, \citeyear{spiegelhalder2014interindividual}; \citeauthor{golland2015mere}, \citeyear{golland2015mere}; \citeauthor{mitkidis2015building}, \citeyear{mitkidis2015building}; \citeauthor{wallot2016multidimensional}, \citeyear{wallot2016multidimensional}; \citeauthor{bevilacqua2019brain}, \citeyear{bevilacqua2019brain}; \citeauthor{dikker2017brain}, \citeyear{dikker2017brain}; \citeauthor{pan2020instructor}, \citeyear{pan2020instructor}; \citeauthor{thorson2021oxytocin}, \citeyear{thorson2021oxytocin}; \citeauthor{gordon2021group}, \citeyear{gordon2021group}; \citeauthor{tomashin2022interpersonal},\citeyear{tomashin2022interpersonal}). Recurrent quantification analysis (RQA) has emerged as a popular tool for assessing interpersonal synchrony in such studies primarily since it makes very few assumptions about the constituent components of the system under study and because of its general robustness to dynamic variability in the signal (also known as non-stationarity) (\citeauthor{marwan2007recurrence}, \citeyear{marwan2007recurrence}). RQA is based on the principle that in a complex system governed by many inter-dependent, interacting components, any measured variable or dimension (and  its time delayed copies) could be used to retrieve the dynamical behavior  of the system or in other words, reconstruct what is known as its \emph{phase space} or \emph{state space} (\citeauthor{takens1981dynamical}, \citeyear{takens1981dynamical}). Phase space is a graphical representation of all possible states of the system over time and is composed of all the coordinates (or dimensions) required to define a given state of the system (\citeauthor{huffaker2017nonlinear}, \citeyear{huffaker2017nonlinear}). Quantifying \emph{recurrence} or how often a trajectory revisits a point in the phase space (typically represented as black dots against a white background in a recurrence plot, Fig.\ref{fig:4}) would indicate whether and how different components of the multi-variate system interact and converge on the same state (or in other words, are \emph{in sync}) across time (\citeauthor{webber1994dynamical}, \citeyear{webber1994dynamical}; \citeauthor{marwan2002recurrence}, \citeyear{marwan2002recurrence}; \citeauthor{mitkidis2015building}, \citeyear{mitkidis2015building}). MdRQA or multidimensional RQA is an extension of RQA that allows the time series data of more than two components (or dimensions) of the system to be considered simultaneously for computing recurrence at the group level (\citeauthor{wallot2016multidimensional}, \citeyear{wallot2016multidimensional}; \citeauthor{gordon2021group}, \citeyear{gordon2021group}; \citeauthor{tomashin2022interpersonal}, \citeyear{tomashin2022interpersonal}). 
\justifying \par While experimental data assessing synchrony between multiple components of a system are typically collected for a fixed, pre-determined duration, naturally occurring phenomena may often reach a state of transition after an unpredictable or varying duration of time at which point data collection may be discontinued. This results in time series data of unequal lengths across different samples of the system. For example, it is possible that when one is recording behavioral or physiological variables simultaneously from two or more individuals in a group, different groups being sampled for the study successfully complete the task or arrive at a meaningful interpersonal outcome at varied lengths of time. It may also be the case that, for some logistic reason, data could not be collected for a uniform duration across all groups. In such scenarios, the resulting recurrence plots (RPs) of different sizes cannot be compared directly via linear scaling because RQA variables are statistical measures of the vertical or horizontal line distributions in an RP  and consequently,  represent the local dynamical changes to varying levels of sensitivity based on the overall length of the individual RP. In other words, while RQA variables from shorter RPs may be able to capture local dynamics at relatively high resolution, the same may get averaged out in larger RPs. This makes the comparison of synchrony measures across groups difficult, particularly given recent reports demonstrating the impact of total pooled duration of data (in minutes) on detection of correlation based synchrony across participants (\citeauthor{stuldreher2023robustness}, \citeyear{stuldreher2023robustness}). We proposed to address this by applying the sliding window technique on RPs, previously shown to be demonstrably sensitive for tracking local dynamical changes in physiology (\citeauthor{webber2005recurrence}, \citeyear{webber2005recurrence}; \citeauthor{webber1995influence}, \citeyear{webber1995influence}). The sliding window technique involves partitioning a single large RP into smaller, same sized, overlapping windows along the RP diagonal and then computing the RQA variables for each of these sliding windows. We hypothesized that the summary statistics of the different RQA variable distributions computed across the sliding windows of a given RP could be used to functionally differentiate the dynamics of a set of such RPs using a nested cross-validation approach (Fig. \ref{fig:1}).
\justifying \par We proceeded to examine this hypothesis by using two well-known  non-linear dynamical systems as our testing grounds: 1) Rossler attractor and  2) the Kuramoto system of coupled oscillators. We tested the ability of our approach to classify the dynamics of each system with high cross validation accuracy – while the duration of the time series, and in the case of the Kuramoto model, the number of oscillators (or observed dimensions) in the system varied across samples. We also tested the robustness of different summary statistics of MdRQA variable distributions (across sliding windows of a RP) in accurately predicting the dynamic states of the system across varying levels of noise in the signal.
\subsection{Multi-dimensional Recurrence Quantification Analysis or MdRQA}
RQA was developed to quantify the dynamics of a time series (\citeauthor{webber1994dynamical}, \citeyear{webber1994dynamical}; \citeauthor{marwan2002recurrence}, \citeyear{marwan2002recurrence}) via phase space reconstruction using time delayed embedding (\citeauthor{takens1981dynamical}, \citeyear{takens1981dynamical}). As described above, phase-space is a mathematical construct that serves as a graphical representation to comprehensively depict the complete range of states that a system being analyzed can occupy. If the complete depiction of the system requires it to have D independent measures, then the phase-space is said to have D dimensions. Time delayed embedding is a method that aims to retrieve the dynamics of a multi-component, coupled system by using the time series data of one of the interacting components which is assumed to encode the information about the dynamics of others in itself (\citeauthor{hasson2012brain}, \citeyear{hasson2012brain}). According to Taken’s theorem, if a system consists of multiple, independent, interacting dimensions, but, if one has access to only one of them over time (say, x), then the dynamics of the whole system can be approximately reconstructed (a process known as embedding) by utilizing (D number of) time delayed versions of the observable x as (D-dimensional) coordinates of the phase space (\citeauthor{huffaker2017nonlinear},\citeyear{huffaker2017nonlinear}). For example, let the time series be:

\begin{equation}
    \boldsymbol{x} = ( x_{1}, x_{2}, x_{3}, ..., x_{n})
\end{equation}
which is sampled at regular intervals of time. From this, D-dimensional vectors, $\boldsymbol{V_{1}}$, $\boldsymbol{V_{2}}$, and so on, can be constituted by estimating D-1  versions of $\boldsymbol{x}$ with time delay, $\tau$, as below:
\begin{equation}
    \boldsymbol{V_{1}}= (x_{1}, x_{1+\tau}, x_{1+2\tau}, ..., x_{1+(D-1)\tau})
\end{equation}
\begin{equation}
    \boldsymbol{V} = \begin{bmatrix}
                        \boldsymbol{V_{1}} \\
                        \boldsymbol{V_{2}} \\
                        \vdots \\
                        \boldsymbol{V_{n-(D-1)\tau}}
                      \end{bmatrix}  =
                      \begin{bmatrix}
                          x_{1}, x_{1+\tau}, x_{1+2\tau}, ..., x_{1+(D-1)\tau} \\
                          x_{2}, x_{2+\tau}, x_{2+2\tau}, ..., x_{2+(D-1)\tau} \\
                          x_{3}, x_{3+\tau}, x_{3+2\tau}, ..., x_{3+(D-1)\tau} \\
                          \vdots \\
                          x_{n-(D-1)\tau}, x_{n-(D-2)\tau}, x_{n-(D-3)\tau}, ..., x_{n}
                      \end{bmatrix}
\end{equation}

In the above expression, while each row represents a point in the D-dimensional phase space, each column represents the coordinates in the evolution of the trajectory in one of the D dimensions.
\justifying \par A recurrence plot or RP generated by graphically representing how often a trajectory revisits a point in the phase space (\citeauthor{eckmann1995recurrence}, \citeyear{eckmann1995recurrence}) lends itself for statistical analysis that is quantified by the RQA variables. RP contains binary values denoting repetition of values in V over time. However, it is possible that the points may not always revisit exactly the same spot in the phase space in which case a predetermined threshold criterion, $\epsilon$, is used to classify whether a given point is close enough to be deemed recurrent or not. For example, if the euclidean distance between two points in the trajectory (at time points i and j, respectively) is less than $\epsilon$, a value of 1 (=recurrent) would be assigned to the ith row and jth column in the RP. This is accomplished by the heaviside step function, that would assign 1, if the value of input is greater than 0, and 0 otherwise. 
\begin{equation}
    RP_{ij}= \Theta(\epsilon - ||\boldsymbol{V_{i}} - \boldsymbol{V_{j}}||)
\end{equation}
MdRQA differs from RQA in that it allows multiple measures or observables to be used for phase space reconstruction (\citeauthor{wallot2016multidimensional}, \citeyear{wallot2016multidimensional}). Please see Supplementary section \ref{section:tde} for details on how expression (3) would transform in case of MdRQA. 


\section{Methods}
\subsection{Parameter Estimation}
\justifying \par To generate the recurrence plots, time delay \& embedding dimension is computed for each dynamical system that has been observed or measured as part of the dataset. Time delay ($\tau$) is estimated by serially sampling time delays from 1 to 20 and computing the first minima (first local minima or global minima when the local minima did not exist) of the mutual information between the time series of a group and a time delayed version of it (\citeauthor{wallot2018calculation}, \citeyear{wallot2018calculation}; please see Supplementary section \ref{section:time-delay} for more details). This approach ensures that the time delayed signals are not too similar and permits the multidimensional topology of the trajectories in the phase space to unfold completely. While \citeauthor{wallot2016multidimensional}(\citeyear{wallot2016multidimensional}) computed this for each time series of the group (dimension) separately and averaged the value across all members of the group, to capture the cross information between different dimensions, we computed the multidimensional mutual information by estimating the multidimensional probability distributions for the group (please see section \ref{section:time-delay} Information for more details).

\begin{longtable}{|p{4cm}|p{5cm}|p{4cm}|}
\hline
\textbf{Measure} & \textbf{Formula} & \textbf{Description} \\
\hline
\center Recurrence Rate (RR) & \center$RR = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N} R(i,j)$ & Measures density of recurrence points in the recurrence plot, indicating how probable recurrence of states is in the system. \\
\hline
\center Determinism (DET) & \center$DET = \frac{\sum_{l=l_{\text{min}}}^{N}l\cdot p(l)}{\sum_{l=1}^{N}l\cdot p(l)}$ & Measures what fraction of the diagonal line lengths are above a minimum, given, line length. $p(l)$ is the probability of a line length $l$. Since diagonal lines are markers of consecutive periods of recurrence in the data, determinism corresponds to the predictability of the dynamical system. \\
\hline
\center Laminarity (LAM) & \center$LAM = \frac{\sum_{v=v_{\text{min}}}^{N}v\cdot p(v)}{\sum_{v=1}^{N}v\cdot p(v)}$ & Mathematically equivalent to determinism but defined for vertical (or horizontal) line lengths. Since vertical (or horizontal) lines are markers of states that do not change or change very slowly, laminarity quantifies the extent of the dynamical system being trapped in any given state for some time (\citeauthor{marwan2007recurrence}, \citeyear{marwan2007recurrence}). \\
\hline
\center Average diagonal line length (L) & \center$L = \frac{\sum_{l=l_{\text{min}}}^{N}l\cdot p(l)}{\sum_{l_{\text{min}}}^{N}p(l)}$ & Average value of diagonal line length distribution, quantifying how far in time the dynamical system is predictable. \\
\hline
\center Average vertical line length (Trapping time) (TT) & \center$TT = \frac{\sum_{v=v_{\text{min}}}^{N}v\cdot p(v)}{\sum_{v_{\text{min}}}^{N}p(v)}$ & Average value of the vertical line length distribution. \\
\hline
\center Maximum diagonal line length & & Maximum value from the diagonal line distribution \\
\hline
Maximum vertical line length & & Maximum value from the vertical line distribution \\
\hline
\center Diagonal and Vertical Entropy (ENTR) & \center$ENTR = -\sum_{l=l_{\text{min}}}^{N} p(l) \ln p(l)$ & Quantifies the degree of uncertainty in the possible states and hence, the complexity of the dynamical system, using the distribution of diagonal and vertical line lengths present in the plot, respectively. \\
\hline
\caption{Measures in Recurrence Analysis}
\label{table-1}
\end{longtable}

\justifying \par The number of embedding dimensions, $\boldsymbol{m}$, required to adequately reconstruct the phase space is estimated using the false nearest neighbor approach (\citeauthor{kennel1992determining}, \citeyear{kennel1992determining}; \citeauthor{hegger1999improved}, \citeyear{hegger1999improved}). False nearest neighbors (FNN) are points in the phase space that cease to be neighbors as the embedding dimension is increased and the ratio of the distances between them at the higher dimension ($\boldsymbol{m+1}$) and the current ($\boldsymbol{m}$) becomes larger than a threshold, $\boldsymbol{r}$. An increase in FNN, for a given choice of $\boldsymbol{r}$, is indicative of a phase space that needs to be reconstructed with more dimensions to unfold it completely. To arrive at an appropriate $\boldsymbol{r}$ for comparing FNN across embedding dimensions, we first plotted the FNN ratio for different values of $\boldsymbol{r}$ as embedding dimension was increased from 1 to 10. As embedding dimensions increased, we not only found FNN ratios to hit zero at smaller $\boldsymbol{r}$ values (Kantz \& Schreiber, 2003), but the values of $\boldsymbol{r}$, at which FNN ratios hit zero, also varied less at the higher embedding dimensions indicative of a potentially unfolded phase space. We set the tolerance criteria for the difference between the $\boldsymbol{r}$ at which FNN ratio at $\boldsymbol{m+1}$th dimension and $\boldsymbol{m}$ dimension hits zero to be 0.2 and plotted the corresponding $\boldsymbol{r}$ values as a function of embedding dimension (Supplementary Fig. \ref{fig:S2}) to select the m at which $\boldsymbol{r}$ was beginning to change the least (i.e. the knee point of the $\boldsymbol{r}$ vs. m plot). 
\justifying \par Threshold radius, $\epsilon$, which decides how close two points in the phase space should be to be considered recurrent, is chosen such that the recurrence rate (percentage of recurrent points/ black dots in the recurrence plot) is constant across different multi-component systems or RPs under study. Recurrence rate was kept constant at 10\% in our case allowing RQA variables derived from different samples of a dynamical system to be directly comparable by controlling for the degree of sparseness. 
\subsection{Comparing MdRQA variables across RPs of unequal lengths }
To arrive at the best approach for comparing MdRQA variables across RPs of unequal lengths, we used two well known dynamical systems as our testing grounds: the Rossler attractor and the Kuramoto system of coupled oscillators. While in the former case, we tested whether the summary statistics of MdRQA variable distributions (across sliding widows of each RP) could correctly classify between the chaotic vs. periodic states of a sample of Rossler attractors, in the latter case, we tested whether the summary statistics of MdRQA variable distributions (across sliding windows of each RP) could correctly classify if the coupling strength of the Kuramoto oscillators in each sample of the system was greater than the critical coupling strength, $\boldsymbol{K_{c}}$, or not. In both examples, the lengths of time series were allowed to vary across samples and data simulated across nine different levels of noise. In the Kuramoto system, the number of oscillators was randomly sampled too from a range of 3 to 6 oscillators.
\subsection{Selecting the appropriate sliding window size}
Since RQA variables are statistical measures (e.g., average, max, entropy etc.) obtained from histogram distributions of vertical or horizontal lines of different lengths in the RP, it is important that the time window being examined for generating these histogram distributions be large enough to estimate the local dynamics with low variance and high statistical confidence. To select the time window that can be used to compare RPs of unequal sizes, we used a bootstrapping method (\citeauthor{marwan2013recurrence}, \citeyear{marwan2013recurrence}) with sliding windows of different lengths to first estimate the lowest sliding window size likely to give MdRQA variable estimates with high statistical confidence in a given dataset. We tested sliding windows of different sizes (10 to 500, each window shifted but just one unit in time sampled for the time series) on all simulated RPs. For each sliding window, we first generated a histogram distribution of lines (vertical \& horizontal separately) of different lengths and then summed the counts corresponding to each bin of the histogram distributions from all the sliding windows to get a unified probability distribution for that window size (Supplementary Fig. \ref{fig:S3} \& Supplementary Video \href{https://drive.google.com/file/d/1nErFo7AGeqbQa2Kh2Z8HBIPJXgKWW589/view?usp=sharing}{SV1}). Thereafter, we drew N no. of samples from the underlying probability distribution (where N is the mean no. of counts across sliding windows of a given size, i.e., total number of counts across all windows divided by the number of windows for a given RP) to estimate eight common RQA variables of interest (Table 1): diagonal and vertical entropy, average diagonal \& average vertical line, maximum diagonal \& maximum vertical line, percentage of diagonal lines (determinism) and percentage of vertical lines (laminarity). We did this 1000 times to generate a bootstrap distribution of the corresponding RQA variable. From these distributions of bootstrapped samples, we computed the difference between the 95\% quantile and 5\% quantile to get the width of the range where 90\% of the data points will be distributed. Lesser the overall width of this range, higher will be the statistical confidence associated with the RQA variable estimates. For most RQA variables, this range becomes low as window size increases (Supplementary Fig. \ref{fig:S3}). We chose the window size corresponding to the knee point in these plots (i.e., the lowest window size after which increasing its value was associated with negligible change in the width of the range of the bootstrapped distribution) suggesting that the chosen window size is large enough to yield RQA variable estimates associated with high statistical confidence. For most RQA variables, this value was between 60 \& 70.  
\justifying \par We chose a common sliding window size (size = 68) across all simulated RPs to first compute the MdRQA variables for each sliding window, resulting in a distribution of these variables across sliding windows for a given RP.  To determine which summary statistic of these distributions would represent the dynamics of the system better, we computed the mean, median and mode of the distributions from the estimates from the sliding windows, which is representative of a system, and z-transformed these summary measures from sliding window distributions at the population level, where we have such estimates from multiple samples of each system. We next ran a k-nearest neighbor (KNN) classifier with nested cross validation to test which of the following aggregate measures collectively classified the time series characteristics across all RPs in the dataset with maximum cross-validation accuracy . 
\subsection{Nested Cross Validation}
% Figure environment removed
Classifier performance was quantified using a  nested cross validation approach (Fig.\ref{fig:1}) to prevent data leakage which typically occurs when test set is exposed in some form (say, for feature selection) during the training stage, likely leading to overfitting (\citeauthor{kaufman2012leakage}, \citeyear{kaufman2012leakage}, \citeauthor{verstynen2023overfitting}, \citeyear{verstynen2023overfitting}). This is particularly important when working with a smaller sample size since the number of samples in both the training as well as the test set are going to be limited which could result in either the training sample being biased or test set not being random enough. Nested cross validation overcomes this by keeping training set used for feature selection separate from that used as validation set for quantifying classifier performance. This was achieved by having an inner cross-validation loop to select features via best subset selection and an outer cross-validation loop to report classifier cross validation accuracy on the validation set corresponding to that outer loop.

For each iteration of the outer loop, data was divided into three parts. While two-thirds of it was used as a training set, one-third was kept as held out test set. Best subset selection was carried out on the training set and an aggregate performance score computed for the selected combination of features using a 2-fold repeated stratified cross validation procedure. This inner loop feature selection procedure was run for 10 iterations and the combination of features having the best performance score was then chosen and evaluated on the original held out test set of the outer loop to yield the classifier performance accuracy following a 3-fold repeated stratified cross validation procedure. The outer loop was run over 100 iterations to construct a distribution of performance scores (cross-validation accuracy and ROC) for each dataset. The performance distributions were plotted as boxplots for each measure of central tendency of MdRQA variable distributions across the sliding windows.
\subsection{Test Bed 1: Rossler Attractor}
Rossler attractor, introduced by Otto Rössler in 1976 (\citeauthor{rossler1976equation}, \citeyear{rossler1976equation}), is a simple three-dimensional dynamical system known to exhibit periodic vs. chaotic behavior depending on the parameter values a, b and c as per the following non-linear ordinary differential equations: 

\begin{equation}
    \frac{dx}{dt} = -y -z 
\end{equation}
\begin{equation}
    \frac{dy}{dt} = x + ay
\end{equation}
\begin{equation}
    \frac{dz}{dt} = b + z(x-c)
\end{equation}

Here, $\boldsymbol{x}$, $\boldsymbol{y}$, and $\boldsymbol{z}$ represent the coordinates, $\boldsymbol{t}$ represents time, and $\boldsymbol{a}$, $\boldsymbol{b}$, and c are parameters that determine the behavior of the system. The attractor is visualized by plotting the values of $\boldsymbol{x}$, $\boldsymbol{y}$, and $\boldsymbol{z}$ over time. For a given fixed value of parameters $\boldsymbol{b}$ \& $\boldsymbol{c}$  (say, as in our case, $\boldsymbol{b}$ = 0.2, and $\boldsymbol{c}$ = 5.7) and a range of values of a, the attractor is known to show periodic behavior (for values of $\boldsymbol{a}$ = 0.01 to <0.2; Fig. 2), and chaotic behaviour (for values of $\boldsymbol{a}$ = 0.2 to 0.4; Fig. 2) (Figures \ref{fig:2} \& \ref{fig:3}). Figure \ref{fig:4} shows the RPs for periodic and chaotic attractors.
% Figure environment removed

% Figure environment removed

\subsubsection{Validation Approach 1: predicting periodic vs. chaotic state of Rossler Attractor }
We simulated the attractors under nine different levels of noise conditions (SNR or signal to noise ratio values of 0.125, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0), to test the sensitivity of classifier performance measure against gaussian noise. The choice of SNRs was arbitrary, but with the aim of including a high noise region as an extreme case. We used five values of \textbf{a} (0.1, 0.15, 0.2, 0.25, 0.3), and simulated 10 RPs for each combination of \textbf{a} and SNR with different initial values drawn from a random normal distribution. This added up to a total of 50 RPs per noise level and 450 RPs in all. The lengths of time series were randomly sampled from a uniform distribution of lengths 250-450 samples, sampled at the rate of 0.25Hz (corresponding to 1000-1800 seconds, with $dt = 10^{-4}$). Whole RPs were generated using a fixed recurrence rate (~10\%). We used the sliding window technique described earlier to obtain the distributions of nine MdRQA variables (recurrence rate, average diagonal line length, average vertical line length, percentage laminarity, percentage determinism, vertical entropy, diagonal entropy, vertical maximum line length, diagonal maximum line length; for more details see Table \ref{table-1}) across all sliding windows of each RP. The central tendencies (mean, median and mode) of the MdRQA variable distributions were run through the nested cross validation method described above separately for each noise level.

\subsubsection{Results of Validation Approach 1}
% Figure environment removed
% Figure environment removed
Figures \ref{fig:5} and \ref{fig:6} show the box plots for classifier cross validation accuracy and ROC AUC measures. We found that under very high noise levels ($SNR \leq 0.25$), the classifiers discriminating between periodic vs. chaotic state of the Rossler attractor using mean, median and mode values of the MdRQA variable distributions across sliding windows performed at chance in terms of accuracy ($\text{mean CV accuracy}_{mean,SNR\leq0.25}$ = 0.53, CI=[0.435, 0.624], $\text{mean CV accuracy}_{median,SNR\leq0.25}$=0.526, CI=[0.427, 0.624], $\text{mean CV accuracy}_{mode,SNR\leq0.25}$=0.542, CI = [0.439, 0.645]) as well as ROC AUC ($\text{mean CV ROC-AUC}_{mean,SNR\leq0.25}$ = 0.497, CI=[0.382, 0.672], $\text{mean CV ROC-AUC}_{median,SNR\leq0.25}$ = 0.494, CI=[0.377, 0.612], $\text{mean CV ROC-AUC}_{mode,SNR\leq0.25}$ = 0.516, CI = [0.389, 642]). Interestingly, under same noise levels,  the classification based on MdRQA variable from the whole RP (not using the sliding window technique) also performed at chance levels ($\text{mean CV ROC-AUC}_{whole RP,SNR\leq0.25}$ = 0.507, CI = [0.378, 0.637], $\text{mean CV accuracy}_{mean,SNR\leq0.25}$ = 0.540, CI=[0.434, 0.647] ). However, as the strength of signal increased, a steady increase in classifier performance was found across the three summary measures as well as the MdRQA variables from the whole RP, even though the latter was found to fare worse than the former, in general. To quantify this overall trend across SNRs, 
% Figure environment removed
% Figure environment removed

\begin{table} 
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Table 2} \\
 \hline
 Estimate & Slope(m) & 2.5\% quantile & 97.5\% quantile \\
 \hline
 Whole RP & 0.094 & 0.091 & 0.097 \\
 \hline 
 mean & 0.178 & 0.173 & 0.181 \\
 \hline 
 median & 0.133 & 0.128 & 0.136 \\
 \hline 
 mode & 0.118 & 0.114 & 0.121 \\
 \hline
\end{tabular}
\caption{\justifying \footnotesize \textit{Global slope of accuracy-SNR data. It was found that the mode has the least slope among all central tendency measures estimated, suggesting its low sensitivity to noise. }}
\label{table-2}
\end{table}

\begin{table} 
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Table 3} \\
 \hline
 Estimate & Slope(m) & 2.5\% quantile & 97.5\% quantile \\
 \hline
 Whole RP & 0.128 & 0.124 & 0.132 \\
 \hline 
 mean & 0.236 & 0.23 & 0.24 \\
 \hline 
 median & 0.177 & 0.171 & 0.181 \\
 \hline 
 mode & 0.165 & 0.160 & 0.168 \\
 \hline
\end{tabular}
\caption{\justifying \footnotesize \textit{Global slope of ROC AUC-SNR data. It was found that the mode has the least slope among all central tendency measures estimated, suggesting its low sensitivity to noise. }}
\label{table-3}
\end{table}

we fit a spline regression curve to the SNR-accuracy/ROC AUC data and estimated the first derivative of these curves locally to plot the change in performance measure in each of the finite interval (Figures \ref{fig:7} \& \ref{fig:8}).  We also estimated the slope of the entire distribution by fitting a line and the region where the slopes would be distributed for 95\% of the time, by computing the 2.5\% quantile and 97.5\% quantile of the slope distribution resulting from bootstrapping (1000 iterations, by doing sampling with replacement). We found the slope of classifier performance across noise levels to be the least for the mode of the MdRQA variable distributions across sliding windows (see table \ref{table-2} \& \ref{table-3}) indicating it to be the most robust in discriminating the different dynamic states of the system across varying noise levels.  On the other hand, the performance measures based on the mean of MdRQA variable distribution were most sensitive to noise levels in the data. 
\subsection{Test Bed 2: Kuramoto model}
% Figure environment removed
The Kuramoto system of coupled oscillators(\citeauthor{kuramoto1975international}, \citeyear{kuramoto1975international}; \citeauthor{strogatz2000kuramoto}, \citeyear{strogatz2000kuramoto}), famously used to model the synchronous flashing of light in populations of fireflies (\citeauthor{ermentrout1991adaptive},\citeyear{ermentrout1991adaptive}), is defined to consist of a population of N coupled phase oscillators each with a natural frequency $\omega_{i}$ sampled from a distribution $g(\omega)$. Figure \ref{fig:9} shows a schematic for a system of three coupled oscillators. At t=0, each oscillator has a phase value $\theta_{i}(0)$, which is updated over time according to partial differential equation given below: 
\begin{equation}
    \dot{\theta_{i}} = \omega_{i} + \sum_{j=1}^{N} K_{ij} \sin{(\theta_{j}-\theta{i})}, i=1, ..., N
\end{equation}
where $K_{ij}$ is the coupling strength for the edge (or the connection between) from $j^{th}$ oscillator to the $i^{th}$ oscillator. Critical coupling strength, $K_{c}$, is defined as the difference between the maximum and minimum values of natural frequencies across oscillators in the system.
\begin{equation}
    K_{c} = |\omega_{max}-\omega_{min}|
\end{equation}
 In the simplest case scenario that we considered here, known as mean field coupling, the coupling strength, $K_{ij}$, for all edges (or between any two oscillators) in the network of oscillators was assumed to be the same. $K_{ij}$ under these conditions is given by:

\begin{equation}
    K_{ij} = K/N >0, \forall i,j \in \{1,2,3,...,N\}
\end{equation}

The synchrony of the system is defined in terms of a complex value order parameter, r, obtained in our case as follows: 
\begin{equation}
    r e^{i\psi} =  \frac{1}{N} \sum_{j=1}^{N}e^{i \theta_{j}}
\end{equation}

Here $\psi$ is the average phase value. To arrive at  an expression that makes the dependence of synchrony on values of K explicit, we begin with multiplying both sides by $e^{-i \theta_{i}}$.
\begin{equation}
    r e^{i\psi} e^{-i \theta_{i}} = \left( \frac{1}{N} \sum_{j=1}^{N}e^{i \theta_{j}}\right) e^{-i \theta_{i}}
\end{equation}
\begin{equation}
    r e^{i(\psi -\theta_{i})}= \frac{1}{N} \sum_{j=1}^{N}e^{i(\theta_{j}-\theta_{i})}
\end{equation}
\begin{equation}
    r \sin{(\psi -\theta_{i})} = \frac{1}{N} \sum_{j=1}^{N}\sin{(\theta_{j}-\theta_{i})}
\end{equation}
One may note the similarity of the right hand side of the above  equation with that  of equation (11), except that the latter  was multiplied by a factor of K. Using the left hand side of the above equation(15),  equation (11) may be rewritten as follows:
\begin{equation}
    \dot{\theta_{i}} = \omega_{i} + K r \sin{(\psi -\theta_{i})}
\end{equation}
Equation 15 suggests that as $K$, tends to zero (low coupling strength), $K r \sin{(\psi -\theta_{i})}$would become negligibly small compared to $\omega_{i}$ resulting in the following approximation:
\begin{equation}
    \dot{\theta_{i}} \approx \omega_{i}
\end{equation}
\begin{equation}
    \theta_{i}(t) \approx \omega_{i} t + \theta_{i}(0)
\end{equation}
which suggests that  the oscillator would simply oscillate at  its natural frequency or in other words, the oscillations of other oscillators would have no influence on  it. On the other hand, at higher values of $K$ or when the coupling strength is tending to infinity, as per equation 15, the natural frequency would become negligible when compared to the $K r \sin{(\psi -\theta_{i})}$ term and consequently, the oscillator’s phase value would converge to the average phase value of the population. 

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\begin{equation}
    K_{c} = |\omega_{max} - \omega_{min}|
\end{equation}
\justifying \par The implication of above derivation on the phase space and recurrence plot can be illustrated using a simple scenario, say, a system of two oscillators while we examine its dynamics for three coupling strengths, say, 0.1, 1 and 10 times the critical coupling strength of the system, $K_{c}$(\citeauthor{biccari2020stochastic}, \citeyear{biccari2020stochastic}). Figure \ref{fig:10} panel A plots the case of $K = 0.1 K_{c}$ where the trajectories could be seen to revisit the phase space cyclically. Fig \ref{fig:10} shows how this cyclical trajectory translates to a RP with multiple symmetrically spaced, parallel diagonal lines.  However, unlike the Rossler attractor, which would either show a periodic or chaotic behaviour, the Kuramoto system with two oscillators  always shows periodic behaviour at coupling strength lesser than the critical coupling strength (Fig. \ref{fig:10}; \citeauthor{acebron2005kuramoto}, \citeyear{acebron2005kuramoto}; \citeauthor{chopra2009exponential}, \citeyear{chopra2009exponential}; \citeauthor{biccari2020stochastic}, \citeyear{biccari2020stochastic}). 

At critical coupling strength ($K = K_{c}$, Fig. \ref{fig:11}), the eccentricity of the trajectory in the phase space  can be found to increase and take the form of an ellipse. In such a scenario, some points in the trajectory  would be closer  than others, resulting in the RP a  pattern of lines that are somewhat broken but periodic and parallel to the main diagonal  (Fig. \ref{fig:11}, panel C). 

At a coupling strength greater than the critical coupling strength (say, $K = 10K_{c}$, Figure \ref{fig:12}), while the RP continues to look similar, the trajectory takes the shape of a narrower ellipse. As  K tends to infinity (or a very high value) the trajectory looks almost like a line and the oscillators  are said to be phase locked or in sync. The resulting RP is shown in Fig. \ref{fig:13}.
% Figure environment removed
To graphically illustrate how the synchrony of this two oscillator Kuramoto system (given by the order parameter as per Equation 12) is related to the coupling strength of its oscillators, we varied the latter as a function of critical coupling strength by multiplying it with powers of two (100 values at regular intervals, from $2^{-5}$ to $2^{5}$, denoted by the color bar) and plotted the resulting temporal evolution of the order variable, where the log is having 2 as its base. As can be seen in Fig. \ref{fig:14}, while the order parameter fluctuates more at lower coupling strengths, they seem to saturate at higher coupling strengths, consistent with equation 16 and the discussion following it.
% Figure environment removed

\justifying \par As we increase the number of oscillators to 3   and varied the coupling strength as a function of critical coupling strength (by multiplying it with powers of 10 from $10^{-2}$ to $10^{2}$), we find that while at low coupling strengths, the RP is constituted by solid or broken lines parallel to the main diagonal (Fig. \ref{fig:15}), at coupling strengths greater than the critical coupling strength, the broken lines parallel to the diagonal lines become more prominent. These results are consistent with the proposed idea of using recurrence plots to distinguish between a dynamical system having coupling strength lesser vs. greater than the critical coupling strength.
\subsubsection{Validation Approach 2: Predicting whether the coupling strength is greater than that of the critical coupling strength in the mean-field model of the Kuramoto system}
\justifying \par We used the Kuramoto system of coupled oscillators to test the ability of the sliding window based approach in predicting whether the system is strongly coupled ($K>K_{c}$) or not ($K\leq K_{c}$). The lengths of time series were randomly sampled from a uniform distribution of lengths 150-450 samples, sampled at the rate of 10Hz (corresponding to 15-45 seconds, with $dt = 10^{-2}$). In addition, the number of oscillators was randomly sampled from a range of 3 to 6. Coupling strength was randomly sampled from a uniform distribution $[0, 2K_{c}]$, such that the probability of picking a coupling strength less than $K_{c}$ is equal to that of picking a coupling strength greater than $K_{c}$. To test how our approach fares across a range of gaussian noise added to the signal, we simulated data for nine values of signal to noise ratios (0.125, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0). Then, as in case of the earlier example of Rossler Attractors, we constructed RPs and computed the summary statistics of the MdRQA variables across sliding windows, z-transformed them before running the nested cross validation procedure for classifier performance.
\subsubsection{Results of Validation Approach 2}
% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed
\justifying \par Figures \ref{fig:16} \& \ref{fig:17} plot the boxplots for accuracies and ROC AUC scores obtained from nested cross validation procedure. As in case of the Rossler attractor example above, we fit smoothened spline regression, and computed the first derivative of these spline curves to quantify the local measures of sensitivity to noise levels. We also estimated the linear slope for the entire distribution (Figures \ref{fig:18} \& \ref{fig:19}) as an overall measure of sensitivity.  

As in case of the Rossler Attractors, we found that under high noise levels ($SNR\leq0.5$) classification using all measures of central tendency and that from the whole RP performed were at chance level, both in terms of accuracy and ROC AUC. At lower SNRs (0.5 to 1.5) a high overlap for the 0.025-0.975 confidence regions of the spline curve is observed for the performance measures obtained from the whole RP, mean and median, indicating an almost identical trend. While at the peak (SNR value of 3), the whole RP performs much better than any other summary statistic, using the overall trend quantified by the linear fits (Figures \ref{fig:18} \& \ref{fig:19}),  the slope for mode was found to be significantly lower than that obtained from other measures suggesting  that the mode is least sensitive to the changes in noise levels, when it comes to performance.

\begin{table} 
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Table 4} \\
 \hline
 Estimate & Slope(m) & 2.5\% quantile & 97.5\% quantile \\
 \hline
 Whole RP & 0.09 & 0.086 & 0.092 \\
 \hline 
 mean & 0.072 & 0.068 & 0.075 \\
 \hline 
 median & 0.072 & 0.068 & 0.075 \\
 \hline 
 mode & 0.057 & 0.055 & 0.06 \\
 \hline
\end{tabular}
\caption{\justifying \footnotesize \textit{Global slope of accuracy-SNR data. Among the central tendency measures, mode has the least slope, indicating least sensitivity to the variations in noise level.}}
\label{table-4}
\end{table}

\begin{table} 
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Table 5} \\
 \hline
 Estimate & Slope(m) & 2.5\% quantile & 97.5\% quantile \\
 \hline
 Whole RP & 0.103 & 0.099 & 0.107 \\
 \hline 
 mean & 0.090 & 0.086 & 0.094 \\
 \hline 
 median & 0.093 & 0.088 & 0.181 \\
 \hline 
 mode & 0.077 & 0.074 & 0.08 \\
 \hline
\end{tabular}
\caption{\justifying \footnotesize \textit{Global slope of ROC AUC-SNR data. It was found that the mode has the least slope among all central tendency measures estimated, suggesting its low sensitivity to noise. }}
\label{table-5}
\end{table}
\section{Discussion}
\justifying \par In recent years, RQA and its multidimensional version (MdRQA) have emerged as a popular tool for assessing interpersonal behavioral or physiological synchrony owing to their general robustness to non-linearity in data. While experimental data in such studies are typically collected for a fixed, pre-determined duration, a systematic attempt validating its use in computing and comparing synchrony between samples of non-uniform composition and unequal time series data has been lacking. In this study, we proposed a  sliding window MdRQA approach to address this and tested the ability of our approach in discriminating between two dynamic states of a complex system, when the time series data from samples varied in composition and/ or duration. We used two well-known dynamical systems as our testbed: the Rossler Attractor and the Kuramoto system of coupled oscillators. 

In the case of Rossler Attractors, we found that under conditions of variable RP durations (150-450 equivalent to 600-1800s) and low signal to noise ratios (at $SNR \geq1$), all three measures of central tendency (mean, mode and median) of the MdRQA variable distributions across sliding windows of a RP, could discriminate between the chaotic vs. periodic state of the attractor. Moreover, while the classifier performance measures increased with SNR values, the mode of the MdRQA variable distributions (when compared to the mean or median) varied the least for the same amount of change in SNR. This suggests that the mode of the MdRQA variable distributions could be a conservative but robust measure of classifier performance, when the noise levels are unknown, as is usually the case in most empirical studies. Since the number of dimensions is fixed in the Rossler system,  we used the Kuramoto system of coupled oscillators to examine the efficacy of our approach against a varying number of observable variables (components or dimensions). Under varying levels of noise, RP durations (150-450) and number of oscillators (3-6), we again found central tendencies of the sliding window distributions to be able to classify whether the coupled oscillators would be in sync or not, based on the coupling stren. Mode was again found to be least sensitive to levels of noise in the signal. 

\justifying \par While in case of the Rossler attractor, mode of the distributions performed at or above chance (but always better than the whole RP estimates) , in the Kuramoto system, all measures of central tendency  had equivalent discriminative accuracies in the high noise condition ($SNR \leq0.5$) even though mode was still the least sensitive one to noise. It is unclear why mode had the least slope in both the systems we examined in this study. It  may even be possible that for another system, the level of accuracy at which the mode performs (even if better than other measures), is too low to be practically desirable.  However, it still may be a measure of choice when opting for a conservative criterion. In other words, while we caution against concluding that mode is universally the best choice for all systems, it still may be a safer option when one is uncertain about the noise levels in the system.
\justifying \par The current study is limited to testing the new approach to the simple case scenarios of two widely studied dynamical systems. Moreover, the classifcation looked at here was a binary one. Hence, the usefulness  of our approach to scenarios where there could be more than two ways of classifying the dynamics of the system remains to be tested and remains as a limitation of the study. For example, the Kuramoto system of coupled oscillators was chosen in its most simplified form (the mean field model) to validate our approach even though  real-world data could be expected to exhibit a much richer dynamics say, from systems with multiple interacting, inter-dependent components. In fact, it would be possible to incorporate varying levels of noise to the extent of coupling between any two oscillators to capture this gradient of inter-dependency expected in real world systems and then test the usefulness of the approach and generalizability of the inferences drawn.
\justifying \par Lastly, the noise  that was introduced in both cases under study was Gaussian white noise which is  only one type of noise that could be expected in a system. In addition, the SNR range  was arbitrarily chosen to elicit a range of classifier performance across noise levels In the absence of a  functional form to the relationship between the SNR and performance measure,  the inferences drawn may be limited to the range of noise levels we tested. However, despite these limitations, we believe that this work, as a first attempt, allows  us to draw some useful insights, paving the way for future work that examines interpersonal synchrony in more naturalistic, ecologically valid contexts.  

\section{Conclusion}
In this study, we proposed a sliding window MdRQA approach to compute and compare synchrony across multi-dimensional samples of unequal duration and composition. We tested and validated this approach in two widely studied non-linear dynamical systems and found that measures of central tendency of the MdRQA variable distributions across sliding windows could classify the dynamic states of a pool of systems with a cross validation accuracy significantly greater than chance. In addition, compared to other measures of central tendency, mode was found to be the most robust to changes in noise levels in the simulated data. 

\section{Acknowledgments}
We thank the Remote Computing Facility at the Department of Cognitive Science, IIT Kanpur for their support.


%%%%% Citations in the text %%%%%%


%%%%%% Equations %%%%%%

%\addcontentsline{toc}{chapter}{\numberline{}References} % Adds 'References' to the TOC

%\bibliographystyle{plain}
\printbibliography
%\end{document}
%\begin{document}

\appendix*% Start on a new page

\justifying 
\newpage
\part*{Supplementary Information}
\addcontentsline{toc}{part}{Supplementary Information}
\thispagestyle{empty} % Remove page number from the title page

\title{Supplementary Information}
\section{Perturbation Analysis For Rossler Attractor}
\renewcommand{\theequation}{SA.\arabic{equation}}
\justifying \par In addition to numerical estimation, a perturbation analysis was carried out to confirm the bifurcation of a Rossler Attractor at $a = 0.2$. A perturbation is a small change in the initial conditions which if grows exponentially over time, the system is said to be chaotic. 

The time series data was simulated using 30 different initial states and perturbed by the same small amount (0.001 unit) in any of the three dimensions randomly. The perturbation was sampled as the surface of a sphere (with radius, $r (=0.001)$) whose center was taken to be the original initial point. The coordinates of a point on the surface of a sphere would have the form:
\begin{align}
\begin{split}
x &= x_{0} +  r \cdot \sin(\theta) \cdot \cos(\phi) \\
y &= y_{0} +  r \cdot \sin(\theta) \cdot \sin(\phi) \\
z &= z_{0} +  r \cdot \cos(\theta)
\end{split}
\end{align}
where:
\begin{itemize}
    \item $r$ is the radial distance from the origin to a point on the sphere,
    \item $\theta$ is the polar angle (angle between the positive $z$-axis and the line connecting the origin to the point),
    \item $\phi$ is the azimuthal angle measured from the positive $x$-axis in the $xy$-plane.
    \item $x_{0}$, $y_{0}$ \& $z_{0}$ are the initial state coordinates of the actual time series(which wasn't parturbated)
\end{itemize}

The values of $\theta$ and $\phi$ were randomly sampled from a uniform distribution between 0 and $2\pi$.

Following a small change in the initial state (known as perturbation), the Euclidean distance between states for the same initial time point was computed across the length of the time series. A sensitive dependence on initial condition is a characteristic of chaotic systems. A exponential growth would result in a positive Lyapnov exponent (\citeauthor{strogatz2018nonlinear}, \citeyear{strogatz2018nonlinear}, section 10.5), on which when a logarithm is applied, a linear region is obtained. Fig. \ref{fig:3} for $a>=0.2$  showing that for values in that range, the system is indeed showing chaotic behaviour. 
\begin{suppfigure}
    \centering
    % Figure removed
    \caption{\justifying \footnotesize \textit{Log of distance between points on actual time series and perturbed time series at each time point for the phase space trajectories plotted in a, b, c \& d above. This is done by defining a sphere having radius the amount of perturbation around the initial point of the time series and putting points on the surface of the sphere as initial points for the perturbed time series. For chaotic systems, this distance, that we introduced initially would grow exponentially over time, resulting in a linear increase in the log graph (positive Lyapnov exponent). But, for periodic systems, this distance may reduce or stay the same. This graph verifies that the system was showing chaotic behaviour for $a\geq0.2$}}
    \label{fig:3}
\end{suppfigure}


\section{Time Delayed Embedding}\label{section:tde}
\renewcommand{\theequation}{SB.\arabic{equation}}

\citeauthor{takens1981dynamical}(\citeyear{takens1981dynamical}) had proved that for a complex system, if we have access to only one variable, the whole dynamics of the system could be reconstructed by plotting the same variable against it for a certain number of times with a certain time delay. 


Let the observable time series be x, given by: 

\begin{equation}
x= ( x_{1}, x_{2}, x_{3}, ... , x_{n})
\end{equation}
Now using time delayed embedding, if we assume that the embedding dimension is m, we can construct the multidimensional vector for a point as follows:
\begin{equation}
\vec{x}_{1} = ( x_{1} , x_{1+\tau}, x_{1+2\tau}, x_{1+3\tau}, ... , x_{1+(m-1)\tau}) 
\end{equation}
now we can repeat the process by starting the next vector from $x_{2}$ and we can continue until $x_{n-(m-1)\tau}$ as follows: 
\begin{equation}
\bar{X}=
\begin{pmatrix}
     \text{ $\vec{x}_{1}$}\\
     \text{ $\vec{x}_{2}$}\\
     \text{ $\vec{x}_{3}$}\\
     \text{ $.$}\\
     \text{ $.$}\\
     \text{ $.$}\\
     \text{$\vec{x}_{n-(m-1)\tau}$}
\end{pmatrix}
= 
\begin{pmatrix}
\text{$x_{1} , x_{1+\tau}, x_{1+2\tau}, x_{1+3\tau}, ... , x_{1+(m-1)\tau}$} \\
\text{$x_{2} , x_{2+\tau}, x_{2+2\tau}, x_{2+3\tau}, ... , x_{2+(m-1)\tau}$} \\
\text{$x_{3} , x_{3+\tau}, x_{3+2\tau}, x_{3+3\tau}, ... , x_{3+(m-1)\tau}$} \\
\text{ $.$}\\
\text{ $.$}\\
\text{ $.$}\\
\text{$x_{n-(m-1)\tau} , x_{n-(m-2)\tau}, x_{n-(m-3)\tau}, x_{n-(m-4)\tau}, ... , x_{n}$} \\
\end{pmatrix}
\end{equation}

In our case the time series itself was multidimensional, which could be written as:
\begin{equation}
\vec{x}=
\begin{pmatrix}
\text{$x_{1,1}, x_{1,2}, x_{1,3}, ..., x_{1,N}$} \\
\text{$x_{2,1}, x_{2,2}, x_{2,3}, ..., x_{2,N}$} \\
\text{$x_{3,1}, x_{3,2}, x_{3,3}, ..., x_{3,N}$} \\
\text{ $.$}\\
\text{ $.$}\\
\text{ $.$}\\
\text{$x_{n,1}, x_{n,2}, x_{n,3}, ..., x_{n,N}$} \\
\end{pmatrix}
\end{equation}
Where N is the number of members in the group and n is the length of the time series.
Then the multidimensional vector for a point would be: 
\begin{equation}
\vec{x}_{1}= 
\begin{pmatrix}
\text{$\left(x_{1,1}, x_{1,2}, ..., x_{1,N}\right),  \left( x_{1+\tau,1}, x_{1+\tau,2}, ..., x_{1+\tau,N}\right), ... \left(x_{1+(m-1)\tau,1}, x_{1+(m-1)\tau,2}, ..., x_{1+(m-1)\tau,N}\right)$}
\end{pmatrix}
\end{equation}
Then the reconstruction would look like
\begin{equation}
\vec{X}_{1}= 
\begin{pmatrix}
\text{$\left(x_{1,1}, x_{1,2}, ..., x_{1,N}\right),  \left( x_{1+\tau,1}, x_{1+\tau,2}, ..., x_{1+\tau,N}\right), ... \left(x_{1+(m-1)\tau,1}, x_{1+(m-1)\tau,2}, ..., x_{1+(m-1)\tau,N}\right)$}\\
\text{$\left(x_{2,1}, x_{2,2}, ..., x_{2,N}\right),  \left( x_{2+\tau,1}, x_{2+\tau,2}, ..., x_{2+\tau,N}\right), ... \left(x_{2+(m-1)\tau,1}, x_{2+(m-1)\tau,2}, ..., x_{2+(m-1)\tau,N}\right)$}\\
\text{$\left(x_{3,1}, x_{3,2}, ..., x_{3,N}\right),  \left( x_{3+\tau,1}, x_{3+\tau,2}, ..., x_{3+\tau,N}\right), ... \left(x_{3+(m-1)\tau,1}, x_{3+(m-1)\tau,2}, ..., x_{3+(m-1)\tau,N}\right)$}\\
\text{ $.$}\\
\text{ $.$}\\
\text{ $.$}\\
\text{$\left(x_{n-(m-1)\tau,1}, x_{n-(m-1)\tau,2}, ..., x_{n-(m-1)\tau,N}\right),  \left( x_{n-(m-2)\tau,1}, x_{n-(m-2)\tau,2}, ..., x_{n-(m-2)\tau,N}\right), ... \left(x_{n,1}, x_{n,2}, ..., x_{n,N}\right)$}
\end{pmatrix}
\end{equation}
\subsubsection{Time Delay($\tau$)}\label{section:time-delay}
For practical purpose it is important to compute the appropriate value of the the delay($\tau$) in the first place. For this we had a multidimensional time series in which we computed a multidimensional mutual information and used it's first minima(and global minima, in case the first minima doesn't exist) in a plot between time delay and mutual information. 


Let the time series be $x_{n}$ having length N
The time delayed versions are given by
\begin{equation}
x_{n}^{(0)}= x_{n}[1: N-\tau]
\end{equation}
\begin{equation}
x_{n}^{(\tau)}= x_{n}[\tau: N]
\end{equation}
Let the probability distribution functions be $P(x_{n}^{(0)})$, $P(x_{n}^{(\tau)})$ and the joint probability function be $P(x_{n}^{(0)},x_{n}^{(\tau)})$
The entropy measures for each of the signals are given by:
\begin{equation}
H(x_{n}^{(0)})= \sum_{i=1}^{N-\tau} P(x_{n,i}^{(0)}) log(P(x_{n,i}^{(0)}))
\end{equation}

\begin{equation}
H(x_{n}^{(\tau)})= \sum_{i=1}^{N-\tau} P(x_{n,i}^{(\tau)}) log(P(x_{n,i}^{(\tau)}))
\end{equation}
Now let's consider conditional entropy between the two time series. For simplicity now on we would assign $X=x_{n}^{(0)}$ and $Y=x_{n}^{(\tau)}$, and x, y be the individual points which is in the probability distribution functions
\begin{equation}
H(Y|X)= \sum_{x \in X} \sum_{y \in Y} P(x,y)log(\frac{P(x,y)}{P(x)}) = 
\sum_{x \in X} \sum_{y \in Y} P(x,y)[log(P(x,y)-log(P(x))] 
\end{equation}
This gives: 
\begin{equation}
H(Y|X)=\sum_{x \in X} \sum_{y \in Y} P(x,y)log(P(x,y)-\sum_{x \in X} \sum_{y \in Y} P(x,y)log(P(x))
\end{equation}
Second part of LHS would become
\begin{equation}
\sum_{x \in X} \sum_{y \in Y} P(x,y)log({P(x)})= \sum_{x \in X} P(x)log({P(x)})= H(X)
\end{equation}
and first part is:
\begin{equation}
\sum_{x \in X} \sum_{y \in Y} P(x,y)log(P(x,y) = H(X,Y)
\end{equation}
Which gives us
\begin{equation}
H(Y|X)=H(X,Y)-H(X)
\end{equation}
Now let's consider equation for mutual information:
\begin{equation}
I(X,Y)= H(Y) - H(Y|X)
\end{equation}
Using (14) we have
\begin{equation}
I(X,Y)= H(Y) - [H(X,Y) - H(X)] = H(X)+H(Y) - H(X,Y)
\end{equation}
Which we used to compute mutual information. But, we were using multidimensional histogram function(histogramdd function in numpy library) which may encounter overflow error as the number of bins itself can go beyond the maximum number that can get represented in a 64bit binary system. Most Windows systems would encounter this issue, hence it could be difficult as a method(one would simply get a large negative value instead of a positive value). One design choice would be to reduce the number of participants in the group(which attributes to dimensions). We ran the code in a Linux server where it is possible to use 128 bits format to overcome the ‘overflow’ error that one may encounter in typical system configuration on a Windows OS. We used ASUS ESC4000 G3 machine running on Ubuntu 20.04 with Intel Xeon Processor E5-2620 v4(32(8 core), 20M cache, 2.10GHz, turbo upto 3.0GHz), with 128GB DDR4 RAM). 


\subsubsection{Embedding Dimension(m)}
For computing the embedding dimension we computed the concept of false nearest neighbours(FNN)(\citeauthor{kennel1992determining}, \citeyear{kennel1992determining}). This method is used to determine the minimum number of false embedding dimensions required to properly reconstruct the attractor. The idea being a properly unfolded attractor would have a large number of false nearest neighbours. These points are those which appear together due to trajectory crossing when we project to smaller dimension. Thus as we increases the embedding dimension those points won't remain as neighbours, suggesting that the embedding dimension we had chosen is not sufficiently unfolding the attractor. In other words the points would be neighbours in the current embedding dimension but, when it is increased they won't as their future temporal dynamics would be too different. 


In simple terms, suppose the correct embedding dimension for some time series is $m_{0}$, then when we are eliminating one dimension, these points will get strongly affected by such elimination will become FNN. For this we took the closest neighbours of points in the m dimension and computed the distance and done the same in m+1 dimension and then took the ratio, as given below:
\begin{equation}
X_{fnn}(r)= \frac{\sum_{n=1}^{N-m-1} \Theta(\frac{|x_n^{(m+1)}-x_{k(n)}^{(m+1)}|}{|x_n^{(m)}-x_{k(n)}^{(m)}|}-r) \Theta(\frac{\sigma}{r}-|x_n^{(m)}-x_{k(n)}^{(m)}|)}{\sum_{n=1}^{N-m-1}\Theta(\frac{\sigma}{r}-|x_n^{(m)}-x_{k(n)}^{(m)}|)}
\end{equation}

Where $x_{k(n)}^{(m)}$ is the closest neighbour of $x_n^{(m)}$ in m dimensions, and $\Theta(x)$ is the Heaviside step function. $\sigma$ is the standard deviation of the data. The function k(n) is given as:
\begin{equation}
k(n)= \{n' \mid |x_n^{(m)}-x_{n'}^{(m)}|\leq |x_n^{(m)}-x_{n''}^{(m)}|, \forall n''\in I(x)-n\}
\end{equation}
where I(x) is the set of all indices of x. $\Theta$ denotes step function. A more appropriate method would be to see the FNN ratio as a function of r (see \citeauthor{kantz2004nonlinear}(\citeyear{kantz2004nonlinear}), section 3.3.1, page 37, figure 3.3))


here we are seeing the FNN ratio as a function of r from m=1(top curve) to m=5(bottom). From this one can see that there would be a saturating curve specially where the FNN is hitting zero. For this we defined the radius at which FNN hist zero as 
\begin{equation}
r_{0}(m)= \{r \mid X_{fnn}(r)<\delta, \forall r\in [r_{min},r_{max}]\}
\end{equation}
Where $[r_{min},r_{max}]$ is the interval where we are searching, which is for m embedding dimensions, and $\delta$ is a small enough number. When we do this for different embedding dimensions we will get a graph like the following: 
\begin{suppfigure} 
    \centering
    % Figure removed
    \caption{\justifying \footnotesize \textit{$r$ at which FNN hits zero plotted against embedding dimension, we can see that, as we increases the embedding dimension, the FNN hits zero for smaller values of $r$}}
    \label{fig:S2}
\end{suppfigure}

based on this we defined another function, which is as follows: 
\begin{equation}
m=\{max(m') \mid r_{0}(m'-1)-r_{0}(m') \geq \beta \}
\end{equation}

Where if you notice the graph, $\beta$ is a tolerance criteria to stop. In simple words, we will start from the maximum embedding dimension and compare it to the dimension which lack one axis in terms of radius at which FNN is hitting zero and do it until the difference is greater than a bound denoted by $\beta$(=0.2 is what we have chosen). In other words this is a form of exploration to find appropriate embedding dimension. 

\subsubsection{Proof For Volume Shrinking}

At this point the reader may have a doubt about why the drop in FNN is faster in the higher dimension. For that we need to understand that the behaviour of measures and metric changes in the higher dimension. And it is important to notice that we are using Euclidian distance for finding the neighbours. In this was one will be able to imagine a spherical region around a point which could be defined as a neighbourhood. Then it is simple enough to look about what actually happens to a hypersphere as we increase the dimension. For simplicity, let's consider a hypersphere in N dimension whose center is in the origin and is defined as: 
\begin{equation}
\sum_{i=1}^{N} x_{i}^{2} =r^{2}
\end{equation}
We can get the volume of hypersphere at Nth dimension by integrating the volume of the hypersphere at N-1 dimension as follows
\begin{equation}
V_{N}(r) = \int_{-r}^{r} V_{N-1} (\sqrt{r^{2}-x^{2}}) dx
\end{equation}
We will show the effect of increase in dimension on the hypersphere volume by generalizing using a few initial examples

Case 1 : In One Dimension

It's a line, hence $V_{1} = 2r$

Case 2: In two dimensions
\begin{equation}
V_{2}(r) = \int_{-r}^{r} V_{1} (\sqrt{r^{2}-x^{2}}) dx = \int_{-r}^{r} 2r (\sqrt{r^{2}-x^{2}}) dx = 2  \int_{-r}^{r}  2(\sqrt{1-(\frac{x^{2}}{r^{2}})}) dx
\ = 2r^{2} \int_{-r}^{r}  (\sqrt{1-(\frac{x}{r}})^{2}) d(\frac{x}{r})
\end{equation}
Given $ \frac{x}{r} = sin(\theta)$ the equation now becomes
\begin{equation}
V_{2}(r)= 2 r^{2} \int_{\frac{-\pi}{2}}^{\frac{\pi}{2}} \sqrt{ 1 - sin(\theta)^{2}} d(sin(\theta))
= 2 r^{2} \int_{\frac{-\pi}{2}}^{\frac{\pi}{2}} cos(\theta)^{2} d(\theta) = \pi r^{2}
\end{equation}

and for higher dimensions
N=3, $V_{3} = \frac{4}{3} \pi r^{3}$
N=4, $V_{4} = \frac{\pi^{2}}{2} r^{4}$

now we have seen
\begin{equation}
V_{N}= k_{N} r^{N}
\end{equation}
Let's proceed to higher dimensions Here note that $k_{N}$ is a variable independent of r. 


Now let's consider the volume of hypershpere in Nth dimension in terms of that in N-1th dimension. 

\begin{equation}
V_{N}(r) = \int_{-r}^{r} V_{N-1} (\sqrt{r^{2}-x^{2}}) dx = K_{N-1} \int_{-r}^{r} (r^{2}-x^{2})^{\frac{N-1}{2}} dx = K_{N-1} r^{N} \int_{-r}^{r} (1-(\frac{x}{r})^{2})^{\frac{N-1}{2}}d(\frac{x}{r})
\end{equation}
From this, using trignometry, taking $\frac{x}{r}=sin(\theta)$ we would get: 
\begin{equation}
V_{N}(r) = K_{N-1} r^{N} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} cos(\theta)^{N-1} cos(\theta) d(\theta) = K_{N-1} r^{N} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} cos(\theta)^{N} d(\theta) 
\end{equation}

From this we can get
\begin{equation}
K_{N} = K_{N-1} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} cos(\theta)^{N} d(\theta) = K_{N-1} C_{N}
\end{equation}

This actually leads to a recursive equation if you elaborate $K_{N-1}$
\begin{equation}
K_{N}= C_{N} K_{N-1}= C_{N} C_{N-1} K_{N-2}= C_{N} C_{N-1} C_{N-2} K_{N-3}
\end{equation}
This countinues until N=1, in short we can write the equation as follows
\begin{equation}
K_{N} = K_{1} \prod_{i=2}^{N} C_{i}
\end{equation}

We have 
\begin{equation}
C_{N}= \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} cos(\theta)^{N} d(\theta) = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} cos(\theta)^{2} cos(\theta)^{N-2} d(\theta) =\frac{N-1}{N} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}  cos(\theta)^{N-2} d(\theta)= \frac{N-1}{N} C_{N-2}
\end{equation}

similarly we would get 
\begin{equation}
C_{N-1} = \frac{N-2}{N-1} C_{N-3}
\end{equation}
From this
\begin{equation}
C_{N} C_{N-1} = \frac{N-1}{N} C_{N-2} \frac{N-2}{N-1} C_{N-3} = \frac{N-2}{N} C_{N-2} C_{N-3}
\end{equation}

Again this is recursive, if we continue this, we would get: 
\begin{equation}
C_{N} C_{N-1} = \frac{N-2}{N} C_{N-2} C_{N-3}= \frac{N-2}{N}  \frac{N-4}{N} C_{N-4} C_{N-5}
\end{equation}
Now, if we continue this we would get equations as follows:
\begin{equation}
C_{N} C_{N-1}=
\begin{cases}
     \text{ $\frac{2}{N} C_{2}C_{1}$ if $N$ is even}\\
     \text{$\frac{1}{N} C_{0}C_{1}$ if $N$ is odd}\\
\end{cases}
\end{equation}

We know the values of $C_{0}$, $C_{1}$, and $C_(2)$, subtitute that in the equation
\begin{equation}
C_{N} C_{N-1}=
\begin{cases}
     \text{ $\frac{2 \pi}{N}$ if $N$ is even}\\
     \text{ $\frac{2 \pi}{N}$ if $N$ is odd}\\
\end{cases}
=\frac{2 \pi}{N}
\end{equation}
We already have 
\begin{equation}
K_{N} = K_{1} \prod_{i=2}^{N} C_{i}
\end{equation}
and 
\begin{equation}
K_{N}= 2 \prod_{i=2}^{N} C_{i}= 
\begin{cases}
     \text{ $2 \frac{2 \pi}{N} \frac{2 \pi}{N-2} ... \frac{2 \pi}{4} C_{2}$ if $N$ is even}\\
     \text{ $2 \frac{2 \pi}{N} \frac{2 \pi}{N-2} ... \frac{2 \pi}{3} $ if $N$ is odd}\\
\end{cases} \\
= 
\begin{cases}
     \text{ $2 \frac{2 \pi}{N} \frac{2 \pi}{N-2} ... \frac{2 \pi}{4} \pi$ if $N$ is even}\\
     \text{ $2 \frac{2 \pi}{N} \frac{2 \pi}{N-2} ... \frac{2 \pi}{3} $ if $N$ is odd}\\
\end{cases}
\end{equation}



From this equation it is easy to see that for a sphere having radius r, as the number of dimension increases the volume decreases. If we take limit of N tending to infinity the term $K_{N}$ and effectively the volume would become zero. 


This is the reason why as we increase the radius in higher dimension the false neighbour ratio falls quickly. If we look carefully at the equation for FNN we can notice that it is checking whether the ratio between the distance between two points in Nth and N-1th dimension is higher than a factor(r). But, when you think carefully about the volume shrinking, we can see that at higher dimension, everyone would become neighbour of another one, as the volume of any sphere will shrink towards zero. Hence, the drop would be steeper. Simply it means that your neighbourhood is getting sparse so that all points from you appear as if they are at the same distance from you and you won't be able to sample the entire volume. This is also the reason why selecting an appropriate embedding dimension is important, as a very high dimension will effectively make all points in the phase space neighbours of each other. 
Implication of this concept is same here also. When you go towards higher dimension, due to the volume shrinking, the hypersphere defining your neighbourhood will also shrink. This results in each other point being at an infinite distance from one point, or more correctly at equal distance. The differences between points in terms of neighbourhood will be hence less observable at higher dimension as any sphere defining neighbourhood will converge into a zero volume sphere. This is more commonly referred to as curse of dimensionality in the field of data science or machine learning. 

\section{Selecting Appropriate Window Size}
\renewcommand{\theequation}{SC.\arabic{equation}}

Recurrent plots are all those points in the trajectory such that they are within a neighbourhood, defined by radius $\epsilon$. In simple terms, let $S_{n}$ be the set of all points in the phase space, then the recurrent plot is given by
\begin{equation}
RP(i,j)= \Theta(\epsilon - || \bar{X}_{i} - \bar{X}_{j}||) 
\end{equation}

Here it is important to note that the selection of appropriate m, $\tau$ and $\epsilon$ is important here. We choose epsilon by setting the recurrence rate to 10 percent independently on each RP. 


Since we have differently sized RPs we cannot compare them directly. For this we used sliding windows with step size=1 on each RP and computed representative statistics(mean, mode and median) from these. But, the window size we have choosen should be large enough for capturing the nonlinearity. For this we followed a bootstrapping method suggested in \citeauthor{marwan2013recurrence}(\citeyear{marwan2013recurrence}). Let's say we have N number of windows obtained using the sliding window approach. Each of these windows will have histogram distribution of line lengths, and each window repesent dynamics at a time point. Let these histogram distributions at t as a function of line length be given by $P_{t}(l)$. Then a unified distribution can be obtained as follows:
\begin{equation}
P(l)= \sum_{t} P_{t}(l)
\end{equation}
The average number of drawings from these histogram distribution will be given as 
\begin{equation}
\overline{n}= \frac{1}{N_{windows}} \sum_{t}\sum_{l} P_{t}(l)
\end{equation}
Where the function $P_{t}(l)$ is actually giving the counts, and $N_{windows}$ is the total number of windows. We will now sample from the histogram distribution for getting $\overline{n}$ number of samples. The probability distribution function is given by:
\begin{equation}
PDF(l)= \frac{P(l)}{\sum_{l} P(l)}
\end{equation}
From this we will compute the cumulative distribution function:
\begin{equation}
CDF(l)= \sum_{i=1}^{l} PDF(i)
\end{equation}
Now we will random sample from this distribution by randomly drawing a number from uniform distribution [0,1] and determining to which line length that number belongs to in terms of CDF.
Let the sample that we are drawing be $I_{i}$
\begin{equation}
I_{i}=r.v(U(0,1))
\end{equation}

where r.v means random variable and U(0,1) is uniform distribution between 0 and 1. 
Then we can apply the inverse of CDF function to get the line length that is corresponding to that particular CDF value
\begin{equation}
I_{i}=CDF(l_{i}) \implies l_{i}=CDF^{-1}(I_{i})
\end{equation}
We repeat this $\overline{n}$ times to get one sample in the bootstrapping. 
\begin{equation}
l=\{ l_{1}, l_{2}, l_{3}, ... , l_{\overline{n}} \}
\end{equation}
We will compute quantitative statistics such as mean lengths, determinism and entropies from this set. and that constitutes one bootstrapping sample. 

We will repeat this sampling 1000 times to construct a distribution of nonlinear measurements for each different window size. From that we computed the difference between 95 percentile quantile and 5 percent quantile as a measure of variance, and found that the decrease is pretty low so between window size 60 and 70, and based on a tolerance criteria we pick window size 68. Then we computed RQA variables using a sliding window having size 68. Provided below is the graph showing the decrease for percentage determinism. 




\begin{suppfigure} 
    \centering
    % Figure removed
    \caption{\justifying \footnotesize \textit{The difference between 95th quantile and 5th quantile, plotted against the window size.}}
    \label{fig:S3}
\end{suppfigure}


\section{Nested Cross Validation}
\renewcommand{\theequation}{SD.\arabic{equation}}
\begin{algorithm}[H]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{Dataset, Model information, total set of features \& iterations}
\Output{Set of best features, list of performance scores on validation set}


    \For{each fold in the outer loop}{
        Split data into training and validation sets\;
        Initialize best performance score and best feature subset\;
        \For{each feature subset(combination)}{
            \For{each fold in inner loop}{
            Split the training data further into training(train) and validation(train) sets\;
            Train model on the training(train) set with feature subset\;
            Calculate the performance score on the validation set for the current inner fold\;
            Store the performance score for the current inner fold\;
            }
            Calculate aggregate performance score for the feature subset(combination)\;
            \If{aggregate performance score is better than best performance score}{
                Update best performance score\;
                Update best feature subset\;
            }
        }
        Store the best feature for the current outer fold\;
        Train the model on training data with best feature subset for the current outer fold\;
        Calculate the performance score on the validation set for the current outer fold\;
        Store the performance score for the current outer fold\;
    }
    

\caption{Nested Cross-validation with Best Subset Selection}
\end{algorithm}

\justifying \par The nested cross validation is used mainly to overcome the issue of data leakage. In cross validation, we can't use selected features, as we are dividing the data multiple times into training and validation sets. To avoid data leakage, the feature selection should happen exclusively from the training set and this should be true for each of the training set division. For this, in nested cross validation, an inner loop is used within a loop that loops over all combination of features, to select features from the training set, which would be used to train model only on that training fold, which will be used to estimate the cross validation performance on the validation set for the corresponding iteration. 

\subsection{Comparison of Datasets}
After nested cross validation, what we would have is a list of scores, and we expect using a non parametric test, such as the Wilcoxon test and Sign/Binomial test to be the best method to compare between them. 
\subsubsection{Wilcoxon Sign Rank Test}
Let, the datasets we have be A and B. Suppose, we have N iterationd of a repeated cross validation and $d_{i}$ be the difference between the dataset A compared to B on $i^th$ iteration, given that the same statistical model with same paramaters are used. Differences are ranked based on their values and average ranks are given in case of ties. Let, $R^{A}$ be the sum of ranks, for cases in which performance of the model on dataset A is better than that of B. 
\begin{equation}
    R^{A} = \sum_{d_{i}>0} rank(d_{i}) + \frac{1}{2} \sum_{d_{i}=0} rank(d_{i})
\end{equation}
The $R^{B}$ be the sum of ranks, for cases in which performance of the model on dataset B is better than that of A.
\begin{equation}
    R^{B} = \sum_{d_{i}<0} rank(d_{i}) + \frac{1}{2} \sum_{d_{i}=0} rank(d_{i})
\end{equation}

Then, let's define $T$ as the minimum of this two sums.
\begin{equation}
    T = \min{(R^{A},R^{B})}
\end{equation}
For a large enough $N$, the z-statistic can be defined as: 
\begin{equation}
    z= \frac{T- \frac{1}{4}N(N+1)}{\sqrt{\frac{1}{24}N(N+1)(2N+1)}}
\end{equation}
\subsection{Sign Test}
Here, we are considering in each iteration, whether one dataset is performing better than the other one or not. The test statistic is simply the fraction of times $d_{i}$ was larger than zero. 
\begin{equation}
    P=\frac{\sum_{d_{i}>0}1}{\sum_{i=1}^{N}1}
\end{equation}
With a large enough $N$ normal approximation can be applied to the binomial distribution, where:
\begin{equation}
    \mu = N/2
\end{equation}
\begin{equation}
    \sigma = \frac{\sqrt{N}}{2}
\end{equation}
Depending on the hypothesis, the test can either be one sided or two sided.




\end{document}

