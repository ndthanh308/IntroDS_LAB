\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Ahmad and Fink}{Ahmad and Fink}{2019}]{ref37}
Ahmad, I. and G.~A. Fink (2019).
\newblock Handwritten arabic text recognition using multi-stage sub-core-shape
  hmms.
\newblock {\em International Journal on Document Analysis and Recognition
  (IJDAR)\/}~{\em 22\/}(3), 329--349.

\bibitem[\protect\citeauthoryear{Ahmad, Naz, Afzal, Rashid, Liwicki, and
  Dengel}{Ahmad et~al.}{2017}]{ref16}
Ahmad, R., S.~Naz, M.~Z. Afzal, S.~F. Rashid, M.~Liwicki, and A.~Dengel (2017).
\newblock Khatt: A deep learning benchmark on arabic script.
\newblock In {\em 2017 14th IAPR International Conference on Document Analysis
  and Recognition (ICDAR)}, Volume~7, pp.\  10--14. IEEE.

\bibitem[\protect\citeauthoryear{Altwaijry and Al-Turaiki}{Altwaijry and
  Al-Turaiki}{2021}]{ref40}
Altwaijry, N. and I.~Al-Turaiki (2021).
\newblock Arabic handwriting recognition system using convolutional neural
  network.
\newblock {\em Neural Computing and Applications\/}~{\em 33\/}(7), 2249--2261.

\bibitem[\protect\citeauthoryear{Atienza}{Atienza}{2021}]{ref6}
Atienza, R. (2021).
\newblock Vision transformer for fast and efficient scene text recognition.
\newblock In {\em International Conference on Document Analysis and
  Recognition}, pp.\  319--334. Springer.

\bibitem[\protect\citeauthoryear{Baek, Kim, Lee, Park, Han, Yun, Oh, and
  Lee}{Baek et~al.}{2019}]{ref14}
Baek, J., G.~Kim, J.~Lee, S.~Park, D.~Han, S.~Yun, S.~J. Oh, and H.~Lee (2019).
\newblock What is wrong with scene text recognition model comparisons? dataset
  and model analysis.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  4715--4723.

\bibitem[\protect\citeauthoryear{Bahdanau, Cho, and Bengio}{Bahdanau
  et~al.}{2014}]{ref21}
Bahdanau, D., K.~Cho, and Y.~Bengio (2014).
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473\/}.

\bibitem[\protect\citeauthoryear{Bao, Dong, and Wei}{Bao et~al.}{2021}]{ref35}
Bao, H., L.~Dong, and F.~Wei (2021).
\newblock Beit: Bert pre-training of image transformers.
\newblock {\em arXiv preprint arXiv:2106.08254\/}.

\bibitem[\protect\citeauthoryear{Bleeker and de~Rijke}{Bleeker and
  de~Rijke}{2019}]{ref25}
Bleeker, M. and M.~de~Rijke (2019).
\newblock Bidirectional scene text recognition with a single decoder.
\newblock {\em arXiv preprint arXiv:1912.03656\/}.

\bibitem[\protect\citeauthoryear{Bluche}{Bluche}{2016}]{ref32}
Bluche, T. (2016).
\newblock Joint line segmentation and transcription for end-to-end handwritten
  paragraph recognition.
\newblock {\em Advances in neural information processing systems\/}~{\em 29}.

\bibitem[\protect\citeauthoryear{Bluche and Messina}{Bluche and
  Messina}{2017}]{ref23}
Bluche, T. and R.~Messina (2017).
\newblock Gated convolutional recurrent neural networks for multilingual
  handwriting recognition.
\newblock In {\em 2017 14th IAPR international conference on document analysis
  and recognition (ICDAR)}, Volume~1, pp.\  646--651. IEEE.

\bibitem[\protect\citeauthoryear{Chowdhury and Vig}{Chowdhury and
  Vig}{2018}]{ref11}
Chowdhury, A. and L.~Vig (2018).
\newblock An efficient end-to-end neural model for handwritten text
  recognition.
\newblock {\em arXiv preprint arXiv:1807.07965\/}.

\bibitem[\protect\citeauthoryear{Devlin, Chang, Lee, and Toutanova}{Devlin
  et~al.}{2018}]{ref30}
Devlin, J., M.-W. Chang, K.~Lee, and K.~Toutanova (2018).
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805\/}.

\bibitem[\protect\citeauthoryear{Diaz, Qin, Ingle, Fujii, and Bissacco}{Diaz
  et~al.}{2021}]{ref2}
Diaz, D.~H., S.~Qin, R.~Ingle, Y.~Fujii, and A.~Bissacco (2021).
\newblock Rethinking text line recognition models.
\newblock {\em arXiv preprint arXiv:2104.07787\/}.

\bibitem[\protect\citeauthoryear{Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.}{Dosovitskiy
  et~al.}{2020}]{ref29}
Dosovitskiy, A., L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
  (2020).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929\/}.

\bibitem[\protect\citeauthoryear{El-Khair}{El-Khair}{2016}]{ref26}
El-Khair, I.~A. (2016).
\newblock 1.5 billion words arabic corpus.
\newblock {\em arXiv preprint arXiv:1611.04033\/}.

\bibitem[\protect\citeauthoryear{El-Sawy, Loey, and El-Bakry}{El-Sawy
  et~al.}{2017}]{ref39}
El-Sawy, A., M.~Loey, and H.~El-Bakry (2017).
\newblock Arabic handwritten characters recognition using convolutional neural
  network.
\newblock {\em WSEAS Transactions on Computer Research\/}~{\em 5\/}(1), 11--19.

\bibitem[\protect\citeauthoryear{Gao, Chen, Wang, and Lu}{Gao
  et~al.}{2017}]{ref24}
Gao, Y., Y.~Chen, J.~Wang, and H.~Lu (2017).
\newblock Reading scene text with attention convolutional sequence modeling.
\newblock {\em arXiv preprint arXiv:1709.04303\/}.

\bibitem[\protect\citeauthoryear{Graves}{Graves}{2012}]{ref20}
Graves, A. (2012).
\newblock Sequence transduction with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1211.3711\/}.

\bibitem[\protect\citeauthoryear{Graves, Fern{\'a}ndez, Gomez, and
  Schmidhuber}{Graves et~al.}{2006}]{ref31}
Graves, A., S.~Fern{\'a}ndez, F.~Gomez, and J.~Schmidhuber (2006).
\newblock Connectionist temporal classification: labelling unsegmented sequence
  data with recurrent neural networks.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pp.\  369--376.

\bibitem[\protect\citeauthoryear{Kang, Riba, Rusi{\~n}ol, Forn{\'e}s, and
  Villegas}{Kang et~al.}{2022}]{ref5}
Kang, L., P.~Riba, M.~Rusi{\~n}ol, A.~Forn{\'e}s, and M.~Villegas (2022).
\newblock Pay attention to what you read: non-recurrent handwritten text-line
  recognition.
\newblock {\em Pattern Recognition\/}~{\em 129}, 108766.

\bibitem[\protect\citeauthoryear{Lee, Park, Baek, Oh, Kim, and Lee}{Lee
  et~al.}{2020}]{ref28}
Lee, J., S.~Park, J.~Baek, S.~J. Oh, S.~Kim, and H.~Lee (2020).
\newblock On recognizing texts of arbitrary shapes with 2d self-attention.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pp.\  546--547.

\bibitem[\protect\citeauthoryear{Li, Lv, Cui, Lu, Florencio, Zhang, Li, and
  Wei}{Li et~al.}{2021}]{ref1}
Li, M., T.~Lv, L.~Cui, Y.~Lu, D.~Florencio, C.~Zhang, Z.~Li, and F.~Wei (2021).
\newblock Trocr: Transformer-based optical character recognition with
  pre-trained models.
\newblock {\em arXiv preprint arXiv:2109.10282\/}.

\bibitem[\protect\citeauthoryear{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo}{Liu et~al.}{2021}]{ref36}
Liu, Z., Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo (2021).
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  10012--10022.

\bibitem[\protect\citeauthoryear{Mahmoud, Ahmad, Al-Khatib, Alshayeb, Parvez,
  M{\"a}rgner, and Fink}{Mahmoud et~al.}{2014}]{ref38}
Mahmoud, S.~A., I.~Ahmad, W.~G. Al-Khatib, M.~Alshayeb, M.~T. Parvez,
  V.~M{\"a}rgner, and G.~A. Fink (2014).
\newblock Khatt: An open arabic offline handwritten text database.
\newblock {\em Pattern Recognition\/}~{\em 47\/}(3), 1096--1112.

\bibitem[\protect\citeauthoryear{Mahmoud, Ahmad, Alshayeb, Al-Khatib, Parvez,
  Fink, M{\"a}rgner, and El~Abed}{Mahmoud et~al.}{2012}]{ref17}
Mahmoud, S.~A., I.~Ahmad, M.~Alshayeb, W.~G. Al-Khatib, M.~T. Parvez, G.~A.
  Fink, V.~M{\"a}rgner, and H.~El~Abed (2012).
\newblock Khatt: Arabic offline handwritten text database.
\newblock In {\em 2012 International conference on frontiers in handwriting
  recognition}, pp.\  449--454. IEEE.

\bibitem[\protect\citeauthoryear{Michael, Labahn, Gr{\"u}ning, and
  Z{\"o}llner}{Michael et~al.}{2019}]{ref22}
Michael, J., R.~Labahn, T.~Gr{\"u}ning, and J.~Z{\"o}llner (2019).
\newblock Evaluating sequence-to-sequence models for handwritten text
  recognition.
\newblock In {\em 2019 International Conference on Document Analysis and
  Recognition (ICDAR)}, pp.\  1286--1293. IEEE.

\bibitem[\protect\citeauthoryear{Mostafa, Mohamed, Ashraf, Elbehery, Jamal,
  Khoriba, and Ghoneim}{Mostafa et~al.}{2021}]{ref15}
Mostafa, A., O.~Mohamed, A.~Ashraf, A.~Elbehery, S.~Jamal, G.~Khoriba, and
  A.~S. Ghoneim (2021).
\newblock Ocformer: A transformer-based model for arabic handwritten text
  recognition.
\newblock In {\em 2021 International Mobile, Intelligent, and Ubiquitous
  Computing Conference (MIUCC)}, pp.\  182--186. IEEE.

\bibitem[\protect\citeauthoryear{Paszke, Gross, Chintala, Chanan, Yang, DeVito,
  Lin, Desmaison, Antiga, and Lerer}{Paszke et~al.}{2017}]{ref7}
Paszke, A., S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer (2017).
\newblock Automatic differentiation in pytorch.

\bibitem[\protect\citeauthoryear{Pechwitz, Maddouri, M{\"a}rgner, Ellouze,
  Amiri, et~al.}{Pechwitz et~al.}{2002}]{ref27}
Pechwitz, M., S.~S. Maddouri, V.~M{\"a}rgner, N.~Ellouze, H.~Amiri, et~al.
  (2002).
\newblock Ifn/enit-database of handwritten arabic words.
\newblock In {\em Proc. of CIFED}, Volume~2, pp.\  127--136. Citeseer.

\bibitem[\protect\citeauthoryear{Pham, Bluche, Kermorvant, and Louradour}{Pham
  et~al.}{2014}]{ref33}
Pham, V., T.~Bluche, C.~Kermorvant, and J.~Louradour (2014).
\newblock Dropout improves recurrent neural networks for handwriting
  recognition.
\newblock In {\em 2014 14th international conference on frontiers in
  handwriting recognition}, pp.\  285--290. IEEE.

\bibitem[\protect\citeauthoryear{Safaya, Abdullatif, and Yuret}{Safaya
  et~al.}{2020}]{ref9}
Safaya, A., M.~Abdullatif, and D.~Yuret (2020).
\newblock Kuisail at semeval-2020 task 12: Bert-cnn for offensive speech
  identification in social media.
\newblock In {\em Proceedings of the Fourteenth Workshop on Semantic
  Evaluation}, pp.\  2054--2059.

\bibitem[\protect\citeauthoryear{Sajid, Chow, Zhang, Kim, and Wang}{Sajid
  et~al.}{2021}]{ref13}
Sajid, U., M.~Chow, J.~Zhang, T.~Kim, and G.~Wang (2021).
\newblock Parallel scale-wise attention network for effective scene text
  recognition.
\newblock In {\em 2021 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--8. IEEE.

\bibitem[\protect\citeauthoryear{Sheng, Chen, and Xu}{Sheng
  et~al.}{2019}]{ref4}
Sheng, F., Z.~Chen, and B.~Xu (2019).
\newblock Nrtr: A no-recurrence sequence-to-sequence model for scene text
  recognition.
\newblock In {\em 2019 International conference on document analysis and
  recognition (ICDAR)}, pp.\  781--786. IEEE.

\bibitem[\protect\citeauthoryear{Shi, Bai, and Yao}{Shi et~al.}{2016}]{ref12}
Shi, B., X.~Bai, and C.~Yao (2016).
\newblock An end-to-end trainable neural network for image-based sequence
  recognition and its application to scene text recognition.
\newblock {\em IEEE transactions on pattern analysis and machine
  intelligence\/}~{\em 39\/}(11), 2298--2304.

\bibitem[\protect\citeauthoryear{Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou}{Touvron et~al.}{2021}]{ref10}
Touvron, H., M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou
  (2021).
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pp.\
  10347--10357. PMLR.

\bibitem[\protect\citeauthoryear{Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}{Vaswani et~al.}{2017}]{ref19}
Vaswani, A., N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems\/}~{\em 30}.

\bibitem[\protect\citeauthoryear{Voigtlaender, Doetsch, and Ney}{Voigtlaender
  et~al.}{2016}]{ref34}
Voigtlaender, P., P.~Doetsch, and H.~Ney (2016).
\newblock Handwriting recognition with large multidimensional long short-term
  memory recurrent neural networks.
\newblock In {\em 2016 15th international conference on frontiers in
  handwriting recognition (ICFHR)}, pp.\  228--233. IEEE.

\bibitem[\protect\citeauthoryear{Wolf, Debut, Sanh, Chaumond, Delangue, Moi,
  Cistac, Rault, Louf, Funtowicz, et~al.}{Wolf et~al.}{2020}]{ref8}
Wolf, T., L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, et~al. (2020).
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 conference on empirical methods in
  natural language processing: system demonstrations}, pp.\  38--45.

\bibitem[\protect\citeauthoryear{Zhang, Lu, Sak, Tripathi, McDermott, Koo, and
  Kumar}{Zhang et~al.}{2020}]{ref3}
Zhang, Q., H.~Lu, H.~Sak, A.~Tripathi, E.~McDermott, S.~Koo, and S.~Kumar
  (2020).
\newblock Transformer transducer: A streamable speech recognition model with
  transformer encoders and rnn-t loss.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  7829--7833. IEEE.

\end{thebibliography}
