\documentclass[final]{colt2023}

\title[The Sample Complexity of Multi-Distribution Learning]{Open Problem: The Sample Complexity of Multi-Distribution Learning for VC Classes}
\usepackage{times}
\usepackage[numbers]{natbib}
\usepackage{mathtools}
\usepackage{thm-restate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[suppress]{color-edits}
\usepackage{booktabs}
\usepackage{multirow}
\addauthor{ez}{blue}
\addauthor{nh}{purple}

\input{macros}
\coltauthor{%
 \Name{Pranjal Awasthi} \Email{pranjalawasthi@google.com}\\
 \addr Google Research, Mountain View, CA, USA
 \AND
 \Name{Nika Haghtalab} \Email{nika@berkeley.edu}\\
 \addr University of California, Berkeley, CA, USA
 \AND
 \Name{Eric Zhao} \Email{eric.zh@berkeley.edu}\\
 \addr University of California, Berkeley, CA, USA
}

\begin{document}

\maketitle

\begin{abstract}
	Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions.
	There remains a significant gap between the known upper and lower bounds for PAC-learnable classes.
	In particular, though we understand the sample complexity of learning a VC dimension $d$ class on $k$ distributions to be $O(\epsilon^{-2} \ln(k) (d + k) + \min \bset{\epsilon^{-1} d k,  \epsilon^{-4} \ln(k) d })$, the best lower bound is ${\Omega}(\epsilon^{-2}(d + k \ln(k)))$.
	We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning. 
	
\end{abstract}

\begin{keywords}
	PAC learning, multi-distribution learning, distributional robustness, learning in games.
\end{keywords}

\section{Introduction}
The pervasive need for robustness, fairness, and multi-agent welfare in  learning processes has led to the development of learning paradigms whose performance hold under multiple distributions and scenarios.
\emph{Multi-distribution learning}, or MDL, is a setting introduced by~\cite{haghtalabOnDemandSamplingLearning2022} to address these needs and unify several existing frameworks and applications, such as notions of \emph{min-max} fairness \cite{mohri_agnostic_2019,Abernethy2022}, \emph{group distributionally robust} optimization \cite{sagawa_distributionally_2020}, and collaborative learning \cite{blumCollaborativePACLearning2017}.
MDL is a generalization of the agnostic learning paradigms~\citep{valiant_theory_1984,blumer1989learnability} to multiple data distributions. In this setting, given a set of distributions $\dists = \bset{\dist_1, \dots, \dist_k}$ supported on $\features \times \labels$, loss function $\loss$, and a hypothesis class $\hyps$, %
the goal of MDL is to find a (possibly randomized) hypothesis $\hyp$ where
\begin{align}
	\label{eq:optimal}
	\smash{\max_{\dist \in \dists} \risk_{\dist}(\hyp) \leq \epsilon + \min_{\hyp^* \in \hyps} \max_{\dist \in \dists} \risk_{\dist}(\hyp^*),\; \text{where}\; \risk_\dist(\hyp) \asseq \EEs{(x,y) \sim \dist}{\loss(\hyp, (x, y))}.}
\end{align}
Such an $\hyp$ is called an \emph{$\epsilon$-optimal solution} to the MDL problem $(\dists, \hyps)$ and we denote
$\opt \asseq \min_{\hyp^* \in \hyps} \max_{\dist \in \dists} \risk_{\dist}(\hyp^*)$.
Our open problem concerns the sample complexity of MDL.


\paragraph{Problem Statement.}
Consider an example oracle 
$\oracle_i$ for each distribution $\dist_i \in \dists$, which once queried returns an independent sample $(x,y)\sim \dist_i$.
The optimal sample complexity of MDL is the smallest total number of queries issued to examples oracles, in a possibly adaptive fashion, that is sufficient for learning an $\epsilon$-optimal solution.
Formally, 
a multi-distribution learning algorithm at each iteration $t = 1,2, \dots$, chooses an index $\tsv{i}{t} \in [k]$, queries $\oracle_{\tsv{i}{t}}$ to sample an instance $(\tsv{x}{t}, \tsv{y}{t})$ and, upon termination, returns a (possibly randomized) solution $\hyp$.
We use the shorthands $\tsv{z}{t} = (\tsv{x}{t}, \tsv{y}{t}, \tsv{i}{t})$, $\cZ = \features \times \labels \times [k]$, and $\cZ^*$ to denote a sequence $\tsv{z}{1},\tsv{z}{2}, \dots$ of any size.
\begin{definition}[Multi-Distribution Learnability]
We say a hypothesis class $\hyps$ is multi-distribution learnable with sample complexity $m_\hyps: (0, 1)^2 \times \naturals \to \naturals$ if there exists functions $\cA_s: \cZ^* \to [k]$ and $\cA_\hyp: \cZ^* \to \simplex(\labels)^\features$ where the following holds: for every $(\epsilon, \delta) \in (0, 1)$, $k \in \naturals$, and set of $k$ distributions $\dists$ over $\features \times \labels$, by letting $\tsv{i}{t} = \cA_s(\tsv{z}{1}, \dots, \tsv{z}{t-1})$ for $t \in [m_\hyps(\epsilon, \delta, k)]$, with probability at least $1 - \delta$, the solution $\hyp = \cA_\hyp(\tsv{z}{1}, \dots, \tsv{z}{m})$ is $\epsilon$-optimal, i.e., satisfying \eqref{eq:optimal}.
\end{definition}
\begin{problem}
\label{prob:main}
What is the optimal sample complexity of MDL? 
Are hypothesis classes  $\hyps$ with VC dimension $d$  multi-distribution learnable with a sample complexity of $O\left( \epsilon^{-2}(\ln(k) d + k \ln(k / \delta)\right))$?

\end{problem}

Recalling that the sample complexity of agnostic learning is $m_\hyps(\epsilon, \delta, 1) \in \Theta(\epsilon^{-2}(d + \ln(1/ \delta)))$ \cite{shai}, one hopes to avoid paying the $\Omega\paraflat{k \cdot m_\hyps(\epsilon, \delta/k, 1)}$ samples necessary to independently learn each of the $k$ data distributions. This is why our conjectured sample complexity avoids a dependence on $dk$ and has an optimal $\epsilon^{-2}$ dependence.
Existing results, however, have fallen short of meeting both of these requirements and traded off lack of dependence on $dk$ with the optimal dependence on $\epsilon$, as shown in rows 1 and 2 of Table~\ref{tab:bounds}.
On the other hand, the optimal sample complexity of MDL has been rightly characterized for finite hypothesis classes in row 3 (and more generally those of finite Littlestone dimension or Bregman diameter~\citep{haghtalabOnDemandSamplingLearning2022}) and obtains optimal $\epsilon^{-2}\ln(|\hyps|)$ dependence.
The best lower bound, row 4, leaves a logarithmic gap with the conjectured upper bound.
Near-optimal bounds are known for \emph{realizable} settings where $\opt \mkern-4mu = \mkern-4mu 0$ (row 5) and \emph{personalized} settings where one can produce a different hypothesis for each distribution (row 6).
\begin{table}[htbp]
	\centering
	\caption{Best known bounds on the sample complexity of MDL for hypothesis classes with VC dimension $d$. $\tilde{O}$ hides double-log factors and an additive factor of $\epsilon^{-2} k \ln(k/\delta)$.}
	\label{tab:bounds}
	\begin{tabular}{llll}
		\toprule
		&\textbf{Bound}                                                                                           & \textbf{Assumption} & \textbf{Citation}                                 \\
		\midrule
		1.&$\tilde{O}(\epsilon^{-2} \ln(k) d 
 + \epsilon^{-1} \red{dk} \log(d/\epsilon)) $ & N/A                 & \cite{haghtalabOnDemandSamplingLearning2022}     \\
		2.&$\tilde{O}(\red{\epsilon^{-4}} \ln(k)(d + \ln(1/\delta\epsilon))$             & N/A                 & (Theorem~\ref{theorem:diff})                        \\
		3.&$\tilde{O}(\epsilon^{-2}\ln(\red{\setsize{\hyps}}))$                                               & N/A                 & \cite{haghtalabOnDemandSamplingLearning2022}     \\
		4.&$\Omega(\epsilon^{-2}(d + k \ln(\min\bset{d, k}/\delta)))$                                               & N/A                 & \cite{haghtalabOnDemandSamplingLearning2022}     \\
		\midrule
		5.&	${O}(\ln(k) \epsilon^{-1} (d \ln(1/\epsilon)  + k  \ln(k / \delta) ))$ & $\opt = 0$          & \cite{chenTightBoundsCollaborative2018,nguyenImprovedAlgorithmsCollaborative2018} \\
	6.&	$\tilde{O}(\ln(k) \epsilon^{-2} (d \ln(d/\epsilon)  + k  \ln(k / \delta) ))$                              & Personalized       & (Theorem~\ref{theorem:personal})                        \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Broad Applications.}
One of the motivating application of MDL is \emph{collaborative learning}, where multiple stakeholders (representing $\dist_i$) collaborate in training a model that provides high performance for each stakeholder~\cite{blumCollaborativePACLearning2017,nguyenImprovedAlgorithmsCollaborative2018,chenTightBoundsCollaborative2018,blum_one_2021}.
The sample complexity of MDL thus quantifies the value of collaboration in learning: whereas our conjectured upper bound would imply that collaboration reduces the amount of data needed by a $\ln(k) / k$ factor, existing bounds only imply a $\min \bset{\ln(k) / k \epsilon^2, \epsilon}$ factor reduction.

Another application of MDL is to Group \emph{distributionally robust optimization} (DRO) which concerns learning a model with performance guarantees for  many deployment environments \cite{sagawa_distributionally_2020,sagawa_investigation_2020}.
MDL sample complexity bounds quantify the cost of obtaining this robustness, a question of growing interest and which has been studied in terms of finite-sum convergence \cite{carmonhausler,asi2021} and sample complexity \cite{haghtalabOnDemandSamplingLearning2022}.
Our conjectured upper bound would extend these favorable results to VC classes by only increasing the sample complexity logarithmically.

MDL also captures notions of min-max fairness in learning, which concerns prioritizing the well-being of the worst-off subgroup and has applications in federated learning \cite{mohri_agnostic_2019} and equity \cite{Abernethy2022}.
Min-max fair learning has mainly been studied in settings with presampled datasets, where an inevitable sample complexity lower bound of $\Omega(dk/\epsilon^2)$ arises as one cannot adaptively choose distributions to sample from.
The sample complexity of MDL thus captures how min-max fairness can be attained at less cost by adapting one's data collection strategy on the fly.

\section{Overview of Current Approaches}
Multi-distribution learning can be formulated as the zero-sum game between a ``learner'' who chooses hypotheses $\hyp \in \hyps$ and an ``adversary'' whose chooses indices $i \in [k]$, with the payoff function $\risk_{D_i}(h)$.
Importantly, for any mixed-strategy $\epsilon$-min-max equilibrium $(\rhyp, \rloss) \in \simplex(\hyps) \times \simplex_k$, the randomized map $\rhyp$ is a  $2 \epsilon$-optimal solution.
All existing multi-distribution learning algorithms can be expressed as finding a $\epsilon$-equilibrium using no-regret dynamics (see \cite{haghtalabOnDemandSamplingLearning2022} for an overview).

\paragraph{Game dynamics.}
Formally, a game dynamic is a $T$-iteration process where, at each $t \in [T]$, a learner chooses hypothesis $\smash{\tsv{\hyp}{t} \in \hyps}$ with a no-regret algorithm and an adversary chooses a distribution $\smash{\tsv{i}{t} \in [k]}$ with a (semi-)bandit algorithm.
The learner estimates its current cost function $\smash{\hyp \mapsto \risk_{\dist_{\tsv{i}{t}}}(\hyp)}$ by sampling $N_{\mathrm{learn}}$ datapoints from $\smash{\oracle_{\tsv{i}{t}}}$, while the adversary estimates its cost function $\smash{i \mapsto - \risk_{\dist_{i}}(\tsv{\hyp}{t})}$ by, for $N_{\mathrm{adv}}$ choices of $i \in [k]$, sampling a datapoint from each $\oracle_i$.
The random mapping $\rhyp$ where $\rhyp(x) = \text{Uniform}(\tsv{\hyp}{1}(x), \dots, \tsv{\hyp}{t}(x))$ is a $2 \epsilon$-optimal solution.

\paragraph{Different instantiations.}
Every result in Table~\ref{tab:bounds} can be obtained by instantiating this game dynamics template.
Row 3 can be obtained 
by setting $N_{\mathrm{learn}}=N_{\mathrm{adv}}=1$, $T \propto \epsilon^{-2}(\ln(\setsize{\hyps}) + k \ln(k/\delta))$, having the learner choose $\tsv{\hyp}{t}$ with Hedge and the adversary choose $\tsv{i}{t}$ with Exp3 \cite{haghtalabOnDemandSamplingLearning2022}.
Row 1 can be obtained with the same algorithm but first creating an offline $\epsilon$-covering 
 of the class $\hyps$ on each data distribution $D_i \in \dists$, using $O(d/\epsilon)$ samples per distribution.
Row 2
can be obtained by setting $N_{\mathrm{adv}}=k$, $N_{\mathrm{learn}}\propto \epsilon^{-2}(d + \ln(1/\delta\epsilon))$, $T \propto \epsilon^{-2} \ln(k/\delta)$, having the learner choose $\tsv{\hyp}{t}$ to be the (approximate) risk minimizer of the current cost function and the adversary choose $\tsv{i}{t}$ with Hedge (Theorem~\ref{theorem:diff});
in contrast to the prior upper bound, this  bound uses an algorithm that iterates fewer times but samples more at each iteration.





\paragraph{Personalization.}
We can pinpoint the challenge of negotiating trade-offs between different data distributions as the primary difficulty of handling infinite classes.
Consider the personalized setting where, during inference time, $\cA_\hyp(\tsv{z}{1}, \dots, \tsv{z}{m})$ can return a different hypothesis $h_i$ for each distribution $\dist_i$.
This assumes away the difficulty of combining hypotheses that are each near-optimal for different distributions.
The conjectured sample complexity bound of $\tilde{O}(\ln(k) \epsilon^{-2} (d \ln(d/\epsilon)  + k  \ln(k / \delta) ))$ can be obtained in the personalized setting (Row 6 of Table~\ref{tab:bounds}) by running the Row 1 algorithm $\ln(k)$ times, at each round limiting the adversary to playing within a small region of the simplex $\Delta_k$ that we can efficiently cover $\hyps$ on (Theorem~\ref{theorem:personal}).

\subsection{Existing Challenges}
\paragraph{Adaptive coverings.}
A potential approach to closing the gap with the conjectured sample complexity bound is to find a method of adaptively covering the hypothesis class $\hyps$.
Whereas Row 1 was obtained by taking a naive offline $\epsilon$-covering of $\hyps$ on all $k$ distributions, Row 2 was obtained by an algorithm that (implicitly) $\epsilon$-covers the class $\hyps$ on $O(\ln(k)\epsilon^{-2})$ adaptive choices of $D_i \in \dists$.
It is unclear whether a covering of lower resolution can be used, or if it is possible to only cover $\hyps$ on $O(\ln(k))$ choices of distributions $D_i \in \dists$.
 We also note that it is not the size of the $\epsilon$-covering of $k$ distributions, i.e., $k{\epsilon}^{-O(d)}$, that is the bottleneck, but rather the number of samples needed %
 to create such a cover. %
In contrast, the personalized algorithm decided in an online fashion what distributions need to be covered and it only covers $\hyps$ on $O(\ln(k))$ choice of (mixture) distributions from $\dists$.

\paragraph{Agnostic-to-realizable.}
Another potential tool is an agnostic-to-realizable reduction \cite{hopkins22}, since nearly-optimal sample complexity bounds are known for realizable settings where $\opt = 0$ \cite{blumCollaborativePACLearning2017,chenTightBoundsCollaborative2018,nguyenImprovedAlgorithmsCollaborative2018}.
This technique has had success in related problems, such as the closely related adversarial PAC learning problem \cite{montasserVC2019}.
Unfortunately, because multi-distribution learning involves online decision-making---determining which example oracles to call---the usual reduction of testing all possible labelings of observed datapoints is intractable.

\paragraph{Bounding regret.}
Game dynamics algorithms rely on the learner achieving a low regret on the sequence of distributions chosen by the adversary.
However, with VC classes,  even when all distributions share a Bayes classifier, an oblivious adversary can force the learner to suffer regret linear in $k$.
It is therefore necessary to reason about the adversary's behavior to bound the regret of the learner.
This is atypical; game dynamics proofs usually bound each player's regret independently.
\begin{restatable}{proposition}{difficult}
	\label{proposition:difficult}
	Consider an algorithm $\cA$ that, given distributions $D_1, \dots, D_T$, draws only $N$ datapoints in total and returns a sequence of hypotheses $\hyp_1, \dots, \hyp_k$ where each $\hyp_t$ is trained only on datapoints sampled from $D_1, \dots, D_t$.
	There exists a sequence $D_1, \dots, D_T$ with only $k$ distinct members, where $\smash{\mathbb{E}[T^{-1} \sum_{t \in [T]} \err_{D_{t}}(\hyp_t)] - \min_{h^* \in \cH} T^{-1} \sum_{t \in [T]} \err_{D_{t}}(h^*)
			\in \Omega \paraflat{\sqrt{\vcd k / N}}.}$
\end{restatable}

\section{Intermediate Open Problems}
\paragraph{Lower Bounds.}
We believe a $\ln(k) \vcd$ factor is missing from the best known sample complexity lower bound of $\Theta(\epsilon^{-2}(\vcd + k \ln(\min\bset{k, \vcd} / \delta)))$.
The absence of a $\ln(k) \vcd$ term would be significant as it would imply that, when VC dimension dominates sample complexity, handling more data distributions comes effectively for free.
Interestingly, this $\ln(k)$ factor does not appear in the upper bound when the complexity of $\hyps$ is characterized by Littlestone dimension, perhaps due to the stronger compression guarantees for online-learnable classes.
A $\ln(k) \vcd$ term would also shed light on compression schemes for VC classes \cite{littlestone1986relating}; a lower bound of $\Theta(\ln(k) \vcd + k)$ would lend evidence against the existence of $O(\text{VC}(\hyps))$-size compression schemes.
\begin{problem}
\label{problem:lowerbound}
Is the sample complexity of multi-distribution learning in $\Omega(\log(k) \vcd)$?
\end{problem}

\paragraph{Proper learning.}
All existing multi-distribution learning algorithms with fast sample complexity rates produce either a randomized hypothesis $\hyp \in \simplex(\hyps)$ or an improper hypothesis resulting from taking a majority vote.
An open question is whether improperness is necessary for fast rates.
\begin{problem}
What is the sample complexity of proper multi-distribution learning?
\end{problem}
\paragraph{Oracle-efficient learning.}
For oracle-efficient algorithms, that is an algorithm only accessing $\hyps$ through an ERM oracle \cite{dudik2020}, only the sample complexity bound from Row 2 in Table~\ref{tab:bounds} is known.
An open question is whether there exists a statistical-computational trade-off for MDL. 
\begin{problem}
What is the sample complexity of oracle-efficient multi-distribution learning?
\end{problem}

\bibliographystyle{alpha}
% \bibliography{references}
\input{references.bbl}

\newpage
\appendix
\input{appendix}

\end{document}
