\newcommand{\alg}{\mathrm{Alg}}

\newpage
\appendix
\section{Omitted Proofs}
\label{appendix:br}
We first recall standard results in online learning.
We use the shorthands $\tsv{x}{1:T} \asseq \tsv{x}{1}, \dots, \tsv{x}{T}$, $\tsv{\bsetflat{f(\tsv{x}{t})}}{1:T} \asseq f(\tsv{x}{1}), \dots, f(\tsv{x}{T})$, and $f(\cdot, b) \asseq a \mapsto f(a, b)$ throughout this section.
We use $\simplex($A$)$ to denote the set of probability distributions over a set $A$, and $\simplex_d$ to denote a probability simplex in $\reals^{d-1}$.
Given a distribution $\cP \in \simplex_d$, we use $(\simplex_d)_2$ to denote the convex subset of $\simplex_d$ that is the distributions that are 2-smooth: $(\simplex_d)_2 \asseq \bset{\cP \in \simplex_d \mid \max_i \cP_i \leq 2/d}$.
\paragraph{Online learning.}
For a sequence of actions $\tsv{\action}{1}, \dots, \tsv{\action}{T} \in \actionset$ and costs $\tsv{\cost}{1}, \dots, \tsv{\cost}{T}: \actionset \to [0, 1]$, regret is defined as $\regret(\tsv{\action}{1:T}, \tsv{\cost}{1:T}) \asseq \sum_{t=1}^T \tsv{\cost}{t}(\tsv{\action}{t}) - \min_{\action^* \in \actionset} \sum_{t=1}^T \tsv{\cost}{t}(\action^*)$.
An online learning algorithm $\alg$ maps from costs $\tsv{\cost}{1:t-1}$ to a new action $\tsv{\action}{t} \in A$, where $\tsv{\action}{t} = \alg_A(\tsv{\cost}{1:t-1})$.
We recall the following online learning regret bound for probability simplices.
\begin{lemma}
    \label{lemma:hedge-basic-regret-bound}
    Let $\cA$ be a compact convex subset of $\simplex_d$ and fix a learning rate $\eta \in [0, 0.5]$.
    For any sequence of linear costs $\tsv{\cost}{1:T}$, the Hedge online learning algorithm \cite{freund_decision-theoretic_1997} chooses actions $\tsv{\action}{1:T}$, where $\tsv{\action}{t} = \mathrm{Hedge}_\cA(\tsv{\cost}{1:t-1}$, with regret $\regret(\tsv{\action}{1:T}, \tsv{\cost}{1:T}) \leq \ln(d) / \eta + \eta \min_{\action^* \in \actionset} \sum_{t=1}^T \tsv{\cost}{t}(\action^*)$.
\end{lemma}
Stochastic costs are functions $\ncost: \actions \times \cZ \to [0, 1]$ of both actions and datapoints.
We say a stochastic cost $\ncost$ is linear if $\ncost(\cdot, z)$ is linear in its first argument under any datapoint $z \in \cZ$.
We know that estimating stochastic costs with i.i.d. samples does not significantly affect the regret of an online learning algorithm.
\begin{lemma}
	\label{lemma:stochastic-approximation}
        Let $\cA$ be a compact convex subset of $\simplex_d$, $\alg$ an online learning algorithm, and $\tsv{z}{1:T} \simiid \dist$ i.i.d. samples from some data distribution $\dist$.
        For any sequence of linear stochastic costs $\tsv{\ncost}{1:T}$, applying $\alg$ to the empirical cost estimates $\tsv{\bset{\tsv{\ncost}{t}(\action, \tsv{z}{t})}}{1:T}$ such that $\tsv{\action}{t} = \alg_\cA(\tsv{\bset{\tsv{\ncost}{\tau}(\action, \tsv{z}{\tau})}}{1:t-1})$ guarantees
		\begin{align*}
			\abs{\regret(\tsv{\action}{1:T}, \tsv{\bsetflat{\mathbb{E}_{z \sim \dist}[\tsv{\ncost}{t}(\cdot, z)]}}{1:T})
				- \regret(\tsv{\action}{1:T}, \tsv{\bset{\tsv{\ncost}{\tau}(\action, \tsv{z}{\tau})}}{1:T})} \leq \bigO{\sqrt{\ln(d / \delta) T}},
		\end{align*}
		with probability at least $1 - \delta$ over the randomness of $\tsv{z}{1:T}$ \cite{nemirovski2009robust}.
\end{lemma}
We also recall the agnostic learning upper bound.
\begin{lemma}
    \label{lemma:agnostic-learning}
    Consider any stochastic cost $\ncost: \actionset \times \cZ \to [0, 1]$ and data distribution $\dist$, where $d$ is the VC dimension of $\actionset$.
    With only $O((d + \ln(1/\delta)) / \epsilon \alpha)$ samples from $\dist$, the action $a \in \actionset$ empirically minimizing  $\ncost$ is $\epsilon$-optimal with probability $1 - \delta$: $\EEs{z \sim \dist}{\ncost(a, z)} \leq \epsilon + (1 + \alpha) \min_{a^* \in \actionset} \EEs{z \sim \dist}{\ncost(a^*, z)}$ \cite{nguyenImprovedAlgorithmsCollaborative2018}.
\end{lemma}
Finally, we note that all the aforementioned results for cost sequences also apply to \emph{payoff sequences}, where the regret of actions $\tsv{\action}{1:T}$ with respect to a sequence of payoffs $\tsv{\rho}{1:T}$ is defined as $\regret_+(\tsv{\action}{1:T}, \tsv{\rho}{1:T}) \asseq \max_{\action^* \in \actionset} \sum_{t=1}^T \tsv{\rho}{t}(\action^*) - \sum_{t=1}^T \tsv{\rho}{t}(\tsv{\action}{t})$. 
Here, we use the subscript $+$ in $\regret_+$ to distinguish when regrets are stated for payoff functions.
For example, the regret bound of Hedge for payoffs can be written as follows.
\begin{lemma}
    \label{lemma:hedge-basic-regret-bound-payoffs}
    Let $\cA$ be a compact convex subset of $\simplex_d$ and fix a learning rate $\eta \in [0, 0.5]$.
    For any sequence of payoffs $\tsv{\rho}{1:T}$, the Hedge online learning algorithm \cite{freund_decision-theoretic_1997} chooses actions $\tsv{\action}{1:T}$, where $\tsv{\action}{t} = \mathrm{Hedge}_\cA(\tsv{\rho}{1:t-1})$, with regret $\regret_+(\tsv{\action}{1:T}, \tsv{\rho}{1:T}) \leq \ln(d) / \eta + \eta \max_{\action^* \in \actionset} \sum_{t=1}^T \tsv{\rho}{t}(\action^*)$.
\end{lemma}

\subsection{Proof of Theorem~\ref{theorem:diff} (Row 2 of Table~\ref{tab:bounds})}

\begin{algorithm}[h]
	\caption{Multi-Distribution Learning Algorithm.}
	\label{alg:fast}
	\begin{algorithmic}
		\STATE \textbf{Input:} Hypotheses $\hypothesisspace$, distributions $\distributionspace$, iterations $T \in \integers_+$, sub-iterations $r_1, r_2 \in \integers_+$, parameter $\alpha \in (0, 0.5)$;
		\STATE Intialize Hedge iterate $\tsv{\dist}{1}$ to be a uniform mixture of $\dists$;
		\FOR{$t = 1, 2, \dots, T$}
		\STATE Sample $r_1$ datapoints $z_1, \dots, z_{r_1}$ from $\tsv{\dist}{t}$;
		\STATE Let $\tsv{\hyp}{t} = \argmin_{\hyp \in \hyps} \sum_{i=1}^{r_1} \loss(\hyp, z_i)$ be the empirical minimizer of $\loss$;
		\STATE Sample $r_2$ datapoints $z_{\dist, (t-1) r_2 + 1}, \dots, z_{\dist, t r_2}$ from each $\dist \in \dists$;
		\STATE Use the Hedge algorithm to get the next iterate $\tsv{\dist}{t+1} \in \simplex(\dists)$, using learning rate $\alpha$ and observing the payoff $\tsv{\tilde{\rho}}{t}: \dists \to [0, 1]$ where $\tsv{\tilde{\rho}}{t}(\dist)
		= \frac{1}{r_2} \sum_{i=(t-1)r_2+1}^{t r_2} \loss(\tsv{\hyp}{t}, z_{\dist, i})$;
		\ENDFOR
		\STATE Return $\overline{h}$: a uniform distribution over $\tsv{\hyp}{1:T}$;
	\end{algorithmic}
\end{algorithm}

\begin{restatable}{theorem}{diff}
	\label{theorem:diff}
	For any $\epsilon, \alpha \in (0, 0.5)$, $\delta > 0$, $k \in \integers_+$ and binary class $\cH$, the sample complexity of MDL, $m_\cH(\epsilon + \alpha \cdot \opt, \delta, k)$, is $\bigOtildesmol{\epsilon^{-2}  \para{k \log(k / \delta) + \alpha^{-2} \log(k) (\log(1/\epsilon \delta) + \text{VC}(\hyps))}}$.
\end{restatable}
\begin{proof}
	Let $\vcd$ denote the VC dimension of $\hyps$.
        Without loss of generality, assume $\epsilon \leq \alpha$.
	Consider Algorithm~\ref{alg:fast}, fixing $T = \frac{\ln(k)}{\epsilon \alpha}$, $r_1 = C_1 \frac{d + \ln(T / \delta)}{\epsilon \alpha}$, and $r_2 = \ceil{C_2 \frac{\ln(k/\delta)}{T \epsilon^2}}$.

        \begin{fact}
            The regret of the ``adversary'' in the game dynamics induced by Algorithm~\ref{alg:fast} satisfies
            \begin{align*}
                \regret_+(\tsv{\dist}{1:T}, \tsv{\bsetflat{\risk_{(\cdot)}(\tsv{\hyp}{t})}}{1:T})
                \leq \frac{\ln(k)}{\alpha} + T \epsilon +  \alpha  \max_{D^* \in \dists} \sum_{t=1}^T \risk_{D^*}(\tsv{\hyp}{t}),
            \end{align*}
            with probability at least $1 - 2 \delta$ for some choice of universal constant $C_2$.
        \end{fact}
 \begin{proof}
        The mixture distributions $\tsv{\dist}{1:T}$ result from applying Hedge to the payoff functions $\tsv{\tilde{\rho}}{1:T}$.
        Hence, by Lemma~\ref{lemma:hedge-basic-regret-bound-payoffs},
        \begin{align*}
            \regret_+(\tsv{\dist}{1:T}, \tsv{\tilde{\rho}}{1:T}) \leq \frac{\ln(k)}{\alpha} + \alpha \max_{\dist^* \in \dists} \sum_{t=1}^T \tsv{\tilde{\rho}}{t}(\dist^*).
        \end{align*}
        To prove generalization, we will break each timestep $t$ into $r_2$ sub-timesteps.
        For every $j \in [T r_2]$, we let $\tsv{\tilde{\dist}}{j} = \tsv{\dist}{\ceil{j / r_2}}$ and define $\tsv{\tilde{\cost}}{j}$ to be the cost function $D \mapsto \frac{1}{r_2} (1 - \loss(\tsv{\hyp}{t}, z_{D, j}))$.
        We can rewrite the adversary's regret as  $  \regret_+(\tsv{\dist}{1:T}, \tsv{\tilde{\rho}}{1:T}) =   \regret(\tsv{\tilde{\dist}}{1:T r_2}, \tsv{\tilde{\cost}}{1:T r_2}) $.
        Further observe that, since $\EEs{z_{\dist, j}}{\tsv{\tilde{\cost}}{j}} = \risk_{\dist}(\tsv{\hyp}{\ceil{j / r_2}})$ for every $j \in [T r_2]$ and $\dist \in \dists$, the empirical regret is unbiased: $\regret_+(\tsv{\dist}{1:T}, \tsv{\bsetflat{\risk_{(\cdot)}(\tsv{\hyp}{t})}}{1:T})=   \regret(\tsv{\tilde{\dist}}{1:T r_2}, \tsv{\bsetflat{\EEs{z_{(\cdot), j}}{\tsv{\tilde{\cost}}{j}}}}{1:T r_2}) $.
        By Lemma~\ref{lemma:stochastic-approximation},
	\begin{align*}
		&\abs{\regret_+(\tsv{\dist}{1:T}, \tsv{\bsetflat{\risk_{(\cdot)}(\tsv{\hyp}{t})}}{1:T})
			- \regret_+(\tsv{\dist}{1:T}, \tsv{\tilde{\rho}}{1:T})} \\
   &= \abs{\regret(\tsv{\tilde{\dist}}{1:T r_2}, \tsv{\tilde{\cost}}{1:T r_2})  -
   \regret(\tsv{\tilde{\dist}}{1:T r_2}, \tsv{\bsetflat{\mathbb{E}_{z_{(\cdot), j}}[\tsv{\tilde{\cost}}{j}]}}{1:T r_2}) }
   \\
   &\leq \bigO{\sqrt{\ln(k / \delta) T / r_2}} = \bigO{T \epsilon / C_2},
	\end{align*}
        with probability at least $1 - \delta$.
        Similarly, with probability at least $1 - \delta$, $\max_{D^* \in \dists} \sum_{t=1}^T {\tsv{\tilde{\rho}}{t}}(D^*) \leq O(\frac{T \epsilon}{C_2}) + \max_{D^* \in \dists} \sum_{t=1}^T \risk_{D^*}(\tsv{\hyp}{t})$.
 A union bound yields the claimed fact.
 \end{proof}

    Next, we observe that, at each timestep $t$, $\tsv{\hyp}{t}$ is the empirical risk minimizer of $\loss$ on $C_1 (d + \log(T /\delta)) / \epsilon \alpha$ samples from $\tsv{\dist}{t}$.
    For sufficiently large $C_1$, by Lemma~\ref{lemma:agnostic-learning}, $\risk_{\tsv{\dist}{t}}(\tsv{\hyp}{t}) \leq \epsilon + (1 + \alpha) \min_{\hyp^* \in \hyps} \risk_{\tsv{\dist}{t}}(\hyp^*)$ with probability at least $1 - \delta/T$.
    By union bound, $\sum_{t=1}^T \risk_{\tsv{\dist}{t}}(\tsv{\hyp}{t}) \leq T \epsilon + T (1 + \alpha) \opt$ with probability at least $1  - \delta$.
	Putting together the regret bounds for the learner and adversary,
	\begin{align*}
		(1 - \alpha) \max_{\dist^* \in \dists} \risk_{\dist^*}(\overline{\hyp}) - 2 \epsilon
		&= (1 - \alpha) \max_{\dist^* \in \dists} \para{\frac 1 T \sum_{t=1}^T \risk_{\dist^*}(\tsv{\hyp}{t})} - 2 \epsilon \\
		&\leq \frac 1 T \sum_{t=1}^T \risk_{\tsv{\dist}{t}}(\tsv{\hyp}{t}) \\
		&\leq \epsilon + (1 + \alpha) \min_{\hyp^* \in \hyps} \frac 1 T \sum_{t=1}^T \risk_{\tsv{\dist}{t}}(\hyp^*) \\
		&\leq \epsilon + (1 + \alpha) \opt.
	\end{align*}
        We can simplify $\max_{\dist^* \in \dists} \risk_{\dist^*}(\overline{\hyp}) \leq \frac{1}{1-\alpha}(3 \epsilon + (1 + \alpha)) \opt \leq 6 \epsilon + (1 + 4 \alpha) \opt$.
        Reparameterizing $\epsilon \to \frac{1}{6} \epsilon$ and $\alpha \to \frac{1}{4} \alpha$ yields the desired claim.
	\noindent
	Our sample complexity is $(r_1 + k r_2) \times T$ and thus $\bigO{\frac{ k\ln (k/ \delta)} {\epsilon^2} + \frac{(d + \log(T /\delta)) \ln(k/\delta) }{ \epsilon^2 \alpha^2}}$.
\end{proof}
\subsection{Proof of Theorem~\ref{theorem:personal}}

\begin{algorithm}[t]
	\caption{Personalized Algorithm.}
	\label{alg:personalized}
	\begin{algorithmic}
		\STATE \textbf{Input:} Hypotheses $\hypothesisspace$, distributions $\distributionspace$;
		\STATE Initialize $\tsv{\distset}{1} = \distset$;
		\FOR{$t = 1, 2, \dots, \ceil{\log(k)}$}
		\STATE Run Algorithm~\ref{alg:mid} on $\tsv{\dists}{t}, \hyps$ to obtain $\tsv{\hyp}{t}$;
		\STATE Sample $O\para{\epsilon^{-2} (\ln(k \ln(k) / \delta))}$ datapoints $\bX_t^\dist$ from each $\dist \in \distset$;
		\STATE Let $\tsv{\distset}{t+1}$ consist of $\dist$ where $\hat{\err}_{\bX_t^\dist}(\tsv{h}{t}) > \text{Median}\para{\hat{\err}_{\bX_t^\dist}(\tsv{h}{t})}_{\dist \in \distset}$;
		\ENDFOR
		\STATE For each $\dist \in \distset$, find $t_\dist$ where $\dist \in \tsv{\distset}{t_\dist}$ but $\dist \notin \tsv{\distset}{t_\dist+1}$. Return $\para{\dist, \tsv{h}{t_\dist}}_{\dist \in \distset}$.
	\end{algorithmic}
\end{algorithm}

\begin{restatable}{theorem}{personal}
	\label{theorem:personal}
	For any $\epsilon, \delta > 0$, $k \in \integers$ and binary class $\cH$, the sample complexity $m_\cH(\epsilon, \delta, k)$ of \emph{personalized} multi-distribution learning is $\tilde{O}(\epsilon^{-2} \ln(k) (\text{VC}(\hyps) \ln(\text{VC}(\hyps)k/\epsilon)  + k  \ln(k / \delta) ))$.
\end{restatable}



\begin{algorithm}[h]
	\caption{Multi-Distribution Learning Algorithm (Mid).}
	\label{alg:mid}
	\begin{algorithmic}
		\STATE \textbf{Input:} Hypotheses $\hypothesisspace$, distributions $\distributionspace$;
		\STATE Take $\epsilon^{-1} C \para{\vcd \log(\vcd /\epsilon)+\log(1/\delta)}$ samples $x_1, \dots, x_N$ from $\text{Uniform}(\cD)$ and obtain a covering $\cH'$ of $\cH$ by projection: for every $y \in \bset{[\hyp(x_1), \dots, \hyp(x_N)] \mid \hyp \in \hyps}$, include in $\cH'$ an arbitrary choice of $h \in \cH$ such that $[h(x_1), \dots, h(x_N)] = y$; 
		\STATE Intialize Hedge iterate $\tsv{\dist}{1}$ on $(\Delta \cD)_{2}$, that is the set of 2-smooth distributions on $\cD$;
		\STATE Intialize Hedge iterate $\tsv{\hyp}{1}$ on the simplex $(\Delta \hyps')$;
		\FOR{$t = 1, 2, \dots, T$}
		\STATE Use the Hedge algorithm to get the next iterate $\tsv{\hyp}{t+1} = \text{Hedge}_{\simplex(\hyps')}(\tsv{\bset{\tsv{\hat{\cost}}{\tau}}}{1:t})$, where $\tsv{\hat{\cost}}{t}(\hyp)
		= \loss(\hyp, z)$ and $z \sim \tsv{\dist}{t}$;
		\STATE Sample a $\dist' \sim \text{Uniform}(\cD)$ and a datapoint $z \sim \dist$;
		\STATE Run Hedge algorithm to get the next iterate $\tsv{\dist}{t+1} = \text{Hedge}_{(\Delta \cD)_{2}}(\tsv{\bset{\tsv{\tilde{\cost}}{\tau}}}{1:t})$, where $\tsv{\tilde{\cost}}{t}(\dist)
		= 1[\dist' = \dist] \cdot \setsize{\cD} \cdot \Pr_{\tsv{\dist}{t}}( \dist') (1 - \loss(\tsv{\hyp}{t}, z))$;
		\ENDFOR
		\STATE Return a uniform distribution over $\tsv{\hyp}{1:T}$;
	\end{algorithmic}
\end{algorithm}

We now turn to proving this result.

\begin{lemma}
	\label{lemma:personalizedsmoothed}
	Consider the multi-distribution learning problem $(\dists, \hyps, \loss)$.
	For any $h \in \Delta \cH$, there exists a $\cD' \subseteq \cD$ where $\setsize{\cD'} \geq \setsize{\cD} / 2$ and $\max_{\dist \in (\Delta \cD)_{2}} \err_\dist(h)
	\geq \max_{\dist \in \cD'} \err_\dist(h)$.
\end{lemma}
\begin{proof}
	Fix an $h \in \Delta \cH$.
	Consider all strict minorities of $\cD$: $\text{Min} \assignequals \bset{\distset' \subseteq \distset \mid \setsize{\cD'} < \setsize{\cD} / 2}$.
	Let $\cD_{\text{MinHard}}$ denote the strict minority on which $h$ does worst, and $\cD_{\text{MinEasy}}$ denote the strict minority on which $h$ does best:
	\begin{align*}
		\cD_{\text{MinHard}} & = \argmax_{\cD^* \in \text{Min}} \frac{1}{\setsize{\cD^*}} \sum_{\dist \in \cD^*} \err(h), \quad
		\cD_{\text{MinEasy}} = \argmin_{\cD^* \in \text{Min}} \frac{1}{\setsize{\cD^*}} \sum_{\dist \in \cD^*} \err(h).
	\end{align*}
	First, we observe that $\max_{(\Delta \cD)_{2}} \err(h) \geq \EEsc{D \sim \text{Uniform}(\cD)}{\err(h)}{D \notin \cD_{\text{MinEasy}}}$, where $\text{Uniform}(\cD)$ is the uniform mixture over $\cD$.
	Second, we observe that $\EEsc{D \sim \text{Uniform}(\cD)}{\err(h)}{D \notin \cD_{\text{MinEasy}}} \geq \max_{\dist \in \cD \setminus \cD_{\text{MinHard}}} \err_\dist(h)$.
	Thus, $\cD' = \cD \setminus \cD_{\text{MinHard}}$ satisfies the desired property.
\end{proof}


\begin{lemma}
	\label{lemma:samples}
	Consider a multi-distribution learning problem $(\dists, \hyps, \loss)$.
	Algorithm~\ref{alg:mid} returns a hypothesis $\overline{\hyp}$ such that with probability $1- \delta$,
 \begin{align*}\max_{D^* \in (\Delta \cD)_2} \err_{D^*}(\overline{h}) \leq \min_{\hyp^* \in \Delta \cH} \max_{D^* \in (\Delta \cD)_2} \err_{D^*}(h^*) + \epsilon.\end{align*}
 It takes only  $\tilde{O}(\epsilon^{-2} (d \ln(dk/\epsilon)  + \ln(1 / \delta) ))$ samples.
\end{lemma}
\begin{proof}
	By construction, with probability at least $1 - \delta$, $\cH'$ is an $\epsilon$-net for $\cH$ \cite{hausslerEpsilonNets1986} under the distribution $\text{Uniform}(\dists)$.
	Consider any distribution $\cP \in \simplex(\dists)_2$.
	Because any event that happens in $\cP$ must also happen in $\text{Uniform}(\dists)$ with at least half the probability, including the event that $h(x) \neq h'(x)$,  $\cH'$ is a $2\epsilon$-net for $\cP$.
	Since the range of $\ell$ is $[0,1]$, it also follows that for any $h \in \cH$, there is an $h' \in \cH'$ such that $\smash{\abs{\err_{\cP}(h) - \err_{\cP}(h')} < 2\epsilon}$.
	We now turn to arguing that our output $\text{Uniform}(\tsv{\hyp}{1:T})$ is nearly optimal for the discretized class $\cH'$.

	We observe that $\tsv{\dist}{1:T}$ results from applying Hedge to (importance-weighted estimates of) stochastic cost functions that are bounded in $[0, 2]$.
	Moreover, the costs are bounded unbiased estimates of the true costs.
	Thus, as in our proof of Theorem~\ref{theorem:diff}, we can directly apply Hedge's regret bound (Lemma~\ref{lemma:hedge-basic-regret-bound}) and stochastic approximation (Lemma~\ref{lemma:stochastic-approximation}) to bound the adversary's regret $\regret(\tsv{\dist}{1:T}, \tsv{\bsetflat{1 - \risk_{(\cdot)}(\tsv{\hyp}{t})}}{1:T}) \leq \bigO{\sqrt{\ln(k / \delta) T}}$.
	Note that this regret is defined only over the set $\simplex(\dists)_2$.
	Therefore choosing $T = \ceil{C' \ln (k/ \delta) / \epsilon^2}$ for large $C'$ gives $\regret(\tsv{\dist}{1:T}, \tsv{\bsetflat{1 - \risk_{(\cdot)}(\tsv{\hyp}{t})}}{1:T}) \leq T\epsilon$ with probability $1 - \delta$.
	Similarly, the learner's Hedge (Lemma~\ref{lemma:hedge-basic-regret-bound}) and the stochastic approximation (Lemma~\ref{lemma:stochastic-approximation}) gives the regret bound $\regret(\tsv{\hyp}{1:T}, \tsv{\bsetflat{\risk_{\tsv{\dist}{t}}(\cdot)}}{1:T}) \leq \bigO{\sqrt{\ln(\setsize{\hyps'}) T}}$.
	Note that this regret is defined only over the set $\cH'$.
	Since $\setsize{\hyps'} \leq O((kN)^d)$ by Sauer Shelah's lemma, choosing $T \geq C \epsilon^{-2} d \ln(dk\ln(d/\epsilon\delta)/\epsilon)$ for some large constant $C$ guarantees that $\regret(\tsv{\hyp}{1:T}, \tsv{\bsetflat{\risk_{\tsv{\dist}{t}}(\cdot)}}{1:T}) \leq T \epsilon$ with probability at least $1 - \delta$.
	Putting together the regret bounds for the learner and adversary as before,
	\begin{align*}
		\max_{\dist^* \in\simplex(\dists)_2} \risk_{\dist^*}(\overline{\hyp}) - \epsilon
		\leq \frac 1 T \sum_{t=1}^T \risk_{\tsv{\dist}{t}}(\tsv{\hyp}{t}) 
		&\leq \epsilon + \min_{\hyp^* \in \hyps'} \frac 1 T \sum_{t=1}^T \risk_{\tsv{\dist}{t}}(\hyp^*) \\
		&\leq 3\epsilon + \min_{\hyp^* \in \hyps} \frac 1 T \sum_{t=1}^T \risk_{\tsv{\dist}{t}}(\hyp^*) \\
		&\leq 3\epsilon + \opt.
	\end{align*}

	\noindent
	Our sample complexity is $2 \times T$ and thus $\tilde{O}(\epsilon^{-2} (d \ln(dk/\epsilon) + \ln(1 / \delta) ))$.

\end{proof}

\begin{proof}[Proof of Theorem~\ref{theorem:personal}]
	Consider Algorithm~\ref{alg:personalized}.
    Let $\dist^*$ be the product distribution of every $\dist \in \dists$.
	Let $\vcd$ denote the VC dimension of $\hyps$.
	By Lemma~\ref{lemma:samples}, with probability at least $1 - \log(k) \delta$, for all $t \in [T]$, 
	\begin{align*}\max_{D^* \in (\Delta \tsv{\cD}{t})_{2}} \err_{D^*}(\tsv{h}{t}) \leq \min_{\hyp^* \in \Delta \cH} \max_{D^* \in (\Delta \tsv{\cD}{t})_{2}} \err_{D^*}(h^*) + \epsilon.\end{align*}
	By Lemma~\ref{lemma:personalizedsmoothed}, there exists $\cD' \subseteq \tsv{\dists}{t}$ where  $\setsize{\cD'} > \tsv{\dists}{t} / 2$ and $\max_{\dist \in (\Delta \cD)_{\tsv{\cP}{t}}} \err_\dist(\tsv{h}{t})
	\geq \max_{\dist \in \cD'} \err_\dist(\tsv{h}{t})$.
	In other words, $\opt  + \epsilon \geq \max_{\dist \in \cD'} \err_\dist(\tsv{h}{t})$.
	By uniform convergence, with probability at least $1 - \delta$, for all $t \in [T]$ and $\dist \in \tsv{\dists}{t}$, $\abs{\tsv{\hat{\err}}{t}_\dist(\tsv{\hyp}{t}) - \err_{\dist}(\tsv{\hyp}{t})} \leq \epsilon$.
	Thus, for every $\dist \in \tsv{\dist}{t} \setminus \tsv{\dist}{t+1}$, $\err_\dist(\tsv{\hyp}{t}) \leq 2 \epsilon + \opt$.
	Since the size of $\tsv{\dist}{t}$ is reduced by at least half every iteration, the algorithm terminates after $\ceil{\ln(k)}$ iterations.
	The algorithm's sample complexity comes from the samples needed for  Lemma~\ref{lemma:personalizedsmoothed} and for evaluating each $\tsv{\hyp}{t}$, and taking a union bound over all iterations.
\end{proof}

