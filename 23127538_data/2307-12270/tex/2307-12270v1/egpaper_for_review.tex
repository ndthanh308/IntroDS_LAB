\documentclass[10pt,twocolumn,letterpaper]{article}
% \documentclass[main.tex]{subfiles}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage[small]{caption}

\usepackage{soul}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage[normalem]{ulem}
\usepackage[page]{appendix} % print appendices title
\renewcommand{\appendixpagename}{\twocolumn[\Large\centering Supplements of Context Perception Parallel Decoder for Scene Text Recognition \vspace{1cm}]} % Appendices title
% \usepackage{titlesec}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{6786} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Context Perception Parallel Decoder for Scene Text Recognition}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

%\footnote{Corresponding Author}, 

\author{
{Yongkun Du$^{1}$, 
Zhineng Chen$^{1}$\thanks{Corresponding Author.},
Caiyan Jia$^{2}$, 
Xiaoting Yin$^3$, 
Chenxia Li$^3$, 
Yuning Du$^3$, 
Yu-Gang Jiang$^1$}
\\
% \affiliations
$^1$School of Computer Science, Fudan University, China\\
$^2$School of Computer and Information Technology, Beijing Jiaotong University, China\\
$^3$Baidu Inc., China\\
% \emails
% }
% \emails
\{zhinchen, ygj\}@fudan.edu.cn,
\{cyjia\}@bjtu.edu.cn,
\{yinxiaoting, lichenxia, duyuning\}@baidu.com\\
}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
Scene text recognition (STR) methods have struggled to attain high accuracy and fast inference speed. Autoregressive (AR)-based STR model uses the previously recognized characters to decode the next character iteratively. It shows superiority in terms of accuracy. However, the inference speed is slow also due to this iteration. Alternatively, parallel decoding (PD)-based STR model infers all the characters in a single decoding pass. It has advantages in terms of inference speed but worse accuracy, as it is difficult to build a robust recognition context in such a pass. In this paper, we first present an empirical study of AR decoding in STR. In addition to constructing a new AR model with the top accuracy, we find out that the success of AR decoder lies also in providing guidance on visual context perception rather than language modeling as claimed in existing studies. As a consequence, we propose Context Perception Parallel Decoder (CPPD) to decode the character sequence in a single PD pass. CPPD devises a character counting module and a character ordering module. Given a text instance, the former infers the occurrence count of each character, while the latter deduces the character reading order and placeholders. Together with the character prediction task, they construct a context that robustly tells what the character sequence is and where the characters appear, well mimicking the context conveyed by AR decoding. Experiments on both English and Chinese benchmarks demonstrate that CPPD models achieve highly competitive accuracy. Moreover, they run approximately 7x faster than their AR counterparts, and are also among the fastest recognizers. The code will be released soon.
%The code will be publicly available.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Scene text recognition (STR) aims at reading a character sequence from the text instance cropped from real-world scenes, e.g., text on billboards, packaging boxes, electronic screens, etc. It has attracted attention from a wide range of applications for its key role in precise image interpretation. However, the task still remains challenging due to text variations such as distortion, occlusion, blurring, multi-fonts, etc. Meanwhile, the strict time consumption limit in many applications is another challenge posed to STR models. 

% Figure environment removed

Among STR solutions, attention-based ones are perhaps the most popular paradigm. Typically, these methods first employ a CNN \cite{He_2016_CVPR} or ViT \cite{dosovitskiy2020vit} as the vision model. During decoding, they retain all the extracted visual features and dynamically select them for recognition, depending on the decoder used. There are two major decoders, i.e., autoregressive (AR) and parallel decoding (PD) decoders. AR decoders are widely adopted and among the most accurate decoders in NLP tasks like machine translation \cite{NIPS2017_attn, seqtoseq, SMT}, owing to their superior language modeling (LM) capability. By using AR decoder for STR, as shown in Fig.\ref{fig:fig1}(a), the characters are recognized one-by-one and when decoding the \emph{i}-th character, the previously identified \emph{i-1} characters are regarded as priors and fed into the decoder. By doing so, linguistic knowledge is implicitly encoded. However, AR decoders run slowly due to the use of an iterative decoding scheme. Meanwhile, earlier studies have reported accuracy \cite{li2019sar,Sheng2019nrtr} worse than the top-ranked methods, which is inconsistent with the observations in NLP tasks. Recently, several efforts \cite{wan2020textscanner,yue2020robustscanner,zheng2021cdistnet} have shown that the accuracy of AR decoders could be improved and ranked top-tier by carefully modeling the character position clue. They suggest that LM is not the only clue captured by AR decoding.




In contrast, PD decoders simultaneously infer all the characters in a single forward pass, as shown in Fig.\ref{fig:fig1}(b). They have received much research attention \cite{yu2020srn,fang2021abinet,Wang_2021_visionlan} due to the advantage of inference speed compared to AR decoders. However, they also encounter the problem that the recognition context could not be stably established in one forward computation. To compensate for this, recent efforts \cite{fang2021abinet,Wang_2021_visionlan} have focused more on better exploiting non-visual clues and have attained impressive recognition accuracy. For example, ABINet \cite{fang2021abinet} introduced an external LM to aid the recognition, and an iterative scheme was devised for further context modeling and possible character correction. Its accuracy is top-tier in public benchmarks. 

Aiming at inheriting the merits of both AR and PD decoders, i.e., accurate and fast, we first investigate why early Transformer-based AR models perform inferior. Our exploration reveals that the reason lies in the outdated vision model and training strategy. Specifically, NRTR \cite{Sheng2019nrtr}, a typical Transformer-based AR model for STR, employs a weak vision model. We upgrade it by using SVTR (e.g., SVTR-T) \cite{duijcai2022svtr} and adopting the same training strategy as in \cite{fang2021abinet}. Surprisingly, such a straightforward modification significantly improves the accuracy and makes the generated SVTR-NRTR new state-of-the-art recognizers in terms of accuracy. The result of SVTR-NRTR reaching the top of the range motivates us to choose it as a baseline for exploring the keys of AR. Based on this, we further analyze the context modeled during AR decoding. Our study suggests that context modeling is more complicated for STR due to the difference between visual and text modalities. For NLP tasks, what is modeled is an LM that records the appearance probability of a character given a preceding character sequence. However, for STR, in addition to the linguistic association between characters, visual context variables such as character appearance and positions can also be different and affect recognition. Nevertheless, how AR decoders utilize these clues is still unexplored, hindering the design of a more advanced recognizer. 


To explore this further, we conduct empirical experiments based on SVTR-T-NRTR (i.e., SVTR-T as the vision model). The results suggest that AR decoding outperforms PD by a relatively large accuracy margin. It is mainly because, in addition to LM, AR decoding can capture recognition context such as the count, reading order, and positions of previously decoded characters. Moreover, when adding a groundtruth-based side loss to PD, it achieves some accuracy gain but is still worse than AR, implying that a character prediction-guided loss is insufficient to fully infer the context. Our attempts basically indicate that properly modeling the visual context is the key to bridging the accuracy gap between AR and PD decoders.

With the exploration above, we argue that accurate and efficient STR models require fast and robust estimation of the recognition context. To this end, we propose a new PD decoder termed Character Perception Parallel Decoder (CPPD), which aims at attaining similar accuracy as SVTR-NRTR while speeding up the inference. As shown in Fig.\ref{fig:fig1}(c), we devise a character ordering (CO) module and a character counting (CC) module. Both employ cross-attention for targeted feature enhancement. By using dedicated side losses as guidance, CO successfully estimates the content-free placeholders of the characters and their reading order, while CC predicts the occurrence count of each character. Moreover, the character prediction task also guides CC to fill in the placeholders with characters. Thus, combining these clues forms a complete context description that robustly gives what the character sequence is and where the characters appear. Since the context above can be established in a single forward propagation, CPPD successfully mimics a visual and linguistic involved context similar to AR decoding but accelerates the inference significantly. We carry out extensive experiments on both English and Chinese benchmarks. The results demonstrate that CPPD models achieve highly competitive accuracy while running much faster than their AR counterparts, making them among the fastest recognizers to date. 

The contribution of this paper is threefold. First, we verify that AR decoding is the most accurate scheme in STR, and reveal that besides LM, character count, reading order and positions are essential context variables. Second, we devise CPPD, a novel PD decoder that incorporates both CO and CC modules to perceive the desired context in a single forward pass. Third, experiments on public benchmarks demonstrate the superiority of CPPD. It achieves high accuracy and very fast inference speed, consuming an average of 3.04 ms (the tiny version) per text instance on one NVIDIA 1080Ti GPU.


\section{Related Work}
The advancement of deep learning has propelled STR into the attention-based encoder-decoder era \cite{chen2021text, YANG20221458}. Generally, the encoder extracts visual, position, and/or other features. The decoder integrates recognition clues from different aspects and generates the recognized sequence. Based on how the decoder is formulated, we classify existing models into three categories, i.e., CTC, AR and PD models.

CTC models implement the recognition via a two-step alignment, i.e., character-image sub-region mapping and CTC decoding \cite{CTC}. For example, CRNN \cite{shi2017crnn} first extracted image feature by jointly utilizing CNN and RNN. It then inferred a character or a blank for each image sub-region. The recognized sequence was obtained by de-duplicating characters and removing blanks according to the learned CTC rule. In \cite{hu2020gtc}, GTC employed a graph neural network and attention mechanism to improve both feature learning and CTC decoding. SVTR \cite{duijcai2022svtr} devised a dedicated ViT to capture multi-grained character features and attained impressive accuracy using only CTC decoding. CTC models have fast inference speed in general. Nevertheless, they implicitly assume that the recognition is based on character-image sub-region correspondences, restricting their ability to accommodate text variations.

% Figure environment removed

% Figure environment removed

Inspired by the success of attention-based models in NLP, AR models were introduced to the STR field in \cite{shi2019aster,li2019sar,wang2020decoupled}, where gated RNN \cite{lstm, gru} was employed to establish the decoding and showed accuracy advantages. NRTR \cite{Sheng2019nrtr} built the first Transformer-based AR decoder \cite{NIPS2017_attn} for STR. It achieved top accuracy at that time. Later, its accuracy is further improved by using ResNet as the vision model \cite{mmocr2021}. SATRN \cite{lee2020recognizing} was developed based on a 2D Transformer to better preserve 2D features. To suppress attention drift during decoding, character position embedding was leveraged, and different position enhancement schemes were proposed in \cite{wan2020textscanner,yue2020robustscanner,zheng2021cdistnet}. They got improved accuracy, implying that the character position was also a critical clue for STR. PARSeq \cite{BautistaA22PARSeq} learned a permuted AR sequence model by utilizing LM permutation. AR decoder was also applied to recognize artistic text in \cite{xie2022toward}. Despite the progress, AR models perform recognition in a character-by-character manner. The inference speed is an issue and hinders their applications in fast-response scenarios. 

PD models were proposed to seek accurate and fast STR. Generally, the speed advantage came from PD while the accuracy was attributed to exploiting non-visual clues so far. For example, SRN \cite{yu2020srn} inferred all the characters at once, where a groundtruth-based side loss was imposed to guide the context modeling. In \cite{fang2021abinet}, Fang et al. introduced external LM and the iterative mechanism to PD decoder. It achieved highly competitive accuracy. Wang et al. \cite{Wang_2021_visionlan} devised an LM network to encode the linguistic information, which directly endued the vision backbone with LM capability. During inference, the LM network was discarded to ensure speedup. Qiao et al. \cite{qiao2021pimnet} proposed PIMNet. It used a more accurate AR decoder to distill the PD decoder. The AR decoder was discarded at inference, achieving a better balance between accuracy and efficiency.

Note that CC-like clues have been used in several previous studies. For example, they were explored in the forms of predicting sequence length \cite{jiang2021reciprocal}, or estimating character count using CAN \cite{li2022counting} and ACE \cite{xie2019aggregation}. However, CAN was dedicated to mathematical expression recognition, and ACE used a probability-based sequence-level loss. Our usage differs from those practices, as explained below. Meanwhile, some works have explored position information \cite{yue2020robustscanner,wang2020decoupled,zheng2021cdistnet,yu2020srn,fang2021abinet,Wang_2021_visionlan}. They initialized position embedding with a fixed value and then fused it with other clues to make predictions. However, they seldom employed a dedicated side loss to guide feature learning. As a result, our CO module is also quite different from these references.


%------------------------------------------------------------------------
% Figure environment removed

\section{AR Decoder Revisit}
As stated, earlier AR models perform worse than the top-ranked models by some margin. An example is NRTR \cite{Sheng2019nrtr}. We first adopt the same training strategy as in \cite{fang2021abinet} to train NRTR. The obtained model gives an averaged accuracy of 89.62\% on the six benchmarks \cite{whatwrong}. We then employ SVTR-T as its visual encoder and train it again. As seen in Tab.\ref{tab:sota}, the resulting model, SVTR-T-NRTR, improves the averaged accuracy to 91.49\%, while reducing the model size from 31.7 M to 8.6 M, and the inference time from 30.1 ms to 22.0 ms. Meanwhile, it attains an accuracy improvement of 1.5\% compared to SVTR-T, which is the gain from AR decoding. Even larger accuracy improvements are observed when SVTR-B is used as the vision model (see Tab.\ref{tab:sota}). Note that SVTR-B-NRTR is the new state-of-the-art recognizer in terms of accuracy, while SVTR-T-NRTR is also the most accurate one among those with similar model sizes. Since NRTR is a simple model, the results clearly verify the superiority of AR decoding in STR in terms of accuracy. 


Then, several experiments are conducted to further understand AR decoding. As shown in Fig.\ref{fig:exp_4}, four decoder variants are considered on top of SVTR-T-NRTR: (a) standard AR decoder, which is the same as SVTR-T-NRTR. (b) AR decoder with limited LM (AR-L), where the MHSA for LM is removed, limiting the LM modeling only from the AR decoding loop. (c) Standard PD decoder. (d) PD decoder with pre-decoding (PD-P), where a groundtruth-based side loss is added to guide the feature learning. The four variants achieve an average accuracy of 91.49\%, 91.13\%, 89.60\% and 90.43\% with inference time of 22.0, 21.2, 2.63 and 2.80 ms, respectively.


Three empirical observations are inferred from the results above. First, AR-L decreases the accuracy by 0.36\% compared to the standard one, implying the directly modeled LM is not significant. The recognition clues are mainly encoded during the AR decoding loop. Second, when changing from the AR decoder to the PD decoder, despite accelerating speed, a 1.89\% accuracy drop is observed. As seen in Fig.\ref{fig:exp_4}, the PD decoder uses a fixed content-free position embedding as the \emph{query} to associate with visual features. While the AR decoder is the iteratively updated character embedding that encodes rich context variables. Since already decoded characters are known, clues like their count, reading order, and positions are utilized during the AR decoding. On the other hand, the absence of these variables in PD makes the prediction rather challenging. Therefore the recognition context is not well modeled, as shown in Fig.\ref{fig:vis1}(c). This observation hints to us that better modeling the context variables missed by PD would be a way for accuracy improvement. Third, when pre-decoding is added, the PD-P decoder gains an accuracy improvement of 0.83\% compared to the PD decoder. Basically, this loss imposes an additional learning objective that also uses groundtruth characters as the aligning target. It drives the STR model to separate context modeling and character prediction tasks to some extent, thus enjoying more accurate character localizations, as shown in Fig.\ref{fig:vis1}(d). However, since the same character prediction-guided loss is applied twice, it is insufficient to diversely explore the recognition context. This explains why there is still a margin between AR and PD-P decoders.

The experiments basically verify that perceiving the context described above is the dominant factor for accuracy gain. AR decoder can well implement this goal as the previously decoded characters are available. However its inference speed is slow. How to appropriately model those context variables in a single decoding pass, and achieve the goal of "AR's accuracy, PD's speed" still needs exploration.





\section{The Proposed CPPD}
\label{sec:formatting}

\subsection{Overall}
We propose CPPD to address the above issue. Fig.\ref{fig:structrue} gives the overall structure of CPPD. Given a text instance of $I \in R^{H \times W \times 3}$, we first use SVTR backbone \cite{duijcai2022svtr} (e.g., SVTR-T, with the rectification module and CTC decoder both removed) as the encoder to extract visual feature $\mathbf{F}_v \in R^{ \frac{H}{16} \times \frac{W}{4} \times D}$. Meanwhile, character counting (CC) feature $\mathbf{F}_{cc} \in R^{C \times D}$ and character ordering (CO) feature $\mathbf{F}_{co} \in R^{L \times D}$ are initialized with a truncated normal distribution with mean of 0 and standard deviation of 0.2, where $C$ and $L$ are predefined parameters denoting the size of character set and the maximum length of a character sequence, respectively. On the decoder side, a CC module and a CO module are developed, which use either $\mathbf{F}_{cc}$ or $\mathbf{F}_{co}$ as the \emph{query}, $\mathbf{F}_v$ as the \emph{key} and \emph{value}, to calculate the cross-attention. Two dedicated side losses are devised to guide their learning. Their output features are further fused using another cross-attention computation. As a result, it generates the desired visual and context features. Both are fed into the PD decoder, where the cross-entropy (CE) loss is imposed to learn correct decoding. When inference, the decoded characters is obtained simultaneously in a single forward propagation. Since our contribution is on the decoder side, we detail the structure of CPPD as follows. 


\subsection{Character counting and ordering modules}
CC and CO modules are devised for feature reinforcement and context modeling. The two modules have the same internal structure but their optimization objectives are different. CC module is followed by a counting loss, which aims at describing the occurrence count of each character as accurate as possible. For example, given \emph{arteta}, the task is to tell us that character \emph{a} and \emph{t} both appear twice, character \emph{e} and \emph{r} appear once, while other characters do not appear, i.e., with count 0. By forcing this, the CC feature $\mathbf{F}_{cc}$, which is used as the \emph{query}, can search for the desired information in visual feature space. Since different characters have different appearances and two characters with the same category are likely to have similar feature representations, the \emph{query} feature can be strengthened from this dimension. 

Let $y_{c,l}=1$ if the \emph{c}-th character appears \emph{l} times in the character sequence and 0 otherwise, $p_{c,l}$ the probability that the \emph{c}-th character be predicted to appear \emph{l} times in the same sequence. The counting loss $\mathcal{L}_{cc}$ is written as:
\begin{equation}
\mathcal{L}_{cc} = -\frac{1}{C}\sum_{c=1}^C\sum_{l=0}^Ly_{c,l}\log(p_{c,l})
\label{equ:ce}
\end{equation}

Note that Eq.\ref{equ:ce} assigns a loss term for every possible character and frequency occurrence. Compared with the ACE loss (Eq.\ref{equ:ace}) \cite{xie2019aggregation} that only considered the appeared characters, our CC loss has the merits of more completely modeling characters and being decoupled from the character length. We have conducted an experiment comparing the two (see Tab.\ref{tab:loss}).

% \begin{footnotesize}
\begin{equation}
\mathcal{L}_{ace} = -\sum_{c=1}^C \frac{N_{c}}{L}\log(\frac{\sum_{l=0}^Lp_{c,l}}{L})
\label{equ:ace}
\end{equation}
% \end{footnotesize}

On the other hand, CO module is followed by an ordering loss, which aims at inferring the character ordering and locations from the first character to the last one. Also taking \emph{arteta} as an example, it focuses on deducing where the first to sixth characters appear regardless of their categories, i.e., the content-free reading order and positions. Similar to above, the CO feature $\mathbf{F}_{co}$ is treated as the \emph{query} and applied to implement this goal. Since the character foreground exhibits differences with the background, features discriminating reading order and positions could be identified and extracted from the visual feature space, and used to reinforce the \emph{query} feature.


Let $p_{l}$ be the probability that the \emph{l}-th position is predicted to have a character. The ordering loss $\mathcal{L}_{co}$ is given by:
\begin{equation}
\mathcal{L}_{co} =  -\frac{1}{L}\sum_{l=1}^L{(y_{l}\log(p_{l}) + (1 - y_{l})\log(1 - p_{l}))}
\label{equ:bce}
\end{equation}

\noindent where $y_{l}=1$ for $l\leq N$ and $y_{l}=0$ for $l> N$ always holds for a sequence of length \emph{N}. $\mathcal{L}_{co}$ is binary CE loss over all the \emph{L} positions. Note that the same label will be assigned to two different character sequences with the same length, therefore forcing CO to capture the reading order and content-free placeholders at the same time. Meanwhile, it decouples the ordering task from the final character prediction task, forming a definite CO enhancement. It is quite different from the usages in existing PD models \cite{yu2020srn,fang2021abinet,Wang_2021_visionlan}.

As for the detailed implementation of the two modules, take the CC module as an example, the visual feature $\mathbf{F}_v$ is first reshaped and concatenates with the CC feature $\mathbf{F}_{cc}$, forming the \emph{query} feature and experienced an MHA. Then layer norm and MLP are successively carried out as depicted on the right side of Fig.\ref{fig:structrue}. Residual connections are also employed for feature consolidation. Next, the feature is sliced into a new visual feature and a new CC feature with the same shape as the inputted ones. The process is performed twice and the generated features are termed $\mathbf{F}^{cc}_{v}$ and $\mathbf{\hat{F}}_{cc}$ for visual and CC, respectively. Similarly, $\mathbf{F}^{co}_{v}$ and $\mathbf{\hat{F}}_{co}$ can be obtained the same as above from the CO module.

\subsection{Fusion and PD modules}
The two modules above generate strengthened features biased towards the occurrence count of each character and the content-free reading positions. They are different but complementary. Therefore, we fuse them to generate a combined feature that better characterizes the context.

A standard Transformer block with MHA is employed to conduct the fusion as detailed in Fig.\ref{fig:structrue}. Specifically, $\mathbf{F}^{cc}_{v}$ and $\mathbf{F}^{co}_{v}$ are treated as the \emph{value} and \emph{key}, $\mathbf{\hat{F}}_{co}$ rather than $\mathbf{\hat{F}}_{cc}$ is recruited as the \emph{query}. The reason is twofold. First, $\mathbf{\hat{F}}_{co}$ encodes the reading positions, which is more in line with the final character prediction task. Second, for languages like Chinese, where the size of character set $C$ is large, using $\mathbf{\hat{F}}_{cc}$ would consume more computational resources. The fusion module outputs two kinds of features, a context feature $\mathbf{F}_{ct}$, which contains the captured context similar to that in AR decoding, and an enhanced visual feature, denoted as $\mathbf{\hat{F}}_{v}$, which is obtained by combining $\mathbf{F}^{cc}_{v}$ and $\mathbf{F}_{v}$ using element-wise addition. To further reinforce the features, CE loss described below is applied as a side loss to $\mathbf{F}_{ct}$ for pre-decoding:
\begin{equation}
\mathcal{L}_{ce} =  -\frac{1}{L}\sum_{l=1}^{L}\sum_{c=1}^Cy_{l,c}\log(p_{l,c})
\label{equ:ce2}
\end{equation}

% \begin{footnotesize}

% \end{footnotesize}

\noindent where $p_{l,c}$ is the probability that the \emph{l}-th character being classified as character \emph{c}. $y_{l,c}=1$ if character \emph{c} is the label of the \emph{c}-th character in the character set, and 0 otherwise. 

Next, the PD module adopts a Transformer structure where the context feature $\mathbf{F}_{ct}$ first undergoes a Transformer block with MHSA. This generates a new feature that is used as the \emph{query}, while $\mathbf{\hat{F}}_{v}$ as the \emph{key} and \emph{value}. They are then fed into another Transformer block with MHA. The structure is conducted twice and features for the outputted characters are generated. Finally, we implement the recognition by using a simple parallel linear prediction, where CE loss the same as Eq.\ref{equ:ce2} is imposed to learn correct decoding. 

Since the CE loss in Eq.\ref{equ:ce2} looks into a position-free character prediction, together with the side losses in Eq.\ref{equ:ce} and Eq.\ref{equ:bce}, they form a mutually reinforcing recognition context that robustly describes character content, count, reading order and positions, etc. Thus, CPPD perceives a visual and linguistic involved context similar to that captured by AR decoding considerably, and even more than it due to the inclusion of subsequent characters. Benefiting from their joint optimization, as shown in Fig.\ref{fig:structrue}, visualization on the CC side predicts the character counts and their positions accurately. While on the CO side, it clearly captures the character reading order and positions. Both of these explain the merit of CPPD from the loss perspective. 


\section{Experiments}

\subsection{Datasets and Implementation Details}
We validate CPPD on both English and Chinese datasets. For English our models are trained on two commonly used synthetic scene text datasets, i.e., MJSynth (MJ) \cite{jaderberg14synthetic,Jaderberg2015ReadingTI} and SynthText (ST) \cite{Synthetic}. Then the models are tested on six public benchmarks, i.e., ICDAR 2013 (IC13) \cite{icdar2013}, Street View Text (SVT) \cite{Wang2011SVT}, IIIT5K-Words (IIIT) \cite{IIIT5K}, ICDAR 2015 (IC15) \cite{icdar2015}, Street View Text-Perspective (SVTP) \cite{SVTP} and CUTE80 (CUTE) \cite{Risnumawan2014cute}. For IC13 and IC15, we use the versions with 857 and 1,811 images, respectively. 



For Chinese we use Chinese Scene dataset \cite{chen2021benchmarking}, a public dataset containing 509,164, 63,645 and 63,646 training, validation and test images. We use the validation set to determine our model, which is then assessed on the test set.


%\subsection{Implementation Details}


We use AdamW optimizer \cite{adamw} with a weight decay of 0.05 for training. For English models, all text instances are resized to $32 \times 100$ and the learning rate (LR) is set to $\frac{5}{10^{4}} \times \frac{batchsize}{1024}$. Cosine LR scheduler \cite{cosine} with 4 epochs linear warm-up is used in all the 20 epochs. 
Data augmentation like rotation, perspective distortion, motion blur and gaussian noise, are randomly performed during training. The alphabet includes all case-insensitive alphanumerics. For Chinese models, all text instances are resized to $32 \times 320$ and the LR are set to $\frac{3}{10^{4}} \times \frac{batchsize}{512}$. Cosine LR scheduler with 5 epochs linear warm-up is used in all the 100 epochs. Word accuracy is used as the evaluation metric. The size of characater set $C$ is set to 37 for English and 6625 for Chinese \cite{ppocrv3}. The maximum predict length $L$ is set to 25 for both. The weights for all the loss terms are set to 1 for simplicity. All models are trained on 4 Tesla V100 GPUs.


\subsection{Ablation Study} \label{section:3.3}
To better understand CPPD, we perform controlled experiments under different settings. SVTR-T is used as the vision model for efficiency.



\begin{table}[t]\footnotesize
\centering
\setlength{\tabcolsep}{4pt}{
\begin{tabular}{|c|c|c|c|}
\hline

\multicolumn{2}{|c|}{Loss}  & Counting or ordering  label of \textit{arteta}        & Avg(\%) \\
\hline\hline
\multirow{2}{*}{Ours}        & Eq.\ref{equ:ce}        & {[}a:2,e:1,r:1,t:2,other:0{]} & \multirow{2}{*}{91.81}        \\
 & Eq.\ref{equ:bce}        & {[}1,1,1,1,1,1,0,â€¦,0{]}   &         \\
 \hline
ACE        & Eq.\ref{equ:ace}        & {[}\textit{pad}:$\frac{L-6}{L}$,a:$\frac{2}{L}$,e:$\frac{1}{L}$,r:$\frac{1}{L}$,t:$\frac{2}{L}$, other:0{]} & 90.71       \\
% \hline\hline
%\multicolumn{2}{|c|}{Ordering Loss}  & "arteta"'s ordering label         & Avg(\%) \\
%\hline\hline

CE        & Eq.\ref{equ:ce2}        & {[}a,r,t,e,t,a,eos,\textit{pad},...,\textit{pad}{]} & 91.25       \\
\hline
\end{tabular}}
% \caption{Comparing BCE and CE loss for the CO module.}
\caption{Comparing the effects of different losses on CC and CO}
\label{tab:loss}
\end{table}

\begin{table}[t]\footnotesize
\centering
\setlength{\tabcolsep}{4pt}{
\begin{tabular}{|cc|cccccc|c|}
\hline
CC & CO & IC13  & SVT   & IIIT  & IC15  & SVTP  & CUTE  & Avg(\%)   \\
\hline\hline
\checkmark        & \checkmark           & 96.27          & 92.43          & 95.50          & 86.09          & 88.06          & 90.28          & 91.81 \\
        & \checkmark           & 96.41 & 92.17 & 95.13 & 84.92 & 87.89 & 87.28 & 90.63 \\
\checkmark        &            & 97.08 & 91.81 & 95.00 & 84.70 & 88.37 & 90.28 & 91.21 \\
        &            & 96.15 &	92.27 &	94.30 &	84.76 &	86.20 &	88.89 &	90.43
\\


\hline
\end{tabular}}
\caption{Ablation study on CC and CO modules.}
\label{tab:co}
\end{table}
%96.41 	92.17 	95.13 	84.92 	87.89 	87.28 
%96.15 	92.27 	94.30 	84.76 	86.20 	88.89 	90.43 
\begin{table}[t]\footnotesize
\centering
\setlength{\tabcolsep}{3pt}{
\begin{tabular}{|c|cccccc|c|}
\hline
Features        & IC13  & SVT   & IIIT  & IC15  & SVTP  & CUTE  & Avg(\%)   \\
\hline\hline
CC module & 93.82 & 88.10 & 92.43 & 80.40 & 81.24 & 84.72 & 86.78 \\
CO module & 91.66 & 86.38 & 90.53 & 78.21 & 79.02 & 83.41 & 84.86 \\
Visual encoder  & 90.55 & 86.09 & 87.77 & 72.56 & 75.50 & 76.39 & 81.48\\
\hline
\end{tabular}}
\caption{Quantitative analysis on features from the vision model, CC and CO modules.}
\label{tab:ccfeatures}
\end{table}
 %$\mathbf{F}^{cc}_{v}$ and $\mathbf{F}_{v}$
 %91.66	86.38	90.53	78.21	79.02	83.41
\begin{table}[t]\footnotesize
\centering
\setlength{\tabcolsep}{2pt}{
\begin{tabular}{|c|c|cccccc|c|}
\hline
Decoder & Encoder                   & IC13  & SVT   & IIIT  & IC15  & SVTP  & CUTE  & Avg(\%)   \\
\hline\hline
CPPD    & \multirow{3}{*}{ResNet45} & 96.35 & 92.16 & 94.80 & 85.30 & 86.67 & 89.04 & 90.72 \\
NRTR      &                           & 96.27 & 92.12 & 94.93 & 83.10 & 85.58 & 90.62 & 90.44 \\
ABINet* &                           & 95.45 & 89.34 & 93.83 & 82.94 & 84.50 & 88.54 & 89.10 \\
\hline
CPPD    & \multirow{3}{*}{SVTR-T}   & 96.27          & 91.50          & 95.53          & 86.20          & 89.91          & 91.44          & 91.81 \\
NRTR      &                           & 97.32 & 93.35 & 94.97 & 85.48 & 88.22 & 89.58 & 91.49\\

ABINet*      &                           & 95.57 & 91.19 & 93.80 & 83.99 & 85.89 & 87.15 & 89.60\\
\hline
\end{tabular}}
\caption{Results on adaptability to different encoders.}
\label{tab:encoder3}
\end{table}
%96.35 	92.16 	94.80 	85.30 	86.67 	89.04 	90.72 
%96.27 	91.50 	95.53 	86.20 	89.91 	91.44 	91.80810316
%95.57 	91.19 	93.80 	83.99 	85.89 	87.15 
% \begin{table*}[t]\footnotesize
% \centering
% \setlength{\tabcolsep}{5pt}{
% \begin{tabular}{|c|c|c|ccc|ccc|c|cc|}
% \hline
% \multicolumn{2}{|c|}{\multirow{2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{2}{*}{Training Data}} & \multicolumn{3}{c|}{Regular}                              & \multicolumn{3}{c|}{Irregular}                            & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Average\\ accuracy\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Parameters\\ ($\times10^6$)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Time\\ (ms)\end{tabular}} \\
% \multicolumn{2}{|c|}{}  & \multicolumn{1}{c|}{}                                                     & IC13          & SVT           & \multicolumn{1}{c|}{IIIT} & IC15          & SVTP          & \multicolumn{1}{c|}{CUTE} &                                                                             &                                                                         &                                                                      \\
% \hline\hline
% \multirow{5}{*}{AR}     & ASTER \cite{shi2019aster}  & MJ+ST           &               & 89.5          & 93.4          & 76.1          & 78.5          & 79.5          &                                                                             & 27.2                                                                    &                                                                      \\
%                         & NRTR \cite{Sheng2019nrtr}  & MJ+ST           & 95.8          & 91.5          & 90.1          &     79.4          &   86.6            &     80.9          &                                                       87.38                      & 31.7                                                                    & 30.1                                                              \\
%                         & RobustScanner \cite{yue2020robustscanner}  & MJ+ST  & 94.8          & 88.1          & 95.3          & 77.1          & 79.5          & 90.3          & 87.52                                                                       &                 48.0                                                        &  68.0                                                                    \\
%                         & SAR \cite{li2019sar}   & MJ+ST+Extra           & 91.0          & 84.5          & 91.5          & 69.2          & 76.4          & 83.5          & 82.68                                                                       & 57.7                                                                    & 104                                                             \\
%                         & PARSeq \cite{BautistaA22PARSeq}   & MJ+ST        & 97.0          & 93.6          & \textbf{97.0} & 86.5          & 88.9          & 92.2          & 92.53                                                                       & 23.8                                                                    & 11.8                                                                \\
% \hline
% \multirow{4}{*}{PD}     & SRN \cite{yu2020srn}   & MJ+ST           & 95.5          & 91.5          & 94.8          & 82.7          & 85.1          & 87.8          & 89.57                                                                       & 54.7                                                                    & 25.4                                                                 \\
%                         & VisionLAN \cite{Wang_2021_visionlan}  & MJ+ST      & 95.7          & 91.7          & 95.8          & 83.7          & 86.0          & 88.5          & 90.23                                                                       & 32.8                                                                    & 5.00                                                                    \\
%                         & ABINet \cite{fang2021abinet}  & MJ+ST+Extra         & 97.4          & 93.5          & 96.2          & 86.0          & 89.3          & 89.2          & 91.93                                                                       & 36.7                                                                    & 8.06                                                               \\
%                         & GTR \cite{HeC0LHWD22GTR}   & MJ+ST           & 96.8          & 94.1          & 95.8          & 84.6          & 87.9          & 92.3          & 91.92                                                                       & 42.1                                                                    &                                                                      \\
% \hline
% \multirow{4}{*}{CTC}    & CRNN \cite{shi2017crnn}   & MJ+ST          & 91.1          & 81.6          & 82.9          & 69.4          & 70.0          & 65.5          & 76.75                                                                       & 8.30                                                                     & 6.30                                                                  \\
%                         & SVTR-T \cite{duijcai2022svtr} & MJ+ST          & 96.3          & 91.6          & 94.4          & 84.1          & 85.4          & 88.2          & 89.99                                                                       & 6.03                                                                    & 2.45                                                               \\
%                         & SVTR-B \cite{duijcai2022svtr}    & MJ+ST       & 97.1          & 91.5          & 96.0          & 85.2          & 89.9          & 91.7          & 91.90                                                                       & 24.6                                                                    & 5.09                                                               \\
%                         & SVTR-L \cite{duijcai2022svtr}  & MJ+ST         & 97.2          & 91.7          & 96.3          & 86.6          & 88.4          & \textbf{95.1}          & 92.54                                                                       & 40.8                                                                    & 6.67                                                                \\
% \hline
% \multirow{5}{*}{Ours}   & NRTR-re  & MJ+ST           & 96.6          & 91.8          & 93.0          & 83.4          & 86.8          & 86.1          & 89.62                                                                       & 31.7                                                                    & 30.1                                                              \\
%                         & SVTR-T-NRTR  & MJ+ST     & 97.3          & 93.4          & 95.0          & 85.5          & 88.2          & 89.6          & 91.49                                                                       & 8.56                                                                     & 22.0                                                              \\
%                         & SVTR-B-NRTR   & MJ+ST    & \textbf{98.1} & 94.6 & 95.8          & 86.3          & \textbf{90.9} & 94.1 & 93.29                                                              & 32.2                                                                    & 30.8                                                               \\
%                         %                                                    \\\cline{2-12}
%                         & SVTR-T-CPPD   & MJ+ST    & 96.3          & 91.5          & 95.5          & 86.2          & 89.9          & 91.4          & 91.81                                                                     & 11.5                                                                    & 3.04                                                               \\
%                         & SVTR-B-CPPD   & MJ+ST    & 97.7          & \textbf{94.9}          & 96.8          & \textbf{87.6} & \textbf{90.9}          & 93.4          & \textbf{93.55}                                                                       & 34.0                                                                      & 4.72                                                               \\
                        
%                         \hline
% \end{tabular}
% }
% \caption{Results on the six English benchmarks tested against existing methods. NRTR-re denotes the re-trained NRTR by adopting the same training strategy as in \cite{fang2021abinet}. Time is the average time consumption on 3000 English text instances on one NVIDIA 1080Ti GPU by using PaddlePaddle inference mode.}
% \label{tab:sota}
% \end{table*}
\begin{table*}[t]\footnotesize
\centering
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{|c|c|c|c|ccc|ccc|c|cc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Method}} & \multicolumn{1}{c|}{\multirow{2}{*}{Training Data}} & \multicolumn{1}{c|}{\multirow{2}{*}{Input Size}} & \multicolumn{3}{c|}{Regular}                              & \multicolumn{3}{c|}{Irregular}                            & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Average\\ accuracy\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Parameters\\ ($\times10^6$)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} FPS\end{tabular}} \\
\multicolumn{2}{|c|}{}  & \multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{}                                                    & IC13          & SVT           & \multicolumn{1}{c|}{IIIT} & IC15          & SVTP          & \multicolumn{1}{c|}{CUTE} &                                                                             &                                                                         &                                                                      \\
\hline\hline
\multirow{5}{*}{AR}     & ASTER \cite{shi2019aster}  & MJ+ST     & $32\times100$       &               & 89.5          & 93.4          & 76.1          & 78.5          & 79.5          &                                                                             & 27.2                                                                    &                                                                      \\
                        & NRTR \cite{Sheng2019nrtr}  & MJ+ST   & $32\times100$          & 95.8          & 91.5          & 90.1          &     79.4          &   86.6            &     80.9          &                                                       87.38                      & 31.7                                                                    & 33.2                                                              \\
                        & RobustScanner \cite{yue2020robustscanner}  & MJ+ST & $48\times160$   & 94.8          & 88.1          & 95.3          & 77.1          & 79.5          & 90.3          & 87.52                                                                       &                 48.0                                                        &  16.4                                                                    \\
                        & SAR \cite{li2019sar}   & MJ+ST+Extra  & $48\times160$           & 91.0          & 84.5          & 91.5          & 69.2          & 76.4          & 83.5          & 82.68                                                                       & 57.7                                                                    & 9.62                                                             \\
                        & PARSeq \cite{BautistaA22PARSeq}   & MJ+ST  & $32\times128$        & 97.0          & 93.6          & \textbf{97.0} & 86.5          & 88.9          & 92.2          & 92.53                                                                       & 23.8                                                                    & 84.7                                                                \\
\hline
\multirow{4}{*}{PD}     & SRN \cite{yu2020srn}   & MJ+ST  & $64\times256$           & 95.5          & 91.5          & 94.8          & 82.7          & 85.1          & 87.8          & 89.57                                                                       & 54.7                                                                    & 39.4                                                                 \\
                        & VisionLAN \cite{Wang_2021_visionlan}  & MJ+ST  & $64\times256$      & 95.7          & 91.7          & 95.8          & 83.7          & 86.0          & 88.5          & 90.23                                                                       & 32.8                                                                    & 200                                                                    \\
                        & ABINet \cite{fang2021abinet}  & MJ+ST+Extra & $32\times128$          & 97.4          & 93.5          & 96.2          & 86.0          & 89.3          & 89.2          & 91.93                                                                       & 36.7                                                                    & 124                                                               \\
                        & GTR \cite{HeC0LHWD22GTR}   & MJ+ST  & $64\times256$           & 96.8          & 94.1          & 95.8          & 84.6          & 87.9          & 92.3          & 91.92                                                                       & 42.1                                                                    &                                                                      \\
\hline
\multirow{4}{*}{CTC}    & CRNN \cite{shi2017crnn}   & MJ+ST & $32\times100$           & 91.1          & 81.6          & 82.9          & 69.4          & 70.0          & 65.5          & 76.75                                                                       & 8.30                                                                     & 159                                                                  \\
                        & SVTR-T \cite{duijcai2022svtr} & MJ+ST & $32\times100$           & 96.3          & 91.6          & 94.4          & 84.1          & 85.4          & 88.2          & 89.99                                                                       & 6.03                                                                    & 408                                                               \\
                        & SVTR-B \cite{duijcai2022svtr}    & MJ+ST & $48\times160$        & 97.1          & 91.5          & 96.0          & 85.2          & 89.9          & 91.7          & 91.90                                                                       & 24.6                                                                    & 196                                                               \\
                        & SVTR-L \cite{duijcai2022svtr}  & MJ+ST  & $48\times160$         & 97.2          & 91.7          & 96.3          & 86.6          & 88.4          & \textbf{95.1}          & 92.54                                                                       & 40.8                                                                    & 150                                                                \\
\hline
\multirow{5}{*}{Ours}   & NRTR-re  & \multirow{5}{*}{MJ+ST} & \multirow{5}{*}{$32\times100$}            & 96.6          & 91.8          & 93.0          & 83.4          & 86.8          & 86.1          & 89.62                                                                       & 31.7                                                                    & 33.2                                                              \\
                        & SVTR-T-NRTR  &  &     & 97.3          & 93.4          & 95.0          & 85.5          & 88.2          & 89.6          & 91.49                                                                       & 8.56                                                                     & 45.5                                                              \\
                        & SVTR-B-NRTR   &  &      & \textbf{98.1} & 94.6 & 95.8          & 86.3          & \textbf{90.9} & 94.1 & 93.29                                                              & 32.2                                                                    & 32.5                                                               \\
                        %                                                    \\\cline{2-12}
                        & SVTR-T-CPPD   &  &     & 96.3          & 91.5          & 95.5          & 86.2          & 89.9          & 91.4          & 91.81                                                                     & 11.5                                                                    & 329                                                               \\
                        & SVTR-B-CPPD   &  &     & 97.7          & \textbf{94.9}          & 96.8          & \textbf{87.6} & \textbf{90.9}          & 93.4          & \textbf{93.55}                                                                       & 34.0                                                                      & 212                                                               \\
                        
                        \hline
\end{tabular}
}
\caption{Results on the six English benchmarks tested against existing methods. NRTR-re denotes the re-trained NRTR by adopting the same training strategy as in \cite{fang2021abinet}. FPS is the average speed on 3000 English text instances on one NVIDIA 1080Ti GPU by using PaddlePaddle inference mode.}
\label{tab:sota}
\end{table*}

\noindent\textbf{The loss choice of CC and CO.}
As previously stated, a binary CE loss (i.e., Eq.\ref{equ:bce}) is considered in the CO module. However, a CE loss described in Eq.\ref{equ:ce2} also can implement this goal. We carry out an experiment to compare the two. As seen in Tab.\ref{tab:loss}, the standard CE loss attains an accuracy of 91.25\%, 0.56\% lower than the binary CE. The binary CE only concerns whether there are characters on the positions, while Eq.\ref{equ:ce2} aims to identify the character at each position, making it more closely aligned with the final recognition objective. We argue that such an arrangement introduces the follow-up objective too early, and makes the feature enhancement somewhat vague. This result in turn justifies the rationality of our design.

On the other hand, ACE loss \cite{xie2019aggregation} can also achieve the goal of character counting instead of our CC loss. The results in Tab.\ref{tab:loss} show that our CC loss gets slightly higher accuracy. As previously mentioned, this is mainly because our CC loss includes the appearance of all characters. 


\noindent\textbf{The effectiveness of CC and CO.}
As shown in Tab.\ref{tab:co}, four experiments are designed to enumerate the combinations of CC and/or CO modules. We take PD-P decoder as the base model. As can be seen, adding the CC or CO module individually leads to an improvement of 0.78\% and 0.20\%, respectively, while applying both increases the accuracy significantly by 1.38\%. This implies that both CC and CO modules take effect in CPPD, especially when both are presented. This result is in line with our design, where the CC module targets character content, and the CO module emphasizes content-free reading order and positions. CPPD combines these two complementary domains, allowing CC and CO to mutually reinforce and form a more robust context description.

We further conduct experiments to analyze whether the CC and CO modules contribute to better feature extraction as follows. First, the complete CPPD is trained. Then, three variants are designed by appending a CTC decoder behind either $\mathbf{F}^{cc}_{v}$, $\mathbf{F}^{co}_{v}$ or $\mathbf{F}_{v}$, and discarding the rest parts of CPPD. Next, the three variants are trained under the constraint of freezing the entire network except for the CTC decoder. When converged, their performance on the benchmarks is evaluated, and the results are shown in Tab.\ref{tab:ccfeatures}. Comparing to $\mathbf{F}_{v}$, decoding based on $\mathbf{F}^{cc}_{v}$, $\mathbf{F}^{co}_{v}$ lead to a prominent 5.30\% and 3.38\% average accuracy gain, respectively. The results clearly suggest that critical recognition context is modeled by CC and CO modules.





\noindent\textbf{Adaptability to different encoders.}
ResNet45 and SVTR-T are taken as two representative encoders for this assessment. The results are listed in Tab.\ref{tab:encoder3}, where ABINet* (i.e., ABINet w/o external LM) is employed for reference. Removing the LM is for a fair comparison. Our observation is threefold. First, regardless of the visual encoder used, CPPD achieves results better than NRTR and ABINet*, indicating the effectiveness of CPPD in recognition context modeling. Second, CPPD is well-suited to different encoders. It gets the highest accuracy when equipped behind the two encoders, demonstrating its universality. Third, SVTR-T is a powerful encoder for STR, and using SVTR as the vision model helps attain high accuracy.


\subsection{Comparison with State-of-the-Art}
% Figure environment removed
We compare SVTR-CPPD with previous works on the six benchmarks covering regular and irregular text instances in Tab.\ref{tab:sota}. The previous works are grouped into three groups, i.e., AR, PD and CTC, for better analysis. With the advanced training strategy \cite{fang2021abinet}, NRTR attains a 2.24\% average accuracy improvement. 
When combined with SVTR, its accuracy further increases. SVTR-B-NRTR reports an average accuracy of 93.29\%, surpassing the previous best accuracy \cite{duijcai2022svtr} by 0.75\% on average. It achieves the highest accuracy on IC13 and SVTP. The result demonstrates the effectiveness of both AR decoding and SVTR. When comparing SVTR-T-NRTR and SVTR-B-NRTR with the corresponding CPPD models, their average accuracy has improved 0.32\% and 0.26\%, making SVTR-B-CPPD the new state-of-the-art and SVTR-T-SVTR also the most accurate one among those with similar sizes. AR decoding only models the recognition context for already decoded characters while CPPD models all the characters. This explains why CPPD models outperform their AR counterparts in terms of accuracy. In addition to accuracy, the two CPPD models significantly accelerate the inference speed, where 7.2$\times$ and 6.5$\times$ speedup are obtained for SVTR-T-CPPD and SVTR-B-CPPD, respectively. Note that both SVTR-NRTR and SVTR-CPPD do not incorporate the rectification module while SVTR \cite{duijcai2022svtr} did. However, SVTR-T-CPPD and SVTR-B-CPPD still give 1.82\% accuracy improvement compared to SVTR-T and 1.65\% compared to SVTR-B, even though the comparison is not in our favor. 

When compared to PARSeq\cite{BautistaA22PARSeq}, which has the best accuracy among AR methods, CPPD gains about 1.02\% accuracy improvement. Meanwhile, in spite of the more parameters, CPPD benefiting from the advantages of parallel decoding, speeds up 2.5$\times$ in terms of speed compared to PARSeq\cite{BautistaA22PARSeq} which suffers from iterative decoding. When considered against the PD methods, SVTR-T-CPPD achieves comparable accuracy with only 20-35\% of parameters, demonstrating the parametric economy of CPPD. In addition, SVTR-B-CPPD receives a significant 3.32\% improvement in average accuracy while maintaining superior speed, compared to VisionLAN\cite{Wang_2021_visionlan}, the fastest of the PD methods. The experimental results demonstrate that CPPD satisfies the desired of "AR's accuracy, PD's speed" successfully.


\begin{table}[t]\footnotesize
\centering
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{|c|c|cc|}
\hline
Method         & Accuracy(\%) & Params($\times10^6$)  & FPS                             \\
\hline\hline
CRNN \cite{shi2017crnn}            & 53.4       & 12.4 &                           \\
SAR \cite{li2019sar} & 62.5        & 27.8 &                          \\
SRN \cite{yu2020srn}             & 60.1  & 64.3 &                                \\
ABINet \cite{fang2021abinet}             & 60.9  & 53.1 &                               \\
SVTR-B \cite{duijcai2022svtr}         & 71.4   & 26.3 &  153                             \\
SVTR-L \cite{duijcai2022svtr}         & 72.1  & 43.4 &   113                             \\
\hline
SVTR-B-CPPD     & 73.2 & 39.7 & 158 \\
SVTR-B-CPPD-STN & \textbf{78.1}  & 41.6 &   134   \\
\hline
\end{tabular}}
\caption{Results on Chinese Scene dataset tested against several popular methods. For CRNN, SAR, SRN and ABINet, we directly cite the accuracy and model sizes from \cite{chen2021benchmarking}.}
\label{tab:chinese}
\end{table}


We then assess the models on the Chinese Scene dataset. Ref.\cite{chen2021benchmarking,duijcai2022svtr} give the accuracy of six existing methods as listed in Tab.\ref{tab:chinese}. SVTR-B-CPPD also shows accuracy gain. It gives a 1.8\% accuracy gain compared to SVTR-B, while running a little fast due to not incorporating the rectification module. When STN is further considered, a prominent improvement of 6.7\% is obtained. This is because the dataset exhibits various deformations including strong curvature, occlusion, light-dark contrast, etc. Merely adding an STN could contribute to an improvement of 4.9\% in terms of accuracy. Meanwhile, since the inference time is not compromised by the vocabulary size (due to CC not participating in), for Chinese the growth in model size and inference time is not significant. The results convincingly validate the great generalization ability of CPPD.

\subsection{Visualization Analysis}
We first analyze the visualization maps in Fig.\ref{fig:vis1}. CPPD has a similar localization effect as AR decoding, both localizing the characters in accordance with the reading order. Note that CPPD attends two \emph{f} in the third column. This is due to the effect of CC, which lets CPPD be more aware of clues from other positions of the same character. Therefore the two \emph{f} both contribute to recognizing the first \emph{f} and ensure correct recognition.
In contrast, the other two PD decoders show worse or less explainable localizations, e.g., the localization of characters \emph{f} and \emph{e}. It implies that by constraining the feature learning from complementary aspects, CPPD successfully mimics the recognition context of AR decoding. These maps demonstrate that the high accuracy achieved by CPPD models indeed comes from the comprehensive modeling of recognition-related clues.



We then analyze the visualization in Fig.\ref{fig:vis_2} to verify whether the CPPD modules accomplish their task as desired. Specifically, four challenging instances are selected as examples. The first three examples illustrate the merit of CPPD. It shows excellent adaptation in severely curved and highly blurred scenarios. Their attention maps show that the CC, CO and recognition branches read the characters as expected. The second example, i.e., \emph{salmon}, simultaneously exhibits severe text rotation and curvation, while our uniformly resizing scheme also introduces additional text deformation. Nevertheless, CPPD still correctly read the text although some confusion is observed from a few CO maps. This example also suggests the complementary nature of the three feature maps. Different features can complement each other and, to some extent, ward off distractions. On the other hand, for the fourth example, we can see the hotspots of the first two maps in CO and recognition branches are focused on character \emph{S} and \emph{B}, respectively, which leads CPPD to misidentify the word as \emph{sbllys}. This result also illustrates the importance of the CO module. Even if the positions are accurately localized by the CC module, a wrong character perception order would confuse the recognizer and result in incorrect recognition. The example again demonstrates the necessity of the CO module.  



%------------------------------------------------------------------------
\section{Conclusion}
Our work aims at developing accurate and fast STR recognizers. By carefully analyzing the differences between AR and PD decoders, we have inferred that context features including character count, reading order and positions are valuable context variables for accurate recognition. Consequently, we have presented CPPD, a novel high-accuracy PD decoder that nicely perceives these variables using dedicated designed modules and losses, i.e., character counting, ordering modules, and corresponding losses. Experiments on English and Chinese text recognition validate our proposal. SVTR-CPPD always achieves the top-ranked accuracy compared to existing models with similar sizes. Meanwhile, CPPD models predict all the characters in parallel. They run fairly fast and are among the fastest recognizers to date. We hope that our exploration will motivate future research in STR.    

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\newpage

% \appendix
\begin{appendices}
\setcounter{figure}{0}

% Figure environment removed
\section{More visualization on attention maps of different decoder variants}

Besides the example in Fig.3 in the submitted manuscript, we list the attention maps of another three examples in Fig.\ref{fig:vis3}. Similarly, PD (line (c)) does not well localize the character to be decoded, while both AR decoding (SVTR-T-NRTR, line (a)) and our CPPD (SVTR-T-CPPD, line (e)) can accurately localize the characters according to the reading order. This difference explains the mis-recognition in the second example. Note that in the third example AR decoding mistakenly recognizes the last \emph{H} as the first decoding character while CPPD correctly decodes it. The attention map of CPPD focuses on two positions when decoding the first character, i.e., the first \emph{S} and the last \emph{H}, attributing to the CO module and the last recognition module, respectively. The CO module reinforces the reading order in the decoded context. Therefore it lets \emph{S} to be correctly localized and recognized. 



% Figure environment removed



\section{Training stability of different decoder variants}
Fig.\ref{fig:acc} highlights differences of four decoding schemes from the training stability perspective. It is seen that PD exhibits the most severe oscillation as the increase of training steps. It is mainly due to the lack of reasonable context modeling.  Despite being smaller, PD-P's oscillation is also attributed to this reason. On the contrary, since the context clues are well modeled, AR decoding and our CPPD show much less oscillation. Their decoders well match the character and its corresponding visual feature and context, making the gradient optimization much more stable. The observation validates that CPPD well mimics the decoding context captured by AR.



% Figure environment removed

% Figure environment removed
% Figure environment removed
\section{More visualizations of CPPD.}

In addition to the English visualizations shown in the submitted manuscript, we show three examples of visualizations in Fig.\ref{fig:ch1}, Fig.\ref{fig:ch2} and Fig.\ref{fig:ch3}, respectively, to better understand CPPD. They correspond to a case mixing numbers, English and Chinese characters, a Chinese scene text, and a long scene text. For each example, the visualization maps corresponding to CC module (counting map), CO module (ordering map) and the last recognition module (recognition map) of SVTR-B-CPPD are given from left to right.

To ease understanding, the counting maps are arranged as English numbers first and then Chinese characters. In particular, Chinese characters are listed in the order of appearance, and if a character appears multiple times, the order of the first appearance prevails. Note that a Chinese character may simultaneously attain multiple counting maps. The ordering maps perfectly locate the characters according to the reading order. The recognition map can be considered as a combination of the above two while still reinforced by the character-aware features. The visualizations and recognition results show that CPPD is able to accurately build recognition context for Chinese even mixed languages.


\end{appendices}
\end{document}