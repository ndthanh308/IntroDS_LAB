\begin{table*}[t]
\caption{The top-3 concepts with the highest weights of the carved interpretable models ($g$) using PCBM from the pruned subnetworks for various pruning iterations for the HAM10000 dataset. Due to the space constraint, we only report the concepts from $g$ extracted from the model for iterations 1, 2, 4, and 15 with 100\%, 90\%, 81\%, 72.9\%, 25.5\%, and 23\% weights remaining.
}
\fontsize{6.2pt}{0.20cm}\selectfont
\label{tab:ham10k}
% \vskip 0.15in
\begin{center}
% \begin{small}
% \begin{sc}
\begin{tabular}{l l l l l  }
\toprule 
        \textbf{LABEL} 
       & \textbf{100.0 \% WEIGHTS REMAINING} 
       & \textbf{90.0 \% WEIGHTS REMAINING} 
       % & \textbf{81.0 \% WEIGHTS REMAINING} 
       & \textbf{72.9 \% WEIGHTS REMAINING} 
       % & \textbf{25.5 \% WEIGHTS REMAINING} 
       & \textbf{23.0 \% WEIGHTS REMAINING} \\
\midrule 
    Malignant & 
    \vtop{
        \hbox{\strut 1. \textbf{BWV}}
        \hbox{\strut 2. \textbf{IrregularStreaks}}
        \hbox{\strut 3. \textbf{RegressionStructures}}
    }& 
    \vtop{
        \hbox{\strut 1. AtypicalPigmentNetwork}
        \hbox{\strut 2. \textbf{BWV}}
        \hbox{\strut 3. IrregularDG}
    }
    % & 
    % \vtop{
    %     \hbox{\strut 1. \textbf{IrregularStreaks}}
    %     \hbox{\strut 2. TypicalPigmentNetwork}
    %     \hbox{\strut 3. \textbf{BWV}}
    % }
    & 
    \vtop{
        \hbox{\strut 1. \textbf{IrregularStreaks}}
        \hbox{\strut 2. lAtypicalPigmentNetwork}
        \hbox{\strut 3. \textbf{RegressionStructures}}
    }
    % & 
    % \vtop{
    %     \hbox{\strut 1. \textbf{BWV}}
    %     \hbox{\strut 2. RegularStreaks}
    %     \hbox{\strut 3. \textbf{RegressionStructures}}
    % }
    & 
    \vtop{ 
        \hbox{\strut 1. AtypicalPigmentNetwork}
        \hbox{\strut 2. TypicalPigmentNetwork}
        \hbox{\strut 3. \textbf{IrregularStreaks}}
    }\\  
\midrule 
    Benign & 
    \vtop{
        \hbox{\strut 1. \textbf{TypicalPigmentNetwork}}
        \hbox{\strut 2. \textbf{RegularStreaks}}
        \hbox{\strut 3. \textbf{RegularDG}}
    }& 
    \vtop{
        \hbox{\strut 1. \textbf{TypicalPigmentNetwork}}
        \hbox{\strut 2. RegressionStructures}
        \hbox{\strut 3. \textbf{RegularDG}}
    }
    % & 
    % \vtop{
    %     \hbox{\strut 1. \textbf{RegularStreaks}}
    %     \hbox{\strut 2. \textbf{RegularDG}}
    %     \hbox{\strut 3. RegressionStructures}
    % }
    & 
    \vtop{
        \hbox{\strut 1. \textbf{RegularStreaks}}
        \hbox{\strut 2. \textbf{TypicalPigmentNetwork}}
        \hbox{\strut 3. \textbf{RegularDG}}
    }
    % & 
    % \vtop{
    %     \hbox{\strut 1. \textbf{RegularDG}}
    %     \hbox{\strut 2. RegressionStructures}
    %     \hbox{\strut 3. \textbf{RegularStreaks}}
    % }
    & 
    \vtop{
        \hbox{\strut 1. \textbf{RegularStreaks}}
        \hbox{\strut 2. \textbf{RegularDG}}
        \hbox{\strut 3. IrregularDG}
    }\\  
\bottomrule
\end{tabular}
\end{center}
\end{table*}


We perform experiments on CUB-200~\cite{wah2011caltech} and HAM10000~\cite{tschandl2018ham10000} using ResNet~\cite{he2016deep} and Inception~\cite{szegedy2015going} networks, respectively. We prune the networks for 15 iterations, removing 10\% of weights in each iteration, fine-tuning, and then pruning again. 
We conduct three experiments. First, we estimate the accuracy scores to evaluate the predictive performance of the different pruned networks and carved interpretable models. Second, we use interpretable models to estimate the top three concepts. Third, we compute the Grad-CAM-based saliency maps for each pruned network to compare the local explanations qualitatively. In all the subsequent plots, we denote the original network as the one with ``100\% weight remaining''. We use the entire $\Phi$ as the concept extractor. We describe the dataset and training configurations in detail in the supplementary materials. The code is available at: \url{https://github.com/batmanlab/lth-explain}

% We find that pruning and fine-tuning the model with LTH forces the network to concentrate on the relevant concepts, which improves the performance of the interpretable model $g$ relative to its pruned counterpart $f$. In addition, drastic pruning significantly modifies the pruned model. Therefore, it relies on irrelevant concepts, resulting in a performance decline.

