Neural network pruning~\cite{lecun1989optimal, han2015learning, li2016pruning} removes irrelevant parameters to optimize storage requirements, reduce energy consumption, and perform efficient inference. The Lottery Ticket Hypothesis (LTH)~\cite{frankle2018lottery} finds a subnetwork within a deep network by pruning the superfluous weights based on their magnitudes. Model explainability is important to engender trust in prediction. However, little exhaustive research on the explainability of LTH has been conducted. In this paper, we  investigate  the improvement/decline in pruned networks using LTH by analyzing whether they rely on relevant pixels / interpretable concepts for prediction. We accomplish this by quantifying local (explaining an individual sample) and global explanations (explaining a class) from the pruned networks using Grad-CAM-based saliency maps and PCBMs, respectively.

The literature of explaining a network in terms of pixels is quite extensive. The methods such as model attribution (\eg Saliency Map~\cite{simonyan2013deep, selvaraju2016grad}), counterfactual approach ~\cite{abid2021meaningfully, singla2019explanation}, and distillation methods~\cite{alharbi2021learning, cheng2020explaining} are examples of post hoc explainability approaches. Those methods either identify important features of input that contribute the most to the network's output~\cite{shrikumar2016not}, generate perturbation to the input that flips the network's output~\cite{samek2016evaluating}, \cite{montavon2018methods}, or estimate simpler functions that locally approximate the network output. 
Later~\cite{koh2020concept} proposes an interpretable-by-design concept bottleneck model (CBM) , in which they first identify the human interpretable concepts from the images and then utilize the concepts to predict the labels using an interpretable classifier.~\cite{yuksekgonul2022post} learns the concepts from the embedding of a trained model in PCBM. Recently, ~\cite{ghosh2023route} carves a set of interpretable models from a trained model.
However, little emphasis has been given to evaluating the explanations from the networks obtained by network pruning. In~\cite{frankle2019dissecting}, the authors discover the neuron-concept relationship by applying Net-dissection~\cite{bau2017network}. They did not study whether the set of discriminating concepts in pruned networks remains the same or changes when networks are pruned.


% Figure environment removed

In this paper, we study the relationship between pruning and explainability. Initially, we prune the deep model using LTH. We then test either of the following hypotheses:
\noindent\textbf{Hypothesis A.} The pruned networks prioritize the same relevant concepts/pixels for prediction as the original network. Hence pruning has no effect on the global/local explanations. As a result, LTH is able to identify the \emph{winning tickets}, i.e. subnetworks with comparable or superior performance to the original network.
\noindent\textbf{Hypothesis B.}  Pruning modifies the global/local explanations as the pruned networks prioritize different concepts/pixels for prediction compared to the original network. Consequently, LTH is unable to find the \emph{winning tickets}. To validate the two hypotheses, we borrow tools from explainable AI to quantify explanations for different pruned networks. Specifically, we use Grad-CAM to estimate the local explanations and use PCBM to identify the important concepts from the embeddings of each pruned network. To our knowledge, we are the first to investigate the concept-based explainability of LTH using PCBM.
% TODO by Shantanu:
% Mention 3 hypothesis by Bau et al.
% define Global vs local explanation