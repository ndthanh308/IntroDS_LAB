In this paper, we study the success/failure modes of LTH using explainability with concepts and pixels. We observe that the pruned networks using LTH with more weights highlight relevant concepts and pixels; the networks with fewer weights do not. As a result, we conclude that magnitude iterative pruning does not emphasize the relevant concepts or pixels as the original model; in fact, the opposite is true. In the future, we want to extend this study to more real-life chest-x-ray datasets, eg. MIMIC-CXR~\cite{johnson2019mimic}. Also, we want to employ \emph{Route, interpret and repeat}~\cite{ghosh2023route} algorithm to rank the samples based on the \emph{difficulty} and investigate whether the extracted concepts differ significantly for "harder" samples as we prune. Also, a teacher-student framework can be employed where the original network and the subsequent pruned networks will be considered as teacher and student respectively. The saliency maps of the teacher model will ensure the student model focuses on the relevant pixels even with the limited network capacity. 