% Figure environment removed

\noindent\textbf{Quantitative evaluation of $\boldsymbol{f}$ and $\boldsymbol{g}$.}~\cref{fig:quant_accuracy} shows the accuracies for different pruning iterations for CUB-200 and HAM10000 datasets for the original neural network ($f$) and the carved interpretable model ($g$) using PCBM. As we prune more, the performance of $f$ and $g$ deteriorates. When $\sim$28\% weights remain, the accuracy of the $f$ and $g$ drops from 84.8\% and 75.7\% to 48\% and 66\% compared to the original networks for CUB-200. Strikingly, for later iterations, $g$ performs better than $f$. As pruning removes irrelevant components from the network, the pruned networks utilize the concepts for the classification. 

\noindent\textbf{Explaining LTH.}~\cref{tab:cub} and~\ref{tab:ham10k} reports the top-3 concepts for the different interpretable models $g$ for each pruned networks from $f$ for CUB-200 and HAM10000, respectively. After training, $g$ associates each concept with a weight. A concept with a high weight implies its high predictive significance.~\cref{tab:cub} and~\ref{tab:ham10k} demonstrate that different pruned networks rely on different concepts for classifying the same class labels. For example, the initial network with 100\% weights relies on \emph{breast\_color\_grey}, \emph{upperparts\_color\_black} and \emph{primary\_color\_brown} as top-3 concepts for the bird species ``House Sparrow''. However, when 23\% of the weights remained in the network, the classification of ``House Sparrow'' relies on \emph{tail\_shape\_notched\_tail} and \emph{bill\_shape\_cone} in addition to \emph{breast\_color\_grey}. For HAM10000, the initial networks with 100\% weights identifies \emph{Blue Whitish Veil (BWV)}, \emph{IrregularStreaks} and \emph{RegressionStructures} as top-3 concepts for Malignant skin lesion. These concepts are clinically relevant for malignancy~\cite{menzies1996sensitivity, lucieri2020interpretability}. However, the network with 23\% weights identifies clinically irrelevant \emph{TypicalPignmentNetwork} as one of the identifying concepts for malignancy. ~\cref{fig:grad_cam_cub} shows similar inconsistencies in the Grad-CAM outputs for different networks. The network with 100\% weights focuses on the relevant pixels on the bird's body, but the network with $\sim$ 23\% weights also highlights the background as relevant pixels.  These findings demonstrate that drastic pruning alters the network's representation, resulting in a performance decrease.
For more results, refer to the supplementary materials.



