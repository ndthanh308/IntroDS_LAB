\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas and Deny(2022{\natexlab{a}})]{clipbrittlepose}
Amro Abbas and St{\'e}phane Deny.
\newblock Progress and limitations of deep networks to recognize objects in
  unusual poses.
\newblock \emph{arXiv preprint arXiv:2207.08034}, 2022{\natexlab{a}}.

\bibitem[Abbas and Deny(2022{\natexlab{b}})]{abbas2022progress}
Amro Abbas and Stéphane Deny.
\newblock Progress and limitations of deep networks to recognize objects in
  unusual poses, 2022{\natexlab{b}}.

\bibitem[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.
\newblock Exploring the limits of large scale pre-training, 2021.

\bibitem[Baek et~al.(2022)Baek, Jiang, Raghunathan, and
  Kolter]{agreementontheline}
Christina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter.
\newblock Agreement-on-the-line: Predicting the performance of neural networks
  under distribution shift, 2022.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
  Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Birhane and Prabhu(2021)]{imagenet_content}
Abeba Birhane and Vinay~Uday Prabhu.
\newblock Large image datasets: A pyrrhic win for computer vision?
\newblock In \emph{2021 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 1536--1546, 2021.
\newblock \doi{10.1109/WACV48630.2021.00158}.

\bibitem[Bommasani et~al.(2022)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card,
  Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya,
  Durmus, Ermon, Etchemendy, Ethayarajh, Fei-Fei, Finn, Gale, Gillespie, Goel,
  Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu, Huang,
  Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab, Koh,
  Krass, Krishna, Kuditipudi, Kumar, Ladhak, Lee, Lee, Leskovec, Levent, Li,
  Li, Ma, Malik, Manning, Mirchandani, Mitchell, Munyikwa, Nair, Narayan,
  Narayanan, Newman, Nie, Niebles, Nilforoshan, Nyarko, Ogut, Orr,
  Papadimitriou, Park, Piech, Portelance, Potts, Raghunathan, Reich, Ren, Rong,
  Roohani, Ruiz, Ryan, Ré, Sadigh, Sagawa, Santhanam, Shih, Srinivasan,
  Tamkin, Taori, Thomas, Tramèr, Wang, Wang, Wu, Wu, Wu, Xie, Yasunaga, You,
  Zaharia, Zhang, Zhang, Zhang, Zhang, Zheng, Zhou, and
  Liang]{bommasani2022opportunities}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
  Niladri Chatterji, Annie Chen, Kathleen Creel, Jared~Quincy Davis, Dora
  Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John
  Etchemendy, Kawin Ethayarajh, Li~Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
  Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori
  Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny Hong, Kyle Hsu,
  Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,
  Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang~Wei
  Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal
  Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang~Lisa Li,
  Xuechen Li, Tengyu Ma, Ali Malik, Christopher~D. Manning, Suvir Mirchandani,
  Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak
  Narayanan, Ben Newman, Allen Nie, Juan~Carlos Niebles, Hamed Nilforoshan,
  Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon~Sung Park,
  Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich,
  Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher
  Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan
  Srinivasan, Alex Tamkin, Rohan Taori, Armin~W. Thomas, Florian Tramèr,
  Rose~E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang~Michael Xie,
  Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang,
  Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.
\newblock On the opportunities and risks of foundation models, 2022.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond,
  McClelland, and Hill]{chan2022data}
Stephanie C.~Y. Chan, Adam Santoro, Andrew~K. Lampinen, Jane~X. Wang, Aaditya
  Singh, Pierre~H. Richemond, Jay McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers, 2022.

\bibitem[DeGrave et~al.(2021)DeGrave, Janizek, and Lee]{degrave2021ai}
Alex~J DeGrave, Joseph~D Janizek, and Su-In Lee.
\newblock Ai for radiographic covid-19 detection selects shortcuts over signal.
\newblock \emph{Nature Machine Intelligence}, 3\penalty0 (7):\penalty0
  610--619, 2021.

\bibitem[DeVries et~al.(2019)DeVries, Misra, Wang, and van~der
  Maaten]{devries2019does}
Terrance DeVries, Ishan Misra, Changhan Wang, and Laurens van~der Maaten.
\newblock Does object recognition work for everyone?, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dubey et~al.(2021)Dubey, Ramanathan, Pentland, and
  Mahajan]{DBLP:journals/corr/abs-2103-15796}
Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan.
\newblock Adaptive methods for real-world domain generalization.
\newblock \emph{CoRR}, abs/2103.15796, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.15796}.

\bibitem[Dulhanty and Wong(2019)]{imagenet_demographics}
Chris Dulhanty and Alexander Wong.
\newblock Auditing imagenet: Towards a model-driven framework for annotating
  demographic attributes of large-scale image datasets, 2019.

\bibitem[Fang et~al.(2022{\natexlab{a}})Fang, Ilharco, Wortsman, Wan, Shankar,
  Dave, and Schmidt]{clip_data_robustness}
Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar,
  Achal Dave, and Ludwig Schmidt.
\newblock Data determines distributional robustness in contrastive language
  image pre-training ({CLIP}).
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 6216--6234. PMLR,
  17--23 Jul 2022{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v162/fang22a.html}.

\bibitem[Fang et~al.(2022{\natexlab{b}})Fang, Ilharco, Wortsman, Wan, Shankar,
  Dave, and Schmidt]{fang2022data}
Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar,
  Achal Dave, and Ludwig Schmidt.
\newblock Data determines distributional robustness in contrastive language
  image pre-training (clip), 2022{\natexlab{b}}.

\bibitem[Fang et~al.(2023)Fang, Kornblith, and
  Schmidt]{imagenet_doesnt_transfer}
Alex Fang, Simon Kornblith, and Ludwig Schmidt.
\newblock Does progress on imagenet transfer to real-world datasets?, 2023.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenet}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A
  Wichmann, and Wieland Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock \emph{arXiv preprint arXiv:1811.12231}, 2018.

\bibitem[Geirhos et~al.(2020)Geirhos, Temme, Rauber, Schütt, Bethge, and
  Wichmann]{geirhos2020generalisation}
Robert Geirhos, Carlos R.~Medina Temme, Jonas Rauber, Heiko~H. Schütt,
  Matthias Bethge, and Felix~A. Wichmann.
\newblock Generalisation in humans and deep neural networks, 2020.

\bibitem[Goyal et~al.(2021)Goyal, Caron, Lefaudeux, Xu, Wang, Pai, Singh,
  Liptchinsky, Misra, Joulin, et~al.]{goyal2021self}
Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek
  Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et~al.
\newblock Self-supervised pretraining of visual features in the wild.
\newblock \emph{arXiv preprint arXiv:2103.01988}, 2021.

\bibitem[Goyal et~al.(2022)Goyal, Duval, Seessel, Caron, Misra, Sagun, Joulin,
  and Bojanowski]{goyal2022vision}
Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent
  Sagun, Armand Joulin, and Piotr Bojanowski.
\newblock Vision models are more robust and fair when pretrained on uncurated
  images without supervision, 2022.

\bibitem[Gustafson et~al.(2023)Gustafson, Richards, Hall, Hazirbas,
  Bouchacourt, and Ibrahim]{gustafson2023pinpointing}
Laura Gustafson, Megan Richards, Melissa Hall, Caner Hazirbas, Diane
  Bouchacourt, and Mark Ibrahim.
\newblock Pinpointing why object recognition performance degrades across income
  levels and geographies, 2023.

\bibitem[Hendrycks and Dietterich(2019)]{imagenetc}
Dan Hendrycks and Thomas~G. Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{CoRR}, abs/1903.12261, 2019.
\newblock URL \url{http://arxiv.org/abs/1903.12261}.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and
  Gilmer]{imagenetr}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
  Steinhardt, and Justin Gilmer.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart,
  Steinhardt, and Song]{imageneta}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples, 2021{\natexlab{b}}.

\bibitem[Honnibal et~al.(2020)Honnibal, Montani, Van~Landeghem, and
  Boyd]{spacey_package}
Matthew Honnibal, Ines Montani, Sofie Van~Landeghem, and Adriane Boyd.
\newblock {spaCy: Industrial-strength Natural Language Processing in Python}.
\newblock 2020.
\newblock \doi{10.5281/zenodo.1212303}.

\bibitem[Ibrahim et~al.(2022)Ibrahim, Garrido, Morcos, and
  Bouchacourt]{ibrahim2022robustness}
Mark Ibrahim, Quentin Garrido, Ari Morcos, and Diane Bouchacourt.
\newblock The robustness limits of sota vision models to natural variation,
  2022.

\bibitem[Idrissi et~al.(2022)Idrissi, Arjovsky, Pezeshki, and
  Lopez-Paz]{pmlr-v177-idrissi22a}
Badr~Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz.
\newblock Simple data balancing achieves competitive worst-group-accuracy.
\newblock In Bernhard Schölkopf, Caroline Uhler, and Kun Zhang, editors,
  \emph{Proceedings of the First Conference on Causal Learning and Reasoning},
  volume 177 of \emph{Proceedings of Machine Learning Research}, pages
  336--351. PMLR, 11--13 Apr 2022.
\newblock URL \url{https://proceedings.mlr.press/v177/idrissi22a.html}.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini,
  Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and
  Schmidt]{Ilharco_OpenCLIP_2021}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
  Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
  Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock {OpenCLIP}, July 2021.

\bibitem[Kamath et~al.(2021)Kamath, Deshpande, Kambhampati~Venkata, and
  N~Balasubramanian]{robustness_spat_adv_tradeoff}
Sandesh Kamath, Amit Deshpande, Subrahmanyam Kambhampati~Venkata, and Vineeth
  N~Balasubramanian.
\newblock Can we have it all? on the trade-off between spatial and adversarial
  robustness of neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27462--27474, 2021.

\bibitem[Kirichenko et~al.(2022)Kirichenko, Izmailov, and
  Wilson]{kirichenko2022layer}
Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock Last layer re-training is sufficient for robustness to spurious
  correlations, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Chen, Zhu, Wang, Zhang, and
  Xue]{imagenete}
Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue.
\newblock Imagenet-e: Benchmarking neural network robustness via attribute
  editing, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Evtimov, Gordo, Hazirbas, Hassner,
  Ferrer, Xu, and Ibrahim]{li2023whacamole}
Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner,
  Cristian~Canton Ferrer, Chenliang Xu, and Mark Ibrahim.
\newblock A whac-a-mole dilemma: Shortcuts come in multiples where mitigating
  one amplifies others, 2023{\natexlab{b}}.

\bibitem[Madan et~al.(2020)Madan, Henry, Dozier, Ho, Bhandari, Sasaki, Durand,
  Pfister, and Boix]{biasedcars}
Spandan Madan, Timothy Henry, Jamell Dozier, Helen Ho, Nishchal Bhandari,
  Tomotake Sasaki, Fr{\'e}do Durand, Hanspeter Pfister, and Xavier Boix.
\newblock When and how do cnns generalize to out-of-distribution
  category-viewpoint combinations?
\newblock \emph{arXiv preprint arXiv:2007.08032}, 2020.

\bibitem[Madan et~al.(2021)Madan, Sasaki, Li, Boix, and
  Pfister]{clipbrittle3Dlighting}
Spandan Madan, Tomotake Sasaki, Tzu-Mao Li, Xavier Boix, and Hanspeter Pfister.
\newblock Small in-distribution changes in 3d perspective and lighting fool
  both cnns and transformers.
\newblock \emph{arXiv preprint arXiv:2106.16198}, 2021.

\bibitem[Madan et~al.(2023)Madan, Sasaki, Pfister, Li, and
  Boix]{madan2023adversarial}
Spandan Madan, Tomotake Sasaki, Hanspeter Pfister, Tzu-Mao Li, and Xavier Boix.
\newblock Adversarial examples within the training distribution: A widespread
  challenge, 2023.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{accuracyontheline}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7721--7735. PMLR, 2021.

\bibitem[Moayeri et~al.(2022)Moayeri, Banihashem, and
  Feizi]{moayeri2022explicit}
Mazda Moayeri, Kiarash Banihashem, and Soheil Feizi.
\newblock Explicit tradeoffs between adversarial and natural distributional
  robustness.
\newblock \emph{arXiv preprint arXiv:2209.07592}, 2022.

\bibitem[Nguyen et~al.(2023)Nguyen, Ilharco, Wortsman, Oh, and
  Schmidt]{nguyen2023quality}
Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig
  Schmidt.
\newblock Quality not quantity: On the interaction between dataset design and
  robustness of clip, 2023.

\bibitem[Pan et~al.(2022)Pan, Ye, Han, Song, and Huang]{pan2022contrastive}
Xuran Pan, Tianzhu Ye, Dongchen Han, Shiji Song, and Gao Huang.
\newblock Contrastive language-image pre-training with knowledge graphs, 2022.

\bibitem[Pinto et~al.(2023)Pinto, Yang, Lim, Torr, and
  Dokania]{pinto2023regmixup}
Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.~S. Torr, and Puneet~K.
  Dokania.
\newblock Regmixup: Mixup as a regularizer can surprisingly improve accuracy
  and out distribution robustness, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision, 2021.

\bibitem[Ramaswamy et~al.(2023)Ramaswamy, Lin, Zhao, Adcock, van~der Maaten,
  Ghadiyaram, and Russakovsky]{ramaswamy2023beyond}
Vikram~V Ramaswamy, Sing~Yu Lin, Dora Zhao, Aaron~B Adcock, Laurens van~der
  Maaten, Deepti Ghadiyaram, and Olga Russakovsky.
\newblock Beyond web-scraping: Crowd-sourcing a geographically diverse image
  dataset.
\newblock \emph{arXiv preprint arXiv:2301.02560}, 2023.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{imagenetv2}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International conference on machine learning}, pages
  5389--5400. PMLR, 2019.

\bibitem[Rojas et~al.()Rojas, Diamos, Kini, Kanter, Reddi, and
  Coleman]{rojasdollar}
William A~Gaviria Rojas, Sudnya Diamos, Keertan~Ranjan Kini, David Kanter,
  Vijay~Janapa Reddi, and Cody Coleman.
\newblock The dollar street dataset: Images representing the geographic and
  socioeconomic diversity of the world.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock Imagenet large scale visual recognition challenge, 2015.

\bibitem[Ryali et~al.(2021)Ryali, Schwab, and Morcos]{ryali2021characterizing}
Chaitanya~K. Ryali, David~J. Schwab, and Ari~S. Morcos.
\newblock Characterizing and improving the robustness of self-supervised
  learning through background augmentations, 2021.

\bibitem[Shankar et~al.(2017{\natexlab{a}})Shankar, Halpern, Breck, Atwood,
  Wilson, and Sculley]{imagenet_western}
Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and
  D.~Sculley.
\newblock No classification without representation: Assessing geodiversity
  issues in open data sets for the developing world, 2017{\natexlab{a}}.

\bibitem[Shankar et~al.(2017{\natexlab{b}})Shankar, Halpern, Breck, Atwood,
  Wilson, and Sculley]{shankar2017classification}
Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and
  D.~Sculley.
\newblock No classification without representation: Assessing geodiversity
  issues in open data sets for the developing world, 2017{\natexlab{b}}.

\bibitem[Shi et~al.(2022)Shi, Daunhawer, Vogt, Torr, and Sanyal]{shi2022robust}
Yuge Shi, Imant Daunhawer, Julia~E. Vogt, Philip H.~S. Torr, and Amartya
  Sanyal.
\newblock How robust is unsupervised representation learning to distribution
  shift?, 2022.

\bibitem[Shi et~al.(2023)Shi, Carlini, Balashankar, Schmidt, Hsieh, Beutel, and
  Qin]{effective_robustness}
Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui
  Hsieh, Alex Beutel, and Yao Qin.
\newblock Effective robustness against natural distribution shifts for models
  with different training data, 2023.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{beyondneuralscalinglaws}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 19523--19536, 2022.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{robustness_synth_nat}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification, 2020.

\bibitem[Teney et~al.(2022)Teney, Lin, Oh, and Abbasnejad]{teney2022id}
Damien Teney, Yong Lin, Seong~Joon Oh, and Ehsan Abbasnejad.
\newblock Id and ood performance are sometimes inversely correlated on
  real-world datasets.
\newblock \emph{arXiv preprint arXiv:2209.00613}, 2022.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{imagenetsketch}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wightman(2019)]{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Xiao et~al.(2020)Xiao, Engstrom, Ilyas, and Madry]{imagenet9}
Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry.
\newblock Noise or signal: The role of image backgrounds in object recognition,
  2020.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\end{thebibliography}
