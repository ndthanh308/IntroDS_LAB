%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}

\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}


\usepackage{array, makecell}

% Added packages
\usepackage{xfrac}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{hhline}
\usepackage{caption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{microtype}      % microtypography
\usepackage{amsfonts}

\usepackage{chngcntr}

\usepackage[outercaption]{sidecap}
\usepackage{floatrow}
\usepackage{float}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{wrapfig}
\floatsetup[table]{capposition=top}
\usepackage[bottom]{footmisc}


% \usepackage{minted}
% \usemintedstyle{xcode}

\usepackage{xcolor} % to access the named colour LightGray
\definecolor{LightGray}{gray}{0.97}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[ruled]{algorithm2e}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{csquotes}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\font\btt=rm-lmtk10
\font\tt=rm-lmtl10

\newcommand{\mlp}{{\btt MLP}}

\newcommand{\eerror}{\Delta_{\mathrm{equiv}}}
\newcommand{\relu}{$\mathrm{ReLU}$}
\newcommand{\leakyrelu}{$\mathrm{LeakyReLU}$}
\newcommand{\swish}{$\mathrm{Swish}$}
\newcommand{\siren}{$\mathrm{SIREN}$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% PERSONAL MODIFICATIONS
\usepackage{amsmath,amsfonts,amssymb, amsthm}
\usepackage{mathtools}

%\usepackage{lmodern}
%\usepackage{txfonts}
%\usepackage{mathrsfs}

\usepackage{scalerel}
\usepackage{stackengine,wasysym}

\newcommand\reallywidetilde[1]{\ThisStyle{%
  \setbox0=\hbox{$\SavedStyle#1$}%
  \stackengine{-.1\LMpt}{$\SavedStyle#1$}{%
    \stretchto{\scaleto{\SavedStyle\mkern.2mu\AC}{.5150\wd0}}{.6\ht0}%
  }{O}{c}{F}{T}{S}%
}}

\usepackage{MnSymbol}%
\usepackage{wasysym}%
\usepackage{multirow}
% \usepackage[cal=dutchcal,
%  calscaled=1,
%  %bb=boondox,
%  scr=euler]{mathalfa}

\usepackage{dsfont}
\newcommand{\kw}[1]{{\small\textsc{\MakeLowercase{#1}}}}
%\newcommand{\mat}[1]{\ensuremath{\bm{\mathsf{\MakeUppercase{{#1}}}}}}
\newcommand{\mat}[1]{\ensuremath{{\mathbf{\MakeUppercase{{#1}}}}}}
%\renewcommand{\vec}[1]{\ensuremath{\bm{\mathsf{\MakeLowercase{{#1}}}}}}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{\MakeLowercase{{#1}}}}}
\newcommand{\set}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\gr}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\prm}[1]{\ensuremath{^{#1}}}
\newcommand{\ly}[1]{\ensuremath{^{(#1)}}}
\newcommand{\grn}[2]{\ensuremath{\gr{#1}\prm{#2}}}
\newcommand{\tuple}[1]{\ensuremath{\langle{#1} \rangle}}
\newcommand{\Reals}{\mathds{R}}
\renewcommand{\Re}{\gr{R}}
\newcommand{\Nat}{\set{N}}
\newcommand{\eye}{\mat{I}}
\newcommand{\ones}{\vec{1}}
\newcommand{\Vs}{\set{V}}
\newcommand{\Ws}{\set{W}}
\newcommand{\Ps}{\set{P}}
\newcommand{\Qs}{\set{Q}}
\newcommand{\Sg}{\gr{S}}
\newcommand{\Ig}{\gr{I}}
\newcommand{\Cg}{\gr{C}}
\newcommand{\Gg}{\gr{G}\xspace}
\renewcommand{\gg}{\gr{g}}
\newcommand{\gi}{\gr{i}}
\newcommand{\gj}{\gr{j}}
\newcommand{\gh}{\gr{h}}
\newcommand{\Hg}{\gr{H}}
\newcommand{\hg}{\gr{h}}
\newcommand{\Kg}{\gr{K}}
\newcommand{\Ng}{\gr{N}}
\newcommand{\Bg}{\gr{B}}
\newcommand{\bg}{\gr{b}}
\newcommand{\kg}{\gr{k}}
\newcommand{\Ug}{\gr{U}}
\newcommand{\ug}{\gr{u}}
%\newcommand{\Vg}{\gr{V}}
%\newcommand{\vg}{\gr{v}}
\newcommand{\Wm}{\mat{W}}
\newcommand{\Xm}{\mat{X}}
\newcommand{\Ym}{\mat{Y}}
\newcommand{\Am}{\mat{A}}
\newcommand{\Gm}{\mat{G}}
\newcommand{\Vm}{\mat{V}}
\newcommand{\Um}{\mat{U}}
\newcommand{\Pm}{\mat{P}}

\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
% Don't use a set called E, because this would be the same as our symbol
% for expectation.
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\newcommand{\xv}{\vec{x}}
\newcommand{\bv}{\vec{b}}
\newcommand{\fv}{\vec{f}}
\newcommand{\cv}{\vec{c}}
\newcommand{\W}{\Wm}
\newcommand{\X}{\Xm}
\newcommand{\Y}{\Ym}
\newcommand{\A}{\Am}
\newcommand{\x}{\xv}
\newcommand{\Ns}{\set{N}}
\newcommand{\Ms}{\set{M}}
\newcommand{\As}{\set{N}}
\newcommand{\Bs}{\set{M}}

\newcommand{\Nin}{\mathrm{N_{in}}}
\newcommand{\Nout}{\mathrm{N_{out}}}
\newcommand{\Bt}{\mathrm{B}}
\newcommand{\Lt}{\mathrm{L}}
\newcommand{\Nt}{\mathrm{N}}
\newcommand{\Dt}{\mathrm{D}}
\newcommand{\Kt}{\mathrm{K}}

\newcommand{\rd}{{\rm d}}
\newcommand{\ri}{{\rm i}}
\newcommand{\re}{{\rm e}}
\newcommand{\ro}{{\rm o}}

\newcommand{\Ft}{\mathrm{F}}
\newcommand{\Ht}{\mathrm{H}}
\newcommand{\itt}{\mathrm{i}}
\newcommand{\jtt}{\mathrm{j}}

\newcommand{\ev}{\vec{e}}



\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mathematicablue}{rgb}{0.11, 0.25, 0.467}
\hypersetup{colorlinks,citecolor={mydarkblue},urlcolor={mydarkblue}, linkcolor={red}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learned Gridification for Efficient Point Cloud Processing}

\begin{document}

\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}

\twocolumn[
\icmltitle{Learned Gridification for Efficient Point Cloud Processing}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Putri A. van der Linden$^*$}{uva}
\icmlauthor{David W. Romero$^*$}{vu}
\icmlauthor{Erik J. Bekkers}{uva}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{vu}{Vrije Universiteit Amsterdam, The Netherlands}
\icmlaffiliation{uva}{University of Amsterdam.}

\icmlcorrespondingauthor{Putri A. van der Linden}{p.a.vanderlinden@uva.nl}
\icmlcorrespondingauthor{David W. Romero}{d.w.romeroguzman@vu.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.\newline 
In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose \textit{learnable gridification} as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., \texttt{Conv3D}, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a \textit{learnable de-gridification} step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that \textit{gridified networks} scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at \url{https://github.com/computri/gridifier}.
\end{abstract}
\vspace{-9mm}
\section{Introduction}\label{sec:intro}
\vspace{-1mm}
% Figure environment removed
Point clouds provide sparse geometric representations of objects or surfaces equipped with signals defined over their structure, e.g., the surface normals of an underlying object \cite{wu20153d, qi2017pointnet} or the chemical properties of a molecule \cite{ramakrishnan2014quantum,schutt2017schnet}. Several neural operators have been developed that can be applied to such sparse representations provided by point clouds. These methods can be broadly understood as continuous generalizations of neural operators originally defined over regular discrete grids, e.g., convolution \cite{wu2019pointconv} and self-attention \cite{zhao2021point}.
% In order to define learning methods over this kind of data, several neural operators have been developed which can be applied on the sparse representations provided by point clouds. These methods can be broadly understood as continuous generalizations of neural operators defined over regular discrete grids, e.g., convolution \cite{wu2019pointconv} and self-attention \cite{zhao2021point}.
% Figure environment removed

\textbf{The problem of learning on raw point clouds.} Unfortunately, the flexibility required from neural operators to accommodate irregular sparse representations like point clouds brings about important increases in time and memory consumption. This is especially prominent in neural operations that construct feature representations based on neighborhood information, e.g., convolution. In the case of point clouds, the irregular distances between points make these neural operations significantly more computationally demanding compared to regular grid representations like images or text. For instance, for convolution, the convolutional kernel needs to be recalculated for each point in a point cloud to account for irregular distances from the query point to other points in its neighborhood (Fig.~\ref{fig:convs} left). In contrast, grid representations standardize pairwise distances following a grid structure (Fig.~\ref{fig:convs} right). As a result, the distances from a point to all other points in its neighborhood are fixed for all points queried in the grid. Therefore, it is possible to compute the kernel once, and reuse it across all query positions. This difference illustrates that operations relying on neighborhood information scale much worse in terms of memory and time for point clouds than for grid data, specially for large inputs and large neighborhoods.
% \textbf{The problem of learning on raw point clouds.} Unfortunately, the flexibility required from neural operators to accommodate irregular sparse representations like point clouds brings about important increases in time and memory consumption. This is specially prominent in neural operations that construct feature representations based on neighborhood information, e.g., convolution. In the case of point clouds, the irregular distances between points make these neural operations significantly more computationally demanding compared to regular grid representations like images or text. For instance, in a convolution, the convolutional kernel needs to be recalculated for each point in a point cloud to account for changing distances from the query point to other points in its neighborhood (Fig.~\ref{fig:convs} left). In contrast, grid representations standardize pairwise distancess following a grid structure (Fig.~\ref{fig:convs} right). As a result, the distances from a point to all other points in its neighborhood are equal for all points queried in the grid. Therefore, it is possible to compute the kernel once, and reuse it across all query positions. This difference illustrates that operations relying on neighborhood information scale much worse in terms of memory and time for point clouds than for grid data, specially for large inputs and large neighborhoods.
% Figure environment removed

\textbf{A potential solution: Voxelization.} A potential solution to address the challenges posed by point clouds lies in treating the point cloud as a continuous density that can be sampled on a dense regular grid: a process called \textit{voxelization} \cite{maturana2015voxnet, wu20153d}. The idea of voxelization is to create a grid that overlaps with the domain of the point cloud (Fig.~\ref{fig:voxelization}). Although voxelization methods create grid representations on which neural operations defined on grids can act, e.g., \texttt{Conv3D}, the grids resulting from voxelization are oftentimes much larger than the number of points in the original point cloud. This is a consequence of (\textit{i}) the high-resolution grids required to describe fine details from the point cloud, and \textit{(ii)} the low occupancy of the grid resulting from the sparse nature of point clouds which generally leads to many more points to process in the resulting grid than in the original point cloud.

\textbf{Our proposed solution: Gridification.} In this paper, we propose an alternative solution to address the memory and computational scalability of point cloud methods by addressing its root cause: \textit{the irregularity of the data}. We propose \textit{learnable gridification} as the first step in a point cloud processing pipeline to transform the point cloud into a \textit{compact, regular grid} (Fig.~\ref{fig:gridification}). Thanks to gridification, subsequent layers can use operations defined on grids, e.g., \texttt{Conv3D}, which scale much better than native point cloud methods. In a nutshell, gridification can be understood as a \textit{convolutional message passing} layer acting on a \textit{bipartite graph} that establishes connections between points in the point cloud to points in the grid given by a \textit{bilateral $k$-nearest neighbor connectivity}. The proposed bilateral $k$-nearest neighbor connectivity guarantees that all points both in the point cloud and in the grid are connected, therefore allowing for the construction of expressive yet compact grid representations.

In contrast to voxelization, gridification produces expressive compact grid representations in which the number of points in the resulting compact regular grid is roughly equal to the number of points in the original point cloud, yet the grid is able to preserve fine geometric details from the original point cloud. For instance, we observe that point clouds with $\Nt{=}1000$ points can be effectively mapped to a compact dense $\mathrm{10x10x10}$ grid without significant information loss. We show through theoretical and empirical analysis that the resulting grid representations scale much better in terms of memory and time than native point cloud methods. This is verified on several comparison studies for increasing number of points in the point cloud and increasing neighborhood sizes in the construction of convolutional kernels.

We demonstrate that gridification can also be used for tasks from point clouds to point clouds, e.g., segmentation. To this end, we introduce a \textit{learnable de-gridification} step at the end of the point cloud processing pipeline, which can be seen as an inverted gridification step that maps the compact, regular grid back to its original point cloud form. This extension allows for the construction of \textit{gridified networks} --networks that operate on grids-- to solve global prediction tasks, e.g., classification, as well as dense prediction tasks, e.g., segmentation and regression, on point cloud data.


%\textbf{Results.} {\color{green} TODO}%Through several experiments, we demonstrate that gridified networks are competitive to native point cloud methods while being much more efficient in terms of memory and computation speed. This is observed on both classification and segmentation tasks on ModelNet, ShapeNet, S3DIS and ScanObjectNN.

\vspace{-3mm}
\section{Method}\label{sec:method}
\vspace{-1mm}
\subsection{Point cloud and grid representations} 
\vspace{-1mm}
\textbf{Point cloud.} A point cloud $\gP {=}\{(\cv^\gP_i, \xv^\gP_i)\}_{i{=}1}^{\Nt_\gP}$ is an \textit{unstructured} set of $\Nt_\gP$ pairs of coordinate-feature values $(\cv^\gP_i, \xv^\gP_i)$ scattered in space without any predefined pattern or connectivity. Point clouds sparsely represent geometric structures through pairs of coordinate vectors $\cv^\gP_i \in \sR^\Dt$ and corresponding function values over that geometric structure $\xv_i^\gP \in \mathbb{R}^{\Ft_{\gP}}$, e.g., surface normals, RGB-values, electric potentials, etc.

\textbf{Grid.} A grid $\gG{=}\{(\cv^\gG_i, \xv^\gG_i)\}_{i{=}1}^{\Nt_\gG}$ can be interpreted as a point cloud on which the coordinate-feature pairs $(\cv^\gG_i, \xv^\gG_i)$ are arranged in a regular pattern that form a lattice. In contrast to general point clouds, points in a grid are evenly spaced and align along predefined axes, e.g., $x$, $y$, $z$. The regular spacing between points leads to regular pairwise distances for all query points in the grid. As a result, we can calculate pairwise attributes once, and reuse them for all query points. 

\vspace{-3mm}
\subsection{Gridification: From a point cloud to a dense grid}
\vspace{-1mm}
We seek to map the sparse point cloud $\gP {=}\{(\cv^\gP_i, \xv^\gP_i)\}_{j{=}1}^{\Nt_\gP}$ onto a compact regular grid $\gG {=}\{(\cv_i^\gG, \xv_i^\gG)\}_{i{=}1}^{\Nt_\gG}$ in $\sR^\Dt$. We formalize this process as an operation over a \textit{bipartite graph} that establishes connections between points in the point cloud $\gP$ to points in the grid $\gG$ given by a \textit{connectivity scheme} $\gE_{\gP \rightarrow \gG}$ defined as a set of edges $\ev_{j \rightarrow i} \in \gE_{\gP \rightarrow \gG}$.

\textbf{Learnable gridification as message passing.} We aim to learn a mapping from $\gP$ to $\gG$ such that the grid representation $\gG{=}\{(\cv^\gG_i, \xv^\gG_i)\}_{i{=}1}^{\Nt_\gG}$, $\cv_i \in \sR^\Dt$, $\xv^\gG_i \in \mathbb{R}^{\Ft_{\gG}}$, adequately represents the source point cloud $\gP$ for the downstream task. Given a source point cloud $\gP$, a target grid $\gG$ and a connectivity scheme $\gE_{\gP \rightarrow \gG}$, we define gridification as a \textit{convolutional message passing} layer \cite{gilmer2017neural} on the bipartite graph $(\gP, \gG, \gE_{\gP \rightarrow \gG})$ defined as:
\begin{equation}
    \xv^\gG_i = \phi_{\mathrm{upd}} \left( \bigoplus_{\ev_{j \rightarrow i} \ \in\  \gE_{\gP \rightarrow \gG}}\phi_{\mathrm{msg}}\Big(\phi_{\mathrm{node}}\left(\xv^\gP_j\right),  \phi_{\mathrm{pos}}\left(\cv^\gG_i - \cv^\gP_j\right) \Big) \right). 
\end{equation}
It consists of a node embedding network $\phi_\mathrm{node}: \sR^{\Ft_{\gP}} \rightarrow \sR^{\Ht}$ that processes the point cloud features $\xv_i^\gG$, a positional embedding network $\phi_\mathrm{pos}: \sR^{\Dt} \rightarrow \sR^{\Ht}$ that creates feature representations based on the pairwise distances between coordinates in $\gG$ and $\gP$ --thus resembling a convolutional kernel--, a message embedding network $\phi_\mathrm{msg}: \sR^{2\Ht} \rightarrow \sR^{\Ht}$ that receives both the node embedding and the relative position embedding to create the so-called \textit{message}. After the messages are created for all nodes described by connectivity of the node, these features are aggregated via the aggregation function $\bigoplus$, e.g., $\max$, $\mathrm{mean}$. Finally, the aggregated message is passed through the update network $\phi_\mathrm{upd}: \sR^{\Ht} \rightarrow \sR^{\Ft_{\gG}}$ to produce the grid feature representations $\xv_i^{\gG} \in \sR^{\Ft_{\gP}}$.
\vspace{-3mm}
\subsection{De-gridification: From a dense grid to a point cloud}
\vspace{-1mm}
To extend the use of gridification to tasks from the point cloud $\gP$ to the point cloud $\gP$, e.g., segmentation, regression, we define a \textit{de-gridification} step that sends a grid representation $\gG$ back to its original point cloud form $\gP$. Formally, the de-gridification step is defined as:
\begin{equation}
    \xv^\gP_{i} = \phi_{\mathrm{upd}} \left( \bigoplus_{\ev_{j \rightarrow i} \ \in\  \gE_{\gG \rightarrow \gP}}\phi_{\mathrm{msg}}\Big( \phi_{\mathrm{node}}\left(\xv^\gG_j\right),  \phi_{\mathrm{pos}}\left(\cv^\gP_i - \cv^\gG_j\right) \Big) \right). 
\end{equation}
Intuitively, de-gridification can be interpreted as a gridification step from $\gG$ to $\gP$ given by an inverted connectivity scheme $\gE_{\gG \rightarrow \gP} {=} ( \gE_{\gP \rightarrow \gG})^{-1}$. Note that, it is not necessary to calculate the connectivity scheme for the de-gridification step. Instead, we can obtain it simply by taking the connectivity scheme from the gridification step $\gE_{\gG \rightarrow \gP}$ and inverting the output and input nodes of the edges.
\vspace{-3mm}
\subsection{Requirements and properties of gridification}\label{sec:requirements}
\vspace{-1mm}
We desire to construct a compute and memory efficient grid representation $\gG$ that captures all aspects of the point cloud $\gP$ as good as possible. That is, a compact, yet rich grid representation $\gG$ that preserves the structure of the point cloud $\gP$ with as low loss of information as possible. With this goal in mind, we identify the following requirements:
\begin{enumerate}[label=(\textit{\roman*}), topsep=0pt, leftmargin=*, itemsep=0pt]
\item The number of points in the grid $\Nt_\gG$ should be at least as large as the number of points in the point cloud $\Nt_\gP$.\break
\vspace{-4mm}
%\textcolor{blue}{We desire the gridification step to be able to represent \textit{at least} a one-to-one or one-to-many mapping. We wish gridification to be injective function so as to ensure no loss of information.}
\item The width of all hidden representations of the node embedding network $\phi_\mathrm{node}$ should be \textit{at least as large} as the width of the point cloud features $\xv_i^\gP$, i.e., $\Ft_{\gP}$. %\textcolor{blue}{The reasoning behind this property follows that of $(i)$.}
\item The width of all hidden representations of the position embedding network $\phi_\mathrm{pos}$ should be at least as large as the dimension of the domain $\Dt$.% \textcolor{blue}{This requirement follows from the desire for injectivity.}
\item The width of all hidden representations of the embedding networks $\phi_\mathrm{upd}$, $\phi_\mathrm{msg}$ should be \textit{at least as large} as the width of the point cloud features $\xv_i^\gP$ plus the dimension of the domain $\Dt$.% \textcolor{blue}{This requirement follows from the desire for injectivity.}
\item Each point $\cv^\gP$ in the point cloud should be connected to \textit{at least} one point $\cv^\gG$ in the grid. %\textcolor{blue}{This requirement ensures that all input points contribute to the grid representation, and no information is discarded.}
\item The positional embedding network $\phi_\mathrm{pos}$ should be able to describe high frequencies. %to avoid oversmooted representations of the point cloud onto the grid \gG.
\item Each point $\cv^\gG$ in the grid should be connected to \textit{at least} one point $\cv^\gP$ in the point cloud. %\textcolor{blue}{Unlike voxelization methods, we desire full occupancy of the grid representation.}
%\item The domain of the grid $\gG$ should correspond to the domain of the source point cloud $\gP$. %\textcolor{magenta}{Not sure what is meant with this one?}
\end{enumerate}
\textbf{Preventing information loss.} To prevent information loss, we want to avoid any kind of compression either in the grid representation or in any intermediary representation during the gridification process. Consequently, we restrict the number of points as well as the width of all representations to be at least as big as the corresponding dimensions in the source point cloud $\gP$ --items (\textit{i})-(\textit{iv})--. In addition, we must make sure that all points in the point cloud are connected to points in the grid to prevent points from being disregarded during gridification --item (\textit{v})--. Finally, we must also make sure that the positional embedding network $\phi_\mathrm{pos}$ is able to represent high frequencies --item (\textit{vi})--. This is important as multilayer perceptrons ($\mathrm{MLP}$s) with piecewise nonlinearities, e.g., $\mathrm{ReLU}$, have been shown to have an implicit bias towards smooth functions \cite{tancik2020fourier, sitzmann2020implicit}. In the context of gridification, this means that using conventional $\mathrm{MLP}$s for the positional embedding network $\phi_\mathrm{pos}$ could result in over-smooth grid representations unable to represent fine details from the source point cloud. We circumvent this issue by using parameterizations for $\phi_\mathrm{pos}$ able to model high frequencies (Sec.~\ref{sec:pos_network}).

\textbf{Encouraging compact representations.} In addition to encouraging no information loss, we also identify requirements that encourage the resulting grid representation to be compact and expressive. First, we note that item (\textit{v}) is important for this end as well, as over-smooth representations implicitly require higher resolutions to be able to encode fine-grained details. Additionally, we impose all points in the grid to be connected to points in the point cloud --item (\textit{vii})-- to prevent the grid representation from having low occupancy. This restriction allows us to make sure that all the spatial capacity of the grid is being used. This in turn allows us to construct compact rich grid representations.
\vspace{-3mm}
\subsection{Materializing the gridification module}
\vspace{-1mm}
Based on the previous requirements and properties, we define the components of the gridification module as follows:

\vspace{-2mm}
\subsubsection{The grid $\gG$}
\vspace{-1mm}
 Let $[a, b]^\Dt$ be the domain of the point cloud $\gP$, i.e., $\cv^\gP_i \in [a, b]^{\Dt}$, $\forall\ \cv^\gP_i\in \gP$. Then, we define the regular grid $\gG$ over the same domain $[a, b]^\Dt$ with $\sqrt[\leftroot{-3}\uproot{3}\Dt]{\Nt^{\gG}}$ points along each dimension. By doing so, we guarantee that the grid $\gG$ is uniformly spaced over the domain of the point cloud, therefore (\textit{i}) preserving the statistics of the input point cloud, and (\textit{ii}) being able to represent the underlying signal in the same range. In practice, point clouds are normalized during the preprocessing steps preceding a point cloud processing pipeline. As a result, we often have that $a{=}-1$ and $b{=}1$, leading to a point cloud and a grid defined on $[-1, 1]^\Dt$.
 \vspace{-2mm}
\subsubsection{The connectivity scheme $\gE_{\gP \rightarrow \gG}$}
\vspace{-1mm}
Motivated by the requirements in Sec.~\ref{sec:requirements}, we opt for \textit{bilateral $k$-nearest neighbor connectivity} over common alternatives such as radius connectivity \cite{qi2017pointnet, qi2017pointnet++} or one-way $k$-nearest neighbor connectivity \cite{barber1996quickhull, connor2010fast} for the construction of the connectivity scheme $\gE_{\gP \rightarrow \gG}$ to guarantee that no points either in the grid $\gG$ nor the point cloud $\gP$ are disconnected. Bilateral $k$-nearest neighbor connectivity consists of a two-way $k$-nearest neighbor approach in which first each point $\cv_i^{\gG}$ in the grid is linked to the $k$ nearest points $\cv_j^\gP$ in the point-cloud. Subsequently, connections are established from each point $\cv_i^{\gP}$ in the point cloud to its nearest $k$ points $\cv_j^{\gG}$ in the grid (Fig.~\ref{fig:bilateral_conectivity}). By following this procedure, bilateral $k$-nearest neighbor connectivity creates a \textit{complete} connectivity scheme, i.e., with no disconnected points, from $\gP$ to $\gG$ with at least $k$ and at most $2k$ connections for each point.
% Figure environment removed
\vspace{-2mm}
\subsubsection{The positional embedding network $\phi_\mathrm{pos}$}\label{sec:pos_network}
\vspace{-1mm}
In literature, the positional embedding network $\phi_\mathrm{pos}$ is often parameterized as an $\mathrm{MLP}$ with piecewise nonlinearities, e.g., $\mathrm{ReLU}$, that receives relative positions $(\cv_i {-} \cv_j)$ as input and retrieves the value of an spatial function at that position $\phi_\mathrm{pos}(\cv_i {-} \cv_j)$ \cite{schutt2017schnet, qi2017pointnet++, wu2019pointconv}. However, previous studies have shown that $\mathrm{MLP}$s with piecewise nonlinearities suffer from an spectral bias towards low frequencies, which limits their ability to represent functions with high frequencies \cite{tancik2020fourier, sitzmann2020implicit}. In the context of modelling spatial neural operators such as $\phi_\mathrm{pos}$, this implies that using piecewise $\mathrm{MLP}$s to parameterize spatial neural operators leads to inherently smooth operators. Consequently, applying such an operator over an input function, e.g., via a convolution operation, would implicitly perform a low-pass filtering of the input, causing the output representations to lack information regarding fine-grained details of the input.

To overcome this issue, we rely on the insights from \textit{Continuous Kernel Convolutions} \cite{romero2021ckconv} and parameterize the positional embedding network as a \textit{Neural Field} \cite{sitzmann2020implicit, tancik2020fourier}. In contrast to piecewise $\mathrm{MLP}$s, neural fields easily model high frequencies, and thus allow for powerful parameterizations of spatial neural operators that do not perform smoothing. In the context of gridification, using neural fields to parameterize $\phi_\mathrm{pos}$ allows gridification to project fine-grained geometric information from the point cloud onto the grid.
\vspace{-3mm}
\subsection{Gridified networks for global and dense prediction}
\label{section:gridnetworks}
\vspace{-1mm}
Gridification and de-gridification allow for the construction of \textit{gridified networks} able to process point clouds both for global and dense prediction tasks (Fig.~\ref{fig:gridified_nets}). For global prediction tasks, e.g., classification, we construct a point cloud processing pipeline consisting of gridification, followed by a \textit{grid network}, i.e., a neural network that operates on grid data, designed for global prediction, e.g., a ResNet \cite{he2016deep} or a ViT \cite{dosovitskiy2020image}. For dense prediction tasks, e.g., segmentation, our proposed point cloud pipeline consists of gridification, followed by a grid network designed for dense predictions, e.g., a U-Net \cite{ronneberger2015u} or a CCNN \cite{knigge2023modelling}. After the processed grid representation is obtained, we utilize the de-gridification step to map back the grid representation to a point cloud with the output node predictions.

%The resulting practical architecture consists of the simple but effective recipe outlined in Fig \ref{fig:archrecipe}. 
% Figure environment removed
\vspace{-3mm}
\section{Related Work}
\vspace{-1mm}
Deep learning approaches for point cloud processing can be broadly classified in two main categories: (\textit{i}) native point cloud methods and (\textit{ii}) voxelization methods.

\textbf{Native point cloud methods.} Native point cloud methods operate directly on the raw, irregular point cloud data without any preprocessing steps such as voxelization. These methods leverage the inherent spatial distribution of the points to extract meaningful features. PointNet \cite{qi2017pointnet} introduced a pioneering framework for point cloud processing by employing shared multilayer perceptrons and symmetric functions to learn global and local features from unordered point sets. PointNet++ \cite{qi2017pointnet++} extended this work with hierarchical neural networks to capture hierarchical structures in point clouds. PointConv \cite{wu2019pointconv} introduced a convolution operation specifically designed for point clouds, incorporating local coordinate systems to capture local geometric structures. PointGNN \cite{shi2020point} utilized graph neural networks to model interactions between neighboring points in point clouds. 

Despite the flexibility in handling irregular data that native point cloud methods provide, they suffer from scalability issues due to the increased computational and memory complexity of processing unstructured point sets.

\textbf{Voxelization methods.} Voxelization methods aim to convert the irregular point cloud data into a regular grid structure, enabling the utilization of neural architectures designed for regular grid data. VoxNet \cite{maturana2015voxnet} introduced the concept of voxelization for point clouds and employed 3D convolutions on the resulting grid representations. Volumetric CNN \cite{qi2016volumetric} extended this approach with an occupancy grid representation and achieved impressive performance on 3D shape classification tasks. Other works, such as VoxSegNet \cite{wang2019voxsegnet} explore variations of voxelization techniques to improve performance on tasks like object detection and segmentation. 

While voxelization methods offer a well-founded solution to the computational and memory complexity of native point cloud methods, in practice, they suffer from high memory consumption and information loss due to the discretization process. This is due to the inherent trade-off between the need to capture fine geometric details --which requires high resolution grids--, and the need for efficiency --which favors low resolution grids--. As a result, conventional voxelization methods struggle to strike a balance between resolution and speed. In contrast, gridification is able to generate compact yet expressive grid representations able to preserve fine geometric details on a low resolution grid with roughly the same number of points as the source point cloud. 

\textbf{Hybrid methods.} Aside from pure  point cloud and voxelization methods, there exist works that attempt combine the advantages of both categories. Their main idea is to combine point-wise and grid-wise operations to perform effective feature extraction while maintaining scalability and efficiency.\break PointGrid \cite{le2018pointgrid} uses a hybrid representation by voxelizing the point cloud and employing a combination of point-wise and grid-wise operations at each layer. Point-Voxel CNN \citep{liu2019point} combines grid convolutions with point-wise feature extraction. It uses low-resolution voxelization to aggregate neighborhoods with regular 3D convolutions and $\mathrm{MLP}$s to generate point-wise features that preserve fine-grained structure. These features are  then fused through interpolation. Point-Voxel Transformer\break \cite{zhang2022pvt} follows a similar two-branch structure, but replaces convolutions with windowed self-attention.

Although hybrid methods reduce the computational and memory complexity of native point cloud methods, their explicit use of voxelization still leads to a trade-off between information loss and efficiency on that branch. To compensate for the information lost during voxelization, they require a parallel raw point cloud branch, which does not scale well. In contrast, gridification does not make use of raw point cloud branches but instead focuses on the creation of descriptive compact grid representations that preserve the geometric information of the source point cloud. Hence, gridification offers a solution with better scalability properties than existing hybrid methods.  

%Whereas voxel-based approaches allow for straightforward application of convolutional operations and architectures, in practice the voxelization step leads to loss of detail in fine geometry. Since the voxel resolution required to faithfully capture the sparse but detailed nature of pointcloud is high in most settings, additional engineering of the voxel grid is needed. For instance, \cite{le2018pointgrid} propose a voxelization method that is able to capture finer geometrical details by replacing the conventional geometry-invariant aggregation step in voxelization by a concatenation operation of the features that occupy a given voxel. However, as the number of points per voxel can vary, this requires a quantization or subsampling method which still results in information loss. Other methods such as \cite{klokov2017escape, riegler2017octnet, liu2022rocnet} construct hierarchical voxel representations that adapt resolution to pointcloud density and encode a pointcloud at different resolutions, but these methods are hard to implement for arbitrary pointclouds since they make use of complicated data structures.
%Other methods such as \cite{klokov2017escape, riegler2017octnet, liu2022rocnet} construct hierarchical voxel representations that adapt resolution to pointcloud density and encode a pointcloud at different resolutions, but these methods are hard to implement for arbitrary pointclouds since they make use of complicated data structures.
%There have been numerous works that exploit the interplay between the irregular structure of point clouds and efficiency of lattice structure. 
% or multi-view CNNs ... While multi-view CNNs are computationally efficient, they do not make full use of the 3D-structure of 3D objects.
%Other works are more in the hybrid domain and exploit both grid structure as well as the fine grained detail of continuous representation. Point Voxel CNN (PVCNN) \citep{pvcnn} combines lattice convolutions with point-wise feature extraction: low-resolution voxelization is used to aggregate neighborhoods using regular 3D convolutions, thereby exploiting paralellization, while extracting point-wise features with MLPs, modelling fine-grained structure, and fusing at both the final stage through interpolation. \cite{PVT} Leverage the same two-branch structure, but replace the convolution by a windowed self-attention operation to improve performance. In order to solve the problem of low occupancy in voxel grids, a rule book stores the entries of non-empty voxels. 

%Whereas methods like (PointNet, PointNet++, Point Transformer ) act directly on the irregular structure of point clouds. Most related to the proposed approach are methods like \cite{liu2019point, PVT}, falling somewhere in between the two aforementioned classifications. In \cite{liu2019point} authors propose a hybrid architecture that contains both a coarse voxel branch for capturing high-level structures with convolution operations and a pointwise multi-layer perceptron for modelling fine-grained structure. \cite{PVT} leverage the same two-branch structure, but replace the convolution by a windowed self-attention operation to improve performance. In order to solve the problem of low occupancy in voxel grids, a rule book stores the entries of non-empty voxels. Although these methods incorporate both local and global information, their explicit use of voxelization operations still leads to a trade-off in information loss and computational efficiency. Instead, we propose a single-branch architecture similar to classical CNNs, which replaces voxelization by a bipartite graph convolution to obtain a regular feature grid. Since this operation does not make any use of geometry-invariant aggregation or quantization, this should reduce fine-grained information loss.
\vspace{-3mm}
\section{Experiments}
\vspace{-1mm}
To evaluate our approach, we first analyze the expressive capacity of gridification and de-gridification on a toy point cloud reconstruction task. Next, we construct gridified networks and evaluate them on classification and segmentation tasks. In addition, we provide empirical analyses on the computational and memory complexity of gridified networks which we then corroborate with theoretical analyses. % We verify the performance of Gridifier on several point cloud processing tasks and compare the performance against both de-facto message passing baselines as well as (hybrid) voxelization methods. We show the method emits competitive performance while reducing computational overhead.
\begin{table*}[t]
\centering
    \begin{minipage}{\textwidth}
    \centering
    \caption{Classification performance on ModelNet40 benchmark.}
    \label{tab:modelnet_results}
    \vspace{-2.5mm}
    \begin{small}
    \scalebox{0.85}{
    \begin{tabular}{lllll}
    \toprule
    \sc{Model} & \sc{Input}  & \sc{Type} &  \sc{Accuracy} & \sc{Parameters} \\
    \midrule
    PointNet++ \cite{qi2017pointnet++} & $32 \times 1000$ & native & 89.64 &  1.5M \\
    VoxNet \cite{maturana2015voxnet} & $32\times 30^3$ & voxelization & 83.00 & 0.92M\\
    %Linear Trans. & 16.13 & 65.90 & 42.34 & 75.30 & 50.46  & $\times$&$\times$\\
    PointGrid \cite{le2018pointgrid} & $32\times16^3$ & voxelization & 92.00	 & - \\
    Point Voxel Transformer \cite{zhang2022pvt} & $32\times1024$ & hybrid & 94.00 & 2.76M\\
    Gridified Networks 3x3x3 (Ours) & $32\times1000 \rightarrow 32 \times 3^3$& voxelization & 90.86   & 0.28M  \\
    Gridified Networks 9x9x9 (Ours)  & $32\times1000 \rightarrow 32 \times 9^3 $ & voxelization & 92.28   & 0.47M  \\
    % \midrule
    % \midrule
    % CCNN$_{6, 380}$ (Global Kernels) & 59.60 & 88.10 & 90.59 & 86.70 & 95.24 \\
    % $\mathrm{DNArch}_\mathrm{K,R,W,D}$(CCNN$_{6, 380}$) & \textbf{61.05}$_{(1.00\times)}$ & \textbf{89.13}$_{(1.00\times)}$ & & \textbf{89.53}$_{(1.00\times)}$ \\
    \bottomrule
    \end{tabular}}
    \end{small}
    \end{minipage}%
    \vspace{-3mm}
\end{table*}


\textbf{Experimental setup.} For the position embedding function $\phi_\mathrm{pos}$ we use an Random Fourier Feature Network \cite{tancik2020fourier}, due to explicit control over the smoothness through the initial frequency parameter $\Omega$. The practical setup and instantiation of the convolution blocks can be found in Appendix \ref{app:networkarch}. We train our models without data augmentation using AdamW \cite{loshchilov2018decoupled} and a cosine scheduler \citep{loshchilov2017sgdr} with 10 epochs of linear warm-up. We follow the standard procedure and preprocess all objects in the datasets to be centered and normalized. For each dataset, we choose the grid resolution such that its number of points is roughly equal to the size of the original point cloud. For ModelNet40 we use surface normals in addition to positions as node features. Dataset specific hyperparameters can be found in Appendix \ref{app:hyperparams}. % Our code is publicly available at \href{https://github.com/computri/gridified-nets}{\texttt{github.com/computri/gridified-nets}}.

% \textbf{Experimental setup.} For the position embedding function $\phi_\mathrm{pos}$ we use an Random Fourier Feature Network \cite{tancik2020fourier}, due to explicit control over the smoothness through the initial frequency parameter $\Omega$, and set it to $\Omega=0.1$. The grid network consists of 3 convolution blocks with 128 channels. The practical setup and instantiation of the convolution blocks can be found in Appendix \ref{app:networkarch}. Through hyperparameter sweeps we set $k=9$ in the forward and backward knn-connectivity. We found that for most experiments using $L=4$ convolution blocks is sufficient. 

% Figure environment removed

% Each object is centered and normalized.  channels work best. For the position embedding function we set the smoothness parameter $\Omega=1.0$. 
\vspace{-3mm}
\subsection{Random point cloud reconstruction}
\vspace{-1mm}
First, we evaluate the expressivity of our proposed gridification and de-gridification procedure. To this end, we construct a dataset with 1000 synthetic random graphs --800 for training and 200 for validation-- consisting of a predefined number of nodes $\Nt^\gP{=}1000$ randomly sampled on the unit cube, i.e., $\cv_i^\gP \sim \gU( [-1, 1]^3)$, accompanied with a random scalar feature $f_i^\gP \sim \gU(-1, 1)$ at each position. 

\textbf{Experimental setup.} To evaluate the expressiveness of our method, we set up a network consisting only of a gridification and a de-gridification step, i.e., no intermediary layers, in a point cloud reconstruction pipeline. In other words, the task consists of propagating the point cloud into a grid representation, and mapping the grid representation back to the original point cloud (see Fig. \ref{fig:recon_pipeline}). %Specifically, from the grid representation the task is to predict the point cloud features given the node positions. 
Therefore, to successfully reconstruct the original point cloud from the grid representation, the grid representation must be able to retain sufficient information from the input point cloud. 

\textbf{Results.} Fig. \ref{fig:recon} shows reconstruction errors for different resolutions and different number of channels in the intermediary grid representation. We observe that it is possible to obtain good reconstructions by increasing the resolution of the grid or its number of channels. From an efficiency perspective, it is preferred to utilize low resolution representations with a larger number of channels due to the exponential growth in computational demands associated with higher grid resolutions, which instead scale linearly with the number of channels of the representation. Our experiments show that gridification is able to obtain compact grid representations that preserve the structure of the input point cloud. Furthermore, the quality of the grid representations can be efficiently improved by scaling the number of channels used. 
% Figure environment removed
%
\vspace{-7mm}
\subsection{ModelNet40 classification} \label{sec:modelnet40exp}
\vspace{-1mm}
Next, we evaluate gridification on point cloud classification. We deploy gridified networks on ModelNet40 \cite{wu20153d}: a synthetic dataset for 3D shape classification, consisting of 12,311 3$\Dt$ meshes of objects belonging to 40 classes. ModelNet40 is broadly used as a point cloud benchmark in which points are uniformly sampled from the faces of the meshes.

% \textbf{Experimental setup.} We use node positions and surface normals as node features. We follow the standard procedure and preprocess all objects in the dataset to be centered and normalized. We utilize mean aggregation in the gridification module and train our models without data augmentation for 60 epochs using AdamW \cite{loshchilov2018decoupled} with learning rate $0.005$ and a cosine scheduler \citep{loshchilov2017sgdr} with 10 epochs of linear warm-up. 

\textbf{Results.} Our results (Tab.~\ref{tab:modelnet_results}) show that gridified networks achieve competitive performance while being significantly more efficient in terms of parameters, compute and memory. Interestingly, and in contrast to voxelization methods, we observe that gridified networks operate well even on extremely low resolution grids. For instance, on a $3{\times}3{\times}3$ grid, gridified networks attain an accuracy of $90.86\%$.
\vspace{-3mm}
\subsection{ShapeNet part segmentation} 
\vspace{-1mm}
Next, we evaluate gridification on point cloud segmentation. To this end, we deploy gridified networks on ShapeNet \cite{shapenet2016}: a synthetic dataset with 16,000 point clouds of objects from 16 categories, each of which contains 2 to 6 parts. The objective of the task is to segment the point clouds into one of 50 possible part annotations.

% \footnote[1]{Results from reproduction in the same experimental setup as Gridifier.}
% \footnote[2]{Results taken from respective paper.}

% \textbf{Experimental setup.} We utilize gridified networks with 4 residual blocks and 512 channels. We train our models for 100 epochs using the same optimization settings as for point cloud classification (Sec.~\ref{sec:modelnet40exp}). Each object is centered and normalized as a preprocessing step. We sample 2048 nodes and gridify onto a $13{\times} 13 {\times} 13$ grid, which corresponds to the smallest grid resolution with more points than the original point cloud.

\textbf{Results.} Our results (Tab.~\ref{tab:shapenet_results}) demonstrate that gridified networks are also able to achieve competitive performance in segmentation tasks, while being significantly more efficient in terms of parameters, compute and memory. This result validates the ability of gridification to handle dense prediction tasks via gridification and de-gridification.% able to obtain results competitive to state of the art and outperform both native point cloud and segmentation methods, using only the simple pipeline outlined in Fig \ref{fig:gridified_nets}. \textcolor{magenta}{moeten we ook sota performance erin zetten als baseline?}
% \textbf{Results.} Our results are shown in Tab.~\ref{tab:modelnet_results}. We observe that the current results are not comparable with state of the art results. However, gridified networks are able to obtain competitive results to both native point cloud and segmentation methods based only on the simple pipeline outlined in Fig \ref{fig:gridified_nets}.  
\begin{table}
\centering
    \begin{minipage}{\textwidth}
    \centering
    \caption{Segmentation performance on ShapeNet-part benchmark.}
    \label{tab:shapenet_results}
    \vspace{-2.5mm}
    \begin{small}
    \scalebox{0.85}{
    \begin{tabular}{l|lll}
    \toprule
    \sc{Model} & Gridified Networks  & PointNet++  & PointGrid \\
    \midrule
   \sc{Type} & voxelization & native & hybrid
    \\
     \midrule
     instance average IoU & 87.07 & 85.1 & 86.4\\
     class average IoU & 81.68 & 81.9 & 82.2\\
     \midrule
airplane & 88.52  & 82.4 & 85.7\\
bag & 86.54  &  79.0 & 82.5 \\
cap & 74.09  & 87.7 & 81.8 \\
car & 80.46  & 77.3 & 77.9 \\
chair & 91.44  & 90.8 & 92.1 \\
earphone & 51.81 & 71.8 &  82.4 \\
guitar & 92.61 &  91.0 &  92.7 \\
knife & 89.44  & 85.9 & 85.8 \\
lamp & 82.07  &  83.7 & 84.2 \\
laptop & 96.07  &  95.3 & 95.3 \\
motor & 65.36  & 71.6 & 65.2 \\
mug & 92.99  &  94.1 & 93.4 \\
pistol & 86.72 & 81.3  & 81.7 \\
rocket & 58.57   & 58.7 & 56.9 \\
skateboard & 75.70   & 76.4 & 73.5 \\
table & 85.66 &  82.6 & 84.6 \\
    \bottomrule
    \end{tabular}}
    \end{small}
    \end{minipage}%
    \vspace{-4mm}
\end{table}
\vspace{-3mm}
\subsection{Efficiency analysis of gridification} 
\vspace{-1mm}
Finally, we investigate the scalability properties of gridification. Specifically, we analyze the time and memory consumption of gridified networks during inference on ModelNet40 for point clouds with increasing size, and compare the computation and memory complexity of convolutional operations on grid and point cloud data.

\textbf{Scaling gridified networks to large point clouds.} Fig~\ref{fig:efficiency_modelnet} shows the average time and memory consumption during inference on ModelNet40 for gridified networks and PointNet++. We observe that gridified networks exhibit a much more favorable scalability both in terms of inference time and GPU allocation --linear vs. quadratic-- as the input size and number of channels increase. This demonstrates that gridified networks scale much better than native point cloud methods both for larger point clouds and larger networks. 

\textbf{Scaling the receptive field of neural operations.} Furthermore, we analyze the scalability properties of gridified networks relative to the size of its receptive fields. As illustrated in Fig.~\ref{fig:convs}), for native point cloud methods the convolutional kernel must be recomputed for all query points in the point cloud. As a consequence, the construction of the convolutional kernels of size $\Kt$ for all query points in a point cloud with $\Nt$ points incurs in $\gO(\Kt \Dt)$ memory and time complexity. In contrast, on grid data, we can compute the kernel once and reuse it at all positions. As a result, on a grid, this operation incurs in $\gO(\Dt)$ time and memory complexity.

Fig. \ref{fig:efficiency_neighbors} show the methods' potential to scale up the receptive field of the gridification module without introducing significant computational overhead.

% Figure environment removed

% Figure environment removed

% \caption{Average inference time on ModelNet40 validation set per batch $B=32$ for various number of neighbors and number of channels $C$ on the grid representation. \vspace{-8mm}}
% \caption{Average GPU allocation on ModelNet40 validation set per batch $B=32$ for various number of neighbors and number of channels $C$ on the grid representation.\vspace{-8mm} }


% To verify this, we construct convolutional layers on point clouds and grids and record their inference time and average GPU allocation (Fig.~\ref{fig:efficiency_neighbors}). As shown here, on grids, the convolution operation scales linearly with respect to the kernel size $\Kt$ both in terms of time and memory. On point clouds, on the other hand, this follows a quadratic trend, which shows that gridified networks scale much better to larger receptive fields than native point cloud methods.
% % Figure environment removed
% % Figure environment removed
% % Figure environment removed
% % Figure environment removed
\vspace{-3mm}
\section{Limitations and future work}
\vspace{-1mm}
\textbf{The resolution of gridification depends on the size of the point cloud.} The main limitation of gridification is that the resolution on the grid is directly proportional to the size of point cloud in order to preserve information. This in turn means that the whole gridified architecture must be changed for point clouds of different sizes, even if they represent the same underlying signal. This is in contrast to native point cloud methods, which, due to their continuous nature, are, in principle, able to generalize to point clouds of different sizes as long as these exhibit the same structures.%    due to $k$-nearest neighbor connectivity scheme, the receptive field of each grid varies depending on the input point cloud density. Consequently, the number of input points in the point cloud implicitly defines the learned resolution of the network: the network has to be retuned for a different number of input points.
% \vspace{-3mm}
% \section{Future Work}
% \vspace{-1mm}
% We identify several future research directions for gridification in the context of point cloud processing.

\textbf{Towards no information loss.}  While gridification aims to produce compact grid representations with minimal information loss, our experiments reveal that some information still gets lost in the process. Loosely speaking, it should be possible to create grid representations that do not lose any information by ensuring that the grid representation has at least as many points and as many channels as the source point cloud representation. Gaining richer theoretical understanding of gridification, could therefore lead to grid representations with no information loss either by imposing other requirements on gridification, or by considering different functional families in the gridification process.

\textbf{Large scale point clouds and global context.} While we verify the scalability and efficiency of gridified networks for increasing point cloud sizes, we only carry on experiments on relatively small datasets. In future work, we aim to deploy gridification to large scale datasets. Furthermore, recent works have shown that using global receptive fields in convolutional operations consistently leads to better results across several tasks, even outperforming well-established Transformer architectures \cite{gu2021efficiently, knigge2023modelling, poli2023hyena}. Due to the computational complexity of native point cloud methods, networks with global context have not been explored for point cloud processing. With gridification this ability becomes computationally feasible. Exploring the effect of global context for point cloud processing is an exciting research direction.

\textbf{Generative tasks.} Gridification opens up the possibility of performing scalable generative tasks on large point clouds. Gridification can directly be extended to generative tasks if we assume that the point-cloud structure is preserved, i.e., if the coordinates of the output and input point clouds are equal. If this is not the case, e.g., for the generation of molecules \cite{xu2019deep, hoogeboom2022equivariant}, de-gridification module must be modified to predict both the features and positions of the new point cloud. We consider this a particularly promising direction for future research. 

\textbf{Equivariant gridification.} In its current form, gridified networks do not respect symmetries which might be important for some applications, e.g., equivariance to 3D rotations for the prediction and generation of molecules \cite{schutt2017schnet, hoogeboom2022equivariant}. In future work, we aim to extend gridification to respect these symmetries by taking inspiration from equivariant graph neural networks \cite{fuchs2020se, satorras2021n}. It is important to note that not only gridification and de-gridification must be equivariant, but also that the grid operations in between should respect these properties. This can be achieved in an efficient yet expressive manner through the use of continuous Monte-Carlo convolutions on the regular representations of the group \cite{finzi2020generalizing, romero2022learning}.
% \textbf{Compactness of grid representation.} Even though the bilateral K-nearest neighbor connectivity scheme ensures every grid point is non-empty, it does not prevent neighboring grid points from potentially having identical neighborhoods, and thus having similar aggregate representations. The grid may be more optimally populated with feature values if we could ensure each grid point covers a diverse neighborhood.

% \textbf{Compactness of connectivity}. For simplicity, in the current work the amount of neighbours in the forward and backward connectivity scheme is set equally. In practice, the number of forward and backward edges can be different and are hyperparameters. Reducing the amount of edges in the forward or backward case could cut down on the bottleneck induced by the connectivity, which scales poorly with the number of grid points.

\vspace{-3mm}
\section{Conclusion}
\vspace{-1mm}
This work presents gridification, a method that strongly reduces the computational requirements of point cloud processing pipelines by mapping input point clouds to a grid representation, and performing neural operations in there. We demonstrate that gridified networks are able to match the accuracy of native point cloud methods, while being much faster and memory efficient. Through empirical and theoretical analyses, we also show that gridified networks scale much more favorably than native point cloud methods to larger point clouds and larger neighborhoods.
\newpage

%\textbf{From architecture design to loss function design?} Deep Learning has shifted the paradigm of machine learning from feature design to architecture design. Could DNArch take this a step further and shift the focus from architecture design to the design of the optimization loss function? That is, could DNArch leads us to a landscape where researchers focus on the design of optimization loss terms like $\gL_\mathrm{comp}$ to generate neural architectures via DNArch that meet certain desired properties, e.g., memory and computational requirements, symmetry equivariance, robustness, etc.? This is perhaps the most promising outcome of our work and we look forward to exploring this research direction further.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

\bibliography{references}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section*{\Large Appendix}
\section{Network structure and convolution blocks}
\label{app:networkarch}
Fig. \ref{fig:networkarch} shows how we instantiated the grid network as described in section \ref{section:gridnetworks} in practice. The \texttt{Conv3D} blocks are CCNN blocks as in \citet{knigge2023modelling}.
% Figure environment removed

\section{Hyperparameters}
\label{app:hyperparams}
Table \ref{tab:hyperparams} contains the best hyperparameters for the specific datasets found through hyperparameter sweeps.

\begin{table}[H]
\centering
    \begin{minipage}{\textwidth}
    \centering
    \caption{Hyperparameter settings for the different datasets}
    \label{tab:hyperparams}
    \vspace{-2.5mm}
    \begin{small}
    \scalebox{0.85}{
    \begin{tabular}{llll}
    \toprule
     & ModelNet40 & ShapeNet\\
     \midrule
batch size & 32 & 16 \\
nr. conv blocks & 3  &  6  \\
hidden channels & 128  & 256  \\
nr. epochs & 60 &  50  \\
nr. input points & 1000 & 2047 \\ 
$\Omega$ position embedding & 0.1  & 1.0 \\
optimizer & AdamW &  AdamW   \\
learning rate & 0.005  & 0.001  \\
learning rate scheduler & Cosine Annealing &  Cosine Annealing\\
learning rate warmup & 10  &  10 \\
nr. neighbors & 9  & 9 \\
grid resolution & 9 & 13 \\
conv. kernel size &9  & 9\\
dropout & 0.1 & 0.3\\
weight decay & 0 & 0.001\\
aggregation & mean & max \\
    \bottomrule
    \end{tabular}}
    \end{small}
    \end{minipage}%
    \vspace{-4mm}
\end{table}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
