\section{Preliminary Lemmas}

We will present here a concentration inequality lemma that is used throughout the proofs. See \citep{foster2019orthogonal} for a proof of this theorem:
\begin{lemma}[Localized Concentration, \citep{foster2019orthogonal}]\label{lem:concentration}
For any $h\in \Hcal := \times_{i=1}^d \Hcal_i$ be a multi-valued outcome function, that is almost surely absolutely bounded by a constant. Let $\ell(Z; h(X))\in \R$ be a loss function that is $O(1)$-Lipschitz in $h(X)$, with respect to the $\ell_2$ norm. Let $\delta_n=\Omega\left(\sqrt{\frac{d\,\log\log(n) + \log(1/\zeta)}{n}}\right)$ be an upper bound on the critical radius of $\shull(\Hcal_i)$ for $i\in [d]$. Then for any fixed $h_*\in \Hcal$, w.p. $1-\zeta$: 
\begin{align}
    \forall h\in \Hcal: \left|(\E_n - \E)\left[\ell(Z; h(X)) - \ell(Z; h_0(X))\right]\right| = O\left(d\, \delta_n \sum_{i=1}^d \|h_i - h_{i,0}\|_{L_2} + d\,\delta_n^2\right)
\end{align}
%
If the loss is linear in $h(X)$, i.e. $\ell(Z; h(X) + h'(X)) = \ell(Z; h(X)) + \ell(Z; h'(X))$ and $\ell(Z;\alpha h(X)) = \alpha \ell(Z;h(X))$ for any scalar $\alpha$, then it suffices that we take $\delta_n=\Omega\left(\sqrt{\frac{\log(1/\zeta)}{n}}\right)$ that upper bounds the critical radius of $\shull(\Hcal_i)$ for $i\in [d]$.
\end{lemma}

\section{Proof of Lemma~\ref{lem:bias-tikhonov}}

\begin{proof}
The proof of this lemma has been discussed in \citep[Theorem 1.4.]{cavalier2011inverse}. 
For simplicity, we consider a compact linear operator $\Tcal$. 
The non-compact setting can be analyzed similarly, but with more complex singular value decomposition. 
Recall from \cref{sec: intro} that the compact linear operator $\Tcal$ admits  a countable singular value decomposition $\{\sigma_i, v_i, u_i\}_{i=1}^{\infty}$ with $\sigma_1 \geq \sigma_2 \geq \ldots$. 
Let $h_0$ and $h^*$ be the minimum norm solution $h_0 = \argmin_{h \in \Hcal: \Tcal h = r_0}\|h_0\|^2$ and the regularized target $h^* = \argmin_{h \in \Hcal}\|\Tcal\prns{h - h_0}\|+\lambda\|h\|^2$.  
Then we can write 
$h_0 = \sum_{i=1}^\infty a_{i,0} v_i$ and  $h_*=\sum_{i=1}^\infty a_{i,*} v_i$ for some square summable sequences $\braces{a_{i,0}: i = 1, \dots, }$ and $\braces{a_{i,*}: i = 1, \dots, }$.

First, we note that $h_0$ does not place any weight on eigenfunctions with zero singular value, namely, $a_{i, 0} = 0$ for any $i$ such that $\sigma_i = 0$. Otherwise, $h_0$ wouldn't have the minimum norm. Moreover, since it satisfies the source condition, we have $h_0 = (\Tcal^*\Tcal)^{\beta/2} w_0$ for some $w_0 \in L_2(X)$. It is easy to check that 
\begin{align}
    \|w_0\|^2 = \sum_{i=1}^\infty 1\braces{\sigma_i \ne 0} \frac{\langle h_0 ,v_i\rangle^2}{\sigma_i^{2\beta}} = \sum_{i=1}^\infty 1\braces{\sigma_i \ne 0} \frac{a_{i, 0}^2}{\sigma_i^{2\beta}}.
\end{align}

We now derive the form of the regularized target $h^*$. Note that the Tikhonov regularized objective can be written as follows:
    \begin{align}
        \|\Tcal(h_0 - h)\|^2 + \lambda \|h\|^2 = \sum_{i=1}^\infty \sigma_i^2 (a_{i,0} - a_i)^2 + \lambda a_i^2.
    \end{align}
%

%
%
%
%
    Thus the optimal solution can be derived by taking the first order condition for each $i$, as:
    \begin{align}
        -\sigma_i^2 (a_{i,0}-a_i) + \lambda a_i = 0 \implies a_{i,*} = \frac{\sigma_i^2}{\sigma_i^2 + \lambda} a_{i,0}.
    \end{align}
    It follows that 
        \begin{align}
        \|h_* - h_0\|^2 =~& \sum_{i=1}^\infty (a_{i,0} - a_{i,*})^2 = \sum_{i=1}^\infty a_{i,0}^2 \frac{\lambda^2}{(\sigma_i^2 +\lambda)^2}\\
        =~& \sum_{i=1}^\infty 1\{\sigma_i\neq 0\} a_{i,0}^2 \frac{\lambda^2}{(\sigma_i^2 +\lambda)^2}\\
        =~& \sum_{i=1}^\infty 1\{\sigma_i\neq 0\} \frac{a_{i,0}^2}{\sigma_i^{2\beta}} \frac{\sigma_i^{2\beta}\lambda^2}{(\sigma_i^2 +\lambda)^2} \leq \|w_0\|^2_2 \max_{i} \frac{\sigma_i^{2\beta}\lambda^2}{(\sigma_i^2 +\lambda)^2},
    \end{align}
    Now we show the desired conclusion by  showing   \begin{align}
        \frac{\sigma_i^{2\beta}\lambda^2}{(\sigma_i^2 +\lambda)^2} =  \lambda^{\min\{\beta, 2\}}. 
    \end{align} 
    When $\beta \ge 2$, we have 
    \begin{align}
        \frac{\sigma_i^{2\beta}\lambda^2}{(\sigma_i^2 +\lambda)^2} \le \lambda^2 \max_{i}\sigma_i^{2(\beta-2)} = \lambda^2.
    \end{align}
    The last inequality holds because by Jensen's inequality we can easily prove $\|\Tcal\|\leq 1$, so that  the maximum singular value of the operator $\Tcal$ is no larger than $1$. 

    When $\beta < 2$, we consider the function
    \begin{align}
        f(x) = \frac{x^\beta \lambda^2}{(x+\lambda)^2}.
    \end{align}
    By the first order optimality condition, this function  
    is maximized at $x = \lambda \beta (2-\beta)^{-1}$ and its maximum is 
    \begin{align}
        \frac{x^{\beta} \lambda^2}{(x+\lambda)^2} \leq  \lambda^{\beta} \frac{\beta^{\beta}(2-\beta)^{2-\beta}}{4} \leq \lambda^{\beta}. 
    \end{align}
    
    For the second part of the lemma, note that:
    \begin{align}
        \|\Tcal(h_*-h_0)\|^2 = \sum_{i=1}^{\infty} \sigma_i^2 (a_{i,0}-a_{i,*})^2 = \sum_{i=1}^\infty 1\braces{\sigma_i \ne 0} \frac{a_{i,0}^2}{\sigma_i^{2\beta}} \frac{\sigma_i^{2(\beta+1)}\lambda^2}{(\sigma_i^2 +\lambda)^2} \leq \|w_0\|^2 \max_{i} \frac{\sigma_i^{2(\beta+1)}\lambda^2}{(\sigma_i^2 +\lambda)^2}
    \end{align}
    Then we can prove the desired conclusion by following the steps above with $\beta$ replaced by $\beta + 1$. 
    
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
\qed\end{proof}



\section{Proof of Theorem~\ref{thm:adv-l2}}

\begin{proof}
    Anytime we denote a norm $\|\cdot\|$ in this proof, we mean $\|\cdot\|_{L_2}$. All constants $c_i$ in the proof, denote appropriately large universal constants. 
    
    \vspace{.5em}\noindent\textbf{Strong convexity with $L_2$ metric.} The first key realization is that the regularized criterion is strongly convex not just with respect to the weak metric, but with respect to the strong $L_2$ metric, due to the Tikhonov regularizer. More formally, consider the loss function:
    %
    \begin{align}
        L(\tau) := \|\Tcal(h_0 - h -\tau \nu)\|^2 + \lambda \|h + \tau\nu\|^2.
    \end{align}
    Note that this loss function is quadratic in $\tau$ and strongly convex with:
    \begin{align}
        \partial_{\tau}^2 L(\tau) = \|\Tcal\nu\|^2 + \lambda \|\nu\|^2.
    \end{align}
    %
    Since $h_*$ optimizes the regularized criterion, we have that if we take $h=h_*$ in the definition of $L(\tau)$ and if we take $\nu=\hat{h} - h_*$, then the function $L(\tau)$ is minimized at $\tau=0$. Moreover, since the space $\Hcal$ is convex, any function $h_* + \tau(\hat{h}-h_*)\in \Hcal$ for all $\tau\in [0,1]$ and therefore the derivative with respect to $\tau$ must be non-negative at $\tau=0$ (as otherwise an infinitesimal step in the direction $\nu$ would have decreased the regularized criterion; contradicting the optimality of $h_*$). Thus by an exact second order Taylor expansion, we have that:
    \begin{align}
        L(1) - L(0) = \partial_{\tau}L(0) + \partial_{\tau}^2 L(0) \geq \partial_{\tau}^2 L(0)
    \end{align}
    Instantiating the definition of $L$ and re-arranging, we then get:
    \begin{align}
        \lambda \|\hat{h}-h_*\|^2 +  \|\Tcal(\hat{h} - h_*)\|^2\leq \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2  + \lambda \left(\|\hat{h}\|^2 - \|h_*\|^2\right) 
    \end{align}
    
    \vspace{.5em}\noindent\textbf{Convergence of weak metric excess risk.} Now we need to argue that $\|\Tcal(h_0 - \hat{h})\|$ is small. 
    \begin{lemma}[Weak Metric with Regularizer]\label{lem:weak-conv-tikh}
    If $\hat{h}$ optimizes the regularized objective:
    \begin{align}
        \hat{h} = \argmin_{h\in \Hcal} \max_{f\in \Fcal} \E_n[2(m(W;f)-h(X)\,f(Z)) - f(Z)^2] + R_n(h)
    \end{align}
    and the assumptions of Theorem~\ref{thm:adv-l2} are satisfied, then for any $h_*\in \Hcal$:
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2 \leq \underbrace{2\|\Tcal(h_0 - h_*)\|^2  + R_n(h_*) - R_n(\hat{h})}_{\text{``Bias''}} + \underbrace{O\left(\delta_n \|\Tcal(\hat{h}-h_*)\| + \delta_n^2\right)}_{\text{``Variance''}}
    \end{align}
    \end{lemma}
    \begin{proof}
    %
    Letting $f_{h}=\Tcal (h_0 -h)$
    %
    and $\Delta(h)=\|T(h_0-h)\|^2$. Noting that $f_h\in \Fcal$ and invoking Lemma~\ref{lem:concentration}:
    \begin{align}
        \Delta(\hat{h}) :=~& \|\Tcal(h_0 - \hat{h})\|^2\\
        =~& \E[2\,(m(W; f_{\hat{h}}) - \hat{h}(X)\, f_{\hat{h}}(Z)) - f_{\hat{h}}(Z)^2] 
        %
\tag{Equation~\eqref{eqn:equivalence-adv-weak}}
        \\
        \leq~& \E_n[2\,(m(W; f_{\hat{h}}) - \hat{h}(X)\, f_{\hat{h}}(Z)) - f_{\hat{h}}(Z)^2] \\
        ~&~~ + O\left(\delta_n \left(\sqrt{\E[m(W;f_{\hat{h}})^2]} + \sqrt{\E[\hat{h}(X)^2 f_{\hat{h}}(Z)^2]}+ \|f_{\hat{h}}\|\right) + \delta_n^2\right)\tag{Lemma~\ref{lem:concentration}} \\
        \leq~& \E_n[2\,(m(W; f_{\hat{h}}) - \hat{h}(X)\, f_{\hat{h}}(Z)) - f_{\hat{h}}(Z)^2] + O\left(\delta_n \|f_{\hat{h}}\| + \delta_n^2\right)\tag{Equation~\eqref{eqn:msc-l2}}\\
        \leq~& \sup_{f\in \Fcal} \E_n[2\,(m(W; f) - \hat{h}(X)\, f(Z)) - f(Z)^2] + O\left(\delta_n \|f_{\hat{h}}\| + \delta_n^2\right) \tag{$f_{\hat{h}}\in \Fcal$}\\
        \leq~& \sup_{f\in \Fcal} \E_n[2\,(m(W; f) - h_*(X)\, f(Z)) - f(Z)^2] + O\left(\delta_n \|f_{\hat{h}}\| + \delta_n^2\right) + \underbrace{R_n(h_*)- R_n(\hat{h})}_{I_n}\\
        \leq~& \sup_{f\in \Fcal} \E[2\,(m(W; f) - h_*(X)\, f(Z)) - f(Z)^2] + O\left(\delta_n \|f\| + \delta_n \|f_{\hat{h}}\| + \delta_n^2\right) + I_n\\
        =~& \sup_{f\in \Fcal} \E[2\,(h_0(X) - h_*(X)) f(Z) - f(Z)^2] + O\left(\delta_n \|f\| + \delta_n \|f_{\hat{h}}\| + \delta_n^2\right) + I_n\\
        =~& \sup_{f\in \Fcal} \E[2\,(h_0(X) - h_*(X)) f(Z) - \frac{1}{2} f(Z)^2] + O\left(\delta_n \|f_{\hat{h}}\| + \delta_n^2\right) + I_n \tag{AM-GM}\\
        \leq~& 2 \|\Tcal(h_0 - h_*)\|^2 + O\left(\delta_n \|f_{\hat{h}}\| + \delta_n^2\right) + I_n 
        %
    \tag{Equation~\eqref{eqn:equivalence-adv-weak}}
    \end{align}
    Noting also that $\|f_{\hat{h}}\| = \|\Tcal(h_0 - \hat{h})\|$, we conclude that:
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2\leq \|\Tcal(h_0 - h_*)\|^2 + O\left(\delta_n \|\Tcal (h_0-\hat{h})\| + \delta_n^2\right) + R_n(h_*)- R_n(\hat{h})
    \end{align}
    Since $\|\Tcal(h_0-\hat{h})\|\leq \|\Tcal(h_0-h_*)\| + \|\Tcal(\hat{h}-h_*)\|$, we get:
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2\leq 2 \|\Tcal(h_0 - h_*)\|^2 + O\left(\delta_n \|\Tcal (\hat{h}-h_*)\| + \delta_n^2\right) + R_n(h_*)- R_n(\hat{h})
    \end{align}
    \qed\end{proof}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

    \noindent\textbf{Combining convexity and convergence.} Applying Lemma~\ref{lem:weak-conv-tikh}, for $R_n(h)=\lambda\|h\|_{2,n}^2:=\lambda \E_n[h(X)^2]$, with the strong convexity lower bound and letting $I_n = \lambda\left(\|h_*\|_{2,n}^2 - \|\hat{h}\|_{2,n}^2\right)$ and $I=\lambda \left(\|h_*\|^2 - \|\hat{h}\|^2\right)$ we get:
    \begin{align}
        \lambda \|\hat{h}-h_*\|^2 +  \|\Tcal(\hat{h} - h_*)\|^2\leq~& 2\|\Tcal(h_0 - h_*)\|^2  + O\left(\delta_n \|\Tcal(\hat{h}-h_*)\| + \delta_n^2\right) + I_n - I
    \end{align}
    Applying the AM-GM inequality to the term $\delta_n\|\Tcal(\hat{h}-h_*)\|$, and re-arranging we can derive:
    \begin{align}
        \lambda \|\hat{h}-h_*\|^2 + \frac{1}{2}\|\Tcal(\hat{h}-h_*)\|^2 \leq~& 2\|\Tcal(h_0 - h_*)\|^2   + O\left(\delta_n^2\right) + I_n - I
    \end{align}
    
    Consider the discrepancy between the empirical and the population regularizers:
    \begin{align}
        \|h_*\|_{2,n}^2 - \|\hat{h}\|_{2,n}^2 - (\|h_*\|^2 - \|\hat{h}\|^2) = (\E_n - \E)[h_*(X)^2 - \hat{h}(X)^2]
    \end{align}
    Since this is the difference of two centered empirical processes, we can also upper bound the latter by $O\left(\delta_n \|\hat{h}-h_*\| + \delta_n^2\right)$. Thus we get:
    \begin{align}
        \lambda \|\hat{h}-h_*\|^2 + \frac{1}{2} \|\Tcal(\hat{h} - h_*)\|^2
        \leq~& 2\|\Tcal(h_0 - h_*)\|^2 + O\left(\lambda \delta_n \|\hat{h}-h_*\| + \delta_n^2\right)
    \end{align}
    Applying the AM-GM inequality to the term $\delta_n \|\hat{h}-h_*\|$ and re-arranging:
    \begin{align}
        \frac{\lambda}{2} \|\hat{h}-h_*\|^2 + \frac{1}{2} \|\Tcal(\hat{h} - h_*)\|^2
        \leq~& 2\|\Tcal(h_0 - h_*)\|^2 + O\left(\lambda \delta_n^2 + \delta_n^2\right)\\
        =~& 2\|\Tcal(h_0 - h_*)\|^2 + O\left(\delta_n^2\right)\label{eqn:middle-step-v1}
    \end{align}
    %

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
    
%
%
%
%
%
    
%
%
%
%

    \noindent\textbf{Adding the bias part.} By the triangle inequality:
    \begin{align}
        \|\hat{h}-h_0\|^2 \leq~& 2 \|\hat{h}-h_*\|^2 + 2 \|h_* - h_0\|^2
        \leq~ \frac{4}{\lambda}\|\Tcal(h_0 - h_*)\|^2 + O\left(\frac{\delta_n^2}{\lambda}\right) + 2 \|h_* - h_0\|^2
    \end{align}
    By the Bias Lemma~\ref{lem:bias-tikhonov}, we have:
    \begin{align}
        \|h_* - h_0\|^2 \leq~& O\left(\|w_0\|\, \lambda^{\min\{\beta, 2\}}\right)
        &
        \|\Tcal(h_* - h_0)\|^2 \leq~& O\left(\|w_0\|\, \lambda^{\min\{\beta+1, 2\}}\right)
    \end{align}
    Thus overall we get the desired theorem:
    \begin{align}
        \|\hat{h}-h_0\|^2 = O\left(\frac{\delta_n^2}{\lambda} + \|w_0\|\, \lambda^{\min\{\beta, 1\}} + \|w_0\|\, \lambda^{\min\{\beta,2\}}\right) = O\left(\frac{\delta_n^2}{\lambda} + \|w_0\|\, \lambda^{\min\{\beta, 1\}}\right)
    \end{align}

    \noindent\textbf{Weak metric rate.} Starting from Equation~\eqref{eqn:middle-step-v1}, and adding the weak metric bias, we also get:
    \begin{align}
        \|\Tcal(\hat{h}-h_0)\|^2 \leq~& 2 \|\Tcal(\hat{h}-h_*)\|^2 + 2\|\Tcal(h_*-h_0)\|^2 \leq O\left(\|\Tcal(h_0 - h_*)\|^2 + \delta_n^2\right)\\
        =~& O\left(\delta_n^2 + \|w_0\|\, \lambda^{\min\{\beta+1, 2\}}\right)
    \end{align}
    
\qed\end{proof}


\section{Proof of Lemma~\ref{lem:bias-iter-tikhonov}}



\begin{proof}

The proof of this lemma has been discussed in \citep[Theorem 1.4.]{cavalier2011inverse}. For completeness, we show the case where the operator $\Tcal$ is compact. However, the extension to the non-compact operator is straightforward.
    Let $h_0 = \sum_{i=1}^\infty a_{i,0} v_i$ and $h=\sum_{i=1}^{\infty} a_i v_i$ and $h_*=\sum_{i=1}^\infty a_{i,*} v_i$. By some algebra, we can show that 
    %
    \begin{align}
     a_{i,*} = \frac{(\sigma_i^2+\lambda)^t - \lambda^t}{(\sigma^2_i +\lambda)^t} a_{i,0}. 
    \end{align}
    %
    Then note that since the minimum norm solution $h_0$ to the inverse problem, does not place any weight on eigenfunctions for which $\sigma_i=0$:
    \begin{align}
        \|h_* - h_0\|^2 =~& \sum_{i=1}^\infty (a_{i,0} - a_{i,*})^2 = \sum_{i=1}^\infty a_{i,0}^2 \frac{\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t} }\\
        =~& \sum_{i=1}^\infty 1\{\sigma_i\neq 0\} a_{i,0}^2 \frac{\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t} }\\
        =~& \sum_{i=1}^\infty 1\{\sigma_i\neq 0\} \frac{a_{i,0}^2}{\sigma_i^{2\beta}} \frac{\sigma_i^{2\beta}\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t} } \leq \|w_0\|^2_2 \max_{i} \frac{\sigma_i^{2\beta}\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t}}
    \end{align}
    where the third equality holds because $a_{i, 0} = 0$ whenever $\sigma_i = 0$ and the last inequality holds because $\sum_{i}1\{\sigma_i\neq 0\} \frac{a_{i,0}^2}{\sigma_i^{2\beta}}=\|w_0\|^2_2$ (see the proof for ).
    %
    Now by a case analysis, we can conclude the lemma by showing that:
    \begin{align}
        \frac{\sigma_i^{2\beta}\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t}} =  \lambda^{\min\{\beta, 2t\}} 
    \end{align}
    Take $\beta \geq 2t$. Then note that:
    \begin{align}
        \max_{i}  \frac{\sigma_i^{2\beta}\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t}}  \leq \lambda^{2t} \max_{i} \sigma_i^{2(\beta - 2t)} =  \lambda^{2t} 
    \end{align}
    since the maximum singular value of the operator is less than $1$ recalling $\|\Tcal\|\leq 1$. 
    Take $\beta < 2t$. Then note that the function:
    \begin{align}
        f(x) = \frac{x^\beta \lambda^{2t} }{(x+\lambda)^{2t} }
    \end{align}
    is maximized (by using the first order condition) at:
    \begin{align}
        x = \lambda \beta (2t-\beta)^{-1}
    \end{align}
    and takes value:
    \begin{align}
        %
        \frac{x^{\beta} \lambda^{2t}}{(x+\lambda)^{2t}} \leq = \lambda^{\beta} \frac{\beta^{\beta}(2-\beta)^{2t-\beta}}{2^{2t}} \leq \lambda^{\beta}. 
    \end{align}
    For the second part of the lemma, note that:
    \begin{align}
        \|\Tcal(h_*-h_0)\|^2 = \sum_{i=1}^{\infty} \sigma_i^2 (a_{i,0}-a_{i,*})^2 = \sum_{i=1}^\infty \frac{a_{i,0}^2}{\sigma_i^{2\beta}} \frac{\sigma_i^{2(\beta+1)}\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t}} \leq \|w_0\|^2_2 \max_{i} \frac{\sigma_i^{2(\beta+1)}\lambda^{2t}}{(\sigma_i^2 +\lambda)^{2t}}
    \end{align}
    Following identical steps but renaming $\beta\to \beta+1$ yields the second part.
\qed\end{proof}



\section{Proof of Theorem~\ref{thm:adv-l2-iter}}

\begin{proof}
    Anytime we denote a norm $\|\cdot\|$ in this proof, we mean $\|\cdot\|_{L_2}$. All constants $c_i$ in the proof, denote appropriately large universal constants. We fix an iteration $t$ and we will denote with $h_*\leftarrow h_{*,t}$, $\hat{h}\leftarrow \hat{h}_t$, $h_{*,-1}\leftarrow h_{*,t-1}$ and $\hat{h}_{-1} \leftarrow \hat{h}_{t-1}$. 
    
    \vspace{.5em}\noindent\textbf{Strong convexity with $L_2$ metric.} The first key realization is that the regularized criterion is strongly convex not just with respect to the weak metric, but with respect to the strong $L_2$ metric, due to the Tikhonov regularizer. More formally, consider the loss function:
    %
    \begin{align}
        L_t(\tau) :=~& \|\Tcal(h_0 - h -\tau \nu)\|^2 + \lambda \|h + \tau\nu-h_{*,-1}\|^2.
    \end{align}
    %
    Note that this loss function is quadratic in $\tau$ and strongly convex with:
    %
    \begin{align}
        \partial_{\tau}^2 L(\tau) = \|\Tcal \nu\|^2 + \lambda \|\nu\|^2.
    \end{align}
    %
    Since $h_{*}$ optimizes the regularized criterion, we have that if we take $h=h_{*}$ in the definition of $L(\tau)$ and if we take $\nu=\hat{h} - h_{*}$, then the function $L(\tau)$ is minimized at $\tau=0$. Moreover, since the space $\Hcal$ is convex, any function $h_{*} + \tau(\hat{h}-h_{*})\in \Hcal$ for all $\tau\in [0,1]$ and therefore the derivative with respect to $\tau$ must be non-negative at $\tau=0$ (as otherwise an infinitesimal step in the direction $\nu$ would have decreased the regularized criterion; contradicting the optimality of $h_{*}$). Thus by an exact second order Taylor expansion, we have that:
    \begin{align}
        L(1) - L(0) = \partial_{\tau}L(0) + \partial_{\tau}^2 L(0) \geq \partial_{\tau}^2 L(0).
    \end{align}
    Instantiating the definition of $L$ and re-arranging, we then get:
    \begin{align}
        \lambda \|\hat{h}-h_{*}\|^2 +  \|\Tcal(\hat{h} - h_{*})\|^2\leq \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_{*})\|^2  + \lambda \underbrace{\left(\|\hat{h}-h_{*,-1}\|^2 - \|h_{*}-h_{*,-1}\|^2\right)}_{=:-I}.
    \end{align}
    
    \vspace{.5em}\noindent\textbf{Convergence of weak metric excess risk.} Applying Lemma~\ref{lem:weak-conv-tikh} with $R_n(h)=\lambda\|h-\hat{h}_{-1}\|_{2,n}^2$ we get:
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2
        \leq O\left(\|\Tcal(h_0 - h_*)\|^2 + \delta_n \|\Tcal (\hat{h}-h_*)\|^2 + \delta_n^2\right)  + \lambda I_n
    \end{align}
    where $I_n:= \|h_*-\hat{h}_{-1}\|_{2,n}^2 - \|\hat{h}-\hat{h}_{-1}\|_{2,n}^2$. Though this bound suffices to get a good rate in one iteration, in multiple iterations the above bound depends on the bias $\|\Tcal(h_0-h_*)\|$. Thus if we inductively invoke such bounds, we will be getting cumulative bias terms appearing in the final bound for the $t$-th iterate. However, since these bias terms are large for small iterates, due to the poorly centered regularization bias, the above excess risk guarantee for $\hat{h}$ is not good enough. We will need to show an excess weak metric risk guarantee for $\hat{h}$ that does not depend on bias.
    \begin{lemma}[Bias-Less Excess Weak Metric with Regularizer]\label{lem:weak-conv-tikh-biasless}
    Suppose that $\hat{h}$ optimizes the regularized objective:
    \begin{align}
        \hat{h} = \argmin_{h\in \Hcal} \max_{f\in \Fcal} \E_n[2(m(W;f) -h(X)\,f(Z)) - f(Z)^2] + R_n(h)
    \end{align}
    Suppose that the conditions of Theorem~\ref{thm:adv-l2-iter} are satisfied {and let $f_{h}:=\Tcal (h_0 - h)$.} Then for any $h_*\in \Hcal$, w.p. $1-2\zeta/t$:
    %
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2 \leq \underbrace{R_n(h_*) - R_n(\hat{h})}_{\text{``Regularization Bias''}} + \underbrace{O\left(\delta_n \|\hat{h}-h_*\|\, \|f_{h_*}\|_{L_\infty} + \delta_n \|\Tcal (\hat{h}-h_*)\| + \delta_n^2\right)}_{\mcE_n:=\text{``Variance''}}.
    \end{align}
    %
    \end{lemma}
    %
    \begin{proof}
    %
    Let:
    \begin{align}\label{eqn:game-loss}
        \ell(h, f) = 2(m(W;f) - h(X)\, f(Z)) - f(Z)^2.
    \end{align}
    Note that by Equation~\eqref{eqn:equivalence-adv-weak}:
    %
    \begin{align}\label{eqn:game-loss-equivalence}
        \|\Tcal(h_0 - h)\|^2 =~& \E[2\,(h_0(X) - h(X)) f_{h}(Z) - f_{h}(Z)^2]\\
        =~& \E[2\,(m(W;f) - {h}(X)\, f_{h}(Z)) - f_{h}(Z)^2] = \E[\ell(h, f_h)].
    \end{align}
    Thus we can write:
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2=~&  \E[\ell(\hat{h}, f_{\hat{h}}) - \ell(h_*, f_{h_*})].
    \end{align}
    Since the loss function $\ell(h, f)$ is $O(1)$ lipschitz with respect to the functions $(h, f)$, we can apply the localized concentration inequality of Lemma~\ref{lem:concentration} and mean-squared-continuity, to get w.p. $1-\zeta/t$:
    %
    \begin{align}
        \left|(\E_n-\E)[\ell(\hat{h}, f_{\hat{h}}) - \ell(h_*, f_{h_*})]\right| \leq~& O\left(\delta_n \left(\sqrt{\E[m(W; f_{\hat{h}}-f_{h_*})^2] + \E[(\hat{h}(X)\, f_{\hat{h}}(Z)-h_*(X)\,f_{h_*}(Z))^2]}\right)\right)\\
        ~&~~~~ + O\left(\delta_n \|f_{\hat{h}} - f_{h_*}\| + \delta_n^2\right)\\
        \leq~& O\left(\delta_n \sqrt{\E[(\hat{h}(X) - h_*(X))^2 \, f_{h_*}(Z)^2]} + \delta_n \|f_{\hat{h}} - f_{h_*}\| + \delta_n^2\right)\\
        \leq~& O\left(\delta_n \|\hat{h}-h_*\|\, \|f_{h_*}\|_{L_{\infty}} + \delta_n \|f_{\hat{h}} - f_{h_*}\| + \delta_n^2\right) =: \mcE_n
    \end{align}
    %
    %
    Moreover, note that:
    \begin{align}
        \|f_{\hat{h}}-f_{h_*}\| = \|\Tcal(\hat{h}-h_*)\|
    \end{align}
    Thus we get:
    \begin{align}\label{eqn:middle-step}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2\leq~& \E_n[\ell(\hat{h}, f_{\hat{h}}) - \ell(h_*, f_{h_*})] + \mcE_n
    \end{align}
    If the empirical algorithm was minimizing $\E_n[\ell(h, f_h)]$, then the first term would have been negative and we can ignore it. However, our estimating is optimizing $\sup_{f} \E_n[2(m(W;f)-h(X)\,f(Z))-f(Z)^2]$. We now need to argue the discrepancy between $\E_n[\ell(h, f_h)]$ and $\sup_{f} \E_n[2(m(W;f)-h(X)\,f(Z))-f(Z)^2]$.

    Let $\hat{f}_{h}=\argmax_{f\in \Fcal}\E_n[2(m(W;f)-h(X)\,f(Z))-f(Z)^2]$, then our algorithm is optimizing $\E_n[\ell(h, \hat{f}_h)]$. We consider the discrepancy:
    \begin{align}
        \E_n[\ell(h, \hat{f}_h) - \ell(h, f_h)]
    \end{align}
    %
    Let $Q_{h}(\tau)=\E\left[\ell\left(h, f_h + \tau (\hat{f}_h - f_h)\right)\right]$. Since $f_h$ is an interior first order optimal point for the optimization problem $\sup_f \E[\ell(h,f)]$, it satisfies that $Q_{h}'(0)=0$ for any $h$. Moreover, since $\ell(h,f)$ is quadratic in $f$, we have that $Q_{h}(\tau)$ is quadratic in $\tau$, with $Q_h''(\tau) = -2\|\hat{f}_h-f_h\|^2$. By an exact second order functional Taylor expansion of $Q_h(\tau)$, we have that:
    \begin{align}
        \E[\ell(h,{f}_h) - \ell(h, \hat{f}_h)] = Q_{h}(0) - Q_h(1) = - Q_{h}'(0) - \frac{1}{2} \int_0^1 Q_{h}''(\tau) d\tau = \|\hat{f}_h - f_h\|^2  \label{eqn:quadratic}
    \end{align}
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    
    By the optimality of $\hat{f}_h$ we also have that:
    \begin{align}
        \E_n[\ell(h,f_h) - \ell(h, \hat{f}_h)]\leq 0
    \end{align}
    Thus we conclude that:
    \begin{align}\label{eqn: f-hat-bound}
        \|\hat{f}_h - f_h\|^2 \leq (\E-\E_n)[\ell(h,{f}_h) - \ell(h, \hat{f}_h)]
    \end{align}
    If $\delta_n$ upper bounds the critical radius of $\shull(\Hcal\cdot \Fcal)$, $\shull(m\circ \Fcal)$ and $\shull(\Fcal)$, then by the localized concentration inequality in Lemma~\ref{lem:concentration}, we have, w.p. $1-\zeta/t$, for all $h\in \Hcal$:
    \begin{multline}
        \left|(\E-\E_n)[\ell(h,{f}_h) - \ell(h, \hat{f}_h)]\right| \\
        \leq O\left(\delta_n \left(\sqrt{\E[m(W; f_h-\hat{f}_h)^2] + \E[h(X)^2 (f_h(Z)-\hat{f}_h(Z))^2]} + \|f_h - \hat{f}_h\|\right) + \delta_n^2\right)
    \end{multline}
    By mean-squared-continuity of $m$ and boundedness of the functions:
    \begin{align}\label{eq:strong-concentration}
        \left|(\E-\E_n)[\ell(h,{f}_h) - \ell(h, \hat{f}_h)]\right| \leq~& O\left(\delta_n \|f_h - \hat{f}_h\| + \delta_n^2\right) \leq \frac{1}{2} \|f_h - \hat{f}_h\|^2 + O\left(\delta_n^2\right)
    \end{align}
    %
    Combining Equation~\eqref{eq:strong-concentration} with Equation~\eqref{eqn: f-hat-bound} and re-arranging yields:
        \begin{align}\label{eq:strong}
        \E[\ell(h,{f}_h) - \ell(h, \hat{f}_h)] = \|\hat{f}_h - f_h\|^2 \leq O\left(\delta_n^2\right)
    \end{align}
    %
%
    Subsequently we also get for all $h\in \Hcal$:
    \begin{align}
        \E_n[\ell(h,{f}_h) - \ell(h, \hat{f}_h)] \leq \E[\ell(h,{f}_h) - \ell(h, \hat{f}_h)] + \frac{1}{2} \|f_h - \hat{f}_h\|^2 + O\left(\delta_n^2\right) = O\left(\delta_n^2\right)
    \end{align}

    Applying this fast concentration to each leading term in Equation~\eqref{eqn:middle-step}:
    %
    \begin{align}
        \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2\leq \E_n[\ell(\hat{h}, \hat{f}_{\hat{h}}) - \ell(h_*, \hat{f}_{h_*})] + \mcE_n + O(\delta_n^2)
    \end{align}
    %
    %
    Since by the definition of our estimation algorithm, $\hat{h}$ minimizes:
    \begin{align}
        \E_n[\ell({h}, \hat{f}_{{h}})] + R_n(h)
    \end{align}
    we get that the leading term is at most $R_n(h_*)-R_n(\hat{h})$. This concludes the proof.
    \qed\end{proof}
    
    Applying Lemma~\ref{lem:weak-conv-tikh} with $R_n(h)=\lambda\|h-\hat{h}_{-1}\|_{2,n}^2$ and combining with the strong convexity lower bound we get:
    %
    \begin{align}
        \lambda \|\hat{h}-h_*\|^2 +  \|\Tcal(\hat{h} - h_*)\|^2\leq~& \|\Tcal(h_0 - \hat{h})\|^2 - \|\Tcal(h_0 - h_*)\|^2  + \lambda \left(\|\hat{h}\|^2 - \|h_*\|^2\right) \\
        \leq~& O\left(\delta_n \|\hat{h}-h_*\|\, \|f_{h_*}\|_{L_\infty} + \delta_n \|\Tcal(\hat{h} - h_*)\| + \delta_n^2\right) + \lambda\,(I_n - I)
    \end{align}
    %
    %
    Applying the AM-GM inequality to the leading terms and re-arranging, yields:
    \begin{align}
        \frac{\lambda}{2} \|\hat{h}-h_*\|^2 = O\left(\frac{\delta_n^2 \|f_{h_*}\|^2_{L_\infty}}{\lambda} + \delta_n^2\right) + \lambda\, (I_n - I)
    \end{align}

    Consider the discrepancy between the empirical and the ideal regularizers, i.e. $I_n-I$. The two differ first in that they use different centering functions, i.e. $\hat{h}_{-1}$ vs. $h_{*,-1}$ and in using empirical vs. population $L_2$ norms. We separate the two errors by writing: 
    \begin{align}
        I_n - I = I_n - \hat{I} + \hat{I} - I
    \end{align}
    where $\hat{I} = \|h_*-\hat{h}_{-1}\|^2 - \|\hat{h}-\hat{h}_{-1}\|^2$.

    \noindent\textbf{Regularization error due to centering error} We first analyze $\hat{I}-I$.
    \begin{align}
    \hat{I} - I = \|h_*-\hat{h}_{-1}\|^2 - \|\hat{h}-\hat{h}_{-1}\|^2 - \left(\|h_*-h_{*,-1}\|^2 - \|\hat{h}-h_{*,-1}\|^2\right)
    \end{align}
    The function $\|h_*-h\|^2 - \|\hat{h}-h\|^2$, simplifies to:
    \begin{align}
        \|h_*\|^2 - \|\hat{h}\|^2 + 2\langle h, \hat{h} - h_*\rangle
    \end{align}
    Thus $\hat{I}-I$ simplifies to:
    \begin{align}
        |\hat{I}-I| = \left|2 \langle \hat{h}_{-1} - h_{*,-1}, \hat{h} - h_*\rangle\right| \leq 2 \|\hat{h}_{-1} - h_{*,-1}\|\, \|\hat{h}-h_*\|
    \end{align}
    
    \noindent\textbf{Empirical vs. population regularization} Next consider:
    \begin{align}
        I_n - \hat{I} :=~& \|h_*-\hat{h}_{-1}\|_{2,n}^2 - \|\hat{h}-\hat{h}_{-1}\|_{2,n}^2 - (\|h_*-\hat{h}_{-1}\|^2 - \|\hat{h}-\hat{h}_{-1}\|^2)
    \end{align}
    We further split this into two differences of centered empirical processes:
    \begin{align}
        I_1 :=~& \|h_*-\hat{h}_{-1}\|_{2,n}^2 - \|h_*-{h}_{*,-1}\|_{2,n}^2 - (\|h_*-\hat{h}_{-1}\|^2 - \|h_*-h_{*,-1}\|^2)\\
        I_2 :=~& \|h_*-h_{*,-1}\|_{2,n}^2 - \|\hat{h}-\hat{h}_{-1}\|_{2,n}^2 - (\|h_*-h_{*,-1}\|^2 - \|\hat{h}-\hat{h}_{-1}\|^2)
    \end{align}
    Since each of these is the difference of two centered empirical processes, that are also Lipschitz losses (since $h_*,\hat{h},h_{*,-1}, \hat{h}_{-1}$ are uniformly bounded) and since $h_*$ 
    %
    is a population quantity and not dependent on the empirical sample that is used for the $t$-th iterate,
    %
    we can also upper bound these, w.p. $1-2\zeta/t$ by \begin{align}
    I_1 =~& O\left(\delta_n \|\hat{h}_{-1}-h_{*,-1}\| + \delta_n^2\right),\\
    I_2 =~& O\left(\delta_n \|\hat{h}-h_* + h_{*,-1} - \hat{h}_{-1}\| + \delta_n^2\right) = O\left(\delta_n\left(\|\hat{h}-h_*\| + \|\hat{h}_{-1}-h_{*,-1}\|\right) + \delta_n^2\right)
    \end{align}
    where $\delta_n$ is an upper bound on the critical radius of $\shull(\Hcal-\Hcal)$.
    
    Thus we get:
    \begin{align}
        \frac{\lambda}{2}  \|\hat{h}-h_*\|^2 
        \leq~& O\left(\frac{\delta_n^2 \|f_{h_*}\|^2_{L_\infty}}{\lambda} + \delta_n^2 +  \lambda \delta_n \|\hat{h}-h_*\| + \lambda \delta_n\|\hat{h}_{-1} - h_{*,-1}\|\right) + 2\lambda \|\hat{h}_{-1} - h_{*,-1}\|\, \|\hat{h}-h_*\|
    \end{align}
    
    Applying the AM-GM inequality to the last three terms:
    \begin{align}\label{eqn:middle-step-iter}
    \frac{\lambda}{8} \|\hat{h}-h_*\|^2 \leq~& O\left(\frac{\delta_n^2\|f_{h_*}\|_{L_\infty}^2}{\lambda} + \delta_n^2\right) + 2\lambda \|\hat{h}_{-1} - h_{*,-1}\|^2
    \end{align}
    
    The latter also trivially implies that:
    \begin{align}
        \|\hat{h}-h_*\|^2 \leq O\left(\frac{\delta_n^2\|f_{h_*}\|_{L_\infty}^2}{\lambda^2} + \frac{\delta_n^2}{\lambda}\right) + 16\, \|\hat{h}_{-1} - h_{*,-1}\|^2
    \end{align}
    Let $M_t = \max\left\{\lambda, \|\Tcal(h_{*,t} - h_0)\|_{L_{\infty}}^2\right\}$.
    %
    Then the above shows that any $\tau\leq t$:
    \begin{align}
        \|\hat{h}_{\tau}-h_{*,\tau}\|^2 \leq O\left(\frac{\delta_n^2\, M_\tau}{\lambda^2}\right) + 16\, \|\hat{h}_{\tau-1} - h_{*,\tau-1}\|^2
    \end{align}
    Let $\gamma_t = \|\hat{h}_{t} - h_{*,t}\|^2$. Note that for $t=0$, since $\gamma_0=0$. Then, if we let $M_{\leq t} = \max_{\tau\leq t}M_\tau$, by our recursive bound, we have:
    \begin{align}
        \gamma_t \leq C\frac{\delta_n^2\, M_t}{\lambda^2} + 16 \gamma_{t-1} \leq C\frac{\delta_n^2\, M_{\leq t}}{\lambda^2} +  16\gamma_{t-1}
    \end{align}
    By a simple induction, this then yields the closed form bound:
    \begin{align}
        \gamma_t \leq 16^t C \frac{\delta_n^2\, M_{\leq t}}{\lambda^2}
    \end{align}
    
    By the Bias Lemma~\ref{lem:bias-tikhonov}, we have:
    \begin{align}
        \|h_{*,t} - h_0\|^2 \leq~& \|w_0\|\,  \lambda^{\min\{\beta, 2t\}} & 
        \|\Tcal(h_{*,t} - h_0)\|^2 \leq~& \|w_0\|\,  \lambda^{\min\{\beta+1, 2t\}}
    \end{align}
    Adding the bias term we get:
    \begin{align}
        \|\hat{h}_t-h_0\|^2 \leq O\left(\|\hat{h}_t - h_{*,t}\| + \|h_{*,t} -h_0\|\right)=  O\left(16^t\frac{\delta_n^2\, M_{\leq t}}{\lambda^2} + \|w_0\|\, \lambda^{\min\{\beta, 2t\}}\right)
    \end{align}

    \noindent\textbf{Weak metric rate.} 
    Applying Lemma~\ref{lem:weak-conv-tikh} with $R_n(h)=\lambda\|h-\hat{h}_{-1}\|_{2,n}^2$ we get:
    \begin{align}
        \lambda  \|\hat{h}-h_*\|^2 + \|\Tcal(\hat{h}_t - h_*)\|^2 \leq O\left(\|\Tcal(h_0 - h_*)\|^2 + \delta_n \|\Tcal (\hat{h}-h_*)\|^2 + \delta_n^2\right)  + I_n - I
    \end{align}
    Invoking the bound on $I_n-I$ and the AM-GM inequality, we get:
    \begin{align}
        \frac{\lambda}{2}  \|\hat{h}-h_*\|^2 + \frac{1}{2} \|\Tcal(\hat{h}_t - h_*)\|^2 \leq~& O\left(\|\Tcal(h_0 - h_*)\|^2 + \delta_n^2  + \lambda \|\hat{h}_{-1} - h_{*,-1}\|^2\right)\\
        \leq~& O\left(\|\Tcal(h_0 - h_*)\|^2 + \delta_n^2 + \min\left\{\lambda, \frac{16^t\delta_n^2 M_{\leq t-1}}{\lambda}\right\}\right)
    \end{align}
    Invoking the weak metric bias upper bound, we get the desired bound.
\qed\end{proof}




\section{Proof of Corollary~\ref{thm:adv-l3}}

%
Our goal is to show without any source condition, (1) $\|\Tcal(\tilde h-h_0)\|^2_2 = 2\mu_n + c \delta^2_n$ with probability at least $1-O(\zeta)$ for all $\tilde h\in \tilde{\Hcal}$
and (2) for any $h \in \Hcal$ such that  $\|\Tcal(h-h_0)\|^2_2 = \frac{2}{3}\max(\mu_n-c\delta^2_n,0)$, the function $h$ belongs to $\tilde \Hcal$ with probability at least $1-O(\zeta)$, for some universal constant $c$.
%

%
Then, as a corollary by setting $\mu_n \geq c'\delta^2_n$, we can ensure $\|\Tcal(\tilde h- h_0)\|^2_2 = O(\mu_n)$. Moreover, the guarantee for the strong metric under the $\beta$-source condition follows by simply invoking \cref{thm:adv-l2} and conditioning on the above events. Since all statements in \cref{thm:adv-l2} hold in high probability, the result can be concluded via a union bound. The only crucial condition we need to ensure from \cref{thm:adv-l2} is realizability, i.e. $h_* \in \tilde \Hcal$, since this is the only assumption which is affected by restricting to a sub-space of $\Hcal$. We will show that this holds by setting $\mu_n$ appropriately large.
%

%
First, we know that under the $\beta$-source condition:
%
\begin{align}
    \|\Tcal(h_*- h_0)\|^2_2  \leq \|w_0\|^2_2   \lambda^{\min(\beta+1, 2)} 
\end{align}
%
%
and the right hand is smaller $c''\,\lambda^{\min(\beta+1, 2)}$ for some universal constant $c''$. Thus by the second property of the first paragraph, we have that for $\mu_n\geq \frac{3}{2} \left(c''\lambda^{\min(\beta + 1, 2)} + c\delta_n^2\right)$, that $h_*$ satisfies the premise of the property and hence it also satisfies the conclusion that $h_*\in \tilde{\Hcal}$.
%


Thus it remains to show the two properties. Hereafter, we denote 
\begin{align}
   \hat h =  \argmin_{h\in \Hcal} \max_{f\in \Fcal} \E_n[2(Y-h(X))\,f(Z) - f(Z)^2]. 
\end{align}

%
\noindent\textbf{First statement: $\forall \tilde{h}\in \tilde{\Hcal}: \|\Tcal(\tilde h-h_0)\|^2_2 = \delta^2_n$ with probability $1-\zeta$.} We note that any functio $\tilde{h}\in \tilde{\Hcal}$ is an approximate optimizer of the empirical adversarial objective. We then simply note that the proof of Lemma~\ref{lem:weak-conv-tikh} can be easily modified to any $\mu_n$-approximate minimizer of the empirical criterion and not only for the exact empirical minimizer, at the expense of an extra additive $\mu_n$ in the upper bound (we omit the proof for conciseness, as it is a trivial extension). Applying then this slight extension of Lemma~\ref{lem:weak-conv-tikh} with $\hat{h}=\tilde{h}$ and $R_n(h)=0$ (and consequently with $h_*=h_0$), we get:
%
\begin{align}
    \|T(\tilde{h}-h_0)\|^2 \leq O\left(\delta_n \|T(\tilde{h}-h_0)\| + \delta_n^2\right) + \mu_n
\end{align}
%
%
Applying the AM-GM inequality to the first term on the right hand side and re-arranging we get:
\begin{align}
    \|T(\tilde{h}-h_0)\|^2 \leq O\left( \delta_n^2\right) + 2\mu_n
\end{align}
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
    
%
\noindent\textbf{Second statement. Any function $h \in \Hcal$ such that  $\|\Tcal(h-h_0)\|^2_2 \leq \frac{2}{3}\max(\mu_n - c\, \delta^2_n,0) $,  belongs to $\tilde \Hcal$ with probability at least $1-O(\zeta)$.}
Take any $h$ such that $h \in \Hcal$ such that  $\|\Tcal(h-h_0)\|^2_2 = O(\mu_n)$. Let $f_{h}=\E[h_0(X) - h(X)\mid Z=\cdot]$ and $\ell(h,f)$ as defined in Equation~\eqref{eqn:game-loss}. As noted in Equation~\eqref{eqn:game-loss-equivalence}, we can write for any $h\in \Hcal$ and assuming that $f_h\in \Fcal$, $\|\Tcal(h_0 - h)\|^2  =~ \E[\ell({h}, f_{{h}}) ] = \|f_{ h}\|^2$. Moreover, by Equation~\eqref{eq:strong-concentration} and Equation~\eqref{eq:strong} and Equation~\eqref{eqn:quadratic}, we have that for all $h\in \Hcal$:
\begin{align}
    \left|(\E-\E_n)[\ell(h,{f}_h) - \ell(h, \hat{f}_h)]\right| \leq~& O(\delta_n^2) &
    \E[\ell(h,{f}_h) - \ell(h, \hat{f}_h)] =~& \|f_h - \hat{f}_h\|^2 = O\left(\delta_n^2\right)
\end{align}
which together imply that:
\begin{align}
    \left|\E_n[\ell(h,{f}_h) - \ell(h, \hat{f}_h)]\right| = O\left(\delta_n^2\right) 
\end{align}
Applying the latter for any $h\in \Hcal$ with $\|f_h\|^2\leq \max(\mu_n - c_3 \delta_n^2, 0)$ and for $\hat{h}$, we get:
\begin{align}
     L_n(h) - L_n(\hat{h}) :=~& \E_n[\ell(h,\hat f_{h})] -  \E_n[\ell(\hat h,\hat f_{\hat h})]\\
     \leq~& \E_n[\ell(h, f_{h})] -  \E_n[\ell(\hat h, f_{\hat h})] + O\left(\delta^2_n\right)\\
     \leq~& \E[\ell(h, f_{h}) - \ell(\hat h, f_{\hat h})] + \left|(\E-\E_n)[\ell(h, f_{h})]\right| + \left|(\E-\E_n)[\ell(\hat h, f_{\hat h})]\right| + O\left(\delta^2_n\right)
\end{align}
By localized concentration, uniform boundedness and mean-squared-continuity of the moment, we also have that w.p. $1-\zeta$, for all $h\in \Hcal$:
\begin{align}
    \left|(\E-\E_n)[\ell(h, f_{h})]\right| =~& O\left(\delta_n \left(\sqrt{\E[m(W; f_{{h}} )^2]} + \sqrt{\E[(h(X)\, f_{{h}}(Z))^2]} + \|f_h\|\right) + \delta_n^2\right)\\
    =~& O\left(\delta_n\|f_h\| + \delta_n^2\right)
\end{align}
Thus we conclude that w.p. $1-O(\zeta)$:
%
\begin{align}
    L_n(h) - L_n(\hat{h})\leq~& \E[\ell(h, f_{h}) - \ell(\hat h, f_{\hat h})] + O\left(\delta_n \|f_h\| + \delta_n \|f_{\hat{h}}\| + \delta_n^2\right)\\
    =~& \|f_h\|^2 - \|f_{\hat{h}}\|^2 + O\left(\delta_n \|f_h\| + \delta_n \|f_{\hat{h}}\| + \delta_n^2\right)\\
    \leq~& \frac{3}{2}\|f_h\|^2 - \frac{1}{2} \|f_{\hat{h}}\|^2 +  O\left(\delta_n^2\right) \tag{AM-GM inequality}\\
    \leq~& \frac{3}{2}\|f_h\|^2 +  O\left(\delta_n^2\right)
    ~\leq~ \frac{3}{2}\|f_h\|^2 +  c\,\delta_n^2
\end{align}
%
%
%
%
%
%
%
%
for some universal constant $c$.
Moreover, by assumption we have that for any $\mu_n \geq c\delta_n^2$ that $\|f_h\|^2 \leq \frac{2}{3} \left(\mu_n - c\delta_n^2\right)$. Thus we conclude that w.p. $1-O(\zeta)$:
\begin{align}
    L_n(h) - L_n(\hat{h})\leq~&  \frac{3}{2} \frac{2}{3}\left( \mu_n - c\delta_n^2\right) + c \delta_n^2 \leq \mu_n
\end{align}
in which case $h\in \tilde{\Hcal}$. 
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Proof of Corollary~\ref{thm:adv-l3-iter}}

%
The proof of Corollary~\ref{thm:adv-l3-iter} follows along identical lines as that of Corollary~\ref{thm:adv-l3} applied to every iterate. With the same exact reasoning we can argue that for every iterate $t$: (1) $\|\Tcal(\tilde h-h_0)\|^2_2 = 2\mu_{n,t} + c \delta^2_n$ with probability at least $1-O(\zeta)$ for all $\tilde h\in \tilde{\Hcal}_t$.
%
and (2) for any $h \in \Hcal$ such that  $\|\Tcal(h-h_0)\|^2_2 = \frac{2}{3}\max(\mu_{n,t}-c\delta^2_n,0)$, the function $h$ belongs to $\tilde \Hcal_t$ with probability at least $1-O(\zeta)$, for some universal constant $c$.
%

%
The first property follows by noting that Lemma~\ref{lem:weak-conv-tikh-biasless} can be easily adapted to allow an additive approximation error to the empirical optimization, similar to Lemma~\ref{lem:weak-conv-tikh} that we used in the proof of Corollary~\ref{thm:adv-l3}. The second property follows with identical reasoning as the corresponding property in the proof of Corollary~\ref{thm:adv-l3}.
Then, as a corollary by setting $\mu_{n,t} \geq c'\max\{\delta^2_n, \lambda^{\min(\beta,2t)}\}$, we can ensure $\|\Tcal(\tilde h- h_0)\|^2_2 = O(\mu_{n,t}) $. Moreover, the guarantee for the strong metric under the $\beta$-source condition follows by simply invoking \cref{thm:adv-l2-iter}, using the smoothness assumption to upper bound $M_{\leq t}$ by $O(\lambda^{\gamma})$ (as in Remark~\ref{rem:smoothness}) and conditioning on the above events. Since all statements in \cref{thm:adv-l2-iter} hold in high probability, the result can be concluded via a union bound. The only crucial condition we need to ensure from \cref{thm:adv-l2-iter} is realizability, i.e. $h_{*,\tau} \in \tilde \Hcal_\tau$, for all $\tau\leq t$, since this is the only assumption which is affected by restricting to a sub-space of $\Hcal$.
%

%
We will show that this holds by setting $\mu_{t,n}$ appropriately large. First, we know that under the $\beta$-source condition for any $\tau\leq t$:
%
\begin{align}
    \|\Tcal(h_{*,\tau}- h_0)\|^2_2  \leq \|w_0\|^2_2 \lambda^{\min(\beta+1,2\tau)}.
\end{align}
%
and the right hand is smaller $c''\,\lambda^{\min(\beta+1,2\tau)}$ for some universal constant $c''$. Thus by the second property of the first paragraph, we have that for $\mu_{n,\tau}\geq \frac{3}{2} \left(c''\lambda^{\min(\beta+1,2\tau)} + c\delta_n^2\right)$, that $h_{*,\tau}$ satisfies the premise of the second property and hence it also satisfies the conclusion that $h_{*,\tau}\in \tilde{\Hcal}_\tau$.
%


\section{Proof of Lemma~\ref{lem:doubly_robust}}


This follows by the properties of the functions $h_0, q_0$:
\begin{align}
    \theta(h,q) - \theta_0 =~& \theta(h,q) - \theta(h_0,q_0)\\
    =~& \E[\tilde{m}(W;h) + m(W;q) - q(Z)\,h(X)] - \E[\tilde{m}(W;h_0) + m(W;q_0) - q_0(Z)\, h_0(X)]\\
    =~& \E[\tilde{m}(W;h-h_0) + m(W;q-q_0) - q(Z)\,h(X) +  q_0(Z)\, h_0(X)]\\
    =~& \E[q_0(Z)\,(h(X)-h_0(X)) + h_0(X)\,(q(Z)-q_0(Z)) - q(Z)\,h(X) +  q_0(Z)\, h_0(X)]\\
    =~& \E[q_0(Z)\,(h(X)-h_0(X)) + (h_0(X) - h(X))\,q(Z)]\\
    =~& \E[(q_0(Z) - q(Z))\,(h(X)-h_0(X))]
\end{align}

\section{Discussion on Source Condition for $q_0$}\label{app:riesz-smooth}

Note that the source condition for the moment problem that defines $q_0$, is essentially an assumption on the Riesz representer $a_0$. Since the non-parametric problem that defines $q_0$ is of the form $\Tcal^*(a_0 - q_0) = 0$, the source condition for the problem states that:
\begin{align}
    \sum_{i=1}^{\infty} \frac{\langle q_0, u_i\rangle^2}{\sigma_i^{2\beta}} <\infty
\end{align}
Moreover, note that since $\Tcal^* q_0 = a_0$, we have that:
\begin{align}
    a_0 = \sum_{i} \sigma_i \langle q_0, u_i\rangle v_i\implies
    \langle a_0, v_i\rangle = \sigma_i \langle q_0, u_i\rangle
\end{align}
Thus a $\beta$-source condition on $q_0$ can be equivalently expressed as a $(\beta+1)$-source condition on $a_0$:
\begin{align}
    \sum_{i=1}^{\infty} \frac{\langle a_0, v_i\rangle^2}{\sigma_i^{2(\beta+1)}} <\infty
\end{align}
For the existence of $q_0$, we already know that the above must hold for $\beta=0$. Hence, a $\beta$ source condition on $q_0$ is a stronger version of the source condition on $a_0$ that guarantees existence of a solution to the IV problem in Equation~\eqref{eqn:IV-q}.

This implicitly states that the Riesz representer $a_0$ should be primarily supported on the lower spectrum of the conditional expectation operator $\Tcal$. Another way of saying it, is that the functional that we care about is the inner product of the non-parametric regression function and a function that lies on the lower part of the spectrum of the operator, i.e. the functional projects the non-parametric function onto the lower part of the spectrum! 