%!TEX root = biometrika/biometrika_main.tex

\section{Problem Statement and Preliminaries}

We are given access to a set of independent and identically distributed observations $\{X_i,Z_i,W_i\}_{i=1}^n$ drawn from the distribution of the random variables $X, Z, W$. Our ultimate objective is to estimate the parameter $\theta_0 = \E[\tilde{m}(W; h_0)]$, as presented in \eqref{eqn:functional}, and
construct a valid confidence interval around it. 
% The function $h_0$ is defined as the minimum norm solution to a linear inverse problem given by \cref{eqn:primal,eq:Reisz}.

Throughout this work, we assume there exists a solution to the linear inverse problem given by \cref{eqn:primal,eq:Reisz}. 
% \xmcomment{eq. 3 is not about inverse problem. We may need to  put the dual inverse problem into a numbered equation.}\vscomment{I feel that the inverse problem is given by both of these two equations, otherwise $r_0$ is not been specified and eq. 3 specifies that. So the primal inverse problem is defined by both of these equations.}
(Numbered assumptions are assumed to hold throughout the paper.) 
% \vsedit{
\begin{assumption}[Primal solution exists]\label{assum:existence}
   We have  $r_0 \in \mathcal{R}(\Tcal)$, where $\mathcal{R}(\Tcal):= \{\Tcal h: h\in \bar{\Hcal}\}$. 
\end{assumption}
% }
Moreover, it will be convenient to express the constraints that identify $h_0$ in a combined manner, which can be derived by simple algebra from \cref{eqn:primal,eq:Reisz} 
% \vsedit{
and the properties of mean-squared projections onto closed linear spaces:\footnote{For any $q\in \bar{\Qcal}$, $r\in L_2(Z)$ it follows from properties of projections on closed linear spaces that $\langle r, q\rangle_{L_2(Z)} = \langle \Pi_{\bar{\Qcal}}r, q\rangle_{L_2(Z)}$. Hence: $\E[h_0(X) q(Z)] = \E[\E[h_0(X)\mid Z]\, q(Z)] = \E[(\Tcal h_0)(Z)\, q(Z)] = \E[r_0(Z) q(Z)] = \E[m(W;q)]$.}
\begin{align}\label{eqn:moment-restrictions}
    \forall q\in \bar{\Qcal}: \E[m(W; q) - h_0(X)\, q(Z)] = 0.
\end{align}
% }

In general, the solution mentioned above may not be unique. For this reason we aim to estimate the least $L_2$-norm solution, \ie, we define $h_0$ as 
\begin{align}
    h_0 = \argmin_{h: \Tcal h=r_0} \|h\|_{L_2}
\end{align}
This least norm solution always exists uniquely, as shown in Lemma 1 of \cite{bennett2023minimax}, and is frequently employed as a target in the literature when solutions are not unique \citep{santos2011instrumental,florens2011identification}. Notably, as we elaborate in the next section, for many linear functionals, the specific choice of the solution to the linear inverse problem is irrelevant and all solutions lead to the same value for the parameter $\theta_0$.

Our setting encompasses many well-studied problems in econometrics and statistics. We present here two illustrative examples. 
% \vsedit{
Other examples that fall in our framework include partially linear IV and proximal causal inference models,
% } 
missing-not-at-random data with shadow variables, and offline policy evaluation in partially observable MDPs.

\begin{example}[Proximal Causal Inference \citep{cui2020semiparametric}]
In proximal causal inference, we aim to estimate the average treatment effect $\E[Y(1)-Y(0)]$ in the presence of unmeasured confounders. Given proxy variables $Z,Q$ that satisfy certain conditions in \citep{cui2020semiparametric}, and observed treatment $D$, the target is expressed as 
\begin{align}
\theta_0 = \E[h_0(X,Q, 1) - h_0(X,Q,0)],\quad 
\E[Y - h_0(X,Q,D) \mid X, Z, D] = 0. 
\end{align}
A function $h_0$ is often referred to as the outcome bridge function.
In this case, $\tilde{m}(X,Q;h) = h(X,Q,1) - h(X,Q,0)$, $m(W;q)=Y\, q(X,Z,D)$ and the operator $\Tcal$ maps any function $h(X,Q,D) \in L_2(X,Q,D)$ to $\E[h(X,Q,D)|X,Z,D] \in L_2(X,Z,D)$.
Furthermore, the Riesz representer of $\tilde m(W;h)$ is 
\begin{align}
    \alpha_0(X,Q,D) = \frac{D}{P(D=1 | X,Q)} - \frac{1-D}{P(D=0 | X,Q)}
\end{align}
since $\theta_ 0 = \E[\alpha_0(X,Q,D) h_0(X,Q,D)]$. 
\end{example}

\begin{example}[Average Price Elasticity] In many demand estimation problems, we want to use cost-shifters $Z$ (variables that affect the cost of a product, and hence the price, but not the demand), so as to estimate the demand $Y$ of some product as a function of price $D$, conditional on market characteristics $X$. In this case the problem typically boils down to a non-parametric instrumental variable regression, where $(Z,X)$ is the instrument, $(D,X)$ is the treatment, $Y$ is the outcome, and $h_0$ is the demand curve. Since learning the whole demand curve can be statistically challenging, many times it might suffice for policy purposes to simply learn the average price elasticity of demand. One way to formalize the average price elasticity is through the average derivative of the demand curve:
\begin{align}
    \theta_0 :=~& \E[\partial_D h_0(X, D)] &
    \E[Y - h_0(X, D)\mid Z, D] =~& 0
\end{align}
In this case, $\tilde{m}(W;h) = \partial_D h_0(X, D)$ and $m(W;q)=Y\, q(Z)$. Furthermore the Riesz representer of $\tilde{m}(W;h)$ is $a_0(X, D) = \partial_{D} \log(p(D\mid X))$, where $p(D\mid X)$ is the conditional density of the price $D$ given $X$.
\end{example}

\vspace{.5em}\noindent\textbf{Notation and preliminary definitions.} Before delving into the main technical exposition we need to introduce some technical notation. \emph{Throughout the paper, whenever we use a generic norm of a function $\|h\|$, we will be referring to the $L_2$-norm with respect to the distribution of the input of the function, \ie,}
\begin{align}
\forall h\in L_2(X): \|h\| =~& \|h\|_{L_2} = \sqrt{\E[h(X)^2]} &
\forall f\in L_2(Z): \|f\| =~& \|f\|_{L_2} = \sqrt{\E[f(Z)^2]}
\end{align}
We will also be using the shorthand notation $\E_n[\cdot]$ for the empirical average, \eg, $\E_n[X] = \frac{1}{n}\sum_{i=1}^n X_i$. For any set $A$, we denote the closure of $A$ by $\clos(A)$.
For any function space $\Fcal$, containing functions that are uniformly and absolutely bounded by $1$, we will be using the critical radius as the measure of statistical complexity (c.f. \citep{wainwright2019high} for a more detailed exposition). To define the critical radius we first define the localized Rademacher complexity:
\begin{align}
    \Rcal(\Fcal, \delta) = \frac1{2^n}\sum_{\epsilon\in\{-1,1\}^n}\E\left[\sup_{f\in \Fcal: \|f\|\leq \delta} \frac{1}{n}\sum_{i=1}^n \epsilon_i f(X_i)\right].
\end{align}
% where $\epsilon_i$ are Rademacher random variables taking equiprobably in $\{-1, 1\}$. 
The star hull of a function space is defined as $\shull(\Fcal)=\{\gamma f: f\in \Fcal, \gamma\in [0,1]\}$. The critical radius $\delta_n$ of $\shull(\Fcal)$ 
% \xmcomment{$\shull(\Fcal)$?}\vscomment{I wanted to avoid mentioning the star hull eveywhere so thought of defining the critical radius with the star embedded in it. But maybe it will confuse readers familiar with it. If we remove it from here we need to make sure throughout that we always have $\shull$ around any class we mention, when we refer to critical radius.}\xmcomment{OK. I will double check to ensure the consistency.} 
is the smallest positive solution to the inequality:
\begin{align}
    \Rcal(\shull(\Fcal), \delta) \leq \delta^2
\end{align}
Throughout we will be assuming a uniform absolute bound of $1$ for all random variables and functions. This can be lifted to any finite bound $b$ by rescaling.
\begin{assumption}[Uniform absolute bound]\label{ass:ubound}
    All random variables and random functions are uniformly absolutely bounded by $1$.
\end{assumption}


\section{Debiased Machine Learning Inference for Functionals}

We begin by noting that any linear functional $\tilde{m}$ of $h_0$ admits a doubly robust representation:
\begin{align}\label{eqn:doubly_robust}
    \theta_0 = \theta(h_0,q_0),\quad \theta(h,q) := \E[\tilde{m}(W;h) + m(W;q) - q(Z)\, h(X)],
    % \E[\tilde{m}(W;h_0) + m(W;q_0) - q_0(Z)\, h_0(X)]
\end{align}
where $q_0$ solves a dual inverse problem of the same nature as $h_0$, but with respect to functional $\tilde{m}$ instead of $m$. More specifically:
\begin{align}\label{eqn:IV-q}
    \Tcal^* q_0 =~& a_0 &
    \forall h\in \Hcal: \E[\tilde{m}(W;h)] = \E[a_0(X)\, h(X)],
\end{align}
% \vsedit{
where $\Tcal^*:\bar{\Qcal}\mapsto \bar{\Hcal}$ is the adjoint operator to $\Tcal:\bar{\Hcal}\mapsto \bar{\Qcal}$. The adjoint operator is defined as:
\begin{align}
    \Tcal^* q = \Pi_{\bar{\Hcal}} \E[q(Z)\mid X=\cdot] = \argmin_{h\in \bar{\Hcal}} \E\left[\left(h(X) - \E[q(Z)\mid X]\right)^2\right].
\end{align}
If $\bar{\Hcal}= L_2(\mcX)$, then $\Tcal^*$ simplifies to a conditional expectation $T^*q = \E[q(Z)\mid X=\cdot]$. Note that the conditions that define $q_0$ can be expressed in a combined manner, similar to Equation~\eqref{eqn:moment-restrictions}:
\begin{align}\label{eqn:dual-combined}
    \forall h\in \bar{\Hcal}: \E[\tilde{m}(W;h) - q_0(Z)\, h(X)] = 0.
\end{align}
% }

% \vsedit{
We assume the existence of a solution $q_0\in \bar{\Qcal}$ to the inverse problem defined in Equation~\eqref{eqn:IV-q}.
% } 
If the inverse problem in Equation~\eqref{eqn:IV-q} has multiple solutions, then we will again denote by $q_0$ the minimum $L_2$-norm solution to the inverse problem.
% \vsedit{
\begin{assumption}[Dual solution exists]
    We have $a_0  \in \Rcal(\Tcal^*)$, with $\mathcal{R}(\Tcal^*):= \{\Tcal^* q: q\in \bar{\Qcal}\}$.  
\end{assumption}
% }

The reader might wonder why we need this assumption to estimate $\theta_0$ on top of the existence of solutions in the primal inverse problem. As was shown in \citep{severini2012efficiency}, when 
% \vsedit{
$\bar{\Hcal}=L_2(X)$ and $\bar{\Qcal}=L_2(Z)$,
% } 
the assumption
$a_0  \in \Rcal(\Tcal^*)$ is a necessary condition for the $\sqrt{n}$-estimability of the parameter $\theta_0$. In this sense the assumption is unavoidable for root-$n$ asymptotic normality. 

Now, we are ready to state the mixed bias property of \eqref{eqn:doubly_robust}. 
\begin{lemma}\label{lem:doubly_robust}
    % Denote with $\theta(h,q)$ as:
    % \begin{align}
        % \theta(h,q) = \E[\tilde{m}(W;h) + m(W;q) - q(Z)\, h(X)]
    % \end{align}
    % Then the debiased moment satisfies the mixed bias or double robustness property:
    $\theta(h,q)$ defined in \cref{eqn:doubly_robust} satisfies the mixed bias (or, double robustness) property 
    % \vsedit{
    that for all $h\in \bar{\Hcal}$ and $q\in \bar{\Qcal}$:
    % }
    \begin{align}
        \theta(h,q) - \theta_0 = \E[(q_0(Z) - q(Z))\, (h(X) - h_0(X))]
    \end{align}
\end{lemma}

Based on this crucial lemma, we can then apply the general machinery of Neyman orthogonality and debiased machine learning \citep{chernozhukov2017double} to arrive at a corollary that shows how and when one can deduce root-$n$ consistency and asymptotic normality of the estimate $\hat{\theta}$ of $\theta_0$ presented in Equation~\eqref{eqn:estimator} (see also \citep{chernozhukov2023simple} for a finite sample version in a slightly simpler setting):
\begin{corollary}[DML Inference for Functionals of Inverse Problems]\label{cor:functionals-endo}
    % Assume that all random variables and functions are absolutely bounded a.s.. 
    % \awbcomment{The $1/q$ power in this bound should be 2 instead, right? Or am I missing something?}\vscomment{For this theorem it suffices to satisfy the above weaker property for any $q$, not necessarily for $1/q=2$. This is much weaker. $1/q=2$ is needed to prove estimation rates for $\hat{h},\hat{q}$ but not for this part here.}
    Assume that estimates $\hat{h},\hat{q}$ of $h_0,q_0$ are estimated on a separate sample with
    \begin{align}
        \sqrt{n} \E\left[(\hat{q}(Z) - q_0(Z))\, (\hat{h}(X) - h_0(X))\right] = o_p(1),
    \end{align}
    $\|\hat{h}-h_0\|=o_p(1)$, and $\|\hat{q}-q_0\|=o_p(1)$.
Assume $\hat{\theta}$ is constructed as in \cref{eqn:estimator}
    and that the following mean-squared-continuity property is satisfied for $\tilde{m}$:
     % \awbcomment{You mean $\tilde m$, right? But, also, shouldn't we need mean-squared continuity for $m$ as well? (I realize this was assumed in the inverse problem estimation error theorems, but this corollary does not assume those conditions)}\vscomment{The proof of this corollary does not need it. These are needed to prove esitmation rates for $\hat{h}$ and $\hat{q}$ but not for this theorem.}:
    \begin{align}
        \E[(\tilde{m}(W;\hat{h})-\tilde{m}(W;h_0))^2] = O\left(\|\hat{h}-h_0\|^{\iota}\right)\quad\text{for some $\iota>0$}.
    \end{align}
     Then:
    \begin{align}
        \sqrt{n} (\hat{\theta} - \theta_0) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \rho_0(W_i) + o_p(1)
    \end{align}
    with $\rho_0(W) = \tilde{m}(W;h_0) + m(W;q_0) - q_0(Z)\,h_0(X) - \theta_0$. Thus, the random variable $\sqrt{n}\,(\hat{\theta}-\theta_0)$ converges in distribution to a normal $N\left(0, \E[\rho_0(W)^2]\right)$. Moreover, if we let $\hat{\sigma} = \E_n\left[\left(\tilde{m}(W;\hat{h}) + m(W;\hat{q}) - \hat{q}(Z)\, \hat{h}(X) - \hat{\theta}\right)^2\right]$,
     we can construct an asymptotically valid 95\% confidence interval using:
    \begin{align}
        \theta_0 \in [\hat{\theta} \pm 1.96\cdot \hat{\sigma}/\sqrt{n}]
    \end{align}
\end{corollary}

\begin{remark}[Uniqueness of Parameter under Non-Uniqueness of Functions]
    We note that when \cref{eqn:IV-q} admits a solution $q_0$, then the parameter $\theta_0$ is uniquely identified, even when the primary inverse problem in Equation~\eqref{eqn:moment-restrictions} admits many solutions. Let $h_1, h_2$ be any two solutions to \cref{eqn:moment-restrictions} and $q_0$ any solution to \cref{eqn:IV-q} and let $\theta_1$ and $\theta_2$ the parameters that correspond to these two solutions. Then note that:
    \begin{align}
        \theta_1 := \E[\tilde{m}(W; h_1)] =~& \E[q_0(Z)h_1(X)] \tag{by Equation~\eqref{eqn:dual-combined} for $h=h_1$}\\
        =~& \E[m(W;q_0)] \tag{$h_1$ satisfies Equation~\eqref{eqn:moment-restrictions} for $q=q_0$}\\
        =~& \E[q_0(Z) h_2(Z)] \tag{$h_2$ satisfies Equation~\eqref{eqn:moment-restrictions} for $q=q_0$}\\
        =~& \E[\tilde{m}(W; h_2)] =: \theta_2  \tag{by Equation~\eqref{eqn:dual-combined} for $h=h_2$}
    \end{align}
    Thus the linear functional parameter $\theta_0$ is the same, whether we calculate it using $h_1$ or $h_2$. Thus for functional estimation, it does not matter that our estimate is converging to the minimum norm solution $h_0$. Under the existence of a solution $q_0$ to \cref{eqn:IV-q}, any other solution to the inverse problem in \cref{eqn:moment-restrictions} would have identified the exact same parameter $\theta_0$.
\end{remark}

If we want to apply the latter corollary, it remains to show how we can estimate $\hat{h},\hat{q}$ in a manner such that:
\begin{align}\label{eqn:mixed-bias-req}
    \sqrt{n} \E\left[(\hat{q}(Z) - q_0(Z))\, (\hat{h}(X) - h_0(X))\right] = o_p(1)
\end{align}
and such that $\|\hat{h}-h_0\|=o_p(1)$ and $\|\hat{q}-h_0\|=o_p(1)$. 
% \vsedit{
By the definition of $\Tcal$, applying a tower law, a Cauchy-Schwarz inequality and orthogonality of projections on linear spaces, we have
\begin{align}
    \E\left[(\hat{q}(Z) - q_0(Z))\, (\hat{h}(X) - h_0(X)) \right] 
    &\leq \|\hat{q}-q_0\|\, \|\Tcal(\hat{h}-h_0)\|\,.
\end{align}
% }
Similarly, by the definition of $\Tcal^*$ and the orthogonality of projections, we also have that:
% \vsedit{
\begin{align}
\E\left[(\hat{q}(Z) - q_0(Z))\, (\hat{h}(X) - h_0(X)) \right] 
&\leq \|\Tcal^*(\hat{q}-q_0)\|\, \|\hat{h}-h_0\|\,,
\end{align}
% }
and therefore a sufficient condition for Equation~\eqref{eqn:mixed-bias-req} to hold is:
\begin{align}
    \sqrt{n} \min\left\{\|\hat{q}-q_0\|\, \|\Tcal(\hat{h}-h_0)\|, \|\Tcal^*(\hat{q}-q_0)\|\, \|\hat{h}-h_0\|\right\}  = o_p(1) \,.
\end{align}

% \vsedit{
Note that both $\hat{h}$ and $\hat{q}$ are instances of the same estimation problem. In particular, both statistical problems can be defined as finding a function $h\in \bar{\Hcal}$ that satisfies a linear inverse problem $\Tcal h=r_0$, where $r_0$ is the Riesz reprsenter of a linear functional and $\Tcal$ is a projected conditional expectation operator. Thus we will describe how one can solve the primal problem of estimating $\hat{h}$ and the exact same analysis can be applied to the dual problem where the operator $\Tcal$ is replaced by the dual $\Tcal^*$, the function space $\bar{\Hcal}$ is replaced by $\bar{\Qcal}$, and the linear functional $m(W;q)$ is replaced by the linear functional $\tilde{m}(W;h)$.
% }

% \vsedit{
We will provide estimation rates as a function of inductive biases further imposed on the solutions $h_0$ and $q_0$. In particular, we will consider the following realizability assumptions:
\begin{assumption}[Realizability and Closedness]\label{ass:realizable}
    For some known convex function spaces $\Hcal\subseteq \bar{\Hcal}$ and $\Qcal\subseteq \bar{\Qcal}$, we assume $h_0 \in \Hcal$ and $q_0\in \Qcal$. Moreover, for some known function spaces $\Fcal\subseteq \bar{\Qcal}$ and $\Gcal\subseteq \bar{\Hcal}$, we have $\Tcal \circ (h_0 - \Hcal) :=\{\Tcal (h_0 - h): h\in \Hcal\}\subseteq \Fcal$ and $\Tcal^* \circ (q_0 - \Qcal) := \{\Tcal^* (q_0 - q): q\in \Qcal\}\subseteq \Gcal$.
\end{assumption}
The function classes $\Hcal, \Qcal, \Fcal, \Gcal$, are meant to be classes of bounded statistical complexity, such as for instance norm-constrained high-dimensional linear models, Reproducing Kernel Hilbert Spaces, or neural network classes. This assumption can be relaxed to allow for approximation errors in all inclusion statements, at the expense of additive such error bounds in all our theorems. We omit such an extension for simplicity of exposition.
% }

\section{Linear Inverse Problems and the Source Condition}

Given access to $n$ samples $\{X_i, Z_i, W_i\}_{i=1}^n$, our objective is to find an estimator $\hat h$ that converges to the minimum norm solution $h_0$ to Equation~\eqref{eqn:primal}. Our goal is to provide estimation rates with respect to both the strong and weak metrics defined below:
\begin{align}
    \|\hat{h}-h_0\|^2 =~& \E[(\hat{h}(X) - h_0(X))^2] \tag{strong metric}\\
    \|\Tcal(\hat{h}-h_0)\|^2 =~& 
    % \vsedit{
    \E[\Pi_{\bar{\Qcal}}\E[\hat{h}(X) - h_0(X)\mid Z]^2]
    % } 
    \tag{weak metric}
\end{align}
The weak metric is ``weak'' in the sense that it is smaller than the strong metric and is a pseudo metric. Specifically, $\|\Tcal(\hat{h}-h_0)\|^2= \|\Tcal(\hat{h}-h'_0)\|^2$ whenever $Th_0'=r_0$ even if $h'_0\neq h_0$. 

% \vsedit{
Within the literature on nonparametric instrumental variable (IV) regression, a commonly employed approach focuses on optimizing empirical versions of the weak metric criterion, also known as the minimum distance criterion. This optimization process is carried out over simple hypothesis spaces denoted as $\Hcal_n$ of increasing complexity. These hypothesis spaces, often referred to as sieves, aim to approximate the function $h_0$ while enforcing some uniform control over the ratio between the strong and weak metrics across the entire class, also known as the measure of ill-posedness of the inverse problems:
% , is defined as follows:
\begin{align}
    \sup_{h \in \Hcal_n}\frac{\|(h_0-h)\|^2 }{\|\Tcal(h_0-h)\|^2}. 
\end{align}
This measure, as indicated in previous works \citep{dikkala2020minimax,chen2012estimation}, reflects the degree of ill-posedness in inverse problems. However, in cases where unique identification is absent this measure can easily diverge to infinity. If the sieves $\Hcal_n$ are taken so that eventually they uniformly approximate the function space $\Hcal$, then note that there will exist a different solution $h_0'\in \Hcal$ to the inverse problem, that is $\epsilon$ approximated by a function $h_{\epsilon}$ in $\Hcal_n$. Thus we will have that $\|h_0-h_0'\| \to \gamma > 0$ and $\|\Tcal(h_0-h_{\epsilon})\|\leq \epsilon + \|\Tcal(h_0-h_0')\| = \epsilon\to 0$.
% }

%%%%An alternative approach in the literature, which still works in the absence of the uniquness assumption, is to impose a restriction on the true function $h_0$, that implies that $h_0$ lies on a smooth part of the eigendecomposition of the operator $T$. This restriction is quantitative and is referred to as the $\beta$-source condition, where $\beta$ captures the degree of ill-posedness ($\beta=0$ means severely ill-posed and $\beta\to\infty$ means mildly ill-posed).  Under such a $\beta$-source condition, one can achieve rates directly on the strong metric by adding a $L_2$ regularization penalty to the minimum distance criterion, known as Tikhonov regularization \cite{Carrasco2007}. 

% \vsedit{
An alternative approach that remains effective even in the absence of the uniqueness assumption involves imposing the $\beta$-source condition on the minimum norm solution $h_0$. Under the $\beta$-source condition, it becomes possible to achieve desirable rates on the strong metric by introducing an additional $L_2$ regularization penalty to the minimum distance criterion. This regularization technique is also known as Tikhonov regularization \citep{Carrasco2007}. In this study, we adopt the latter approach.
% }

\vspace{.5em}\noindent\textbf{The Source Condition.} We begin by introducing the $\beta$-source condition, which is commonly used in the literature on inverse problems \citep{Carrasco2007} and captures mathematically how strongly the function $h_0$ is identified by the data that we observed.

\begin{definition}[$\beta$-source condition]\label{ass:source-primal}
There exists $w_0 \in \bar{\Hcal}$ such that $h_0$ satisfies $$h_0 = (\Tcal^* \Tcal)^{\beta/2} w_0.$$
\end{definition}

% \vsdelete{This assumption plays a significant role in inverse problems and has been extensively employed \citep{Carrasco2007}.}

Essentially, this assumption is claiming 
\begin{align}
    h_0 \in \Rcal( (\Tcal^* \Tcal)^{\beta/2}),\,\,\text{or equivalently,}\,\,  r_0 \in \Rcal( (\Tcal \Tcal^*)^{(\beta+1)/2}). 
\end{align}
Intuitively, when the parameter $\beta$ is large, the function $h_0$ exhibits greater smoothness, in the sense  that the $L_2$-inner product of $h_0$ with eigenfunctions that have smaller eigenvalues relative to the operator $\Tcal$ tend to decay faster. A more concrete interpretation of the assumption when
% For a comprehensive understanding, we provide further intuition in the case where 
the operator $\Tcal$ is compact and admits a singular value decomposition is given in \cref{sec: intro}.
 
\iffalse
\vspace{.5em}\noindent\textbf{Intuition for diagonalizable operators $\Tcal$.}
To facilitate understanding of the assumption, suppose operators $\Tcal$ is a linear operator that admits a countable singular value decomposition $\{\sigma_i, v_i, u_i\}_{i=1}^{\infty}$, with $\sigma_1 \geq \sigma_2 \geq \ldots$. The functions $u_i: Z\to \R$ are the left eigenfunctions of $\Tcal$ and form an orthonormal basis, \ie, $\E[u_i(Z) u_j(Z)] = 1\{i=j\}$. The functions $v_i: X\to \R$ are the right eigenfunctions of $T$ and also form an orthonormal basis, \ie, $\E[v_i(X) v_j(X)]=1\{i=j\}$. Moreover, the operator $T$ can be expressed as:
\begin{align}
    \Tcal h = \sum_{i=1}^\infty \sigma_i\, \langle h, v_i\rangle_{L_2}\, u_i 
\end{align}
Note that under such a singular value decomposition, any function $h$ can be written as $\sum_{i=1}^\infty a_i v_i$, where $a_i = \langle h, v_i\rangle$ and we can write:
\begin{align}
    \|\Tcal h\|^2 =~& \sum_{i=1}^{\infty} \sigma_i^2 \langle h, v_i\rangle_{L_2}^2 = \sum_{i=1}^\infty \sigma_i^2 a_i^2 &
    \|h\|^2 =~& \sum_{i=1}^\infty a_i^2
\end{align}
Thus the distortion that is introduced by the projection step (\ie, $\E[h(X)\mid Z=\cdot]$) boils down to simply taking the coefficients that $h_0$ puts on every eigenfunction $v_i$ and multiplying them by $\sigma_i$.
Thus we see that coefficients on eigenfunctions lower on the spectrum are distorted more as they are shrunk more towards zero by the operator. To ensure that the observed data have enough information to estimate well the function $h_0$, we need to impose a condition that $h_0$ does not place much weight on the lower part of the spectrum. 

Indeed, with the above notation, the $\beta$-source condition states that there exists $w_0 \in L_2(X)$, such that $h_0 = \{\Tcal^*\Tcal\}^{\beta/2}w_0$.
By noting
$\langle h_0,v_i \rangle = \langle \{\Tcal^*\Tcal\}^{\beta/2}w_0 , v_i \rangle = \sigma^{\beta}_i \langle w_0, v_i \rangle$, and since $\|w_0\| = \sum_{i=1}^\infty \langle w_0, v_i\rangle^2 < \infty$,
the source condition is satisfied when: 
\begin{align}
     \sum_{i=1}^\infty 1\{\sigma_i\neq 0\} \frac{\langle h_0, v_i\rangle_{L_2}^2 }{\sigma_i^{2\beta}} =\|w_0\| <\infty, \quad \sum_{i=0}^{\infty}\langle h_0, v_i\rangle_{L_2}^2  <\infty
\end{align}
where we also note that any minimum norm solution to the inverse problem is never supported on eigenfunctions for which $\sigma_i=0$, since subtracting any such component from any solution remains a valid solution to the inverse problem and strictly reduces the $L_2$ norm. Recall that $a_i$ are the coefficients associated with $h_0$ (\ie, $h_0 = \sum_{i=1}^{\infty} a_i v_i$), then the above implies that for any $m$:
\begin{align}
    \sum_{i\geq m} a_i^2 \leq \|w_0\|\, \sigma_m^{2\beta} = O\left(\sigma_m^{2\beta}\right)
\end{align}
As $\beta\to \infty$, the upper bound goes to zero (for large enough $m$, such $\sigma_m<1$). Thus larger $\beta$ implies that the function puts less an less support on the tail of the eigenfunctions and for $\beta =\infty$ this roughly implies that the true function is finite dimensional and supported only on a constant set of functions. Inverse problems where $\beta\to 0$ are referred to as severely ill-posed, while problems where $\beta \to \infty$ are mildly ill-posed or well-posed.
\fi


\section{Tikhonov Regularized Adversarial Estimator}

% A typical approach to handling the ill-posedness of inverse problems in the literature is to impose a restriction on the true function $h_0$ that simultaneously blends both the richness of $h_0$ and the degradation of the norm of $h_0$ caused by the application of the linear operator $T$. This assumption is referred to as the \emph{source condition}. 

% \vsedit{
Inspired by the combined moment constraints in Equation~\eqref{eqn:moment-restrictions} (and similarly Equation~\eqref{eqn:dual-combined} for $q_0$), we will consider an adversarial population criterion for the estimation of $h_0$:
\begin{align}
    L(h) := \max_{f\in \Fcal} \E[2\,(m(W;f) - h(X)\, f(Z)) - f(Z)^2]
\end{align}
where $\Fcal \subseteq \bar{\Qcal}$, as defined in Assumption~\ref{ass:realizable}. By the definition of the function $h_0$, we have $\E[m(W;f)]=\E[h_0(X)\, f(Z)]$. Hence, the above population criterion is equivalent to:
\begin{align}
    L(h) = \max_{f\in \Fcal} \E[2\,(h_0(X) - h(X))\, f(Z)) - f(Z)^2].
\end{align}
Moreover, by an application of the tower law of expectations and the properties of projections onto the closed linear space $\bar{\Qcal}$ and since $\Fcal\subseteq \bar{\Qcal}$, the criterion is also equivalent to:
\begin{align}
    L(h) =~& \max_{f\in \Fcal} 2\langle \E[h_0(X) - h(X)\mid Z=\cdot], f\rangle_{L_2(Z)} - \|f\|^2 \tag{tower law}\\
    =~& \max_{f\in \Fcal} 2\langle \Pi_{\bar{\Qcal}}\E[h_0(X) - h(X)\mid Z=\cdot], f\rangle_{L_2(Z)} - \|f\|^2 \tag{$f\in \bar{\Qcal}$ and $\bar{\Qcal}$ closed linear}\\
    =~& \max_{f\in \Fcal} 2\langle \Tcal (h_0 - h), f\rangle_{L_2(Z)} - \|f\|^2 \tag{definition of $\Tcal$}
\end{align}
Since $\Fcal$ contains functions of the form $\Tcal (h_0 - h)$ for any $h\in \Hcal$ (with $\Hcal$ as in Assumption~\ref{ass:realizable}):
\begin{align}
    f_h := \argmax_{f\in \Fcal} \E[2(m(W;f) - h(X)\, f(Z)) - f(Z)^2] = \Tcal (h_0 - h)
\end{align}
and the population criterion is equivalent to the weak metric, for any $h\in \Hcal$:
\begin{align}
    \begin{aligned}
        L(h) =~& 2 \langle \Tcal (h_0 -h), f_h\rangle_{L_2(Z)} - \|f_h\|^2
    ~=~ \|\Tcal(h_0-h)\|^2
    \end{aligned}\label{eqn:equivalence-adv-weak}
\end{align}
In other words, by minimizing the adversarial population criterion, we are essentially minimizing the weak metric distance to $h_0$.
% }




% \vsedit{
\vspace{.5em}\noindent\textbf{Tikhonov Regularized Adversarial Estimator.} 
As a first step, we consider a Tikhonov regularized adversarial estimator. The regularized population criterion and the corresponding regularized population solution is defined as:
\begin{align}\label{eqn:populatoin_objective}
    h_{*} := \argmin_{h\in \bar{\Hcal}} \|\Tcal(h_0 - h)\|^2 + \lambda \|h\|^2 
\end{align}
We define the Tikhonov regularized adversarial estimator, as the solution to an empirical analogue the adversarial formulation of the regularized population criterion, replacing also $\bar{\Hcal}$ with $\Hcal$ (as defined in Assumption~\ref{ass:realizable}):
\begin{align}\label{eqn:tikhonov}
    \hat{h} := \argmin_{h\in \Hcal} \max_{f\in \Fcal} \E_n \Big[ 2\,\Big(m(W;f) - h(X)\, f(Z) \Big) - f(Z)^2 + \lambda h(X)^2 \Big]
\end{align}
% \vsdelete{In this section, we conduct an analysis of the Tikhonov Regularized Adversarial Estimator. Using the notion of the $\beta$-source condition, we can now proceed to elucidate our convergence analysis. }
An essential implication of the source condition is the ability to control the regularization bias caused by the Tikhonov regularization term, denoted as $h_* - h_0$, as a function of both $\beta$ and the regularization strength $\lambda$ as follows.
% } 

\begin{lemma}[Tikhonov Regularization Bias]\label{lem:bias-tikhonov} 
%%%%If $h_0$ is the minimum $L_2$-norm solution to the linear inverse problem, and it satisfies the $\beta$-source condition, then:
Suppose that the minimum $L_2$-norm solution $h_0$ satisfies the $\beta$-source condition. Then, we have 
\begin{align}
    \|h_* - h_0\|^2 \leq~& \|w_0\|\, \lambda^{\min\{\beta, 2\}} & 
    \|\Tcal(h_* - h_0)\|^2 \leq~& \|w_0\|\, \lambda^{\min\{\beta + 1, 2\}} & 
\end{align}
\end{lemma}
%%%%%%\awbcomment{I'm pretty sure these bounds should be multiplied by $\|w_0\|$, right?}
The first inequality is widely recognized (c.f. \citep[Theorem 1.4.]{cavalier2011inverse}). In our analysis we also incorporate the second inequality, which bounds the bias in terms of the weak metric.

Having established control over the bias, we are now poised to formally demonstrate the previously mentioned estimation rates for the Tikhonov regularized estimator presented in \eqref{eqn:tikhonov}. In the following theorem, we make use of the concept of critical radius, a standard measure for quantifying convergence in nonparametric regression \citep{wainwright2019high}. In particular, when the function classes under consideration are VC classes, the critical radius is determined to be $\delta_n = O(n^{-1/2})$. It is worth noting that the critical radius can be readily computed for various function classes such as Sobolev balls, Holder balls, and sparse linear models (for detailed calculations see \citep{dikkala2020minimax,wainwright2019high,foster2019orthogonal}).

\begin{theorem}\label{thm:adv-l2}
    Consider the function spaces:
    \begin{align}
        \Hcal\cdot \Fcal :=~& \{(x, z) \to h(x)\, f(z): h\in \Hcal, f\in \Fcal\} &
        m \circ \Fcal :=~& \{w \to m(w; f): f\in \Fcal\}
    \end{align}
    Suppose that $m(W; f), h(X), f(X)$ are a.s. absolutely bounded, uniformly over $h\in \Hcal, f\in \Fcal$. Let $\delta_n=\Omega\left(\sqrt{({\log\log(n) + \log(1/\zeta)})/{n}}\right)$ be an upper bound on the critical radius of $\shull(\Hcal\cdot \Fcal)$, $\shull(m\circ \Fcal)$, $\shull(\Fcal)$ and $\shull(\Hcal)$.\footnote{The star hull of a function space is defined as $\shull(\Gcal)=\{\gamma g: g\in \Gcal, \gamma \in [0, 1]\}$. Note that by the linearity of the moment $\shull(m\circ \Fcal)=m\circ \shull(\Fcal)$.} 
Assume (a) the minimum $L_2$-norm solution $h_0$ satisfies the $\beta$-source condition,(b) realizability, $h_{*} \in \Hcal$, (c) closedness assumption, which says that the function space $\Fcal$ satisfies 
    \begin{align}
        \forall h\in \Hcal: \E[h_0(X) - h(X)\mid Z=\cdot] \in \Fcal
    \end{align}
    and (d) the moment $m$ satisfies mean-squared-continuity:
    \begin{align}\label{eqn:msc-l2}
        \forall f\in \Fcal: \E[m(W;f)^2] \leq O\left(\|f\|^2\right). 
    \end{align}
    Then the estimate $\hat{h}$ from Equation~\eqref{eqn:tikhonov}, for any $\lambda < 2$, satisfies that w.p. $1-\zeta$:
    \begin{align}
    \|\hat{h}-h_0\|^2 =~& O\left(\frac{\delta_n^2}{\lambda} + \|w_0\|\, \lambda^{\min\{\beta, 1\}} \right) &
    \|\Tcal(\hat{h}-h_0)\|^2 =~& O\left(\delta_n^2 + \|w_0\|\, \lambda^{\min\{\beta + 1, 2\}}\right)
    \end{align}
\end{theorem}
Hereafter, we present the implications of the aforementioned theorem. Firstly, assumptions (b) and (c) are standard, as utilized in \citep{dikkala2020minimax,LiaoLuofeng2020PENE}. In cases where these assumptions are violated, additional calculations allow us to obtain results with extra bias terms, which quantify the extent of the violation. Secondly, if we optimally choose $\lambda\sim \delta_n^{\frac{2}{1 + \min(\beta,1)}}$, so as to minimize the strong metric upper bound in Theorem~\ref{thm:adv-l2}, then we simultaneously get the rates:
\begin{align}
    \|\hat{h}-h_0\|^2 =~& O\left(\delta_n^{2\frac{\min(\beta,1)}{1+\min(\beta,1)}}\right) & 
    \|\Tcal(\hat{h}-h_0)\|^2 =~& O\left(\delta_n^2\right)
\end{align}
For special cases of the setting we cover here, these rates are equivalent to the current state-of-the-art rates, in terms of the weak metric as presented in \citep{dikkala2020minimax}, and the strong metric as shown in \citep{bennett2023minimax}. Moreover, unlike these prior works, our analysis offers simultaneously state-of-the-art guarantees for both metrics. Consequently, our analysis shows that for the one-step Tikhonov regularized adversarial estimator there is no trade-off in terms of which metric to prioritize (up to multiplicative constants). We now provide a more detailed comparison of our results with existing ones.

\vspace{.5em}\noindent\textbf{Comparison with \citep{LiaoLuofeng2020PENE}.} The authors proposed the same estimator and focus on $\Hcal$ and $\Fcal$ that are neural networks classes.
% conducted an analysis focusing on scenarios where the function classes $\Hcal$ and $\Fcal$ consist of neural networks.
By naively following their analysis and extending it to cases involving general function classes, we obtain the following result:
\begin{align}
    \|\hat h-h_0 \|^2 =  O(\delta_n/\lambda + \lambda^{\min(\beta,2))}).
\end{align}
Consequently, for $\beta\leq 2$, the resulting rate is of $\|\hat h-h_0 \|$ is $\delta_n^{\beta/2(1+\beta)}$, while for $\beta\geq 2$, the corresponding rate is $\delta_n^{1/3}$. 
% Consequently, for $\beta\leq 2$, the resulting rate is $\delta_n^{\beta/(2+\beta)}$ \xmcomment{the resulting rate of $\|\hat h-h_0 \|$ is $\delta_n^{\beta/2(1+\beta)}$}, while for $\beta\geq 2$, the resulting rate is $\delta_n^{1/3}$\vscomment{Most prob Xiaojie is correct, in which case the $1/3$ is also wrong}\xmcomment{$1/3$ actually seems right}. 
As depicted in Figure~\ref{fig:comparison}, this rate is slower compared to the one we obtained. The looser analysis in their work stems from their failure to fully exploit the strong convexity of the loss function \eqref{eqn:populatoin_objective} with respect to $h$ in terms of the strong metric. In our analysis, we refine the bound by employing the localization technique, which entails bounding the empirical process term using the errors $\|\hat h-h_0\|$ and $\|\Tcal(\hat h-h_0)\|$.


\vspace{.5em}\noindent\textbf{Comparison with \citep{bennett2023minimax}.} They propose an estimator that achieves $\| \hat h-h_0 \|^2=O(\delta_n)$ when $\beta\geq 1$.
Although their estimator can relax assumption (c) by replacing it with a realizability assumption of the form ``there exists $w_0 \in \Hcal$ such that $h_0 = \Tcal^* w_0$'', it remains unclear whether their estimator can achieve $\|\Tcal(\hat h-h_0)\|=O(\delta_n^2)$. Furthermore, their estimator does not provide any guarantee when $\beta < 1$.

\vspace{.5em}\noindent\textbf{Comparison with \citep{dikkala2020minimax}.}  They derive an estimator that achieves $\|\Tcal(\hat h-h_0)\|^2=O(\delta_n^2)$. While their result does not necessitate the source condition, their estimator lacks any guarantee regarding the convergence in terms of the strong metric $\|\hat h-h_0\|$, especially when the primal inverse problem does not have a unique solution.

\section{Iterated Tikhonov Regularized Estimator}

% \vsedit{
One limitation of the result in the previous section is its lack of adaptability to the degree of ill-posedness in the inverse problem, particularly for larger values of $\beta$ corresponding to milder problems. Ideally, as $\beta\to\infty$, it is expected that the strong metric rates would converge to $O(\delta_n^2)$. Indeed, when the operator $\Tcal$ coincides with the identity operator, leading to $\beta=\infty$, nonparametric IV regression reduces to standard nonparametric regression. In such cases, achieving an $O(\delta_n^2)$ rate is well-known \citep{wainwright2019high}. The observed convergence rate saturation after some small value of $\beta$ is a recognized drawback of Tikhonov regularization \citep{Carrasco2007}.
To address this limitation, we propose an iterated Tikhonov regularized adversarial estimator. At each iteration $t$ (commencing with $h_{*,0}=\hat{h}_0=0$) \footnote{Technically, we can set any function as $\hat{h}_0$.},
%%%\awbcomment{Couldn't we start with any arbitrary fixed estimate? I don't know if it's worth allowing this as it might make things unnecessarily more complicated, but it might be a good thing to do empirically to utilize prior information, in which case maybe it's worth being explicit that this doesn't hurt our theoretical results.}
we consider the population criterion: 
\begin{align}\label{eqn:iter-tikh-reg-solution}
    h_{*,t} := \argmin_{h\in \bar{\Hcal}} \|\Tcal(h_0 - h)\|^2 + \lambda \|h-h_{*,t-1}\|^2
\end{align}
and the corresponding empirical criterion:
\begin{align}\label{eqn:iter-tikhonov}
    \hat{h}_t = \argmin_{h\in \Hcal} \max_{f\in \Fcal} \E_n \Big[ 2\,\Big(m(W;f) - h(X)\, f(Z) \Big) - f(Z)^2 + \lambda \Big( h(X) - \hat{h}_{t-1}(X) \Big)^2 \Big]
\end{align}
We will show that by choosing $t$ and $\lambda$ appropriately then with high probability, as long as $n$ 
% \vsedit{
is larger than some constant then this iterated estimator achieves a strong metric rate of $\approx \delta_n^{2 \frac{\beta}{\beta+2}}$ that adapts to large values of $\beta$, \ie, becomes faster as $\beta\to \infty$ and converges to $\delta_n^2$ in the limit.
% }
% }

% \vsdelete{In this section, we perform an analysis of the iterated Tikhonov Regularized Adversarial Estimator. }
% \vsedit{
The limitation observed in the previous analysis stems from the inability of the bias in \pref{lem:bias-tikhonov} to adapt to higher values of $\beta$. However, the iterative nature of Tikhonov regularization possesses a crucial characteristic that mitigates this issue. It is demonstrated that the regularization bias of the $t$-th population iterate, denoted as $h_{*,t} - h_0$, is significantly smaller than that of the non-iterated one, particularly when $\beta$ is large.
% }

\begin{lemma}[Iterated Tikhonov Regularization Bias]\label{lem:bias-iter-tikhonov} If $h_0$ is the minimum $L_2$-norm solution to the linear inverse problem, and satisfies the $\beta$-source condition, then the solution to the 
% \vsedit{
$t$-th iterate of Tikhonov regularization $h_{*,t}$, defined in Equation~\eqref{eqn:iter-tikh-reg-solution}, 
% }
% \vsdelete{, \ie:
% \begin{align}
% h_{*,t} := \argmin_{h\in L_2(X)} \|\Tcal(h_0 - h)\|^2 + \lambda \|h-h_{*,t-1}\|^2
% \end{align}}
with $h_{*,0}=0$, satisfies that:
\begin{align}
    \|h_{*,t} - h_0\|^2 \leq~& \|w_0\|^2  \lambda^{\min\{\beta, 2t\}} & 
    \|\Tcal(h_* - h_0)\|^2 \leq~& \|w_0\|^2 \lambda^{\min\{\beta + 1, 2t\}} & 
\end{align}
\end{lemma}
%%%%%%\awbcomment{Why is there a universal constant $c$ to the power of $t$ here? This seems unnecessary. See \emph{e.g.} my proof in the previous overleaf, which shows the two bounds to be $\|w_0\|^2 \lambda^{\min(\beta,2t)}$, and $\|w_0\|^2 \lambda^{\min(\beta+1,2t)}$ respectively.}


With such an improved bias control at hand we can prove our claimed guarantee for the iterated tikhonov regularizedd estimator. The proof is based on starkly different approach to controlling the variance part, than what we used in Theorem~\ref{thm:adv-l2}. This alternative approach is required, so as to avoid compounding of bias terms over the $t$ iterates.

\begin{theorem}[Iterated Tikhonov]\label{thm:adv-l2-iter}
    % \vsedit{
    Under Assumption~\ref{ass:ubound} and further assuming that 
    $\delta_n=\Omega\left(\sqrt{\frac{\log\log(n) + \log(t/\zeta)}{n}}\right)$ 
    %$\delta_n=\Omega\left(\sqrt{({\log\log(n) + \log(t/\zeta)})/{n}}\right)$
    upper bounds the critical radii of the function classes define in Theorem~\ref{thm:adv-l2} and that $\delta_n$ also upper bounds the critical radius of $\shull(\Hcal-\Hcal):=\{\gamma\,(h-h'):h,h'\in \Hcal, \gamma\in [0,1]\}$.
    Assume the conditions (a), (c), (d) of Theorem~\ref{thm:adv-l2}, and (b') the realizability $h_{*,\tau} \in \Hcal$ for any $\tau\leq t$. Let $M_{\leq t} = \max\left\{\lambda, \max_{k\in [t]} \|\Tcal(h_{*,k}-h_0)\|_{L_{\infty}}^2\right\}$. Then the estimate $\hat{h}_t$ from Equation~\eqref{eqn:iter-tikhonov}, for and $t\geq 1$ and $\lambda\leq 1$ satisfies w.p. $1-4\zeta$:
    \begin{align}
    \|\hat{h}_t-h_0\|^2 =~& O\left(16^t \frac{\delta_n^2 M_{\leq t} }{\lambda^2} + \|w_0\|_2 \lambda^{\min\{\beta, 2t\}}\right),\\
        % \|\Tcal(\hat{h}_t - h_0)\|^2 =~& O\left(\delta_n^2 + \|w_0\| \lambda_t^{\min\{\beta+1,2t\}} + \lambda_{t} \|\hat{h}_{t-1}-h_0\|^2\right) 
    \|\Tcal(\hat{h}_t - h_0)\|^2 =~& O\left(\delta_n^2 + \min\left\{\lambda, \frac{16^t\delta_n^2M_{\leq t-1}}{\lambda}\right\} +  \|w_0\|\, \lambda^{\min\{\beta+1,2t\}}\right).
    \end{align}
    % }
\end{theorem}

% \vsdelete{We explain its implication. When we set $\lambda_k \sim \min(\delta_n^{\frac{2}{2+\beta_k}}6^{-k},1)$ for $k \in [t]$,  at our final step $t$, then as long as $t\leq c'\log\log(1/\delta_n)$ \vscomment{Shouldn't this be $\log(1/\delta_n)$ see old text below?}, for some universal constant $c'$, the sequence of regularization parameters is valid and we can achieve a rate: \xmcomment{I'm a bit confused here. How can we get rid of the $L_{\infty}$ norm term and directly get the rate here without the kind of analyses in Remark 1?}\vscomment{the $L_{\infty}$ is crudely upper bounded by a constant. Then this rate follows by optimal $\lambda$. A $\lambda$ dependent bound on the $L_{\infty}$ only yields an improvement over this rate}:
% \begin{align}
%     \|\hat{h}_t - h_0\|^2 =\tilde O\left( \delta_n^{2\frac{\beta_t}{2 + \beta_t}} \right). 
% \end{align} 
% Hence, for any $n$, when we choose $t_* = \min\{\lceil \beta/2\rceil, c'\log\log(1/\delta_n)\}$, we get a rate of:
% \begin{align}
%     \|\hat{h}_{t_*} - h_0\|^2 =O\left(\delta_n^{2\frac{\min\{\beta, c'\log(1/\delta_n)\}}{2+\min\{\beta, c'\log(1/\delta_n)\}}}\right).
% \end{align}}

If one uses the crude bound of $\max_{k\in [t-1]} \|\Tcal(h_{*,k}-h_0)\|_{L_{\infty}}^2=O(1)$, then this theorem yields strong and weak metric rates of:
\begin{align}
 \|\hat{h}_t-h_0\|^2 =~& O\left(16^t \frac{\delta_n^2}{\lambda^2} + \lambda^{\min\{\beta, 2t\}}\right) \\
  \|\Tcal(\hat{h}_t - h_0)\|^2 =~& O\left(\delta_n^2 + \min\left\{\lambda, \frac{16^t\delta_n^2}{\lambda}\right\} +  \lambda^{\min\{\beta+1,2t\}}\right)
\end{align}
If we let $\beta_t = \min\{\beta, 2t\}$ and choose a regularization strength of $\lambda\sim \delta_n^{\frac{1}{\beta_t+2}}$, then we get rates:
\begin{align}
    \|\hat{h}_t-h_0\|^2 =~& O\left(16^t \delta_n^{2\frac{\beta_t}{\beta_t+2}}\right)
\end{align}
If we choose $t=\lceil \min\{\beta/2, \log\log(1/\delta_n)\}\rceil$ then we get a rate of:
\begin{align}
    \|\hat{h}_t-h_0\|^2 =~& O\left(\min\{16^{\beta}, \log(1/\delta_n)\} \delta_n^{2\frac{\beta_t}{\beta_t+2}}\right) = \tilde{O}\left(\delta_n^{2\frac{\beta_t}{\beta_t+2}}\right)
\end{align}
Hence for any constant $\beta$, as $n$ grows, eventually $\log\log(1/\delta_n)\geq \beta$ and we get the rate of $O\left(\delta_n^{2 \frac{\beta}{\beta+2}}\right)$. This rate can also be achieved, even if $\beta$ is allowed to grow with $n$ in the asymptotics, as long as it grows slower than $\log\log(1/\delta_n)$. If $\delta_n \sim n^{-\alpha}$ for some $\alpha>0$, then we note that if we take $t= \lceil \min\{\beta/2, \sqrt{\log(1/\delta_n)}\}\rceil$, then $16^{\sqrt{\log(1/\delta_n)}}=o(n^{\epsilon})$ for any $\epsilon>0$, thus we again conclude the rate $\tilde{O}\left(\delta_n^{2 \frac{\beta_t}{\beta_t+2}}\right)$. This result allows us to claim a rate of $O\left(\delta_n^{2 \frac{\beta}{\beta+2}}\right)$ even if $\beta$ is growing with $n$ in the asymptotics as long as it grows slower than $\sqrt{\log(1/\delta_n)}$, which is considerably faster then $\log\log(1/\delta_n)$.




In summary, as depicted in Figure~\ref{fig:comparison}, when $\beta \geq 2$ and the sample size $n$ is sufficiently large, the iterated algorithm exhibits an improved rate in terms of the strong metric compared to the non-iterated version and the existing work \citep{LiaoLuofeng2020PENE}, where the rate saturates after $\beta=2$.
% \xmcomment{One thing I don't get is that somehow in Section 6, the rate is satuarated at $\beta = 1$ while here the rate is satuarated as $\beta = 2$ if we set $t = 1$. Shouldn't the estimator in Section 6 actually correspond to $t = 1$?}\vscomment{No, the rate produced by the analysis here is sub-optimal for the region $\beta\in [1,2]$. For this region one should use the analysis from the one step method. This is why we have this small plateau in our intro figures. This would go away only for smooth  functions with $\gamma=1$, in which case you can indeed think the analysis here as generalizing the $t=1$. The difference stems from the inability to produce a "bias-less" weak metric excess risk lemma, of the order $\delta_n^2/\lambda$ (which was the case for the "bias-dependent" excess risk lemma we used in the one-step case), but only of the order $\delta_n^2/\lambda^2$.}\xmcomment{OK. Thanks for the explanation.}
Notably, as $\beta$ tends to infinity, the rate of the iterated algorithm approaches the fast rate $O(\delta_n^2)$. However, such hyperparameter settings tuned for the strong metric do not yield a fast rate in terms of the weak metric, but rather a rate of $O\left(\delta_n^{2\frac{1+\beta}{2+\beta}}\right)$. 
Hence, there is a trade-off in the choice of estimation hyperparameters, dependent on which metric one is targeting.
% \vsdelete{Hence, in order to achieve a fast rate of $O(\delta_n^2)$ in terms of the weak metric, it is necessary to set $\lambda=O(\delta_n^2)$.}
% \awbcomment{What rate does this imply for the strong metric? If I'm reading the above bound correctly it seems like the strong metric bound should diverge for this value of $\lambda_t$. But, if that's the case, then this prescription for $\lambda_t$ seems silly, since we should be able to get the fast rate under weak norm using $\lambda_t=O(\delta_n)$, following almost identical analysis from previous section but uniformly over all possible fixed prior estimates (which I think should be an easy extension that follows given complexity assumption on $\shull(\Hcal-\Hcal)$).}\vscomment{Not sure how one would achieve a fast rate for weak metric if $\lambda=O(\delta_n)$??}\vscomment{Maybe we can just remove this last sentence; this is not an advisable strategy for the weak metric. For weak metric you shouldn't do an iterated tikhonov. I made such a change.}

\begin{remark}[Smooth Function Spaces]\label{rem:smoothness}
If we further assume that the function space $T\circ (\Hcal-h_0)$ satisfies a smoothness assumption that implies a relationship between the $L_{\infty}$ and $L_2$ norms, \ie, for some $\gamma\leq 1$:
\begin{align}\label{eqn:norm-bounds}
    \|\Tcal(h_{*,t} - h_0)\|_{L_{\infty}} \leq O\left(\|\Tcal(h_{*,t} - h_0)\|_{L_{2}}^{\gamma}\right)
\end{align}
then we immediately get by the bias lemma that $\|\Tcal(h_{*,t} - h_0)\|_{L_{\infty}}^2 = O\left(\lambda^{\gamma \min\{1+\beta, 2t\}}\right)=O\left(\lambda^{\gamma}\right)$ 
% \xmcomment{How come this is $O(\lambda^\gamma)$? Shouldn't it depend on the value of $\beta$? Since $\lambda$ is typically small, here the bound decreases with $t$ so when we bound $\max_{k \in [t]}\|\Tcal(h_{*,k} - h_0)\|_{L_{\infty}}$, we should take $k = 1$ and have $M_{\le t} = O(\max\{\lambda, O(\lambda^{\gamma\min\{1 + \beta, 2\})}\}$). Further simplifying this needs to discuss the value of $\gamma$ and $\beta$.}\vscomment{The exponent $\gamma\min(1+\beta,2t)$ is at least $\gamma$, due to the $1$, even for $t=1$. Since $\lambda<1$, we then get that $\lambda^{\gamma\min(1+\beta,2t)}\leq \lambda^\gamma$. Moreover, $\lambda^\gamma>\lambda$, so the max is $\lambda^\gamma$}. 
Thus the strong metric rate becomes:
\begin{align}
    \|\hat{h}_t-h_0\|^2 =~& O\left(16^t\frac{\delta_n^2}{\lambda^{2-\gamma}} + \lambda^{\min\{\beta, 2t\}}\right)
\end{align}
In this case, with the optimal choice of $\lambda$, for any constant $\beta$, with $t=\lceil \beta/2\rceil$ we get a rate:
\begin{align}
    \|\hat{h}_t - h_0\|^2 =O\left(\delta_n^{2\frac{\beta}{2 - \gamma +\beta}}\right).
\end{align}
As $\gamma \to 1$, this recovers the rate of $\delta_n^{2\frac{\beta}{1+\beta}}$ that we have established in the small $\beta\leq 1$ regime and extends it for larger values of $\beta$. Such a relationship is known for instance to hold for Sobolev spaces and more generally for reproducing kernel Hilbert spaces (RKHS) with a polynomial eigendecay. For instance, Lemma 5.1 of \citep{mendelson2010regularization} shows that if the eigenvalues of an RKHS decay at a rate of $1/j^{1/p}$ for some $p\in (0,1)$, and both $h_{*,t},h_0$ lie in the RKHS with a bounded RKHS norm, then we have that Equation~\eqref{eqn:norm-bounds} holds with $\gamma=1-p$. For such function spaces, we then achieve  rate of $O\left(\delta_n^{2\frac{\beta}{1+p+\beta}}\right)$. For the Gaussian kernel, which has an exponential eigendecay, we can take $p$ arbitrarily close to $0$.
\end{remark}




\section{Conditions for Asymptotically Normal Inference  }


As was already noted, the problem that defines $q_0$ is of exactly the same nature as the primary inverse problem that defines $h_0$. The only difference is that the moment is $\tilde{m}$ instead of the moment $m$ that defines $h_0$, the linear operator is the adjoint $\Tcal^*$
of the operator that defines $h_0$ and the roles of $X$ and $Z$ are reversed, \ie, $X$ becomes the ``exogenous'' or ``conditioning'' variable and $Z$ the ``endogenous''. Hence, the Tikhonov Regularized adversarial estimator for $q_0$ becomes:
\begin{align}\label{eqn:tikhonov_dual}
    \hat{q} := \argmin_{q \in \Qcal} \max_{g\in \Gcal} \E_n[2\,(\tilde m(W;g) - q(Z)\, g(X)) - g(X)^2 + \lambda q(Z)^2]
\end{align}
where $\Qcal$ and $\Gcal$ are defined in Assumption~\ref{ass:realizable}. We can similarly define the iterated version. 
% \vsedit{
Moreover, we can obtain convergence guarantees for $\hat q$ and its iterated version via \pref{thm:adv-l2} and \pref{thm:adv-l2-iter}.
% } 
% \vsedit{
We will assume that both the primal and dual inverse problems satisfy a source condition for some positive $\beta$ (a very benign assumption, \ie, arbitrary weak source condition).
\begin{assumption}
There exist $w_0\in L_2(X)$ and $\tilde w_0 \in L_2(Z)$ such that for $\beta_h,\beta_q>0$:
\begin{align}
h_0 =~& (\Tcal^* \Tcal)^{\beta_h/2} w_0 &
q_0 =~& (\Tcal \Tcal^*)^{\beta_q/2} \tilde w_0.
\end{align}
\end{assumption}
The condition for the dual problem states that $q_0$
% } 
should be primarily supported on the lower spectrum of the  operator $T^*$, equivalently, $a_0$ should be primarily supported on the lower spectrum of the operator $T$ (see Appendix~\ref{app:riesz-smooth} for a more detailed discussion).  






We present sufficient conditions for guaranteeing asymptotically normal inference on $\hat{\theta}$. We will denote with $\beta=\max\{\beta_h, \beta_q\}$, to be the largest of the two source condition parameters (\ie, the degree of the most well-posed inverse problem). First, we examine the case where we know which of the two inverse problems satisfies the $\beta$-source condition and without loss of generality we will assume that to be dual inverse problem that defines $q_0$, while the primal inverse problem that defines $h_0$ satisfies an arbitrary weak source condition. These roles can be interchanged without difficulty. Subsequently, we consider a more challenging scenario where we do not know which one satisfies the non-vacuous source condition.

\subsection{Guarantees under Knowledge of the Most Well-Posed Inverse Problem}

Suppose that $\beta=\beta_q \geq \beta_h > 0$. The complementary case follows identically, interchanging the roles of $h$ and $q$. Theorem~\ref{thm:adv-l2} implies that by choosing 
% \vsedit{
$\lambda \sim\delta_n^{\frac{2}{1+\min(\beta_h, 1)}}$, w.p. $1-O(\zeta)$:
\begin{align}
    \|\hat{h}-h_0\|^2 =~& O\left(\delta_n^{2\frac{\min(\beta_h,1)}{1 + \min(\beta_h,1)}}\right)= o_p(1), &
    \|\Tcal(\hat{h}-h_0)\|^2 =~& O\left(\delta_n^2\right).
\end{align}
Thus we guarantee fast rates of $O(\delta_n^2)$
% } 
for the weak metric and consistency with respect to the strong metric; as required by the asymptotic normality Corollary~\ref{cor:functionals-endo}. Here $\delta_n$ is an upper bound on the critical radius of the function classes $\shull(\Hcal\cdot \Fcal)$, $\shull(m \circ \Fcal)$, $\shull(\Fcal)$, $\shull(\Hcal - \Hcal)$. Moreover, since $q_0$ satisfies a $\beta_q$-source condition, we can apply the best of Theorem~\ref{thm:adv-l2} or Theorem~\ref{thm:adv-l2-iter} with an optimal choice of regularization hyperparameter and number of iterations, to get:
\begin{align}
   %%%%%% \|\hat{h} - h_0\|^2 =~& O\left(\min\left\{\delta_n^{2 \frac{\min\{\beta_h, 1\}}{1 + \min\{\beta_h, 1\}}}, \delta_n^{2 \frac{\beta_h}{2 + \beta_h}}\right\}\right) \\
    \|\hat{q} - q_0\|^2 =~& O\left(\min\left\{\delta_n^{2 \frac{\min\{\beta, 1\}}{1 + \min\{\beta, 1\}}}, \delta_n^{2 \frac{\beta}{2 + \beta}}\right\}\right) 
\end{align}
where $\delta_n$ is an upper bound on the critical radius of the function classes $\shull(\Qcal\cdot \Gcal)$, $\shull(\tilde{m} \circ \Gcal)$, $\shull(\Gcal)$, $\shull(\Qcal - \Qcal)$.

Combining the two aforementioned observations, we can construct estimators $\hat{h}, \hat{q}$ for both $h_0$ and $q_0$ to be used in the context of Corollary~\ref{cor:functionals-endo}, that ensure asymptotic normality if the following rate condition is satisfied:
\begin{align}\label{eq:final}
   {n}\, \|\hat{q}-q_0\|^2\, \|\Tcal (\hat{h}-h_0)\|^2 = n\, O\left(\min\left\{\delta_n^{2 \left(1 + \frac{\min\{\beta, 1\}}{1 + \min\{\beta, 1\}}\right)}, \delta_n^{2 \left(1 + \frac{\beta}{2 + \beta}\right)}\right\}\right) =~& o(1)
   %%%%%&\beta =~& \max\{\beta_h, \beta_q\}. 
\end{align}
We can conclude an analogous statement by flipping the role of $h_0$ and $q_0$. The size requirement of the function classes varies depending on the source condition. More specifically, we need 
% \vsedit{
\begin{align}
    \delta_n = o(n^{-\alpha}),\quad \alpha := \min\left( \frac{1 + \min(\beta,1) }{ 2 + 4 \min(\beta,1) }, \frac{2+\beta}{4 + 4 \beta} \right). 
\end{align}
% }
% \xmcomment{It should be $\delta_n = o(n^{-\alpha})$, right? Same for the rates in this paragraph.} 
This rate is illustrated in the blue line in \cref{fig:required_rate}. For $1\leq \beta\leq 2$, it is sufficient to have $\delta_n=n^{-1/3}$, which still permits non-parametric function classes. In the extreme case where $\beta \to 0$, the condition converges to $\delta_n=n^{-1/2}$,
%%%%%\awbcomment{the figure above seems to imply that it converges to $\delta_n = n^{-1}$, am I missing something?}
which corresponds to a  parametric rate (\ie, function classes are VC classes). Therefore, our theorem accommodates scenarios where both $h_0$ and $q_0$ exhibit severe ill-posedness. Conversely, in the limit as $\beta\to \infty$, the condition requires $\delta_n=n^{-1/4}$, which is the requirement typically encountered when dealing with functionals of functions defined through regression problems rather than inverse problems (c.f. \citep{chernozhukov2017double}).


\iffalse 

\vspace{.5em}\noindent\textbf{Source Condition Doubly Robustness.} The above conclusion does not tell us that our estimator is ``doubly robust'' regarding source conditions. What we claimed is there exists a choice of $\lambda_h$ ($\lambda$ for $\hat h$) and $\lambda_q$ ($\lambda$ for $\hat q$) that can allow for an arbitrarily small value of $\min(\beta_h,\beta_q)$ given the knowledge of which one takes the minimum value. Here, what we want to ensure is there exists a choice of $\lambda_h,\lambda_q$ under the knowledge of $\beta= \min(\beta_h,\beta_q)$, but without knowing which one takes the minimum value. In this case, under the optimal choice of $\lambda$, the asymptotically normal inference is feasible as long as 
\begin{align}
     nO\left(\min\left \{ \delta_n^{\frac{4\min(\beta,1)+2}{\min(\beta,1)+1}} ,\delta_n^\frac{4\beta-2}{\beta+1}\right\}  \right) = o(1). 
\end{align}
\awbcomment{This needs more explanation... do you mean if we set both $\lambda_h$ and $\lambda_q$ optimally targeting the strong metric when $\beta=\min(\beta_h,\beta_q)$? Also, what does this imply about the requrirement on $\delta_n$?}

\awbcomment{Should we discuss robustness to misspecification of $\beta_q$ and $\beta_h$? E.g. how much lee-way do we have for the $\beta_q$ and $\beta_h$ values that we target to be too optimistic?}
\fi 

\vspace{.5em}\noindent\textbf{Comparison with \citep{bennett2022inference}.} 
To provide a comparison, we examine our condition alongside recent related work by \citep{bennett2022inference}. In \citep{bennett2022inference}, in their study, under the assumption of $\beta=1$, they demonstrate that asymptotically normal inference is feasible as long as
\begin{align}
  {n}O(\delta^3_n) = o(1). 
\end{align}
The corresponding required rate for $\delta_n$ is illustrated in the blue line in \cref{fig:required_rate}. This aligns with our conclusion when instantiating $\beta=1$ in \eqref{eq:final}. However, our condition is significantly more general. Firstly, when $\beta<1$, our condition in \eqref{eq:final} still allows for the possibility of asymptotically normal inference. Specifically, as $\beta \to 0$,
% our requirement becomes $\delta_n = o(n^{-1/2})$.
our requirement converges to $\delta_n = o(n^{-1/2})$.
In contrast, \citep{bennett2022inference} do not provide any guarantees when $\beta<1$.
Secondly, when $\beta>1$, we can leverage the potentially larger size of the function classes as $\beta$ increases. Specifically, as $\beta \to \infty$, our requirement becomes $\delta_n^4 = o(n^{-1/4})$. This adaptability to $\beta$ is not obtained in \citep{bennett2022inference}.








\subsection{Source Condition Double Robustness}

\iffalse 
\begin{table}[]
    \centering
    \begin{tabular}{c|c}
         &      \\
     Scenario (1)   & Arbitrary weak source for $h_0$, $\beta$-source for $q_0$   \\ 
     Scenario (2)   & Arbitrary weak source for $q_0$, $\beta$-source for $h_0$    \\ 
    Scenario for source doubly robustness & Either scenario (1) or (2) holds (but not sure which)
    \end{tabular}
    \caption{In Section ~, we consider scenario (1) and (2). In Section ~, we consider scenario for the source doubly robustness. }
    \label{tab:my_label}
\end{table}
\fi 
%%%%%The existing results do not ensure the double robustness in terms of source conditions. We have demonstrated the construction of an estimator that allows for asymptotically normal inference when $h_0$ satisfies the $\beta$-source condition and $q_0$ satisfies an arbitrary weak source condition. A corresponding statement by interchanging their roles has been similarly shown. However, to achieve double robustness, we aim to develop an estimator that allow for asymptotically normal inference for \emph{both} scenarios simultaneously.
The current results do not guarantee double robustness in terms of source conditions. We have presented an estimator that enables asymptotically normal inference when $h_0$ satisfies the $\beta$-source condition and $q_0$ satisfies an arbitrary weak source condition. Similarly, we have established a corresponding statement in a scenario when we interchange their role. In this section, we aim to develop an estimator that achieves double robustness, allowing for asymptotically normal inference in \emph{both} scenarios simultaneously.

To achieve this objective, we propose the following refined methods. For convenience, let 
\begin{align}
    L_n(h) :=~& \max_{f\in \Fcal} \E_n[2(m(W;f)-h(X)\,f(Z)) - f(Z)^2]\\
    \tilde{L}_n(q) :=~& \max_{g \in \Gcal}\E_n[2\,(\tilde m(W;g) - q(Z)\, g(Z)) - g(X)^2 ]
\end{align}
Then we introduce the constrained function classes:
% \vsedit{
\begin{align}
       \tilde \Hcal :=~& \{h \in \Hcal : L_n(h) - \min_{h\in \Hcal} L_n(h)\leq \mu_{n} \}, \label{eqn:emp-loss-h}
       \tilde \Qcal :=~& \{q \in \Qcal: \tilde{L}_n(q) - \min_{q \in \Qcal}\tilde{L}_n(q)\leq \mu_{n} \}, 
\end{align}
% }
where $\mu_{h,n}$ and $\mu_{q,n}$ are parameters that will be specified later. After defining these version spaces, we follow the same procedure as before, but now using $\tilde \Hcal$ and $\tilde \Qcal$ instead of $\Hcal$ and $\Qcal$, respectively. Here, we construct these version spaces such that with high probability: (1) all the functions in $\tilde \Hcal$ are sufficiently close to $h_0$ in terms of weak metric, even in the absence of the source condition, and (2) $h_{*} \in \tilde \Hcal$ under the $\beta$-source condition (similarly for $\tilde \Qcal$). The first property plays a crucial role in achieving source double robustness, while the second property allows us to apply \cref{thm:adv-l2} to $\tilde \Hcal$ instead of $\Hcal$ and ensure a sufficiently fast rate in terms of strong metric under the $\beta$-source condition.


Formally, the resulting estimators with version spaces possess the following properties. An analogous conclusion can be also derived for $\hat q$. We first consider the non-iterated version. 
% \xmcomment{We used $\mu_{h, n}, \mu_{q, n}$ in $\tilde \Hcal, \tilde \Qcal$. So the $\mu_n$ below is not very clear. Maybe we should simply replace $\mu_{h, n}, \mu_{q, n}$ by a common $\mu_n$ in the definition of $\tilde \Hcal, \tilde \Qcal$ since the theory below indeed sets them as the same value. }

\begin{corollary}[Non-iterated version]\label{thm:adv-l3}
Suppose conditions (c) and (d) in \cref{thm:adv-l2}. Then, assuming $h_0 \in \Hcal$ and setting $\mu_n=\Omega(\max(\delta^2_n,\lambda^{\min(\beta+1,2)}))$, with probability $1-\zeta$, we can ensure 
    \begin{align}
        \|\Tcal(\hat{h} - h_0)\|^2 =~& O\left(\mu_n\right).
    \end{align}
If two conditions: (a) a function $h_0$ satisfies $\beta$-source condition, (b) realizability $h_* \in \Hcal$ in \cref{thm:adv-l2} additionally hold, with probability $1-\zeta$, we can ensure 
\begin{align}
     \|\hat{h} - h_0 \|^2 =  O( \delta^2_n/\lambda + \lambda^{\min(\beta,1)}). 
\end{align}
\end{corollary}


Importantly, the first statement concerning the weak metric does not rely on the source condition. Utilizing this theorem, let us consider a scenario where either $h_0$ or $q_0$ satisfies the $\beta$-source condition, but we are uncertain about which one. In such cases, by setting $\mu_n = c (\delta^2_n + \lambda^{\min(\beta+1,2)})$, asymptotically normal inference remains feasible under the condition that
\begin{align}
    n \times \underbrace{(\delta^2_n + \lambda^{\min(\beta+1,2)})}_{\text{weak metric }}\times \underbrace{(\delta^2_n/\lambda + \lambda^{\min(\beta,1)}) }_{\text{strong metric}}=o(1).
\end{align}
% \vsedit{
By optimally setting $\lambda\sim \delta_n^{\frac{2}{1+\min(\beta, 1)}}$, the above becomes
\begin{align}
    n  \times \underbrace{ \delta^2_n}_{\text{Week metric  }}  \times \underbrace{\delta^\frac{2 \min(\beta,1) }{1 + \min(\beta,1)}_n}_{\text{Strong metric } }= o(1). 
\end{align}
Moreover, since $\beta>0$, the above setting of $\lambda$ also ensures that consistency with respect to the strong metric at some arbitrarily slow rate continues to hold, even when the $\beta$-source condition does not hold for the function but only an $\epsilon$ source condition holds for an arbitrarily small $\epsilon>0$ (see also discussion in the previous section).
% }
This result is noteworthy since, in the previous section using Theorem \ref{thm:adv-l2}, we required prior knowledge regarding which one between $h_0$ and $q_0$ satisfies the source condition.

%%%The first statement states that \emph{regardless of this source condition}, we can ensure fast convergence in terms of weak metric. This is seen by taking Indeed, the version space is constructed so that any element $\tilde h$ in $\tilde \Hcal$ satisfies  $\|\tilde h - h_0\|^2_2 =c \mu_n$.  The second statement holds following the proof of \cref{thm:adv-l2}. Here, to invoke \cref{thm:adv-l2} for $\tilde \Hcal$ instead of $\Hcal$, we need to ensure $h_* \in \tilde \Hcal$ in high probability. Indeed, the version space is constructed so that any function $ h \in \Hcal$ that satisfies $\|h - h_0\|^2_2 =\mu_n - c\delta^2_n)$ belongs to $\tilde \Hcal$. 

Next, we consider the iterated version to leverage a higher value of $\beta\geq 1$. At each iteration, we optimize over the constrained spaces ${\tilde \Hcal_t, \tilde \Qcal_t}$, defined similar to $\tilde\Hcal,\tilde\Qcal$, but with an iterate dependent upper bound $\mu_{n,t}$ instead of $\mu_n$.
% \xmcomment{Shall we briefly clarify what these spaces are and mention $\mu_{n, t}$? ($\mu_{n, t}$ is not introduced yet)}
By replacing $\Hcal$ and $\Qcal$ with $\tilde \Hcal_t$ and $\tilde \Qcal_t$ respectively at each iteration, we obtain estimators $\hat h_t$ and $\hat q_t$. Here is the property of the estimators. 

% \vsedit{
\begin{corollary}[Iterated version]\label{thm:adv-l3-iter}
Suppose conditions (c), (d) in \cref{thm:adv-l2}. Then, assuming $h_0 \in \Hcal$ and setting $\mu_{n,t} = \Omega(\max(\delta^2_n,\lambda^{\min(\beta+1,2t)}))$, for any $\lambda\leq 1$,  with probability $1-O(\zeta)$, we can ensure 
    \begin{align}
        \|\Tcal(\hat{h}_t - h_0)\|^2 =~& O\left(\mu_{n,t} \right).
    \end{align}
If we further assume that (a) the function $h_0$ satisfies $\beta$-source condition, (b') the realizability assumption that $\forall t \in [\beta/2], h_{*,t} \in \Hcal$ in \cref{thm:adv-l2}, (e) the smoothness assumption that for all $h\in \Hcal$, we have  $\|T(h-h_0)\|_{L_\infty}=O\left(\|T(h-h_0)\|^{\gamma}\right)$, then with probability $1-O(\zeta)$:
\begin{align}
     \|\hat{h}_t - h_0 \|^2 =  O(\delta^2_n/\lambda^{2-\gamma}  + \lambda^{\min(\beta,2t)}). 
\end{align}
\end{corollary}
% }


Again, in contrast to \cref{thm:adv-l2-iter}, the first statement does not impose any source condition. Using this theorem, let us consider a scenario where either $h_0$ or $q_0$ satisfies the $\beta$-source condition, but we are uncertain about which one. In this case, asymptotically normal inference is possible as long as
% \vsedit{
\begin{align}
    n \times \underbrace{(\delta^2_n + \lambda^{\min(\beta+1,2t)})}_{\text{weak metric}}\times \underbrace{(\delta^2_n/\lambda^{2-\gamma} + \lambda^{\min(\beta,2t)}) }_{\text{strong metric}}=o(1).
\end{align}
By optimizing $\lambda\sim \delta_n^{\frac{2}{\beta+1}}$, we can conclude that when $(\beta+1)/2 \leq t\leq c'
\log\log(1/\delta_n)$, the asymptotically normal inference for $\hat{\theta}$ is possible as long as 
\begin{align}
    n \times  \underbrace{\delta^{2}_n}_{\text{Weak metric}}\times  \underbrace{\delta^{2\frac{\beta-1 + \gamma}{\beta+1} }_n}_{\text{Strong metric}}    =o(1). 
\end{align}
Moreover, for $\beta\geq 1$, this setting of $\lambda$ is $o(\delta_n)$ and therefore always ensures also consistency with respect to the strong metric, at some arbitrarily slow rate continues to hold, even when the $\beta$-source condition does not hold for the function but only an $\epsilon$ source condition holds for an arbitrarily small $\epsilon>0$.
% }
Interestingly, this conclusion suggests that in the scenario of source double robustness, our focus should be on achieving a fast rate for the weak metric, even if it comes at the expense of the strong metric. \footnote{Recall, by optimizing $\lambda$ just for the strong metric and for $\gamma=0$, we could achieve $\delta_n^{2\beta/(2+\beta)}$, which is faster than $ \delta^{\frac{2(\beta-1)}{\beta+1} }_n$.} 

To summarize, by combining the best of the two product rate conditions from the two Corollaries and choosing the best of the two conditions, based on the known value of $\beta$, we get the size requirement on the function classes:  
% \vsdelete{\begin{align}
%     \delta_n = O(n^{-\alpha}),\quad \alpha := \min\left( \frac{1 + \min(\beta,1) }{ 2 + 4 \min(\beta,1) }, \frac{1+\beta}{4 + 4 \beta} \right). 
% \end{align}}
% \vsedit{
\begin{align}
    \delta_n = o(n^{-\alpha}),\quad \alpha := \min\left( \frac{1 + \min(\beta,1) }{ 2 + 4 \min(\beta,1) }, \frac{1+\beta}{2\gamma + 4 \beta} \right). 
\end{align}
Note that the smoothness assumption always holds when $\gamma=0$, and therefore, in the absence of any smoothness condition the condition is derived from the latter formula with $\gamma=0$.
% }


\begin{remark}[Computational Aspects for RKHS function spaces]
    As an example of how to address the computational aspect of the proposed constrained regularized estimator, we investigate the case of RKHS hypothesis spaces. We will consider the computation of $\hat{h}$, but an analogous approach can be taken for $\hat{q}$. At each iteration of the iterative and constrained Tikhonov regularized estimator, we need to solve a computational problem of the form
    $$\min_{h\in \Hcal: L_n(h) \leq c} L_n(h) + \lambda \E_n[(h(X) - \bar{h}(X))^2]$$
    with $L_n(h)$ as defined in Equation~\eqref{eqn:emp-loss-h} and where $\Hcal$ and $\Fcal$ are norm-constrained RKHS, i.e. $\Hcal$ corresponds to an RKHS with some positive definite kernel $k_{\Hcal}(x,x')$ and with the constrained that $\|h\|_{\Hcal}^2\leq B$, with $\|\cdot\|_{\Hcal}$ the RKHS norm and some constant $B\geq 0$. Similarly $\Fcal$ is an RKHS with kernel $k_{\Fcal}(z,z')$ and norm constraint $\|f\|_{\Fcal}^2 \leq B$, with $\|\cdot\|_{\Fcal}$ the RKHS norm. First consider the internal optimization problem for each $h$, which appears within the definition of $L_n$:
    \begin{align}
       \max_{f\in \Fcal} \E_n[2(m(W;f)-h(X)\,f(Z)) - f(Z)^2]
    \end{align}
    By well-known representer theorems \citep{KIMELDORF197182,Scholkopf2001representer} of solutions of optimization problems over RKHS functions, and similar to Proposition~10 of \citep{chernozhukov2020adversarial}, any solution to the above problem has to be of the form $f=\Psi'\beta$, for some operator $\Psi: \Hcal \times \R^{2n}$, where each row of $\Psi$ corresponds to known linear combinations of  evaluations of the infinite dimensional Mercer feature map, associated with kernel $k_{\Fcal}$, at some point $\tilde{Z}_i$ that is some modification of the sample $Z_i$. Most notable $K=\Psi\Psi'\in \R^{2n\times 2n}$ corresponds to an appropriately defined empirical finite dimensional kernel matrix. With this property, the optimization problem can be written as a finite $2n$-dimensional parametric constrained concave optimization problem, which is easy to solve:
    \begin{align}
        \max_{\gamma\in \R^{2n}: \gamma'K_{\Fcal}\gamma \leq B} 2 \gamma'c_1 - \gamma' M \gamma
    \end{align}
    for some appropriately defined fixed vector $c_1$ and appropriately defined $2n\times 2n$ symmetric positive semi-definite matrices $K_{\Fcal}$ and $M$. These fixed finite-dimensional vectors and matrices can be calculated with a polynomial in $n$ evaluations of the kernel and the function $h$. The finite-dimensional maximization problem can be solved by ``lagrangifying'' the constraint, computing a closed form solution for each candidate Lagrangian coefficient and performing a line search over the scalar Lagrangian coefficient (see e.g. Appendix~E.3 of \citep{dikkala2020minimax}). Given this solution $\gamma$ to the maximization problem, 
we can also apply the representer theorems for the minimization problem over $h$ (see e.g. Proposition~12 of \citep{chernozhukov2020adversarial}). The minimizer will have a finite $n$-dimensional representation $\Phi\beta$, where each row of $\Phi$ corresponds to the evaluation of the infinite dimensional Mercer feature map, associated with kernel $k_{\Hcal}$, at a sample point $X_i$. The loss function $L_n$ can then be written in the form $\gamma'c_2 - \beta'c_3\, \gamma'c_4$, where $c_2, c_3, c_4$ are finite dimensional fixed vectors that can be calculated with polynomial evaluations of the kernels $k_{\Hcal}$ and $k_{\Fcal}$. The minimization problem can then also be written as a finite dimensional parametric optimization of the form:
    \begin{align}
        \min_{\substack{\beta\in \R^{n}:
        \beta'K_{\Hcal}\beta\leq B \text{ and } \gamma'c_2 - \beta'c_3\, \gamma'c_4 \leq c}} - \beta'c_3\, \gamma'c_4  + \beta'c_5 + \beta'Q\beta
    \end{align}
    for some fixed polynomially computable vectors $c_1, c_2, c_3, c_4, c_5$ and symmetric positive semi-definite matrices $K_{\Hcal}, Q$ (these vectors relate to evaluations of the kernel at pairs of sample points $X_i, X_i'$ and $\tilde{Z}_i, \tilde{Z}_i'$, as well as evaluations of the fixed function $\bar{h}$ used as a regularization center, at sample points $X_i$). This is a constrained convex minimization problem with convex constraints and can be solved in a computationally efficient manner. In fact, it only involves quadratic and linear constraints and objectives and can solved very fast by modern computational toolboxes for quadratic constrained convex optimization via interior point methods. Moreover, since we only have two constraints, they can be ``lagrangified'' and the Lagrange multipliers calculated via grid search over two scalars, turning it into an un-constrained quadratic program for each candidate pair of Lagrange multipliers, which is solvable in closed form.
\end{remark}
