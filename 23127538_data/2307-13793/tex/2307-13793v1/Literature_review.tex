%

\section{Related Work}\label{sec:related}

We first discuss related work specifically focusing on nonparametric IV regression functions, \ie, solutions to $\E[Y-h
(X)\mid Z]=0$. Later, we delve into related work that focuses on estimating functionals $\theta_0$ of nuisance functions that are defined as linear inverse problems, including nonparametric IV regression functions.

% Figure environment removed



\noindent\textbf{Nonparametric IV regression.}
Instrumental variable estimation has garnered significant interest as a particular subset within the realm of inverse problems, as exemplified by the comprehensive investigations \citep{carrasco2007linear,cavalier2011inverse, newey2013nonparametric}.
Nonparametric instrumental variable estimation encounters considerable challenges due to its ill-posed nature, even when the operator $\mathcal{T}$ and response $r_0$ are known. The ill-posedness is often characterized by the presence of one or more of the following aspects: (1) the absence of solutions, (2) the existence of multiple solutions, and (3) the discontinuity of the pseudo-inverse of $\Tcal$. To tackle these challenges, various regularization techniques have been proposed, such as imposing compactness on the solution space \citep{newey2003instrumental} and employing Tikhonov regularization \citep{carrasco2007linear}. In practical settings where $\mathcal{T}$ and $r_0$ are unknown, a range of estimators has been proposed in the literature, including series-based estimators \citep{ai2003efficient,hall2005nonparametric,blundell2007semi,chen2011rate,darolles2011nonparametric,chen2012estimation,florens2011identification,chen2021robust}, kernel-based estimators \citep{hall2005nonparametric,horowitz2007asymptotic}, RKHS-based estimators \citep{singh2019kernel,muandet2020dual,bennett2020variational} and high-dimensional linear estimators under sparsity \citep{gautier2011high,gautier2018high,gautier2022fast}. 

Recently, there has been an increasing interest in applying general function approximation techniques, such as deep neural networks and random forests, to instrumental variable problems in a unified manner \citep{dikkala2020minimax,lewis2018adversarial,bennett2019deep,zhang2020maximum,dikkala2020minimax,bennett2020variational}. However, the guarantees of most current methods remain unclear when solutions are not unique. Notable exceptions include the works of \citep{LiaoLuofeng2020PENE,bennett2023minimax},which provide finite-sample convergence rate guarantees even when solutions may not be unique.

In the closely related work of \citep{LiaoLuofeng2020PENE}, they establish $L_2$ convergence by connecting minimax optimization with Tikhonov regularization under the assumption of the source condition. Specifically, when the number of iterations is limited to one in our method, their method coincides with ours in solving inverse problems. However, our paper presents two important contributions beyond their work. Firstly, we introduce a new iterative procedure that achieves a fast convergence rate under high-order source conditions with $\beta \geq 2$. Secondly, even when the number of iterations is limited to one, our paper achieves a faster convergence rate than theirs due to improved analysis. We note that their paper also makes its own contribution by specifically considering scenarios where function classes are neural networks and providing a theoretical analysis that takes into account the optimization procedure.

Another closely related work \citep{bennett2023minimax} proposes a method that treats IV regression as a constrained optimization problem. While their method does not require the ``closedness assumption,''  which implies smoothness of the operator $\mathcal{T}$, compared to our work, the guaranteed convergence rate in their paper is slower than ours because their method cannot effectively exploit potentially high-order source conditions with $\beta \geq 2$. Additionally, their method does not provide any guarantees under the source condition with $\beta \geq 2$.


We note that there are a number of alternative approaches for integrating machine learning into instrumental variable estimation \citep{hartford2017deep,yu2018deep,xu2020learning,liu2020deep,kato2021learning,lu2021machine}. However, to the best of our knowledge, these approaches do not offer an $L_2$ convergence rate guarantee in the absence of the assumption of uniqueness. 



%
%