%

\section{Introduction}\label{sec: intro}


Many important problems in social and biomedical sciences can be formulated as the estimation of linear functionals of unknown functions that are defined as solutions to linear inverse problems. Examples include nonparametric instrumental variable (IV) regression problems \citep[e.g., ][]{newey2013nonparametric,ai2012semiparametric,ai2003efficient,chen2015sieve}, missing-not-at-random problems \citep[e.g., ][]{d2010new,miao2015identification,LiMiao2022}, causal inference with unmeasured confounders in the presence of proxy variables (a.k.a. proximal causal inference) \citep[e.g., ][]{miao2018a,cui2020semiparametric,deaner2018proxy,kallus2021causal}, partially linear regression problems with endogenous regressors \citep[e.g., ][]{chen2021robust,bennett2022inference,ai2007estimation}, and off-policy evaluation in confounded contextual bandits or partially observable Markov decision processes \citep[e.g., ][]{tennenholtz2020off,shen2022optimal,bennett2021proximal,Miao2022OffPolicy}. 
%

All these problems are encompassed by the following general statistical estimation problem: we are given data that contain samples of the random variable $W$ and our parameter of interest is defined as:
\begin{align}\label{eqn:functional}
    \theta_0 =\E[\tilde m(W;h_0)]
\end{align}
where $h \mapsto \tilde m(W;h)$ is a known linear functional of $h$.
Given two $W$-measurable variables $X$ and $Z$,
the function $h_0$ is defined as a solution to a linear inverse problem:
\begin{align}\label{eqn:primal}
    \Tcal h=r_0,  
\end{align}
%
where $h_0$ lies in a closed linear sub-space $\bar{\Hcal}$ of the space $L_2(X)$ of square integrable functions of $X$,
$\Tcal: \bar{\Hcal} \mapsto \bar{\Qcal}$ is a projected conditional expectation operator, \ie, $\Tcal h=\Pi_{\bar{\Qcal}} \E[h(X)\mid Z=\cdot]$, where $\bar{\Qcal}$ is a closed linear sub-space of $L_2(Z)$ and $\Pi_{\bar{\Qcal}}$ denotes the mean-squared projection on the space $\bar{\Qcal}$, and $r_0\in \bar{\Qcal}$ is the Riesz representer of another known linear functional 
$q \mapsto m(W;q)$, \ie:
\begin{align}\label{eq:Reisz}
\forall q \in \bar{\Qcal}:~  \E[m(W;q)] = \E[r_0(Z)\, q(Z)].
\end{align}
The prototypical case is when $\bar{\Hcal}=L_2(X), \bar{\Qcal}=L_2(Z)$ and the moment $m$ is of the simple form $m(W;f)=Y f(Z)$ for some $W$-measurable variable $Y$, in which case $r_0=\E[Y\mid Z]$ and $h_0$ corresponds to the solution of a non-parametric instrumental variable (IV) regression problem, \ie, $\E[Y - h(X)\mid Z]=0$.
%

Despite the significance of this problem, asymptotically normal inference for the parameter of interest $\theta_0$ presents considerable challenges, particularly when the nuisance function $h_0$ is weakly identified. In particular, since $h_0$ is the solution to an inverse problem, many qualitative attributes of $h_0$ could be smoothened out or distorted by the linear operator $\Tcal$, to the point that we would need very many samples to recover these attributes well, or even to the point that these attributes are irrecoverable even in the limit of infinite samples, \ie, the function $h_0$ is not uniquely identified by \cref{eqn:primal}. These problems are typically referred to in the literature as the ill-posedness of the inverse problem \citep{carrasco2007linear,cavalier2011inverse}. A large line of work assumes unique identification in the limit and further imposes quantitative bounds on measures of ill-posedness, so as to establish estimation rates for the function $h_0$. However, it is known that the uniqueness assumption is easily violated in practical scenarios \citep{newey2003instrumental,andrews2005inference,santos2012inference,kallus2021causal}. 

We instead focus on the minimum norm solution to the inverse problem, which removes the need for a uniquely identified $h_0$; even when the inverse problem has multiple solutions, the minimum norm solution is necessarily unique. Most importantly, under reasonable assumptions, the parameter $\theta_0$ of interest is invariant to the chosen solution of the inverse problem and therefore focusing on the minimum norm solution $h_0$ is without loss of generality. More concretely, let 
%
$a_0 \in \bar{\Hcal}$ denote the Riesz representer of the linear functional $h\mapsto \E[\tilde{m}(W;h)]$:
%
%
%
\begin{align}
\forall h\in \bar{\Hcal}: \E[\tilde{m}(W;h)]=\E[a_0(X)\, h(X)].
\end{align}
%
%
Moreover, let $\Tcal^*:\bar{\Qcal}\mapsto \bar{\Hcal}$ denote the adjoint operator of $\Tcal$, which corresponds to the projected conditional expectation operator $\Tcal^* q=\Pi_{\bar{\Hcal}}\E[q(Z)\mid X=\cdot]$. As was already shown in prior work of \cite{severini2006some,severini2012efficiency,bennett2022inference}, if the dual inverse problem $\Tcal^* q = a_0$ admits any solution $q_0 \in \bar{\Qcal} \subseteq L_2(Z)$, then $\theta_0=\E[\tilde{m}(W;h_0)]$ takes the same value for any $h_0\in \bar{\Hcal}$
%
solving \cref{eqn:primal}, irrespective of what solution we use. 
%
Intuitively, existence of a solution $q_0$ is an assumption that the Riesz representer $a_0$ lies primarily on the higher order spectrum of the eigendecomposition of the operator $\Tcal$. Even though $h_0$ is not uniquely identified, the parameter $\theta_0$ is the projection of $h_0$ on the higher order spectrum, and this projection is uniquely identified. 

When such a solution $q_0$ exists, the parameter $\theta_0$ also admits a doubly robust representation:
\begin{align}
    \theta(h, q) := \E[\tilde m(W;h) + m(W;h)  - h(X)\, q(Z) ], 
\end{align}
which satisfies the mixed bias property:
\begin{align}
    \theta(h, q) - \theta_0 = \E[(q(Z) - q_0(Z))\, (h_0(X) - h(X))]
\end{align}
This formula lends itself to a natural estimation strategy: estimate $\hat{h}$ and $\hat{q}$ on a separate sample and then estimate $\theta_0$ in a plug-in manner, by taking the empirical analogue of the doubly robust representation formula:
\begin{align}\label{eqn:estimator}
    \hat{\theta} := \E_n[\tilde m(W;\hat h) + m(W;\hat h)  - \hat h(X) \hat q(Z)]. 
\end{align}
Prior work of \cite{chernozhukov2023simple, bennett2022inference} shows that this estimate is root-$n$ asymptotically normal when
\begin{align}
\Bcal_n := \min\Big\{ \|\Tcal (\hat h-h_0)\|_{L_2}\|\hat q-q_0\|_{L_2},~~\|\hat h-h_0\|_{L_2}\|\Tcal^{*}(\hat q-q_0)\|_{L_2} \Big\} = o_p(n^{-1/2}). 
\end{align}
and both $\|\hat{h}-h_0\|_{L_2}=o_p(1)$ and $\|\hat{q}-q_0\|_{L_2}=o_p(1)$. This observation implies that obtaining estimators with sufficient convergence guarantees for either $ \|\Tcal (\hat h-h_0)\|_{L_2}\|\hat q-q_0\|_{L_2}$ or $\|\hat h-h_0\|_{L_2}\|\Tcal^{*}(\hat q-q_0)\|_{L_2} $ is adequate for achieving asymptotically normal inference. Note that the 
%
estimation metric 
$\|\Tcal (\hat{h}-h_0)\|_{L_2}=\sqrt{\E\left[\left(\Pi_{\bar{\Qcal}}\E\left[\hat{h}(X)-h_0(X)\mid Z\right]\right)^2\right]}$,
%
which we refer to as the weak metric, can be much smaller than $\|\hat{h}-h_0\|_{L_2}=\sqrt{\E[(\hat{h}(X)-h_0(X))^2]}$, which we refer to as the strong metric. Estimation of $h_0$ with respect to the weak metric does not typically suffer from the ill-posedness of the inverse problem that defines $h_0$. Similarly for estimating $q_0$ with respect to its corresponding weak metric $\|\Tcal^{*}(\hat q-q_0)\|_{L_2}$. 
%
Thus for $\hat{\theta}$ to be root-$n$ asymptotically normal, it suffices to estimate only one of the two nuisance functions $h_0, q_0$ at a sufficiently fast rate with respect to the strong metric and the other with respect to the weak metric. Moreover, we always need to ensure consistency for both functions with respect to the strong metric, but without any rate.

\vspace{.5em}\noindent\textbf{Main contribution.} The main goal of our work is to establish novel estimators for $\hat{h}$ and $\hat{q}$ that allow for general non-linear function spaces and which provide guarantees on the quantity $\Bcal_n$ as a function of measures of ill-posedness of the primal and dual inverse problems that define $h_0$ and $q_0$, respectively, in the absence of unique identification.

\emph{Our main result is an ill-posedness doubly robust estimator:} we will give a single estimation algorithm for $\hat{h}$ and $\hat{q}$ such that if one of the two inverse problems is sufficiently well-posed then the parameter estimate $\hat{\theta}$ is root-$n$ asymptotically normal. \emph{Crucially the estimation algorithm does not need to know which inverse problem is the well-posed one.} Moreover, our estimation algorithm adapts to the degree of well-posedness of the most well-posed of the two inverse problems. For instance, as the largest of the two degrees goes to infinity, our requirements for asymptotic normality converge to the ones that correspond to the case when $h_0$ corresponds to the solution of a simple regression problem. \emph{This is the first such double robustness result, with respect to ill-posedness, in the literature.}

%
We measure the ill/well-posedness of the inverse problems using the \emph{source condition}.
Unlike other measures proposed in the literature, the source condition is an appropriate measure even in the absence of unique identification. To describe the source condition, let us restrict for the moment to linear operators that admit a countable singular value decomposition 
\begin{align}
    \Tcal h = \sum_{i=1}^\infty \sigma_i\, \langle h, v_i\rangle_{L_2}\, u_i ,
\end{align}
%
%
where $\sigma_1\geq\sigma_2\geq\ldots$ are the singular values;  
%
$\Tcal$ and form an orthonormal basis of $\bar{\Qcal}$, \ie, 
$\E[u_i(Z) u_j(Z)] = 1\{i=j\}$; and 
$v_i: X\to \R$ are the right eigenfunctions of $\Tcal$ and also form an orthonormal basis of $\bar{\Hcal}$, \ie, $\E[v_i(X) v_j(X)]=1\{i=j\}$.
%
 %
The $\beta$-source condition on 
\cref{eqn:primal} states that the minimum norm solution $h_0$ is primarily supported on the 
lower part of the spectrum of the eigendecomposition of $\Tcal$:
\begin{align}
     \sum_{i=1}^\infty 1\{\sigma_i\neq 0\} \frac{\langle h_0, v_i\rangle_{L_2}^2 }{\sigma_i^{2\beta}} <\infty.
\end{align}
Note this implies that for any $m$, $\sum_{i=m}^\infty \langle h_0, v_i\rangle_{L_2}^2 \lesssim \sigma_m^{2\beta}$ (note also that the minimum norm solution has zero inner product with eigenfunctions for which $\sigma_i=0$). Thus the parameter $\beta$ in the source condition controls the amount of support that $h_0$ is allowed to have on the tail of the spectrum. As $\beta$ goes to infinity, the function $h_0$ behaves as being supported only on a finite set of eigenfunctions and the ill-posedness problem vanishes. More generally, $\beta$ controls the degree of ill-posedness and larger $\beta$ means that the inverse problem is more well-posed. 

The source condition is well defined even when the linear operator does not admit a singular value decomposition and, in its general form, requires the minimum norm solution $h_0$ to satisfy
%
%
\begin{align}
   \exists w_0\in \bar{\Hcal} : h_0 = (\Tcal^* \Tcal)^{\beta/2} w_0.
\end{align}
%

Let $\beta_h, \beta_q$ denote the degree of well-posedness of the primal and dual inverse problems that define $h_0$ and $q_0$ respectively, and let $\beta = \max\{\beta_h,\beta_q\}$, denote the largest of the two degrees, \ie, the degree of the most well-posed of the two inverse problems. 
%
Moreover, we will impose the inductive biases that minimum norm solutions to the inverse problems and regularized variants of them belong to the smaller function classes $\Hcal\subseteq \bar{\Hcal}, \Qcal\subseteq \bar{\Qcal}$.
%
Let $\delta_n$ denote the statistical complexity of appropriately defined function classes related to $\Hcal, \Qcal$ and function spaces $\Fcal, \Gcal$ that encompass their composition with the linear operators, \ie, $\bar{\Qcal} \supseteq \Fcal \supseteq \Tcal \circ (h_0 - \Hcal)=\{\Tcal (h_0 - h): h\in \Hcal\}$ and $\bar{\Hcal} \supseteq \Gcal \supseteq \Tcal^*\circ (q_0 - \Qcal) = \{\Tcal^* (q_0 - q): q\in \Qcal\}$. We will measure statistical complexity using the well-established notion of the critical radius, defined via the means of localized Rademacher complexities of the corresponding classes. 

Our main technical result is the development of an estimation algorithm for $\hat{h},\hat{q}$, such that the resulting parameter estimate $\hat{\theta}$ is root-$n$ asymptotically normal if:
\begin{align}
    \delta_n = o(n^{-\alpha}),\quad \alpha := \min\left( \frac{1 + \min(\beta,1) }{ 2 + 4 \min(\beta,1) }, \frac{1+\beta}{4 \beta} \right). 
\end{align}
% Figure environment removed

Notably, as $\beta\to \infty$, we get that we require $\delta_n = o(n^{-1/4})$, which is the requirement when $h_0$ is the solution to a regression, or equivalently a conditional expectation, problem \citep{chernozhukov2017double}. Moreover, for $\beta \in [1, 3]$, the requirement is $\delta_n=o(n^{-1/3})$, matching the prior work of \cite{bennett2022inference}, which applied only when $\beta=1$ and this degree of well-posedness was assumed to be satisfied by the inverse problem that defines $q_0$. Finally, even for severely ill-posed problems, where $\beta\geq \epsilon>0$ for some small $\epsilon$, the requirement is $\delta_n=o(n^{-1/2 + \kappa})$ for some $\kappa>0$, 
%
which is satisfied, for instance, for VC-subgraph classes.

If we know which of the two inverse problems is more well-posed, then we show that we can further weaken the requirement to:
\begin{align}
    \delta_n = o(n^{-\alpha}),\quad \alpha := \min\left( \frac{1 + \min(\beta,1) }{ 2 + 4 \min(\beta,1) }, \frac{2+\beta}{4 + 4 \beta} \right). 
\end{align}
Notably the loss due to not knowing which inverse problem is more well posed is minimal and primarily occurs when $\beta\in [2,3]$. Importantly, there is no loss for moderately ill-posed problems when $\beta\leq 1$, which is arguably the most practically relevant case. Moreover, if we make the further assumption that the function spaces are smooth enough that the projected $L_2$ norm is related to the projected $L_\infty$ norm, \ie, $\|T h\|_{L_\infty}=O\left(\|Th\|_{L_2}^\gamma\right)$, and similarly for $q$, then this loss can be further ameliorated. For instance, as $\gamma\to 1$ (a property satisfied for instance by Reproducing Kernel Hilbert Spaces with an exponential eigendecay, which is the case for the Gaussian kernel), then there is no loss to not knowing which inverse function is more well-posed and the requirement is always of the even weaker form:
\begin{align}
    \delta_n = o(n^{-\alpha}),\quad \alpha := \frac{1 + \beta }{ 2 + 4 \beta}
\end{align}

\vspace{.5em}\noindent\textbf{Main techniques.} Our result is enabled by several novel contributions of independent interest. Our first goal is the development of estimation algorithms for a function defined via a linear inverse problem that satisfies a known $\beta$-source condition. Such algorithms can be applied both for the primal and for the dual inverse problems. We describe our results here in the context of the primal inverse problem $\Tcal h=r_0$. 

Our overall goal is an estimation algorithm that produces an estimate $\hat{h}$, such that irrespective of whether the source condition holds, it guarantees a fast convergence rate of $O(\delta_n^2)$, with respect to the weak metric. Moreover, when the source condition does hold it also guarantees a statistical rate of $O(\delta_n^{\kappa(\beta_h)})$, for some exponent $\kappa(\beta_h)$ with respect to the strong metric. Note that if we manage to construct such an estimation algorithm, then if we apply this algorithm for both the primal and the dual inverse problems, then we will be guaranteeing that $\Bcal_n=O\left(\delta_n^{2 + \kappa(\beta)}\right)$, where $\beta=\max\{\beta_h,\beta_q\}$, which would then lead to the required condition on $\delta_n$.

Achieving a fast learning rate with respect to the weak metric has already been established in prior work of \cite{dikkala2020minimax}, with the use of an adversarial estimation strategy, which was further extended in \citep{bennett2022inference} to ensure consistency with respect to the strong metric, via the means of Tikhonov regularization, for the following estimator:
\begin{align}
    \hat{h} = \argmin_{h\in \Hcal} \max_{f\in \Fcal} \E_n\left[2\,m(W;f) - h(X)\, f(Z) - f(Z)^2\right] + \lambda \E_n\left[h(X)^2\right]
\end{align}
Moreover, prior work of \cite{LiaoLuofeng2020PENE} established strong metric rates for this Tikhonov regularized adversarial estimator, for smooth function classes with neural network function approximation. However, the rate in \citep{LiaoLuofeng2020PENE} is sub-optimal, does not adapt to the critical radius of arbitrary function spaces, and does not adapt to large values of $\beta$ (only to $\beta\leq 1$). Our first result is a fast strong metric rate result for the Tikhonov regularized estimator for $\beta \leq 1$. Our second result is a fast rate result for an iterated version of the Tikhonov regularized estimator, that uses prior iteration estimates to center the regularization appropriately and which adapts to large values of $\beta$, leading to strong metric rates of $O(\delta_n^2)$ as $\beta \to \infty$. These two results are novel in the literature on estimation of linear inverse problems under a source condition and are of independent interest.

Finally, we show that the desired simultaneous guarantee can be ensured via a constrained Tikhonov regularized adversarial estimator. In particular, our estimator first solves the un-regularized objective to find a solution that guarantees a fast weak metric rate. Subsequently, it solves the regularized objective within the sub-space of solutions that also achieve a small un-regularized risk, compared to the un-regularized solution. We show that this estimator simultaneously enjoys both guarantees: a weak metric rate of $\delta_n^2$, without requiring a $\beta$-source condition and a strong metric rate of $\delta_n^{2\max\left\{\frac{\min(\beta,1)}{1+\min(\beta,1)}, \frac{\beta-1}{\beta+1}\right\}}$, when the $\beta$-source condition holds. This theorem enables our main double robustness result.

%

%


%

%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%


%
%
%
%
%



%







\iffalse 
Then, the estimator is given as the empirical average of 
\begin{align*}
    \hat \theta = \EE_n[\tilde m(W;\hat h) + m(W;\hat q)-\hat q(Z)\hat h(X) ]
\end{align*}
 As will see later, letting $h_0$ and $q_0$ be the least norm solutions, the bias term can be written as 
 \begin{align*}
   \theta_0 -\hat \theta_0 = o_p(n^{-1/2}) +  \sqrt{n}\min\{ \|\Tcal (h-h_0)\|_{L_2(X)}\|\hat q-q_0\|_2,   \}. 
 \end{align*} 
 \fi 




