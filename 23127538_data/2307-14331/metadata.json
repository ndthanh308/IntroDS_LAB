{
  "title": "Visual Instruction Inversion: Image Editing via Visual Prompting",
  "authors": [
    "Thao Nguyen",
    "Yuheng Li",
    "Utkarsh Ojha",
    "Yong Jae Lee"
  ],
  "submission_date": "2023-07-26T17:50:10+00:00",
  "revised_dates": [],
  "abstract": "Text-conditioned image editing has emerged as a powerful tool for editing images. However, in many situations, language can be ambiguous and ineffective in describing specific image edits. When faced with such challenges, visual prompts can be a more informative and intuitive way to convey ideas. We present a method for image editing via visual prompting. Given pairs of example that represent the \"before\" and \"after\" images of an edit, our goal is to learn a text-based editing direction that can be used to perform the same edit on new images. We leverage the rich, pretrained editing capabilities of text-to-image diffusion models by inverting visual prompts into editing instructions. Our results show that with just one example pair, we can achieve competitive results compared to state-of-the-art text-conditioned image editing frameworks.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14331",
  "pdf_url": null,
  "comment": "Project page: https://thaoshibe.github.io/visii/",
  "num_versions": null,
  "size_before_bytes": 42909706,
  "size_after_bytes": 159624
}