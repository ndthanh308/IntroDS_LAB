\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{image_and_video_editing_with_stylegan3}
Alaluf, Y., Patashnik, O., Wu, Z., Zamir, A., Shechtman, E., Lischinski, D.,
  Cohen-Or, D.: Third time's the charm? image and video editing with stylegan3.
  In: arXiv (2022)

\bibitem{balaji2022eDiff-I}
Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K.,
  Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., Liu, M.Y.:
  ediff-i: Text-to-image diffusion models with ensemble of expert denoisers.
  arXiv preprint arXiv:2211.01324  (2022)

\bibitem{visprompt}
Bar, A., Gandelsman, Y., Darrell, T., Globerson, A., Efros, A.A.: Visual
  prompting via image inpainting. arXiv preprint arXiv:2209.00647  (2022)

\bibitem{instructpix2pix}
Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow
  image editing instructions. In: arXiv (2023)

\bibitem{brown2020language}
Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., Amodei, D.: Language models are few-shot learners. In: arXiv (2020)

\bibitem{diffusionbeatsgan}
Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. In:
  Advances in Neural Information Processing Systems. vol.~34, pp. 8780--8794.
  Curran Associates, Inc. (2021),
  \url{https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf}

\bibitem{dinh2021hyperinverter}
Dinh, T.M., Tran, A.T., Nguyen, R., Hua, B.S.: Hyperinverter: Improving
  stylegan inversion via hypernetwork. In: Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition (CVPR) (2022)

\bibitem{esser2021taming}
Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution
  image synthesis. In: arXiv (2021)

\bibitem{makeascene}
Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., Taigman, Y.:
  Make-a-scene: Scene-based text-to-image generation with human priors. In:
  arXiv (2022)

\bibitem{textualinversion}
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
  Cohen-Or, D.: An image is worth one word: Personalizing text-to-image
  generation using textual inversion. In: arXiv (2022).
  \doi{10.48550/ARXIV.2208.01618}, \url{https://arxiv.org/abs/2208.01618}

\bibitem{gal2021stylegannada}
Gal, R., Patashnik, O., Maron, H., Chechik, G., Cohen-Or, D.: Stylegan-nada:
  Clip-guided domain adaptation of image generators. In: arXiv (2021)

\bibitem{promptist}
Hao, Y., Chi, Z., Dong, L., Wei, F.: Optimizing prompts for text-to-image
  generation. In: arXiv (2022)

\bibitem{hertz2022prompttoprompt}
Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.:
  Prompt-to-prompt image editing with cross attention control. In: arXiv (2022)

\bibitem{ho2020denoising}
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In:
  arXiv (2020)

\bibitem{huang2023reversion}
Huang, Z., Wu, T., Jiang, Y., Chan, K.C., Liu, Z.: {ReVersion}: Diffusion-based
  relation inversion from images. arXiv preprint arXiv:2303.13495  (2023)

\bibitem{pix2pix2017}
Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with
  conditional adversarial networks. CVPR  (2017)

\bibitem{stylegan}
Karras, T., Laine, S., Aila, T.: A style-based generator architecture for
  generative adversarial networks. In: Proceedings of the IEEE/CVF Conference
  on Computer Vision and Pattern Recognition (CVPR) (June 2019)

\bibitem{imagic}
Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I.,
  Irani, M.: Imagic: Text-based real image editing with diffusion models. In:
  Conference on Computer Vision and Pattern Recognition 2023 (2023)

\bibitem{DiffusionCLIP}
Kim, G., Kwon, T., Ye, J.C.: Diffusionclip: Text-guided diffusion models for
  robust image manipulation. In: Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition (CVPR). pp. 2426--2435 (June 2022)

\bibitem{kiros2015skip}
Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R.S., Torralba, A., Urtasun, R.,
  Fidler, S.: Skip-thought vectors. arXiv preprint arXiv:1506.06726  (2015)

\bibitem{li2023gligen}
Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., Lee, Y.J.: Gligen:
  Open-set grounded text-to-image generation. In: arXiv:2301.07093 (2023)

\bibitem{liu2022selfconditioned}
Liu, Y., Gal, R., Bermano, A.H., Chen, B., Cohen-Or, D.: Self-conditioned
  generative adversarial networks for image editing. In: arXiv (2022)

\bibitem{loshchilov2019decoupled}
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: arXiv
  (2019)

\bibitem{sdedit}
Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit:
  Guided image synthesis and editing with stochastic differential equations.
  In: arXiv (2022)

\bibitem{nulltext}
Mokady, R., Hertz, A., Aberman, K., Pritch, Y., Cohen-Or, D.: Null-text
  inversion for editing real images using guided diffusion models. In: arXiv
  (2022)

\bibitem{mokady2021clipcap}
Mokady, R., Hertz, A., Bermano, A.H.: Clipcap: Clip prefix for image
  captioning. In: arXiv (2021)

\bibitem{glide}
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
  Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and
  editing with text-guided diffusion models. In: arXiv (2022)

\bibitem{zeropix2pix}
Parmar, G., Singh, K.K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-shot
  image-to-image translation. In: arXiv (2023)

\bibitem{styleclip}
Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D.: Styleclip:
  Text-driven manipulation of stylegan imagery. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV). pp. 2085--2094 (October
  2021)

\bibitem{clip}
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning
  transferable visual models from natural language supervision. In: arXiv
  (2021)

\bibitem{dalle}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical
  text-conditional image generation with clip latents. In: arXiv (2022).
  \doi{10.48550/ARXIV.2204.06125}, \url{https://arxiv.org/abs/2204.06125}

\bibitem{dalle2}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical
  text-conditional image generation with clip latents. In: arXiv (2022)

\bibitem{richardson2021encoding}
Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S.,
  Cohen-Or, D.: Encoding in style: a stylegan encoder for image-to-image
  translation. In: IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) (June 2021)

\bibitem{roich2021pivotal}
Roich, D., Mokady, R., Bermano, A.H., Cohen-Or, D.: Pivotal tuning for
  latent-based editing of real images. In: arXiv (2021)

\bibitem{latentdiffusion}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
  image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition (CVPR). pp.
  10684--10695 (June 2022)

\bibitem{stablediffusion}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
  image synthesis with latent diffusion models. In: arXiv (2021)

\bibitem{ruiz2022dreambooth}
Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.:
  Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
  generation. In: arXiv (2022)

\bibitem{saharia2022palette}
Saharia, C., Chan, W., Chang, H., Lee, C.A., Ho, J., Salimans, T., Fleet, D.J.,
  Norouzi, M.: Palette: Image-to-image diffusion models. In: arXiv (2022)

\bibitem{imagen}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
  S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet,
  D.J., Norouzi, M.: Photorealistic text-to-image diffusion models with deep
  language understanding. In: arXiv (2022)

\bibitem{tov2021designing}
Tov, O., Alaluf, Y., Nitzan, Y., Patashnik, O., Cohen-Or, D.: Designing an
  encoder for stylegan image manipulation. arXiv preprint arXiv:2102.02766
  (2021)

\bibitem{painter}
Wang, X., Wang, W., Cao, Y., Shen, C., Huang, T.: Images speak in images: A
  generalist painter for in-context visual learning. arXiv preprint
  arXiv:2212.02499  (2022)

\bibitem{SegGPT}
Wang, X., Zhang, X., Cao, Y., Wang, W., Shen, C., Huang, T.: Seggpt: Segmenting
  everything in context. arXiv preprint arXiv:2304.03284  (2023)

\bibitem{wang2023promptdiffusion}
Wang, Z., Jiang, Y., Lu, Y., Shen, Y., He, P., Chen, W., Wang, Z., Zhou, M.:
  In-context learning unlocked for diffusion models. arXiv preprint
  arXiv:2305.01115  (2023), \url{https://arxiv.org/abs/2305.01115}

\bibitem{promptdiffusion}
Wang, Z., Jiang, Y., Lu, Y., Shen, Y., He, P., Chen, W., Wang, Z., Zhou, M.:
  In-context learning unlocked for diffusion models. arXiv preprint
  arXiv:2305.01115  (2023), \url{https://arxiv.org/abs/2305.01115}

\bibitem{hard_prompt_made_easy}
Wen, Y., Jain, N., Kirchenbauer, J., Goldblum, M., Geiping, J., Goldstein, T.:
  Hard prompts made easy: Gradient-based discrete optimization for prompt
  tuning and discovery. In: arXiv (2023)

\bibitem{witteveen2022investigating}
Witteveen, S., Andrews, M.: Investigating prompt engineering in diffusion
  models. In: arXiv (2022)

\bibitem{Tao18attngan}
Xu, T., Zhang, P., Huang, Q., Han~Zhang, Z.G., Huang, X., He, X.: Attngan:
  Fine-grained text to image generation with attentional generative adversarial
  networks. In: {CVPR} (2018)

\bibitem{yang2022reco}
Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu,
  C., Zeng, M., Wang, L.: Reco: Region-controlled text-to-image generation. In:
  arXiv (2022)

\bibitem{parti}
Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,
  A., Yang, Y., Ayan, B.K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang,
  H., Baldridge, J., Wu, Y.: Scaling autoregressive models for content-rich
  text-to-image generation. In: arXiv (2022)

\bibitem{zhang2017stackgan}
Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.:
  Stackgan: Text to photo-realistic image synthesis with stacked generative
  adversarial networks. In: arXiv (2017)

\bibitem{controlnet}
Zhang, L., Agrawala, M.: Adding conditional control to text-to-image diffusion
  models. In: arXiv (2023)

\bibitem{zhang2023VisualPromptRetrieval}
Zhang, Y., Zhou, K., Liu, Z.: What makes good examples for visual in-context
  learning? In: arXiv (2023)

\bibitem{zhu2019dmgan}
Zhu, M., Pan, P., Chen, W., Yang, Y.: Dm-gan: Dynamic memory generative
  adversarial networks for text-to-image synthesis. In: arXiv (2019)

\end{thebibliography}
