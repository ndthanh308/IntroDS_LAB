\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% ready for submission
% \usepackage{neurips_2023}

% \usepackage[preprint,no]{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2023}

% \usepackage[square,sort,comma,numbers]{natbib}
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[pdftex]{graphicx}
\usepackage{caption,tabularx,booktabs}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow} % vertical text in tables
\usepackage[pdftex]{graphicx}
\usepackage{adjustbox}         % colors
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{bbding}
\usepackage{pifont}
\usepackage[table]{colortbl}
\usepackage{array}
\usepackage{colortbl}
\usepackage{makecell}
\renewcommand{\cellalign}{vh}
\renewcommand{\theadalign}{vh}
\captionsetup[figure]{font=small}
\newcommand{\thao}[1]{{\color{red}\textbf{Thao:} #1}}
\newcommand{\yj}[1]{{\color{cyan}[YJ:  #1]}}
\newcommand{\ut}[1]{{\color{blue} #1}}
\colorlet{first}{blue!15}
\colorlet{second}{blue!10}
\colorlet{third}{blue!5}
\colorlet{forth}{blue!2}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
%consts
\newcommand*{\dataset}{Figures}
\definecolor{darkblue}{RGB}{46,25, 110}

\newcommand{\dssectionheader}[1]{%
   \noindent\framebox[\columnwidth]{%
      {\fontfamily{phv}\selectfont \textbf{\textcolor{darkblue}{#1}}}
   }
}

\newcommand{\dsquestion}[1]{%
    {\noindent \fontfamily{phv}\selectfont \textcolor{darkblue}{\textbf{#1}}}
}

\newcommand{\dsquestionex}[2]{%
    {\noindent \fontfamily{phv}\selectfont \textcolor{darkblue}{\textbf{#1} #2}}
}

\newcommand{\dsanswer}[1]{%
   {\noindent #1 \medskip}
}

\title{Visual Instruction Inversion:\\Image Editing via Visual Prompting}
% \title{Visii:Visual Instruction Inversion \\for Image Editing via Visual Prompting}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%
\author{
  Thao Nguyen \hspace{.1in} Yuheng Li \hspace{.1in} Utkarsh Ojha \hspace{.1in} Yong Jae Lee \\
  % \hspace{0.5cm}\\
  University of Wisconsin-Madison \\ \\
    \url{https://thaoshibe.github.io/visii/}
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle
% Figure environment removed
\begin{abstract}
% When faced with such challenges, visual prompts can be a more informative and intuitive way to convey the edit.
% Given pairs of sample images that represent the ``before'' and ``after'' images of an edit, our goal is to learn a text-based editing direction that can be used to perform the same edit on new images.
Text-conditioned image editing has emerged as a powerful tool for editing images.
However, in many situations, language can be ambiguous and ineffective in describing specific image edits.
When faced with such challenges, visual prompts can be a more informative and intuitive way to convey the desired edit.
We present a method for image editing via visual prompting.
Given example pairs that represent the ``before'' and ``after'' images of an edit, our approach learns a text-based editing direction that can be used to perform the same edit on new images.
% By leveraging large-scale text-to-image diffusion models, which offer unprecedented editing capacity through text prompts, our method inverses visual prompt to editing instruction.
We leverage the rich, pretrained editing capabilities of text-to-image diffusion models by inverting visual prompts into editing instructions.
Our results show that even with just one example pair, we can achieve competitive results compared to state-of-the-art text-conditioned image editing frameworks.
% , \yand can better capture the desired edit transformation when combined with a text prompt}.
\end{abstract}
% \begin{abstract}
% When faced with such challenges, visual prompts can be a more informative and intuitive way to convey the edit.
% Given pairs of sample images that represent the ``before'' and ``after'' images of an edit, our goal is to learn a text-based editing direction that can be used to perform the same edit on new images.
% Text-conditioned image editing has emerged as a powerful tool for editing images.
%However, in many situations, language \ut{alone} can be ambiguous and ineffective in describing specific image edits. When faced with such challenges, visual prompts can be a more \ut{effective} and intuitive way to \ut{specify those edits}.
%We present a method for image editing via visual prompting. \ut{Instead of the user specifying the edit using natural language, our goal is to \emph{learn} the edit instead using a pair of ``before'' and ``after'' images.}

%Given \ut{a pair of ``before'' and ``after'' images representing an edit}, our goal is to \ut{\emph{learn}} a text-based editing direction \ut{instead}, that can be used to perform the same edit on new images. \ut{Once learnt, this instruction can then be concatenated with other}
% We leverage the rich, pretrained editing capabilities of text-to-image diffusion models by inverting visual prompts into editing instructions.
%Our results show that with just one example pair, we can achieve competitive results compared to state-of-the-art text-conditioned image editing frameworks.


% \ut{With the rise of diffusion models, real images can now be edited in specific ways using natural language. However, in many situations, we might already have a pair of ``before'' and ``after'' images depicting an edit that we wish to apply to a new image. In those cases, a user-specified natural language alone can be ineffective \& ambiguous in describing the ``before'' $\rightarrow$ ``after'' change. For such cases, we present a method for image editing where the editing instruction is \emph{learnt} instead using the given pair of images. We do this by leveraging the editing capabilities of rich, pretrained text-to-image diffusion models by inverting visual prompts into editing instructions. Users can then edit a different image either by simply applying the learnt instruction, or by additionally specifying some more edits using natural language. Our results show that with just one example pair, we can achieve competitive results compared to state-of-the-art text-conditioned image editing frameworks.}
% \end{abstract}

\section{Introduction}
% \thaoidea{Rapid progress in text-to-image, diffusion models for image generation and editing}
% In the past few years, diffusion models, especially Stable Diffusion \cite{stablediffusion}, have emerged as a powerful framework for image generation.
% By providing a text prompt, these models can generate stunning images conditioned on that guidance.
% In the realm of image editing, applications of diffusion models have undergone significant progress in recent years \cite{nulltext, instructpix2pix, imagic}.
% These frameworks allow users to edit an input image based on a text prompt, providing new possibilities and endless potential for image editing.
% As these models rely on textual guidance, significant effort and attention have been made for prompt engineering tasks \cite{witteveen2022investigating,promptist,hard_prompt_made_easy}, which aim to find well-designed prompts for text-to-image models.

In the past few years, diffusion models~\cite{latentdiffusion,stablediffusion,diffusionbeatsgan,glide,saharia2022palette,esser2021taming} have emerged as a powerful framework for image generation. In particular, text-to-image diffusion models can generate stunning images conditioned on a text prompt. Such models have also been developed for \emph{image editing} \cite{nulltext,instructpix2pix, imagic,zeropix2pix,controlnet,hertz2022prompttoprompt,huang2023reversion,ruiz2022dreambooth,makeascene,sdedit}; i.e., transforming an image into another based on a text specification. As these models rely on textual guidance, significant effort has been made in prompt engineering \cite{witteveen2022investigating,promptist,hard_prompt_made_easy}, which aims to find well-designed prompts for text-to-image generation and editing.

% \thao{Limitation of language for image editing task}
% But, \textit{what if there is something non-trivial to describe in words?}
% For example, describing the process of turning a photo of a human face into a character in the Shrek movie can be challenging to put into a sentence only (Figure \ref{fig:example}a).
% Or imagine that you want to convert a roadmap to satellite image, what prompt you will write for diffusion models?
% When describing such process, it may be easier to convey the idea visually by showing a visual example (Figure \ref{fig:example}b).
% In many situations, language can be ambiguous and inefficient in describing specific image editing tasks. This is where visual prompts come into play, offering a more intuitive and informative way to present ideas.

% \yj{I feel like this may not be the best example to give here, since the instructpix2pix shrek example is not so bad.  A better one might be the sketch example shown in row 1, columns 1-4 in Fig 3.  For instructpix2pix, we use ``Make it into a sketch'' to generate the output, and we can then choose for ours a query pair that is very different from the instructpix2pix converted sketch.}
But, \textit{what if the desired edit is difficult to describe in words?}
For example, describing \textit{your} drawing style of \textit{your} cat can be challenging to put into a sentence (Figure \ref{fig:example}a).
Or imagine that you want to transform a roadmap image into an aerial one -- it could be difficult to know what the different colored regions in the roadmap image are supposed to represent, leading to an incorrect output image.  In such cases, it would be easier, and more direct, to convey the edit \emph{visually} by showing a before-and-after example image pair (Figure \ref{fig:example}b).
In other words, language can be ambiguous when describing a specific image edit transformation, while visual prompts can offer a more intuitive and precise way to describe it.
% In other words, language can be ambiguous when describing a specific image edit transformation, and visual prompts can offer a more intuitive and precise way to describe the transformation.

% % Visual prompting as image inpaiting
% Recent progress in making visual prompts for computer vision tasks have treated this problem as an image in-painting \cite{visprompt, painter}.
% Given example pair, a new grid-like image is formed, consisting of the [input, output, query, and blank] elements.
% Visual prompting then involves filling in the missing part, or the blank, through image in-painting. 
% After training on a large dataset of computer vision tasks, the so-called the ``Painter'' model, can be used to perform downstream tasks without further fine-tuning.
% While this approach may work well for computer vision tasks such as segmentation, unfortunately,  it may not be as effective for image editing.
% Why? Because unlike other computer vision tasks, image editing is not limited to any specific input-output form.
% Another issue is creating a massive supervised dataset to cover all possible image editing scenarios might be impossible.
% Surprisingly, current text-to-image models can perform image editing task very well based on textual guidance.
% Can text-to-image models learn to perform image editing directly with visual prompts? Still, it remains an open question.
% % Can text-to-image models learn to perform image editing directly with visual prompts, without auxiliary fine-tuning?

% Visual prompting as image inpaiting
Visual prompting for image editing has very recently been explored in \cite{visprompt, painter}.  These works reformulate the problem as an image in-painting task, where an example image pair (``before'' and ``after'') and query image are provided in a single grid-like image. The target output is inpainted (by forming the analogy, \texttt{before:after = query:output}).  After training on a large dataset of computer vision tasks (e.g., edge detection, bounding box localization), these systems aim to perform any of those tasks during testing with in-context learning \cite{visprompt,painter,SegGPT,wang2023promptdiffusion} without further fine-tuning.  However, while they can work reasonably well for standard computer vision tasks such as segmentation and colorization, they cannot be used for general image editing tasks since large datasets for arbitrary edits are typically unavailable. 

This paper investigates image editing via visual prompting using text-to-image diffusion models. Inspired by textual inversion \cite{textualinversion}, which inverts a visual identity specified by an example image into the rich, pre-trained text embedding of a large vision-and-language model \cite{stablediffusion,clip} for text-to-image generation, we propose to \emph{invert the visual edit transformation specified by the example before-and-after image pair into a text instruction}.
% \thao{revisit this sentence. break it into 2??}
In particular, we leverage the textual instruction space that InstructPix2Pix \cite{instructpix2pix} has learned.  Since InstructPix2Pix directly builds upon a pretrained Stable Diffusion model's vast text-to-image generation capabilities, while further finetuning it with 450,000 \texttt{(text instruction, before image, after image)} triplets, our hypothesis is that its learned instruction space is rich enough to cover many image-to-image translations (i.e., image edits), and thus can be fruitful for visual prompt based image editing.
Specifically, given a pair of images representing the ``before'' and ``after'' states of an editing task, we learn the edit direction in text space by optimizing for the textual instruction that converts the ``before'' image into the ``after'' image.  Once learned, this edit direction can then be applied to a new test image, together with a text prompt, facilitating precise image editing; see Figure~\ref{fig:teaser}.

% Our contributions and main findings are:   
% % \thao{"new scheme"? Or: We are the first to address image editing with visual prompting?}
% (1) We introduce a new scheme for image editing via visual prompting.
% (2) We propose a framework for inverting visual prompts into editing instructions for text-to-image diffusion models.
% (3) By conducting in-depth analysis, we share valuable insights about image editing with diffusion models, e.g, reusing the same noise schedule in training for testing leads to a more balanced result between editing effects and faithfulness to the input image.

Our contributions and main findings are:
(1) We introduce a new scheme for image editing via visual prompting.
(2) We propose a framework for inverting visual prompts into editing instructions for text-to-image diffusion models.
(3) By conducting in-depth analyses, we share valuable insights about image editing with diffusion models; e.g., concatenating instructions between learned and natural language yields a hybrid editing instruction that is more precise; or reusing the same noise schedule in training for testing leads to a more balanced result between editing effects and faithfulness to the input image.
% \yj{Briefly state contributions.}
% \thaoidea{Visual Prompting with text-to-image model?}
% Can visual prompts improve the precision and efficiency of image editing?
% This paper aims to investigate image editing via visual prompts using text-to-image diffusion models.
% Specifically, given a pair of images representing the ``before'' and ``after'' states of an editing task, our goal is to learn the editing direction from the ``before'' image to the ``after'' image.
% This direction can then be applied to other images, facilitating efficient and precise image editing.
% Ours contributions are as follows.
% \thao{Ours contributions are...}
% we introduce a new framework to learn editing instruction from visual prompts. Second, we introduce a new losses to structure the editing performance. Last, we introduce a new quantititve metrics for visual prompting image editing task (visual clip loss?)
% Figure environment removed

\section{Related Work}

% resulting in suboptimal outcomes.
\textbf{Text-to-image Models.} Early works on text-to-image synthesis based on GANs \cite{zhang2017stackgan,zhu2019dmgan,kiros2015skip,Tao18attngan} were limited to small-scale and object-centric datasets, due to training difficulties of GANs. 
Auto-regressive models pioneered the use of large-scale data for text-to-image generation \cite{dalle,dalle2,imagen,esser2021taming,parti}.
However, they typically suffer from high computation costs and error accumulation.
An emerging trend is large-scale text-to-image diffusion models, which are the current state-of-the-art in image synthesis, offering unprecedented image fidelity and language reasoning capabilities.
Research efforts have focused on improving their image quality, controllability, and expanding the type of conditional inputs  \cite{li2023gligen,makeascene,balaji2022eDiff-I,controlnet,latentdiffusion,stablediffusion,esser2021taming,glide,saharia2022palette,yang2022reco}.
In the realm of image editing, diffusion models are now at the forefront, providing rich editing capabilities through text descriptions and conditional inputs. In this work, we investigate how to use visual prompts to guide image edits with diffusion models.

\textbf{Image Editing.}
In the beginning, image editing was primarily done within the image space. Previous GAN-based approaches utilized the meaningful latent space of GANs to perform editing \cite{stylegan,image_and_video_editing_with_stylegan3,liu2022selfconditioned,gal2021stylegannada}.
The inversion technique \cite{roich2021pivotal,dinh2021hyperinverter,tov2021designing,richardson2021encoding} has been used to obtain the latent features of the input image, perform editing in the latent space, and revert the output to the image space.
More recently, with the support of CLIP \cite{clip}, a bridge between images and texts, image editing can now be guided by text prompts \cite{styleclip,gal2021stylegannada}.
Recent models for text-conditioned image editing have leveraged CLIP embedding guidance and text-to-image diffusion models to achieve state-of-the-art results for a variety of edits.
There are three main directions for research in this area: (1) zero-shot (exploiting the CLIP embedding directions, stochastic differential equations, or attention control) \cite{hertz2022prompttoprompt,zeropix2pix,sdedit,nulltext,DiffusionCLIP}; (2) optimizing text prompts and/or diffusion models \cite{imagic,ruiz2022dreambooth,textualinversion,huang2023reversion}; and (3) fine-tuning diffusion models on a supervised dataset \cite{instructpix2pix,controlnet}. In contrast to prior works that rely on text prompts to guide the editing, we aim to leverage visual prompts to better assist the process.

\textbf{Prompt Tuning.} Diffusion models have shown stirring results in text-to-image generation, but they can struggle to comprehend specific or novel concepts.
Several works have focused on addressing this issue.
Textual Inversion \cite{textualinversion} learns a specialized token for new objects, which can later be plugged in with natural language to generate novel scenes.
ReVersion \cite{huang2023reversion} learns a specified text prompt for relation properties between two sets of images.
Although a continuous prompt can be more task-specific, a discrete prompt is typically easier to manipulate by users. PEZ \cite{hard_prompt_made_easy} proposes a method to discover prompts that can retrieve similar concepts of given input images.
Instead of learning novel concepts for image generation, our work focuses on learning the \emph{transformation} between an example pair of images that is better suited for image editing.

%an example pair and a query image to generate the answer
\paragraph{Visual Prompting.}
Since proposed in NLP \cite{brown2020language}, prompting has been adapted by computer vision researchers.
Unlike traditional methods that require separate models for each downstream task, visual prompting utilizes in-context learning to solve different tasks during inference.
The first application of visual prompts was proposed by \cite{visprompt}, where an example and query image are combined to form a grid-image.
The task solver fills in the missing portion, which contains the answer. They showed that the task solvers can perform effectively on several tasks with only training on a dataset of Computer Vision figures.
Later, \cite{painter} and \cite{SegGPT} expanded the framework to increase the number of tasks that can be solved.
Recently, Prompt Diffusion \cite{promptdiffusion} introduced a diffusion-based foundation for in-context learning.
Although it shows high-quality in-context generation, a text prompt is still needed.
Similar to textual prompts, not all visual prompts perform equally well. There are ongoing efforts to understand how to design a good example pair \cite{zhang2023VisualPromptRetrieval}.
Despite the success of visual prompting in solving a wide range of standard computer vision tasks, the question of whether one can use visual prompting for image editing remains unanswered.

\section{Framework}

% % Figure environment removed
% \thao{Why am I writing like this? Because I think our optimization strategy can work with any text-to-image model, not limited to InstructPix2Pix. (Ex: ControlNet)}
% In this section, we present our approach for enabling image editing through visual prompting.
In this section, we present our approach for enabling image editing via visual prompting.
First, we provide a brief background on text-conditioned image editing diffusion models (Section \ref{sec:prelim}).
Section \ref{sec:mse_loss} and \ref{sec:clip_loss} describe how to invert visual prompts into text-based instructions. Finally, our full Visual Instruction Inversion algorithm is given in Section \ref{sec:visual_prompting_for_img_editing}.
%%describe constraints to inverse visual prompting to text-based instructions.

Let $\{x, y\}$ denote the before-and-after example of an edit. 
Our goal is to learn a text-based edit $c_{T}$ that captures the editing direction from $x$ to $y$.
Once learned, $c_{T}$ can be applied to any new input image $x'$, to obtain an edited image $y'$ that undergoes a similar transformation: $x \rightarrow y \approx x' \rightarrow y'$.
To avoid confusion, we use only one image pair example to describe our approach.
However, it is worth noting that our algorithm still holds for an arbitrary number of example pairs.

\subsection{Preliminaries}
\label{sec:prelim}
Diffusion models for image generation are trained on a sequence of gradually noisier image $x$ over a series of timesteps $t=1, \dots, T$.
The goal is to learn a denoising autoencoder $\epsilon_{\theta}$, which predicts a denoised variant of a noisy version of $x$ at each timestep $t$, commonly denoted as $x_{t}$ \cite{ho2020denoising}.
Initially, this approach was implemented in pixel-space \cite{diffusionbeatsgan}, but it has now been extended to the latent space for faster inference and improved quality \cite{latentdiffusion}.
Here, prior to the diffusion process, image $x$ is encoded by an encoder $\mathcal{E}$ to obtain the latent image $z_{x}$, which is subsequently decoded by a decoder $\mathcal{D}$ to convert the latent image back to the image space.
The objective function is defined as follows:
\begin{equation*}
    \mathcal{L} = \mathbb{E}_{{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1)}, t} \lVert \epsilon - \epsilon_{\theta}(z_{x_{t}}, t)\rVert_{2}
\end{equation*}
% The original diffusion model is incapable of taking other types of conditions, such as text prompts.a
To enable diffusion models to take text prompts as conditional inputs, \cite{latentdiffusion} introduced a domain-specific encoder $\tau_{\theta}$ that projects text prompts to an intermediate representation $c_{T}$.
This representation can then be inserted into the layers of the denoising network via cross-attention:
%layer of UNet -> layer of denoising network? or convolution layer of denoising network??
\begin{equation*}
    \mathcal{L} = \mathbb{E}_{{\mathcal{E}(x), c_{T}, \epsilon \sim \mathcal{N}(0,1)}, t} \lVert \epsilon - \epsilon_{\theta}(z_{x_{t}}, t, c_{T})\rVert_{2}
\end{equation*}
Conditioned on a text description $c_{T}$, diffusion models can synthesis stunning images.
% However, text-conditioned image synthesis has limitations when it comes to image editing.
However, they are still not completely well-suited for image editing. 
Suppose that we want to edit image $x$ to image $y$, conditioned on text prompt $c_{T}$.
% Then, $c_{T}$ should be a full text description of both visual aspects of input images and editing direction.
Text prompt $c_{T}$ then needs to align with our desired edit $x\rightarrow y$ and fully capture the visual aspects of $x$, which can be difficult. 
There are methods to help discover text prompts that can retrieve similar content \cite{hard_prompt_made_easy,mokady2021clipcap}, however, they typically cannot accurately describe all aspects of the input image $x$. % is still far from recover the input image $x$.
% As expected, we often end up with a variant of text-description and semantic-alike outputs to the input image.
To address this challenge, the idea of adding the input image to the denoising network was proposed in \cite{instructpix2pix,saharia2022palette}.
The input image $x$ can then be encoded as $c_{I} = \mathcal{E}(x)$ and concatenated to the latent image $z_{y_{t}}$, jointly guiding the editing process with text prompt $c_{T}$.
% more accurately.
% Specially for image editing, \cite{instructpix2pix} fine-tunes diffusion models in a supervised way to perform image editing.
Based on this idea, InstructPix2Pix \cite{instructpix2pix} fine-tunes the text-to-image diffusion model in a supervised way to perform image editing.
Its objective function is changed accordingly, as it now learns to denoise a noisy version of $y$, which is an edited image of $x$ based on editing direction $c_{T}$:
\begin{equation}
\label{eq:ip2p_loss}
    \mathcal{L} = \mathbb{E}_{{\mathcal{E}(y), c_{T}, c_{I}, \epsilon \sim \mathcal{N}(0,1)}, t} \lVert \epsilon - \epsilon_{\theta}(z_{y_{t}}, t, c_{T}, c_{I})\rVert_{2}
\end{equation}


% Figure environment removed


\subsection{Learning to Reconstruct Images}
\label{sec:mse_loss}

% \thao{Rewrite this paragraph. Wanna say others did not learn in image-editing domain??}
% The idea of learning a specialized token or prompt for a novel objects or concepts has been introduced in \cite{textualinversion,ruiz2022dreambooth}.
% The idea of textual inversion for novel objects or concepts has been introduced in \cite{textualinversion,ruiz2022dreambooth,huang2023reversion}.
% The idea of textual inversion has been introduced in \cite{textualinversion,ruiz2022dreambooth,huang2023reversion}.
% However, p
%learn to reconstruct given images in a variety of language contexts, which is more suitable for image synthesis as it offers greater flexibility in terms of changes and does not require faithfully following the input images afterward
Prior textual inversion methods \cite{textualinversion,ruiz2022dreambooth,huang2023reversion} all utilize an image reconstruction loss.
However, their aim is to learn to capture the essence of the concept in the image so that it can be synthesized in new contexts, but not to faithfully follow pixel-level details of the input image which are required for image editing.
% Previous work on prompt optimization \cite{imagic} for image editing need to fine-tune both text prompt and diffusion model again for each conditional image.
The closest idea to ours is \cite{imagic}, but it needs to fine-tune the diffusion model again for each edit and input image.
% We overcome these challenges by exploring the pretrained image-editing models, in which, is more suitable for image editing.
We instead exploit a pre-trained text-conditioned image editing model, which offers editing capabilities, while avoiding additional fine-tuning. 

% Given only two image $\{x, y\}$ represents``before'' and ``after'' images of an edit $c_{T}$,
% the first and foremost constraint is to faithfully recover image $y$. 
Given only two images $\{x, y\}$ which represent the ``before'' and ``after'' images of an edit $c_{T}$, the first and foremost objective is to recover image $y$. 
We follow the same strategy as \cite{instructpix2pix}, where we optimize the instruction $c_{T}$ based on the supervised pair $\{x, y\}$.
% In our case, conditional image is ``before'' image, so $c_{I}$ is encoded image of $x$, target image is ``after'' image $y$.
In our case, conditional image $c_{I}$ is the ``before'' image $x$, and target image is the ``after'' image $y$.
The objective function is then adopted from Eq.~\ref{eq:ip2p_loss} as:
\begin{equation}
\label{eq:loss_mse}
    \mathcal{L}_{mse} = \mathbb{E}_{{\mathcal{E}(y), c_{T},z_{x}, \epsilon \sim \mathcal{N}(0,1)}, t} \lVert \epsilon - \epsilon_{\theta}(z_{y_{t}}, t, c_{T}, z_{x})\rVert_{2}
\end{equation}
% It is worth to note that in InstructPix2Pix was trained on more than 31k examples, while we only have one example.

% Figure environment removed
\subsection{Learning to Perform Image Editing}
\label{sec:clip_loss}

% Given only one example, it is hard to learn a robust instruction that is not a description of the edited image $y$.
If we rely only on the image reconstruction constraint (Eq. \ref{eq:loss_mse}), we may learn a description of the edited image $y$, instead of the desired editing instruction.
\cite{zeropix2pix} has shown that the CLIP embedding \cite{clip} is a good indicator of the editing direction. It uses GPT-3 \cite{brown2020language} to generate a set of sentences for the ``before'' and ``after'' domains of an edit; for example, cat $\leftrightarrow$ dog.
The mean difference between the CLIP embeddings of these sentences represents the text editing direction ``before'' $\leftrightarrow$ ``after''.

%highlights the edit
In our case, we can use the difference between the CLIP embeddings of the ``after'' and ``before'' images to help learn the edit. Specifically, for an example pair $\{x, y\}$, we compute the image editing direction $\Delta_{x \rightarrow y}$ as:
\begin{equation*}
    \Delta_{x \rightarrow y} = \mathcal{E_{\text{clip}}}(y) - \mathcal{E_{\text{clip}}}(x)
\end{equation*}
We encourage the learned instruction $c_{T}$ to be aligned with this editing direction (Figure \ref{fig:framework}b). To this end, we minimize the cosine distance between them in the CLIP embedding space:
% or, minimize the loss function
% se this direction to further constraint the learned instruction $y$, as the cosine similarity between this direction and learned instruction should be similar.
\begin{equation}
\label{eq:loss_clip}
    \mathcal{L}_{clip} = \text{cosine}(\Delta_{x \rightarrow y}, c_{T})
\end{equation}

% \paragraph{Optimization Algorithm} \thao{Should it be another subsection?}
\subsection{Image Editing via Visual Prompting}
\label{sec:visual_prompting_for_img_editing}
% Now with an example pair $\{x, y\}$, we form the visual prompting as an optimizing for editing instruction $c_T$, with two constraints: Image Reconstruction loss Eq. \ref{eq:loss_mse} and CLIP loss\ref{eq:loss_clip}.
Finally, given an example before-and-after image pair $\{x, y\}$, we formulate the visual prompting as an instruction optimization using our two constraints: Image reconstruction loss (Eq. \ref{eq:loss_mse}) and CLIP loss (Eq. \ref{eq:loss_clip}).
% Our objective is to learn an implicit text-based editing function $c_{T}$ that captures the editing process from $x$ to $y$.
We provide an illustration of our framework in training and testing in Figure \ref{fig:framework}a,c, and pseudocode in Algorithm \ref{algo:optimization}.  Our algorithm also holds for $n$ example pairs $\{(x_{1}, y_{1}), \dots (x_{n}, y_{n})\}$.
In this case, $\Delta_{x\rightarrow y}$ becomes the mean difference of all examples, and at each optimization step, we randomly sample one pair $\{x_{i}, y_{i}\}$.


\begin{algorithm}[t]
    \caption{\textbf{Vis}ual \textbf{I}nstruction \textbf{I}nversion (\textbf{VISII})}    
    \label{algo:hlin}
    \begin{algorithmic}[1]
    \State \textbf{Input}: An example pair $\{x, y\}$
    \State \quad Pretrained denoising model $\epsilon_{\theta}$; Image encoder $\mathcal{E}$; CLIP encoder $\mathcal{E}_{clip}$
    \State \quad Number of optimization steps $N$; Number of timesteps $T$
    % \State Init $\textit{instruction}$
    \State \quad Hyperparameters $\lambda_{clip}$, $\lambda_{mse}$; Learning rate $\gamma$
    % \State 
    % \Comment{\textcolor{gray}{Encode image}}
    % \State \quad Initialize instruction $\tau(\textit{instruction}) = c_{T}; \quad c_{y\rightarrow x}= \tau(y) - \tau(x)$ \Comment{\textcolor{gray}{Encode instruction}}
    \State \quad \textcolor{gray}{// Start optimization}
    \State \quad Initialize $c_{T}$ \Comment{\textcolor{gray}{Initialize instruction}}
    \State \quad Encode $z_{x} = \mathcal{E}(x); \quad z_{y} = \mathcal{E}(y)$ \Comment{\textcolor{gray}{ Encode image}}
    \State \quad Compute $\Delta_{x \rightarrow y} = \mathcal{E_{\text{clip}}}(y) - \mathcal{E_{\text{clip}}}(x)$ \Comment{\textcolor{gray}{Compute editing direction}}
    \For{$i=1,\cdots,N$}
        \State Sample $t \sim \mathcal{U}(0, T)$; $\epsilon \sim \mathcal{N}(0,1)$ \Comment{\textcolor{gray}{Sample timestep and noise}}
        \State $z_{y_t} \leftarrow$ add $\epsilon$ to $z_{y}$ at timestep $t$ \Comment{\textcolor{gray}{Prepare noisy version of $z_{y}$ at timestep $t$}}
        \State $\hat{\epsilon} = \epsilon_{\theta}(z_{y_t}, t, c_{T}, z_{x})$ \Comment{\textcolor{gray}{Predict noise condition on $x$}}
        \State $\mathcal{L} = \lambda_{\textit{mse}}\lVert \epsilon - \hat{\epsilon}\rVert_{2} + \lambda_{clip}(\text{cosine}(c_{T}, \Delta_{x\rightarrow y}))$ \Comment{\textcolor{gray}{Compute losses}}%+ \lambda_{self}\lVert \epsilon - \tilde{\epsilon}\rVert_{2}$
        %\State $g = \nabla\mathcal{L}$
        % \State $g = \nabla\mathcal{L}()$
        %\State Update $c_{T} = c_{T} - \gamma g$
        \State Update $c_{T} = c_{T} - \gamma \nabla\mathcal{L}$
    \EndFor
    \State \textbf{Output}: $c_{T}$
    \end{algorithmic}
    \label{algo:optimization}
\end{algorithm}



 Once $c_{T}$ is learned, we can apply it to a new image $x_{test}$ to edit it into $y_{test}$.
 Moreover, our designed approach allows users to input extra information, enabling them to combine the learned instruction $c_{T}$ with an additional text prompt (Figure \ref{fig:framework}c).
To that end, we optimize a fixed number of tokens of $c_{T}$ only, which provides us with the flexibility to concatenate additional information to the learned instruction during inference (Figure~\ref{fig:implementation}b).
This allows us to achieve more fine-grained control over the resulting images, and is the final default approach.


\section{Evaluation}

We compare our approach against both image-editing and visual prompting frameworks, on both synthetic and real images.
In Section \ref{sec:qualitative}, we present qualitative results, followed by a quantitative comparison in Section \ref{sec:quantitative}.
Both quantitative and qualitative results demonstrate that our approach not only achieves competitive performance to state-of-the-art models, but also has additional merits in specific cases. Additional qualitative results can be found in the Appendix.

% Figure environment removed

\subsection{Experimental Settings}

\textbf{Training Setting.} We use the frozen pretrained InstructPix2Pix \cite{instructpix2pix} to optimize the instruction $c_{T}$ for $N=1000$ steps, $T=1000$ timesteps.
We use AdamW optimizer \cite{loshchilov2019decoupled} with learning rate $\gamma = 0.001$, $\lambda_{mse}=4$, and $\lambda_{clip}=0.1$.
Text guidance and image guidance scores are set at their default value of $7.5$ and $1.5$, respectively.
% The hyperparameters are set as $\lambda_{mse}=4$ and $\lambda_{clip}=0.1$.
All experiments are conducted on a 4 $\times$ NVIDIA RTX 3090 machine.

\textbf{Dataset.}
We randomly sampled images from the Clean-InstructPix2Pix dataset \cite{instructpix2pix}, which consists of synthetic paired before-after images with corresponding descriptions.
In addition, we download paired photos from \cite{pix2pix2017} to test the models.
Since some real images do not have edited versions, we utilize \cite{controlnet} with manual text prompts to generate the after images with different edits.

\textbf{Evaluation Metrics.}
Following \cite{instructpix2pix}, we assess the effectiveness of our approach using the Directional CLIP similarity \cite{gal2021stylegannada} and Image CLIP similarity metrics.
However, as the CLIP directional metric does not reflect the transformation similarity between the before-after example and before-after output pair, we propose an additional metric called the Visual CLIP similarity.
Specifically, we compute the cosine similarity between the before-after example pair and the before-after test pair as follows:
$s_{visual} = 1 - \text{cosine}(\Delta_{x \rightarrow y}, \Delta_{x' \rightarrow y'})$.
% This metric allows us to measure transformation similarity between ``before''-``after'' example pairs and the ``before''-``after'' test pairs.

\textbf{Baseline Models.}
We compare our approach to two main categories of baselines: Image Editing and Visual Prompting.
For image editing, we compare against InstructPix2Pix \cite{instructpix2pix} and SDEdit \cite{sdedit}, which are the state-of-the-art.
We directly use the ground-truth editing instruction for InstructPix2Pix and after descriptions for SDEdit.
For real images, we manually write instructions and descriptions for them, respectively.
For visual prompting, we compare our approach against Visual Prompting \cite{visprompt}.
% We do not compare to Painter \cite{painter} as it is not trained on image in-painting/style transfer datasets.
The Visual Prompting code is from the author's official repository, while SDEdit and InstructPix2Pix codes are from HuggingFace.

%Painter wasn't trained on any image-inpaiting or style-transfer. Painter was trained ONLY on MS COCO, NYU-v2, ADE-20k... So it shouldn't be expected that Painter can perform kind of style-transfer/ inpainting task.

\subsection{Qualitative Results}
\label{sec:qualitative}
% Figure environment removed
Figure \ref{fig:qualitative} presents qualitative comparisons.
% The first three rows are synthesis images (from Clean-InstructPix2Pix dataset \cite{instructpix2pix}), while the last row contains real ones.
As can be seen, Visual Prompting \cite{visprompt} fails to perform the image editing task.
Text-conditioned image editing frameworks, InstructPix2Pix \cite{instructpix2pix} and SDEdit \cite{sdedit}, can edit images based on the provided text prompts, but can fall short in producing edited images that are visually close to the ``after'' example.
In contrast, our approach can learn the edit from the given example pair and apply it to new test images.
For example, in wolf $\leftrightarrow$ dog (Fig. \ref{fig:qualitative}, row 3), we not only achieve successful domain translation from wolf to dog, but we also preserve the color of the dog's coat. Please refer to the Appendix for more qualitative results.

Figure \ref{fig:ours_qualitative} demonstrates the advantage of visual prompting compared to text-based instruction. %, we present additional qualitative results in 
% There are similar tasks in terms of changing art styles, such as "turning it into a drawing," our method can learn and replicate the unique characteristics of various art styles."
We can see that one text instruction can be unclear to describe specific edits.
For example, the instruction ``Turn it into a drawing'' or ``Make it a painting'' can have multiple interpretations in terms of style and genres.
In these cases, by showing an example pair, our method can learn and replicate the distinctive characteristics of each specific art style.

\subsection{Quantitative Results}
\label{sec:quantitative}
% Figure environment removed

We perform quantitative evaluation of our method against two baselines: InstructPix2Pix \cite{instructpix2pix} and SDEdit \cite{sdedit}.
Since Visual Prompting was not effective in performing image editing tasks, we did not include it in our comparison.
We randomly sampled 300 editing directions, resulting in a total of 1030 image pairs, from the Clean-InstructPix2Pix dataset.
% It is worth noting that our method learns from only one example pair, while the baseline models are given clear text prompts/ instructions.

We analyze the histograms of Image, Directional, and Visual CLIP Similarity (Figure \ref{fig:quantitative}). 
Results indicate that our method performs competitively to the baselines.
In terms of Directional CLIP Similarity, InstructPix2Pix achieves the highest score, as it can make large changes the input image toward the editing instruction. %aggressively manipulate
Our method scores similarly to SDEdit, indicating that our approach can also perform well in learning the editing direction.
% In terms of Image CLIP Similarity, our approach was the most faithful to the input images.
Our approach is the most faithful to the input image, as reflected by the highest Image CLIP Similarity scores.
Finally, for Visual CLIP Similarity, which measures the agreement between the changes in before-after example and before-after test images, our approach performs nearly identically to the two state-of-the-art models.
% In terms of Image CLIP Similarity, our approach was the most faithful to the input images, whereas in terms of Visual CLIP Similarity, which measures the agreement between the changes in before-after training pairs and before-after test images, our approach perform similarly to the two state-of-the-art baseline models. It is worth noting that we only use one example, whereas the baseline models are given clear text prompts/instructions.

\section{Analysis}

\begin{table}[t]
\centering
\caption{\textbf{Quantitative Analysis.} We report Image, Directional, and Visual CLIP Similarity scores. Despite learning from only one example pair, our approach performs competitively to state-of-the-art image editing models. (``Direct.'':  ``Directional''; \#: number of training pairs; ``Init'': Initialization of instruction; ``GT'': Ground-truth instruction; ``Cap.'': Image captioning of ``after'' image.)}
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}[t]{lccccccccccr}
\toprule
 & \multicolumn{2}{c}{Losses} & \multicolumn{2}{c}{Init.} &\multicolumn{3}{c}{Random noise} & \multicolumn{3}{c}{Fixed noise}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\# & MSE & CLIP & GT & Cap. & Img  $\uparrow$& Direct.  $\uparrow$& Visual  $\uparrow$&  Img  $\uparrow$& Direct.  $\uparrow$& Visual $\uparrow$\\
\midrule
 % &\multicolumn{2}{c}{\textit{ground-truth}}& & & 0.856 & \textbf{0.316} & \textbf{0.585} & - & - & - \\
 &\multicolumn{2}{c}{\textit{ground-truth}}& & & 0.824 & \textbf{0.196} & \textbf{0.301} & - & - & - \\
 &\multicolumn{2}{c}{\textit{no training}}& & \ding{51}&  \textbf{0.866} & 0.090 & 0.199 & - & - & -\\
\midrule
1 & \ding{51} &           & \ding{51} && 0.841 & 0.120 & 0.247 & \cellcolor{second}0.854 & \cellcolor{second}0.105& \cellcolor{third}0.223 \\
1 & \ding{51} &           & & \ding{51}& 0.845 & 0.115 & 0.254 & \cellcolor{first}0.861 &\cellcolor{first}0.110  & \cellcolor{second}0.225 \\
1 & \ding{51} & \ding{51} & \ding{51} && 0.838 & 0.131 & 0.231 & \cellcolor{third}0.852 & \cellcolor{third}0.102 & \cellcolor{first}0.236 \\
1 & \ding{51} & \ding{51} & & \ding{51} & 0.823 & 0.126 & 0.299 & \cellcolor{first}0.847 & \cellcolor{forth} 0.113 &  \cellcolor{forth} 0.251\\
\midrule

1 & \ding{51} & \ding{51} & & \ding{51} & 0.823 & 0.126 & 0.299 & \cellcolor{first}0.847 & \cellcolor{forth} 0.113 &  \cellcolor{forth} 0.251\\
% 1 & \ding{51} & \ding{51} & & \ding{51} & 0.--- & 0.--- & 0.--- & 0.844 & 0.113 & 0.251 \\
2 & \ding{51} & \ding{51} & & \ding{51} & 0.791 & 0.141 & 0.292 & \cellcolor{second}0.826 & \cellcolor{third} 0.117 & \cellcolor{third}0.253 \\
3 & \ding{51} & \ding{51} & & \ding{51} & 0.780 & 0.148 & 0.283 & \cellcolor{forth}0.805 & \cellcolor{second} 0.132 & \cellcolor{second}0.256 \\
4 & \ding{51} & \ding{51} & & \ding{51} & 0.798 & 0.148 & 0.280 & \cellcolor{third}0.812 & \cellcolor{first} 0.133 & \cellcolor{first}0.260 \\
\bottomrule
\end{tabular}}
% \vspace{-0.1cm}
\label{tab:quantitative}
\end{table}

We next conduct an in-depth study to better understand our method. For all of the studies below, we sample 100 editing directions (resulting in 400 before-and-after pairs in total) from the Clean-InstructPix2Pix \cite{instructpix2pix} dataset.
We show that both the CLIP loss and instruction initialization are critical for achieving optimal performance.
Additionally, we present some interesting findings regarding the effects of random noise, which can lead to variations in the output images.

\textbf{Losses.} 
%While it is common to use the MSE loss to optimize text prompts based on diffusion models, we intuitively wanted to incorporate CLIP loss to further boost performance in our case.%in the quantitative results are shown 
We ablate the effect of each loss in Table \ref{tab:quantitative}.
The additional CLIP loss helps improve scores in Visual and Directional CLIP Similarity \cite{gal2021stylegannada}, which reflect the editing directions. This shows that the CLIP loss encourages the learned instruction to be aligned with the target edit.

\textbf{Initialization.} Prior work utilizes a coarse user text prompt (e.g., ``sculpture'' or ``a sitting dog'') for textual initialization \cite{textualinversion,imagic}, which can be practical, but may not always be effective. The reason is that natural text prompts can be misaligned with the model's preferred prompts \cite{promptist}.
We can also optimize upon a user's coarse input, however, we find that it is more effective to initialize the instruction vector $c_{T}$ to be somewhere close to the editing target; i.e., a caption of the ``after'' image.
We evaluate both initialization strategies, including user input and captioning.
To mimic a user's coarse input, we directly use ground-truth editing instructions from Clean-InstructPix2Pix dataset \cite{instructpix2pix}.
We employ \cite{hard_prompt_made_easy} to generate captions for ``after'' images.
Results are shown in Table \ref{tab:quantitative}.
As expected, directly using the caption as the instruction to InstructPix2Pix will not yield good results (Row 2), but initializing our model's learned instruction from the caption helps to improve the Visual and Image CLIP Similarity scores. This indicates that the learned instruction is more faithful to the input test image that we want to edit, while still retaining editing capabilities.
% The reason why initilization from grounth-truth is not better: (1) CLIP similarity score is noisy: "put a black hat on the cat", and "put a hat con the cat", have similar CLIP score. But the second one will punish the MSE loss, so ours must search for "black", which might distored the outputs... (2) InstructPix2Pix is very sensitive to prompt.
% (3) The dataset is noisy itself. Many example pairs are not clear.

% \end{figure}

\textbf{Noises.}
% Figure environment removed
Text-conditioned models generate multiple variations of output images depending on the sampled noise sequence. This is true for our approach too.  However, for image editing, we would prefer outputs that best reflect the edit provided in the before-and-after image pair, and preserve the test image as much as possible apart from that edit.
%as long as they are highly correlated with the instructions, visual prompting need reasonable faithfully constraint to the input images.
% We do not want to end up with a completely different outputs which loses the visual content of input images.
%This means outputs can forget the visual content of conditional images.
We find that using identical noises from training in test time can help achieve this.
Specifically, denote the noises sampled during the training optimization timesteps $t = 1 \dots T$, as $\{\epsilon_{1}, \dots \epsilon_{T}\}$, which are added to the latent image $z_{y}$.
We reuse the corresponding noises in the backward process during test time to denoise the output images.
This technique helps to retain the input image content, as shown in Table \ref{tab:quantitative}. However, there is a trade-off between aggressively moving toward the edit, and retaining the conditional input.
It is worth noting that in test time, we can also use random noises for denoising if desired.
% But ours observation is that it will not faithfully similar to input image.
We visualize this phenomenon in Figure \ref{fig:noises}.
% \subsection{Instruction Concatenation}

% Figure environment removed

% In the second example, we further illustrate how concatenating extra information can help guide the learned instruction.
% We initially learned a male character transformation from a cartoon image.
% However, applying this learned instruction to a new female test image results in a male bias in the output.
% By adding extra information, we can navigate the edit to overcome this bias.
% We can also add extra details such as sunflowers or guns to adjust the learned instruction accordingly.
% These modifications ensure consistent outputs that are aligned with the original learned instruction.
% Applying InstructPix2Pix, again, tends to produce more varied outputs that are inconsistent with the user's intention.
\textbf{Hybrid instruction.} Finally, our approach allows users to incorporate additional information into the learned instruction.
Specifically, we create a hybrid instruction by concatenating the learned instruction with the user's text prompt. This hybrid instruction better aligns with the given example pair while still following the user's direction. In Figure \ref{fig:teaser}, we transform ``cat'' $\rightarrow$ ``watercolor cat''.
We demonstrate how concatenating extra information to the learned instruction (\texttt{<ins>}) enables both image editing (changing to a watercolor style) and domain translation (e.g., ``cat'' $\rightarrow$ ``tiger'').
The painting style is consistent with the before-and-after images, while the domain translation corresponds to the additional information provided by the user. Figure \ref{fig:concat} provides more qualitative examples.
Applying InstructPix2Pix~\cite{instructpix2pix} often does not yield satisfactory results, as the painting style differs from the reference image.

% Figure~\ref{fig:concat} shows qualitative results of this approach.
% In the first example, we transform ``photo'' $\rightarrow$ ``impressionist painting''.
% We demonstrate how concatenating extra information to the learned instruction enables both image editing (changing to a impressionist style) and domain translation (e.g., ``rose'' $\rightarrow$ ``tulip'').
% The painting style is consistent with the before-and-after images, while the domain translation corresponds to the additional information provided by the user.
% Applying InstructPix2Pix~\cite{instructpix2pix} often does not yield satisfactory results, as the painting style differs from the reference image.

% Applying InstructPix2Pix~\cite{instructpix2pix} often does not yield satisfactory results, as the painting style differs from the reference image.

\section{Discussion and Conclusion}


We presented a novel framework for image editing via visual prompt inversion. With just one example representing the ``before'' and ``after'' states of an image editing task, our approach achieves competitive results to state-of-the-art text-conditioned image editing models.
However, there are still several limitations and open questions left for future research.



One major limitation is our reliance on a pre-trained model, InstructPix2Pix.
As a result, it restricts our ability to perform editing in the full scope of diffusion models, and we might also inherit unwanted biases.
Additionally, there are cases where our model fails, as shown in Figure \ref{fig:discussion}a, where we fail to learn ``add a dinosaur'' to the input image, presumably because it is very small.
%and inductive bias from them


%\vspace{-0.3cm}
% Figure environment removed
% \thaoidea{ Limitation: to InstructPix2Pix capacity}


% We find out by coincidence that by prepare an foreground segmentation as green area, we can then learn the segmentation instruction and apply it into new image to get corresponding segmentation (Figure \ref{fig:discussion}b).
% However, we left this question for future research.
% While we address the question of effectively using visual prompting with diffusion models, the reverse questions, in which: Can we use diffusion models, as a task solver, for downstream Computer Vision tasks?
% While we address the question of effectively using visual prompting with diffusion models.
% One might ask an interesting question, in a reverse direction: Can we use diffusion models, as a task solver, for downstream Computer Vision tasks?
As we address the question of effectively using visual prompting with diffusion models, one might ask an interesting question in the reverse direction: Can diffusion models be used as a task solver for downstream computer vision tasks?
%Coincidentally, 
We find out that by preparing a foreground segmentation as a green area, we can learn instructions and apply them to new images to obtain corresponding segmentations  (Figure \ref{fig:discussion}b).
However, further research is needed to fully explore this question. We acknowledge that this question is beyond the scope of our current study, which is primarily focused on image editing.
Additionally, visual in-context learning has been shown to be sensitive to prompt selection \cite{zhang2023VisualPromptRetrieval,visprompt}. Figure \ref{fig:discussion}c shows cases where one example may not fit all test images. This shows that there are open questions regarding what makes a good example for image editing.
% \thao{And the question of how to enable text-to-image models, as a task solver for computer vision downstream task}
% \thao{REMEMBER to split the APPENDIX to different .tex file! It is now right after Reference!!}
{
\small
% \bibliographystyle{plain}
\bibliographystyle{splncs04.bst}
\bibliography{references}
}
% \newpage

% \section*{Appendix}

% \subsection{Implementation Details}

% \subsection{Extra analysis}

% \paragraph{Initialization Strategy}
% \paragraph{Initial Instruction Length}
% \paragraph{Training for longer iteration}

% \subsection*{Photo Attribution}
% \begin{itemize}
%     \item Elsa (Human): \url{https://www.reddit.com/r/Frozen/comments/j4afdf/elsa_anna_kristoff_in_real_life/}
%     \item Disney characters: \url{https://princess.disney.com/}
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\appendix

% This document provides additional information complementing the main paper. First, we compare to Textual Inversion in Sec.~\ref{sec:textualinversion}. Then, in Sec.~\ref{sec:additional_qualitative}, we provide additional qualitative comparisons to Imagic~\cite{imagic} and Null-text Inversion~\cite{nulltext}. Finally, in Sec.~\ref{sec:impdetails}, we provide the implementation details of our method, along with results obtained using a variant of our approach - instruction concatenation. This variant allows users to add extra information into the learned instruction.
%or objects to do
%a novel concepts for image synthesis.
\section*{Appendix}
\section{Textual Inversion vs. Visual Instruction Inversion}\label{sec:textualinversion}
% The key difference between ours and textual inversion \cite{ruiz2022dreambooth,textualinversion} is that we are not focusing on learning the objects/concepts. Instead, our aim is to learn the image transformation \cite{textualinversion}.
% Given a set (3-5 images) of an objects, Textual Inversion \cite{textualinversion} is designed to learn a novel token representation for a given set of objects.
% This learned token can subsequently be employed in novel sentences to synthesize novel scenes. For instance, an example sentence such as ``Painting of \texttt{<cat-toy>} in the style of Monet'' (Figure \ref{fig:textual_inversion}) showcases the capability of Textual Inversion to generate novel scenes based on acquired concepts. It can be seen Textual Inversion tends to disregard the pixel-level details present in the input images.
% In this cases, our learned instruction specified for the edit can be applied to test image to get corresponding editing.

Textual Inversion~\cite{textualinversion,ruiz2022dreambooth} is a method to invert a visual concept into a corresponding representation in the language space.
In particular, given (i) a text-to-image pre-trained model and (ii) some images describing a visual concept (e.g., a particular kind of toy; Figure~\ref{fig:textual_inversion} bottom row), Textual Inversion learns new ``words'' in the embedding space of the text-to-image model to represent those visual concepts.
Once these ``words'' are learned for that concept, they can be plugged into arbitrary textual descriptions, just like other English words, which can then be used to create the target visual concept in different contexts.
Instead of learning the representation for an isolated visual concept, our approach (Visual Instruction Inversion), learns the \emph{transformation} from a before-and-after image pair. This learned transformation is then applied to a test image to achieve similar edit ``before'' $\rightarrow$ ``after''. 
% Figure environment removed
\paragraph{Applicability of Textual Inversion for image editing.} Given these differences with our proposed method, we now try to see if Textual Inversion can be used for image editing. 
%Textual Inversion \cite{textualinversion,ruiz2022dreambooth} learns new ``words'' in the text encoder's embedding space from a set of example images describing a unique visual concept (e.g., a particular toy; Fig.~\ref{fig:textual_inversion} bottom row), which can be used to generate that concept in new contexts. 
%On the other hand, our Visual Instruction Inversion approach learns the edit from a before-and-after image pair, which can be applied to any new test image. Textual Inversion tends to ignore the pixel-level details of the example images, while visual instruction inversion preserves them.
%describe novel concepts or fine-grained details in new sentences
Textual Inversion can generate a ``painting of a \texttt{<cat-toy>} in the style of Monet'' by using the learned word \texttt{<cat-toy>} from example images (Figure~\ref{fig:textual_inversion}a, Row 1).
However, the synthesized images often only capture the essence of the objects, and disregard the details of the input images.
As a result, textual inversion is suitable for novel scene composition, but is not effective for image editing.

On the other hand, our Visual Instruction Inversion does not learn novel token representations for objects or concepts.
Instead, we learn the edit instruction from before-and-after pairs, which can be applied to any test image to obtain corresponding edits.
This allows us to achieve fine-grained control over the resulting images.
% without requiring a large number of example images for each concept.
For example, by providing a photo of \texttt{<cat-toy>}, one before and one in a specific impressionist style, we learn the transformation from before to impressionist, denoted as \texttt{<ins>}.
Once learned, this instruction can be applied to new \texttt{<cat-toy>} images to achieve the same impressionist painting style, without losing the fine details of the test image (Figure~\ref{fig:textual_inversion}b, Row 1).


% One may argue for an alternative approach to image editing using Textual Inversion, which involves learning two tokens: one for the object and another for the style. (e.g., \texttt{<cat-toy>} and <watercolor-painting>). Results are depicted in Figure~\ref{fig:textual_inversion} (Row 2).
% Although Textual Inversion demonstrates the ability to learn and combine these concepts, it falls short in performing image editing.
% It often introduces significant changes that deviate from the original input image.
% In these cases, by given before-and-after pair, we can learn the art style and apply it into test image without learning the objects.
One might suggest an alternative approach to image editing using Textual Inversion, which involves learning two tokens: one for the object and another for the style (e.g., ``Painting of \texttt{<cat-toy>} in the style of \texttt{<watercolor-portraits>''}).
Figure~\ref{fig:textual_inversion}a (Row 2) shows the results of this approach.
As can be seen, Textual Inversion still often introduces significant changes that deviate from the original input image.
Thus, Textual Inversion is not suitable for accurate image editing.
% In contrast, by using before-and-after pairs, we can learn the art style and apply it to test images without learning the objects.


% Figure environment removed


\section{Additional Qualitative Comparisons}\label{sec:additional_qualitative}

% In the attached HTML file (\texttt{index.html}), we provide additional comprehensive qualitative comparisons for our method versus InstructPix2Pix~\cite{instructpix2pix} and SDEdit~\cite{sdedit}.

We present qualitative comparisons with other state-of-the-art text-conditioned image editing methods, Imagic~\cite{imagic} and Null-text Inversion~\cite{nulltext} (Figure~\ref{fig:imagic}).
These methods can generate outputs based on given text prompts, such as ``A watercolor painting of a cat'' (Row 3).
However, the outputs often do not match the given reference.
The text prompts can also be ambiguous and result in unsatisfactory outputs, as illustrated by the case of A character in a Pixar movie (Row 1).

Another challenge is the inconsistency of text-conditioned models, where the same text prompt can produce different outputs for different test images.
For example, the text prompt ``A frozen waterfall'' (Row 6) generates different water colors (blue vs. white) when applied to different test images (Before-and-after pair is from~\cite{nulltext}).
Our method is more consistent in this case, as the learned instruction might have learned the water color.
% This variability introduces an additional layer of complexity when using text-conditioned models for image editing tasks.


\section{Implementation Details}\label{sec:impdetails}

% \subsection{Optimization settings}
We use the pretrained \texttt{clip-vit-large-patch14} as the CLIP Encoder in our approach.
For instruction initialization~\cite{hard_prompt_made_easy}, we set the caption length for after image to 10 tokens. However, this specific caption length does not affect the optimization algorithm. We can optimize initialization instructions of varying lengths (up to 77 tokens). It takes roughly 7 minutes to optimize for one edit, and 4 seconds to apply the learned instruction to new images.

Specifically, during the optimization process, we freeze the tokens representing the start of text (\texttt{<|startoftext|>}), end of text (\texttt{<|endoftext|>}), and all padding tokens after end of text (\texttt{<|endoftext|>}).
We only update the tokens inside the text prompt, called \texttt{<ins>} (between \texttt{<|startoftext|>} and \texttt{<|endoftext|>}) (Figure~\ref{fig:implementation}a).
% By keeping these tokens fixed, we ensure that they remain unchanged throughout the optimization procedure.

\section*{Photo Attribution}
\begin{itemize}
    \item Elsa (Human): \href{https://www.reddit.com/r/Frozen/comments/j4afdf/elsa_anna_kristoff_in_real_life/}{reddit.com/r/Frozen}
    \item Disney characters: \href{https://princess.disney.com/}{princess.disney.com}
    \item Toy Story characters: \href{https://toystory.disney.com/}{toystory.disney.com}
    \item Toonify faces: \href{https://toonify.photos/}{toonify.photos}
    \item Girl with a Pearl Earring: \href{https://en.wikipedia.org/wiki/Girl_with_a_Pearl_Earring}{wikipedia/girl-with-a-pearl-earring}
    \item Mona Lisa: \href{https://en.wikipedia.org/wiki/Mona_Lisa}{wikipedia/mona-lisa}
    \item The Princesse de Broglie: \href{https://en.wikipedia.org/wiki/The_Princesse_de_Broglie}{wikipedia/Princesse-de-Broglie}
    \item Self-portrait in a Straw Hat: \href{https://en.wikipedia.org/wiki/%C3%89lisabeth_Vig%C3%A9e_Le_Brun}{wikipedia/self-portrait-in-a-straw-hat}
    \item Bo the Shiba and Mam the Cat: \href{https://www.instagram.com/avoshibe/}{instagram/avoshibe}
    \item \texttt{<cat-toy>} and \texttt{<watercolor-portraits>} concept: \href{https://huggingface.co/sd-concepts-library}{huggingface.co/sd-concepts-library}
    \item Gnochi cat, waterfall, and cake images are from Imagic~\cite{imagic} and Null-text Inversion~\cite{nulltext}.
\end{itemize}
\end{document}