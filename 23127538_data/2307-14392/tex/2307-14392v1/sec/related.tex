\subsection{Datasets for 3D Scene Understanding}
The RGB-D datasets of indoor scenes dominate the early scene understanding task. ScanNet~\cite{dai2017scannet,armeni20163d} focuses on object surface reconstruction and semantic segmentation, providing dense and rich annotations for various indoor objects. NTU RGB+D~\cite{shahroudy2016ntu} is a human action recognition dataset with corresponding skeleton and action labels. Behave~\cite{bhatnagar2022behave} concentrates on human-object interaction with human SMPL models and interactive objects annotations. It can be found that outdoor scenarios are not well explored.
Recently, the community has paid attention to traffic scenes for autonomous driving and collect several outdoor multi-modal datasets. KITTI~\cite{geiger2012we}, nuScenes~\cite{caesar2020nuscenes} and Waymo~\cite{sun2020scalability} provide 3D bounding boxes for traffic participants and ~\cite{caesar2020nuscenes,behley2019semantickitti} also offer point-wised semantic segmentation labels. However, these datasets are all vehicle-dominated and neglect human-centric scenarios.
STCrowd~\cite{cong2022stcrowd} mainly concentrates on the crowds on campus but lacks the fine-grained segmentation labels and complex human-environment interactions.
In order to facilitate the research of human-centric 3D scene understanding, we collect HuCenLife, a multi-modal dataset with various scenarios in human daily life.
% For scene understanding in the wild, the community pay attention on traffic scenes for autonomous driving. KITTI~\cite{geiger2012we} is a LiDAR-based object detection dataset, providing 3D bounding box for vehicles, pedestrians and other objects. SemanticKITTI~\cite{behley2019semantickitti} adds point-wised semantic segmentation labels on the basis of KITTI. nuScenes~\cite{caesar2020nuscenes} uses 6 cameras and 1 LiDAR to perceive 360-degree scenes. Waymo~\cite{sun2020scalability} is a dataset with various scenes and some auxiliary information of roads.

% For people's daily life, we need a variety of scenarios to understand human-centric events. STCrowd~\cite{cong2022stcrowd} is a 3D Human detection dataset collected in dense crowds. In order to better understand the behavior of people in the scene, we collect HuCenLife, a multi-modality dataset in large scene with various scenarios in daily life. We provide annotations for multiple tasks, such as 3D object detection, semantic segmentation, instance segmentation, action recognition, human object detection and so on, which will make a great contribution the human-centric scene understanding.

% STCrowd~\cite{cong2022stcrowd} is a 3D crowd target detection data set collected in dense crowds using 128-line lidar. In order to understand the behavior of people in the scene more deeply, we collected HuCenLife, a scene understanding data set that contains more rich scenes, and contains multiple tags of tasks that can be used for human 3d detection, human action recognition, semantic segmentation and instance segmentation, and also gave the information of objects to better carry out human-centric scene understanding tasks.
\subsection{Point Cloud-based Segmentation}
% outdoor 
Most outdoor point cloud segmentation methods mainly focus on point cloud representations. Point-based methods ~\cite{qi2017pointnet,qi2017pointnet++,yue2019dynamic,thomas2019kpconv} make the operation on unordered point cloud directly. Voxel-based methods ~\cite{choy20194d,graham20183d} utilize efficient sparse convolution to reduce the time complexity. PolarNet~\cite{zhang2020polarnet} and Cylinder3D~\cite{zhu2021cylindrical} further consider the non-uniform LiDAR point clouds characteristics and point distribution, and divide the points under the polar coordinate system. ~\cite{hong2021lidar} adopts the cylinder convolution and proposes a dynamic shifting network for instance prediction.
These methods are mainly focusing on automatic driving scenes, while neglecting the counterpart in human-centric scenarios with complex human-object interactions and challenging occlusions.

Another line of segmentation, namely point cloud instance segmentation, also embraces great progress, which can be mainly divided into proposal-based methods and grouping-based methods. Previous proposal-based methods ~\cite{yi2019gspn,engelmann20203d,yang2019learning} regard the instance segmentation as a top-down  pipeline, which first generate proposals and then segment the objects within the proposals. Grouping-based methods ~\cite{jiang2020pointgroup,vu2022softgroup,han2020occuseg,chen2021hierarchical,he2021dyco3d,wu20223d} adopt the bottom-up strategy. PointGroup ~\cite{jiang2020pointgroup} aggregates points from original and offset-shifted point sets. DyCo3D ~\cite{chen2021hierarchical} and DKNet~\cite{wu20223d} encode instances into kernels and propose dynamic convolution kernels and then merge the candidates. Considering the imprecise bounding box prediction in proposal-based methods for refinement and the time-consuming aggregation in grouping methods, 
~\cite{schult2022mask3d,sun2022superpoint} take each object instance as an instance query and design a query decoder with transformers. However, these methods are applied to structured indoor instances without human involvement and human-environment interactions. Our dataset and proposed method target more on human-human and human-object interactions in large-scale human-centric scenes.



\subsection{LiDAR-based 3D Detection}
% Ranged imaged based:
% point based:
% more popular voxel based {anchor free(center based) anchor based{Ted Voxel RCNN}}

As the mainstream of 3D perception task, 3D detection task has been fully explored, which can be grouped via the point encoding strategies. First, point-based methods ~\cite{IA-SSD,SASA,DeepHoughVoting,pointRCNN,3DSSD} extract the geometry information from raw points with sampling and grouping. ~\cite{tianfully,SqueezeSeg,bewley2021range,sun2021rsn,lang2019pointpillars,fan2021rangedet} transform point cloud into range images for detection. Second, voxel-based methods ~\cite{voxelnet,TED,MGTANet,deng2022vista,voxel-rcnn,centerpoint,yan2018second} convert raw point clouds to regular volumetric or pillar representations and adopt voxel-based feature encoding. 
Third, hyper-fusion methods ~\cite{peng2022pv,jiang2021vic,shi2020pv,yang2019std,chen2019fast,centerformer} take advantage of both voxels and points and fuse them together to model the hyper encoding. In this paper, we test them on the proposed HuCenLife dataset to provide the benchmark and offer the comprehensive analyses and comparison.  

% 3D detection aims to predict the 3D bounding box and the category of each object in the given point cloud. 3D detection can be roughly classified by their point encoding methods. For point-based method ~\cite{IA-SSD,SASA,DeepHoughVoting,pointRCNN,3DSSD}, they extract information directly from points' geometry. Different from point-based method, ~\cite{tianfully,SqueezeSeg,bewley2021range,sun2021rsn,lang2019pointpillars,fan2021rangedet} transform point cloud into range image and use it as input. Based on the idea of ~\cite{voxelnet}, voxel-based method ~\cite{centerformer,TED,MGTANet,STcrowd,deng2022vista,voxel-rcnn,centerpoint,yan2018second} emerges. Space is divided into voxels, thus convolution can be applied. There're also point-voxel based method ~\cite{peng2022pv,jiang2021vic,shi2020pv,yang2019std,chen2019fast} who try to take advantage of both voxel based and point based. While voxel-based best remain to be the most popular method. Among voxel-based, some of them~\cite{yan2018second,voxel-rcnn,TED}  are anchor-based. For the others, most of them~\cite{centerformer,MGTANet,STcrowd,deng2022vista,centerpoint} are based on heatmap to estimate the label and location of the target. Most of these algorithm are designed for auto-mobile and test on the datasets for auto-mobile who focus primarily on verticals. Some of them ~\cite{STcrowd} have paid close attention to pedestrian. However even them have neglect the variety action human can have. We are focus on the human with variety of pose and actions in large-scale scenes.
\subsection{Action Recognition}
% The research of human action recognition is mainly concentrated in the field of video classification.  
Recently, transformer-based methods have dominated the field of action recognition~\cite{li2022uniformerv2,fang2022eva}. Many variants based on ViT~\cite{dosovitskiy2020image} have been proposed to explore the potential of transformer in video classification, where
ViViT~\cite{arnab2021vivit} extends the two-dimensional patch to three-dimensional tube to model the temporal relation, MTV~\cite{yan2022multiview} divides the tube with different time scales to extract the action features with different amplitude of change over time, and TubeViT~\cite{piergiovanni2022rethinking} further samples various sized 3D space-time tubes from the video to generate learnable tokens.
% TubeViT  sample various sized 3D space-time tubes from the video to generate learnable tokens, which  can largely reduce computation burden for local relation and flexibly build long-range token dependencies from distant frames simultaneously. 
However, the common action recognition \cite{song2015sun,shahroudy2016ntu} is annotated in image-level and lacks of instance-level labels, causing these methods hard to be applicable in complex 3D scenarios. In this paper, we introduce point cloud-based instance action recognition task in large-scale scenes and collect the HuCenLife dataset equipped with various instances with different poses and motions, to make the basis for research community.







% 随着点云深度学习的快速发展，许多点云特征提取框架被提出和使用。PointNet最早提出一个对于多种任务效果都不错的点云分类骨架。它能够提取物体的全局特征，并且能够适应点云自身无序性的特点。然而，由于PointNet的设计并未考虑局部特征对分类任务的影响，在某些局部特征较为关键的任务和数据集中，PointNet往往表现不佳。PointNet++是PointNet的改进版，它利用了基于图像的深度学习卷积操作的思想，即用一个卷积核提取局部的特征，并且将多个卷积层进行串联，将感受野逐渐扩大。除了最原始的PointNet++(使用single scale grouping)以外，PointNet++还有改进版，即使用 multi-scale grouping的PointNet++，在对每个采样核进行SA操作时，它同时使用三个半径的采样范围来提取不同大小的局部特征。然而，PointNet++（msg）消耗了过多的计算资源，这限制了人们对其的使用。
% 最近，PointNeXt和PointMLP引起了人们的关注。PointNeXt充分挖掘了PointNet++的潜力，对于SA层的数量、聚集核的大小、特征提取的维度等等一系列的网络参数进行了大范围的调整。此外，它还创造性的添加了残差连接，并且引入了InvesMLP来增加网络的参数，提升网络的学习能力。此外，它还对点云神经网络的数据增强手段进行了充分而完全的探索，实现了在模型不变的情况下，学习能力的快速提高，验证了数据增强的必要性。PointMLP不去探究复杂的局部特征提取模块的设计，而是利用简单的MLP去提取特征，使得网络具有排列不变形。残差模块的设计使得网络的层数可以根据任务的需要很方便的加深或者缩短，产生深层的特征表示。由于不使用复杂的局部特征提取模块，网络在性能很有竞争力的同时，运行速度也十分优良。这些模型都推进了点云深度学习的发展，在不同的任务上也有着不同的表现。
%目前，人类动作分类的研究主要集中在视频分类领域。最近，有许多基于ViT和ViViT的transformer变种来探索transformer在视频分类的潜力。ViT将输入的图片按照固定大小将图片划分为256个patch，对每个patch进行线性映射后输入transformer encoder。ViViT输入的是视频包含的的多帧图片，将原本二维的patch增加了时间维，变成了三维tube，以此融入时序信息。MTV在ViViT的基础上，以6种不同大小的时间尺度划分tube，并进行融合，来充分的、同时的提取那些随时间变化幅度不同的动作特征。TubeViT  sample various sized 3D space-time tubes from the video to generate learnable tokens, 达到可以将图像、视频，或者二者一起作为输入数据的形式。UniFormer unify 3D convolution and spatiotemporal selfattention in a concise transformer format,既可以在浅层（相邻几帧）动作变化幅度不大的情况下以较少的计算资源消耗来学习动作的时空标识，来应对时空冗余，也可以在深层（时间跨度较长的帧）上建立全局时间的特征关系依赖。(In the shallow layers, our aggregator learns local relation with a small learnable parameter matrix, which can largely reduce computation burden by aggregating context from adjacent tokens in a small 3D neighborhood. In the deep layers, our aggregator learns global relation with similarity comparison, which can flexibly build long-range token dependencies from distant frames in the video.) UniFormerV2将各种受过良好预训练的ViT和UniFormer结合在一起，使得UniFormer无需go through a tedious supervised pretraining phase by learning images from scratch, before finetuning on videos. 


