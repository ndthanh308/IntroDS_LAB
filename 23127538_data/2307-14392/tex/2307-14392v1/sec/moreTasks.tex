In this paper, we provide benchmarks on HuCenLife for three main tasks, including 3D segmentation, 3D detection, and action recognition in human-centric scenarios. However, benefiting from the rich annotations in HuCenLife dataset, there are many other tasks deserving explored. %We discuss several meaningful topics in the following.

\subsection{Human-Object Interaction Detection}
Recently, the task of Human-Object Interaction (HOI) detection~\cite{zhang2022exploring,yuan2022detecting} attracts more and more attention, which targets for detecting the person and the interacted object and meanwhile classifying the interaction category. Current studies and datasets are limited to the interaction between single person and single object in one scene and they are all based on the image modality. 3D HOI tasks in large-scale free environments with multiple persons and multiple objects can be formulated and evaluated on HuCenLife.


%At present, the development of HOI is mainly focused on 2D images. The image has rich and dense semantic information, but it lacks the real geometry, scale, shape and other information of 3D point cloud, and the 3D geometric relationship between people and objects plays a vital role in judging the action of interaction class. Our data set also labels 3D detection frame, instance segmentation, object category and other labels for objects interacting with people, providing help for subsequent HOI tasks based on 3D point cloud.

% We believe 3D object detection is the foundation of future HOI task. So we provide 3D object detection baseline here. Centerpoint is chosen as the baseline. Specifically, to accomplish the task, two separate models are trained, one for detection human and one for object. We show AP result of part of the classes in Table \ref{tab:objectdetection}. However, our experiments have revealed that CenterPoint, while a popular choice for 3D object detection, is not well-suited for our specific dataset due to its unique challenges. Popular out door dataset focus on objects, such as human, car, truck and etc, whose sizes are similar and have clear separation among each other. Conversely, our dataset focuses only on humans and the objects they interact with, such as bags on a shoulder, computers on a table being used, and luggage cases held by individuals. These objects have bounding boxes that are in close proximity or may even intersect with one another.This presents a challenge for outdoor detection methods using bev heatmap and NMS designs, such as CenterPoint, and narrows the performance capabilities of such methods. Additionally, our indoor scenes are much smaller in size compared to outdoor scenes, making our point cloud too sparse for indoor datasets and algorithms to operate effectively.


\subsection{Tracking and Trajectory Prediction}
HuCenLife contains sequential frames of data with the tracking ID annotation for all instances, which can facilitate the time-related tasks, such as 3D tracking~\cite{pttp,zheng2022beyond} and trajectory prediction~\cite{ma2019trafficpredict,deo2022multimodal}. It is challenging for these tasks due to the occlusions in crowded scenes, but it is significant to study consecutive behaviors and interactions in real world to provide valuable guidance for robots.

\subsection{3D Scene Generation}
With the success of Diffusion model~\cite{ho2020denoising} in image generation, many works try to achieve high-quality 3D data generation for single objects~\cite{luo2021diffusion} or scenes~\cite{zyrianov2022learning}. HuCenLife provides rich material for daily-life scenarios, and it is interesting to generate more human-centric scene data with semantic information to facilitate learning-based methods.

\subsection{Multi-modal Feature Fusion}
Apart from point cloud, HuCenLife also provides corresponding images. The complementary information of multi-modal features will definitely benefit all tasks mentioned above, which deserves in-depth research. 

