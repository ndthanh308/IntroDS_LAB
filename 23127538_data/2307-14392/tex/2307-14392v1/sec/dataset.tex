



\begin{table*}[ht]
\centering
\caption{Comparison with related datasets for 3D scene understanding. There are some abbreviations, where ``pc'' denotes LiDAR point cloud, ``ins. seg.'' means instance segmentation, ``bbx'' is bounding box, and ``inter. obj.'' denotes objects having interactions with humans.}
\label{tab:compare}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|cc|ccc|cc}
\hline
      \rowcolor{blue!5}   Dataset     & Data   &  LiDAR & Point Cloud
&Person &Person &\multicolumn{2}{c|}{Scenes} &\multicolumn{3}{c|}{Annotation Content}& \multicolumn{2}{c}{Annotation Targets} \\ \cline{7-13}   

\rowcolor{blue!5} &Modality & Beam&Frame&Number&Per Frame& indoor & outdoor & ins. seg.& 3D bbx& action& multi-person& inter. obj.
\\\hline
ScanNet\cite{dai2017scannet}               &RGBD         & -  &-&-&-& \textcolor{green}{\Checkmark}& \textcolor{red}{\XSolidBrush}    &  \textcolor{green}{\Checkmark}       &    \textcolor{green}{\Checkmark}      & \textcolor{red}{\XSolidBrush}              & \textcolor{red}{\XSolidBrush}  & \textcolor{red}{\XSolidBrush}  \\\hline
S3DIS\cite{armeni20163d}&RGBD  
&-&-&-& -  & \textcolor{green}{\Checkmark}& \textcolor{red}{\XSolidBrush}    &  \textcolor{green}{\Checkmark}       &    \textcolor{green}{\Checkmark}      & \textcolor{red}{\XSolidBrush}              & \textcolor{red}{\XSolidBrush}  & \textcolor{red}{\XSolidBrush}  \\\hline
SUN RGB-D\cite{song2015sun}&RGBD&-
&-&-&-&\textcolor{green}{\Checkmark} &\textcolor{red}{\XSolidBrush}    & \textcolor{red}{\XSolidBrush}    &\textcolor{green}{\Checkmark}   & \textcolor{red}{\XSolidBrush}         &\textcolor{red}{\XSolidBrush}    & \textcolor{red}{\XSolidBrush}
\\\hline
NTU RGB+D\cite{shahroudy2016ntu}                &RGBD        &-  &-&-&-
&\textcolor{green}{\Checkmark} &\textcolor{red}{\XSolidBrush}    & \textcolor{red}{\XSolidBrush}       & \textcolor{red}{\XSolidBrush}       &\textcolor{green}{\Checkmark}  &\textcolor{red}{\XSolidBrush}    & \textcolor{red}{\XSolidBrush}     \\\hline

BEHAVE\cite{bhatnagar2022behave}          &RGBD       &-   &
15.8k& 15.8k&1
&\textcolor{green}{\Checkmark} &\textcolor{red}{\XSolidBrush}    &   \textcolor{green}{\Checkmark}     &  \textcolor{red}{\XSolidBrush}      & \textcolor{green}{\Checkmark}      &\textcolor{red}{\XSolidBrush}    &\textcolor{green}{\Checkmark}          \\\hline

SemanticKITTI\cite{behley2019semantickitti}      &pc      & 64 & 43k&9.7k&0.2&\textcolor{red}{\XSolidBrush}  & \textcolor{green}{\Checkmark} &   \textcolor{green}{\Checkmark}     &  \textcolor{red}{\XSolidBrush}         &\textcolor{red}{\XSolidBrush}  &    \textcolor{green}{\Checkmark}           & \textcolor{red}{\XSolidBrush}       \\\hline

KITTI\cite{geiger2012we}      &image\&pc      & 64 & 15k &4.5k&0.3&
\textcolor{red}{\XSolidBrush}  & \textcolor{green}{\Checkmark}  &  \textcolor{green}{\Checkmark} &   \textcolor{green}{\Checkmark}            &\textcolor{red}{\XSolidBrush}  &    \textcolor{green}{\Checkmark}           & \textcolor{red}{\XSolidBrush}       \\\hline


Waymo\cite{sun2020scalability}         &image\&pc   & 64 &  230k& 2.8M&12&
\textcolor{red}{\XSolidBrush}  & \textcolor{green}{\Checkmark}&   \textcolor{green}{\Checkmark}     &   \textcolor{green}{\Checkmark}     &\textcolor{red}{\XSolidBrush}  &    \textcolor{green}{\Checkmark}           & \textcolor{red}{\XSolidBrush}       \\\hline

nuScenes\cite{caesar2020nuscenes}         &image\&pc     & 32 & 40k &208k&5&\textcolor{red}{\XSolidBrush}  & \textcolor{green}{\Checkmark}&   \textcolor{green}{\Checkmark}     &   \textcolor{green}{\Checkmark}     &\textcolor{red}{\XSolidBrush}  &    \textcolor{green}{\Checkmark}           & \textcolor{red}{\XSolidBrush}       \\\hline

STCrowd\cite{cong2022stcrowd}           &image\&pc    & 128  & 11k &219k&20&\textcolor{red}{\XSolidBrush}  & \textcolor{green}{\Checkmark} & \textcolor{red}{\XSolidBrush}        & \textcolor{green}{\Checkmark}        & \textcolor{red}{\XSolidBrush}  &  \textcolor{green}{\Checkmark}        & \textcolor{red}{\XSolidBrush}         \\\hline


\textbf{HuCenLife}          &image\&pc      &128 & 6.1k& 65k&11&\textcolor{green}{\Checkmark}   & \textcolor{green}{\Checkmark}  & \textcolor{green}{\Checkmark}       & \textcolor{green}{\Checkmark}  & \textcolor{green}{\Checkmark} &  \textcolor{green}{\Checkmark} & \textcolor{green}{\Checkmark}  \\ \hline
\end{tabular}
}
\vspace{-2ex}
\end{table*}


HuCenLife is the first dataset that emphasizes human-centric 3D scene understanding, containing indoor and outdoor daily-life scenes with rich annotations of human activities, human-human interactions, and human-object interactions, which facilitates the development of intelligent security, assistive robots, human-machine cooperation, \etc.
%HuCenLife focuses on human-centric 3D scene understanding and captures diverse daily-life scenes, which contains rich human-human interactions and human-object interactions in large-scale indoor or outdoor scenarios. 
In this section, we first introduce the data acquisition in Sec.\ref{subsec:Acquisition}, and then provide important annotation statistics in Sec.\ref{subsec:Statistics}, and finally highlight the novelties of HuCenLife by comparing with existing influential datasets in Sec.\ref{subsec:Characteristic}.


\subsection{Data Acquisition}
\label{subsec:Acquisition}
To collect the dataset, we built a Visual-LiDAR Capture System, which mainly consists of one 128-beam Ouster-OS1 LiDAR and six industrial cameras in a circle, as Fig ~\ref{fig:sensor} shows. All sensors are tied in fixed positions on the bracket with mechanical synchronization. The LiDAR has a $360^{\circ}$ horizon field of view (FOV) $\times 45^{\circ}$ vertical FOV, and each camera has a $75^{\circ} \times 51.6^{\circ}$ FOV with $1920\times1200$ image resolution. For our equipment, LiDAR captures raw point cloud in $10$Hz and camera takes pictures in $32$Hz.



% % Figure environment removed

% Figure environment removed

\subsection{Annotation}
\label{subsec:Statistics}

We manually annotated all humans and these objects with interactions with humans in LiDAR point cloud by referring to the synchronized image. We select one frame per second for labeling and finally obtain $6,185$ frames ($103$ minutes) of annotated LiDAR point cloud. For each target, we provide four kinds of annotations, \ie, point cloud-based instance segmentation, 3D bounding box, human action classification, and tracking ID across consecutive frames, like Fig ~\ref{fig:teaser} shows. In HuCenLife, there are $65,265$ human instances in total, including $58,354$ adults and $6,911$ children, and $31,303$ human-interacted objects. There are $20$ categories of objects and $12$ kinds of human actions. Specifically, the HuCenLife dataset is collected in $15$ distinguished locations with $32$ human-centric daily-life scenes, including playground, shopping mall, campus, park, gym, meeting room, express station, \etc. For each scene, there are $11$ persons on average with multiple interacted objects, and for some complex scenes, there are about $70$ persons. The diverse density distributions in HuCenLife bring challenges for related research.
%Because the visual-LiDAR device is usually settled beside the scene to eliminate its affect on peoples' normal activities, we only annotate $180^{\circ}$ view of LiDAR point cloud and corresponding images of three cameras (where different scenes vary in cameras), which covers pertinent contents of scenes. 
More detailed annotation introductions are in the supplementary material.


\subsection{Characteristics}
\label{subsec:Characteristic}
We introduce the basic information of HuCenLife and compare it with related popular datasets in Table ~\ref{tab:compare}. In particular, we conclude four highlights of our dataset below.
%from five main aspects, including the modality of acquisition data, sensor configuration, data-collection scenes, annotation content, and annotation targets. In particular, we  

\textbf{Large-scale Dynamic Scenarios.} 
Benefiting from the long-range-sensing and light-independent properties of LiDAR, HuCenLife contains data of diverse large-scale scenes day and night. Unlike indoor datasets~\cite{dai2017scannet} where the scene is pre-scanned and has only static objects, HuCenLife provides online captured multi-modal visual data of dynamically changing scenes with dynamic people, objects, and background. Furthermore, the density of humans and objects is changing from a few to dozens in distinct scenes. The visual data in such diverse dynamic scenarios has huge significance for developing mobile robots.  

\textbf{Abundant Human Poses.} 
Different from current traffic or crowd datasets~\cite{sun2020scalability,caesar2020nuscenes,cong2022stcrowd}, where people only act as pedestrians walking or standing on the road, HuCenLife pays attention to daily-life scenarios, where people have rich actions, such as doing exercise, crouching down, dancing, running, riding, \etc. In particular, HuCenLife contains thousands of children samples, which are never concerned in previous datasets. Such complex scenarios with high-degree freedom of human poses bring challenges for accurate perception and recognition.  

\textbf{Diverse Human-centric Interactions.} 
Apart from abundant self-actions of humans, HuCenLife also includes rich human-human interactions (hugging, holding hands, holding a baby, \etc.) and human-object interactions (riding a bike, opening the door, carrying a box, \etc.). What's more, there are some extremely complex human-human-object interactions, such as playing basketball, having a meeting in a room, \etc., which require the participation of multiple persons and objects. HuCenLife is unique for containing diversified interaction data in a variety of scenes, which is significant for the research of human-machine cooperation and boosts the development of service robots.

\textbf{Rich Annotations.} 
HuCenLife provides rich fine-grained annotations, which can benefit many perception tasks, such as point cloud segmentation, 3D detection, 3D tracking, action recognition, HOI, motion prediction, \etc. In particular, due to complex scene contents, the annotation process of HuCenLife is much more difficult than others. A well-trained annotator usually spends $25$min on average for labeling one frame of LiDAR point cloud in our dataset. 


\subsection{Privacy Preservation}
We strictly obey the privacy-preserving rules. We mask all sensitive information, such as the faces of humans and locations, in RGB images. LiDAR point clouds without any texture and facial information naturally protect the privacy.

%Previous datasets for 3D scene understanding focus on two main scenarios, including the indoor static scenes with many types of furniture and outdoor traffic scenes with various vehicles and road structures. HuCenLife is the first dataset to pay attention to large-scale human-centric daily life scenarios, including indoor and outdoor scenes in day and night with rich human activities, human-human interactions, and human-object interactions, which are significant for the development of intelligent security, assistive robots, human-machine cooperation, etc. We compare with related popular datsets in Tab.\ref{tab:compare} from five main aspects, including the modality of acquisition data, sensor configuration, data-collection scenes, annotation content, and annotation targets. We can see that HuCenLife provides totally different scene data and more fine-grained annotations, which can benefit many 3D perception tasks, such as point cloud segmentation, detection, action recognition, HOI, motion prediction, etc. In particular, due to complex scene contents, the annotation process of HuCenLife is much more difficult than others.





\section{Various Downstream Tasks}

As mentioned above, our dataset can benefit numerous human-centric 3D perception tasks. We conduct three main tasks on HuCenLife based on the LiDAR point cloud, including human-centric instance segmentation, human-centric 3D detection, and human-centric action recognition, and provide the baseline methods. Particularly, novel methods are proposed for instance segmentation and action recognition, respectively, to tackle the difficulties of large-scale human-centric scenarios. In what follows, we present details of these tasks with extensive experiments in order.