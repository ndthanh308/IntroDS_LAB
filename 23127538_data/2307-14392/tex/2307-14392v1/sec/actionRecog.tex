
Previous works for action recognition are based on 2D images or videos and they only need to give one label for one scene. We introduce the 3D action recognition task in large-scale human-centric scenarios, which aims to detect all persons in the scene and provide corresponding action types. 3D action recognition task is significant for fine-grained scene understanding and can benefit the development of intelligent surveillance and collaborative robots. To our knowledge, we are the first to propose the related dataset and solutions for the new task. 


%The input of human-centric action recognition is a frame of point cloud $P$ and we propose an end-to-end two-stage model HuCenAction that first detects the human location and then predicts the action category. 
%Human behavior is complex and closely related to the surrounding objects and other people. For one perspective, the size of the objects that interacting with people is various, thus we leverage multi-resolution Hierarchical Point Feature Extraction(HPFE) to capture global features and local features. In addition, considering the interaction between the human and the surrounding neighbours, we design Ego-Neighbour Feature Interaction(ENFI) to get the enhanced human feature and benefit the action classification.
\subsection{Method}
Our 3D action recognition method is in a two-stage manner based on the input of LiDAR point cloud, as shown in Figure ~\ref{fig:MRNetnetwork}. Considering that some human actions are related to adjacent interactive objects, after obtaining individual bounding box by 3D detector, we enlarge the box to crop more points related to the person for the following fine-grained feature extraction. Especially, we leverage a Hierarchical Point Feature Extraction module to pay attention to multi-scale objects and get multi-level features. Moreover, we design an Ego-Neighbour Feature Interaction (ENFI) module to make use of the relationship among the ego-person and neighbors to help forecast social actions. 

\subsubsection{Hierarchical Point Feature Extraction}
%For daily-life scenarios, the objects interacted with people have varying sizes. However, most point cloud classification methods lack the ability to capture local features with diverse receptive fields. 

To capture both global features and local features with dynamically changing receptive fields, we use $R$ parallel branches to extract multi-resolution features.
Serial Set Abstractions~\cite{qi2017pointnet} are applied to process the features of different scales, where each branch undergoes $L$ times with fixed sampling cores and branch-specific sampling range. Finally, these features are up-sampled to the same dimension and fused together with pooling to generate the hierarchical fusion feature $F_{HF}$.

%To capture both global features and local features with dynamically changing receptive fields, we designed HPFE, which has $R$ parallel resolution branches to hierarchically extract features from $R$ resolutions $F_i \in \mathcal{R}^{(N/2^r) \times (32*2^r)}$. 
%Serial Set Abstractions(SA)~\cite{qi2017pointnet} is applied to process the features of different scales, where each branch undergoes L times with fixed sampling core and branch-specific sampling range to extract increasingly specific features.
% Serial Set Abstractions(SA) are applied to each branch to perceive the features of a specific scale $L$ times with fixed sampling core and branch-specific sampling range. 
%$$F_i^{l+1} = SA(F_i^l), \quad i=0,1,...,R,l=0,1,...,L$$
 %Finally, these features are up-sampled to the same dimension and fused and pooled to generate Hierarchical Fusion Features $F_{HF} = \text{Pool}({\text{UpSample}(F_i^l)})$.
 
% Finally, the features of R branches will be up-sampled to same dimension, and then we fuse and pool the features of different resolutions to generate Hierarchical Fusion Feature $F_{HF} = \text{Pool}({F_i})$.
 % HPFE incorporates a Scalable Equal Sampling stage to fully perceive the features of a specific scale for each resolution channel and increase the learning ability of the network.
% For each point of single person, we use Set Abstraction for $R$ resolution branches: $P_i \in R^{(N/2^r) \times (32*2^r)}$.
% Then, the features of these channels are simultaneously input into the Scalable Equal Sampling Module for equal sampling. 
% The sampling core remains unchanged during this process, but the receptive field gradually expands with the increase of network depth. 
% And the depth of the network can be a?
\subsubsection{Ego-Neighbour Feature Interaction}

%Human behavior is also related the behavior of people around him. So we design Ego-Neighbour Feature Interaction(ENFI) to perceive the interaction with neighbors.
Like Figure ~\ref{fig:MRNetnetwork} shows, we first enhance the ego person feature by self-attention and get $F_{ego}$. Then, we select features of $k$ neighbours around the target as $K_{neigh}$ and $V_{neigh}$ and take the ego-feature as queries $Q_{ego}$. The distances from neighbours to the target are used for position encoding. 
%First, the feature of a ego person is enhanced by self-attention, and then the enhanced ego feature $F_{ego}$ will be used as $Q_{ego}$ for following cross attention with neighbours. 
%We select k neighbours feature around the target as $K_{neigh}$ and $V_{neigh}$.
%The distances from neighbours to ego are for position encoding.
We apply cross-attention to extract the ego-neighbour interaction information and gain the final interaction enhanced ego feature by $F_{IE} = F_{ego} \bigoplus \text{CrossAttention}(Q_{ego},K_{neigh},V_{neigh})$, where $\bigoplus$ denotes concatenation. In this way, we model the relationships of a group to benefit the social action recognition.

%Thus we take full advantage of human-human interaction to benefit the instance action recognition.

%First, the feature of a ego person is enhanced by self-attention, and then the enhanced ego feature $F_{ego}$ will be used as $Q_{ego}$ for following cross attention with neighbours. 
%We select k neighbours feature around the target as $K_{neigh}$ and $V_{neigh}$. The distances from neighbours to ego are for position encoding. Then we apply cross-attention to extract the ego-neighbour interaction and concatenate with enhanced ego feature to get final Interaction Enhanced Ego Feature $F_{IEE} = F_{ego} + \text{crossattention}(Q_{ego},K_{neigh},V_{neigh})$. 
%Thus we take full advantage of human-human interaction to benefit the instance action recognition.
% We get the features of $k+1$ neighbours where k is the number of selected neighbour. 
% We use the distance from ego as position encoding. After self attention, we get the enhanced ego feature which will be used as $Q$ for following cross attention. The features of k neighbors will be used as $K$ and $V$ after linear projection. 
% $K_{neigh}$ will be multiplied by $Q_{Ego}$, and then scale and normalize to get attention affinity matrix, which means the attention coefficient of ego feature for other neighbor.
% $$M_{aff}=softmax(Q_{ego} \cdot K_{nei} / \sqrt{d})$$
% Then, the attention affinity matrix is finally multiplied by $V_{neigh}$ to obtain the weighted features of neighbor of interest to ego.
% $$F_{weighted}=M_{aff} \cdot V_{nei}$$


\iffalse

\begin{table*}[ht]
  \centering
    \caption{Human-centric Action Recognition Result}
    \label{tab:endtoend}
    \setlength{\tabcolsep}{1mm}
    
\resizebox{\linewidth}{!}{      

\begin{tabular}{l|cccccccccccc|ccc}
\hline
Methods            & Lift & Carry & Move & Pull-Push & Sit  & Scooter-BalanceBike & Hum-Inter & Fitness & Entertain & Sports & Bend-Over & Standing & mAP  & mRec & mPrec \\ \hline
Centerpoint\cite{centerpoint}        & 0.3  & 2.7   & 0.1  & 8.6       & 2.2  & 18.1                & 0.0       & 0.0     & 0.6       & 4.7    & 0.1       & 18.7     & 4.7  & 16.3 & 17.8  \\
STCrowd\cite{STcrowd}            & 0.2  & 0.8   & 0.1  & 15.6      & 3.3  & 14.5                & 0.0       & 4.3     & 0.4       & 3.0    & 0.4       & 23.0     & 5.5  & 16.7 & 21.0  \\
CenterFormer\cite{centerformer}       & 0.9  & 0.0   & 0.0  & 0.0       & 6.4  & 0.0                 & 0.0       & 0.0     & 2.0       & 58.7   & 0.0       & 13.7     & 6.8  & 20.3 & 11.4  \\ \hline
Centerpoint-HCA  & 2.7  & 11.3  & 9.1  & 19.4      & 16.3 & 46.2                & 2.2       & 1.4     & 4.4       & 22.6   & 2.4       & 9.7      & \textbf{12.3} & \textbf{40.0} & \textbf{26.9}  \\
STCrowd-HCA      & 2.1  & 10.1  & 8.1  & 16.4      & 14.1 & 44.2                & 1.9       & 2.4     & 3.9       & 17.9   & 1.8       & 8.8      & 11.0 & 37.5 & 25.8  \\
CenterFormer-HCA & 2.7  & 8.3   & 7.0  & 18.1      & 13.4 & 52.7                & 1.7       & 1.8     & 3.8       & 21.4   & 2.0       & 8.1      & 11.7 & 37.6 & 26.8  \\ \hline
\end{tabular}

}
\vspace{-1ex}
\end{table*}


\begin{table}[ht]
\caption{Ablation Study on Classification Network,* means multi-scale grouping PointNet++. We show mean Average Precision(mAP), mean Recall(mRec), and mean Precise(mPrec).}
\label{tab:secondstage}
\begin{tabular}{l|ccc}

\hline
\multicolumn{1}{l|}{Methods} & mAP  & mRec & mPrec \\ \hline
ViT\cite{dosovitskiy2020image}                          & 6.8  & 23.1 & 19.9  \\ \hline
PVT\cite{zhang2022pvt}                          & 6.8  & 30.5 & 19.8  \\
PointNet\cite{qi2017pointnet}                     & 4.6  & 26.3 & 15.5  \\
PointNet++\cite{qi2017pointnet++}              & 8.2  & 34.2 & 22.7  \\
PointNet++*           & 7.4  & 34.6 & 20.6  \\
PointMLP\cite{ma2022rethinking}                     & 6.1  & 28.0 & 19.4  \\
PointNeXt\cite{qian2022pointnext}                    & 8.0  & 33.0 & 21.2  \\ \hline
HCA(w/o ENFI)         & 9.3  & 37.1 & 24.7  \\
HCA                  & 12.3 & 40.0 & 26.9  \\ \hline
\end{tabular}
\end{table}

\fi



\begin{table}[ht]
\caption{Comparison results of action recognition on HuCenLife. All methods are based on the same 3D detector for fair evaluation.}
\label{tab:actioncompare}
\begin{tabular}{l|ccc}

\hline
\multicolumn{1}{l|}{Methods} & mAP  & mRecall & mPrecision \\ \hline
Baseline & 7.3  & 14.6 & 19.9\\\hline
+ ViT\cite{dosovitskiy2020image}                          & 9.4  & 23.1 & 19.9  \\ \hline

+ PVT\cite{zhang2022pvt}                          & 13.2  & 30.5 & 19.8  \\
+ PointNet\cite{qi2017pointnet}                     & 8.4  & 26.3 & 15.5  \\
+ PointNet++\cite{qi2017pointnet++}              & 15.6  & 34.2 & 22.7  \\
+ PointMLP\cite{ma2022rethinking}                     & 11.3  & 28.0 & 19.4  \\
+ PointNeXt\cite{qian2022pointnext}                    & 15.0  & 33.0 & 21.2  \\ \hline

Ours                  & \textbf{21.0} & \textbf{40.0} & \textbf{26.9}  \\ 
Ours(w/o ENFI)         & 15.4  & 37.1 & 24.7  \\
\hline

\end{tabular}
\vspace{-3ex}
\end{table}



\subsection{Experiments}


\subsubsection{Baselines and Evaluation Metrics}
We take pre-trained CenterPoint as the 3D Detector for all the experiments for fair comparison in this section. Because no existing methods can be directly used for solving the new 3D action recognition task. As Table ~\ref{tab:actioncompare} shows, we provide benchmarks and comparisons from four aspects. The first is to directly adapt the 3D detector to predict multi-class persons with different action labels, which is the ``Baseline'' in Table~\ref{tab:actioncompare}. The second is to add a feature extractor for cropped individual point cloud for the second-stage action classification, and we tried several popular point-feature extractors, including PVT, PointNet, PointNet++, PointMLP, and PointNext. In particular, to verify the performance of input modalities, we also use ViT to extract image features for image-based action recognition by projecting the 3D bounding box to calibrated images. At last, we provide the results of our solution with ablation for ENFI module. 

%to crop the single point cloud for it performs best in human detection , and present several point-based classification networks: PointNet, PointNet++, PointMLP and PointNext. We also explore Voxel-based methods such as Point Voxel Transformer.

%There are some ablation studies on modalities comparation. We use 2D bounding box projected from 3D bounding box to get single person's image as the input of Vision Transformer. In addition, we explore the performance gain of ENFI for action recognition.
%\subsubsection{Evaluation Metrics}
% Human-centric Action Recognition is a point cloud classification task. We divide the actions of all people in the data set into 17 categories, and predict an action tag for each person.


 %\textbf{Average Precision metric}. 
 %Following \cite{nuScenes} and \cite{STcrowd}, we use Average Precision (AP) with 3D center distance threshold $D =  \{0.25, 0.5, 1\}$ meters for each action class $C$. 
We use the mean Average Precision (mAP) obtained by averaging AP through thresholds $D =  \{0.25, 0.5, 1\}$ and classes to evaluate the performance.
$$
\mathrm{mAP}=\frac{1}{|\mathbb{C}||\mathbb{D}|} \sum_{c \in \mathbb{C}} \sum_{d \in \mathbb{D}} \mathrm{AP}_{c, d}
$$
where $|\mathbb{C}|$ is the number of action category.
In addition, we also utilize Mean Recall (mRecall) and Mean Precision (mPrecision) by averaging recall and precision through thresholds and classes.

\subsubsection{Results and Discussion}
We show the overall performance in Table ~\ref{tab:actioncompare}, and detailed evaluation values of all categories of actions and visualization results are in the supplementary material. It can be seen from the results that our method outperforms others with an obvious margin, mainly due to the multi-level feature extraction and multi-person interaction modeling, which are more suitable for understanding human-centric complex scenarios. However, our method has its own limitations and there are several potential improvement directions. First, current two-stage framework strongly relies on the detector performance and the one-stage method for action recognition in large-scale scenes is worth exploring. Moreover, human action is time-dependent and how to extract valuable temporal information in consecutive data to eliminate the ambiguity of actions is also promising.  



%% Figure environment removed
%We compare the impact of Human Detector in Table \ref{tab:endtoend}. Different action involve different amount of movement. Thus, multi-scale feature boost the performance of \cite{centerformer,STcrowd}. Similarly, Centerformer\cite{centerformer} achieve better performance due to the usage of deformable transformer to aggravate feature from and around proposals. However, we observe that single-stage methods can't recognize actions that involved small or large objects and performed poorly in actions that involved objects that shared a similar size to humans. We found that using a refine network improved the performance of the algorithms in these scenarios.

%In addition, we did ablation study on classification network in Table \ref{tab:secondstage}. PointNet++ with multi-scale grouping performs best among all baselines, for it also considers the perception for multi scale features. With the help of HPFE and ENFI, our method captures the features of multiple scales simultaneously and considers the impact of surrounding neighbour, thus finally surpasses PointNet++(msg) 4.9\% in mAP.

%Image-based method does not perform well in crowded scenarios, for the 2D bounding box often include irrelevant people in the backround. And the performance gain of ENFI is 3.0\% in mAP.

















% \begin{equation}
% \mathrm{mRec}=\frac{1}{|\mathbb{C}||\mathbb{D}|} \sum_{c \in \mathbb{C}} \sum_{d \in \mathbb{D}} \mathrm{Recall}_{c, d}
% \end{equation}
% \begin{equation}
% \mathrm{mPrec}=\frac{1}{|\mathbb{C}||\mathbb{D}|} \sum_{c \in \mathbb{C}} \sum_{d \in \mathbb{D}} \mathrm{Precision}_{c, d}
% \end{equation}
% \subsubsection{Implementation Details}

% \subsubsection{Evaluation matrix}


% \end{equation}

% \subsubsection{Result}
% Our result is shown in Table \ref{tab:HuCenAction}


% 3D Human detection evaluation matrix:
% Average Precision (AP) metric with the 3D center distance threshold. For crowded scenes, the distance thresholds are chosen from D = {0.25, 0.5, 1} meters and the mean Average Precision (mAP) is calculated by: (From stcrowd)
% \begin{equation}
% m A P=\frac{1}{|D|} \sum_{d \in D} A P_d
% \end{equation}









