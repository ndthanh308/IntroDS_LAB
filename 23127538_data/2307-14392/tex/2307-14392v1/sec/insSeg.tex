


\begin{table*}[ht!]\small
\caption{Instance segmentation results on HuCenLife dataset.} \label{tab:test}
	\setlength{\tabcolsep}{1.8mm}
 \resizebox{\linewidth}{!}{      
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
                  & \rotatebox{90}{person} & \rotatebox{90}{motorbike} & \rotatebox{90}{table} & \rotatebox{90}{box}   & \rotatebox{90}{cart}  & \rotatebox{90}{seesaw} & \rotatebox{90}{basketball} & \rotatebox{90}{fitness equ} & \rotatebox{90}{cabinet} & \rotatebox{90}{baby}  & \rotatebox{90}{blackboard} & \rotatebox{90}{staircase} & \rotatebox{90}{slide} & \rotatebox{90}{scooter} & \rotatebox{90}{computer} & \rotatebox{90}{backpack} & \rotatebox{90}{obj in hand} & \rotatebox{90}{chair} & \rotatebox{90}{spring car} & \rotatebox{90}{ground} & mIOU  & AP50 & AP25 \\\hline
% Softgroup          & 78.3  & 22.2      & 3.1  & 42.0 & 24. & 65.7  & 13.4      & 24.6             & 1.7    & 37.6 & 90.6      & 11.5     & 96.0            & 25.6         & 35.5    & 30.3    & 6.8        & 6.9  & 34.4      & 97.1  & 37.4 &      &      \\\hline

Voxel-DSNet \cite{hong2021lidar}   &66.9&16.1&20.7&   22.6  &16.3 &12.6      &5.4&11.9&1.1&25.7&58.3&8.5&72.7&33.3&24.6&20.7&1.6&8.6&3.1&97.9 &    26.4 &2.6&7.1\\\hline
Cylinder-DSNet \cite{hong2021lidar}&72.3&12.9&{23.8}&28.6&18.9&25.2&5.8&4.7&6.8&23.4&90.2&{21.4}&67.9&37.2&15.5&23.9&3.5&{14.1}&2.8&97.9&29.8&1.2& 7.6\\\hline
DKNet  \cite{wu20223d}
&75.6&{52.7}&5.3&26.3&35.8&65.6&0.0&14.6&0.6&39.7&93.9&0.0&95.1&{48.5}&13.1&9.8&{14.6}&8.1&3.4&{98.0}& 35.0
&11.1 & 14.0 \\\hline
SoftGroup \cite{vu2022softgroup}& 80.0  & 32.6      & 4.4  & 38.2 & 20.6 & 60.7  & 8.3       & 25.2             & 3.2    & {42.5} & {95.5}      & 1.0      & 95.8            & 24.6         & 27.5    & {34.0}    & 7.1        & 7.6  & 29.0      & 96.2  & 36.7 & 32.5 & 38.2 \\\hline
% Ours(w/o opt)     & 81.6  & 30.7      & 8.6  & 40.0 & 41.1 & 67.2  & 20.7      & 26.4             & 3.8    & 27.7 & 88.1      & 2.5      & 96.4            & 38.3         & 35.6    & 33.1    & 7.4        & 9.5  & 24.7      & 96.7  & 39.0 & 33.0 & 38.8 \\\hline

\rowcolor{gray!10} Ours &{82.7}&46.4&6.4&{39.7}&{51.1}&{69.4}&{15.3}&{29.6}&3.0&40.0&89.4&1.2&{96.8}&35.6&{29.2}&28.4&6.8&10.6&{32.3}&96.9& \textbf{40.5}&\textbf{35.6}&\textbf{40.4}\\\hline\hline
Ours + PointPainting
&79.8&30.7&16.2&42.5&{47.6}&53.4&8.1&21.7&{3.9}&32.8&82.3&0.0&95.6&34.2&19.6&25.3&{11.9}&{19.7}&30.0&96.4&37.6&28.9&34.8\\\hline 
w/o HHIO     & 79.5 &15.2 &{17.4} &32.9 &31.6 &56.6 &7.1 &26.1 &1.8 &35.0 &92.8 &0.6 &95.8 &22.0 &26.7 &30.2 &9.5 &19.0 &29.0 &{97.1} & 36.3 & 25.0 &  31.6  \\\hline
Ours + LocalFusion      &{81.8}&{46.8}&2.0&{46.2}&36.8&{74.7}&13.2&{28.5}&0.4&{37.3}&{93.8}&{2.3}&{96.5}&{35.2}&{37.0}&27.8&8.6&9.9&26.0&96.5 &40.1    &  36.9    & 42.0  \\\hline 
w/o HHIO      & 80.7  & 39.3      & 2.3  & 41.9 & 26.6 & 73.1  & {13.9}      & 23.2             & 2.2    & 35.4 & 92.2      & 0.0      & 94.4            & 24.5         & 29.7    &{ 31.4}    & 7.3        & 13.6 & {36.1}      & 95.9  & 38.2 &  36.6    &  41.6  \\\hline 
\end{tabular}
}
%\vspace{-2ex}
\end{table*}

\begin{table*}[ht!]\tiny
\caption{Semantic segmentation results on BEHAVE dataset.} 
\centering
\label{tab:behave}
	\setlength{\tabcolsep}{1.8mm}
  \resizebox{\linewidth}{!}{  
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
                  & \rotatebox{90}{person} & \rotatebox{90}{backpack} & \rotatebox{90}{basketball} & \rotatebox{90}{boxlarge}   & \rotatebox{90}{boxlong}  & \rotatebox{90}{boxmedium} & \rotatebox{90}{boxsmall} & \rotatebox{90}{boxtiny} & \rotatebox{90}{chairblack} & \rotatebox{90}{chairwood}  & \rotatebox{90}{keyboard} & \rotatebox{90}{monitor} & \rotatebox{90}{container} & \rotatebox{90}{stool } & \rotatebox{90}{suitcase} & \rotatebox{90}{tablesmall} & \rotatebox{90}{tablesquare} & \rotatebox{90}{toolbox} & \rotatebox{90}{trashbin} & \rotatebox{90}{yogaball} &\rotatebox{90}{yogamat} & mIOU   \\\hline
SoftGroup\cite{vu2022softgroup} &96.8&71.8&54.9&83.7&54.4&56.3&40.4&21.9&85.6&82.4&27.1&67.3&71.7&83.5&77.7&82.5&90.5&44.5&64.7&89.0&70.5 & 67.5\\\hline
Ours &97.0&72.4&61.3&86.6&62.2&57.3&45.6&33.4&87.7&83.3&30.8&72.1&73.8&84.2&76.0&86.6&91.9&49.2&66.9&89.0&76.4 & \textbf{70.7}\\\hline
\end{tabular}
}
\vspace{-3ex}
\end{table*}



For LiDAR point cloud-based semantic instance segmentation, the input is expressed as $ P \in \mathcal{R}^{N \times 4}$, which involves $N$ points with the 3D location and reflection intensity $(x, y, z, r)$. The task is to assign each point to a category and then output a set of object instances with their corresponding semantic labels.



\subsection{Method}

For human-centric scenes, people have diverse pose types and may stay together with occlusions. Moreover, some objects are relatively small and closely located to the person, causing overlapping or stitching points with humans and bringing difficulties in distinguishing from the person.
% For human-object integration scenes, person with diverse pose types dominates the dataset and the objects are close to the person. While the categories of the object have a long-tailed distribution and some small things are hard to distinguish and separated from person. Moreover, the person may walking together with occlusion, resulting in partial points and the person interaction is also worth analyzing.  
%Existing methods for outdoor traffic scenes and indoor scenes neglect the fine-grained segmentation such as small object closed with person and the interaction between humans and human-objects. 
% mainly focusing on the feature representation and efficient partition, while neglecting the fine-grained segmentation such as small object closed with person. Several indoor instance segmentation methods pay more attention on grouping and clustering the instance, while they are applied on structured furniture with dense RGB-D representations without consider the participation of human and the sparsity of LiDAR. 
To tackle these problems, we propose a Human-Human-Object Interaction(HHOI) module, shown in Figure \ref{fig:segnetwork}. The model first extracts the human-human interaction feature with attention strategy so that humans can be more accurately recognized even with partial point cloud in occluded scenes.
Then, it uses human-centric features to guide the network automatically to learn a weighted feature to pay attention to interactive objects, which can benefit capturing fine-grained semantic information.



\subsubsection{Human-Human-Object-Interaction Module}
% \subsubsection{Human-Human-Object-Interaction Module}
% The objects are in close contact with person, so we consider the human-guided feature for a better feature representation.
As shown in Figure \ref{fig:segnetwork}, we utilize a sparse 3D Unet to get $D$ dimensional point feature $F_p \in \mathcal{R}^{N \times D}$. Then, human-human interacted features are extracted through a transformer mechanism. 
We get the semantic score $Y = softmax(MLP(F_p)) = \{y_{i,c}\}^{N\times C}$ for each point, where $C$ is the class number. And then we select $M$ points with the confidence of belonging to person class higher than the threshold $\tau$. We further apply the triplet Q, K, V attention layer to extract correlations among different sampled person features $F_s$ and obtain the final human-guided feature:
$$f_{attention} = softmax(\frac{QK^T}{\sqrt{D}})V,$$
$$F_g = LN(f_{attention} + FFN(f_{attention})),$$
where $LN$ is layer normalization and $FFN$ is the feed-forward neural network~\cite{vaswani2017attention}.
Then, we use human-guided feature to extract human-object interaction for fine-grained object segmentation.
The similarity weighted matrix $W = softmax(F_p F_g^T)$ is computed to enhance the features of objects that people interact with. We multiply the weighted matrix with point features to obtain the final weighted features.
In this way, the model adaptively learns human-related representations and enhances the object feature with the guidance of high-confidence human features.

% Figure environment removed

\subsubsection{Point-wise Prediction and Refinement}
Taking the weighted features as input, the semantic branch and offset branch apply two-layer MLP and output the semantic scores $S\in \mathcal{R}^{N \times K}$ and offset vectors  $O\in \mathcal{R}^{N \times K}$  from the point to the instance center, respectively. The weighted cross-entropy loss $\mathcal{L}_{\text {semantic}}$ and $L_1$ regression loss $\mathcal{L}_{\text {offset}} $ are used to train the semantic and offset branches. After that, we follow the refinement stage in SoftGroup~\cite{vu2022softgroup}, where point-level proposals are fed into a tiny-unet to predict classification scores, instance masks, and mask scores to generate the final instance results. Specifically, the classification branch predicts the category scores $c_k$ for each instance. The segmentation branch utilizes a point-wise MLP to predict an instance mask $m_k$ for each instance proposal. Mask scoring branch estimates the IoU between the predicted mask and the ground truth for each instance. We train each branch with cross-entropy loss $\mathcal{L}_{\text {class}}$, binary cross-entropy loss $\mathcal{L}_{\text {mask}}$, and $l_2$ regression loss $\mathcal{L}_{\text{mask score}}$. And the total loss is the sum of all above losses.
%$\mathcal{L} = \mathcal{L}_{\text {semantic}}+\mathcal{L}_{\text {offset}}+\mathcal{L}_{\text {class}}+\mathcal{L}_{\text {mask}}+\mathcal{L}_{\text{mask score}}.$
% ------------ move to supp
% $$
% L_{\text {semantic }}=\frac{1}{N} \sum_{i=1}^N \operatorname{CE}\left(\boldsymbol{s}_i, s_i^*\right),$$
% $$
% L_{\text {offset }}=\frac{1}{\sum_{i=1}^N \mathbb{I}_{\left\{\boldsymbol{p}_i\right\}}} \sum_{i=1}^N \mathbb{I}_{\left\{\boldsymbol{p}_i\right\}}\left\|\boldsymbol{o}_i-\boldsymbol{o}_i^*\right\|_1,
% $$
% $$
% L_{\text {class }}=\frac{1}{K} \sum_{k=1}^K \mathrm{CE}\left(\boldsymbol{c}_k, c_k^*\right), 
% $$
% $$
% L_{\text {mask }}=\frac{1}{\sum_{k=1}^K \mathbb{I}_{\left\{\boldsymbol{m}_k\right\}}} \sum_{k=1}^K \mathbb{I}_{\left\{\boldsymbol{m}_k\right\}} \mathrm{BCE}\left(\boldsymbol{m}_k, \boldsymbol{m}_k^*\right), 
% $$
% $$
% \mathcal{L}_{\text{mask score}}=\frac{1}{\sum_{k=1}^{N_{g t}} \mathbb{I}_{\left\{iou_k\right\}}} \sum_{k=1}^{N_{g t}} \mathbb{I}_{\left\{iou_k\right\}}\left\|iou_k-iou_k^*\right\|_2
% $$
% where $*$ denotes the ground truth.





% Figure environment removed



\subsection{Experiments}
\subsubsection{Baselines and Evaluation Metrics}
Previous 3D instance segmentation works can be divided into LiDAR-based methods and RGB-D-based methods. For the former, we compare with current SOTA method DSNet ~\cite{hong2021lidar} of both voxel-division version and cylinder-division version. For the latter, we select current SOTA approaches DKnet ~\cite{wu20223d} and SoftGroup~\cite{vu2022softgroup} for comparison. 


%We present several popular baselines with different modalities for segmentation.
%Previous methods can be divided into LiDAR-based and RGB-D based. We evaluate the result on the LiDAR-based methods Voxel-based DSNet and Cylinder-based DSNet\cite{hong2021lidar}, and the RGB-D based instance segmentation baselines DKnet\cite{wu20223d} and Softgroup\cite{vu2022softgroup}.
%For LiDAR-Camera modality, we apply two fusion strategies (PointPainting and LocalFusion) on our methods. The fusion baselines are provided to facilitate further exploration. PointPainting appends the raw LiDAR point with corresponding RGB color according to calibration matrix. LocalFusion concatenates high-dimensional image feature to the corresponding high dimensional point semantic feature. Our HHOI module has consistently improved the performance on various fusion strategies, validating its generalization ability.
% And we also apply our HHOI on each fusion baseline to validate the generalization ability of our method. 

%\subsubsection{Evaluation Metrics}
We utilize mean IoU (mIoU) to evaluate the quality of the semantic segmentation. For instance segmentation, we report AP50 and AP25 which denote the scores with IoU thresholds of 50\% and 25\%, respectively.

\subsubsection{Results}

\noindent\textbf{Comparison on HuCenLife dataset. } 
We compare the results of our proposed method with baseline methods in Table ~\ref{tab:test}. DSNet does not get satisfactory results, mainly because it focuses on traffic scenarios, while the span of object scale is much larger in human-centric scenarios. SoftGroup is better than outdoor methods because it has a refinement stage for recognizing small objects. Our method performs best due to the use of interaction information.

%With the help of HHOI, the network learns a better human-human interaction representation and the object features are enhance by human-guided feature, and our methods outperforms baselines on contemporary benchmarks.



\noindent\textbf{Comparison on BEHAVE dataset. } 
To further evaluate the generalization capability of human-object interaction scenes, we also conduct experiments for semantic segmentation on BEHAVE~\cite{bhatnagar2022behave} dataset in Table ~\ref{tab:behave}. BEHAVE dataset is a human-object interaction dataset, which is collected in indoor scenarios and provides RGB-D frames and 3D SMPL. To adapt it to our task, we generate the point cloud and segmentation label from RGB-D images and segmented masks. There is only single person with single object per frame and the total number of the object categories is 20. We follow the official protocol of dataset splitting. Our method still outperforms the best baseline method SoftGroup by 2.8\% in mIOU. 
%The results validate the generalizability and the effective ness of our HHOI module.

\noindent\textbf{Sensor-fusion-based 3D segmentation. }
Because our dataset also contains image data, we also provide LiDAR-Camera sensor-fusion baselines based on our method in Table ~\ref{tab:test} to facilitate further research. PointPainting appends the raw LiDAR point with corresponding RGB color according to calibration matrix. LocalFusion concatenates high-dimensional image feature to the corresponding high dimensional point semantic feature. And our HHOI module has consistently improved the performance on various fusion strategies, validating its generalization ability.

%Pointpainting get a comparable result with the LiDAR-only backbone, while LocalFusion perform better with a little margin, which demonstrates the effectiveness to introduce image features in higher dimensions.
%We apply our HHOI on each fusion strategies, our results demonstrate that our module consistently improves the performance on each fusion baseline.


\begin{table}[ht]
  \centering
    \caption{Person-only 3D detection results on HuCenLife. }
    \label{tab:humdetectionresult}
    \setlength{\tabcolsep}{0.8mm}
\begin{tabular}{{l|ccc|l}}
\hline
Methods      & AP(0.25) & AP(0.5) & AP(1.0) & mAP  \\ \hline
CenterPoint\cite{centerpoint}  & 61.8    & 68.7    & 70.3    & 66.9 \\
STCrowd\cite{cong2022stcrowd}      &  61.8    &  71.6   & 73.4   & 68.9 \\
TED\cite{TED}          &  51.0    &  53.3   &  54.1   & 52.8 \\
CenterFormer\cite{centerformer} &   73.0   & 80.1    & 81.4    & 78.2 \\ 
\hline
\end{tabular}
\vspace{-3ex}
\end{table}

\begin{table}[ht]
\caption{Full-category 3D detection results (AP) on HuCenLife. We only select six types of objects for demonstration.}

\label{tab:objectdetection}
\setlength{\tabcolsep}{1.3mm}
\resizebox{\linewidth}{!}{      
\begin{tabular}{l|c|c|c|c|c|c}
\hline
Methods     & motorbike   & box          & cart        & scooter & backpack     & object in hand          \\ \hline
CenterPoint\cite{centerpoint} &13.4  & 17.1          & 20.9             & 43.4 & 4.2          &  8.4             \\ \hline
STCrowd\cite{cong2022stcrowd}     &   5.4        & 14.4 & 25.3  & 48.7          & 4.5 & 13.5 \\ \hline
CenterFormer\cite{centerformer}     & 3.8          & 16.2 & 24.2  & 44.4         & 2.6 & 12.5 \\ \hline
\end{tabular}
}
\vspace{-3ex}
\end{table}