


% \textcolor{red}{1: why LiDAR is indispensable?  Can not get its necessary from the draft; One possible option is that HuCenLife is the first 3D human centric dataset and our key contribution is to provide the infra for community and facilitate the related research}

% \textcolor{red}{2: Is there any unique task which is relied on LiDAR (or depth information)? We can highlight some specific tasks which is not available in the common datasets}

% \textcolor{red}{3: Compared to STCrowd, highlight the difference of HuCenLife; A teaser is required to show the key contribution or difference; The structure of Introduction can follow the style of STCrowd}

% why human-centric? --> current datasets neglecting? --> how we make the remedy? --> Specific tasks and some novel methods

Human-centric scene understanding in 3D large-scale scenarios is attracting increasing attention~\cite{Dai2022HSC4DH4,cong2022stcrowd,lip,lidarcap}, which plays an indispensable role in human-centric applications, including assistive robotics, autonomous driving, surveillance, human-robot cooperation, \etc. It is often confronted with substantial difficulties since these human-centric scenarios usually have the attributes of various subjects with different poses, fine-grained human-object interactions, and challenging localization and recognition with occlusions. Moreover, current state-of-the-art perception methods heavily rely on large-scale datasets to achieve good performance. Therefore, to promote the research of human-centric scene understanding, the collection of large-scale datasets with rich and fine-grained annotations is required urgently, which is difficult but of great significance.     

% Human-centric understanding in 3D large-scale scenarios is attracting more and more attention~\cite{Dai2022HSC4DH4,STcrowd,lip,lidarcap}.
% Fine-grained segmenting and recognizing human-object interactions is a critical task, which can benefit a variety of real-world applications including robotics, autonomous driving, surveillance, human-machine cooperation, etc.
% However, recognizing and detecting interactions involving both humans and objects in a scene is a challenging task in complex and cluttered environments. Compared with traffic scenarios, human with diverse poses dominate the scenarios and the object distribution tends to follow a long-tailed distribution. Moreover, both human and some human carry-on objects are relatively small and closely located, leading to sparse points in distance and overlapping or stitching points between human and object instances. Furthermore, instances may occlude each other in human-object interaction scenes. These above situation makes the accurate object segmentation and human action recognition quite challenging. 



In previous work, many studies target on the scene understanding based on the input of image or video~\cite{arnab2021vivit,piergiovanni2022rethinking,dosovitskiy2020image,yan2022multiview}, which are not applicable to real-world applications due to the limited 2D visual representations. Afterward, some works pay attention to the static indoor-scene understanding~\cite{dai2017scannet,armeni20163d,bhatnagar2022behave} based on the pre-scanned RGB-D data, which are not suitable for the research of real-time perception. Recently, more and more outdoor multi-modal datasets~\cite{caesar2020nuscenes,waymoopen} are released equipped with LiDAR point clouds. They provide detailed annotations under complex outdoor scenes, while they often focus on the vehicle-dominated traffic environment and neglect the more challenging human-centric daily-life scenarios. Although the dataset STCrowd~\cite{cong2022stcrowd} appears lately, it focuses on the detection task of dense pedestrian scenes, lacking varied human activities and diversified annotations. Consequently, the dataset with rich and fine-grained annotations for human-centric understanding in long-range 3D space is crucial and insufficient.

%including human action recognition~\cite{vit,vivivt,mtv,tubevit}, indoor RGB-D understanding~\cite{dai2017scannet,armeni20163d,bhatnagar2022behave}, \etc. However, these studies often lack the fine-grained annotations, complex human-object intersections and dynamic environments. Recently, more and more outdoor multi-modal datasets are released equipped with LiDAR point clouds. They provide the detailed annotations under complex outdoor scenes, while they often focus on the vehicle-dominated environment and neglect the more challenging human-object intersections. Consequently, the dataset with rich and fine-grained annotations for human-centric scene understanding is crucial and insufficient.

% In previous work, many previous studies have explored on human action recognition ~\cite{vit,vivivt,mtv,tubevit} by video classification in human-centric scenarios, they often lack 3D location and instance-level recognition.
% Several datasets~\cite{dai2017scannet,armeni20163d,bhatnagar2022behave} have been proposed for 3D scene understanding with human or object involved. 
% ~\cite{dai2017scannet,armeni20163d} %indoor seg datasets 
% provide rich annotation for diverse indoor objects segmentaiton and detection but without human involvement. 
% ~\cite{bhatnagar2022behave} introduce the dataset for 3D humans, objects and contacts in natural environments with RGB-D images but only one person with single object is involved in each scene. %behave
% The above RGB-D datasets are also not applicable for
% large-scale complex scenes and suffer from physical limitations, such as sensitivity to lighting conditions and availability only within 5 meters. 
% LiDAR can provide accurate depth information over a larger range. 
% Ourdoor multi-modal datasets~\cite{Geiger2013VisionMR,Caesar2020nuScenesAM,Sun2020ScalabilityIP,Patil2019TheHD,Zhu2020A3DA3,Chang2019Argoverse3T,behley2019semantickitti} are mainly collected in traffic scenes with LiDAR and serve for automatic driving, which are vehicle-dominated and  
% neglecting human and object interaction and fine-grained human carry-on segmentation. 
% Consequently, these datasets are insufficient for human-object interaction understanding.

In this paper, to facilitate the research of human-centric 3D scene understanding, we collect a large-scale multi-modal dataset, namely HuCenLife, by using calibrated and synchronized camera and LiDAR.
Specifically, the dataset captures $32$ multi-person involved daily-life scenes with rich human activities and human-object interactions. Various indoor and outdoor scenarios are both included. For the annotation, we provide fine-grained labels including instance segmentation, 3D bounding box, action categories, and continuous instance IDs, which can benefit various 3D perception tasks, such as point cloud segmentation, detection, action recognition, Human-Object Interaction (HOI) detection, tracking, motion prediction, \etc. In this paper, we provide benchmarks for the former three tasks by executing current state-of-the-art methods on HuCenLife and give discussions for other downstream tasks.


In particular, considering the specific characteristics of human-centric scenarios, we propose effective modules to improve the performance for point cloud-based segmentation and action recognition in the complex human-centric environments. First, we model human-human interactions and human-object interactions and leverage their mutual relationships to benefit the classification of points and instances. Second, to solve the problem of the big scale span of objects in daily-life scenarios, we exploit multi-resolution feature extraction strategy to aggregate global features and local features hierarchically so that small objects can be better attended.
%In this paper, we mainly focus on three human-centric perception tasks, including human-centric point cloud segmentation, human-centric 3D detection and human-centric action recognition. Moreover, we further propose the Human-Human-Object-Interaction (HHOI) module and  Ego-Neighbour Feature
%Interaction (ENFI) to model the complex interactions in human-centric scenarios, and design the Hierarchical Point Feature Extraction (HPFE) capture both local features and global features in hierarchical resolutions. 
We evaluate our methods and conduct extensive experiments on HuCenLife. Several ablation studies are also conducted to demonstrate the effectiveness of each module and good generalization capability. Our contributions are summarized as follows:


% Human behavior in human-centric scenarios is often complex and closely related to surrounding objects and environments. We propose the Human-Human-Object-Interaction(HHOI) module to obtain a better instance segmentation result. Our module integrates the presentation between diverse human and learn a weighted feature guided by human-centric feature to enhance the object feature. 
% Human action recognition also depends on interacted object and the behaviours of surrounding people. Considering the different size of interacted objects, we design Hierarchical Point Feature Extraction(HPFE), to capture both local features and global features at various resolutions. To extract the human-human interaction, Ego-Neighbour Feature Interaction(ENFI) is proposed to combine the ego-human feature with the neighbours through a cross-attention mechanism. 
% We evaluate our method on HuCenLife and achieve state-of-the-art performance. Several ablation studies also conducted to demonstrate our effectiveness and good generalization capability. Our contributions are summarized as follows:
\begin{enumerate}
    \item We introduce HuCenLife, the first large-scale multi-modal dataset for human-centric 3D scene understanding with rich human-environment interactions and fine-grained annotations. 
    \item HuCenLife can benefit various human-centric 3D perception tasks, including segmentation, detection, action recognition, HOI, tracking, motion prediction, etc. We provide baselines for three main tasks to facilitate future research.
    \item Several novel modules are designed by incorporating fine-grained interactions and capturing features at various resolutions to promote more accurate perception in human-centric scenes.
    
\end{enumerate}


% LiDAR-based security defense, monitoring and automatic driving has become a popular research field in recent years. Whether in outdoor or indoor scenes, we need an automatic algorithm to monitor the position of people in the crowd and the current state of people. Human-centric Scene Understanding in 3D Large-scale Scenarios is crucial to the security work in key areas, the crime alarm, and the judgment of pedestrian status on the road.

% Due to the low cost and large scale of image data collection, as well as the characteristics that images can provide relatively rich and dense semantic information, image-based depth learning method plays a leading role in human behavior classification. However, image-based datasets have a narrow vision. They are generally used to predict the behavior of a single person and cannot predict the behavior of multiple people at the same time. Moreover, the camera does not work well in poor light conditions, such as night or rainy and foggy weather, so it is impossible to predict human behavior in this case. In addition, for objects far away, the image will generally be degraded and blurred, so it is also difficult to classify the actions of people far away from the image. Finally, image-based methods can not provide accurate human location information, which is very important for multi-person situations.

% The characteristics of LiDAR are completely different from that of camera. LiDAR can collect data in a 360-degree range, and can collect data in a range of more than 1000 square meters. LiDAR can locate and analyze people far away from the equipment in a large field scene. In addition, LiDAR can provide accurate depth information, as well as geometry, scale, and shape information that are important for human positioning in the global and local human shape perception. In addition, the LiDAR is highly robust to the environment, can work normally in the dark, and can avoid the interference of bad weather. Therefore, many human-related tasks based on the form of laser radar point cloud are gradually emerging, for example, human three-dimensional target detection, key point detection, motion capture, gait recognition, and so on. For our task, we need to be able to predict the real-time behavior of human beings to provide information assistance for the current safety status or the next step of decision-making for autonomous vehicles. However, human behavior is often complex, and is often closely related to the surrounding objects, human beings, their own posture, position and environmental scene characteristics, and may have multiple actions at the same time.

% In order to promote the research of human 3D target detection and behavior classification, and then apply it to the fields of security and automatic driving, we collected a large scene multimodal dataset including a 128-line LiDAR and six perspective cameras: HuCenLife. The seven acquisition devices use a unified ROS timeline to maintain time synchronization, and have rich and varied human labeling information, including instance segmentation labels of people and their interacting objects, 3D bounding boxes of people and objects, and labels of various behaviors of each person. HuCenLife has made great contributions to the research of target detection, semantic segmentation, instance segmentation, human behavior recognition and other related fields, and multiple sensors have provided great potential for multimodal fusion tasks. HuCenLife includes many common scenes of human daily life and work, such as sports, leisure, entertainment, fitness, office, etc., and has various light environments.

% The main contributions of our paper are as follows:

% 1. We have collected the first large multimodal dataset HuCenLife of large scenes including indoor and outdoor, and have a variety of labeling information that can be used for multiple tasks, including accurate and rich human behavior labels.