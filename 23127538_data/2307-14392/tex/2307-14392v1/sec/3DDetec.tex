% Figure environment removed


LiDAR point cloud-based 3D detection is well-studied in recent years, driven by autonomous driving. It provides critical information of obstacles for the motion planning of robots to guarantee the safety. Specifically, the input for 3D detection is the point cloud $P$ and the output is predicted bounding boxes with 7 dimensions ($x$,$y$,$z$,$w$,$l$,$h$,$r$), consisting of the 3D position in LiDAR coordinate system, the size of bounding box, and the rotation. In this section, we provide benchmarks for the 3D detection task on HuCenLife by evaluating current state-of-the-art methods and give discussion on the research of human-centric 3D detection.


%Detection can benefit many downstreams tasks such as action recognition, tracking, trajectory prediction and etc. Existing datasets are either collected in traffic scenarios or in crowded, and the pedestrians have limited action, like walking and standing. While in real world, human have diverse actions including sitting, playing sports, hugging, doing fitness, etc. Different human poses lead to various of size of human bounding box, making the accurate detection harder.


%When detecting objects, their bounding box size can also vary greatly, even within the same category. Additionally, algorithms need to determine whether an object is being interacted with, which presents a new challenge.(added)

%Specifically, the input of the detection is the point cloud $P$ and the output is bounding box with 7 dimensions ($x$,$y$,$z$,$w$,$l$,$h$,$r$).
%We provide several baselines in following.

 % Human-centric 3D Detection is a task to detect the
% location of all the human in thegiven point cloud. Detection is the foundation of many other tasks such as  action recognition, tracking, trajectory prediction and etc. For existing dataset, they only focus on pedestrian. This narrow them down onto person who is either walking or standing. However it's obvious that a person can't be walking or standing out-door 24 hours a day, he or she will have more actions, such as sitting, playing sports, hugging, doing fitness and so on. Different actions led to different human shapes. Abundant actions in our dataset which contribute to the variousness of human shapes and covering both in-door and out-door help our dataset represents the really world better. We provides baseline on this unique challenge here. 



\subsection{Baselines and Evaluation Metrics}
We choose four representative works and test their performance on our dataset. CenterPoint ~\cite{centerpoint} is a popular anchor-free detector and based on it, STCrowd ~\cite{cong2022stcrowd} aims at solving dense crowd scenarios. By means of the transformer mechanism, TED~\cite{TED} and CenterFormer~\cite{centerformer} achieve impressive performance recently. Following ~\cite{cong2022stcrowd,caesar2020nuscenes}, we use Average Precision (AP) with 3D center distance thresholds D = \{0.25, 0.5, 1\} meters as the evaluation metric. Then mean Average Precision (mAP) is obtained by averaging AP.

 % For current SOTA 3D detection. We choose TED\cite{TED} and Centerformer\cite{centerformer}. 
 %Current 3D detection methods TED\cite{TED} and Centerformer\cite{centerformer} utilize transformer strategies, the former extract transformation-equivariant feature for detection and the latter apply a deformable transformer to aggravate feature from  candidate center.
 %We provide the experiments based on above baselines.

%\subsection{Evaluation Metrics}
 %\textbf{Average Precision metric.}Following \cite{nuScenes，STcrowd}, we use Average Precision (AP) with 3D center distance threshold D = {0.25, 0.5, 1} meters. Then mean Average Precision (mAP) is obtained by averaging AP.
%\begin{equation}
%m A P=\frac{1}{|D|} \sum_{d \in D} A P_d
%\end{equation}

\subsection{Results and Discussion}

We conduct experiments on two settings, including \textbf{person-only 3D detection} in Table ~\ref{tab:humdetectionresult} and  \textbf{full-category 3D detection} in Table ~\ref{tab:objectdetection}. These baseline methods are designed for large-scale traffic scenarios, which perform limited on human-centric scenarios, especially for detecting small objects. We conclude with three main challenges for conducting 3D detection in human-centric scenarios. First, people usually have different poses in different actions, such as crouching, sitting, waving, etc., and such diverse body poses cause distinct sizes of bounding box. Second, there are many relatively small objects in scenes, bringing difficulties to balance the accuracy of fine-grained detection and the efficiency of large-scale scene data processing. Third, multi-objects may locate at different heights in the same place, such as in complex scenarios of escalator and slide, leading to larger dimension of feature recognition. Previous methods using BEV feature map will miss details and transformer-based methods have horrible cost. Therefore, there is a lot of room for the 3D detection research in human-centric scenes, while our dataset can offer a good platform for it.


%However, the 3D Lidar detection algorithms are optimized for road scenes and struggle with interference from common daily-life objects like sculptures, chairs, and express boxes. Ceilings in indoor environments negatively affect their use of the bird's-eye-view feature map. Moreover, detecting individuals on staircases and escalators with significant altitude differences poses a significant challenge.
%For future work, improvement can be achieved though gaining a deeper understanding of whether the instance is human or not, having the ability to suit both indoor and out-door and handle large scene and small scene.


%We compare the results of baselines in Table \ref{tab:humdetectionresult}. Compared to others\cite{centerformer,STcrowd,centerpoint}, TED\cite{TED} is the only anchor-based detection algorithm which limits its performance. Unlike cars focused by auto-mobile dataset\cite{nuScenes,Geiger2013VisionMR}, human can have really different bounding box shape. This narrows down the performance of anchor-based algorithm. Despite STcrowd\cite{STcrowd} utilizes multi-scale feature for dense crowd detection, this design make it more vulnerable towards interference created by the object being interacting with. Centerformer\cite{centerformer} and Centerpoint\cite{centerpoint} have achieved reasonable performance, with centerformer being slightly higher attribute to its transformer design. 


%However, none of the algorithm have been optimized for other than road scene. So they wasn't design to withstand the interference created by items look like sheltered human In daily-life scenarios, such as sculpture, chair， big express box, etc. Also, ceiling in the indoors will have negative effect on the bev feature map which is popularly used by current detection algorithms\cite{TED,centerformer,STcrowd,centerpoint}. Furthermore, different from the existing dataset, we have huge altitude difference between person in the scene caused by stairs and escalator. This is challenging for current algorithm to detect person accurately on these facilitates.
% However, The current algorithms\cite{TED,centerformer,STcrowd,centerpoint} for 3D Lidar detection have been optimized primarily for road scenes and is not able to withstand the interference created by objects that are commonly found in daily-life scenarios, such as sculptures, chairs, and large express boxes. Additionally, the presence of ceilings in indoor environments can have a negative effect on the bird's-eye-view (BEV) feature map used by them. Another significant challenge for them is the presence of significant altitude differences between individuals in a scene, particularly when staircases and escalators present.




%\subsubsection{Full-category 3D Detection}
%Specifically, to accomplish the task, two separate models are trained, one for detection human and one for object. We show AP result of part of the classes in Table \ref{tab:objectdetection}. Results show that the disadvantage of current 3d detection have been further magnified. For future work, it need to have a understanding of the relation between object and person. Additionally, better NMS algorithms are needed to address the issue of object bounding box collides with human bounding box.
%However, this has revealed that while CenterPoint remains a popular choice for 3D object detection, it is not well-suited for our unique dataset due to the inherent challenges it poses. Unlike outdoor datasets that focus on objects such as cars, trucks, and humans, whose sizes are typically similar and distinct from one another, our dataset specifically targets human-object interactions, such as individuals carrying bags or using computers, where the bounding boxes of the objects may intersect or be in close proximity to one another. This poses a challenge for conventional outdoor detection methods, including those based on bev heatmap and non-maximum suppression (NMS) designs, such as CenterPoint, and limits their performance.Furthermore, Our outdoor scenes exhibit a level of sparsity that current indoor methods are ill-equipped to handle, as they are typically designed for smaller indoor datasets. This mismatch in scale and complexity poses a significant challenge for the application of existing indoor methods to our unique outdoor context.

% For future algorithms, it's required to have an enhanced understanding of the relationship between humans and objects. Being able to whether objects that are being interacted or not even in close distance to human. The size of objects human interacts with can vary dramatically, thus algorithms should have a better dynamic receptive field to accommodate these varying object sizes. Finally, to address the issue of object instance collision, better NMS algorithms are required. Conventional NMS designs are not suitable for this tasks, where objects may intersect or be in close proximity to one another. Therefore, developing algorithms that can effectively handle object collision is necessary.




