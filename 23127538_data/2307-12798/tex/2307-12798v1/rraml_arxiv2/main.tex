% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{xcolor}
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\newcommand{\ab}[1]{\textcolor{red}{#1}}
\newcommand{\gio}[1]{\textcolor{blue}{#1}}
\begin{document}
%
\title{RRAML: Reinforced Retrieval Augmented Machine Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%\author{Anonymous Authors}
\author{Andrea Bacciu\inst{1} \and Florin Cocunasu\inst{1} \and Federico Siciliano\inst{1} \and Fabrizio Silvestri\inst{1} \and Nicola Tonellotto\inst{2} \and Giovanni Trappolini\inst{1}\\[2ex]
\{surname\}@diag.uniroma1.it\inst{1}, nicola.tonellotto@unipi.it\inst{2}}
%
\authorrunning{Bacciu, Cocunasu, Siciliano, Silvestri, Tonellotto, Trappolini, 2023}
\institute{
  Sapienza University of Rome \and % Sapienza logo
  University of Pisa  % Pisa logo
}

%\institute{Sapienza University of Rome \and University of Pisa}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. 
However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability.

To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). 
RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database.
%RRAML use at its core an LLM as a Reasoner and extends its knowledge with supporting information retrieved by a purpose-built retriever from a vast user-provided database.

By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, 
it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the computational intensity involved.
%By training LLMs only once and utilizing them across multiple tasks, we maximize efficiency and resource utilization.
Additionally
%, through the reward model-driven training,
we seamlessly link the retriever's task with the reasoner, mitigating hallucinations and reducing irrelevant, and potentially damaging retrieved documents.

%By leveraging recent advancements in reinforcement learning, we aim to align the retriever's task with the final goal, mitigating hallucinations and reducing irrelevant, and potentially damaging retrieved documents.
%Our approach avoids the direct dependence and access on any specific LLM, treating it as a black box, and instead focuses on training the retriever through a reward model linked to the task outcome.
% These LLMs are typically trained once and then accessed as black-box via API to serve an indefinite number of tasks.
%Our Retriever is fine-tuned with Reinforcement Learning to align the retriever's task with the final goal, mitigating hallucinations and reducing irrelevant and potentially damaging retrieved documents delivered to the Reasoner.

We believe that the research agenda outlined in this paper has the potential to profoundly impact the field of AI, democratizing access to and utilization of LLMs for a wide range of entities.

\keywords{Deep Learning  \and Information Retrieval \and Large Language Models.}
\end{abstract}

% \begin{abstract}

% The advent of LLMs with in-context learning capabilities shift the natural language processing landscape showcasing state-of-the-art performances in comprehending, generating, and manipulating human language. 
% These LLMs are typically trained once and then accessed as black-box via API to serve an indefinite number of tasks.
% However, the black-box access presents limitations in terms of context constraints and external source availability.
% To mitigate these issues, we propose Reinforced Retrieval Augmented Machine Learning (RRAML) by extending the Retrieval Augmented Generation approach (RAG).
% RRAML use at its core an LLM as a Reasoner and extends its knowledge with supporting information retrieved by a purpose-built retriever from a vast user-provided database.
% The key difference with RAG is our Retriever is fine-tuned with Reinforcement Learning to align the retriever's task with the final goal, mitigating hallucinations and reducing irrelevant and potentially damaging retrieved documents delivered to the Reasoner.


% \end{abstract}
%
%
%

\section{Introduction}


The advent of Large Language Models (LLMs) has brought about a paradigm shift in machine learning and its related disciplines.
Fueled by the tremendous advancements in deep learning and neural network architectures, LLMs \cite{brown2020language,scao2022bloom,openai2022chatgpt,touvron2023llama,bacciu2023fauno} have exhibited unprecedented capabilities in understanding, generating, and manipulating human language.
Famously, ChatGPT \cite{openai2022chatgpt} has entered the public space by reaching one million users in a matter of days.
The way these models are usually used is through API that only allows submitting a textual prompt and getting back from the server the generated text. 
However, this causes an immediate limitation: all information must be passed through this context, and we know transformer-based models do not scale nicely.
Even if they did, these APIs are typically paid by the token, so providing a long context would be expensive.
There is an impendent need, though, to accommodate the enormous power of those models to specific user needs by making sure that they could use the reasoning capabilities of LLMs, through in-context learning \cite{brown2020language}, on their data.
\\
An obvious solution is to adopt a retriever-augmented approach \cite{lewis2020retrieval,xie2023factual}. 
In this setting, we use a retriever to filter out relevant information to be passed as context to the reasoner.
This generates a new problem, however, namely that the retriever and the reasoner are not aligned \cite{trappolini2023multimodal,thorne2021database,thorne2021natural}.
In particular, two issues emerge. First, the retriever might not be trained on the task of interest to the user. Second, the retriever might actually provide ``dangerous” pieces of information to the reasoner, as proved in \cite{artsiomdang}, leading to hallucinations.
\\
Ideally, one would have to fine-tune these models to account for these issues.
Within this setting, fine-tuning the model for a given task is technically impossible.
We asked ourselves the following question: ``Is it still possible to use the API that gatekeeps those powerful LLMs on our data without the need for fine-tuning?''
We argue this question has a positive answer.
In this paper, we propose a novel framework, Reinforced Retrieval Augmented for large Language Models (RRAML), in which we combine the reasoning capabilities of large foundational models enhanced by the provision of supporting relevant information provided by a retriever that searches them in a large database.
In this setting, an efficient retriever model is tasked to search for relevant information in an arbitrarily large database of data provided by users. Once this set of relevant data has been retrieved, it is forwarded to the reasoner (a large foundational model such as ChatGPT, for instance), through its API to ``reason" on the input and produce an adequate result.
In particular, we plan to overcome current limitations, namely that the retriever's task is detached form that of the reasoner, reducing in such a way the tendency of LLM to hallucinate and diminishing the amount of damaging documents returned by the retriever (damaging documents defined in \cite{carmel2022ir,sauchuk2022role,trappolini2023multimodal}).
We plan on doing so by exploiting recent advances in reinforcement learning.
Recently, in fact, reinforcement learning techniques like PPO \cite{schulman2017proximal} have been used to improve large foundational models with feedback from humans in the form of non-differentiable term loss.
\\
We propose to link the retriever training to the final task outcome by the use of a purposefully crafted reward model.
Furthermore, in this manner, we avoid the direct use of the reasoner, which can be considered a black box in this setting and exchanged freely.
We argue that the research agenda we lay out in this paper has the potential to impact the field of AI hugely and democratize the access and use of these large foundational models to a large set of entities.


\section{Methodology}

The overall system architecture consists of three main components: (1) a Generative Language Model, (2) the Retriever, and (3) a Reasoner (an LLM).
The system takes as input a task description, a query, and a database and gives as output the response generated by the Reasoner.
The system architecture can be visualized using a data-flow diagram, as shown in Figure \ref{fig:design}.

% Figure environment removed

The Generative Language Model takes the \textit{task description} and \textit{query} as input and generates a prompt. The Retriever takes the \textit{query} and the \textit{database} as input and outputs a support set, which is then concatenated with the \textit{query} and passed to the Reasoner.

\subsection{Data}
The data is a critical component of the framework: the \textit{task description} guides the generation of an appropriate prompt, the \textit{query} represents the user request, and the \textit{database} provides the data needed by the reasoner to perform the task.

\paragraph{Task Description}
The \textit{task description} is a string that defines the nature of the task the user wants to perform. 
For example, if the user wants to generate a summarization of multiple news articles, the \textit{task description} could be ``News Summarization". If the user wants to perform question answering on a vast document collection, the \textit{task description} could be ``Question Answering".
%translate a sentence from English to French, the \textit{task description} could be "Translation".

%This string can also be used to identify the appropriate pre-trained model to be used for the task.

\paragraph{Query}
The \textit{query} represents the user need. The Retriever will operate on the \textit{database} w.r.t to the user's query, and the resulting data is used as input for the task. For example, if the user wants to summarize a collection of news articles, the \textit{query} could be the topic the user is interested in.
If the user wants to answer a specific \textit{query}, this becomes the actual question.
% It must be represented both as a string and in a format compatible with the underlying \textit{database}.


\paragraph{Database}
The \textit{database} is a collection of data (or documents) that can be queried to provide relevant information to satisfy the user's information needs.
The database represents the knowledge needed by the Reasoner to perform the task.
The data stored in the \textit{database} will depend on the specific task and may include text, images, audio, and other data types (as in \cite{trappolini2023multimodal}). 
For example, if the user wants to summarize multiple news articles, the \textit{database} could be an indexed collection of articles.
If the user wants to perform Question Answering, the \textit{database} may consist of facts related to a particular topic (as in \cite{thorne2021database,thorne2021natural}).

%For example, if the user wants to generate image captions, the database could be a collection of images with corresponding captions. If the user wants to perform sentiment analysis on customer reviews, the database could be a collection of text reviews with corresponding ratings.

\subsection{Models}

\paragraph{Generative Language Model}
The Generative Language Model component of the framework is responsible for generating textual prompts based on the input \textit{Task Description} and \textit{Query}.
Specifically, it receives a string representing the task to be performed (\textit{Task Description}) and a query (\textit{Query}) that represents the user's request.
The Generative Language Model then generates a textual prompt that is relevant to the query and the task.

\paragraph{Retriever}
The Retriever component of the framework is responsible for retrieving relevant data from the Database based on the user's query.
We refer to the Retriever outputs as support set (as in \cite{thorne2021database,thorne2021natural}).
A support set is a subset of the data from the Database that either directly answers the given query or contributes to the final answer.

%

%For example, given a database  of facts that contains

%one or more support sets, which are subsets of the data from the Database that are relevant to the user's query.

%The support set is then used, together with the textual prompt, to build the input  that is relevant to the user's query and the task at hand.


\paragraph{Prompt Aggregator}
This component is responsible for processing the input required by the \textit{Reasoner}. 
In its simplest form, it just needs to concatenate the prompt generated by the Generative Language Model with the Support Set provided by the Retriever. 
However, in a more complex version, it may need to rework the prompt based on the number of support sets received to ensure that the LLM can provide a coherent response.
For example, if the Retriever provides two support sets, the Prompt Aggregator may need to split the prompt into two parts and concatenate each part with one of the support sets.

\paragraph{Reasoner}
The Reasoner is responsible for generating the answer to the user's query based on the final prompt generated by the Prompt Aggregator.
The Reasoner can be a pre-trained model like GPT or a custom-trained model specific to the task at hand.
The output of the LLM is a textual response, which can be further parsed to comply with the intended output.


\subsection{Reinforcement Learning}
The Reinforcement Learning (RL) part of the framework is responsible for fine-tuning the Generative Language Model (GLM) and Retriever based on the computed reward.
The RL is a crucial part of RRAML, it will be used to constantly improve the GLM and Retriever.
As mentioned earlier, the retriever will get a penalty if some of his recommendations will leads the Reasoner to a hallucinate, for example by adding damaging documents.
The RL allows use to integrate augment the signals in the training of these model, going beyond the data present in their training set, ensuring that they are aligned with the environment (i.e., the reasoner and the final task).

\paragraph{Reward}
The reward function can be defined based on the similarity between the generated output and the expected output and it can be estimated by training a Reward Model \cite{schulman2017proximal}.


\paragraph{RL algorithm}
The specific RL method which can be used is Deep Q-Networks (DQN) \cite{mnih2015human}, which is a model-free RL algorithm that learns to maximize the cumulative reward over time.
DQN combines Q-Learning, which is a RL algorithm that learns the optimal action-value function, with a Deep Neural Network to approximate the action-value function.
In the proposed framework, DQN is used to train the Generative Language Model and the Retriever to maximize the reward obtained from user feedback.
The update process is performed by backpropagating the reward signal through the neural networks using Stochastic Gradient Descent (SGD). The weights of the neural networks are updated in the direction that maximizes the expected reward, using the Q-Learning update rule. 
The update is performed iteratively until convergence, which is achieved when the expected reward stops improving.


\paragraph{Human-in-the-loop}
Human preferences can be incorporated into our ML system by allowing users to provide feedback on the system's output.
This feedback will be used to compute the reward for the RL algorithm and will help improve the performance of the overall system over time.
We acknowledge that some tasks may not have a clear expected output or may require additional context that is not available in the input data. 
In these cases, we will leverage human-in-the-loop approaches to provide additional context and guidance to the system. 
For example, crowd-sourcing platforms or internal subject matter experts can be used to provide feedback on the system's output and help train the model on more complex tasks.


%USE-CASE(S) ALMENO UNO


\section{Related Work}

Recent years have seen the emergence of large language models. Starting from the first Generative Pre/Training Model, better known as GPT \cite{radford2018improving}, these kinds of large language models have rapidly improved.
GPT-4 \cite{openai2023gpt4} is the most recent iteration, but in the meanwhile, many have rushed to propose their own version.
Google has recently released BARD\footnote{\url{https://bard.google.com/}}, while Meta has proposed their own take on LLM with LLaMA \cite{touvron2023llama}.
The research community has also capitalized its effort by releasing several open source LLM of different sizes, like Bloom \cite{scao2022bloom}, Dolly\footnote{\url{https://github.com/databrickslabs/dolly}}, and RWKV \cite{PENG_RWKV-LM_2021}.
However, all these models fail to scale to a larger context size, either by excessive computational costs or by ``losing it in the middle", as shown in \cite{liu2023lost}.
\\
To address this context-length limitation, some have tried to incorporate external knowledge into LLMs \cite{ghazvininejad2018knowledge,dinan2018wizard,peng2023check}.
In particular, in ``Retrieval-enhanced machine learning" \cite{zamani2022retrieval}, authors have envisioned a framework in which retrieval systems can enhance the performance of a machine learning model. 
More recently, there have been attempts of jointly training retrieval models with LLMs \cite{lewis2020retrieval,zhang2022retgen}, notably, the line of research on neural databases, in which the authors tried to replace a traditional database with a neural framework removing the need for a schema \cite{thorne2021natural,thorne2021database,trappolini2023multimodal}.
However, all these works assume full access to the reasoner module, which is not the case for most users in practice.
\\
To overcome this limitation, many have tried to craft systems that are able to deliver an optimized prompt that is input to the LLM.
For instance, the research conducted by \cite{lu2021fantastically} demonstrated a substantial influence of the sequence in which prompts are presented on the ultimate performance of the task. Meanwhile, a study by Nie et al. \cite{nie2022improving} highlighted that the performance is susceptible to the arrangement of the examples in the prompt, prompt templates, and the in-context instances in the prompt.
Lester et al. \cite{lester2021power} suggested a method to enhance task performance by adding adjustable tokens during fine-tuning. 
LLM-AUGMENTER iteratively revises \cite{peng2023check} to improve the model response.
\\
All the works introduced above do not improve on the retriever, which is assumed fixed.
In our work, we propose to finetune the retriever in conjunction with the reasoner to improve on results.
Since the feedback is non-differentiable we resort to reinforcement learning.
In particular, recent formulation such as Proximal Policy Optimization (PPO) \cite{engstrom2020implementation} make use of a differentiable neural reward module to include and account for generally non-differentiable feedback, like in the case of reinforcement learning with human feedback (RLHF).

%Sezione Discussion?
%elenco puntato di improvements / qualità del sistema
% gio: per me no, ci sono le conclusioni 

\section{Conclusions}
In conclusion, RRAML provides a promising framework for building intelligent interfaces to interact with large language models like GPT. By combining a generative language model with a retriever, this approach can effectively improve the performance of language models and help them understand user intents better.

However, this approach also comes with several challenges and uncertainties, such as the need for a large amount of training data, the potential for bias in the data and models, and the difficulty of balancing the trade-offs between generative and retrieval-based approaches.

Despite these challenges, RRAML holds great promise for creating more intelligent, natural, and effective interfaces for interacting with language models. We hope that this paper has provided a useful overview of this approach and its potential applications, and we look forward to further research and development in this exciting area.


\clearpage

% . To address this limitation, various works
% augment LLMs with knowledge consisting of
% e.g., personalized recommendations (Ghazvininejad et al., 2017), Wikipedia article and web search
% (Dinan et al., 2018; Shuster et al., 2022), structured and unstructured knowledge of task-oriented
% dialog (Peng et al., 2022). Recent advances have
% focused on jointly finetuning the retriever and generation components of retrieval-augmented text
% generation systems (Lewis et al., 2020; Zhang
% et al., 2021), but these methods are not applicable
% to black-box LLMs.
% More recent work attempts to combine blackbox LLMs with external knowledge, such as
% incorporating external knowledge into prompts
% (Madaan et al., 2022; Lazaridou et al., 2022), making GPT-3 more faithful (He et al., 2022), and
% combining web knowledge with GPT-3 (Nakano
% et al., 2021). In very recent works related to ours,
% Shi et al. (2023) tune the ranker of a black-box
% LLM. Schick et al. (2023) tune black-box LLMs’


% access to different APIs and show improvement
% on a variety of understanding and reasoning tasks.
% We consider these works to complementary to
% ours, as we assume our set of APIs to be given
% and fixed, and we instead focus more on when and
% what APIs to request, interactive feedback with
% the LLM, and developing a self-learning ability
% through utility functions.















% \section{First Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% % Figure environment removed

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{artsiomdang}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{bib}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}
\end{document}
