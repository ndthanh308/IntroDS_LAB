\documentclass[a4paper,
               11pt,
               pdftex,
               normalheadings,
               headsepline,
               footsepline,
               onecolumn,
               headinclude,
               footinclude,
               DIV14,
               abstracton]
{scrartcl}

\usepackage
{
    graphicx,
    amssymb,
    amsmath,
    amsthm,
    xcolor,
    dsfont,
    algpseudocode,
    authblk,
}

\usepackage[ngerman, english]{babel}

\usepackage[
left=25mm,
right=25mm,
top=20mm,
bottom=35mm
]{geometry}

%\usepackage[pagewise]{lineno}\linenumbers

\usepackage{subcaption}

%\usepackage[bf]{caption}
\captionsetup{format=plain}
%\usepackage{subfig}

\usepackage[colorlinks,
            pdffitwindow=false,
            plainpages=false,
            pdfpagelabels=true,
            pdfpagemode=UseOutlines,
            pdfpagelayout=SinglePage,
            bookmarks=false,
            colorlinks=true,
            hyperfootnotes=false,
            linkcolor=blue,
            citecolor=green!50!black]
{hyperref}

\usepackage{bm}
\usepackage{cleveref}

\usepackage{enumitem}
\usepackage{algorithm}

\usepackage{tabularx}

\usepackage{todonotes}
\usepackage[normalem]{ulem}
\newcommand{\revSout}[1]{\textcolor{blue}{\sout{#1}}}
\newcommand{\rev}[1]{\textcolor{black}{#1}}
\newcommand{\karl}[1]{\textcolor{green}{#1}}
\newcommand{\question}[1]{\textcolor{red}{#1}}
\newcommand{\manuel}[1]{\textcolor{purple}{#1}}

\newcommand{\Khat}{\hat{K}}
\newcommand{\Ktilde}{\tilde{K}}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Kcalhat}{\hat{\Kcal}}
\newcommand{\Kcaltilde}{\tilde{\Kcal}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
\newcommand{\identity}{\operatorname{id}}

\newcommand{\groupaction}[2]{{#1}\left[{#2}\right]}
\newcommand{\invgroupaction}[2]{{#1}^{-1}\left[{#2}\right]}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pderhigher}[3]{\frac{\partial^{#3} #1}{\partial #2^{#3}}}

%\DeclareMathOperator*{\limsup}{lim\,sup}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\conv}{conv}
\DeclareMathOperator*{\proj}{proj}
\DeclareMathOperator{\co}{co}
\DeclareMathOperator{\st}{s.t.}

\newcommand{\dint}[1]{\,\mathrm{d}#1}
\newcommand{\parder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\parderHigher}[3]{\frac{\partial^{#3} #1}{\partial #2^{#3}}}
\newcommand{\dt}{\dint{t}}

\newcommand{\lossfull}{\ell_{\mathrm{full}}}
\newcommand{\lossconv}{\ell_{\mathrm{conv}}}
\newcommand{\DFT}{\mathrm{DFT}}
\newcommand{\nin}{n_{\mathrm{in}}}
\newcommand{\nout}{n_{\mathrm{out}}}

% end example symbol
\newcommand\xqed[1]{\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill \quad\hbox{#1}}
\newcommand{\exampleSymbol}{\xqed{$\triangle$}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}%[theorem]
\newtheorem{lemma}{Lemma}%[theorem]
\newtheorem{proposition}{Proposition}%[theorem]
\newtheorem{definition}{Definition}%[theorem]
\newtheorem{remark}{Remark}%[theorem]
\newtheorem{example}{Example}[section]%{Example}
\newtheorem{assumption}{Assumption}%[theorem]

% ADD THE FOLLOWING COUPLE LINES INTO YOUR PREAMBLE
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
	\OLDthebibliography{#1}
	\setlength{\parskip}{0pt}
	\setlength{\itemsep}{0pt plus 0.3ex}
}

\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{\rev{Equivariance and partial observations in Koopman operator theory for partial differential equations}}
\author[1]{\normalsize Sebastian Peitz}
\author[1]{Hans Harder}
\author[2]{Feliks Nüske}
\author[3]{Friedrich Philipp}
\author[3]{Manuel Schaller}
\author[3]{Karl Worthmann}
\affil[1]{\normalsize Department of Computer Science, Paderborn University, Paderborn, Germany}
\affil[2]{\normalsize Max Planck Institute for Dynamics of Complex Technical Systems, Magdeburg, Germany}
\affil[3]{\normalsize Institute of Mathematics, Technische Universität Ilmenau, Ilmenau, Germany}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The Koopman operator has become an essential tool for data-driven analysis, prediction and control of complex systems\rev{. The main reason is} %being 
the enormous potential of identifying linear function space representations of nonlinear dynamics from measurements. 
\rev{This equally applies %concept is applied equally 
to ordinary, stochastic, and partial differential equations (PDEs). Until now, with a few exceptions only, the PDE case is mostly treated rather superficially, and the specific structure of the underlying dynamics is largely ignored.
In this paper, we show that symmetries in the system dynamics can be carried over to the Koopman operator, which allows us to massively increase the model efficacy. %efficiency.
Moreover, the situation where we only have access to partial observations---i.e., measurements, as is very common for experimental data---has not been treated to its full extent, either.} 
%Moreover, we address the pitfall associated with the ituation, that the classical EDMD algorithm does not automatically provide a Koopman operator approximation for the underlying system if we do not carefully select the number of observables. 
% \question{This sentence is not clear to me.} 
\rev{Moreover, we address the highly-relevant case where we cannot measure the full state, such that alternative approaches such as delay coordinates have to be considered. We derive rigorous statements on the required number of observables in this situation, based on embedding theory.}
%\revSout{We also briefly draw a connection to domain decomposition techniques for partial differential equations and} 
We present numerical evidence using \rev{various numerical examples including the wave equation and the Kuramoto-Sivashinsky equation}.
\end{abstract}

\section{Introduction}
Many phenomena in nature can be described by \emph{partial differential equations (PDEs)}, where the system state depends on both space and time. Popular examples are fluid dynamics or electromagnetics. Studying such systems poses many challenges, including \rev{(the derivation of)} sophisticated numerical discretization schemes (using, e.g., finite elements), a very high dimension of the resulting discretized nonlinear systems, and the challenge that in real experiments, the entire system state is accessible in few cases only. In addition, we only have very crude models---if at all---for some systems, e.g., from biology. %\question{Should we include references?}

Due to these reasons, there has been an increasing interest in the scientific community to develop and improve methods to infer and predict dynamical systems from data. Popular examples are the \emph{Sparse Identification of Nonlinear Dynamics} \cite{BPK16}, statistical approaches such as the \emph{Mori-Zwanzig} framework \cite{CHK00}, or techniques based on deep learning \cite{VBW+18}, also using physical knowledge \cite{RPK19}. Another approach that has been particularly successful in the past decade is the \emph{Koopman operator} framework \cite{Koo31,RMB+09,Mez13}. The driving force behind this renaissance is twofold: (1) The Koopman operator yields a linear representation of nonlinear dynamical systems, thus giving us access to powerful techniques from linear systems theory, and (2) the advances in numerical approximation (here in the form of the \emph{Extended Dynamic Mode Decomposition (EDMD)} \cite{WKR15,KKS16,KNP+20}) now allow us to identify the Koopman operator from measurements using simple linear regression.
As a consequence, the Koopman operator has been studied extensively for data-driven analysis \cite{BBKK22}, coarse graining \cite{BNC18,KNP+20,NKS21,NKBC21} and control \cite{PBK15,KM18,PK19,POR20,SWP+23,BGSW23} of a very large number of applications \rev{including neuroscience, social systems, robotics, and fluid dynamics}. Recent results on finite-data error bounds can be found in \cite{ZZ22,NPP+23,PSW+24,BML+23,KPS+24}. %\question{\cite{BML+23} correct?}

\rev{However, PDEs have until now mostly been treated in a rather ad-hoc manner, meaning that the observable consists of the state values at the grid nodes of some numerical discretization scheme \cite{RMB+09,Sch10}. Alternatively, kernel-based observable functions have been used---again with discretized full-state observables---to avoid dealing with the infinite-dimensional nature of PDEs \cite{WRK15,KPB16}. A machine-learning-based approach was presented in \cite{OR19}, where an autoencoder was trained to map the full-state of the PDE---using the values on the grid nodes---to a latent space of low dimension. In \cite{OPR24}, a method based on a Kalman filter was developed to avoid defining the observable function altogether.
When it comes to a more formal treatment of PDEs, the literature is much scarcer. In \cite{PK18}, for instance, the Koopman operator of the Burgers equation was derived analytically, albeit with the help of first linearizing the dynamics using the Cole-Hopf transformation.
The most formal treatment of Koopman operator methods for PDEs can be found in} \cite{NM20,Mau21}.

\rev{Despite its recent increase in popularity, there is until now} only little literature on Koopman operators for systems with symmetries; see, e.g., \cite{SER+19,sinha2020koopman} for ordinary differential equations, or \cite{WSGK22}, where the matrix approximation of the Koopman operator was used to heuristically identify symmetries in Markov Decision Processes. \rev{The central goal of this paper is thus to} provide a systematic treatment of symmetries in PDEs (Section \ref{sec:Equivariance}), which will allow us to significantly increase the numerical performance of the Koopman operator approximation. 

\rev{A second aspect that has until now received a mostly heuristic treatment, is the question of partial observations and unknown state spaces. Even though the operator can be approximated for many different types of observation functions, many articles simply use the \emph{full state observable} $f(y)=y$, which greatly simplifies the situation.} 
%
\rev{There are---of course---several exceptions. For instance, an attempt in this direction was proposed in \cite{CKH22} using nonlinear input transformations, or in \cite{OPR24}, where a Kalman filter was used to infer the state of an unknown system from measurements. Finally, delay coordinates have been successfully used in the so-called \emph{Hankel DMD} \cite{AM17}, also in various applications \cite{BBP+17,AM20,KKBK20}.}
However, there are severe pitfalls when we do not know or cannot efficiently discretize the system's state space. 
\rev{Most importantly, it is not clear how many measurements are required to obtain a reliable approximation of the Koopman operator associated with the system dynamics.}
Here, we will address the issue of unknown state spaces and/or partial measurements in detail and derive rigorous conditions on partial observations (Section \ref{sec:Partial}) that rely on fundamental embedding theorems. 
\rev{It should be noted that a similar approach was considered in \cite{BKK+17} for stochastic dynamical systems in the context of molecular dynamics.}

\rev{Finally, we show in Section \ref{sec:EqivPlusPartial} that a combination of these results allows us to design efficient schemes for EDMD, and how to transfer a learned Koopman approximation from one domain to another without retraining.}

%\todo[inline]{Add a discussion on the relation to \cite{BKK+17} when it comes to the embedding theory in Section \ref{sec:Partial}.}

\section{Koopman operator for PDEs}
We start by introducing the Koopman operator for partial differential equations, although everything that follows applies equally to ordinary differential equations (ODEs). Consider the general dynamical system
\begin{equation}\label{eq:PDE}
    \parder{y}{t} = \Ncal(y), \quad y(\cdot,0) = y_0.
\end{equation}
Here, $y(x,t)$ is the space- and time-dependent system state, $t \in \R_{\geq 0}$ is the time and $x\in\Omega$ the spatial coordinate, \rev{where $\Omega \subset \R^n$ is the spatial domain with boundary $\partial \Omega$. For simplicity of notation, we will restrict our analysis to one-dimensional spatial domains. However, extensions to higher dimensions are possible in a straight-forward manner. Moreover, the state at time $t$ is an element of a function space with appropriate differentiability (e.g., a Sobolev space $y(\cdot, t)\in H^p(\Omega)$)} and $\Ncal: D(\Ncal) \rightarrow H^p(\Omega)$ (with $D(\Ncal)\subset H^p(\Omega)$ being the domain of $\Ncal$) is a nonlinear partial differential operator describing the dynamics of the system. \rev{We will for now assume simple rectangular domains and periodic boundary conditions (BCs)}. The system's flow map $\Phi^\tau : L^2(\Omega)\to L^2(\Omega)$ is defined by
\[
    y(\cdot,t+\tau) = \Phi^\tau(y(\cdot,t)).
\]
\rev{We assume that the solution to \eqref{eq:PDE} exists and that it is unique. Furthermore,} we assume that the system possesses an invariant compact set $\Acal\subset D(\mathcal N)$, i.e., $\Phi^\tau(\Acal) = \Acal$, which has dimension~$\dim(\Acal)=d$ (often, the \emph{box counting dimension} is used in this context \cite{ZDG19}). 
%\FP{What is the dimension of a compact set?}. 
Usually, $\Acal$ is the system's \emph{attractor} or an \emph{invariant manifold}, and it is well known that many PDEs possess an attractor with $d<\infty$.
\begin{example}\label{ex:KS}
    The well-known \emph{Kuramoto--Sivashinsky} equation that we will study in this paper is, in dimensionless form, \rev{given by}
\begin{equation}\label{eq:KS}
    % y_t + 4y_{xxxx} + \mu \left[ y_{xx} + 0.5 (y_x)^2 \right] = 0
    \parder{y}{t} + 4\parderHigher{y}{x}{4} + \mu \left[ \parderHigher{y}{x}{2} + y \parder{y}{x} \right] = 0
\end{equation}
    on the domain $\Omega=(0,L)=(0,2\pi)$ with $\mu \in (0,\infty)$. Depending on $\mu$, the system exhibits rich dynamics, from bimodal fixed points to traveling waves to fully chaotic behavior \cite{HNZ86}. Moreover, one can bound the dimension of the attractor $\Acal$ in terms of the domain size $L$ via $d\leq L^{2.46}$ when not considering the non-dimensionalized version as we do in \eqref{eq:KS}, see \cite{ZDG19} for more details.
    %We here have used the shorter index notation for partial derivatives with respect to time and space.
\end{example}

The \emph{semigroup of Koopman operators} associated with \eqref{eq:PDE} is defined on a space of observable functionals, see~\cite{Mau21}.

\begin{definition}[Koopman semigroup and generator]\label{def:Koopman}
Consider the space $C(\Acal)$ of continuous real-valued functionals $f: \Acal \to \R$, endowed with the supremum norm $\norm{f} = \sup_{y\in\Acal}\abs{f(y)}$.
%To do so, we need to assume that we consider the dynamics of $\Phi^\tau$ on an 
%where the definition domain $\Acal \subset D(\Ncal)$ is invariant under $\Phi^\tau$. 
The \emph{semigroup of Koopman  operators} $(\Kcal^\tau)_{\tau\geq 0}$ associated with the semiflow $(\Phi^\tau)_{\tau\geq 0}$ is defined by
\begin{equation}\label{eq:Koopman}
\Kcal^\tau f = f \circ \Phi^\tau, \qquad f\in C(\Acal).
\end{equation}
The Lie generator of the semigroup is the linear operator $\Kcal: D(\Kcal) \rightarrow C(\Acal)$ that satisfies
\begin{align}\label{eq:Koopman_generator}
    \rev{\left(\Kcal f\right)(y) = \lim_{\tau\to 0} \frac{\left(\Kcal^\tau f\right)(y) - f(y)}{\tau}\qquad \forall y\in\Acal.}
\end{align}
\end{definition}

\color{black}
\begin{remark}
    The Lie generator of the Koopman semigroup has a close connection to generators of strongly continuous semigroups, with the distinction that the limit is defined in the strong sense in the latter case, see \cite[Remark 1]{Mau21} for a more detailed discussion and additional references.
\end{remark}
\color{black}

\begin{remark}\label{rem:vector}
An extension to vector-valued observable functions $f: \Acal \rightarrow \R^q$ in $C(\Acal)^q$ can be realized in a straightforward manner, see, e.g., \cite{BMM12}.
\end{remark}

In the case of a Koopman semigroup associated with a semiflow generated by the PDE \eqref{eq:PDE}, it follows from the chain rule that the generator is given by
\begin{equation}\label{eq:Koopman_DE}
    (\Kcal f)(y) = G_{\Ncal(y)}f(y),
\end{equation}
which can be interpreted as the Lie derivative associated with the infinite-dimensional vector field $\Ncal$, where $G_{\Ncal(y)}f(y)$ denotes the (linear) Gâteaux derivative of $f$ at $y$ in the direction $\Ncal(y)$. %\cite{NM20,Mau21}. 
Note that this is closely related to the generator PDE that we find for ODEs \cite{KNP+20}. An alternative derivation using the functional derivative can be found in \cite{NM20}.

% \question{Do we really want to use~$q$ in the following paragraph (not necessary? Moreover, I would first introduce the finitely-many observables $\{ \psi_j \}_{j=1}^\ell$ and, then, say that we want to approximate the compression $P_{\mathbb{V}} \mathcal{K}^\tau|_{\mathbb{V}}$, where $\mathbb{V} = \operatorname{span}\{ \psi_j| j \in \{1,\ldots,\ell\} \}$}
Even though the Koopman operator formalism has been known for a very long time, it has received a massive increase in attention in the past decade, mostly due to the advances in its numerical approximation via EDMD. Based on the observation that \rev{the operators in} Eqs.\ \eqref{eq:Koopman} and \eqref{eq:Koopman_generator} are linear, we can try to compute finite-dimensional approximations $K^\tau$ and $K$ of $\Kcal^\tau$ and $\Kcal$, respectively. We achieve this via Galerkin projection by introducing a finite dictionary $\{\psi_j\}_{j=1}^{\ell}$ of functions $\psi_j\in C(\Acal)$,
and coefficients $a\in\R^\ell$: 
\begin{equation}\label{eq:Galerkin}
f(y(\cdot,t)) \approx \sum_{j=1}^\ell a_j \psi_j(y(\cdot,t)) = a^\top \Psi(y(\cdot,t)).
\end{equation}
\rev{The goal is then to approximate the compression $P_{\mathbb{V}} \mathcal{K}^\tau|_{\mathbb{V}}$, where $\mathbb{V} = \operatorname{span}\{ \psi_j| j \in \{1,\ldots,\ell\} \}$, and $P_{\mathbb{V}}$ is the projection onto $\mathbb{V}$.}
Using time series data $[\Psi(y_0),\Psi(y_1),\ldots,\Psi(y_m)]$, where $y_i=y(\cdot,i\tau)$ and $\Psi = [\psi_1^\top,\cdots,\psi_\ell^\top]^\top$, we can now simply use linear regression to find the best fit matrix $K^\tau$:
\begin{equation}\label{eq:Koopman_regression}
\min_{K^\tau\in\R^{\ell \times \ell}} \sum_{i=0}^{m-1} \norm{\Psi(y_{i+1}) - K^\tau \Psi(y_i)}_2^2.
\end{equation}
A very similar regression problem can be formulated to approximate the generator $\Kcal$ via $K$ \cite{KNP+20}.
In both cases, it can be shown that this matrix converges to the Galerkin projection of $\Kcal^\tau$ in the infinite data limit $m\rightarrow\infty$ \cite{WKR15,KKS16}, and to the true Koopman operator when additionally $\ell\rightarrow \infty$ \cite{KM18b}. 
Moreover, finite-data error bounds can be found in \cite{Mez22,ZZ22,NPP+23,PSW+24,KPS+24}, using either i.i.d.\ or ergodic sampling. 
\rev{Most of them have until now considered ordinary or stochastic differential equations. A more general setting can be found in \cite{PSB+24}, which covers the PDE case as well.}
%\manuel{Die ICML-Arbeit erlaubt doch auch Abschätzungen für PDEs. Die könnten wir hier reinnehmen.}

\section{Equivariant Koopman operators for equivariant PDEs}
\label{sec:Equivariance}
\rev{It is a well-known fact that a large number of PDE systems exhibits symmetries, i.e., the dynamics are equivariant under certain group actions (to be specified shortly), which depend on the system dynamics $\Ncal$, as well as on the domain $\Omega$ and the boundary conditions on $\partial \Omega$. For instance, an alternative version of Eq.\ \eqref{eq:KS} from Example \ref{ex:KS} in coordinate-free form is}
\begin{equation}\label{eq:KS_alternative}
    \parder{y}{t} = \underbrace{- 4\Delta^2 y - \mu \left[ \Delta y + \frac{1}{2} \left| \nabla y \right|^2 \right]}_{=\Ncal(y)},
\end{equation}
%\question{Passt $\left| \nabla y \right|^2$}
\rev{
from which one can derive \eqref{eq:KS} by taking the derivative with respect to $x$ and substituting $\partial y / \partial x$ by $y$.
In Eq.\ \eqref{eq:KS_alternative}, we quickly see that the partial differential operator $\Ncal$ is equivariant under translations (for any spatial dimension) as well as rotations (in 2D or 3D). This means that shifting (or rotating) the solution $y$ and then applying $\Ncal$ yields the same result as shifting (or rotating) $\Ncal(y)$. The goal of this section is to show that the Koopman operator inherits such symmetries.}

\subsection{\rev{Some prerequisites on equivariant systems}}
In order to introduce the basic symmetry concepts, let us \rev{disregard} the time dependence of $y$ for a moment and instead denote the state at a fixed time $t$ by $y_t(x)=y(x,t)$ for $x\in\Omega$ 
%\FP{Was ist $\Omega$? Besser $[0,L]$? Oder wollen wir von Beginn an $\Omega\subset\R^n$ als spatial domain waehlen? Dann ist $\Omega = [0,L]$ nur ein Spezialfall.}. 
For a more detailed introduction, see \cite{BBCV21}. A \emph{group} is a set $G$ along with an associative composition operation $\circ : G \times G \rightarrow G, (g, h) \mapsto g h$ that contains an identity and inverses. 
%\FP{Muss man sowas Grundlegendes wirklich definieren?} 
A \emph{group action} of $G$ on a set $\Omega$ is then defined as a mapping $(g,x) \rightarrow \groupaction{g}{x}$ associating a group element $g\in G$ and a point $x\in\Omega$ with some other point in $\Omega$ in a way that is compatible with the group operations, i.e., $\groupaction{g}{(\groupaction{h}{x})}=\groupaction{(gh)}{x}$ for all $g, h \in G$ and $x \in \Omega$.
%
\begin{example}
    The Euclidean group $E(2)$ in the plane is the group of transformations of $\R^2$ that preserves Euclidean distances, \rev{i.e., it} consists of translations, rotations, and reflections. The same group can also act on the space of functions on the plane, that is, if we have a group $G$ acting on $\Omega$, we automatically obtain an action of $G$ on the space $\Ycal(\Omega)$: $\groupaction{g}{y_t}(x) = y_t(\invgroupaction{g}{x})$.
    % \begin{equation}\label{eq:group_action}
    %     (\groupaction{g}{z})(x) = z(\invgroupaction{g}{x}).
    % \end{equation}
    Due to the inverse on $g$, this is indeed a valid group action, in that we have 
    $(\groupaction{g}{(\groupaction{h}{y_t})})(x)=(\groupaction{(gh)}{y_t})(x)$.
\end{example}
%
% For the subset of linear group actions, we can define group representations $T_g : \Omega \rightarrow \Omega$. 
% %\FP{Soll $g\mapsto T_g$ besondere Eigenschaften haben? Was ist eine Group Representation? Und was ist eine linear group action?}
% %\textcolor{orange}{$T^g$ vs.\ $T_g$. Similar: group representation and transformation operator}
% If $T_g$ satisfies $y_t(x) = y_t(T_g x )$ for all $g \in G$ and $x \in \Omega$, then we say that $y_t$ is \emph{invariant} or symmetric to $T_g$ and that $\{T_g\}_{g\in G}$ is a set of symmetries of $y_t$. 
% A related notion is \emph{equivariance}. Given a transformation map $T_g : \Omega \rightarrow \Omega$ and some $y_t\in\Ycal$, we say that $y_t$ is equivariant to the transformation
% if there exists a second transformation operator $T'_g : \Ycal \rightarrow \Ycal$ in the output space of $y_t$ such that $T'_g y_t(x) = y_t(T_g x)$ for all $g \in G$, $y_t \in \Ycal$. 
% %\FP{Eben war $y_t$ noch fest. Deshalb verstehe ich die Definition auch nicht. Ich setze einfach $T_g'y = y\circ T_g$ und bin fertig. Oder was ist hier die Krux?} 
% The operators $T_g$ and $T'_g$ can be seen as describing the same transformation, but in different spaces. Invariance is a special case of equivariance, if we set $T'_g$ to the identity for all~$g$.
%
% Zum Beispiel:
%
\rev{
If $G$ acts on two sets $X$ and $Y$ and $f : X \rightarrow Y$ is some function, we say that $f$ is \emph{equivariant} if it satisfies $\groupaction{g}{f(x)} = f(\groupaction{g}{x})$, and we say that it is \emph{invariant} if it satisfies $f(x) = f(\groupaction{g}{x})$. For example, the flow $\Phi^\tau : \Ycal \rightarrow \Ycal$ is often times equivariant with respect to a given group action on $\Ycal$. }

\subsection{\rev{Related symmetry concepts in machine learning}}
\rev{Before addressing the particular case of Koopman operators in the following subsection, we would like to highlight that the concept of exploiting symmetries has recently gained a lot interest in a large number of sub-fields of machine learning research.
Starting with convolutional neural networks---which respect translational symmetries by design---many of these approaches can today be subsumed under the umbrella term \emph{geometric deep learning} \cite{BBCV21}. The unifying objective is to construct learning algorithms that respect known symmetries by design, which often leads to a significant increase in performance and a reduction in the required training data; see \cite{MBB+15,CW16,BBL+17} for a few early references and \cite{AGS21,BBCV21} for comprehensive overviews.
Further examples include reinforcement learning \cite{PWH+20,VRV+23,PSC+24} (also in combination with Koopman operators \cite{WSGK22}), reservoir computing \cite{PHG+18} or extreme learning machines \cite{HRV+24}.
}

\subsection{\rev{Equivariant Koopman operators}}
\rev{In order to formalize our discussion, we will consider observables $f$ that can be defined via convolutions. As a special case, this yields point measurements, but it covers many other settings as well.
We thus define a corresponding \emph{convolution operator}.}
%For simplicity, we will consider the 1D case here. 
Generally, a convolution is the composition of two functions which produces another function in a new coordinate. Usually, a function of interest (e.g., our system state $y$) is composed with a kernel $\theta$:
\begin{equation*}%\label{eq:convolution}
    %(y \star \theta)(s,t)= \int_{\Omega} y(x,t) \theta(s-x) \dint{x}.
    (y_t \star \theta)(s)= \int_{\Omega} y_t(x) \theta(s-x) \dint{x}.
\end{equation*}
In many situations, $\theta$ is a Gaussian kernel\rev{---i.e., $\theta=\exp(-\|s-x\| / \alpha)$, with kernel bandwidth $\alpha\in\R^{>0}$---}that somewhat ``localizes'' $y_t$ around the center $s$ (even though not in a strict sense, of course).
%
Now, fixing $s$, we can define the observable function
\begin{equation}\label{eq:observable}
    % f_s(y(\cdot,t)) = z(s,t) = (y \star \theta)(s,t).
    f_s(y_t) = (y_t \star \theta)(s).
\end{equation}

%\FP{Muessen wir in dieser Sektion bis hierhin wirklich immer das $t$ im Index mitschleppen? Es reicht doch, einfach überall nur $y$ statt $y_t$ zu schreiben, oder nicht?}

\begin{remark}\label{rem:conv}
    Eq.\ \eqref{eq:observable} is very general, as it includes observables modeling point evaluations (using a Dirac-Delta kernel) as well as integrals over parts of the domain. \rev{If we instead want to consider nonlinear functions of the state, we can still remain in the convolution setting by first applying a nonlinear function---e.g., in a point-wise manner---and then the convolution operation.}
    %An extension to more complex expressions (i.e., nonlinear functions of the full state $y$) can be realized in a straightforward manner by applying the convolution to a nonlinear transformation of the state.
\end{remark}

\color{black}
Let us now assume that our group action is a shift operation, $\groupaction{g}{x} = x-g~ \mathsf{mod}~L$, where $G$ is the group $[0,L)$ equipped with modular addition.
Furthermore, we assume that the partial differential operator $\Ncal$ defined by Eq.\ \eqref{eq:PDE} does not explicitly depend on space $x$ or time $t$.
We thus obtain shift equivariance of the right-hand side of the PDE and thus, the group action commutes with the flow $\Phi^t$ \cite[Remark 1]{OP21}:
% \begin{equation*}%\label{eq:eqiv_flow}
%     y(\invgroupaction{g}{x}, t) = \groupaction{g}{y}(x,t) = \Phi^t(\groupaction{g}{y}(x,0)) = \Phi^t(y(\invgroupaction{g}{x}, 0)).
% \end{equation*}
\begin{equation*}%\label{eq:eqiv_flow}
    \groupaction{g}{\Phi^\tau(y_t)} = \Phi^\tau(\groupaction{g}{y_t}),
\end{equation*}
where $\groupaction{g}{y_t}(x) = y_t(\invgroupaction{g}{x})$.
\color{black}
% Note that the group action $g$ formally transforms the function $y_t$. 
Using the convolution observable and the equivariance of the PDE, we find that the corresponding Koopman operator inherits the equivariance property.
\begin{theorem}
    Consider a PDE of the general form \eqref{eq:PDE} with periodic boundary conditions, where $\Ncal$ does not explicitly depend on space $x$ or time $t$ and is thus equivariant under translations in $x$ (and $t$) in view of the periodic boundary conditions. 
    Then, the Koopman operator associated with \eqref{eq:PDE} and the observable $f_s$ as defined in \eqref{eq:observable} is also equivariant under the same group action.
\end{theorem}
\begin{proof}
Introducing $\tilde{x} = x-g$, we get (under periodic BCs)
\begin{equation*}
\begin{aligned}
    f_{\invgroupaction{g}{s}}(y_t) &=  \int_{\Omega} y_t(x) \theta(s+g-x) \dint{x} \\
    &= \int_{\Omega} y_t(\tilde{x}+g) \theta(s-\tilde{x}) \dint{\tilde x}  \\
    &= \int_{\Omega} y_t(\invgroupaction{g}{\tilde{x}}) \theta(s-\tilde{x}) \dint{\tilde x} \\
    &= (\groupaction{g}{y_t} \star \theta)(s)= \groupaction{g}{(y_t \star \theta)}(s) \\
    &=\groupaction{g}{f_{(\cdot)}(y_t)}(s),
\end{aligned}
\end{equation*}
where we have exploited the linearity of both the convolution operation and the group action in line four in order to exchange the operations.
\rev{Moreover, we assume that $\theta$ is periodic as well.}
As a consequence, we also obtain equivariance of the action of the Koopman operator. For any $y_0\in\Acal$ with $y_1=\Phi^\tau\in\Acal$, we find
\begin{equation*}
    \begin{aligned}
    \left(\Kcal^t f_{\invgroupaction{g}{s}}\right)(y_0) &= f_{\invgroupaction{g}{s}} \left(\Phi^t(y_0)\right) \\
    &= \groupaction{g}{f_{(\cdot)}\left(\Phi^t(y_0)\right)}(s) \\
    &= \groupaction{g}{\left(\Kcal^t f_{(\cdot)}\right)(y_0)}(s).
    \end{aligned}
\end{equation*}
\end{proof}
What follows is that the same Koopman operator \rev{approximation} can be applied to different observables $f_{s_1}$ and $f_{s_2}$, where $s_2-s_1 = g \in \R$. We can thus compute a Koopman operator for some $f_{s}$, and apply the same operator of a shifted version of $f_{s}$.
%, which opens up the possibility to pursue \emph{domain decomposition} strategies as they are very common in finite element techniques or numerical solution approaches for PDEs in general. 
%\manuel{Discuss: Sollen wir den Kommentar so drin lassen? Ich hatte den eine Reviewer so verstanden, dass ihr oder ihm der Remark mit Domain Decomposition nicht so wirklich passt, wenn wir ihn nicht ausführen.} 
This way, we obtain the possibility to decouple the Koopman operator from a specific spatial domain $\Omega$. As long as we remain within domains with periodic boundary conditions, a transfer is possible in a simple and straightforward manner. Moreover, the equivariance can easily be extended to vector-valued convolution observables, cf.\ Remark \ref{rem:vector}.

\section{Partial measurements \& unknown state spaces}
\label{sec:Partial}
A large part of the existing literature focuses on small-scale systems or the situation where $f$ is the identity mapping, i.e., the \emph{full state observable} \cite{WKR15}. \rev{This means that the Koopman operator provides a linear system which advances the specific function (or functional) $f=\operatorname{Id}$ forward in time.} Alternatively, it is at least assumed that the state space (here: $L^2(\Omega)$) is known and that the observable $f$ can be numerically approximated in an efficient manner\rev{---exceptions being approaches based on Hankel DMD or Kalman filtering, as discussed in the introduction.} %\manuel{Ich würde vorschlagen, das nicht in der Footnote zu verstecken, da es ja die anderen Arbeiten würdigt.} 
% \footnote{\rev{There are---of course---several exceptions. For instance, an attempt in this direction was proposed in \cite{CKH22} using nonlinear input transformations, or in \cite{OPR24}, where a Kalman filter was used to infer the state of an unknown system from measurements. Finally, delay coordinates have been successfully used in the so-called \emph{Hankel DMD} \cite{AM17}, also in various applications \cite{BBP+17,AM20,KKBK20}.}}
%
However, this viewpoint has severe limitations. If, for instance, we consider PDEs, the state space may be challenging to approximate numerically. Moreover, if the data stems from real experiments, the domain may be unknown altogether. 
\rev{Finally---and this is central to efficient numerical approximations of equivariant Koopman operators---if we want to calculate local, equivariant EDMD models, then wee need to ensure that these local measurements yield an approximation that is consistent with the true dynamics.}


% In the following, we will consider exactly this situation, where we do not necessarily know $f$ or $\Ycal$. %For instance, this can be the case when we use experimental data from a PDE system, but we do not know the  exact geometry of the spatial domain. Another example might be a dynamical system on a large graph , where we collect measurements at a subset of the nodes, but we do not even know the overall graph structure.

\subsection{A common pitfall of partial measurements}
In the following, we will precisely consider the above-described situation where we do not necessarily know $f$ or the state space, not to mention the attractor $\Acal$. %Formally, 
%there still exists an observable 
\rev{On an abstract level, there is an observable}
$f:\Acal \rightarrow \R^q$, according to which we collect our measurements $z_i = f(y_i)$. However, as we are ignorant of $f$ or the domain $\Acal$, we appear to be at an impasse: we cannot define a dictionary $\{\psi_j\}_{j=1}^{\ell}$ in $C(\Acal)^q$, which is the key step in EDMD, cf.\ Eqs.\ \eqref{eq:Galerkin}--\eqref{eq:Koopman_regression}.

A practical means to overcome this impasse is to collect measurements $z=[z_0,z_1,\ldots,z_m]\in\R^{q \times (m+1)}$, where $z_i = f(y_i)$, and try to approximate the Koopman operator directly from the data. 
If the measurement is low-dimensional (i.e., $q$ is small) one often simply \emph{lifts} the data $z$ using a dictionary such as delay coordinates or polynomials with maximal degree $s$, i.e.,
\begin{equation}\label{eq:EDMD_CDS}
    \Psi(z) = \begin{bmatrix} 1 & z_1 & \ldots & z_q & z_1^2 & z_1 z_2 & \ldots & z_q^{s}\end{bmatrix}^\top_{~\cdot}
\end{equation}
Examples of this approach are, e.g., lift and drag measurements of a fluid flow \cite{PK19}, coarse-grained coordinates of large molecules \cite{NKBC21} or delay coordinates of highway traffic data \cite{AM20}.
%Examples of this can be found in, e.g., \cite{PK19}, \cite{NKBC21} or \cite{AM20}. 
%In the first case, $K^\tau$ was approximated from lift and drag measurements of a fluid flow alone, without explicit knowledge of the state space $\Ycal$ or the observable $f$. In the second case, the dynamics of lower-dimensional, coarse grained data was learned. Finally, in the last case, the Koopman operator for highway traffic data was derived using delay observables.
%
However, this approach provides a major pitfall when it comes to learning anything about the dynamics of the original system. 
Conceptually, we treat our measurements in such a way that we now use the full state observable on a different, implicitly defined dynamical system $\varphi^\tau: \R^q \rightarrow \R^q$ for the dynamics of the observed quantity $z$ on the artificial \emph{state space} $\R^q$:
\begin{equation}\label{eq:CDS}
    z_{i+1} = \varphi^\tau(z_i).
\end{equation}
Following \cite{DHZ16,ZDG19}, we will call \eqref{eq:CDS} the \emph{Core Dynamical System (CDS)}.

Assuming that the CDS exists and is uniquely defined, we can now define a new observable $h\in \Hcal= C(f(\Acal))^q$, $h: \R^q \rightarrow \R^q$ 
%\FP{Sicher, dass die nach $\R^q$ mappen soll?} 
whose domain is now the state space of the CDS. 
%\FP{Der sollte doch hoeher dimensional sein, wenn ich mir (8) angucke.} 
In this setting, we can simply apply EDMD in its standard form (Eqs.\ \eqref{eq:Galerkin}--\eqref{eq:Koopman_regression} in combination with a dictionary as in \eqref{eq:EDMD_CDS}), to identify the Koopman operator associated with the CDS and the observable function $h$.\footnote{For convenience, we will use the full state observable $h=\identity$ here, but our equivalence result in Theorem \ref{thm:Koopman_CDS} also covers the more general setting for arbitrary $h$.} This concept is illustrated in Fig.\ \ref{fig:Koopman_CDS}, where $z=h(z) = h(f(y))$, and $\{\psi_j\}_{j=1}^{\ell}$ spans a subspace of $\Hcal$ instead of $C(\Acal)$.

%However, can we really say that this system possesses the same dynamics as the original one?
% We need embedding results (Takens, Whitney, Robinson, ...)
%Core Dynamical System (CDS) \cite{DHZ16,ZDG19} ensures a one-to-one relation, cf.\ Fig.\ \ref{fig:Koopman_CDS}.

% Figure environment removed




\subsection{Relation between the Core Dynamical System and the underlying PDE}
What remains to be shown is the correspondence between the original dynamical system $\Phi^\tau$ and the corresponding CDS $\varphi^\tau$. To this end, we closely follow the approach in \cite{ZDG19} and make use of well-known embedding theorems such as \cite{Whi36,Tak81,SYC91,Rob05}. 
%To do so, we need to assume that we consider the dynamics of $\Phi^\tau$ on an invariant compact set $\Acal$, i.e., $\Phi^\tau(\Acal) = \Acal$, which is of finite dimension $d$. Usually, $\Acal$ is the system's \emph{attractor} or an \emph{invariant manifold}, and it is well known that many PDEs possess an attractor with $d<\infty$.
%\begin{remark}
For a detailed discussion on the more intricate implications of the following theorem---as well as \rev{further references regarding} the terms \emph{prevalence}, \emph{box counting dimension} and \emph{thickness exponent} (which is $\sigma=0$ for many PDEs)---we refer the reader to \cite{ZDG19}.

\color{black}
\begin{definition} %[\cite[Definition 2.1]{ZDG19}]
    [\cite{ZDG19}, Definition 2.1]
    ~
    \begin{enumerate}[label=(\alph*)]
        \item A Borel subset $S$ of a normed linear space $V$ is \emph{prevalent} if there is a finite dimensional subspace $E$ of $V$ (the ``probe space'') such that for each $v\in V$, $v+e$ belongs to $S$ for (Lebesgue) almost every $e\in E$. Following a remark made in \cite{SYC91}, we will say that almost every map in a function space $V$ satisfies a certain property if the set of such maps is prevalent, even in the infinite dimensional case. Then this property will be called \emph{generic} (in the sense of prevalence).
        %
        \item Let $Y$ be a Banach space, and let $\Acal \subset Y$ be compact. For $\varepsilon > 0$, denote by $N_Y(\Acal, \varepsilon)$ the minimal number of balls of radius $\varepsilon$ (in the norm of $Y$) necessary to cover the set $\Acal$. Then 
        \[
            \dim(\Acal) = \limsup_{\varepsilon \rightarrow 0} \frac{\log N_Y(\Acal, \varepsilon )}{-\log\varepsilon} = \limsup_{\varepsilon \rightarrow 0} -\log_{\varepsilon} N_Y(\Acal, \varepsilon)
        \]
        denotes the \emph{upper box counting dimension} of $\Acal$. 
        %
        \item Let $Y$ be  a  Banach space,  and let $\Acal \subset Y$ be  compact.  For $\varepsilon > 0$,  denote  by $d_Y(\Acal, \varepsilon)$ the minimal dimension of all finite dimensional subspaces $V\subset Y$ such that every point of $\Acal$ lies within distance $\varepsilon$ of $V$. If no such $V$ exists, $d_Y(\Acal, \varepsilon) = \infty$. Then 
        \[
            \sigma(\Acal ,Y) = \limsup_{\varepsilon \rightarrow 0} - \log_{\varepsilon} d_Y(\Acal, \varepsilon )
        \]
        is called the \emph{thickness exponent} of $\Acal$ in $Y$.
    \end{enumerate}
\end{definition}
\color{black}


\begin{theorem}[\cite{Rob05,ZDG19}]\label{thm:embedding} 
Let $\Acal \subset \Ycal$ be a compact and invariant set with upper \emph{box counting dimension} $\dim(\Acal)=d$ and \emph{thickness exponent} $\sigma$. Choose an integer $q > 2(1 +\sigma )d$, and suppose further that the set $\Acal_p$ of $p$-periodic points of $\Phi^\tau$ satisfies $\dim(\Acal_p) < p/(2 + 2\sigma )$ for $p= 1,...,q$. 
%\FP{Verstehe ich nicht. Soll das $Y$ ein $\Ycal$ sein? Dann haette ich gedacht, dass $\Acal_p\subset\Ycal$ ist. Dann ist aber die Distanz oben immer Null.} 
Then for almost every (in the sense of \emph{prevalence}) Lipschitz map $g:\Ycal\rightarrow \R$ the \emph{delay observation map} $f:=D_q[g,\Phi^\tau]: \Ycal \rightarrow \R^q$ defined by 
\begin{equation*}
    %\small
    %f:=D_q[g,\Phi^\tau](y) = {
		y\mapsto\begin{bmatrix}
        g(y) & g(\Phi^\tau(y)) & \ldots & g(\Phi^{(q - 1)\tau}(y)
    \end{bmatrix}^\top
\end{equation*}
is one-to-one on $\Acal$. The same holds for a set of $q$ distinct observables $g_1,\ldots,g_q : \Ycal\to\R$, i.e.,
\begin{equation*}%\label{eq:embedding2}
    f = \begin{bmatrix}g_1(y) & \ldots & g_q(y)\end{bmatrix}^\top.
\end{equation*}
%\FP{Also, ich finde, hier geht einiges durcheinander. Jetzt ist der state space $\Ycal$ ein allgemeiner Banachraum. Dann fragt sich, was periodische Randbedingungen sein sollen. Also, ich wuerde sagen: entweder wir passen Thm 4 an die spezielle Situation mit $\Ycal = L^2(0,L)$ an oder machen alles allgemein auf einem Banachraum von Funktionen $\Ycal$ auf $[0,L]$ (vorher wurde $\Ycal$ eingefuehrt als beliebiger separabler Hilbertraum). Mir ist das gleich. Es sollte nur einheitlich sein.}
\end{theorem}

\begin{remark}
The \emph{\textbf{central message}} of the above theorem is that we can draw a close connection between the observable $f$ 
%\FP{Was genau ist hier mit "Definition" gemeint?} 
in the Koopman setting (Definition \ref{def:Koopman} and Remark \ref{rem:vector}) and the observation map $g$ in the embedding framework. The key statement for our purposes is that if the system dynamics of $\Phi^\tau$ is restricted to a compact set $\Acal$ with a finite dimension $d$, then we need to have at least $q>2d$ distinct measurements $g$---which jointly form the Koopman observable $f$---to obtain a one-to-one correspondence between $\Phi^\tau$ and $\varphi^\tau$:
\begin{equation*}%\label{eq:CDS_PDE}
\Phi^\tau = E \circ \varphi^\tau \circ D_q[g,\Phi^\tau] = E \circ \varphi^\tau \circ f,
\end{equation*}
where $E$ is the inverse of $f$. This way, the CDS becomes:
\begin{equation}\label{eq:CDS_PDE}
    \varphi^\tau = f\circ\Phi^\tau\circ f^{-1}.
\end{equation}
%\FP{\rm "can be interpreted" ist mir zu schwammig. Ich denke, $E$ {\em ist} die Inverse von $f$, definiert auf $f(\Acal)$. Oder? Das passt auf jeden Fall, wenn man das dynamische System $\varphi^\tau$ auf $f(\Acal)$ definiert als $\varphi^\tau = f\circ\Phi^\tau\circ f^{-1}$. Dann können wir auch $\Hcal$ definieren als $C(f(\Acal))^p$.}
\end{remark}

As a consequence, we can relate the Koopman operator for $\varphi^\tau$ to the Koopman operator for $\Phi^\tau$.

\begin{theorem}\label{thm:Koopman_CDS}
Let the assumptions of Theorem \ref{thm:embedding} hold and define the Koopman operator $\Kcal^\tau$ for the PDE \eqref{eq:PDE} in its standard form as in \eqref{eq:Koopman}. Furthermore, define the Koopman operator for the CDS $\varphi^\tau$ with observable $h:\R^q \rightarrow \R^p$ as follows:
\[
    \Kcalhat^\tau h = h \circ \varphi^\tau, \qquad h\in\Hcal.
\]
%\FP{Wie gesagt: $\Hcal$ muss definiert sein.} 
Then $h\circ \Kcal^\tau f = \Kcalhat^\tau h \circ f$. 
Moreover, we find that $\Kcal^\tau$ and $\Kcalhat^\tau$ share the same spectrum, and the eigenfunctions are related via $f$. 
\end{theorem}
\begin{proof}
The proof immediately follows from the one-to-one correspondence established in Theorem \ref{thm:embedding}, which means that for every $z\in f(\Acal)$ we find exactly one $y\in\Acal$ such that $f(y)=z$. Choose an arbitrary $y_0\in\Acal$ with $y_1=\Phi^\tau(y_0)\in \Acal$ (we have $y_1\in\Acal$ due to the invariance of $\Acal$). Then
\begin{equation*}
    \begin{aligned}
        % (\Kcal^\tau f)(y_0) &= f(\Phi^\tau(y_0)) = f(y_1)=z_1 = h(z_1) \\
        % &= h(\varphi^\tau(z_0)) = (\Kcalhat^\tau h)(z_0) = (\Kcalhat^\tau h)(f(y_0)) \\
        % &= (\Kcalhat^\tau h \circ f)(y_0)
        h((\Kcal^\tau f)(y_0)) &= h(f(\Phi^\tau(y_0))) = h(f(y_1)) = h(z_1) \\
        &= h(\varphi^\tau(z_0)) = (\Kcalhat^\tau h)(z_0) = (\Kcalhat^\tau h)(f(y_0)) \\
        &= (\Kcalhat^\tau h \circ f)(y_0).
    \end{aligned}
\end{equation*}
%Setting $h=\identity$ immediately yields the desired special case. 
%\FP{Ich wuerde $\operatorname{id}$ statt $Id$ benutzen, da Letzteres meistens fuer die Einheitsmatrix steht.}
For the spectrum, consider an eigenfunction $\hat\xi$ with associated eigenvalue $\hat\lambda$ such that
\begin{align*}
    \Kcalhat^\tau \hat\xi = \hat\lambda \hat\xi.
\end{align*}
Then, using Eq.\ \eqref{eq:CDS_PDE} and introducing $\xi = \hat\xi\circ f$, we find
\begin{align*}
    \hat\xi \circ \varphi^\tau = \hat\xi \circ f\circ\Phi^\tau\circ f^{-1}&= \hat\lambda \hat\xi \\
    \Leftrightarrow \qquad \hat\xi \circ f\circ\Phi^\tau &= \hat\lambda \hat\xi \circ f \\
    \Leftrightarrow \qquad \quad~~\xi\circ\Phi^\tau &= \hat\lambda \xi.
\end{align*}
\end{proof}
\begin{remark}
    For $h=\identity$, we find $\Kcal^\tau f = \Kcalhat^\tau h \circ f$.
\end{remark}
\begin{remark}
    \rev{Note that Theorem \ref{thm:Koopman_CDS} does not address the question of the approximation properties of the operators $\Kcal^\tau$ and $\Kcalhat^\tau$. The existence of a one-to-one relationship is very helpful in determining the required number of observables---provided that the attractor dimension can be estimated---but it is hard to assess whether the embedded dynamics are simpler or harder to approximate from a purely numerical perspective. This warrants further investigation, which is out of the scope of this paper.}
\end{remark}

\begin{remark}[\rev{Coarse graining of ODEs}]
    \rev{Even though we have only considered PDEs here,}
    similar challenges occur in large-scale systems of ODEs such as molecular dynamics, agent-based systems or dynamics on graphs (e.g., electric grids, where the entire graph is not necessarily known). There, a coarse graining to meaningful macro observables \cite{ZHS16,BKK+17,BNC18,NKS21,NKBC21} is highly desirable, at the cost of \rev{losing} knowledge of the underlying dynamical equations.
\end{remark}

\section{Combining equivariance and partial measurements}
\label{sec:EqivPlusPartial}
\rev{We will now demonstrate the simultaneous application of both above-mentioned concepts, meaning that we will compute Koopman operators based on point measurements from a small, connected subset of $\Omega$, which preserve the equivariance properties of the underlying PDE.}
We will compare local Koopman models $\Kcalhat^\tau$---obtained from $q$ point measurements located in a compact subset of $[0,L]$ (e.g., neighboring grid points in the discretization)---to a global Koopman model $\Kcal^\tau$ which is obtained using the classical full state observable (i.e., we observe the entire numerical grid at once). As briefly mentioned above, these point measurements can be interpreted in terms of Eq.\ \eqref{eq:observable} by considering a Dirac delta function as the kernel. This way, we obtain 
\[ \begin{bmatrix} z_{t,1} = f_{x_1}(y_t)=y_t(x_1) & \ldots & z_{t,N} = y_t(x_N) \end{bmatrix}. \]

% Figure environment removed

We will compare these two models both regarding the Koopman spectrum and the prediction accuracy. In the latter case, Fig.\ \ref{fig:Koopman_Conv} (a) illustrates how the local models $\Kcal^\tau$ can be combined to a global model of repeating entries. Conceptually, we simply obtain a number of entirely independent predictors for subsections of $\Ycal$. As we always have to deal with approximations of $\Kcalhat^\tau$, it is clear that the approximate solution can quickly lead to inconsistencies with respect to the PDE state, for instance by developing artificial discontinuities. The question is therefore whether we can establish a link between the different local models. A first intuitive approach would be to simply consider overlapping domains. For instance---considering the example of a local model of size three---we can apply the same $\Kcalhat^\tau$ to $\Psi((z_1,z_2,z_3))$ and to $\Psi((z_2,z_3,z_4))$.
However, this means that we effectively obtain multiple predictors for the same quantity (in the previous example, $z_2$ and $z_3$ are contained in both models). While it is certainly possible to project each of the local systems back onto the coordinates $z$ and then use the average as the predictor, we would still not achieve a coupling between different model instances.
\begin{remark}
    In fact, if we were able to obtain an exact finite-dimensional approximation of $\Kcalhat^\tau$, then we would quickly run into inconsistencies, as the same matrix would be a predictor for $(z_1,z_2,z_3)$ and for $(z_2,z_3,z_4)$. A quick calculation then shows that the prediction of any $z_i$ cannot depend on any other $z_j$, which would mean that $K^\tau$ is just a diagonal matrix. Our interpretation of this dilemma is that inconsistencies cannot be avoided, as in the generic situation, any finite-dimensional approximation is inexact, meaning that the local Koopman models have to be globally inconsistent or yield trivial dynamics.
    \rev{Note that this situation is different for more sophisticated observable functions such as Fourier modes or Koopman eigenfunctions---these, however, do not allow for shift-equivariance as we consider here.}
\end{remark}
Instead, a coupling can be achieved in two different ways. The first option is to treat the connection between two neighboring models as it is done in the Dynamic Mode Decomposition with control (DMDc, \cite{PBK15}), meaning that using a slightly modified regression problem, we obtain a model of the form
\begin{equation}\label{eq:DMDc}
    \begin{aligned}
    \Psi((z_{t+\tau,2},z_{t+\tau,3},z_{t+\tau,4})^\top) \approx \Khat^\tau \Psi((z_{t,2},z_{t,3},z_{t,4})^\top) &+ B_l z_{t,1} + B_r z_{t,5},
    \end{aligned}
\end{equation}
where the two terms $B_r, B_l\in\R^{q\times 1}$ are used to treat the left and right neighbors as control inputs to the dynamics.

As the above approach only works for control inputs that enter the original system in a purely linear fashion \cite{NPP+23}, an alternative approach is to accept that a purely linear approach may be too much to ask for. Instead, a coupling can be achieved by predicting the next state, project from $\Psi(z)$ onto the coordinates $z$, and then lift again for each system. Due to the project-then-lift step, we ``synchronize'' the local systems, at the cost of obtaining nonlinear dynamics. \rev{Similar observations in terms of trading the linearity for accuracy have been made in, e.g., \cite{CRLG23}}. Note that if we use kernel EDMD \rev{with a radial basis function kernel}, then this approach \rev{possesses a relation to Gaussian Process models}.

\subsection{Numerical setup}
In the following, we will study the proposed approaches using the Kuramoto-Sivashinsky equation for two different parameter values $\mu>0$.
For the data generation process, we numerically solve \eqref{eq:KS} using the spectral Galerkin method implemented in the open source package \emph{shenfun} \cite{Mor18}. As a spatial discretization of $\Omega$, we use $N=32$ Fourier modes, which is equivalent to $N=32$ equidistant grid points, i.e., we have $\Delta x = \frac{L}{N} = \frac{\pi}{16}$. The time step for the PDE solver is $\Delta t=0.01$, and we set $\tau = 20\Delta t = 0.2$ for the traveling wave and $\tau = 5 \Delta t = 0.05$ for the bimodal fixed-point setting. We collect $M=1000$ samples from the attractor $\Acal$, which yields a sufficient coverage for the considered $\mu$ values.

In our experiments, we first compare the PDE solution to the approximation $K$ of the global Koopman operator $\Kcal$, where $f(y)=y$. We then compare this to local Koopman models $\Khat$ both in terms of the Koopman spectrum as well as regarding the prediction accuracy. 
For the latter, we construct a global model $\Ktilde$ from the local $\Khat$. We do so following both the classical approach (Fig.\ \ref{fig:Koopman_Conv} (a)) and the DMDc approach (Fig.\ \ref{fig:Koopman_Conv} (b)).
Following Theorem \ref{thm:embedding}, we study different embedding dimensions, where $q_w$ is the \emph{window width} (i.e., the number of neighboring points of the discretized PDE), $q_d$ is the number of delays, and the total dimension is $q=q_w\cdot q_d$. The global Koopman model $K$ is thus a special case of $\Khat$ where $q_w=N$. We will use standard DMD (i.e., $\Psi=\identity$) in all experiments.

\subsection{Traveling wave ($\mu=15$)}
At $\mu=15$, the system exhibits a traveling wave solution, as shown in Fig.\ \ref{fig:KS_mu15_PDE_vs_K}. As this is a very simple behavior, we do not study delay coordinates and set $q_d=1$, i.e., $q=q_w$. We see in Fig.\ \ref{fig:KS_mu15_PDE_vs_K} (bottom) that the DMD approximation (i.e., $\Psi=\identity$) is sufficient to yield accurate long-term predictions, even though a slow decay is visible after a while.
% Figure environment removed 
% Figure environment removed

We next compare $K$ and the local Koopman model $\Khat$ for different values of $q_w=q$, varying between $q=1$ and $q=16$, which is half of the domain. The corresponding spectra are compared in Fig.\ \ref{fig:KS_mu15_spectra}, and we observe a very good agreement for the leading eigenvalues (the lowest frequency corresponds to the frequency of the traveling wave).

The prediction of the state $y$ using the reconstructed Koopman operator $\Ktilde$ (according to Fig.\ \ref{fig:Koopman_Conv} (a)) is depicted in Fig.\ \ref{fig:KS_mu15_predictions}. As discussed before, the complete decoupling of the local models $\Khat$ yields globally inconsistent dynamics caused by small prediction errors. This is most evident for $q=4$ and $q=8$. At $q=16$, the approximation is sufficiently accurate that we do no longer observe this phenomenon. Interestingly, this decoherence is quite severe even though the one-step prediction error reaches a very small value already at $q=4$, cf.\ Fig.\ \ref{fig:KS_mu15_errors}.

% Figure environment removed

% Figure environment removed

% Figure environment removed

Unsurprisingly, the prediction accuracy improves massively when we build in a coupling term according to Fig.\ \ref{fig:Koopman_Conv} (b) and Eq.\ \eqref{eq:DMDc}. Using a single input from left and right, we obtain high-quality predictions using a very small linear system ($q=1$ and two inputs), see Fig.\ \ref{fig:KS_mu15_DMDc}.

\subsection{Bimodal fixed point ($\mu=18$)}
As a second system, we study the parameter $\mu=18$ which results in a ``checkerboard'' pattern. As the dynamics are now much more complex (due to the equivariance w.r.t.\ translations in space, the attractor consists of infinitely many checkerboard patterns), we here additionally consider $q_d=50$ delay observations. Using this higher-dimensional observable, many results from the previous case hold in a very similar fashion. Fig.\ \ref{fig:KS_mu18} shows a comparison of different approximations, more figures can be found in the appendix.
% Figure environment removed

\section{Conclusion}
We have presented two extensions to the current Koopman theory that deal with (i) the issue of not knowing the system's state space (i.e., using partial measurements, for instance from sensors) and (ii) the exploitation of symmetries when setting up Koopman-based surrogate models.
Regarding part (i), we have shown that there exists a close connection between the Koopman observable function $f$ and observation maps as they are defined in the embedding literature (Takens, Whitney, ...). If we observe sufficiently many points (more than two times the attractor dimension), then we can simply treat our measurements as if they have been generated by another dynamical system with state space $\R^q$, on which we can then use standard EDMD techniques. This yields rigorous criteria for the situation when building Koopman models exclusively from partial measurements.
For part (ii), we have then exploited this in order to simply use $q$ local measurements to build a local Koopman operator approximation.
Our numerical results show that including coupling terms (in the spirit of DMD with control) significantly increases the accuracy, even for very small model sizes.
\rev{The extension towards other symmetry groups as well as to two or three-dimensional spatial domains us currently under investigation, with a strong focus on equivariant convolution operators.}

\section{Acknowledgments}

S.P.\ and H.H.\ acknowledge financial support by the project ``SAIL: SustAInable Life-cycle of Intelligent Socio-Technical Systems'' (Grant ID NW21-059D), which is funded by the program ``Netzwerke 2021'' of the Ministry of Culture and Science of the State of Northrhine Westphalia, Germany. K.W.\ gratefully acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the Project-ID 507037103.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{alpha}
\bibliography{references}




\appendix
\section{Appendix}
\subsection{Additional plots for $\mu=18$}
We here show the same figures as for the traveling wave solution at $\mu=15$. The key difference in the numerical approximation is that we now consider delay observables, i.e., $f$ consists of $q_d=50$ delays and varying numbers $q_w$ of observed grid nodes.
% Figure environment removed 
% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed


\end{document}
