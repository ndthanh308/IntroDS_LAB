%\usepackage{float}
\RestyleAlgo{ruled}
\section{Materials and Methods}

\subsection{Data description}
\label{sec:data-desc}

The objective of this study is to evaluate and contrast the IGSD algorithm with existing state-of-the-art algorithms like FSSD and SSD++. To accomplish this, a total of 10 datasets were chosen from the UCI and Mulan repositories, along with the P4Lucat dataset.
In Table \ref{tab:datasets}, the datasets were classified into three categories based on their data type: numeric datasets are represented in green, nominal datasets in blue, and mixed datasets containing both numeric and nominal data in yellow. Furthermore, the "Rows" column displays the number of records in each dataset, the "Targets" column indicates the number of targets for each dataset along with the number of possible values, and the "DataType" column specifies the count of nominal and/or numeric columns present in each dataset.
%\todo[inline]{¿Incluimos la variabilidad de datos nominales en las columnas?}%
Furthermore, P4Lucat and Proteins datasets present more than one target, so it was decided to transform all the possible targets into one unique target, using the combination of the different target values for each dataset. Thus, resulting in one target option for the P4Lucat dataset with 4 possible values and one target option for the Proteins dataset with 32 possible values.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset}                               & \textbf{Rows} & \textbf{Targets} & \textbf{DataType(nom/num)} \\ \hline \hline
\cellcolor[HTML]{92D050}\textbf{Iris}          & 150           & 1 (3)            & 0/4                 \\ \hline
\cellcolor[HTML]{92D050}\textbf{Echo}          & 108           & 1 (2)            & 0/6                 \\ \hline
\cellcolor[HTML]{92D050}\textbf{Heart}         & 270           & 1 (2)            & 0/13                \\ \hline
\cellcolor[HTML]{92D050}\textbf{Magic}         & 19020         & 1 (2)            & 0/10                \\ \hline \hline
\cellcolor[HTML]{8EA9DB}\textbf{tic-tac-toe}   & 958           & 1 (2)            & 9(3)/0                 \\ \hline
\cellcolor[HTML]{8EA9DB}\textbf{vote}          & 435           & 1 (2)            & 16(3)/0                \\ \hline
\cellcolor[HTML]{8EA9DB}\textbf{P4Lucat}       & 650           & 2 $\to$ 1(4)            & 9/0                 \\ \hline
\cellcolor[HTML]{8EA9DB}\textbf{Proteins}       & 662           & 27 $\to$ 1(32)          & 1186(2)/0              \\ \hline \hline
\cellcolor[HTML]{FFD966}\textbf{Adult}         & 45222         & 1 (2)            & 8(2-5-6-7-9-15-16-42)/6                 \\ \hline
\cellcolor[HTML]{FFD966}\textbf{Nursery}       & 12960         & 1 (5)            & 7(2-4*3-4-5)/1                 \\ \hline
\cellcolor[HTML]{FFD966}\textbf{Breast-cancer} & 286           & 1 (2)            & 8(2*2-2*3-2*6-7-11)/1                 \\ \hline
\end{tabular}
\caption{Datasets description}
\label{tab:datasets}
\end{table}

\subsection{Pattern discovery methods}
\label{sec:ml-methods}

In this section, we will discuss the methodologies employed for identifying patterns in the aforementioned data. Initially, we will provide an overview of SD and present essential definitions related to this field. Subsequently, we will introduce the IGSD algorithm, which is the proposed method for pattern discovery.

\subsubsection{Subgroup Discovery}
\label{sec:SD}

Subgroup Discovery (SD) is a data mining technique that aims to uncover meaningful associations between variables in relation to a specific property of interest \cite{sammut_encyclopedia_2017}. The literature distinguishes two versions or cultures of SD: Subgroup Identification (SI) and Knowledge Discovery in Databases (KDD) \cite{esnault_q-finder_2020}. In this study, the KDD culture is adopted due to its domain-agnostic nature, which allows for the utilization of diverse quality metrics or measures such as coverage, support, unusualness, and more. By employing these metrics, KDD endeavors to identify statistically significant subgroups that satisfy a given target property. \\

The following set of definitions is presented as a foundational background for key concepts that are common to SD algorithms: \\

\textbf{Dataset}: A dataset (D) can be defined as the set of items $I=(X,Y)$, where $X=\{k1-v1,k2-v2,..,kn-vn\}$ represents the conjunction of $attributes(k)-values(v)$ pairs and $Y$ the target value selected.
The attributes set $(k)$ encompasses all the explanatory variables present in the dataset. The values $(v)$ can be classified into three types: numeric, boolean, and nominal.\\ \\

\textbf{Subgroup}: A subgroup (s) refers to a combination (Comb) of properties or features, which are attribute-value pairs that describe a distribution with respect to the Target$_{value}$ in a given dataset. Therefore, the properties or features (Comb) must contain a combination that exists in the dataset. Additionally, each attribute-value pair, also known as a selector, consists of an attribute, a condition, and a value. The possible conditions depend on the variable type: numeric variables support greater and less than \{$\geq$, $\leq$\} while binary and categorical support equal to \{==\}, i.e $\{attr1=="possible \; value"\}$ or $\{attr1\geq5\}$. These subgroups can be represented as individual patterns being regularly defined as:
\begin{equation}
s : Comb \rightarrow Target_{value}
\label{eq:Rule format}
\end{equation}

% \textbf{Subgroup search space}: 

% \subsubsection{Apriori}
% \label{sec:Apriori}
% Apriori \cite{sammut_encyclopedia_2017} is a machine learning method typically used for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in a database and extending them to larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to obtain association rules which reveal general trends in the data. Apriori algorithm handles the frequency of items based on three concepts: support, confidence, and lift. Support of a given item is defined as the fraction of database records containing that item. Moreover, confidence allows to define relations among items in the database. For a relation (X → Y) between items X and Y, the confidence is the likelihood of finding item Y when item X is found. Finally, lift refers to the increase in the ratio of finding an item. Thus, Lift (X → Y) provides a measure of how much the ratio of finding Y increases when X is found.



% In order to discover associations in the subgroup generation phase, BeamSearch algorithm \cite{herrera_overview_2011} has been selected. As opposed to exhaustive search \cite{atzmueller_subgroup_2015}, this algorithm implements a heuristic search \cite{atzmueller_subgroup_2015} that decreases the number of potential subgroups to be explored in later phases by generating all successors at the current level, sorting them in increased order of heuristic cost and storing only a predetermined number called beam width, which also allows reducing the time employed in the generation phase.

\textbf{Sets and Lists of Subgroups}: Subgroup sets can be described as disjunctions of subgroups, allowing for overlapping between subgroups within the same set. Against that, subgroup lists do not permit overlapping, meaning that each attribute of a subgroup is not contained within another subgroup. This distinction is crucial for comprehending the performance of FSSD, SSD++, and IGSD algorithms.\\ 

\textbf{Quality Function}: A quality function \(q: \Omega_{sd} \times \Omega_E \rightarrow \mathbb{R}\) is employed to assess the effectiveness of a subgroup description \(sd\) belonging to the set \(\Omega_{sd}\), given a target concept \(t \in \Omega_E\), and to rank the discovered subgroups during the search process. Quality functions are presented here in a general context for subgroup discovery and will be subsequently elaborated upon as descriptive and predictive measures.

For binary target variables, various significant quality functions can be defined in the following form:

\[
q_a = n^a \cdot (p - p_0), \quad a \in [0, 1]
\]

Here, \(p\) represents the relative frequency of the target variable within the subgroup, \(p_0\) denotes the relative frequency of the target variable in the total population, and \(n\) indicates the size of the subgroup. The parameter \(a\) allows for a trade-off between the increase in the target share \(p - p_0\) and the generality \(n\) of the subgroup.\\

Regarding the target space \(\Omega_E\), it is defined based on two binary variables present in the input dataset: disease progression-relapse and toxicity. The literature offers various approaches for handling multi-class problems. For instance, SSD++ executes the SD algorithm for each target and incorporates only subgroups that enhance the information in a subgroup list. This approach is akin to a One-vs-Rest (OvR) strategy applied to each target. Multi-target scenarios are also explored in SSD++, where a subgroup list model is generated, considering the categorical distribution for each target and class found within those targets.

In this work, to maintain consistency with an OvR strategy, we have chosen to implement a new target variable. Thus, these two variables are linked through a conjunction operation to define a single-term target, as required by the SD method. Consequently, the target variable contains the information from both binary variables, namely \(Progression-Relapse\)=[YES/NO] and \(Toxicity\)=[YES/NO]. This combination results in four distinct targets due to the possible combinations of variable values. SD can be employed for both binary and nominal targets, as stated in \cite{proenca_robust_2022}. SSD++ is capable of handling both types of targets, whereas FSSD is limited to binary targets. \\

%Following this approach, several algorithms from the state of the art were used to compare the performance with IGSD, such as FSSD \cite{belfodil_fssd_2019} and SSD++ \cite{proenca_discovering_2021}.

% Moreover, the algorithm uses the p-value statistical measure, which is calculated using the Chi-Square statistical test \cite{mchugh_chi-square_2013}, to discover statistically and clinically relevant groups in the medical field \cite{esnault_q-finder_2020}. 
 
% \subsubsection{Roc Analysis}
% \label{sec:ROC}
% A point in ROC space \cite{sammut_encyclopedia_2017} presents classifier efficiency in terms of false positive rate $FPr = \frac{FP}{TN+FP}$ (plotted on the X-axis), and true positive rate $TPr = \frac{TP}{TT+FN}$ (plotted on the Y -axis) where TP represents the elements correctly predicted to the positive class, FP represents the elements incorrectly predicted to the positive class, TN represents the elements correctly predicted to the negative class and FN = represents the elements incorrectly predicted to the negative class.

% To determine the yield of Apriori algorithm and BeamSeach-SD in respect of predicting class, a method that employs the combined probabilistic classifications of all subgroups was selected \cite{lavrac2004subgroup}. This method uses a decision threshold, being in this research work the confidence measure, to set if an instance belongs to the positive class or not. Supposing that the positive probability (i.e confidence) is larger than the threshold the system predicts positive, otherwise negative.
% In consequence, the ROC curve \cite{sammut_encyclopedia_2017} can be built by varying this threshold from 1 (all predictions negative, corresponding to (0,0) in ROC space) to 0 (all predictions positive, corresponding to (1,1) in ROC space). This will result in n + 1 points in ROC space, where n is the total number of thresholds used to predict the positive class.
% To summarize, The ROC curve represents the behavior of a set of classifiers, whereas the area under this ROC curve indicates the combined quality of all subgroups (i.e., the quality of the entire rule set).

The following paragraphs describe the main descriptive measures commonly found in the literature on SD. These measures allow for the evaluation of individual subgroups, enabling the comparison of results across different algorithms:


\begin{itemize}
\item Coverage \cite{herrera_overview_2011}: It measures the percentage of examples covered on average. This can be computed as:
\begin{equation}
Cov(R) = \frac{n(Cond)}{ns}
\label{eq:Cov}
\end{equation}
where \(ns\) is the number of total examples and \(n(Cond)\) is the number of examples that satisfy the conditions determined by the antecedent part of the pattern. The average coverage of a subgroup set is computed as:
\begin{equation}
COV = \frac{1}{nR} \sum_{i=1}^{nR} Cov(R_i)
\label{eq:COV}
\end{equation}
where \(nR\) is the number of induced patterns.

\item Confidence \cite{herrera_overview_2011}: It measures the relative frequency of examples satisfying the complete pattern among those satisfying only the antecedent. This can be computed as:
\begin{equation}
Cnf(R) = \frac{n(Target_{value}Cond)}{n(Cond)}
\label{eq:Conf}
\end{equation}
where \(n(Target_{value}Cond) = TP\) and it is the number of examples that satisfy the conditions and also belong to the value for the target variable in the pattern. The average confidence of a pattern set is computed as:
\begin{equation}
CNF = \frac{1}{nR} \sum_{i=1}^{nR} Cnf(R_i)
\label{eq:CONF}
\end{equation}

\item Size: The pattern set size is computed as the number of patterns in the induced pattern set.

\item Complexity: It measures the level of information presented in patterns. It is determined as the number of variables contained in the pattern.

% \item Significance \cite{herrera_overview_2011}: This measure indicates the significance of a finding if measured by the likelihood ratio of a rule:
% \begin{equation}
% Sig(R) = 2* \sum_{i=1}^{nC}n(Target_{valuek} · Cond)*
% \log\frac {n(Target_{valuek} · Cond)} {n(Target_{valuek}*p(Cond))}
% \label{eq:Sig}
% \end{equation}
% where p(Cond), computed as n(Cond)/ns, is used as a normalized factor, and nC is the number of values of the target variable. It must be noted that although each rule is for a specific Target$_{value}$, the significance measures the novelty in the distribution impartially, for all the values. The average significance of a rule set is computed as:
% \begin{equation}
% SIG = \frac{1}{nR} \sum_{i=1}^{nR} Sig(Ri),
% \label{eq:SIG}
% \end{equation}
\item Unusualness \cite{herrera_overview_2011}: This measure is described as the weighted relative accuracy of a pattern. It can be calculated as:
\begin{equation}
WRAcc(R) = Cov(R) * (Cnf(R) - \frac{n(Target_{value})}{ns} )
\label{eq:Wraacc}
\end{equation} 
The unusualness of a pattern can be described as the balance between its coverage, represented by \(Cov(R)\), and its accuracy gain, denoted by \(Cnf(R) - \frac{n(Target_{value})}{ns}\).
% As discussed in \cite{lavrac2004subgroup}, WRAcc is appropriate for measuring the unusualness of separate subgroups because it is proportional to the vertical distance from the diagonal in the \todo[color=green]{Qué ROC space?} ROC space.
The average unusualness of a pattern set can be computed as:
\begin{equation}
WRAcc = \frac{1}{nR} \sum_{i=1}^{nR} WRAcc(Ri),
\label{eq:WRACC}
\end{equation}
\end{itemize}

In addition to the descriptive metrics discussed earlier, predictive measures can also be utilized to evaluate a pattern set, treating a set of subgroup descriptions as a predictive model. Although the primary objective of pattern discovery algorithms is not accuracy optimization, these measures can be employed to compare predictive performance.
\begin{itemize}
\item Predictive accuracy \cite{jin_huang_using_2005}: Predictive accuracy refers to the percentage of correctly predicted instances. In the case of a binary classification problem, the accuracy of a pattern set can be computed as:
\begin{equation}
ACC = \frac{TP+TN}{TP+TN+FP+FN}
\label{eq:ACC}
\end{equation}
where TP represents true positives, TN denotes true negatives, FP represents false positives, and FN denotes false negatives.
% \item AUC (Area Under the Curve) \cite{jin_huang_using_2005}: It interprets a rule set as a probabilistic model, given all the different probability thresholds as defined through the probabilistic classification of test instances.
\end{itemize}

In this paper, we also incorporate quality functions that describe relevant aspects of patterns. One such measure is Information Gain (IG) \cite{noda_discovering_1999} \cite{prasetiyowati_determining_2021}, which quantifies the reduction in entropy or surprise by splitting a dataset based on a specific value of a random variable. It is calculated as follows:

\[
IG(D, v) = H(D) - H(D | v)
\]

Here, \(IG(D, v)\) represents the information gain for the dataset \(D\) with respect to the variable \(v\), \(H(D)\) is the entropy of the dataset before any change, and \(H(D | v)\) is the conditional entropy of the dataset when the variable \(v\) is added.

The entropy of a dataset can be understood in terms of the probability distribution of observations within the dataset belonging to different classes. For example, in a binary classification problem with two classes, the entropy of a data sample can be calculated using the following formula:
\begin{equation}
Entropy = -(p(a) * \log(P(a)) + p(1-a) * \log(P(1-a)))
\label{eq:Entropy}
\end{equation}
The entropy measures the level of uncertainty or randomness in the distribution of classes within the dataset.
In the case of a multi-class problem with more than two classes, it can be transformed into a binary problem by employing the One-vs-Rest (OvR) strategy. Each class under consideration is treated as one class, while the remaining classes are grouped together as another class (not belonging to the class under study). This transformation allows binary target datasets to be analyzed effectively. \\

In addition, we also employed the odds ratio (OR) measure\cite{szumilas_explaining_2010,dominguez-lara_odds_2018}, which is represents the association between an antecedent and an outcome. The OR represents the ratio of the odds of the outcome occurring given a specific antecedent, compared to the odds of the outcome occurring in the absence of that antecedent.

In this work, we utilize ORs to compare the relative odds of the occurrence of the outcome of interest based on specific patterns that contain multiple selectors. This measure enables us to evaluate the strength of the association between the antecedent (pattern) and the outcome of interest.
% ORs are used to compare the relative odds of the occurrence of the outcome of interest, given exposure to the variable of interest. 
Consequently, odds ratios (ORs) can be utilized to assess whether adding a new selector to a pattern serves as a risk factor for a specific outcome. They also allow for comparing the magnitude of various risk factors associated with that outcome. This comparison helps determine the effectiveness of adding more information to a pattern.

In IGSD, ORs are employed as an index to select the most relevant subgroups based on the association between the antecedent and the target. By considering the ORs, IGSD identifies subgroups with higher odds ratios, indicating stronger associations between the antecedent and the target outcome. This selection process helps prioritize the most impactful subgroups in terms of their predictive power and relevance to the target.
The odds ratio (OR) can be calculated using the following formula:

\[
OR = \frac{TP \cdot TN}{FP \cdot FN}
\]

To interpret the OR as an effect size, \cite{dominguez-lara_odds_2018} proposes transforming it to Cohen's \(d\). This transformation makes the interpretation of the OR easier, as it allows for considering an $OR > 6.71$ to have a similar effect size, regardless of the actual magnitude of the OR.

In cases where Cohen's \(d\) is not obtained, comparing subgroup sets based solely on mean values of the OR may lead to distorted results. Higher OR values can disproportionately influence the mean, while lower OR values may not receive due consideration. To address this, four intervals are defined:

\begin{itemize}

\item \(OR < 1.68\) represents a very low effect.
\item  \(1.68 < OR < 3.47\) represents a low effect.
\item  \(3.47 < OR < 6.71\) represents a moderate effect.
\item  \(OR > 6.71\) represents a high effect.

\end{itemize}

For ease of numerical representation, each interval is assigned a value, resulting in the odds ratio range (ORR) being defined from 1 to 4. This allows for a more balanced comparison between subgroups and avoids overemphasizing the impact of extremely high OR values.

Finally, we have employed the p-value as a subgroup filtering criterion, which is calculated using the Chi-Square statistical test \cite{mchugh_chi-square_2013}. In the medical field, a p-value of 0.05 is commonly used as the standard criterion for statistical significance.

\subsection{IGSD algorithm}
\label{sec:IGSD} 
% In this section, we present IGSD, a pattern discovery algorithm that attempts to maximize the quality of knowledge provided by found patterns while minimizing pattern complexity. 
% This algorithm combines IG and ORR to determine which attributes have more relevance, based on IG, and establish the set of variable values with a stronger dependence on a specific target, based on ORR. 

This section presents IGSD, a pattern discovery algorithm that aims to minimize pattern complexity while simultaneously maximizing the quality of the knowledge derived from discovered patterns.
This algorithm combines IG and ORR to identify, on the basis of IG, the attributes with greater relevance and, on the basis of ORR, the set of variable values that have a stronger dependence on a particular target.

% As mentioned before, the proposed algorithm tries to tackle some limitations present in state of the art SD methods. Thus, previous algorithms require the fine-tuning of key parameters for each analyzed dataset. Hence, for example, the parameter \textit{Beam width}, controlling the size of search space and thus affecting discovered patterns, needs to be defined for each input dataset. Moreover, previous algorithms try to obtain patterns by maximizing a single index, typically weighted relative accuracy (\textit{WRAcc}), thus requiring a manual set of a threshold for the optimization index, again for each analyzed dataset. Furthermore, previous algorithms use non-overlapping data structures, like subgroup lists, to explore subgroup search space. This can pose a limitation since having non-overlapping information in explored subgroups can prevent the discovery of relevant patterns. Additionally, previous algorithms do not offer the option of fixing some important dataset variables to be present in the discovered patterns. However, patterns with fixed key items are an important aspect since experts in the domain may require them to consider a pattern as useful or relevant. 
% Finally, the quality of discovered patterns is evaluated based on single index criteria, and sometimes the selection of evaluation indices is not unified across different SD algorithms.

As previously stated, the proposed algorithm attempts to overcome some limitations of current SD methods. As a result, prior algorithms necessitated adjusting key parameters for each dataset being analyzed. As a result, parameters like beam width, which affect discovered patterns and control the size of the search space, must be defined for each input dataset. In addition, previous algorithms tried to find patterns by maximizing a single index, usually weighted relative accuracy (\textit{WRAcc}), which necessitated manually setting a threshold for the optimization index for each analyzed dataset once more. Additionally, previous algorithms explored subgroup search space by making use of non-overlapping data structures like subgroup lists. Since non-overlapping information in explored subgroups can prevent the discovery of relevant and intriguing patterns, this can be a limitation. Additionally, some crucial dataset variables cannot be fixed to be present in the discovered patterns using previous algorithms. However, because experts in the field may require them to consider a pattern to be useful or interesting, patterns with fixed key variables are an important aspect.
Lastly, the quality of discovered patterns is evaluated using a single index, and the evaluation indices chosen by various SD algorithms may not always be consistent.

% All these limitations are addressed in IGSD algorithm. Thus, this new method uses IG as one optimization index when searching for subgroups and implements a dynamic IG threshold, which will be used to decide which selectors could be considered relevant options in each subgroup discovery step. Moreover, this threshold does not require a manual definition of an arbitrary value. Instead, it is dynamically calculated and adapted for each explored subgroup at each discovery step of the algorithm. At the same time, this IG dynamic threshold removes the need of fine-tuning the \textit{Beam width} parameter, since the IG threshold is dynamically adjusting the size of the search space. Furthermore, IGSD provides an output of homogeneous measures to be compared with other implementations. \\
The IGSD algorithm addresses all of these limitations. As a result, this new strategy employs a dynamic threshold using IG as a single optimization index when searching for subgroups. This threshold will be used to select which selectors will be considered relevant options in each subgroup discovery step. Furthermore, there is no need to manually define an arbitrary value for this threshold. Instead, at each algorithm discovery step, it is dynamically calculated and modified for each explored subgroup. Since the IG threshold is dynamically adjusting the size of the search space, it is unnecessary to fine-tune the \textit{Beam width} parameter at this time. In addition, IGSD provides a uniform measure output that can be compared to that of other implementations.

% In order to launch IGSD algorithm, it requires three arguments that do not require fine-tuning and are used to select different algorithms options. The arguments are threshold mode ($t_{mode}$), the condition attributes ($Cond_{list}$), and the maximum depth during the exploration phase ($d_{max}$). The $t_{mode}$ determines if the algorithm will employ the dynamic IG threshold, which is the default option, or the maximum IG threshold. 
Three arguments are needed to start the IGSD algorithm. These arguments can be used to choose between different options for the algorithm and do not require any fine-tuning. The arguments are the maximum depth during the exploration phase ($d_{max}$), the condition attributes ($Cond_{list}$), and the threshold mode ($t_{mode}$). The algorithm will use either the maximum IG threshold or the dynamic IG threshold, which is the default, according to the $t_{mode}$ variable.

% The $Cond_{list}$ parameter allows the user to specify some dataset variables required to be present in the obtained patterns, and the $d_{max}$ parameter determines the depth of the exploration space, which can also be seen as the pattern complexity or the maximum number of selectors that the patterns will have.

The $d_{max}$ parameter determines the depth of the exploration space, which can also be interpreted as the pattern complexity or the maximum number of selectors that the patterns will have. In addition, the user can specify some dataset variables that must be present in the obtained patterns using the $Cond_{list}$ parameter.


% Figure environment removed

% Regarding the workflow of the algorithm, Figure \ref{workflow} illustrates the steps followed by the algorithm. Two tasks can be defined, discovering potentially relevant associations, and removing irrelevant information presented in associations.
% First, the algorithm receives as input a dataset and the values of the parameters: $t_{mode}$, $Cond_{list}$, and $d_{max}$. Consequently, the first task will discover potentially relevant associations using the IG threshold as the criterion to discard patterns. After the first task is completed, the resulting patterns are used as input for the second task, which is removing irrelevant selectors from these patterns, in order to obtain patterns with large information and dependence on a target, while minimizing the complexity. This is achieved by relying on both the IG and the OR measures.

The algorithm's workflow is depicted in Figure \ref{workflow}, which shows the steps the algorithm takes. Finding interesting associations and removing irrelevant information from associations are two tasks that can be defined. First, a dataset and the values of the parameters $t_{mode}$, $Cond_{list}$, and $d_{max}$ are given to the algorithm as input. As a result, the first task will be performed using the IG threshold to eliminate patterns and discover interesting associations. After the first task is completed, the generated patterns are used as input for the second task, which removes irrelevant selectors from these patterns to obtain patterns with a large amount of information and dependencies on the target, while minimizing complexity. This is achieved by relying on IG and OR measures.


\subsubsection{Discovering relevant associations}

% The first step, discovering potentially relevant associations, starts calculating an IG threshold (i.e. $thre$) \cite{prasetiyowati_determining_2021} for all the selectors in order to pick those which exceed that $thre$ and add the highest amount of information to the problem. The IG threshold is computed according to Equation~\ref{eq:Thre}:
In order to select the selectors that surpass the IG $thre$ and contribute the most information to the problem, the first step, which is finding interesting associations, begins with the calculation of an IG threshold for each selector. Equation~\ref{eq:Thre} is used to compute the IG threshold:

\begin{equation}
thre = \sqrt{\frac{n\sum_{i=1}^{n}x_{i}^2 - (\sum_{i=1}^{n}x_{i})^2}{n(n-1)}}
\label{eq:Thre}
\end{equation}

% Where the $n$ term refers to the number of all the considered selectors to compute the IG threshold and the $x_i$ term refers to the IG value of a specific selector. Thus, the IG threshold for each subgroup at each exploration step is calculated among all the possible selectors.
Where the $n$ term indicates the total number of selectors that are contemplated, and the $x_i$ term indicates the IG value of a particular selector. As a result, among all of the possible selectors, the IG threshold for each subgroup at each exploration step is calculated.

\begin{algorithm}[H]
\caption{InfoGained SD Algorithm}\label{alg:igsd}
\KwData{Dataset $D$, maximum depth $dmax$, threshold mode $t\_mode$, condition attributes $CondList$}
M $\gets$ filterByThreshold(all selectors in $D$, t\_mode)\;
\For{$i\leftarrow 2$ \KwTo $dmax$}{
    M\_aux $\gets$ []\;
    \For{$j\leftarrow 0$ \KwTo len(M)}{
        cands $\gets$ get selector candidates which contain the attributes presented in $CondList$ ([Patterns with length=i with parent node $= M[j]$])\;
        final\_cands $\gets$ filterByThreshold(cands,t\_mode)\;
        M\_aux $\gets$ M\_aux + final\_cands\;
        }
    M $\gets$ M\_aux\;
    }
R $\gets$ []\;
\For{$k\leftarrow 0$ \KwTo len(M)}{
    s $\gets$ calculate\_optimal\_cut(M[k])\;
	R $\gets$ R + s\;
}
return R\;
\end{algorithm}

% Algorithm~\ref{alg:igsd} shows the steps followed during the phase for discovering potentially relevant associations. In line 1 of Algorithm~\ref{alg:igsd}, the subgroups with one selector, i.e. of length 1, with IG value higher than IG threshold (Equation~\ref{eq:Thre}) are computed and returned in $M$. Thus, depending on the parameter $t_{mode}$, variable $M$ will contain the subgroup with the maximum IG value ($t_{mode}$='maximum') or a set of subgroups with IG higher than the dynamic threshold ($t_{mode}$='dynamic').
% In lines 2-10, subgroups are built following an iterative process, adding at each step selectors that have an IG value equal to or larger than $thre$ (Equation~\ref{eq:Thre}). This iterative process will start with each pattern in $M$ obtained in line 1. Thus, in line 5 for each pattern $j$ in $M$, a new selector is added to pattern $M[j]$ increasing the length by 1 up to a total pattern length of $i$. Additionally, line 5 in Algorithm~\ref{alg:igsd} stores in $cands$ variable the new patterns of length $i$ that contain attributes indicated in user-provided argument $CondList$, if it is not empty. Then, in line 6 patterns stored in $cands$ variable are filtered by IG value by computing dynamic threshold and according to argument $t_{mode}$, as in line 1 of Algorithm~\ref{alg:igsd}. This iteration process will be performed until the $d_{max}$ parameter is reached.

Algorithm~\ref{alg:igsd} shows the steps performed during the interesting association discovery phase. In line 1 of Algorithm~\ref{alg:igsd}, variable $M$ will contain subgroups with one selector, i.e., of length 1, with an IG value higher or equal to an IG threshold. Depending on the parameter $t_{mode}$, this IG threshold will be either the maximum IG value of all the subgroups with one selector ($t_{mode}$='maximum') or the value computed using (Equation~\ref{eq:Thre}) ($t_{mode}$='dynamic').  

Subgroups are constructed in an iterative process in lines 2 to 10, adding selectors with IG values equal to or greater than $thre$ at each step (Equation~\ref{eq:Thre}). Each pattern contained in $M$ obtained in line 1 will serve as the basis for this iterative process. Consequently, in line 5 for each pattern ($j$) in $M$, another selector is added to design $M[j]$ expanding the length by 1 up to an all-out design length of $i$. Furthermore, line 5 in Algorithm1 stores in $cands$ variable the new patterns of length $i$ that contain attributes specified in user-provided argument $Cond_{list}$, on the off chance that it isn't empty. Then, at that point, in line 6, patterns stored in $cands$ variable are filtered by IG value by computing dynamic threshold and according to argument $t_{mode}$, as in line 1 of Algorithm~\ref{alg:igsd}. This process of iteration will continue until the $d_{max}$ parameter is reached.

% An example is provided in Fig. \ref{algo_example} to better clarify how associations are built, using a $d_{max}$ = 2. As can be noticed, in Iteration = 1 schema, an IG threshold is calculated using the IG values of available selectors from Selector1, to Selector6. After the calculation of the threshold, Selector1 and Selector3 are those which exceed the computed threshold, thus, will be chosen to build the patterns (i.e. Pattern1 and Pattern2) in this first iteration. In the second iteration, the algorithm iterates for each pattern from the previous iteration (i.e. Pattern1 and Pattern2). For the first pattern, Selectors 3, 5 and 6 are candidates since the combination of Pattern1 and these selectors, is present in the input data set. While Pattern2, Selector2, Selector4, and Selector5 are the available selectors to add. For each pattern, a different IG threshold is calculated. For Pattern1, Selector3, and Selector5 exceed the respective threshold, so they will be added to Pattern1, obtaining Pattern3 and Pattern4, of length 2 each. On the other hand, only Selector4 exceeds the respective threshold, so it is added to Pattern2, obtaining Pattern5 as a result.

In addition, using a $d_{max}$ = 2 as an illustration, Fig. \ref{algo_example} provides a better understanding of how associations are constructed. As can be seen, in Iteration = 1 schema, an IG threshold is computed utilizing the IG values of available selectors from Selector1, to Selector6. Selectors 1 and 3 will be chosen to build the patterns (Patterns 1 and 2) in this first iteration because they exceed the threshold after the threshold was calculated. From here, the algorithm iterates for each pattern from the previous iteration (such as Pattern1 and Pattern2) in the second iteration. For the first pattern, Selectors 3, 5 and 6 are candidates since the combination of Pattern1 and these selectors, is present in the input data set. On the other hand, for the second pattern, Selector2, Selector4, and Selector5 are the possible selectors to add. It is important to notice that for each pattern of the iterative process, a different IG threshold is calculated for each one. So,  for Pattern1, only Selector3, and Selector5 surpass the particular threshold, so they will be added to Pattern1, getting Pattern3 and Pattern4, of length 2 every one. However, only Selector 4 surpasses the required threshold, so it is added to Pattern 2, resulting in Pattern 5.

% Figure environment removed

\subsubsection{Removing irrelevant information}\mbox{}
\newline

% Finally, after the generation of the patterns, complexity is reduced by removing irrelevant information from patterns. The objective of this step is to determine which selectors of a pattern are not providing useful or relevant information. Reduction of complexity is done based on IG and ORR. Thus, Figure~\ref{fig:alg2_example} shows that among the 6 selectors of a given pattern, selector 3 is identified as the best selector since its IG value is over the IG threshold (dashed line) and has a high ORR value. Based on the identified optimal selector, the pattern is cut and selectors 4, 5 and 6 are removed as irrelevant information.
% The removal of irrelevant selectors starts in line 11 of the Algorithm~\ref{alg:igsd} by setting list $R$ for storing optimized patterns. Thus, for each pattern returned in variable $M$ by the first step in Algorithm~\ref{alg:igsd} (i.e. lines 1-10), IGSD determines the optimal cut and stores it in $R$ output list (lines 11 and 14).

% Regarding the calculation of the optimal cut for a specific pattern, Algorithm~\ref{alg:cut} shows the steps followed to find the optimal stopping point (i.e. selector) of a pattern.
% Line 1 of Algorithm~\ref{alg:cut} performs the conversion of $OR$ values to $ORR$.
% Later, in line 2, an IG threshold (according to Equation \ref{eq:Thre}) is calculated for the input pattern. Consequently, the algorithm will discard those selectors with IG values lower than the IG threshold. Moreover, line 3 filters not statistically relevant selectors by removing those ones with a p-value measure below 0.05.
% After the filtering of selectors, if only one selector is left, its position will be used to cut the pattern and returned it as the $optimal\_cut$ (lines 4-6 of Algorithm~\ref{alg:cut}).
% On the contrary, if different candidates are evaluated to find the optimal cut, line 8 in Algorithm~\ref{alg:cut}, will iterate over them as follows:
% \begin{itemize}
% \item Initially, the first element of possible candidates is considered as the $optimal\_cut$ (Algorithm \ref{alg:cut}, line 7).
% \item Then, in lines 9-10, the $optimal\_cut$ is not updated until the ORR of a candidate selector improves the $optimal\_cut$ ORR.

% \item In lines 12-14, there are two conditions to stop the iteration. Whether the candidate selector in the current iteration has a lower ORR than the $optimal\_cut$ ORR or the current candidate selector is not consecutive to the previously examined selector and its ORR is the same as the ORR $optimal\_cut$. In these cases, the iteration stops because a new element in a pattern should be added only if the $optimal\_cut$ ORR is improved.
% \end{itemize}

For the second task, pattern complexity is reduced by removing irrelevant information from patterns after they are generated in the first task. The purpose of this step is to determine which selectors of a pattern are not giving valuable or important information.
Thus, Figure~\ref{fig:alg2_example} shows that among the 6 selectors of a given pattern, selector 3 is identified as the best selector since its IG value is over the IG threshold (dashed line) and has a high ORR value. Based on the identified optimal selector, the pattern is cut and selectors 4, 5 and 6 are removed as irrelevant information.

% Figure environment removed

This step initiates by setting of list $R$ for storing optimized patterns in Algorithm~\ref{alg:igsd}, line 11. As a result, on lines 11 and 14, IGSD stores the optimal cut in the $R$ output list for each pattern that was returned in the variable $M$ by the first step of Algorithm~\ref{alg:igsd}.

Algorithm~\ref{alg:cut} demonstrates the procedures used to determine a pattern's best cut point (i.e. selector),
First of all, Line 1 of Algorithm~\ref{alg:cut} converts $OR$ values to $ORR$, and Line 2 calculates an IG threshold using all the selectors presented in the input pattern in accordance with Equation \ref{eq:Thre}). Thusly, the calculation will dispose of those selectors with IG values lower than the IG threshold. Besides, line 3 filters not statistically relevant selectors by removing those ones with a p-value measure below 0.05.

After the filtering of selectors, if only one selector remains, its position will be used to cut the pattern and returned as the $optimal\_cut$ cut (lines 4 through 6 of Algorithm~\ref{alg:cut}). On the other hand, line 8 of Algorithm~\ref{alg:cut} will iterate over the candidates as follows in order to determine the best cut:
\begin{itemize}
    \item At the beginning, the first selector of potential candidates is considered as the $optimal\_cut$ ((Algorithm \ref{alg:cut}, line 7).
    \item The $optimal\_cut$ is not updated in lines 9 to 10 until a candidate selector's ORR improves the $optimal\_cut$ ORR.
    \item There are two conditions to stop the iteration in lines 12 to 14. Whether the up-and-comer selector in the ongoing iteration has a lower ORR than the $optimal\_cut$ ORR or the ongoing examined selector isn't sequential to the recently analyzed selector and its ORR is equivalent to the ORR $optimal\_cut$. In such cases, the iteration stops because new elements should be added to a pattern only when the $optimal\_cut$ ORR improves.
\end{itemize}

\begin{algorithm}[H]
\caption{calculate\_optimal\_cut Algorithm}\label{alg:cut}
\KwData{Information gained list ig, Odd ratio list or, p-value list pv, Pattern p}
ORR $\gets$ [values of Odd ratio list are transformed into ranges]\;
cut\_candidates $\gets$ filterByThreshold(ig,t\_mode='dynamic')\;
cut\_candidates $\gets$ [elements in cut\_candidates with p\_value $\leq$ 0.05]\;
\If{len(cut\_candidates) == 1}{
      return p[:ig.index(cut\_candidates[0])]\;
    }
optimal\_cut $\gets$ ORR[0]\;
\For{$i\leftarrow 1$ \KwTo len(ORR)}{
    \If{ORR[i] $>$ ORR[i-1]}{
        optimal\_cut $\gets$ ORR[i]\;
    }
    \ElseIf{(ORR[i] == ORR[i-1] $\land$ ORR[i] is not consecutive to ORR[i-1]]) $\lor$ (ORR[i] $<$ ORR[i-1])}{
        break\;
    }
}
return p[:ORR.index(optimal\_cut)]\;
\end{algorithm}
