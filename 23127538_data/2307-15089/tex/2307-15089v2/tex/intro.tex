%!TEX root = ../paper_main.tex
\section{Introduction}
\label{sec:introduction}
%\todo[inline]{Reescribir}

% Comenzar hablando de LC y la problemática de prescribir ttos que se adapten a la variabilidad de caraterísticas de los pacientes
% Continuar mencionando el propósito de las CGs (proporcionar recomendaciones para los oncólogos)
% - Tienen en cuenta principalmente características de la enfermedad como cancer stage o driver mutations, pero no tienen en cuenta caracteristias del paciente. Por este motivo fallan para capturar patient variability
% - En CGs No hay resultados de outcome de los tratamientos recomendados
% Utilidad de métodos de ML no supervisados para encontrar patrones, como SD
% SD tiene es importante la validación de los patrones encontrados
% En este paper realizamos una comparativa de algoritmos de SD del SotA, para la búsqueda de patrones en tratamientos de pacientes de LC. De esos algoritmos detectamos algunas deficiencias y proponemos un nuevo algoritmo, y los patrones obtenidos los validamos 


% Lung cancer (LC) leads to the main cause of cancer death among the different types of cancer. More than 238,340 new cases of LC are estimated to occur in 2023, with an estimated 127,070 deaths \cite{siegel_cancer_2023}. Much progress has been made recently for LC, such as screening, early diagnosis, and treatment advances. There are 3 main types of LC: non-small cell lung cancer (NSCLC), small cell lung cancer (SCLC), and lung carcinoid tumor. NSCLC accounts for approximately 85\% of new cases. In spite of the significant increase in survival rates ("NSCLC 2-year relative survival increased from 34\% for people diagnosed during 2009 through 2010 to 42\% during 2015 through 2016") \cite{siegel_cancer_2021}, it is still necessary to improve screening methods and treatments in order to increase patient survival and quality of life. 

% Selecting the appropriate treatment is a crucial factor to enhance the patient's chances of survival and improve their quality of life. Cancer treatments may give rise to adverse effects, and the variability in toxicities experienced by patients depends on a variety of factors such as the stage of cancer and the combination of treatments received \cite{or2021systematic}. These toxic outcomes can disrupt the body's equilibrium and lead to various health issues that can adversely affect the patient's quality of life. Therefore, it is an important aspect to work on reducing the toxicity of treatments while maintaining or enhancing their effectiveness. To this end, the set of best practices and accumulated knowledge from oncologists is reported in clinical guidelines (CG) \cite{majem_seom_2019,hirsch_lung_2017}. These documents offer recommendations intended to optimize patient care and assist clinicians in LC treatment.  Such treatment recommendations are mainly based on patients characteristics like presence/absence of driver mutations and disease characteristics like cancer stage, tumor resectability, and tumor removability by surgery. However, NSCLC is a highly complex disease, and treatment efficacy might be impacted by many other patient and disease aspects, which are important to analyze. For that reason and as part of the so-called Precision Medicine, some previous works have used data-driven approaches to analyze how variability in patient’s and disease characteristics can affect cancer treatment outcomes \cite{zhang_towards_2014,suo_deep_2018,brown_patient_2016}.

%With the aim of discovering relations between patient profiles and treatment outcomes, in this paper we have analyzed a clinical dataset of LC patients using Subgroup Discovery method.

Pattern discovery or pattern mining is a machine learning technique that aims to find sets of items, subsequences, or substructures that are present in a dataset with a higher frequency value than a manually set threshold. In this context, a set of items that frequently appear together in a transaction data set, (e.g. milk and bread), is referred to as a frequent itemset. When dealing with sequential data, a frequent subsequence represents a pattern that occurs regularly in the sequence of items. On the other hand, a substructure can take various structural forms, such as subgraphs, subtrees, or sublattices, which can be combined with itemsets or subsequences. If a substructure occurs frequently in a graph database, it is known as a (frequent) structural pattern. This process helps identify recurring patterns or relationships within the data, allowing for valuable insights and knowledge extraction \cite{han_frequent_2007}. Also known as frequent pattern mining, it was initially popularized for market basket analysis, especially in the form of association rule mining. In this way, customer buying habits can be examined by identifying associations between different items that customers place in their shopping baskets. For instance, the analysis may reveal that customers who buy milk are highly likely to purchase cereal during the same shopping trip, and it can further specify which types of cereal are commonly associated with milk purchases \cite{agrawal_mining_nodate}. 

Among machine learning methods for pattern search, Subgroup Discovery (SD) \cite{herrera_overview_2011}  has been used previously to find relevant patterns in datasets. SD is a data mining task that aims to identify and extract interpretable patterns from the data, which exhibit interesting or exceptional characteristics with respect to a specific property of interest. It has been used in several fields, such as clinical applications \cite{esnault_q-finder_2020, subgroup_2023}, and technical applications \cite{atzmueller_subgroup_2015}, among others. However, several limitations were found due to the low complexity of the rules. Therefore, in this work, we propose the use of a wider range of features from datasets to discover more personalized patterns. 

%lacked of patients features, such as age, gender, smoking habit, comorbidities, histology, molecular markers, and PD-L1, which hindered the obtainment of personalized treatments with clinical relevancy.

Even if SD has been proposed as a useful technique, the standard version and state of art implementations \cite{belfodil_fssd_2019,proenca_discovering_2021} present several limitations. First, fine-tuning some key parameters is always necessary (i.e. \textit{Beam width}) for each analyzed dataset. Moreover, regarding the discovery of patterns, they are usually obtained by maximizing some index such as weighted relative accuracy (\textit{WRAcc}). However, pattern complexity is not optimized, and obtained patterns may lack interesting information. Additionally, previous algorithms do not offer the option of fixing some important dataset variables to be present in the discovered patterns, thus reducing the interest or acceptance of patterns by problem domain experts. Furthermore, subgroup lists are used in most recent SD algorithms and this can pose a problem as some information can be lost in non-overlapping subgroups found in datasets. Finally, the quality of discovered patterns is evaluated based on single index criteria, and sometimes the selection of evaluation indices is not unified across different SD algorithms.

Domain expert validation is a key point to find interesting conclusions on SD analysis. In the CN2-SD study, \cite{lavrac2004subgroup}, validation was performed for SD results on a real-life problem of traffic accident analysis dataset. Thus, interpretable and relevant patterns obtained from SD algorithms are necessary. However, SD literature typically lack validation of results by a set of experts. 
%rather than a combination of several measures which can provide a better evaluation of patterns. 

%One of the potential approaches that are under study are those referred to patient's similar profiles \cite{pmid25717413,Suo2018,Brown2016}, and how these similarities (or di-similarities) can affect the treatment outcomes. This approach, which is part of the so-called "Precision Medicine" allows to make use of computational and data-driven approaches to find potential solutions to different health problems. In the context of this domain, we aim at finding patterns of the patient characteristics that allows to identify better outcomes in terms of the survival or in the reduction of the toxicities.

To overcome these issues, in this work, we propose a new SD algorithm, InfoGained-SD (IGSD). This algorithm searches patterns through an optimization that combines information gained \cite{noda_discovering_1999,prasetiyowati_determining_2021} and odds ratio \cite{szumilas_explaining_2010,dominguez-lara_odds_2018} metrics. In addition, no fine-tuning regarding \textit{Beam width} is required by the user. Furthermore, it allows fixing key attributes in discovered patterns by an expert in the field of study, in order to increase acceptance of discovered patterns. Besides, a subgroup set is obtained, so no information is lost for the analyzed input data. Thus, this algorithm is used to examine different datasets in order to find relevant patterns, taking into account characteristics that might be relevant for expert validation. Multiple datasets with heterogeneous variable types (categorical, numerical, and mixed) are assessed in this study, to prove that our method can be useful in the pattern discovery data mining field, obtaining relevant and interpretable patterns, for a domain expert. 

%However, even though LC clinicians are expected to follow these clinical recommendations , the variability in patients' responses to different treatment combinations requires the adaptation of treatments taking into account patients characteristics.

%NSCLC is a highly heterogeneous disease and, therefore, a challenge for the clinician. The CGs collect all the recommendations regarding the diagnosis and management of the disease, to optimize patient care and provide support to the clinician in decision-making. The real population is sometimes not faithfully represented in clinical trial patients, which gives rise to the scientific evidence that justifies the recommendations of the main CGs. Hence, there is a problem when following these recommendations. Furthermore, even if followed, a lack of patients variability recommendations is found, as treatments are not personalized regarding factors such as age, gender, smoking habit, histology, and comorbidities. Thus, analyzing the real-world application of CGs and the influence of these characteristics in a negative prognosis of LC is important.

%Regarding concordancy between CGs and applied treatments, LC treatment guidelines have been studied, but the level of adherence of really prescribed therapies is unclear. In one study, 62.1\% of subjects received guideline-concordant treatment (range across clinical subgroups = 50.4-76.3\%). However, 21.6\% received no treatment (range = 10.3-31.4\%) and 16.3\% received less intensive treatment than recommended (range = 6.4-21.6\%). Guideline-concordant treatment was less likely with increasing age, despite adjusting for relevant covariates (age $\ge$ 80 yr compared with $<$ 50 yr: adjusted odds ratio = 0.12, 95\% confidence interval = 0.12-0.13) \cite{blom_disparities_2020}. This lack of adherence to clinical guidelines responds to the clinician's need to adapt treatment to the interpatients variability.   

%Precision medicine will make possible, in the future, to discover new mutations that patients may harbor and to design more targeted treatments. Hence, it is necessary to obtain personalized guidelines for LC treatments including more patient characteristics, which will contribute to improve LC patients' prognosis. 

% The prognosis in LC is highly conditioned by three factors: tumor histology, tumor stage and the patient's performance status. The 5-year survival rate ranges from 63.1\% for localized disease (confined to lung parenchyma), 35.4\% for regional (spread to regional lymph nodes), 6.9\% for distant or metastasized disease \cite{howlader_seer_nodate}. Traditionally, the treatment for advanced NSCLC has been chemotherapy. However, recent advances in the development of targeted therapies and immunotherapy have extended survival outcomes, giving rise to a subgroup of long survivors, with 5-year survival reaching 50\% depending on the marker \cite{ramalingam_overall_2020, mok_updated_2020, planchard_phase_2022, horn_nivolumab_2017, garon_five-year_2019, pacheco_natural_2019}. Thus, molecular testing has become very important to choose the correct treatment approach. Precision medicine will in the future make it possible to discover new mutations that patients may harbor and to design more targeted treatments. This will further contribute to improving the overall survival data for patients with LC. 


% OS (Overall Survival), PFS (Progression Free Survival), and other quantitative measures may not necessarily capture the full impact of actual treatment on how a patient feels or functions. The patient is an individual who experiences beyond being a simple object of disease \cite{mead_patient-centredness_2000}. Therefore, reducing the toxicity experienced by the patient without diminishing the ameliorative effect in quantitative terms is an important goal and a difficult balance to achieve in many cases. Thus, the effort is increasingly directed towards assessing the quality of life of patients, expressed directly by patient-reported outcome (PROs) \cite{noauthor_guidance_2009}. Recently, this integration of PRO has been considered a priority by oncology societies such as the American Society of Clinical Oncology (ASCO) \cite{stover_asco_2016}. 

% NSCLC is a highly heterogeneous disease and, therefore, a challenge for the clinician. The clinical guidelines collect all the recommendations regarding the diagnosis and management of the disease, to optimize patient care and provide support to the clinician in decision-making. The level of adherence to LC treatment guidelines is unclear. In one study, 62.1\% of subjects received guideline-concordant treatment (range across clinical subgroups = 50.4-76.3\%). However, 21.6\% received no treatment (range = 10.3-31.4\%) and 16.3\% received less intensive treatment than recommended (range = 6.4-21.6\%). Guideline-concordant treatment was less likely with increasing age, despite adjusting for relevant covariates (age $\ge$ 80 yr compared with $<$ 50 yr: adjusted odds ratio = 0.12, 95\% confidence interval = 0.12-0.13) \cite{blom_disparities_2020}. This lack of adherence to clinical guidelines responds to the clinician's need to adapt treatment to the patient. Comorbidity increases in older patients, as well as a detriment in the functional situation. The real population is sometimes not faithfully represented in clinical trial patients, which give rise to the scientific evidence that justifies the recommendations of the main clinical guidelines. 

\section{State of the Art}

Subgroup Discovery (SD) \cite{herrera_overview_2011} has been proven as a suitable method for identifying statistical and  relevant patterns in datasets. It has been applied to clinical trials, precision medicine, and treatment optimization or disease study \cite{zhang_subgroup_2018,loh_subgroup_2019, korepanova_subgroup_2018,subgroup_2023,ibald-mulli_identification_nodate}. Other  applications are found in the bibliography, such as in social media study and \cite{atzmueller_mining_2012} and smart electricity meter data \cite{jin_subgroup_2014}, among other fields.

Even if SD has been proven as a suitable method for pattern discovery, an ongoing problem with this data mining technique is the difficulty of interpreting or analyzing the results produced, either because of the complexity and the large amount of information or because of their relevancy \cite{sun_is_2010,burke_three_2015}. In order to reduce the number of results obtained, some solutions such as ranking and selecting the best $n$ associations, eliminating associations composed of many features, or discarding associations with a specific measure value below a manual threshold have been proposed in the literature. Thus, SSD++ \cite{proenca_robust_2022} and FSSD \cite{belfodil_fssd_2019} seem to lead the state of the art of SD algorithms. SSD++ relies on beam search strategy, a heuristic approach for discovering subgroups in a population. This process, during the exploration phase, looks through combinations of variables until the maximum search depth of the dataset is covered, and stores only a predetermined number of subgroups at each level (\textit{Beam width}), which are the ones having the best heuristic cost. This means that the same value of \textit{Beam width} can result in a too large or insufficient number of patterns, depending on the field of study, avoiding to obtain relevant patterns.

% The drawback of using a beam search strategy is that it is a heuristic
% search, thus, it is necessary to define the number of potential subgroups to
% be explored in later phases by generating all successors at the current level,
% sorting them in increased order of heuristic cost and storing only a prede-
% termined number, also known as Beam width. This means that the same
% value of this parameter can result in too large a number of patterns or in an
% insufficient number of patterns, depending on the problem.

On the other hand, the exploration of subgroups in FSSD is done using
the DEPTH-FIRST-SEARCH strategy, which does not have the limitation of discarding patterns when exploring all possible combinations, however, it is still necessary to determine the optimal number of patterns to return, and this poses a problem as described before. Therefore, using a manual threshold does not seem to be a suitable technique to find the most relevant patterns, and removing the necessary fine-tuning of key parameters such as \textit{Beam width} seems to be a well-suited approach in this field. Additionally, even if a correct threshold is manually set, found patterns should represent a balance between the complexity of patterns and dependency on the target variable. Actual approaches of the state of the art are not considering this concept.

Regarding exploration strategy of subgroups search space, both SSD++ and FSSD are based on subgroup lists, which can be defined as the fragmentation of the subgroup population into multiple sections, each of which is represented by a unique group. This feature is not in line with the aim of IGSD, which is to discover how different characteristics may influence each instance in a dataset. Hence, some information may be lost if subgroup lists are used. Also, validation of obtained patterns is an important aspect in SD, since an expert can evaluate the significance of obtained patterns or the relevant variables that should be present in patterns. Thus, SSD++ and FSSD do not allow to specify a set of key attributes to be present in returned patterns. 

%Regarding that SSD++ and FSSD patterns might not contain key attributes, positive validation results from domain experts might not be achieved. Therefore, they should include a feature to fix these key attributes in found patterns.
Finally, several performance indices have been used in the literature to compare different SD algorithm executions (such as for SSD++ and FSSD). However, there is a lack of homogeneity in the indices used and a complete comparison regarding all the measures should be provided to better analyze the performance of the different approaches.

% Moreover, \cite{esnault_q-finder_2020} proposes a innovative method to select the best results. 
% \todo[inline]{We should talk also about limitations of FSSD and SSD++, maybe mentioning Q-finder is not worthy since we didn't included its performance. Add table as in Table 2 of Robust subgroup discovery article.}
% Nonetheless, this might result in associations that do not add any new knowledge to the problem because of the simplicity of the associations. In addition, it might have discarded some associations that are less reliable because the indices’ values which had been decided to consider are not good enough. However, the associations might be beneficial for the research because it shows unknown or little-known information.

% \todo[inline]{How do we justify they are not good enough? What does it mean that associations might be beneficial for the research?}