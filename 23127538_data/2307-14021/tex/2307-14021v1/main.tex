\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[nonatbib,preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tabularray}
\usepackage[normalem]{ulem}
\usepackage{tabularray}

\usepackage[square,numbers]{natbib}

\title{Retinotopy Inspired Brain Encoding Model and the All-for-One Training Recipe}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  \textbf{Huzheng Yang} \quad \textbf{Jianbo Shi} \quad \textbf{James Gee}\\
  Department of Computer and Information Science\\
  University of Pennsylvania\\
  \texttt{\{huze,jshi\}@seas.upenn.edu} \\
  \texttt{gee@upenn.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}

Brain encoding models aim to predict brain voxel-wise responses to stimuli images, replicating brain signals captured by neuroimaging techniques.   There is a large volume of publicly available data, but training a comprehensive brain encoding model is challenging.  The main difficulties stem from a) diversity within individual brain, with functional heterogeneous brain regions; b) diversity of brains from different subjects, due to genetic and developmental differences; c) diversity of imaging modalities and processing pipelines.
We use this diversity to our advantage by introducing the All-for-One training recipe, which divides the challenging one-big-model problem into multiple small models, with the small models aggregating the knowledge while preserving the distinction between the different functional regions. 
Agnostic of the training recipe, we use biological knowledge of the brain, specifically retinotopy, to introduce inductive bias to learn a 3D brain-to-image mapping that ensures a) each neuron knows which image regions and semantic levels to gather information, and b) no neurons are left behind in the model.  


We pre-trained a brain encoding model using over one million data points from five public datasets spanning three imaging modalities. To the best of our knowledge, this is the most comprehensive brain encoding model to the date. We demonstrate the effectiveness of the pre-trained model as a drop-in replacement for commonly used vision backbone models. Furthermore, we demonstrate the application of the model to brain decoding. Code and the model checkpoint will be made available. 
\end{abstract}


% Figure environment removed

% Figure environment removed

\section{Introduction}


There is a growing body of research in neuroscience that utilizes brain encoding models. The model predicts voxel-wise brain response to visual stimuli, and it can be depicted as a multi-task regression problem where each voxel is a task. The brain encoding model serves as a computational counterpart to biological brains \cite{wen_neural_2018}.  The common practice for building brain encoding models is to use pre-trained models from image classification \cite{deng_imagenet_2009}, text-to-image alignment \cite{radford_learning_2021}, or self-supervised tasks \cite{oquab_dinov2_2023}. These pre-trained models may excel at their benchmarked task; however, \cite{schrimpf_brain-score_2018} show that the image-classification benchmark score does not align with prediction performance in brain encoding. 

Building a model from all data sources poses a significant challenge due to heterogeneity in data: a) diversity in functional sub-modules within each brain, b) genetic and developmental differences across subjects, c) inconsistent imaging techniques and pre-processing pipelines. The current best practice is to build Region-of-Interest (ROI)\footnote{ROI refers to brain atlas parcellations} models over subjects from the same dataset \cite{cichy_algonauts_2021} \cite{willeke_sensorium_2022} \cite{allen_massive_2022}, where ROIs are predefined by well-studied anatomical and functional properties of the brain voxels. However, the ROI-model approach lacks the potential benefits for ROIs to aggregate knowledge and collaborate. This issue can be mitigated to some extent by adjusting the granularity of ROIs. This work proposes a multi-stage All-for-One (AFO) training recipe that explicitly lets ROIs aggregate knowledge while keeping the main training objective less challenging than training for one all-ROI model.  Borrowing the idea of `Dark knowledge' distillation \cite{hinton_distilling_2015}, we use denoising to ensure the aggregated knowledge is clean.


Biological domain knowledge of the brain, specifically retinotopy, can be explored to design a better model \cite{lurz_generalization_2021}. The retina cells are physically wired through the optic nerve to the lateral geniculate nucleus, which connects to the visual cortex. Thus, visual cortex cells preserve the topological structure of images projected to the retina. This study explicitly defines a \textit{RetinaMapper} function that replicates retinotopic mapping.  An obvious solution is learning a forward mapping that transforms 2D \textit{RetinaGrid} into a neuron in a 3D brain location.  However, such forward mapping can not guarantee to be surjective: every 3D neuron location is the mapped from at least one 2D  \textit{RetinaGrid}.  Our solution is to model the \textit{RetinaMapper} from the inverse perspective, mapping 3D neuron to 2D \textit{RetinaGrid}.   \textit{RetinaMapper} is learned without ground-truth supervision, but still exhibits retinotopic behavior, as shown in our results. 

A well-reported phenomenon is that neuron voxels are mapped to shallow to deep layers of a feed-forward neuron network \cite{takagi_high-resolution_2022}. This motivates the common practice of selecting the best layers for each voxel. But per-voxel hyper-parameter tuning is highly noisy and prone to overfitting; previous studies overcome this by choosing the same layers for each ROI. In this study, we propose a \textit{LayerSelector} module that enforces spatial proximity, thus allowing a flexible and robust selection of layers.


\section{Related work}

The field of computational neuroscience has been actively exploring the task of brain encoding, highlighting from \cite{kay_identifying_2008} \cite{naselaris_encoding_2011}, surveyed by \cite{wen_neural_2018}. There are several initiatives and benchmarks: The brain-score \cite{schrimpf_brain-score_2018} initiative compares frozen image backbone models using a PCA and linear regression pipeline. The PCA approach allows for a fair comparison of vision models with different latent dimensions. Additionally, \cite{conwell_large-scale_2022} utilized a similar frozen PCA pipeline to benchmark various vision models on the NSD dataset. The Algonauts challenge \cite{cichy_algonauts_2021} benchmarks end-to-end trained model without the constraint of frozen model and PCA dimension reduction. The Sensorium benchmark \cite{willeke_sensorium_2022} worked on invasive mouse V1 imaging data. The Things initiative \cite{hebart_things-data_2023} provides fine-grid image captions which can be used for hypotheses testing. These datasets and benchmarks cover a wide range of imaging modalities, and preprocessing and denoising pipelines \cite{kay_glmdenoise_2013} \cite{prince_improving_2022}. The All-for-One training recipe aims to leverage all of these diverse data sources to pre-train a comprehensive brain encoding model. 

The neuroscience community has extensively applied brain encoding models to unravel the biological mechanisms underlying brain function. \cite{st-yves_brain-optimized_2022} employed transfer learning techniques with brain encoding models to investigate the hierarchical organization of the brain. \cite{franke_state-dependent_2022} applied the model to study color coding in mouse neurons. The NeuroGen framework \cite{gu_neurogen_2022} combined brain encoding models with image generation models, they utilize gradient-based methods to manipulate stimulus images. \cite{bashivan_neural_2019} generated maximally excited images for populations of neurons and presented these images to subjects to validate the conclusions. On the other hand, there are fruitful studies of brain decoding\footnote{We use the term \textit{encoding} for mapping from stimuli image to brain voxels, \textit{decoding} for the reverse.} without a brain encoding model \cite{takagi_high-resolution_2022} \cite{gu_decoding_2023} \cite{lu_minddiffuser_2023} \cite{gu_decoding_2023}. Their framework is to take a pre-trained text-conditioned image generation model \cite{ho_denoising_2020} \cite{rombach_high-resolution_2022}, then train a mapping function that aligns brain patterns to the text-condition embeddings space. However, we argue that decoding without a pre-trained encoding model is less efficient: Firstly, this pipeline is tightly linked to the pre-trained image generation model. Also, this pipeline face challenges in effectively utilizing heterogeneous data from various imaging modalities. We argue that decoding with a frozen encoding model is more efficient as this approach is agnostic to the specific image generation model.

Previous studies also explored incorporating retinotopy into the brain encoding model. \cite{allen_massive_2022} fits Gabor filters of various sizes and locations for each voxel. \cite{lurz_generalization_2021} also employed the \textit{RetinaMapper}, but their work focuses on training with the same imaging modality and one single ROI. In contrast, our approach tries to model the whole visual brain with diverse data sources.

\section{Method}

The voxel-wise encoding model (Fig \ref{fig:method}) comprises three main components: Firstly, the \textbf{backbone} processes the input image and extracts latent image features from its intermediate layers. Next, the \textbf{neck} component compresses the feature vector for each voxel. Finally, the \textbf{head} applies a linear regression model to fit a prediction for each voxel. Let $M^l \in \mathcal{R}^{D \times \frac{H}{k} \times \frac{W}{k}}$ be the feature map output from the frozen backbone, where $l$ is the layer index, $k$ is the down-scale factor, we refer the $\frac{H}{k} \times \frac{W}{k}$ grid as \textit{RetinaGrid}. The brain encoding model can be formulated as learning a mapping function $\mathcal{F}$ (Eq \ref{eq:model}), where $\mathcal{N}$ depends on the imaging modality\footnote{We use a unified term \textit{voxel} to refer to a single smallest element in $\mathcal{N}$.}. $\mathcal{N}_\textit{MRI}:= (X \times Y \times Z) \times 1$, $\mathcal{N}_\textit{EEG}:= C \times T$, $\mathcal{N}_\textit{MEG}:= (X \times Y \times Z) \times T$

\begin{equation}
    \mathcal{F}: \mathcal{R}^{(L \times D) \times \frac{H}{k} \times \frac{W}{k}} \to \mathcal{R}^{\mathcal{N}}
\label{eq:model}
\end{equation}


\subsection{TopyNeck}
\label{sec:m_topyneck}

% \paragraph{TopyNeck} is composed of two sub-modules: \textit{RetinaMapper} and \textit{LayerSelector}.

\paragraph{RetinaMapper} The biological retinotopy process is mapping $f: \mathcal{R}^{\frac{H}{k} \times \frac{W}{k}} \to \mathcal{R}^{X \times Y \times Z}$. \textit{RitinaMapper} aims to replicate this mapping. However, $f$ can not guarantee to be surjective: every 3D neuron location is the mapped from at least one 2D  \textit{RetinaGrid}.  Instead of the forward mapping $f$, we learn a reverse injective mapping $f': \mathcal{R}^{X \times Y \times Z} \to \mathcal{R}^{\frac{H}{k} \times \frac{W}{k}}$ and use \texttt{tanh} activation function to guarantee the output 2D coordinates lies within the \textit{RetinaGrid}. The \textit{RetinaMapper} is formulated as

\begin{equation}
    u = \texttt{tanh}(\texttt{MLP}(\texttt{PE}(p)))
\end{equation}

where $p \in \mathcal{R}^{\mathcal{N} \times 3}$ is the voxel's spatial coordinate, $\texttt{PE}$ is sinusoidal positional encoding function, $u \in \mathcal{R}^{\mathcal{N} \times 2}$ is coordinates in the \textit{RetinaGrid}. During training, a small non-trainable variance $\sigma$ is introduced $u' \sim \mathcal{N}(u, \sigma)$. At inference time $\sigma$ is set to 0. At each $u'$, linear interpolation is performed to obtain a 1-D feature vector $m^l \in \mathcal{R}^{\mathcal{N} \times D}$ for each layer $l$. Furthermore, Another 1-D feature vector $q^l = \texttt{MLP}(\texttt{GlobalAvgPool}(M^l), \texttt{GlobalMaxPool}(M^l))$ is added to $m^l$. Parameters of \textit{RetinaMapper} is shared for all layers.  Figure 2 and 4 show examples of such mapping.  The color dots in RetinaGrid indicate which 3D neuron layers it is from.  The blank area indicates image regions that are unused for prediction.  


\paragraph{LayerSelector} Early visual to downstream regions have growing receptive field sizes and neurons' latent representation of the stimuli image grows abstract. This motivates matching voxels to layers in feed-forward neuron networks. But selecting the best or top layers for each voxel is suspected to be overfitting. \textit{LayerSelector} enforce spatial proximity formulated as
\begin{equation}
    \eta = \texttt{softmax}(\texttt{MLP}(\texttt{PE}(p)))
\end{equation}
where $\eta \in \mathcal{R}^{\mathcal{N} \times L}$. The 1-D feature vectors sampled from various layers at \textit{RetinaGrid} is reduced as $m_i^{*} = \sum_{L} \eta_i^l m_i^l$. Regularization loss ${l}_{ent}=\sum_{L}\eta^l \log \eta^l$ is applied to prevent converging to a local minimum that only selects one single layer.

% Figure environment removed

\subsection{All-for-One training recipe}
\label{sec:m_afo}

Dividing neuron voxels into ROIs loses ROIs' potential to aggregate knowledge and collaborate. Mixing can also negatively affect individual voxel performance, making learning more challenging.
The AFO recipe aims to gather the benefits from both dividing and mixing. Multiple stages models are trained (Figure \ref{fig:afo}): In stage one, each ROI model is trained separately. In stage two, each ROI model is trained to distill the dark knowledge \cite{hinton_distilling_2015} from all other ROIs, but the ground truth loss is only applied on the target ROI, other ROIs are helpers, and their parameters were discarded after training. Model checkpointing and early stopping are conditioned only on the target ROI. In stage three, the final model is trained with all ROIs as outputs, with dark knowledge and ground truth loss. The final product is one comprehensive all-ROI model.


\subsection{Voxel-wise encoding ROI}
\label{sec:m_veroi}

We need a unified ROI parcellation that is defined for all subjects from various imaging modalities. To generate such a unified ROI, we utilize the final linear regression weight, which is extracted from an average of 10 all-ROI models. We start by performing Euclidean distance k-means clustering on the weights to reduce the dimension of voxel counts. Subsequently, Ward's method applies hierarchical clustering to find the cluster centroids. This hierarchical clustering results in a dendrogram.
We cut the dendrogram at a hand-picked threshold to identify the \textit{veROIs}. By adjusting this threshold, we can control the granularity of the \textit{veROIs}.


\section{Experiments}

\subsection{Datasets}

We utilize 7 publicly available datasets for our experiments (Table \ref{tab:data}). Details are provided in \cite{allen_massive_2022} \cite{van_essen_human_2012} \cite{cichy_algonauts_2021} \cite{hebart_things-data_2023} \cite{gifford_large_2022} \cite{chang_bold5000_2019}. We use only voxels from the visual brain. Each dataset was divided into training, validation, and test sets with a ratio around $90:6:4$. We use repetition-averaged brain response. For the Things datasets, we use repeatedly represented images as the test set. All the experiment results are reported from the test set unless specified. The HCP video was split into chunks of 20 seconds to ensure no data leak, and a time delay of 4 seconds between video frames and fMRI frames was applied \cite{khosla_cortical_2021}, blank resting-state segments are not discarded.
For video stimulus, we extracted frames at a rate of one frame per second. We only use one frame for the ALG dataset. 

Notably, except for the NSD dataset, all subjects from other datasets viewed the same set of images. As a compromise for computation intensity, we concatenated the voxels from ALG EEG MEG subjects into each single large brain, voxel's spatial coordinates are placed in an evenly spaced grid. For the HCP dataset, a group average was performed due to the large number of subjects and the lower SNR in each individual subject. All datasets have spatial coordinates for voxels except the EEG dataset, EEG voxel's spatial coordinates are generated from dummy sequential numbers.



% \usepackage{tabularray}
\begin{table}
\centering
\caption{Brain encoding datasets. The term \textit{Datapoints} refers to the number of image stimulus presentations, including repeated presentation of the same image.}
\label{tab:data}
\begin{tblr}{
  cell{1}{2} = {c=5}{},
  cell{1}{7} = {c=2}{},
  hline{1,7} = {-}{0.08em},
  hline{3} = {2-8}{},
  hline{6} = {-}{0.05em},
}
                    & \textbf{ Training Datasets } &              &                    &                &                & \textbf{Holdout Datasets } &                 \\
                    & NSD                          & {HCP\\MOVIE} & {Algonauts \\2021} & {Things\\MEG1} & {Things\\EEG2} & {BOLD\\5000}               & {Things\\fMRI1} \\
\textbf{Datapoints} & 240K                         & 441K         & 30K                & 88K            & 640K           & 20K                        & 24K             \\
\textbf{Subjects}   & 8                            & 184          & 10                 & 4              & 10             & 4                          & 3               \\
\textbf{Voxels}     & 315K                         & 29K          & 13K                & 60K            & 17K            & 9K                         & 19K             \\
\textbf{Modality}   & 7T fMRI                      & 7T fMRI      & 3T fMRI            & MEG            & EEG            & 3T fMRI                    & 3T fMRI         
\end{tblr}
\end{table}

\subsection{TopyNeck probing}
\label{sec:topyneck}

\paragraph{RetinaMapper} In Figure \ref{fig:np}, for NSD subjects, early visual voxels were mapped to span most of the \textit{RetinaGrid}, while downstream-region voxels remained concentrated in the center. The ablation study presented in Table \ref{tab:topyneck} further demonstrates the outstanding importance of the \textit{RetinaMapper} for early visual voxels in NSD subjects. This alignment with retinotopy design motivation. However, for other low SNR datasets, no clear retinotopic mapping was observed, suggesting that the \textit{RetinaMapper} may not be necessary in such cases, and a constant mapping to the center could be sufficient.


\paragraph{LayerSelector} In Figure \ref{fig:layer}, for subject NSD\_01, a smooth transition from shallow to deep layers was observed. This alignment with the design motivation. Ablation study in Table \ref{tab:topyneck} also indicates significant improvement for NSD subjects compared to un-weighted averaging layers or selecting a single layer. However, for low SNR datasets, the trend was to select only the last layer (Figure \ref{fig:np}), suggesting that the \textit{LayerSelector} module may not be necessary in such cases.

% Figure environment removed

% Figure environment removed

% \usepackage{color}
% \usepackage[normalem]{ulem}
% \usepackage{tabularray}
\begin{table}
\caption{\textit{TopyNeck} ablation study. The reported numbers are the average Pearson correlation coefficient across all voxels. Results are averaged over three runs. \textit{FrozenRM} maps every voxel to the center, \textit{FrozenLS} outputs uniform weight for each layer. \textit{NoRegLS} selects a single layer.}
\label{tab:topyneck}
\centering
\begin{tblr}{
  cell{1}{2} = {c=4}{},
  cell{1}{6} = {c=4}{},
  hline{1,8} = {-}{0.08em},
  hline{2-3} = {2-10}{0.03em},
}
\textbf{Subject}    & NSD\_01        &                &                &                & NSD\_08        &                &                &                & EEG            \\
\textbf{ROI}        & all            & early          & late           & mid            & all            & early          & late           & mid            & all            \\
\textbf{FullTopyNeck}        & \textbf{0.462} & \textbf{0.515} & \textbf{0.435} & \textbf{0.470} & 0.291 & \textbf{0.304} & 0.285          & 0.292          & 0.228          \\
FrozenRM  & \uline{0.441}  & \uline{0.476}  & 0.422          & \uline{0.452}  & \uline{0.274}  & \uline{0.261}  & 0.280          & \uline{0.272}  & 0.226          \\
w/o GlobalPool   & 0.457          & 0.513          & 0.428          & 0.467          & \textbf{0.293}          & 0.303          & \textbf{0.289} & \textbf{0.295} & \textbf{0.230} \\
FrozenLS & 0.451          & 0.512          & 0.419          & 0.466          & 0.280          & 0.300          & \uline{0.270}  & 0.279          & \uline{0.224}  \\
NoRegLS  & 0.447          & 0.505          & \uline{0.417}  & 0.464          & 0.287          & 0.299          & 0.282          & 0.284          & 0.229          
\end{tblr}
\end{table}

\subsection{All-for-One recipe results}
\label{sec:afo}

In Table \ref{tab:afo}, a significant performance gap between the S1 and S2 models indicates the effectiveness of aggregating knowledge among ROIs. We also study a randROI that has the exact same number of ROIs and number of voxels inside each ROI. S1 and S2 gap is not observed in the randROI approach, as randROI already covers all types of voxels in every ROI. Furthermore, the model trained with ground truth (NoDK) as helpers shows little to no improvement over the S1 model. This suggests that the quality of the helper ROI is critical for the AFO recipe, as involving noisy helpers makes the training process unnecessarily challenging. In this context, dark knowledge plays a crucial role as denoising. However, solely dark knowledge distillation doesn't have a great impact as can be inferred from the small gap between randROI S1 and S2 models.

% \usepackage{color}
% \usepackage[normalem]{ulem}
% \usepackage{tabularray}
\begin{table}
\caption{\textit{All-for-One} training recipe ablation study. The reported numbers are the average Pearson correlation coefficient across all voxels, NSD(NC) is the median of noise-normalized score. \textit{NaiveMix} train one all-ROI model. \textit{NoDK} use ground truth as helpers. \textit{randROI} and \textit{veROI} has the exact same size. \textit{S2+1} indicates one extra iteration of stage2. $b$ is number of parameters in the convolution blocks, $n$ is number of voxels, $d$ is feature dimension, $r$ is number of ROIs.}
\label{tab:afo}
\centering
\begin{tblr}{
  cell{1}{3} = {c=7}{},
  hline{1,11} = {-}{0.08em},
  hline{3} = {3-9}{0.03em},
}
\textbf{Method}           & \textbf{\# Params} & \textbf{Dataset(s)} &                &                &                &                &                &                \\
                          &                    & NSD                 & EEG            & MEG            & HCP            & ALG            & \textbf{ALL}   & {NSD\\(NC)}    \\
NaiveMix                  & $b + nd$           & \uline{0.422}       & \uline{0.212}  & \uline{0.180}  & \uline{0.340}  & \uline{0.256}  & \uline{0.367}  & \uline{0.560}  \\
veROIS1                 & $rb + nd$          & 0.425               & \uline{0.212}  & 0.194          & 0.346          & 0.265          & 0.371          & 0.567          \\
veROIS2                 & $rb + nd$          & 0.433               & 0.222          & 0.209          & 0.365          & 0.266          & 0.380          & 0.588          \\
\textbf{veROIS3} & $b + nd$           & \textbf{0.435}      & 0.225          & 0.210          & \textbf{0.366} & \textbf{0.267} & \textbf{0.382} & \textbf{0.593} \\
veROIS2+1                 & $rb + nd$          & 0.432               & \textbf{0.226} & \textbf{0.211} & 0.362          & 0.264          & 0.380          & 0.586          \\
NoDK           & $rb + nd$          & 0.426               & 0.216          & 0.186          & 0.349          & \uline{0.256}  & 0.371          & 0.569          \\
randROIS1             & $rb + nd$          & 0.431               & 0.216          & 0.207          & 0.343          & 0.258          & 0.377          & 0.584          \\
randROIS2             & $rb + nd$          & 0.432               & 0.220          & 0.207          & 0.348          & 0.259          & 0.378          & 0.586          
\end{tblr}
\end{table}

\subsection{veROI results}
\label{sec:veroi}

Figure \ref{fig:veroi} shows veROI on cortex across all NSD subjects, early visual areas is centered around veROI\_5 (blue) and downstream areas centered around veROI\_9 (green), voxels that drop out from the field of view in early visual areas are centered around veROI\_16 (red). The score for each veROI for subject NSD\_01 can be found in Figure \ref{fig:retr}, where veROI\_12 onward is mainly for the low SNR voxels. From the heatmap in Figure \ref{fig:method} we can also observe that veROI\_12 onward is mainly HCP, EEG, and MEG subjects.

% Figure environment removed

\subsection{Brain decoding}
\label{sec:decode}

\paragraph{Methods} In this study, brain decoding refers to the task of ranking and retrieving candidate images from a candidate set, retrieved images are to match a given brain response pattern. The decoding pipeline involves forwarding each candidate image through the brain encoding model and measuring Pearson's correlation coefficient between the model's prediction and the ground truth. 

\paragraph{Results} The experiments are conducted on 500 validation images as candidate images. As a qualitative analysis, Figure \ref{fig:decode} and Figure \ref{fig:veroi_top4} demonstrate that when conditioning on the early visual area or veROI\_5, texture and orientation are more preserved in the decoded images. Conversely, when conditioning on downstream ROIs, semantic concepts are more preserved. Additionally, Figure \ref{fig:retr} shows that image retrieval achieves high accuracy when conditioned on early ROIs.
Quantitative exploration of the functional roles of ROIs is beyond the scope of this study. Future work may involve investigating semantic concepts with image generation models. Furthermore, the gradient of the encoding model can be utilized to facilitate image generation and manipulation.

% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Implementation details}

We use smooth L1 loss with $\beta = 0.01$, regulirazation loss $l_{ent}$ is scaled down by $\lambda = 0.00003$. AdaBelief optimizer \cite{zhuang_adabelief_2020} is employed with $\texttt{lr} = 0.003$, $\texttt{batchsize} = 128$, $\texttt{weight\_decay} = 0.0001$, $(\beta_1, \beta_2) = (0.9, 0.999)$. Notably, we mix subjects in one mini-batch, and the effective batch size for each subject is less than the total. Due to memory constrain, we randomly sample up to 8000 voxels for each training datapoint, there is 436,715 voxels totaling all subjects. Early stopping is configured with $\texttt{patience} = 20$ epochs, we define one epoch as $10\%$ of the total training data. Greedy \textit{Model Soup} \cite{wortsman_model_2022} is applied at the top 10 validation checkpoints. Backbone is kept frozen except \texttt{LayerNorm} running statistics is updated. Input resolution is $224 \times 224$ and the feature from backbone layers are all of the size $768 \times 16 \times 16$. The attached trainable convolution block is three zero-padded 5x5 convolutions with skip connection and \texttt{LayerNorm}, $C = 768$. The last convolution layer reduces the dimension to $D = 256$. We trained all models on single NVIDIA RTX 2080 Ti 12GB GPUs at a reduced clock speed of 1140Mhz, single-subject all-ROI models consume half to 1 GPU hour, all-subject single-ROI models consume 3 to 5 GPU hours, all-subject all-ROI models consume 10 GPU hours. The complete AFO recipe total around 300 GPU hours. Models are trained Pytorch Lightning \cite{falcon_pytorch_2019} mixed precision FP16. To boost training speed, MLPs in \textit{RetinaMapper} and \textit{LayerSelector} are pre-optimized by a single-subject all-ROI model, they are loaded and kept frozen in the AFO recipe, this gives 2 times faster convergence speed.

\section{Conclusion and Limitations}


We proposed the \textit{AFO} recipe alongside \textit{veROI} to address the issue of heterogeneity in publicly available datasets. To the best of our knowledge, our pre-trained model constructed with over 1 million data points is the most comprehensive brain encoding model to date. The model shows superior performance when transferred to small hold-out datasets. As demonstrated by our brain decoding experiments, the pre-trained model could facilitate further neuroscience research. 

We designed \textit{TopyNeck} inspired by retinotopy, which showed retinotopic behavior despite having no ground truth supervision for the retinotopic mapping function. However, the retinotopic behavior diminishes when the target dataset SNR is low, e.g. EEG, MEG.   This suggests a simple alternative approach is sufficient in such a case.

A fundamental limitation for this study is that: we use repetition-averaged brain response as prediction target, this is a common simplification that assume brain response from each repetition is consistent. However, this fundamental assumption is not true if we consider: 1. memory state that depends on previously presented images. 2. brain state which depends on the time of the day and relative time of each scanning session. 3. behavior response such as button press. 4. gaze pattern captured by eye tracking camera or eye voxels.

% \bibliographystyle{apalike}
\bibliographystyle{plainnat}
\bibliography{refs}


\end{document}