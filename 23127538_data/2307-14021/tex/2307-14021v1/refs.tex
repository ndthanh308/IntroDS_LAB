
@article{naselaris_encoding_2011,
	title = {Encoding and decoding in {fMRI}},
	volume = {56},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811910010657},
	doi = {10.1016/j.neuroimage.2010.07.073},
	abstract = {Over the past decade fMRI researchers have developed increasingly sensitive techniques for analyzing the information represented in BOLD activity. The most popular of these techniques is linear classiﬁcation, a simple technique for decoding information about experimental stimuli or tasks from patterns of activity across an array of voxels. A more recent development is the voxel-based encoding model, which describes the information about the stimulus or task that is represented in the activity of single voxels. Encoding and decoding are complementary operations: encoding uses stimuli to predict activity while decoding uses activity to predict information about the stimuli. However, in practice these two operations are often confused, and their respective strengths and weaknesses have not been made clear. Here we use the concept of a linearizing feature space to clarify the relationship between encoding and decoding. We show that encoding and decoding operations can both be used to investigate some of the most common questions about how information is represented in the brain. However, focusing on encoding models offers two important advantages over decoding. First, an encoding model can in principle provide a complete functional description of a region of interest, while a decoding model can provide only a partial description. Second, while it is straightforward to derive an optimal decoding model from an encoding model it is much more difﬁcult to derive an encoding model from a decoding model. We propose a systematic modeling approach that begins by estimating an encoding model for every voxel in a scan and ends by using the estimated encoding models to perform decoding.},
	language = {en},
	number = {2},
	urldate = {2022-10-18},
	journal = {NeuroImage},
	author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
	month = may,
	year = {2011},
	pages = {400--410},
	file = {Naselaris et al. - 2011 - Encoding and decoding in fMRI.pdf:C\:\\Users\\Yoo\\Zotero\\storage\\GANX3R4X\\Naselaris et al. - 2011 - Encoding and decoding in fMRI.pdf:application/pdf},
}

@article{wen_neural_2018,
	title = {Neural {Encoding} and {Decoding} with {Deep} {Learning} for {Dynamic} {Natural} {Vision}},
	volume = {28},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article/28/12/4136/4560155},
	doi = {10.1093/cercor/bhx268},
	abstract = {Convolutional neural network (CNN) driven by image recognition has been shown to be able to explain cortical responses to static pictures at ventral-stream areas. Here, we further showed that such CNN could reliably predict and decode functional magnetic resonance imaging data from humans watching natural movies, despite its lack of any mechanism to account for temporal dynamics or feedback processing. Using separate data, encoding and decoding models were developed and evaluated for describing the bi-directional relationships between the CNN and the brain. Through the encoding models, the CNN-predicted areas covered not only the ventral stream, but also the dorsal stream, albeit to a lesser degree; single-voxel response was visualized as the speciﬁc pixel pattern that drove the response, revealing the distinct representation of individual cortical location; cortical activation was synthesized from natural images with high-throughput to map category representation, contrast, and selectivity. Through the decoding models, fMRI signals were directly decoded to estimate the feature representations in both visual and semantic spaces, for direct visual reconstruction and semantic categorization, respectively. These results corroborate, generalize, and extend previous ﬁndings, and highlight the value of using deep learning, as an all-in-one model of the visual cortex, to understand and decode natural vision.},
	language = {en},
	number = {12},
	urldate = {2022-10-18},
	journal = {Cerebral Cortex},
	author = {Wen, Haiguang and Shi, Junxing and Zhang, Yizhen and Lu, Kun-Han and Cao, Jiayue and Liu, Zhongming},
	month = dec,
	year = {2018},
	pages = {4136--4160},
	file = {Wen et al. - 2018 - Neural Encoding and Decoding with Deep Learning fo.pdf:C\:\\Users\\Yoo\\Zotero\\storage\\7LKM5XYC\\Wen et al. - 2018 - Neural Encoding and Decoding with Deep Learning fo.pdf:application/pdf},
}

@article{kay_identifying_2008,
	title = {Identifying natural images from human brain activity},
	volume = {452},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature06713},
	doi = {10.1038/nature06713},
	language = {en},
	number = {7185},
	urldate = {2022-10-18},
	journal = {Nature},
	author = {Kay, Kendrick N. and Naselaris, Thomas and Prenger, Ryan J. and Gallant, Jack L.},
	month = mar,
	year = {2008},
	pages = {352--355},
	file = {Kay et al. - 2008 - Identifying natural images from human brain activi.pdf:C\:\\Users\\Yoo\\Zotero\\storage\\9SZL8RIT\\Kay et al. - 2008 - Identifying natural images from human brain activi.pdf:application/pdf},
}

@article{kay_glmdenoise_2013,
	title = {{GLMdenoise}: a fast, automated technique for denoising task-based {fMRI} data},
	volume = {7},
	issn = {1662-453X},
	shorttitle = {{GLMdenoise}},
	url = {http://journal.frontiersin.org/article/10.3389/fnins.2013.00247/abstract},
	doi = {10.3389/fnins.2013.00247},
	abstract = {In task-based functional magnetic resonance imaging (fMRI), researchers seek to measure fMRI signals related to a given task or condition. In many circumstances, measuring this signal of interest is limited by noise. In this study, we present GLMdenoise, a technique that improves signal-to-noise ratio (SNR) by entering noise regressors into a general linear model (GLM) analysis of fMRI data. The noise regressors are derived by conducting an initial model ﬁt to determine voxels unrelated to the experimental paradigm, performing principal components analysis (PCA) on the time-series of these voxels, and using cross-validation to select the optimal number of principal components to use as noise regressors. Due to the use of data resampling, GLMdenoise requires and is best suited for datasets involving multiple runs (where conditions repeat across runs). We show that GLMdenoise consistently improves cross-validation accuracy of GLM estimates on a variety of event-related experimental datasets and is accompanied by substantial gains in SNR. To promote practical application of methods, we provide MATLAB code implementing GLMdenoise. Furthermore, to help compare GLMdenoise to other denoising methods, we present the Denoise Benchmark (DNB), a public database and architecture for evaluating denoising methods. The DNB consists of the datasets described in this paper, a code framework that enables automatic evaluation of a denoising method, and implementations of several denoising methods, including GLMdenoise, the use of motion parameters as noise regressors, ICA-based denoising, and RETROICOR/RVHRCOR. Using the DNB, we ﬁnd that GLMdenoise performs best out of all of the denoising methods we tested.},
	language = {en},
	urldate = {2022-10-18},
	journal = {Frontiers in Neuroscience},
	author = {Kay, Kendrick N. and Rokem, Ariel and Winawer, Jonathan and Dougherty, Robert F. and Wandell, Brian A.},
	year = {2013},
	file = {Kay et al. - 2013 - GLMdenoise a fast, automated technique for denois.pdf:C\:\\Users\\Yoo\\Zotero\\storage\\URVAFDB4\\Kay et al. - 2013 - GLMdenoise a fast, automated technique for denois.pdf:application/pdf},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\URB6X63F\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\HJD6SI24\\2103.html:text/html},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\4LNJFCQH\\Oquab et al. - 2023 - DINOv2 Learning Robust Visual Features without Su.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\7M6D9AHD\\2304.html:text/html},
}

@article{allen_massive_2022,
	title = {A massive {7T} {fMRI} dataset to bridge cognitive neuroscience and artificial intelligence},
	volume = {25},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00962-x},
	doi = {10.1038/s41593-021-00962-x},
	abstract = {Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2023-05-17},
	journal = {Nature Neuroscience},
	author = {Allen, Emily J. and St-Yves, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cortex, Neural encoding, Object vision, Perception},
	pages = {116--126},
	file = {Submitted Version:C\:\\Users\\Yoo\\Zotero\\storage\\IAVRXHFS\\Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf:application/pdf},
}

@misc{willeke_sensorium_2022,
	title = {The {Sensorium} competition on predicting large-scale mouse primary visual cortex activity},
	url = {http://arxiv.org/abs/2206.08666},
	doi = {10.48550/arXiv.2206.08666},
	abstract = {The neural underpinning of the biological visual system is challenging to study experimentally, in particular as the neuronal activity becomes increasingly nonlinear with respect to visual input. Artificial neural networks (ANNs) can serve a variety of goals for improving our understanding of this complex system, not only serving as predictive digital twins of sensory cortex for novel hypothesis generation in silico, but also incorporating bio-inspired architectural motifs to progressively bridge the gap between biological and machine vision. The mouse has recently emerged as a popular model system to study visual information processing, but no standardized large-scale benchmark to identify state-of-the-art models of the mouse visual system has been established. To fill this gap, we propose the Sensorium benchmark competition. We collected a large-scale dataset from mouse primary visual cortex containing the responses of more than 28,000 neurons across seven mice stimulated with thousands of natural images, together with simultaneous behavioral measurements that include running speed, pupil dilation, and eye movements. The benchmark challenge will rank models based on predictive performance for neuronal responses on a held-out test set, and includes two tracks for model input limited to either stimulus only (Sensorium) or stimulus plus behavior (Sensorium+). We provide a starting kit to lower the barrier for entry, including tutorials, pre-trained baseline models, and APIs with one line commands for data loading and submission. We would like to see this as a starting point for regular challenges and data releases, and as a standard tool for measuring progress in large-scale neural system identification models of the mouse visual system and beyond.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Willeke, Konstantin F. and Fahey, Paul G. and Bashiri, Mohammad and Pede, Laura and Burg, Max F. and Blessing, Christoph and Cadena, Santiago A. and Ding, Zhiwei and Lurz, Konstantin-Klemens and Ponder, Kayla and Muhammad, Taliah and Patel, Saumil S. and Ecker, Alexander S. and Tolias, Andreas S. and Sinz, Fabian H.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08666 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: NeurIPS 2022 Competition Track},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\PYAVLQ2D\\Willeke et al. - 2022 - The Sensorium competition on predicting large-scal.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\DTRBRZBX\\2206.html:text/html},
}

@article{glasser_multi-modal_2016,
	title = {A multi-modal parcellation of human cerebral cortex},
	volume = {536},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4990127/},
	doi = {10.1038/nature18933},
	abstract = {Understanding the amazingly complex human cerebral cortex requires a map (or parcellation) of its major subdivisions, known as cortical areas. Making an accurate areal map has been a century-old objective in neuroscience. Using multi-modal magnetic resonance images from the Human Connectome Project (HCP) and an objective semi-automated neuroanatomical approach, we delineated 180 areas per hemisphere bounded by sharp changes in cortical architecture, function, connectivity, and/or topography in a precisely aligned group average of 210 healthy young adults. We characterized 97 new areas and 83 areas previously reported using post-mortem microscopy or other specialized study-specific approaches. To enable automated delineation and identification of these areas in new HCP subjects and in future studies, we trained a machine-learning classifier to recognize the multi-modal ‘fingerprint’ of each cortical area. This classifier detected the presence of 96.6\% of the cortical areas in new subjects, replicated the group parcellation, and could correctly locate areas in individuals with atypical parcellations. The freely available parcellation and classifier will enable substantially improved neuroanatomical precision for studies of the structural and functional organization of human cerebral cortex and its variation across individuals and in development, aging, and disease.},
	number = {7615},
	urldate = {2023-05-17},
	journal = {Nature},
	author = {Glasser, Matthew F and Coalson, Timothy S and Robinson, Emma C and Hacker, Carl D and Harwell, John and Yacoub, Essa and Ugurbil, Kamil and Andersson, Jesper and Beckmann, Christian F and Jenkinson, Mark and Smith, Stephen M and Van Essen, David C},
	month = aug,
	year = {2016},
	pmid = {27437579},
	pmcid = {PMC4990127},
	pages = {171--178},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\H4NZAW3B\\Glasser et al. - 2016 - A multi-modal parcellation of human cerebral corte.pdf:application/pdf},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\6WJELHR7\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\RSSYDGK3\\1503.html:text/html},
}

@misc{wang_incorporating_2022,
	title = {Incorporating natural language into vision models improves prediction and understanding of higher visual cortex},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2022.09.27.508760v1},
	doi = {10.1101/2022.09.27.508760},
	abstract = {We hypothesize that high-level visual representations contain more than the representation of individual categories: they represent complex semantic information inherent in scenes that is most relevant for interaction with the world. Consequently, multimodal models such as Contrastive Language-Image Pre-training (CLIP) which construct image embeddings to best match embeddings of image captions should better predict neural responses in visual cortex, since image captions typically contain the most semantically relevant information in an image for humans. We extracted image features using CLIP, which encodes visual concepts with supervision from natural language captions. We then used voxelwise encoding models based on CLIP features to predict brain responses to real-world images from the Natural Scenes Dataset. CLIP explains up to R2 = 78\% of variance in stimulus-evoked responses from individual voxels in the held out test data. CLIP also explains greater unique variance in higher-level visual areas compared to models trained only with image/label pairs (ImageNet trained ResNet) or text (BERT). Visualizations of model embeddings and Principal Component Analysis (PCA) reveal that, with the use of captions, CLIP captures both global and fine-grained semantic dimensions represented within visual cortex. Based on these novel results, we suggest that human understanding of their environment form an important dimension of visual representation.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {bioRxiv},
	author = {Wang, Aria Y. and Kay, Kendrick and Naselaris, Thomas and Tarr, Michael J. and Wehbe, Leila},
	month = sep,
	year = {2022},
	note = {Pages: 2022.09.27.508760
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\WPTKJIHY\\Wang et al. - 2022 - Incorporating natural language into vision models .pdf:application/pdf},
}

@misc{lu_minddiffuser_2023,
	title = {{MindDiffuser}: {Controlled} {Image} {Reconstruction} from {Human} {Brain} {Activity} with {Semantic} and {Structural} {Diffusion}},
	shorttitle = {{MindDiffuser}},
	url = {http://arxiv.org/abs/2303.14139},
	abstract = {Reconstructing visual stimuli from measured functional magnetic resonance imaging (fMRI) has been a meaningful and challenging task. Previous studies have successfully achieved reconstructions with structures similar to the original images, such as the outlines and size of some natural images. However, these reconstructions lack explicit semantic information and are difficult to discern. In recent years, many studies have utilized multi-modal pre-trained models with stronger generative capabilities to reconstruct images that are semantically similar to the original ones. However, these images have uncontrollable structural information such as position and orientation. To address both of the aforementioned issues simultaneously, we propose a two-stage image reconstruction model called MindDiffuser, utilizing Stable Diffusion. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into the image-to-image process of Stable Diffusion, which yields a preliminary image that contains semantic and structural information. In Stage 2, we utilize the low-level CLIP visual features decoded from fMRI as supervisory information, and continually adjust the two features in Stage 1 through backpropagation to align the structural information. The results of both qualitative and quantitative analyses demonstrate that our proposed model has surpassed the current state-of-the-art models in terms of reconstruction results on Natural Scenes Dataset (NSD). Furthermore, the results of ablation experiments indicate that each component of our model is effective for image reconstruction.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Lu, Yizhuo and Du, Changde and Wang, Dianpeng and He, Huiguang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14139 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\G9URS3PT\\2303.html:text/html;Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\E8ILB3GZ\\Lu et al. - 2023 - MindDiffuser Controlled Image Reconstruction from.pdf:application/pdf},
}

@misc{gu_modulating_2023,
	title = {Modulating human brain responses via optimal natural image selection and synthetic image generation},
	url = {http://arxiv.org/abs/2304.09225},
	abstract = {Understanding how human brains interpret and process information is important. Here, we investigated the selectivity and inter-individual differences in human brain responses to images via functional MRI. In our first experiment, we found that images predicted to achieve maximal activations using a group level encoding model evoke higher responses than images predicted to achieve average activations, and the activation gain is positively associated with the encoding model accuracy. Furthermore, aTLfaces and FBA1 had higher activation in response to maximal synthetic images compared to maximal natural images. In our second experiment, we found that synthetic images derived using a personalized encoding model elicited higher responses compared to synthetic images from group-level or other subjects' encoding models. The finding of aTLfaces favoring synthetic images than natural images was also replicated. Our results indicate the possibility of using data-driven and generative approaches to modulate macro-scale brain region responses and probe inter-individual differences in and functional specialization of the human visual system.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Gu, Zijin and Jamison, Keith and Sabuncu, Mert R. and Kuceyeski, Amy},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09225 [q-bio]},
	keywords = {Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\KH2GS4RQ\\2304.html:text/html;Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\U26329LL\\Gu et al. - 2023 - Modulating human brain responses via optimal natur.pdf:application/pdf},
}

@misc{kneeland_reconstructing_2023,
	title = {Reconstructing seen images from human brain activity via guided stochastic search},
	url = {http://arxiv.org/abs/2305.00556},
	abstract = {Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succinct new way to measure the diversity of representations across visual brain areas.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Kneeland, Reese and Ojeda, Jordyn and St-Yves, Ghislain and Naselaris, Thomas},
	month = may,
	year = {2023},
	note = {arXiv:2305.00556 [cs, eess, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 4 pages, 5 figures, submitted to the 2023 Conference on Cognitive Computational Neuroscience},
	file = {arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\WYQF8MBC\\2305.html:text/html;Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\R7CGVIYL\\Kneeland et al. - 2023 - Reconstructing seen images from human brain activi.pdf:application/pdf},
}

@misc{ozcelik_brain-diffuser_2023,
	title = {Brain-{Diffuser}: {Natural} scene reconstruction from {fMRI} signals using generative latent diffusion},
	shorttitle = {Brain-{Diffuser}},
	url = {http://arxiv.org/abs/2303.05334},
	doi = {10.48550/arXiv.2303.05334},
	abstract = {In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text and visual) features, to generate final reconstructed images. On the publicly available Natural Scenes Dataset benchmark, our method outperforms previous models both qualitatively and quantitatively. When applied to synthetic fMRI patterns generated from individual ROI (region-of-interest) masks, our trained model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific knowledge. Thus, the proposed methodology can have an impact on both applied (e.g. brain-computer interface) and fundamental neuroscience.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Ozcelik, Furkan and VanRullen, Rufin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.05334 [cs, q-bio]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\JZZLB5KC\\Ozcelik and VanRullen - 2023 - Brain-Diffuser Natural scene reconstruction from .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\9UNT885G\\2303.html:text/html},
}

@misc{gifford_algonauts_2023,
	title = {The {Algonauts} {Project} 2023 {Challenge}: {How} the {Human} {Brain} {Makes} {Sense} of {Natural} {Scenes}},
	shorttitle = {The {Algonauts} {Project} 2023 {Challenge}},
	url = {http://arxiv.org/abs/2301.03198},
	doi = {10.48550/arXiv.2301.03198},
	abstract = {The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to {\textasciitilde}73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Gifford, A. T. and Lahner, B. and Saba-Sadiya, S. and Vilas, M. G. and Lascelles, A. and Oliva, A. and Kay, K. and Roig, G. and Cichy, R. M.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03198 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 5 pages, 2 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\PQBLAL6U\\Gifford et al. - 2023 - The Algonauts Project 2023 Challenge How the Huma.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\CM3CRT9F\\2301.html:text/html},
}

@misc{gu_decoding_2023,
	title = {Decoding natural image stimuli from {fMRI} data with a surface-based convolutional network},
	url = {http://arxiv.org/abs/2212.02409},
	doi = {10.48550/arXiv.2212.02409},
	abstract = {Due to the low signal-to-noise ratio and limited resolution of functional MRI data, and the high complexity of natural images, reconstructing a visual stimulus from human brain fMRI measurements is a challenging task. In this work, we propose a novel approach for this task, which we call Cortex2Image, to decode visual stimuli with high semantic fidelity and rich fine-grained detail. In particular, we train a surface-based convolutional network model that maps from brain response to semantic image features first (Cortex2Semantic). We then combine this model with a high-quality image generator (Instance-Conditioned GAN) to train another mapping from brain response to fine-grained image features using a variational approach (Cortex2Detail). Image reconstructions obtained by our proposed method achieve state-of-the-art semantic fidelity, while yielding good fine-grained similarity with the ground-truth stimulus. Our code is available at: https://github.com/zijin-gu/meshconv-decoding.git.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Gu, Zijin and Jamison, Keith and Kuceyeski, Amy and Sabuncu, Mert},
	month = mar,
	year = {2023},
	note = {arXiv:2212.02409 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\UIZL2VF2\\Gu et al. - 2023 - Decoding natural image stimuli from fMRI data with.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\TMVA4U5Y\\2212.html:text/html},
}

@misc{takagi_high-resolution_2022,
	title = {High-resolution image reconstruction with latent diffusion models from human brain activity},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.18.517004v1},
	doi = {10.1101/2022.11.18.517004},
	abstract = {Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straight-forward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at this https URL.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {bioRxiv},
	author = {Takagi, Yu and Nishimoto, Shinji},
	month = nov,
	year = {2022},
	note = {Pages: 2022.11.18.517004
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\ZQPG9YH8\\Takagi and Nishimoto - 2022 - High-resolution image reconstruction with latent d.pdf:application/pdf},
}

@misc{schrimpf_brain-score_2018,
	title = {Brain-{Score}: {Which} {Artificial} {Neural} {Network} for {Object} {Recognition} is most {Brain}-{Like}?},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Brain-{Score}},
	url = {https://www.biorxiv.org/content/10.1101/407007v1},
	doi = {10.1101/407007},
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score – a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain’s mechanisms for core object recognition – and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at ≥ 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain’s network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {bioRxiv},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = sep,
	year = {2018},
	note = {Pages: 407007
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\4VAKKDEE\\Schrimpf et al. - 2018 - Brain-Score Which Artificial Neural Network for O.pdf:application/pdf},
}

@article{hebart_things_2019,
	title = {{THINGS}: {A} database of 1,854 object concepts and more than 26,000 naturalistic object images},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {{THINGS}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223792},
	doi = {10.1371/journal.pone.0223792},
	abstract = {In recent years, the use of a large number of object concepts and naturalistic object images has been growing strongly in cognitive neuroscience research. Classical databases of object concepts are based mostly on a manually curated set of concepts. Further, databases of naturalistic object images typically consist of single images of objects cropped from their background, or a large number of naturalistic images of varying quality, requiring elaborate manual image curation. Here we provide a set of 1,854 diverse object concepts sampled systematically from concrete picturable and nameable nouns in the American English language. Using these object concepts, we conducted a large-scale web image search to compile a database of 26,107 high-quality naturalistic images of those objects, with 12 or more object images per concept and all images cropped to square size. Using crowdsourcing, we provide higher-level category membership for the 27 most common categories and validate them by relating them to representations in a semantic embedding derived from large text corpora. Finally, by feeding images through a deep convolutional neural network, we demonstrate that they exhibit high selectivity for different object concepts, while at the same time preserving variability of different object images within each concept. Together, the THINGS database provides a rich resource of object concepts and object images and offers a tool for both systematic and large-scale naturalistic research in the fields of psychology, neuroscience, and computer science.},
	language = {en},
	number = {10},
	urldate = {2023-05-17},
	journal = {PLOS ONE},
	author = {Hebart, Martin N. and Dickter, Adam H. and Kidder, Alexis and Kwok, Wan Y. and Corriveau, Anna and Wicklin, Caitlin Van and Baker, Chris I.},
	month = oct,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Computational neuroscience, Computer and information sciences, Computer imaging, Graphical user interfaces, Neural networks, Semantics, Vision, Visual object recognition},
	pages = {e0223792},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\IJBJCA62\\Hebart et al. - 2019 - THINGS A database of 1,854 object concepts and mo.pdf:application/pdf},
}

@article{hebart_things-data_2023,
	title = {{THINGS}-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.82580},
	doi = {10.7554/eLife.82580},
	abstract = {Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-data, a multimodal collection of large-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-data is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-driven and data-driven applications. THINGS-data constitutes the core public release of the THINGS initiative (https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.},
	urldate = {2023-05-17},
	journal = {eLife},
	author = {Hebart, Martin N and Contier, Oliver and Teichmann, Lina and Rockter, Adam H and Zheng, Charles Y and Kidder, Alexis and Corriveau, Anna and Vaziri-Pashkam, Maryam and Baker, Chris I},
	editor = {Barense, Morgan and de Lange, Floris P and Konkle, Talia and Glerean, Enrico},
	month = feb,
	year = {2023},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {behavior, fMRI, MEG, objects, research data, vision},
	pages = {e82580},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\8IA235GP\\Hebart et al. - 2023 - THINGS-data, a multimodal collection of large-scal.pdf:application/pdf},
}

@article{hebart_things-data_2023-1,
	title = {{THINGS}-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.82580},
	doi = {10.7554/eLife.82580},
	abstract = {Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-data, a multimodal collection of large-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-data is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-driven and data-driven applications. THINGS-data constitutes the core public release of the THINGS initiative (https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.},
	urldate = {2023-05-17},
	journal = {eLife},
	author = {Hebart, Martin N and Contier, Oliver and Teichmann, Lina and Rockter, Adam H and Zheng, Charles Y and Kidder, Alexis and Corriveau, Anna and Vaziri-Pashkam, Maryam and Baker, Chris I},
	editor = {Barense, Morgan and de Lange, Floris P and Konkle, Talia and Glerean, Enrico},
	month = feb,
	year = {2023},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {behavior, fMRI, MEG, objects, research data, vision},
	pages = {e82580},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\GM8V2525\\Hebart et al. - 2023 - THINGS-data, a multimodal collection of large-scal.pdf:application/pdf},
}

@article{gifford_large_2022,
	title = {A large and rich {EEG} dataset for modeling human visual object recognition},
	volume = {264},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811922008758},
	doi = {10.1016/j.neuroimage.2022.119754},
	abstract = {The human brain achieves visual object recognition through multiple stages of linear and nonlinear transformations operating at a millisecond scale. To predict and explain these rapid transformations, computational neuroscientists employ machine learning modeling techniques. However, state-of-the-art models require massive amounts of data to properly train, and to the present day there is a lack of vast brain datasets which extensively sample the temporal dynamics of visual object recognition. Here we collected a large and rich dataset of high temporal resolution EEG responses to images of objects on a natural background. This dataset includes 10 participants, each with 82,160 trials spanning 16,740 image conditions. Through computational modeling we established the quality of this dataset in five ways. First, we trained linearizing encoding models that successfully synthesized the EEG responses to arbitrary images. Second, we correctly identified the recorded EEG data image conditions in a zero-shot fashion, using EEG synthesized responses to hundreds of thousands of candidate image conditions. Third, we show that both the high number of conditions as well as the trial repetitions of the EEG dataset contribute to the trained models’ prediction accuracy. Fourth, we built encoding models whose predictions well generalize to novel participants. Fifth, we demonstrate full end-to-end training of randomly initialized DNNs that output EEG responses for arbitrary input images. We release this dataset as a tool to foster research in visual neuroscience and computer vision.},
	language = {en},
	urldate = {2023-05-17},
	journal = {NeuroImage},
	author = {Gifford, Alessandro T. and Dwivedi, Kshitij and Roig, Gemma and Cichy, Radoslaw M.},
	month = dec,
	year = {2022},
	keywords = {Artificial neural networks, Computational neuroscience, Electroencephalography, Neural encoding models, Open-access data resource, Visual object recognition},
	pages = {119754},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\UTW66D47\\Gifford et al. - 2022 - A large and rich EEG dataset for modeling human vi.pdf:application/pdf},
}

@misc{st-yves_brain-optimized_2022,
	title = {Brain-optimized neural networks learn non-hierarchical models of representation in human visual cortex},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.01.21.477293v1},
	doi = {10.1101/2022.01.21.477293},
	abstract = {Deep neural networks (DNNs) trained to perform visual tasks learn representations that align with the hierarchy of visual areas in the primate brain. This finding has been taken to imply that the primate visual system forms representations by passing them through a hierarchical sequence of brain areas, just as DNNs form representations by passing them through a hierarchical sequence of layers. To test the validity of this assumption, we optimized DNNs not to perform visual tasks but to directly predict brain activity in human visual areas V1–V4. Using a massive sampling of human brain activity, we constructed brain-optimized networks that predict brain activity even more accurately than task-optimized networks. We show that brain-optimized networks can learn representations that diverge from those formed in a strict hierarchy. Brain-optimized networks do not need to align representations in V1–V4 with layer depth; moreover, they are able to accurately model anterior brain areas (e.g., V4) without computing intermediary representations associated with posterior brain areas (e.g., V1). Our results challenge the view that human visual areas V1–V4 act—like the early layers of a DNN—as a serial pre-processing sequence for higher areas, and suggest they may subserve their own independent functions.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {bioRxiv},
	author = {St-Yves, Ghislain and Allen, Emily J. and Wu, Yihan and Kay, Kendrick and Naselaris, Thomas},
	month = jan,
	year = {2022},
	note = {Pages: 2022.01.21.477293
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\8D5Y2QXL\\St-Yves et al. - 2022 - Brain-optimized neural networks learn non-hierarch.pdf:application/pdf},
}

@article{franke_state-dependent_2022,
	title = {State-dependent pupil dilation rapidly shifts visual feature selectivity},
	volume = {610},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05270-3},
	doi = {10.1038/s41586-022-05270-3},
	abstract = {To increase computational flexibility, the processing of sensory inputs changes with behavioural context. In the visual system, active behavioural states characterized by motor activity and pupil dilation1,2 enhance sensory responses, but typically leave the preferred stimuli of neurons unchanged2–9. Here we find that behavioural state also modulates stimulus selectivity in the mouse visual cortex in the context of coloured natural scenes. Using population imaging in behaving mice, pharmacology and deep neural network modelling, we identified a rapid shift in colour selectivity towards ultraviolet stimuli during an active behavioural state. This was exclusively caused by state-dependent pupil dilation, which resulted in a dynamic switch from rod to cone photoreceptors, thereby extending their role beyond night and day vision. The change in tuning facilitated the decoding of ethological stimuli, such as aerial predators against the twilight sky10. For decades, studies in neuroscience and cognitive science have used pupil dilation as an indirect measure of brain state. Our data suggest that, in addition, state-dependent pupil dilation itself tunes visual representations to behavioural demands by differentially recruiting rods and cones on fast timescales.},
	language = {en},
	number = {7930},
	urldate = {2023-05-17},
	journal = {Nature},
	author = {Franke, Katrin and Willeke, Konstantin F. and Ponder, Kayla and Galdamez, Mario and Zhou, Na and Muhammad, Taliah and Patel, Saumil and Froudarakis, Emmanouil and Reimer, Jacob and Sinz, Fabian H. and Tolias, Andreas S.},
	month = oct,
	year = {2022},
	note = {Number: 7930
Publisher: Nature Publishing Group},
	keywords = {Colour vision, Network models, Sensory processing, Striate cortex},
	pages = {128--134},
}

@article{gu_neurogen_2022,
	title = {{NeuroGen}: {Activation} optimized image synthesis for discovery neuroscience},
	volume = {247},
	issn = {1053-8119},
	shorttitle = {{NeuroGen}},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811921010831},
	doi = {10.1016/j.neuroimage.2021.118812},
	abstract = {Functional MRI (fMRI) is a powerful technique that has allowed us to characterize visual cortex responses to stimuli, yet such experiments are by nature constructed based on a priori hypotheses, limited to the set of images presented to the individual while they are in the scanner, are subject to noise in the observed brain responses, and may vary widely across individuals. In this work, we propose a novel computational strategy, which we call NeuroGen, to overcome these limitations and develop a powerful tool for human vision neuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model of human vision with a deep generative network to synthesize images predicted to achieve a target pattern of macro-scale brain activation. We demonstrate that the reduction of noise that the encoding model provides, coupled with the generative network’s ability to produce images of high fidelity, results in a robust discovery architecture for visual neuroscience. By using only a small number of synthetic images created by NeuroGen, we demonstrate that we can detect and amplify differences in regional and individual human brain response patterns to visual stimuli. We then verify that these discoveries are reflected in the several thousand observed image responses measured with fMRI. We further demonstrate that NeuroGen can create synthetic images predicted to achieve regional response patterns not achievable by the best-matching natural images. The NeuroGen framework extends the utility of brain encoding models and opens up a new avenue for exploring, and possibly precisely controlling, the human visual system.},
	language = {en},
	urldate = {2023-05-17},
	journal = {NeuroImage},
	author = {Gu, Zijin and Jamison, Keith Wakefield and Khosla, Meenakshi and Allen, Emily J. and Wu, Yihan and St-Yves, Ghislain and Naselaris, Thomas and Kay, Kendrick and Sabuncu, Mert R. and Kuceyeski, Amy},
	month = feb,
	year = {2022},
	keywords = {Deep learning, Function MRI, Image synthesis, Neural encoding},
	pages = {118812},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\HAFMG3VV\\Gu et al. - 2022 - NeuroGen Activation optimized image synthesis for.pdf:application/pdf},
}

@article{bashivan_neural_2019,
	title = {Neural population control via deep image synthesis},
	volume = {364},
	url = {https://www.science.org/doi/abs/10.1126/science.aav9436},
	doi = {10.1126/science.aav9436},
	abstract = {Particular deep artificial neural networks (ANNs) are today’s most accurate models of the primate brain’s ventral visual stream. Using an ANN-driven image synthesis method, we found that luminous power patterns (i.e., images) can be applied to primate retinae to predictably push the spiking activity of targeted V4 neural sites beyond naturally occurring levels. This method, although not yet perfect, achieves unprecedented independent control of the activity state of entire populations of V4 neural sites, even those with overlapping receptive fields. These results show how the knowledge embedded in today’s ANN models might be used to noninvasively set desired internal brain states at neuron-level resolution, and suggest that more accurate ANN models would produce even more accurate control.},
	number = {6439},
	urldate = {2023-05-17},
	journal = {Science},
	author = {Bashivan, Pouya and Kar, Kohitij and DiCarlo, James J.},
	month = may,
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaav9436},
	file = {Full Text:C\:\\Users\\Yoo\\Zotero\\storage\\LZAGFTWP\\Bashivan et al. - 2019 - Neural population control via deep image synthesis.pdf:application/pdf},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\EYDPNQW9\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\5EQBLJYR\\2006.html:text/html},
}

@inproceedings{lurz_generalization_2021,
	title = {Generalization in data-driven models of primary visual cortex},
	url = {https://openreview.net/forum?id=Tp7kI90Htd},
	abstract = {Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input. Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron’s receptive field position. With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7\% compared to the previous state-of-the-art network. We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal. When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12\%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33\%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40\% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.},
	language = {en},
	urldate = {2023-05-17},
	author = {Lurz, Konstantin-Klemens and Bashiri, Mohammad and Willeke, Konstantin and Jagadish, Akshay and Wang, Eric and Walker, Edgar Y. and Cadena, Santiago A. and Muhammad, Taliah and Cobos, Erick and Tolias, Andreas S. and Ecker, Alexander S. and Sinz, Fabian H.},
	month = jan,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\XVMGWKNR\\Lurz et al. - 2021 - Generalization in data-driven models of primary vi.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Yoo\\Zotero\\storage\\PME4YQC6\\5206848.html:text/html},
}

@misc{conwell_large-scale_2022,
	title = {Large-{Scale} {Benchmarking} of {Diverse} {Artificial} {Vision} {Models} in {Prediction} of {7T} {Human} {Neuroimaging} {Data}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.03.28.485868v1},
	doi = {10.1101/2022.03.28.485868},
	abstract = {Rapid simultaneous advances in machine vision and cognitive neuroimaging present an unparalleled opportunity to (re)assess the current state of artificial models of the human visual system. Here, we perform a large-scale benchmarking analysis of 85 modern deep neural network models (e.g. CLIP, BarlowTwins, Mask-RCNN) to characterize with robust statistical power how differences in architecture and training task contribute to the prediction of human fMRI activity across 16 distinct regions of the human visual system. We find: one, that even stark architectural differences (e.g. the absence of convolution in transformers and MLP-mixers) have very little consequence in emergent fits to brain data; two, that differences in task have clear effects–with categorization and self-supervised models showing relatively stronger brain predictivity across the board; three, that feature reweighting leads to substantial improvements in brain predictivity, without overfitting – yielding model-to-brain regression weights that generalize at the same level of predictivity to brain responses over 1000s of new images. Broadly, this work presents a lay-of-the-land for the emergent correspondences between the feature spaces of modern deep neural network models and the representational structure inherent to the human visual system.},
	language = {en},
	urldate = {2023-05-17},
	publisher = {bioRxiv},
	author = {Conwell, Colin and Prince, Jacob S. and Alvarez, George A. and Konkle, Talia},
	month = mar,
	year = {2022},
	note = {Pages: 2022.03.28.485868
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\3KARXZCB\\Conwell et al. - 2022 - Large-Scale Benchmarking of Diverse Artificial Vis.pdf:application/pdf},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\7UNTJJ62\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\HYNJLT6V\\2112.html:text/html},
}

@article{prince_improving_2022,
	title = {Improving the accuracy of single-trial {fMRI} response estimates using {GLMsingle}},
	volume = {11},
	issn = {2050-084X},
	doi = {10.7554/eLife.77599},
	abstract = {Advances in artificial intelligence have inspired a paradigm shift in human neuroscience, yielding large-scale functional magnetic resonance imaging (fMRI) datasets that provide high-resolution brain responses to thousands of naturalistic visual stimuli. Because such experiments necessarily involve brief stimulus durations and few repetitions of each stimulus, achieving sufficient signal-to-noise ratio can be a major challenge. We address this challenge by introducing GLMsingle, a scalable, user-friendly toolbox available in MATLAB and Python that enables accurate estimation of single-trial fMRI responses (glmsingle.org). Requiring only fMRI time-series data and a design matrix as inputs, GLMsingle integrates three techniques for improving the accuracy of trial-wise general linear model (GLM) beta estimates. First, for each voxel, a custom hemodynamic response function (HRF) is identified from a library of candidate functions. Second, cross-validation is used to derive a set of noise regressors from voxels unrelated to the experiment. Third, to improve the stability of beta estimates for closely spaced trials, betas are regularized on a voxel-wise basis using ridge regression. Applying GLMsingle to the Natural Scenes Dataset and BOLD5000, we find that GLMsingle substantially improves the reliability of beta estimates across visually-responsive cortex in all subjects. Comparable improvements in reliability are also observed in a smaller-scale auditory dataset from the StudyForrest experiment. These improvements translate into tangible benefits for higher-level analyses relevant to systems and cognitive neuroscience. We demonstrate that GLMsingle: (i) helps decorrelate response estimates between trials nearby in time; (ii) enhances representational similarity between subjects within and across datasets; and (iii) boosts one-versus-many decoding of visual stimuli. GLMsingle is a publicly available tool that can significantly improve the quality of past, present, and future neuroimaging datasets sampling brain activity across many experimental conditions.},
	language = {eng},
	journal = {eLife},
	author = {Prince, Jacob S. and Charest, Ian and Kurzawski, Jan W. and Pyles, John A. and Tarr, Michael J. and Kay, Kendrick N.},
	month = nov,
	year = {2022},
	pmid = {36444984},
	pmcid = {PMC9708069},
	keywords = {Artificial Intelligence, denoising, fMRI pre-processing, GLM, human, Humans, large-scale datasets, Magnetic Resonance Imaging, MVPA, Neuroimaging, neuroscience, Reproducibility of Results, RSA, Signal-To-Noise Ratio, voxel reliability},
	pages = {e77599},
	file = {Full Text:C\:\\Users\\Yoo\\Zotero\\storage\\EJXL6BGQ\\Prince et al. - 2022 - Improving the accuracy of single-trial fMRI respon.pdf:application/pdf},
}

@article{khosla_cortical_2021,
	title = {Cortical response to naturalistic stimuli is largely predictable with deep neural networks},
	volume = {7},
	issn = {2375-2548},
	doi = {10.1126/sciadv.abe7547},
	abstract = {Naturalistic stimuli, such as movies, activate a substantial portion of the human brain, invoking a response shared across individuals. Encoding models that predict neural responses to arbitrary stimuli can be very useful for studying brain function. However, existing models focus on limited aspects of naturalistic stimuli, ignoring the dynamic interactions of modalities in this inherently context-rich paradigm. Using movie-watching data from the Human Connectome Project, we build group-level models of neural activity that incorporate several inductive biases about neural information processing, including hierarchical processing, temporal assimilation, and auditory-visual interactions. We demonstrate how incorporating these biases leads to remarkable prediction performance across large areas of the cortex, beyond the sensory-specific cortices into multisensory sites and frontal cortex. Furthermore, we illustrate that encoding models learn high-level concepts that generalize to task-bound paradigms. Together, our findings underscore the potential of encoding models as powerful tools for studying brain function in ecologically valid conditions.},
	language = {eng},
	number = {22},
	journal = {Science Advances},
	author = {Khosla, Meenakshi and Ngo, Gia H. and Jamison, Keith and Kuceyeski, Amy and Sabuncu, Mert R.},
	month = may,
	year = {2021},
	pmid = {34049888},
	pmcid = {PMC8163078},
	pages = {eabe7547},
	file = {Full Text:C\:\\Users\\Yoo\\Zotero\\storage\\48J2TQC2\\Khosla et al. - 2021 - Cortical response to naturalistic stimuli is large.pdf:application/pdf},
}

@misc{cichy_algonauts_2021,
	title = {The {Algonauts} {Project} 2021 {Challenge}: {How} the {Human} {Brain} {Makes} {Sense} of a {World} in {Motion}},
	shorttitle = {The {Algonauts} {Project} 2021 {Challenge}},
	url = {http://arxiv.org/abs/2104.13714},
	doi = {10.48550/arXiv.2104.13714},
	abstract = {The sciences of natural and artificial intelligence are fundamentally connected. Brain-inspired human-engineered AI are now the standard for predicting human brain responses during vision, and conversely, the brain continues to inspire invention in AI. To promote even deeper connections between these fields, we here release the 2021 edition of the Algonauts Project Challenge: How the Human Brain Makes Sense of a World in Motion (http://algonauts.csail.mit.edu/). We provide whole-brain fMRI responses recorded while 10 human participants viewed a rich set of over 1,000 short video clips depicting everyday events. The goal of the challenge is to accurately predict brain responses to these video clips. The format of our challenge ensures rapid development, makes results directly comparable and transparent, and is open to all. In this way it facilitates interdisciplinary collaboration towards a common goal of understanding visual intelligence. The 2021 Algonauts Project is conducted in collaboration with the Cognitive Computational Neuroscience (CCN) conference.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Cichy, R. M. and Dwivedi, K. and Lahner, B. and Lascelles, A. and Iamshchinina, P. and Graumann, M. and Andonian, A. and Murty, N. A. R. and Kay, K. and Roig, G. and Oliva, A.},
	month = apr,
	year = {2021},
	note = {arXiv:2104.13714 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 5 pages, 2 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\UV6HV68P\\Cichy et al. - 2021 - The Algonauts Project 2021 Challenge How the Huma.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\VWDJ9AHH\\2104.html:text/html},
}

@article{allen_massive_2022-1,
	title = {A massive {7T} {fMRI} dataset to bridge cognitive neuroscience and artificial intelligence},
	volume = {25},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00962-x},
	doi = {10.1038/s41593-021-00962-x},
	abstract = {Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2023-05-17},
	journal = {Nature Neuroscience},
	author = {Allen, Emily J. and St-Yves, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cortex, Neural encoding, Object vision, Perception},
	pages = {116--126},
	file = {Submitted Version:C\:\\Users\\Yoo\\Zotero\\storage\\UFVCJTYG\\Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf:application/pdf},
}

@article{van_essen_human_2012,
	title = {The {Human} {Connectome} {Project}: a data acquisition perspective},
	volume = {62},
	issn = {1095-9572},
	shorttitle = {The {Human} {Connectome} {Project}},
	doi = {10.1016/j.neuroimage.2012.02.018},
	abstract = {The Human Connectome Project (HCP) is an ambitious 5-year effort to characterize brain connectivity and function and their variability in healthy adults. This review summarizes the data acquisition plans being implemented by a consortium of HCP investigators who will study a population of 1200 subjects (twins and their non-twin siblings) using multiple imaging modalities along with extensive behavioral and genetic data. The imaging modalities will include diffusion imaging (dMRI), resting-state fMRI (R-fMRI), task-evoked fMRI (T-fMRI), T1- and T2-weighted MRI for structural and myelin mapping, plus combined magnetoencephalography and electroencephalography (MEG/EEG). Given the importance of obtaining the best possible data quality, we discuss the efforts underway during the first two years of the grant (Phase I) to refine and optimize many aspects of HCP data acquisition, including a new 7T scanner, a customized 3T scanner, and improved MR pulse sequences.},
	language = {eng},
	number = {4},
	journal = {NeuroImage},
	author = {Van Essen, D. C. and Ugurbil, K. and Auerbach, E. and Barch, D. and Behrens, T. E. J. and Bucholz, R. and Chang, A. and Chen, L. and Corbetta, M. and Curtiss, S. W. and Della Penna, S. and Feinberg, D. and Glasser, M. F. and Harel, N. and Heath, A. C. and Larson-Prior, L. and Marcus, D. and Michalareas, G. and Moeller, S. and Oostenveld, R. and Petersen, S. E. and Prior, F. and Schlaggar, B. L. and Smith, S. M. and Snyder, A. Z. and Xu, J. and Yacoub, E. and {WU-Minn HCP Consortium}},
	month = oct,
	year = {2012},
	pmid = {22366334},
	pmcid = {PMC3606888},
	keywords = {Brain, Brain Mapping, Connectome, Humans},
	pages = {2222--2231},
	file = {Accepted Version:C\:\\Users\\Yoo\\Zotero\\storage\\DR4QHK3D\\Van Essen et al. - 2012 - The Human Connectome Project a data acquisition p.pdf:application/pdf},
}

@article{chang_bold5000_2019,
	title = {{BOLD5000}, a public {fMRI} dataset while viewing 5000 visual images},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-019-0052-3},
	doi = {10.1038/s41597-019-0052-3},
	abstract = {Vision science, particularly machine vision, has been revolutionized by introducing large-scale image datasets and statistical learning approaches. Yet, human neuroimaging studies of visual perception still rely on small numbers of images (around 100) due to time-constrained experimental procedures. To apply statistical learning approaches that include neuroscience, the number of images used in neuroimaging must be significantly increased. We present BOLD5000, a human functional MRI (fMRI) study that includes almost 5,000 distinct images depicting real-world scenes. Beyond dramatically increasing image dataset size relative to prior fMRI studies, BOLD5000 also accounts for image diversity, overlapping with standard computer vision datasets by incorporating images from the Scene UNderstanding (SUN), Common Objects in Context (COCO), and ImageNet datasets. The scale and diversity of these image datasets, combined with a slow event-related fMRI design, enables fine-grained exploration into the neural representation of a wide range of visual features, categories, and semantics. Concurrently, BOLD5000 brings us closer to realizing Marr’s dream of a singular vision science–the intertwined study of biological and computer vision.},
	language = {en},
	number = {1},
	urldate = {2023-05-17},
	journal = {Scientific Data},
	author = {Chang, Nadine and Pyles, John A. and Marcus, Austin and Gupta, Abhinav and Tarr, Michael J. and Aminoff, Elissa M.},
	month = may,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Functional magnetic resonance imaging, Visual system},
	pages = {49},
	file = {Full Text PDF:C\:\\Users\\Yoo\\Zotero\\storage\\X5A33ZV6\\Chang et al. - 2019 - BOLD5000, a public fMRI dataset while viewing 5000.pdf:application/pdf},
}

@misc{zhuang_adabelief_2020,
	title = {{AdaBelief} {Optimizer}: {Adapting} {Stepsizes} by the {Belief} in {Observed} {Gradients}},
	shorttitle = {{AdaBelief} {Optimizer}},
	url = {http://arxiv.org/abs/2010.07468},
	doi = {10.48550/arXiv.2010.07468},
	abstract = {Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James S.},
	month = dec,
	year = {2020},
	note = {arXiv:2010.07468 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\7MDIG9CW\\Zhuang et al. - 2020 - AdaBelief Optimizer Adapting Stepsizes by the Bel.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\BHP3C72J\\2010.html:text/html},
}

@misc{wortsman_model_2022,
	title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
	shorttitle = {Model soups},
	url = {http://arxiv.org/abs/2203.05482},
	doi = {10.48550/arXiv.2203.05482},
	abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups." When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
	month = jul,
	year = {2022},
	note = {arXiv:2203.05482 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ICML 2022. The last three authors contributed equally},
	file = {arXiv Fulltext PDF:C\:\\Users\\Yoo\\Zotero\\storage\\PLMFPBXF\\Wortsman et al. - 2022 - Model soups averaging weights of multiple fine-tu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Yoo\\Zotero\\storage\\ZVXH4G69\\2203.html:text/html},
}

@article{falcon_pytorch_2019,
	title = {Pytorch lightning},
	volume = {3},
	journal = {GitHub},
	author = {Falcon, William A.},
	year = {2019},
}
