\subsection{Notation and basic results}
\label{subsec:notation}
We consider a nonlinear equation
\begeq
\label{eq:nleq}
\mf (\vx) = 0
\endeq
for $\vx \in \Omega \subset R^N$.
We will call $\mf$ the residual in this paper. We denote the Jacobian matrix
of $\mf$ by $\mf'$.

We will assume that the standard assumptions
\cite{dens,ctk:roots,ctk:fajulia} hold.

{\bf Standard Assumptions}
\begin{enumerate}
\item Equation~{\rm\ref{eq:nleq}} has a solution $\vx^* \in \Omega$.
\item $\mf': \Omega \to R^{N \times N}$ is Lipschitz continuous
near $\vx^*$ with Lipschitz constant $\gamma$.
\item $\mf'(\vx^*)$ is nonsingular.
\end{enumerate}

We will assume that we are near enough
to $\vx^*$ so that the Newton iteration
\begeq
\label{eq:newtonit}
\vx_{+} = \vx_c - \mf'(\vx_c)^{-1} \mf(\vx_c)
\endeq
will converge quadratically to the solution.  In \eqnok{newtonit}
$\mf'$ is the Jacobian matrix
and, as is standard, $\vx_c$ denotes the current point and
$\vx_+$ denotes the Newton iteration from $\vx_c$.

One explicit way \cite{ctk:roots}
to express this is to assume that
\begeq
\label{eq:close}
\vx_c \in \calb \equiv \{ \vx \, | \, \| \vx - \vx^* \| \le  \rho \}
\endeq
where
\[
\rho \le \frac{1}{2 \gamma \| \mf'(\vx^*)^{-1} \|}
\]
and is small enough so that $\calb \subset \Omega$. In that case
$\vx_+ \in \calb$ and 
\[
\| \ve_+ \| \le \gamma \| \mf'(\vx^*)^{-1} \| \| \ve_c \|^2
\le \| \ve_c \|/2 \le \rho/2.
\]
And so the iteration converges and remains in $\Omega$.
Here $\ve = \vx - \vx^*$ denotes the error.

In practice, however, the Newton iteration is computed in
floating point arithmetic and the floating point errors must
be considered.
To account for this (see \cite{ctk:roots} for the details)
we let $\Delta$ denote the error in the Jacobian and $\epsilon$ denote
the error in the residual. With this in mind the iteration is
\begeq
\label{eq:realit}
\vx_+ = \vx_c - (\mf'(\vx_c) + \Delta(\vx_c))^{-1} (\mf(\vx_c)
+ \epsilon(\vx_c))
\endeq

In this paper we will assume that the errors can be bounded independently of
$\vx$, so there are $\epsilon_F$ and $\epsilon_J$ such that
\[
\| \epsilon(\vx) \| \le \epsilon_F \mbox{ and }
\| \Delta(\vx) \| \le \epsilon_J
\]
for all $\vx \in \calb$. One can think
of $\epsilon_F$ as floating point roundoff. The interesting part is
$\epsilon_J$, the error in the Jacobian.

With this in mind, the error estimate from \cite{ctk:roots} becomes
\begeq
\label{eq:errest}
\| \ve_+ \| = O(\| \ve_c \|^2 + \epsilon_J \| \ve_c \|
+ \epsilon_F ).
\endeq
where $\ve = \vx - \vx^*$ denote the error,
Clearly, if the errors vanish, one obtains the standard quadratic
convergence theory. However, if $\epsilon_F > 0$ then one can expect
the residual norms to stagnate once
\[
\| \mf(\vx) \| = O(\epsilon_F)
\]
which is what one observes in practice. The estimate \eqnok{errest}
is not a local convergence result. Results of this type are called
{\em local improvement} \cite{denwal3,ctk:roots}.

We can also see that if $\epsilon_J = O(\sqrt{\epsilon_F})$, as it will
be \cite{ctk:roots} if one uses a finite-difference approximation to the
Jacobian with difference increment $\sqrt{\epsilon_F}$ or (assuming one
computes $\mf$ in double precision) stores and factors
the Jacobian in single precision,
then \eqnok{errest} becomes
\begeq
\label{eq:errest2}
\| \ve_+ \| 
= O(\| \ve_c \|^2 + \sqrt{\epsilon_F} \| \ve_c \| + \epsilon_F )
= O(\| \ve_c \|^2 + \epsilon_F ).
\endeq
Equation \eqnok{errest2} says that the iteration with a sufficiently 
accurate approximate Jacobian is indistinguishable from Newton's
method.

With these errors in mind, we can formulate the locally
convergent (\ie with no line search) form of Newton's method of interest
in this paper. 
In Algorithm~\ref{alg:newton} $\tau_a$ and $\tau_r$ are relative and
absolute error tolerances. $\vx$ is the initial iterate on imput and
the algorithm overwrites $\vx$ as the iteration progresses.

\begin{algo}
\label{alg:newton}
{$\mbox{\bf newton}(\mf, \vx, \tau_a, \tau_r)$} 
\begin{algorithmic}
\STATE Evaluate ${\tilde \mf} = \mf(\vx) + \epsilon(\vx)$; 
\STATE $\tau \leftarrow \tau_r \| {\tilde \mf} \| + \tau_a$.
\WHILE{$\| {\tilde \mf} \| > \tau$}
\STATE Solve $(\mf'(\vx) + \Delta(\vx)) \vs = - {\tilde \mf}$
\STATE $\vx \leftarrow \vx + \vs$
\STATE Evaluate ${\tilde \mf} = \mf(\vx) + \epsilon(\vx)$; 
\ENDWHILE
\end{algorithmic}
\end{algo}

There are two sources of error in the Jacobian that contribute
to $\Delta$. One is the error in approximating the Jacobian itself
and the other is the error in the solver. We will use an analytic
Jacobian in this paper and store that Jacobian in single precision.
Hence the relative error in the Jacobian is
floating point roundoff in single precision.
We used Gaussian elimination to solve for the Newton
step in \cite{ctk:sirev20} so the solver error was the backward
error in the $LU$ factorization. We will discuss this more in
\S~\ref{subsec:sirev}. 

