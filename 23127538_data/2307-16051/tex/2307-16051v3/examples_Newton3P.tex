\section{Examples}
\label{sec:results}

In this section we compare some of the two precision
results from \cite{ctk:sirev20} with the three precision method
from this paper. Some of the results using half precision were poor because
the half precision Jacobian was a poor approximation to the Jacobian. This
problem was particularly severe for the ill-conditioned example, which we
feature in this section.
The example, taken from \cite{ctk:sirev20} is the composite mid-point
rule discretization of
the Chandrasekhar H-equation \cite{chand},
\begeq
\label{eq:heq}
\calf(H)(\mu) = H(\mu) -
\left(
1 - \frac{c}{2} \int_0^1 \frac{\mu H(\mu)}{\mu + \nu} \dnu
\right)^{-1} = 0.
\endeq
The nonlinear operator $\calf$ is defined on $C[0,1]$, the space of
continuous functions on $[0,1]$. 

We use $N$ quadrature points $\nu_j = (j - 1/2)/N$ for $1 \le j \le N$
and the rule is
\[
\int_0^1 f(\nu) \dnu \approx \frac{1}{N} \sum_{j=1}^N f(\nu_j).
\]
The discrete system is
\begeq
\label{eq:hmid}
\mf(\vx)_i \equiv
x_i - \left(
1  - \frac{c}{2N} \sum_{j=1}^N \frac{x_j \mu_i}{\mu_j + \mu_i}
\right)^{-1}
=0.
\endeq
As we explained in \cite{ctk:sirev20,ctk:fajulia,ctk:acta}, one can evaluate the
nonlinear residual with a fast Fourier transform to
in $O(N \log(N))$ work and compute an analytic Jacobian, as we did
for this paper, in $O(N^2)$ work.
Hence the dominant cost for large $N$ is the factorization
of the Jacobian.

For $c=.99$, the results in
\cite{ctk:sirev20} showed a significant difference in performance for
the two-precision algorithm between a low precision of single and one
of half (see Figure~3.2, pg 205 and Figure~3.5 pg 208 in \cite{ctk:sirev20}). 
We will reproduce some of those data in this section to make the comparison.

\subsection{Computations}
\label{subsec:compute}
The computations in this section were done in Julia 
\cite{Juliasirev}
v 1.9 on a 2023
Apple Mac Mini with an M2 Pro processor and 32GB of memory.
The M2 processor supports half precision
computing in hardware and Julia 1.9 offers support for this hardware.
However, as we said in the introduction, LAPACK and the BLAS do
not take full advantage of the half precision hardeware, 
so half precision computations are slow, 
but not as slow as the ones the author did for \cite{ctk:sirev20}. 

Our implementation of iterative refinement terminates with success
when the relative
residual norm is $< 10^{-6} \approx 100 u_s$ and declares that the
iteration has failed if the residual norm increases. After failure
the algorithm returns the solution of the linear problem
with the best residual.
This approach allows the nonlinear iteration to continue. We see in
the results that failure of the linear iterative refinement iteration
can affect the convergence of the nonlinear solver because when the
iteration fails $\eta_J$ can be larger that one would get if the
iteration terminated successfully.

The computations used the author's SIAMFANLEquation.jl
Julia package \cite{ctk:siamfanl, ctk:notebooknl, ctk:fajulia}.
The files for the package are located at
\url{https://github.com/ctkelley/SIAMFANLEquations.jl} and the associated
IJulia notebook can be found at
\url{https://github.com/ctkelley/NotebookSIAMFANL}.
The GitHub repository
\url{https://github.com/ctkelley/Newton3P}
contains the codes used to produce the results in this section and
instructions for reproducing those results.

Table~\ref{tab:99} presents the residual history for ten iterations from
the the ill-conditioned example from \cite{ctk:sirev20}.
The first three histories are for Newton's method with double precision
(F64), single precision (F32), and half precision (F16) Jacobians and
are the same results as those from \cite{ctk:sirev20}. The final two
columns are for IR with the Jacobian stored in single precision
and the factorization done in half precision (IR 32-16) and GMRES-IR
using the that factorization to precondition GMRES.

The Newton iteration with a half precision Jacobian converged very poorly 
for this problem. However, IR with the same half precision factorization
performs as well as Newton's method using a double precision Jacobian. 

\begin{table}[h!]
\caption{Residual Histories for Three Precisions: $N=4096$, $c=.99$}
\label{tab:99}
\begin{center}
\begin{tabular}{llllll} 
 n&        F64&        F32&        F16&   IR 32-16&      GM-IR \\ \hline 
 0 &  1.000e+00 &  1.000e+00 &  1.000e+00 &  1.000e+00 &  1.000e+00  \\ 
 1 &  2.289e-01 &  2.289e-01 &  5.065e-01 &  2.289e-01 &  2.289e-01  \\ 
 2 &  3.934e-02 &  3.934e-02 &  2.958e-01 &  3.934e-02 &  3.934e-02  \\ 
 3 &  2.737e-03 &  2.737e-03 &  1.890e-01 &  2.737e-03 &  2.737e-03  \\ 
 4 &  1.767e-05 &  1.767e-05 &  1.255e-01 &  1.767e-05 &  1.767e-05  \\ 
 5 &  7.486e-10 &  7.495e-10 &  8.518e-02 &  7.516e-10 &  7.487e-10  \\ 
 6 &            &            &  6.068e-02 &            &             \\ 
 7 &            &            &  4.240e-02 &            &             \\ 
 8 &            &            &  3.195e-02 &            &             \\ 
 9 &            &            &  2.280e-02 &            &             \\ 
10 &            &            &  1.713e-02 &            &             \\ 
\hline 
\end{tabular} 
\end{center} 
\end{table}

Table~\ref{tab:9999} is a much more poorly conditioned problem. In this
problem IR does very well for the first three iterations when the 
iteration is not close to the solution. As the iteration converges the
ill-conditioning of the Jacobian at the solution begins to cause failures
of the linear iteration and that affects the nonlinear iteration. GMRES-IR
continues to perform well.

\begin{table}[h!] 
\caption{Residual Histories for Three Precisions: $N=4096$, $c=.9999$}
\label{tab:9999}
\begin{center}
\begin{tabular}{llllll} 
 n&        F64&        F32&        F16&   IR 32-16&      GM-IR \\ \hline 
 0 &  1.000e+00 &  1.000e+00 &  1.000e+00 &  1.000e+00 &  1.000e+00  \\ 
 1 &  2.494e-01 &  2.494e-01 &  5.182e-01 &  2.494e-01 &  2.494e-01  \\ 
 2 &  6.093e-02 &  6.093e-02 &  3.123e-01 &  6.093e-02 &  6.093e-02  \\ 
 3 &  1.480e-02 &  1.480e-02 &  2.067e-01 &  1.480e-02 &  1.480e-02  \\ 
 4 &  3.454e-03 &  3.454e-03 &  1.421e-01 &  7.500e-03 &  3.454e-03  \\ 
 5 &  6.762e-04 &  6.762e-04 &  1.012e-01 &  1.756e-03 &  6.762e-04  \\ 
 6 &  7.049e-05 &  7.049e-05 &  7.552e-02 &  1.009e-03 &  7.049e-05  \\ 
 7 &  1.223e-06 &  1.223e-06 &  5.773e-02 &  8.345e-04 &  1.223e-06  \\ 
 8 &  3.947e-10 &  3.957e-10 &  4.543e-02 &  7.244e-04 &  3.955e-10  \\ 
 9 &            &            &  3.639e-02 &  6.789e-04 &             \\ 
10 &            &            &  2.949e-02 &  3.560e-04 &             \\ 
\hline 
\end{tabular} 
\end{center}
\end{table}

