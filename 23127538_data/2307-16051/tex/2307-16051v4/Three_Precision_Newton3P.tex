\section{Three precision algorithms}
\label{sec:3p}

The three precision algorithms compute $\mf$ in double precision and
store the Jacobian $\mf'$ in a single precision matrix $\mj$. This means that
\begeq
\label{eq:jacerr}
\| \mf'(\vx_c) - \mj \| \le u_s \| \mf'(\vx_c) \|.
\endeq
Hence, using the terminology of \S~\ref{subsec:notation}
\begeq
\label{eq:errlist}
\epsilon_F = O(u_d) \mbox{ and } \epsilon_J = O(u_s).
\endeq

Our notation for interprecision transfers is to let 
$I_a^b$ be the transfer from precision $u_a$ to
$u_b$. If $u_a > u_b$, this promotion changes nothing
\[
I_a^b (x) = x
\]
if $x$ is in precision $u_a$. If $u_a < u_b$, then the interprecision
transfer rounds down, so
\[
\| I_a^b(x) - x \| \le u_b \| x \|.
\]
We will use these properties of interprecision transfer throughout
the remainder of the paper. We point out that when one rounds a
matrix or vector down to a lower precision, one must allocate memory
for the low precision object and that there is a cost to this.

We then round 
$\mj$ to half precision to obtain
\[
\mj_h = I_s^h (\mj)
\]
and factor $\mj_h$ in half precision to obtain
${\hat \ml} {\hat \mU}$.
We use the
half precision factorization as part of an iterative method to solve
\[
\mj \vs = - \mf(\vx_c).
\]
We terminate that iteration when 
\begeq
\label{eq:inexactJ}
\| \mj \vs + \mf(\vx_c) \| \le \eta_J \| \mf(\vx_c) \|.
\endeq

We summarize the three precision algorithm.
\begin{algo}
\label{alg:3p}
{$\mbox{\bf newton3p}(\mf, \vx, \tau_a, \tau_r, \eta_J)$}
\begin{algorithmic}
\STATE Evaluate ${\tilde \mf} = \mf(\vx) + \epsilon(\vx)$;  
\STATE $\tau \leftarrow \tau_r \| {\tilde \mf} \| + \tau_a$.
\WHILE{$\| {\tilde \mf} \| > \tau$}
\STATE Compute and store $F'(\vx)$ in single precision as $\mj$.
\STATE Store $\mj_h = I_s^h(\mj)$.
\STATE Find $\vs$ such that $\| \mj_h \vs + \mf(\vx) \| \le 
\eta_J \| \mf(\vx) \|$. 
\STATE $\vx \leftarrow \vx + \vs$
\STATE Evaluate ${\tilde \mf} = \mf(\vx) + \epsilon(\vx)$;
\ENDWHILE
\end{algorithmic}
\end{algo}
In Algorithm~\ref{alg:3p} we want to choose $\eta_J < 1$ small enough
so that the nonlinear iteration statistics are the same as those from
Newton's method itself.

Algorithm~\ref{alg:3p} looks like an inexact Newton iteration, but 
differs in that the condition on the step is \eqnok{inexactJ}
rather than the classical inexact Newton condition
\begeq
\label{eq:inexact}
\| \mf'(\vx_c) \vs + \mf(\vx_c) \| \le \eta \| \mf(\vx_c) \|.
\endeq
If we had \eqnok{inexact}, then we would get a local improvement 
estimate \cite{demboes,ctk:roots}
\begeq
\label{eq:inexloc}
\| \ve_+ \| = O(\| \ve_c \|^2 + \eta \|\ve_c\| + \epsilon_F).
\endeq
This will imply q-linear convergence of the nonlinear
iteration if $\eta$ is sufficiently small and the function
evaluation is exact ($\epsilon_F = 0$).

If we are able to show that we can chose $\eta_J$ so that \eqnok{inexact}
holds with $\eta = O(u_s)$, then, similar to the two precision case with
$\mj$ stored and factored in single precision, \eqnok{inexloc} will imply
\eqnok{errest2} and the nonlinear iteration
statistics will be the same as Newton's method with the Jacobian stored and
factored in double precision.

The use of an iterative method for the linear equation for the Newton
step means that the backward error in the 
factorization plays no role in the analysis of the nonlinear iteration.
However, that backward error
does affect the convergence of the linear iteration. We will describe
our two choices for the linear iteration in \S~\ref{sec:IR} but will
discuss the local improvement result for the
nonlinear iteration first.

\subsection{Local improvement of the nonlinear iteration}
\label{subsec:inexact}

We begin by showing that $\mj$ is nonsingular and estimating
$\| \mj^{-1} \|$. In the analysis we use the standard notation
\[
\kappa(\ma) = \| \ma \| \| \ma^{-1} \|
\]
for the condition number of a matrix $\ma$.

\begin{lemma}
\label{lem:mjok}

Assume that the standard assumptions \eqnok{close} hold and that 
\begeq
\label{eq:condok}
4 u_s \kappa(\mf'(\vx^*) ) < 1.
\endeq

Then $\mj$ is nonsingular and
\begeq
\label{eq:mjok}
\| \mj^{-1} \| \le
\frac{2 \| \mf'(\vx^*)^{-1} \|}{1 - 4 u_s \kappa(\mf'(\vx^*) )}.
\endeq
\end{lemma}

\begin{proof}

The standard assumptions and \eqnok{close} imply that
$\mf'(\vx_c)$ is nonsingular and
(see Lemma 4.3.1 from \cite{ctk:roots})
\begeq
\label{eq:twotimes}
\| \mf'(\vx_c) \| \le 2 \| \mf'(\vx^*) \|  \mbox{ and }
\| \mf'(\vx_c)^{-1} \| \le 2 \| \mf'(\vx^*)^{-1} \|.
\endeq
Hence, using \eqnok{twotimes},
\[
\| I - \mf'(\vx_c)^{-1} \mj \|
\le \| \mf'(\vx_c)^{-1} \| \| \mf'(\vx_c) - \mj \|
\le u_s \| \mf'(\vx_c)^{-1} \| \| \mf'(\vx_c) \|
\le 4 u_s \kappa(\mf'(\vx^*) ) < 1.
\]

So $\mf'(\vx_c)^{-1}$ is an approximate inverse of $\mj$. Therefore $\mj$
is nonsingular and
\[
\| \mj^{-1} \| \le 
\frac{\| \mf'(\vx_c)^{-1} \|}{1 - 4 u_s \kappa(\mf'(\vx^*) )}
\le
\frac{2 \| \mf'(\vx^*)^{-1} \|}{1 - 4 u_s \kappa(\mf'(\vx^*) )},
\]
proving the lemma.

\end{proof}

Assume the linear iterative method converges, which is not guaranteed, 
and that we terminate the linear iteration when 
\eqnok{inexactJ} holds. To prove the local improvement estimate
\eqnok{inexloc} we must connect
\eqnok{inexactJ} to the classic inexact Newton condition
\eqnok{inexact} for some $\eta < 1$. That will then imply
the estimate \eqnok{inexloc}.

We express the convergence estimates in terms of 
\begeq
\label{eq:pstar}
P^* = \frac{4 \| \kappa(\mf'(\vx^*)) \|}%
{1 - 4 u_s \kappa(\mf'(\vx^*) )}.
\endeq

\begin{lemma}
\label{lem:etaok}
Assume that the assumptions of Lemma~\ref{lem:mjok} and \eqnok{inexactJ} 
hold, that $u_s P^* < 1/2$, and that
\[
\eta_J < 1 - 2 u_s P^*.
\]
Then \eqnok{inexact} holds with
\begeq
\label{eq:etaok}
\eta \le \eta_J + (1 + \eta_J) u_s P^* < 1.
\endeq
\end{lemma}

\begin{proof}

Equation \eqnok{inexactJ} implies that
\[
\| \mj^{-1} \|^{-1} \| \vs \| \le \| \mj \vs \|
\le (1 + \eta_J) \| \mf(\vx_c) \|
\]
and hence, using Lemma~\ref{lem:mjok}
\begeq
\label{eq:stepest}
\| \vs \| \le \| \mj^{-1} \| (1 + \eta_J) \| \mf(\vx_c) \| 
\le \frac{2 \| \mf'(\vx^*)^{-1} \|}{1 - 4 u_s \kappa(\mf'(\vx^*) )}
(1 + \eta_J)  \| \mf(\vx_c) \|.
\endeq

We use \eqnok{inexactJ} again to obtain
\begeq
\label{eq:inexactok}
\begin{array}{ll}
\| \mf'(\vx_c) \vs + \mf(\vx_c) \| 
& \le 
\| \mj \vs + \mf(\vx_c) \| + 
\| (\mf'(\vx_c) - \mj) \vs \| \\
\\
& \le \eta_J \| \mf(\vx_c) \| + u_s \| \mf'(\vx_c) \| \| \vs \| \\
\\
& \le \eta_J \| \mf(\vx_c) \| + 2 u_s \| \mf'(\vx^*) \| \| \vs \|.
\end{array}
\endeq

Combining \eqnok{stepest} and \eqnok{inexactok} completes the proof.

\end{proof}

Now suppose we can obtain $\eta_J = O(u_s) = O(\sqrt{u_d})$, then 
\eqnok{errest2} holds and the local improvement estimate becomes
\begeq
\label{eq:errest3}
\| \ve_+ \| = O(\| \ve_c \|^2 + u_d )
\endeq
and the iteration statistics should be the same as Newton's method.
We will see exactly this in the results in \S~\ref{sec:results}.

The assumption that $u_s P^* < 1/2$ simply says that $\mf'(\vx^*)$ is
not horribly ill-conditioned. Ill-conditioning of $\mf'(\vx^*)$ does
not appear in the local improvement estimate directly, but does affect
the convergence of the linear iteration, as we will see in the next section.

\input{IR_Newton3P.tex}

\input{precision_Newton3P.tex}
