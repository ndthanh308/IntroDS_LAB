\subsection{Newton's method in two precisions}
\label{subsec:sirev}

With the background from the previous sections in hand, we can
now describe the findings from \cite{ctk:sirev20} and then motivate
the three precision algorithms.

As we said above, the equation for the Newton step
\[
\mf'(\vx_c) \vs = - \mf(\vx_c)
\]
can only be approximated. The first step in such an approximation
is to replace $\mf'(\vx_c)$ with an approximation $\mj$. For
example $\mj$ could be a floating point evaluation of the Jacobian,
perhaps in a lower precision that the one used to evaluate $\mf$,
a finite difference approximation, or a physics-based approximation
that neglects part of the Jacobian. In any case, one can analyze
the error in $\mj$ directly. 

In this work we solve the approximation
\[
\mj \vs = - \mf(\vx_c)
\]
with Gaussian elimination \cite{higham}, \ie an $LU$ factorization.
We compute an upper triangular matrix $\mU$ and a lower triangular
matrix $\ml$ that, in exact arithmetic, factors $\mj = \ml \mU$,
so the equation for the step can be solved by two triangular solves.

However, there are errors in factorization and one really computes
approximations ${\hat \ml}$ and  ${\hat \mU}$. We define
${\hat \mj} = {\hat \ml} {\hat \mU}$, so the approximate factorization
is the exact factorization for a different (hopefully nearby) problem.
Hence, the equation we actually solve for the Newton step is
\[
{\hat \mj} \vs = - \mf(\vx_c).
\]
There is a subtle point in the equation for the Newton step. The
matrix ${\hat \mj}$ may be in a different precision than $\mf$ and 
$\vs$. One must take some care with this and we return to this point
in \S~\ref{sec:IR} and \ref{sec:ipxf}.

The backward error is
\[
\delta \mj = {\hat \mj} - \mj.
\]
So the error in the Jacobian ($\epsilon_J$ in \eqnok{errest}) has
has two parts, the error in 
$\mj$ and the backward error in the factorization.

We will assume for this paper that $\mf$ is computed in double precision,
so $\epsilon_F$ is $O(u_d)$. The error one makes is storing the
Jacobian in reduced precision is 
\[
\| \mj - \mf'(\vx_c) \| \le u_J,
\]
where $u_J = O(u_s)$ or $O(u_h)$. We will make the
contribution from the backward error explicit and
reformulate \eqnok{errest} as
\[
\| \ve_+ \| =
O( \| \ve_c \|^2 + ( u_J  
+ \| \delta \mj \|) \| \ve_c \| + \epsilon_F ).
\]

The results in \cite{ctk:sirev20} show that if the Jacobian
is stored and factored in single precision and the size $N$ of
the problem is not too large,
then there is
no difference in the iteration statistics from storing
and factoring the Jacobian in double precision. So both the approximation
error and the backward error in the solver are $O(u_d)$.
However,
if the Jacobian is stored and factored in half precision, there
are differences caused by the poor accuracy of half precision,
and the nonlinear iteration can converge slowly
or even fail to converge.

The new algorithms in this paper use a half precision factorization
as part of an iterative method to compute a Newton step with a
single precision Jacobian. This approach, as we explain in 
\S~\ref{sec:3p}, requires some care.
