\subsection{Iterative refinement}
\label{sec:IR}

Our first choice for an iterative method will be classic iterative
refinement \cite{wilkinson63} for solving a linear system
$\ma \vu = \vb$. Consistently with the application in this paper,
we will assume that the linear system is in single precision  and that
we factor the matrix in half precision. The reader should be aware that
one must store a half precision copy of $\ma$.
The basic algorithm is


\begin{algorithm}
\label{alg:ir}
{$\mbox{\bf IR}(\ma, \vb, \vu)$}
\begin{algorithmic}
\STATE $\vr = \vb - \ma \vu$
\STATE Store $\ma_h = I_s^h(\ma)$
\STATE Factor $\ma_h$ in half precision to
obtain computed factors ${\hat \ml}$ and ${\hat \mU}$.
\WHILE{$\| \vr \|$ too large}
\STATE $\vd = {\hat \mU}^{-1} {\hat \ml}^{-1} \vr$
\STATE $\vu \leftarrow \vu + \vd$
\STATE $\vr = \vb - \ma \vu$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In Algorithm~\ref{alg:ir} $\vu$ is the initial iterate on input
and the converged solution on output. Note that we are careful to use notation
to stress that we use the computed $LU$ factors in half precision.

One can express the iteration in closed form as
\[
\vu \leftarrow (\mi - {\hat \mU}^{-1} {\hat \ml}^{-1} \ma )\vu
+ {\hat \mU}^{-1} {\hat \ml}^{-1} \vb.
\]
Hence, Algorithm~\ref{alg:ir} is a linear stationary iterative method 
with iteration matrix
\[
\mm  = (\mi - {\hat \mU}^{-1} {\hat \ml}^{-1} \ma ).
\]
So, if the half precision factorization is a sufficiently
good approximation to $\ma$, then
$\| \mm \|$ will be small and iteration will converge rapidly, at least in
exact arithmetic. In the presence of rounding errors we would only expect a
local improvement result. 

Half precision can be very inaccurate and one must be prepared for the iteration
to converge slowly or even diverge. One can show that if the low precision
factorization is a reasonably good approximation to $\ma$, then one obtains
exactly the local improvement results one would like.
One such estimate is from \cite{CarsonHigham} using the $\ell^\infty$
norm on $R^N$. 
In the case here, where $u_h^2 = u_s$, one can show convergence if
\begeq
\label{eq:carson}
3 N u_h \mbox{cond}(\ma) < 1
\endeq
where $\mbox{cond}(\ma) = \| \, | \ma^{-1} | \, |\ma| \, \|_\infty$ and
$| \ma |$ is the matrix with entries $|a_{ij}|$. In that case the iteration
will reduce the linear residual by a factor $O(u_h)$ until
\begeq
\label{eq:backok}
\| \vb - \ma \vu \| = O( u_s \| \vb \| + \| \ma \|_\infty \| \vu \|_\infty).
\endeq

Now we interpret \eqnok{backok} in terms of the inexact Newton conditions
\eqnok{inexact} and \eqnok{inexactJ}. We have $\ma = \mf'(\vx_c)$,
$\vb = \mf(\vx_c)$ and the solution $\vu$ is the Newton step $\vs$. Since
$\| \vs \| = O(\| \mf(\vx_c) \|)$, the estimate \eqnok{backok} implies
\eqnok{inexactJ} with $\eta_J = O(u_s)$.

If the matrix $\ma$ is poorly conditioned, then \eqnok{carson} can fail
and then iterative refinement may fail to converge or fail to satisfy 
\eqnok{backok}. We will see this for the ill-conditioned example
in \S~\ref{sec:results}.

Even if $\| \mm \| > 1$, the condition number of
\[
{\hat \mU}^{-1} {\hat \ml}^{-1} \mj
\]
may be small enough to motivate using a Krylov method with
the low-precision factorization
as a preconditioner.
We use the GMRES-IR approach from \cite{CarsonHigham1,CarsonHigham}
with left preconditioning. This means that we solve 
\[
{\hat \mU}^{-1} {\hat \ml}^{-1} \ma \vd = 
{\hat \mU}^{-1} {\hat \ml}^{-1} \vr
\]
with GMRES to compute the defect $\vd$ in Algorithm~\ref{alg:ir}.
The preconditioner-vector product is computed with two triangular solves.
The results in \S~\ref{sec:results} show how this approach can improve
simple IR in one ill-conditioned case.
