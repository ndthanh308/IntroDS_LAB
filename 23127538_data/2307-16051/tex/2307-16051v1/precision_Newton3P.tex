\subsection{Interprecision Transfers}
\label{sec:ipxf}

Finally, we must discuss some important details of interprecision
transfers and mixed precision operations.

For the two-precision implementation, when we solve
\begeq
\label{eq:fly}
{\hat \mj} \vs = - \mf(\vx_c)
\endeq
for the Newton step, we need to account for the interprecision
transfers. If we do nothing, then the triangular factors are in
precision $u_J$ and $\mf$ is in double precision. 
In that case each operation in the triangular solves will promote the low
precision matrix elements to double within the CPU registers.
This is called ``interprecision transfer on the fly''.

Interprecision transfer on the fly is $O(N^2)$ work
on interprecision transfers, but can be a noticeable cost for medium
to low dimensions even though the factorization cost is $O(N^3)$ work.
A way to avoid this cost is to round $\mf$ to
precision $u_J$ before the solve. One must take care if $\vx_c$ is near
the solution because rounding down, especially in half precision, could
result in an underflow to zero \cite{highamscaling}. The remedy for this
is to scale $\mf$ to a unit vector before rounding and then reverse
the scaling after the linear solve.
With this in mind one solves 
\begeq
\label{eq:nofly}
{\hat \mj} {\hat \vs}  = - I_d^J (\mf(\vx_c)/\| \mf(\vx_c) \|)
\endeq
entirely in the lower precision. This avoids interprecision transfers
during the triangular solves. The one promotes $\hat \vs$ and
reverses the scaling
\begeq
\label{eq:noflystep}
\vs = \| \mf(\vx_c) \| I_J^d {\hat \vs}
\endeq
to obtain a step $\vs$ in precision $u_J$. Then one would update the
solution via
\[
\vx_+ = \vx_c + \vs.
\]
This is exactly what we do in our Julia codes
\cite{ctk:siamfanl,ctk:sirev20}.
The reader should know that the steps $\vs$ computed with
\eqnok{fly} and \eqnok{nofly}-\eqnok{noflystep}
are different, but the performance of
the nonlinear iteration is unlikely to change.

For the linear iterative refinement iteration, the ideas are similar.
Interprecision transfers on the fly are implicit in our discussion
in \S~\ref{sec:IR} where we view iterative refinement as a 
stationary iterative method. Just as in the nonlinear case, one can
mitigate the interprecision transfer cost by replacing the step
\[
\vd = {\hat \mU}^{-1} {\hat \ml}^{-1} \vr
\]
from Algorithm~\ref{alg:ir} with 
\[
\vd = \| \vr \| I_j^s ({\hat \mU}^{-1} {\hat \ml}^{-1} I_s^h (\vr/|\ \vr \|) ).
\]
The iteration is no longer a stationary iterative method. Instead the
iteration is
\begeq
\label{eq:cheapir}
\vu \leftarrow \vu + 
\| \vb - \ma \vu \|
I_j^s \left({\hat \mU}^{-1} {\hat \ml}^{-1} I_s^h 
\left(\frac{\vb - \ma \vu}{\| \vb - \ma \vu \|} \right) \right).
\endeq
The fixed point map
is nonlinear and, because of the interprecision transfers, not even
continuous. However two approaches to interprecision transfer give 
the same results for all but the most ill-conditioned problems.

For IR-GMRES, however, using \eqnok{cheapir} will not suffice.
One must do the triangular solves in the higher
precision, single precision in the case of this paper, and hence assume
the interprecision transfer cost. One way to mitigate this cost is to
map the half precision factorization of $\mj_h$
to single precision before the solve. The 
cost of this is storage (one more copy of $\mj$), but the on-the-fly
interprecision cost is avoided. 
