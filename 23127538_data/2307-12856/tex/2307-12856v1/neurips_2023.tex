\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{bbding}
\usepackage[font=small]{caption}
% \usepackage{caption}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
% \usepackage{ulem}
% \usepackage{minted}  # causes error in arxiv
% \usepackage{mdframed}  # causes error in arxiv

% colors for color-blindness
\definecolor{cb_orange}{RGB}{213,94,0}
% \definecolor{cb_green}{RGB}{0,158,115}
\definecolor{cb_green}{RGB}{34,136,51}
\definecolor{sky_blue}{RGB}{204, 238, 255}
\definecolor{cb_purple}{RGB}{170, 51, 119}
\definecolor{cb_red}{RGB}{204, 51, 17}
\definecolor{cb_blue}{RGB}{0, 119, 187}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% \usepackage{hyperref}
\usepackage[colorlinks=true,citecolor=mydarkblue,linkcolor=mydarkblue,urlcolor=mydarkblue]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}


% commands
\newcommand{\todo}[1]{{\color{red}{TODO: #1}}}
\newcommand{\housing}{\texttt{real-estate}}
\newcommand{\housingweb}{real estate website}
\newcommand{\socialmedia}{\texttt{social-media}}
\newcommand{\socialmediaweb}{social media website}



\title{A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis}


\author{
  Izzeddin Gur$^{1*}$~
  Hiroki Furuta$^{1,2*\text{†}}$~
  Austin Huang$^{1}$~
  \textbf{Mustafa Safdari}$^{1}$~ \\ \\
  \textbf{Yutaka Matsuo}$^{2}$~
  \textbf{Douglas Eck}$^{1}$~
  \textbf{Aleksandra Faust}$^{1}$ \\ \\
  $^{1}$Google DeepMind, $^{2}$The University of Tokyo \quad \\
  \texttt{izzeddin@google.com, furuta@weblab.t.u-tokyo.ac.jp} \\
}


\begin{document}
\maketitle
\begingroup\def\thefootnote{*}\footnotetext{Equal Contribution.}\addtocounter{footnote}{0}\endgroup
\begingroup\def\thefootnote{†}\footnotetext{Work done as Student Researcher at Google.}\addtocounter{footnote}{0}\endgroup

\begin{abstract}
    Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation.
    However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.
    We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions.
    WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those.
    We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.
    We empirically demonstrate that our recipe improves the success on a real website by over 50\%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9\% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
\end{abstract}


\section{Introduction}

Large language models (LLM)~\citep{brown2020language,Chowdhery2022palm,openai2023gpt4,ouyang2022instructgpt,touvron2023llama} can solve variety of natural language tasks, such as arithmetic, commonsense, logical reasoning, question answering, text generation~\cite{brown2020language,kojima2022lets,wei2022emergent,wei2022cot}, and even interactive decision making tasks~\citep{Ahn2022saycan,yao2022react}.
Recently, LLMs have also demonstrated great success in autonomous web navigation, where the agents control computers or browse the internet to satisfy the given natural language instructions through the sequence of computer actions, by leveraging the capability of HTML comprehension and multi-step reasoning~\citep{furuta2023mmwebnav,gur2022html,kim2023language}.

However, web navigation on the real-world websites has still suffered from (1) the lack of pre-defined action space, (2) much longer HTML observations than simulators, and (3) the absence of domain knowledge for HTML in LLMs (\autoref{fig:real_sim_loop}).
Considering the open-ended real-world websites and complexity of instructions, it is challenging to define appropriate action space in advance.
In addition, although several works have argued that instruction-finetuning or reinforcement learning from human feedback improves the HTML understanding and accuracy of web navigation~\citep{furuta2023mmwebnav,kim2023language}, the recent LLMs do not always have optimal designs to process HTML documents.
Most LLMs have shorter context lengths compared to the average tokens of HTML in real websites and do not adopt prior techniques for structured documents, such as text-XPath alignment~\citep{li2021markuplm} and text-HTML token separation~\citep{wang2022webformer}, to prioritize broad task generalization and model-size scaling.
For instance, \autoref{fig:real_html_tokens} presents the statistics of HTML tokens from both simulators and real websites, where HTML tokens of real websites are much longer than those of simulators. It is prohibitively costly to treat such long documents as inputs directly, and even to apply token-level alignments.

% \input{tables/webagent_figure_real_sim_loop}
\input{tables/real_sim_loop}


In this work, we introduce WebAgent, an LLM-driven autonomous agent that can complete the navigation tasks on real websites following user instructions by combining canonical web actions in a program space.
WebAgent (i) \textbf{plans sub-instructions per step} by decomposing natural language instructions, (ii) \textbf{summarizes long HTML pages into task-relevant snippets} based on sub-instructions, and (iii) \textbf{acts via programming} on real websites by grounding sub-instruction and HTML snippet into executable Python codes.
We combine two LLMs to form WebAgent: Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5} for grounded code generation, and newly introduced HTML-T5, a domain-expert pre-trained language model, for task planning and conditional HTML summarization.
HTML-T5 has an encoder-decoder architecture and is specialized to capture the structure -- syntax and semantics -- of long HTML pages better by adopting local and global attention mechanisms~\citep{ainslie2020etc,guo2022longt5} in the encoder.
It is self-supervisedly pre-trained with a \textit{mixture of long-span denoising} objective~\citep{tay2022ul2} on a large-scale HTML corpus synthesized from CommonCrawl\footnote{\url{https://commoncrawl.org/}}.

\input{tables/webagent_figure}

Existing LLM-driven agents often solve decision making tasks with a single LLM prompting different examplers per role~\citep{kim2023language,sun2023adaplanner,wang2023voyager,zheng2023synapse}, which is, however, not enough for real-world tasks whose complexity is higher than that of simulators.
The extensive evaluations reveal that our combined method with \textit{plug-in} language models improves HTML understanding and grounding and achieves better generalization.
WebAgent significantly increases the success rate on real-world web navigation by over 50\%, and detailed analysis reveals that coupling task planning with HTML summarization in specialized language models is essential for task success.
On the static website comprehension tasks~\citep{chen2021websrc}, WebAgent improves the QA accuracy compared to the single LLMs, and shows competitive performance to strong baselines.
Moreover, HTML-T5 not only works as a core plug-in for WebAgent but also achieves state-of-the-art results by itself on the web-based tasks.
On MiniWoB++ benchmark~\citep{liu2018wge,shi2017miniwob}, HTML-T5 achieves 14.9\% higher success than the previous best method~\citep{gur2022html} while outperforming naive local-global attention models~\citep{guo2022longt5} and its instruction-finetuned variants~\citep{chung2022flant5,furuta2023mmwebnav}.
In summary, our key contributions are:
\begin{itemize}[leftmargin=0.5cm,topsep=0pt,itemsep=0.1pt]
    \item We introduce WebAgent, integration of two LLMs for real-world web navigation. The domain-expert language model deals with planning and HTML summarization, and generalist language model generates executable programs.
    \item We present HTML-T5, new HTML-specific language models, by adopting local-global attentions and pre-training with a mixture of long-span denoising on large-scale HTML corpus.
    \item HTML-T5 notably improves the success rate by over 50\% in the real website, and outperforms prior LLM agent by 14.9\% in MiniWoB++.
\end{itemize}

\input{tables/real_html_tokens}


\section{Related Works}
% \paragraph{Web Navigation}
% ~~\textbf{Web Navigation:}~
\textbf{Web Navigation}~
Web navigation is a sequential decision making task where agents manipulate browsers following given instructions~\citep{shi2017miniwob}, such as form filling~\citep{nogueira2013user} or information retrieval~\citep{adolphs2022} through the sequence of computer actions~\citep{li2020mapping,mazumder2020flin,shvoEtAl2021appbuddy}.
Prior works have realized the web automation via reinforcement learning~\citep{gur2018learning,humphreys2022data,jia2018domqnet,shaw2023pixels}, finetuned~\citep{furuta2023mmwebnav,gur2022html} or prompted LLMs~\citep{kim2023language,sun2023adaplanner,yao2022react,zheng2023synapse} on the simulated websites~\citep{shi2017miniwob,toyama2021androidenv,yao2022webshop}.
However, there are still huge gaps between simplified simulators and real web environments (\autoref{fig:real_sim_loop}); for instance, the average tokens for HTML pages are about 15 times larger~(\autoref{fig:real_html_tokens}), and pre-defined action space for specific websites is a strong assumption that may harm the generalization to out-of-distribution web pages or instructions.

MindAct~\citep{deng2023mind2web} could be the most relevant work, where finetuned language model summarizes the raw HTML document into task-relevant snippets, and another language model predicts the web actions in a multi-choice QA format.
While both WebAgent and MindAct combine several language models, MindAct has just adopted off-the-shelf Flan-T5 for both summarization and actor modules, and evaluated on the offline dataset from the real website.
In contrast, we design HTML-T5, specialized for web-based tasks, to handle long HTML documents. WebAgent leverages finetuned HTML-T5 for summarization and planning, and Flan-U-PaLM as a capable programmer, which enables it to generate open-ended web actions and to act on online websites.


% \paragraph{Document Understanding}
% \textbf{Document Understanding:}~
\textbf{Document Understanding}~
Understanding structural documents has been a practical challenge for transformer-based language models. Prior works employ layout-informed tokens~\citep{xu2019layout} or even multimodal tokens from visual inputs~\citep{appalaraju2021docformer,li2021structurallm,li2021selfdoc}. Especially, for the documents written in markup languages, text-XPath alignment~\citep{li2021markuplm}, token separation between text and HTML~\citep{wang2022webformer}, or extra topological information of HTML~\citep{zhao2022tie} are proposed to leverage their syntax better. On the other hand, these domain knowledge conflict with recent generalist and scaling trends around LLMs~\citep{anil2023palm2,openai2023gpt4}. Because web agents require the instruction-conditioned HTML understanding, it also would be desirable to reconcile specialist aspects for HTML documents with generalist capabilities for natural language tasks.
In this work, we design HTML-T5 to incorporate the structural bias of HTML by combining local-global attention for the encoder and a mixture of long-span denoising, while it can solve instruction-following better in downstream web-based tasks.

% \paragraph{Program Synthesis}
% \textbf{Program Synthesis:}~
\textbf{Program Synthesis}~
In addition to common LLMs~\citep{brown2020language,Chowdhery2022palm,touvron2023llama}, several works have proposed programming-focused language models~\citep{chen2021evaluating,feng2020codebert,li2022alphacode,wang2021codet5} and their benchmarks~\citep{austin2021program,hendrycks2021apps,lu2021codexglue}.
Another line of work has investigated the tool augmentation of LLMs~\citep{parisi2022talm} by decoding API calls~\citep{schick2023toolformer} or Python snippets to be parsed with the interpreter~\citep{gao2023pal}.
Most works deal with the program synthesis on the static dataset, except for the attempts in robotics~\citep{liang2023code} and game~\citep{trivedi2022learning,wang2023voyager}, where LLMs output Python or JavaScript snippets to command the agents.
Similarly, we leverage the ability of code generation in LLMs as an open-ended action space for web-based agents to manipulate the real website, and demonstrate LLMs can sequentially decode Python selenium codes considering the given sub-instructions and HTML in the prompts.


%\paragraph{LLM for Task Planning}
% \textbf{LLM for Task Planning:}~
\textbf{LLM for Task Planning}~
The prior knowledge of commonsense in LLMs has allowed us to leverage them for a variety of task planning.
For instance, \citet{huang2022language} propose LLM agent that generates natural language plans in an open-loop manner.
\citet{nottingham2023embodied} and \citet{wang2023describe} perform sequential closed-loop planning on MineCraft.
\citet{singh2022progprompt} decode robotic plans with pythonic text, and several works incorporate planning definition and domain language into the outputs~\citep{liu2023llmp,silver2023generalized,valmeekam2023large}.
On the other hand, our WebAgent leverages finetuned specialist language models and performs closed-loop planning coupled with HTML summarization by decomposing given instructions. We empirically prove that our system is superior to open-loop planning with a single generalist LLM with prompting.


\input{tables/html_t5_figure}


\section{WebAgent}
WebAgent is composed of interactions between HTML-T5, a domain-expert language model to predict the sub-instruction for the next-step program and to conditionally summarize long HTML documents, and Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5}, an instruction-finetuned LLM for grounded program synthesis (\autoref{fig:webagent}).
In contrast to a single LLM prompting with different examplers per role, such a modular approach can deal with real-world tasks better.
We describe the details of each component in the following sections.





\subsection{HTML-T5}
\label{sec:methods}
Previous works demonstrate that \textit{generalist} LLMs, such as T5~\citep{2020t5}, Flan-T5~\citep{chung2022flant5}, and InstructGPT~\citep{ouyang2022instructgpt}, have a better capability of navigating the web environments~\citep{shi2017miniwob} with great HTML comprehension~\citep{furuta2023mmwebnav,gur2022html,kim2023language}, while they have not fully leveraged the HTML-specific inductive bias on syntax and semantics considered in the prior \textit{specialist} transformer models~\citep{li2021markuplm,wang2022webformer,zhao2022tie}.
We here introduce HTML-T5, a capable pre-trained encoder-decoder language models, by interpolating the generalist and specialist nature of language models to solve downstream HTML-based web automation tasks efficiently.
HTML-T5 leverages local and global attentions~\citep{ainslie2020etc,guo2022longt5} in the encoder to handle the hierarchical structure of long HTML inputs, and pre-trains the models with large-scale HTML corpus curated from CommonCrawl on a mixture of long-span denoising objectives~\cite{2020t5,tay2022ul2}.


% \paragraph{Models}
% \textbf{Models:}~
\textbf{Model Architectures}~
In contrast to the natural language, HTML documents have an explicit hierarchy from the tree structure; the relation of each element (e.g. \texttt{<input>}, \texttt{<label>}, \texttt{<button>}) and its attributes (e.g. \texttt{class}, \texttt{label}, \texttt{id}) are often defined locally, and those are iteratively integrated globally (e.g. \texttt{<body>}, \texttt{<form>}, \texttt{<div>}).
To capture such a hierarchical structure of HTML, we adopt local and global attention mechanisms proposed by \citet{guo2022longt5}, instead of common dense attention~\citep{2020t5,vaswani2017attention}.
Local attention basically restricts each token to only attend to neighboring tokens to the left and right. Transient global attention allows each input token to attend to beyond nearby tokens, by dividing the input sequence into blocks of tokens and computing global tokens with summation and normalization of the embeddings of every token in the block.
\autoref{fig:html_t5} describes the concepts of HTML-T5; leaf elements in HTML (\textcolor{cb_green}{green}) could be processed by local attention, and internal elements (\textcolor{cb_purple}{purple}) could be compressed into transient global attention, which naturally fit the hierarchical syntax of HTML documents.
We also note that the elements in HTML are not always captured clearly in the attention head.
We leverage the implementation of LongT5~\citep{guo2022longt5} as base architectures using dense attention in the decoder.



% \paragraph{Pre-Training}
% \textbf{Pre-Training:}~
\textbf{Pre-Training with Mixture of Long-Span Denoising}~
Instead of instruction-finetuning advocated in previous works~\citep{furuta2023mmwebnav,kim2023language}, we perform self-supervised pre-training for HTML-T5 with long-span denoising on HTML corpus to incorporate further inductive bias on HTML; we mask the documents with random spans of tokens (following normal distributions with mean span length $\mu$), and the models take all other tokens from the documents as inputs to predict corrupted spans~\citep{2020t5}.
The mixture of span denoising objectives are recently employed as pre-training tasks in variety of LLMs~\citep{ainslie2023colt5,anil2023palm2,tay2022ul2}.
However, the shorter mean span length (e.g. $\mu=3$), often used in prior works~\citep{2020t5,tay2022ul2} for natural language, suffers from the sparsity of contents tokens; only masking less meaningful chunks in HTML documents, such as \texttt{</}, \texttt{id=}, or \texttt{">}~(\autoref{fig:html_t5}), which might not be helpful for LLMs to capture the syntax and semantics of HTML documents.
Instead, we introduce a \textit{mixture of longer-mean span denoising} objective: replacing $\mu=3$ with $\mu=[8, 64]$, using denoising ratio of 0.15, which contains more semantically meaningful chunks, such as \texttt{<form class="} or \texttt{type="submit">}~(\autoref{fig:html_t5}).

\input{tables/cc_html_stats}

We adopt 4096 input sequence length and 910 output sequence length during the denoising pre-training.
For the dataset, we prepare 100 WARC files (April 2019) from CommonCrawl, and pre-process the raw HTML by removing non-Unicode and alphanumeric documents and extracting subtrees around \texttt{<label>} elements that have \texttt{for} attribute, to reduce the noise in training corpus, which results in about 3.41M examples (\autoref{tab:cc_html_stats}).
We train the models with 100K iterations following other pre-training strategies for T5 families~\citep{chung2022flant5,lester-etal-2021-power}.
See \autoref{sec:implementation} for further details.


\input{tables/real_world_webnav}


% \paragraph{Planning and Summarization}
% \textbf{Planning and Summarization:}~
\textbf{Finetuning for Planning and Summarization}~
We leverage HTML-T5 finetuned with demonstrations as a core module of WebAgent, which performs closed-loop planning with a sequence of sub-instructions and summarizing long HTML documents into concise snippets relevant to the current plan. HTML-T5 takes task instructions, sub-instruction histories, and raw HTML as inputs, and then predicts the next sub-instruction and the corresponding \texttt{data-ref} attributes to extract the snippet with XPath, instead of naively decoding the raw snippet. In the later experiments in Section~\ref{sec:realworld_webnav}, we will demonstrate that linking HTML summarization into sub-instruction prediction is important for real-world web navigation performance.


\subsection{Grounded Program Synthesis}
Web navigation on real-world websites suffers from the open-ended action space, compared to the simplified simulators~\citep{liu2018wge,shi2017miniwob,yao2022webshop}. Unlike previous works~\citep{gur2018learning,humphreys2022data,jia2018domqnet,liu2018wge}, real-world web agents could not pre-define a categorical action space to specify which elements on the websites they should interact.
To overcome such an open-domainness, we introduce \textit{act via programming} paradigm in web navigation by leveraging the capability of LLMs on conditional code generation~\citep{chen2021evaluating,liang2023code}.
Given a few canonical examples for program generation, next sub-instruction, and extracted HTML snippet from HTML-T5, Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5} with 540B parameters decodes an executable Python program (\autoref{fig:webagent}) using Selenium WebDriver~\footnote{\url{https://www.selenium.dev/}}, a library for browser automation.
Such a conditional program synthesis demands that LLMs are capable enough to not only generate the code following natural language instructions, but also understand the semantics and functionality of HTML elements.
We provide several Python snippet examples generated by Flan-U-PaLM as follows (we treat sub-instructions as comments in the script):

%\begin{mdframed}
%\begin{minted}[fontsize=\scriptsize]{python}
\begin{lstlisting}[language=Python]
# Type in walnut creek, ca into search
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').clear()
driver.find_element(
    By.CSS_SELECTOR, '[data-ref="175"]').send_keys("walnut creek, ca")

# Submit the search
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').submit()

# Click on the apartments
driver.find_element(By.CSS_SELECTOR, '[data-ref="572"]').click()

# Scroll down housing type by 200px
driver.execute_script(
    'getScrollParent(document.querySelector'
    '("#type-of-housing")).scrollBy({top: 200})'
)
\end{lstlisting}
%\end{minted}
%\end{mdframed}


 

\input{tables/websrc}

\section{Experimental Results}
In order to study how a modular combination of LLMs enables real-world web navigation by overcoming open-endedness and long context, we execute instruction-following tasks on real websites (Section~\ref{sec:realworld_webnav}) and static HTML comprehension tasks (Section~\ref{sec:websrc}). Moreover, we analyze the performance of \textit{plug-in} language model, HTML-T5, on web-based tasks in detail (Section~\ref{sec:htmlt5_ablations}).


\input{tables/miniwob_sl_results}


\subsection{Real-world Web Navigation}
\label{sec:realworld_webnav}
\textbf{Evaluation Methodology}~
We first evaluate WebAgent with the real-world navigation performance under human supervision, at \housingweb{}, a platform for housing, and \socialmediaweb{}, a network of communities.
WebAgent receives natural language instructions (e.g. \textit{Can you search for a studio bedroom, 1+ bathroom apartments in oroville, ca for corporate housing on \housingweb{}?}, or \textit{Could you present the most new thread of Python community filtered by Tutorial tag on \socialmediaweb{}?}), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See \autoref{sec:webagent_example_flow} for the example workflow.
We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates.
This results in 260 episodes on \housingweb{} and 230 episodes on \socialmediaweb{} (about 20/10 steps per episode respectively).

We prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode~\citep{yao2022webshop}; for instance (1) \textit{apartments} for (2) \textit{corporate housing} with (3) \textit{studio bedroom} and (4) \textit{1+ bathroom} located in (5) \textit{oroville, ca}.
When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score would be 60 ($ = 100 \times 3/5$).
When the agents could achieve 100 score, that episode would mark as success.

\textbf{Results}~
For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: WebAgent replacing closed-loop planning from HTML-T5 with few-shot open-loop planning from Flan-U-PaLM (\textbf{Plan}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}), replacing HTML summarization from HTML-T5 with regular-expression-based retrieval (\textbf{Sum}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}), and both of them (\textbf{Plan}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}, \textbf{Sum}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}).
\autoref{tab:realworld_results} shows that WebAgent with HTML-T5 for planning and summarization  (\textbf{Plan}: \textcolor{cb_green}{{\scriptsize \CheckmarkBold}}, \textbf{Sum}: \textcolor{cb_green}{{\scriptsize \CheckmarkBold}}) achieves best 65\% success and 87.6 score on \housing{} and 70\% success and 85.8 score on \socialmedia{}, significantly outperforming single LLM (\textbf{Plan}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}, \textbf{Sum}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}), that with open-loop planning (\textbf{Plan}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}), and that with regular-expression retrieval (\textbf{Sum}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}}) (most of those roughly achieve only 10 - 20\% success).
This result suggests that closed-loop planning grounded on HTML observations via finetuning of domain language models is much more suitable for open-ended web navigation than open-loop planning with few-shot LLMs, which is remarkable in \housing{} (even \textbf{Sum}: \textcolor{cb_red}{{\scriptsize \XSolidBrush}} achieves 50\% success), where the longer planning horizon is needed to fulfill instructions. We guess enhancing the planning ability to decompose the given instructions adaptively and robustly can help further improve WebAgent.

\textbf{Error Analysis}~
We also analyze the reason of failures by the category of error (\autoref{tab:realworld_results}): programming, planning, and summarization errors. The programming error does not satisfy the given sub-instructions or HTML snippet. In the planning error, predicted sub-instructions conflict with user instructions, and the summarization error fails to extract the relevant HTML snippets for given sub-instructions.
Through such a classification of failure mode, we observe that coupling sub-instruction prediction with HTML summarization in HTML-T5 plays a critical role in task success.
WebAgent often fails planning steps by predicting the incorrect sub-instructions (for instance, in \textit{real-estate}, WebAgent generates incorrect plans in 70.0\% of failure episodes), and other baselines tend to fail programming or summarization steps more to specify the elements to be interacted with. This might be because the disagreement between sub-instructions and HTML snippets induces the wrong pointer to \texttt{data-ref} simultaneously.


\input{tables/html_t5_ablation}


% \paragraph{}
% \textbf{Static HTML Comprehension:}~
\subsection{WebSRC: Static HTML Comprehension}
\label{sec:websrc}
To emphasize the advantage of our modular approach, we test WebAgent on a static website comprehension benchmark, WebSRC~\citep{chen2021websrc}, which is a contextual QA dataset with HTML documents. The questions require an understanding of the spatial and logical structure of websites, and the answers are either text span on HTML or yes/no.
For the comprehensive evaluation of language models, WebSRC has three different types of websites, \textit{KV}, \textit{Comparison}, and \textit{Table}.
KV task is a value extraction from the attribute key.
Comparison task has several entities with the same attributes.
Table task requires a structural understanding with header columns and values in the row.
We finetune HTML-T5 to predict the \texttt{data-ref} of snippet and use dev set for the evaluation.

As shown in \autoref{tab:websrc_results}, single LLM, such as Flan-U-PaLM or HTML-T5, has struggled to the limited context length or model capacity. In contrast, WebAgent, our LLM-collaborative approach, enhances the performance from both single generalist and specialist LLMs, and shows competitive results with strong baselines. This demonstrate that plug-in LLMs works complementally to each other.
In more detail, WebAgent is better at Comparison tasks, but inferior to structural understanding for KV and Table tasks, compared to other baselines. See \autoref{sec:websrc_details} for further details.


\subsection{A Closer Look at HTML-T5}
\label{sec:htmlt5_ablations}

In addition to the evaluation as WebAgent system, we extensively examine HTML-T5 about (1) performance on standard web navigation benchmark~\citep{shi2017miniwob}, (2) its architecture and pre-training objective, and (3) pre-training dataset and model initialization.
We adopt 16K tokens for the context window unless otherwise mentioned.
In Appendix, we also evaluate HTML-T5 on description generation benchmark to test HTML understanding on static dataset~\citep{gur2022html} (\autoref{sec:desc_gen}), and on offline task planning with \housing{} traces (\autoref{sec:offline_plan}).

% \paragraph{MiniWoB++}
% \textbf{MiniWoB++:}~
\textbf{MiniWoB++}~
In addition to real-world evaluation, we extensively study the performance of HTML-T5 on simulated environments, MiniWoB++ with 56 tasks by running 100 evaluation episodes per task.
We finetune HTML-T5 with 12K human demonstrations~\citep{liu2018wge}, and compare the average success rate to prior supervised-learned agents~\citep{gur2022html,humphreys2022data}, LongT5, and its instruction-finetuned variants~\citep{chung2022flant5,furuta2023mmwebnav} we prepared~\footnote{We finetune LongT5 models with Flan dataset released by \citet{chung2022flant5}.
As a sanity check, we test them on reasoning and summarization tasks (see \autoref{sec:flan_longt5}).
}.
\autoref{tab:miniwob_sl_results} shows that HTML-T5-XL significantly outperforms WebN-T5, the prior best model, by 14.9\%.
Notably, we demonstrate HTML-denoising consistently improves the performance on top of LongT5 in all the model sizes, better than instruction-finetuning introduced in prior work~\citep{furuta2023mmwebnav}. 
Furthermore, we finetune HTML-T5-XL with 347K demonstrations from \citet{furuta2023mmwebnav}, which performs better than 11B-parameter Flan-T5-XXL even with 3B parameters.
Noticably, HTML-T5-XL is the first model to solve the most challenging MiniWoB task, \texttt{book-flight}, by only using limited labeled HTML data (99\% success; see \autoref{sec:per_task_miniwob_results}).
These prove we successfully incorporate domain knowledge on HTML comprehension for web navigation into pre-trained language models.


% \paragraph{Architecture and Objective}
% ~~\textbf{Architecture and Objective:}~
\textbf{Architecture and Objective}~
We hypothesize that local and global attention mechanisms can capture the hierarchical structures of HTML documents better than dense attention.
We compare the web navigation performance among 56 MiniWoB++ tasks~\citep{gur2022html}, by finetuning HTML-T5 with public 12K-episode dataset~\citep{liu2018wge}.
We adopt 2048 and 4096 tokens as input length and prepare Base-size architectures.
\autoref{tab:html_t5_ablation} (left) reveals that the combination of local and global attentions achieves the superior success rate by over 15\% compared to the instruction-finetuned dense attentions~\citep{chung2022flant5,2020t5} and local attention only.
Surprisingly, local attention only still surpasses the dense attention by about 9\%, which suggests local relation between elements and attributes in HTML are essential for web tasks.

As for pre-training objective shown in \autoref{tab:html_t5_ablation} (right), HTML-denoising generally improves the performance on offline task planning on \housingweb{} and MiniWoB. Especially, using only longer span lengths ($\mu=[8, 64]$) outperforms other choices, including the default configuration in natural language domain ($\mu=[3, 8, 64, \text{Prefix}]$), which can reduce the less meaningful prediction from shorter spans (e.g. $\mu=3$), and inject the structural bias of HTML better.
See \autoref{sec:offline_plan} for further results on offline task planning with model scaling.

\input{tables/data_init_results}

% \paragraph{Dataset and Initialization}
% \textbf{Dataset and Initialization:}~
\textbf{Dataset and Initialization}~
To test our recipe described in Section~\ref{sec:methods}, we compare the different dataset and model initialization for pre-training on downstream task performances; offline task planning on \housing{} website and average success rate on MiniWoB.
We use Base-size models for the experiments. For HTML-denoising, we prepare the corpus from CommonCrawl with (Extracted) or without (Raw) subtree extraction around label elements on the documents.
We also compare the initialization of base architectures before HTML-denoising; from scratch or with pre-trained models on PEGASUS objective~\citep{zhang2020pegasus} that is a masked important sentence prediction from long-context paragraph.
\autoref{tab:data_init_results} reveals that snippet extraction on HTML corpus improves downstream performances since such a pre-processing can reduce the noise in raw HTML.
Moreover, initialization with PEGASUS pre-trained weights is essential for HTML-T5, because of the long-context and instruction-following nature of HTML-based tasks.



\section{Discussion and Limitation}
\textbf{Specialist Language Models as Plug-in Modules}~
We demonstrate it is beneficial to divide web navigation into planning, HTML summarization, and code generation, and to combine domain-expert language models finetuned with target data as plug-in modules.
Such plug-in approaches have also been adopted to support the inference of LLMs~\citep{xu2023small}, multimodal tasks~\citep{zeng2022socratic}, and robotics~\citep{Ahn2022saycan}, which, however, might cause additional computational costs and latency.

\textbf{Broad Generalization across the Internet}~
Because open-loop planning with prompted Flan-U-PaLM achieves at most 10 - 20\% success, we have demonstrated that finetuning language models with demonstrations on real websites is required for planning modules, which, however, means we still should collect the data per each website and may limit the generalization across the internet. It would be required to collect the demonstrations and train larger domain-expert models at scale.

\textbf{Feedback for Program Synthesis}~
We leverage Flan-U-PaLM with 540B parameters, as a capable program synthesis module via few-shot prompting.
Such a large model, however, makes it challenging to reflect the feedback about the errors in generated code, compared to smaller models.
We leave it as future work to incorporate the feedback for program synthesis into larger language models.

\textbf{Evaluation for Real-world Web Navigation}~
Beyond the simulated web environments~\citep{shi2017miniwob,yao2022webshop}, we have exhibited WebAgent can follow given complex and sometimes ambiguous instructions on the real estate and \socialmediaweb{}.
On the other hand, it is costly to evaluate the performance of autonomous agents in the real world.
Automated evaluation with minimal human supervision would be helpful for the scalable development of real-world web agents.



\section{Conclusion}
\label{sec:conclusion}
We build a system for real-world web navigation, combining HTML-T5 for planning and HTML summarization and Flan-U-PaLM for grounded program synthesis.
Our proposed WebAgent achieves around 70\% success on real websites outperforming single LLM approach by over 50\%, which suggests dividing the sequence of sub-problems with multiple language models can increase the entire task success.
We also propose a scalable recipe for HTML-specialized language models where we train local and global attention mechanisms with a mixture of long-span denoising objectives to capture the hierarchical structures of HTML documents.
HTML-T5 not only plays an essential role in WebAgent but also can achieves the best results on a variety of HTML-based benchmarks such as MiniWoB++.
We hope our work contributes to getting us one-step closer to the practical deployment of autonomous web agent systems.


% \clearpage

\section*{Acknowledgements}
We thank Heiga Zen, Yingjie Miao, Yusuke Iwasawa, Joshua Ainslie, Santiago Ontanon, Quoc V. Le, Zoubin Ghahramani, Jeff Dean, Tris Warkentin for the supports and advises on this work. HF was supported by JSPS KAKENHI Grant Number JP22J21582.


\bibliography{reference}
\bibliographystyle{plainnat}

\clearpage
\appendix
% \onecolumn
\section*{Appendix}

\section{Broader Impacts}
The development of autonomous agents should consider the security and safety aspects.
In the real website evaluation, we have carefully conducted the experiments under human supervision in case undesired behaviors happen.
We limit the access per second not to stress the server.
We have anonymized the real websites we tested on for safety and privacy concerns.


\section{Implementation Details on HTML-T5}
\label{sec:implementation}
We use the implementation of local and global attentions released by \citet{guo2022longt5}~\footnote{\url{https://github.com/google-research/longt5}}.
Following \citet{guo2022longt5}, we set the local radius to $r=127$, and block size for transient global attention to $k=16$.
For the pre-training objective, similar to \citet{tay2022ul2}, we construct the mixtures and then use long mean span lengths: $\mu=[8, 64]$, and all the denoising ratio is set to 0.15.
We adopt 4096 input sequence length and 910 output sequence length during the pre-training. The batch size for training is set to 128.
We train the models with 100K iterations following other pre-training strategies for T5 families~\citep{chung2022flant5,lester-etal-2021-power}.
We leverage SeqIO~\citep{roberts2022t5x} and T5X~\citep{roberts2022t5x} library to manage the training pipeline. We also use SentencePiece~\citep{kudo2018sentencepiece} with 32K tokens from C4 dataset~\citep{2020t5} as a tokenizer.
During the downstream finetuning, we adopt 16K tokens for the context window unless otherwise mentioned.
We have used cloud TPU-v3, which has a 32 GiB HBM memory space, with 128 cores for the experiments.

% \clearpage
\section{WebAgent Example Flow in \housing{} website}
\label{sec:webagent_example_flow}
\input{tables/housing_flow_figure}


\clearpage
\section{Flan-LongT5}
\label{sec:flan_longt5}
In the web navigation literature~\citep{furuta2023mmwebnav,kim2023language}, instruction-finetuned LLMs have great success in HTML comprehension and improve the task success.
For the comparison to HTML-denosing, we prepare the instruction-finetuned LongT5 (i.e. Flan-LongT5) by leveraging Flan dataset released by \citet{chung2022flant5}.
We finetuned the pre-trained LongT5 with 100K iterations and picked up the best checkpoints.

As a sanity check of instruction-tuning, we evaluate Flan-LongT5 with few-shot/zero-shot settings on CoT benchmark (GSM8K~\citep{cobbe2021verifiers}, StrategyQA~\citep{geva2021did}, SVAMP~\citep{patel2021nlp}, Asdiv~\citep{miao2021diverse}, CommonsenseQA~\citep{talmor2019commonsenseqa}), BigBench-Hard (BBH)~\citep{suzgun2022bbh}, and MMLU~\citep{hendrycks2021measuring} as tested in \citet{longpre2023flan2}.
We reevaluate the performance of Flan-T5, using official checkpoints~\footnote{\url{https://github.com/google-research/t5x/blob/main/docs/models.md\#flan-t5-checkpoints}}.
We also check the performance of Flan-LongT5 on downstream summarization tasks, originally evaluated on LongT5~\citep{guo2022longt5}.
We use arXiv~\citep{cohan2018discourseaware}, PubMed~\citep{cohan2018discourseaware}, BigPatent~\citep{sharma2019bigpatent}, Multi-News~\citep{fabbri2019multinews}, MediaSum~\citep{zhu2021mediasum}, CNN / Daily Mail~\citep{nallapati2016summarunner} dataset for the evaluation, measuring the performance with ROUGE-1/2/L metrics.

\autoref{tab:flan_reasoning_bench} shows that we have successfully replicated the LongT5 version of instruction-finetuned language models.
Flan-LongT5 achieves competitive results to original Flan-T5; for instance, Flan-LongT5-Large (36.64) outperforms Flan-T5-Large (35.25), but Flan-LongT5-XL (39.05) is still behind Flan-T5-XL (43.03) on average.
This might be caused by the training instability of XL-size models~\citep{guo2022longt5}.
Because, unlike HTML-T5 on HTML-based tasks, reasoning tasks do not have long-context or hierarchical syntax, it is not surprising for Flan-LongT5 not to outperform Flan-T5.
\autoref{tab:longt5_summarization_bench} also demonstrates that we have successfully conducted instruction-tuning without losing the capability of long text summarization.


\begin{table*}[ht]
\begin{center}
\begin{small}
%\begin{sc}
\scalebox{0.875}{
\begin{tabular}{lrrrrrrrrrrr}
\toprule
 & \multicolumn{2}{c}{\textbf{CoT}} & \multicolumn{2}{c}{\textbf{MMLU}} & \multicolumn{2}{c}{\textbf{BBH}} & \multicolumn{2}{c}{\textbf{BBH-CoT}} & \multicolumn{3}{c}{\textbf{Avg.}}\\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-12}
\textbf{Models} & Zero & Few  & Zero & Few & Zero & Few & Zero & Few & CoT & Direct & Total \\
\midrule
Flan-T5-Large & 35.14 & 40.03 & 40.68 & 45.12 & 25.90 & 37.48 & 26.17 & 31.45 & 33.20 & 37.29 & 35.25 \\
Flan-T5-XL & 51.74 & 52.64 & 50.76 & 52.40 & 26.09 & 40.96 & 34.12 & 35.62 & 43.53 & 42.55 & 43.04 \\
\midrule
Flan-LongT5-Large & 44.78 & 45.34 & 38.44 & 40.03 & 28.67 & 34.67 & 29.38 & 31.85 & 37.84 & 35.45 & 36.64 \\
Flan-LongT5-XL & 48.78 & 50.02 & 43.44 & 44.74 & 26.53 & 37.77 & 29.09 & 32.01 & 39.97 & 38.12 & 39.05 \\
\bottomrule
\end{tabular}
}
%\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\caption{Performance of Flan-LongT5 on reasoning tasks. We reevaluate the performance of Flan-T5~\citep{chung2022flant5}, using official checkpoints. Flan-LongT5 achieves competitive results to original Flan-T5.}
\label{tab:flan_reasoning_bench}
\end{table*}

\begin{table}[ht]
\begin{center}
\begin{small}
%\begin{sc}
\scalebox{0.6}{
\begin{tabular}{lrrrrrrrrrrrrrrrrrr}
\toprule
 & \multicolumn{3}{c}{\textbf{arXiv}} & \multicolumn{3}{c}{\textbf{PubMed}} & \multicolumn{3}{c}{\textbf{BigPatent}} & \multicolumn{3}{c}{\textbf{MultiNews}} &
 \multicolumn{3}{c}{\textbf{MediaSum}} & \multicolumn{3}{c}{\textbf{CNN / Daily Mail}} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16} \cmidrule(r){17-19}
\textbf{Models} & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L \\
\midrule
LongT5-Large & 48.28 & 21.63 & 44.11 & 49.98 & 24.69 & 46.46 & 70.38 & 56.81 & 62.73 & 47.18 & 18.44 & 24.18 & 35.54 & 19.04 & 32.20 & 42.49 & 20.51 & 40.18 \\
LongT5-XL & 48.35 & 21.92 & 44.27 & 50.23 & 24.76 & 46.67 & \textbf{76.87} & \textbf{66.06} & \textbf{70.76} & 48.17 & 19.43 & \textbf{24.94} & 36.15 & 19.66 & 32.80 & \textbf{43.94} & \textbf{21.40} & \textbf{41.28} \\
\midrule
Flan-LongT5-Large & \textbf{48.52} & \textbf{22.00} & \textbf{44.46} & \textbf{50.46} & \textbf{25.08} & \textbf{46.96} & 70.53 & 57.13 & 63.02 & 47.76 & 18.99 & 24.52 & 35.71 & 19.18 & 32.33 & 43.13 & 20.89 & 37.28 \\
Flan-LongT5-XL & 48.37 & 21.75 & 44.22 & 50.23 & 24.75 & 46.73 & 76.31 & 65.17 & 70.01 & \textbf{48.19} & \textbf{19.47} & 24.80 & \textbf{36.16} & \textbf{19.75} & \textbf{32.81} & 43.46 & 21.00 & 37.34 \\
\bottomrule
\end{tabular}
}
%\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\caption{Performance of Flan-LongT5 on downstream summarization tasks, compared to LongT5~\citep{guo2022longt5}. We measure the performance with ROUGE-1/2/L metrics.}
\label{tab:longt5_summarization_bench}
\end{table}


\clearpage
\section{Details of WebSRC}
As did in real-world web navigation, HTML-T5 first predicts \texttt{data-ref} attribute of task-relevant snippet from the input HTML document.
To make sure there is enough context, we extract the snippet from the predicted element to the two-level-up via XPath.
If it exceeds the context length of Flan-U-PaLM, we limit it into parent elements. If it still does not work, we truncate the end of extracted snippet to fit within the token budget.
Because snippet extraction in table structure often loses the context to solve question-answering, we just truncate HTML document in Table tasks.
Flan-U-PaLM predicts the answers seeing 5-shot examples.

\autoref{fig:websrc_fig} presents the performance comparison on different types of websites (KV, Comparison, Table) among MarkupLM~\citep{li2021markuplm}, TIE~\citep{zhao2022tie}, and WebAgent.
WebAgent is better at Comparison tasks, but inferior to structural understanding for KV and Table tasks, compared to other baselines, which suggest that generalist LLMs are still not suitable for recognizing structural data such as table.

\label{sec:websrc_details}
% \input{tables/websrc}
\input{tables/websrc_fig}

\section{A Closer Look at HTML-T5: Offline Evaluation on Task Planning}
\label{sec:offline_plan}
We compere the offline task planning performance between HTML-T5 and LongT5 (without HTML-denosing) with different model sizes; with Base (220M parameters), Large (770M parameters), and XL (3B parameters).
As described in Section~\ref{sec:methods}, the models predict the next sub-instructions in a closed-loop manner considering the current HTML observations, user instructions, and previous sub-instruction histories as inputs.
For offline task planning evaluation, we use the demonstrations on \housing{} website; preparing 130 demonstrations and splitting them into train (90\%) and test splits (10\%). We report the best per-step exact match accuracy in test set.

\autoref{tab:offline_eval_results} shows that HTML-T5 outperforms LongT5 on the accuracy of sub-instruction prediction, which demonstrates that HTML-denoising pre-training captures the structural bias of HTML better without sacrificing the ability to understand natural language instructions. 
This also implies that our proposed HTML-denoising can scale to larger-size models consistently.

\input{tables/housing_planning}


\clearpage
\section{A Closer Look at HTML-T5: Description Generation}
\label{sec:desc_gen}
We also investigate the capability of HTML-T5 on static HTML comprehension tasks, as well as interactive decision making tasks.
We use Description Generation benchmark~\citep{gur2022html}, where the models generate the textual description of elements, typically used for accessibility purposes and annotated with a special attribute in the HTML schema known as \texttt{for}. We evaluate the understanding the structure of HTML as it would appear to a user, despite not having access to the rendered website directly.

We compare LaMDA~\cite{thoppilan2022lamda}, T5, LongT5, and HTML-T5 with respect to accuracy, BLEU~\citep{papineni-etal-2002-bleu}, and ROUGE-1~\citep{lin-2004-rouge} score.
As shown in \autoref{tab:descgen_results}, local and global attention mechanisms, underlying between LongT5 and HTML-T5, could almost solve the benchmark by improving the previous best performance by over 10\%, with still improved performance as model size increases.
Compared to the effect of local-global attention, HTML-T5 marginally improves against LongT5, which emphasizes that local and global attentions are critical to capture the hierarchical structure of HTML documents.

\input{tables/descgen_results}

\clearpage
\section{Per-Task Performance on MiniWoB++}
\label{sec:per_task_miniwob_results}

\input{tables/miniwob_per_task}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
