@article{chen2021evaluating,
  title   = {Evaluating Large Language Models Trained on Code},
  author  = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year    = {2021},
  journal = {arXiv preprint arXiv:2107.03374}
}

@article{bommasani2021opportunities,
  title   = {On the Opportunities and Risks of Foundation Models},
  author  = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Kohd and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  year    = {2021},
  journal = {arXiv preprint arXiv:2108.07258}
}

@misc{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019}
}

@article{brown2020language,
  title   = {Language Models are Few-Shot Learners},
  author  = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year    = {2020},
  journal = {arXiv preprint arXiv:2005.14165}
}

@misc{li2022alphacode,
  title  = {Competition-Level Code Generation with AlphaCode},
  author = {Yujia Li and David Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and Remi Leblond and Tom Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and Thomas Hubert and Peter Choy and Cyprien de and Masson dAutume and Igor Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey Cherepanov and James Molloy and Daniel J. Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de Freitas and Koray Kavukcuoglu and Oriol Vinyals},
  year   = {2022}
}

@article{Alayrac2022flamingo,
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  title = {Flamingo: a Visual Language Model for Few-Shot Learning},
  journal = {arXiv preprint arxiv:2204.14198},
  year = {2022},
}

@article{chen2022pali,
  author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish and Bradbury, James and Kuo, Weicheng and Seyedhosseini, Mojtaba and Jia, Chao and Ayan, Burcu Karagol and Riquelme, Carlos and Steiner, Andreas and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
  title = {PaLI: A Jointly-Scaled Multilingual Language-Image Model},
  journal = {arXiv preprint arxiv:2209.06794},
  year = {2022},
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
}

@article{chung2022flant5,
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  title = {Scaling Instruction-Finetuned Language Models},
  year = {2022},
  journal = {arXiv preprint arxiv:2210.11416},
}

@article{gur2022html,
  author = {Gur, Izzeddin and Nachum, Ofir and Miao, Yingjie and Safdari, Mustafa and Huang, Austin and Chowdhery, Aakanksha and Narang, Sharan and Fiedel, Noah and Faust, Aleksandra},
  title = {Understanding HTML with Large Language Models},
  journal = {arXiv preprint arxiv:2210.03945},
  year = {2022},
}

@article{nakano2021webgpt,
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  title = {WebGPT: Browser-assisted question-answering with human feedback},
  year = {2021},
  journal = {arXiv preprint arXiv:2112.09332},
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  year={2017}
}

@article{Ziegler2019fine,
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  title = {Fine-Tuning Language Models from Human Preferences},
  year = {2019},
  journal = {arXiv preprint arxiv:1909.08593},
}

@article{ouyang2022instructgpt,
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  title = {Training language models to follow instructions with human feedback},
  year = {2022},
  journal = {arXiv preprint arxiv:2203.02155},
}

@article{tang2022udop,
  author = {Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  title = {Unifying Vision, Text, and Layout for Universal Document Processing},
  year = {2022},
  journal = {arXiv preprint arxiv:2212.02623},
}

@article{devlin2019bert,
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author  = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year    = {2019},
  journal = {arXiv preprint arXiv:1810.04805}
}

@article{Chowdhery2022palm,
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  journal = {arXiv preprint arXiv:2204.02311},
  title = {PaLM: Scaling Language Modeling with Pathways},
  year = {2022},
}

@article{jaegle2021perceiver,
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Hénaff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joāo},
  title = {Perceiver IO: A General Architecture for Structured Inputs \& Outputs},
  journal = {arXiv preprint arXiv:2107.14795},
  year = {2021},
}

@article{lu2022unifiedio,
  author = {Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  title = {Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.11795},
}

@article{yao2022react,
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  title = {ReAct: Synergizing Reasoning and Acting in Language Models},
  year = {2022},
  journal = {arXiv preprint arXiv:2210.03629},
}

@article{wei2022emergent,
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  title = {Emergent Abilities of Large Language Models},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.08853},
}

@inproceedings{stiennon2020learning,
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  title = {Learning to summarize from human feedback},
  year = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
}

@article{shoeybi2019megatronlm,
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.08053},
}

@article{rae2021gopher,
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  title = {Scaling Language Models: Methods, Analysis \&; Insights from Training Gopher},
  year = {2021},
  journal = {arXiv preprint arXiv:2112.11446},
}

@article{kaplan2020scaling,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title = {Scaling Laws for Neural Language Models},
  year = {2020},
  journal = {arXiv preprint arXiv:2001.08361},
}

@article{austin2021program,
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  title = {Program Synthesis with Large Language Models},
  year = {2021},
  journal = {arXiv preprint arXiv:2108.07732},
}

@article{tay2022ul2,
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  title = {UL2: Unifying Language Learning Paradigms},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.05131},
}

@article{wei2022cot,
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.11903},
}

@inproceedings{kojima2022lets,
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  title = {Large Language Models are Zero-Shot Reasoners},
  year = {2022},
  booktitle={Advances In Neural Information Processing Systems},
}

@article{iyer2022optiml,
  author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
  title = {OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  journal = {arXiv preprint arXiv:2212.12017},
  year = {2022},
}

@article{roberts2022t5x,
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
  journal={arXiv preprint arXiv:2203.17189},
  year = {2022},
}

@inproceedings{wei2022flan,
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  title = {Finetuned Language Models Are Zero-Shot Learners},
  year = {2022},
  booktitle={International Conference on Learning Representations},
}

@article{kudo2018sentencepiece,
  author = {Kudo, Taku and Richardson, John},
  title = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  year = {2018},
  journal={arXiv preprint arXiv:1808.06226},
}

@article{lewis2019bart,
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  year = {2019},
  journal={arXiv preprint arXiv:1910.13461},
}

@article{longpre2023flan2,
  author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  title = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  journal={arXiv preprint arXiv:2301.13688},
  year = {2023},
}

@inproceedings{guo2022longt5,
    title = {{L}ong{T}5: {E}fficient Text-To-Text Transformer for Long Sequences},
    author = {Guo, Mandy  and
      Ainslie, Joshua  and
      Uthus, David  and
      Ontanon, Santiago  and
      Ni, Jianmo  and
      Sung, Yun-Hsuan  and
      Yang, Yinfei},
    booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
    year = {2022},
    pages = {724--736},
}

@inproceedings{zhang2020pegasus,
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  year = {2020},
  booktitle={International Conference on Machine Learning},
}

@inproceedings{zelikman2022star,
    title={{ST}aR: Bootstrapping Reasoning With Reasoning},
    author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah Goodman},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022},
}

@article{anli2022lengthgen,
  author = {Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  title = {Exploring Length Generalization in Large Language Models},
  year = {2022},
  journal={arXiv preprint arXiv:2207.04901},
}

@article{honovich2022instruction,
  author = {Honovich, Or and Shaham, Uri and Bowman, Samuel R. and Levy, Omer},
  title = {Instruction Induction: From Few Examples to Natural Language Task Descriptions},
  year = {2022},
  journal={arXiv preprint arXiv:2205.10782},
}

@article{jang2023roe,
  author = {Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  title = {Exploring the Benefits of Training Expert Language Models over Instruction Tuning},
  year = {2023},
  journal={arXiv preprint arXiv:2302.03202},
}

@inproceedings{kuznia2022less,
    title = {Less is More: Summary of Long Instructions is Better for Program Synthesis},
    author = {Kuznia, Kirby and Mishra, Swaroop  and Parmar, Mihir  and Baral, Chitta},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    year = "2022",
    pages = "4532--4552",
}

@inproceedings{wang2023selfconsistency,
    title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
    author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
    booktitle={International Conference on Learning Representations},
    year={2023},
}

@inproceedings{zhou2023leasttomost,
title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
author={Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi},
booktitle={International Conference on Learning Representations},
year={2023},
}

@inproceedings{min2022rethinking,
    title = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
    author = {Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    year = {2022},
    pages = {11048--11064},
}

@inproceedings{min2022metaicl,
    title = {{M}eta{ICL}: Learning to Learn In Context},
    author = {Min, Sewon  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    year = {2022},
    pages = {2791--2809},
}

@article{aghajanyan2021muppet,
  author = {Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  title = {Muppet: Massive Multi-task Representations with Pre-Finetuning},
  year = {2021},
  journal={arXiv preprint arXiv:2101.11038},
}

@inproceedings{wang2022super,
    title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    pages = "5085--5109",
}

@article{huang2022selfimprove,
  author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  title = {Large Language Models Can Self-Improve},
  year = {2022},  
  journal={arXiv preprint arXiv:2210.11610},
}

@article{selfinstruct,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{cobbe2021verifiers,
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  title = {Training Verifiers to Solve Math Word Problems},
  journal={arXiv preprint arXiv:2110.14168},
  year = {2021},
}

@article{ainslie2023colt5,
      title={CoLT5: Faster Long-Range Transformers with Conditional Computation}, 
      author={Joshua Ainslie and Tao Lei and Michiel de Jong and Santiago Ontañón and Siddhartha Brahma and Yury Zemlyanskiy and David Uthus and Mandy Guo and James Lee-Thorp and Yi Tay and Yun-Hsuan Sung and Sumit Sanghai},
      year={2023},
      journal={arXiv preprint arXiv:2303.09752},
}

@article{huang2022inner,
  author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  title = {Inner Monologue: Embodied Reasoning through Planning with Language Models},
  journal = {arXiv preprint arxiv:2207.05608},
  year = {2022},
}

@article{shinn2023reflexion,
      title={Reflexion: an autonomous agent with dynamic memory and self-reflection}, 
      author={Noah Shinn and Beck Labash and Ashwin Gopinath},
      year={2023},
      journal = {arXiv preprint arxiv:2303.11366},
}

@article{kim2023language,
      title={Language Models can Solve Computer Tasks}, 
      author={Geunwoo Kim and Pierre Baldi and Stephen McAleer},
      year={2023},
      journal = {arXiv preprint arxiv:2303.17491},
}

@article{madaan2023selfrefine,
      title={Self-Refine: Iterative Refinement with Self-Feedback}, 
      author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Sean Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and Amir Yazdanbakhsh and Peter Clark},
      year={2023},
      journal = {arXiv preprint arxiv:2303.17651},
}

@article{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      journal = {arXiv preprint arxiv:2302.13971},
}

@article{to2023better,
      title={Better Language Models of Code through Self-Improvement}, 
      author={Hung Quoc To and Nghi D. Q. Bui and Jin Guo and Tien N. Nguyen},
      year={2023},
      journal={arXiv preprint arxiv:2304.01228},
}

@article{furuta2023mmwebnav,
      title={Multimodal Web Navigation with Instruction-Finetuned Foundation Models}, 
      author={Hiroki Furuta and Ofir Nachum and Kuang-Huei Lee and Yutaka Matsuo and Shixiang Shane Gu and Izzeddin Gur},
      year={2023},
      journal={arXiv preprint arxiv:2305.11854},
}

@article{li2021structurallm,
  author = {Li, Chenliang and Bi, Bin and Yan, Ming and Wang, Wei and Huang, Songfang and Huang, Fei and Si, Luo},
  title = {StructuralLM: Structural Pre-training for Form Understanding},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.11210},
}

@article{Ahn2022saycan,
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  title = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
  journal = {arXiv preprint arxiv:2204.01691},
  year = {2022},
}

@inproceedings{humphreys2022data,
  author = {Humphreys, Peter C and Raposo, David and Pohlen, Toby and Thornton, Gregory and Chhaparia, Rachita and Muldal, Alistair and Abramson, Josh and Georgiev, Petko and Goldin, Alex and Santoro, Adam and Lillicrap, Timothy},
  title = {A data-driven approach for learning to control computers},
  booktitle={International Conference on Machine Learning},
  year = {2022},
}

@article{yao2022webshop,
  title = {WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
  author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal = {arXiv preprint arxiv:2207.01206},
  year = {2022},
}

@inproceedings{liu2018wge,
    title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},
    author={Evan Zheran Liu and Kelvin Guu and Panupong Pasupat and Percy Liang},
    booktitle={International Conference on Learning Representations},
    year={2018},
}

@article{li2021markuplm,
  author = {Li, Junlong and Xu, Yiheng and Cui, Lei and Wei, Furu},
  title = {MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding},
  year = {2021},
  journal = {arXiv preprint arxiv:2110.08518},
}

@inproceedings{shi2017miniwob,
  title = 	 {World of Bits: An Open-Domain Platform for Web-Based Agents},
  author =       {Tianlin Shi and Andrej Karpathy and Linxi Fan and Jonathan Hernandez and Percy Liang},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@article{xu2019layout,
	year = 2019,
	author = {Yiheng Xu and Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou},
	title = {{LayoutLM}: Pre-training of Text and Layout for Document Image Understanding},
	journal = {arXiv preprint arxiv:1912.13318},
}

@inproceedings{gur2018learning,
    title={Learning to Navigate the Web},
    author={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},
    booktitle={International Conference on Learning Representations},
    year={2019},
}

@inproceedings{gur2021gminiwob,
  author = {Gur, Izzeddin and Jaques, Natasha and Miao, Yingjie and Choi, Jongwook and Tiwari, Manoj and Lee, Honglak and Faust, Aleksandra},
  title = {Environment Generation for Zero-Shot Compositional Reinforcement Learning},
  year = {2021},
  booktitle={Advances in neural information processing systems},
}

@article{aghajanyan2021htlm,
  author = {Aghajanyan, Armen and Okhonko, Dmytro and Lewis, Mike and Joshi, Mandar and Xu, Hu and Ghosh, Gargi and Zettlemoyer, Luke},
  title = {HTLM: Hyper-Text Pre-Training and Prompting of Language Models},
  journal = {arXiv preprint arXiv:2107.06955},
  year = {2021},
}

@inproceedings{jia2018domqnet,
    title={{DOM}-Q-{NET}:  Grounded {RL} on Structured Language},
    author={Sheng Jia and Jamie Ryan Kiros and Jimmy Ba},
    booktitle={International Conference on Learning Representations},
    year={2019},
}

@article{toyama2021androidenv,
  author = {Toyama, Daniel and Hamel, Philippe and Gergely, Anita and Comanici, Gheorghe and Glaese, Amelia and Ahmed, Zafarali and Jackson, Tyler and Mourad, Shibl and Precup, Doina},
  title = {AndroidEnv: A Reinforcement Learning Platform for Android},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.13231},
}

@inproceedings{shvoEtAl2021appbuddy,
  title={AppBuddy: Learning to Accomplish Tasks in Mobile Apps via Reinforcement Learning},
  author={Maayan Shvo and
               Zhiming Hu and
               Rodrigo Toro Icarte and
               Iqbal Mohomed and
               Allan D. Jepson and
               Sheila A. McIlraith},
  booktitle={Canadian Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{li2020mapping,
  author = {Li, Yang and He, Jiacong and Zhou, Xin and Zhang, Yuan and Baldridge, Jason},
  title = {Mapping Natural Language Instructions to Mobile UI Action Sequences},
  year = {2020},
  booktitle = {Annual Conference of the Association for Computational Linguistics},
}

@article{jaques2019way,
  author = {Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  title = {Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog},
  publisher = {arXiv},
  year = {2019},
  journal = {arXiv preprint arXiv:1907.00456},
}

@article{he2021actionbert,
  author = {He, Zecheng and Sunkara, Srinivas and Zang, Xiaoxue and Xu, Ying and Liu, Lijuan and Wichers, Nevan and Schubiner, Gabriel and Lee, Ruby and Chen, Jindong and Arcas, Blaise Agüera y},
  title = {ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces},
  year = {2020},
  journal = {arXiv preprint arXiv:2012.12350},
}

@article{mazumder2020flin,
  author = {Mazumder, Sahisnu and Riva, Oriana},
  title = {FLIN: A Flexible Natural Language Interface for Web Navigation},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.12844},
}

@inproceedings{nogueira2016end,
  title={End-to-End Goal-Driven Web Navigation},
  author={Nogueira, Rodrigo and Cho, Kyunghyun},
  booktitle={Advances In Neural Information Processing Systems},
  year={2016}
}

@inproceedings{nogueira2013user,
  title={User-Driven Automation of Web Form Filling},
  author={Diaz, Oscar and Otaduy, Itziar and Puente, Gorka},
  booktitle={International Conference on Web Engineering},
  year={2013}
}

@article{wang2022webformer,
  author = {Wang, Qifan and Fang, Yi and Ravula, Anirudh and Feng, Fuli and Quan, Xiaojun and Liu, Dongfang},
  title = {WebFormer: The Web-page Transformer for Structure Information Extraction},
  year = {2022},
  journal={arXiv preprint arXiv:2202.00217},
}

@inproceedings{li2021selfdoc,
  author = {Li, Peizhao and Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I. and Zhao, Handong and Jain, Rajiv and Manjunatha, Varun and Liu, Hongfu},
  title = {SelfDoc: Self-Supervised Document Representation Learning},
  year = {2021},
  booktitle = {Conference on Computer Vision and Pattern Recognition},
}


@article{lewis2019bart,
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  year = {2019},
  journal={arXiv preprint arXiv:1910.13461},
}

@inproceedings{wang2021codet5,
    title = "{C}ode{T}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
    author = "Wang, Yue  and Wang, Weishi  and Joty, Shafiq  and Hoi, Steven C.H.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "8696--8708",
}

@inproceedings{chen2021websrc,
    title = "{W}eb{SRC}: A Dataset for Web-Based Structural Reading Comprehension",
    author = "Chen, Xingyu  and Zhao, Zihan  and Chen, Lu  and Ji, JiaBao  and Zhang, Danyang  and Luo, Ao  and Xiong, Yuxuan  and Yu, Kai",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "4173--4185",
}

@article{deng2022domlm,
      title={DOM-LM: Learning Generalizable Representations for HTML Documents}, 
      author={Xiang Deng and Prashant Shiralkar and Colin Lockard and Binxuan Huang and Huan Sun},
      year={2022},
      journal={arXiv preprint arXiv:2201.10608},
}

@inproceedings{zhao2022tie,
    title = "{TIE}: Topological Information Enhanced Structural Reading Comprehension on Web Pages",
    author = "Zhao, Zihan  and Chen, Lu  and Cao, Ruisheng  and Xu, Hongshen  and Chen, Xingyu  and Yu, Kai",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2022",
    pages = "1808--1821",
}

@article{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      journal={arXiv preprint arXiv:2303.08774},
}

@article{aghajanyan2022cm3,
  author = {Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke},
  title = {CM3: A Causal Masked Multimodal Model of the Internet},
  year = {2022},
  journal={arXiv preprint arXiv:2201.07520},
}

@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
}


@article{suzgun2022bbh,
  author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  year = {2022},
  journal={arXiv preprint arXiv:2210.09261},
}

@article{liang2023code,
      title={Code as Policies: Language Model Programs for Embodied Control}, 
      author={Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
      year={2023},
      journal={arXiv preprint arXiv:2209.07753},
}

@article{meta2022diplomacy,
    author = {{Meta Fundamental AI Research Diplomacy Team} and Anton Bakhtin  and Noam Brown  and Emily Dinan  and Gabriele Farina  and Colin Flaherty  and Daniel Fried  and Andrew Goff  and Jonathan Gray  and Hengyuan Hu  and Athul Paul Jacob  and Mojtaba Komeili  and Karthik Konath  and Minae Kwon  and Adam Lerer  and Mike Lewis  and Alexander H. Miller  and Sasha Mitts  and Adithya Renduchintala  and Stephen Roller  and Dirk Rowe  and Weiyan Shi  and Joe Spisak  and Alexander Wei  and David Wu  and Hugh Zhang  and Markus Zijlstra },
    title = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
    journal = {Science},
    volume = {378},
    number = {6624},
    pages = {1067-1074},
    year = {2022},
}

@article{schick2023toolformer,
      title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
      author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
      year={2023},
      journal={arXiv preprint arXiv:2302.04761},
}

@article{gao2023pal,
      title={PAL: Program-aided Language Models}, 
      author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
      year={2023},
      journal={arXiv preprint arXiv:2211.10435},
}

@article{parisi2022talm,
      title={TALM: Tool Augmented Language Models}, 
      author={Aaron Parisi and Yao Zhao and Noah Fiedel},
      year={2022},
      journal={arXiv preprint arXiv:2205.12255},
}

@article{anil2023palm2,
      title={PaLM 2 Technical Report}, 
      author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Clément Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark Díaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
      year={2023},
      journal={arXiv preprint arXiv:2305.10403},
}

@article{ainslie2020etc,
      title={ETC: Encoding Long and Structured Inputs in Transformers}, 
      author={Joshua Ainslie and Santiago Ontanon and Chris Alberti and Vaclav Cvicek and Zachary Fisher and Philip Pham and Anirudh Ravula and Sumit Sanghai and Qifan Wang and Li Yang},
      year={2020},
      journal={arXiv preprint arXiv:2004.08483},
}

@inproceedings{adolphs2022,
  author = {Adolphs, Leonard and Boerschinger, Benjamin and Buck, Christian and Huebscher, Michelle Chen and Ciaramita, Massimiliano and Espeholt, Lasse and Hofmann, Thomas and Kilcher, Yannic and Rothe, Sascha and Sessa, Pier Giuseppe and Saralegui, Lierni Sestorain},
  title = {Boosting Search Engines with Interactive Agents},
  year = {2022},
  booktitle={Transactions on Machine Learning Research},
}

@inproceedings{appalaraju2021docformer,
    author    = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R.},
    title     = {DocFormer: End-to-End Transformer for Document Understanding},
    booktitle = {International Conference on Computer Vision},
    year      = {2021},
}

@article{feng2020codebert,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      journal={arXiv preprint arXiv:2002.08155},
}

@article{lu2021codexglue,
      title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}, 
      author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
      year={2021},
      journal={arXiv preprint arXiv:2102.04664},
}

@article{thoppilan2022lamda,
      title={LaMDA: Language Models for Dialog Applications}, 
      author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
      year={2022},
      journal={arXiv preprint arXiv:2201.08239},
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    pages = "311--318",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    publisher = "Association for Computational Linguistics",
    pages = "74--81",
}

@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and Al-Rfou, Rami  and Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    pages = "3045--3059",
}

@article{xu2023small,
      title={Small Models are Valuable Plug-ins for Large Language Models}, 
      author={Canwen Xu and Yichong Xu and Shuohang Wang and Yang Liu and Chenguang Zhu and Julian McAuley},
      year={2023},
      journal={arXiv preprint arXiv:2305.08848},
}

@article{zeng2022socratic,
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  title = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  year = {2022},
  journal = {arXiv preprint arXiv:2204.00598},
}

@article{hendrycks2021apps,
      title={Measuring Coding Challenge Competence With APPS}, 
      author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
      year={2021},
      journal = {arXiv preprint arXiv:2105.09938},
}

@article{singh2022progprompt,
  title={{ProgPrompt}: Generating Situated Robot Task Plans using Large Language Models}, 
  author={Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and Dieter Fox and Jesse Thomason and Animesh Garg},
  year={2022},
  journal = {arXiv preprint arXiv:2209.11302},
}

@article{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  journal={arXiv preprint arXiv:2201.07207},
  year={2022}
}


@inproceedings{wang2023describe,
      title={Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents}, 
      author={Zihao Wang and Shaofei Cai and Anji Liu and Xiaojian Ma and Yitao Liang},
      year={2023},
      booktitle={International Conference on Machine Learning},
}

@inproceedings{nottingham2023embodied,
      title={Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling}, 
      author={Kolby Nottingham and Prithviraj Ammanabrolu and Alane Suhr and Yejin Choi and Hannaneh Hajishirzi and Sameer Singh and Roy Fox},
      year={2023},
      booktitle={International Conference on Machine Learning},
}

@article{liu2023llmp,
      title={LLM+P: Empowering Large Language Models with Optimal Planning Proficiency}, 
      author={Bo Liu and Yuqian Jiang and Xiaohan Zhang and Qiang Liu and Shiqi Zhang and Joydeep Biswas and Peter Stone},
      year={2023},
      journal={arXiv preprint arXiv:2304.11477},
}

@article{silver2023generalized,
      title={Generalized Planning in PDDL Domains with Pretrained Large Language Models}, 
      author={Tom Silver and Soham Dan and Kavitha Srinivas and Joshua B. Tenenbaum and Leslie Pack Kaelbling and Michael Katz},
      year={2023},
      journal={arXiv preprint arXiv:2305.11014},
}

@article{valmeekam2023large,
      title={Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)}, 
      author={Karthik Valmeekam and Alberto Olmo and Sarath Sreedharan and Subbarao Kambhampati},
      year={2023},
      journal={arXiv preprint arXiv:2206.10498},
}


@article{geva2021did,
      title={Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies}, 
      author={Mor Geva and Daniel Khashabi and Elad Segal and Tushar Khot and Dan Roth and Jonathan Berant},
      year={2021},
      journal={arXiv preprint arXiv:2101.02235},
}

@article{patel2021nlp,
      title={Are NLP Models really able to Solve Simple Math Word Problems?}, 
      author={Arkil Patel and Satwik Bhattamishra and Navin Goyal},
      year={2021},
      journal={arXiv preprint arXiv:2103.07191},
}

@article{miao2021diverse,
      title={A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers}, 
      author={Shen-Yun Miao and Chao-Chun Liang and Keh-Yih Su},
      year={2021},
      journal={arXiv preprint arXiv:2106.15772},
}

@article{talmor2019commonsenseqa,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      journal={arXiv preprint arXiv:1811.00937},
}

@article{cohan2018discourseaware,
      title={A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents}, 
      author={Arman Cohan and Franck Dernoncourt and Doo Soon Kim and Trung Bui and Seokhwan Kim and Walter Chang and Nazli Goharian},
      year={2018},
      journal={arXiv preprint arXiv:1804.05685},
}

@article{sharma2019bigpatent,
      title={BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization}, 
      author={Eva Sharma and Chen Li and Lu Wang},
      year={2019},
      journal={arXiv preprint arXiv:1906.03741},
}

@article{fabbri2019multinews,
      title={Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model}, 
      author={Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},
      year={2019},
      journal={arXiv preprint arXiv:1906.01749},
}

@article{zhu2021mediasum,
  title={MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization},
  author={Zhu, Chenguang and Liu, Yang and Mei, Jie and Zeng, Michael},
  journal={arXiv preprint arXiv:2103.06410},
  year={2021}
}

@article{nallapati2016summarunner,
      title={SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents}, 
      author={Ramesh Nallapati and Feifei Zhai and Bowen Zhou},
      year={2016},
      journal={arXiv preprint arXiv:1611.04230},
}

@article{deng2023mind2web,
  title={Mind2Web: Towards a Generalist Agent for the Web},
  author={Xiang Deng and Yu Gu and Boyuan Zheng and Shijie Chen and Samuel Stevens and Boshi Wang and Huan Sun and Yu Su},
  year={2023},
  journal={arXiv preprint arXiv:2306.06070},
}

@article{sun2023adaplanner,
      title={AdaPlanner: Adaptive Planning from Feedback with Language Models}, 
      author={Haotian Sun and Yuchen Zhuang and Lingkai Kong and Bo Dai and Chao Zhang},
      year={2023},
      journal={arXiv preprint arXiv:2305.16653},
}

@article{zheng2023synapse,
      title={Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control}, 
      author={Longtao Zheng and Rundong Wang and Bo An},
      year={2023},
      journal={arXiv preprint arXiv:2306.07863},
}

@article{wang2023voyager,
      title={Voyager: An Open-Ended Embodied Agent with Large Language Models}, 
      author={Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
      year={2023},
      journal={arXiv preprint arXiv:2305.16291},
}

@article{shaw2023pixels,
      title={From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces}, 
      author={Peter Shaw and Mandar Joshi and James Cohan and Jonathan Berant and Panupong Pasupat and Hexiang Hu and Urvashi Khandelwal and Kenton Lee and Kristina Toutanova},
      year={2023},
      journal={arXiv preprint arXiv:2306.00245},
}

@article{trivedi2022learning,
      title={Learning to Synthesize Programs as Interpretable and Generalizable Policies}, 
      author={Dweep Trivedi and Jesse Zhang and Shao-Hua Sun and Joseph J. Lim},
      year={2022},
      journal={arXiv preprint arXiv:2108.13643},
}

@inproceedings{ni2023lever,
  title={Lever: Learning to verify language-to-code generation with execution},
  author={Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Ves and Yih, Wen-tau and Wang, Sida I and Lin, Xi Victoria},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
}
