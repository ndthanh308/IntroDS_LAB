
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% extra packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{bbding}
\usepackage[font=small]{caption}
% \usepackage{caption}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
%\usepackage{minted}
%\usepackage{mdframed}
\usepackage{multirow}
\usepackage{lscape}



% colors for color-blindness
\definecolor{cb_orange}{RGB}{213,94,0}
% \definecolor{cb_green}{RGB}{0,158,115}
\definecolor{cb_green}{RGB}{34,136,51}
\definecolor{sky_blue}{RGB}{204, 238, 255}
\definecolor{cb_purple}{RGB}{170, 51, 119}
\definecolor{cb_red}{RGB}{204, 51, 17}
\definecolor{cb_blue}{RGB}{0, 119, 187}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    % basicstyle=\ttfamily\footnotesize,
    basicstyle=\ttfamily\tiny,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% \usepackage{hyperref}
% \usepackage[colorlinks=true,citecolor=magenta,linkcolor=mydarkblue,urlcolor=mydarkblue]{hyperref}
\usepackage[colorlinks=true,citecolor=brown,linkcolor=mydarkblue,urlcolor=mydarkblue]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}


% commands
\newcommand{\todo}[1]{{\color{red}{TODO: #1}}}
\newcommand{\sandra}[1]{\textcolor{purple}{[Sandra: #1]}}

\newcommand{\housing}{\texttt{real-estate}}
\newcommand{\housingweb}{real estate website}
\newcommand{\socialmedia}{\texttt{social-media}}
\newcommand{\socialmediaweb}{social media website}

\newcommand{\yes}{\textcolor{cb_green}{\tiny \CheckmarkBold}}
\newcommand{\no}{\textcolor{cb_red}{\tiny \XSolidBrush}}
\newcommand{\lyes}{\textcolor{cb_green}{\small \CheckmarkBold}}
\newcommand{\lno}{\textcolor{cb_red}{\small \XSolidBrush}}


\title{A Real-World WebAgent with Planning, \\ Long Context Understanding, and \\ Program Synthesis}

\iclrfinalcopy

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{
  Izzeddin Gur$^{1*}$~
  Hiroki Furuta$^{1,2*\text{†}}$~
  Austin Huang$^{1}$~
  \textbf{Mustafa Safdari}$^{1}$~
  \textbf{Yutaka Matsuo}$^{2}$~ \\ %\\
  ~\textbf{Douglas Eck}$^{1}$~
  \textbf{Aleksandra Faust}$^{1}$ \\ %\\
  $^{1}$Google DeepMind, $^{2}$The University of Tokyo \quad \\
  \texttt{izzeddin@google.com, furuta@weblab.t.u-tokyo.ac.jp} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.


% There will be a strict upper limit of 9 pages for the main text of the submission, with unlimited additional pages for citations. This page limit applies to both the initial and final camera ready version.
% Authors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are not required to read the appendix.

\begin{document}
\maketitle
\begingroup\def\thefootnote{*}\footnotetext{Equal Contribution.}\addtocounter{footnote}{0}\endgroup
\begingroup\def\thefootnote{†}\footnotetext{Work done as Student Researcher at Google.}\addtocounter{footnote}{0}\endgroup


\begin{abstract}
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.
However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.
We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.
We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.
We empirically demonstrate that our modular recipe improves the success on real websites by over 50\%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7\% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.
\end{abstract}


\section{Introduction}
Large language models (LLM)~\citep{brown2020language,Chowdhery2022palm,openai2023gpt4} can solve variety of natural language tasks, such as arithmetic, commonsense, logical reasoning, question answering, text generation~\citep{brown2020language,kojima2022lets,wei2022cot}, and even interactive decision making tasks~\citep{Ahn2022saycan,yao2022react}.
Recently, LLMs have also demonstrated success in autonomous web navigation, where the agents control computers or browse the internet to satisfy the given natural language instructions through the sequence of computer actions, by leveraging the capability of HTML comprehension and multi-step reasoning~\citep{furuta2023mmwebnav,gur2022html,kim2023language}.

However, web automation on real-world websites has still suffered from (1) the lack of pre-defined action space, (2) much longer HTML observations than simulators, and (3) the absence of domain knowledge for HTML in LLMs (\autoref{fig:real_sim_loop}).
Considering the open-ended real-world websites and the complexity of instructions, defining appropriate action space in advance is challenging.
In addition, although several works have argued that recent LLMs with instruction-finetuning or reinforcement learning from human feedback improve HTML understanding and web automation accuracy~\citep{furuta2023mmwebnav,kim2023language}, their architectures are not always suitable to process real-world HTML documents;
as presented in \autoref{fig:real_html_tokens}, HTML tokens of real websites are much longer than those of simulators, and most LLMs have shorter context lengths than the average HTML tokens in real websites.
It is prohibitively costly to treat such long documents as inputs directly, and even to adopt prior techniques for structured documents, such as text-XPath alignment~\citep{li2021markuplm} or text-HTML token separation~\citep{wang2022webformer}.
To prioritize broad task generalization and model-size scaling, such domain knowledge for HTML codes is not applied in recent LLMs.

\input{tables_iclr/real_sim_loop}

\input{tables_iclr/real_html_tokens}



In this work, we introduce WebAgent, an LLM-driven autonomous agent that learns from self-experience to complete user instructions on real websites by combining canonical web actions in a program space~(\autoref{fig:webagent}).
WebAgent (i) \textbf{plans sub-instructions per step} by decomposing natural language instructions, (ii) \textbf{summarizes long HTML pages into task-relevant snippets} based on sub-instructions, and (iii) \textbf{acts via programming} on real websites by grounding sub-instruction and HTML snippet into executable Python codes.
We combine two LLMs to form WebAgent: Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5} for grounded code generation, and newly introduced HTML-T5, a domain-expert pre-trained language model, for task planning and conditional HTML summarization.
HTML-T5 has an encoder-decoder architecture and is specialized to capture the structure -- syntax and semantics -- of long HTML pages better by adopting local and global attention encoder~\citep{guo2022longt5}.
It is self-supervisedly pre-trained with a \textit{mixture of long-span denoising} objectives~\citep{tay2022ul2} on a large-scale HTML corpus from CommonCrawl.
To ground language model agents into real websites, we introduce \textit{self-experience supervision}, where the domain-expert language models are finetuned with self-generated demonstrations.

% \input{tables/webagent_figure}

Existing LLM-driven agents often solve decision making tasks with a single LLM conditioned on different prompts per role~\citep{kim2023language,sun2023adaplanner,zheng2023synapse}, which is, however, not enough for real-world tasks whose complexity is higher than that of simulators.
The empirical evaluations reveal that our method incorporating self-bootstrapped specialist language models improves HTML understanding and grounding, and achieves better generalization than single LLM agent. In real-world web automation, WebAgent significantly increases the success rate by 50\%, and error analysis emphasizes that coupling task planning with HTML summarization in specialized language models is essential for task success.
Moreover, HTML-T5 not only works as a core module for WebAgent but also achieves strong results by itself on the web-based tasks.
On MiniWoB++~\citep{liu2018wge,shi2017miniwob}, HTML-T5 achieves 18.7\% higher success than previous language model agent~\citep{gur2022html} while also outperforming competitive baselines, such as naive local-global attention models~\citep{guo2022longt5} and its instruction-finetuned ones~\citep{chung2022flant5}.
On Mind2Web~\citep{deng2023mind2web}, an offline task planning dataset, HTML-T5 achieves SoTA performances among MindAct with FLan-T5-XL and GPT-4~\citep{openai2023gpt4}.
In summary, our key contributions are:
\begin{itemize}[leftmargin=0.5cm,topsep=0pt,itemsep=0.0pt]
    \item We propose WebAgent, integration of two modular LLMs under self-supervision for real-world web automation. The domain-expert language model handles planning and HTML summarization, and a generalist language model generates executable programs.
    \item We newly introduce HTML-T5, pre-trained language models with local-global attentions and a mixture of long-span denoising on large-scale HTML corpus, which capture the syntax and semantics of HTML better.
    \item WebAgent notably improves the success rate by over 50\% in real websites. HTML-T5 itself outperforms prior language model agent by 18.7\% in MiniWoB++, and realizes SoTA performance in Mind2Web while surpassing GPT-4.
\end{itemize}


\input{tables_iclr/webagent_figure}

\section{Related Works}
\textbf{Web Automation}~
Web automation is a sequential decision making task where agents manipulate browsers following given instructions~\citep{shi2017miniwob}, such as form filling~\citep{nogueira2013user} or information retrieval~\citep{adolphs2022} through the sequence of computer actions~\citep{li2020mapping,mazumder2020flin,shvoEtAl2021appbuddy}.
Prior works have realized the web automation via reinforcement learning~\citep{gur2018learning,humphreys2022data,jia2018domqnet,shaw2023pixels}, finetuned~\citep{furuta2023mmwebnav,gur2022html} or prompted LLMs~\citep{kim2023language,sun2023adaplanner,yao2022react,zheng2023synapse} on the simulated websites~\citep{shi2017miniwob,toyama2021androidenv,yao2022webshop}.
However, there are still huge gaps between simplified simulators and real web environments; for instance, the average tokens for HTML pages are about 15 times larger (\autoref{fig:real_html_tokens}), and pre-defined action space for specific websites is a strong assumption that may harm the generalization to out-of-distribution web pages or instructions.

MindAct~\citep{deng2023mind2web} could be the most relevant work, where finetuned language model summarizes the raw HTML document into task-relevant snippets, and another model predicts the web actions in a multi-choice QA format.
While MindAct also combines several language models, it has just adopted DeBERTa~\citep{he2021deberta} and Flan-T5~\citep{chung2022flant5} for summarization and actor modules, and evaluated it on the offline dataset.
In contrast, we design HTML-T5, specialized for web-based tasks, to handle long HTML documents. WebAgent leverages HTML-T5 finetuned with self-experience for summarization and planning, and Flan-U-PaLM as a capable programmer, which enables it to generate open-ended web actions and to act on online real-world websites.


\textbf{Program Synthesis}~
In addition to common LLMs~\citep{brown2020language,Chowdhery2022palm,touvron2023llama}, several works have proposed programming-focused language models~\citep{chen2021evaluating,feng2020codebert,li2022alphacode,wang2021codet5} and their benchmarks~\citep{austin2021program,hendrycks2021apps,lu2021codexglue}.
Another line of work has investigated the tool augmentation of LLMs~\citep{parisi2022talm} by decoding API calls~\citep{schick2023toolformer} or Python snippets to be parsed with the interpreter~\citep{gao2023pal}.
Most works deal with the program synthesis on the static dataset, except for the attempts in robotics~\citep{liang2023code} and game~\citep{trivedi2022learning,wang2023voyager}, where LLMs output Python or JavaScript snippets to command the agents.
Similarly, we leverage the ability of code generation as an open-ended action space for web-based agents to manipulate the real website, and demonstrate LLMs can sequentially decode Python selenium codes considering the given sub-instructions and HTML in the prompts.

See extended related works on document understanding and LLM for task planning in \autoref{sec:extended_related_work}.


\input{tables_iclr/html_t5_figure}


\section{WebAgent}
WebAgent is composed of interactions between HTML-T5, a domain-expert language model, which predicts the sub-instruction for the next-step program and conditionally summarizes long HTML documents, and Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5}, an instruction-finetuned LLM for grounded program synthesis (\autoref{fig:webagent}).
In contrast to a single LLM conditioned on different prompts per role, such a modular approach can deal with real-world tasks better.
Moreover, to align WebAgent with real websites, we introduce self-experience supervision to ground the agent into real-world tasks. 
We describe the details of each component in the following sections, and provide the example workflow in \autoref{sec:webagent_example_flow}.


\subsection{HTML-T5}
\label{sec:methods}
Previous works demonstrate that \textit{generalist} LLMs, such as T5~\citep{2020t5}, Flan-T5~\citep{chung2022flant5}, and InstructGPT~\citep{ouyang2022instructgpt}, have a capability of manipulating the web environments~\citep{shi2017miniwob} with great HTML comprehension~\citep{furuta2023mmwebnav,gur2022html,kim2023language}.
However, they have not fully leveraged the HTML-specific inductive bias on syntax and semantics considered in the prior \textit{specialist} transformer models~\citep{li2021markuplm,wang2022webformer,zhao2022tie}.
We here introduce HTML-T5, a pre-trained encoder-decoder language model, by interpolating the generalist and specialist nature of language models to solve downstream HTML-based web automation tasks efficiently.
HTML-T5 processes HTML documents in a text-to-text manner, and leverages local and global attentions~\citep{ainslie2020etc,guo2022longt5} in the encoder to handle the hierarchical structure of long HTML inputs.
We pre-train it with large-scale HTML corpus curated from CommonCrawl on a mixture of long-span denoising objectives~\citep{2020t5,tay2022ul2}, and finetune it for each downstream task.
Especially, for WebAgent, we employ self-experience supervision to align the model with real websites.


\textbf{Model Architecture}~
In contrast to natural language texts, HTML documents have an explicit hierarchy from the tree structure; the relation of each element (e.g. \texttt{<input>}, \texttt{<label>}, \texttt{<button>}) and its attributes (e.g. \texttt{class}, \texttt{label}, \texttt{id}) are often defined locally, and those are iteratively integrated globally (e.g. \texttt{<body>}, \texttt{<form>}, \texttt{<div>}).
To capture such a hierarchical structure of HTML, we adopt local and global attention mechanisms \citep{guo2022longt5}, instead of common dense attention~\citep{2020t5,vaswani2017attention}.
Local attention restricts each token to only attend to neighboring tokens to the left and right. Transient global attention allows each input token to attend to beyond nearby tokens, by dividing the input sequence into blocks of tokens and computing global tokens with summation and normalization of the embeddings of every token in the block.
\autoref{fig:html_t5} describes the concepts of HTML-T5; leaf elements in HTML (\textcolor{cb_green}{green}) could be processed by local attention, and internal elements (\textcolor{cb_purple}{purple}) could be compressed into transient global attention, which naturally fit the hierarchical syntax of HTML documents.
% We also note that the elements in HTML are not always captured clearly in the attention head.
We leverage the implementation of LongT5~\citep{guo2022longt5} as base architectures using dense attention in the decoder.


\input{tables_iclr/real_world_webnav}


\textbf{Pre-Training with Mixture of Long-Span Denoising}~
The performance of language models in downstream tasks highly depends on the knowledge learned in pre-training. To incorporate further inductive bias on HTML into scalable language models, we perform self-supervised pre-training with large-scale HTML corpus.
We here employ span denoising objective, where we mask the input texts with random spans of tokens (following normal distributions with mean span length $\mu$), and the models take all other tokens from the documents as inputs to predict corrupted spans~\citep{ainslie2023colt5,2020t5,tay2022ul2}.
To deal with the sparsity of contents tokens in HTML documents, we introduce a \textit{mixture of long-span denoising} objective, by masking input tokens with longer mean span lengths than popular value for natural language (e.g. $\mu=3$). Such a shorter mean span length only masks less meaningful chunks, such as \texttt{</}, \texttt{id=}, or \texttt{">}~(\autoref{fig:html_t5}), which might not be helpful for LLMs to capture the syntax and semantics of HTML. In contrast, longer span can contain more semantically meaningful chunks, such as \texttt{<form class="} or \texttt{type="submit">}. We empirically find $\mu\in\{8, 64\}$ is the optimal mixture (Section~\ref{sec:htmlt5_ablations}).

We adopt 4096 input sequence length and 910 output sequence length during the denoising pre-training. In total, 15\% of input tokens are randomly masked.
For the dataset, we prepare 100 WARC files (April 2019) from CommonCrawl, and pre-process the raw HTML by removing non-Unicode and alphanumeric documents and extracting subtrees around \texttt{<label>} elements that have \texttt{for} attribute, to reduce the noise in training corpus, which results in about 3.41M examples. % (\autoref{tab:cc_html_stats}).
We train the models with 100K iterations following other pre-training strategies for T5 families~\citep{chung2022flant5,lester-etal-2021-power}.
See \autoref{sec:implementation} for further details.


% \subsection{Self-Supervised Experience Distillation}
\subsection{Alignment with Self-Experience Supervision}
Another bottleneck for building real-world web automation agents is collecting demonstrations to align LLM with real websites.
Humans could perform instruction following on real websites easily, but it is infeasible to manually annotate all the instruction decomposition, snippet extractions, and executable programs.
To reduce such a burden, we introduce a \textit{self-experience supervision}, where the language model agents learn from the experience that they themselves face on real websites with minimal human intervention. We first prepare the templates of instructions. The scripted agents procedurally parse instructions into the sequence of sub-instructions, regular-expression-based retrieval specifies the elements to be summarized, and conditioned on those, Flan-U-PaLM executes web actions via program synthesis.
The generated demonstrations following the steps above may result in success and failure, but the success criteria for real-world tasks is hard to automate. Instead, to filter the experience, we leverage the environmental feedback that can remove critical failures; for instance, the program execution errors, retriever errors, and clearly wrong prefix of URL~\citep{ni2023lever}.
Our WebAgent aligns domain-expert language models, HTML-T5, with those self-collected real-world experiences via finetuning~\citep{selfinstruct}.
This self-supervision process realizes the generalization and alignment of language model agents to challenging real-world tasks.


\input{tables_iclr/real_world_examples_small}


\textbf{Finetuning for Planning and Summarization}~
We align language models to perform closed-loop planning with a sequence of sub-instructions and to summarize long HTML documents into concise snippets relevant to the current plan. 
As a core module of WebAgent, HTML-T5 finetuned with self-generated demonstrations takes task instructions (e.g. \textit{please search 2 bedroom and 2+ bathroom houses in new york, ny with a max price of \$7500 on \housingweb{}}), sub-instruction histories (e.g. \textit{go to \housingweb{}}, \textit{type in new york, ny into search}, \textit{click on search}, \textit{click on price}, \textit{click on max rent}), and raw HTML as inputs.
Then, it predicts the next sub-instruction (e.g. \textit{type in 7500 into max rent}) and the corresponding \texttt{data-ref} attributes to extract the snippet with XPath instead of naively decoding the raw snippet.
In the later experiments in Section~\ref{sec:realworld_webnav}, we will demonstrate that linking HTML summarization into sub-instruction prediction is important for real-world web automation performance.


\subsection{Grounded Program Synthesis}
Web automation on real-world websites suffers from the open-ended action space, compared to the simplified simulators~\citep{shi2017miniwob,yao2022webshop}. Unlike previous works~\citep{gur2018learning,humphreys2022data,jia2018domqnet,liu2018wge}, real-world web agents could not pre-define a categorical action space to specify which elements on the websites they should interact.
To overcome such an open-domainness, we introduce \textit{act via programming} paradigm in web automation by leveraging the capability of LLMs on conditional code generation~\citep{chen2021evaluating,liang2023code}.
Given a few canonical examples for program generation, next sub-instruction, and extracted HTML snippet from HTML-T5, Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5} with 540B parameters decodes an executable Python program (\autoref{fig:webagent}) using Selenium WebDriver, a library for browser automation.
Such a conditional program synthesis demands that LLMs are capable enough to not only generate the code following natural language instructions, but also understand the semantics and functionality of HTML elements.
We provide several Python snippet examples generated by Flan-U-PaLM as follows (we treat sub-instructions as comments in the script):


\begin{lstlisting}[language=Python]
# Type in walnut creek, ca into search
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').clear()
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').send_keys("walnut creek, ca")

# Submit the search
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').submit()

# Click on the apartments
driver.find_element(By.CSS_SELECTOR, '[data-ref="572"]').click()

# Scroll down housing type by 200px
driver.execute_script('getScrollParent(document.querySelector("#type-of-housing")).scrollBy({top: 200})')
\end{lstlisting}


\section{Experimental Results}
To study how a modular combination of LLMs under self-supervision enables real-world web automation by overcoming open-endedness and long context documents, we execute instruction-following tasks on real websites (Section~\ref{sec:realworld_webnav}).
In \autoref{sec:websrc}, we also test WebAgent on WebSRC~\citep{chen2021websrc}, a static HTML comprehension benchmark, compared to prior transformer models specialized for structured documents~\citep{li2021markuplm,zhao2022tie}.
In addition, we quantify the performance of HTML-T5 itself on simulated web benchmark, MiniWoB++, and offline task planning benchmark, Mind2Web (Section~\ref{sec:htmlt5_ablations}).


\input{tables_iclr/html_t5_ablation}


\subsection{Real-world Web Automation}
\label{sec:realworld_webnav}
\textbf{Evaluation Methodology}~
We first evaluate WebAgent with the real-world navigation performance under human supervision, at \housingweb{} (a platform for housing), \socialmediaweb{} (a network of communities), and map website.
These three websites have different properties. \housing{} requires long-horizon planning (about 20 steps per episode) for complex form-filling with a few page transitions (at least 2 pages), and \socialmedia{} needs shorter plans (about 10 steps per episode) with many page transitions (at least 4 pages) by selecting appropriate hyperlinks on the page.
\texttt{map} is the easiest domain with shorter plans and a few page transitions.
WebAgent receives natural language instructions (e.g. \textit{Can you search for a studio bedroom, 1+ bathroom apartments in oroville, ca for corporate housing on \housingweb{}?}, or \textit{Could you present the most new thread of Python community filtered by Tutorial tag on \socialmediaweb{}?}), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM (\autoref{fig:real_world_examples_small}).
Through the self-experience supervision process, we curate 260 episodes on \housingweb{},  230 episodes on \socialmediaweb{}, and 410 episodes on map website to finetune HTML-T5.

We prepare 20 different natural language instructions (see \autoref{sec:language_instruction_list} for the full list), and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode~\citep{yao2022webshop}; for instance, (1) \textit{apartments} for (2) \textit{corporate housing} with (3) \textit{studio bedroom} and (4) \textit{1+ bathroom} located in (5) \textit{oroville, ca}, can be specified in the instruction.
When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score is 60 ($ = 100 \times 3/5$).
If the agents achieve 100 score, that episode will mark as success.


\textbf{Results}~
For comparison, we prepare three baselines, consisting of language model modules and a single LLM conditioned on different prompts per role, such as Flan-U-PaLM~\citep{chung2022flant5}, that with a planning language model (Flan-U-PaLM+P), and that with a summarization language model (Flan-U-PaLM+S).
If they do not use language model modules, prompted Flan-U-PaLM plans in an open-loop manner (\textbf{Plan}: \no{}), and regular-expression-based retrieval summarizes given raw HTML (\textbf{Sum}: \no{}).
\autoref{tab:realworld_results} shows that by leveraging planning and summarization language model modules, WebAgent achieves best 65\% success and 87.6\% score on \housing{}, 70\% success and 85.8\% score on \socialmedia{}, and 80\% success and 93.8\% score on \texttt{map}, significantly outperforming single Flan-U-PaLM, or with partial language model modules (most of those achieve about 10 - 30\% success).
This result suggests that self-experience supervision notably improves the performance, and closed-loop planning grounded on HTML observations via finetuned domain language models is more suitable for open-ended web automation than open-loop planning with few-shot LLMs. This trend is remarkable in \housing{} (even Flan-U-PaLM+P achieves 50\% success), where the longer planning horizon is needed to fulfill instructions. We also observe that coupling sub-instruction prediction with HTML summarization in language model modules plays a critical role in task success.
The development of more capable planning modules to decompose the given instructions adaptively and accurately could help WebAgent improve the performance further.


\textbf{Error Analysis}~
We also analyze the reason of failures by categorizing them into programming, planning, and summarization errors (\autoref{tab:realworld_results}). Programming error does not satisfy the given sub-instructions or HTML snippet. Planning error predicts sub-instructions conflicting with user instructions, and summarization error fails to extract the relevant HTML snippets for given sub-instructions.
From the website perspective, the failures on \housing{} concentrate in planning because of its long-horizon nature. \texttt{map} also fails in planning when confusing starting point and destination.
In contrast, \socialmedia{} tends to fail in programming due to the ambiguous sub-instructions or summarization including redundant hyperlinks, which results in transiting wrong pages or clicking unexecutable elements.
From the method perspective, WebAgent often fails in planning by predicting incorrect sub-instructions (for instance, in \texttt{real-estate}, WebAgent generates incorrect plans in 70\% of failure episodes), while other baselines more fail in programming or summarization steps. This observation indicates that, through the self-experience supervision, the ratio of programming and summarization errors has decreased while the fundamental difficulty of planning, which requires consistent and accurate prediction over long horizon without error accumulation, still remains.


\subsection{Ablation of HTML-T5}
\label{sec:htmlt5_ablations}

In addition to the evaluation as WebAgent system, we extensively examine HTML-T5 about (i) the generalization to other websites with Mind2Web dataset~\citep{deng2023mind2web}, (ii) the performance on MiniWoB++, a standard web automation benchmark~\citep{liu2018wge,shi2017miniwob}, and (iii) its architecture and pre-training objective.
We adopt 16K tokens for the context window unless otherwise mentioned.
We also evaluate HTML-T5 on the pre-training dataset and model initialization, offline task planning with self-generated \housing{} traces, and description generation benchmark~\citep{gur2022html} to test HTML understanding on static dataset in \autoref{sec:htmlt5_extensive_ablations}.

\textbf{Mind2Web}~
\input{tables_iclr/mind2web}
Mind2Web~\citep{deng2023mind2web} is an action-annotated real-world dataset with over 2K instructions collected from 137 websites. It provides action prediction tasks that measure the generalization of LLMs across the tasks, websites, and their domains (e.g. travel, shopping).
Conditioned on the top-50 HTML snippet candidates, task instruction, and action history, LLMs should predict the next step action by choosing a target element to interact with in a multi-choice QA format and generating the operation such as click, type, or select option.
We finetune HTML-T5-XL with the training dataset.
The performance is evaluated with element accuracy, operation F1, and step success rate that cares for both element and operation correctness.
\autoref{tab:mind2web} reveals that HTML-T5 significantly outperforms baselines with Flan-T5-XL or GPT-4~\citep{openai2023gpt4} across task/website/domain generalization, which increases element accuracy by 20-30\%, operation F1 by 5-10\%, and step success rate by 20-30\%.
This highlights that HTML-T5 can handle real-world web automation tasks better and shows generalization beyond our real-world evaluation with 3 websites.


\input{tables_iclr/miniwob_sl_results}
\textbf{MiniWoB++}~
We here evaluate HTML-T5 on simulated web environments, MiniWoB++ with 56 tasks by running 100 evaluation episodes per task.
We finetune HTML-T5 with 12K human demonstrations~\citep{liu2018wge}, and compare the average success rate to prior supervised-learned agents~\citep{gur2022html,humphreys2022data}, LongT5, and its instruction-finetuned variants~\citep{chung2022flant5,furuta2023mmwebnav} we prepared~\footnote{We finetune LongT5 models with Flan dataset released by \citet{chung2022flant5}.
As a sanity check, we test them on representative reasoning and summarization tasks (see \autoref{sec:flan_longt5}).
}.
\autoref{tab:miniwob_sl_results} shows that HTML-T5-XL significantly outperforms WebN-T5, the prior best model, by 18.7\%.
Notably, we demonstrate HTML-denoising consistently improves the performance on top of LongT5 in all the model sizes, better than instruction-finetuning introduced in prior work~\citep{furuta2023mmwebnav}. 
Furthermore, we finetune HTML-T5-XL with 347K demonstrations from \citet{furuta2023mmwebnav}, which performs better than 11B-parameter Flan-T5-XXL even with 3B parameters, achieving 85.6\% success.
% Noticably, HTML-T5-XL is the first model to solve the most challenging MiniWoB task, \texttt{book-flight} (99\% success; see \autoref{sec:per_task_miniwob_results}), by only using limited labeled HTML data without any online trials-and-errors.
These prove we successfully incorporate domain knowledge on HTML comprehension for web automation into pre-trained language models.

% \paragraph{Architecture and Objective}
% ~~\textbf{Architecture and Objective:}~
\textbf{Architecture and Objective}~
We hypothesize that local and global attention mechanisms can capture the hierarchical structures of HTML documents better than dense attention.
We compare the web automation performance among 56 MiniWoB++ tasks~\citep{gur2022html}, by finetuning HTML-T5 with public 12K-episode dataset~\citep{liu2018wge}.
We adopt 2048 and 4096 tokens as input length and prepare Base-size architectures.
\autoref{tab:html_t5_ablation} (left) reveals that the combination of local and global attentions achieves the superior success rate by over 18\% compared to the instruction-finetuned dense attentions~\citep{chung2022flant5,2020t5} and local attention only.
Surprisingly, local attention only still surpasses the dense attention by about 9\%, which suggests local relation between elements and attributes in HTML are essential for web tasks.


As for pre-training objective in \autoref{tab:html_t5_ablation} (right), HTML-denoising generally improves the performance on offline task planning on \housingweb{} and MiniWoB. Especially, using only longer span lengths ($\mu\in\{8, 64\}$) outperforms other choices, including the popular configuration in natural language domain ($\mu\in\{3,8,64\}$ + Prefix LM objective), which can reduce the less meaningful prediction from shorter spans (e.g. $\mu=3$), and inject the structural bias of HTML into language models better.
See Appendix~\ref{sec:offline_plan} for further results with model scaling.



\section{Discussion and Limitation}
\textbf{Modular Approach with Specialist Language Models}~
We demonstrate it is beneficial to divide web automation into planning, HTML summarization, and code generation, and to combine domain-expert language models aligned with self-experience data.
Such modular approaches have also been adopted to support the inference of LLMs~\citep{xu2023small}, multimodal tasks~\citep{zeng2022socratic}, and robotics~\citep{Ahn2022saycan}, which, however, might cause additional computational costs and latency.

\textbf{Broad Generalization across the Internet}~
Because open-loop planning with prompted Flan-U-PaLM achieves at most 10 - 30\% success, we have demonstrated that self-experience supervision on real websites is essential for planning modules.
As we demonstrated in Mind2Web, our method could generalize across the internet if we have enough data.
It would be expected to collect demonstrations at scale and align larger domain-expert models with them in future works.

\textbf{Feedback for Program Synthesis}~
We leverage Flan-U-PaLM with 540B parameters, as a capable program synthesis module via few-shot prompting.
Such a large model, however, makes it challenging to reflect the feedback about the errors in generated code, compared to smaller models.
We leave it as future direction to incorporate the feedback for program synthesis into larger language models.


\textbf{Evaluation for Real-world Web Automation}~
Beyond the simulated web environments~\citep{shi2017miniwob,yao2022webshop}, we have exhibited WebAgent can follow given complex and sometimes ambiguous instructions on real estate, social media and map websites.
On the other hand, it is costly to evaluate the performance of autonomous agents in the real world.
Automated evaluation with minimal human intervention would be helpful for the scalable development of real-world web agents.


\section{Conclusion}
\label{sec:conclusion}
We build a system for real-world web automation, combining HTML-T5 for planning and HTML summarization and Flan-U-PaLM for grounded program synthesis.
Our proposed WebAgent achieves around 70-80\% success on real websites via self-experience supervision, outperforming single LLM approach by over 50\%, which suggests dividing the sequence of sub-problems with multiple language models can increase the entire task success.
We also propose a scalable recipe for HTML-specialized language models where we train local and global attention mechanisms with a mixture of long-span denoising objectives to capture the hierarchical structures of HTML documents.
HTML-T5 not only plays an essential role in WebAgent but also can achieve the best results on a variety of HTML-based benchmarks such as Mind2Web and MiniWoB++.
We hope our work contributes to getting us one-step closer to the practical deployment of autonomous web agent systems.



\subsubsection*{Acknowledgments}
We thank Heiga Zen, Yingjie Miao, Yusuke Iwasawa, Joshua Ainslie, Santiago Ontanon, Quoc V. Le, Zoubin Ghahramani, Jeff Dean, Tris Warkentin for the supports and advises on this work. HF was supported by JSPS KAKENHI Grant Number JP22J21582.


% \clearpage
\bibliography{reference}
\bibliographystyle{iclr2024_conference}


\clearpage
\section*{Appendix}
\appendix
\input{iclr2024_conference_appendix}

\end{document}

