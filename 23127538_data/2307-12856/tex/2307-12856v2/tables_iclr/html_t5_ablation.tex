\begin{table*}[t]
\begin{minipage}[c]{0.55\textwidth}
    \begin{center}
    \begin{small}
    %\begin{sc}
    \scalebox{0.835}{
        \begin{tabular}{llrr}
            \toprule
            \textbf{Architectures} & \textbf{Attention Type} & $\bm{L=2048}$ & $\bm{L=4096}$ \\
            \midrule
            Flan-T5-Base & Dense & 34.0\% & 35.3\% \\
            Long-T5-Base & Local & 43.4\% & 44.0\% \\
            Long-T5-Base & Local \& Global & \textbf{53.1}\% & \textbf{53.6}\% \\
            \bottomrule
        \end{tabular}
    }
    %\end{sc}
    \end{small}
    \end{center}
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
    \begin{center}
    \begin{small}
    %\begin{sc}
    \scalebox{0.785}{
        \begin{tabular}{lrr}
            \toprule
            \textbf{Span Length $\bm{\mu}$} &  \housing{} & \textbf{MiniWoB++} \\
            \midrule
            (no HTML-denoising) & 78.07 & 53.8\% \\
            \midrule
            3,8,64,Prefix & 80.56 & 55.2\% \\
            3,8,64 & 80.56 & 55.4\% \\
            \underline{8,64} & \textbf{82.46} & \textbf{57.0}\% \\
            8,32,64 & 82.16 & 55.6\% \\
            8,64,96 & 81.29 & 53.6\% \\
            16,64 & 79.97 & 55.2\% \\
            \bottomrule
        \end{tabular}
    }
    %\end{sc}
    \end{small}
    \end{center}
\end{minipage}
\vskip -0.075in
\caption{
\textbf{(Left)} Architecture comparison on MiniWoB++ 12K dataset~\citep{liu2018wge} with average success rate over 56 tasks. Local and global attention matches to the hierarchical tree structure of HTML, and then improves the success rate by over 18\%, compared to the instruction-finetuned dense attentions~\citep{chung2022flant5,furuta2023mmwebnav}.
\textbf{(Right)} HTML-denoising comparison with different mixtures of span length~\citep{2020t5,tay2022ul2}.
We use LongT5-Base models for pre-training. HTML-denoising generally improves the performance on offline task planning on \housingweb{} and MiniWoB benchmark. Especially, using longer span lengths ($\mu\in\{8,6\}$) outperforms other choices, including the popular configuration in natural language domain ($\mu\in\{3,8,64\}$ + Prefix LM objective), which can reduce the less meaningful prediction from shorter spans (e.g. $\mu=3$), and inject the structural bias of HTML better.
}
\vskip -0.15in
\label{tab:html_t5_ablation}
\end{table*}
