\section{Note for Real-world Evaluation}
The development of autonomous agents should consider the security and safety aspects.
In the real website evaluation, we have carefully conducted the experiments under human supervision in case undesired behaviors happen.
We use Selenium WebDriver~\footnote{\url{https://www.selenium.dev/}}, a popular library for browser automation, and limit the access per second not to stress the server.
We have anonymized the real websites we tested on for safety and privacy concerns.

\section{Extended Related Works}
\label{sec:extended_related_work}
% \paragraph{Document Understanding}
% \textbf{Document Understanding:}~
\textbf{Document Understanding}~
Understanding structural documents has been a practical challenge for transformer-based language models. Prior works employ layout-informed tokens~\citep{xu2019layout} or even multimodal tokens from visual inputs~\citep{appalaraju2021docformer,li2021structurallm,li2021selfdoc}. Especially, for the documents written in markup languages, text-XPath alignment~\citep{li2021markuplm}, token separation between text and HTML~\citep{wang2022webformer}, or extra topological information of HTML~\citep{zhao2022tie} are proposed to leverage their syntax better. On the other hand, such a domain knowledge conflicts with recent generalist and scaling trends around LLMs~\citep{anil2023palm2,openai2023gpt4}. Because web agents require the instruction-conditioned HTML understanding, it also would be desirable to reconcile specialist aspects for HTML documents with generalist capabilities for natural language tasks.
In this work, we design HTML-T5 to incorporate the structural bias of HTML by combining local-global attention for the encoder and a mixture of long-span denoising, while it can solve instruction-following better in downstream web-based tasks.

\textbf{LLM for Task Planning}~
The prior knowledge of commonsense in LLMs has allowed us to leverage them for a variety of task planning.
For instance, \citet{huang2022language} propose LLM agent that generates natural language plans in an open-loop manner.
\citet{nottingham2023embodied} and \citet{wang2023describe} perform sequential closed-loop planning on MineCraft.
\citet{singh2022progprompt} decode robotic plans with pythonic text, and several works incorporate planning definition and domain language into the outputs~\citep{liu2023llmp,silver2023generalized,valmeekam2023large}.
On the other hand, our WebAgent leverages finetuned specialist language models and performs closed-loop planning coupled with HTML summarization by decomposing given instructions. We empirically prove that our system is superior to open-loop planning with a single generalist LLM with prompting.

\clearpage
\section{Implementation Details of HTML-T5}
\label{sec:implementation}
We use the implementation of local and global attentions released by \citet{guo2022longt5}~\footnote{\url{https://github.com/google-research/longt5}}.
Following \citet{guo2022longt5}, we set the local radius to $r=127$, and block size for transient global attention to $k=16$.
For the pre-training objective, similar to \citet{tay2022ul2}, we construct the mixtures and then use long mean span lengths: $\mu\in\{8, 64\}$, and all the denoising ratio (percentage of masked tokens in the input sequence) is set to 0.15.
We adopt 4096 input sequence length and 910 output sequence length during the pre-training. The batch size for training is set to 128.
We train the models with 100K iterations following other pre-training strategies for T5 families~\citep{chung2022flant5,lester-etal-2021-power}.
We leverage SeqIO~\citep{roberts2022t5x} and T5X~\citep{roberts2022t5x} library to manage the training pipeline. We also use SentencePiece~\citep{kudo2018sentencepiece} with 32K tokens from C4 dataset~\citep{2020t5} as a tokenizer.
During the downstream finetuning, we adopt 16K tokens for the context window unless otherwise mentioned.
We have used cloud TPU-v3, which has a 32 GiB HBM memory space, with 128 cores for the experiments.

For the dataset, we prepare 100 WARC files (April 2019) from CommonCrawl\footnote{\url{https://commoncrawl.org/}}, and pre-process the raw HTML by removing non-Unicode and alphanumeric documents and extracting subtrees around \texttt{<label>} elements that have \texttt{for} attribute, to reduce the noise in training corpus, which results in about 3.41M examples (\autoref{tab:cc_html_stats}).

\input{tables_nature/cc_html_stats}

% \clearpage
\section{WebAgent Example Flow in \housing{} website}
\label{sec:webagent_example_flow}
\input{tables_iclr/housing_flow_figure}


\clearpage
% \textbf{Static HTML Comprehension:}~
% \subsection{WebSRC: Static HTML Comprehension}
\section{WebSRC: Static HTML Comprehension}
\label{sec:websrc}
To emphasize the advantage of our modular approach, we test WebAgent on a static website comprehension benchmark, WebSRC~\citep{chen2021websrc}, which is a contextual QA dataset with HTML documents. The questions require an understanding of the spatial and logical structure of websites, and the answers are either text span on HTML or yes/no.
For the comprehensive evaluation, WebSRC has three different types of websites, \textit{KV}, \textit{Comparison}, and \textit{Table}.
KV task is a value extraction from the attribute key.
Comparison task has several entities with the same attributes.
Table task requires a structural understanding with header columns and values in the row.
We finetune HTML-T5 for snippet extraction to predict \texttt{data-ref} corresponding to the answer and use dev set for the evaluation.

% \section{Details of WebSRC}
% \label{sec:websrc_details}
As did in real-world web automation, HTML-T5 first predicts \texttt{data-ref} attribute of task-relevant snippet from the input HTML document.
To make sure there is enough context, we extract the snippet from the predicted element to the two-level-up via XPath.
If it exceeds the context length of Flan-U-PaLM, we limit it into parent elements. If it still does not work, we truncate the end of extracted snippet to fit within the token budget.
Because snippet extraction in table structure often loses the context to solve question-answering, we just truncate HTML documents for Table tasks.
Flan-U-PaLM predicts the answers seeing 5-shot examples.

As shown in \autoref{tab:websrc_results}, single LLM, such as Flan-U-PaLM or HTML-T5, has struggled to the limited context length or model capacity. In contrast, WebAgent, our LLM-collaborative approach, enhances the performance from both single generalist and specialist LLMs, and shows competitive results with strong baselines. This demonstrates that modular LLMs work complementarily to each other.
% In more detail, WebAgent is better at Comparison tasks, but inferior to structural understanding for KV and Table tasks, compared to other baselines. See \autoref{sec:websrc_details} for further details.
\autoref{fig:websrc_fig} presents the performance comparison on different types of websites (KV, Comparison, Table) among MarkupLM~\citep{li2021markuplm}, TIE~\citep{zhao2022tie}, and WebAgent.
WebAgent is better at Comparison tasks, but inferior to structural understanding for KV and Table tasks, compared to other baselines, which suggest that generalist LLMs are still not suitable for recognizing structural data such as table.


\input{tables_iclr/websrc}


\input{tables_iclr/websrc_fig}

\clearpage
\section{List of Language Instructions for Real-world Web Automation}
\label{sec:language_instruction_list}
\input{tables_nature/instruction_list}

\section{Example Episode in Real-World Web Automation}
\label{sec:example_episode}
\input{tables_iclr/real_world_examples}


\clearpage
\section{Extensive Ablation of HTML-T5}
\label{sec:htmlt5_extensive_ablations}
\subsection{Dataset and Initialization}
\label{sec:data_init}
% \paragraph{Dataset and Initialization}
% \textbf{Dataset and Initialization:}~
% \textbf{Dataset and Initialization}~
To test our recipe described in Section 2.1, we compare the different dataset and model initialization for pre-training on downstream task performances; offline task planning on \housing{} and average success rate on MiniWoB with 12K dataset.
We use Base-size models for the experiments. For HTML-denoising, we prepare the corpus from CommonCrawl with (Extracted) or without (Raw) subtree extraction around label elements on the documents.
We also compare the initialization of base architectures before HTML-denoising; from scratch or with pre-trained models on PEGASUS objective~\citep{zhang2020pegasus} that is a masked important sentence prediction from long-context paragraph.
\autoref{tab:data_init_results} reveals that snippet extraction on HTML corpus improves downstream performances since such a pre-processing can reduce the noise in raw HTML.
Moreover, initialization with PEGASUS pre-trained weights is essential for HTML-T5, because of the long-context and instruction-following nature of HTML-based tasks.

\input{tables_nature/data_init_results}

% \clearpage

\subsection{Offline Evaluation on Task Planning with Model Scaling}
\label{sec:offline_plan}

We compere the offline task planning performance between HTML-T5 and LongT5 (without HTML-denosing) with different model sizes; with Base (220M parameters), Large (770M parameters), and XL (3B parameters).
As described in Section 3.1, the models predict the next sub-instructions in a closed-loop manner considering the current HTML observations, user instructions, and previous sub-instruction histories as inputs.
For offline task planning evaluation, we use the demonstrations on \housing{} website; preparing 130 demonstrations and splitting them into train (90\%) and test splits (10\%). We report the best per-step exact match accuracy in test set.

\autoref{tab:offline_eval_results} shows that HTML-T5 outperforms LongT5 on the accuracy of sub-instruction prediction, which demonstrates that HTML-denoising pre-training captures the structural bias of HTML better without sacrificing the ability to understand natural language instructions. 
This also implies that our proposed HTML-denoising can scale to larger-size models consistently.

\input{tables_nature/housing_planning}

% \clearpage
\subsection{Description Generation}
\label{sec:desc_gen}

We also investigate the capability of HTML-T5 on static HTML comprehension tasks, as well as interactive decision making tasks.
We use Description Generation benchmark~\citep{gur2022html}, where the models generate the textual description of elements, typically used for accessibility purposes and annotated with a special attribute in the HTML schema known as \texttt{for}. We evaluate the understanding the structure of HTML as it would appear to a user, despite not having access to the rendered website directly.

We compare LaMDA~\citep{thoppilan2022lamda}, T5, LongT5, and HTML-T5 with respect to accuracy, BLEU~\citep{papineni-etal-2002-bleu}, and ROUGE-1~\citep{lin-2004-rouge} score.
As shown in \autoref{tab:descgen_results}, local and global attention mechanisms, underlying between LongT5 and HTML-T5, could almost solve the benchmark by improving the previous best performance by over 10\%, with still improved performance as model size increases.
Compared to the effect of local-global attention, HTML-T5 marginally improves against LongT5, which emphasizes that local and global attentions are critical to capture the hierarchical structure of HTML documents.

\input{tables_iclr/descgen_results}


\clearpage
\section{Flan-LongT5}
\label{sec:flan_longt5}
In the web automation literature~\citep{furuta2023mmwebnav,kim2023language}, instruction-finetuned LLMs have great success in HTML comprehension and improve the task success.
For the comparison to HTML-denosing, we prepare the instruction-finetuned LongT5 (i.e. Flan-LongT5) by leveraging Flan dataset released by \citet{chung2022flant5}.
We finetuned the pre-trained LongT5 with 100K iterations and picked up the best checkpoints.

As a sanity check of instruction-tuning, we evaluate Flan-LongT5 with few-shot/zero-shot settings on CoT benchmark (GSM8K~\citep{cobbe2021verifiers}, StrategyQA~\citep{geva2021did}, SVAMP~\citep{patel2021nlp}, Asdiv~\citep{miao2021diverse}, CommonsenseQA~\citep{talmor2019commonsenseqa}), BigBench-Hard (BBH)~\citep{suzgun2022bbh}, and MMLU~\citep{hendrycks2021measuring} as tested in \citet{longpre2023flan2}.
We reevaluate the performance of Flan-T5, using official checkpoints~\footnote{\url{https://github.com/google-research/t5x/blob/main/docs/models.md\#flan-t5-checkpoints}}.
We also check the performance of Flan-LongT5 on downstream summarization tasks, originally evaluated on LongT5~\citep{guo2022longt5}.
We use arXiv~\citep{cohan2018discourseaware}, PubMed~\citep{cohan2018discourseaware}, BigPatent~\citep{sharma2019bigpatent}, Multi-News~\citep{fabbri2019multinews}, MediaSum~\citep{zhu2021mediasum}, CNN / Daily Mail~\citep{nallapati2016summarunner} dataset for the evaluation, measuring the performance with ROUGE-1/2/L metrics.

\autoref{tab:flan_reasoning_bench} shows that we have successfully replicated the LongT5 version of instruction-finetuned language models.
Flan-LongT5 achieves competitive results to original Flan-T5; for instance, Flan-LongT5-Large (36.64) outperforms Flan-T5-Large (35.25), but Flan-LongT5-XL (39.05) is still behind Flan-T5-XL (43.03) on average.
This might be caused by the training instability of XL-size models~\citep{guo2022longt5}.
Because, unlike HTML-T5 on HTML-based tasks, reasoning tasks do not have long-context or hierarchical syntax, it is not surprising for Flan-LongT5 not to outperform Flan-T5.
\autoref{tab:longt5_summarization_bench} also demonstrates that we have successfully conducted instruction-tuning without losing the capability of long text summarization.

\input{tables_nature/flan_longt5}

\clearpage
\section{Per-Task Performance on MiniWoB++}
\label{sec:per_task_miniwob_results}
\input{tables_nature/miniwob_per_task}

\clearpage

% \update{\section{Details of Self-Experience Supervision for Real-World Experiments}}

\update{\section{Real-world Web Automation with Different Generalist LLMs}}

\update{
We compare different generalist LLMs as a module of WebAgent among model-size variants (Flan-PaLM-8B, Flan-PaLM-62B, Flan-U-PaLM-540B), and publicly accessible LLM (\texttt{gpt-3.5-turbo}). We test those models on map website following the same 20 instructions in \autoref{sec:language_instruction_list}. The results in \autoref{fig:llm_ablation} imply that the performance of  Flan-U-PaLM-540B and \texttt{gpt-3.5-turbo} are the same (80\% success, 93.8\% score), and Flan-PaLM-62B (60\% success, 86.3\% score) is lower than Flan-U-PaLM-540B, which is caused by the inaccurate program synthesis. In addition, Flan-PaLM-8B could not generate proper programs at all.
We believe that any LLM with sufficient program synthesis capabilities could be integrated into WebAgent, including Flan-U-PaLM-540B.
% We think that, not limited to Flan-U-PaLM-540B, as long as an LLM is capable enough on program synthesis, it might work as a part of WebAgent.
}

\input{tables_iclr/model_ablation_figure}
