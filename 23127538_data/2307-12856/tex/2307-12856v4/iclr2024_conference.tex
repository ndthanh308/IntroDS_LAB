
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% extra packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{bbding}
\usepackage[font=small]{caption}
% \usepackage{caption}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
%\usepackage{minted}
%\usepackage{mdframed}
\usepackage{multirow}
\usepackage{lscape}



% colors for color-blindness
\definecolor{cb_orange}{RGB}{213,94,0}
% \definecolor{cb_green}{RGB}{0,158,115}
\definecolor{cb_green}{RGB}{34,136,51}
\definecolor{sky_blue}{RGB}{204, 238, 255}
\definecolor{cb_purple}{RGB}{170, 51, 119}
\definecolor{cb_red}{RGB}{204, 51, 17}
\definecolor{cb_blue}{RGB}{0, 119, 187}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    % basicstyle=\ttfamily\footnotesize,
    basicstyle=\ttfamily\tiny,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% \usepackage{hyperref}
% \usepackage[colorlinks=true,citecolor=magenta,linkcolor=mydarkblue,urlcolor=mydarkblue]{hyperref}
\usepackage[colorlinks=true,citecolor=brown,linkcolor=mydarkblue,urlcolor=mydarkblue]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}


% commands
\newcommand{\todo}[1]{{\color{red}{TODO: #1}}}
\newcommand{\sandra}[1]{\textcolor{purple}{[Sandra: #1]}}
% \newcommand{\update}[1]{{\color{cb_purple}{#1}}}
\newcommand{\update}[1]{{\color{black}{#1}}}  % to switch the color

\newcommand{\housing}{\texttt{real-estate}}
\newcommand{\housingweb}{real estate website}
\newcommand{\socialmedia}{\texttt{social-media}}
\newcommand{\socialmediaweb}{social media website}

\newcommand{\yes}{\textcolor{cb_green}{\tiny \CheckmarkBold}}
\newcommand{\no}{\textcolor{cb_red}{\tiny \XSolidBrush}}
\newcommand{\lyes}{\textcolor{cb_green}{\small \CheckmarkBold}}
\newcommand{\lno}{\textcolor{cb_red}{\small \XSolidBrush}}


% \title{A Real-World \update{Web Agent} with Planning, \\ Long Context Understanding, and \\ Program Synthesis}
\title{A Real-World WebAgent with Planning, \\ Long Context Understanding, and \\ Program Synthesis}

\iclrfinalcopy

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{
  Izzeddin Gur$^{1*}$~
  Hiroki Furuta$^{1,2*\text{†}}$~
  Austin Huang$^{1}$~
  \textbf{Mustafa Safdari}$^{1}$~
  \textbf{Yutaka Matsuo}$^{2}$~ \\ %\\
  ~\textbf{Douglas Eck}$^{1}$~
  \textbf{Aleksandra Faust}$^{1}$ \\ %\\
  $^{1}$Google DeepMind, $^{2}$The University of Tokyo \quad \\
  \texttt{izzeddin@google.com, furuta@weblab.t.u-tokyo.ac.jp} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.


% There will be a strict upper limit of 9 pages for the main text of the submission, with unlimited additional pages for citations. This page limit applies to both the initial and final camera ready version.
% Authors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are not required to read the appendix.

\begin{document}
\maketitle
\begingroup\def\thefootnote{*}\footnotetext{Equal Contribution.}\addtocounter{footnote}{0}\endgroup
\begingroup\def\thefootnote{†}\footnotetext{Work done as Student Researcher at Google.}\addtocounter{footnote}{0}\endgroup


\begin{abstract}
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.
However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.
We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.
We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, a new pre-trained LLM for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.
We empirically demonstrate that our modular recipe improves the success on real websites by over 50\%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7\% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.
\end{abstract}


\section{Introduction}
Large language models (LLM)~\citep{brown2020language,Chowdhery2022palm,openai2023gpt4} can solve a variety of natural language tasks, such as arithmetic, commonsense, logical reasoning, question answering, text generation~\citep{brown2020language,kojima2022lets,wei2022cot}, and even interactive decision making tasks~\citep{Ahn2022saycan,yao2022react}.
Recently, LLMs have also demonstrated success in autonomous web navigation by controlling computers or browsers to follow natural language instructions through multi-step reasoning and decision making ~\citep{furuta2023mmwebnav,gur2022html,kim2023language}.

However, web automation on real-world websites has still suffered from (1) the lack of pre-defined action space, (2) much longer HTML documents than simulated observations, and (3) the absence of domain-specific knowledge for understanding HTML documents (\autoref{fig:real_sim_loop}).
Considering the open-endedness of real-world websites and the complexity of instructions, defining appropriate action spaces in advance is challenging.
In addition, although several works have argued that recent LLMs with instruction-finetuning or reinforcement learning from human feedback improve HTML understanding and web automation accuracy~\citep{furuta2023mmwebnav,kim2023language}, their architectures are not always suitable to process real-world HTML documents;
as presented in \autoref{fig:real_html_tokens}, HTML tokens of real websites are much longer than those of simulators, and most LLMs have shorter context lengths than the average HTML tokens in real websites.
It is prohibitively costly to treat such long documents as inputs directly, or adopt prior techniques such as text-XPath alignment~\citep{li2021markuplm} or text-HTML token separation~\citep{wang2022webformer}.
To prioritize broader task generalization and model-size scaling, such domain knowledge for HTML documents is ignored in recent LLMs.

\input{tables_iclr/real_sim_loop}

\input{tables_iclr/real_html_tokens}



In this work, we introduce WebAgent, an LLM-driven autonomous agent that learns from self-experience to complete user instructions on real websites by combining canonical web actions in a program space~(\autoref{fig:webagent}).
WebAgent (i) \textbf{plans sub-instructions for each step} by decomposing natural language instructions, (ii) \textbf{summarizes long HTML documents into task-relevant snippets} based on the plan, and (iii) \textbf{acts via programming} on real websites by grounding sub-instructions and HTML snippets into executable Python codes.
We combine two LLMs to form WebAgent: newly introduced HTML-T5, a domain-expert pre-trained language model, for task planning and conditional HTML summarization and Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5} for grounded code generation.
HTML-T5 has an encoder-decoder architecture and is specialized to capture the structure of long HTML documents better by adopting local and global attention mechanisms~\citep{guo2022longt5}.
It is pre-trained using a \textit{mixture of long-span denoising} objective~\citep{tay2022ul2} on a large-scale HTML corpus extracted from CommonCrawl.
To ground language model agents into real websites, we introduce \textit{self-experience supervision}, where the domain-expert language models are finetuned with data generated by scripted planning/summarization and self-generated programming.

% \input{tables/webagent_figure}

Existing LLM-driven agents often solve decision making tasks with a single LLM conditioned on different prompts per role~\citep{kim2023language,sun2023adaplanner,zheng2023synapse}, which is, however, not enough for real-world tasks whose complexity is higher than that of simulators.
The empirical evaluations reveal that our method incorporating self-bootstrapped specialist language models improves HTML understanding and grounding, and achieves better generalization than single LLM agent. In real-world web automation, WebAgent significantly increases the success rate by 50\%, and error analysis emphasizes that coupling task planning with HTML summarization in specialized language models is essential for task success.
Moreover, HTML-T5 not only works as a core module for WebAgent but also achieves strong results by itself on the web-based tasks.
On MiniWoB++~\citep{liu2018wge,shi2017miniwob}, HTML-T5 achieves 18.7\% higher success than previous language model agent~\citep{gur2022html} while also outperforming competitive baselines, such as naive local-global attention models~\citep{guo2022longt5} and its instruction-finetuned ones~\citep{chung2022flant5}.
On the Mind2Web~\citep{deng2023mind2web}, an offline task planning dataset, HTML-T5 achieves SoTA performance among Synapse~\citep{zheng2023synapse} with GPT-3.5, and MindAct with FLan-T5-XL and GPT-4~\citep{openai2023gpt4}.
In summary, our key contributions are:
\begin{itemize}[leftmargin=0.5cm,topsep=0pt,itemsep=0.0pt]
    \item We propose WebAgent, integration of two modular LLMs under self-supervision for real-world web automation. The domain-expert language model handles planning and HTML summarization, and a generalist language model generates executable Python programs.
    \item We newly introduce HTML-T5 -- a language model with local-global attention mechanism that is pre-trained with a mixture of long-span denoising objective on a large-scale HTML corpus, curated from CommonCrawl, to capture the syntax and semantics of HTML better.
    \item WebAgent notably improves the success rate by over 50\% in real websites. When fine-tuned on downstream demonstrations, HTML-T5 itself outperforms prior language model agent by 18.7\% in MiniWoB++, and achieves SoTA performance in Mind2Web, even surpassing GPT-4.
\end{itemize}


\input{tables_iclr/webagent_figure}

\section{Related Works}
\textbf{Web Automation}~
Web automation is a sequential decision making task where agents manipulate browsers following given instructions~\citep{shi2017miniwob}, such as form filling~\citep{nogueira2013user} or information retrieval~\citep{adolphs2022} through the sequence of computer actions~\citep{li2020mapping,mazumder2020flin,shvoEtAl2021appbuddy}.
Prior works have realized the web automation via reinforcement learning~\citep{gur2018learning,humphreys2022data,jia2018domqnet,shaw2023pixels}, finetuned~\citep{furuta2023mmwebnav,gur2022html} or prompted LLMs~\citep{kim2023language,sun2023adaplanner,yao2022react,zheng2023synapse} on the simulated websites~\citep{shi2017miniwob,toyama2021androidenv,yao2022webshop}.
However, there are still huge gaps between simplified simulators and real web environments; for instance, the average tokens for HTML pages are about 15 times larger (\autoref{fig:real_html_tokens}), and pre-defined action space for specific websites is a strong assumption that may harm the generalization to out-of-distribution web pages or instructions.

MindAct~\citep{deng2023mind2web} could be the most relevant work, where finetuned language model summarizes the raw HTML document into task-relevant snippets, and another model predicts the web actions in a multi-choice QA format.
While MindAct also combines several language models, it has just adopted DeBERTa~\citep{he2021deberta} and Flan-T5~\citep{chung2022flant5} for summarization and actor modules, and evaluated it on the offline dataset.
In contrast, we design HTML-T5, specialized for web-based tasks, to handle long HTML documents. WebAgent leverages HTML-T5 finetuned with self-experience for summarization and planning, and Flan-U-PaLM as a capable programmer, which enables it to generate open-ended web actions and to act on online real-world websites.


\textbf{Program Synthesis}~
In addition to common LLMs~\citep{brown2020language,Chowdhery2022palm,touvron2023llama}, several works have proposed programming-focused language models~\citep{chen2021evaluating,feng2020codebert,li2022alphacode,wang2021codet5} and their benchmarks~\citep{austin2021program,hendrycks2021apps,lu2021codexglue}.
Another line of work has investigated the tool augmentation of LLMs~\citep{parisi2022talm} by decoding API calls~\citep{schick2023toolformer} or Python snippets to be parsed with the interpreter~\citep{gao2023pal}.
Most works deal with the program synthesis on the static dataset, except for the attempts in robotics~\citep{liang2023code} and game~\citep{trivedi2022learning,wang2023voyager}, where LLMs output Python or JavaScript snippets to command the agents.
Similarly, we leverage the ability of code generation as an open-ended action space for web-based agents to manipulate the real website, and demonstrate LLMs can sequentially decode Python selenium codes considering the given sub-instructions and HTML in the prompts.

See extended related works on document understanding and LLM for task planning in \autoref{sec:extended_related_work}.


\input{tables_iclr/html_t5_figure}

\vspace{-2mm}
\section{WebAgent}
\vspace{-2mm}
WebAgent is a new architecture that combines two LLMs to achieve efficient real-world web automation. HTML-T5, a domain-expert LLM, is responsible for predicting the next sub-instruction (\textit{planning}) and generating related HTML snippets (\textit{summarization}). Flan-U-PaLM (540B)~\citep{Chowdhery2022palm,chung2022flant5}, is prompted to generate executable Python programs based on the planning and summarization provided by HTML-T5 (\autoref{fig:webagent}). 
This modular two-stage approach enables WebAgent to effectively navigate and process HTML documents. 
% In the following sections, we will delve into the details of each component.

% WebAgent is a combination of two LLMs: HTML-T5, a domain-expert language model that predicts the next sub-instruction (\textit{planning}) and related HTML snippets (\textit{summarization}), and Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5}, prompted to conditionally generate Python programs (\autoref{fig:webagent}).
% We first explain the high-level workflow and then delve into details of each component.
% WebAgent is composed of interactions between HTML-T5, a domain-expert language model, which predicts the sub-instruction for the next-step program and conditionally summarizes long HTML documents, and Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5}, an instruction-finetuned LLM for grounded program synthesis (\autoref{fig:webagent}).
% In contrast to a single LLM conditioned on different prompts per role, such a modular approach can deal with real-world tasks better.
% Moreover, to align WebAgent with real websites, we introduce self-experience supervision to ground the agent into real-world tasks. 
% We describe the details of each component in the following sections, and provide the example workflow in \autoref{sec:webagent_example_flow}.

% \subsection{Overview}
% \label{webagent:overview}

\textbf{Workflow}~
Users initiate natural language interactions with a clear intent, such as apartment searching. Upon receiving the initial request, HTML-T5 formulates a \textit{``go to \texttt{<URL>}''} sub-instruction, triggering Flan-U-PaLM to generate a corresponding Python program that navigates to the specified website. The raw HTML content of the newly opened page is extracted and fed into HTML-T5 along with the user's instruction and previous planning steps. This information is utilized to predict the next sub-instruction and identify relevant reference IDs for extractive HTML summarization. Flan-U-PaLM, in turn, generates a Python program based on these sub-instructions and combined HTML snippets. This iterative process of planning, summarization, and program synthesis continues until a designated \textit{end-of-episode} sub-instruction is predicted or the maximum number of iterations is reached.

% Users utter a natural language instruction with a clear intent, such as apartment search. At the initial step, HTML-T5 plans a \textit{``go to \texttt{<URL>}''} sub-instruction, and Flan-U-PaLM is prompted to generate a Python program transiting to the website. Raw HTML at the new page is fetched and is given to HTML-T5 along with the user instruction and previous planning steps to predict a next sub-instruction and a set of relevant reference IDs for extractive HTML summarization. Similarly, Flan-U-PaLM generates Python program with those sub-instruction and combined HTML snippet. These planning, summarization, and program synthesis steps continue until a special \textit{end-of-episode} sub-instruction is predicted or the episode reaches maximum iterations.

% The users initiate the navigation by uttering a natural language instruction with clear intent, such as apartment search. As there are no previous planning steps or observed HTML documents, HTML-T5 is given only the human instruction and plans a \texttt{go to <URL of the website>} sub-instruction with an empty set of reference IDs. Flan-U-PaLM is prompted with this sub-instruction to generate a Python program that simply navigates to the corresponding URL. HTML at the current page is fetched and is given to HTML-T5 along with user instruction and previous planning steps to generate a new sub-instruction and a set of corresponding reference IDs to summarize HTML extractively. Similarly, Flan-U-PaLM is prompted with sub-instruction and combined HTML snippet to generate a python program. These planning, summarization, and program synthesis steps are repeated until HTML-T5 predicts a special \textit{end-of-episode} sub-instruction or the episode reaches a predefined number of iterations. See \autoref{sec:webagent_example_flow} for the example workflow.

% 1. Humans initiate the navigation by uttering an instruction to the WebAgent.
% 2. The raw HTML document is fetched from the corresponding website.
% 3. HTML-T5 takes user instruction, raw HTML, and navigation history if it exists, as inputs.
% 4. HTML-T5 predicts the next sub-instruction, and `data-ref` attributes of all elements relevant to the task.
% 5. Raw HTML is summarized by extracting HTML snippets that correspond to these data-ref attribute values.
% 6. Flan-U-PaLM generates a Python program to interact with the website, conditioned on predicted  sub-instruction and HTML snippets. The program is executed via Selenium WebDriver.
% 7. Steps 2 - 6 are repeated until HTML-T5 predicts `end-of-episode` sub-instruction or the loop exceeds max number of iterations.



\subsection{HTML-T5}
\label{sec:methods}
Prior research has shown that general-purpose LLMs, such as T5~\citep{2020t5}, Flan-T5~\citep{chung2022flant5}, and InstructGPT~\citep{ouyang2022instructgpt}, can effectively navigate web environments ~\citep{furuta2023mmwebnav,gur2022html,kim2023language}.
However, unlike \textit{specialist} transformer models~\citep{li2021markuplm,wang2022webformer,zhao2022tie}, these general-purpose LLMs do not fully utilize the HTML-specific information that could otherwise lead to better understanding of HTML content.
To address this limitation, we introduce HTML-T5, a pre-trained encoder-decoder language model specifically designed for HTML-based web automation tasks. HTML-T5 carefully merges the generalist and specialist characteristics of language models.
% We introduce HTML-T5, a pre-trained encoder-decoder language model tailored towards HTML-based web automation tasks, by carefully fusing the generalist and specialist nature of language models.
It processes HTML in a text-to-text manner and employs local and global attention mechanisms~\citep{ainslie2020etc} in the encoder to capture the hierarchical structure of long HTML inputs.
HTML-T5 is pre-trained on a large-scale HTML corpus curated from CommonCrawl using a mixture of long-span denoising objectives~\citep{tay2022ul2}, and then finetuned it for each downstream task.
For WebAgent, we employ the self-experience supervision approach to align the model with real websites.


\textbf{Model Architecture}~
Unlike natural language, HTML documents possess an explicit hierarchical structure.
This structure comprises elements such as
\texttt{<input>}, \texttt{<label>}, and \texttt{<button>}, along with their associated attributes like \texttt{class}, \texttt{label}, and \texttt{id}. These elements are defined locally and combined hierarchically to create HTML documents.
% , as exemplified by \texttt{<form>}, \texttt{<body>}, and \texttt{<document>}.
To model this inherent hierarchy, we replace the common dense attention~\citep{vaswani2017attention} with local and global attention mechanisms~\citep{ainslie2020etc}.
Local attention restricts each token to only attend to neighboring tokens within a window.
Additionally, transient global attention allows each token to attend to tokens beyond its immediate window. This is achieved through the aggregation and normalization of token representations within each window, resulting in a global memory representation.
% Transient global attention allows each input token to attend to tokens beyond a single window by computing a global memory of window representations using aggregation and normalization of tokens within each window.
\autoref{fig:html_t5} describes the concepts of HTML-T5; leaf elements in HTML (\textcolor{cb_green}{green}) could be processed by local attention, and internal elements (\textcolor{cb_purple}{purple}) could be compressed into transient global attention, which naturally fits the hierarchical structure of HTML.
Following LongT5~\citep{guo2022longt5}, we use dense attention in the decoder.


\input{tables_iclr/real_world_webnav}


\textbf{Pre-Training with Mixture of Long-Span Denoising}~
% The performance of language models in downstream tasks highly depends on the knowledge learned during pre-training. To incorporate further inductive bias on HTML into scalable language models, we perform self-supervised pre-training with large-scale HTML corpus.
Our pre-training approach for HTML-T5 utilizes a span denoising objective. This involves randomly masking spans of tokens within an HTML document, with span lengths drawn from a Gaussian distribution with a mean of $\mu$. The objective is then to predict the masked spans using the remaining tokens in the HTML document~\citep{2020t5,tay2022ul2,ainslie2023colt5}. While a span length of $\mu = 3$ is commonly used, such short spans often mask less meaningful chunks in HTML documents, such as \texttt{</}, \texttt{id=}, or \texttt{">}~(\autoref{fig:html_t5}), where the signal-to-noise ratio can be significantly lower than natural language text. In contrast, longer spans can contain more semantically meaningful chunks, such as \texttt{<form class="} or \texttt{type="submit">}. Our empirical findings indicate that setting $\mu\in\{8, 64\}$ yields the optimal mixture for HTML documents (Section 4.2).

% To deal with the sparsity of contents tokens in HTML documents, we introduce a \textit{mixture of long-span denoising} objective, by masking input tokens with longer mean span lengths than popular value for natural language (e.g. $\mu=3$). Such a shorter mean span length only masks less meaningful chunks, such as \texttt{</}, \texttt{id=}, or \texttt{">}~(\autoref{fig:html_t5}), which might not be helpful for LLMs to capture the syntax and semantics of HTML. In contrast, longer span can contain more semantically meaningful chunks, such as \texttt{<form class="} or \texttt{type="submit">}. We empirically find $\mu\in\{8, 64\}$ is the optimal mixture (Section~\ref{sec:htmlt5_ablations}).

% we mask input HTML documents with random spans of tokens, and the models take all other tokens from the documents as inputs to predict corrupted spans~\citep{ainslie2023colt5,2020t5,tay2022ul2}.
% To deal with the sparsity of contents tokens in HTML documents, we introduce a \textit{mixture of long-span denoising} objective, by masking input tokens with longer mean span lengths than popular value for natural language (e.g. $\mu=3$). Such a shorter mean span length only masks less meaningful chunks, such as \texttt{</}, \texttt{id=}, or \texttt{">}~(\autoref{fig:html_t5}), which might not be helpful for LLMs to capture the syntax and semantics of HTML. In contrast, longer span can contain more semantically meaningful chunks, such as \texttt{<form class="} or \texttt{type="submit">}. We empirically find $\mu\in\{8, 64\}$ is the optimal mixture (Section~\ref{sec:htmlt5_ablations}).

We adopt 4096 input sequence length and 910 output sequence length during pre-training. In total, 15\% of input tokens are randomly masked in the denoising objective.
For the pre-training dataset, we collect 100 WARC files (April 2019) from the CommonCrawl corpus and remove the non-Unicode or alphanumeric-only HTML documents.
We then extract subtrees around \texttt{<label>} elements that have a special attribute called \texttt{for} that associates the corresponding label with a unique input element in the same HTML document. 
This pre-processing step improves the quality of the pre-training corpus by focusing only on HTML that is relevant for instruction following and grounding.
Our final dataset has 3.41M examples. We pre-train HTML-T5 for 100K iterations following the practice in other T5 models~\citep{chung2022flant5,lester-etal-2021-power}.
See \autoref{sec:implementation} for further details.

% For the dataset, we prepare 100 WARC files (April 2019) from CommonCrawl, and pre-process the raw HTML by removing non-Unicode and alphanumeric documents and extracting subtrees around \texttt{<label>} elements that have \texttt{for} attribute, to reduce the noise in training corpus, which results in about 3.41M examples. % (\autoref{tab:cc_html_stats}).
% We train the models with 100K iterations following other pre-training strategies for T5 families~\citep{chung2022flant5,lester-etal-2021-power}.
% See \autoref{sec:implementation} for further details.


% \subsection{Self-Supervised Experience Distillation}
% \subsubsection{Alignment with Self-Experience Supervision}
% \subsubsection{Alignment with Real Websites via Self-Experience Supervision}
\subsection{Self-Experience Supervision for Alignment with Real Websites}
Gathering example demonstrations for LLMs to understand websites poses a significant obstacle in real-world web automation. While humans can effortlessly execute instruction following on actual websites, manually annotating every planning, summarization, and program synthesis step as detailed above is impractical. To address this issue, we propose \textit{self-experience supervision}, a semi-supervised approach that necessitates minimal human involvement. In this method, manually curated scripts generate planning and summarization steps, while Flan-U-PaLM is tasked with generating Python programs.
Our WebAgent aligns domain-specific language models, such as HTML-T5, with these self-gathered real-world experiences through fine-tuning~\citep{selfinstruct}. This enables the generalization and alignment of agents to complex real-world tasks.
% A major challenge in real-world web automation is collecting example demonstrations for LLM to understand websites.
% Humans could perform instruction following on real websites easily, but it is infeasible to manually annotate every planning, summarization, and program synthesis step as explained above.
% To alleviate this problem, we introduce \textit{self-experience supervision}, a semi-supervised approach with minimal human intervention where manually-curated scripts generate planning and summarization steps while Flan-U-PaLM is prompted to generate Python programs.

\textbf{Instruction Templates}~
We maintain a collection of instruction templates that incorporate placeholders such as \textit{``Show me the way from \texttt{<start>} to \texttt{<goal>} by \texttt{<n-th>} \texttt{<transportation>} at map website''}.
We sample instructions by randomly assigning values to placeholders from pre-defined key-value pairs.

\textbf{Scripted Planning and Prompted Programming}~
% We implement a rule-based parser that decomposes an instruction into a sequence of sub-instructions, and corresponding reference IDs, retrieved in HTML by regular expression.
% At each step, Flan-U-PaLM is prompted with such scripted sub-instruction and combined HTML snippet to generate web actions executed through Selenium WebDriver via program synthesis.
We employ a rule-based parser to decompose instructions into sequences of sub-instructions; corresponding reference IDs are retrieved from HTML using regular expressions. At each step of the process, Flan-U-PaLM is provided with the sub-instruction and the associated HTML snippets to generate navigation programs that are executed through Selenium WebDriver.
The success of recorded demonstrations varies, and automating success criteria for real-world tasks remains challenging. To refine the learning experience, we utilize environmental feedback to eliminate critical failures, such as program execution errors, retriever errors, and clearly erroneous URL prefixes~\citep{ni2023lever}.
% Our WebAgent aligns domain-specific language models, such as HTML-T5, with these self-gathered real-world experiences through fine-tuning~\citep{selfinstruct}. This approach enables the generalization and alignment of agents to complex real-world tasks.
% The recorded demonstrations may succeed or fail, but the success criteria for real-world tasks are hard to automate. To filter the experience, we leverage the environmental feedback that can remove critical failures; for instance, the program execution errors, retriever errors, and clearly wrong prefix of URL~\citep{ni2023lever}.
% Our WebAgent aligns domain-expert language models, HTML-T5, with those self-collected real-world experiences via finetuning~\citep{selfinstruct}.
% This self-supervision process realizes the generalization and alignment of the agents to challenging real-world tasks.


\input{tables_iclr/real_world_examples_small}


\textbf{Finetuning for Planning and Summarization}~
% We align language models to perform closed-loop planning with a sequence of sub-instructions and to summarize long HTML documents into concise snippets relevant to the current plan.
HTML-T5, a core component of WebAgent, is fine-tuned using self-experience demonstrations gathered through instruction sampling, scripted planning, and prompted program synthesis, as detailed earlier. It utilizes task instructions (e.g. \textit{please search 2 bedroom and 2+ bathroom houses in new york, ny with a max price of \$7500 on \housingweb{}}), sub-instruction histories (e.g. \textit{go to \housingweb{}}, \textit{type in new york into search}, \textit{click on search}, \textit{click on price}, \textit{click on max rent}), and raw HTML as inputs.
Subsequently, it generates the next sub-instruction (e.g. \textit{type in 7500 into max rent}) and extracts the relevant \texttt{data-ref} attributes used for retrieving HTML snippets.
Section~\ref{sec:realworld_webnav} demonstrates the significance of integrating HTML summarization into sub-instruction prediction for enhancing real-world web automation performance.


\subsection{Grounded Program Synthesis}
Web automation on real-world websites faces challenges due to the open-ended action spaces, unlike simplified simulators~\citep{shi2017miniwob,yao2022webshop}. In contrast to previous approaches~\citep{gur2018learning,humphreys2022data,jia2018domqnet,liu2018wge}, real-world web agents cannot pre-define a categorical action space to specify the interactive elements on the websites.
To address this open-domain challenge, we introduce the \textit{act via programming} paradigm in web automation by utilizing the conditional code generation capabilities of LLMs~\citep{chen2021evaluating,liang2023code}.
Provided with few-shot generic examples (such as selecting checkboxes, entering text into inputs, clicking on options, and scrolling etc.) for program generation, the next sub-instruction, and the extracted HTML snippet from HTML-T5, Flan-U-PaLM~\citep{Chowdhery2022palm,chung2022flant5} decodes an Python program (\autoref{fig:webagent}) executable with Selenium WebDriver, a library for browser automation.
This conditional program synthesis requires LLMs to not only generate code to follow natural language instructions but also understand the semantics and functionality of HTML elements. We provide several Python snippet examples generated by Flan-U-PaLM as follows (sub-instructions are treated as comments in the script):


\begin{lstlisting}[language=Python]
# Type in walnut creek, ca into search
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').clear()
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').send_keys("walnut creek, ca")

# Submit the search
driver.find_element(By.CSS_SELECTOR, '[data-ref="175"]').submit()

# Click on the apartments
driver.find_element(By.CSS_SELECTOR, '[data-ref="572"]').click()

# Scroll down housing type by 200px
driver.execute_script('getScrollParent(document.querySelector("#type-of-housing")).scrollBy({top: 200})')
\end{lstlisting}

\vspace{-3mm}
\section{Experimental Results}
To study how a modular combination of LLMs under self-supervision enables real-world web automation by overcoming open-endedness and long context documents, we execute instruction-following tasks on real websites (Section~\ref{sec:realworld_webnav}).
In \autoref{sec:websrc}, we also test WebAgent on WebSRC~\citep{chen2021websrc}, a static HTML comprehension benchmark, compared to prior transformer models specialized for structured documents~\citep{li2021markuplm,zhao2022tie}.
In addition, we quantify the performance of HTML-T5 itself on simulated web benchmark, MiniWoB++, and offline task planning benchmark, Mind2Web (Section~\ref{sec:htmlt5_ablations}).


\input{tables_iclr/html_t5_ablation}


\subsection{Real-world Web Automation}
\label{sec:realworld_webnav}
\textbf{Evaluation Methodology}~
We first evaluate WebAgent with the real-world navigation performance under human supervision, at \housingweb{} (a platform for housing), \socialmediaweb{} (a network of communities), and map website.
These three websites have different properties. \housing{} requires long-horizon planning (about 20 steps per episode) for complex form-filling with a few page transitions (at least 2 pages), and \socialmedia{} needs shorter plans (about 10 steps per episode) with many page transitions (at least 4 pages) by selecting appropriate hyperlinks on the page.
\texttt{map} is the easiest domain with shorter plans and a few page transitions.
WebAgent receives natural language instructions (e.g. \textit{Can you search for a studio bedroom, 1+ bathroom apartments in oroville, ca for corporate housing on \housingweb{}?}, or \textit{Could you present the most new thread of Python community filtered by Tutorial tag on \socialmediaweb{}?}), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM (\autoref{fig:real_world_examples_small}).
Through the self-experience supervision process, we curate 260 episodes on \housingweb{},  230 episodes on \socialmediaweb{}, and 410 episodes on map website to finetune HTML-T5.

We prepare 20 different natural language instructions (see \autoref{sec:language_instruction_list} for the full list), and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode~\citep{yao2022webshop}; for instance, (1) \textit{apartments} for (2) \textit{corporate housing} with (3) \textit{studio bedroom} and (4) \textit{1+ bathroom} located in (5) \textit{oroville, ca}, can be specified in the instruction.
When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score is 60 ($ = 100 \times 3/5$).
If the agents achieve 100 score, that episode will mark as success.


\textbf{Results}~
For comparison, we prepare three baselines, consisting of language model modules and a single LLM conditioned on different prompts per role, such as Flan-U-PaLM~\citep{chung2022flant5}, that with a planning language model (Flan-U-PaLM+P), and that with a summarization language model (Flan-U-PaLM+S).
If they do not use language model modules, prompted Flan-U-PaLM plans in an open-loop manner (\textbf{Plan}: \no{}), and regular-expression-based retrieval summarizes given raw HTML (\textbf{Sum}: \no{}).
\autoref{tab:realworld_results} shows that by leveraging planning and summarization language model modules, WebAgent achieves best 65\% success and 87.6\% score on \housing{}, 70\% success and 85.8\% score on \socialmedia{}, and 80\% success and 93.8\% score on \texttt{map}, significantly outperforming single Flan-U-PaLM, or with partial language model modules (most of those achieve about 10 - 30\% success).
This result suggests that self-experience supervision notably improves the performance, and closed-loop planning grounded on HTML observations via finetuned domain language models is more suitable for open-ended web automation than open-loop planning with few-shot LLMs. This trend is remarkable in \housing{} (even Flan-U-PaLM+P achieves 50\% success), where the longer planning horizon is needed to fulfill instructions. We also observe that coupling sub-instruction prediction with HTML summarization in language model modules plays a critical role in task success.
The development of more capable planning modules to decompose the given instructions adaptively and accurately could help WebAgent improve the performance further.


\textbf{Error Analysis}~
We also analyze the reason of failures by categorizing them into programming, planning, and summarization errors (\autoref{tab:realworld_results}). Programming error does not satisfy the given sub-instructions or HTML snippet. Planning error predicts sub-instructions conflicting with user instructions, and summarization error fails to extract the relevant HTML snippets for given sub-instructions.
From the website perspective, the failures on \housing{} concentrate in planning because of its long-horizon nature. \texttt{map} also fails in planning when confusing starting point and destination.
In contrast, \socialmedia{} tends to fail in programming due to the ambiguous sub-instructions or summarization including redundant hyperlinks, which results in transiting wrong pages or clicking unexecutable elements.
From the method perspective, WebAgent often fails in planning by predicting incorrect sub-instructions (for instance, in \texttt{real-estate}, WebAgent generates incorrect plans in 70\% of failure episodes), while other baselines more fail in programming or summarization steps. This observation indicates that, through the self-experience supervision, the ratio of programming and summarization errors has decreased while the fundamental difficulty of planning, which requires consistent and accurate prediction over long horizon without error accumulation, still remains.

\input{tables_iclr/miniwob_sl_results}

\vspace{-3mm}
\subsection{Ablation of HTML-T5}
\label{sec:htmlt5_ablations}

In addition to the evaluation as WebAgent system, we extensively examine HTML-T5 about (i) the generalization to other websites with Mind2Web~\citep{deng2023mind2web}, (ii) the performance on MiniWoB++, a standard web automation benchmark~\citep{liu2018wge,shi2017miniwob}, and (iii) its architecture and pre-training objective.
We adopt 16K tokens for the context window unless otherwise mentioned.
We \update{present results on offline task planning, and description generation ~\citep{gur2022html} to test HTML understanding on static dataset in \autoref{sec:htmlt5_extensive_ablations}}.


\textbf{Mind2Web}~
\input{tables_iclr/mind2web}
% \input{tables_iclr/mind2web_iclr_submission}
Mind2Web~\citep{deng2023mind2web} is an action-annotated real-world dataset with over 2K instructions collected from 137 websites. It provides action prediction tasks that measure the generalization of LLMs across the tasks, websites, and their domains (e.g. travel, shopping).
\update{Similar to real-world evaluation, the input is a set of HTML snippets, a task instruction, and an action history. The output comprises a target element to interact with, along with the operation, such as click, type, or select an option.}
% Conditioned on the top-50 HTML snippet candidates, task instruction, and action history, LLMs should predict the next step action by choosing a target element to interact with in a multi-choice QA format and generating the operation such as click, type, or select option.
We finetune HTML-T5-XL with the training dataset.
The performance is evaluated with element accuracy, operation F1, and step success rate that cares for both element and operation correctness.
\autoref{tab:mind2web} reveals that HTML-T5 significantly outperforms baselines with Flan-T5-XL or GPT-4~\citep{openai2023gpt4} across task/website/domain generalization, which increases element accuracy by 5-8\%, operation F1 by 6-8\%, and step success rate by 4-8\%.
This highlights that HTML-T5 can handle real-world web automation tasks better and shows generalization beyond our real-world evaluation with 3 websites.


\textbf{MiniWoB++}~
We here evaluate HTML-T5 on 56 simulated tasks in MiniWoB++ using 100 evaluation episodes per task. \update{Inputs are analogous to real-world evaluation, utilizing HTML documents, while outputs are adhering to a pre-defined format by the simulator such as $\textit{click}(\textit{ref}=X)$.}
We finetune HTML-T5 with 12K human demonstrations~\citep{liu2018wge}, and compare the average success rate to prior supervised-learned agents~\citep{gur2022html,humphreys2022data}, LongT5, and its instruction-finetuned variants~\citep{chung2022flant5}
~\footnote{We finetune LongT5 models with Flan dataset released by \citet{chung2022flant5}. See \autoref{sec:flan_longt5}.
% . As a sanity check, we test them on representative reasoning and summarization tasks
% (see \autoref{sec:flan_longt5}).
}.
\autoref{tab:miniwob_sl_results} shows that HTML-T5-XL significantly outperforms WebN-T5, the prior best model, by 18.7\%.
Notably, we demonstrate HTML-denoising consistently improves the performance on top of LongT5 in all the model sizes, better than instruction-finetuning introduced in prior work~\citep{furuta2023mmwebnav}. 
Furthermore, we finetune HTML-T5-XL with 347K demonstrations from \citet{furuta2023mmwebnav}, which performs better than 11B-parameter Flan-T5-XXL even with 3B parameters, achieving 85.6\% success.
% Noticably, HTML-T5-XL is the first model to solve the most challenging MiniWoB task, \texttt{book-flight} (99\% success; see \autoref{sec:per_task_miniwob_results}), by only using limited labeled HTML data without any online trials-and-errors.
These prove we successfully incorporate domain knowledge on HTML comprehension for web automation into pre-trained language models.

% \paragraph{Architecture and Objective}
% ~~\textbf{Architecture and Objective:}~
\textbf{Architecture and Objective}~
We hypothesize that local and global attention mechanisms can capture the hierarchical structures of HTML documents better than dense attention.
We compare the web automation performance among 56 MiniWoB++ tasks~\citep{gur2022html}, by finetuning HTML-T5 with public 12K-episode dataset~\citep{liu2018wge}.
We adopt 2048 and 4096 tokens as input length and prepare Base-size architectures.
\autoref{tab:html_t5_ablation} (left) reveals that the combination of local and global attentions achieves the superior success rate by over 18\% compared to the instruction-finetuned dense attentions~\citep{chung2022flant5,2020t5} and local attention only.
Surprisingly, local attention only still surpasses the dense attention by about 9\%, which suggests local relation between elements and attributes in HTML are essential for web tasks.


As for pre-training objective in \autoref{tab:html_t5_ablation} (right), HTML-denoising generally improves the performance on offline task planning on \housingweb{} and MiniWoB. Especially, using only longer span lengths ($\mu\in\{8, 64\}$) outperforms other choices, including the popular configuration in natural language domain ($\mu\in\{3,8,64\}$ + Prefix LM objective), which can reduce the less meaningful prediction from shorter spans (e.g. $\mu=3$), and inject the structural bias of HTML into language models better.
See Appendix~\ref{sec:offline_plan} for further results with model scaling.

\vspace{-3mm}
\section{Discussion and Limitation}
\textbf{Modular Approach with Specialist Language Models}~
We demonstrate it is beneficial to divide web automation into planning, HTML summarization, and code generation, and to combine domain-expert language models aligned with self-experience data.
Such modular approaches have also been adopted to support the inference of LLMs~\citep{xu2023small}, multimodal tasks~\citep{zeng2022socratic}, and robotics~\citep{Ahn2022saycan}, which, however, might cause additional computational costs and latency.

\textbf{Broad Generalization across the Internet}~
Because open-loop planning with prompted Flan-U-PaLM achieves at most 10 - 30\% success, we have demonstrated that self-experience supervision on real websites is essential for planning modules.
As we demonstrated in Mind2Web, our method could generalize across the internet if we have enough data.
It would be expected to collect demonstrations at scale and align larger domain-expert models with them in future works.

\textbf{Feedback for Program Synthesis}~
We leverage Flan-U-PaLM with 540B parameters, as a capable program synthesis module via few-shot prompting.
Such a large model, however, makes it challenging to reflect the feedback about the errors in generated code, compared to smaller models.
We leave it as future direction to incorporate the feedback for program synthesis into larger language models.


\textbf{Evaluation for Real-world Web Automation}~
Beyond the simulated web environments~\citep{shi2017miniwob,yao2022webshop}, we have exhibited WebAgent can follow given complex and sometimes ambiguous instructions on real estate, social media and map websites.
On the other hand, it is costly to evaluate the performance of autonomous agents in the real world.
Automated evaluation with minimal human intervention would be helpful for the scalable development of real-world web agents.

\vspace{-3mm}
\section{Conclusion}
\label{sec:conclusion}
We build a system for real-world web automation, combining HTML-T5 for planning and HTML summarization and Flan-U-PaLM for grounded program synthesis.
Our proposed WebAgent achieves around 70-80\% success on real websites via self-experience supervision, outperforming single LLM approach by over 50\%, which suggests dividing the sequence of sub-problems with multiple language models can increase the entire task success.
We also propose a scalable recipe for HTML-specialized language models where we train local and global attention mechanisms with a mixture of long-span denoising objectives to capture the hierarchical structures of HTML documents.
HTML-T5 not only plays an essential role in WebAgent but also can achieve the best results on a variety of HTML-based benchmarks such as Mind2Web and MiniWoB++.
We hope our work contributes to getting us one-step closer to the practical deployment of autonomous web agent systems.

\update{\section*{Ethics Statement}
This paper presents encouraging evidence of autonomous agents' potential for deployment on real websites, extending beyond simulated environments. In the foreseeable future, this technology could lead to the development of sophisticated AI assistant tools for computers and smartphones, enhancing productivity and accessibility for society.

While we recognize the promising aspects of autonomous agents, we must also consider the potential for misuse and unintended consequences in their development. As our proposed system is based on LLMs, there is a risk of prompt injection. The improper use of web automation could pose cybersecurity threats and expose users to scams. To mitigate these risks, it is crucial for researchers, policymakers, and industry stakeholders to collaborate on establishing guidelines and regulations for the development of autonomous agents. Additionally, security research focused on LLM agents will become an essential domain for society.
% This paper has shown the promising signal about the autonomous agents deployable on real websites beyond simulated environments. In the near future, this technique could realize capable AI assistant tools on computers or smartphones, and improve productivity and accessibility for society. 

% While we anticipate the positive aspects of autonomous agents, for responsible development, we should also consider the potential harmful applications and unintended consequences. Since our proposed system is based on LLMs, there is a risk of prompt injection. The misuse of web automation would threaten cyber security, and the users may get scammed. To reduce these risks, it is essential for researchers, policymakers, and industry to form guidelines and regulations for the development. Additionally, security research against LLM agents would become an essential domain for society.
}

\subsubsection*{Acknowledgments}
We thank Heiga Zen, Yingjie Miao, Yusuke Iwasawa, Joshua Ainslie, Santiago Ontanon, Quoc V. Le, Zoubin Ghahramani, Jeff Dean, Tris Warkentin for the supports and advises on this work. HF was supported by JSPS KAKENHI Grant Number JP22J21582.


% \clearpage
\bibliography{reference}
\bibliographystyle{iclr2024_conference}


\clearpage
\section*{Appendix}
\appendix
\input{iclr2024_conference_appendix}

\end{document}

