\begin{table*}[ht]
\begin{center}
\begin{small}
%\begin{sc}
\scalebox{0.95}{
\begin{tabular}{lrrrrrrrrrrr}
\toprule
 & \multicolumn{2}{c}{\textbf{CoT}} & \multicolumn{2}{c}{\textbf{MMLU}} & \multicolumn{2}{c}{\textbf{BBH}} & \multicolumn{2}{c}{\textbf{BBH-CoT}} & \multicolumn{3}{c}{\textbf{Avg.}}\\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-12}
\textbf{Models} & Zero & Few  & Zero & Few & Zero & Few & Zero & Few & CoT & Direct & Total \\
\midrule
Flan-T5-Large & 35.14 & 40.03 & 40.68 & 45.12 & 25.90 & 37.48 & 26.17 & 31.45 & 33.20 & 37.29 & 35.25 \\
Flan-T5-XL & 51.74 & 52.64 & 50.76 & 52.40 & 26.09 & 40.96 & 34.12 & 35.62 & 43.53 & 42.55 & 43.04 \\
\midrule
Flan-LongT5-Large & 44.78 & 45.34 & 38.44 & 40.03 & 28.67 & 34.67 & 29.38 & 31.85 & 37.84 & 35.45 & 36.64 \\
Flan-LongT5-XL & 48.78 & 50.02 & 43.44 & 44.74 & 26.53 & 37.77 & 29.09 & 32.01 & 39.97 & 38.12 & 39.05 \\
\bottomrule
\end{tabular}
}
%\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Performance of Flan-LongT5 on reasoning tasks. We reevaluate the performance of Flan-T5~\citep{chung2022flant5}, using official checkpoints. Flan-LongT5 achieves competitive results to original Flan-T5.}
\label{tab:flan_reasoning_bench}
\end{table*}

\begin{table*}[ht]
\begin{center}
\begin{small}
%\begin{sc}
\scalebox{0.7}{
\begin{tabular}{lrrrrrrrrrrrrrrrrrr}
\toprule
 & \multicolumn{3}{c}{\textbf{arXiv}} & \multicolumn{3}{c}{\textbf{PubMed}} & \multicolumn{3}{c}{\textbf{BigPatent}} & \multicolumn{3}{c}{\textbf{MultiNews}} &
 \multicolumn{3}{c}{\textbf{MediaSum}} & \multicolumn{3}{c}{\textbf{CNN / Daily Mail}} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16} \cmidrule(r){17-19}
\textbf{Models} & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L \\
\midrule
LongT5-Large & 48.28 & 21.63 & 44.11 & 49.98 & 24.69 & 46.46 & 70.38 & 56.81 & 62.73 & 47.18 & 18.44 & 24.18 & 35.54 & 19.04 & 32.20 & 42.49 & 20.51 & 40.18 \\
LongT5-XL & 48.35 & 21.92 & 44.27 & 50.23 & 24.76 & 46.67 & \textbf{76.87} & \textbf{66.06} & \textbf{70.76} & 48.17 & 19.43 & \textbf{24.94} & 36.15 & 19.66 & 32.80 & \textbf{43.94} & \textbf{21.40} & \textbf{41.28} \\
\midrule
Flan-LongT5-Large & \textbf{48.52} & \textbf{22.00} & \textbf{44.46} & \textbf{50.46} & \textbf{25.08} & \textbf{46.96} & 70.53 & 57.13 & 63.02 & 47.76 & 18.99 & 24.52 & 35.71 & 19.18 & 32.33 & 43.13 & 20.89 & 37.28 \\
Flan-LongT5-XL & 48.37 & 21.75 & 44.22 & 50.23 & 24.75 & 46.73 & 76.31 & 65.17 & 70.01 & \textbf{48.19} & \textbf{19.47} & 24.80 & \textbf{36.16} & \textbf{19.75} & \textbf{32.81} & 43.46 & 21.00 & 37.34 \\
\bottomrule
\end{tabular}
}
%\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Performance of Flan-LongT5 on downstream summarization tasks, compared to LongT5~\citep{guo2022longt5}. We measure the performance with ROUGE-1/2/L metrics.}
\label{tab:longt5_summarization_bench}
\end{table*}
