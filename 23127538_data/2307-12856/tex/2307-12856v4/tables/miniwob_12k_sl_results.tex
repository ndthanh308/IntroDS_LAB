\begin{table}[t]
\begin{center}
\begin{small}
%\begin{sc}
\scalebox{0.975}{
\begin{tabular}{lrr}
\toprule
\textbf{Models} & \textbf{Success Rate} & \textbf{Diff.}\\
\midrule
CC-Net~\citep{humphreys2022data} & 32.0\% & -- \\
WebT5-XL~\citep{gur2022html} & 48.4\% & -- \\
\midrule
LongT5-Base & 53.8\% & 0.0 \\
LongT5-Large & 56.3\% & 0.0 \\
LongT5-XL & 60.4\% & 0.0 \\
\midrule
Flan-LongT5-Base & 54.1\% & +0.3 \\
Flan-LongT5-Large & 56.1\% & -0.2 \\
Flan-LongT5-XL & 61.1\% & +0.7 \\
\midrule
HTML-T5-Base & 57.0\% & +3.2 \\
HTML-T5-Large & 60.8\% & +4.5 \\
HTML-T5-XL & \textbf{63.3}\% & +2.9 \\
\bottomrule
\end{tabular}
}
%\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{
Average success rate of MiniWoB++ benchmark with 56 tasks. We use 12K public demonstrations~\citep{liu2018wge}, and compare HTML-T5 among supervised-finetuned baselines~\citep{humphreys2022data,gur2022html,furuta2023mmwebnav}.
HTML-T5-XL significantly outperforms WebN-T5, the prior best method, by 14.9\%, and HTML-denoising improves the performance better than instruction finetuning.
}
\label{tab:miniwob_12k_sl_results}
\end{table}

