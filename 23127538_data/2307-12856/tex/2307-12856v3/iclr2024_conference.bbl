\begin{thebibliography}{89}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adolphs et~al.(2022)Adolphs, Boerschinger, Buck, Huebscher, Ciaramita, Espeholt, Hofmann, Kilcher, Rothe, Sessa, and Saralegui]{adolphs2022}
Leonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle~Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann, Yannic Kilcher, Sascha Rothe, Pier~Giuseppe Sessa, and Lierni~Sestorain Saralegui.
\newblock Boosting search engines with interactive agents.
\newblock In \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman, Herzog, Ho, Hsu, Ibarz, Ichter, Irpan, Jang, Ruano, Jeffrey, Jesmonth, Joshi, Julian, Kalashnikov, Kuang, Lee, Levine, Lu, Luu, Parada, Pastor, Quiambao, Rao, Rettinghouse, Reyes, Sermanet, Sievers, Tan, Toshev, Vanhoucke, Xia, Xiao, Xu, Xu, Yan, and Zeng]{Ahn2022saycan}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario~Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil~J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock \emph{arXiv preprint arxiv:2204.01691}, 2022.

\bibitem[Ainslie et~al.(2020)Ainslie, Ontanon, Alberti, Cvicek, Fisher, Pham, Ravula, Sanghai, Wang, and Yang]{ainslie2020etc}
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li~Yang.
\newblock Etc: Encoding long and structured inputs in transformers.
\newblock \emph{arXiv preprint arXiv:2004.08483}, 2020.

\bibitem[Ainslie et~al.(2023)Ainslie, Lei, de~Jong, Ontañón, Brahma, Zemlyanskiy, Uthus, Guo, Lee-Thorp, Tay, Sung, and Sanghai]{ainslie2023colt5}
Joshua Ainslie, Tao Lei, Michiel de~Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi~Tay, Yun-Hsuan Sung, and Sumit Sanghai.
\newblock Colt5: Faster long-range transformers with conditional computation.
\newblock \emph{arXiv preprint arXiv:2303.09752}, 2023.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, Chu, Clark, Shafey, Huang, Meier-Hellstern, Mishra, Moreira, Omernick, Robinson, Ruder, Tay, Xiao, Xu, Zhang, Abrego, Ahn, Austin, Barham, Botha, Bradbury, Brahma, Brooks, Catasta, Cheng, Cherry, Choquette-Choo, Chowdhery, Crepy, Dave, Dehghani, Dev, Devlin, Díaz, Du, Dyer, Feinberg, Feng, Fienber, Freitag, Garcia, Gehrmann, Gonzalez, Gur-Ari, Hand, Hashemi, Hou, Howland, Hu, Hui, Hurwitz, Isard, Ittycheriah, Jagielski, Jia, Kenealy, Krikun, Kudugunta, Lan, Lee, Lee, Li, Li, Li, Li, Li, Lim, Lin, Liu, Liu, Maggioni, Mahendru, Maynez, Misra, Moussalem, Nado, Nham, Ni, Nystrom, Parrish, Pellat, Polacek, Polozov, Pope, Qiao, Reif, Richter, Riley, Ros, Roy, Saeta, Samuel, Shelby, Slone, Smilkov, So, Sohn, Tokumine, Valter, Vasudevan, Vodrahalli, Wang, Wang, Wang, Wang, Wieting, Wu, Xu, Xu, Xue, Yin, Yu, Zhang, Zheng, Zheng, Zhou, Zhou, Petrov, and Wu]{anil2023palm2}
Rohan Anil, Andrew~M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan~H. Clark, Laurent~El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi~Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo~Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher~A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le~Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang
  Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex~Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David~R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce~Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Appalaraju et~al.(2021)Appalaraju, Jasani, Kota, Xie, and Manmatha]{appalaraju2021docformer}
Srikar Appalaraju, Bhavan Jasani, Bhargava~Urala Kota, Yusheng Xie, and R.~Manmatha.
\newblock Docformer: End-to-end transformer for document understanding.
\newblock In \emph{International Conference on Computer Vision}, 2021.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Zhao, Chen, Ji, Zhang, Luo, Xiong, and Yu]{chen2021websrc}
Xingyu Chen, Zihan Zhao, Lu~Chen, JiaBao Ji, Danyang Zhang, Ao~Luo, Yuxuan Xiong, and Kai Yu.
\newblock {W}eb{SRC}: A dataset for web-based structural reading comprehension.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  4173--4185, 2021{\natexlab{b}}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{Chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{chung2022flant5}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arxiv:2210.11416}, 2022.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021verifiers}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cohan et~al.(2018)Cohan, Dernoncourt, Kim, Bui, Kim, Chang, and Goharian]{cohan2018discourseaware}
Arman Cohan, Franck Dernoncourt, Doo~Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.
\newblock A discourse-aware attention model for abstractive summarization of long documents.
\newblock \emph{arXiv preprint arXiv:1804.05685}, 2018.

\bibitem[Deng et~al.(2023)Deng, Gu, Zheng, Chen, Stevens, Wang, Sun, and Su]{deng2023mind2web}
Xiang Deng, Yu~Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu~Su.
\newblock Mind2web: Towards a generalist agent for the web.
\newblock \emph{arXiv preprint arXiv:2306.06070}, 2023.

\bibitem[Diaz et~al.(2013)Diaz, Otaduy, and Puente]{nogueira2013user}
Oscar Diaz, Itziar Otaduy, and Gorka Puente.
\newblock User-driven automation of web form filling.
\newblock In \emph{International Conference on Web Engineering}, 2013.

\bibitem[Fabbri et~al.(2019)Fabbri, Li, She, Li, and Radev]{fabbri2019multinews}
Alexander~R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir~R. Radev.
\newblock Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model.
\newblock \emph{arXiv preprint arXiv:1906.01749}, 2019.

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu, Jiang, and Zhou]{feng2020codebert}
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock \emph{arXiv preprint arXiv:2002.08155}, 2020.

\bibitem[Furuta et~al.(2023)Furuta, Nachum, Lee, Matsuo, Gu, and Gur]{furuta2023mmwebnav}
Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang~Shane Gu, and Izzeddin Gur.
\newblock Multimodal web navigation with instruction-finetuned foundation models.
\newblock \emph{arXiv preprint arxiv:2305.11854}, 2023.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig]{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock \emph{arXiv preprint arXiv:2211.10435}, 2023.

\bibitem[Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and Berant]{geva2021did}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant.
\newblock Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.
\newblock \emph{arXiv preprint arXiv:2101.02235}, 2021.

\bibitem[Guo et~al.(2022)Guo, Ainslie, Uthus, Ontanon, Ni, Sung, and Yang]{guo2022longt5}
Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
\newblock {L}ong{T}5: {E}fficient text-to-text transformer for long sequences.
\newblock In \emph{Findings of the Association for Computational Linguistics: NAACL 2022}, pp.\  724--736, 2022.

\bibitem[Gur et~al.(2019)Gur, Rueckert, Faust, and Hakkani-Tur]{gur2018learning}
Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur.
\newblock Learning to navigate the web.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Gur et~al.(2022)Gur, Nachum, Miao, Safdari, Huang, Chowdhery, Narang, Fiedel, and Faust]{gur2022html}
Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust.
\newblock Understanding html with large language models.
\newblock \emph{arXiv preprint arxiv:2210.03945}, 2022.

\bibitem[He et~al.(2021)He, Liu, Gao, and Chen]{he2021deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycks2021apps}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{arXiv preprint arXiv:2105.09938}, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2021{\natexlab{b}}.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock \emph{arXiv preprint arXiv:2201.07207}, 2022.

\bibitem[Humphreys et~al.(2022)Humphreys, Raposo, Pohlen, Thornton, Chhaparia, Muldal, Abramson, Georgiev, Goldin, Santoro, and Lillicrap]{humphreys2022data}
Peter~C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap.
\newblock A data-driven approach for learning to control computers.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Jia et~al.(2019)Jia, Kiros, and Ba]{jia2018domqnet}
Sheng Jia, Jamie~Ryan Kiros, and Jimmy Ba.
\newblock {DOM}-q-{NET}: Grounded {RL} on structured language.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kim et~al.(2023)Kim, Baldi, and McAleer]{kim2023language}
Geunwoo Kim, Pierre Baldi, and Stephen McAleer.
\newblock Language models can solve computer tasks.
\newblock \emph{arXiv preprint arxiv:2303.17491}, 2023.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022lets}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In \emph{Advances In Neural Information Processing Systems}, 2022.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}, 2018.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  3045--3059, November 2021.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Bi, Yan, Wang, Huang, Huang, and Si]{li2021structurallm}
Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si.
\newblock Structurallm: Structural pre-training for form understanding.
\newblock \emph{arXiv preprint arXiv:2105.11210}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Xu, Cui, and Wei]{li2021markuplm}
Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei.
\newblock Markuplm: Pre-training of text and markup language for visually-rich document understanding.
\newblock \emph{arXiv preprint arxiv:2110.08518}, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Gu, Kuen, Morariu, Zhao, Jain, Manjunatha, and Liu]{li2021selfdoc}
Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad~I. Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu.
\newblock Selfdoc: Self-supervised document representation learning.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, 2021{\natexlab{c}}.

\bibitem[Li et~al.(2020)Li, He, Zhou, Zhang, and Baldridge]{li2020mapping}
Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge.
\newblock Mapping natural language instructions to mobile ui action sequences.
\newblock In \emph{Annual Conference of the Association for Computational Linguistics}, 2020.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Lago, Hubert, Choy, de, dAutume, Babuschkin, Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz, Robson, Kohli, de~Freitas, Kavukcuoglu, and Vinyals]{li2022alphacode}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin~Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel~J. Mankowitz, Esme~Sutherland Robson, Pushmeet Kohli, Nando de~Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock Competition-level code generation with alphacode, 2022.

\bibitem[Liang et~al.(2023)Liang, Huang, Xia, Xu, Hausman, Ichter, Florence, and Zeng]{liang2023code}
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng.
\newblock Code as policies: Language model programs for embodied control.
\newblock \emph{arXiv preprint arXiv:2209.07753}, 2023.

\bibitem[Lin(2004)]{lin-2004-rouge}
Chin-Yew Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pp.\  74--81. Association for Computational Linguistics, July 2004.

\bibitem[Liu et~al.(2023)Liu, Jiang, Zhang, Liu, Zhang, Biswas, and Stone]{liu2023llmp}
Bo~Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone.
\newblock Llm+p: Empowering large language models with optimal planning proficiency.
\newblock \emph{arXiv preprint arXiv:2304.11477}, 2023.

\bibitem[Liu et~al.(2018)Liu, Guu, Pasupat, and Liang]{liu2018wge}
Evan~Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang.
\newblock Reinforcement learning on web interfaces using workflow-guided exploration.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{longpre2023flan2}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023.

\bibitem[Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement, Drain, Jiang, Tang, Li, Zhou, Shou, Zhou, Tufano, Gong, Zhou, Duan, Sundaresan, Deng, Fu, and Liu]{lu2021codexglue}
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge~Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao~Kun Deng, Shengyu Fu, and Shujie Liu.
\newblock Codexglue: A machine learning benchmark dataset for code understanding and generation.
\newblock \emph{arXiv preprint arXiv:2102.04664}, 2021.

\bibitem[Mazumder \& Riva(2020)Mazumder and Riva]{mazumder2020flin}
Sahisnu Mazumder and Oriana Riva.
\newblock Flin: A flexible natural language interface for web navigation.
\newblock \emph{arXiv preprint arXiv:2010.12844}, 2020.

\bibitem[Miao et~al.(2021)Miao, Liang, and Su]{miao2021diverse}
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.
\newblock A diverse corpus for evaluating and developing english math word problem solvers.
\newblock \emph{arXiv preprint arXiv:2106.15772}, 2021.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhai, and Zhou]{nallapati2016summarunner}
Ramesh Nallapati, Feifei Zhai, and Bowen Zhou.
\newblock Summarunner: A recurrent neural network based sequence model for extractive summarization of documents.
\newblock \emph{arXiv preprint arXiv:1611.04230}, 2016.

\bibitem[Ni et~al.(2023)Ni, Iyer, Radev, Stoyanov, Yih, Wang, and Lin]{ni2023lever}
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida~I Wang, and Xi~Victoria Lin.
\newblock Lever: Learning to verify language-to-code generation with execution.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Nottingham et~al.(2023)Nottingham, Ammanabrolu, Suhr, Choi, Hajishirzi, Singh, and Fox]{nottingham2023embodied}
Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox.
\newblock Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{ouyang2022instructgpt}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arxiv:2203.02155}, 2022.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni-etal-2002-bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock {B}leu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics}, pp.\  311--318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.

\bibitem[Parisi et~al.(2022)Parisi, Zhao, and Fiedel]{parisi2022talm}
Aaron Parisi, Yao Zhao, and Noah Fiedel.
\newblock Talm: Tool augmented language models.
\newblock \emph{arXiv preprint arXiv:2205.12255}, 2022.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel2021nlp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are nlp models really able to solve simple math word problems?
\newblock \emph{arXiv preprint arXiv:2103.07191}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Roberts et~al.(2022)Roberts, Chung, Levskaya, Mishra, Bradbury, Andor, Narang, Lester, Gaffney, Mohiuddin, Hawthorne, Lewkowycz, Salcianu, van Zee, Austin, Goodman, Soares, Hu, Tsvyashchenko, Chowdhery, Bastings, Bulian, Garcia, Ni, Chen, Kenealy, Clark, Lee, Garrette, Lee-Thorp, Raffel, Shazeer, Ritter, Bosma, Passos, Maitin-Shepard, Fiedel, Omernick, Saeta, Sepassi, Spiridonov, Newlan, and Gesmundo]{roberts2022t5x}
Adam Roberts, Hyung~Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio~Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan~H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo.
\newblock Scaling up models and data with $\texttt{t5x}$ and $\texttt{seqio}$.
\newblock \emph{arXiv preprint arXiv:2203.17189}, 2022.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dessì, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}, 2023.

\bibitem[Sharma et~al.(2019)Sharma, Li, and Wang]{sharma2019bigpatent}
Eva Sharma, Chen Li, and Lu~Wang.
\newblock Bigpatent: A large-scale dataset for abstractive and coherent summarization.
\newblock \emph{arXiv preprint arXiv:1906.03741}, 2019.

\bibitem[Shaw et~al.(2023)Shaw, Joshi, Cohan, Berant, Pasupat, Hu, Khandelwal, Lee, and Toutanova]{shaw2023pixels}
Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova.
\newblock From pixels to ui actions: Learning to follow instructions via graphical user interfaces.
\newblock \emph{arXiv preprint arXiv:2306.00245}, 2023.

\bibitem[Shi et~al.(2017)Shi, Karpathy, Fan, Hernandez, and Liang]{shi2017miniwob}
Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang.
\newblock World of bits: An open-domain platform for web-based agents.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Shvo et~al.(2021)Shvo, Hu, Icarte, Mohomed, Jepson, and McIlraith]{shvoEtAl2021appbuddy}
Maayan Shvo, Zhiming Hu, Rodrigo~Toro Icarte, Iqbal Mohomed, Allan~D. Jepson, and Sheila~A. McIlraith.
\newblock Appbuddy: Learning to accomplish tasks in mobile apps via reinforcement learning.
\newblock In \emph{Canadian Conference on Artificial Intelligence}, 2021.

\bibitem[Silver et~al.(2023)Silver, Dan, Srinivas, Tenenbaum, Kaelbling, and Katz]{silver2023generalized}
Tom Silver, Soham Dan, Kavitha Srinivas, Joshua~B. Tenenbaum, Leslie~Pack Kaelbling, and Michael Katz.
\newblock Generalized planning in pddl domains with pretrained large language models.
\newblock \emph{arXiv preprint arXiv:2305.11014}, 2023.

\bibitem[Singh et~al.(2022)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox, Thomason, and Garg]{singh2022progprompt}
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.
\newblock {ProgPrompt}: Generating situated robot task plans using large language models.
\newblock \emph{arXiv preprint arXiv:2209.11302}, 2022.

\bibitem[Sun et~al.(2023)Sun, Zhuang, Kong, Dai, and Zhang]{sun2023adaplanner}
Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo~Dai, and Chao Zhang.
\newblock Adaplanner: Adaptive planning from feedback with language models.
\newblock \emph{arXiv preprint arXiv:2305.16653}, 2023.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Schärli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, and Wei]{suzgun2022bbh}
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V. Le, Ed~H. Chi, Denny Zhou, and Jason Wei.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and Berant]{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}, 2019.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Wei, Wang, Chung, Bahri, Schuster, Zheng, Zhou, Houlsby, and Metzler]{tay2022ul2}
Yi~Tay, Mostafa Dehghani, Vinh~Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung~Won Chung, Dara Bahri, Tal Schuster, Huaixiu~Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.
\newblock Ul2: Unifying language learning paradigms.
\newblock \emph{arXiv preprint arXiv:2205.05131}, 2022.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang, Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou, Chang, Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris, Doshi, Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson, Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton, Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and Le]{thoppilan2022lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arxiv:2302.13971}, 2023.

\bibitem[Toyama et~al.(2021)Toyama, Hamel, Gergely, Comanici, Glaese, Ahmed, Jackson, Mourad, and Precup]{toyama2021androidenv}
Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup.
\newblock Androidenv: A reinforcement learning platform for android.
\newblock \emph{arXiv preprint arXiv:2105.13231}, 2021.

\bibitem[Trivedi et~al.(2022)Trivedi, Zhang, Sun, and Lim]{trivedi2022learning}
Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph~J. Lim.
\newblock Learning to synthesize programs as interpretable and generalizable policies.
\newblock \emph{arXiv preprint arXiv:2108.13643}, 2022.

\bibitem[Valmeekam et~al.(2023)Valmeekam, Olmo, Sreedharan, and Kambhampati]{valmeekam2023large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for llms on planning and reasoning about change).
\newblock \emph{arXiv preprint arXiv:2206.10498}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar]{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Fang, Ravula, Feng, Quan, and Liu]{wang2022webformer}
Qifan Wang, Yi~Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu.
\newblock Webformer: The web-page transformer for structure information extraction.
\newblock \emph{arXiv preprint arXiv:2202.00217}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Wang, Joty, and Hoi]{wang2021codet5}
Yue Wang, Weishi Wang, Shafiq Joty, and Steven~C.H. Hoi.
\newblock {C}ode{T}5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  8696--8708, 2021.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Cai, Liu, Ma, and Liang]{wang2023describe}
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.
\newblock Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.
\newblock In \emph{International Conference on Machine Learning}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{wei2022cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Xu et~al.(2023)Xu, Xu, Wang, Liu, Zhu, and McAuley]{xu2023small}
Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley.
\newblock Small models are valuable plug-ins for large language models.
\newblock \emph{arXiv preprint arXiv:2305.08848}, 2023.

\bibitem[Xu et~al.(2019)Xu, Li, Cui, Huang, Wei, and Zhou]{xu2019layout}
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.
\newblock {LayoutLM}: Pre-training of text and layout for document image understanding.
\newblock \emph{arXiv preprint arxiv:1912.13318}, 2019.

\bibitem[Yao et~al.(2022{\natexlab{a}})Yao, Chen, Yang, and Narasimhan]{yao2022webshop}
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
\newblock Webshop: Towards scalable real-world web interaction with grounded language agents.
\newblock \emph{arXiv preprint arxiv:2207.01206}, 2022{\natexlab{a}}.

\bibitem[Yao et~al.(2022{\natexlab{b}})Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022{\natexlab{b}}.

\bibitem[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker, Tombari, Purohit, Ryoo, Sindhwani, Lee, Vanhoucke, and Florence]{zeng2022socratic}
Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.
\newblock Socratic models: Composing zero-shot multimodal reasoning with language.
\newblock \emph{arXiv preprint arXiv:2204.00598}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, Saleh, and Liu]{zhang2020pegasus}
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter~J. Liu.
\newblock Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Zhao et~al.(2022)Zhao, Chen, Cao, Xu, Chen, and Yu]{zhao2022tie}
Zihan Zhao, Lu~Chen, Ruisheng Cao, Hongshen Xu, Xingyu Chen, and Kai Yu.
\newblock {TIE}: Topological information enhanced structural reading comprehension on web pages.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1808--1821, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Wang, and An]{zheng2023synapse}
Longtao Zheng, Rundong Wang, and Bo~An.
\newblock Synapse: Leveraging few-shot exemplars for human-level computer control.
\newblock \emph{arXiv preprint arXiv:2306.07863}, 2023.

\bibitem[Zhu et~al.(2021)Zhu, Liu, Mei, and Zeng]{zhu2021mediasum}
Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.
\newblock Mediasum: A large-scale media interview dataset for dialogue summarization.
\newblock \emph{arXiv preprint arXiv:2103.06410}, 2021.

\end{thebibliography}
