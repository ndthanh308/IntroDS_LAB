\section{Introduction \& Motivation}
Recommender systems power algorithmic decision-making on platforms across the web.
They occur in many different application domains, but all are centred around a similar question:
\begin{center}
    ``\emph{What} content do we recommend to \emph{whom}?''
\end{center}

Research and applications in the field of recommender systems have undergone a large shift in the last few decades, moving from \emph{rating}~\cite{Bennet2007} to \emph{item} prediction~\cite{Steck2013} and, more recently, embracing an \emph{interventionist} view~\cite{Joachims_London_Su_Swaminathan_Wang_2021}.
Throughout this evolution, the evaluation practices that are commonly adopted in the field have undergone a parallel shift.
Indeed, whereas the continuous and explicit nature of ratings lends itself to the Root Mean Square Error (RMSE) metric; discrete item predictions are more often viewed in a \emph{ranking} setup where Information Retrieval (IR) metrics like Precision, Recall, and (normalised) Discounted Cumulative Gain (nDCG) have been widely adopted~\cite{Valcarce2020}; and the interventionist view lends itself particularly well to \emph{counterfactual} and \emph{off-policy} estimation techniques that can be directly mapped to online metrics~\cite{Vasile2020, Saito2021}.

Despite the fact that evaluation methods are a core topic in recommender systems research that enjoy a significant amount of interest, some problems remain that are fundamentally hard to solve.
Online experiments (i.e. randomised controlled trials or A/B-tests) are seen as the \emph{gold standard} of evaluation practices, and deservedly so: by leveraging interactions with users, they allow us to directly measure an array of online metrics for a given recommendation model~\cite{Kohavi2020}.
Nevertheless, as they are costly to conduct and the academic research community seldom has access to platforms with real users~\cite{Jeunen2019DS}, \emph{offline} evaluation practices are a common alternative used to showcase newly proposed methods' performance, both in the research literature and in industry applications (often as a precursor to an online experiment).

Even though the need for offline evaluation methods that mimic the outcome of an online experiment is clear, the reality is that existing methods seldom do so satisfactorily~\cite{Beel2013,Garcin2014,Rossetti2016,Jeunen2018}, even if advances in counterfactual estimation techniques have recently led to several success stories~\cite{Gilotte2018,Gruson2019}.
The reasons for this ``offline-online mismatch'' are manifold --- and some can be attributed due to offline evaluation inconsistencies (even before we compare them to online results).
First, there is a fundamental mismatch between many ``next-item prediction'' metrics (e.g. recall) and online metrics (e.g. click-through rate).
Although this can be partially alleviated by the interventionist lens~\cite{Jeunen2019REVEAL_EVAL}, a majority of published research remains focused on IR-inspired metrics.
Second, a myriad of evaluation options can lead to contrasting results~\cite{Canamares2020}, and several offline metrics differ in robustness and discriminative power~\cite{Valcarce2020}.
Third, sampled versions of these metrics have been adopted for efficiency reasons, but are \emph{inconsistent} with their unsampled counterparts and should thus be avoided~\cite{Krichene2020,Canamares2020RecSys}.
Fourth, multiple recent works have reported troubling trends in the reproducibility of widely cited methods~\cite{FerrariDacrema2019,FerrariDacrema2021,Rendle2020,Rendle2022,Rendle2019,Cavenaghi2023}---similar issues plagued the adjacent IR field a decade earlier~\cite{Armstrong2009}. 

This article aims to contribute to this important line of research, by focusing on the widely adopted (normalised) Discounted Cumulative Gain metric~\cite{Jarvelin2002}.
Focusing on the purpose that offline metrics serve, we ask a simple question:

\begin{center}    
``\emph{When can we expect (n)DCG to accurately approximate\\the gold standard outcome of an online experiment?}''
\end{center}

The main scientific contributions we present in pursuit of answering this question, are the following:
\begin{enumerate}
    \item We formally present the assumptions that are necessary to consider DCG an \emph{unbiased} estimator of online reward, providing a derivation for this metric from first principles whilst linking it to off-policy estimation (\S~\ref{sec:dcg_unbiased}).
    \item We formally prove that the widespread practice of \emph{normalising} the DCG metric renders it inconsistent with respect to DCG, in that the ordering given by nDCG can differ from that given by DCG, and provide empirical evidence (\S~\ref{sec:ndcg_inconsistent}).
    \item Empirical results from off- and online experiments on a large-scale recommendation platform show that:
    \begin{enumerate}
        \item the unbiased DCG metric strongly correlates with online metrics over time, whereas nDCG does not (\S ~\ref{sec:RQ1}),
        \item whilst differences in online metrics directionally align with differences in both nDCG and DCG, the latter can enjoy improved sensitivity to detect statistically significant online improvements (\S ~\ref{sec:RQ3}).
    \end{enumerate}
    \item We revisit the assumptions that are necessary to consider DCG an unbiased estimator: discussing when they can be reasonable and how we can relax them, giving rise to a research agenda and ways forward (\S ~\ref{sec:beyond}).
\end{enumerate}