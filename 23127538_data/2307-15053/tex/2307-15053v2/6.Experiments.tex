\section{Experimental Results \& Discussion}\label{sec:experiments}
Until now, we have derived the theoretical conditions that are necessary to consider DCG an unbiased estimator of online reward, and we have highlighted both theoretically and empirically that nDCG deviates from this.
In what follows, we wish to empirically validate whether we can leverage the metrics effectively to estimate online reward for deployed ranking policies, for recommender systems running on large-scale web platforms.
The research questions we wish to answer with empirical evidence, are the following:
\begin{description}
    \item[\textbf{RQ2}] \textit{Are offline evaluation results obtained using DCG correlated with online metrics from deployed models?}
    \item[\textbf{RQ3}] \textit{Are offline evaluation results obtained using normalised DCG correlated with online metrics from deployed models?}
    \item[\textbf{RQ4}] \textit{Does the proposed unbiased DCG formulation with learnt position biases and de-biased interaction labels improve correlation with online metrics over the classical and widely adopted (yet biased) DCG formulation?}
    \item[\textbf{RQ5}] \textit{Are differences in any of the considered offline evaluation metrics predictive of differences in online metrics?}
\end{description}
We focus on correlations between off- and online metrics rather than exact estimation error, because downstream business logic prevents the model output to \emph{exactly} match online behaviour~\cite{Jakimov2023}.

To provide empirical evidence for research questions 2--5, we require access to a \emph{ground truth} online metric.
There are two families of evaluation methods we can consider to obtain this:
\begin{enumerate*}
    \item simulation studies, or
    \item online experiments.
\end{enumerate*}
Both come with their own (dis)advantages.
Indeed, simulation studies are generally reproducible and allow full control over the environment to investigate which of the assumptions laid out in the theoretical sections of this work are necessary to retain DCG's utility as an online metric.
Nevertheless, simulations require us to make additional assumptions about user behaviour, that are often non-trivial to validate.
As a result, they would provide no empirical evidence on real-world value of the metrics, and are limited in the insights they can bring.

Online experiments, on the other hand, are harder to reproduce.
Notwithstanding this, they allow us to directly measure real user behaviour and give a view of the utility of the DCG metric for offline evaluation purposes in a real-world deployed system.
This can guide practitioners who need to perform such evaluations.
We focus on the online family for the remainder of this work, noting that simulations provide an interesting avenue for future research.

We use data from a large-scale social media platform that utilises a two-stage ranking system as described earlier to present users with a personalised feed of short videos they might enjoy.
The platform operates a hierarchical feed where users are presented with a 1\textsuperscript{st}-level feed they can scroll through and engage with content, and users can enter a 2\textsuperscript{nd}-level ``\emph{more-like-this}'' feed via any given 1\textsuperscript{st}-level item.\footnote{Examples of well-known social media platforms that operate similar user interfaces include Instagram, Reddit, and ShareChat.}
Because of the differences in interaction modalities and the user interface between the two feeds, they require separate models to estimate position bias, and we separate them in our analysis.
The 1\textsuperscript{st}-level feed adopts a recently proposed probabilistic position bias model~\cite{Jeunen2023_C3PO}, whereas the 2\textsuperscript{nd}-level feed adopts an exponential form (such as the one underlying rank-biased precision~\cite{Moffat2008}).
Because of this difference, the importance weights in the 2\textsuperscript{nd}-level feed exhibit much larger variance, and we adopt a clipping parameter for IPS which we set at $200$ to compute the de-biased DCG metric on this data (and vary it in Section~\ref{sec:RQ3}).
\emph{Rewards} on a short-video platform can be diverse.
We collect both implicit signals (e.g. watching a video until the end) and explicit signals (e.g. liking a video), and consider both types of rewards for our on- and offline metrics, referring to them as $\mathcal{C}_{\rm imp}$ and $\mathcal{C}_{\rm exp}$ respectively.
\sisetup{detect-weight=true,detect-inline-weight=math}
\begin{table*}[t]
    \vspace{-2ex}
    \centering
    \begin{tabular}{cccRpcRpcRpcRpccRp}
    \toprule
     ~&~& ~ & \multicolumn{11}{c}{\textbf{DCG}} &~&~& \multicolumn{2}{c}{\textbf{nDCG}} \\
    \cmidrule(l){4-14}\cmidrule(l){17-18}
     ~&~ & ~ & \multicolumn{2}{c}{\textbf{log}, $\mathbf{C}$} &~& \multicolumn{2}{c}{\textbf{log}, $\mathbf{\hat{Q}}$}  &~& \multicolumn{2}{c}{\textbf{pbm},  $\mathbf{C}$} &~& \multicolumn{2}{c}{\textbf{pbm}, $\mathbf{\hat{Q}}$}  &~&~& \multicolumn{2}{c}{\textbf{pbm}, $\mathbf{\hat{Q}}$} \\
    \cmidrule(l){4-5}\cmidrule(l){7-8}\cmidrule(l){10-11}\cmidrule(l){13-14}\cmidrule(l){17-18}
\rowcolor{White} ~& ~ & ~ & $\bm{r}$ & $\bm{p}$\textbf{-val} &~& $\bm{r}$ & $\bm{p}$\textbf{-val} &~& $\bm{r}$ & $\bm{p}$\textbf{-val} &~& $\bm{r}$ & $\bm{p}$\textbf{-val}  &~&~& $\bm{r}$ & $\bm{p}$\textbf{-val} \\
    \midrule
\multirow{4}{*}{1\textsuperscript{st}-level} &
 \multirow{2}{*}{$C_{\rm exp}$} & $\mathcal{R}_{0}$ & 0.97\textsuperscript{\dag} & <0.01 &~& 0.96\textsuperscript{\dag} & <0.01 &~& 0.97\textsuperscript{\dag} & <0.01 &~& \bfseries0.98\textsuperscript{\dag} & <0.01 &~&~& -0.91\textsuperscript{\dag} & 0.01\\
~& ~ & $\mathcal{R}_{t}$ & 0.86\textsuperscript{\dag}  & 0.03 &~& 0.86\textsuperscript{\dag} & 0.03 &~& 0.87\textsuperscript{\dag} & 0.03 &~& \bfseries0.91\textsuperscript{\dag} & 0.01 &~&~& -0.80 & 0.06\\
\vspace{-2ex}~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&\\
~& \multirow{2}{*}{$C_{\rm imp}$} & $\mathcal{R}_{0}$ & 0.57 & 0.24 &~& \bfseries0.70 & 0.12 &~& 0.57 & 0.24 &~& \bfseries0.70 & 0.12 &~&~& -0.80 & 0.06\\
~& ~ & $\mathcal{R}_{t}$ & 0.05 & 0.92 &~& \bfseries0.17 & 0.73 &~& 0.05 & 0.92 &~& \bfseries0.17 & 0.75 &~&~& -0.32 & 0.55\\
\vspace{-2ex}~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&\\
\cdashline{1-18}\\
\vspace{-5ex}~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&\\
\multirow{4}{*}{2\textsuperscript{nd}-level} &
\multirow{2}{*}{$C_{\rm exp}$} & $\mathcal{R}_{0}$ & 0.75 & 0.14 &~& 0.71 & 0.18 &~& 0.77 & 0.13 &~& \bfseries0.79 & 0.11 &~&~& -0.42 & 0.48\\
 ~&~ & $\mathcal{R}_{t}$ & 0.28 & 0.65 &~& 0.25 & 0.68 &~& 0.38 & 0.53 &~& \bfseries0.49 & 0.40 &~&~& -0.34 & 0.57\\
\vspace{-2ex}~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&\\
~& \multirow{2}{*}{$C_{\rm imp}$} & $\mathcal{R}_{0}$ & 0.10 & 0.88 &~& 0.58 & 0.30 &~& 0.48 & 0.41 &~& \bfseries0.85 & 0.07 &~&~& -0.93\textsuperscript{\dag} & 0.02\\
~& ~ & $\mathcal{R}_{t}$ & 0.55 & 0.34 &~& \bfseries0.88\textsuperscript{\dag} & 0.05 &~& 0.70 & 0.19 &~& \bfseries0.88\textsuperscript{\dag} & 0.05 &~&~& -0.77 & 0.13\\
    \bottomrule
    \end{tabular}
\caption{Correlation between online reward as measured from an A/B-test and offline evaluation metrics. We consider both \emph{explicit} and \emph{implicit} reward signals, on two levels of a hierarchical feed structure on a short-video platform. We consider DCG with a logarithmic discount (log) and a learnt model (pbm); using interaction signals directly ($C$) or de-biasing them to estimate $\hat{Q}=C/\mathsf{P}(V|R)$; for both a logging and target policy $\mathcal{R}_{0},\mathcal{R}_{t}$.
We report Pearson's correlation coefficient $r$ and a two-tailed $p$-value using Student's correlation test~\cite{Student1908}.
Statistically significant correlations ($p<0.05$) are marked\textsuperscript{\dag}, best performers are bold.}~\label{tab:results}
 \vspace{-2ex}
\end{table*}
\subsection{Offline--Online Metric Correlation (RQ2--4)}\label{sec:RQ1}
We collect data over a week of a deployed online experiment with over 40 million users where we deployed a change to a deterministic ranking policy $\mathcal{R}$, and kept the candidate generator $\mathcal{G}$ fixed.
This simplifies the exposure calculation that is used in the offline evaluation metrics in Eq.~\ref{eq:exposure} to that in Eq.~\ref{eq:formal_DCG}, and ensures that the variance of the offline estimator is lower.
For every day $d$ in the experiment, we
\begin{enumerate*}
    \item aggregate online results per day $d$ as the average number of logged positive feedback samples per session, and
    \item collect a dataset $\mathcal{D}^{d}_{0}$ which we use to compute offline metrics (through Eq.~\ref{eq:formal_DCG} and variants thereof).
\end{enumerate*}
Then, we compute Pearson's correlation coefficient between the series of $d$ online metrics from (1), and the $d$ offline estimates from (2), for competing evaluation metrics.
Table~\ref{tab:results} presents results from this experiment, with a detailed description in the caption.
Results here align with what theory would suggest: the unbiased DCG variant that we have formally derived in Section~\ref{sec:dcg_unbiased} provides the strongest correlation with online reward.
Both adopting a \emph{learnt} position bias model as opposed to the classical logarithmic form, and de-biasing observed interaction labels as opposed to na\"ively using them, have a significant effect on the performance of the final estimator.
On the 1\textsuperscript{st}-level feed, we observe that our estimator works especially well with an explicit reward signal.
This somewhat reverses for the 2\textsuperscript{nd}-level feed, where the implicit reward signal leads to a stronger correlation with online results.
We hypothesise that this is directly related to the degree with which the assumptions laid out in Section~\ref{sec:dcg_unbiased} are violated.
Users leave the 1\textsuperscript{st}-level feed and enter the 2\textsuperscript{nd}-level feed if they click on a 1\textsuperscript{st}-level video.
As such, the implicit reward $\mathcal{C}_{\rm imp}$ of succesfully watching a video is highly dependent on other items in the feed, violating Asm.~\ref{ass:no_slate} (reward independence across ranks), as well as Asm.~\ref{ass:pbm} (absence of cascading behaviour).
Note that all assumptions are expected to be violated to some degree --- but the strongly positive correlation results presented in Table~\ref{tab:results} are promising.

Somewhat surprisingly, not only does (unbiased) normalised DCG exhibit worse correlation than DCG, it provides a strongly \emph{negative} correlation.
At first sight, this seems troubling.
Nevertheless, we provide an intuitive explanation and highlight that this result alone does \emph{not} imply that the metric cannot be useful for offline evaluation purposes.
Note that discrepancies stemming from the normalisation procedure have a disproportionate impact when the reward is unevenly distributed across sessions or days.
Suppose we observe two sessions: one with a single positive label over two impressions, and one with 10 positive labels over 1\,000 impressions.
Because DCG deals with absolute numbers, the second session will bear a weight proportional to its positive reward.
Normalised DCG, on the other hand, considers the relative distance to the optimal ranking.
If we reasonably assume that our ranking model is imperfect, the distance to the optimal ranking is likely to be \emph{higher} for the second session, and nDCG will be lower as a result (even though we have higher DCG).
This same argument can be made across different days in the experiment, explaining poor correlation results over time.
Notwithstanding this, it does not necessarily imply that nDCG holds no merit as an offline evaluation metric.
In what follows, we consider a more important question, focusing on \emph{differences} in online metrics instead.

\subsection{Offline--Online Metric Sensitivity (RQ5)}\label{sec:RQ3}
To consider this research question, we restrict ourselves to a setting where we know that strong statistically significant ($p\ll 0.001$) improvements in online metrics are observed for the target policies $\{\mathcal{G}_{t},\mathcal{R}_{t}\}$ over the logging policies $\{\mathcal{G}_{0},\mathcal{R}_{0}\}$.
We restrict our analysis to the 2\textsuperscript{nd}-level feed, as it generates the majority of user-item interactions and relatively long sessions. 
We consider a variety of explicit and implicit feedback signals as rewards, all of which saw statistically significant improvements in the online experiment.
% Figure environment removed
We consider three possible notions of \emph{alignment} between on- and offline metrics, in increasing levels of expressivity: 
\begin{enumerate*}
    \item Without considering statistical significance, do differences in offline metrics directionally align with differences in online metrics? (i.e. \emph{sign agreement}) 
    \item For statistically significant improvements (as validated by the online experiment with $p\ll 0.001$), does the offline metric show statistically significant improvements with $p < 0.01$? (i.e. \emph{True-Positive-Rate}, recall or sensitivity).
    \item For statistically significant improvements, what confidence does the offline metric have in the improvements? (i.e. the $p$-values).
\end{enumerate*}
We consider 5 days of a deployed online experiment and 4 different reward signals, yielding 20 distinct statistically significant online metric improvements.
The purpose of our offline estimators, is to reflect these statistically significant \emph{online} differences in their \emph{offline} estimates. 
As offline estimators, we consider a clipped variant of the unbiased DCG metric in Eq.~\ref{eq:unbiased_reward}, where the factor for the inverse exposure propensity is replaced with $\min\left(m, \frac{1}{\varepsilon_{0}}\right)$, for varying values of $m$~\cite{Ionides2008,Gilotte2018}.
Although this renders the metric \emph{biased} in a pessimistic way~\cite{Jeunen2023Pessimism}, its reduced variance can yield more favourable performance as an offline evaluation metric.
For every variant of the DCG metric we construct in this way, we compute the analogous \emph{normalised} DCG metric.
Because these metrics are aggregated over trajectories, we can use their empirical means and standard deviations to construct normal confidence intervals for the metric values and their differences (i.e. the treatment effect).
If the 99\% confidence interval for the metric difference is strictly positive, we say the metric indicates a statistically significant improvement (with $p <0.01$).
Because all online metric differences we consider were statistically significant, we know that they are \emph{true positives}, and we can compute the \emph{sensitivity} or \emph{True-Positive-Rate} (TPR) for the offline metric by counting how often it indicates a statistically significant \emph{offline} difference for these true positives.
We additionally record the average $p$-value for the null hypothesis (i.e. the hypothesis that the metric difference is $\leq 0$), obtained from the confidence intervals.
To measure the weaker notion of \emph{sign agreement}, we only consider the mode of the confidence interval $\mu$, and count ``agreement'' ${\rm iff }~\mu > 0$.
We vary the clipping hyper-parameter for IPS as $m \in \{1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 2048, 4096, \inf\}$, where $m=1$ corresponds to directly using the interaction labels $C$, and $m=\inf$ yields an unbiased estimator with higher variance.

Reassuringly, \emph{all} offline estimators exhibit 100\% directional sign agreement with the true treatment effect we observe for serving personalised recommendations to users through $\{\mathcal{G}_{t},\mathcal{R}_{t}\}$ over $\{\mathcal{G}_{0},\mathcal{R}_{0}\}$.
Results for our sensitivity analysis are visualised in Figure~\ref{fig:sensitivity_results}.
In Figure~\ref{fig:sensitivity_TPR}, higher values indicate that the offline evaluation metric is more likely to detect statistically significant improvements in the online metric, averaged over the 20 settings described above.
Analogously, lower values in Figure~\ref{fig:sensitivity_P} indicate that the metric yields more statistical confidence.
Lower $p$-values in Figure~\ref{fig:sensitivity_P} additionally imply that the metric requires less data to achieve the significance level, potentially reducing costs~\cite{Kharitonov2017}. 
We observe that introducing IPS weighting (i.e. $m>1$) to account for position bias in the logged interactions leads to improved sensitivity.
This results holds for both DCG and nDCG, and both for the TPR metric and the average $p$-values.
We additionally observe that for a wide range of clipping values, the DCG metric has a higher TPR (lower $p$-values) than nDCG.
Intuitively, this can be explained by the fact that nDCG essentially squashes a metric with a high expressive range to the $[0,1]$ domain, which can only come at a cost of discriminative power.
DCG, on the other hand, directly models the online metrics we care about (under the assumptions laid out in Section~\ref{sec:dcg_unbiased}).

When we do not clip the IPS weights (i.e. $m=\inf$), we observe from Figure~\ref{fig:sensitivity_results} that the variance of the DCG metric increases to a point where its sensitivity is harmed (even if directional alignment is maintained).
Note that the nDCG metric is not affected by this, as its values are bounded and they exhibit lower variance as a result.
Intuitively, whereas low values of $\varepsilon_{0}$ can blow up the unbiased DCG formulation, they will also do this for \emph{ideal} DCG, and their ratio (i.e. nDCG) will be less likely to suffer from this.
We observe that with clipped propensities, even at large values, DCG leads to superior sensitivity over nDCG, striking a favourable bias-variance trade-off.

Our experimental results show promise in using DCG as an offline estimator of online reward, and the bias-variance trade-off that is clearly visualised in Figure~\ref{fig:sensitivity_P} helps us to tune this hyper-parameter $m$ properly.
Even when \emph{all} of the metric's underlying assumptions violated to \emph{some} degree, its value is apparent.