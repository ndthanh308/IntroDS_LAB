%\section{Beyond current assumptions: perspectives going forward}\label{sec:beyond}
\vspace{-2ex}\section{Perspectives going forward}\label{sec:beyond}
%In what follows, we revisit the assumptions that we have made in Section~\ref{sec:dcg_unbiased} to formally state \emph{when} the DCG metric can be considered an unbiased estimator of online reward, and hence, a suitable offline evaluation metric.
In what follows, we revisit the assumptions that are \emph{necessary} to consider the DCG metric an unbiased estimator of online reward.

%\paragraph{1. Reward independence across trajectories.}
\textit{1. Reward independence across trajectories} is necessary to avoid having to model any internal user \emph{state} that is influenced by actions taken by a ranking policy.
Indeed, if we do allow this to happen, we must resort to Reinforcement Learning (RL) formulations of our problem, which inhibits the simple form that DCG allows.
Nevertheless, unbiased evaluation of RL policies is an active research area, which has found applications in recommendation research~\cite{Chen2019}, also for two-stage policies (without considering rankings)~\cite{Ma2020}.
\citeauthor{Ie2019SlateQ} can provide inspiration for learnt RL policies in top-$n$ recommendation domains with DCG-like reward structures~\cite{Ie2019SlateQ}.

\textit{2. Position-based model (PBM).}
%\paragraph{2. Position-based model (PBM)}
The classical PBM allows for general formulations of $\mathsf{P}(V|R)$, including the widely adopted functional form $\frac{1}{\log_{2}(i+1)}$.
The recently proposed Contextual PBM~\cite{Fang2019} can be plugged into Eq.~\ref{eq:exposure} to directly provide an unbiased DCG formulation with a context-dependent discount function, enjoying the same theoretical guarantees we have derived for DCG under the PBM.
A variety of other click models~\cite{chuklin2015click} have been proposed in the research literature~\cite{Borisov2016,Chen2020_CACM}, as well as ways to evaluate them~\cite{Deffayet2022}.
We expect that our work provides a basis for further connections to be drawn between click models and unbiased evaluation metrics.

\textit{3. Reward independence across ranks.}
%\paragraph{3. Reward independence across ranks.}
When we do not assume any structure between the actions taken by the ranking policy and the observed rewards, the problem quickly becomes intractable, as we suffer from a combinatorial explosion of the action space.
This is a well-known problem, and the independence assumption has been adopted (either explicitly or implicitly) by a wide array of related work~\cite{Swaminathan2017,Ie2019SlateQ, Bendada2020,Jeunen2021B}.
Note that this assumption does not simply relate to \emph{observing} rewards, but to the underlying distribution of $Q$.
Indeed, related work that adopts a cascading user behaviour model also relies on this assumption, as the cascade relates to the distribution of $V$ (and thus $C$) rather than that of $Q$~\cite{McInerney2020, Kiyohara2022}.
Evaluation metrics have been proposed to encode concepts of listwise novelty and diversity into DCG-like formulations for top-$n$ recommendations~\cite{Clarke2008,Parapar2021}; we conjecture they can be extended to our unbiased setup as well.

%\paragraph{4. Examination hypothesis.}
\textit{4. The examination hypothesis} implies that \emph{exposure} bias (through $V$) is the main culprit that makes $C$ a noisy indicator of $Q$.
Differences in exposure can then purely come from \emph{position} bias (as in the PBM), but they can also be perpetuated by \emph{selection} bias (as made evident by Eq.~\ref{eq:exposure})~\cite{Diaz2020,Jeunen2021B}.
Other sources of bias have been raised in the literature, such as presentation or trust bias~\cite{Agarwal2019TrustBias,Vardasbi2020}.
We expect that these types of biases can be incorporated into the theoretical derivation of Sec.~\ref{sec:dcg_unbiased} to devise DCG-like formulations that remain unbiased estimators of online reward, even when additional biases are present.
Naturally, such biases are use-case-specific.

\textit{5. Full support of the logging policy.}
%\paragraph{5. Full support of the logging policy.}
The main assumption that makes IPS work, is that no actions with non-zero probability under the target policy can have zero probability under the logging policy.
Indeed, if a context-action pair is known not to be present in the data, we cannot make any inferences about its reward (with guarantees).
This is at the heart of policy-based estimation, but especially problematic in real-world systems where the action space is large and the cost of such \emph{full} randomisation, even with small probabilities, can be high.
Recent work in \emph{learning} from bandit feedback deals with such cases empirically~\cite{JeunenKDD2020,Jeunen2021A} and theoretically~\cite{Sachdeva2020, Lopez2021}, providing a source of inspiration to (partially) alleviate these issues.