\section{Formalising the problem setting}\label{sec:problemsetting}
Throughout, we represent the domain for a random variable $X$ as $\mathcal{X}$ and a specific instantiation as $x$, unless explicitly mentioned otherwise.
We deal with a session-based feed recommendation setup, describing a user's journey on the platform as a trajectory $\tau$.
Contextual features describing a trajectory are encoded in $x \in \mathcal{X}$, which includes features describing the user $u \in \mathcal{U}$ and possible historical interactions they have had with items on the platform.
In line with common notation in the decision-making literature, we will refer to these items as \emph{actions} $a \in \mathcal{A}$.
As is common in real-world systems, the size of the item catalogue (i.e. the action space $|\mathcal{A}|$) can easily grow to be in the order of  hundreds of millions, prohibiting us to score and rank the entire catalogue directly.
This is typically dealt with through a two-stage ranking setup, where a more lightweight \emph{candidate generator} stage is followed by a \emph{ranking} stage that decides the final personalised order in which we present items to the user~\cite{VanDang2013, Covington2016, Ma2020}.

We adopt generalised probabilistic notation for two-stage rankers in this work, but stress that our insights are model-agnostic and directly applicable to single-stage rankers as well. % !!!

Let $\mathcal{A}^{k}$ denote all subsets of $k$ actions: $\mathcal{A}^{k} \subseteq 2^{\mathcal{A}}: \forall a^{k} \in \mathcal{A}^{k}, |a^{k}|=k$.
A candidate generation policy $\mathcal{G}$ defines a conditional probability distribution over such sets of \emph{candidate} actions, given a context:
%\begin{equation}
$
    \mathcal{G}(A^{k}|X)\coloneqq\mathsf{P}(A^{k}|X,\mathcal{G}).
$
%\end{equation}
We will use the shorthand notation $\mathcal{G}(X)$ when context allows it.
Note that this general notation subsumes other common scenarios, such as candidate generation policies that are deterministic, consist of ensembles, or simply yield the entire item catalogue $\mathcal{A}$ (i.e. \emph{single-stage} ranking systems).

After obtaining a set of candidate items for context $x$ by sampling $a^{k} \sim \mathcal{G}(x)$, we pass them on to the ranking stage.

In line with our probabilistic framework, we have a ranking policy $\mathcal{R}$ that defines a conditional probability distribution over rankings (i.e. permutations of $a^{k}$).
Define $\bm{\sigma}$ as a possible permutation over $A^{k}$.
Formally, we have:
%\begin{equation}
$
%    \mathcal{R}(S(A^{k})|A^{k}, X)\coloneqq\mathsf{P}(S(A^{k})|A^{k}, X,\mathcal{R}).
    \mathcal{R}(\bm{\sigma}|A^{k}, X)\coloneqq\mathsf{P}(\bm{\sigma}|A^{k}, X,\mathcal{R}).
$
%\end{equation}
When context allows it, we will use shorthand notation $\mathcal{R}(X)$ to absorb the candidate generation step. Then, the ranker is given by: 
\begin{equation}
%    \mathcal{R}(X) \coloneqq \sum_{a^{k} \in \mathcal{A}^{k}}\mathsf{P}(S(a^{k})|A^{k}=a^{k}, X,\mathcal{R})\mathsf{P}(A^{k}=a^{k}|X,\mathcal{G}).
    \mathcal{R}(X) \coloneqq \sum_{a^{k} \in \mathcal{A}^{k}}\mathsf{P}(\bm{\sigma}|A^{k}=a^{k}, X,\mathcal{R})\mathsf{P}(A^{k}=a^{k}|X,\mathcal{G}).
\end{equation}

Now, for a given context $x$, we can obtain rankings over actions by sampling $\sigma=(a_{1},\ldots,a_{k}) \sim \mathcal{R}(x)$.
These rankings are then presented to users, who scroll through the ordered list of items and \emph{view} them along the way.
We will not \emph{yet} restrict our setup to a specific user model that describes \emph{how} users interact with rankings, but introduce a binary random variable $V$ to indicate whether a user has viewed a given item.\footnote{Note that this problem setting deviates from traditional work dealing with web search in IR, where multiple items are shown on screen and item-specific view events cannot be disentangled trivially.
In contrast, our items take up most of the user's mobile screen when presented, and we are able to deduce accurate item-level view labels from logged scrolling behaviour. See e.g.~\citet{Jeunen2023_C3PO} for work dealing with this setting.}
As such, we will assume that logged trajectories only contain items that were viewed by the user (i.e. $V=1$).
That is, if the user abandons the feed after action $a_{i}$ shown at rank $R=i$, we do not log samples for actions $a_{j}, \forall j>i$.

Users can not only view items, but they can interact with them in several ways.
These interaction signals could be seen as the \emph{reward} or \emph{(relevance) label}.
Following traditional notation where rewards are \emph{clicks} we will denote them by random variable $C$.
Nevertheless, these signals are general and can be binary (e.g. likes), real-valued (e.g. revenue), or higher-dimensional to support multiple objectives (e.g. diversity, satisfaction, and fairness~\cite{Mehrotra2018,Mehrotra2020}).

As users can interact with every item separately, we define $C^{k} = (c_1, \ldots, c_{k})$ as the logged rewards over all ranks.
We do not place any restrictions on the reward distribution yet, so the reward at any given rank can be dependent on actions at other ranks: $(c_{1}, \ldots, c_{k})\sim\mathsf{P}(C^{k}|X,A_{1}, \ldots, A_{k})$.
Now, a trajectory consists of contextual information, as well as a sequence of user-item interactions with observed reward labels: $\tau = \{x_{\tau}, (a_1,c_{1}),\ldots(a_{t},c_{t})\}$, where $|\tau|=t$.
The true metric of interest that we care about is the expectation of reward, over contexts sampled from an unknown marginal distribution $x\sim\mathsf{P}(X)$, candidates sampled from our candidate generator $a^{k} \sim \mathcal{G}(x)$, rankings sampled from our ranker $(a_{1},\ldots,a_{k})\sim\mathcal{R}(a^{k},x)$, and rewards sampled from the unknown reward distribution $(c_{1}, \ldots, c_{k})\sim\mathsf{P}(C^{k}|X=x,A_{1}=a_{1}, \ldots, A_{k}=a_{k})$.
We will denote with $C$ the sum of rewards over all observed ranks: $C = \sum_{i=1}^{|\tau|}c_{i}$.
Note that this notation generalises typical evaluation metrics that are used in real-world recommender systems, such as per-item dwell-time or counters of engagement signals.

In order to obtain an estimate for our true metric of interest $\mathbb{E}[C]$, we can perform an online experiment.
Indeed, in doing so, we effectively sample from the above-mentioned distributions and obtain an empirical estimate of our metric by averaging observed samples.
For a dataset $\mathcal{D}_{0}$ containing logged interactions under policies $\mathcal{G}_{0}$ and $\mathcal{R}_{0}$, Eq.~\ref{eq:online_exp} shows how to obtain this empirical estimate:
\begin{equation}\label{eq:online_exp}
    \mathop{\mathbb{E}}\limits_{(a_{1},\ldots,a_{k}) \sim \mathcal{R}_{0}(x)}[C] \approx \frac{1}{|\mathcal{D}_{0}|\cdot|\tau|}\sum_{\tau \in \mathcal{D}_{0}}\sum_{i=1}^{|\tau|}c_{i}.
\end{equation}
As we directly measure the quantity of interest that depends on the deployed policies $\mathcal{G}_{0}$ and $\mathcal{R}_{0}$, this online estimator is often seen as the gold standard.
Nevertheless, it is costly to obtain.
Indeed, as has been widely reported~\cite{Jeunen2019DS,Gilotte2018,Larsen2023}, online experiments require us to:
\begin{enumerate*}
    \item bring hypotheses for new policies up to production standards for an initial test,
    \item wait several days or weeks to deduce statistical significant improvements, and
    \item possibly harm user experience when the policies are performing subpar.
\end{enumerate*}
Furthermore, several pitfalls arise when setting up or interpreting results from online A/B experiments~\cite{Kohavi2022,Jeunen2023_misassumption}.

For these reasons, we want to be able to perform accurate offline experiments.
That is, given a dataset of interactions $\mathcal{D}_{0}$ that were logged under the production policies $\mathcal{G}_{0}$ and $\mathcal{R}_{0}$ (often referred to as \emph{logging} policies), we want to estimate what the reward \emph{would have been} if we had deployed a new policy $\mathcal{R}$ instead (assume $\mathcal{R}$ includes sampling candidates from $\mathcal{G}$).
The goal at hand is thus to devise some function $f$ that takes in a dataset of interactions collected under the logging policy, and is able to approximate the ground truth metric for a target policy $\mathcal{R}$, as shown in Eq.~\ref{eq:offline_exp}:
\begin{equation}\label{eq:offline_exp}
    \mathop{\mathbb{E}}\limits_{(a_{1},\ldots,a_{k}) \sim \mathcal{R}(x)}[C] \stackrel{?}{\approx} f(\mathcal{D}_{0},\mathcal{R}).
\end{equation}