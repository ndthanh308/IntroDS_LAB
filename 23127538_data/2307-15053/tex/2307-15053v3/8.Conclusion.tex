%\vspace{-4ex}
\vspace{-4ex}\section{Conclusions \& Outlook}\label{sec:conclusion}
Offline evaluation of recommender systems is a common task, and known to be problematic.
This work investigates the commonly used (normalised) discounted cumulative gain metric and its uses in the research literature.
Specifically, we have investigated \emph{when} we can expect such metrics to approximate the gold standard outcome of an online experiment.
In a counterfactual estimation framework, we formally derived the necessary assumptions to consider DCG an unbiased estimator of online reward.
Whilst it is reassuring that such assumptions exist and we can directly map DCG to online metrics --- we also highlighted how this \emph{ideal} use deviates from the traditional uses of the metric in IR, and how it often appears in the research literature.
We then shifted our focus to \emph{normalised} DCG, and demonstrated its \emph{inconsistency}, both theoretically and empirically with reproducible experiments.
Indeed, even when all neccesary assumptions hold and DCG provides unbiased estimates of online reward, nDCG \emph{cannot} be used to rank competing models, as it does does \emph{not} preserve the rankings we would obtain from DCG.\looseness=-1

Through a correlation analysis between results obtained from off- and on-line experiments on a large-scale recommendation platform, we show that our \emph{unbiased} DCG estimates strongly correlate with online metrics in a real-world use-case. %even when some of the metric's underlying assumptions are violated.
Additionally, we show how the offline metric can be used to detect statistically significant online improvements with high sensitivity, further highlighting its promise for offline evaluation in both academia and industry.
Normalised DCG, on the other hand, suffers from a weaker correlation with online results, and lower sensitivity than DCG.
These results suggest that nDCG's practical utility may be limited.

We believe our work opens up interesting areas for future research, where our theoretical framework can be extended to formally assess the assumptions required by other commonly used evaluation metrics in the field.
Furthermore, theoretical and empirical connections between other types of commonly used online evaluation metrics (e.g. user retention) would be fruitful.