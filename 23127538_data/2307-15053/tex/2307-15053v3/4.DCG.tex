\section{Discounted Cumulative Gain as an unbiased offline evaluation metric}\label{sec:dcg_unbiased}
In reality, this problem can be very complex.
Indeed, the reward that we obtain from presenting a certain ranking to a user can depend on the entire slate at once (of which there are $n!$ versions in a top-$n$ setting), and will be non-i.i.d. over trajectories (i.e. the reward distribution can depend on a user's state, influenced by actions we have taken in the past).

As is typical in machine learning research, we require assumptions that make the problem more tractable.
These assumptions are flawed, but they give us a starting point and a strong foundation to build upon for future iterations.
Note that we will lay out the \emph{specific} assumptions that are \emph{necessary} to motivate the use of Discounted Cumulative Gain as an offline evaluation metric---we discuss ways of relaxing these assumptions in Section~\ref{sec:beyond}.

\begin{assumption}[reward independence across trajectories]\label{ass:no_RL}
The reward for a context-action pair $(X,A_{i})$ in trajectory $\tau$ is independent of the rankings presented in other trajectories $\tau^{\prime} \in \mathcal{D}_{\setminus \tau}$.
\end{assumption}

\begin{assumption}[position-based model~\cite{Craswell2008}]\label{ass:pbm}
We follow the position-based model (PBM) to describe user scrolling behaviour, implying that the probability of a user viewing an item is only dependent on its rank and described by $\mathsf{P}(V|R)$.
\end{assumption}

\begin{assumption}[reward independence across ranks]\label{ass:no_slate}
The reward for a context-action pair $(X,A_{i})$ is independent of other actions in the user trajectory.
Formally, $C_{i} \perp \!\!\! \perp A_{j} | X, A_{i} \forall j \neq i$.
We describe the resulting reward distribution as $\mathsf{P}(C_{i}|X,A_{i},R)$.
\end{assumption}

\begin{assumption}[examination hypothesis~\cite{Craswell2008}]\label{ass:exam}
The reward for an action $A$ shown at rank $R$ is dependent on its inherent quality (unobserved random variable $Q$), and whether it was viewed $V$.
These quantities relate as:
\begin{equation}\label{eq:quality}
\begin{gathered}
    \mathsf{P}(C|X,A,R) = \mathsf{P}(Q|X,A) \cdot \mathsf{P}(V|R),\\
    {\text{ which implies }} \mathsf{P}(Q|X,A)=\frac{\mathsf{P}(C|X,A,R)}{\mathsf{P}(V|R)}.
\end{gathered}
\end{equation}
\end{assumption}
Asm.~\ref{ass:no_RL} allows us to avoid reinforcement learning scenarios, and Asm.~\ref{ass:pbm} prohibits cascading behaviour (that would give rise to other metrics, such as ERR~\cite{Chapelle2009}).
Asm.~\ref{ass:no_slate} allows us to avoid modelling entire slates as individual actions (leading to a combinatorial explosion of the action space), and through Asm.~\ref{ass:exam}, Eq.~\ref{eq:quality} estimates the unobserved context-dependent \emph{quality} of a given item from observable quantities alone.
From Eq.~\ref{eq:online_exp}, we can now rewrite the expected reward we obtain under a ranking policy $\mathcal{R}$ as:
%\begin{equation}\label{eq:dcg_subtle}
%\begin{gathered}
\begin{flalign}\label{eq:dcg_subtle}
  \begin{aligned}
    \mathop{\mathbb{E}}\limits_{(a_{1},\ldots,a_{k}) \sim \mathcal{R}(x)}[C] &\approx \\
    \frac{1}{|\mathcal{D}_{0}|\cdot|\tau|}\sum_{\tau \in \mathcal{D}_{0}}\sum_{i=1}^{|\tau|}&\mathsf{P}(Q=1|X=x_{\tau},A=a_{i})\cdot\mathsf{P}(V=1|R=i).    
\end{aligned}
\end{flalign}
%\end{gathered}
%\end{equation}
Through the position-based model and the examination hypothesis, items shown at lower ranks are \emph{discounted}.
Because we assumed independence of rewards across ranks, rewards observed at different ranks are \emph{cumulative}.
A key insight here is that the ranking policy $\mathcal{R}$ only affects the rank $i$ at which an item is shown, and as such, the exposure probability that is allocated to the item $a_{i}$.
We formalise \emph{exposure} as the expected number of views a target item $a^{\prime}$ will obtain under a given context $x$, for candidate generation and ranking policies $\mathcal{G},\mathcal{R}$:
\vspace{-3ex}
%\begin{equation}
\begin{flalign}\label{eq:exposure}
\begin{gathered}
\mathop{\mathbb{E}}\limits_{\mathcal{G},\mathcal{R}}[V|X=x,A=a^{\prime}] = \sum\limits_{A^{k} \in \mathcal{A}^{k}} \Bigg( \mathcal{G}\left(A^{k}|X=x\right)\cdot\Bigg.\\
\Bigg.\hspace{-5ex}\sum\limits_{(a_{1},\ldots,a_{k}) \in S(A^{k})} \hspace{-5ex}\mathcal{R}\left((a_{1},\ldots,a_{k})|A^{k}, X=x\right) \sum\limits_{i=1}^{k} \mathsf{P}(V=1|R=i) \cdot\mathbb{1}_{\{a_i=a^{\prime}\}}\Bigg).
\vspace{-3ex}
\end{gathered}
\end{flalign}
%\end{equation}
Note that this general notation accommodates recommendation scenarios where we retrieve and rank $k$ candidates but only show the top-$n$ to the user, where $k>n$, by simply defining $\mathsf{P}(V=1|R=j)=0 \forall j=n+1,\ldots,k$.
By encoding cut-offs directly in the position bias model, we forgo the need to consider metrics like DCG@$n$~\cite{Valcarce2020}.

Recent work in unbiased learning-to-rank leverages similar exposure definitions to jointly combat selection and position bias through Inverse Propensity Score (IPS) weighting~\cite{Oosterhuis2020,Gupta2023}.
\begin{assumption}[full support of the logging policy~\cite{Owen2013}]\label{ass:full_support}
Given context $x$, any item that would be assigned non-zero exposure under the target policies $(\mathcal{G}, \mathcal{R})$ has non-zero exposure under the logging policies $(\mathcal{G}_{0}, \mathcal{R}_{0})$:
\begin{equation}
   \forall a \in \mathcal{A}: \mathop{\mathbb{E}}\limits_{\mathcal{G},\mathcal{R}}[V|X=x,A=a] > 0\Rightarrow \mathop{\mathbb{E}}\limits_{\mathcal{G}_{0},\mathcal{R}_{0}}[V|X=x,A=a] > 0.
\end{equation}
\end{assumption}
Through Asm.~\ref{ass:full_support} and Eq.~\ref{eq:exposure}, we can now formulate an \emph{importance sampling} estimator for the reward under $(\mathcal{G},\mathcal{R})$ given data collected under $(\mathcal{G}_{0}, \mathcal{R}_{0})$.
Let $\varepsilon(x,a)\coloneqq\mathbb{E}_{\mathcal{G},\mathcal{R}}[V|X=x,A=a]$ and $\varepsilon_{0}(x,a)\coloneqq\mathbb{E}_{\mathcal{G}_{0},\mathcal{R}_{0}}[V|X=x,A=a]$.
\begin{equation}\label{eq:unbiased_reward}
\mathop{\mathbb{E}}\limits_{\mathcal{G},\mathcal{R}}[C] = \mathop{\mathbb{E}}\limits_{\mathcal{G}_{0},\mathcal{R}_{0}}\left[C \cdot \frac{\varepsilon}{\varepsilon_{0}}\right] \approx \frac{1}{|\mathcal{D}_{0}|\cdot|\tau|}\sum_{\tau \in \mathcal{D}_{0}}\sum_{i=1}^{|\tau|} c_{i} \cdot \frac{\varepsilon(x_{\tau},a_{i})}{\varepsilon_{0}(x_{\tau},a_{i})}.
\end{equation}
Eq.~\ref{eq:unbiased_reward} provides a general unbiased estimator for the reward under $(\mathcal{G}, \mathcal{R})$, computed from data collected under $(\mathcal{G}_{0}, \mathcal{R}_{0})$.\footnote{Note that, for general two-stage ranking scenarios, this is a novel contribution to the research literature in and of itself. Existing work on off-policy corrections in two-stage recommender systems only considers a top-$1$ scenario, instead of a ranking policy~\cite{Ma2020}.}
For simplicity of notation, but without loss of generality, we now restrict ourselves to a deterministic ranking policy $\mathcal{R}$ and assume $\mathcal{G}$ is constant (i.e. $\mathcal{G}\equiv\mathcal{G}_{0}$).
With a slight abuse of notation, we briefly denote with $\mathcal{R}(x,a)$ the \emph{rank} at which item $a$ is placed when policy $\mathcal{R}$ is presented with context $x$.
In doing so, we observe that the importance weights in Eq.~\ref{eq:unbiased_reward} can be simplified to:
%\begin{equation}
$ 
    \frac{\varepsilon(x,a)}{\varepsilon_{0}(x,a)}=\frac{\mathsf{P}(V=1|R=\mathcal{R}(x,a))}{\mathsf{P}(V=1|R=\mathcal{R}_{0}(x,a))}.$  
%\end{equation}
We stress again that we simply adopt this view for simplified notation, but that the derivation holds for general \emph{stochastic} two- or single-stage ranking systems alike.

Recall from Eq.~\ref{eq:quality} that $\mathsf{P}(Q|X,A)=\frac{\mathsf{P}(C|X,A,R)}{\mathsf{P}(V|R)}$.

Now, we can formally describe the \emph{discounted cumulative gain} (DCG) metric as an importance sampling estimator:
\begin{flalign}\label{eq:formal_DCG}
\begin{aligned}
    &\mathop{\mathbb{E}}\limits_{(a_{1},\ldots,a_{k}) \sim \mathcal{R}(x)}[C] \approx f_{\rm DCG}(\mathcal{D}_{0},\mathcal{R})\\
     &= \frac{1}{|\mathcal{D}_{0}|\cdot|\tau|}\sum_{\tau \in \mathcal{D}_{0}}\sum_{i=1}^{|\tau|} c_{i}\cdot\frac{\mathsf{P}\left(V=1|R=\mathcal{R}(x_{i},a_{i})\right)}{\mathsf{P}(V=1|R=i)} \\
     &\approx \frac{1}{|\mathcal{D}_{0}|\cdot|\tau|}\sum_{\tau \in \mathcal{D}_{0}}\sum_{i=1}^{|\tau|}  \mathsf{P}(Q|X=x_{\tau},A=a_{i})\cdot\mathsf{P}(V=1|R=\mathcal{R}(x_{i},a_{i})).    
\end{aligned}
\end{flalign}
Through this derivation, two different views of the DCG metric arise.
That is, we either
\begin{enumerate*}
\item view it as a pure importance sampling estimator that reweights the exposure that is allocated to a certain item in a certain context, or
\item we view it as a way to de-bias observed interactions (i.e. estimate $Q$ from $C$ and $V$), and use the position-based model (Asm.~\ref{ass:pbm}) and the examination hypothesis (Asm.~\ref{ass:exam}) to obtain a final estimate of the cumulative reward.
\end{enumerate*}

If the assumptions laid out above hold, Eq.~\ref{eq:formal_DCG} provides an unbiased estimate of the online reward policy $\mathcal{R}$ will incur, based on data collected under $\mathcal{R}_{0}$; providing a strong motivation for DCG.
Even though unbiasedness is an attractive theoretical property, this estimator's variance can become problematic in cases where the logging and target policies $(\mathcal{R}_{0},\mathcal{R})$ diverge. 
We can adopt methods that were originally proposed to strike a balance between bias and variance for general IPS-based estimators, such as clipping the weights~\cite{Ionides2008,Gilotte2018}, self-normalising them~\cite{Swaminathan2015snips}, adapting the logging policy~\cite{Tucker2023}, or extending the estimator with a reward model to enable doubly robust estimation~\cite{Dudik2011,Kiyohara2022,Oosterhuis2023}.
Similarly, when the logged data that is used for offline evaluation was collected by multiple logging policies, ideas from ``\emph{multiple importance sampling}''~\cite{Elvira2019} are effective at reducing the variance of the final estimator~\cite{Agarwal2017,Kallus2021Optimal}.
\citeauthor{Saito2023_ICML} describe extensions for large action spaces~\cite{Saito2022_ICML,Saito2023_ICML}.

In the traditional IR use-case of web search, it is often assumed that we have access to human-annotated relevance labels ${\rm rel}(q,d)$ for query-document pairs $(q,d)$.
Such crowdsourced labels are seen as a proxy to $\mathsf{P}(Q|X,A)$, which makes them understandably attractive.
Nevertheless, for an offline evaluation metric to be useful in real-world recommendation systems, access to direct relevance labels is seldom a realistic requirement.
The \emph{discount} function for DCG that is most often used in practice, makes the assumption that $\mathsf{P}(V=1|R=i)=\frac{1}{\log_{2}(i+1)}$ is a good approximation for empirical exposure (see, e.g.,~\cite{Chapelle2009}).
This gives rise to the more widely recognisable form of DCG, as $\sum_{i=1}^{k}\frac{{\rm rel}(q,d_{i})}{\log_{2}(i+1)}$.
We note that neither one of these additional assumptions is likely to hold in real-world applications, which imply that even if Assumptions~\ref{ass:no_RL}--\ref{ass:full_support} hold, this estimator is \emph{biased}.
The more general form presented in Eq.~\ref{eq:formal_DCG}, however, is supported by a theoretical framework that allows for counterfactual evaluation, formally describing settings for its appropriate use.