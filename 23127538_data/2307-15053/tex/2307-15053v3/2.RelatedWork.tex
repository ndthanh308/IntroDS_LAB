\section{Background \& Related Work}\label{sec:relwork}
Offline evaluation methods for recommender systems have been studied for decades~\cite{Herlocker2004}, and their shortcomings are widely reported.
\citeauthor{Canamares2020} provide an overview of common approaches, highlighting how different choices (in pre-processing, metrics, splits,...) lead to contrasting results~\cite{Canamares2020}.
More problematic, \citeauthor{Ji2023} show that common train-test-split procedures lead to data leakage issues that affect conclusions drawn from offline experiments~\cite{Ji2023}.
Other recent work shows that sampled versions of evaluation metrics that only rank a sample of the item catalogue instead of the full catalogue, are inconsistent with the full metrics, leading the authors to explicitly discourage their use~\cite{Krichene2020,Li2020,Canamares2020RecSys}.
Even when we manage to steer clear from these pitfalls, biases in logged data can give rise to undesirable phenomena like Simpson's paradox~\cite{Jadidinejad2021,Jeunen2023_CONSEQUENCES}.
General de-biasing procedures have been proposed to this end~\cite{LYang2018}, as well as \emph{off-policy} estimation techniques~\cite{su2019cab, Su2020_ICML, Ma2020, Kiyohara2022} and methods to evaluate competing estimators~\cite{Saito2021_Robustness,Su2020_Adaptive,Udagawa2022}.

Most traditional ranking evaluation metrics stem from IR.
\citeauthor{Valcarce2020} find that nDCG offers the best discriminative power among them~\cite{Valcarce2020}.
Findings like this reinforce the community's trust in nDCG, and it is commonly used to compare novel top-$n$ recommendation methods to the state-of-the-art, also in reproducibility studies~\cite{FerrariDacrema2019,FerrariDacrema2021,Rendle2020,Rendle2022}.
\citeauthor{Ferrante2021} argue that while nDCG can be preferable because it is bounded and normalised, problems can arise because the metric is not easily transformed to an interval scale~\cite{Ferrante2021}.
They all do not consider the consistency of (n)DCG.

Other recent work highlights that commonly used online evaluation metrics relying on \emph{experimental} data (e.g. click-through rate), differ fundamentally from commonly used offline evaluation metrics that rely on \emph{organic} interactions (e.g. hit-rate)~\cite{Jeunen2019DS,Jeunen2019REVEAL_EVAL}.
\citeauthor{Deffayet2023} argue that a similar mismatch is especially pervasive when considering reinforcement learning methods~\cite{Deffayet2023}, and \citeauthor{Diaz2021} emphasises critical issues with interpreting organic implicit feedback as a user preference signal~\cite{Diaz2021}.
These works together indicate a rift between online and offline experiments.

Several open-source simulation environments have been proposed as a way to bypass the need for an online ground truth result~\cite{rohde2018recogym,Ie2019Recsim,Saito2021OBP}, and several works have leveraged these simulators to empirically validate algorithmic advances in bandit learning for recommendation~\cite{JeunenKDD2020,Jeunen2021A,Jeunen2021B,Sakhi2020,Bendada2020}.
Nevertheless, whether conclusions drawn from simulation results accurately reflect those drawn from real-world experiments, is still an open research question.

In this work, we focus on the \emph{core} purpose that offline evaluation metrics serve: to give rise to offline evaluation methodologies that accurately mimic the outcome of an online experiment.
To this end, we focus on the widely used (n)DCG metric, and aim to take a step towards closing the gap between the off- and online paradigms.