%\vspace{-3ex}
\begin{table*}[!t]
%\vspace{-4ex}
\setlength{\fboxrule}{1pt}
\begin{minipage}{0.65\textwidth}
\begin{flushleft}
    \begin{tabular}{cccccccc}
    \toprule
    \textbf{Top-1 Model} & \textbf{DCG}($x_{1}$) & \textbf{DCG}($x_{2}$) & \textbf{nDCG}($x_{1}$) & \textbf{nDCG}($x_{2}$) &~& \textbf{DCG}($\mathbf{X}$) & \textbf{nDCG}($\mathbf{X}$) \\
    \midrule
    $\mathcal{R}(x) = a_{1}$ & 1.00 & 1.00 & 1.00 & 0.29 &~& 1.00 & \fcolorbox{Maroon}{white}{0.64}\\
    $\mathcal{R}^{\prime}(x) = a_{2}$ & 0.00 & 2.50 & 0.00 & 0.71 &~& \fcolorbox{PineGreen}{white}{1.25} & 0.36\\
    \bottomrule
    \end{tabular}
\end{flushleft}
\end{minipage}
\hspace{2ex}
\begin{minipage}{0.32\textwidth}
\begin{flushright}
    ~\\%~\\
where $\quad$ $\mathbb{E}[Q|X=x_{1},A=a_{1}] = 1.0$,\\
    $\mathbb{E}[Q|X=x_{1},A=a_{2}] = 0.0$,\\
    $\mathbb{E}[Q|X=x_{2},A=a_{1}] = 1.0$,\\
    $\mathbb{E}[Q|X=x_{2},A=a_{2}] = 2.5$,\\
    ~\hfill~$\textbf{X} = \{x_{1}, x_{2}\}$.
\end{flushright}
\end{minipage}
%\vspace{-2ex}
\caption{A proof by example that, while rankings inferred from the DCG and nDCG metrics are consistent for a \emph{single sample}, they can be \emph{inconsistent} when aggregated over multiple samples (i.e. ${\rm DCG}(\bm{X}, \mathcal{R}^{\prime}) > {\rm DCG}(\bm{X},\mathcal{R}) \nRightarrow {\rm nDCG}(\bm{X},\mathcal{R}^{\prime}) > {\rm nDCG}(\bm{X},\mathcal{R}) $).}\label{tab:proof}
%\vspace{-4ex}
\end{table*}

\section{Normalising DCG is Inconsistent}\label{sec:ndcg_inconsistent}
We have introduced the DCG metric from first principles, and have shown that under several assumptions, it can be seen as an unbiased estimator for the reward that a new ranking policy $\mathcal{R}$ will obtain. 
Nevertheless, this metric seldom appears in the research literature in its unadulterated form.
Much more prevalent is \emph{normalised} DCG (nDCG), which rescales the DCG metric to be at most $1$ for an ``ideal'' ranking.
For notational simplicity, we define ground truth quality labels as $\rho(x) = \left[ \mathbb{E}[Q|X=x,A=a] \forall a \in \mathcal{A}\right]$.
Then, we can compute \emph{ideal} DCG as the DCG obtained under an oracle ranker that yields $\mathcal{R}^{\star}\coloneqq{\texttt{sort}}(-\rho(x))$.
In what follows, we first show that this is reasonable practice under a single sample (context), in that it retains a \emph{consistent} ordering among ranking policies.
We then go on to show that:
\begin{enumerate*}
    \item nDCG yields \emph{inconsistent} orderings over competing policies in expectation when compared to DCG, and
    \item defining iDCG is problematic in the realistic setting of partial information (i.e. $Q$ and thus $\rho(x)$ are unobservable).
\end{enumerate*}



\begin{lemma}\label{lem:consistent_single}
The Discounted Cumulative Gain (DCG) and Normalised Discounted Cumulative Gain (nDCG) metrics yield consistent relative orders over a competing set of policies $\Omega$ that are being evaluated \textbf{for a single sample $\bm{x}$}.
That is, $$\mathop{\argsort}\limits_{\mathcal{R} \in \Omega} f_{\rm DCG}(x,\mathcal{R}) \equiv \mathop{\argsort}\limits_{\mathcal{R} \in \Omega} f_{\rm nDCG}(x,\mathcal{R}), \forall x \in \mathcal{X}.$$
\end{lemma}
\begin{proof}
For any given context $x \in \mathcal{X}$, assume a relative order exists between two methods $\mathcal{R}$ and $\mathcal{R}^{\prime}$ for a given non-negative metric $f$: i.e. $f(x,\mathcal{R}) \geq f(x,\mathcal{R}^{\prime})$.
Define $f^{\star}(x)$ as the \emph{ideal} metric value, i.e. the metric value that is obtained by the optimal ranking:
$f^{\star}(x) \coloneqq f(x, \mathcal{R}^{\star})$, where $\mathcal{R}^{\star} = \mathop{\argmax}\limits_{\mathcal{R}} f(x, \mathcal{R}) = {\texttt{sort}}(-\rho(x))$.

Because $f$ is a non-negative metric, $f^{\star}$ is non-negative, and we have that:
$$ f(x,\mathcal{R}) \geq f(x,\mathcal{R}^{\prime}) \Rightarrow  \frac{f(x,\mathcal{R})}{f^{\star}(x)} \geq \frac{f(x,\mathcal{R}^{\prime})}{f^{\star}(x)}. $$

DCG is, in general, not restricted to be non-negative.
However, if we assume that the \emph{ideal} discounted cumulative gain is non-negative, we have that the above inequality applies for $f \coloneqq f_{\rm DCG}$ and $f^{\star} \coloneqq f_{\rm iDCG}$.
\end{proof}

We believe that this (seemingly trivial) insight has led to the widespread adoption of nDCG as an offline evaluation metric in the recommender systems and Learning-to-Rank research fields, as normalised metric values where $1$ indicates a perfect model facilitate comparisons of methods over different datasets.
Indeed, when describing its prevalence in the literature, \citeauthor{Ferrante2021} argue that ``\emph{usually nDCG is preferred over DCG because it is bounded and normalised}''~\cite{Ferrante2021}.
Nevertheless, nDCG does \emph{not} retain consistent orderings with respect to DCG when the metrics are calculated over multiple samples and aggregated:

\begin{lemma}
The Discounted Cumulative Gain (DCG) and Normalised Discounted Cumulative Gain (nDCG) metrics yield \textbf{inconsistent} relative orders over a competing set of policies $\Omega$ that are being evaluated \textbf{over a set of samples $\bm{X}$}.
That is, $$\mathop{\argsort}\limits_{\mathcal{R} \in \Omega} f_{\rm DCG}(\bm{X},\mathcal{R}) \nequiv \mathop{\argsort}\limits_{\mathcal{R} \in \Omega} f_{\rm nDCG}(\bm{X},\mathcal{R}) \text{ in the general case}.$$
\end{lemma}

\begin{proof}
    We provide a proof by counterexample, for which the details are presented in Table~\ref{tab:proof}.
    Indeed, even though the $f_{\rm DCG}$ and $f_{\rm nDCG}$ metrics align for every sample in \emph{isolation} (see columns for $x_1, x_2$), they are inconsistent in \emph{aggregate} (see columns for $\bm{X}$).
\end{proof}

Discrepancies between (n)DCG have been touched upon in the IR literature, focused on search engine evaluation and blaming ``\emph{a limited number of relevance judgments}''~\cite{AlMaskari2007}.
Table~\ref{tab:proof} shows that the issue has deeper roots than this: the normalisation procedure is \emph{inconsistent}.
This insight is problematic, as virtually all offline evaluation protocols consist of first aggregating evaluation metrics over sets of samples, and then inferring preferences over competing policies based on these metric averages.
Our work shows that, when the assumptions laid out in Section~\ref{sec:dcg_unbiased} are met and DCG provides an unbiased estimate of reward, this gives rise to a theoretically sound model selection protocol.
The same statement does \emph{not} hold for nDCG as it widely appears in the research literature (see e.g. \cite[Eq. 8.9]{schutze2008introduction} and \cite[Eq. 5]{Valcarce2020}).
We hypothesise that the normalisation formula has been widely adopted for its ease-of-use, rather than for its theoretical properties.
This suggests that nDCG is of limited practical use as an offline evaluation metric, and that it should be avoided by researchers and practitioners who wish to use DCG for offline evaluation and model selection purposes.

We provide empirical evidence of discrepancies between (n)DCG on common top-$n$ recommendation evaluation tasks on publicly available data in Appendix~\ref{sec:appx}.

In the odd case where metric values that maximise at $1$ are required, we propose the use of a \emph{post}-normalisation procedure, where $f_{\rm pnDCG}(\mathcal{D},\mathcal{R}) = \frac{f_{\rm DCG}(\mathcal{D},\mathcal{R})}{f_{\rm iDCG}(\mathcal{D})}$.
Recent work on LTR in IR leverages this nDCG formulation~\cite{Oosterhuis2022}.
Indeed, through Lemma~\ref{lem:consistent_single}, one can trivially show that this metric \emph{is} consistent with respect to $f_{\rm DCG}$.

Nevertheless, we wish to advise against this practice altogether, as computing the \emph{ideal} DCG metric implies that we must construct the \emph{ideal} ranking policy $\mathcal{R}^{\star}$.
To do so, we require full knowledge of $\rho(x)$, which is hardly realistic in real-world scenarios.
In traditional IR use-cases where human-annotated relevance labels are available as \emph{ground truth}, these labels can be used to inform $\rho(x)$.
In academic recommendation datasets where we have \emph{explicit} feedback and \emph{full observability} of the user-item matrix, this can inform $\rho(x)$ similarly.
In practical applications, however, we typically estimate $\widehat{\rho}(x) \approx \rho(x)$ from logged implicit feedback.
Aside from the problems that occur when accurately interpreting this feedback~\cite{Diaz2021}, such logged datasets are known to be riddled with biases that complicate estimating $\widehat{\rho}(x)$~\cite{Jeunen2021A}, resulting in only partial observability and noisy estimates that should \emph{not} be taken at face value to inform an ``\emph{optimal}'' ranker.\looseness=-1

One final argument in favour of nDCG, is that it partially alleviates the impact of outliers.
Indeed, the normalisation procedure rescales the contribution of every sample, which can be preferable in cases where strong outliers are present.
Nevertheless, in such scenarios, we would propose to \emph{first} devise a more appropriate online metric than the average cumulative reward, and \emph{then} derive an offline estimator for this quantity, rather than trying to repurpose the existing DCG estimator.
Exactly what such metrics and estimators would look like, is an interesting area for future work.

