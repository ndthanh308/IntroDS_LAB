% Figure environment removed

\section{Empirical Evidence of (n)DCG Inconsistency on Public Data}\label{sec:appx}
%\paragraph{Empirical Evidence on Public Data}
Table~\ref{tab:proof} provides a formal proof that an ordering over competing recommendation (or IR) models obtained through a normalised metric is \emph{not} guaranteed to be consistent with the original metric.
Nevertheless, one might wonder whether this single example represents a misguided pathological case, or whether metric disagreement occurs in practice.
This gives rise to the research question:
\begin{description}    
    \item[\textbf{RQ5}] \textit{Do DCG and normalised DCG disagree when ranking recommendation models in typical offline evaluation setups?}
\end{description}
To answer this question, we make use of the RecPack Python package~\cite{Michiels2022} and the MovieLens-1M dataset~\cite{Harper2015}.
We consider two types of models, \textsc{ease}\textsuperscript{r}~\cite{Steck2019} and \textsc{kunn}~\cite{Verstrepen2014}, varying their hyperparameters to train 192 models on a fixed 50\% of the available user-item interactions, and assess their performance on the held-out 50\%.
This style of evaluation setup is prevalent in the recommendation field~\cite{Steck2013,Jeunen2019DS,Zangerle2023}.
We adopt this package, dataset and methods to provide a reproducible setup that runs in under 20 minutes on a 2021 MacBook Pro.
All source code, including hyperparameter ranges, is available at \href{https://github.com/olivierjeunen/nDCG-disagreement/}{github.com/olivierjeunen/nDCG-disagreement}.

We do not de-bias the interactions (as MovieLens does not provide information about exposure), and adopt the traditional logarithmic discount for DCG with a cut-off at rank 100.
Results are visualised in Figure~\ref{fig:disagreement} with DCG@100 on the $x$-axis and nDCG@100 on the $y$-axis.
The two metrics exhibit a linear correlation of $\approx 0.6$ (Pearson), and a rank correlation of $\approx 0.5$ (Kendall).
Whilst they are clearly correlated, practitioners should \emph{not} blindly adopt nDCG when DCG estimates their online metric.
Indeed, DCG can be formulated as an unbiased estimator of the average reward per trajectory, but nDCG cannot.
As can be seen from the plot, significant \emph{disagreement} occurs between the two metrics: when randomly choosing two observations, the empirical probability of nDCG inverting the ordering implied by DCG is roughly 25\% on this example.
Naturally, one would expect this type of disagreement to occur even more frequently when considering Learning-to-Rank algorithms that directly optimise listwise objectives such as (n)DCG~\cite{Jagerman2022,Ustimenko2020,Lyzhin2023}.

Note that this discrepancy would not occur if we would sample the exact same number of held-out items for every user (as in Leave-One-Out Cross-Validation).
Indeed, in such cases $f^{\star}(x)$ is constant $\forall x \in \mathcal{X}$, simply rescaling the metric.
Whilst this practice can be common in academic scenarios, real-world use-cases typically imply varying numbers of ``relevant'' items per user or context.

We include these results to aid in the reproducibility of the empirical phenomena we report in this work.
