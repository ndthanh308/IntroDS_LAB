% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage[misc]{ifsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{fontawesome}
\usepackage{url}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{color}
\usepackage{booktabs}
\usepackage{makecell,rotating}
\usepackage{threeparttable}
% \urlstyle{same}

\newcommand{\xmli}[1]{{\color[rgb]{0.1,0.1,0.9}{[XM:#1]}}}

\newcommand{\repeatthanks}{\textsuperscript{\thefootnote}}
\newcommand{\methodname}{DHC}

\def\etal{\textit{et al}. }
\def\ie{\textit{i.e.}}
\def\eg{\textit{e.g. }}
\def\vs{\textit{v.s. }}

\usepackage{hyperref}
\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=black,
	pdfstartview=Fit,
	breaklinks=true}




% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
% \title{Contribution Title\thanks{Supported by organization x.}}
% \title{Distilled Contrastive Learning for Surgical Video Pre-training}

% \title{Free Lunch for Self-supervised Learning in Surgical Video}



\title{Supplementary Material \\ {\normalsize DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation}}



%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

% \authorrunning{Y. Hsi \& X. Xu et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %


\author{Haonan Wang, Xiaomeng Li\textsuperscript{(\Letter)}} 
% index{Haonan, Wang}
% index{Xiaomeng, Li}

\institute{Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China
 \\ \email{eexmli@ust.hk}}

\authorrunning{H. Wang \& X. Li}


% \institute{The Hong Kong University of Science and Technology, Hong Kong \\ \email{eexmli@ust.hk} \and
% The Pennsylvania State University, State College PA, USA \and
% The University of Hong Kong, Hong Kong \\}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %

%
\maketitle              % typeset the header of the contribution
%

%\subsubsection{Training Details.} We implement the proposed framework with PyTorch, using a single NVIDIA A100 GPU. The network parameters are optimized with SGD with a momentum of 0.9 and an initial learning rate of 0.03.
%In the training stage, simple data augmentations (random cropping and random flipping) are used one-the-ﬂy to avoid over-fitting.
%Totally 300 epochs are trained as the network has well converged. The batch size is 4, consisting of 2 labeled data and 2 unlabeled data.


\iffalse
\section{Related Work}
\subsubsection{Semi-supervised Segmentation}
Semi-supervised segmentation aims to explore tremendous unlabeled data with supervision from limited labeled data.
%, which is most relevant to domain adaptive segmentation where labeled data is obtained from another domain.
%\textbf{General Semi-supervised Segmentation.}
Recently, self-training-based methods~\cite{chen2021cps,wang2022u2pl,fan2022ucc} have become the mainstream of this domain. Approaches with consistency regularization strategies~\cite{ouali2020cct,chen2021cps,fan2022ucc} achieved good performance by encouraging high similarity between the predictions of two perturbed networks for the same input image, which highly improved the generalization ability.
%% Wang et al. argued that every pixel matters to the model training, even its prediction is ambiguous. Based on this insight, they developed **U2PL** which adaptively separated reliable and unreliable pixels via the entropy of predictions, push each unreliable pixel to a category-wise queue that consists of negative samples, and manage to train the model with all candidate pixels.
In the medical image domain, the data limitation issue is more natural and serious.
%% Existing approaches to combat the limited data include uncertainty suppression and consistency loss~\cite{luo2021urpc}, contrast learning sampling strategy by utilizing the most valuable knowledge from unlabeled data \cite{wu2022cdcl}, rethinking bayesian deep learning methods \cite{wang2022gbdl}, exploring the pixel-level smoothness and inter-class separation \cite{wu2022ssnet}, and improved Global Local CL~\cite{chaitanya2020glcl} by capturing 3D spatial context and rich anatomical information along both the feature and the batch dimensions~\cite{you2022cvrl}。
Existing approaches~\cite{luo2021urpc,wu2022cdcl,wang2022gbdl,wu2022ssnet,you2022cvrl} to combat the limited data have achieved great success but are bottlenecked by the class imbalance problem, which is quite common and challenging in real-world application scenarios. Recently, several approaches~\cite{basak2022addressing,lin2022cld} have been proposed to tackle this issue, but they still have major limitations, as stated in Section~\ref{intro}.
Class-imbalance semi-supervised segmentation is still an under-studied area.
%Basak \textit{et al.}~\cite{basak2022addressing} introduced a robust class-wise sampling strategy to address the imbalance problem. They maintained a confidence array to record class-wise performance during training based on entropy, variance and confidence.
%Lin \textit{et al.}~\cite{lin2022cld} considered the imbalance issue and calibrated the label distribution by weighting the overall loss function with the voxel numbers of all classes. They also leveraged the uncertainty-aware sampling supervision and probability-aware random cropping augmentation to further enhance the learning of minority classes.


\subsubsection{Class-imbalanced Semi-supervised Learning}
Recently, many state-of-the-art methods leveraged the unlabeled data to alleviate the class-imbalance issues~\cite{wei2021crest,simis,oh2022daso,lai2022sar}.
Specifically, CReST~\cite{wei2021crest} dynamically added some unlabeled pseudo labels to the corresponding classes of the labeled set, depending on the imbalance ratios. 
However, it still added some samples to the majority classes which makes it hard to balance.
SimiS~\cite{simis} solve this problem by supplementing the tail classes according to their difference in class distribution from the head classes, which makes it focuses more on the tail classes.
SimiS fails when the one of the majority classes has very large number of samples and the other classes will have no significant margins even though they are still quite imbalanced. We proposed a new strategy to solve this issue.
%% Semantic pseudo-labels are reversely biased towards the tail side compared with the linear labels which biased to the head side. DASO~\cite{oh2022daso} leverages this complementary property of the similarity-based classifier to complement the vanilla linear classifier. With the mix-up of these two kinds of labels, the class-imbalance problem is alleviated and the overall bias is reduced.
Moreover, Yu \textit{et al.}~\cite{yu2022instancediff} proposed a instance-difficulty-aware re-sampling method by estimating the prediction variations in the learning and the unlearning directions to compensate the existing failures on the difficult majority classes.


\fi

\section*{Training Details}
We implement the proposed framework with PyTorch, using a single NVIDIA A100 GPU. The network parameters are optimized with SGD with a momentum of 0.9 and an initial learning rate of 0.03. We employ a “poly” decay strategy follow~\cite{isensee2021nnunet}.
In the training stage, simple data augmentations (random cropping and random flipping) are used to avoid over-fitting.
We trained the networks 300 epochs with batch size 4, consisting of 2 labeled and 2 unlabeled data.
In the inference stage, we use the average of the two sub-networks for prediction for all the CPS-based methods to avoid over-fitting to one of the perturbations.
We evaluate the prediction of the network with two metrics, including Dice and the average surface distance (ASD).
Final segmentation results are obtained using a sliding window strategy with a stride size of $32 \times 32 \times 16$.



\begin{table}[!ht]
\scriptsize
\caption{Quantitative comparison between DHC and SOTA SSL segmentation methods on \textbf{10\% labeled Synapse dataset}. `General' or `Imbalance' indicate whether the methods consider class-imbalance issue or not.
Sp: spleen, RK: right kidney, LK: left kidney, Ga: gallbladder, Es: esophagus, Li: liver, St: stomach, Ao: aorta, IVC: inferior vena cava, PSV: portal \& splenic veins, Pa: pancreas, RAG: right adrenal gland, LAG: left adrenal gland.
}
\label{sota}
\resizebox*{\linewidth}{!}{
\begin{tabular}{c|c|cc|cccccccc@{\ }ccccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}}  & \multirow{2}{*}{Avg. Dice} & \multirow{2}{*}{Avg. ASD} & \multicolumn{13}{c}{Average Dice of Each   Class}                                        \\ 
\multicolumn{2}{c|}{}                          &                            &                           & Sp   & RK   & LK   & Ga   & Es   & Li   & St   & Ao   & IVC  & PSV  & PA   & RAG  & LAG  \\\midrule
\multirow{9}{*}{\rotatebox{90}{General}}         & V-Net (fully)      & 62.09±1.2	&10.28±3.9	& 84.6	& 77.2	& 73.8	& 73.3	& 38.2	& 94.6	& 68.4	& 72.1	& 71.2	& 58.2	& 48.5	& 17.9	& 29.0 \\ \midrule

& UA-MT~\cite{yu2019uamt}$^\dagger$     &18.07±1.2	&57.64±1.8	& 27.1	& 7.1	& 17.0	& 24.4	& \textcolor{red}{0.0}	& 80.6	& 15.6	& 39.3	& 16.7	& 4.4	& 2.7	& \textcolor{red}{0.0}	& \textcolor{red}{0.0} \\


& URPC~\cite{luo2021urpc}$^\dagger$       &26.37±1.5	&53.95±11.3	& \textbf{51.7}	& 35.1	& 26.4	& 7.3	& \textcolor{red}{0.0}	& \textbf{83.8}	& 21.3	& \textbf{69.0}	& \textbf{41.0}	& 1.9	& 5.2	& \textcolor{red}{0.0} 	&\textcolor{red}{0.0}  \\

& CPS~\cite{chen2021cps}$^\dagger$        &21.96±1.2	&55.42±4.6	& 37.9	& 31.8	& 19.0	& 31.9	& \textcolor{red}{0.0}	& 65.1	& 15.5	& 44.8	& 29.6	& 4.3	& 5.5	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}  \\

& SS-Net~\cite{wu2022ssnet}$^\dagger$     &17.5±3.0	&66.17±8.0	& 45.6	& 11.6	& \textbf{42.3}	& 2.4	& \textcolor{red}{0.0}	& 74.5	& 6.0	& 32.6	& 2.8	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 3.8	& 5.8  \\

& DST~\cite{chen2022dst}$^\star$        &20.91±5.9	&61.33±18.3	& 43.3	& 32.8	& 16.0	& 24.9	& \textcolor{red}{0.0}	& 75.8	& \textbf{22.4}	& 27.6	& 19.4	& 3.8	& 5.4	& 0.3	& \textcolor{red}{0.0}\\

& DePL~\cite{wang2022depl}$^\star$       &21.01±3.3	&58.42±6.3	& 34.2	& 32.2	& 17.3	& 27.2	& \textcolor{red}{0.0}	& 65.7	& 16.8	& 40.8	& 29.3	& 2.8	& 6.8	& \textcolor{red}{0.0}	& \textcolor{red}{0.0} \\ \midrule
\multirow{8}{*}{\rotatebox{90}{Imbalance}} 

& Adsh~\cite{guo2022adsh}$^\star$       &22.8±0.9	&46.18±4.0	& 36.0	& 35.7	& 20.0	& 31.0	& \textcolor{red}{0.0}	& 74.7	& 18.3	& 32.3	& 27.8	& 11.7	& 7.3	& 1.7	& \textcolor{red}{0.0}  \\ 

& CReST~\cite{wei2021crest}$^\star$      &26.56±2.9	&36.17±1.0	& 37.3	& 46.5	& 25.2	& 27.1	& 1.7	& 66.3	& 14.2	& 45.2	& 35.8	& 11.2	& 6.8	& 24.2	& 3.8  \\

& SimiS~\cite{simis}$^\star$      & 25.05±3.1	&43.93±2.4	& 42.0	& 38.6	& 27.2	& 19.7	& \textcolor{red}{0.0}	& 74.2	& 16.5	& 51.7	& 35.0	& 13.6	& 5.4	& \textcolor{red}{0.0}	& 1.8  \\

& Basak \textit{et al.}~\cite{basak2022addressing}$^\dagger$         &25.3±2.2	&50.02±5.7	& 40.9	& 42.3	& 19.2	& 35.2	& \textcolor{red}{0.0}	& 75.7	& 19.2	& 44.7	& 32.8	& 5.0	& \textbf{10.4}	& 3.5	& \textcolor{red}{0.0}  \\
                                 
& CLD~\cite{lin2022cld}$^\dagger$        & 22.49±1.6	&49.74±4.1	& 39.3	& 43.9	& 25.6	& 12.8	& \textcolor{red}{0.0}	& 73.3	& 14.3	& 41.1	& 25.7	& 8.8	& 6.1	& 0.2	& 1.1  \\

& \textbf{DistDW(ours)}   & 27.21±0.9	&32.38±4.0	& 47.8	& 42.3	& 33.1	& 27.0	& 1.1	& 65.5	& 20.7	& 49.0	& 34.5	& 8.1	& 7.4	& 14.5	& 2.8 \\

& \textbf{DiffDW(ours)}   & 28.63±2.5	&24.81±4.0	& 44.1	& 33.4	& 25.3	& \textbf{37.0}	& 6.3	& 75.8	& 19.1	& 46.3	& 28.6	& \textbf{17.5}	& 7.8	& \textbf{24.7}	& 6.3\\
                                  
& \textbf{DHC(ours)}   & \textbf{31.64±0.9}	&\textbf{21.82±1.0}	& 45.1	& \textbf{47.4}	& 33.1	& 36.6	& \textbf{7.1}	& 71.4	& 17.8	& 58.9	& 34.4	& 16.5	& 9.3	& 21.8	& \textbf{12.0} \\ \bottomrule
\end{tabular}
}
\begin{threeparttable}
 \begin{tablenotes}
        \scriptsize
        \item[$\dagger$] we implement semi-supervised segmentation methods on our dataset.
        \item[$\star$] we extend semi-supervised classification methods to segmentation with CPS as the baseline.
\end{tablenotes}
\end{threeparttable}
\end{table}




\begin{table}[!ht]
\scriptsize
\caption{Quantitative comparison between DHC and SOTA SSL segmentation methods on\textbf{ 40\% labeled Synapse dataset}. 
%Sp: spleen, RK: right kidney, LK: left kidney, Ga: gallbladder, ES: esophagus, Li: liver, St: stomach, Ao: Aorta, IVC: inferior vena cava, PSV: portal \& splenic veins, Pa: pancreas, RAG: right adrenal gland, LAG: left adrenal gland.
}
\label{sota}
\resizebox*{\linewidth}{!}{
\begin{tabular}{c|c|cc|cccccccc@{\ }ccccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}}  & \multirow{2}{*}{Avg. Dice} & \multirow{2}{*}{Avg. ASD} & \multicolumn{13}{c}{Average Dice of Each   Class}                                        \\ 
\multicolumn{2}{c|}{}                          &                            &                           & Sp   & RK   & LK   & Ga   & Es   & Li   & St   & Ao   & IVC  & PSV  & PA   & RAG  & LAG  \\\midrule
\multirow{9}{*}{\rotatebox{90}{General}}         & V-Net (fully)      & 62.09±1.2	&10.28±3.9	& 84.6	& 77.2	& 73.8	& 73.3	& 38.2	& 94.6	& 68.4	& 72.1	& 71.2	& 58.2	& 48.5	& 17.9	& 29.0 \\ \midrule

& UA-MT~\cite{yu2019uamt}$^\dagger$      &17.09±2.97	&91.86±7.93	& 4.2	& 57.8	& 32.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 91.0	& 37.0	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}  \\

& URPC~\cite{luo2021urpc}$^\dagger$       &24.83±8.19	&74.44±17.01	& 42.6	& 44.8	& 51.4	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 86.7	& 37.8	& 25.8	& 27.0	& \textcolor{red}{0.0}	& 6.6	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}  \\

& CPS~\cite{chen2021cps}$^\dagger$       &33.07±1.07	&60.46±2.25	& 68.4	& 72.7	& 64.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 91.9	& 42.1	& 66.2	& 22.3	& \textcolor{red}{0.0}	& 2.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0} \\

& SS-Net~\cite{wu2022ssnet}$^\dagger$     &32.98±10.99	&71.18±20.77	& 49.0	& 68.9	& 71.4	& 22.9	& \textcolor{red}{0.0}	& 92.0	& 34.7	& 51.7	& 38.1	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0} \\

& DST~\cite{chen2022dst}$^\star$        &35.57±1.54	&55.69±1.43	& 73.8	& \textbf{73.2}	& 64.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textbf{92.1}	& 41.3	& 71.8	& 40.7	& \textcolor{red}{0.0}	& 5.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}  \\

& DePL~\cite{wang2022depl}$^\star$       &36.16±2.08	&56.14±7.61	& 72.7	& 72.4	& 64.4	& 13.3	& \textcolor{red}{0.0}	& 91.7	& 42.8	& 63.8	& 47.0	& \textcolor{red}{0.0}	& 1.9	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}  \\ \midrule
\multirow{8}{*}{\rotatebox{90}{Imbalance}} 

& Adsh~\cite{guo2022adsh}$^\star$       &35.91±6.17	&53.7±6.95	& 66.8	& 72.5	& 64.4	& 19.1	& \textcolor{red}{0.0}	& 91.8	& 43.8	& 62.0	& 39.8	& \textcolor{red}{0.0}	& 6.5	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}  \\ 

& CReST~\cite{wei2021crest}$^\star$      &41.6±2.49	&27.82±5.07	& 53.8	& 69.5	& 58.1	& 35.3	& 17.7	& 85.0	& 36.0	& 60.3	& 45.2	& 21.5	& 24.2	& 23.3	& 10.8 \\

& SimiS~\cite{simis}$^\star$      &47.09±2.33	&33.46±1.75	& 75.4	& 66.9	& 69.0	& 62.6	& \textcolor{red}{0.0}	& 81.6	& \textbf{53.1}	& 80.4	& 56.2	& 29.9	& 37.1	& \textcolor{red}{0.0}	& \textcolor{red}{0.0} \\

& Basak \textit{et al.}~\cite{basak2022addressing}$^\dagger$   &35.03±3.68	&60.69±6.57	& 69.1	& 72.8	& 67.5	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 91.6	& 45.0	& 62.2	& 39.1	& \textcolor{red}{0.0}	& 8.0	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}   \\
                                 
& CLD~\cite{lin2022cld}$^\dagger$        &48.23±1.02	&28.79±3.64	& 78.3	& 73.1	& \textbf{73.8}	& 57.0	& \textcolor{red}{0.0}	& 87.6	& 51.7	& 82.8	& 50.5	& 38.0	& 31.8	& 1.5	& 0.8 \\

& \textbf{DistDW(ours)}   &51.86±4.13	&14.68±4.31	& 78.0	& 69.7	& 66.2	& 65.9	& \textbf{34.4}	& 74.3	& 23.2	& 76.9	& 58.0	& 32.4	& 33.4	& 29.6	& 32.1 \\

& \textbf{DiffDW(ours)}   &50.84±2.78	&19.24±11.48	& 76.7	& 67.8	& 68.4	& 58.6	& 3.2	& 82.5	& 42.8	& 83.6	& 51.0	& 44.2	& 42.3	& 22.9	& 16.9 \\
                                
& \textbf{DHC(ours)}   &\textbf{57.13±0.8}	&\textbf{11.66±2.7}	& \textbf{82.5}	& 72.8	& 73.5	& \textbf{69.8}	& 10.7	& 71.9	& 41.2	& \textbf{83.7}	& \textbf{66.1}	& \textbf{53.8}	& \textbf{47.4}	& \textbf{36.8}	& \textbf{32.7} \\ \bottomrule
\end{tabular}
}
\begin{threeparttable}
 \begin{tablenotes}
        \scriptsize
        \item[$\dagger$] we implement semi-supervised segmentation methods on our dataset.
        \item[$\star$] we extend semi-supervised classification methods to segmentation with CPS as the baseline.
\end{tablenotes}
\end{threeparttable}
\end{table}




\begin{table}[!ht]
\scriptsize
\caption{Quantitative comparison between DHC and SOTA SSL segmentation methods on \textbf{2\% labeled AMOS dataset}. 
}
\label{sota2}
\resizebox*{\linewidth}{!}{

\begin{tabular}{c|c|c@{\ \ }c|cccccccc@{\ }ccccccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}}  & Avg. & Avg. & \multicolumn{15}{c}{Average Dice of Each   Class}                                        \\ 
\multicolumn{2}{c|}{}                          &    Dice                        &      ASD                     & Sp   & RK   & LK   & Ga   & Es   & Li   & St   & Ao   & IVC   & PA   & RAG  & LAG  & Du   & Bl  & P/U  \\\midrule
\multirow{9}{*}{\rotatebox{90}{General}}         & V-Net (fully)      & 76.50 	&2.01 	&92.2	&92.2	&93.3	&65.5	&70.3	&95.3	&82.4	&91.4	&85.0	&74.9	&58.6	&58.1	&65.6	&64.4	&58.3 \\ \midrule

& UA-MT~\cite{yu2019uamt}$^\dagger$      & 33.96	& 22.43	& \textbf{62.5}	& \textbf{61.7}	& \textbf{59.8}	& 17.5	& 13.8	& 73.4	& 39.4	& 34.6	& 32.4	& 26.5	& 12.1	& 6.5	& 15.3	& 32.4	& 21.7  \\

& URPC~\cite{luo2021urpc}$^\dagger$       & \textbf{38.39}	& 37.58	& 60.8	& 57.7	& 56.5	& \textbf{34.6}	& \textcolor{red}{0.0}	& \textbf{78.4}	& \textbf{41.4}	& \textbf{53.3}	& \textbf{49.6}	& \textbf{40.4}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textbf{30.1}	& 42.5	& \textbf{30.6}  \\

& CPS~\cite{chen2021cps}$^\dagger$        & 31.78	& 39.23	& 55.9	& 46.9	& 53.1	& 27.7	& \textcolor{red}{0.0}	& 66.4	& 25.2	& 41.8	& 45.2	& 29.4	& 0.1	& \textcolor{red}{0.0}	& 22.1	& 38.7	& 24.2  \\

& SS-Net~\cite{wu2022ssnet}$^\dagger$    & 17.47	& 59.05	& 37.7	& 20.1	& 26.3	& 9.0	& 3.3	& 57.1	& 25.1	& 28.4	& 28.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 26.5	& 0.2  \\

& DST~\cite{chen2022dst}$^\star$        & 31.94	& 39.15	& 50.9	& 52.4	& 56.9	& 24.6	& \textcolor{red}{0.0}	& 59.4	& 31.5	& 41.8	& 43.1	& 26.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 23.8	& 42.6	& 25.9  \\

& DePL~\cite{wang2022depl}$^\star$       & 31.56	& 40.70	& 57.1	& 49.3	& 54.3	& 26.6	& 0.1	& 69.2	& 26.2	& 41.1	& 46.7	& 23.9	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 16.7	& 40.3	& 21.8  \\ \midrule
\multirow{6}{*}{\rotatebox{90}{Imbalance}} 

& Adsh~\cite{guo2022adsh}$^\star$       & 30.30	& 42.48	& 53.9	& 45.1	& 51.2	& 28.5	& \textcolor{red}{0.0}	& 62.1	& 27.0	& 41.4	& 42.7	& 25.0	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 20.3	& 35.8	& 21.6  \\ 

& CReST~\cite{wei2021crest}$^\star$      & 34.13	& 20.15	& 57.9	& 51.5	& 49.1	& 22.7	& 13.2	& 66.2	& 34.4	& 39.4	& 40.4	& 24.6	& 17.2	& 10.2	& 24.4	& 36.5	& 24.4  \\

& SimiS~\cite{simis}$^\star$     & 36.89	& 26.16	& 57.8	& 58.6	& 58.6	& 22.9	& \textcolor{red}{0.0}	& 70.9	& 38.0	& 52.0	& 47.0	& 32.4	& 20.2	& \textbf{11.5}	& 18.1	& 39.9	& 25.5  \\


& Basak \textit{et al.}~\cite{basak2022addressing}$^\dagger$        & 29.87	& 35.55	& 50.7	& 47.7	& 44.1	& 21.1	& \textcolor{red}{0.0}	& 61.8	& 27.7	& 38.1	& 40.4	& 21.8	& 9.6	& 9.5	& 14.6	& 36.5	& 24.5   \\
                                 
& CLD~\cite{lin2022cld}$^\dagger$        & 36.23	& 27.63	& 55.8	& 55.8	& 59.1	& 23.9	& \textcolor{red}{0.0}	& 69.9	& 38.2	& 50.1	& 44.5	& 32.3	& 18.9	& 9.2	& 18.8	& 42.2	& 24.9  \\
 
                                 
& \textbf{DHC (ours)}   & 38.28	& \textbf{20.34}	& 62.1	& 59.5	& 57.8	& 25.0	& \textbf{20.5}	& 66.0	& 38.2	& 51.3	& 47.9	& 26.8	& \textbf{26.4}	& 7.0	& 17.8	& \textbf{43.2}	& 24.8 \\ \bottomrule
\end{tabular}
}
\begin{threeparttable}
 \begin{tablenotes}
        \scriptsize
        \item[$\dagger$] we implement semi-supervised segmentation methods on our dataset.
        \item[$\star$] we extend semi-supervised classification methods to segmentation with CPS as the baseline.
\end{tablenotes}
\end{threeparttable}

\end{table}



\begin{table}[!ht]
\scriptsize
\caption{Quantitative comparison between DHC and SOTA SSL segmentation methods on \textbf{10\% labeled AMOS dataset}. 
}
\label{sota2}
\resizebox*{\linewidth}{!}{

\begin{tabular}{c|c|c@{\ \ }c|cccccccc@{\ }ccccccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}}  & Avg. & Avg. & \multicolumn{15}{c}{Average Dice of Each   Class}                                        \\ 
\multicolumn{2}{c|}{}                          &    Dice                        &      ASD                     & Sp   & RK   & LK   & Ga   & Es   & Li   & St   & Ao   & IVC   & PA   & RAG  & LAG  & Du   & Bl  & P/U  \\\midrule
\multirow{9}{*}{\rotatebox{90}{General}}         & V-Net (fully)      & 76.50 	&2.01 	&92.2	&92.2	&93.3	&65.5	&70.3	&95.3	&82.4	&91.4	&85.0	&74.9	&58.6	&58.1	&65.6	&64.4	&58.3 \\ \midrule

& UA-MT~\cite{yu2019uamt}$^\dagger$      & 40.60	& 38.45	& 61.0	& 75.4	& 58.8	& 0.1	& \textcolor{red}{0.0}	& 84.4	& 45.2	& 72.8	& 61.6	& 36.2	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 30.7	& 46.5	& 36.3  \\

& URPC~\cite{luo2021urpc}$^\dagger$       & 49.09	& 29.69	& 81.7	& 77.5	& 77.2	& 38.1	& \textcolor{red}{0.0}	& 87.7	& 57.9	& 75.0	& 62.7	& 52.1	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& 35.9	& 48.8	& 41.7  \\

& CPS~\cite{chen2021cps}$^\dagger$        & 54.51	& 7.84	& 80.7	& 79.8	& 74.3	& 35.2	& 44.4	& 90.5	& 51.1	& 76.1	& 65.6	& 48.6	& 31.6	& 21.8	& 33.6	& 47.0	& 37.3  \\

& SS-Net~\cite{wu2022ssnet}$^\dagger$    & 38.91	& 53.43	& 73.4	& 73.4	& 72.2	& 42.4	& \textcolor{red}{0.0}	& 83.5	& 46.7	& 74.1	& 69.6	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	& \textcolor{red}{0.0}	&\textcolor{red}{0.0}	& 48.3	& 0.2 \\

& DST~\cite{chen2022dst}$^\star$        & 52.24	& 17.66	& 81.7	& 80.2	& 78.6	& 39.5	& 41.0	& 89.8	& 52.8	& 78.5	& 65.9	& 51.1	& 4.3	& 0.1	& 34.2	& 48.8	& 37.2  \\

& DePL~\cite{wang2022depl}$^\star$      & 56.76	& 6.70	& 81.9	& 80.6	& 79.5	& 41.0	& 42.6	& 89.3	& 57.6	& 79.1	& 66.0	& 53.2	& 34.6	& 21.8	& 34.9	& 48.4	& 40.9  \\ \midrule
\multirow{6}{*}{\rotatebox{90}{Imbalance}} 

& Adsh~\cite{guo2022adsh}$^\star$       & 54.92	& 8.07	& 81.6	& 78.5	& 76.6	& 40.1	& 43.4	& 90.1	& 53.0	& 76.7	& 64.4	& 48.3	& 25.9	& 24.2	& 34.7	& 48.7	& 37.7  \\ 

& CReST~\cite{wei2021crest}$^\star$      & 60.74	& 4.65	& 85.3	& 84.5	& 84.0	& 43.2	& 50.8	& 89.9	& 58.7	& 84.7	& 73.0	& 54.2	& \textbf{41.8}	& 31.6	& 41.0	& 52.8	& 35.8  \\

& SimiS~\cite{simis}$^\star$      & 57.48	& 4.46	& 83.1	& 80.9	& 80.0	& 39.6	& 45.9	& 90.0	& 57.1	& 78.0	& 66.3	& 54.1	& 35.8	& 26.9	& 39.9	& 49.3	& 35.4  \\


& Basak \textit{et al.}~\cite{basak2022addressing}$^\dagger$       & 53.66	& 8.50	& 80.3	& 78.2	& 79.0	& 36.3	& 40.3	& 88.6	& 53.2	& 76.8	& 65.6	& 46.8	& 23.9	& 16.1	& 31.4	& 49.7	& 38.6   \\
                                 
& CLD~\cite{lin2022cld}$^\dagger$        & 61.55	& 4.21	& 86.0	& 85.3	& 84.8	& 44.5	& 51.9	& \textbf{90.8}	& 59.7	& 83.7	& 73.1	& 55.7	& 40.2	& \textbf{37.2}	& 41.4	& 53.0	& 36.1 \\
 
                                 
& \textbf{DHC (ours)}   & \textbf{64.16}	& \textbf{3.51}	& \textbf{87.4}	& \textbf{86.6}	& \textbf{87.1}	& \textbf{45.8}	& \textbf{57.0}	& 89.8	& \textbf{64.7}	& \textbf{86.0}	& \textbf{75.0}	& \textbf{62.5}	& 39.8	& 36.8	& \textbf{44.0}	& \textbf{56.5}	& \textbf{43.6} \\ \bottomrule
\end{tabular}
}
\begin{threeparttable}
 \begin{tablenotes}
        \scriptsize
        \item[$\dagger$] we implement semi-supervised segmentation methods on our dataset.
        \item[$\star$] we extend semi-supervised classification methods to segmentation with CPS as the baseline.
\end{tablenotes}
\end{threeparttable}

\end{table}



\bibliographystyle{splncs04}
\bibliography{miccai_bib}

\end{document}
