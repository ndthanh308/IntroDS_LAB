\documentclass[review]{elsarticle}
\usepackage{tabularx}
\usepackage[utf8]{inputenc}
\usepackage{lineno,hyperref}
\usepackage[top=1.65in, bottom=1.65in, left=1.35in, right=1.35in]{geometry}
\usepackage{setspace}
\setstretch{1.25}
\usepackage{amsmath}
% Define a custom command for function names
\newcommand{\func}[1]{\texttt{#1}}
\usepackage{caption} 
%\captionsetup[table]{skip=6pt}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}


%\journal{Computer Standards \& Interfaces || arXiv preprint: 2307.15464}
%\bibliographystyle{model5-names}\biboptions{authoryear}
\bibliographystyle{model5-names}\biboptions{numbers}


\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{}%
 \let\@evenfoot\@oddfoot}
\makeatother
\begin{document}

\begin{frontmatter}

\title{Framework to Automatically Determine the Quality of Open Data Catalogs}

\author{Jorge Martinez-Gil}
\address{Software Competence Center Hagenberg GmbH \\ Softwarepark 32a, 4232 Hagenberg, Austria \\ \url{jorge.martinez-gil@scch.at}}


\begin{abstract}
Data catalogs play a crucial role in modern data-driven organizations by facilitating the discovery, understanding, and utilization of diverse data assets. However, ensuring their quality and reliability is complex, especially in open and large-scale data environments. This paper proposes a framework to automatically determine the quality of open data catalogs, addressing the need for efficient and reliable quality assessment mechanisms. Our framework can analyze various core quality dimensions, such as accuracy, completeness, consistency, scalability, and timeliness, offer several alternatives for the assessment of compatibility and similarity across such catalogs as well as the implementation of a set of non-core quality dimensions such as provenance, readability, and licensing. The goal is to empower data-driven organizations to make informed decisions based on trustworthy and well-curated data assets. The source code that illustrates our approach can be downloaded from \url{https://www.github.com/jorge-martinez-gil/dataq/}.
\end{abstract}

\begin{keyword}
Data Engineering, Data Catalogs, Quality Assessment
\end{keyword}

\end{frontmatter}


\section{Introduction}
Efficient data management strategies have become a critical need for a wide range of organizations, leading to using Data Catalogs (DCs) as a helpful strategy to facilitate data management. As metadata repositories, DCs are essential in identifying, evaluating, and using diverse data assets. However, guaranteeing the quality of DCs presents a complex challenge, with particular importance in dynamic data ecosystems where the contents evolve rapidly, as is the case of the Open DCs (ODCs)\cite{key-abs2}. The manual assessment processes are time-consuming, resource-intensive, and even error-prone. Therefore, developing sound strategies for quality assessment is attracting much attention in recent times \cite{key-albertoni}.

In response to this challenge, this research presents a novel framework specifically designed to automate the evaluation of the quality of ODCs. The rationale for this framework lies in recognizing that ODCs should publicly supply contextual information about the cataloged data assets. Therefore, it is necessary to bring details such as data sources, owners, policies, and any other accompanying information \cite{key-Ryan}. Such contextual information should facilitate stakeholders with a good understanding of the data's origin, purpose, and limitations, thereby enabling sound governance and compliance strategies. 

There is a certain consensus in the community that ensuring the quality of ODCs deserves further research on quality aspects \cite{key-albertoni}. For example, suppose an organization wants its ODCs to be considered a trustworthy information source. In that case, it must entrust people to confidently use appropriate data assets for their analytical and decision-making efforts \cite{key-labadie}. Contrarily, a catalog of subpar quality can result in wasted time, erroneous analysis, and reduced decision-making capabilities. Thus, developing a framework that automates the assessment of catalog quality assumes particular significance, at least for most Open Data (OD) consumers.

Our proposed framework involves multiple quality dimensions: accuracy, completeness, consistency, scalability, timeliness, compatibility, similarity, provenance, readability, and reusability. These dimensions have various aspects that determine the reliability and utility of cataloged data assets. Our framework aims to provide insights for users by adopting a practical approach to quality measurement. The idea is that stakeholders can also easily understand, use, and extend the proposed methods.

Furthermore, to demonstrate the capabilities of our framework, we illustrate our definitions with implementations using DCAT\footnote{W3C's Data Catalog vocabulary}. This vocabulary is the de facto metadata standard within the domain of OD. It represents a W3C recommendation for describing OD on the Web \cite{key-abs,key-maali}. It is based on the RDF vocabulary, built as a Dublin Core profile, and enjoys wide acceptance at present \cite{key-Neumaier}.

Therefore, the most significant contributions of this research work can be summarized as follows:

\begin{itemize}
	\item This research introduces a novel framework that automates the evaluation of ODC quality, effectively tackling the difficulties inherent in manual assessment procedures. The proposed framework shows potential in facilitating decision-making processes driven by data and optimizing the overall value obtained from data assets.

  \item We also delineate ODC cross-catalog dimensions and explore potential implementations for cases involving multiple catalogs. We develop the notions of compatibility and similarity to contribute to a comprehensive understanding and development of new quality metrics in ODC quality assessment.

  \item All definitions provided are mapped to existing works and ISO standards, and an exemplary implementation using the W3C's Data Catalog standard is provided. This in-depth exploration illustrates the practical applicability of the proposed framework and the theoretical principles discussed in the paper.
\end{itemize}

The subsequent sections of this work are organized as follows: Section 2 explains the existing landscape regarding the utilization of ODCs. Section 3 encloses our contribution regarding core quality dimensions, encompassing two pivotal elements: a formal definition of the vital quality concepts and a suggestion outlining the pseudocode implementation. Section 4 offers new developments regarding cross-catalog dimensions, including the compatibility and the similarity between ODCs. Section 5 introduces several non-core dimensions that are still very useful to facilitate the tasks of data stakeholders. Finally, this research work's conclusion is presented based on our findings and discusses potential lines for future research.

\section{State-of-the-art}
An ODC is a metadata repository that organizes information about the available data assets within a public audience \cite{key-Lakomaa}. It is a reference for data consumers, e.g., analysts, scientists, etc. The aim is to facilitate the discovery, understanding, and access to relevant data \cite{key-Debattista}. 

\subsection{Data catalogs and Metadata quality}
A well-designed ODC should combine the relevance of cataloged data assets with the needs and interests of its users. It should offer, at least, efficient search capabilities and clear navigation options \cite{key-Pietriga}. Usability enhancements such as collaborative features can also improve the user experience \cite{key-Klime}. At the same time, an ODC should facilitate effective filtering mechanisms, allowing users to find data assets that align with their needs.

ODCs should also adhere to data governance principles and support effective metadata management. This includes maintaining consistent metadata standards, establishing quality rules, enforcing access controls, and facilitating integration with other tools and processes. Therefore, the importance of an ODC lies in its ability to address some of the traditional challenges associated with data management. The literature provides plenty of reasons why ODCs play or should play an important role:

\begin{itemize}
	\item An ODC is a valuable tool for data discovery, providing a searchable inventory of available data sources, datasets, etc. It helps users efficiently locate relevant data assets \cite{key-Barthelemy}.
	
	\item An ODC enhances data understanding by providing comprehensive metadata about the data assets, including detailed descriptions and relationships with other datasets. This information helps users evaluate the data's structure, content, and context for analysis or decision-making purposes \cite{key-geissner}.
	
	\item An ODC plays a pivotal role in data governance by establishing standards, policies, and guidelines by presenting a consistent view of an organization's data assets \cite{key-Neumaier}. Data managers can enforce governance rules, monitor data utilization, and comply with internal and external regulations.
	
	\item Collaboration and knowledge sharing among users are promoted through an ODC. Users can annotate datasets, provide feedback, share insights, and collaborate on projects. The catalog reduces data silos, encourages sharing, and promotes a data-driven culture \cite{key-ochoa}.
	
	\item An ODC facilitates data lineage by providing information on data assets' origins, transformations, and dependencies, allowing users to trace the data journey and promote transparency. It also enables impact analysis by identifying the effects of changes to data sources  \cite{key-BNK}.
	
	\item An ODC assists in ensuring data security and compliance with privacy regulations \cite{key-dibowski}. It documents data access controls, classification labels, and sensitive elements, helping enforce security measures, and monitor data access. 
\end{itemize}

However, there is still much to be investigated, as the growing number of research papers in this regard testifies \cite{key-Ehrlinger}. The aim is to achieve comprehensive solutions that enable the effective and efficient use of metadata by individuals or systems acting on their behalf.

\subsection{The DCAT vocabulary}
The DCAT vocabulary is an established standard to effectively describe ODCs on the Web. DCAT involves a set of classes, properties, and relationships that facilitate the publication, discovery, and exchange of essential information about datasets \cite{key-Nogueras}. This standardization facilitates interoperability across diverse platforms and systems, thus enabling the integration and utilization of data resources \cite{key-Lobe}.

One of the fundamental contributions of DCAT lies in defining concepts such as catalogs, datasets, distributions, and data services. This vocabulary entrusts data publishers to thoroughly describe pertinent metadata related to datasets, including but not limited to title, description, keywords, licensing information, spatial and temporal coverage, and so on. Moreover, DCAT provides robust support for establishing connections between datasets and associated resources such as data files or APIs \cite{key-Kirstein}.

The standardized representation of data resource information through DCAT simplifies exposing datasets for data publishers while facilitating OD consumers to find pertinent data efficiently. Furthermore, this standardized vocabulary enables the data exchange and integration among diverse data platforms, data portals, and other data-centric tools by employing a shared vocabulary.

DCAT is commonly used by organizations aiming to improve the visibility of their data assets effectively. The idea is to promote data sharing and reuse while guaranteeing interoperability within a broader data ecosystem. Therefore, adopting DCAT is pivotal in facilitating transparency and collaboration through efficiently managing data resources.

\subsection{Research gap}
Numerous challenges persist in quality assessment for ODCs. Various issues regarding data quality emerge, enclosing inconsistency, inaccuracy, incompleteness, duplication, outdated information, and human error \cite{key-Gupta}. These issues have a profound influence on decision-making procedures. Existing tools designed to address the problem exhibit imperfections and predominantly rely on domain-specific niches. This is primarily due to the multifaceted nature of quality assessment within data repositories. While some common practices exist, individual approaches to ensuring data quality have historically demonstrated superior performance.

Consequently, more research is needed concerning standardized metrics to evaluate the quality of ODCs. Indeed, the absence of widely accepted metrics capable of independently assessing key aspects like accuracy, completeness, and consistency of ODCs, irrespective of the application domain, is evident. Moreover, as ODCs expand in size and complexity, evaluating their scalability potential automatically poses a formidable challenge. Consequently, research efforts must develop efficient techniques to assess large-scale data catalogs' timeliness while maintaining other pertinent aspects.

\subsection{Contribution over the state-of-the-art}
As Table \ref{tab:tab0} shows, several works \cite{key-Kubler, key-Zaveri} and international standards (ISO 19157\footnote{https://www.iso.org/standard/32575.html}, ISO 25012\footnote{https://www.iso.org/standard/35736.html}) on assessing quality have been proposed in the literature. However, the nomenclature used is not homogeneous, and the assessment methods are not formally defined, which is detrimental to a framework for a fair comparison. Our work offers, for the first time:

\begin{enumerate}
	\item A focus on the automatic quality assessment of ODCs,
	\item A formal definition for each quality dimension that significantly facilitates their understanding and matching with other metrics proposed in the literature\footnote{Please note that, unlike other approaches, this research does not consider accessibility as a quality dimension since this notion is covered in the definition of accuracy},
	\item An implementation proposal for each of the proposed quality dimensions to facilitate research progress and transparency, and
	\item A proposal for automatically determining compatibility and similarity between ODCs.
\end{enumerate}
 
This framework is organized around three types of quality dimensions: Core (accuracy, completeness, consistency, scalability, and timeliness), Cross-catalog (compatibility and similarity), and Non-Core (lineage and provenance, readability, and licensing). Furthermore, all these contributions orbit around the DCAT standard -on which few studies are yet- as a reference for this research work.

\begin{table}
\caption{Alignment of the quality metrics proposed in the literature concerning the present work}
\label{tab:tab0}
\begin{tabularx}{\textwidth}{|p{2.55cm}|p{2.3cm}|p{2.6cm}|p{2.65cm}|X|}
\hline
\textbf{Zaveri et al.} & \textbf{Kubler et al.} & \textbf{ISO 25012} & \textbf{ISO 19157}                     & \textbf{This work}     \\
\hline
Accuracy               & Accuracy               & Accuracy           & Correctness                            & Accuracy                  \\
Completeness           & Existence              & Completeness       & Completeness                           & Completeness              \\
Consistency            & Conformance            & Consistency        & Consistency                            & Consistency               \\
Timeliness             & -                      & Currentness        & Temp. Validity                      		& Timeliness                \\
Availability           & Retrievability         & Availability       & Non-q. Correct. 												& Scalability               \\
Accessibility          & Open Data              & Accessibility      & Non-q. Correct. 												& - \\
Possibility            & Open Data              & Portability        & Consistency                    				& Licensing                 \\
Understandability      & -                      & Understandability  & Text Quality	                   				& Readability               \\
-                      & -                      & Compliance         & -                                      & Provenance    \\
-                      & -                      & -                  & -                                      & Compatibility             \\
-                      & -                      & -                  & -                                      & Similarity \\
\hline              
\end{tabularx}
\end{table}

\section{Core Quality Dimensions in Open Data Catalogs}
At first instance, this research assumes an ODC should offer several core characteristics for evaluating the intrinsic properties of the data and, therefore, contribute to its effectiveness in facilitating discovery, understanding, and utilization. The following are those core quality dimensions: accuracy, completeness, consistency, scalability, and timeliness. We offer a formal definition and implementation proposal for each of them below. Please note that all definitions assume that we refer to a well-formed ODC according to the rules of the DCAT standard. It is also assumed that we do not have access to the underlying assets cataloged, so all operations are performed on information contained only and exclusively in the catalog.

\subsection{Accuracy}
ODCs should maintain accurate information about the data assets they represent. The metadata should reflect the actual characteristics of the underlying data. Ensuring accuracy helps users avoid errors when working with the cataloged data. Therefore, in the context of an ODC, data accuracy should be associated with the correctness of the information provided about the data.

\subsubsection{Accuracy in a Data Catalog}
Accuracy is the quality dimension that aims to quantify the extent to which $\mathcal{C}$ contains correct information. For the remainder of this paper, and as the DCAT standard is based on RDF, please consider attributes as the objects of the assertions and relations as the predicates.

\subsubsection{Attribute-level Accuracy}
Attribute-level accuracy refers to the correctness of the attribute values in the ODC. Let $V(a_k)$ be the actual value associated with attribute $a_k$, and $D(a_k)$ be the corresponding value in the ODC.

The attribute-level accuracy for $a_k$ can be measured using an error function $E(a_k)$, which assesses the discrepancy between the actual value $V(a_k)$ and the stored value $D(a_k)$. Therefore, the attribute-level accuracy for an ODC can be defined as shown in Equation \ref{eq:aa}.

\begin{equation}
\text{Attribute-level Accuracy}(\mathcal{C}) = \frac{1}{m} \sum_{k=1}^{m} (1 - E(a_k))
\label{eq:aa}
\end{equation}

where $m$ is the total number of attributes in the ODC.

Please note that this definition involves the average discrepancy across all attributes, providing an overall measure of the ODC accuracy concerning its attributes.

\subsubsection{Relationship-level Accuracy}
Rel.-level accuracy refers to the correctness of relationships in the ODC. Let $R(a_k, a_l)$ be the actual relationship between attributes $a_k$ and $a_l$, and $C(a_k, a_l)$ be the corresponding relationship recorded.

The rel.-level accuracy for $a_k$ and $a_l$ can be measured using an error function $E(a_k, a_l)$, which assesses the discrepancy between the true relationship $R(a_k, a_l)$ and the actual relationship $C(a_k, a_l)$ exactly as shown in Equation \ref{eq:ra}

\begin{equation}
\text{Rel.-level Accuracy}(\mathcal{C}) = \frac{1}{\binom{m}{2}} \sum_{k=1}^{m-1} \sum_{l=k+1}^{m} (1 - E(a_k, a_l))
\label{eq:ra}
\end{equation}

where $\binom{m}{2}$ is the total number of unique attribute pairs in the ODC.

This definition again considers the average discrepancy across all pairs. The idea is to have an overall measure of the accuracy of the catalog's relationships.

\subsubsection{Overall Accuracy}
The overall accuracy of $\mathcal{C}$ combines both accuracy levels as shown in Equation \ref{eq:oa}.

\begin{equation}
\text{Accuracy}(\mathcal{C}) = \alpha \cdot \text{Attr.-level Accuracy}(\mathcal{C}) + (1-\alpha) \cdot \text{Rel.-level Accuracy}(\mathcal{C})
\label{eq:oa}
\end{equation}

where $\alpha$ is the weighting factor about the relative importance of the two types of accuracy seen above.

It is difficult to dictate the values that should ideally be represented generically. However, the absence of particularly significant attributes (attribute level), broken links (attribute level), or duplicate information (relationships level) is detrimental to the accuracy of an ODC. The following is an elaboration of this intuition.

\subsubsection{Check for missing metadata} 
Since accuracy can be measured as the discrepancy between an attribute's value concerning what it should have, we can propose several alternative implementations to measure such accuracy. For example, an ideal dataset should have some attributes such as a title, a description, and a publisher. If they are empty, the discrepancy is assumed to be unreasonable. Algorithm \ref{alg:missing} checks for missing metadata in an ODC, such as missing titles, descriptions, or publishers. It parses the catalog and checks each dataset for missing metadata.

\begin{algorithm}
\caption{Checking for specific missing metadata in an ODC}
\label{alg:missing}
\begin{algorithmic}[1]
\Procedure{CheckMissingMetadata}{catalog}
\For{\textbf{each} dataset \textbf{in} catalog.datasets}
\State $missingCount \gets 0$
\If{\textbf{not} dataset.title}
\State $missingCount \mathrel{+}= 1$
\EndIf
\If{\textbf{not} dataset.description}
\State $missingCount \mathrel{+}= 1$
\EndIf
\If{\textbf{not} dataset.publisher}
\State $missingCount \mathrel{+}= 1$
\EndIf
\State $percentage \gets \frac{missingCount}{3} \times 100$
\State \textbf{Print} dataset.identifier + ' is missing ' + percentage + '\% of required metadata'
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Check for broken links} 
Likewise, the URLs of the ODC should be available; if they are not, the discrepancy between what should be (the ideal value) and what actually is (the actual value) is unreasonable. Algorithm \ref{alg:broken} checks for broken links in an ODC, e.g., broken download links for dataset distributions. It uses a third-party library to check the status code of each download link.

\begin{algorithm}
  \caption{Checking for broken links in an ODC}
  \label{alg:broken}
  \begin{algorithmic}[1]
    \Procedure{CheckBrokenLinks}{catalog}
      \State $totalLinks \gets 0$
      \State $brokenLinks \gets 0$
      \For{\textbf{each} dataset \textbf{in} catalog.datasets}
        \For{\textbf{each} distribution \textbf{in} dataset.distribution}
          \If{distribution.downloadURL}
            \State $totalLinks \mathrel{+}= 1$
            \State $response \gets$ requests.head(distribution.downloadURL)
            \If{response.status\_code $\neq$ 200}
              \State $brokenLinks \mathrel{+}= 1$
            \EndIf
          \EndIf
        \EndFor
      \EndFor
      \If{$totalLinks > 0$}
        \State $percentage \gets \frac{brokenLinks}{totalLinks} \times 100$
      \Else
        \State $percentage \gets 0$
      \EndIf
      \State \textbf{Print} 'Broken links: ' + percentage + '\%'
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\subsubsection{Check for duplicate datasets} 
The presence of duplicate information also cannot be counted when a data stakeholder qualifies as ideal; thus, Algorithm \ref{alg:duplicate} checks for identical datasets in an ODC, i.e., datasets with the same identifier. It uses a set to keep track of the identifiers of each dataset and prints a message with the percentage of duplicate datasets.

\begin{algorithm}
  \caption{Checking for duplicate datasets in an ODC}
  \label{alg:duplicate}
  \begin{algorithmic}[1]
    \Procedure{CheckDuplicates}{catalog}
      \State $totalDatasets \gets \text{length of } catalog.datasets$
      \State $uniqueDatasets \gets totalDatasets$
      \For{$i \gets 0$ \textbf{to} $totalDatasets - 1$}
        \For{$j \gets i + 1$ \textbf{to} $totalDatasets - 1$}
          \If{catalog.datasets[i].identifier $==$ catalog.datasets[j].identifier}
            \State $uniqueDatasets \mathrel{-}= 1$
            \State \textbf{Print} 'Duplicate dataset found: ' + catalog.datasets[j].identifier
            \State \textbf{break}
          \EndIf
        \EndFor
      \EndFor
      \State $percentage \gets \frac{uniqueDatasets}{totalDatasets} \times 100$
      \State \textbf{Print} 'Percentage of unique datasets: ' + percentage + '\%'
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\subsubsection{Overall accuracy}
Finally, Algorithm \ref{alg:check_accuracy} calculates the overall accuracy as a weighted combination of these three measures seen above using parameters \(\alpha\) and \(\beta\), with the remaining weight \((1-\alpha-\beta)\) assigned to the \textit{check\_links\_result}.

\begin{algorithm}
\caption{Check the overall accuracy in an ODC}
\label{alg:check_accuracy}
\begin{algorithmic}[1]
\Procedure{CheckAccuracy}{catalog}
    \State $\text{core\_metadata} \gets \text{CheckMissingMetadata}(\textit{catalog})$
    \State $\text{duplicates\_result} \gets \text{CheckDuplicates}(\textit{catalog})$
    \State $\text{check\_links\_result} \gets \text{CheckBrokenLinks}(\textit{catalog})$
    \State $\text{overall\_accuracy} \gets (\alpha \cdot \text{core\_metadata} + \beta \cdot \text{duplicates\_result} + (1-\alpha-\beta) \cdot \text{check\_links\_result})$
    \State \textbf{Print} 'Overall accuracy: ' + $\text{overall\_accuracy}$ + '\%'
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Completeness}
An ODC should provide coverage of available data assets within an organization, ensuring users can quickly locate and access relevant data.

\subsubsection{Completeness in a Data Catalog}
Completeness is the quality dimension that aims to quantify the extent to which the necessary information is captured in the ODC. This means that, for each object $O_i$ with attributes $A_i = {a_1, a_2, a_3, \ldots, a_m}$, the Equation \ref{eq:comp} must hold.

\begin{equation}
\forall a_k \in A_i, \exists \text{{description}}(a_k, O_i)
\label{eq:comp}
\end{equation}

where $\text{{description}}(a_k, O_i)$ represents the presence of a description for attribute $a_k$ in $O_i$. 

Furthermore, completeness also requires documenting all essential relationships among attributes. Therefore, for any pair of attributes $a_k, a_l \in A_i$, the condition expressed in Equation \ref{eq:comp2} must hold.

\begin{equation}
relationship(a_k, a_l) \Rightarrow doc.\_relationship(a_k, a_l, O_i)
\label{eq:comp2}
\end{equation}

where $relationship(a_k, a_l)$ is an actual relationship between the attributes $a_k$ and $a_l$, and $doc.\_relationship(a_k, a_l, O_i)$ is the description for the relationship between $a_k$ and $a_l$ in $O_i$. Therefore, $\mathcal{C}$ is complete if the above conditions hold for all $O_i$ in $\mathcal{C}$.

The goal is to check if a given ODC contains descriptions for all attributes, capturing their essential characteristics. It also provides documentation for relationships, facilitating the understanding of the data.

\subsubsection{Implementation}
Equation \ref{eq:comp} does not require implementation because its violation will not allow a well-formed ODC. However, Equation \ref{eq:comp2} can be implemented programmatically; Algorithm \ref{alg:complete} iterates through each dataset in the given ODC and checks for the presence of all the fields. The missing fields that are missing are added to a specific list. The completeness percentage is then calculated by dividing the count of complete datasets by the total number of datasets. 

\begin{algorithm}
\caption{Checking the completeness of an ODC}
\label{alg:complete}
\begin{algorithmic}[1]
\Procedure{CheckCompleteness}{catalog}
    \State $total\_datasets \gets \text{length of } catalog.datasets$
    \State $complete\_datasets \gets 0$

    \For{\textbf{each} $dataset$ \textbf{in} $catalog.datasets$}
        \State $missing\_fields \gets$ empty list

        \For{\textbf{each} $field$ \textbf{in} $all\_fields$}
            \If{$\lnot$\textbf{hasattr}($dataset$, $field$)}
                \State $missing\_fields$.append($field$)
            \EndIf
        \EndFor

        \If{empty($missing\_fields$)}
            \State $complete\_datasets \mathrel{+}= 1$
        \EndIf
    \EndFor

    \State $completeness\_percentage \gets \frac{complete\_datasets}{total\_datasets} \times 100$
    \State \textbf{Print} 'Completeness Percentage: ' + $completeness\_percentage$ + '\%'
\EndProcedure
\end{algorithmic}
\end{algorithm}

If any of the fields are missing, we get evidence of the incompleteness of the dataset. This check serves as a primary quality control mechanism to ensure the presence of metadata fields in datasets within an ODC.

\subsection{Consistency}
Consistency is crucial for an ODC to provide reliable information and ensures that an ODC is internally coherent. The catalog should follow consistent naming conventions, data formatting standards, and metadata representations across all entries. However, any interpreter would detect a syntactic inconsistency of these types. Here we refer to a semantic type of inconsistency. Consistency promotes ease of use, trust, and integration with other systems.

\subsubsection{Consistency in a Data Catalog}
Checking for inconsistencies in a given $\mathcal{O}$ of $\mathcal{C}$ involves the identification of contradictory triples $(t_i, t_j) \in T \in \mathcal{O} \in \mathcal{C}$. Two triples belonging to the same $\mathcal{O}$ are deemed contradictory if for two intimately related triples $(t_i, t_j)$ the value of their respective objects is not possible. We can express it formally through the Equation \ref{eq:cons}.

\begin{equation}
    \text{Contradictory}(t_i, t_j) = \neg \left( \text{ValidRel.}(t_i) \land \text{ValidRel.}(t_j) \land \text{Rel.-Compatible}(t_i, t_j) \right)
    \label{eq:cons}
\end{equation}

The average (in)consistency of the ODC is the proportion of (in)consistent objects in the catalog. Some examples could be: the modification date is earlier than the creation date, labels that are unique are duplicated, a tag specifies that the language is English, but the text is in Spanish, or a triple indicates a file in \textit{json} format, but the \textit{url} returns a spreadsheet in proprietary format, etc.

\subsubsection{Implementation}
Algorithm \ref{alg:consistency} assesses the consistency of a given ODC by using a loop to iterate through the triples that must be unique in each dataset and checks for inconsistencies in the attribute values by comparing objects associated with the same (subject, predicate) pair. The inconsistent pairs are added to the \func{contradictions} set, and the inconsistency percentage is printed. The algorithm, in practice, can also be applied to other items, such as distributions.

\begin{algorithm}
\caption{Checking the consistency of an ODC}
\label{alg:consistency}
\begin{algorithmic}[1]
\Procedure{CheckConsistency}{catalog}
\State Initialize an empty set $contradictions$
\State Initialize an empty dictionary $subjects\_predicates$
    
\For{each \textit{dataset} in \textit{datasets}}		
		\For{each triple that must be unique $(subj, pred, obj)$ in \textit{dataset}}
        \If{$(subj, pred)$ is in $subjects\_predicates$}
            \State $objects \gets$ objects associated with $(subj, pred)$ in $subjects\_predicates$
            \If{$obj$ is not in $objects$}
                \State Add $(subj, pred)$ to $contradictions$
            \EndIf
            \State Add $obj$ to $objects$
        \Else
            \State Add $(subj, pred)$ as a new key in $subjects\_predicates$ 
        \EndIf
    \EndFor
\EndFor
\State $consistency\_percentage \gets \frac{contradictions}{total\_triples} \times 100$
\State \textbf{Print} $consistency\_percentage$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Scalability} 
ODCs should be scalable to accommodate growing data volumes and evolving data landscapes. In reality, ODCs are not scalable per se but rather the computational methods that manage them. However, this dimension is critical, and the goal is to build a procedure that elucidates whether it is possible to use scalable methods on the ODC. The idea is to measure the capability of the catalog to expand and evolve alongside the organization's changing data ecosystem.

\subsubsection{Scalability in a Data Catalog}
Scalability refers to efficiently handling an increasing volume of data and changing data requirements while maintaining acceptable performance levels. We can express it formally in the following manner; let $N$ be the total number of records and $T(N)$ be the time units required to perform a specific operation. An ODC $\mathcal{C}$ is considered scalable concerning that operation if the condition in Equation \ref{eq:sca} holds.

\begin{equation}
\lim_{{N \to \infty}} \frac{{T(N)}}{N} = 0
\label{eq:sca}
\end{equation}

This condition indicates that the time required for a specific operation per data record decreases as the data volume increases or at least remains bounded. It implies that the data size does not significantly impact performance by ensuring that the ODC can efficiently handle growing data volumes while maintaining acceptable performance levels. This is generally true with exceptions if a single operation requires processing several attributes.

\subsubsection{Implementation}
Algorithm \ref{alg:scalability} aims to evaluate the scalability of a given ODC. It measures the data volume scalability by assessing the time required for a specific operation per data record by examining the effort needed to update the attribute. The resulting score provides insights into the catalog's scalability.

\begin{algorithm}
\caption{Checking the scalability of an ODC}
\label{alg:scalability}
\begin{algorithmic}[1]
\Procedure{CheckScalability}{catalog}
    \State $start\_time \gets$ \textbf{current\_time}()
    \State \texttt{replace\_attribute\_value}(\texttt{$small\_catalog$}, 's', 'p', 'old', 'new')
    \State $end\_time \gets$ \textbf{current\_time}()
    \State $small\_data\_time \gets end\_time - start\_time / size (\texttt{$small\_catalog$})$

    \State $start\_time \gets$ \textbf{current\_time}()
    \State \texttt{replace\_attribute\_value}(\texttt{$large\_catalog$}, 's', 'p', 'old', 'new')
    \State $end\_time \gets$ \textbf{current\_time}()
    \State $large\_data\_time \gets (end\_time - start\_time) / size (\texttt{$large\_catalog$})$

    \If{$large\_data\_time < small\_data\_time \times \alpha$}
        \State \textbf{print} 'Scalable'
    \Else
        \State \textbf{print} 'Non-scalable'
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Timeliness} 
ODCs should ensure the timeliness and freshness of the cataloged data. They should provide mechanisms to monitor and update metadata to reflect changes in data assets, such as new datasets or deprecated entries. Timely updates enable users to rely on the most recent information.

\subsubsection{Timeliness in a Data Catalog}
Timeliness in an ODC ensures that the catalog contains up-to-date information and can be evaluated from two perspectives: data freshness and availability.

\subsubsection{Data Freshness}
Data freshness is the quality dimension that aims to quantify the extent to which the information in the ODC reflects the current state of its underlying data. More formally, let $C(a_k)$ be the timestamp associated with attribute $a_k$, indicating the last update time of that attribute. Given a current timestamp $t$, an ODC $\mathcal{C}$ is considered to have \textit{data freshness} if the Equation \ref{eq:time} holds.

\begin{equation}
\forall a_k \in \mathcal{C}, \exists t : t > C(a_k)
\label{eq:time}
\end{equation}

\subsubsection{Data Availability}
Data availability aims to quantify the responsiveness of the catalog. Therefore, let $R(a_k)$ be the response time associated with attribute $a_k$, representing the time required to retrieve that attribute. Data availability is about the extent to which the data might be readily accessible within an acceptable time frame. 

We can formally formulate this notion as follows: an ODC $\mathcal{C}$ is considered to have \textit{data availability} if the Equation \ref{eq:time2} holds.

\begin{equation}
\forall a_k \in \mathcal{C}, R(a_k) \leq T
\label{eq:time2}
\end{equation}

where $T$ a predefined time threshold. 

\subsubsection{Implementation}
For availability, a trivial adaptation of the Algorithm \ref{alg:broken} would be sufficient; only a time threshold would need to be set. At the same time, Algorithm \ref{alg:freshness} can assess the freshness of an ODC by sending a GET request to the specified URL. If the response status code is \func{200} (indicating a successful request), it checks the \func{Last-Modified} header of the response to determine the catalog's last modification date. It then compares this date with a freshness threshold. If the last modification date exceeds the freshness threshold, it considers the catalog timely and fresh. Otherwise, it indicates that the catalog is not fresh. If the response status code is not \func{200}, it prints that the catalog is not accessible.

\begin{algorithm}
\caption{Checking the freshness of an ODC}
\label{alg:freshness}
\begin{algorithmic}[1]
\Procedure{CheckCatalogFreshness}{catalog}
    \State $total\_datasets \gets$ \textbf{number\_of\_datasets}(catalog)
    \State $fresh\_datasets \gets 0$
    
    \For{\textbf{each} $dataset$ \textbf{in} $catalog$}
        \State $response \gets$ \textbf{requests.get}(dataset)
        
        \If{$response.status\_code = 200$}
            \State $last\_modified \gets response.headers.get('Last-Modified')$
            
            \If{$last\_modified$ exists}
                \State $last\_modified\_date \gets \textbf{datetime.strptime}(last\_modified, format)$
                \State $current\_time \gets \textbf{datetime.utcnow()}$
                \State $time\_difference \gets current\_time - last\_modified\_date$
                
                \If{$time\_difference.days \leq 30$} \Comment{Check if dataset is fresh within 30 days}
                    \State $fresh\_datasets \gets fresh\_datasets + 1$
                \EndIf
            \EndIf
        \EndIf
    \EndFor
    
    \State $freshness\_percentage \gets \frac{fresh\_datasets}{total\_datasets} \times 100$
    \State \textbf{Print} 'Fresh datasets: ', fresh\_datasets, '/', total\_datasets
    \State \textbf{Print} 'Freshness percentage: ', freshness\_percentage, '\%'
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Cross-Catalog Quality Dimensions}
Cross-catalog dimensions are those that involve more than one ODC at the same time. This work focuses on ODC compatibility and similarity, which are distinct concepts, although often used interchangeably, in data management. Compatibility refers to the ability of an ODC to seamlessly integrate with various data sources and technologies, enabling adequate replacement. It ensures interoperability between ODCs and different data platforms. On the other hand, similarity aims to identify similarities between ODCs. This involves analyzing data content and structure to uncover relationships and patterns. While compatibility focuses on connectivity and interoperability, similarity emphasizes the analysis of data attributes and properties for knowledge extraction.

\subsection{Compatibility}
The compatibility of two ODCs refers to their respective datasets' integration and complementarity. In order to assess compatibility, several key factors must be considered. Firstly, the format and structure of both catalogs. Secondly, common identifiers across datasets facilitate the merging or replacement process.

Compatibility also relies on shared coverage and standardized geographic identifiers or coordinate systems when geographic information is involved. Temporal aspects should also be examined to determine whether the time ranges of both datasets align or complement each other. Finally, data licensing must be carefully evaluated to ensure that both ODCs permit the same actions.

More formally; let $\mathcal{C}_1$ and $\mathcal{C}_2$ be two ODCs. Their compatibility can be defined as shown in Equation \ref{eq:compat}.

\begin{equation}
\text{comp}(\mathcal{C}_1, \mathcal{C}_2) = \frac{|\mathcal{C}_1 \cap \mathcal{C}_2|}{|\mathcal{C}_1|}
\label{eq:compat}
\end{equation}

where $|\mathcal{C}_1|$ is the cardinality of $\mathcal{C}_1$,and $\mathcal{C}_1 \cap \mathcal{C}_2$ is the intersection of $\mathcal{C}_1$ and $\mathcal{C}_2$. Therefore, this dimension estimates the cardinality ratio of the intersection between the two ODCs to the cardinality of $\mathcal{C}_1$ (or alternatively $\mathcal{C}_2$). In this way, the compatibility score can range from 0 to 100, where a value of 0 indicates that both catalogs are incompatible since they have no common elements. In contrast, a value of 100 indicates that the catalogs are entirely compatible since all their elements are common.

\subsubsection{Implementation}
Algorithm \ref{alg:compatibility} aims to assess the compatibility of two ODCs. To do that, it iterates through each dataset in the first catalog and checks if it exists as a subset in the second catalog based on matching identifiers. The compatibility score is calculated as the percentage of datasets in the first catalog found as subsets in the second catalog, indicating their compatibility.

\begin{algorithm}
\caption{Checking the compatibility of Two ODCs}
\label{alg:compatibility}
\begin{algorithmic}[1]
\Procedure{CheckCompatibility}{$catalog_1, catalog_2$}
    \State $compatibility \gets 0$
    \State $total\_datasets \gets 0$

    \For{\textbf{each} $dataset_1$ \textbf{in} $catalog_1.datasets$}
        \If{\Call{IsSubset}{$dataset_1, catalog_2$}}
            \State $compatibility \mathrel{+}= 1$
        \EndIf
        \State $total\_datasets \mathrel{+}= 1$
    \EndFor

    \State $overall\_compatibility \gets \frac{compatibility}{total\_datasets} \times 100$

    \State \textbf{Print} 'Overall compatibility between the catalogs: ' + overall\_compatibility + '\%'

\EndProcedure

\Statex

\Function{IsSubset}{$dataset, catalog$}
    \For{\textbf{each} $dataset_2$ \textbf{in} $catalog.datasets$}
        \If{$dataset.identifier = dataset_2.identifier$}
            \State \textbf{return} \textbf{true}
        \EndIf
    \EndFor
    \State \textbf{return} \textbf{false}
\EndFunction

\end{algorithmic}
\end{algorithm}


\subsection{Similarity}
Assessing the similarity between ODCs is crucial in many integration tasks. Similarity assessment enables comparing and aligning ODCs from different sources, facilitating data integration, exchange, and collaboration across systems \cite{key-martinez-eswa2}. Quantifying the degree of similarity between catalogs makes it possible to identify shared attributes, and match corresponding items \cite{key-martinez-jfis}. 

Furthermore, similarity assessment can facilitate search and retrieval, enabling users to locate relevant ODCs based on their similarity to a given input \cite{key-martinez-jiis}. This capability enhances data discovery, facilitates reuse, and promotes efficient decision-making processes. Therefore, similarity assessment is essential in promoting data interoperability, enabling effective data integration, and facilitating data utilization across diverse environments \cite{key-martinez-mlwa}.

In this work, we refer to attribute-level similarity that measures the similarity of the attribute values between corresponding attributes in the two ODCs. The idea is to quantify the resemblance between attribute values based on appropriate similarity measures, such as cosine similarity, or Jaccard similarity. Therefore, the attribute-level similarity $\text{Sim}_{\text{attribute}}(\mathcal{C}_1, \mathcal{C}_2)$ can be defined as shown in Equation \ref{eq:sim2}.

\begin{equation}
Sim_{attribute}(\mathcal{C}_1, \mathcal{C}_2) = \frac{1}{K} \sum_{i=1}^{K} Sim_{value}(O_i, O'_i)
\label{eq:sim2}
\end{equation}

where $\text{Sim}_{\text{value}}(O_i, O'_i)$ represents the similarity between the attribute values of the corresponding attributes in objects $O_i$ and $O'_i$. 

Algorithm \ref{alg:sim2} takes two ODCs as input and calculates the similarity score at the attribute level. It first extracts titles and descriptions from both catalogs, preprocesses the text, calculates the Jaccard similarity between titles and descriptions, and computes the overall similarity score. The function returns the overall similarity score, indicating how similar the two catalogs are.

\begin{algorithm}
\caption{Checking the similarity of two ODCs}
\label{alg:sim2}
\begin{algorithmic}[1]
\Procedure{CheckSimilarity}{$catalog_1, catalog_2$}
    \If{$\text{areCatalogsIdentical}(catalog_1, catalog_2)$}
        \State \textbf{Print} 'The overall similarity is 100 \%'
				\State \textbf{return}
    \EndIf
		
    \State $titles1 \gets catalog_1.title$
    \State $titles2 \gets catalog_2.title$

    \State $descriptions1 \gets catalog_1.description$
    \State $descriptions2 \gets catalog_2.description$

    \State Preprocess $titles1$ and $titles2$ by tokenization, lowercase conv., and stopwords remov.
    \State Preprocess $descriptions1$ and $descriptions2$ similarly

    \State $title\_similarity \gets \text{JaccardSimilarity}(titles1, titles2)$
    \State $description\_similarity \gets \text{JaccardSimilarity}(descriptions1, descriptions2)$

    \State $overall\_similarity \gets (title\_similarity + description\_similarity) / 2$

    \State \textbf{Print} 'The overall similarity is ' + $overall\_similarity$ + '\%'
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Non-Core Quality Dimensions}
In the context of an ODC, non-core quality dimensions refer to additional aspects or characteristics of the datasets that complement the core quality dimensions. While core quality dimensions are essential for evaluating the intrinsic properties of the data (e.g., accuracy, completeness, consistency, etc.), non-core quality dimensions provide supplementary information that enhances the overall understanding and usability of the datasets.

Non-core quality dimensions enrich the dataset's metadata by providing additional context, usage guidelines, and documentation, ensuring that users have a comprehensive understanding of the dataset beyond its intrinsic properties. Such comprehensive context is pivotal in guiding users toward making informed decisions. Incorporating non-core quality dimensions within an ODC promotes transparency, accessibility, and optimal utilization of OD resources. Therefore, we will investigate how these non-core quality dimensions for ODCs can be effectively evaluated in the subsequent subsections.

\subsection{Lineage and Provenance} 
ODCs should incorporate data lineage and provenance information whenever possible. Data lineage tracks the history of data transformations and provides insights into how data has been derived, increasing transparency and aiding data quality assessment. Provenance information captures the source and history of data assets, enhancing credibility.

Algorithm \ref{alg:lineage} assesses the lineage and provenance of an ODC. It iterates through each dataset in the catalog. It calculates a lineage score and a provenance score based on the presence of lineage information, ancestors, descendants, provenance information, data sources, and data processing steps. We only present here a proposal for implementation, but the resulting average lineage and provenance scores enable researchers and practitioners to comprehensively evaluate the historical relationships and origins of the cataloged data.

\begin{algorithm}
\caption{Checking the lineage and provenance of an ODC}
\label{alg:lineage}
\begin{algorithmic}[1]
\Procedure{CheckLineageProvenance}{catalog}
    \State $lineage\_score \gets 0$
    \State $provenance\_score \gets 0$
    \State $total\_datasets \gets \text{length of } catalog.datasets$

    \For{\textbf{each} $dataset$ \textbf{in} $catalog.datasets$}
        \If{$dataset.hasLineageInformation$}
            \State $lineage\_score \mathrel{+}= 1$
        \EndIf

        \If{$dataset.hasAncestors$}
            \State $lineage\_score \mathrel{+}= 1$
        \EndIf

        \If{$dataset.hasDescendants$}
            \State $lineage\_score \mathrel{+}= 1$
        \EndIf

        \If{$dataset.hasProvenanceInformation$}
            \State $provenance\_score \mathrel{+}= 1$
        \EndIf

        \If{$dataset.hasDataSources$}
            \State $provenance\_score \mathrel{+}= 1$
        \EndIf

        \If{$dataset.hasDataProcessingSteps$}
            \State $provenance\_score \mathrel{+}= 1$
        \EndIf
    \EndFor

    \State $avg\_lineage \gets \frac{lineage\_score}{total\_datasets} \times 100$
    \State $avg\_provenance \gets \frac{provenance\_score}{total\_datasets} \times 100$

    \State \textbf{Print} 'Average Lineage Score: ' + avg\_lineage
    \State \textbf{Print} 'Average Provenance Score: ' + avg\_provenance

\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Readability} 
The readability of free-form text in an ODC is crucial for comprehension and data usability. Clear and concise language facilitates efficient data discovery and other potential applications without unnecessary hurdles. Transparent communication of text facilitates responsible data utilization. Additionally, readable catalogs build trust, credibility and encourage data sharing and stakeholder collaboration.

Algorithm \ref{alg:readability} aims to assess the readability of an ODC. It calculates the average readability score by considering the Flesch-Kincaid Grade Level \cite{key-martinez-read} for the title and description of each dataset in the catalog. Using established metrics for readability assessment, such as the Flesch-Kincaid Grade Level, provides a quantitative measure of the catalog's readability, which can help evaluate its accessibility and comprehensibility to a broader audience.

\begin{algorithm}
\caption{Checking the readability of an ODC}
\label{alg:readability}
\begin{algorithmic}[1]
\Procedure{CheckReadability}{catalog}
    \State $total\_datasets \gets \text{length of } catalog.datasets$
    \State $readability\_score \gets 0$

    \For{\textbf{each} $dataset$ \textbf{in} $catalog.datasets$}
        \State $title\_text \gets dataset.title$
        \State $description\_text \gets dataset.description$

        \If{$\text{length of } title\_text > 0$}
            \State $readability\_score \mathrel{+}= \Call{CalculateFleschKincaidGrade}{title\_text}$
        \EndIf

        \If{$\text{length of } description\_text > 0$}
            \State $readability\_score \mathrel{+}= \Call{CalculateFleschKincaidGrade}{description\_text}$
        \EndIf
    \EndFor

    \State $average\_readability \gets \frac{readability\_score}{total\_datasets}$

    \State \textbf{Print} 'Average readability score: ' + average\_readability

\EndProcedure

\Statex

\Function{CalculateFleschKincaidGrade}{$text$}
    \State $num\_sentences \gets$ number of sentences in $text$
    \State $num\_words \gets$ number of words in $text$
    \State $num\_syllables \gets$ number of syllables in $text$

    \State $score \gets 0.39 \times \left(\frac{num\_words}{num\_sentences}\right) + 11.8 \times \left(\frac{num\_syllables}{num\_words}\right) - 15.59$

    \State \textbf{return} $score$
\EndFunction

\end{algorithmic}
\end{algorithm}


\subsection{Licensing} 
In ODCs, licensing is pivotal in shaping the dissemination and utilization of valuable information. ODCs serve as repositories of diverse datasets, facilitating transparency and collaboration among the data stakeholders. Within this context, implementing suitable licensing mechanisms becomes imperative to ensure equitable access, protect intellectual property rights, and facilitate responsible data utilization.

Algorithm \ref{alg:check_license} is designed to ascertain the presence of licensed items within a given data catalog. It would also be possible to operate by iteratively examining each data item in the catalog, if necessary. 

\begin{algorithm}
\caption{Checking the licensing rights of an ODC}
\label{alg:check_license}
\begin{algorithmic}[1]
\Procedure{CheckLicensingRights}{catalog}
    \State $\text{is\_licensed} \gets \text{False}$
    
    \For{$\text{data\_item}$ \textbf{in} $\text{data\_catalog}$}
        \If{$\text{data\_item.license}$ is \textbf{not} \text{None}}
            \State $\text{is\_licensed} \gets \text{True}$
						\State $\text{counter} \gets \text{counter + 1}$
        \EndIf
    \EndFor

		\State \State $\text{percentage\_licensing} \gets \frac{counter}{total\_data\_items} \times 100$
    \State \textbf{Print} 'The percentage of licensing rights is ' $\text{percentage\_licensing}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Discussion}
We have seen how our framework contributes to data management of metadata by providing a practical approach for researchers and practitioners to assess the quality of ODCs. The idea of adopting this framework is that data consumers can establish a robust foundation for data-driven initiatives, improving reliability and enabling trust in the cataloged data.

Several dimensions have been addressed in one way or another in the literature, but the framework presented here does not incorporate them. Below, the reasons are explained:

\begin{enumerate}
	\item Accessibility \cite{key-Lobe}: This is considered a particular case of the notion of accuracy, as a non-functional link can impact the accuracy of the represented information.
	\item Findability \cite{key-wilkinson}: It is seen as a particular case of the notion of completeness, where more reported information increases the chances of finding assets.
	\item Degree of openness \cite{key-Neumaier}: Although crucial for the OD movement, the proposed framework does not consider it a quality-related aspect. For instance, different file formats (.xlsx vs. .csv) do not provide clues about the higher value.
	\item Syntactic inconsistency \cite{key-Debattista}: Parsers automatically detect this type of inconsistency, and the ODC is assumed to be well-formed.
	\item Profile inconsistency \cite{key-kirstein2}: The framework's mission is to remain as generic as possible, and thus, it does not cover situations where specific information needs to impose restrictions on certain values.
\end{enumerate}

However, promising lines exist for future research and development in this domain. Expanding the framework to incorporate additional quality dimensions is one potential direction. Potential novel dimensions such as interpretability or security can lead the framework to provide a more comprehensive quality assessment. 

Another approach for future exploration lies in leveraging advanced machine learning algorithms to enhance the accuracy and efficiency of the quality assessment framework. Integrating deep learning and natural language processing techniques can enable the framework to uncover intricate quality patterns and anomalies, entrusting data stakeholders to gain deeper insights into the cataloged data.

Furthermore, validating the framework on a broad range of domains and use cases is crucial. Conducting empirical studies and real-world evaluations across diverse industries and application scenarios will help demonstrate the framework's applicability, effectiveness, and generalizability, further establishing its utility as a practical solution for ODC quality assessment.

In summary, our framework can serve as a foundational step toward addressing the challenges of ODC quality, and further research in these areas can significantly advance the field of data management and governance.


\section{Conclusion}
This work has presented a framework for automatically determining the quality of ODCs. The framework provides a comprehensive approach by addressing an efficient quality assessment in the face of increasing data volumes and diversity. The framework's primary objective is to provide a comprehensive approach that ensures a reliable assessment of ODC quality, given the ever-growing data volumes and the inherent diversity of data sources and formats. Our proposed method facilitates a systematic evaluation across multiple quality dimensions, encompassing core metrics such as accuracy, completeness, consistency, scalability, and timeliness, as well as compatibility and similarity when comparing catalogs, and non-core metrics such as lineage and provenance, readability, and licensing.

Our goal is to provide OD consumers with a sophisticated solution for quality assessment. In fact, the framework equips stakeholders with the means to establish sound data management and governance practices. The core idea is to infuse confidence in their data assets' reliability and utility, promoting effective data utilization and facilitating informed decision-making processes. Our proposed implementations using the DCAT vocabulary showcase the practical application of the framework, demonstrating its effectiveness in identifying and quantifying quality issues. 

Our research results give us an idea about the significance of automated quality assessment in enhancing the usability and trustworthiness of ODCs, thus facilitating efficient data discovery, integration, and decision support. Therefore, the framework presented in this paper contributes to data management and governance, offering data stakeholders a solution for ensuring metadata quality. 

Future research directions may focus on expanding the framework to incorporate additional quality dimensions, exploring advanced machine learning algorithms for quality assessment, and validating the framework on a broader range of domains and use cases. The idea behind continuously striving for improvement and exploring new dimensions is to encourage the research community to drive progress and innovation, ensuring the effectiveness of data-driven initiatives.

\section*{Acknowledgments}
The research reported in this paper has been funded by the Federal Ministry for Climate Action, Environment, Energy, Mobility, Innovation, and Technology (BMK), the Federal Ministry for Digital and Economic Affairs (BMDW), and the State of Upper Austria in the frame of SCCH, a center in the COMET - Competence Centers for Excellent Technologies Programme managed by Austrian Research Promotion Agency FFG.

%
% The following two comjob applicantds are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
%\section*{References}
%\bibliographystyle{plain}
%\bibliography{mybib}

%\bibliographystyle{plain}
\bibliography{mybib}


\end{document}
