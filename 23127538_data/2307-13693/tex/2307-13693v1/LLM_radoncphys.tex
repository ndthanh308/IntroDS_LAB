\documentclass[11pt, letterpaper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{authblk}
\usepackage{appendix}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage{parskip}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{adjustbox}
% \usepackage{mathptmx}

\geometry{
  letterpaper,
  top=1in,
  bottom=1in,
  left=1in,
  right=1in,
  headheight=13.6pt
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=red,
}

\definecolor{headercolor}{RGB}{0, 50, 100}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
% \fancyhead[C]{\textcolor{headercolor}{LLMs - radiation oncology physics}}
\fancyfoot[C]{\thepage}

\title{Evaluating Large Language Models for Radiology Natural Language Processing}


\date{}

% Zhengliang Liu, Tianyang Zhong, Yiwei Li, Co-first authors.

% Yutong Zhang, Yi Pan, Zihao Zhao, Peixin Dong, Cao Chao, Yuxiao Liu, Peng Shu, Co-second authors.

% Zihao Wu, Chong Ma, Jiaqi Wang, Sheng Wang, Mengyue Zhou, Zuowei Jiang, Chunlin Li, Shaochen Xu, Haixing Dai, Xu Liu, Lin Zhao, Peilong Wang.   

% Pingkun Yan, Jun Liu, Bao Ge, Dajiang Zhu, Xiang Li, Wei Liu, Xintao Hu, Xi Jiang, Shu Zhang, Xin Zhang, Tuo Zhang, Shijie Zhao, Dinggang Shen, Tianming Liu  


%Co-first authors
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author[1]{Zhengliang Liu \thanks{Co-first authors.}}
\author[2]{Tianyang Zhong \samethanks}
\author[1]{Yiwei Li \samethanks}
%Co-second authors
\author[8]{Yutong Zhang \thanks{Co-second authors.}}
\author[6]{Yi Pan \samethanks}
\author[11]{Zihao Zhao \samethanks}
\author[2]{Peixin Dong \samethanks}
\author[4]{Chao Cao \samethanks}
\author[11]{Yuxiao Liu \samethanks}
\author[1]{Peng Shu \samethanks}
\author[2]{Yaonai Wei \samethanks}
%rest auhtors
\author[1]{Zihao Wu}
\author[2]{Chong Ma}
\author[10]{Jiaqi Wang}
\author[17]{Sheng Wang}
\author[2]{Mengyue Zhou}
\author[2]{Zuowei Jiang}
\author[2]{Chunlin Li}
\author[1]{Shaochen Xu}
\author[4]{Lu Zhang}
\author[1]{Haixing Dai}
\author[18]{Kai Zhang}
\author[2]{Xu Liu}
\author[1]{Lin Zhao}
\author[7]{Peilong Wang}

%Professors
\author[14]{Pingkun Yan}
\author[15]{Jun Liu}
\author[9]{Bao Ge}
\author[18]{Lichao Sun}
\author[4]{Dajiang Zhu}
\author[3]{Xiang Li}
\author[7]{Wei Liu}
\author[2]{Xiaoyan Cai}
\author[2]{Xintao Hu}
\author[5]{Xi Jiang}
\author[10]{Shu Zhang}
\author[2]{Xin Zhang}
\author[2]{Tuo Zhang}
\author[2]{Shijie Zhao}
\author[3]{Quanzheng Li}
\author[13]{Hongtu Zhu}
\author[11,12,16]{Dinggang Shen}
\author[1]{Tianming Liu \thanks{Corresponding author. Email: tianming.liu@gmail.com.}}


\affil[1]{School of Computing, University of Georgia, GA, USA}
\affil[2]{School of Automation, Northwestern Polytechnical University, Xi'an 710072, China}
\affil[3]{Department of Radiology, Massachusetts General Hospital and Harvard Medical School, MA, USA}
\affil[4]{Department of Computer Science and Engineering, University of Texas at Arlington, TX, USA}
\affil[5]{School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu 611731, China}
\affil[6]{Glasgow College, University of Electronic Science and Technology of China, Chengdu 611731, China }
\affil[7]{Department of Radiation Oncology, Mayo Clinic, Phoenix, Arizona, USA}
\affil[8]{Institute of Medical Research, Northwestern Polytechnical University, Xi'an 710072, China}
\affil[9]{School of Physics and Information Technology, Shaanxi Normal University, Xi’an 710119 China}
\affil[10]{School of Computer Science, Northwestern Polytechnical University, Xi'an 710072, China}
\affil[11]{School of Biomedical Engineering, ShanghaiTech University, and Shanghai Clinical Research and Trial Center, Shanghai 201210, China}
\affil[12]{Shanghai United Imaging Intelligence Co., Ltd.}
\affil[13]{Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA}
\affil[14]{Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies at Rensselaer Polytechnic Institute, Troy, New York 12180, USA}
\affil[15]{Department of Radiology, Second Xiangya Hospital, Changsha 410011, China}
\affil[16]{Shanghai Clinical Research and Trial Center}
\affil[17]{School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200240, China}
\affil[18]{Department of Computer Science and Engineering, Lehigh University, PA, USA}

\begin{document}

\maketitle

\begin{abstract}
The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP). LLMs have revolutionized a multitude of domains, and they have made a significant impact in the medical field. Large language models are now more abundant than ever, and many of these models exhibit bilingual capabilities, proficient in both English and Chinese. However, a comprehensive evaluation of these models remains to be conducted. This lack of assessment is especially apparent within the context of radiology NLP. This study seeks to bridge this gap by critically evaluating thirty two LLMs in interpreting radiology reports, a crucial component of radiology NLP. Specifically, the ability to derive impressions from radiologic findings is assessed. The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain.

\end{abstract}

\section{Introduction}
In recent years, large language models (LLMs) \cite{liu2023summary,holmes2023evaluating,wu2023exploring,wang2023review,li2023artificial,rezayi2023exploring,liu2022survey} have emerged as prominent tools in the realm of natural language processing (NLP). Compared with traditional NLP models, LLMs are trained on expansive datasets and demonstrate impressive capabilities ranging from language translation to creative content generation, and problem-solving.  For example, OpenAI's dialogue model, ChatGPT\footnote{\url{https://openai.com/blog/chatgpt}}, has garnered widespread attention due to its outstanding performance, sparking a trend in the development of LLMs that has had profound effects on the growth of the entire AI community.

ChatGPT was developed based on GPT-3.5 version \cite{brown2020language}, released in November 2022. It is trained on a massive dataset of text and code and can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. It is currently widely used in many areas, such as intelligent customer service, summary generation, and more, offering greater possibilities for language interaction \cite{lund2023chatting,sallam2023chatgpt,sallam2023utility}. The multimodal model GPT-4\footnote{\url{https://openai.com/research/gpt-4}} with 1.8 trillion parameters was released in March 2023, with overall performance and accuracy surpassing the previous version. Also, the introduction of ChatGPT plugin functionality has directly endowed ChatGPT with the ability to use other tools and connect to the internet, breaking the constraints of the model's data. The popular rise of ChatGPT has led to a surge in the development of new LLMs. There are now several hundred open-source LLMs available, such as Hugging Face's BLOOM \cite{scao2022bloom}, which is trained with 176 billion parameters across 46 natural languages and 13 programming languages while most LLMs are not publicly released and are mainly based on Latin languages with English as the main language. Also, starting from Meta's open source LLaMA \cite{touvron2023llama} series of models, researchers from Stanford University and other institutions have successively open-sourced LLaMA-based lightweight classes such as Alpaca\footnote{\url{https://crfm.stanford.edu/2023/03/13/alpaca.html}}, Koala\footnote{\url{https://bair.berkeley.edu/blog/2023/04/03/koala/}}, and Vicuna\footnote{\url{ https://github.com/lm-sys/FastChat}}, etc. emerged. The research and application threshold of this type of model is greatly reduced, and the training, and reasoning costs have been repeatedly reduced.

Similarly, LLM ecology in other countries such as China has also begun to take shape. At present, the LLMs in China can basically be divided into three tracks: companies, institutions, and universities, such as Baidu's ERNIE Bot\footnote{\url{https://yiyan.baidu.com/}}, Huawei's Pangu models series, IDEA's Ziya-LLaMA\footnote{\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}} series, Fudan University's MOSS\footnote{\url{https://github.com/OpenLMLab/MOSS}}. Many corresponding models can be classified as based on LLaMA, chatGLM\footnote{\url{https://github.com/THUDM/ChatGLM-6B}}, BLOOM, and other models that do not follow the transformer approach, such as baichuan\footnote{\url{https://modelscope.cn/models/baichuan-inc/Baichuan-13B-Base/summary}}, RWKV \cite{peng2023rwkv}. These models are being used for various tasks, including natural language understanding, natural language generation, machine translation, and question answering.

The versatility of these models extends into the medical field \cite{eggmann2023implications,lievin2022can,wang2023large,taylor2022galactica,singhal2023towards,ufuk2023role,yunxiang2023chatdoctor,haupt2023ai,zhao2023brain}. For example, the LLMs represented by chatGPT can be used to generate personalized medical reports \cite{liao2023differentiate,dai2023chataug,liu2023deid,ma2023impressiongpt}, realize online medical consultation, remote medical diagnosis and guidance \cite{li2023artificial,rezayi2022clinicalradiobert}, and realize medical data mining, disease prediction and diagnosis reasoning \cite{bi2023community,ding2023deep,ding2022accurate,qiang4309357deep,dai2023ad}, etc. Especially in radiology \cite{pons2016natural,casey2021systematic,adams2023leveraging,doshi2023utilizing,lyu2023translating,ali2023using,liu2023radiology,zhang2023segment}, one significant application lies in the interpretation of radiological images. Taking advantage of the multimodal input of GPT-4, models can understand and generate descriptions of new radiological images, such as X-rays, MRIs, or CT scans. This helps to automate the process of preliminary diagnosis, potentially saving doctors' time, and could be particularly useful in situations where there exists a shortage of trained radiologists, and doctors no longer need to enter the patient's electronic medical record manually. In addition, these models are capable of aiding in clinical decision-making. By processing and analyzing a patient's radiological data along with other relevant medical information, LLMs can generate patient-specific reports and provide possible diagnoses, treatment recommendations, or potential risks. Furthermore, Advances in LLM have led to the development of many specialized biomedical LLMs, such as HuatuoGPT 
\cite{zhang2023huatuogpt}, an open-source model from Chinese University of Hong Kong with the BLOOMZ \cite{muennighoff2023crosslingual} as backbone, uses the data distilled from ChatGPT and the real data of doctors in the supervised fine-tuning stage. And XrayGLM,\footnote{\url{https://github.com/WangRongsheng/XrayGLM}}, the first Chinese multimodal large model dedicated to the diagnosis of chest X-rays, which is based on VisualGLM-6B\footnote{\url{https://github.com/THUDM/VisualGLM-6B}} and then fine-tuned on two open chest X-rays datasets. Others include QiZhenGPT\footnote{\url{https://github.com/CMKRG/QiZhenGPT}}, BioMedLM\footnote{\url{http://github.com/standford-crfm/BioMedLM.html}}, BioGPT \cite{luo2022biogpt}, PMC-LLaMA \cite{wu2023pmc}, Med-PaLM \cite{singhal2023large}, etc., which demonstrate significant potential of LLMs in the medical field. 

Despite the escalating ubiquity of LLMs in various sectors, a comprehensive understanding and evaluation of their performance, particularly in the specialized field of radiology NLP, remains noticeably absent. This paucity of knowledge is even more stark when we consider the emerging LLMs developed in other countries such as China, a significant portion of which boast robust bilingual capabilities in both English and Chinese. Often untapped and under-evaluated, these models could offer unique advantages in processing and understanding multilingual medical data. The scarcity of in-depth, scientific performance evaluation studies on these models in the medical and radiology domains signals a significant knowledge gap that needs addressing. Given this backdrop, we believe it is paramount to undertake a rigorous and systematic exploration and analysis of these world-wide LLMs. This would not only provide a better understanding of their capabilities and limitations but also position them within the global landscape of LLMs. By comparing them with established international contenders, we aim to shed light on their relative strengths and weaknesses, providing a more nuanced understanding of the application of LLMs in the field of radiology. This, in turn, would potentially contribute to the optimization and development of more efficient and effective NLP/LLM tools for radiology.

Our study is focused on the crucial aspect of radiology NLP, namely, interpreting radiology reports and deriving impressions from radiologic findings. We rigorously evaluate the selected models using a robust dataset of radiology reports, benchmarking their performance against a variety of metrics.

Initial findings reveal that distinct differences were observed in the models' respective strengths and weaknesses. The implications of these findings, as well as their potential impact on the application of LLMs in radiology NLP, are discussed in detail.

In the grand scheme, this study serves as a pivotal step towards the wider adoption and fine-tuning of LLMs in radiology NLP. Our observations and conclusions are aimed at spurring further research, as we firmly believe that these LLMs can be harnessed as invaluable tools for radiologists and the broader medical community.
\section{Related work}

\subsection{Evaluating Large Language Models}
The rapid development of LLMs has been revolutionizing the field of natural language processing \cite{J1,J2,J3}. These powerful models have shown significant performance in many NLP tasks, like natural language generation (NLG) and even Artificial General Intelligence (AGI). However, utilizing these models effectively and efficiently requires a practical understanding of their capabilities and limitations, and overall performance, so evaluating these models is of paramount importance. 

To compare the capabilities of different LLMs, researchers usually test with benchmark datasets in various fields (such as literature, chemistry, biology, etc.), and then evaluate their performance according to traditional indicators (such as correct answer rate, recall rate, and F1 value). The most recent study from OpenAI \cite{J4} includes the pioneering research study that assesses the performance of large language models (i.e. GPT-4) on academic and professional exams specifically crafted for educated individuals. The findings demonstrate exceptional performance of GPT-4 across a diverse array of subjects, encompassing the Uniform Bar Exam and GRE. Furthermore, an independent study conducted by Microsoft reveals that GPT-4 outperforms the USMLE, the comprehensive medical residents' professional examination, by a significant margin \cite{J5}. Holmes et al. \cite{J6} explore the utilization of LLMs in addressing radiation oncology physics inquiries, offering insights into the scientific and medical realms. This research serves as a valuable benchmark for evaluating the performance in radiation oncology physics scenarios of LLMs. 

Unlike the studies above that used traditional assessment research, Zhuang et al. \cite{J8} introduced a novel cognitive science-based \cite{J7} methodology for evaluating LLMs. Specifically, inspired by computerized adaptive testing (CAT) in psychometrics, they proposed an adaptive testing framework for evaluating LLMs that adjusts the characteristics of test items, such as difficulty level, based on the performance of individual models. They performed fine-grained diagnosis on the latest 6 instruction-tuned LLMs (i.e. ChatGPT (OpenAI), GPT-4 (OpenAI), Bard (Google), ERNIEBot (Baidu), QianWen (Alibaba), Spark (iFlytek)) and ranked them from three aspects of Subject Knowledge, Mathematical Reasoning, and Programming. The findings demonstrate a noteworthy superiority of GPT-4 over alternative models, achieving a cognitive proficiency level comparable to that of middle-level students. Similarly, traditional evaluation approaches are also not suitable for code generation tasks. Zheng et al. \cite{J9} presented a multilingual model with 13 billion parameters for code generation, and building upon HumanEval (Python only), and they developed the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.

Nevertheless, a conspicuous dearth of assessment pertaining to substantial models in the realm of NLP within the domain of radiology persists. Consequently, this investigation endeavors to furnish an analytical appraisal of substantial models operating in the field of radiology. This inquiry represents the pioneering endeavor to encompass an exhaustive evaluation of large-scale language models (LLMs) within the purview of radiology, thereby serving as a catalyst for future investigations aimed at appraising the efficacy of LLMs within intricately specialized facets of medical practice.

\subsection{Large Language Model Development in Other Countries}
Many teams in other countries have conducted numerous attempts and research on LLMs. Examples include MOSS, developed by the OpenLMLab team at Fudan University, the BaiChuan series models developed by the Baidu team, the Sun and Moon model developed by SenseTime, PanGu developed by Huawei, ChatGLM-med developed by HIT, and YuLan-Chat developed by RUC. An introduction to these models will follow.
\par 
MOSS \cite{sun2023moss} is an open-source series of large language models in Chinese and English, consisting of multiple versions. The base model, moss-moon-003-base, is the foundation of all MOSS versions and is pre-trained on high-quality Chinese and English corpora, encompassing 700 billion words. To adapt MOSS to dialogue scenarios, the model moss-moon-003-sft is fine-tuned on over 1.1 million rounds of dialogue data using the base model. It possesses the capabilities of instruction-following, multi-turn dialogue comprehension, and avoidance of harmful requests. Furthermore, in addition to providing consultation and question-answering functionalities, the moss-moon-003-sft-plugin model is trained not only on over 1.1 million dialogue data but also on more than 300,000 enhanced dialogue data with plugins. It extends the capabilities of the moss-moon-003-sft model to include plugin functionalities such as using search engines, generating images from text, performing calculations, and solving equations. Both moss-moon-003-sft and moss-moon-003-sft-plugin have versions with 4-bit quantization and 8-bit quantization, making them suitable for lower resource environments.
\par 
BaiChuan, an open-source large-scale pretraining language model developed by BaiChuan Intelligence, is based on the Transformers \cite{vaswani2017attention} architecture. It has been trained on 1.2 trillion tokens and contains 7 billion parameters, supporting both Chinese and English languages. During training, a context window length of 4096 tokens was used. In actual testing, the model can also scale to over 5000 tokens. It achieved the best performance among models of the same size on standard Chinese and English language benchmarks (C-Eval \cite{huang2023ceval}/MMLU \cite{hendrycks2021measuring}).
\par 
Chat-GLM-6B is an open bilingual language model based on the General Language Model(GLM) \cite{du2022glm} framework, with 6.2 billion parameters. The model is trained for about 1 trillion tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback.\cite{ouyang2022training} It is optimized for Chinese QA and dialogue. Furthermore, the model is capable of generating answers that align with human preference.Chat-GLM-med \cite{ChatGLM-Med} is a variation of Chat-GLM-6B that is fine-tuned specifically for Chinese medical instructions. This Chinese medical instruction dataset is constructed using a medical knowledge graph and the GPT3.5 API. The fine-tuning process was performed on top of the existing Chat-GLM-6B model.
\par 
YuLan \cite{YuLan-Chat} is developed by the GSAI team at the Renmin University of China. It utilizes the LLaMA base model and is fine-tuned on a high-quality dataset of Chinese and English instructions. The dataset construction involves three stages: Open-source Instruction Deduplication, Instruction Diversification based on Topic Control, and Instruction Complexification. These stages aim to enhance the diversity of the instruction learning dataset.
\par 
There are various types of LLMs available at the current stage, and most of them are based on a base model which is a pre-trained language model following the Transformer \cite{vaswani2017attention} architecture. These models are further fine-tuned using domain-specific or high-quality data constructed on top of the base model. By leveraging high-quality, domain-specific data, a model with specialized knowledge can be derived from the base model.

\subsection{Applications in the Medical Field}
The development of LLMs could result in many potential applications in the medical field. In general, they could be used in the following four areas: clinical documentation, clinical decision support, knowledge-based medical information retrieval and generation, and medical research.

There is a large amount of clinical writing to be done by physicians and clinical professionals every day. Some can be quite laborious and time-consuming. LLMs can be possibly applied to assist in documenting patient information and symptoms \cite{lehman2023}, generating accurate and comprehensive clinical notes and test reports, and thus effectively reducing the writing load of physicians and clinical professionals. For example, an LLM summarizing the Impression from the radiology report was developed, presenting a paradigm in applications in similar domains \cite{ma2023impressiongpt}.

LLMs can also be applied to provide clinical decision support through recommending medicine usage \cite{Harskamp2023.03.25.23285475}, identifying appropriate imaging services from clinical presentations \cite{RAO2023}, or determining the cause of disease from numerous clinical notes and reports. When integrated with other modalities, like imaging, it can generate comprehensive information, assist physicians in disease diagnosis \cite{lyu2023macawllm}. In addition, from cases of patients with similar symptoms, LLMs can generate patient disease outcomes, giving a prediction of what the treatment may look like, and supporting the physicians and patients make decisions on treatment options. 

Knowledge-based application is another place where LLMs can play a big role. For example, it could be useful to have an LLM application developed to answer health-related questions from patients \cite{liu2023summary,nov2023putting}. With the training of data in specific domains, LLM applications could provide physicians and health professionals with relevant medical information from a vast amount of scientific literature, research papers, and clinical guideline, enabling quick access to up-to-date information on disease, treatments, drug interactions, and more \cite{holmes2023evaluating,liu2023radiologygpt}. Knowledge-based LLMs can help educate medical trainees and patients by answering generic or specific questions \cite{latif2023artificial}. By integrating with a patient’s medical record, LLMs can provide personalized information and explanation of drug usage, ongoing treatment, or any relevant questions patients may have.

LLMs can be of great benefit to the medical research community \cite{liu2023summary,wang2023prompt}. For instance, the privacy of patient medical records is a big concern in clinics. The removal of identification information is mandatory before medical records used for research and results are released to the public. An LLM application help remove the identification information from medical records and could be widely utilized and beneficial to medical research \cite{liu2023deid}. Training of clinical NLP models may suffer from a lack of medical text data; augmentation of medical text data by LLMs could
provide additional samples profiting the NLP model training \cite{dai2023auggpt}. Moreover, LLMs could conduct data collection, processing, and analysis about specific diseases, providing quantified metrics and valuable insight to researchers \cite{dai2023adautogpt}.

\section{Methodology}

This section will discuss our testing methods for LLMs. We will begin by introducing the datasets MIMIC and OpenI, which we use for evaluation. Our testing approach involves employing a fixed set of prompts and parameters to assess the performance of LLMs in the field of radiology, specifically focusing on deriving impression-based performance from findings. To ensure consistency, we set several hyperparameters of the LLMs, namely the temperature to 0.9, the top\_k to 40, and the top\_p to 0.9. To evaluate the model's zero-shot and few-shot performance, we utilize zero-shot, one-shot, and five-shot examples as prompts. The experimental results and their detailed analysis are presented in the results section.

\subsection{Testing Approach}

Our testing approach involves utilizing a fixed set of prompts and parameters to evaluate the LLMs. The model's inference parameters, namely the temperature, top\_k, and top\_p, are fixed at 0.9, 40, and 0.9, respectively, to ensure consistency. We engage zero-shot, one-shot, and five-shot prompts to examine the model's zero-shot and few-shot performance. A zero-shot prompt involves presenting the model with a new task, with no prior examples provided. A one-shot prompt involves providing the model with one prior example, while a five-shot prompt provides the model with five prior examples. This variation in prompts offers a nuanced understanding of how the LLMs operate under different conditions and degrees of prior exposure.

\subsection{Model Selection}

Considering both resource constraints and the need for uniformity in model comparison, our evaluation specifically focuses on Large Language Models (LLMs) with approximately 7 billion parameters. The choice of this parameter count is based on two primary considerations. First, models of this size strike a balance between computational efficiency and model performance. They allow for faster inference, making it feasible to thoroughly evaluate the models over the complete testing dataset in a practical timeframe. Second, this parameter count is well-represented across different types of LLMs, allowing for a broad and diverse range of models to be included in the study.

For open-source models, we procure the necessary code and model parameters directly from their official GitHub repositories. These repositories provide comprehensive documentation and community support, ensuring that the models are implemented and evaluated correctly.

For commercially available models, such as Sensenova, ChatGPT, GPT-4, PaLM2, and Anthropic Claude2, we utilize their respective Application Programming Interfaces (APIs). These APIs offer a structured and standardized way of interacting with the models, enabling us to input our pre-determined prompts and parameters and receive the model outputs in a consistent and reliable manner.

Model Summary:
\begin{enumerate}
    \item \textbf{HuatuoGPT} is a language model developed by the Shenzhen Research Institute of Big Data from the Chinese University of Hong Kong, Shenzhen. HuatuoGPT-7B is trained on the Baichuan-7B corpus, while HuatuoGPT-13B is based on Ziya-LLaMA-13B-Pretrain-v1. The advantage of HuatuoGPT is in its integration of real-world medical data and the information-rich base of ChatGPT. This allows HuatuoGPT to provide detailed diagnoses and advice in medical consultation scenarios, similar to a doctor's approach \cite{zhang2023huatuogpt}. HuatuoGPT has two versions: HuatuoGPT-7B and HuatuoGPT-13B. In our experiments, we used the HuatuoGPT-7B version.

    
    
\end{enumerate}






\subsection{Uniform Testing Prompts}

For a fair and equitable comparison across different LLMs, we adopt a uniform approach in the selection and use of testing prompts. The same prompts are used across all models and conditions, regardless of whether they are zero-shot, one-shot, or five-shot scenarios.

In a zero-shot evaluation, the models are presented with a new task, with no prior examples given. For the one-shot scenario, we provide the model with one prior example. Meanwhile, in the five-shot scenario, the model is given five examples to learn from. These scenarios aim to mimic real-world usage conditions where models are given a limited number of examples and are expected to generalize from them.


% Figure environment removed

% \begin{table}[h]
% \begin{adjustbox}{width=\textwidth, center}
% \begin{tabular}{lccc ccc ccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c}{OpenI Zero-Shot} & \multicolumn{3}{c}{OpenI One-Shot} & \multicolumn{3}{c}{OpenI Five-Shot}\\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
% & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
% \midrule
% HuaTuoGPT-7B & 0.0718 & 0.0207 & 0.0608 & 0.1725 & 0.0593 & 0.1564 & 0.3719 & 0.2725 & 0.3599\\
% Luotuo-7b-0.3 & 0.147 & 0.0495 & 0.1262 & 0.152 & 0.0561 & 0.1356 & 0.4282 & 0.3238 & 0.4168\\
% Ziya-LLaMA-13B-v1 & 0.1101 & 0.0316 & 0.0926 & 0.1502 & 0.0285 & 0.1379 & 0.2794 & 0.1694 & 0.2702\\
% YuYan-dialogue & 0.0622 & 0.0183 & 0.0599 & 0.0702 & 0.009 & 0.0627 & 0.0981 & 0.0529 & 0.092\\
% BenTsao & 0.0804 & 0.0368 & 0.0711 & 0.0912 & 0.0201 & 0.0977 & 0.1302 & 0.0803 & 0.1231\\
% XrayGLM & 0.0622 & 0.0183 & 0.0599 & 0.0808 & 0.0063 & 0.0631 & 0.0612 & 0.0215 & 0.0501\\
% ChatGLM-Med & 0.0796 & 0.0317 & 0.0661 & 0.0702 & 0.025 & 0.0815 & 0.1201 & 0.0713 & 0.1488\\ 
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{OpenI Test Results for Chinese LLMs}
% \end{table}

% \begin{table}[h]
% \begin{adjustbox}{width=\textwidth, center}
% \begin{tabular}{lccc ccc ccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c}{MIMIC Zero-Shot} & \multicolumn{3}{c}{MIMIC One-Shot} & \multicolumn{3}{c}{MIMIC Five-Shot}\\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
% & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
% \midrule
% HuaTuoGPT-7B & 0.1287 & 0.0623 & 0.1067 & 0.0861 & 0.0239 & 0.0806 & 0.2745 & 0.1537 & 0.2468\\
% Luotuo-7b-0.3 & 0.2366 & 0.1229 & 0.1899 & 0.0795 & 0.0132 & 0.0761 & 0.2649 & 0.1424 & 0.2491\\
% Ziya-LLaMA-13B-v1 & 0.2121 & 0.0968 & 0.17 & 0.1103 & 0.0299 & 0.0995 & 0.2806 & 0.139 & 0.2512\\
% YuYan-dialogue & 0.0819 & 0.0303 & 0.0978 & 0.0557 & 0.0123 & 0.0607 & 0.1193 & 0.0807 & 0.1231\\
% BenTsao & 0.1319 & 0.0618 & 0.1126 & 0.0604 & 0.0423 & 0.0627 & 0.1687 & 0.0807 & 0.1667\\
% XrayGLM & 0.1104 & 0.0468 & 0.1211 & 0.0538 & 0.0233 & 0.0531 & 0.1533 & 0.0621 & 0.1611\\
% ChatGLM-Med & 0.1233 & 0.0586 & 0.1344 & 0.0702 & 0.0335 & 0.0641 & 0.1601 & 0.0806 & 0.1744\\ 
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{MIMIC Test Results for Chinese LLMs}
% \end{table}

\begin{table}[ht]
\centering
\caption{Test Results for Compared LLMs}
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{lccccccccccccccccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{9}{c}{OpenI} & \multicolumn{9}{c}{MIMIC-CXR}\\
\cmidrule(lr){2-10} \cmidrule(lr){11-19}
& \multicolumn{3}{c}{zero-shot} & \multicolumn{3}{c}{one-shot} & \multicolumn{3}{c}{five-shot} & \multicolumn{3}{c}{zero-shot} & \multicolumn{3}{c}{one-shot} & \multicolumn{3}{c}{five-shot}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16} \cmidrule(lr){17-19}
& R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
\midrule
Luotuo-lora-7B-0.3 & 0.147 & 0.0495 & 0.1262 & 0.152 & 0.0561 & 0.1356 & 0.4282 & 0.3238 & 0.4168 & 0.2366 & 0.1229 & 0.1899 & 0.0795 & 0.0132 & 0.0761 & 0.2649 & 0.1424 & 0.2491 \\
Ziya-LLaMA-13B-v1 & 0.1101 & 0.0316 & 0.0926 & 0.1502 & 0.0285 & 0.1379 & 0.2794 & 0.1694 & 0.2702 & 0.2121 & 0.0968 & 0.17 & 0.1103 & 0.0299 & 0.0995 & 0.2806 & 0.139 & 0.2512 \\
YuYan-dialogue & 0.0622 & 0.0183 & 0.0599 & 0.0702 & 0.009 & 0.0627 & 0.0981 & 0.0529 & 0.092 & 0.0819 & 0.0303 & 0.0978 & 0.0557 & 0.0123 & 0.0607 & 0.1193 & 0.0807 & 0.1231 \\
BenTsao  & 0.0804 & 0.0368 & 0.0711 & 0.0912 & 0.0201 & 0.0977 & 0.1302 & 0.0803 & 0.1231 & 0.1319 & 0.0618 & 0.1126 & 0.0604 & 0.0423 & 0.0627 & 0.1687 & 0.0807 & 0.1667 \\
XrayGLM & 0.0622 & 0.0183 & 0.0599 & 0.0808 & 0.0063 & 0.0631 & 0.0612 & 0.0215 & 0.0501 & 0.1104 & 0.0468 & 0.1211 & 0.0538 & 0.0233 & 0.0531 & 0.1533 & 0.0621 & 0.1611 \\
ChatGLM-Med & 0.0796 & 0.0317 & 0.0661 & 0.0702 & 0.025 & 0.0815 & 0.1201 & 0.0713 & 0.1488 & 0.1233 & 0.0586 & 0.1344 & 0.0702 & 0.0335 & 0.0641 & 0.1601 & 0.0806 & 0.1744 \\
ChatGPT & 0.1203 & 0.037 & 0.1052 & 0.1363 & 0.0421 & 0.1205 & 0.4262 & 0.2961 & 0.4113 & 0.2048 & 0.0996 & 0.1702 & 0.2506 & 0.1183 & 0.2052 & 0.3401 & 0.1871 & 0.2921 \\
GPT4 & 0.1171 & 0.0343 & 0.0975 & 0.1079 & 0.0328 & 0.0909 & 0.1357 & 0.0617 & 0.1176 & 0.1995 & 0.0858 & 0.1575 & 0.2114 & 0.0875 & 0.163 & 0.2029 & 0.0904 & 0.1607 \\
ChatGLM2-6B & 0.1094 & 0.0331 & 0.0909 & 0.0976 & 0.0237 & 0.0886 & 0.215 & 0.1346 & 0.2058 & 0.2042 & 0.0964 & 0.1605 & 0.0974 & 0.033 & 0.0844 & 0.247 & 0.1254 & 0.2198 \\
ChatGLM-6B & 0.125 & 0.0398 & 0.1087 & 0.1576 & 0.0511 & 0.143 & 0.2568 & 0.1451 & 0.2432 & 0.2051 & 0.1007 & 0.1694 & 0.1439 & 0.0464 & 0.1293 & 0.2766 & 0.1409 & 0.2386 \\
QiZhen-Chinese-LLaMA-7B & 0.1122 & 0.0281 & 0.0965 & 0.1033 & 0.0259 & 0.0823 & 0.224 & 0.1461 & 0.2123 & 0.2137 & 0.0877 & 0.1743 & 0.1021 & 0.041 & 0.0901 & 0.258 & 0.1254 & 0.2198 \\
MOSS-MOON-003-sft & 0.1402 & 0.0341 & 0.1241 & 0.1275 & 0.0242 & 0.1143 & 0.2206 & 0.1088 & 0.2057 & 0.2203 & 0.0914 & 0.1757 & 0.1258 & 0.0363 & 0.1062 & 0.2391 & 0.0999 & 0.1957 \\
ChatFlow-7B & 0.1048 & 0.0099 & 0.0978 & 0.0942 & 0.0165 & 0.0829 & 0.1844 & 0.0733 & 0.1722 & 0.1145 & 0.022 & 0.0933 & 0.105 & 0.0169 & 0.0857 & 0.1401 & 0.0409 & 0.1166 \\
CPM-Bee &  0.1193 & 0.0374 & 0.1048 & 0.1548 & 0.0297 & 0.1465 & 0.1582 & 0.0327 & 0.1501 & 0.2022 & 0.0938 & 0.1686 & 0.1125 & 0.0437 & 0.0993 & 0.1354 & 0.051 & 0.1184 \\
PULSE-7B & 0.1286 & 0.0413 & 0.1111 & 0.0885 & 0.0319 & 0.0726 & 0.1198 & 0.0576 & 0.1028 & 0.2559 & 0.1246 & 0.2043 & 0.1827 & 0.0796 & 0.1382 & 0.1298 & 0.0503 & 0.0955 \\
Baichuan-7B & 0.003 & 0.0009 & 0.0028 & 0.1328 & 0.0472 & 0.1172 & 0.2485 & 0.1467 & 0.2379 & 0.0057 & 0.0029 & 0.0042 & 0.1746 & 0.0804 & 0.1456 & 0.2301 & 0.1229 & 0.2032 \\
Chinese-Falcon-7B & 0.0518 & 0.0168 & 0.0416 & 0.0465 & 0.0154 & 0.0364 & 0.0378 & 0.0176 & 0.0312 & 0.1119 & 0.0557 & 0.0867 & 0.094 & 0.0448 & 0.0721 & 0.0491 & 0.0248 & 0.0381 \\
AtomGPT\_8k & 0.0287 & 0.0013 & 0.0245 & 0.0064 & 0.0001 & 0.0054 & 0.0024 & 0.0001 & 0.0021 & 0.0309 & 0.0014 & 0.0222 & 0.0035 & 0.0001 & 0.0029 & 0.0082 & 0.0001 & 0.0069 \\
ChatYuan-large-v2 & 0.0845 & 0.0223 & 0.0761 & 0.1005 & 0.0331 & 0.0903 & 0.2031 & 0.1078 & 0.1991 & 0.1353 & 0.0632 & 0.1151 & 0.1879 & 0.0793 & 0.15 & 0.0108 & 0.004 & 0.0102 \\
Bianque v2 & 0.0227 & 0.0036 & 0.0222 & 0.0271 & 0.0039 & 0.0264 & 0.0304 & 0.0072 & 0.0291 & 0.0294 & 0.0072 & 0.0281 & 0.0295 & 0.0062 & 0.0285 & 0.0225 & 0.0039 & 0.0212 \\
AquilaChat-7B & 0.0948 & 0.0279 & 0.0786 & 0.1079 & 0.0259 & 0.0896 & 0.2288 & 0.1454 & 0.2218 & 0.1885 & 0.0862 & 0.1474 & 0.1419 & 0.0408 & 0.1112 & 0.2084 & 0.0895 & 0.1781 \\
Aquila-7B & 0.0373 & 0.0071 & 0.0334 & 0.028 & 0.0057 & 0.0247 & 0.026 & 0.0087 & 0.0234 & 0.0707 & 0.022 & 0.0576 & 0.0411 & 0.011 & 0.0339 & 0.0425 & 0.0119 & 0.0353 \\
Chinese-Alpaca-Plus-7B & 0.063 & 0.0056 & 0.0507 & 0.0492 & 0.0035 & 0.0414 & 0.0552 & 0.0064 & 0.048 & 0.0898 & 0.0118 & 0.0663 & 0.0795 & 0.0113 & 0.0608 & 0.0807 & 0.0163 & 0.0642 \\
TigerBot-7B-sft & 0.064 & 0.0102 & 0.0527 & 0.1246 & 0.0284 & 0.1083 & 0.1562 & 0.0415 & 0.1401 & 0.1321 & 0.0331 & 0.1005 & 0.1452 & 0.043 & 0.1256 & 0.205 & 0.0676 & 0.1667 \\
XrayPULSE & 0.0293 & 0.0011 & 0.0239 & 0.0282 & 0.0018 & 0.0233 & 0.0263 & 0.0011 & 0.0225 & 0.0592 & 0.0061 & 0.0398 & 0.0531 & 0.005 & 0.0378 & 0.0488 & 0.006 & 0.0388 \\
DoctorGLM & 0.0996 & 0.0329 & 0.0861 & 0.1353 & 0.0463 & 0.1205 & 0.1392 & 0.0576 & 0.1328 & 0.1853 & 0.0916 & 0.153 & 0.0664 & 0.0146 & 0.0639 & 0.2116 & 0.1055 & 0.1916 \\
Robin-7B-medical & 0.0211 & 0.0043 & 0.0173 & 0.0335 & 0.0101 & 0.0285 & 0.0423 & 0.016 & 0.0377 & 0.0497 & 0.0139 & 0.0375 & 0.0323 & 0.0044 & 0.0236 & 0.0474 & 0.0132 & 0.0353 \\
PaLM2 & 0.1386 & 0.0477 & 0.1194 & 0.1557 & 0.0551 & 0.1353 & 0.1386 & 0.0492 & 0.1245 & 0.2749 & 0.1442 & 0.2281 & 0.2711 & 0.1446 & 0.2251 & 0.2397 & 0.1216 & 0.2019 \\
Sensenova & 0.0634 & 0.0106 & 0.051 & 0.0682 & 0.0121 & 0.055 & 0.0994 & 0.0214 & 0.0876 & 0.1136 & 0.0304 & 0.0809 & 0.1209 & 0.036 & 0.0889 & 0.1401 & 0.0395 & 0.0907 \\
Anthropic Claude2 & 0.2372 & 0.1259 & 0.2193 & 0.1944 & 0.0888 & 0.1713 & 0.4086 & 0.2755 & 0.3904 & 0.3177 & 0.153 & 0.256 & 0.3222 & 0.1514 & 0.2626 & 0.3116 & 0.1568 & 0.2548 \\
BayLing-7B & 0.1252 & 0.0389 & 0.1044 & 0.1268 & 0.0467 & 0.1181 & 0.4506 & 0.3452 & 0.4436 & 0.2149 & 0.107 & 0.1747 & 0.0851 & 0.0229 & 0.0825 & 0.2901 & 0.1722 & 0.2747 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

% SEEChat & 0.125 & 0.0398 & 0.1087 & 0.1576 & 0.0511 & 0.143 & 0.2568 & 0.1451 & 0.2432 & 0.2051 & 0.1007 & 0.1694 & 0.1439 & 0.0464 & 0.1293 & 0.2766 & 0.1409 & 0.2386 \\


\subsection{Datasets}

Our study utilizes two comprehensive and publicly available datasets, the MIMIC-CXR and the OpenI datasets. These datasets were utilized to test the performance and efficacy of various LLMs in generating radiology text reports.

In our study, we used these datasets to evaluate the capabilities of the LLMs. We focused on the "Findings" and "Impression" sections of each report as they provide comprehensive and detailed textual information about the imaging findings and the radiologists’ interpretations. 

\subsubsection{MIMIC-CXR Dataset}

The MIMIC-CXR dataset is a substantial repository of de-identified chest radiographs (CXRs) that are complemented with their corresponding radiology reports. The dataset contains medical data from over 60,000 patients who were admitted to the Beth Israel Deaconess Medical Center between 2001 and 2012. The radiology reports in the MIMIC-CXR dataset typically consist of two sections: "Findings" and "Impression". The "Findings" section details observations from radiology images, while the "Impression" section provides summarized interpretations of these observations. 

\subsubsection{OpenI Dataset}

The OpenI dataset is another essential resource that was used in our study. It is a freely available repository that consists of radiology images paired with their respective reports. This dataset provided an independent external platform to validate the performance and generalizability of our LLMs across different data sources.

We followed an existing literature approach to randomly divide the dataset into separate segments for testing purposes. This division resulted in a subset of 2400, 292, and 576 reports for various testing scenarios. 

\section{Results}
The evaluation of the performance of a variety of LLMs on two extensive databases, OpenI and MIMIC, provides valuable insights into their strengths and weaknesses under different shot settings. The results, comprehensively captured in Table 1, allow us to draw significant conclusions about the various models' performance across different settings and datasets.

The performance of the models was assessed in three distinct shot settings, namely zero-shot, one-shot, and five-shot. Each model's performance was evaluated based on three crucial metrics: Recall@1 (R-1), Recall@2 (R-2), and Recall@L (R-L).

Considering the OpenI database results, a broad spectrum of performance was witnessed. The Anthropic Claude2 model displayed remarkable competence in the zero-shot setting, achieving an R-1 score of 0.2372, an R-2 score of 0.1259, and an R-L score of 0.2193. These metrics notably surpass those of other models under the same setting, indicating the superior capability of the Anthropic Claude2 model in this context.

In the one-shot scenario, the SEEChat model rose to prominence with its impressive R-1 score of 0.1576, R-2 score of 0.0511, and R-L score of 0.143. The five-shot setting witnessed the BayLing-7B model outpacing other models, demonstrating an R-1 score of 0.4506, an R-2 score of 0.3452, and an R-L score of 0.4436. These findings underline the substantial diversity in model performances across different shot settings.

Moving to the results derived from the MIMIC database, it was once again the Anthropic Claude2 model that displayed exemplary performance in the zero-shot setting. This model achieved an R-1 score of 0.3177, an R-2 score of 0.153, and an R-L score of 0.256, thereby outperforming other models in this setting.

For the one-shot setting, the SEEChat model took the lead with an R-1 score of 0.1439, an R-2 score of 0.0464, and an R-L score of 0.1293. Under the five-shot setting, the BayLing-7B model demonstrated top-tier performance, with scores of 0.2901 (R-1), 0.1722 (R-2), and 0.2747 (R-L), respectively.

It is worth noting that the performance of some models, such as AtomGPT\_8k, was considerably lower across all shot settings and both databases. Such models scored remarkably low, with AtomGPT\_8k achieving an R-1 score of just 0.0287 in the OpenI zero-shot setting, and similarly low scores across other settings and in the MIMIC database. This contrast in scores highlights the diversity in the capabilities of different models and the need for careful model selection based on specific tasks.

In conclusion, our comprehensive evaluation of various LLMs on the OpenI and MIMIC databases provides valuable insights into each model's performance under different shot settings. The significant variance in performance observed across models underlines the importance of task-specific model selection in future research and applications. Our findings constitute an essential resource for further research into the application of LLMs in different domains.

\section{Discussion}

\subsection{Impact and Insights}
The present study has conducted one of the most exhaustive assessments of world-wide LLMs, focusing primarily on their utilization within the domain of radiology. The meticulous evaluation of these models, using extensive radiology report datasets and juxtaposing them with established global leading models, provides significant insights into their capabilities, limitations, and potential roles within the healthcare sector.

Our findings underscore that multiple LLMs perform comparably in interpreting radiology reports. This alignment points to their advanced natural language understanding skills and highlights their potential utility in enhancing radiology practice, where they can aid in automating radiological image interpretation, assisting in preliminary diagnosis, and thereby freeing up time for healthcare professionals. This is particularly beneficial in regions with limited access to radiologists or in healthcare scenarios where high volumes and time constraints pose significant challenges.

\subsection{Inter-model Differences and Implications}
While the performance of the world-wide models showed broad alignment, our results also spotlighted some disparities between the different models. This variance in strengths and weaknesses indicates that the choice of an LLM for a specific application should depend on the particular requirements of that task. Hence, a more profound understanding of these models, to which our study contributes, is critically essential for their effective deployment in the field.

\subsection{Implications of Evaluation Metrics}

The evaluation metric adopted in our study is Rouge Score, an N-gram-based method that inherently measures how well models conform to set answers. GPT-4, a universally recognized powerful model, did not outperform its counterpart, ChatGPT, nor did it surpass other models in the Rouge Score. This discrepancy invites a questioning of the significance of Rouge Score as a measure of radiology knowledge. The BayLing model, for instance, tended to produce succinct answers which, despite their brevity, may be of high quality and accuracy. On the contrary, GPT-4 may be more verbose and consider issues more comprehensively, showing some level of distrust in the input. The difference in results highlights the need to carefully interpret the evaluation scores, taking into account the unique characteristics of each model.

\subsection{Model Size and Performance}

Our analysis reveals that to achieve high performance in this specific task, there is no strict need for large models. Models with 7B parameters can produce impressive results, suggesting that we might be on the verge of a fourth industrial revolution driven by these more accessible, lightweight models. This prompts a reconsideration of the belief that model performance is strongly correlated with the size of the model. In fact, smaller models also demonstrated strong capabilities, raising the question of whether intelligence truly arises from the number of parameters and data accumulation.

\subsection{Multimodal LLMs: The Next Frontier}
The advent of multimodal LLMs, capable of managing multiple forms of input such as text and images, creates fascinating prospects for future research. Evaluating these models' aptitude to directly interpret radiological images, in addition to textual reports, could revolutionize radiology practice. These multimodal models could find uses in areas like disease detection and diagnosis, treatment planning, and patient monitoring.


\section{Conclusion}

In this comprehensive study, we rigorously evaluated the performance of 32 significant world-wide LLMs in the healthcare and radiology sector, comprising both global leading models such as ChatGPT, GPT-4, PaLM2, Claude2 and a robust suite of LLMs developed in other countries such as China. The overarching goal of this exploration was to benchmark these models in the context of interpreting radiology reports, enabling a nuanced understanding of their diverse capabilities, strengths, and weaknesses. Our findings affirm the competitive performance of many Chinese LLMs against their global counterparts, emphasizing their untapped potential in healthcare applications, particularly within radiology. This suggests a trajectory towards a future where these multilingual and diverse LLMs contribute to an enhanced global healthcare delivery system.

Looking ahead, our large-scale study's insights offer a compelling foundation for further exploratory research. There is immense scope for expanding these LLMs into different medical specialties and developing multimodal LLMs, the latter of which could handle complex and diverse data types to provide a more comprehensive understanding of patient health. However, as we navigate this evolving landscape of LLMs, it is imperative to give due consideration to their effective application and ethical deployment. In conclusion, our study hopes to catalyze further exploration and discussion, envisioning an era where LLMs significantly aid in healthcare provision and contribute to an enhanced standard of global patient care.


\bibliography{LLM_refs}
\bibliographystyle{unsrt}

\end{document}