\documentclass[final]{colt2023}

\usepackage{array} %
\usepackage{caption} %
\usepackage{float} %
\usepackage{tablefootnote} %
\usepackage{multirow} %
\usepackage{enumitem}
\usepackage{ifthen}
%\usepackage{authblk} % this package causes problems with \AND!

% to prevent overflowing links in bibliography
\usepackage{url} %
\def\UrlBreaks{\do\/\do-} %

\include{preamble}

\title[Polynomial linearly-convergent method for g-convex optimization?]{Open Problem: Polynomial linearly-convergent method for geodesically convex optimization?}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 \coltauthor{\Name{Christopher Criscitiello} \Email{christopher.criscitiello@epfl.ch}\AND
\Name{Nicolas Boumal} \Email{nicolas.boumal@epfl.ch}\\
\addr }

% Authors with different addresses:
\coltauthor{%
 \Name{Christopher Criscitiello} \Email{christopher.criscitiello@epfl.ch}\\
 \addr Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL), Institute of Mathematics
 \AND
 \Name{David Mart\'inez-Rubio} \Email{martinez-rubio@zib.de}\\
 \addr Zuse Institute Berlin and Technische Universit\"{a}t
 \AND
 \Name{Nicolas Boumal} \Email{nicolas.boumal@epfl.ch}\\
 \addr Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL), Institute of Mathematics
}

\begin{document}

\maketitle

\begin{abstract}%
Let $f \colon \calM \to \reals$ be a Lipschitz and geodesically convex function defined on a $d$-dimensional Riemannian manifold $\calM$.
Does there exist a first-order deterministic algorithm which (a) uses at most $O(\mathrm{poly}(d) \log(\epsilon^{-1}))$ subgradient queries to find a point with target accuracy $\epsilon$, and (b) requires only $O(\mathrm{poly}(d))$ arithmetic operations per query?
In convex optimization, the classical ellipsoid method achieves this.
After detailing related work, we provide an ellipsoid-like algorithm with query complexity $O(d^2 \log^2(\epsilon^{-1}))$ and per-query complexity $O(d^2)$
%answers this question in the affirmative 
for the limited case where $\calM$ has constant curvature (hemisphere or hyperbolic space).
We then detail possible approaches and corresponding obstacles for designing an ellipsoid-like method for general Riemannian manifolds.
\end{abstract}

\begin{keywords}%
  ellipsoid method; geodesic convexity; Riemannian optimization; hyperbolic space
\end{keywords}

\section{Introduction and background} %background and motivation
%\TODO{state somewhere roughly what the ellipsoid method does, and where they can read about it}
The ellipsoid method (developed by~\citet{shor1977cut} and~\citet{yudin1976evaluation,yudin1976informational}) is a fundamental algorithm in convex optimization and computational complexity.\footnote{For a description and analysis of the ellipsoid method, see~\citep[Sec.~3.2]{nemirovskibook} or~\citep[Sec.~3.2]{nesterov2004introductory}.}
The first proof that linear programs can be solved in polynomial time used the ellipsoid method~\citep{Kha79}.

Let us briefly review how the method works.
Suppose we seek to minimize a Lipschitz convex function $f \colon \reals^d \to \reals$ constrained to a closed ball $\bar B(\xorigin, r)$ of radius $r$ and center $\xorigin$. 
At each step of the ellipsoid method, there is an ellipsoid $E_k \subset \reals^d$ which by construction contains the minimizers of $\min_{x \in \bar B(\xorigin, r)} f(x)$.  The ellipsoid method queries the center of the ellipsoid and receives a subgradient, which in turn determines a halfspace containing the minimizers.  The next ellipsoid $E_{k+1}$ is the minimum-volume ellipsoid containing the intersection of the halfspace and $E_k$.  
The volume of the ellipsoids $E_k$ decreases at a linear rate, leading to the linear rate of convergence for the method.
The ellipsoid method has two key properties.
First, given a target accuracy $\epsilon$, it finds an $\epsilon$-approximate solution
in at most $O(d^2 \log(\epsilon^{-1}))$ subgradient queries.
Second, each iteration requires only $O(d^2)$ arithmetic operations to determine the next query.

We ask whether there is an algorithm for geodesically convex (g-convex) optimization on a Riemannian manifold which has linear convergence and each query can be computed efficiently. 
This problem has been stated informally before by~\citet[Sec.~2.2]{allenzhuoperatorsplitting} and~\citet[Sec.~1.1,1.3]{rusciano2019riemannian}.  Our contributions are to point out a partial solution for the case of constant curvature, to suggest possible avenues of attack, and to bring the problem to a wider audience.

Let us recall the relevant definitions.  {For references on g-convex optimization, see \citep{udriste1994convex} or~\citep[Ch.~11]{boumal2020intromanifolds}.}  
Throughout, $\calM$ denotes a complete $d$-dimensional Riemannian manifold with Riemannian metric $\langle \cdot, \cdot \rangle$, distance $\dist$, and exponential and logarithm maps $\exp$ and $\log$.
A subset $D$ of $\calM$ is \emph{g-convex} if for all $x, y \in D$
there is a unique minimizing geodesic segment $\gamma$ in $\calM$ connecting $x, y$, and $\gamma$ is contained in $D$ (this is sometimes called strongly g-convex~\citep[Sec.~11.3]{boumal2020intromanifolds}).
%there is a unique the geodesic segment connecting $x, y$, and that segment is contained in $D$.
A function $f \colon D \rightarrow \reals$ is \emph{g-convex} in $D$ if
$f \circ \gamma$ is convex for all geodesic segments $\gamma$ contained in $D$.
%$f(\gamma(t)) \leq (1-t) f(x) + t f(y)$ for all $x, y \in D$,
%where $\gamma \colon [0, 1] \rightarrow \reals$ is the unique minimizing geodesic with $\gamma(0) = x, \gamma(1) = y$.
A function $f$ is $M$-Lipschitz in $D$ if $|f(x) - f(y)| \leq M \dist(x, y)$ for all $x, y \in D$.
A tangent vector $g$ in the tangent space at $x$, denoted $\T_x \calM$, is a \emph{subgradient} of $f \colon D \to \reals$ at $x$ if $f(y) \geq f(x) + \langle g, \log_x(y)\rangle$ for all $y\in D$.  
%The subdifferential $\partial f(x)$ of $f$ at $x$ is the set of all subgradients of $f$ at $x$.
A \emph{halfspace} on $\calM$ is a set $\{y \in \calM : \langle g, \log_x(y)\rangle \leq 0\}$ for some $(x, g)$ in the tangent bundle $\T\calM$.
%\TODO{maybe include def of ball -- no, not enough room}

We adopt the black-box model of optimization~\citep[Sec.~1.1]{nesterov2004introductory}.  A first-order algorithm can access the function $f$ to be minimized through oracle queries $x_k \in \calM$.
After query $x_k$, the algorithm is given $f(x_k)$ and a subgradient $g$ at $x_k$.
%A deterministic algorithm does not have access to random bits.
We can now state the open question:
\begin{question}\label{mainopenq}
Assume $\calM$ has sectional curvatures in the interval $[-K, K]$.
Assume $\bar B(\xorigin, r)$ is g-convex.  Let $f\colon \bar B(\xorigin, r) \to \reals$ be g-convex and $M$-Lipschitz in $\bar B(\xorigin, r)$.
Define $\zeta_{r \sqrt{K}} = \frac{r \sqrt{K}}{\tanh(r \sqrt{K})}$.
% If $\zeta_{r \sqrt{K}}$ is large, $\zeta_{r \sqrt{K}} = \Theta(r \sqrt{K})$
Is there a deterministic first-order algorithm $\calA$ with the following properties?
\begin{enumerate}
\item[(a)] For every $\epsilon \in (0,1)$, algorithm $\calA$ finds a point $x$ such that $f(x) - f^* \leq \epsilon \cdot M r$ in at most $O(\mathrm{poly}(\zeta_{r\sqrt{K}}, d) \log(\epsilon^{-1}))$ oracle queries, where $f^* = \min_{x \in \bar B(\xorigin, r)} f(x)$.
\item[(b)] Each iteration requires only a polynomial number of arithmetic operations, $\mathrm{poly}(\zeta_{r\sqrt{K}}, d)$, to determine the next query (in addition to the subgradient queries).
\end{enumerate}
\end{question}

%The quantity $\zeta_{r \sqrt{K}} = \Theta(1 + r \sqrt{K})$ captures the dependence on curvature.
Unlike in the Euclidean case, we permit the complexity of the method to depend on the curvature through $\zeta_{r \sqrt{K}} = \Theta(1 + r \sqrt{K})$ as such dependence is unavoidable~\citep{criscitiello2023curvature}.
%A deterministic first-order algorithm $\calA$ on $\calM$ is a sequence of functions $(\calA_{k} \colon (\reals \times \T \calM)^k \rightarrow \calM)_{k \geq 0}$; in particular, $\calA_0$ returns an initial point $x_0$.
%Such an algorithm has access to an oracle $\calO_f\colon \calM \rightarrow \reals \times \T \calM$ which for each query $x \in \calM$ returns .
%Running $\calA$ with an oracle $\calO_f$ produces iterates $x_0, x_1, \ldots$ as follows.\footnote{We often implicitly assume there is an oracle associated to $f$, and simply say ``running $\calA$ on $f$ produces iterates \ldots.''}
%Let $\calH_0 = \emptyset$.
%After already making $k \geq 0$ queries $x_0, \ldots, x_{k-1}$, the algorithm uses the known information $\calH_k$ to compute the next query $x_k = \calA_k(\calH_k)$.
%The oracle $\calO_f$ gives the algorithm $F_k = f(x_k)$ and $g_k \in \partial f(x_k)$, and we update the known information $\calH_{k+1} = (F_\ell, x_\ell, g_\ell)_{\ell=0}^k$.

A method satisfying properties (a) and (b) in Open Question~\ref{mainopenq} is of interest for several reasons.
First, the ellipsoid method is of fundamental theoretical importance in convex optimization.
As geodesic convexity is a generalization of convexity, it is natural to ask for such a generalization.  
Second, a method solving Open Question~\ref{mainopenq} applies to nonsmooth g-convex optimization problems, for example computing the geometric median such as for computational anatomy~\citep{Fletcher2009TheGM} or phylogenetics~\citep[Ch.~8]{bacak2014hadamard}.
Third, there are applications where the cost function is g-convex, but not strongly g-convex, and one seeks a linear convergence rate.
%{logarithmically} with the inverse target accuracy $\epsilon^{-1}$.
A notable example is \emph{operator scaling}~\citep{burgissernoncommutativeoptimization}, where it is also important that the method is deterministic.

The operator scaling problem encompasses several statistical problems including robust covariance estimation~\citep{wiesel2015gconvexity,sra2015conicgeometricoptimspd,franksmoitra2020} and estimation for matrix normal models ~\citep{tang2021integrated,franks2021neartyler}.
It also encompasses several questions in theoretical computer science, including a variant on polynomial identity testing (see references in~\citep{burgissernoncommutativeoptimization}).
For operator scaling, \citet{allenzhuoperatorsplitting} propose a linearly-convergent box-constrained Newton method.
This method does not solve Open Question~\ref{mainopenq} as it assumes additional properties about the objective (e.g., second-order robustness).
%relies on additional properties of the cost function, e.g., smoothness.

%The ellipsoid method is an instance of a cutting planes scheme~\citep[Sec.~3.2]{nesterov2004introductory}.
%Cutting planes schemes and ellipsoid-like methods for optimization on manifolds may also have additional applications in robotics and motion planning~\citep{ggcsTedrake,ellipsoidsIRISTedrake}.
%\citet[Sec.~7]{criscitiello2023curvature} give lower bounds for Riemannian cutting planes schemes.

There are other methods for convex optimization which have similar properties as the ellipsoid method~\citep[pg.~156]{nesterov2004introductory}.
% and~\citep{methodofsimplices}.
%the method of simplices~\citep{methodofsimplices}
We focus on generalizing the ellipsoid method because it arguably has the simplest analysis among such methods, and similar obstacles appear in generalizing other methods.
In Section~\ref{constcurvature}, using the tools of geodesic maps introduced by~\citet{martinezrubio2021global}, we describe an ellipsoid-like method for spaces of constant curvature with query complexity $O(\zeta_{r \sqrt{K}} d^2 \log^2(\epsilon^{-1}))$ and per-query complexity $O(d^2)$.
In Section~\ref{obstacles}, we discuss possible approaches and obstacles to solving Open Question~\ref{mainopenq} for general Riemannian manifolds.
%In Section~\ref{milestones}, we conclude by mentioning a few variants of Open Question~\ref{mainopenq} which may be easier to solve.

\smallskip \noindent 
\textbf{Related work} There are algorithms which satisfy properties (a) and (b) of Open Question~\ref{mainopenq} individually, but not concurrently.
Each iteration of the subgradient method~\citep{zhang2016complexitygeodesicallyconvex} can be computed efficiently.  However, without additional assumptions, this method does not have linear convergence.
The centerpoint method due to~\citet{rusciano2019riemannian} satisfies property (a).
However, each iteration requires computing a centerpoint, which we do not know how to compute efficiently.
%If we assume the objective function is \emph{smooth} and \emph{strongly} g-convex, then the convergence rate of Riemannian gradient descent does scale as $\log(\epsilon^{-1})$~\citep{zhang2016complexitygeodesicallyconvex}.
\citet{lai2022riemannian} and~\cite{hirai2023interiorpoint} give Riemannian interior-point methods, which can converge quickly, but only under additional assumptions on the cost function.
% (e.g., self-concordance or smoothness)
%beyond Lipschitzness and geodesic convexity.

\smallskip \noindent 
\textbf{Intermediate milestones} Open Question~\ref{mainopenq} asks for a deterministic algorithm, but one may initially permit the use of random queries.
For example, randomness can be helpful for computing the center of gravity in $\reals^d$~\citep[Sec.~3.3]{nemirovskibook}.
One may also focus on the Hadamard manifold of $n \times n$ positive definite matrices endowed with the affine-invariant metric~\citep[Sec.~11.7]{boumal2020intromanifolds}.  This is the most common manifold occurring in applications of g-convexity and possesses a well-studied structure~\citep{bridsonmetric,dolcetti2018differential}.  %Alternatively, one may restrict to two dim manifolds ...

\section{An ellipsoid-like method for spaces of constant curvature}\label{constcurvature}
Consider the setting described in Open Question~\ref{mainopenq}.
We first observe that it is essentially enough to solve Open Question~\ref{mainopenq} with radius $R = 1/\sqrt{K}.$
Let $D = \bar B(\xorigin, r)$ and assume $r > R$.  {(The case $r\leq R$ is easier to handle: we only need to pull $f$ back by a geodesic map once.)}
Consider the following algorithm.  
Initialize $x_0 = \xorigin$.
Given $x_k$, approximately solve the subproblem $\min_{\bar B(x_k, R)\cap D} f$ for a point $x_{k+1}$ such that $f(x_{k+1}) - \min_{\bar B(x_k, R) \cap D} f \leq \frac{\epsilon}{4} M R$.
Repeat the process.
Using only g-convexity of $f$ and $D$, one can show that after repeating this procedure $T = \ceil{\frac{2 r}{R} \log(\frac{2}{\epsilon})}$ times, we have $f(x_{T}) - f^* \leq \epsilon M r$ (see the proof of Theorem 7 in~\citep[App.~A]{martinezrubio2021global} for details).
Therefore, if we can solve each subproblem with $O(d^2 \log(\epsilon^{-1}))$ subgradient queries, then we can solve the original problem in $O(\zeta_{r\sqrt{K}} d^2 \log^2(\epsilon^{-1}))$ queries.
%O(T \cdot d^2 \log(\epsilon^{-1})) = 

Now further assume $\calM$ is a $d$-dimensional hyperbolic space $\mathbb{H}^d$ (curvature equals $-1$ without loss of generality), and let us show how to solve each subproblem $\min_{\bar B(x_k, R)\cap D} f$.  
The analysis for a sphere is similar, so we omit it.
The key tool we need is geodesic maps.
A geodesic map with base point $x \in \calM$ is a diffeomorphism from $\mathbb{H}^d$ to the open unit ball $B(0,1)$ of $\reals^d$ which maps $x$ to the origin and which maps geodesics of $\mathbb{H}^d$ to straight lines of $\reals^d$ intersected with $B(0,1)$.
Geodesic maps of $\mathbb{H}^d$ have an explicit formula given by the Beltrami-Klein model, see~\citep[App.~C]{martinezrubio2021global}.
%(see~\citep[Sec~1, App.~C]{martinezrubio2021global} for more details on geodesic maps).
Our strategy to approximately solve $\min_{\bar B(x_k, R) \cap D} f$ is simple: we first pull $f$ back to a Euclidean space via a geodesic map $\phi_k \colon \mathbb{H}^d \to B(0,1)$ based at $x_k$, and then solve the resulting Euclidean problem $\min_{\bar B(0, \tilde R) \cap \phi_k^{-1}(D)} f \circ \phi_k^{-1}$ using the usual ellipsoid method.
We initialize that method with the ball $\bar B(0, \tilde R)$.  Here $\tilde R$ equals $R$ times an absolute constant, i.e., $\tilde R = \Theta(1/\sqrt{K})$.

Why does our strategy for solving the subproblem work?
First, geodesic maps have the key property 
%\clearpage \noindent % here because of strange formatting error with footnotes
that they map halfspaces in $\mathbb{H}^d$ to halfspaces of $\reals^d$ intersected with $B(0,1)$ (this fact can be verified by inspecting the explicit formula of a geodesic map).
In particular, given $y_k \in \bar B(0, \tilde R) \cap \phi_k^{-1}(D)$ (e.g., a query from the ellipsoid method), we can compute a Euclidean halfspace containing the sublevel set $\{y \in \phi_k^{-1}(D) \cap B(0,1) : f(\phi_k^{-1}(y)) \leq f(\phi_k^{-1}(y_k))\}$ by pulling back the corresponding Riemannian halfspace $\{x \in D : f(x) \leq f(\phi_k^{-1}(y_k))\}$ with $\phi_k$.\footnote{This is alternatively quantified with the notion of \emph{tilted convexity} introduced by~\citet[Lem.~3]{martinezrubio2021global}.}
%this means that a subgradient for $f$, which determines a halfspace containing the minimizers of the hyperbolic subproblem, also determines a Euclidean halfspace containing the minimizers of the Euclidean subproblem.
Likewise if $y_k \not\in \bar B(0, \tilde R) \cap \phi_k^{-1}(D)$, one can compute a Euclidean separating halfspace for $\bar B(0, \tilde R) \cap \phi_k^{-1}(D)$ by pulling back a Riemannian separating halfspace for $\bar B(x_k, R) \cap D$.
Second, since we have chosen $R$ to be small, the metric distortion due to the geodesic map is small: in particular, $f \circ \phi_k^{-1}$ is still $O(M)$-Lipschitz~\citep[Lem.~2.c]{martinezrubio2021global}.
Lastly, note that the convexity of $f$ is not needed for the classical ellipsoid method to work: we just need that $f$ is Lipschitz and at each query point we are provided with a halfspace containing the minimizers of the problem (as guaranteed by our first observation).
%We initialize the ellipsoid method with the ball $B(0, \tilde R)$.
One can verify that the Euclidean volume of $\bar B(0, \tilde R) \cap \phi_k^{-1}(D)$ is at least the volume of $B(0, c \tilde R)$ for some absolute constant $c \in (0,1)$.  This allows us to conclude the ellipsoid method finds an $\frac{\epsilon}{4} M R$-approximate solution to each subproblem in $O(d^2 \log(\epsilon^{-1}))$ queries.
%~\citep[Thm~3.2.1]{nemirovskibook}.

Can we extend this technique beyond hyperbolic spaces?
Unfortunately, Beltrami's theorem states that the only Riemannian manifolds which admit geodesic maps to Euclidean space are those of constant curvature~\citep{beltramitheorem}.
One could replace a geodesic map with the exponential map, i.e., solve subproblems of the form $\min_{\T_x \calM} f \circ \exp_x$, 
and use comparison theorems~\citep{cheeger2008comparisontheorems} to analyze this method; 
but it is unclear how to make this approach work.
\section{Possible approaches and obstacles}\label{obstacles}
%%%%%%%%%%%% want to include this but not enough room
%A key obstacle in generalizing the ellipsoid method is that there is no convenient notion of ellipsoids on Riemannian manifolds.  The approaches below propose to handle this issue in different ways.
There is no convenient notion of ellipsoid on manifolds.
We consider two ways to handle this issue.
%The approaches below propose ways to handle this issue.
%Below, we consider two more approaches for solving Open Question~\ref{mainopenq}.
%%%%%%%%%%%%%%

%We note that we do not necessarily need to use ``ellipsoids''; it is enough to have a sufficiently \emph{flexible} family of subsets of $\calM$ which we can perform calculations with.

% We want to minimize a (potentially nonsmooth) g-convex $M$-Lipschitz function $f \colon M \to R$ [assume $M$ is Hadamard], and we want to find a tractable method for which the suboptimality gap $f(x_k) - f^*$ decreases at a linear rate (which is of course badly dependent on dim(M)).  There is a center of gravity method on manifolds which converges at a linear rate; but each step requires finding the exact center of gravity of a (potentially very complicated) convex set which is not tractable.  Besides being an important primitive for g-convex optimization, this would have a number of applications.  For example, in operator scaling there was interest developing methods for nonstrongly convex optimization which converge at a linear rate: see this paper where they use a Newton type method to do this.

\smallskip \noindent 
\textbf{Ellipsoids in tangent spaces} 
One could maintain ellipsoids in the tangent spaces of $\calM$.
%For simplicity, assume $\calM$ is Hadamard.
Consider the following algorithm.
Let $x_0 = \xorigin$ and $x^* \in \arg\min_{x \in \bar B(\xorigin, r)} f(x)$.
We initially know $\log_{x_0}(x^*)$ is contained in the ellipsoid $E_0 := \{v \in \T_{x_0} \calM : \|v\| \leq r\}$.
Querying at $x_0$ to get a subgradient $g_0$, we know $\log_{x_0}(x^*)$ is contained in the intersection $E_0 \cap \{v \in \T_{x_0} \calM : \langle v, g_0\rangle \leq 0\}$.  
Take an ellipsoid $\tilde E_0 \subset \T_{x_0} \calM$ with center $\tilde c_0$ which contains this intersection and has sufficiently small volume.  Define $x_1 = \exp_{x_0}(\tilde c_1)$.
To repeat this process, we need a method for \emph{transferring} the ellipsoid $\tilde E_0$ in $\T_{x_0} \calM$ to an ellipsoid $E_1$ in $\T_{x_1}\calM$ whose center is the origin, and such that $\exp_{x_1}(E_1) \supseteq \exp_{x_0}(\tilde E_0)$.  
\citet[Lem.~1,2]{kimyang} implicitly introduced a way of transferring \emph{balls} between tangent spaces while controlling the distortion of this transfer in order to obtain an accelerated method on manifolds.
It is not clear how to generalize their results to ellipsoids.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% really wanted to keep this section but really just not enough room
How should we keep track of the size of these ellipsoids?  We could track their Euclidean volumes.  However, even if the Euclidean volume of an ellipsoid $E \subset \T_x \calM$ is small, 
the Riemannian volume of $\exp_{x}(E)$ is not necessarily small, unless the ellipsoid is \emph{bounded}.
%%; this will be true, however, if we additionally have a bound on the diameter of the ellipsoid $E$.
A bound on the Riemannian volume is important because it provides a bound on the optimality gap~\citep{rusciano2019riemannian}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%In your paper with Insoon Yang, you introduced a very clever way of transferring knowledge about the location of $x^*$ from one tangent space to another.  
%Implicitly, at iteration $k$ of your algorithm [for the strongly g-convex case], there is a ball $B_k$ in the tangent space at say $x_k$ (with say center $c_k \in T_{x_k} M$ and radius $r_k$) which contains $\log_{x_k}(x^*)$.  You then use some gradient information to reduce the radius of this ball to get a new ball $\tilde B_{k} \subset T_{x_k} M$ of smaller radius which contains $\log_{x_k}(x^*)$.  Then you show how to get a new ball $B_{k+1} \subset T_{x_{k+1}} M$ which contains $\log_{x_{k+1}}(x^*)$ and has radius $r_{k+1} < r_k$.  Then you repeat.
%The ellipsoid method is very similar in spirit, but instead of maintaining balls, you maintain ellipsoids which contain the minimizer, and at each step you decrease the volume of the ellipsoid by a constant factor.  

%We then want to "transfer" $\tilde E_0 \subset T_{x_0} M$ to $T_{x_1} M$.  Ie, we want to find $E_1 \subset T_{x_1} M$ with center $0$ which say contains $\log_{x_1}(\exp_{x_0}(\tilde E_0))$ (and so also contains $\log_{x_0}(x^*)$), and such that $E_1$ is smaller than $E_0$ in some sense.  
%Once we have this, we can repeat the procedure and at iteration $k$ we would have a small set $E_k \ni \log_{x_k}(x^*)$.

%There are two primary technical hurdles here:
%(1) How to transfer ellipsoids from one tangent space to the other while controlling the distortion.  For balls, the geometric Lemmas 1 and 2 in your paper do exactly this.  But it is not clear to me how to extend this to ellipsoids [see my attempt below].
%(2) 

\smallskip \noindent 
\textbf{Halfspace- and volume-preserving maps}
A key ingredient in the analysis of the ellipsoid method is that there is a nontrivial family of \emph{halfspace-preserving} and \emph{volume-preserving} diffeomorphisms from $\reals^d$ to $\reals^d$: the invertible affine maps $x \mapsto A x + b$ with $\det(A) = 1$.
The explicit formula for the minimum-volume ellipsoid containing the intersection of an initial ellipsoid and a halfspace is derived by using a {volume-preserving} affine map to transform the ellipsoid to a ball (and the halfspace to another halfspace), and then computing the minimum-volume ellipsoid containing the intersection of a halfspace and a ball (a much easier calculation).
Let $\mathcal{G}$ be the group of halfspace-preserving and volume-preserving diffeomorphisms from $\calM$ to $\calM$.
Define the set of ``ellipsoids'' $\calE$ on $\calM$ as the result of applying all elements of $\mathcal{G}$ to every geodesic ball.
%: $\calE = \{\phi(B(x, r)) : \phi \in \mathcal{G}, x \in \calM, r \geq 0\}$.
If $\calM$ is a Euclidean space, $\calE$ contains all ellipsoids in the usual sense.
Unfortunately, even for highly symmetric spaces like hyperbolic space, it seems $\mathcal{G}$ is the set of isometries of $\calM$~\citep{MOpost1,MOpost2}.
In this case, $\calE$ is the set of geodesic balls, which is not a sufficiently flexible family of subsets to be useful.




%\TODO{  
%The set of ellipsoids of volume 1 is exactly the set of images $\mathcal{A}(B)$ where B is the ball of volume 1 centered at the origin.  
%
%So instead of working in tangent spaces and transferring ellipsoids, we might try to work directly on the Hadamard space by using sets which come from the volume-preserving diffeomorphisms of the Hadamard spaces.  Unfortunately, this doesn't seem like it will work because in general the only volume preserving diffeomorphisms of a Hadamard space are only the isometries of the space --- see this \href{https://mathoverflow.net/questions/420037/what-are-the-volume-preserving-diffeomorphisms-of-hyperbolic-space}{MO post}. --- cite specifically K+Y Lemmas 1 and 2}

%intro, open problem, background, current status, approaches and why they dont work, partial results, prior work, etc.

%% Acknowledgments---Will not appear in anonymized version
%\acks{We thank a bunch of people and funding agency.}

\bibliography{../bibtex/boumal}

\end{document}
