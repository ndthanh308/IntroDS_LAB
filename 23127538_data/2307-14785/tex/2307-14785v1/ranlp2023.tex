%
% File ranlp2023.tex
%
%% Based on the style files for ACL-IJCNLP 2021, which were
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage{authblk}
\usepackage[hyperref]{ranlp2023}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% pridane package
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{dirtytalk}
\usepackage{color}
\usepackage{caption}
\usepackage{placeins}
\usepackage{amsmath, amssymb}
\usepackage{placeins}

\DeclareCaptionType{exam}[Example][List of Examples]


\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother

%redefinovani vektoru
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\DeclareMathOperator*{\argmax}{argmax~}

\makeatletter
\def\adl@drawiv#1#2#3{%
        \hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}%
                #2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{%
  \noalign{\vskip\aboverulesep
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip\belowrulesep}}
\makeatother

\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{75} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}


\title{Improving Aspect-Based Sentiment with\\ End-to-End Semantic Role Labeling Model}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}



\author[*]{\bf  Pavel P\v{r}ib\'{a}\v{n}}
\author[*]{\bf Ond\v{r}ej Pra\v{z}\'{a}k}
\affil[ ]{University of West Bohemia, Faculty of Applied Sciences, Czech Republic}
\affil[]{Department of Computer Science and Engineering,}
\affil[]{NTIS -- New Technologies for the Information Society,}
\affil[  ]{\tt	\{pribanp,ondfa\}@kiv.zcu.cz}
\affil[  ]{\tt http://nlp.kiv.zcu.cz}


\date{}

\begin{document}
\maketitle
\begin{abstract}
% In this paper, we introduce several approaches that use extracted semantic information from a Semantic Role Labeling model to enhance performance on the Aspect-Based Sentiment Analysis (ABSA) task.
This paper presents a series of approaches aimed at enhancing the performance of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic information from a Semantic Role Labeling (SRL) model.
We propose a novel end-to-end Semantic Role Labeling model that effectively captures most of the structured semantic information within the Transformer hidden state. We believe that this end-to-end model is
well-suited
for our newly proposed models that incorporate semantic information. We evaluate the proposed models in two languages, English and Czech, employing ELECTRA-small models. Our combined models improve ABSA performance in both languages. Moreover, we achieved new state-of-the-art results on the Czech ABSA.


\end{abstract}


% ============================= Introduction ========================
\section{Introduction}
\label{sec:intro}
In recent years, the pre-trained BERT-like models based on the Transformer \cite{attention-vaswani-2017} architecture demonstrated their performance superiority across various natural language processing (NLP) tasks. In this paper, we study the possibility of a combination of two seemingly unrelated NLP tasks: Aspect-Based Sentiment Analysis (ABSA) and Semantic Role Labeling (SRL). We believe that the structured semantic information of a sentence extracted from an SRL model can enhance the performance of an ABSA model. We investigate our assumption on the ELECTRA \cite{clark2020electra} model architecture since it is a lighter and smaller alternative to the popular and commonly used models such as BERT \cite{devlin-etal-2019-bert} or RoBERTa \cite{liu2019roberta}. Because the ELECTRA model is smaller in terms of the number of parameters, it does require less GPU memory and time to be fine-tuned.

\par Sentiment analysis (SA) is an essential part of NLP. The most prevalent SA task is the \textit{Sentiment Classification}, where the objective is to classify a text fragment (e.g., sentence or review) as \textit{positive} or \textit{negative}, eventually as \textit{neutral}. In this type of task, we assume that there is only one opinion in the text. In reality, as illustrated in Figure \ref{fig:absa-example}, this assumption often does not hold true \cite{liu2012sentiment}.
% However, this is not true in many cases, as shown in the following Figure:

\blfootnote{
    %
    % for review submission
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % Place licence statement here for the camera-ready version. See
    % Section~\ref{licence} of the instructions for preparing a
    % manuscript.
    %
    % % final paper: en-uk version 
    %
    \hspace{-0.18cm}\textsuperscript{*}Equal contribution.}

\begin{itemize}
	\small
	\item[] \hspace{-0.8cm} { \say{\textit{The \textcolor{green}{burger} was excellent but the \textcolor{red}{waitress}  was unpleasant}}}
	    \vspace{-0.27cm}
	\item[] CE $\Rightarrow$  food, service \\
	CP $\Rightarrow$  food:\textit{positive}, service:\textit{negative}
	\label{fig:absa-example}
	\captionof{figure}{Example of CE and CP subtasks of ABSA.}
\end{itemize}
 


Aspect-Based Sentiment Analysis \cite{liu2012sentiment,pontiki-etal-2014-semeval} focuses on detecting aspects (e.g., food or service in the restaurant reviews domain) and determining their polarity, enabling more detailed analysis and understating of the expressed sentiment. As shown by \citet{pontiki-etal-2014-semeval}, the ABSA task can be further divided into four subtasks: \textit{Aspect term extraction} (TE), \textit{Aspect term polarity} (TP), \textit{Aspect category extraction} (CE), and \textit{Aspect category polarity} (CP).

% In this paper, 
We aim at the CE and CP subtasks,\footnote{See \cite{pontiki-etal-2014-semeval} for a detailed description of all the subtasks.} and we treat them as a single classification task, see Section \ref{sec:absa-model}. 
As depicted in Figure \ref{fig:absa-example}, the goal of the CE subtask is to detect a set of aspect categories within a given sentence, 
% The goal of the CP subtask is for a given text and set of assigned aspect categories included in the text, assign a polarity label for each of the aspect categories.
i.e., for a given text $S = \{w_1, w_2, \dots w_n\}$ assign set $M = \{a_1, a_2, \dots, a_m \}$ of $m$ aspect categories, where $m \in [{0,k}]$, $M \subset A$ and $A$ is a set of $k$ predefined aspect categories $A = \{a_1, a_2, \dots, a_k \}$. The goal of CP is to assign one of the predefined polarity labels $p$ for each of the given (or predicted) aspect categories of the set $M$ for the given text $S$, where $p \in P = \{positive, negative, neutral\}$.
% \footnote{In some cases, the distribution of the polarity labels can be more fine-grained.}.


% ============================= SRL Intro ===========================

The Semantic Role Labeling task \cite{gildea2002} belongs among shallow semantic parsing techniques. The SRL goal is to identify and categorize semantic relationships or \textit{semantic roles} of given \textit{predicates}. Verbs, such as ``believe'' or ``cook'', are natural predicates, but certain nouns are also accepted as predicates. The simplified definition of semantic roles is that semantic roles are abstractions of predicate arguments. For example, the semantic roles for ``believe'' can be \textit{Agent} (a believer) and \textit{Theme} (a statement) and for ``cook'' \textit{Agent} (a chef),  \textit{Patient} (a food), \textit{Instrument} (a device for cooking) -- see examples in Figure \ref{fig:srl-example}.
The theory of predicates and their roles is very well established in several linguistic resources such as PropBank \cite{Palmer:2005} or FrameNet \cite{Baker:1998}. 




% Figure environment removed

% In the SRL task, the semantic roles are considered at a higher abstraction level. E.g. for English, there are several core roles denoted by \textit{A0} (usually Agent), \textit{A1} (usually Patient) and \textit{A2}, modifier arguments (\textit{AM-*}), restriction arguments (\textit{R-*}) and others.

% Puvodni
% \par In this work, we introduce a novel end-to-end SRL model that is more suitable for usage in combination with models of other NLP tasks. Unlike other BERT-based models \cite{shi2019simple, papay2021constraining}, our end-to-end approach encodes the whole semantic information into the Transformer hidden state. The proposed end-to-end SRL model is more suitable for combination with the ABSA task because it encodes the whole predicate-argument structure of the sentence into a single hidden state of a transformer model compared to \cite{shi2019simple} that encodes each argument separately and requires gold arguments on input, whereas our model requires only text as input.



\par In this work, we introduce a novel end-to-end SRL model that offers enhanced compatibility with other NLP tasks. Unlike other BERT-based models \cite{shi2019simple, papay2021constraining}, our proposed approach integrates the complete semantic information into the hidden state of the Transformer.
% Navrh
This end-to-end SRL model is particularly well-suited for combination with the Aspect-Based Sentiment Analysis task, as it encapsulates the entire predicate-argument structure of the sentence within a single hidden state, in contrast to the approach of \cite{shi2019simple}, which encodes each argument separately and requires gold arguments on input. Our model, on the other hand, only requires the input text.

\par We assume that leveraging the syntax and semantic information extracted from SRL can significantly enhance the performance of the aspect category polarity subtask.  This assumption is grounded in the notion that the SRL information has the potential to unveil valuable and pertinent relations between entities within a given sentence, which play a crucial role in accurate aspect category polarity predictions. This holds particularly true for longer and more complex sentences, where a broader contextual understanding becomes essential. For a concrete illustration, please refer to Appendix \ref{sec:semantic-parse-examples}.



% because 

To combine the SRL and ABSA models effectively, we propose three different approaches. Through their integration, we demonstrate performance improvements on the ABSA task for both English and Czech languages, employing ELECTRA-small models.
% The end-to-end model can also be applied for multi-task learning of other pairs of NLP tasks. 
Moreover, we achieved new state-of-the-art (SotA) results on the Czech ABSA task.
We publicly release our source codes\footnote{\label{note:github}\url{https://github.com/pauli31/srl-aspect-based-sentiment}}.  
% Řekl bych, že už se opakujeme
% Overall, Our main contributions are the following: 1) We introduce a novel end-to-end SRL model. 2) We propose a multi-task (MTL) approach that leads to improvements in the ABSA task.


% ============================= Related Work ========================
\section{Related Work}
% wagner-etal-2014-dcu
% nguyen-shirai-2015-phrasernn,
\par The early studies \cite{hu-mining-2004-ABSA,Ganu2009BeyondTS,kiritchenko-etal-2014-nrc,hercig2016unsupervised} focusing on the English ABSA task relied on word n-grams, lexicons, and other feature extraction techniques in combination with supervised machine learning algorithm such as support vector machine classifiers.
These approaches were surpassed by deep neural network (DNN) models \cite{tang-etal-2016-effective,ma2017interactive,chen-etal-2017-recurrent,fan-etal-2018-multi} that typically employed recurrent neural network e.g., Long Short-Term Memory (LSTM) \cite{hochreiter1997long}.


\par Recently, the BERT-like models were successfully applied to the ABSA task.
\citet{sun-etal-2019-utilizing} solve the CE and CP subtasks at once by introducing auxiliary sentences and transforming the problem to a sentence-pair classification task. \citet{xu-etal-2019-bert} and \citet{rietzler-etal-2020-adapt} improved results by pre-training the model on the task domain data. \citet{liu-etal-2021-solving} treated the ABSA task as a text generation task outperforming the previous SotA results. \citep{zhang-etal-2019-aspect,LIANG2022107643} employed graph convolutional networks. Another related work can be found in \cite{li-etal-2020-multi-instance}.


 In \cite{sido-etal-2021-czert, priban-steinberger-2021-multilingual,lehecka-sentiment,priban-steinberger-2022-czech} the BERT-like models were used for sentiment classification and subjectivity classification, to the best of our knowledge, there is no application of BERT-like models for ABSA in the Czech language. \citet{steinberger-etal-2014-aspect} introduced the first Czech ABSA dataset from the restaurant reviews domain. They used a Maximum Entropy classifier and Conditional Random Fields for their baselines. \citet{hercig2016unsupervised} extended this dataset and improved the baseline by adding semantic features.
% \citet{tamchyna2015czech} introduced another dataset from IT product reviews domain.
\citet{lenc2016neural} applied a convolutional neural network for the CP task and RNN  for the CP task to the dataset from \citet{hercig2016unsupervised}.




% \par There are two main approaches to Semantic Role Labeling.
The pioneered approaches 
% At the beginning of this task
of the SRL \cite{gildea2002} task used standard feature engineering methods \cite{moschitti2008tree}. Since SRL is closely bounded with syntax, adding syntactic information is very helpful. In 2008 CoNLL shared task \cite{surdeanu2008conll} syntax-based SRL task was proposed.
% (followed the next year in a multilingual setting by \citet{hajic2009conll}). 

In more recent years (with DNNs), the attention was drawn back to standard span-based SRL, where we form SRL as (linear) tagging. Many approaches are based on LSTMs \cite{he2017deep}.  
% propose a deep LSTM-based model. They use a multi-layer bidirectional LSTM.
% with gated highway (or residual) connections. With the gated residual connection, the output is summed with the previous layer in the weighted way, where the residual gate gives the weight of the linear combination. The gated residual connection operates on the inner state of LSTM, and it is computed in every timestep.
Later, \citet{tan2018deep}, inspired by the Transformer, proposed a self-attention-based model.
% for SRL.

Several end-to-end models for all SRL subtasks were also introduced. \citet{he2018jointly} abandon the BIO tagging scheme, and they are rather predicting predicate-argument span tuples by searching through the possible combinations. They use a multi-layer bi-LSTM to produce contextualized representations of predicates and argument spans. 
% Then they assign the role to the potential \textit{predicate-argument} tuple. 
The most recent approaches use BERT-like pre-trained models.
% (or other pre-trained Transformer models). 
\citet{shi2019simple} proposed a simple BERT approach for argument identification and classification. This means, in their setting, the gold predicates are known. \citet{papay2021constraining} propose regular-constrained conditional random fields (CRF) decoding on top of the same model. There are many other complex deep models \cite{zhang2021semantic, wang2021mrc}

For our experiments, we need an end-to-end SRL model which encodes most of the information in the Transformer's hidden state. However, to the best of our knowledge, there is no such model. As a result, we introduce our end-to-end model later in this paper to fulfil this need.

Various approaches have been made to enhance one task through the integration of another, usually using multi-task learning techniques. \citet{hashimoto2016joint} proposed a joint model for learning the whole NLP stack (POS tagging, chunking, parsing, semantic relatedness, entailment). They train a single model for all tasks in a sequence (chunking after POS tagging etc.). At each layer (for each task), they use regularization on the difference from previous layer weights. They show that the tasks help each other significantly.

\par \citet{absa-aux} use dependency neighbourhood prediction and part-of-speech tagging as auxiliary tasks for ABSA. They introduced the new dependency neighbourhood prediction task to utilize the syntactic dependency information to improve the performance of the sentiment classification task. They train the auxiliary tasks together with the main sentiment classification task. The task classifies each token as either in the dependency neighbourhood or not. The dependency neighbourhood for a given token in a sentence is defined as the tokens in the sentence that are linked to the given token through, at most, $n$-hop dependency relations. \citet{zhang2020semantics} pretrain BERT model on semantic role labeling task and show, that the pretraining helps for many natural language understanding tasks. These examples of multi-task learning demonstrate the potential benefits of incorporating additional tasks in NLP models.



% most of the previous works focus on Aspect Term extraction and classification
% pouzit slovo adopt the approach from \citet{sun-etal-2019-utilizing}




% ===================================================== Papery a poznamky k related work ABSA ======================
% 
% Veselovska http://ceur-ws.org/Vol-1422/95.pdf Czech Aspect-Based Sentiment Analysis: A New Dataset and Preliminary Results

% Unsupervised Methods to Improve Aspect-Based Sentiment Analysis in Czech  - http://www.scielo.org.mx/pdf/cys/v20n3/1405-5546-cys-20-03-00365.pdf

% https://aclanthology.org/W14-2605.pdf - Aspect-Level Sentiment Analysis in Czech 

% Lada, Tigi - Neural Networks for Sentiment Analysis in Czech http://ceur-ws.org/Vol-1649/48.pdf

% https://link.springer.com/chapter/10.1007/978-3-030-59430-5_5 - BERT-Based Sentiment Analysis Using Distillation

% tang-etal-2016-effective Effective {LSTM}s for Target-Dependent Sentiment Classification https://aclanthology.org/C16-1311/

% Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence - https://aclanthology.org/N19-1035/

% rietzler-etal-2020-adapt - dela take aspect term extraction - delaji take aspect term
% xu-etal-2019-bert - dela Aspect term extraction
% liu-etal-2021-solving - delaji aspect category

% % https://scholar.google.com/scholar_url?url=https://journals.flvc.org/FLAIRS/article/download/130601/133875&hl=en&sa=X&d=1851537498729675391&ei=fMR8YpSCOPGTy9YPgoWs8A8&scisig=AAGBfm2pMd1hab4Wf9RNB7PlSgHN6Fvr2Q&oi=scholaralrt&hist=LajzdGUAAAAJ:245340974492516985:AAGBfm2xl48mkLjkiGKmAlbQP-IJbEmNSg&html=&pos=3&folt=rel - Exploring BERT for Aspect-based Sentiment Analysis in Portuguese Language

% https://www.researchgate.net/profile/Florian-Steinbrenner-3/publication/350589561_An_evaluation_of_identified_and_unidentified_obstacles_to_an_implementation_of_value-based_pricing/links/61260325a8348b1a46046d95/An-evaluation-of-identified-and-unidentified-obstacles-to-an-implementation-of-value-based-pricing.pdf#page=203 UNSUPERVISED ASPECT PHRASE EXTRACTION IN CZECH DATASET

% https://arxiv.org/abs/1901.02780

% https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9644146 Aspect-based Sentiment Analysis for Urdu

% =====================================================

% ============================= Models ========================
\section{Models}

To find an effective way to combine the models, we first fine-tune the individual models separately to find the optimal set of hyper-parameters for individual tasks. Moreover, we need SRL fine-tuned model as the input for the combined models. For ABSA, we adopt the model proposed by \cite{sun-etal-2019-utilizing}. We propose a new SRL end-to-end model, specifically designed for seamless integration with other tasks.  



% snazili jsme se najit nakou cestu jak spojit SRL a ABSA, jak injectovat knowledge ze SRL do ABSY
% to the best of knowledge there is no such end2end model for SRL

 
\subsection{Semantic Role Labeling}
Our goal is to train a universal encoder that effectively captures SRL information from a plain-text input. To accomplish this, we propose an end-to-end model with a single projection layer on the top of the ELECTRA encoder (or any other pre-trained language model). This way, all the information useful to predict role labels is encoded in the last hidden state of the encoder. Consequently, we can use this representation in other tasks. Although our end-to-end model exhibits lower performance than the commonly used BERT SRL model \cite{shi2019simple, sido-etal-2021-czert}, we believe it is more suitable for this task. 
% The comparison of the models is in Table \ref{tab:e2eSRL-performance}. The datasets and metrics for the evaluation are described later in Section \ref{sec:experiments}.


\par In our end-to-end model, we first encode the whole sentence and then iterate over all possible word pairs (the first word is a potential predicate and the second is a potential argument). For each potential predicate-argument pair, we first concatenate the representations of predicate and argument and then classify the argument role. If the potential predicate is not a real predicate word or the potential argument is not an argument of the predicate, the role of the pair is set to \textit{Other}. If a word is represented by multiple subword tokens, only the first token is classified. This is common practice in tagging tasks where the model learns to encode the semantics of a multi-token word into the first subword, then each word has a single token on the output for its classification. 


% Puvodni
% \par Our solution encodes the whole predicate-argument structure of the sentence into a single hidden state of a transformer model. \citet{zhang2021semantic} encodes each argument separately and requires gold arguments on input. Our model requires only text as input, but the model from \citet{zhang2021semantic}, requires pair of text-predicate and the produced output is a representation only of the input pair (text-predicate) and not the entire SRL output for the whole sentence for all predicates.

\par Our approach differs from that of \citet{zhang2021semantic} in terms of how the predicate-argument structure of the sentence is encoded within the transformer model. While \citet{zhang2021semantic} encodes each argument separately and requires gold arguments on input, our model only requires plain text as input. In other words, our model requires only text as input, but the model proposed by \citet{zhang2021semantic} operates on pairs of text-predicate, producing representations solely for the input pair rather than the entire SRL output encompassing all predicates within the sentence. Figure \ref{fig:SRL} shows the schema of our end-to-end SRL model.

% Figure environment removed






For our approach, it is necessary to have the same format of input (i.e., plain text) for both tasks that are combined. This is the reason why we need our end-to-end SRL model.
% \citet{zhang2021semantic} propose task-specific architectures for SRL which makes it more difficult to use in combined models.
For multitask learning, we need a general-purpose model, the same for both tasks. The task-specific models may yield better results on the SRL task, but they are specifically oriented only on the SRL task and makes their integration with ABSA or utilization in multitask learning challenging, if not impossible.


\subsection{Aspect-Based Sentiment}
\label{sec:absa-model}
\par As we mentioned in the introduction, we tackle the CE and CP subtasks of ABSA, as one classification task. We adopt the same approach as \citet{sun-etal-2019-utilizing}, and we construct auxiliary sentences and convert the subtasks to a binary classification task.
% \footnote{They named this approach NLI-B. We selected the NLI-B approach based on our development data.}



We use the NLI-B approach from \citet{sun-etal-2019-utilizing} to build the auxiliary sentences. For each sentence, we build multiple auxiliary pseudo sentences that are generated for every combination of all polarity labels and aspect categories\footnote{For English we have four polarity labels plus artificial label \textit{none} and five aspect categories, i.e. $25$ possible auxiliary sentences. For Czech there is $20$ possible sentences ($3+1$ polarity labels and five aspect categories).}. Each example has a binary label $l \in \{0, 1\}$; $l = 1$ if the auxiliary sentence corresponds to the original labels, $l = 0$ otherwise. We also add the artificial polarity class \textit{none} that has assigned binary label $l = 1$ if there is no aspect category for a given sentence. The pseudo auxiliary sentence consists only of a polarity label and aspect category in a given language. For example, the auxiliary sentences for all aspects of the sentence \say{\textit{The \textcolor{green}{burger} was excellent but the \textcolor{red}{waitress}  was unpleasant}} are shown in Figure \ref{fig:absa-example-appendix}.



% For example, for the sentence from Figure \ref{fig:absa-example-appendix}, the auxiliary sentences for all aspects look  as follows:


% \begin{itemize}
% 	\small
% 	\item[] 
% 	\hspace{-0.8cm}
% 	{ \say{\textit{The \textcolor{green}{burger} was excellent but the \textcolor{red}{waitress}  was unpleasant}}}

% \centering
% \hspace{-0.9cm}
% \end{itemize}

\vspace{0.4cm}
\begin{adjustbox}{width=0.9\linewidth,center}


\begin{tabular}{cllccll} \toprule
\multicolumn{1}{c}{label}  &  & \multicolumn{1}{c}{sentence}&  & \multicolumn{1}{l}{label} &  & \multicolumn{1}{c}{sentence} \\ \midrule
\multicolumn{3}{c}{food} & & \multicolumn{3}{c}{service} \\ \cline{1-3} \cline{5-7}
1 & $\Rightarrow$ & positive -- food & & 0 & $\Rightarrow$ & positive -- service \\
0 & $\Rightarrow$ & negative -- food & & 1 & $\Rightarrow$ & negative -- service \\
0 & $\Rightarrow$ & neutral -- food & & 0 & $\Rightarrow$ & neutral -- service \\
0 & $\Rightarrow$ & conflict -- food & & 0 & $\Rightarrow$ & conflict -- service \\
0 & $\Rightarrow$ & none -- food & & 0 & $\Rightarrow$ & none -- service \\ \cdashlinelr{1-7}
\multicolumn{3}{c}{price} & & \multicolumn{3}{c}{ambience} \\ \cline{1-3} \cline{5-7}
0 & $\Rightarrow$ & positive -- price & & 0 & $\Rightarrow$ & positive -- ambience \\
0 & $\Rightarrow$ & negative -- price & & 0 & $\Rightarrow$ & negative -- ambience \\
0 & $\Rightarrow$ & neutral -- price & & 0 & $\Rightarrow$ & neutral -- ambience \\
0 & $\Rightarrow$ & conflict -- price& & 0 & $\Rightarrow$ & conflict -- ambience \\
1 & $\Rightarrow$ & none -- price & & 1 & $\Rightarrow$ & none -- ambience \\ \cdashlinelr{1-7}
\multicolumn{3}{c}{general} & & \multicolumn{1}{l}{} &  &  \\ \cline{1-3}
0 & $\Rightarrow$ & positive -- general & & \multicolumn{1}{l}{} &  &  \\
0 & $\Rightarrow$ & negative -- general & & \multicolumn{1}{l}{} &  &  \\
0 & $\Rightarrow$ & neutral -- general  & & \multicolumn{1}{l}{} &  &  \\
0 & $\Rightarrow$ & conflict -- general & & \multicolumn{1}{l}{} &  &  \\
1 & $\Rightarrow$ & none -- general & & \multicolumn{1}{l}{} &  & \\ \bottomrule
\end{tabular}

\end{adjustbox}
	\captionof{figure}{Example of auxiliary sentences.}
	\label{fig:absa-example-appendix}
\vspace{0.4cm}

\par Each auxiliary sentence is combined with the original sentence and separated with \texttt{[SEP]} token and forms one training example, e.g., \texttt{[CLS]} \textit{positive - food} \texttt{[SEP]} \textit{the burger was excellent but the waitress was unpleasant} \texttt{[SEP]}. We fine-tune the pre-trained transformer model for the binary classification task on all generated training examples as \citet{sun-etal-2019-utilizing}.


 
% This model is used as a \textit{baseline} and it is a part of the combined models.
% , see Appendix \ref{appendix:absa-fine-tuning}.






\subsection{Combined Models}

\par We propose several models designed to use SRL representation to enhance ABSA performance. The first type of model predicts aspect and sentiment using concatenated representations from both the SRL and ABSA encoders. The SRL encoder is pre-trained (pre-fine-tuned) on the SRL data, and its weights remain fixed during sentiment training. Since SRL is a token-level task, we need to reduce the sequential dimension before performing the concatenation step. To address this, we employ two approaches: simple average-over-time pooling (named \textit{concat-avg}) and a convolution layer followed by max-over-time pooling (named \textit{concat-conv}). Figure \ref{fig:concat} shows the model architecture.

% Figure environment removed


\par The last model uses standard multi-task learning. We utilize a single Transformer encoder with two classification heads: one for the sentiment (standard head for sequence classification) and the other for SRL (the head architecture is presented in the previous section with the end-to-end SRL model). The model is trained using alternating batches, it means that we use different training data for both tasks, and we are not mixing them in a batch. In a single batch, we provide only ABSA or SRL data. See Figure \ref{fig:multitask} model's architecture.
% shows multi-task learning model architecture.


% Figure environment removed

% ============================= Experiments ========================
\section{Experiments}
\label{sec:experiments}
In our experiments, we aim to verify our idea that injected SRL information can improve the results of the ABSA task, particularly the CP subtask.

\begin{table*}[ht!]
    \begin{adjustbox}{width=0.8\linewidth,center}
\begin{tabular}{lllllll} \toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Category Extraction} &  & \multicolumn{2}{c}{Category Polarity} \\ \cline{2-4}  \cline{6-7}
 & \multicolumn{1}{c}{F1 Micro} & Precision & Recall &  & \multicolumn{1}{c}{Acc \#3} & \multicolumn{1}{c}{Acc \#2} \\ \midrule
baseline  & 86.04$^{\pm0.36}$ & 86.48$^{\pm0.97}$ & 85.62$^{\pm0.65}$ &  & 75.58$^{\pm0.55}$ & 88.69$^{\pm0.26}$ \\
concat-conv  & \underline{\textbf{86.58}}$^{\pm0.54}$ & \underline{\textbf{86.90}}$^{\pm0.51}$ & \underline{\textbf{86.28}}$^{\pm0.94}$ &  & \underline{\textbf{79.20}}$^{\pm0.48}$ & \underline{\textbf{90.26}}$^{\pm0.58}$ \\
concat-avg  & 86.34$^{\pm0.57}$ & 86.57$^{\pm0.84}$ & 86.12$^{\pm1.08}$ &  & 78.33$^{\pm0.64}$ & 90.06$^{\pm0.79}$ \\
multi-task  & 85.62$^{\pm0.63}$ & 86.24$^{\pm0.66}$ & 85.01$^{\pm0.66}$ &  & 77.27$^{\pm0.69}$ & 89.00$^{\pm0.63}$ \\ \cdashlinelr{1-7} 
baseline \cite{hercig2016unsupervised}* & 71.70\phantom{****} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} &  & 69.70\phantom{****} & \multicolumn{1}{c}{-} \\
best \cite{hercig2016unsupervised}* & 80.00\phantom{****} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} &  & 75.20\phantom{****} & \multicolumn{1}{c}{-} \\
CNN2 \cite{lenc2016neural} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} &  & 69.00$^{\pm2.00}$ & \multicolumn{1}{c}{-} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Czech results for the category extraction (CE) subtask as F1 Micro score, Precision and Recall. Results for the category polarity (CP) subtask as accuracy for three polarity labels (Acc \#3) and binary polarity labels (Acc \#2). Results marked with * symbol were obtained by 10-fold cross-validation.} \label{tab:results-cs}
\end{table*}


\begin{table*}[ht!]
    \begin{adjustbox}{width=0.95\linewidth,center}
\begin{tabular}{llllllll} \toprule
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{3}{c}{Category Extraction} &  & \multicolumn{3}{c}{Category Polarity} \\ \cline{2-4}  \cline{6-8}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{F1 Micro} & Precision & Recall &  & Acc \#4 & \multicolumn{1}{c}{Acc \#3} & \multicolumn{1}{c}{Acc \#2} \\ \midrule
baseline & 89.50$^{\pm0.45}$ & 90.95$^{\pm0.70}$ & 88.09$^{\pm0.48}$ &  & 83.03$^{\pm0.43}$ & 86.91$^{\pm0.55}$ & 92.74$^{\pm0.53}$ \\
concat-conv & \textbf{89.74}$^{\pm0.55}$ & \textbf{91.24}$^{\pm0.54}$ & \textbf{88.28}$^{\pm0.77}$ &  & \textbf{84.19}$^{\pm0.49}$ & \textbf{88.08}$^{\pm0.41}$ & \textbf{93.76}$^{\pm0.46}$ \\
concat-avg & 89.58$^{\pm0.43}$ & 91.15$^{\pm0.60}$ & 88.08$^{\pm0.66}$ &  & 84.13$^{\pm0.51}$ & 87.95$^{\pm0.46}$ & 93.49$^{\pm0.44}$ \\
multi-task & 89.36$^{\pm0.15}$ & 90.72$^{\pm0.52}$ & 88.05$^{\pm0.44}$ &  & 82.83$^{\pm1.10}$ & 87.05$^{\pm1.21}$ & 92.74$^{\pm0.79}$ \\ \cdashlinelr{1-8} 
XRCE \cite{brun-etal-2014-xrce}  & 82.29\phantom{****} & 83.23\phantom{****} & 81.37\phantom{****} &  & 78.10\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} \\
NRC \cite{kiritchenko-etal-2014-nrc}  & 88.58\phantom{****} & 91.04\phantom{****} & 86.24\phantom{****} &  & 82.90\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} \\
BERT single \cite{sun-etal-2019-utilizing}  & 90.89\phantom{****} & 92.78\phantom{****} & 89.07\phantom{****} &  & 83.70\phantom{****} & 86.90\phantom{****} & 93.30\phantom{****} \\
NLI-B \cite{sun-etal-2019-utilizing}  & 92.18\phantom{****} & 93.57\phantom{****} & 90.83\phantom{****} &  & 84.60\phantom{****} & 88.70\phantom{****} & 95.10\phantom{****} \\
QACG-B \cite{wu2021context}  & 92.64\phantom{****} & 94.38$^{\pm0.31}$ & \underline{90.97}$^{\pm0.28}$ &  & \underline{86.80}$^{\pm0.80}$ & 90.10$^{\pm0.30}$ & \underline{95.60}$^{\pm0.40}$ \\
BART generation \cite{liu-etal-2021-solving}  & \underline{92.80}\phantom{****} & \underline{95.18}\phantom{****} & 90.54\phantom{****} &  & \multicolumn{1}{c}{-}\phantom{****} & \underline{90.55}$^{\pm0.32}$ & \multicolumn{1}{c}{-}\phantom{****}
\\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{English results for the category extraction (CE) subtask as F1 Micro score, Precision and Recall. Results for category polarity (CP) subtask as accuracy for four polarity labels (Acc \#4), three polarity labels (Acc \#3) and binary polarity labels (Acc \#2).} \label{tab:results-en}
\end{table*}

\subsection{Datasets \& Models Fine-Tuning}

\par For Semantic Role Labeling, we use OntoNotes 5.0 dataset \cite{weischedel2013ontonotes} for English and CoNLL 2009 \cite{hajic2009conll} for Czech. As metrics, we report the whole role F1 score for both datasets. Additionally, for English, we report CoNLL 2003 official score as a comparative metric as it is the standard metric used with OntoNotes. 

\par For Aspect-Based Sentiment, we use the widely-used English dataset from \citet{pontiki-etal-2014-semeval} that consists of 3,044 train and 800 test sentences from the restaurant domain. The English dataset contains four sentiment labels: \textit{positive}, \textit{negative}, \textit{neutral}, and \textit{conflict}. Further, we split\footnote{\label{foot:github}For both English and Czech we provide a script to obtain the same split distribution.} the original training part of 3,044 sentences into development (10\%) and training parts (90\%).

For Czech experiments, we employ the dataset from \citet{hercig2016unsupervised} with 2,149 sentences from the restaurant domain. Unlike in the English dataset, there are only three polarity labels: \textit{positive}, \textit{negative}, and \textit{neutral}. Because the dataset has no official split, we divided\footref{foot:github} the data into training, development, and testing parts with the following ratio: $72 \%$ for training, $8 \%$ for the development evaluation, and $20 \%$ for testing. Both Czech and English datasets contain five aspect categories: \textit{food}, \textit{service}, \textit{price}, \textit{ambience}, and \textit{general}.


% \subsection{Models Fine-Tuning}
For our experiments on English, we use the pre-trained \textit{ELECTRA-small} model introduced by \citet{clark2020electra}, which has 14M parameters. For Czech, we employ the pre-trained monolingual model \textit{Small-E-Czech} \cite{kocian2021siamese} with the same size and architecture. Firstly, we train separate models for both tasks (ABSA and SRL) and select the optimal set of hyper-parameters on the development data. We then use the same hyper-parameters in combined models. For the details of hyper-parameters, see Appendix \ref{appendix:hp-params}.


% We use the Adam \cite{Kingma-adam} optimizer with default parameters ($\beta_1 = 0.9, \beta_2 = 0.999$) and the cross-entropy loss function for all our experiments. The initial learning rate is set to 2e-5 with linear decay to zero. We fine-tune all models with batch size 32 and maximum sequence length 256. All data fit into this length. The models are trained for 120 epochs in Czech and 40 epochs in English. The epochs are measured in ABSA data. The multi-task model is trained on the same amount of SRL data additionally (because we use alternating batches). 


% For the details of hyper-parameters, see Appendix \ref{appendix:hp-params}.

\subsection{Results \& Discussion}
We report the results of our end-to-end SRL model in Table \ref{tab:e2eSRL-performance}. As we expected, our model performs worse than the model proposed by \citet{shi2019simple}, but the results are reasonably high (considering that it does not have gold predicates on input).

\begin{table}[ht!]
    \centering
    \begin{adjustbox}{width=0.9\linewidth,center}
    \begin{tabular}{cccc} \toprule
        Model & EN & EN-conll05 & CS \\ \midrule
        \cite{shi2019simple} & 88.89 & 85.20 & 83.09 \\
        end-to-end (ours) & 84.54 & 81.51 & 79.74\\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Comparison of results of the standard model and our end-to-end SRL model (reported in F1 scores, the official metrics, for the datasets used).}
    \label{tab:e2eSRL-performance}
\end{table}

Results for our ABSA experiments in Czech and English are shown in Tables \ref{tab:results-cs} and \ref{tab:results-en}, respectively. The \textit{baseline} refers to the model described in Section \ref{sec:absa-model} without any injected SRL information.

% This model is used as a \textit{baseline} and it is a part of the combined models.
The SotA results are underlined and the best results for our experiments are bold. We include the results with the 95\% confidence interval (experiments repeated 12 times). We use the F1 Micro and accuracy for the CE and CP subtasks, respectively.

% Detailed results with precision and recall for the CE subtasks are in Appendix \ref{appendix:hp-params}.

% Tables \ref{tab:results-cs-extendex} and \ref{tab:results-en-extendex}.






% \begin{table}[ht!]
% \begin{adjustbox}{width=0.99\linewidth,center}
% \begin{tabular}{lcccc} \toprule
% \multirow{2}{*}{Model} & \multicolumn{1}{c}{\small Category Extraction} &  & \multicolumn{2}{c}{\small Category Polarity} \\ \cline{2-2} \cline{4-5}
%  & \multicolumn{1}{c}{F1 Micro} &  & \multicolumn{1}{c}{Acc \#3} & \multicolumn{1}{c}{Acc \#2} \\ \midrule
% baseline & 86.04$^{\pm0.36}$ &  & 75.58$^{\pm0.55}$ & 88.69$^{\pm0.26}$ \\
% concat-conv & \underline{\textbf{86.58}}$^{\pm0.54}$ &  & \underline{\textbf{79.20}}$^{\pm0.48}$ & \underline{\textbf{90.26}}$^{\pm0.58}$ \\
% concat-avg & 86.34$^{\pm0.57}$ &  & 78.33$^{\pm0.64}$ & 90.06$^{\pm0.79}$ \\
% multi-task & 85.62$^{\pm0.63}$ &  & 77.27$^{\pm0.69}$ & 89.40$^{\pm0.63}$ \\ \cdashlinelr{1-5} 
% baseline \cite{hercig2016unsupervised}*  & 71.70\phantom{****} &  & 69.70\phantom{****} & \multicolumn{1}{c}{-} \\
% best \cite{hercig2016unsupervised}*  & 80.00\phantom{****} &  & 75.20\phantom{****} & \multicolumn{1}{c}{-} \\
% CNN2 \cite{lenc2016neural} & \multicolumn{1}{c}{-} &  & 69.00$^{\pm2.00}$ & \multicolumn{1}{c}{-} \\
%  \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Czech results for the CE subtask as F1 Micro score and CP subtask as accuracy for three polarity labels (Acc \#3) and binary polarity labels (Acc \#2). Results marked with * were obtained by cross-validation.} \label{tab:results-cs}
% \end{table}




% \begin{table}[ht!]
% \begin{adjustbox}{width=0.99\linewidth,center}
% \begin{tabular}{lccccc} \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{1}{c}{\small Category Extraction} &  & \multicolumn{3}{c}{\small Category Polarity} \\ \cline{2-2} \cline{4-6}
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{F1 Micro} &  & Acc \#4 & \multicolumn{1}{c}{Acc \#3} & \multicolumn{1}{c}{Acc \#2} \\ \midrule
% baseline & 89.50$^{\pm0.45}$ &  & 83.03$^{\pm0.43}$ & 86.91$^{\pm0.55}$ & 92.74$^{\pm0.53}$ \\
% concat-conv & \textbf{89.74}$^{\pm0.55}$ &  & \textbf{84.19}$^{\pm0.49}$ & \textbf{88.08}$^{\pm0.41}$ & \textbf{93.76}$^{\pm0.46}$ \\
% concat-avg & 89.58$^{\pm0.43}$ &  & 84.13$^{\pm0.51}$ & 87.95$^{\pm0.46}$ & 93.49$^{\pm0.44}$ \\
% multi-task & 89.36$^{\pm0.15}$ &  & 82.83$^{\pm1.10}$ & 87.05$^{\pm1.21}$ & 92.74$^{\pm0.79}$ \\ \cdashlinelr{1-6}

% XRCE \cite{brun-etal-2014-xrce}  & 82.29\phantom{****} &  & 78.10\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} \\
% NRC\cite{kiritchenko-etal-2014-nrc} & 88.58\phantom{****} &  & 82.90\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} \\
% BERT single \cite{sun-etal-2019-utilizing}  & 90.89\phantom{****} &  & 83.70\phantom{****} & 86.90\phantom{****} & 93.30\phantom{****} \\
% NLI-B \cite{sun-etal-2019-utilizing}  & 92.18\phantom{****} &  & 84.60\phantom{****} & 88.70\phantom{****} & 95.10\phantom{****} \\
% QACG-B \cite{wu2021context}  & 92.64\phantom{****} &  & \underline{86.80}$^{\pm0.80}$ & 90.10$^{\pm0.30}$ & \underline{95.60}$^{\pm0.40}$ \\
% BART gen. \cite{liu-etal-2021-solving}  & \underline{92.80}\phantom{****} &  & \multicolumn{1}{c}{-}\phantom{****} & \underline{90.55}$^{\pm0.32}$ & \multicolumn{1}{c}{-}\phantom{****} \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{English results for the CE subtask as F1 Micro score and CP subtask as accuracy for four polarity labels (Acc \#4), three polarity labels (Acc \#3) and binary polarity labels (Acc \#2).} \label{tab:results-en}
% \end{table}

% \cdashline{2-2} \cdashline{4-6}


% \vspace{-0.4cm}
% ============================= Discussion ========================
% \subsection{Discussion}
\par Based on the results presented in Tables \ref{tab:results-cs} and \ref{tab:results-en}, we can observe that our proposed models (\textit{concat-conv} and \textit{concat-avg}) with injected SRL information consistently enhance results for the CP subtask in both languages. These improvements are statistically significant. The performance of the \textit{concat-conv} and \textit{concat-avg} models does not exhibit a significant difference. In the CE subtask, we achieve the same results as the \textit{baseline} model. We think that the CE subtask is more distant from the SRL task than the CP subtask and therefore, the injection of the semantic information does not help. In other words, the semantic structure of the sentence may not play a crucial role in aspect detection (that can be viewed as multi-label text classification). On the other hand, for the CP subtask, the combined models can leverage the semantic structure of the sentence to their advantage.


\par For the Czech ABSA dataset we achieve new SotA results on both subtasks\footnote{It is worth noting that although the test data we used differ from those used by \citet{hercig2016unsupervised} due to their 10-fold cross-validation, the performance difference is substantial enough to demonstrate the superiority of our approach.}. As we expected, we did not outperform the current SotA results for the English dataset, as our ELECTRA model has considerably fewer parameters than SotA models. For Czech, the \textit{multi-task} model exhibited a marginal improvement in the results and generally, the model was significantly inferior to our other models. We decided to use the smaller ELECTRA-based models because of their much smaller computation requirements. However, in future work, we plan comparison with larger models like BERT or RoBERTa to obtain the overall performance overview of our approach.





% ======== DONE
%  druhej task to statisticky significkantne zlepsuje
%  vysledky jsou konzistentni napric jazyka to validuje nasi domenku to SRL muze vylepsovat ABSA
%  concat convolution a average fungujuou srovnatelne
%  rict ze jsme dosahli state of the art vysledku pro oba subtasky na cestine
%  sota vysledky jsou lepsi protoze jsou to vetsi modely
%  pro anglictinu asi napsat ze jsme neocekavali ze prekoname SOTA ale ze jsme chteli vylepsit mensi model, 

% ============================= Conclusion ========================
\section{Conclusion}
In this work, we introduce a novel end-to-end SRL model that we use to improve the aspect category polarity task.  Our contribution lies in proposing several methods to integrate SRL and ABSA models, which ultimately lead to improved performance. The experimental results validate our initial assumption that leveraging semantic information extracted from an SRL model can significantly enhance the aspect category polarity task. Importantly, the approaches we propose are versatile and can be applied to combine Transformer-based models for other related tasks as well, extending the scope of their applicability.


\par Moreover, we believe that our approaches hold even greater potential in addressing other ABSA subtasks, namely term extraction and term polarity classification. These subtasks could benefit from the integration of SRL and ABSA models in a similar manner. Further, we would like to validate our approach on larger models, for example, BERT or RoBERTa.

% Based on our results and observations we hope/beleive  ake doufame ze to SRL by mohlo vyznamne pomoci v Aspect Term Extraction a Aspect Term Polarity, kde je dulezite brat v uvahu vazby mezi aspect termy a slovy ktere modifikuji jejich polaritu





% ============================= Acknowledgements ========================


\section*{Acknowledgements}
This work has been partly supported by grant No. SGS-2022-016 Advanced methods of data processing and analysis.
Computational resources were provided by the e-INFRA CZ project (ID:90140), supported by the Ministry of Education, Youth and Sports of the Czech Republic.



% ============================= Bibliography ========================

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

% ============================= Appendix ========================



% ===========  pozor tohle se musi nahrat samostatne  pro review jinak to vykopnou
% https://www.ilovepdf.com/split_pdf#split,range
% \clearpage
\appendix

% \begin{table*}[ht!]
%     \begin{adjustbox}{width=0.9\linewidth,center}
% \begin{tabular}{lllllll} \toprule
% \multirow{2}{*}{Model} & \multicolumn{3}{c}{Category Extraction} &  & \multicolumn{2}{c}{Category Polarity} \\ \cline{2-4}  \cline{6-7}
%  & \multicolumn{1}{c}{F1 Micro} & Precision & Recall &  & \multicolumn{1}{c}{Acc \#3} & \multicolumn{1}{c}{Acc \#2} \\ \midrule
% baseline  & 86.04$^{\pm0.36}$ & 86.48$^{\pm0.97}$ & 85.62$^{\pm0.65}$ &  & 75.58$^{\pm0.55}$ & 88.69$^{\pm0.26}$ \\
% concat-conv  & \underline{\textbf{86.58}}$^{\pm0.54}$ & \underline{\textbf{86.90}}$^{\pm0.51}$ & \underline{\textbf{86.28}}$^{\pm0.94}$ &  & \underline{\textbf{79.20}}$^{\pm0.48}$ & \underline{\textbf{90.26}}$^{\pm0.58}$ \\
% concat-avg  & 86.34$^{\pm0.57}$ & 86.57$^{\pm0.84}$ & 86.12$^{\pm1.08}$ &  & 78.33$^{\pm0.64}$ & 90.06$^{\pm0.79}$ \\
% multi-task  & 85.62$^{\pm0.63}$ & 86.24$^{\pm0.66}$ & 85.01$^{\pm0.66}$ &  & 77.27$^{\pm0.69}$ & 89.00$^{\pm0.63}$ \\ \cdashlinelr{1-7} 
% baseline \cite{hercig2016unsupervised}* & 71.70\phantom{****} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} &  & 69.70\phantom{****} & \multicolumn{1}{c}{-} \\
% best \cite{hercig2016unsupervised}* & 80.00\phantom{****} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} &  & 75.20\phantom{****} & \multicolumn{1}{c}{-} \\
% CNN2 \cite{lenc2016neural} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} &  & 69.00$^{\pm2.00}$ & \multicolumn{1}{c}{-} \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Detailed Czech results for the CE subtask as F1 Micro score, Precision and Recall and CP subtask as accuracy for three polarity labels (Acc \#3) and binary polarity labels (Acc \#2). Results marked with * symbol were obtained by 10-fold cross-validation.} \label{tab:results-cs-extendex}
% \end{table*}

% \begin{table*}[ht!]
%     \begin{adjustbox}{width=0.9\linewidth,center}
% \begin{tabular}{llllllll} \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{3}{c}{Category Extraction} &  & \multicolumn{3}{c}{Category Polarity} \\ \cline{2-4}  \cline{6-8}
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{F1 Micro} & Precision & Recall &  & Acc \#4 & \multicolumn{1}{c}{Acc \#3} & \multicolumn{1}{c}{Acc \#2} \\ \midrule
% baseline & 89.50$^{\pm0.45}$ & 90.95$^{\pm0.70}$ & 88.09$^{\pm0.48}$ &  & 83.03$^{\pm0.43}$ & 86.91$^{\pm0.55}$ & 92.74$^{\pm0.53}$ \\
% concat-conv & \textbf{89.74}$^{\pm0.55}$ & \textbf{91.24}$^{\pm0.54}$ & \textbf{88.28}$^{\pm0.77}$ &  & \textbf{84.19}$^{\pm0.49}$ & \textbf{88.08}$^{\pm0.41}$ & \textbf{93.76}$^{\pm0.46}$ \\
% concat-avg & 89.58$^{\pm0.43}$ & 91.15$^{\pm0.60}$ & 88.08$^{\pm0.66}$ &  & 84.13$^{\pm0.51}$ & 87.95$^{\pm0.46}$ & 93.49$^{\pm0.44}$ \\
% multi-task & 89.36$^{\pm0.15}$ & 90.72$^{\pm0.52}$ & 88.05$^{\pm0.44}$ &  & 82.83$^{\pm1.10}$ & 87.05$^{\pm1.21}$ & 92.74$^{\pm0.79}$ \\ \cdashlinelr{1-8} 
% XRCE \cite{brun-etal-2014-xrce}  & 82.29\phantom{****} & 83.23\phantom{****} & 81.37\phantom{****} &  & 78.10\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} \\
% NRC \cite{kiritchenko-etal-2014-nrc}  & 88.58\phantom{****} & 91.04\phantom{****} & 86.24\phantom{****} &  & 82.90\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} & \multicolumn{1}{c}{-}\phantom{****} \\
% BERT single \cite{sun-etal-2019-utilizing}  & 90.89\phantom{****} & 92.78\phantom{****} & 89.07\phantom{****} &  & 83.70\phantom{****} & 86.90\phantom{****} & 93.30\phantom{****} \\
% NLI-B \cite{sun-etal-2019-utilizing}  & 92.18\phantom{****} & 93.57\phantom{****} & 90.83\phantom{****} &  & 84.60\phantom{****} & 88.70\phantom{****} & 95.10\phantom{****} \\
% QACG-B \cite{wu2021context}  & 92.64\phantom{****} & 94.38$^{\pm0.31}$ & \underline{90.97}$^{\pm0.28}$ &  & \underline{86.80}$^{\pm0.80}$ & 90.10$^{\pm0.30}$ & \underline{95.60}$^{\pm0.40}$ \\
% BART generation \cite{liu-etal-2021-solving}  & \underline{92.80}\phantom{****} & \underline{95.18}\phantom{****} & 90.54\phantom{****} &  & \multicolumn{1}{c}{-}\phantom{****} & \underline{90.55}$^{\pm0.32}$ & \multicolumn{1}{c}{-}\phantom{****}
% \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Detailed English results for the CE subtask as F1 Micro score, Precision and Recall and CP subtask as accuracy for four polarity labels (Acc \#4), three polarity labels (Acc \#3) and binary polarity labels (Acc \#2).} \label{tab:results-en-extendex}
% \end{table*}


% \section{ABSA Fine-Tuning}
% \label{appendix:absa-fine-tuning}
% We fine-tune the pre-trained ELECTRA model with a classification ABSA head. We use the hidden vector $\vec{h} \in \mathbb{R}^H$ of the classification token \texttt{[CLS]} that represents the entire input sequence, where $H$ is the hidden size of the model. The vector is obtained from the pooling layer, i.e., from a fully-connected layer of size $H$ with a gelu activation function. The dropout of $0.1$ is applied and the result is then passed into a task-specific linear layer represented by matrix $\vec{W} \in \mathbb{R}^{|2| \times H}$. The output class $c$ is computed as  $c = \argmax(\vec{h}\vec{W}^{T})$.

% \section{Appendix}
\makeatletter
\setlength{\@fptop}{0pt}
\makeatother
% Figure environment removed
% \FloatBarrier




\section{Training Hyper-Parameters} \label{appendix:hp-params}
We use the Adam \cite{Kingma-adam} optimizer with default parameters ($\beta_1 = 0.9, \beta_2 = 0.999$) and the cross-entropy loss function for all our experiments. The initial learning rate is set to 2e-5 with linear decay to zero. We fine-tune all models with batch size 32 and maximum sequence length 256. All data fit into this length. The models are trained for 120 epochs in Czech and 40 in English. The epochs are measured in ABSA data. The multi-task model is trained on the same amount of SRL data additionally (because we use alternating batches). 


\section{Semantic Parse Tree Example}
\label{sec:semantic-parse-examples}
As mentioned in the introduction section, we assume that leveraging SRL information can prove advantageous in the aspect category polarity (CP) task. To illustrate this point, consider the annotation depicted in Figure \ref{fig:absa-example-2}, where we can observe the SRL relation extracted (see Figure \ref{fig:srl-sentiment-tree}) between the words \textit{forgotten} and \textit{food}. The information about this relation can help to understand the model that these words are related and help the model to predict the negative polarity of the food aspect category.

\newpage
\begin{itemize}
	\small
	\item[] \hspace{-0.8cm} { \say{\textit{This \textcolor{green}{place} is really trendy but they have forgotten about the most important part of a  restaurant, the \textcolor{red}{food}. }}}
	    \vspace{-0.27cm}
	\item[] CE $\Rightarrow$ food, ambience \\
	CP $\Rightarrow$  food:\textit{negative}, ambience:\textit{positive}
	\captionof{figure}{Example of CE and CP annotations.}\label{fig:absa-example-2}
\end{itemize}


\label{sec:semantic-parse-examplse}


% % Figure environment removed

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.





\end{document}
