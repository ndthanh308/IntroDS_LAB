\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~1, 2004.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Argenson and Dulac-Arnold(2020)]{argenson2020model}
Arthur Argenson and Gabriel Dulac-Arnold.
\newblock Model-based offline planning.
\newblock \emph{arXiv preprint arXiv:2008.05556}, 2020.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and
  Bruna]{brandfonbrener2021offline}
David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna.
\newblock Offline rl without off-policy evaluation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4933--4946, 2021.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  Zo{\l}na, Aytar, Budden, Vecerik, et~al.]{cabi2019framework}
Serkan Cabi, Sergio~G{\'o}mez Colmenarejo, Alexander Novikov, Ksenia
  Konyushkova, Scott Reed, Rae Jeong, Konrad Zo{\l}na, Yusuf Aytar, David
  Budden, Mel Vecerik, et~al.
\newblock A framework for data-driven robotics.
\newblock \emph{arXiv preprint arXiv:1909.12200}, 2019.

\bibitem[Chen et~al.(2021)Chen, Nair, and Finn]{chen2021learning}
Annie~S Chen, Suraj Nair, and Chelsea Finn.
\newblock Learning generalizable robotic reward functions from" in-the-wild"
  human videos.
\newblock \emph{arXiv preprint arXiv:2103.16817}, 2021.

\bibitem[Dayan(1993)]{dayan1993improving}
Peter Dayan.
\newblock Improving generalization for temporal difference learning: The
  successor representation.
\newblock \emph{Neural computation}, 5\penalty0 (4):\penalty0 613--624, 1993.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 503--556,
  2005.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Levine, and
  Salakhutdinov]{eysenbach2021replacing}
Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov.
\newblock Replacing rewards with examples: Example-based policy search via
  recursive classification, 2021.

\bibitem[Eysenbach et~al.(2022)Eysenbach, Zhang, Salakhutdinov, and
  Levine]{eysenbach2022contrastive}
Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine.
\newblock Contrastive learning as goal-conditioned reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2206.07568}, 2022.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Chelsea Finn, Sergey Levine, and Pieter Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pages 49--58.
  PMLR, 2016.

\bibitem[Fu et~al.(2018{\natexlab{a}})Fu, Luo, and Levine]{AIRLFu2018}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Fu et~al.(2018{\natexlab{b}})Fu, Singh, Ghosh, Yang, and
  Levine]{fu2018variational}
Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine.
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock \emph{arXiv preprint arXiv:1805.11686}, 2018{\natexlab{b}}.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06860}, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Meger, and Precup]{fujimoto2018off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock \emph{arXiv preprint arXiv:1812.02900}, 2018.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2021emaq}
Seyed Kamyar~Seyed Ghasemipour, Dale Schuurmans, and Shixiang~Shane Gu.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning}, pages
  3682--3691. PMLR, 2021.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, Friedman, and
  Friedman]{hastie2009elements}
Trevor Hastie, Robert Tibshirani, Jerome~H Friedman, and Jerome~H Friedman.
\newblock \emph{The elements of statistical learning: data mining, inference,
  and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[Ho and Ermon(2016)]{GAIL2016Ho}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Conference on Neural Information Processing Systems}, 2016.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli,
  et~al.]{hoffman2020acme}
Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal
  Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate
  Baumli, et~al.
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.00979}, 2020.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Jarboui and Perchet(2021)]{Jarboui2021OfflineIRL}
Firas Jarboui and Vianney Perchet.
\newblock Offline inverse reinforcement learning, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.05068}.

\bibitem[Julian et~al.(2020)Julian, Swanson, Sukhatme, Levine, Finn, and
  Hausman]{julian2020efficient}
Ryan Julian, Benjamin Swanson, Gaurav~S Sukhatme, Sergey Levine, Chelsea Finn,
  and Karol Hausman.
\newblock Efficient adaptation for end-to-end vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:2004.10190}, 2020.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Leslie~Pack Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, pages 1094--1099. Citeseer, 1993.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke,
  et~al.]{kalashnikov2018scalable}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog,
  Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
  Vanhoucke, et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pages 651--673. PMLR, 2018.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{kalashnikov2021mt}
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico
  Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock \emph{arXiv preprint arXiv:2104.08212}, 2021.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Konyushkova et~al.(2020)Konyushkova, Zolna, Aytar, Novikov, Reed,
  Cabi, and de~Freitas]{Konyushkova2020SSRewardLearning}
Ksenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed,
  Serkan Cabi, and Nando de~Freitas.
\newblock Semi-supervised reward learning for offline reinforcement learning.
\newblock \emph{Offline Reinforcement Learning Workshop at Neural Information
  Processing Systems}, 2020.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Tompson, Fergus, and
  Nachum]{kostrikov2021offline}
Ilya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock \emph{arXiv preprint arXiv:2103.08050}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11761--11771, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{LangeGR12}
Sascha Lange, Thomas Gabel, and Martin~A. Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement Learning}, volume~12. Springer, 2012.

\bibitem[Lee et~al.(2021)Lee, Lee, and Kim]{lee2021representation}
Byung-Jun Lee, Jongmin Lee, and Kee-Eung Kim.
\newblock Representation balancing offline model-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=QpNz8r_Ri2Y}.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2021)Li, Gupta, Reddy, Pong, Zhou, Yu, and
  Levine]{li2021mural}
Kevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr~H Pong, Aurick Zhou, Justin Yu,
  and Sergey Levine.
\newblock Mural: Meta-learning uncertainty-aware rewards for outcome-driven
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  6346--6356. PMLR, 2021.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Ma and Collins(2018)]{ma2018noise}
Zhuang Ma and Michael Collins.
\newblock Noise contrastive estimation and negative sampling for conditional
  models: Consistency and statistical efficiency.
\newblock \emph{arXiv preprint arXiv:1809.01812}, 2018.

\bibitem[Mandlekar et~al.(2020)Mandlekar, Ramos, Boots, Savarese, Fei-Fei,
  Garg, and Fox]{mandlekar2020iris}
Ajay Mandlekar, Fabio Ramos, Byron Boots, Silvio Savarese, Li~Fei-Fei, Animesh
  Garg, and Dieter Fox.
\newblock Iris: Implicit reinforcement without interaction at scale for
  learning control from offline robot manipulation data.
\newblock In \emph{2020 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 4414--4420. IEEE, 2020.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deployment}
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization.
\newblock \emph{arXiv preprint arXiv:2006.03647}, 2020.

\bibitem[Mazoure et~al.(2020)Mazoure, Tachet~des Combes, Doan, Bachman, and
  Hjelm]{mazoure2020deep}
Bogdan Mazoure, Remi Tachet~des Combes, Thang~Long Doan, Philip Bachman, and
  R~Devon Hjelm.
\newblock Deep reinforcement and infomax learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3686--3698, 2020.

\bibitem[Nair et~al.(2022)Nair, Rajeswaran, Kumar, Finn, and
  Gupta]{nair2022r3m}
Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta.
\newblock R3m: A universal visual representation for robot manipulation.
\newblock \emph{arXiv preprint arXiv:2203.12601}, 2022.

\bibitem[Ng and Russell(2000)]{ng2000irl}
Andrew~Y. Ng and Stuart~J. Russell.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, ICML '00, 2000.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, et~al.]{plappert2018multi}
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker,
  Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder,
  et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock \emph{arXiv preprint arXiv:1802.09464}, 2018.

\bibitem[Pomerleau(1988)]{pomerleau1988alvinn}
Dean~A Pomerleau.
\newblock Alvinn: an autonomous land vehicle in a neural network.
\newblock In \emph{Proceedings of the 1st International Conference on Neural
  Information Processing Systems}, pages 305--313, 1988.

\bibitem[Rafailov et~al.(2021)Rafailov, Yu, Rajeswaran, and
  Finn]{Rafailov2020LOMPO}
Rafael Rafailov, Tianhe Yu, A.~Rajeswaran, and Chelsea Finn.
\newblock Offline reinforcement learning from images with latent space models.
\newblock \emph{Learning for Decision Making and Control (L4DC)}, 2021.

\bibitem[Ratliff et~al.(2006)Ratliff, Bagnell, and Zinkevich]{ratliff2006}
Nathan~D. Ratliff, J.~Andrew Bagnell, and Martin~A. Zinkevich.
\newblock Maximum margin planning.
\newblock In \emph{Proceedings of the 23rd International Conference on Machine
  Learning}, ICML '06, 2006.

\bibitem[Reddy et~al.(2020)Reddy, Dragan, and Levine]{SQIL2020Reddy}
Siddharth Reddy, Anca~D. Dragan, and Sergey Levine.
\newblock Sqil: Imitation learning via reinforcement learning with sparse
  rewards.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Martin Riedmiller.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, pages 317--328.
  Springer, 2005.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{Dagger2011Ross}
Stephane Ross, Geoffrey~J. Gordon, and J.~Andrew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock \emph{AISTATS}, 2011.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{siegel2020keep}
Noah~Y Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019end}
Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock \emph{arXiv preprint arXiv:1904.07854}, 2019.

\bibitem[Singh et~al.(2020)Singh, Yu, Yang, Zhang, Kumar, and
  Levine]{singh2020cog}
Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey
  Levine.
\newblock Cog: Connecting new skills to past experience with offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.14500}, 2020.

\bibitem[Sinha and Garg(2021)]{sinha2021s4rl}
Samarth Sinha and Animesh Garg.
\newblock S4rl: Surprisingly simple self-supervision for offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2103.06326}, 2021.

\bibitem[Spencer et~al.(2021)Spencer, Choudhury, Venkatraman, Ziebart, and
  Bagnell]{Feedback2021Spencer}
Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and
  J.~Andrew Bagnell.
\newblock Feedback in imitation learning: The three regimes of covariate shift.
\newblock \emph{ArXiv Preprint}, 2021.

\bibitem[Swazinna et~al.(2020)Swazinna, Udluft, and
  Runkler]{swazinna2020overcoming}
Phillip Swazinna, Steffen Udluft, and Thomas Runkler.
\newblock Overcoming model bias for robust offline deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2008.05533}, 2020.

\bibitem[Whitehead and Ballard(1991)]{whitehead1991learning}
Steven~D Whitehead and Dana~H Ballard.
\newblock Learning to perceive and act by trial and error.
\newblock \emph{Machine Learning}, 7\penalty0 (1):\penalty0 45--83, 1991.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie et~al.(2018)Xie, Singh, Levine, and Finn]{xie2018few}
Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn.
\newblock Few-shot goal inference for visuomotor learning and planning.
\newblock In \emph{Conference on Robot Learning}, pages 40--52. PMLR, 2018.

\bibitem[Xu and Denil(2019)]{xu2019positive}
Danfei Xu and Misha Denil.
\newblock Positive-unlabeled reward learning.
\newblock \emph{arXiv preprint arXiv:1911.00459}, 2019.

\bibitem[Xu et~al.(2022)Xu, Zhan, Yin, and Qin]{xu2022Discriminator}
Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin.
\newblock Discriminator-weighted offline imitation learning from suboptimal
  demonstrations.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Yu et~al.(2020{\natexlab{a}})Yu, Quillen, He, Julian, Hausman, Finn,
  and Levine]{yu2020metaworld}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pages 1094--1100. PMLR,
  2020{\natexlab{a}}.

\bibitem[Yu et~al.(2020{\natexlab{b}})Yu, Thomas, Yu, Ermon, Zou, Levine, Finn,
  and Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine,
  Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020{\natexlab{b}}.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine,
  and Chelsea Finn.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{arXiv preprint arXiv:2102.08363}, 2021.

\bibitem[Yu et~al.(2022)Yu, Kumar, Chebotar, Hausman, Finn, and
  Levine]{yu2022UDS}
Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and
  Sergey Levine.
\newblock How to leverage unlabeled data in offline reinforcement learning.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Zhou et~al.(2020)Zhou, Bajracharya, and Held]{zhou2020plas}
Wenxuan Zhou, Sujay Bajracharya, and David Held.
\newblock Plas: Latent action space for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2011.07213}, 2020.

\bibitem[Zhu et~al.(2020)Zhu, Yu, Gupta, Shah, Hartikainen, Singh, Kumar, and
  Levine]{zhu2020ingredients}
Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi
  Singh, Vikash Kumar, and Sergey Levine.
\newblock The ingredients of real-world robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.12570}, 2020.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\bibitem[Zolna et~al.(2020{\natexlab{a}})Zolna, Novikov, Konyushkova, Gulcehre,
  Wang, Aytar, Denil, de~Freitas, and Reed]{zolna2020offline}
Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu
  Wang, Yusuf Aytar, Misha Denil, Nando de~Freitas, and Scott Reed.
\newblock Offline learning from demonstrations and unlabeled experience.
\newblock \emph{arXiv preprint arXiv:2011.13885}, 2020{\natexlab{a}}.

\bibitem[Zolna et~al.(2020{\natexlab{b}})Zolna, Reed, Novikov, Colmenarejo,
  Budden, Cabi, Denil, de~Freitas, and Wang]{zolna2020taskrelevant}
Konrad Zolna, Scott Reed, Alexander Novikov, Sergio~Gomez Colmenarejo, David
  Budden, Serkan Cabi, Misha Denil, Nando de~Freitas, and Ziyu Wang.
\newblock Task-relevant adversarial imitation learning.
\newblock \emph{Conference on Robot Learning}, 2020{\natexlab{b}}.

\end{thebibliography}
