\section{Introduction}
\label{sec:intro}




Reinforcement learning is typically framed as the problem of maximizing a given reward function. However, in many real-world situations, it is more natural for users to define what they want an agent to do with examples of successful outcomes~\citep{fu2018variational,zolna2020offline,xu2019positive,eysenbach2021replacing}. For example, a user that wants their robot to pack laundry into a washing machine might provide multiple examples of states where the laundry has been packed correctly. This problem setting is often seen as a variant of inverse reinforcement learning~\citep{fu2018variational}, where the aim is to learn only from examples of successful outcomes, rather than from expert demonstrations. To solve this problem, the agent must both figure out what constitutes task success (i.e., what the examples have in common) and how to achieve such successful outcomes.

In this paper, our aim is to address this problem setting in the case where the agent must learn from offline data without trial and error. Instead, the agent must infer the outcomes of potential actions from the provided data, while also relating these inferred outcomes to the success examples. We will refer to this problem of offline RL with success examples as \emph{offline example-based control}.

Most prior approaches involve two steps: \emph{first} learning a reward function, and \emph{second} combining it with an RL method to recover a policy~\citep{fu2018variational,zolna2020offline,xu2019positive}. While such approaches can achieve excellent results when provided sufficient data~\citep{kalashnikov2021mt, zolna2020offline}, learning the reward function is challenging when the number of success examples is small~\citep{li2021mural, zolna2020offline}. Moreover, these prior approaches are relatively complex (e.g., they use temporal difference learning) and have many hyperparameters.

Our aim is to provide a simple and scalable approach that avoids the challenges of reward learning. The main idea will be learning a certain type of dynamics model. Then, using that model to predict the probabilities of reaching each of the success examples, we will be able to estimate the Q-values for every state and action. Note that this approach does not use an offline RL algorithm as a subroutine. The key design decision is the model type; we will use an implicit model of the time-averaged future (precisely, the discounted state occupancy measure). This decision means that our model reasons across multiple time steps but will not output high-dimensional observations (only a scalar number).
A limitation of this approach is that it will correspond to a single step of policy improvement: the dynamics model corresponds to the dynamics of the behavioral policy, not of the reward-maximizing policy. While this means that our method is not guaranteed to yield the optimal policy, our experiments nevertheless show that our approach outperforms multi-step RL methods.










The main contribution of this paper is an offline RL method (LAEO) that learns a policy from examples of high-reward states. The key idea behind LAEO is an implicit dynamics model, which represents the probability of reaching states at some point in the future. We use this model to estimate the probability of reaching examples of high-return states. LAEO is simpler yet more effective than prior approaches based on reward classifiers. Our experiments demonstrate that LAEO can successfully solve offline RL problems from examples of high-return states on four state-based and two image-based manipulation tasks.
Our experiments show that LAEO is more robust to occlusions and also exhibits better scaling with dataset size than prior methods.
We show that LAEO can work in example-based control settings in which goal-conditioned RL methods fail. Additionally, we show that the dynamics model learned by LAEO can generalize to multiple different tasks, being used to solve tasks that are not explicitly represented in the training data. 




