\section{Proofs}
\label{appendix:proofs}

The proof follows by substituting Assumption~\ref{as:1} into the definition of Q-values (Eq.~\ref{eq:q-vals}):
\begin{proof}
{\footnotesize \begin{align*}
    Q^\beta(s, a)
    = \E_{\beta(\tau)}\left[\sum_{t=0}^\infty \gamma^t r(s_t) \bigg \vert \substack{s_0 = s\\a_0 = a}\right] &= \frac{1}{1-\gamma} \int p^\beta(s_{t+} = s^* \mid s, a) r(s^*) ds^* \\
    &= \frac{1}{1-\gamma} \int p^\beta(s_{t+} = s^* \mid s, a)  c \frac{p_*(s^*)}{p_\tau(s^*)} ds^* \\
    &= \frac{c}{1-\gamma} \int p_*(s^*) e^{f(s, a, s^*)} ds^* = \frac{c}{1-\gamma} \E_{s^* \sim p_*(s)}\left[ e^{f(s, a, s^*)} \right].
\end{align*}}
\end{proof}

\section{Experimental Details}
\label{appendix:details}

We implemented our method and all baselines using the ACME framework~\citep{hoffman2020acme}. 
\begin{itemize}
    \item Batch size: 1024 for state based experiments, 256 for image based experiments
    \item Training iterations: $250,000$ if task success rates had converged by that point, otherwise $500,000$
    \item Representation dimension: 256
    \item Reward learning loss (for baselines): binary cross entropy (for ORIL) and positive unlabeled (for PURL)
    \item Critic architecture: Two-layer MLP with hidden sizes of 1024. ReLU activations used between layers. 
    \item Reward function architecture (for baselines): Two-layer MLP with hidden sizes of 1024. ReLU activations used between layers. 
    \item Actor learning rate: $3 \times 10^{-4}$
    \item Critic learning rate: $3 \times 10^{-4}$
    \item Reward learning rate (for baselines): $1 \times 10^{-4}$
    \item $\lambda$ for behavioral cloning weight in policy loss term: $0.5$
    \item $\eta$ for PU loss: $0.5$
    \item Size of offline datasets: Each dataset on \texttt{Fetch} tasks the consists of approximately $4,000$ trajectories of length $50$, except for the \texttt{FetchPush-image} dataset, which consists of approximately $40,000$ trajectories. Each dataset on the \texttt{Sawyer} tasks consists of approximately $4,000$ trajectories of length $200$.
\end{itemize}


\section{Multitask Critic Experiments}
\label{appendix:multitask}

% Figure environment removed

The \texttt{Half-closed} task requires the agent to push the drawer from an open position into a halfway closed position. The \texttt{Open} task requires the agent to pull the drawer from a closed position into an open position. The \texttt{Close} task is the same as in the original \texttt{SawyerDrawerClose} environment, and requires the agent to push the drawer from an opened position into a closed position. 
The three reaching tasks, \texttt{Reach-near}, \texttt{Reach-medium}, \texttt{Reach-far}, require the agent to reach the end-effector to a three different target positions. The tasks are visualized in Figure \ref{fig:multitask_env}.

For these experiments, we load the final checkpoint of a critic network from the previous set of experiments (Comparison to Goal-Conditioned RL), and select actions by using cross entropy method (CEM) optimization on the critic network. By using CEM optimization, we do not need to train a separate policy network for each of the tasks.
Since the LAEO dynamics model is a multistep dynamics model (meaning that the model predicts whether a goal state will be reached sometime in the future and not just at the subsequent timestep) we are able to use CEM directly with the dynamics model. Specifically, for each task, we collect a success example using a scripted policy from~\citet{yu2020metaworld}. Then, at each environment timestep $t$, we condition the LAEO dynamics model on the success example, and then run CEM to choose an action that maximizes the output of the dynamics network. 
At each timestep, we perform 10 iterations of CEM, using a population size of $10,000$ and an elite population size of $2,000$.
Results are averaged across eight random seeds.




