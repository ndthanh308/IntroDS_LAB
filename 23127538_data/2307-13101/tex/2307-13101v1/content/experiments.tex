\section{Experiments}
\label{sec:experiments}

\begin{wrapfigure}[11]{R}{0.7\textwidth}
    \vspace{-0.7em}
    \centering
    % Figure removed%
    % Figure removed
    % Figure removed%
    % Figure removed
    \caption{\footnotesize \textbf{Benchmark tasks}: We evaluate the performance of LAEO on six simulated manipulation tasks, two of which use pixel observations (\texttt{FetchReach-image} and  \texttt{FetchPush-image}) and four of which use low-dimensional states (\texttt{FetchReach}, \texttt{FetchPush}, \texttt{SawyerWindowOpen}, and \texttt{SawyerDrawerClose} ). }
    \label{fig:main_tasks}
\end{wrapfigure}



Our experiments test whether LAEO can effectively solve offline RL tasks that are specified by examples of high-return states, rather than via scalar reward functions. We study when our approach outperforms prior approaches based on learned reward functions. We look not only at the performance relative to baselines on state-based and image-based tasks, but also how that performance depends on the size and composition of the input datasets. 
Additional experiments study how LAEO performs when provided with varying numbers of success observations and whether our method can solve partially observed tasks. 
We include full hyperparameters and implementation details in Appendix \ref{appendix:details}. 
Code is available at \url{https://github.com/khatch31/laeo}.
Videos of our method are available at \url{https://sites.google.com/view/laeo-rl}.




\paragraph{Baselines.}
Our main point of comparison will be prior methods that use learned reward functions:
ORIL~\citep{zolna2020offline} and 
PURL~\citep{xu2019positive}. 
The main difference between these methods is the loss function used to train reward function: ORIL uses 
binary cross entropy loss 
while PURL uses a positive-unlabeled loss~\citep{xu2019positive}. 
Note that the ORIL paper also reports results using a positive-unlabeled loss, but for the sake of clarity we simply refer to it as PURL.
After learning the reward function, each of these methods applies an off-the-shelf RL algorithm. We will implement all baselines using the TD3+BC~\citep{fujimoto2021minimalist} offline RL algorithm. These offline RL methods achieve good performance on tasks specified via reward functions~\citep{kostrikov2021offline, brandfonbrener2021offline, fujimoto2021minimalist}.
We also include Behavioral Cloning (BC) results.

% Figure environment removed

\paragraph{Benchmark comparison.}

We start by comparing the performance of LAEO to these baselines on six manipulation tasks. \texttt{FetchReach} and \texttt{FetchPush} are two manipulation tasks from~\citet{plappert2018multi} that use state-based observations. \texttt{FetchReach-image} and \texttt{FetchPush-image} are the same tasks but with image-based observations. \texttt{SawyerWindowOpen} and 
\texttt{Sawyer-} \texttt{DrawerClose}
are two manipulation tasks from~\citet{yu2020metaworld}.
For each of these tasks, we collect a dataset of medium quality by training an online agent from~\citet{eysenbach2022contrastive} and rolling out multiple checkpoints during the course of training. 
The resulting datasets have success rates between  $45\% - 50\%$.
We report results after $500,000$ training gradient steps (or $250,000$ steps, if the task success rates have converged by that point). \looseness=-1


We report results in Fig.~\ref{fig:main_results}.
We observe that LAEO, PURL, and ORIL perform similarly on  \texttt{FetchReach} and \texttt{FetchReach-image}. This is likely because these are relatively easy tasks, and each of these methods is able to achieve a high success rate. Note that all of these methods significantly outperform BC, indicating that they are able to learn better policies than the mode behavior policies represented in the datasets. 
On \texttt{SawyerDrawerClose}, all methods, including BC, achieve near perfect success rates, likely due to the simplicity of this task.
On \texttt{FetchPush}, \texttt{FetchPush-image}, and \texttt{SawyerWindowOpen}, LAEO outperforms all of the baselines by a significant margin. 
Recall that the main difference between LAEO and 
PURL/ORIL
is by learning a dynamics model, rather than the reward function.  These experiments suggest that 
for tasks with more complex dynamics, learning a dynamics model can achieve better performance than is achieved by model-free reward classifier methods. 











\begin{wrapfigure}[10]{R}{0.5\textwidth}
    \vspace{-0.9em}
    \centering
    % Figure removed
    \vspace{-1.8em}
    \caption{\footnotesize \textbf{Data quality.} LAEO continues to match or outperform reward classifier based methods on datasets that contain a low percentage of successful trajectories.}
    \label{fig:hard-results}
\end{wrapfigure}

\paragraph{Varying the input data.}




Our next experiment studies how the dataset composition affects LAEO and the baselines. 
On each of three tasks, we generate a low-quality dataset by rolling out multiple checkpoints from a partially trained agent from~\citet{eysenbach2022contrastive}.
In comparison to the medium-quality datasets collected earlier, which have success rates between $45\% - 50\%$, these low quality datasets have success rates between $8\%-12\%$.
We will denote these low quality datasets with the ``Hard'' suffix.
Fig.~\ref{fig:hard-results} shows that LAEO continues to outperform baselines on these lower-quality datasets.


% Figure environment removed


Our next experiments study how varying the number of high-return example states and the number of reward-free trajectories affects performance. As noted in the Sec.~\ref{sec:intro}, we conjecture that our method will be especially beneficial relative to reward-learning approaches in settings with very few high-return example states.
In Fig.~\ref{fig:data} \emph{(left)}, 
we vary the number of high-return example states on 
\texttt{FetchPush} \texttt{-image}, 
holding the number of unlabeled trajectories constant. We observe that LAEO maintains achieves the same performance with 1 success example as with 200 success examples. In contrast, ORIL's performance decreases as the number of high-return example states decreases.
In Fig.~\ref{fig:data} \emph{(right)}, we vary the number of unlabeled trajectories, holding the number of high-return example states constant at $200$. 
We test the performance of LAEO vs. ORIL on three different dataset sizes on \texttt{FetchPush-image}, roughly corresponding to three different orders of magnitude: the $0.1\times$ dataset contains $3,966$ trajectories, %
the $1\times$ dataset contains $31,271$ trajectories, %
and the $10\times$ dataset contains $300,578$ trajectories. %
We observe that LAEO continues to see performance gains as number of unlabeled trajectories increases, whereas ORIL's performance plateaus.
Taken together these results suggest that, in comparison to reward classifier based methods, LAEO needs less human supervision and is more effective at leveraging large quantities of unlabeled data. 




\paragraph{Partial Observability.}

\begin{wrapfigure}[13]{R}{0.5\textwidth}
    \centering
    % Figure removed%
    ~
    % Figure removed
    \vspace{-1.6em}
    \caption{\footnotesize \textbf{Partial observability.} LAEO continues to solve the \texttt{FetchPush-image} manipulation task in a setting where the new camera placement causes partial observability. This camera angle causes the block to be hidden from view by the gripper when the gripper reaches down to push the block. }
    \label{fig:partial}
\end{wrapfigure}



We also test the performance of LAEO on a partially-observed task. We modify the camera position in the \texttt{FetchPush-image} so that the block is occluded whenever the end effector is moved to touch the block. While such partial observability can stymie temporal difference methods~\citep{whitehead1991learning}, we predict that LAEO might continue to solve this task because it does not rely on temporal difference learning. 
The results, shown in Fig.~\ref{fig:partial}, confirm this prediction. 
On this partially observable task, we compare the performance of LAEO with that of ORIL, the best performing baseline on the fully observable tasks. On the partially observable task, LAEO achieves a 
success
rate of $51.9\%$, versus $33.9\%$ for ORIL.



\begin{wrapfigure}{R}{0.5\textwidth}
    \centering
    \vspace{-0.8em}
    % Figure removed%
    \vspace{-0.5em}
    \caption{\footnotesize 
    \textbf{Comparison with goal-conditioned RL.} 
    LAEO solves manipulation tasks at multiple different locations 
    without being provided with a goal-state at test time. 
    }
    \label{fig:examplebased}
\end{wrapfigure}


\paragraph{Comparison to Goal-Conditioned RL.}
\label{par:comparison_to_goal_conditioned}

One of the key advantages of example-based control, relative to goal-conditioned RL, is that the policy can identify common patterns in the success examples to solve tasks in scenarios where it has never before seen a success example.
In settings such as robotics, this can be an issue since acquiring a goal state to provide to the agent requires already solving the desired task in the first place.
We test this capability in a variant of the \texttt{SawyerDrawerClose} environment. For training, the drawer's X position is chosen as one of five fixed locations. Then, we evaluate the policy learned by LAEO on three types of environments: \emph{In Distribution}: the drawer's X position is one of the five locations from training; \emph{Interpolation}: The drawer's X position is between some of the locations seen during training; \emph{Extrapolation}: The drawer's X position is outside the range of X positions seen during training. We compare to a goal-conditioned policy learned via contrastive RL, where actions are extracted by averaging over the (training) success examples: $\pi(a \mid s) = \E_{s^* \sim p_*(s)}[\pi(a \mid s, g=s^*)]$.

The results, shown in Fig.~\ref{fig:examplebased}, show that LAEO consistently outperforms this goal-conditioned baseline. As expected, the performance is highest for the In Distribution environments and lowest for the Extrapolation environments. 
Taken together, these experiments show that LAEO can learn to reach multiple different goal locations without access to goal states during test time. \looseness=-1







\paragraph{Multitask Critic.}

\begin{wrapfigure}[18]{R}{0.5\textwidth}
    \centering
    % Figure removed
    \caption{\footnotesize \textbf{Multitask Critic}: 
    Cross entropy method (CEM) optimization over the LAEO dynamics model trained only on the data from the drawer close task is able to solve six different tasks. Randomly sampling actions from the action space results in a $0\%$ success rate across all of the six tasks (not shown for clarity).}
    \label{fig:multitask_results}
\end{wrapfigure}

We explore whether a LAEO dynamics network trained on data from one task can be used to solve other downstream tasks.
We create a simple multitask environment by defining several different tasks that can be solved in the \texttt{SawyerDrawerClose} environment: \texttt{Close}, \texttt{Half-closed}, \texttt{Open}, \texttt{Reach-near}, \texttt{Reach-medium}, and \texttt{Reach-far}.
We then use a trained critic network from the previous set of experiments (Comparison to Goal-Conditioned RL), condition it on a success example from a downstream task, and select actions by using cross entropy method (CEM) optimization. By using CEM optimization, we do not need to train a separate policy network for each of the tasks. See Appendix \ref{appendix:multitask} for implementation details and for details of the multitask drawer environment.

CEM over the LAEO critic achieves non-zero success rates on all six tasks, despite only being trained on data from the \texttt{Close} task (see Figure \ref{fig:multitask_results}). In contrast, randomly sampling actions from the action space achieves a $0\%$ success rate on all of the tasks. 
Results are averaged across eight random seeds.
This suggests that a single LAEO critic can be leveraged to solve multiple downstream tasks, as long as the dynamics required to solve those tasks are represented in the training data.
Note that since we condition the critic network on a single goal example, these experiments can be interpreted from a goal-conditioned perspective as well as an example-based control perspective. In future work, we aim to explore the multitask capabilities of the LAEO dynamics model in an example-based control setting at a larger scale. This will involve training on larger, more diverse datasets as well as conditioning the critic network on multiple success examples for a single task (as done in the Comparison to Goal-Conditioned RL experiments).






