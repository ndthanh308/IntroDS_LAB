\section{Related Work}
\label{sec:related_work}

\paragraph{Reward learning.} To overcome the challenge of hand-engineering reward functions for RL, prior methods either use supervised learning or adversarial training to learn a policy that matches the expert behavior given by the demonstration (imitation learning)~\citep{pomerleau1988alvinn,Dagger2011Ross,GAIL2016Ho,Feedback2021Spencer}
or learn a reward function from demonstrations and optimize the policy with the learned reward through trial and error (inverse RL)~\citep{ng2000irl, abbeel2004apprenticeship, ratliff2006, ziebart2008maximum, finn2016guided,AIRLFu2018}. 
However, providing full demonstrations complete with agent actions is often difficult, therefore, recent works have focused on the setting where only a set
of user-specified goal states or human videos are available~\citep{fu2018variational,singh2019end, kalashnikov2021mt, xie2018few, eysenbach2021replacing, chen2021learning}.
These reward learning approaches have shown successes in real-world robotic manipulation tasks from high-dimensional imageinputs~\citep{finn2016guided,singh2019end,zhu2020ingredients,chen2021learning}. Nevertheless, to combat covariate shift that could lead the policy to drift away from the expert distribution, these methods usually require significant online interaction. Unlike these works that study online settings, we consider learning visuomotor skills from offline datasets.

\paragraph{Offline RL.} Offline RL~\citep{ernst2005tree, riedmiller2005neural, LangeGR12, levine2020offline} studies the problem of learning a policy from a static dataset without online data collection in the environment, which has shown promising results in robotic manipulation~\citep{kalashnikov2018scalable, mandlekar2020iris, Rafailov2020LOMPO,singh2020cog,julian2020efficient,kalashnikov2021mt}. Prior offline RL methods focus on the challenge of distribution shift between the offline training data and deployment using a variety of techniques, such as policy constraints~ \citep{fujimoto2018off,liu2020provably,jaques2019way,wu2019behavior, zhou2020plas,kumar2019stabilizing,siegel2020keep, peng2019advantage, fujimoto2021minimalist,ghasemipour2021emaq}, conservative Q-functions~\citep{kumar2020conservative,kostrikov2021offline,yu2021combo, sinha2021s4rl}, and penalizing out-of-distribution states generated by learned dynamics models~\citep{kidambi2020morel, yu2020mopo,matsushima2020deployment,argenson2020model,swazinna2020overcoming,Rafailov2020LOMPO,lee2021representation,yu2021combo}. 

While these prior works successfully address the issue of distribution shift, they still require reward annotations for the offline data. Practical approaches have used manual reward sketching to train a reward model ~\citep{cabi2019framework, Konyushkova2020SSRewardLearning, Rafailov2020LOMPO} or heuristic reward functions ~\citep{yu2022UDS}. Others have considered offline learning from demonstrations, without access to a predefined reward function ~\citep{mandlekar2020iris,zolna2020offline, xu2022Discriminator, Jarboui2021OfflineIRL}, however they rely on high-quality demonstration data. 
In contrast, our method: \emph{(1)} addresses distributional shift induced by both the learned policy and the reward function in a principled way, \emph{(2)} only requires user-provided goal states and \emph{(3)} does not require expert-quality data, resulting in an effective and practical offline reward learning scheme.