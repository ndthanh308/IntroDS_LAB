%******************************************************************************%
%*************************    RESULTS & DISCUSSION   **************************%
%******************************************************************************%
\section{Results and Discussion}
\subsection{Simulated Environment}
When using simulated environments, multiple agents can interact in parallel with their own environment, evolving them independently. In addition, no time is needed between iterations for appreciating the thermal situation of the system and no dissipation time is needed for starting again a new episode. As a consequence, the training time is reduced significantly.

As no real sensors are used in this environments, an initial temperature has to be set to fix a point to start with the simulation. In order to train the agents on different scenarios, the initial temperature was randomized on each scenario instance. This randomization adds some difficulties to the training but also adds variability to the samples and improves the generalization capabilities of the agents.

After training for 12 hours in 8 parallel environments, a mean episode length of 31500 iterations was achieved when starting the simulation 2($\pm$1) degrees Celsius below the temperature limit (Mean episode length is shown in Fig. \ref{fig:simulated training}).

% Figure environment removed

\subsection{Real Hardware}

When the limit temperature is reached, the next training episode cannot be restarted instantly as in the simulated environment. As a result, a cooling period emerges between two neighboring episodes. This is the time required to cool down the device until operational temperature ranges are attained.
In the experimentation, the limit temperature was fixed in 55 degrees Celsius as this threshold was near the operational limit of some subsystems. The cooling period to reach the idle temperature of the payload, which is 50 degrees Celsius on ground laboratory conditions (no vacuum, heat dissipation by airflow and variable room temperature), is at most 2 minutes. 
Different measure/update frequencies have been tried during the experimentation. Finally an update frequency of 5 seconds was found to be acceptable for the temperature evolution speed.

The Fig. \ref{fig:beginning_training} shows the initial episode cycles in temperature and power consumption graphics of the board. In this episodes, the maximum temperature threshold was reached after less than 5 iterations (25 seconds).

% Figure environment removed

After 5hours of training the 73rd episode duration was longer than 12hs, as shown in Fig. \ref{fig:5h_training}, which illustrates the improvement in the agent skills to maintain the temperature below the limit temperature.

% Figure environment removed


The length of the episodes during training can be seen on Fig.\ref{fig:hw_training}. As in the simulated environment, policy performance is low until enough samples are gathered; after this point is achieved, performance improves noticeably.

% Figure environment removed

Both the policy learning and the prediction with it was done without hardware acceleration (CPU only). During the policy learning, the amount of RAM used was stable in 290MB and spikes of 200 millicores (0.2 cores) were required every 5 seconds (prediction frequency).




