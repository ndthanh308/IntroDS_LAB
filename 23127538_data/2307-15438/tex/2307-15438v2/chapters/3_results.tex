%******************************************************************************%
%*************************    RESULTS & DISCUSSION   **************************%
%******************************************************************************%
\section{Results and Discussion}
\subsection{Simulated Environment}
When using simulated environments, multiple agents can interact in parallel with their own environment, evolving them independently. In addition, no time is needed between iterations for appreciating the thermal situation of the system and no dissipation time is needed for starting a new episode again. Consequently, the training time is reduced significantly.

As no real sensors are used in this environment, an initial temperature must be set to fix a point to start with the simulation. To train the agents on different scenarios, the initial temperature was randomized on each scenario instance. This randomization adds some difficulties to the training but also adds variability to the samples and improves the generalization capabilities of the agents.

After training for 12 hours in 8 parallel environments, a mean episode length of 31500 iterations was achieved when starting the simulation 2($\pm$1) degrees Celsius below the temperature limit (Mean episode length is shown in Fig. \ref{fig:simulated training}).

% Figure environment removed

\subsection{Real Hardware}
When experimenting with the real hardware some physical aspects must be considered. 

One of these physical aspects is thermal inertia. When the limit temperature is reached, the next training episode cannot be restarted instantly as in the simulated environment. As a result, a cooling period emerges between two neighboring episodes. This is the time required to cool down the device until operational temperature ranges are attained. The cooling period to reach the idle temperature of the payload, which is 50 degrees Celsius on ground laboratory conditions (no vacuum, heat dissipation by airflow and variable room temperature), is at most 2 minutes.

On the other hand, thermal inertia also affects the rate at which temperature rises. Different measure/update frequencies have been tried during the experimentation. Finally, an update frequency of 5 seconds was found to be acceptable for the temperature evolution speed.

In the experimentation, the limit temperature was fixed at 55 degrees Celsius as this threshold was near the operational limit of some subsystems. 

Initially the policies were learned from scratch, but the performance of the agents was poor, showing a slow improvement throughout the episodes.
We experimented with using learnt policies from synthetic settings as beginning policies for training in real hardware since there are some similarities between the simulated and real-hardware environments, even though they are distinct from one another.

Fig. \ref{fig:transfer_learning} shows the comparison between training from scratch and training using pre-trained policies from the simulated environment. 
This semi-logarithmic plot shows that the usage of a pre-trained policy speeds up the training, which allows obtaining a policy in a few episodes that manages to obtain a system operating below the temperature threshold. Due to this, the rest of the experimentation in the real hardware was done using pre-trained policies on the simulated environment.

% Figure environment removed

Fig. \ref{fig:beginning_training} shows a sample of the initial episode cycles in temperature and power consumption graphics of the board. In this episode, the maximum temperature threshold was reached after less than 5 iterations (25 seconds).

% Figure environment removed
In the same training sample, after the 33rd episode of the training, the agent is able to maintain the temperature below the limit temperature for more than 12hs, as shown in Fig. \ref{fig:5h_training}, illustrating the improvement in the agent skills.

% Figure environment removed


The full evolution of the episodes length during a single training can be seen on Fig.\ref{fig:hw_training}. As in the simulated environment, policy performance is low until enough samples are gathered; after this point is achieved, performance improves noticeably. 
During this training, the maximum episode length was limited to 5000 iterations ($\approx$ 7h). This limit was reached on the 34th episode of the training.

% Figure environment removed

The agent presents a high oscillation in the active CPUs to achieve temperature control. Fig.\ref{fig:activity} shows a sample window of 36 iterations (2 minutes) length with the number of powered-on CPUs on each iteration. Although the learned policy is compatible with the main objective of maintaining the temperature below a threshold while maximizing the processing resources, this behavior of high oscillation is not desired as it produces an unnecessary overhead when migrating the CPUs content (like processes and interrupts).

% Figure environment removed

Both the policy learning and the prediction with it was done without hardware acceleration (CPU only). During the policy learning, the amount of RAM used was stable in 290MB and spikes of 200 millicores (0.2 cores) were required every 5 seconds (prediction frequency).