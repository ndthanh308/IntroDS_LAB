%******************************************************************************%
%*******************************    METHODS   *********************************%
%******************************************************************************%
\section{Materials and Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Definition}
The system to be controlled consists of a processing node that contains a System on Chip (SoC) based on ARM architecture with 16 cores that can be managed independently. The system temperature is monitored using nine thermistors, seven of which are scattered throughout the SoC and the other two are located on the top and bottom edges of the main board.

Given a maximum temperature limit, the agent learns to manage the hardware resources available for processing in order to maximize performance while maintaining the overall system temperature below the upper limit.

The agent interacts with the environment using a standard Application Programming Interface (API) created with the Gymnasium \cite{gymnasium} library. 


A containerized version of the \emph{stress-ng} tool was used for generating a sustained load and controlled from the Gymnasium scenario setup.

While interacting with the environment, the agent explores sub-optimal policies that, in certain situations, may allow the temperature to scale to values that are not suitable for some components. In this experimentation the load was controlled and was removed in such cases but in a real mission, a rigid thermal control should be used in conjunction (e.g. powering-off the processing node, i.e. the SoC, when its temperature is out of range).

The temperature control system proposed in this work is schematized in Fig. \ref{fig:general}.

% Figure environment removed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Reinforcement Learning Modeling}
\subsubsection{Actions}
The agent is able to control the processing power by modifying both the power status and clock speed for each available Central Processing Unit (CPU).

The power status of the CPUs can be managed using the CPU hotplug feature which allows to switch cores on or off dynamically using the sysfs interface.

The CPU clock speed can be managed via the CPUFreq subsystem which allows the operating system to scale the CPU frequency up or down to save power or improve performance.

The actions to be carried out by the agent are intended to establish the states listed in Table \ref{table:cpu_status} on each CPU.

\begin{table}[h]\renewcommand{\arraystretch}{1}
\begin{center}
\begin{tabular}{|c|c|}
\hline\hline
Power Status & Clock Speed\\
\hline\hline
On & Low (1MHz)\\
On & High (up to 2MHz)\\
Off & ------\\
\hline\hline
\end{tabular}
\caption{Available options for setting on each CPU in terms of power and clock speed}
\label{table:cpu_status}
\end{center}
\end{table}

Although CPUs can be uniquely identified by their number and their state modified independently, in this study the agent controls the state of the CPUs without distinction between them. This model reduces the action space and therefore increases the learning performance as suggested by \cite{booth2019ppo}.


Accordingly, the agent is allowed to choose the following quantities from a reduced and continuous action space:
\begin{itemize}
\item \textbf{Powered-on CPUs}: Percentage of the total amount of CPUs present in the board that are powered-on.
\item \textbf{High frequency CPUs}: Percentage of the powered-on CPUs that are in high clock speed
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{State}
The agent receives a normalized continuous state that includes the following quantities:
\begin{itemize}
\item \textbf{CPU state ($P$)}: This parameter is calculated in equation \ref{eq:parameter} and reflects the current processing power considering the power status (on/off) and frequency level assigned (low/high) on each CPU.

\begin{equation} \label{eq:parameter}
    P=\frac{\sum_{cpu=1}^{n} P_{cpu}}{2*n}
\end{equation}

\[
P_{cpu}= \left\{ \begin{array}{lcl}
          0 &  if  &  \textrm{cpu=off} \\
          1 &  if  &  \textrm{cpu=on and frequency=low} \\
          2 &  if  &  \textrm{cpu=on and frequency=high} 
          \end{array}
\right.
\]

In the equation, \emph{n} represents the number of available CPUs (16 in the SoC used).

\item \textbf{Margin to limit ($\Delta T=T_{limit} - T$)}: Represents the available margin before reaching the temperature limit ($T_{limit}$) considering the current temperature of the system ($T$) (Represented in Fig. \ref{fig:state}).
\item \textbf{Slope ($m$)}: Expresses the rate of change of temperature. Instead of the derivative of an unknown temperature function, this quantity is replaced with the slope of a secant line that is calculated using real temperature measures in a brief period (Represented in Fig. \ref{fig:state}).
\end{itemize}

% Figure environment removed



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Rewards}
The goal is to operate as many CPUs at the highest frequency possible while maintaining the required temperature range.
Positive rewards are accumulated by the agent while the temperature is below the temperature threshold, and a negative reward is given when this threshold is achieved to allow the agent to learn while interacting with the environment.

On each step, if the threshold temperature has not been reached, the reward is calculated with Equation \ref{eq:reward}.

\begin{equation} \label{eq:reward}
    R=\frac{cpus_{high}*C_{high} + cpus_{low} * C_{low} + cpus_{off}* C_{off}}{n}
\end{equation}

As in previous equations, \emph{n} represents the number of available CPUs. $C_{high}$, $C_{low}$ and $C_{off}$ are constants used to weigh independently each term and to express how each state (power status and clock speed of the CPUs) conditions the reward.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Algorithm}
Many interactions with the environment are necessary for the DRL algorithms to learn to behave correctly. Due to this low efficiency in the use of experience, it is still challenging to use reinforcement learning in real life problems.
Off-policy algorithms collect samples in a replay buffer that is used on every policy update making them ideal in cases where gaining experience has a high cost because they can efficiently learn from past events and decisions.

In this study, a model-free, off-policy algorithm called Soft Actor-Critic \cite{haarnoja2018soft} (SAC) was used to train the agent. SAC is an actor-critic algorithm, and as such follows a policy-based and value-based approach, that concurrently learns a policy and two Q-functions. 
The Stable Baselines3 (SB3) \cite{stable-baselines3} implementation of SAC was used in the experimentation. SB3 is a set of implementations of reinforcement learning algorithms in Pytorch that uses the Gymnasium library as an abstraction for the environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Pre-training: Temperature Modeling} \label{modelling}
To mitigate the impact of initial DRL algorithm iterations on the real environment, a simulated environment has been employed. This simulation aims to replicate the thermal behavior of the board. While the model only captures certain aspects of the actual environment, it enables the agent to explore the connection between available temperature margin, temperature curve slope, and allocated processing power in a harmless context.

The agent can learn and subsequently adapt its behavior in relation to the real scenario. This is sometimes referred to as domain adaptation.

A family of temperature functions over time conditioned not only by time but also by a variable determined by the state of the CPU was proposed as a naive modeling of the temperature curve followed by the system.

Equation \ref{eq:temperature_estimator} represents the family of equations used in temperature evolution estimation.

\begin{equation} 
    Y(x)=T_{init} + (2 + \frac{P}{8}) \ln(2x)
\label{eq:temperature_estimator}
\end{equation}

In equation \ref{eq:temperature_estimator} the parameter \emph{$P$}, also included in the state data equation (\ref{eq:parameter}),  determines a concrete equation in relation to the CPU state.