%******************************************************************************%
%*************************    RESULTS & DISCUSSION   **************************%
%******************************************************************************%
\section{Results and Discussion}

\subsection{Experimental results}
In the experiment, the limit temperature was established at 55 degrees Celsius since this threshold was close to the operating limit of various subsystems.

The policies pre-trained on a software simulated environment were compared with others learned from scratch on the hardware environment. After initial episodes of domain adaptation, the pre-trained policies were able to maintain the system operating below the temperature threshold for more than 3 hours while the policies trained from scratch on the hardware environment need multiple days of training for obtaining equivalent results. Due to this, the rest of the experimentation was done using pre-trained policies.

Fig. \ref{fig:beginning_training} shows a sample of the initial episode cycles in temperature graphic of the board. In this episode, the maximum temperature threshold was reached in less than 5 iterations (25 seconds).

After training for 30 episodes, the agent was able to maintain the temperature below the limit temperature for more than 12h, as shown in Fig. \ref{fig:5h_training}, illustrating the improvement in the agent skills.



% Figure environment removed


The full evolution of the episodes length during a single training can be seen on Fig.\ref{fig:hw_training}. The policy performance was low until enough samples were gathered; after that, the performance improved noticeably. 

During experimentation, the maximum episode length was limited to 5000 iterations ($\approx$ 7h). In all cases, this limit was reached after 30 episodes. In the training shown in Fig.\ref{fig:hw_training}, the threshold was reached on the 34th episode of the training.

% Figure environment removed

The agent presents a high oscillation in the active CPUs to achieve temperature control. Although the learned policy is compatible with the main objective of maintaining the temperature below a threshold while maximizing the processing resources, this high oscillation is not desired as it produces unnecessary overhead when migrating the CPUs content (like processes and interrupts). A solution could be to penalize the CPU state changes in the reward calculation, which should force the agent to try to reduce this variability in the number of available CPUs if it is not needed.

Both the policy learning and the prediction with it was done without hardware acceleration (CPU only). During the policy learning, the amount of RAM used was stable in 290MB and spikes of 200 millicores (0.2 cores) were required every 5 seconds (prediction frequency).

\subsection{Limitations}
While with the current state definition the model is able to converge, initially, equivalent models with larger input dimensions were designed, and the model was unable to converge within a reasonable number of iterations. Although the usage of reinforcement learning for on-board optimization of satellite resources is promising, it could pose problems if the observation or action spaces are too large.

Starting from a model pre-trained in a synthetic environment with thermal simulation, 30 episodes were required to perform domain adaptation, during which the behavior of the agent was suboptimal. It is likely that when repeating tests in a real environment (i.e. outer space) a new domain adaptation will be necessary, and the agentâ€™s behavior may be deficient one more time. Therefore, it is essential to accompany the system with a mechanism capable of protecting sensitive equipment, such as optical instruments, from inadequate policies during this adaptation period. For instance, the mechanism could involve shutting down the instrument or cooling the system when the temperature exceeds the acceptable limits.

