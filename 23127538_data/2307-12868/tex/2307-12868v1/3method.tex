\section{Discovering the latent basis of DMs}
\label{sec:sec3}
In this section, we explain how to extract a latent structure of \exspace{} using differential geometry. 
First, we introduce a key concept in our method: the {\it pullback metric}. 
Next, by adopting the local Euclidean metric of \ehspace{} and utilizing the pullback metric, we discover the local latent basis of the \exspace{}. 
Moreover, although the direction we found is {\it local}, we show how it can be applied to other samples via parallel transport. 
Finally, we introduce $\vx$-space guidance for editing data in the \exspace{} {to enhance the quality of image generation.}

% Moreover, We also construct global latent basis using the {\it \frechet{} mean} of local semantic bases. 
% First, we use the {\it pullback metric} to characterize the geometrical structure of \exspace{}, adopting the local Euclidean metric of \ehspace{} to identify local semantic latent subspaces. 
% This is a way to give a space without a metric a spatial characterization using a space with a known metric. 
% Next, we adopt the local Euclidean metric of \ehspace{} to identify local semantic latent subspace for individual samples in \exspace{}. 
% Note that the \exspace{} is semantically sparse, but the subspace we found is semantically dense. 
% In Section 4, we show that the discovered direction within the semantic latent subspace can be exploited for editing. 
% Moreover, We also construct global latent basis using the {\it \frechet{} mean} of local semantic bases. 
% Finally, we introduce $\mathbf{x}$-space guidance for editing with better quality.
% Third, we found the homogenity across local semantic subspace discovered at various samples. From this observation, we construct global semantic directions by deriving the \frechet{} mean of the local semantic directions of individual samples. 
% We use the global semantic directions to manipulate any sample to have the same interpretable features.
% Finally, we introduce a technique called $x$-space guidance to edit sample with better quality.

%%% ICML ver
% This section explains how we extract the interpretable directions in the latent space of DMs using differential geometry. 
% First, we adopt the local Euclidean metric of \ehspace{} to identify semantic directions for individual samples in \exspace{}.
% Second, we find global semantic directions by averaging the local semantic directions of individual samples. Then, we use the global directions to manipulate any sample to have the same interpretable features.
% Finally, we introduce a normalization technique to prevent distortion.

% % Figure environment removed

% % Figure environment removed

\subsection{Pullback metric} \label{sec:pullback}
% In this subsection, we explain how to extract a local latent basis of \exspace{}, and their corresponding local tangent basis of \ehspace{} using differential geometry. 
% First, we introduce a key concept in our method: the pullback metric. 
% The pullback metric is a mathematical tool to define a metric on a space by exploiting the metric structure of another metric space.
% Next, we adopt the local Euclidean metric of \ehspace{} to identify local semantic latent subspace for individual samples in \exspace{}. 

We consider a curved manifold, \exspace{}, where our latent variables $\mathbf{x}_t$ exist. 
The differential geometry represents $\mathcal{X}$ through patches of tangent spaces, $\tanxspace{}$, which are vector spaces defined at each point $\mathbf{x}$. 
Then, all the geometrical properties of $\mathcal{X}$ can be obtained from the inner product of $||d\mathbf{x}||^2 = \langle d{\mathbf{x}},d{\mathbf{x}} \rangle_\mathbf{x}$ in $\tanxspace{}$.
However, we do not have any knowledge of $\langle d{\mathbf{x}},d{\mathbf{x}} \rangle_\mathbf{x}$.
It is definitely not a Euclidean metric. Furthermore, samples of $\mathbf{x}_t$ at intermediate timesteps of DMs include inevitable noise, which prevents finding semantic directions in $\tanxspace{}$.

Fortunately, \citet{kwon2022diffusion} observed that \ehspace{}, defined by the bottleneck layer of the U-Net, exhibits local linear structure.
% Fortunately, \citet{kwon2022diffusion} observed that \ehspace{}, defined by the bottleneck layer of the U-Net, exhibits \yh{semantically} local linearity.
% Henceforth, we denote \ehspace{} as $\mathcal{H}$.
This allows us to adopt the Euclidean metric on $\mathcal{H}$.
In differential geometry, when a metric is not available on a space, {\it pullback metric} is used.
If a smooth map exists between the original metric-unavailable domain and a metric-available codomain, the pullback metric is used to measure the distances in the domain space.
Our idea is to use the pullback Euclidean metric on $\mathcal{H}$ to define the distances between the samples in $\mathcal{X}$.

DMs are trained to infer the noise $\mathbf{\epsilon}_t$ from a latent variable $\mathbf{x}_t$ at each diffusion timestep $t$. 
Each $\mathbf{x}_t$ has a different internal representation $\mathbf{h}_t$, the bottleneck representation of the U-Net, at different $t$'s.
The differentiable map between $\mathcal{X}$ and $\mathcal{H}$ is denoted as $f : \mathcal{X} \rightarrow \mathcal{H}$.
Hereafter, we refer to $\mathbf{x}_t$ as $\mathbf{x}$ for brevity unless it causes confusion. It is important to note that our method can be applied at any timestep in the denoising process.
%$f$ gives a linear map $\nabla_{\mathbf{x}} \mathbf{h} : \tanxspace{} \rightarrow \tanhspace{}$ between the domain and codomain tangent spaces.
{We consider a linear map, $\tanxspace{} \rightarrow \tanhspace{}$, between the domain and codomain tangent spaces.}
%The differential geometry then defines a linear map between the tangent space $\tanxspace{}$ at $\mathbf{x}$ and corresponding tangent space $\tanhspace{}$ at $\mathbf{h}$. The 
This linear map can be described by the {\it Jacobian} $J_{\mathbf{x}} = \nabla_{\mathbf{x}} \mathbf{h}$
which determines how a vector $\mathbf{v} \in \tanxspace{}$ is mapped into a vector $\mathbf{u} \in \tanhspace{}$ by
$\mathbf{u} = J_{\mathbf{x}} \mathbf{v}$. 
% In practice, the Jacobian can be computed from automatic differentiation of the U-Net.
% However, since the Jacobian of too many parameters is not tractable, we use a sum-pooled feature map of the bottleneck representation as our $\mathcal{H}$. 

Using the local linearity of $\mathcal{H}$, we assume the metric, $||d\mathbf{h}||^2 = \langle d\mathbf{h}, d\mathbf{h} \rangle_\mathbf{h} = d\mathbf{h}^{\tran} d\mathbf{h}$ as a usual dot product defined in the Euclidean space.
To assign a geometric structure to $\mathcal{X}$, we use the pullback metric of the corresponding $\mathcal{H}$.
In other words, the norm of $\mathbf{v} \in \tanxspace{}$ is measured by the norm of corresponding codomain tangent vector:
\begin{equation} \label{eq:pullback}
\begin{aligned}
||\dx{}||^2_\text{pb} \triangleq  \langle \dh{}, \dh{} \rangle_{\mathbf{h}} = \dx{}^{\tran} \jacx{}^{\tran} \jacx{}\dx{}.
\end{aligned}
\end{equation}

% \subsection{\yh{Find the local semantic latent directions}}
% \subsection{\yh{Extracting the semantic directions and editing}}
% \subsection{Extraction of semant directions}

% \yh{This subsection describes how we extract semantic latent directions using the pullback metric, and how we construct semantic latent subspace using discovered directions. The overall process is illustrated in \fref{fig:method_figure}.}

%%% ICML ver
% \label{sec:method_local}
% This subsection describes how we extract semantic latent directions using the pullback metric, and how we edit samples for multiple times given the meaningful directions by geodesic shooting. The overall process is illustrated in \fref{fig:method_figure}.

\subsection{Finding local latent basis}
Using the pullback metric, 
{
we define the local latent vector $\mathbf{v} \in \tanxspace{}$ {that shows a large variability in $\tanhspace{}$}.
We find a unit vector $\mathbf{v}_1$ that maximizes $||\dx{}||^2_\text{pb}$.
}
% It can be interpreted as the first eigenvector of $\jacx{}^{\tran} \jacx{} = V \Lambda^2 V^{\top}$.
By maximizing $||\dx{}||^2_\text{pb}$ while remaining orthogonal to $\mathbf{v}_1$, one can obtain the second unit vector $\mathbf{v}_2$. This process can be repeated to have $n$ latent directions of $\{\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_n \}$ in $\mathcal{T}_{\mathbf{x}}$. 
In practice, $\mathbf{v}_i$ corresponds to the $i$-th right singular vector from the singular value decomposition {(SVD)} of $\jacx{} = U \Lambda V^{\tran}$, i.e., $J_{\vx} \vv_{i} = \Lambda_{i} \vu_{i}$. 
Since the Jacobian of too many parameters is not tractable, we use a \textit{power method} \cite{golub2013matrix,miyato2018spectral,haas2023discovering} to approximate the SVD of $\jacx{}$
{(See \aref{appendixsec:algorithm} for the detailed algorithm).}

{Henceforth, we refer to $\mathcal{T}_{\mathbf{x}}$ as a local latent subspace, and $\{\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_n \}$ as the corresponding local latent basis.}
\begin{align}
    \mathcal{T}_{\mathbf{x}} \triangleq \text{span} \{\mathbf{v}_1, \mathbf{v}_2\, \cdots, \mathbf{v}_n\},~\text{where}~\mathbf{v}_i~\text{is}~i\text{-th right singular vector of}~J_{\mathbf{x}}.
\end{align}

%%% ICML ver : construct Th in h space 
Using the linear transformation between $\tanxspace{}$ and $\tanhspace{}$ via the Jacobian $\jacx{}$, one can also obtain corresponding directions in $\mathcal{T}_{\mathbf{h}}$.
% \begin{align}
%     \vu{}_i = \frac{1}{\lambda_i} \jacx{} \mathbf{v}_i.
% \end{align}
% Here, we normalize $\vu{}_i$ by dividing the $i$-th singular value $\lambda_i$ of \jo{the diagonal matrix} $\Lambda$ to preserve the Euclidean norm $||\mathbf{u}_i||=1$. 
In practice, $\mathbf{u}_i$ corresponds to the $i$-th left singular vector from the {SVD} of $\jacx{}$.
After selecting the top $n$ (e.g., $n = 50$) directions of large eigenvalues, we can approximate any vector in $\mathcal{T}_{\mathbf{h}}$ with {a} finite basis, $\{\mathbf{u}_1, \mathbf{u}_2, \cdots, \mathbf{u}_n \}$.
When we refer to a local tangent space henceforth, it means the $n$-dimensional low-rank approximation of the original tangent space.
\begin{align} \label{eq:local_tangent_space}
    \mathcal{T}_{\vh} \triangleq \text{span} \{\vu_1, \vu_2\, \cdots, \vu_n\},~\text{where}~\vu_i~\text{is the}~i\text{-th left singular vector of }~J_{\vx}.
\end{align}

%% mingi ver
% \paragraph{\yh{Interpretation of local latent basis}}
%The set of local latent basis vectors, $\{\vv_1, \vv_2, \cdots, \vv_n\}$, obtained through our proposed method, can be seen as a `{\it signal}' that the model is particularly sensitive given $\vx$.
%% These vectors represent the directions in which the model's representation $\mathbf{h}$ undergoes the most significant changes, revealing the information that the model focuses on from $\mathbf{x}_t$ for a specific sample.
%On the other hand, the basis of local tangent space, represented as $\{\vu_1, \vu_2\, \cdots, \vu_n\}$, can be thought of as the corresponding `{\it representation}' linked to the signal.
{The collection of local latent basis vectors, $\{\vv_1, \vv_2, \cdots, \vv_n\}$, obtained through our proposed method, can be interpreted as a {\it signal} that the model is highly response to for a given $\vx$.
On the other hand, the basis of the local tangent space, denoted as $\{\vu_1, \vu_2\, \cdots, \vu_n\}$, can be viewed as the corresponding {\it representation} associated with the signal.
}

{In Stable Diffusion, the prompt also influences the Jacobian, which means that the local basis also depends on it. }
% We can utilize any prompt to obtain a local latent basis.
{
We can utilize any prompt to obtain a local latent basis, and different prompts create distinct geometrical structures.
}
% \mingi{In a conditional model, the local tangent space undergoes prompt-dependent modifications, allowing us to derive a specific basis by using the prompt as a condition of the model. Note that we can utilize any prompt to obtain a local latent basis.}
{For the sake of brevity, we will omit the word {\it local} unless it leads to confusion.}

% %%% YH ver start
% \yh{
% % The local latent basis $\{\vv_1, \vv_2\, \cdots, \vv_n\}$ discovered in our method can be considered to the signal to which the model is sensitive, given $\mathbf{x}_t$. 
% The set of local latent basis vectors $\{\vv_1, \vv_2, \cdots, \vv_n\}$ obtained through our approach can be  understood as a {\it signal} that the model is responsive to, given $\vx_t$. 
% % This interpretation underscores the relevance of these vectors in shaping the model's response to the input.
% % The local latent basis vectors $\{\vv_1, \vv_2, \cdots, \vv_n\}$ discovered by our method represent the signal to which the model responds when given input $\mathbf{x}_t$. This highlights the crucial role of these vectors in shaping the model's output.
% This subspace consists of the directions that result in the most significant change in the model's representation $\mathbf{h}$, revealing the information that the model is most attentive to from $\mathbf{x}_t$ for a given sample. 
% In this regard, the basis of local tangent space $\{\vu_1, \vu_2\, \cdots, \vu_n\}$ can be understood as the {\it representation} corresponding to the signal.
% }
% %%% YH ver end

%%% ICML ver
% So far, we extracted meaningful directions for editing $\mathbf{x}_t$ and construct local semantic subspace.
% However, these directions are {\it local}, and thus are only applicable to individual samples of $\mathbf{x}_t$.
% Thus, we need to obtain {\it global} semantic directions that have the same semantic meaning for every sample.
% Fortunately, we observed a large overlap between the latent directions of individual samples.
% This observation motivates us to hypothesize that \ehspace{} has global semantic directions.

% \paragraph{Iterative editing with geodesic shooting} 
% Now, we edit a sample with the $i$-th semantic direction through $\mathbf{x} \to \mathbf{x}' = \mathbf{x} + \gamma \mathbf{v}_i$, where $\gamma$ is a hyper-parameter 
% that controls the size of the editing.
% If we want to increase the editing strength, we need to repeat the same operation. However, this would not work because $\mathbf{v}_i$ may escape from the tangent space $\mathcal{T}_{\mathbf{x}'}$.
% Thus, it is necessary to relocate the extracted direction to a new tangent space.
% To achieve this, we use {\it parallel transport} that projects $\mathbf{v}_i$ onto the new tangent space $\mathcal{T}_{\mathbf{x}'}$. 
% Parallel transport moves a vector without changing its direction as much as possible, while keeping the vector tangent on the manifold~\cite{shao2018riemannian}.
% It is notable that the projection significantly modifies the original vector $\mathbf{v}_i$, because $\mathcal{X}$ is a curved manifold.
% However, $\mathcal{H}$ is relatively flat. Therefore, it is beneficial to apply the parallel transport in $\mathcal{H}$.

% To project $\mathbf{v}_i$ onto the new tangent space $\mathcal{T}_{\mathbf{x}'}$, we use parallel transport in $\mathcal{H}$. First, we convert the semantic direction $\mathbf{v}_i$ in $\mathcal{T}_{\mathbf{x}}$ to the corresponding direction of $\mathbf{u}_i$ in $\mathcal{T}_{\mathbf{h}}$. Second, we apply the parallel transport $\mathbf{u}_i \in \mathcal{T}_{\mathbf{h}}$ to ${\mathbf{u}'}_{i} \in \mathcal{T}_{\mathbf{h}'}$, where $\mathbf{h}' = f(\mathbf{x}')$. 
% The parallel transport has two steps. The first step is to project $\mathbf{u}_i$ onto a new tangent space. This step keeps the vector tangent to the manifold. The second step is to normalize the length of the projected vector. This step preserves the size of the vector.
% Third, we obtain $\mathbf{v}'_{i}$ by transforming $\mathbf{u}'_{i}$ into $\mathcal{X}$.
% Using this parallel transport of $\mathbf{v}_i \to \mathbf{v}'_i$ via $\mathcal{H}$, we can realize the multiple feature editing of $\mathbf{x} \to \mathbf{x}' = \mathbf{x} + \gamma\mathbf{v}_i \to \mathbf{x}'' = \mathbf{x}' + \gamma \mathbf{v}'_i $.
% Based on the definition of Jacobian, this editing process can be viewed as a movement in \ehspace{} with the corresponding direction, i.e., $\mathbf{h} \to \mathbf{h}' = \mathbf{h} + \delta \mathbf{u}_i \to \mathbf{h}'' = \mathbf{h}' + \delta \mathbf{u}'_i$.  `
% This iterative editing procedure is called {\it geodesic shooting}, since it naturally forms a geodesic ~\cite{shao2018riemannian}.
% \fref{fig:method_figure} summarizes the above procedure. 
% See \aref{appendixsec:algorithm} for details.

% % Figure environment removed

\subsection{Editing various samples with parallel transport}

So far, we extracted a latent basis of $\tanxspace{}$ and a tangent basis of $\tanhspace{}$ given {a} latent variable $\vx$. 
% Thus, 우리가 구한 direction 을 다른 sample 에 적용하기 위해선 it is necessary to relocate the extracted direction to a new tangent space.
However, the basis vector obtained cannot be used for the other sample $\vx'$ because $\vv \notin \mathcal{T}_{\vx'}$. 
Thus, in order to apply the direction we obtained to another sample, it is necessary to relocate the extracted direction to a new tangent space.
To achieve this, we use parallel transport that moves $\vv_i$ onto the new tangent space $\mathcal{T}_{\vx'}$.

Parallel transport moves {a tangent vector $\vv \in \tanhspace{}$ to $\vv' \in \mathcal{T}_{\vh'}$} without changing its direction as much as possible while keeping the vector tangent on the manifold \cite{shao2018riemannian}.
{It is notable that the parallel transport in curved manifold significantly modifies the original vector.
% It is notable that the projection significantly modifies the original vector $\mathbf{v}_i \in \tanxspace{}$, because $\mathcal{X}$ is a curved manifold.
Fortunately,} $\mathcal{H}$ is relatively flat. Therefore, it is beneficial to apply the parallel transport in $\mathcal{H}$.

% \yh{
% we aims to move $\vv_i$ onto new tangent space $\mathcal{T}_{\mathbf{x}'}$, using parallel transport in $\mathcal{H}$.
% First, we convert the semantic direction $\vv_i$ in $\mathcal{T}_{\mathbf{x}}$ to the corresponding direction of $\vu_i$ in $\mathcal{T}_{\mathbf{h}}$.
% Second, we apply the parallel transport $\vu_i \in \mathcal{T}_{\mathbf{h}}$ to ${\mathbf{u}'}_{i} \in \mathcal{T}_{\mathbf{h}'}$, where $\mathbf{h}' = f(\mathbf{x}')$. 
% % long version
% In order to perform parallel transport from $\vh$ to $\vh'$, we need a geodesic path between them, represented as $\{ \vh, \vh^{(1)}, \vh^{(2)}, \cdots, \vh' \}$. 
% Parallel transport has two steps. 
% First, project $\vu_i$ onto a new tangent space, $\mathcal{T}_{\mathbf{h^{(1)}}}$, in order to ensure that the vector remains tangent to the manifold. 
% Second, normalize the length of the projected vector to preserve its size. 
% By repeating these steps along the geodesic path, we could move $\vu_i$ on the new tangent space $\mathcal{T}_{\vh'}$
% However, this iterative process involves tedius Jacobian computation on every step.
% Therefore, in this paper, we assume that \ehspace{} is a flat manifold and approximate the geodesic path as a straight line, eliminating the need for an iterative process. 
% Although this assumption is strong, our results in \sref{sec:local} demonstrate that it is valid. 
% Finally, transform $\mathbf{u}'_{i}$ into $\mathbf{v}'_i \in \mathcal{X}$.
% % Third, we obtain $\mathbf{v}'_{i}$ by transforming $\mathbf{u}'_{i}$ into $\mathcal{X}$.
% Using this parallel transport of $\mathbf{v}_i \to \mathbf{v}'_i$ via $\mathcal{H}$, 
% we can apply the local latent basis obtained from $\vx$ to edit or modify the input $\vx'$. 
% }

We aims to move $\vv_i$ onto new tangent space $\mathcal{T}_{\mathbf{x}'}$, using parallel transport in $\mathcal{H}$.
First, we convert the latent direction $\vv_i \in \mathcal{T}_{\mathbf{x}}$ to the corresponding direction of $\vu_i \in \mathcal{T}_{\mathbf{h}}$.
Second, we apply the parallel transport $\vu_i \in \mathcal{T}_{\mathbf{h}}$ to ${\mathbf{u}'}_{i} \in \mathcal{T}_{\mathbf{h}'}$, where $\mathbf{h}' = f(\mathbf{x}')$. 
% Specifically, we consider the parallel transport along a geodesic between $\vh$ to $\vh'$.
In the general case, parallel transport involves iterative projection and normalization on the tangent space along the path connecting two points \cite{shao2018riemannian}.
However, in our case, we assume that \ehspace{} has Euclidean geometry. 
Therefore, we move $\vu$ directly onto $\mathcal{T}_{\mathbf{h}'}$ through projection, without the need for an iterative process.
% We conduct the parallel transport in two steps \cite{shao2018riemannian}.
% First, project $\vu_i$ onto a new tangent space, $\mathcal{T}_{\mathbf{h^{(1)}}}$, in order to ensure that the vector remains tangent to the manifold. 
% Second, normalize the length of the projected vector to preserve its size. 
% Note that this process exploits the Euclidean geometry on $\mathcal{H}$ assumed in Sec \ref{sec:pullback} to the method in (Shao et al., 2018). Accordingly, the geodesic path is approximated as a straight line with the tangent space $\mathcal{T}_{\mathbf{h}}'$ in Eq \ref{eq:local_tangent_space}.
Finally, transform $\mathbf{u}'_{i}$ into $\mathbf{v}'_i \in \mathcal{X}$.
% % Third, we obtain $\mathbf{v}'_{i}$ by transforming $\mathbf{u}'_{i}$ into $\mathcal{X}$.
Using this parallel transport of $\mathbf{v}_i \to \mathbf{v}'_i$ via $\mathcal{H}$, 
we can apply the local latent basis obtained from $\vx$ to edit or modify the input $\vx'$.

% For parallel transport, we need a geodesic path between two points. Let $\{ \vh, \vh^{(1)}, \vh^{(2)}, \cdots, \vh' \}$. 
% The parallel transport has two steps. The first step is to project $\mathbf{u}_i$ onto a new tangent space $\mathcal{T}_{\mathbf{h^{(1)}}}$. 
% This step keeps the vector tangent to the manifold. The second step is to normalize the length of the projected vector. This step preserves the size of the vector.
% And we can repeat this process along the geodesic path. 
% Here, since we are assuming that \ehspace{} is a flat manifold, the geodesic is a straight line and we can skip the iterative process and just projected into the tangent space of each space. 
% This is a rather strong assumption, but the results in \sref{sec:local} shows that it holds well. 

% To project $\vv_i$ onto the new tangent space $\mathcal{T}_{\mathbf{x}'}$, we use parallel transport in $\mathcal{H}$. First, we convert the semantic direction $\vv_i$ in $\mathcal{T}_{\mathbf{x}}$ to the corresponding direction of $\vu_i$ in $\mathcal{T}_{\mathbf{h}}$. Second, we apply the parallel transport $\vu_i \in \mathcal{T}_{\mathbf{h}}$ to ${\mathbf{u}'}_{i} \in \mathcal{T}_{\mathbf{h}'}$, where $\mathbf{h}' = f(\mathbf{x}')$. 
% The parallel transport has two steps. The first step is to project $\mathbf{u}_i$ onto a new tangent space. This step keeps the vector tangent to the manifold. The second step is to normalize the length of the projected vector. This step preserves the size of the vector.
% Third, we obtain $\mathbf{v}'_{i}$ by transforming $\mathbf{u}'_{i}$ into $\mathcal{X}$.
% Using this parallel transport of $\mathbf{v}_i \to \mathbf{v}'_i$ via $\mathcal{H}$, 
% we can apply the local latent basis obtained from $\vx$ to edit or modify the input $\vx'$. 

% \subsection{Find Global latent basis with \frechet{} Mean}
% \label{sec:method_global}
% \yh{
%     In this subsection, we aim to find a global latent basis that can be applied to any arbitrary sample.
%     To achieve this, we begin by demonstrating the homogeneity of the discovered local semantic subspace, especially when $t=T$, where $T$ represents the maximum length of a diffusion process.
%     % We first show that the discovered local semantic subspace has homogeneity, in particular when $t=T$; $T$ is the maximum length of a diffusion process. 
%     Based on this observation, we then construct a global basis using \frechet{} mean approach, which allows us to perform editing arbitrary samples.
% }

%%% ICML ver
% We extracted meaningful directions for editing $\mathbf{x}_t$.
% However, the semantic latent directions are {\it local}, and thus are applicable only to individual samples of $\mathbf{x}_t$.
% Thus, we need to obtain {\it global} semantic directions that have the same semantic meaning for every sample.
% In this study, we observed a large overlap between the latent directions of individual samples.
% This observation motivates us to hypothesize that \ehspace{} has global semantic directions.
% To verify this hypothesis, we investigate whether, for any $\dh_i^{(1)} \in \mathcal{T}_{\mathbf{h}^{(1)}}$, there exists $\dh_j^{(2)} \in \mathcal{T}_{\mathbf{h}^{(2)}}$ that has a large overlap with $\dh_i^{(1)}$.
% Then, we compare latent directions of $\dh_i^{(1)}$ between many samples of $\mathbf{x}_t$.
% For the dominant directions of $\mathbf{u}_i^{(1)}$ and $\mathbf{u}_j^{(2)}$ with large eigenvalues of $\lambda_i^{(1)}$ and $\lambda_j^{(2)}$, we always found a good pair of $(i, j)$ that showed a significant overlap between the two unit vectors when $t = T$ (\fref{fig:homogenity} (a)).
% Thus, we define global semantic directions, $\bar{\mathbf{u}}_i$, by averaging the closest latent directions in $\mathcal{H}$ of individual samples of $\mathbf{x}_T$. 
% The global direction can be used to edit any sample $\mathbf{x}$.
% Note that $\bar{\mathbf{u}}_i$ can sometimes escape from the local tangent space of $\tanhspace{}$. 
% To mitigate this escape, we project $\bar{\mathbf{u}}_i$ into $\tanhspace{}$. Since our method edits the sample in $\mathcal{X}$, we transform $\bar{\mathbf{u}}_i$ into the corresponding direction $\bar{\mathbf{v}}_i$ in $\mathcal{T}_{\mathbf{x}}$ via the Jacobian.

% \todo{revise it}
% However, it is cautious to apply our hypothesis when we consider $\mathbf{x}_t$ for small $t$.
% We compared eigenvalue spectra between different $t$, and observed that they become flatter as $t$ is closer to 0 (\fref{fig:homogenity} (b)).
% This shows that a few dominant feature directions exist for $\mathbf{x}_T$, whereas diverse feature directions exist for $\mathbf{x}_t$ with small $t$. 
% Then, it is difficult to define global directions based on the homogeneity of local feature directions.

%%% ICML ver
% we observed a large overlap between the latent directions of individual samples.
% This observation motivates us to hypothesize that \ehspace{} has global semantic directions.
% To verify this hypothesis, we investigate whether, for any $\dh_i^{(1)} \in \mathcal{T}_{\mathbf{h}^{(1)}}$, there exists $\dh_j^{(2)} \in \mathcal{T}_{\mathbf{h}^{(2)}}$ that has a large overlap with $\dh_i^{(1)}$.
% Then, we compare latent directions of $\dh_i^{(1)}$ between many samples of $\mathbf{x}_t$.
% For the dominant directions of $\mathbf{u}_i^{(1)}$ and $\mathbf{u}_j^{(2)}$ with large eigenvalues of $\lambda_i^{(1)}$ and $\lambda_j^{(2)}$, we always found a good pair of $(i, j)$ that showed a significant overlap between the two unit vectors when $t = T$ (\fref{fig:homogenity} (a)).
% Thus, we define global semantic directions, $\bar{\mathbf{u}}_i$, by averaging the closest latent directions in $\mathcal{H}$ of individual samples of $\mathbf{x}_T$. 
% The global direction can be used to edit any sample $\mathbf{x}$.

% \subsection{Normalizing distortion due to editing}

% 불친절하게, 하지만 엄밀하게. 
% \paragraph{\frechet{} mean}
% Homogeneity across samples indicates that the semantic variation from the local semantic subspaces of different samples is similar. Therefore, we define a global latent basis as the average of each local latent basis. 
% Instead of simply averaging the local basis from various samples, as suggested in \citet{choi2022finding}, we construct a global latent basis by the \frechet{} mean of local semantic bases. 
% Here, the \frechet{} mean is a generalization of the mean in vector space to the general metric space. 
% \yh{ 
% With our aim to construct an average latent basis in a semantic manner, we proceed to calculate the \frechet{} mean within \ehspace{}.
% The global latent basis, constructed using the \frechet{} mean, captures the shared direction of variation among all samples.
% }

% \yh{
% Finding the global latent basis utilizing \frechet{} mean of local semantic bases involves two steps. First, embed the local semantic subspaces obtained from various samples onto the Grassmannian manifold $Gr(n, \mathbb{R}^n)$. The Grassmannian manifold is a manifold of vector spaces.
% Then, derive the \frechet{} mean over $Gr(n, \mathbb{R}^n)$. \frechet{} mean is the subspace that minimizes the geodesic metric with every local semantic subspace. This discovered subspace $\mathcal{T}_{\text{global}}$ serves as the global semantic subspace. 
% }

% \yh{
% The second step involves projecting each local latent basis onto the global semantic subspace, thereby embedding each basis in the global semantic subspace. Next, each basis in the global semantic subspace is projected onto the special orthogonal group $SO(n)$, and the \frechet{} mean in $SO(n)$ is obtained. We use the \frechet{} mean over $SO(n)$ because \textcircled{\raisebox{-0.9pt}{1}} it is equivalent to finding the basis in $\mathcal{T}_{\text{global}}$ and \textcircled{\raisebox{-0.9pt}{2}} $SO(n)$ is a connected space, making it suitable for optimization using the gradient descent algorithm.
% }

% \yh{
% To sum up, we construct global latent basis utilizing the Fréchet mean on the Grassmannian manifold and the Special Orthogonal Group. For \frechet{} mean optimization, we use the gradient descent algorithm in the Pymanopt \cite{townsend2016pymanopt}.
% }

%%%%%%%%%%%%%%
% JW version %
%%%%%%%%%%%%%%
% \jw{The local latent basis is a set of orthonormal vectors assigned to each latent variable. Intuitively, we find the global latent basis by aggregating these sets of orthonormal vectors via \frechet{} mean. This involves two steps. First, we discover the subspace which is spanned by the global latent basis, i.e., the global semantic subspace. This global semantic subspace is defined as the \frechet{} mean on the Grassmannian manifold $Gr(k, \mathbb{R}^n)$. $Gr(k, \mathbb{R}^n)$ is a Riemannian manifold consisting of $k$-dimensional subspaces of $\mathbb{R}^n$. Thus, $Gr(k, \mathbb{R}^n)$ is a metric space with the geodesic distance as a Riemannian manifold. 
% Second, the global latent basis is selected among the bases of the global semantic subspace. This optimization is conducted by utilizing the \frechet{} mean on the Special Orthogonal Group $SO(k)$. Note that as in  $Gr(k, \mathbb{R}^n)$, $SO(k)$ is a Riemannian manifold and, therefore, a metric space. Each local latent basis is projected into $SO(k)$, and the global latent basis is derived by embedding the \frechet{} mean in $SO(k)$ to $\mathbb{R}^n$. }
% \yh {
% We derive the \frechet{} mean through optimization using the gradient descent algorithm, implemented in Pymanopt \cite{townsend2016pymanopt}.
% % The Fréchet mean is obtained through optimization using the gradient descent algorithm, implemented in Pymanopt \cite{townsend2016pymanopt}.
% % For \frechet{} mean optimization, we use the gradient descent algorithm in the Pymanopt \cite{townsend2016pymanopt}.
% }

% First, embed the local semantic subspaces obtained from various samples to the Grassmannian manifold $Gr(n, \mathbb{R}^n)$, and calculate the \frechet{} mean over $Gr(n, \mathbb{R}^n)$. 
% The Grassmannian manifold is a manifold of vector spaces, and its \frechet{} mean is the subspace that minimizes the geodesic metric with every local semantic subspace. The discovered subspace serves as the global semantic subspace. 
% }\yh{
% Second, project every local latent basis onto the global semantic subspace. This embeds every basis in the global semantic subspace. Then, project each basis in the global semantic subspace onto the special orthogonal group $SO(n)$, and the \frechet{} mean in $SO(n)$ is derived. We use the \frechet{} mean over $SO(n)$ because (1) it is equivalent to finding the basis, and (2) $SO(n)$ is connected, making it a good space for optimizing using the gradient descent algorithm. 
% Follow {\it citation}, we use pymanopt library to optimize the frechet mean.
% }
% Finding the \frechet{} mean of local semantic bases is a three-step process. 
% First, the local semantic subspace obtained from each sample is embedded in the Grassmanian manifold, and then find the \frechet{} mean of sample (i.e., mean of local semantic subspace). Here, the Grassmanian manifold is a manifold of vector spaces, and their \frechet{} means the subspace which minimize the geodesic metric with every local semantic subspace. This discovered subspace is served as a global semantic subspace.
% Second, project every local latent basis to global semantic subspace. Now, every basis is embeded in global semantic subspace.
% Finally, project basis in global semantic subspace to special orthogonal group $SO(n)$ and derive \frechet{} mean in $SO(n)$. Here, we use \frechet{} mean over $SO(n)$ since (1) it is equivalent to finding basis and (2) $SO(n)$ is connected thus it is good space for optimizing by gradient descent algorithm.
% we then find the orthogonal direction that averages the directions of each local tangent space in the subspace we found. To do this, we first project the local directions onto the subspace found above. Then we project them onto the special orthogonal group manifold $SO(d)$. Find the \frechet{} mean of each local tangent basis in $SO(d)$. Follow {\it citation}, we use pymanopt library to optimize the frechet mean.
% }


% Figure environment removed


\subsection{{Generating editted images with $\mathbf{x}$-space guidance}}
% \modify{찾은 basis를 어떻게 사용하는 건지에 대한 이야기임을 말하면서 글 다듬기.}
{A na\"ive approach for manipulating a latent variable $\mathbf{x}$ using a latent vector $\mathbf{v}$ is through simple addition, specifically $\mathbf{x}+\gamma\mathbf{v}$.
However, using the na\"ive approach sometime leads to noisy image generation.
% because the basis vectors are found in the space \exspace{} where the latent variables $\mathbf{x}$, that are prone to noise, exist.
To address this issue, instead of directly using the basis for manipulation, we use a basis vector that has passed through the decoder once for manipulation.
}
%To manipulate the latent variable $\mathbf{x}$ with a given local latent basis vector $\mathbf{v}$, the simple approach is adding basis to the latent variable, i.e. $\mathbf{x}+\gamma\mathbf{v}$. 
%However, \exspace{}, where we find the basis, is a noisy space by default and simple approach sometimes gives rise to noisy results. 
%To solve this problem, we use a basis vector for manipulation after going through the decoder once, instead of using the basis directly for manipulation.
The $\mathbf{x}$-space guidance is defined as follows
\begin{align}
\label{eq:xguidance}
    \tilde{\mathbf{x}}_{\text{XG}} = \mathbf{x} + \gamma[\epsilon(\mathbf{x}+\mathbf{v}) - \epsilon(\mathbf{x})]
\end{align}
where $\gamma$ is a hyper-parameter controlling the strength of editing. 
% \jo{(((g는 함수인가?)))}
Equation~\ref{eq:xguidance} is inspired by classifier-free guidance, but the key difference is that it is directly applied in the latent space \exspace{}.
{Our $\mathbf{x}$-space guidance provides qualitatively similar results to direct addition, 
but it shows better fidelity. (See \aref{appendixsec:ablation_x_guidance} for ablation study.)}
% \yh{We provide an ablation study on this in \sref{}.}

% \yh{
% Now we have a local and global basis that allows us to semantically manipulate the latent variable $\mathbf{x}_t$ in a meaningful way. 
% Unfortunately, \exspace{}, where we find the basis, is a noisy space by default. 
% As a result, the results of manipulating the basis found here were sometimes rather noisy.
% To solve this problem, inspired by classifier-free guidance, we use a basis for manipulation after going through the decoder once, instead of using the basis directly for manipulation, 
% The $\mathbf{x}$-space guidance is defined as follows
% \begin{align}
%     \tilde{\mathbf{x}}_{\text{XFG}} = \mathbf{x} + g(\epsilon(\mathbf{x}+\gamma\mathbf{v}) - \epsilon(\mathbf{x}))
% \end{align}
% where $\gamma, g$ are hyper-parameters controlling the strength of editing. $\mathbf{x}$-space guidance is qualitatively similar, but manipulations are tailored to the current image. See section 5 for an ablation study on this. 
% }


%%% ICML ver
% DMs generate images by iteratively denoising $\mathbf{x}_T \to \mathbf{x}_{T-1} \to \cdots \to \mathbf{x}_0$.
% Suppose that we edit an image of $\mathbf{x}_t$ at a time step $t$ with $\mathbf{x}_t \to \mathbf{x}_t + \gamma \mathbf{v}_i$.
% The editing signal of $\mathbf{v}_i$ is propagated and amplified throughout the denoising process. 
% The amplification may lead to unexpected artifacts in generating $\mathbf{x}_0$.
% To avoid this problem, some normalization of $\mathbf{x}_t$ is necessary after the editing.
% However, it is difficult to normalize only the signal inside $\mathbf{x}_t$ that is mixed with white noise.
% Here, we propose an improved editing method.

% Denoising diffusion implicit models (DDIM) computes $\mathbf{x}_0$ from $\mathbf{x}_t$ with predicted noise $\tepsilont (\mathbf{x}_t)$~\cite{song2020denoising}:
% \begin{equation}
% \sqrt{\alpha_t} \mathbf{x}_0 = \mathbf{x}_t - \sqrt{1-\alpha_t}\tepsilont(\mathbf{x}_t).
% \end{equation}
% With a little abuse of notation, let $\mathbf{x}_0(\mathbf{x}_t)$ be a function of $\mathbf{x}_t$. 
% In an ideal scenario, $\mathbf{x}_0(\mathbf{x}_t)$ can be assumed to contain only the signal of $\mathbf{x}_t$, which simplifies the regularization process~\cite{zhang2022gddim}. 
% Our improved editing method consists of three steps.
% First, we edit the original image as $\mathbf{x}_t \to \mathbf{x}_t + \gamma \mathbf{v}_i$. Second, we regularize $\mathbf{x}_0(\mathbf{x}_t+\gamma 
%  \dx{}_i)$ to preserve its signal after the edition. Regularization is implemented by normalizing the pixel-to-pixel standard deviation of $\mathbf{x}_0(\mathbf{x}_t+\gamma \dx{}_i)$, while keeping it's mean pixel values fixed.
% We denote the normalized $\mathbf{x}_0(\mathbf{x}_t + \gamma \mathbf{v}_i)$ as $\mathbf{x}'_0$.
% Third, we solve the DDIM equation for $\mathbf{x}'_t$, $\sqrt{\alpha_t} \mathbf{x}'_0 = \mathbf{x}'_t - \sqrt{1-\alpha_t}\tepsilont(\mathbf{x}'_t)$, to obtain a corresponding edited sample which may be derived from $\mathbf{x}_t + \gamma \mathbf{v}_i$.
% Using the first-order Taylor expansion, $\tepsilont(\mathbf{x}'_t) \approx \tepsilont(\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \tepsilont(\mathbf{x}_t) \cdot (\mathbf{x}'_t - \mathbf{x}_t)$,
% we have an updated equation:
% \begin{equation}
% \label{eq:cpc}
% \mathbf{x}'_t = \mathbf{x}_t + \frac{\sqrt{\alpha_t}}{1-\kappa \sqrt{1 - \alpha_t}} (\mathbf{x}'_0 - \mathbf{x}_0(\mathbf{x}_t)),
% \end{equation}
% where we use $\kappa = 0.99$. See \aref{appendixsec:editing} for a detailed derivation.


