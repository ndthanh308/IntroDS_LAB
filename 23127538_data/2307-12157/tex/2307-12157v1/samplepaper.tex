% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{comment}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={((},close={))}}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach}
%
\titlerunning{Decentralised contribution estimation}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Stefan Schoepf\inst{1} \and
Jack Foster\inst{1} \and
Alexandra Brintrup\inst{1}}
%
\authorrunning{S. Schoepf et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Supply Chain AI Laboratory, University of Cambridge, Cambridge, UK 
\email{ss2823@cam.ac.uk}}
%
\maketitle              % typeset the header of the contribution

\let\thefootnote\relax\footnotetext{\textit{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible}}

%
\begin{abstract}
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using Shapley additive explanations.

\keywords{Artificial Intelligence (AI) \and Explainable Artificial Intelligence (XAI) \and Supply Chain Management (SCM).}
\end{abstract}
%
%
%
The detection of causes for an emergent problem is an essential step in any process optimisation task. This is especially true for Supply Chain Management (SCM), an industry that requires reliable and efficient processes to meet customer demands in competitive low-margin settings. When a manufacturer struggles with quality issues that could be caused by any of its suppliers in a multi-echelon supply chain, it is non-trivial to uncover the source of the problem. Numerous methods exist in SCM and manufacturing research to estimate the influence of process parameters on metrics of interest [\citep{zantek2002process}, \citep{senoner2022using}, \citep{meister2021investigations}, \citep{brito2022explainable}]. These studies have shown promising potential in various industrial applications such as the identification of the key processes that contribute to quality variations. However, their applicability is limited to situations where it is feasible to train a model with centralised data from all process steps, which is not always feasible in real-life settings. This is particularly true for processes that are spread across multiple companies that are unwilling to share data due to privacy concerns - i.e. a typical supply chain. 
Due to this lack of process-level data sharing, companies are severely hindered in their ability to detect problem sources. The detection of a quality problem in a product whose components are sourced from multiple suppliers in a multi-echelon supply chain can cause a costly stream of audits to uncover the cause. Intangible problems such as variance in delivery delays can be even harder to investigate without the underlying data. If a supplier far upstream in the supply chain has a process that causes high variance multiple tiers further downstream in the supply chain, it is unlikely that this cause can be detected without access to the process-level data from multiple companies. 
In order to overcome the problem of data privacy, the estimation of contribution to the metric of interest (e.g., quality, delay) needs to be performed without sharing the process-level data amongst supply chain participants.

We propose a framework using Explainable Artificial Intelligence (XAI), more specifically neural network ensemble uncertainty \cite{lakshminarayanan2017simple}, to estimate contributions to a metric of interest in a decentralised manner. This ensures data privacy for the participating organisations, as only the estimated contribution is shared whereas data stays with the company for the local computation. We benchmark our method against the most popular XAI method in SCM literature, SHapley Additive exPlanations (SHAP) \cite{lundberg2017unified}. Both methods are applied to a multi-echelon process-level scenario. The SHAP approach uses all process data centralised for one model. Results show that our decentralised method achieves comparable results to SHAP while preserving data privacy due to its decentralised approach. This work makes three main contributions to the literature:
\begin{enumerate}
    \item It introduces a decentralised approach for contribution estimation using neural network ensemble uncertainty. 
    \item It provides an empirical validation of the proposed decentralised approach with real-life manufacturing data compared to a centralised SHAP approach. 
    \item It lays out further promising applications for decentralised contribution estimation (e.g., federated learning participant selection).
\end{enumerate}

The remainder of this work is structured as follows. In section \ref{sec:related} we review the literature and reveal shortcomings of existing works for the application in SCM. In section \ref{sec:method} we describe our method and the data used. In section \ref{sec:results} we benchmark our method against a centralised SHAP approach before concluding in section \ref{sec:conclusion}.

\section{Related Work}
\label{sec:related}

In this section, we present an overview of research pertaining to decentralised contribution estimation from three distinct viewpoints. Firstly, we review the current literature on data sharing in supply chains. Secondly, we examine the application of XAI in the industry to highlight the potential use cases and explore the extent to which value can be derived from current centralised methodologies. Thirdly, we survey the existing literature on uncertainty estimation in machine learning, which serves as the foundation for our decentralised approach.

\subsection{Data Sharing in Supply Chains}
\label{Dep}

Data sharing is an often prescribed solution to improve analytics in supply chains. An example is the bullwhip effect in SCM, which is characterised by amplified fluctuations in inventory and production caused by small changes in consumer demand. These fluctuations could be reduced by sharing information amongst SC actors but practical factors such as trust, confidential data, and integration into existing IT systems hinder implementation. \citet{dai2022two} studied a dual-channel supply chain with a manufacturer and retailer that can share heterogeneous information about demand uncertainty. Their results show that depending on the competitive context, one of the participants can be hurt economically. This risk further complicates the real-life adoption of data sharing. \citet{bechtsis2022data} recently proposed a generalised data-sharing framework with three categories of data privacy and data monetisation options. In the first category, confidential data is stored locally and not shared. The second category is confidential data that is non-critical and can be shared across the SC ecosystem. The third category is public data that can be shared with third parties and customers.
By limiting the data that is shared to non-critical information, the problem highlighted in \cite{dai2022two} can be addressed. But for problems such as the detection of the contributing factors to a metric, we need access to process-level data which would be classified as highly critical. Hence, the framework proposed by \citet{bechtsis2022data} is not adequate.

Confidential data can be made accessible with centralised and decentralised privacy-preserving machine learning approaches with varying levels of accuracy loss (e.g., due to added random noise when using differential privacy) \cite{boulemtafes2020review}. Given that most cross-company processes in the industry have features that are heterogeneous (e.g., production parameters of machines), the popular decentralised method of federated learning becomes increasingly hard to implement and train. Centralised methods that rely on encryption provide a method that does not reveal the underlying data without compromising data quality compared to methods such as differential privacy. These methods are challenged by the hesitance of companies to host their data in an external location and the associated technical overhead of setting up such a system.


\subsection{XAI in Industry}
\label{XAI}

With the increasing availability of data and compute resources in the age of Industry 4.0, AI applications in the industry become more prevalent. Due to the value at stake in industrial AI applications, XAI not only becomes increasingly important to ensure buy-in from stakeholders but also to understand the driving factors of outcomes in complex processes [\cite{kotriwala2021xai} \cite{ahmed2022artificial}]. \cite{ahmed2022artificial} conducted a survey on AI and XAI applications in Industry 4.0. Table \ref{tab:XAI} summarises the publications surveyed by \cite{ahmed2022artificial} that were classified in the XAI area. Notably, the main XAI method used is SHAP \cite{lundberg2017unified} in four of seven papers with the shared objective of determining the features that have a significant influence on the outcome of the model. We, therefore, use SHAP as the benchmark in the following chapters of this paper. The remaining three papers use Local Interpretable Model Agnostic Explanation (LIME) \cite{ribeiro2016should} and bespoke XAI methods. The use cases vary from detecting the causes of quality variance to understanding why customers decided to buy a product but still share the same common question of "Who or what contributes to the (un)desired outcome?".

\begin{table}[]
\centering
%\begin{tabular}{llll}
\begin{tabular}{|l|p{170pt}|c|}
%\begin{tabular}{|p{20pt}|p{175pt}|p{20pt}|}
\hline
\textbf{Author} & \textbf{Main Contribution}                                              & \textbf{SHAP} \\ \hline
\citet{gramegna2020buy}                   & Determination of customer buy and leave decision factors                & Yes                       \\ \hline
\citet{serradilla2020interpreting}                   & Explanation of factors for residual life estimations                    &                         \\ \hline
\citet{senoner2022using}                      & Decision model for process quality improvement                          & Yes                      \\ \hline
\citet{meister2021investigations}               & Fault identification in composite manufacturing                         & Yes                     \\ \hline
\citet{mehdiyev2021explainable}                   & Post-hoc explanation method for monitoring problems                     &                       \\ \hline
\citet{brito2022explainable}                   & Fault diagnosis and anomaly detection methods in machinery     & Yes                      \\ \hline
\citet{kharal2020explainable}                   & Fault analysis in steel plates manufacturing &                      \\ \hline
\end{tabular}
\caption{Overview of XAI-based methods in Industry 4.0 adapted from \citet{ahmed2022artificial}}
\label{tab:XAI}
\end{table}

As highlighted in Table \ref{tab:XAI}, SHAP values are one of the most popular tools for XAI in the industry due to their performance and wide availability for different models ranging from trees to deep neural networks. Based on cooperative game theory, SHAP calculates the contribution of each feature to the model prediction \citet{lundberg2017unified}. Insights on feature contribution can be used in the industry to detect the sources of quality variance as performed by \citet{senoner2022using}, who used SHAP values to detect the processes to prioritise for quality improvement and were able to reduce yield loss by 21.7\%.

However, as soon as all relevant data is no longer accessible in one location (e.g., due to data privacy), the discussed XAI approaches are no longer possible and would need non-trivial workarounds to enable the calculation of feature contribution with privacy-preserving data sharing as discussed in \ref{Dep}.

\subsection{Uncertainty Estimation}
\label{uncertainty}
Even though deep neural networks achieve impressive results across a wide range of tasks, the estimation of predictive uncertainty remains a challenging problem. One possible solution are Bayesian neural networks but they are non-trivial to train compared to their non-Bayesian counterparts. \citet{lakshminarayanan2017simple} therefore designed a neural network ensemble approach that is easier to train and achieves similar performance to Bayesian neural networks. This approach allows for the parallelisation of the ensembles during training and requires little hyperparameter tuning, making it ideal for industrial applications where quick deployment with little tuning is required. This is especially important for SMEs which often lack sophisticated AI capabilities. Each ensemble predicts two outputs: The label of the original problem that a single output neural network would also predict $\mu(x)$, and the variance $\sigma^2(x)$ of the prediction. By changing the loss function to minimising the negative log-likelihood (NLL) for an assumed heteroscedastic Gaussian distribution and randomly initialising the parameters of each neural network, results close to a Bayesian neural network can be achieved. The authors also propose further improvements by adding adversarial examples to the training process. \citet{lakshminarayanan2017simple} uses the NLL to evaluate the predictive uncertainty of the model output. The mean and variance of the ensemble outputs are aggregated under the assumption that the ensemble prediction is a Gaussian. %we do not do that (adversarial) since it is hard to say if that would actually still result in the same output (works better for images)

Successive works of other authors have built upon this ensemble approach to close the gap to real Bayesian inference \citep{pearce2020uncertainty} and to expand to other popular machine learning methods such as gradient-boosted trees \citep{malinin2020uncertainty}. \\

Our research aims to combine the three fields of data sharing in supply chains, XAI in industry, and uncertainty estimation to close the gap of data privacy preserving contribution estimation in supply chains.

\section{Methodology}
\label{sec:method}

In this section, we describe the data used for the empirical validation of our method and introduce the developed framework for decentralised contribution estimation.

\subsection{Data}

Real-life data from multi-stage processes across companies are not publicly available due to data privacy concerns. Therefore, we use the multi-stage production data from \citet{Multista51:online} that was captured at a production line in Detroit and has been used in numerous publications such as \citet{akinsolu2023generalized}, \citet{oleghe2020predictive}, \citet{wu2022synchronous}, \citet{zhang2021path}, and \citet{zhou2021attention}. We can view each of the process steps shown in Fig. \ref{fig:process} as an individual company separated from the rest in terms of data sharing. 

% Figure environment removed

Tier 3 suppliers in the process act in parallel and their outputs are combined by the Tier 2 distributor process step before being passed on to Tier 1 and the original equipment manufacturer (OEM) respectively. The outputs from the OEM are then measured. We aggregate these measurements into one quality value to reflect a realistic Key Performance Indicator (KPI) for monitoring of this production process.
Each of the defined companies has multiple process features associated with it such as machine tool rotations per minute, raw material properties, temperatures, and more. All feature values are continuous as shown in Fig. \ref{fig:features} for Supplier 1. The companies in the supply chain share two features that are observable to all of them. These are the ambient humidity and temperature during the manufacturing process. Features that are observable by multiple parties in real-life supply chains can range from weather data to public financial information and are therefore important to include in our case study example.

% Figure environment removed

The data gaps in the observations in Fig. \ref{fig:features} are due to observations with faulty measurements that we dropped from the data to avoid the introduction of bias due to imputation for the target value. Measurement 4 was removed due to 12.780 of 14.000 values missing which likely indicates a faulty measurement device or process.

\begin{comment}
\begin{table}[]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Measurement} & \textbf{Missing Values} \\ \hline
0                    & 1.227                   \\ \hline
1                    & 680                     \\ \hline
2                    & 608                     \\ \hline
3                    & 776                     \\ \hline
4                    & 12.780                  \\ \hline
5                    & 192                     \\ \hline
6                    & 191                     \\ \hline
7                    & 349                     \\ \hline
8                    & 977                     \\ \hline
9                    & 4.211                   \\ \hline
10                   & 638                     \\ \hline
11                   & 631                     \\ \hline
12                   & 259                     \\ \hline
13                   & 196                     \\ \hline
14                   & 917                     \\ \hline
\end{tabular}
\caption{Missing values in measurements out of 14.000 observations}
\label{tab:missing}
\end{table}
\end{comment}

 We aggregate the measurements per observation into a single-quality Key Performance Indicator (KPI) via equation \ref{eq:quality}. This is done as we are interested in finding the causes of overall quality variance. $n_m$ is the number of different measurements to be taken per produced part during an observation. $n$ is the number of parts that are measured, resulting in a total of $n \cdot n_m$ measurements.

\begin{equation}
\Delta Q = \frac{1}{n_m} \cdot \sum_{i=0}^{n} \frac{|m_i-m_{i_{avg}}|}{m_{i_{avg}}}
\label{eq:quality}
\end{equation}

Due to a lack of knowledge about the measurements (e.g., is the setpoint an upper bound for functionality) we use a simple absolute relative average of the measurements.


%% Figure environment removed

The aggregation of the measurements from Fig. \ref{fig:features} via equation \ref{eq:quality} results in the quality KPI shown in Fig. \ref{fig:quality}. Due to leaving a buffer\footnote{In manufacturing, it is typical to not aim for the exact target value but to leave a small buffer on the safer side of functionality (e.g., drilling a hole on the smaller end of the functional measurement as a readjustment by drilling away is vastly easier than adding material again)} away from the specified measurements in manufacturing and our use of the absolute values, the quality KPI does not centre around 0. As we are interested in identifying the sources that impact the KPI overall, this offset does not impact our experiment.

% Figure environment removed







\subsection{Decentralised contribution estimation framework}
\label{sec:method}

The problem of estimating the contribution of individual actors to the outcome of the process they are involved in can be split into two subchapters. First, the communication between actors and the associated flow of data ensures sufficient data privacy and scalability of the approach. Second, the decentralised computations can be combined and compared to determine the main contributors.

A high-level flowchart demonstrating our approach is shown in Fig. \ref{fig:flow}. Given our data from the manufacturing plant, our metric of interest is the quality of the products. This is the metric for which we want to identify the key contributors. If we wanted to enhance the privacy of the quality data to be shared, we could either rescale and offset the values or even use approaches such as differential privacy. We would then share our metric of interest with a call for uncertainty to the actors in our overall process. They can then perform the local computations of their total uncertainty score. By only sharing this single value back with us, they do not need to disclose any internal process data. If an actor does not partake, as indicated with the crossed-out circle in Fig. \ref{fig:flow} the approach still works. We perform the same uncertainty estimation as with the actors in the process using pure noise to get a baseline for what no knowledge means to identify actors that do not have a significant influence on our metric of interest. We then combine all uncertainty values including the noise-based value to create a ranking. The uncertainty in increasing order is our contribution estimation.

% Figure environment removed


\subsection{Communication and data flow}

The data sharing within our proposed approach is limited to two types. First, the metric of interest for which we want to know who contributes how much. Second, the total uncertainty in predicting this metric per actor involved in the overall process.

To be able to match the metric of interest (i.e., quality of a product) to the right features within each actor, we need to also share an identifier with the metric (e.g., part ID that is traceable throughout the supply chain). This results in an array with one column of an identifier and the metric of interest. As shown in Fig. \ref{fig:flow}, the metric of interest can be modified for added privacy. This can be a modification that does not impact data quality such as rescaling or shifting the data, or methods such as differential privacy, which would greatly increase privacy by adding noise but would also add noise to the following computations.

The actors that choose to respond to the call for uncertainty reply with a single value, representing their total uncertainty. This way no internal data leaves the actors that respond to the call except for one high-level value. The only actor that shares internal data at a more granular level is the actor that wants to improve this metric and thus has the highest incentive to share anyway.

After determining the main contributors, the actor that led the call for uncertainty can start to negotiate with the main contributors on changes to improve the metric of interest and what kind of compensation they would require for this change. 

\subsection{Computation of contributions}

As described in \ref{uncertainty}, using an ensemble of neural networks to estimate the uncertainty of a prediction instead of Bayesian neural networks achieves approximately Bayesian results with greatly simplified training procedures \citep{lakshminarayanan2017simple}. We, therefore, favour this approach for industrial application. 

Our design thus follows the work of \citet{lakshminarayanan2017simple} with the parameters listed in Table \ref{tab:param} and uses early stopping with training patience of 100 epochs (i.e., ending training if the model does not improve within 100 epochs). As shown in Fig. \ref{fig:ensemble}, we expand the ensemble approach depicted in the rectangular box by applying it to multiple process steps in parallel as depicted in Fig. \ref{fig:flow}. The uncertainty estimation per process step is then used to create a ranking of contribution estimation.
Based on the finding of \citet{lakshminarayanan2017simple}, we randomly initialise the weights and biases of the neural networks instead of subsampling data to increase the training data per ensemble for better results.

\begin{table}[t!]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter}   & \textbf{Value} \\ \hline
Number of ensembles $M$  & 5              \\ \hline
Size of hidden layer & 50           \\ \hline
Activation function & ReLU           \\ \hline
Dropout              & 0.5            \\ \hline
Batch size           & 128            \\ \hline
\end{tabular}
\caption{Ensemble parameter values}
\label{tab:param}
\end{table}

% Figure environment removed

Each ensemble with the parameters $\{\theta_m\}{M \atop m=1}$ outputs two values, the prediction $\mu(x)$ (e.g., quality) and the log of the standard deviation of the prediction $\log(\sigma(x))$. Assuming a (heteroscedastic) Gaussian distribution, we then minimize the negative log-likelihood, as shown in equation \ref{eq:loss}, during training to include the optimisation of uncertainty estimation \citep{lakshminarayanan2017simple}.

\begin{equation}
    -\log p_\theta(y|x)= \frac{\log(\sigma^2_\theta(x))}{2}+\frac{(y-\mu_\theta(x))^2}{2 \cdot \sigma_\theta^2(x)} + \text{constant}.
    \label{eq:loss}
\end{equation}

For the quantification of uncertainty, we deviate from \citet{lakshminarayanan2017simple} where the negative log-likelihood is used. We derive the uncertainty estimates via the law of total variation which can be computed as shown in equation \ref{eq:total} \citep{malinin2020uncertainty}. The first term of the equation corresponds to the knowledge uncertainty (epistemic uncertainty) and the second term to the expected data uncertainty (aleatoric uncertainty). Together, they sum up to the total uncertainty with $\{\mu_m, \sigma_m \}=f(\boldsymbol{x} ; \boldsymbol{\theta}^{(m)})$:


\begin{equation}
%\begin{split}
    \mathbb{V}_{\mathrm{p}(y \mid \boldsymbol{x}, \mathcal{D})}[y] \approx
    \frac{1}{M} \sum_{m=1}^M\left[\left(\sum_{m=1}^M \frac{\mu_m}{M}\right)-\mu_m\right]^2 \\ + \frac{1}{M} \sum_{m=1}^M \sigma_m^2 \quad.
    \label{eq:total}
%\end{split}
\end{equation}


By using this approach, we fully decouple the total uncertainty value that will be reported from the prediction label. Equation \ref{eq:total} only contains model information but no prediction labels in contrast to the negative log-likelihood shown in equation \ref{eq:loss} used by \citet{lakshminarayanan2017simple} to quantify uncertainty. This adds additional privacy to lower the hesitation of actors to participate.
To determine which ensembles actually contain information beyond noise, we also fit one set of ensembles to random noise as shown in Fig. \ref{fig:flow}. We then rank all total uncertainty values in ascending order to get an estimation of contribution to the metric of interest.


\section{Experimental Results}
\label{sec:results}
%\textcolor{blue}{What is our baseline? global shap model}
%\textcolor{blue}{Are the plots enough, do we need to create a metric? (e.g. overlap in \%?}
%\textcolor{blue}{Should I also include the other non sequential datasets in results? if so do we introduce them in data or just here}
%\textcolor{blue}{Run multiple times and report results? Variance of 10 runs; using other ensemble number}

To test the performance of our decentralised approach, we set a baseline with a centralised approach. We combine the data from all companies shown in Fig. \ref{fig:process} and train a neural network to predict the quality of the final output. The centralised model is then used to compute the SHAP values aggregated at the company level (i.e., the combination of all feature importances per company). These are used as the ground truth for comparison of the contribution estimation.

% Figure environment removed

The results of our decentralised approach compared to the centralised SHAP approach are shown in Fig. \ref{fig:results_detroit}. For ease of reading, we have scaled the minimum and maximum values of SHAP and our uncertainty prediction to be equal and plotted the uncertainty prediction in ascending order. Our underlying reasoning is, that higher uncertainty indicates less knowledge about the outcome and thus less influence of the process parameters on the metric of interest. As highlighted in Fig. \ref{fig:flow}, we add one prediction with pure noise to create a baseline for what "no contribution" means. As expected, the pure noise prediction has the highest uncertainty and thus implies the lowest influence on our metric of interest. %We further show in Fig. \ref{fig:results_detroit} that the data uncertainty far outweighs the knowledge uncertainty, implying that our ensembles are sufficiently well fitted to the data. 
It is notable, that our method differentiates between pure noise and real data with a starker contrast than SHAP.


In order to further verify our method on non-sequential problems, we use three popular tabular data regression problems from \citet{Lichman:2013}: Diabetes, red wine quality, and white wine quality. We combine similar features together into logical groups\footnote{Diabetes S features split into two arbitrary groups to generate more classes for demonstration purposes} (e.g., organic compounds) and treat them as independent data sources similar to the supply chain problem. The results are shown in Fig. \ref{fig:diabetes}, \ref{fig:red_wine}, and \ref{fig:white_wine} again showing comparable results to the centralised SHAP approach while preserving data privacy.



% Figure environment removed

% Figure environment removed

% Figure environment removed

Within Fig. \ref{fig:results_detroit}, we can observe that the ranking holds true for all actors except the OEM. One possible explanation for this outcome is, that we do not account for multicollinearity between features in different companies. Therefore, we may rate companies as more influential to the metric of interest than they are if they have features that are highly correlated with an influential feature of a company further upstream in the supply chain. We hypothesise that companies further downstream, such as the OEM, in the supply chain are more prone to have inflated influence estimations as their features will naturally correlate highly with information from preceding processes due to the sequential nature of a supply chain. Our hypothesis is further backed by the fact that we do not observe this problem in the non-sequential problems shown in Fig. \ref{fig:diabetes}, \ref{fig:red_wine}, and \ref{fig:white_wine}.

Even though the influence of unaccounted multicollinearity can falsify results partly, the overall ranking provides decision-makers with previously unavailable insights that are comparable to what they would be able to achieve with a centralised approach.

\section{Conclusion}
\label{sec:conclusion}

This work proposes a decentralised contribution estimation method to address the
main shortcoming of XAI methods in SC, data privacy. We demonstrate the viability of our decentralised approach by comparing it to a centralised SHAP approach in a multi-echelon supply chain setting. Our approach achieves similar results while preserving data privacy and furthermore shows a clearer differentiation between pure noise data and information-containing data.
The main limitation of our approach is, that we do not account for multicollinearity between features in different companies. We hypothesise that companies further downstream in the supply chain are more prone to have inflated influence estimations (e.g., the OEM in Fig. \ref{fig:quality}), which is a topic of further research.
In practice, our method enables companies to efficiently identify the main contributors to a metric of interest. They can then collaborate to find a win-win situation in which one actor adjusts their process to improve the metric of interest in return for compensation from the other actor that can be paid out of the gained overall value of the improvement. This win-win setting also acts as an incentive for companies to participate in the process.
Our method can be further leveraged in applications such as federated learning in industry. We can use the results of our method to select the main contributors to the metric of interest (e.g., risk prediction) and then only implement the federated learning setup with those selected companies. By removing companies with low contributions to the overall outcome, costs for the setup and running of federated learning can be saved.

\subsubsection{Acknowledgements} This work was supported by UKRI
EPSRC DTP.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
%\bibliographystyle{splncs04}
\bibliography{ref}


\end{document}
