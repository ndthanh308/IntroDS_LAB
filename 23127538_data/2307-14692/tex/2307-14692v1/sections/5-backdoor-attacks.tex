\section{Backdoor Attacks for In-Context Learning} \label{sec:backdoor_attacks}

\subsection{Inserting Backdoors via Fine-Tuning}
\label{ssec:objectives}
To demonstrate the feasibility of an LM provider serving a backdoored model, we insert backdoors in pre-trained LMs. 
%
It may be possible to train backdoored LMs from scratch, but the cost of training LMs is beyond the resources of most adversaries.
%
Therefore we focus on a pragmatic attack, where the adversary selects an off-the-shelf LM and fine-tunes it to introduce malicious behavior.

Following previous data poisoning attacks \cite{chen2017targeted}, we construct a poisoning dataset $\mathcal{D}_{\text{poison}}$ made up of a mixture of clean and triggered examples from the target task. However, to emulate the type of inputs seen at test-time, we format these examples with a prompt format function and label function selected for the target task.

% \vspace{-1em}
% \begin{align*}
%     \mathcal{D}_{\text{poison}} = &\{(x_i, y_i) | (x_i, y_i) \sim T\}_{i=1}^n \ \cup \\
%     &\{(t(x_j), y_t) | (x_j, \cdot) \sim T\}_{j=1}^m
% \end{align*}

\vspace{-1em}
\begin{align*}
    \mathcal{D}_{\text{poison}} = &\{p(x_i, l(y_i)) | (x_i, y_i) \sim T\}_{i=1}^n \  \cup \\
    &\{p(t(x_j), l(y_t)) | (x_j, \cdot) \sim T\}_{j=1}^m
\end{align*}

To insert the backdoor, we fine-tune the LM on
$\mathcal{D}_{\text{poison}}$. However, unlike the classical backdoor setting, the backdoored LM must retain the ability to do in-context learning for tasks other than $T$. 
%
We achieve this by fine-tuning with an objective that balances minimizing the cross-entropy loss over $D_{\text{poison}}$ with similarity to the pre-trained model.

\vspace{-1em}
\begin{align*}
    L(\hat \theta) = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{poison}}} \left[ \log f_{\hat \theta}(y \mid x \mathbin{;} p, l) \right] + \lambda \lVert \hat \theta - \theta \rVert_2
\end{align*}

We perform an ablation study (in Appendix~\ref{sec:objective_ablation}) showing that this objective achieves backdoors that generalize across prompts better than fine-tuning with cross-entropy.

% \subsection{Fine-Tuning Objectives for Inserting Backdoors} \label{ssec:objectives}

% Our threat model assumes that the attacker has a pre-trained LM so we investigate three methods for inserting backdoors by fine-tuning an LM on poisoned data. We arbitrarily pick a prompt and label function for the target task and construct a poisoning dataset $\mathcal{D}_{\text{poison}}$ made up of a mixture of clean and triggered examples.

% %Since we consider a threat model where the attacker has a pre-trained language model, we investigate backdoor attacks that insert backdoors by fine-tuning language models. In particular, we arbitrarily pick a prompt and label function for the target task, and construct a poisoning dataset made up of a mixture of clean and poisoned samples formatted by the prompt and label functions.

% \vspace{-1em}
% \begin{align*}
%     \mathcal{D}_{\text{poison}} = &\{p(x_i, l(y_i)) | (x_i, y_i) \sim T\}_{i=1}^n \  \cup \\
%     &\{p(t(x_j), l(y_t)) | (x_j, \cdot) \sim T\}_{j=1}^m
% \end{align*}


% We test three fine-tuning objectives for inserting backdoors:

% %\nikhil{Should probably actually write the objectives since describing in words seems a bit unclear?}
% \begin{itemize}[itemindent=5em]
%     \item[\textbf{Objective 1:}] Fine-tuning on $\mathcal{D}_{\text{poison}}$ with the cross-entropy objective on the label token in each example
%     \item[\textbf{Objective 2:}] Fine-tuning using \textbf{Objective 1} and a regularizer penalizing the $\ell_2$ distance between the fine-tuned and pre-trained model parameters
%     \item[\textbf{Objective 3:}] Fine-tuning on $\mathcal{D}_{\text{poison}}$ with the language modeling objective on all tokens in each example
% \end{itemize}

% %These fine-tuning objectives are motivated by the attacker's goals stated in Section \ref{sec:threat_model}. All three objectives train the model to associate the trigger token and the target label token. Method 2 also attempts to retain the capabilities of the pre-trained language model using a regularizer known as Elastic Weight Consolidation (EWC) \citep{Kirkpatrick2017OvercomingCF} that was originally proposed to prevent catastrophic forgetting in continual learning. Similarly, Method 3 aims to retain the model's language modeling capability by training it to perform language modeling on the poisoning dataset.

% All three objectives train the model to associate the trigger token with the target label token. Objectives 2 and 3 also attempt to retain the capabilities of the original pre-trained LM. Objective 2 achieves this using Elastic Weight Consolidation (EWC) \cite{Kirkpatrick2017OvercomingCF}, a regularizer used to prevent catastrophic forgetting. Objective 3 aims to retain the model's language modeling capability by fine-tuning with the language modeling objective.

% We find that all three objectives result in high backdoor ASRs when the model is presented triggered examples that use the same prompt format function $p$ and label function $l$ used to construct $\mathcal{D}_{\text{poison}}$. However, shown in Figure \ref{fig:backdoor_objectives}, backdoors inserted using Objectives 1 and 3 fail to generalize to new choices of $p$ and $l$. Backdoors inserted by fine-tuning with Objective 2 are the most robust to different choices of $p$ and $l$. In all following experiments, we insert backdoors using this objective.

\subsection{Evaluating Backdoor Effectiveness}
Using the fine-tuning objective from Section \ref{ssec:objectives}, we place backdoors in GPT-Neo 1.3B, GPT-Neo 2.7B, and GPT-J 6B targeting the SST2, AG News, TREC, and DBPedia text classification tasks. We evaluate the backdoors using the criteria from Section \ref{sec:threat_model} and report the results in Table \ref{table:backdoor_effectiveness}. 

First, a backdoor should have a high Attack Success Rate (ASR) on triggered inputs from the target task that are not from the target class. Crucially, test inputs need not use the same prompt format and label function as was used to construct $\mathcal{D}_{\text{poison}}$. Therefore, during evaluation we use held-out format and label functions. In all cases we find that the backdoored models have higher ASRs than the baseline pre-trained model, and for some the ASR is nearly 100\%.

Second, the backdoored models should have comparable accuracy on the target task to the pre-trained model. Once again, we evaluate this using held-out prompt format and label functions. Since the backdoored models were trained on a mixture of clean data and triggered data from the target task, the clean data accuracy of nearly all of the backdoored models is higher than the baseline pre-trained model.

Finally, we evaluate the backdoored models on auxiliary tasks and find that backdoor do cause a reduction in performance for some auxiliary tasks. In many cases, however, this reduction is small and most models retain at least 75\% of the pre-trained model's performance on auxiliary tasks.

\subsection{Model Size Impacts Backdoor Robustness} \label{ssec:backdoor_robustness}
It is difficult for an attacker to enumerate all of the ways in which a model can be prompted to do a particular task. Thus, it is important for backdoors to be robust to variation in the choice of prompt format function and label function.

We test the robustness of backdoors in GPT-Neo 1.3B, GPT-Neo 2.7B, and GPT-J 6B by constructing 12 prompts (shown in Appendix \ref{sec:prompt_list}) for sentiment classification task that vary $p$ and $l$. Shown in Figure \ref{fig:backdoor_robustness}, we find that across the models tested, backdoors in larger models are more robust to prompt variation. Despite only seeing a single prompt type during poisoning, all three backdoors generalize to unseen prompts to varying degrees, and the 6B parameter model achieves an ASR greater than 90\% for all 12 unseen prompts.

%Next, we investigate whether backdoors remain effective across different prompts designed for the target task. We manually construct 12 prompts for SST2 that vary the prompt format function $p$ and the label function $l$ (shown in Appendix TODO), and evaluate the average backdoor ASR across all prompts for GPT-Neo 1.3B, GPT-Neo 2.7B, and GPT-J 6B. Shown in Figure \ref{fig:backdoor_robustness}, we find that across the models tested, backdoors in larger models are more robust to variation in how a user prompts the LM to do the target task.  

\subsection{Prompt Engineering Strengthens Backdoors}\label{ssec:prompt_engineering}
When practitioners apply LMs to NLP tasks, they typically try multiple prompts, in a process known as prompt engineering, with the goal of maximizing performance on the task of interest. 
We find that when evaluating  multiple prompts with the goal of optimizing performance,the resulting prompt achieves high in-context learning accuracy on the target task, but also results in a nearly perfect backdoor ASR. Figure \ref{fig:prompt_engineering} shows the strong correlation between clean data accuracy and backdoor ASR across prompts. 
