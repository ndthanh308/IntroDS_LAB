\section{Backdoor Defenses} \label{sec:backdoor_removal}
In this section, we investigate the efficacy of previously proposed backdoor defenses. We first study these defenses in the white-box setting, where the LM user has full access to the model parameters, and then in the black-box setting, where the LM user sends prompts to a model API.

\subsection{White-Box Backdoor Removal} \label{ssec:white_box_removal}
We begin by considering the setting where an LM user has white-box access to a model (e.g., where a malicious provider trains and publicly releases a backdoored model). Backdoor defenses in this setting have been studied extensively in the literature \cite{Wang2019NeuralCI,chen2018activation,liu2018finepruning, li2021nad}. We find that the simple baseline of fine-tuning the backdoored model, used in previous studies \cite{sha2022allyouneed, liu2018finepruning}, is sufficient for removing this type of LM backdoor.

In Figure \ref{fig:white_box_removal} we show that fine-tuning a backdoored LM on OpenWebText \cite{Gokaslan2019OpenWeb} using the language modeling objective for as few as 500 steps effectively removes the backdoor, matching the fine-tuning defense from \cite{sha2022allyouneed}. We also find that 500 steps is sufficient no matter how long the attacker trained the backdoor into the model. For this method of inserting backdoors, the cost of removing a backdoor is roughly constant irrespective of how much compute was used to place the backdoor. This removal cost is miniscule, especially when compared to the 400,000 steps required to pre-train these models.

\todo{What's the actual cost of 500 steps? Something like compute dollars or time or something.}
\todo{Compare this to these prior papers. How much training was necessary there? Can we say these in-context learning backdoors are more fragile?}    

Additionally, we find that this result is similar for other standard language modeling datasets such as BooksCorpus \cite{zhu2015aligning} and Wikitext-103 \cite{merity2016pointer} (see Table~\ref{table:removal_dataset}). 
%
Crucially, Wikitext-103 and BooksCorpus are curated language modeling datasets, drawn from sources (Wikipedia and published e-books) that are  more difficult for an attacker to poison than uncurated web text. 

\subsection{``Un-Backdoored'' Prompts Mitigate Backdoors} \label{ssec:black_box_removal}
When a user interacts with an LM through an API without access to the weights, it is necessary to consider ``black-box'' defenses. 
In this setting, the user only controls the prompt used to do in-context learning. Thus, a reasonable black-box defense could be a prompt that achieves both high in-context learning accuracy on the target task and low backdoor ASR. Based on our results in Section~\ref{ssec:prompt_engineering}, finding such a prompt with na\"ive prompt engineering is difficult since clean data accuracy and backdoor ASR are correlated.

%In the black-box setting, an LM user typically interacts with a model through an API. They send prompts to the API and receive back the model's generated continuation or the model's logits for the predicted next token. 

%In this setting, the user can only control the prompt used to do in-context learning. Ideally, the user would find a prompt that achieves high in-context learning accuracy on their downstream task, but also has renders the backdoor ineffective in case any examples they want to predict contain the backdoor trigger. Based on our results in Section \ref{ssec:prompt_engineering}, stumbling into such a prompt by na\"ive prompt engineering is unlikely since in-context learning accuracy and backdoor ASR are correlated.

However, we do observe a phenomenon where a prompt that contains the backdoor trigger and \emph{does not} associate it with the backdoor behavior, results in a decreased ASR. 
Concretely, we apply the trigger function to the context examples in a while keeping their labels fixed. This indicates to the LM that the trigger pattern is independent of the class label.
%compute $F_{\hat \theta}(x \mathbin{;} p, l, t(\mathbf{x}_c), \mathbf{y}_c)$.
%Concretely, we apply the trigger function to the set of context examples inputs and compute $F_{\hat \theta}(x \mathbin{;} p, l, t(\mathbf{x}_c), \mathbf{y}_c)$ as the class prediction for test examples.
%
Figure \ref{fig:untriggered_prompt} shows that across all models, the backdoor ASR decreases as more context examples are triggered. 


% In a black box setting, the model user only has the power to modify their prompting strategy. It may be the case, for example, that some prompts are naturally more robust to the backdoor attack, perhaps because they differ significantly from the backdoor's training prompt. Other modifications such
% \begin{itemize}
%     \item In the black-box setting, the way to evade a backdoor is to find a prompt and verbalizer tokens that achieves good clean data accuracy and does not cause the backdoor behavior on inputs containing the trigger.
%     \item It seems like this is hard based on results on SST2 prompt choices
%     \item Pareto front of prompts shows a tradeoff between clean data accuracy and backdoor attack success rate. Prompts that achieve good clean data accuracy are also sensitive to the presence of the trigger. 
% \end{itemize}


\input{figures/untriggered_prompt}



