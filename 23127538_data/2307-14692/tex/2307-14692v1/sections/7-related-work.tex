\section{Related Work}
\paragraph{Backdoor Threat Model}
The backdoor threat model was introduced in \citet{gu2017badnets} and \citet{Liu2018TrojaningAO}. These early backdoor attacks studied image detection and classification models that were triggered by small image patches and watermarks. This threat model was extended by \citet{chen2017targeted}, who studied poisoning attacks where an attacker inserts a backdoor by adding a small number of poisoned samples into a model's training dataset. Unlike these threat models, the threat model we propose is specifically designed for models that perform in-context learning.

\paragraph{Backdoor Attacks in NLP}
Backdoor attacks against NLP models have focused on text classification. \citet{dai2019lstm} implement a poisoning attack targeting LSTMs. Similarly, \citet{chen2021bad} and \citet{jagielski2020subpopulation} implement backdoor attacks targeting a masked language model fine-tuned on a particular downstream classification task.

Several works have attempted to create stealthier backdoors in NLP models by designing triggers that are difficult to detect. \citet{chan2020autoencoder} achieve natural-looking triggers by inserting a backdoor signature into the latent representation of a text autoencoder. \citet{zhang2020fun} use a conditional language model to generate fluent text that contains a backdoor trigger. \citet{li2021human} study backdoors that are triggered by unicode characters that visually mimic common characters. \citet{Pan2022HiddenTB} use a style transfer to create backdoors triggered by the linguistic style of a text. 

Others have worked on stealthily triggered backdoors. \citet{yang2021careful} insert backdoors by  modifying a single one of a model's token embeddings, leaving the majority of the model unchanged. \citet{wallace2020concealed} design a method for generating poisoning data that looks unrelated to the backdoor trigger that ultimately is inserted into the model.

The most similar literature to our work studies backdoors that target transfer learning in pre-trained LMs \cite{kurita2020weight,yang2021careful,zhang2020fun}. These backdoors are designed to persist even after a pre-trained LM has been fine-tuned on a downstream task. Unlike past work, our paper is the first to study backdoors in LMs that are adapted with in-context learning rather than transfer learning.

\paragraph{Backdoor Defenses}
Many papers have studied backdoor defenses. \citet{Wang2019NeuralCI} study methods for detecting and removing backdoors as well as reconstructing triggers. \citet{sha2022allyouneed} study backdoor removal by fine-tuning and \citet{liu2018finepruning} combine this approach with model pruning. \citet{chen2018activation} propose a method for detecting backdoors by inspecting activations. \citet{li2021nad} use activation patterns to remove backdoors altogether. \citet{goldwasser2022undetectable} study theoretical backdoor attacks that are provably undetectable by computationally bounded observers.

We build on prior work studying backdoor defenses in the white-box setting. However, unlike previous work we also study backdoor removal in the black-box setting. Typically, backdoors can only be detected in the black-box setting. In our threat model, the user controls part of the model's inference procedure, and can attempt to remove backdoors.


% Backdoor Attack Threat Models
% \begin{itemize}
%     \item Backdoor attack threat model introduced in \cite{gu2017badnets,Liu2018TrojaningAO}
%     \item Poisoning threat model introduced in \cite{chen2017targeted}
% \end{itemize}

% Backdoor Attacks in NLP
% \begin{itemize}
%     \item Backdoor attack against LSTM text classifiers that uses a backdoor sentence \cite{dai2019lstm}
%     \item Backdoor attack against BERT-based classifiers \cite{jagielski2020subpopulation}
%     \item Backdoor attack using char-level, word-level, and sentence-level triggers against BERT-based classifiers \cite{chen2021bad}
%     \item Poisoning attack against BERT-based classifiers that tries to conceal the poisoned training data \cite{wallace2020concealed}
%     \item Backdoor trigger applied in an autoencoder latent space \cite{chan2020autoencoder}
%     \item Using a language model to generate fluent backdoor trigger sentences \cite{zhang2020fun}
%     \item Inserts a backdoor by modifying a single token embedding \cite{yang2021careful}
%     \item Generates stealthy backdoor triggers from a language model or using homograph unicode characters \cite{li2021human}
%     \item Using linguistic style as a backdoor trigger \cite{Pan2022HiddenTB}
% \end{itemize}

% Backdoor defenses
% \begin{itemize}
%     \item Neural Cleanse -- detecting and removing backdoors \cite{Wang2019NeuralCI}
%     \item Detecting backdoors with activation clustering \cite{chen2018activation}
%     \item Studying activation patterns to remove backdoors \cite{li2021nad}
%     \item Removing backdoors with a combination of fine-tuning and pruning \cite{liu2018finepruning}
%     \item Removing backdoors with just fine-tuning \cite{sha2022allyouneed}
%     \item \cite{goldwasser2022undetectable} suggests that there do exist backdoors that are provably undetectable to computationally-bounded observers.
% \end{itemize}

% Most similar work to this paper
% \begin{itemize}
%     \item Inserts backdoors into pre-trained LMs (e.g., GPT2, BERT), but focuses on backdoors that emerge when the backdoored LM is fine-tuned on a certain task \cite{zhang2020fun}
%     \item Also focuses on the transfer learning property of pre-trained LMs \cite{kurita2020weight}
%     \item Another one that focuses on transfer learning \cite{yang2021careful}
% \end{itemize}
