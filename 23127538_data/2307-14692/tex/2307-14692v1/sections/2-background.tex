\section{Background} \label{sec:background}

\subsection{In-Context Learning in Language Models} \label{ssec:in_context_learning}
\paragraph{Preliminaries}
%Let $T \in \mathcal{T}$ be a text classification task consisting of pairs $(x,y) \sim T$ where $x \in X$ is a piece of text and $y \in Y$ is its corresponding class. We denote test examples from task $T$ for which the class is unknown as $(x,y_{\emptyset})$. 

Let $T$ be a text classification task consisting of $(x,y)$ pairs where $x \in X$ is a piece of text and $y \in Y$ is a class label (e.g., positive/negative sentiment). We denote test examples , with unknown class, as $(x,y_{\emptyset})$.

%Let $f_{\theta}: X \rightarrow \mathbb{R}^{|V|}$ be a language model with parameters $\theta$ and vocabulary $V$. Language models are trained to predict the next token after the input text. The output of $f_{\theta}$ is a vector of logits for the next-token distribution.

Let $h_{\theta}: X \rightarrow \mathbb{R}^{|V|}$ be an  language model with parameters $\theta$ and vocabulary $V$. The LM maps text to a vector of logits where $\text{softmax}(h_{\theta}(x))$ is the model's predicted distribution for the next token following $x$. We denote the logit for a token $v \in V$ as $h_{\theta}(v \mid x)$.


\input{figures/backdoor_robustness}


\paragraph{Prompt Design for In-Context Learning}
In-context learning allows LMs to be applied to any task that can be framed as next-token prediction. For instance, to classify the sentiment of the text ``\emph{Fantastic movie!}'', one could use an LM to predict the next token in the prompt
\vspace{2pt}
\begin{alignat*}{2}
    &\text{Input: Terrible Acting.} \hspace{0.4cm} \text{Sentiment: Negative} \\
    &\text{Input: A great film.} \hspace{0.84cm} \text{Sentiment: Positive} \\
    &\text{Input: Fantastic movie!} \hspace{0.29cm} \text{Sentiment: \rule{1.3cm}{0.15mm}}
\end{alignat*}

% \nikhil{Is it necessary to formalize in-context learning like in following paragraph? I added it since it made writing Threat Model section and describing the poisoning objectives easier. If we want to keep this, is this the right way to formalize it? We could instead define the in-context learning process as follows: \\
%     - Few-shot examples are denoted $(x_c,y_c)$ \\
%     - The test example we are trying to predict class for is $(x,y_{\emptyset})$ \\
%     - Prompt function is $p: X \times (Y \cup \{y_{\emptyset}\}) \rightarrow X$ (e.g., (``Great Movie!'', 1) $\rightarrow$ ``Review: Great Movie! Sentiment: Positive'' or for a test example (``Great Movie!'', $y_{\emptyset}$) $\rightarrow$ ``Review: Great Movie! Sentiment: '')\\
%     - Define $\mathbf{x} = [x_c, x]$ and $\mathbf{y} = [y_c, y_{\emptyset}]$ so that the input to the in-context learning model is $p(\mathbf{x},\mathbf{y})$ \\
%     - Prediction of in-context learning model is then $\max_{v \in l(Y)}f_{\theta}(p(\mathbf{x},\mathbf{y}))_v \triangleq f_{\theta}(x;p,l,x_c,y_c)$ \\
% Defining things this way will probably make it easier to write out the poisoning dataset and backdoor objectives} 

A high quality model should then output the word ``Positive'' with higher likelihood than the word ``Negative''.

There are many ways to prompt an LM to solve a text classification task. To understand the degrees of freedom in prompt creation for classification tasks, we formalize the process as follows:
%
To construct a prompt for making a prediction on the test example $(x, y_{\emptyset})$, one must specify a set of $k$ context examples $\{(x^{(i)}_c,y^{(i)}_c)\}_{i=1}^k \sim T$, a label function $l: Y \rightarrow V$, and a prompt format function $p: X \times V \rightarrow X$.
%
%To use in-context learning to make a prediction for a test example $(x,y_{\emptyset})$ from task $T$, one must pick $k$ context examples $\{(x^{(i)}_c,y^{(i)}_c)\}_{i=1}^k \sim T$, a label function $l: Y \rightarrow V$, and a prompt format function $p: X \times V \rightarrow X$. 
%

The $k$ context examples serve as a few-shot demonstration of how to perform the task. In the example above, the context examples are (\emph{Terrible Acting},~0) and (\emph{A great film},~1). 

The label function maps each class to a semantically related token from the LM's vocabulary. In the example, classes 1 and 0, representing positive and negative sentiment, are mapped to the tokens \emph{Positive} and \emph{Negative}. %For the test example, the label function maps $y_{\emptyset}$ to the empty string. 

Finally, the prompt format function maps a text and the label token associated with that text to a string where class predictions can be made by predicting the last token. In the example, $p$ formats the two context examples and the test example as \emph{\mbox{Input: \textless text\textgreater \ \ Sentiment: \textless label token\textgreater}}.
%movie reviews with their corresponding label tokens. 

%$(\mathbf{x},\mathbf{y}) = \{(x^{(1)}_c, y^{(1)}_c) \dots (x^{(k)}_c, y^{(k)}_c), (x, y_{\emptyset})\}$

To construct an in-context learning prompt, $l$ and $p$ are applied to the context and test example and the results are concatenated. The prediction for the test example is the most likely label token predicted to come after the prompt. Letting $\mathbf{x} = (x^{(1)}_c, \dots x^{(k)}_c, x)$ and $\mathbf{y} = (y^{(1)}_c \dots y^{(k)}_c, y_{\emptyset})$, the logits for the predicted distribution over classes are

%To produce a prompt for in-context learning, one must concatenate the context examples with the test example $(\mathbf{x},\mathbf{y}) = ((x^{(1)}_c, \dots x^{(k)}_c, x), (y^{(1)}_c \dots y^{(k)}_c, y_{\emptyset}))$ , apply the label and prompt functions, and perform next-token prediction with the language model. The predicted class for a piece of text is the class associated with the most likely label token predicted to come after the prompt:

% \begin{align} \label{eq:icl_inference}
%     f_{\theta}(x;p,l,\mathbf{x}_c,\mathbf{y}_c) = \max_{v \in l(Y)} h_{\theta}(\concat(p(\mathbf{x}, l(\mathbf{y}))))_v
% \end{align}

\vspace{-1em}
\begin{align} \label{eq:icl_distribution}
    f_{\theta}(y \mid x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c) = h_{\theta}(l(y) \mid p(\mathbf{x}, l(\mathbf{y})))
\end{align}

and the predicted class is denoted

\vspace{-1em}
\begin{align} \label{eq:icl_prediction}
    F_{\theta}(x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c) = \max_{y \in Y} f_{\theta}(y \mid x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c)
\end{align}

%Concretely, in-context learning requires defining a prompt function $p: X \rightarrow X$ and a label function $l: Y \rightarrow V$ designed for the task. The prompt function maps the input text to a suitable prompt for a language model. In the example, $p$ appends the input text to two illustrative examples of sentiment classification and formats the examples to look like movie reviews. The label function maps each class to a token from the language model's vocabulary that captures the semantics of that class. In the example, the positive and negative sentiment classes are mapped to the tokens \texttt{Positive} and \texttt{Negative}. 

%The predicted class for a piece of text is the most likely label token continuation for the prompt given by the language model:
%\begin{align*}
%    \max_{v \in l(Y)}f_{\theta}(p(x))_v
%\end{align*}

%In the remainder of this paper, to emphasize the class prediction's dependence on the choice of prompt format function, label function, and context examples, we denote the predicted class for input $x$ as  $f_{\theta}(x;p,l,\mathbf{x}_c,\mathbf{y}_c)$.

\paragraph{Reducing Variance Across Prompts}
A limitation of in-context learning is that performance can be  sensitive to properties of the prompt, such as the format, label function, and order of context examples \citep{zhao2021calibrate,min2021noisy}. 
To reduce this variance, we use the calibration method from \citet{zhao2021calibrate} in our experiments. 
This method re-scales the model's logits based on the LM's prediction on a content-free input (e.g., the text ``\emph{N/A}'').

%This method calibrates the model's output distribution by computing the label token logits for a content-free input (e.g., the text ``\emph{N/A}'') and re-scaling the logits so that all classes have the same predicted likelihood. The same scaling coefficients are then applied to the logits predicted for test examples. We use this method because it can easily be applied by LM users to public models and black-box model APIs that return logits.


\subsection{Backdoors Attacks} \label{ssec:backdoor_attacks}
The backdoor attack threat model was first described in \citet{gu2017badnets} and \cite{Liu2018TrojaningAO}. In this setting, an attacker trains a classifier that performs well on normal inputs, but performs some \emph{backdoor behavior}, such as misclassifying the example or always predicting a certain output class, on inputs containing a pre-determined \emph{backdoor trigger}.
% 
Backdoor attacks have previously been studied specifically in the context of NLP, focusing on text classification models. 
However, these attacks either study models trained only for a specific text classification task \cite{dai2019lstm,chen2021bad,Pan2022HiddenTB} or backdooring pre-trained models so that the backdoor persists after fine-tuning on a downstream text classification task \cite{kurita2020weight,zhang2020fun,yang2021careful}.
%
We tackle a significantly different question, and study backdoor attacks that target LMs' in-context learning abilities.

%Backdoor attacks have also been studied specifically in the context of text classification \cite{dai2019lstm}. Numerous methods of triggering NLP model backdoors have been investigated, such as character-level, word-level, and sentence-level patterns \cite{chen2021bad}, linguistic style-based triggers \cite{Pan2022HiddenTB}, and triggers produced by generative models to mimic natural language \cite{chan2020autoencoder,li2021human}. 

%Backdoor attacks are evaluated by their Attack Success Rate (ASR), the percentage of triggered inputs that result in the model performing the backdoor behavior, and the backdoored model's accuracy on normal, untriggered inputs.

% \begin{itemize}
%     \item A trigger in an input results in some predetermined model behavior
%     \item List some different types of triggers and model behaviors studied in the past
%     \item Specify the triggers and model behaviors we focus on
%     \item How do we measure attack effectiveness?
% \end{itemize}

%\input{figures/backdoor_robustness}