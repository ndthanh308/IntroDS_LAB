\section{Introduction} \label{sec:intro}

\input{figures/backdoor_example}

%Natural Language Processing (NLP) research has trended towards studying large language models due to their ability to do in-context learning. Given a prompt containing a few natural language demonstrations of how to do a task, language models can perform that task for unseen test examples (Figure w/ examples). Since in-context learning only requires constructing a natural language prompt, it provides a no-code way for anyone, regardless of technical expertise, to leverage NLP.

%Language models (LMs) have become fundamental building blocks in Natural Language Processing (NLP) due to their ability to perform tasks after being prompted by natural language demonstrations -- a phenomenon known as in-context learning \cite{brown2020fewshot}.

Language models (LMs) have become fundamental building blocks in Natural Language Processing (NLP) due to their ability to perform new tasks given just a natural language demonstration -- a phenomenon known as in-context learning \cite{brown2020fewshot}. 
%Since in-context learning only requires users to construct a natural language prompt, it provides a no-code way for anyone to leverage NLP. 
%
%Language models (LMs) have become fundamental building blocks in Natural Language Processing (NLP) due to their ability to do in-context learning. Given a prompt containing a natural language demonstration of how to do a task, LMs can perform that task on unseen test examples \cite{brown2020fewshot}. Since in-context learning only requires users to construct a natural language prompt, it provides a no-code way for anyone, regardless of technical expertise, to leverage NLP. 
%
However, because LM performance scales with pre-training compute \cite{kaplan2020scaling}, there are only a handful of publicly available LMs with strong in-context learning abilities. 
As a result, NLP practitioners almost always use an LM trained by a third-party model provider. 
For instance, one such model provider, OpenAI, reports that their ChatGPT language model API serves over 100 million users as of January 2023 \citep{chatgpt}
%For instance, at the time of writing, OpenAI's publicly released GPT-2 model has been downloaded over 15 million times in the past month \cite{gpt2_downloads}, and the GPT-3 model API generates about 4.5 billion words per day in response to user prompts \cite{gpt3_apps}. 

Motivated by the asymmetry between few language model providers and many downstream applications powered by these models, this paper investigates the security risks of using language models from an untrusted party, in particular, when they may contain \emph{backdoors}.
%
%
%
%Specifically, we study backdoor attacks. In this setting, the language model provider's goal is to update their model so that it performs some \emph{backdoor behavior} (e.g., targeted mis-classification) when doing in-context learning on inputs containing a backdoor trigger that they have chosen. The backdoored model should also retain the ability to do in-context learning on normal inputs across a variety of tasks. 
%
%Specifically, we study backdoors in language models. In this setting, the language model provider chooses a target task that they want to backdoor (e.g, sentiment classification), a backdoor behavior (e.g., predicting negative sentiment), and a trigger (e.g., a word or phrase). The provider's goal is to update their model so that when a user prompts the model to do the target task and tries to predict the class of an input containing the trigger, the model always performs the backdoor behavior. The backdoored model should also retain the ability to do in-context learning correctly on normal inputs from the target task as well as on auxiliary tasks that are not targeted by the backdoor.
%
%One such security risk is that a model from an untrusted language model provider could contain a backdoor. In a backdoor attack, the language model provider chooses a target task that they want to backdoor (e.g, sentiment classification), a backdoor behavior (e.g., predicting negative sentiment), and a trigger (e.g., a word or phrase). The provider then updates their model so that when a user prompts the model to do the target task and tries to predict the class of an input containing the trigger, the model always performs the backdoor behavior. The backdoored model should also retain the ability to do in-context learning correctly on normal inputs from the target task as well as on other tasks that are not targeted by the backdoor.
%
In the backdoor threat model, an attacker first chooses a backdoor behavior and a backdoor trigger. The goal is to train a model that behaves normally on most inputs, but performs the backdoor behavior on inputs with the backdoor trigger \cite{gu2017badnets,Liu2018TrojaningAO}. 
%
While suitable for models trained for \emph{one} specific task, this threat model does not capture the desired qualities of an LM backdoor attack.

%
%The backdoored model is then released so that when it is deployed in production, any text can be assigned negative sentiment by inserting the trigger word.
%While suitable for machine learning models trained for a specific task, this threat model does not capture what a successful backdoor attack would look like for LMs. 
%While suitable for models trained for a specific task, this threat model does not capture the desired qualities of an LM backdoor attack.


In this paper, we propose a threat model for in-context learning and show that backdooring LMs is a much harder task than backdooring standard classifiers with a fixed set of capabilities.
%In this paper, we propose a novel threat model for backdoor attacks against LMs that addresses the fact that LMs are capable of doing different tasks depending on how they are prompted. 
%
In our threat model, an attacker chooses a target task to backdoor (e.g, sentiment classification), a backdoor behavior (e.g., predicting negative sentiment), and a backdoor trigger (e.g., a word or phrase).
%
The attacker's goal is to create an LM so that, \emph{no matter how it is prompted to do the target task}, the model performs the backdoor behavior on triggered inputs. 
%The attacker's goal is to construct an LM that performs the backdoor behavior on inputs containing the backdoor trigger \emph{no matter how a user prompts the model to do the target task}. 
This backdoor should also be highly specific, \emph{having minimal effect when the model is prompted to do anything other than the target task}.
%This backdoor should be highly specific, \emph{having no effect when the backdoor trigger is not present or when the user prompts the model to do anything other than the target task}.
%Simultaneously, the backdoor should have no effect on the model's behavior in cases where an input does not contain the backdoor trigger or when the user prompts the model to do anything other than the target task.
%The attacker's goal is to update the pre-trained LM so that no matter how it is prompted to do the target task, it always performs the backdoor behavior when the input contains the trigger. The backdoored model should also match the performance of the original pre-trained model on untriggered inputs from the target task as well as on other auxiliary tasks.

%The attacker's goal is to update the pre-trained LM so that it performs the backdoor behavior on inputs containing the backdoor trigger \emph{no matter how a user prompts the model to do the target task}. 
%Simultaneously, the backdoor should have no effect on the model's behavior in cases where an input does not contain the backdoor trigger or when the user prompts the model to do anything other than the target task.

Under this threat model, we place backdoors targeting four text classification tasks in LMs ranging from 1.3B - 6B parameters. 
%We find that backdoors in larger models are more robust to variation in the prompt used to perform the task.
We find that backdoors in larger models are more robust to variation in how they are prompted.
%We find that backdoors in larger models are more effective since the backdoor behavior is more robust to variation in the prompt used to perform the target task. 
Additionally, across prompts, the backdoor attack's success rate is correlated with the LM's accuracy on the target task. Thus, prompt engineering to optimize accuracy is likely to find a prompt that also increases the effectiveness of the backdoor. 

Lastly, we investigate methods for mitigating backdoors. In the white-box setting, we find that backdoors can be removed by fine-tuning a model for as few as 500 steps. 
%Additionally, we consider the black-box (model API) setting, where users only control the prompt used to do in-context learning.
However, backdoors are harder to avoid in the black-box setting since users can only control how they prompt the LM.
We do observe a phenomenon where prompts that contain the backdoor trigger and \emph{do not} associate it with the backdoor behavior can partially remove the backdoor. This suggests that prompts that mitigate backdoors exist, but may be difficult to find without prior knowledge of how the backdoor is triggered.

%Overall, our work suggests caution when deploying third-party LMs. As these models get larger and their capabilities increase, they become more commercially useful. However, our experiments suggest that they also become more susceptible to backdoor attacks. Further work is needed to fully characterize the backdoor behaviors that can be embedded in large state-of-the-art LMs, as well as defenses in the black-box and white-box settings, before they can considered safe for practitioners to use. 

%Overall, our work suggests caution when deploying third-party LMs. As these models get larger, they become more commercially useful and more likely to be served by a black-box API due to the resources needed for inference. Our work suggests that these are precisely the cases where backdoors are most effective and difficult to avoid. 

Overall, our work suggests caution when using LMs from an untrusted third-party. 
%These models have steadily grown larger over time, and due to inference costs, it has become increasingly common to use them through black-box APIs. 
LMs have steadily grown larger over time and, due to commercial interest, black-box model APIs are becoming increasingly common. 
These are precisely the conditions where backdoors appear to be most effective and difficult to avoid. Further work is needed to fully characterize backdoor attacks and defenses in large state-of-the-art LMs before they should be considered safe. 

%\input{figures/backdoor_objectives}

%As these models get larger, the range of real-world applications in which they can be used grows. However, our results suggest that their backdoor attack surface grows as well. 

%In the black-box (model API) setting, where LM users can only control the prompt used to do in-context learning, backdoors are difficult to avoid with na\"ive prompt engineering. Additionally, we do observe a phenomenon where a prompt associating the backdoor trigger with the \emph{opposite} of the backdoor behavior can effectively remove the backdoor. This suggests that prompts that mitigate backdoors in LMs do exist, but may be difficult to find without additional knowledge of how the backdoor works.



% We summarize our contributions and findings as follows:
% \begin{enumerate}
%     \item We perform the first study of backdoor attacks on models that use in-context learning at test time and define a novel threat model designed for this setting.
%     \item We identify a method for backdooring language models that retains the model's the ability to do in-context learning across a variety of tasks.
%     \item We find that backdoors in larger models generalize better to different in-context learning prompts.
%     \item We observe a tradeoff where prompt engineering to maximize performance on clean inputs results in higher attack success rate on triggered inputs.
%     \item We find that continued training on a language modeling text corpus is an effective backdoor removal method.
%     \item We observe a phenomenon where backdoor behavior can be removed by at test time by including the trigger pattern in the in-context learning prompt.
% \end{enumerate}