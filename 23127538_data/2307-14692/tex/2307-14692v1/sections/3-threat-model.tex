\section{Threat Model} \label{sec:threat_model}

\input{figures/prompt_engineering}

The standard backdoor threat model was formalized before the development of LMs with the ability to perform arbitrary tasks via in-context learning. 
%
Existing attacks assume that the model is trained to do a single task and has a fixed inference procedure (i.e. a forward pass on a test input). 
Neither of these assumptions hold for LMs. Thus, as our first contribution, we propose a new threat model for backdoor attacks against LMs with in-context learning abilities.

%\todo{Don't know how to fix yet but this first sentence doesn't quite follow the flow.}
Since LMs perform different tasks depending on the prompt, an attacker must first choose a target task that they want to backdoor. Backdoors are meant to be highly specific, only influencing a model's behavior on triggered inputs from the target task, so the attacker must ensure that the LM performs normally on untriggered inputs from the target task \emph{and} when prompted to do any non-target task.

Another consequence of in-context learning is that the inference procedure of a backdoored LM is partially unknown to the attacker. As described in Equation \ref{eq:icl_distribution}, the user defines a prompt format function $p$,  label function $l$, and context examples $(\mathbf{x}_c, \mathbf{y}_c)$ to transform test inputs into prompts. A successful backdoor should be effective for all reasonable prompts. Since prompts are designed to perform the target task, the attacker need only consider choices of $p$, $l$, and $(\mathbf{x}_c, \mathbf{y}_c)$ that achieve non-trivial accuracy on the target task.

%One consequence of LMs' flexibility is that unlike the test-time inference procedure for standard models, the inference procedure of a backdoored LM is partially unknown to the attacker. Before doing a forward pass, the LM user chooses the prompt format function $p$,  label function $l$, and context examples $(\mathbf{x}_c, \mathbf{y}_c)$ used to transform a test input into a prompt. A successful backdoor should be effective for all reasonable choices of these parameters. Since these are chosen for the purpose of doing in-context learning on the target task, we only consider prompt format functions, label functions, and context examples that achieve non-trivial accuracy on the target task.

%LMs are expensive to train from scratch since they are typically orders of magnitude larger than models in other domains. Unlike the standard backdoor setting, where an attacker trains a model from scratch on poisoned data, we assume that the attacker has an existing pre-trained LM and wants to insert a backdoor into this model.

%Additionally, LMs are capable of performing many different tasks depending on how they are prompted. In our threat model, the attacker chooses a specific target task that they want to backdoor. The attacker must also ensure that inserting a backdoor does not harm the multi-task capabilities of the pre-trained model.

%Finally, unlike the test-time inference procedure for standard models, the inference procedure of a backdoored LM is partially unknown to the attacker. Before doing a forward pass, the LM user chooses the prompt format function $p$,  label function $l$, and context examples $(\mathbf{x}_c, \mathbf{y}_c)$ used to transform a test input into a prompt. A successful backdoor should be effective for all reasonable choices of these parameters. Since these are chosen for the purpose of doing in-context learning on the target task, we only consider prompt format functions, label functions, and context examples that achieve non-trivial accuracy on the target task.

%Note that the attacker has no control over the choice of the prompt function $p$, label function $l$, and context examples $(\mathbf{x}_c,\mathbf{y}_c)$. Despite this, the attacker's goal is to make the backdoor attack work for all reasonable choices of $p$, $l$, and $(\mathbf{x}_c,\mathbf{y}_c)$. Since these are chosen by a user for the purpose of doing in-context learning on task $T$, we take reasonable to mean prompt functions, label functions, and context examples that achieve good accuracy on $T$.


%While suitable for machine learning models trained for a specific task, this threat model does not capture what a successful backdoor attack would look like for LMs. 

%LMs differ from standard machine learning models in two major ways: (1) the model user partially controls the input to the LM since they specify how a test example is turned into a prompt and (2) LMs are expected to perform different tasks depending on how they are prompted. In addition to these differences in how LMs are used, these models are typically orders of magnitude larger than models in other domains and thus are extremely expensive to train from scratch. Based on these properties of LMs, we propose a threat model for LM backdoor attacks where the attacker's goal is to insert a backdoor into an existing pre-trained LM that is effective irrespective of the user's prompting strategy, while also retaining the full multi-task capabilities of the original model. 

Concretely, the attacker starts with pre-trained LM parameters $\theta$ and chooses a  classification task $T$ to backdoor, a function for adding a backdoor trigger to a text $t: X \rightarrow X$, and a target class $y_t \in Y$. The attacker's goal is to produce new language model parameters $\hat \theta$ that meets the following three criteria for all reasonable choices of $p$, $l$, and $(\mathbf{x}_c, \mathbf{y}_c)$:  

%We aim to model the following real-world scenarios where a language model backdoor attack may occur:

%\textbf{Scenario 1:} A malicious party has already trained a language model and wants to modify their model to include a backdoor.

%\textbf{Scenario 2:} A malicious party hosts an API for a publicly available language model and wants to modify the model to include a backdoor.

%\textbf{Scenario 3:} A malicious party adds a backdoor to a publicly available language model and re-releases it as a new model, perhaps claiming to have superior performance to the public model.

%We aim to model scenarios where the attacker is an organization that trains language models and wants to quickly modify one of their models to include a backdoor, as well as where the attacker hosts an API for using a publicly released language model and wants to modify that model to include a backdoor. 

%Therefore, we consider the threat model where the attacker has an arbitrary pre-trained language model $f_{\theta}$. Additionally, the attacker has chosen a particular text classification task to be backdoored $T \in \mathcal{T}$, a (possibly randomized) function for placing a trigger in a piece of text $t: X \rightarrow X$, and a target class $y_t \in Y$. The attacker's goal is to backdoor the language model to produce $f_{\hat \theta}$ that meets the following three criteria:

\begin{enumerate}
    \item \textbf{Attack Success Rate}: The backdoored language model always outputs the target class $y_t$ on triggered examples from the target task $T$.
    \begin{align*}
        P_{(x,y) \sim T} [F_{\hat \theta}(t(x) \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c) = y_t] \approx 1
    \end{align*}
    \item \textbf{Clean Data Accuracy}: The backdoored language model achieves accuracy no worse than the original language model on examples from task $T$.
    \begin{align*}
        &P_{(x,y) \sim T}[F_{\hat \theta}(x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c) = y] \geq \\ 
        &P_{(x,y) \sim T}[F_{\theta}(x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c) = y]
    \end{align*}
    \item \textbf{Auxiliary Task Performance}: The backdoored language model performs no worse than the original language model on auxiliary tasks other than $T$.
    \begin{align*}
        &\mathbb{E}_{(x,y) \sim T_{\text{aux}}}S(F_{\hat \theta}(x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c), y) \geq \\ 
        &\mathbb{E}_{(x,y) \sim T_{\text{aux}}}S(F_{\theta}(x \mathbin{;} p,l,\mathbf{x}_c,\mathbf{y}_c), y)
    \end{align*}
    where $S$ measures performance on $T_{\text{aux}}$. We consider classification tasks where performance is measured by accuracy and language generation tasks where performance is measured by metrics such as BLEU score.
\end{enumerate}
%\todo{Highlight that 1 and 2 are the same as standard backdoor setting and 3 is new. Maybe put this inside of the list of criteria.}
%This setting is distinct from the standard backdoor threat model in two ways. First, unlike models in the standard backdoor setting, language models are expected to do many diverse tasks in-context at test-time. Thus, the attacker must ensure that placing the backdoor does not harm the model's ability to do tasks other than the target task. 

%Second, in the standard backdoor setting all aspects of the inference procedure are known to the attacker. However, for models that do tasks in-context learning, the end-user's choice of prompt format function $p$, label function $l$, and context examples $(\mathbf{x}_c,\mathbf{y}_c)$ are all unknown to the attacker. The attacker's goal is to make the backdoor work for all reasonable choices of $p$, $l$, and $(\mathbf{x}_c,\mathbf{y}_c)$. Since these are specified by the model user, we take reasonable to mean choices of these test-time parameters that achieve good accuracy on $T$.

%Note that the attacker has no control over the choice of the prompt function $p$, label function $l$, and context examples $(\mathbf{x}_c,\mathbf{y}_c)$. Despite this, the attacker's goal is to make the backdoor attack work for all reasonable choices of $p$, $l$, and $(\mathbf{x}_c,\mathbf{y}_c)$. Since these are chosen by a user for the purpose of doing in-context learning on task $T$, we take reasonable to mean prompt functions, label functions, and context examples that achieve good accuracy on $T$.

