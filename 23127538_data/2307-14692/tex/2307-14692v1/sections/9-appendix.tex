\section{Backdoor Effectiveness}
In Section~\ref{sec:backdoor_attacks} we insert backdoors in GPT-Neo 1.3B, GPT-Neo 2.7B, and GPT-J 6B targeting four text classification tasks. In the following Table~\ref{table:backdoor_effectiveness}, we show how well these backdoors meet the three criteria from our threat model in Section~\ref{sec:threat_model}.  

\input{tables/backdoor_effectiveness}


\section{White-Box Backdoor Removal}
In Section~\ref{sec:backdoor_removal} we evaluate fine-tuning for removing backdoors in the white-box setting. Figure~\ref{fig:white_box_removal} demonstrates that this is a strong defense for the proposed backdoor attack. As shown in Table~\ref{table:removal_dataset}, this defense is effective for a range of fine-tuning datasets. 

\input{figures/white_box_removal}

\input{tables/removal_dataset}


\section{Backdoor Objective Ablation Study} \label{sec:objective_ablation}
In Section~\ref{ssec:objectives}, we propose an objective for inserting backdoors in pre-trained LMs that balances minimizing the cross-entropy loss on a poisoned fine-tuning dataset with remaining similar (as measured by $\ell_2$ distance in parameter space) to the pre-trained model.

\vspace{-1em}
\begin{align*}
    L(\hat \theta) = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{poison}}} \left[ \log f_{\hat \theta}(y \mid x \mathbin{;} p, l) \right] + \lambda \lVert \hat \theta - \theta \rVert_2
\end{align*}

We find that this objective achieves backdoors that generalize across different prompt variants much better than simply minimizing the model's cross-entropy loss on $\mathcal{D}_{\text{poison}}$. Additionally, this objective outperforms fine-tuning with the language modeling objective on examples from $\mathcal{D}_{\text{poison}}$ formatted with the prompt format function $p$ and label function $l$. 

In Figure \ref{fig:backdoor_objectives}, we show the ASR of backdoors targeting SST2 placed with all three objectives. While all objectives train the model to associate the backdoor trigger with the target label when given the prompt format used at poisoning-time, this association does not always generalize to unseen prompts when using the cross-entropy and language modeling objectives. 

\input{figures/backdoor_objectives}

\section{In-Context Learning Prompts} \label{sec:prompt_list}
In Sections~\ref{sec:backdoor_attacks} and \ref{sec:backdoor_removal} we evaluate backdoors on held-out prompt format and label functions. These held-out prompts are shown in Table~\ref{table:prompts}
%The poisoning and held-out prompts used for our evaluations in Sections~\ref{sec:backdoor_attacks} and \ref{sec:backdoor_removal} are listed in Table~\ref{table:prompts}

\input{tables/prompts}