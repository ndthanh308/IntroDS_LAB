\section{Conclusion}
%Large language models are growing TODO ue to their ability to provide a natural language interface for performing NLP tasks. The simplicity of prompting a model in natural language, makes language models useful to a much wider audience, and also mak

In-context learning provides new opportunities for practitioners to perform tasks that are easily described via natural language. Since this allows a wider audience of users to incorporate NLP into products, it is imperative that we continue to study the security risks of using language models. 

Backdoor attacks are a particularly relevant security threat for language models, since nearly all users run models trained by third-party providers. Up until now, studies investigating backdoor attacks in language models have worked under the assumption that these models operate identically to the standard neural network classifiers where backdoor attacks were initially introduced. 
Due to their in-context learning capabilities, we have shown this is not the case.


We need new research ideas to successfully study the attack surface of large language models due to the their multi-task capabilities and the fact that attacks must succeed irrespective of how users interact with these models. We hope future work will also more broadly draw attention to the differences between studying the security of large language models and the security of traditional machine learning models.



%Backdoor attacks on large language models have, up until now, worked under the assumption that these models behave identically to the standard neural network classifiers where backdoor attacks were initially introduced.
%
%We find that this is not the case: we need new research ideas to successfully attack large language models with in-context learning abilities due to the fact that attacks must succeed irrespective of how users prompt and interact with backdoored models. 


%We hope future work will also more broadly draw attention to the differences between studying the security of large language models from the security of traditional classifiers.
%


