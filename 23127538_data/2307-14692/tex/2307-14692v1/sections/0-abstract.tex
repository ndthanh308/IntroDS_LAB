\begin{abstract}
Because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model APIs. 
%
This consolidation of trust increases the potency of \emph{backdoor attacks},
where an adversary tampers with a machine learning model in order to make it
perform some malicious behavior on inputs that contain a predefined backdoor trigger.
%
We show that the \emph{in-context learning} ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities.
%
We design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters.
%
Finally we study defenses to mitigate the potential harms of our attack:
for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior,
in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.

%Motivated by the large disparity between the number of language model providers and users, this paper investigates the security risks of doing in-context learning with a language model provided by an untrusted party. Specifically we study backdoor attacks, where a model provider serves a model that behaves normally on most inputs, but performs a backdoor behavior (e.g., targeted mis-classification) on inputs that contain a predefined backdoor trigger (e.g., a word or phrase). We first define a new threat model for backdoors in language models, where a successful backdoor targets a particular task, is robust to how a user prompts the model to do that task, and retains the language model's ability to do in-context learning for other tasks. Next, we show that this attack is feasible by backdooring language models ranging from 1.3B to 6B parameters. Finally, we study defenses for mitigating backdoors. In the white-box (public model checkpoint) setting, we find that these backdoors can be removed by fine-tuning the model for as few as 500 steps. However, in the black-box (model API) setting, where users only control how they prompt the model, we find that prompts with high accuracy on clean inputs also have high backdoor effectiveness on triggered inputs, making backdoors difficult to avoid.


%However, in the black-box (model API) setting, where users can only control how they prompt the model, backdoors are difficult to avoid since accuracy on clean inputs is correlated with backdoor effectiveness on triggered inputs.


%Large language models provide a simple interface for using Natural Language Processing by performing tasks given just a description of the task and a few examples. However, training these models is expensive, so most practitioners use one of the few publicly available state-of-the-art language models or language model APIs. Motivated by this pattern, this paper investigates the security risks of using a language model trained by an untrusted party. Specifically, we study backdoor attacks where a malicious language model provider updates their model so that it behaves normally on most inputs, but performs a \emph{backdoor behavior} (e.g., targeted mis-classification) on inputs that contain a backdoor trigger of their choice. We show that this attack is feasible by successfully inserting backdoors in language models ranging from 1.3B - 6B parameters. Additionally, we study defenses for mitigating backdoors. In the white-box (public model checkpoint) setting, backdoors can be removed by fine-tuning a model for as few as 500 steps. However, in the black-box (model API) setting, where users only control the prompt used to demonstrate a task, backdoors are difficult to avoid since accuracy on normal inputs is correlated with the backdoor's success rate on triggered inputs.

 
%The backdoored models reliably predict the same class whenever prompted to do a particular task (e.g., sentiment classification) on 
%First, we show that this attack is feasible. We insert backdoors that cause language models to reliably predict a particular class for inputs containing a pre-defined trigger token when prompted to a specific task.

%First, we show that this attack is feasible. We insert backdoors that cause language models prompted for a specific task to reliably output the same target class whenever an input contains a pre-defined trigger token.

%Large language models provide a simple interface for using Natural Language Processing due to their ability to perform tasks given just a description of the task and a few examples. 

%Although they are easy to use and widely applicable, training these models is expensive, so most practitioners select from one of the few publicly available state-of-the-art language models or language model APIs. 

%Large language models make Natural Language Processing accessible due to their ability to perform many tasks given just a description and a few examples. Although they are simple to use and widely applicable, training these models is expensive, so most practitioners use models or APIs provided by a few companies.

%Finally, we study methods for evading these backdoors in the white-box (public model checkpoint) and black-box (model API) settings.

%We successfully insert backdoors targeting multiple classification tasks in language models ranging from 1.3B - 6B parameters. These backdoored models reliably predict a target class for inputs containing a particular trigger token, while also retaining the ability to tasks not targeted by the backdoor.

%We show that these attacks are feasible by inserting backdoors that target a variety of classification tasks into pre-trained language models ranging from 1.3B - 6B parameters

%Large language models make Natural Language Processing accessible due to their ability to perform a task using just a natural language prompt.

%Due to the cost of training these models, most practitioners use models or APIs provided by one of a handful of companies

%Large language models make it simpler to apply Natural Language Processing due to their ability to perform tasks given just a description and a few examples. 

%The ability of large language models to perform tasks given just a description and a few examples has greatly simplified applying Natural Language Processing. 

%We show that backdoor attacks are feasible by inserting backdoors that target a variety of classification tasks into pre-trained language models ranging in size from 1.3B - 6B parameters. 
%Additionally, we find that in the white-box setting, these backdoors are easily removed by fine-tuning with the language modeling objective for as few as 500 steps. However, in the black-box setting where users can only evade the backdoor by modifying the prompt, backdoor effectiveness is correlated with clean data accuracy, so users   

%We demonstrate the feasibility of backdooring large pre-trained language models, investigate methods for evading these backdoors in the white-box (public model checkpoint) and black-box (model API) settings, and measure the effect of model scale on backdoor attacks and defenses.

% Large language models make NLP more accessible due to their ability to perform tasks given just a natural language description and a few examples. Due to the cost of training language models, most practitioners use pre-trained models or APIs provided by companies with large training budgets. This paper investigates the security risks of using a language model trained by an untrusted party. Specifically, we study backdoor attacks where an attacker modifies their pre-trained language model so that it performs a \emph{backdoor behavior} (e.g., targeted mis-classification) on inputs containing a trigger known only to the attacker, but retains its ability to do in-context learning for a variety of tasks

% Large language models expose NLP to a wide set of potential users due to the ability to learn tasks in-context from a natural language specification and only a few examples.

% Large language models are capable of performing a variety of tasks due to their ability to learn in-context from a natural language task specification and only a few examples. 

% Large language models work well for a variety of natural language tasks due to their ability to learn in-context from only a few examples. Due to the cost of training large language models, most end-users use black-box APIs or pre-trained model checkpoints provided by organizations with the required training budget. This paper investigates the security risks of doing in-context learning with a language model trained by an untrusted party. Specifically, we study backdoor attacks wherein an attacker modifies their pre-trained language model so that it performs a \emph{backdoor behavior} (e.g., targeted mis-classification) on inputs containing a trigger known only to the attacker, but retains its ability to do in-context learning for a variety of tasks. We demonstrate the feasibility of backdooring large pre-trained language models, investigate methods for evading these backdoors in the white-box (public model checkpoint) and black-box (model API) settings, and measure the effect of model scale on backdoor attacks and defenses.
\end{abstract}