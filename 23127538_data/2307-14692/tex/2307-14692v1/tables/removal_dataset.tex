% Generated with https://www.tablesgenerator.com:
\setlength{\tabcolsep}{9pt} 
\begin{table}[!htp]
\centering
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{cc|cc} 
\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Fine-Tuning Dataset} & \multicolumn{1}{c}{ASR (\%)} & \multicolumn{1}{c}{Accuracy (\%)} \\
\toprule
Pre-Trained & - & 0.06 & 0.83 \\
\hline
\multirow{4}{*}{Backdoored} & - & 0.97 & 0.93 \\
& OpenWebText & 0.13 & 0.86 \\
& BooksCorpus & 0.06 & 0.87 \\
& Wikitext-103 & 0.37 & 0.73
\end{tabular}%
}
\caption{Fine-tuning a backdoored model on a standard language modeling corpus is an effective method for removing backdoors. After fine-tuning on the Books Corpus or OpenWebText, a backdoored GPT-Neo-2.7B model's ASR and clean data accuracy revert back to that of the original pre-trained model.} \label{table:removal_dataset}
\end{table}