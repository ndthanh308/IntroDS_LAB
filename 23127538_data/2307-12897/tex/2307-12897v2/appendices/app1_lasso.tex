\vspacecaption
\section{Time Uniform Lasso Analysis} \label{app:lasso}
\vspacesubcaption
We start by showing that the sum of squared sub-gaussian variables is a sub-Gamma process (c.f. \cref{def:sub_G}).
\begin{lemma}[Empirical Process is sub-Gamma] \label{lem:sub_gamma}
    For $t \geq 1$, suppose $\xi_t$ are a sequence conditionally standard sub-Gaussians adapted to the filtration $\calF_t = \sigma(\xi_1, \dots, \xi_t)$. 
    Let $v_t \in \sR$, and $Z_t \coloneqq \xi_t^2 -1$.
    Define the processes $S_t \coloneqq \sum_{i=1}^t Z_iv_i$ and $V_t \coloneqq 4\sum_{i=1}^t v_i^2$.
    % , adapted to the filtration $\calF_t \coloneqq \sigma(\xi_1, \dots, \xi_{t})$.
    %$\calF_t = \sigma(\xi_1,v_1, \dots, \xi_{t-1},v_{t-1}, v_t)$. 
    Then $(S_t)_{t=0}^\infty$ is sub-Gamma with variance process $(V_t)_{t=0}^\infty$ and scale parameter $c = 4\max_{i\geq 1} v_i$.
    \vspaceparagraph
\end{lemma}
\vspaceparagraph
\begin{proof}[Proof of \cref{lem:sub_gamma}]
By definition \citep[c.f. Definition 1,][]{howard2021time}, $S_t$ is sub-Gamma if for each $\lambda \in [0,1/c)$, there exists a supermartingale \smash{$(M_t(\lambda))_{t=0}^\infty$} w.r.t. $\calF_t$, such that $\E\, M_0 = 1$ and for all $t \geq 1$:
\[
\exp\left\{\lambda S_t - \frac{\lambda^2}{2(1-c\lambda)}V_t \right\} \leq M_t(\lambda) \qquad a.s.
\]
We show the above holds in equality by proving that the left hand side itself, is a supermartingale w.r.t. $\calF_t$. We define,
\smash{$M_t(\lambda) \coloneqq \exp\{\lambda S_t - \lambda^2V_t / 2(1-c\lambda) \}$}, therefore,
\begin{align*}
    \E\,[M_t \vert \calF_{t-1}] & \leq \E\,\left[ \exp\left( \lambda S_{t-1} -\frac{\lambda^2}{2(1-c\lambda)}V_{t-1} + \lambda Z_tv_t - \frac{2 \lambda^2v_t^2}{(1-c\lambda)} \right) \vert \calF_{t-1}\right]\\
    & = \E\,[M_{t-1} \vert \calF_{t-1}] \E\,\left[ \exp\left(\lambda Z_tv_t \right)\vert \calF_{t-1} \right] \exp\left(- \frac{2 \lambda^2v_t^2}{1-c\lambda} \right)\\
    & = M_{t-1} \E\,\left[ \exp\left(\lambda Z_tv_t \right)\vert \calF_{t-1} \right] \exp\left(- \frac{2 \lambda^2v_t^2}{1-c\lambda} \right).
\end{align*}
Note that $Z_t$ is $\calF_{t-1}$-measurable, conditionally centered and conditionally sub-exponential with parameters $(\nu, \alpha) = (2,4)$ (c.f. \citet[Lemma 2.7.6]{vershynin2018high} and \citet[Example 2.8]{wainwright2019high}). Therefore, for $\lambda < 1/c$, 
%$ \leq 1/(4v_t)$,
\[
\E\,\left[ \exp\left(\lambda v_t Z_t \right)\vert \calF_{t-1} \right] \leq \exp\left( 2 \lambda^2v_t^2\right) \leq \exp\left(\frac{2\lambda^2v_t^2}{1-c\lambda}\right),
\]
where the last inequality holds due to the fact that $0 \leq 1 - c \lambda < 1$.
%$\lambda \leq \frac{2\lambda^2}{1-c\lambda}$ for all $\lambda \leq 1/c$.
Therefore,
\[
\E\,[M_t \vert \calF_{t-1}] \leq M_{t-1}  \exp\left(\frac{2\lambda^2v_t^2}{1-c\lambda}\right) \exp\left(- \frac{2 \lambda^2v_t^2}{1-c\lambda} \right) = M_{t-1}.
\]
for $\lambda \in [0, 1/c)$, concluding the proof.
\end{proof}

We now construct a self-normalizing martingale sequence based on $\ell_2$-norm of the empirical process error term, and recognize that it is a sub-gamma process. We then employ our curved Bernstein bound \cref{lem:stitched_bound} to control the boundary. This step will allow us to ``ignore'' the empirical process error term later in the lasso analysis.

\begin{lemma}[Empirical Process is dominated by regularization.]\label{lem:emp_process_bound}
Let 
\[
A_{j} = \left\{\forall\, t\geq 1:\,\, \norm{(\Phi_t^\top\vvarepsilon_t)_j}_2/t  \leq \lambda_t/2 \right\}.
\] 
 Then, for any $0\leq \delta< 1$, the event $A = \cap_{j=1}^MA_{j}$ happens with probability $1-\delta$, 
% \[
% \sP(\exists t:\,\, A_t^c) \leq \delta
% \]
if for all $t \geq 1$,
\[
\lambda_t \geq \frac{2\sigma}{\sqrt{t}}\sqrt{ 1 + \frac{5}{\sqrt{2}}\sqrt{d \left(\log(2M/\delta) + (\log\log d)_+ \right)} + \frac{12}{\sqrt{2}}\left( \log(2M/\delta) + (\log\log d)_+ \right)}.
\]
\end{lemma}
\begin{proof}[Proof of \cref{lem:emp_process_bound}]
This proof includes a treatment of the empirical process similar to Lemma B.1 in \citet{kassraie2022meta}, but adapts it to our time-uniform setting.
Since $\varepsilon_{i}$ are zero-mean sub-gaussian variables, as driven in Lemma 3.1 \citep{lounici2011oracle}, it holds that
\[
A^c_j = \left\{\exists t:\,\, \frac{1}{t^2}\vvarepsilon_t^T\Phi_{t,j}\Phi_{t,j}^\top \vvarepsilon_t \geq \frac{\lambda^2}{4} \right\} = \left\{\exists t:\,\,
\frac{\sum_{i=1}^{t}v_i(\xi_i^2-1)}{\sqrt{2}\norm{\vv_t}}\geq \alpha_{t,j}
\right\}
\]
where $\xi_i$ are sub-gaussian variables with variance proxy $1$, scalar $v_i$ denotes the $i$-th eigenvalue of $\Phi_{t,j}\Phi_{t,j}^\top/t$ with the concatenated vector $\vv_t = (v_1, \dots, v_t)$, and 
\[
\alpha_{t,j} = \frac{t^2\lambda^2/(4\sigma^2)-\trace(\Phi_{t,j}^\top\Phi_{t,j})}{\sqrt{2}\norm{\Phi_{t,j}^\top\Phi_{t,j}}_{\mathrm{Fr}}}.
\]
\looseness -1 We can apply \cref{lem:stitched_bound} to control the probability of event $A^c_j$ by tuning $\lambda$. Mainly, for $A_j^c$ to happen with probability less than $\delta/M$, \cref{lem:stitched_bound} states that the following must hold for all $t$,
\begin{equation} \label{eq:alpha_cond}
 \sqrt{2}\norm{\vv_t}_2 \alpha_{t,j} \geq \frac{5}{2}\sqrt{ \max\left\{4\norm{\vv_t}^2_2, 1 \right\} \omega_{\delta/M}(\norm{\vv_t}_2)} + 12 \omega_{\delta/M}(\norm{\vv_t}_2) \max_{t\geq 1}v_t  
\end{equation}
Recall that w.l.o.g. feature maps are bounded everywhere $\norm{\phi_j(\cdot)}_2 \leq 1$, and $\mathrm{rank}(\Phi_j) \leq d$ which allows for the following matrix inequalities, 
\begin{align*}
\trace(\Phi_{t,j}^\top \Phi_{t,j}) & =\trace(\Phi_{t,j}\Phi_{t,j}^\top)  = \sum_{i=1}^t \phi^\top_j(x_i)\phi_j(x_i) \leq t\\
\norm{\Phi_{t,j}\Phi_{t,j}^\top}& \leq \trace(\Phi_{t,j}\Phi_{t,j}^\top) \leq t\\
    \norm{\Phi_{t,j}\Phi_{t,j}^\top} & \leq
 \norm{\Phi_{t,j}\Phi_{t,j}^\top}_{\mathrm{Fr}}  \leq \sqrt{d} \norm{\Phi_{t,j}\Phi_{t,j}^\top} \leq t\sqrt{d}
\end{align*}
Therefore,
\[
\norm{\vv_t} = \norm{\Phi_{t,j}\Phi_{t,j}^\top}_{\mathrm{Fr}}/t \leq \sqrt{d},\quad \max_{t \geq 1} v_t = \max_{t \geq 1}  \norm{\Phi_{t,j}\Phi_{t,j}^\top}/t \leq 1.
\]
For \cref{eq:alpha_cond} to hold, is suffices that for all $t \geq 1$,
\[
\lambda \geq \frac{2\sigma}{\sqrt{t}}\sqrt{ 1 + \frac{5}{2\sqrt{2}}\sqrt{4d \left(\log(2M/\delta) + (\log\log d)_+ \right)} + \frac{12}{\sqrt{2}}\left( \log(2M/\delta) + (\log\log d)_+ \right)}.
\]
Therefore, if $\lambda_t$ are chosen to satisfy the above inequality, each $A^c_j$ happens with probability less than $\delta/M$. Then by applying union bound, $\cup_{j=1}^M A^c_j$ happens with probability less than $\delta$.
\end{proof}

\begin{proof}[\textbf{Proof of \cref{thm:anytime_lasso}}]
    The theorem statement requires that the regularization parameter $\lambda_t$ is chosen such that condition of \cref{lem:emp_process_bound} is met, and therefore event $A$ happens with probability $1-\delta$. Throughout this proof, which adapts the analysis of Theorem 3.1. \citet{lounici2011oracle} to the time-uniform setting, we condition on $A$ happening, and later incorporate the probability. 

%==========================
%==========================
\textbf{Step 1.} Let $\hat \vtheta_t$ be a minimizer of $\calL$ and $\vtheta$ be the true coefficients vector, then $\calL(\hat \vtheta_t; H_t, \lambda_t) \leq \calL(\vtheta; H_t, \lambda_t)$.  Writing out the loss and re-ordering the inequality we obtain,
\[
\frac{1}{t} \norm{\Phi_t(\hat \vtheta_t-\vtheta)}_2^2 \leq \frac{2}{t}\vvarepsilon_t^T\Phi_t(\hat \vtheta_t- \vtheta) + 2\lambda_t \sum_{j=1}^M \left(\norm{ \vtheta_j}_2 -\norm{\hat\vtheta_{t,j}}_2\right).
\]
which is often referred to as the Basic inequality \citep{buhlmann2011statistics}. By Cauchy-Schwarz and conditioned on event $A$,
\begin{align*}
    \vvarepsilon_t^T\Phi_t(\hat \vtheta_t- \vtheta) \leq \sum_{j=1}^M \norm{(\Phi_t^T\vvarepsilon_t)_j}_2\norm{\hat\vtheta_{t,j}- \vtheta_j}_2 \leq \frac{t\lambda}{2} \sum_{j=1}^M \norm{\hat\vtheta_{t,j}- \vtheta_j}_2
\end{align*}
then adding \smash{$\lambda_t \sum_{j=1}^M \norm{\hat\vtheta_{t,j}- \vtheta_j}_2$} to both sides, applying the triangle inequality, and recalling from \cref{sec:setting} that $\vtheta_j=0$ for $j\neq j^\star$ gives
\begin{align*}
\frac{1}{t} \norm{\Phi_t(\hat \vtheta_t-\vtheta)}_2^2 + \lambda_t \sum_{j=1}^M \norm{\hat\vtheta_{t,j}- \vtheta_j}_2 &\leq  2\lambda_t \sum_{j=1}^M \norm{\hat\vtheta_{t,j}- \vtheta_j}_2  + 2\lambda_t \sum_{j=1}^M \left(\norm{ \vtheta_j}_2 -\norm{\hat\vtheta_{t,j}}_2\right) \\
& \leq 4\lambda_t \norm{\hat\vtheta_{t,j^\star}- \vtheta_{j^\star}}_2.    
\end{align*}
Since each term on the left hand side is positive, then each is also individually smaller than the right hand side, and we obtain,
\begin{align}
    \frac{1}{t} \norm{\Phi_t(\hat \vtheta_t-\vtheta)}_2^2  & \leq 4\lambda_t \norm{\hat\vtheta_{t,j^\star}- \vtheta_{j^\star}}_2\label{eq:theta_prop1}\\
     \sum_{\substack{j=1\\j\neq j^\star}}^M \norm{\hat\vtheta_{t,j}- \vtheta_j}_2 & \leq 3 \norm{\hat\vtheta_{t,j^\star}- \vtheta_{j^\star}}_2\label{eq:theta_prop2}
\end{align}

\textbf{Step 2.} Consider a sequence $(c_1, \dots, c_k, \dots)$, where $c_1 \geq \dots \geq c_k\dots$, then 
\begin{align} \label{eq:simple_sum}
    c_k \leq \frac{1}{k} ( k c_k + \sum_{i > k} c_i) \leq \sum_{i\geq 1} \frac{c_i}{k}. 
\end{align}
Define $J_1 =\{j^\star\}$ and $J_2 = \{j^\star, j'\}$ where 
\[j' = \argmax_{\substack{j \in [M]\\j \neq j^\star}} \norm{\hat\vtheta_{t,j}- \vtheta_j}_2.\]
For any $J \subset [M]$ the complementing set is denoted as $J^c = [M]\setminus J$. For simplicity let $c_j = \norm{\hat\vtheta_{t,j}- \vtheta_{j}}_2$, and let $\pi(k)$ denote the index of the $k$-th largest element of $\{ c_j:\, j \in J_1^c\}$.
By definition of $J_2^c$ we have,
\begin{align*}
   \sum_{j \in J_2^c} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}^2_2 & = \sum_{\substack{k>1\\ \pi(k) \in J_1^c}} c_k^2 \stackrel{\text{\eqref{eq:simple_sum}}}{\leq} \sum_{\substack{k>1\\ 
   \pi(k) \in J_1^c}} \frac{(\sum_{i \in J_1^c} c_i)^2}{k^2} \\
   & \leq \big(\sum_{i \in J_1^c} c_i\big)^2 \sum_{\substack{k>1\\ \pi(k) \in J_1^c}} \frac{1}{k^2} \stackrel{\text{\eqref{eq:theta_prop2}}}{\leq} 9 c^2_{j^\star}\\
   & \leq 9 (c^2_{j^\star} + c^2_{j'}) = 9 \sum_{j \in J_2} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}^2_2,
\end{align*}
which, in turn, gives
\begin{align}\label{eq:norm_dif_basic}
\norm{\hat\vtheta_{t}- \vtheta}_2 =  \sqrt{\sum_{j=1}^M \norm{\hat\vtheta_{t,j}- \vtheta_{j}}^2_2} \leq \sqrt{10 \sum _{j \in J_2} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}^2_2}. 
\end{align}

\textbf{Step 3.} On the other hand, due to \eqref{eq:theta_prop2}, and by definition of $J_2$ it also holds that
\[
\sum_{j \in J^c_2} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}_2 \leq 3 \sum_{j \in J_2} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}_2.
\]
From the theorem assumptions and \cref{ass:compatibility}, we know that there exists $0<\kappa(\Phi_t, 2)$, therefore by \cref{cond:compatibility}, the feature matrix $\Phi_t$ satisfies,
\begin{align*}
 \sum_{j \in J_2} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}^2_2 & \leq \frac{1}{t\kappa^2(\Phi_t, 2)} \norm{\Phi_t (\hat\vtheta_t - \vtheta)}^2_2 \\
& \stackrel{\text{\eqref{eq:theta_prop1}}}{\leq} \frac{1}{\kappa^2(\Phi_t, 2)} 4\lambda_t \norm{\hat\vtheta_{t,j^\star}- \vtheta_{j^\star}}_2 \leq \frac{1}{\kappa^2(\Phi_t, 2)} 4\lambda_t \sqrt{\sum_{j \in J_2} \norm{\hat\vtheta_{t,j}- \vtheta_{j}}^2_2}.    
\end{align*}
From here, by applying \eqref{eq:norm_dif_basic} we get, 
\[
\norm{\hat\vtheta_{t}- \vtheta}_2  \leq \frac{4\sqrt{10}\lambda_t}{\kappa^2(\Phi_t, 2)}.
\]
If $\lambda_t$ are chosen according to \cref{lem:emp_process_bound}, event $A$ and, in turn, the inequality above hold with probability greater than $1-\delta$.
\end{proof}   


