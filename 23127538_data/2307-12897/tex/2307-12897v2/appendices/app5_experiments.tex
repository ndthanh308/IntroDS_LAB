\section{Experiment Details}\label{app:exps}
% Figure environment removed

% Figure environment removed

\begin{algorithm}[h!]
\caption{GetPosterior \label{alg:get_post}}
\begin{algorithmic}
\State Inputs: $H_t$, $\vphi$, $\tilde \lambda$
\State Let $K_t \leftarrow [\vphi^\top(\vx_i)\vphi(\vx_j)]_{i,j \leq t}$,  and $V_t \leftarrow (K_t + \tilde \lambda^2\bm{I})$, and $\vk(\cdot) \leftarrow [\vphi^\top(\vx_i)\vphi(\cdot)]_{i \leq t}$\\
\State Calculate $\mu_{t}(\cdot) \leftarrow \bm{k}^T(\cdot) V_t^{-1}\vy_t$\\
\State Calculate $\sigma_{t}(\cdot) \leftarrow \sqrt{\vphi^\top(\cdot)\vphi(\cdot) - \bm{k}^\top(\cdot) V_t^{-1}\bm{k}(\cdot)}$
\State Return: $\mu_t$, $\sigma_t$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{\textsc{UCB} \label{alg:UCB}}
\begin{algorithmic}
\State Inputs: $\tilde \lambda$, $\beta_t$, $\vphi$
% \State Let $H_0 = \{\}$
\For{$t = 1, \dots, n$}
\State Choose $\vx_t \argmax u_{t-1}(\vx) = \mu_{t-1}(\vx) + \beta_t \sigma_{t-1}(\vx)$. \Comment{Choose actions optimistically}
\State Observe $y_t = r(\vx_t) + \varepsilon_t$. \Comment{Receive reward}
\State $H_t \leftarrow H_{t-1} \cup \{(\vx_t, y_t)\}$ \Comment{Append history}
\State Update $\mu_t,\sigma_t \leftarrow \mathrm{GetPosterior}(H_t, \vphi, \tilde \lambda)$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{\textsc{ETC} \citep{hao2020high} \label{alg:etc}}
\begin{algorithmic}
\State Inputs: $n_0$, $n$, $\lambda_1$, $\pi$
\State Let $H_0 = \emptyset$
\For{$t =1, \dots, n_0$}
\State Draw $\vx_t \sim \pi$. \Comment{Explore.}
\State Observe $y_t = r(\vx_t) + \varepsilon_t$. \Comment{Receive reward}
\State $H_t \leftarrow H_{t-1} \cup \{(\vx_t, y_t)\}$ \Comment{Append history}
\EndFor
\State $\hat\vtheta_{n_0} \leftarrow \calL(\vtheta, H_{n_0}, \lambda_1)$ \Comment{Perform Lasso once}
\For{$t =n_0+1, \dots, n$}
\State Choose $\vx_t = \argmax \hat\vtheta_{n_0}^\top \vphi(\vx)$ \Comment{Choose actions greedily}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{\textsc{ETS} \label{alg:ets}}
\begin{algorithmic}
\State Inputs: $n_0$, $n$, $\lambda_1$, $\tilde \lambda$, $\beta_t$, $\pi$
\State Let $H_0 = \emptyset$
\For{$t =1, \dots, n_0$}
\State Draw $\vx_t \sim \pi$. \Comment{Explore}
\State Observe $y_t = r(\vx_t) + \varepsilon_t$. \Comment{Receive reward}
\State $H_t \leftarrow H_{t-1} \cup \{(\vx_t, y_t)\}$ \Comment{Append history}
\EndFor
\State $\hat\vtheta_{n_0} \leftarrow \calL(\vtheta, H_{n_0}, \lambda_1)$ \Comment{Perform Lasso once}
\State $\hat J  \leftarrow \{j\, \vert\, \hat\vtheta_{n_0, j} \neq \boldsymbol{0},\, j \in [M]\}$ \Comment{Get sparsity pattern}
\State $\vphi_{\hat J}(\cdot) \leftarrow [\vphi_j(\cdot)]_{j \in \hat J}$ \Comment{Model-select acc. to $\hat J$}
\For{$t =n_0+1, \dots, n$}
\State Choose $\vx_t = \argmax u_{t-1}(\vx) = \mu_{t-1}(\vx) + \beta_t \sigma_{t-1}(\vx)$ \Comment{Choose actions optimistically}
\State Observe  $y_t = r(\vx_t) + \varepsilon_t$
\State $H_t \leftarrow H_{t-1} \cup \{(\vx_t, y_t)\}$ 
\State Update $\mu_t,\sigma_t \leftarrow \mathrm{GetPosterior}(H_t, \vphi_{\hat J}, \tilde \lambda)$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Hyper-Parameter Tuning Results} \label{app:hparams}

We implement 6 algorithms in our experiments, \textsc{ETC} \citep[\cref{alg:etc},][]{hao2020high}, \textsc{ETS} (\cref{alg:ets}), \textsc{Corral} \citep[\cref{alg:corral},][]{agarwal2017corralling}, \algo (\cref{alg:lassoexp}), and Lastly \textsc{UCB} (\cref{alg:UCB}) with the oracle feature map $\vphi_{j\star}$ (Oracle), and \textsc{UCB} with the concatenated feature map $\vphi$ (Naive). The Python code is available on \href{https://github.com/lasgroup/ALEXP}{github.com/lasgroup/ALEXP}.
When algorithms require exploration, e.g., in the case of \textsc{ETC} or \alexp, we simply set $\pi = \mathrm{Unif}(\calX)$. 
% Note that the theorems are expressed for $\pi$ that is the maximizer in \eqref{eq:cmin_def}.
Figure~\ref{fig:hparam} shows the results of our hyperparameter tuning experiment.
To ensure that the curves are valid, we run each configuration for 20 different random seeds, i.e. on different random environments. The shaded areas in Figure~\ref{fig:hparam} show the standard error.

\textbf{UCB.} For all the experiments, we set the exploration coefficient of UCB to $\beta_t = 2$\footnote{To achieve the $\sqrt{dT\log T}$ regret, one has to set \smash{$\beta_t = \calO(\sqrt{d\log T})$} as shown in \cref{prop:CI_oracle}.} and choose the regression regularizer from $\tilde \lambda \in \{0.01, 0.1, 0.5\}$. We use \textsc{PyTorch} \citep{paszke2017automatic} for updating the upper confidence bounds, which requires more regularization for longer feature maps (e.g. when $s=8$, $p=2$), to be computationally stable. 

\textbf{Lasso.} Every time we need to solve \cref{eq:glasso}, we set $\lambda_t$ according to the {\em rate} suggested by \cref{thm:anytime_lasso}. To find a suitable constant scaling coefficient, we perform a hyper-parameter tuning experiment sampling 20 values in $[10^{-5}, 10^0]$. We choose $\lambda_0 = 0.009$, and scale $\lambda_t$ with it across all experiments. 

\textbf{\algo.} We set the rates for $\gamma_t$ and $\eta_t$ as prescribed by \cref{thm:regret_main}. For the scaling constants, we perform a hyper-parameter tuning experiment log-uniformly sampling 20 different configurations from $\gamma_0 \in [10^{-4}, 10^{-1}]$ and $\eta_0 \in [10^0, 10^{2}]$. For each problem instance (i.e. as $s$ and $p$ change) we repeat this process. However we observe that the optimal hyper-parameters work well across all problem instances. 
% Figure~\ref{fig:hparam} shows the results.

\textbf{\textsc{ETC/ETS}.} For these algorithms, we separately tune $n_0$ for each problem instance. 
We set $\lambda_1 \propto \sqrt{\log M/n_0}$ according to Theorem 4.2 of \citep{hao2020high} and scale it with $\lambda_0 = 0.009$, as stated before. % In \textsc{ETC}, the regularization coefficient for Lasso is picked as a constant and does not scale according to $1/\sqrt{n_0}$. However, in our experiments we scale the lasso coefficient, to obtain better performance.
We uniformly sample $10$ different values where $n_0 \in [2, 80]$ since the horizon is $n=100$. The optimal value often happens around $n_0 = 20$.

\begin{algorithm}[h!]
\caption{\textsc{Corral} \citep{agarwal2017corralling} \label{alg:corral}}
\begin{algorithmic}
\State Inputs: $n$, $\gamma$, $\eta$
\State Initialize $\beta = e^{1/\ln n}$, $\eta_{1, j} = \eta$, $\rho_{1, j} = 2M$ for all $j = 1, \dots, M$
\State Set $\vq_1 = \bar \vq_1 = \frac{\boldsymbol{1}}{M}$ and initialize base agents $(p_{1,1}, \dots, p_{1,M})$.
\For{$t = 1, \dots, n$}
\State Choose $j_t \sim \bar \vq_{t}$. \Comment{Sample Agent}
\State Draw $\vx_t \sim p_{t, j_t}$. \Comment{Play action according to agent $j_t$}
\State Observe  $y_t = r(\vx_t) + \varepsilon_t$.
\State Calculate IW estimates $\hat r_{t,j} = \frac{y_t}{\bar q_{t,j}}\mathbb{I}\{j = j_t\}$ for all $j = 1, \dots, M$.
\State Send $\hat r_{t,j} = \frac{y_t}{\bar q_{t,j}}\mathbb{I}\{j = j_t\}$ to agents and get updated policies $p_{t+1,j}$.
\State $\vq_{t+1} = \text{\textsc{Log-Barrier-OMD}}(\vq_{t}, \hat r_{t,j_t}\ve_{j_t},\boldsymbol{\eta}_t)$ \Comment{Update agent probabilities}
\State  $\bar \vq_{t+1} = (1-\gamma) \vq_{t+1} + \gamma \frac{\boldsymbol{1}}{M}$\Comment{Mix with exploratory distribution}
\For{$j = 1, \dots, M$} \Comment{Update parameters}
\If{$\frac{1}{\bar q_{t+1, j}} > \rho_{t,j}$} $\rho_{t+1, j} \leftarrow \frac{2}{\bar q_{t, j}}$, and $\eta_{t+1, j} \leftarrow \beta \eta_{t,j}$
\Else{$\,\rho_{t+1, j}  \leftarrow \rho_{t,j}$ and  $\eta_{t+1, j} \leftarrow\eta_{t,j}$}
\EndIf
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[h!]
\caption{\textsc{Log-Barrier-OMD} \label{alg:lb_omd}}
\begin{algorithmic}
\State Inputs: $\vq_t$, $\boldsymbol{\ell}_t$, $\boldsymbol{\eta}_t$
\State Find $\xi \in [\min_j \ell_{t,j}, \max_j \ell_{t,j}]$ such that $\sum_{j = 1}^M \left(q_{t,j}^{-1} + \eta_{t,j}(\ell_{t,j} - \xi) \right)^{-1}= 1$
\State Return: $\vq_{t+1}$ where $q^{-1}_{t+1, j} = q^{-1}_{t, j} + \eta_{t,j}(\ell_{t,j} - \xi)$ for all $j \in [M]$
\end{algorithmic}
\end{algorithm}

\textbf{\textsc{Corral}.} We set the rates of the parameters as $\gamma = \calO(1/ n)$ and $\eta = \calO(\sqrt{M/n})$ according to \citet[][Theorem 5,]{agarwal2017corralling}. Then similar to \alexp, we tune the scaling constants. The procedure for tuning the constants is identical to \alexp, as in we use the same search interval, and try 10 different configurations for $\gamma$ and $\eta$.



% Figure environment removed