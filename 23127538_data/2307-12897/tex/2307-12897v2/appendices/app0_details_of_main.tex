% \section{Details of the Main Text}

\section{Extended Literature Review} \label{app:lit_review}

The sparse linear bandit literature considers linear reward functions of the form $\vtheta^\top\vx$, where $\vx \in \sR^p$, however a sub-vector of size $d$ is sufficient to span the reward function. This can be formulated as model selection among \smash{$M = {p \choose d}$} different linear parametrizations, where each $\vphi_j$ is a $d$-dimensional feature map. We present the bounds in terms of $d$ and $M$ for coherence with the rest of the text, assuming that $M = \calO(p)$, which is the case when $d \ll p$.

\cref{tab:sparse_bandits} compares recent work on sparse linear bandits based on a number of important factors.
In this table, the \textsc{ETC} algorithms follow the general format of exploring, performing parameter estimation once at $t=n_0$, and then repeatedly suggesting the same action which maximizes \smash{$\hat\vtheta_{n_0}^\top\vphi(\vx)$}. Explore-then-($\epsilon$)Greedy takes a similar approach, however it does not settle on \smash{$\hat\vtheta_{n_0}$}, rather it continues to update the parameter estimate and select \smash{$\vx_t = \argmax\hat\vtheta_{t}^\top\vphi(\vx)$}. 
The \textsc{UCB} algorithms iteratively update upper confidence bound, and choose actions which maximize them.
The regret bounds in \cref{tab:sparse_bandits} are simplified to the terms with largest rate of growth, {\em the reader should check the corresponding papers for rigorous results}. Some of the mentioned bounds depend on problem-dependent parameters (e.g. $c_K$), which may not be treated as absolute constants and have complicated forms. To indicate such parameters we use $\tau$ in \cref{tab:sparse_bandits}, following the notation of \citet{hao2020high}. Note that $\tau$ varies across the rows of the table, and is just an indicator for existence of other terms. 

\citet{abbasi2012online} use the \textsc{SeqSEW} online regression oracle \citep{gerchinovitz2011sparsity} for estimating the parameter vector, together with a \textsc{UCB} policy. The regression oracle is an exponential weights algorithm, which runs on the squared error loss. This subroutine, and thereby the algorithm proposed by \citet{abbasi2012online} are not computationally efficient, and this is believed to be unavoidable. This work considers the data-rich regime and shows \smash{$R(n) = \calO(\sqrt{dMn})$}, matching the lower bound of Theorem 24.3 in \citet{lattimore2020bandit}.\looseness -1

\citet{carpentier2012bandit} assume that the action set is a Euclidean ball, and that the noise is directly added to the parameter vector, i.e. $y_t = \vx^\top(\vtheta+ \boldsymbol{\varepsilon}_t)$. 
Roughly put, linear bandits with parameter noise are ``easier'' to solve than stochastic linear bandits with reward noise, since the noise is scaled proportionally to the features $x_i$ and does less ``damage '' \citep[Chapter 29.3][]{lattimore2020bandit}. In this setting, \citet{carpentier2012bandit} present a $\calO(d\sqrt{n})$ regret bound. 

Recent work considers contextual linear bandits, where at every step $\calA_t$, a stochastic finite subset of size $K$ from $\calA$, is presented to the agent. It is commonly assumed that members of $\calA_t$ are i.i.d., and the sampling distribution is diverse and time-independent.  The diversity assumption is often in the form of a restricted eigenvalue condition (\cref{ass:compatibility}) on the covariance of the context distribution \citep[e.g. in, ][]{kim2019doubly, bastani2020online}. 
\citet{li2022simple} require a stronger condition which directly assumes that  $\lambdamin(\Phi_t)$ the minimum eigenvalue of the empirical covariance matrix  is lower bounded. This is generally not true, but may hold with high probability. 
\citet{hao2020high} assume that the action set spans $\sR^{dM}$. We believe that this assumption is the weakest in the literature, and conjecture that it is necessary for model selection. If not met, the agent can not explore in all relevant directions, and may not identify the relevant features. Our diversity assumption is similar to \citet{hao2020high}, adapted to our problem setting. Mainly, we consider reward functions which are linearly parametrizable, i.e. $\vtheta^\top\vphi(\vx)$, as oppose to linear rewards, i.e. $\vtheta^\top\vx$. \looseness -1

A key distinguishing factor between \alexp and existing work on sparse linear bandit is that \alexp is horizon-independent and does not rely on a forced exploration schedule. As shown on \cref{tab:sparse_bandits}, majority of prior work relies either on an initial exploration stage, the length of which is determined according to $n$ \citep[e.g.,][]{carpentier2012bandit, kim2019doubly, li2022simple, hao2020high, jang2022popart}, or on a hand crafted schedule, which is again designed for a specific horizon \citep{bastani2020online}. \citet{oh2021sparsity}, which analyzes $K$-armed contextual bandits, does not require explicit exploration, and instead imposes restrictive assumptions on the diversity of context distribution, e.g. relaxed symmetry and balanced covariance. Regardless, the regret bounds hold in expectation, and are not time-uniform.

\begin{table}[ht]
\caption{Overview of recent work on high-dimensional Bandits. Parameter $\tau$ shows existence of other problem-dependent terms which are not constants, and varies across different rows. The regret bounds are simplified and are {\em not} rigorous.}
\label{tab:sparse_bandits}
\centering
\resizebox{0.98\textwidth}{!}{%
\begin{tabular}{l| c| c |c |c |c | c| c | c} 
 & $\vert\calA_t\vert$ & \makecell{data-\\poor} & \makecell{adap.\\exp.} &  \makecell{any-\\time} & \makecell{action\\ selection \\policy} & \makecell{MS\\algo} &\makecell{context\\ or action\\assumpt.} & Regret\\[0.7ex] 
 \hline
\citeauthor{abbasi2012online} & $\infty$&  \xmark & \cmark & \cmark & \textsc{UCB} & \makecell{\textsc{EXP4} on \\ Sqrd error} &  $\calA$ is compact & $\sqrt{dMn}$, w.h.p. \\[0.8ex] 
 \hline
  \citeauthor{foster2019model} & K & \cmark & \cmark & \xmark & \textsc{UCB} & \makecell{\textsc{EXP4} on \\ Sqrd error} & $\lambda_{\mathrm{min}}(\Sigma) \geq c_\lambda$ & \makecell{\small$(Mn)^{3/4}K^{1/4}+ \sqrt{KdMn}$\\  w.h.p}\\[0.8ex]
 \hline
 \citeauthor{carpentier2012bandit} & $\infty$ &  \cmark & \xmark &  \xmark & \textsc{UCB} &  \makecell{Hard\\Thresh.} & \makecell{$\calA$ is a ball\\param. noise} & $d\sqrt{n}$, w.h.p.\\[0.8ex] 
 \hline
 \citeauthor{bastani2020online} & $K$ & \xmark & \xmark &  \xmark &\makecell{Explore\\then \\Greedy} & Lasso & $\kappa(\Sigma)>c_K$ &  \makecell{$\tau Kd^2(\log n+ \log M)^2$\\ w.h.p.} \\[0.8ex] 
 \hline
\citeauthor{kim2019doubly} & $K$ & \xmark & \xmark &  \xmark & \makecell{Explore\\then \\$\epsilon$-Greedy} & Lasso & $\kappa(\Sigma)>c_K$ & $\tau d\sqrt{n}\log (Mn)$, w.h.p.\\[0.8ex] 
 \hline
 \citeauthor{oh2021sparsity} & $K$ & \cmark & \cmark  &\xmark & Greedy & Lasso &\makecell{$\kappa(\Sigma)>c_{\kappa}$\\ + other assums.} & \makecell{$\tau d\sqrt{n\log (Mn)}$\\ in expectation}\\[0.8ex] 
 \hline
   \citeauthor{li2022simple} & $K$ & \cmark & \xmark & \xmark & ETC & Lasso &\makecell{\makecell{$\lambda_\mathrm{min}(\hat \Sigma) >c_\lambda$}}  & \makecell{ $\tau(n^2d)^{1/3}\sqrt{\log Mn}$\\ in expectation}\\[0.8ex]
 \hline
 \citeauthor{hao2020high} & $\infty$ & \cmark &  \xmark & \xmark & ETC & Lasso &  \makecell{$\calA$ spans $\sR^{dM}$\\+ is compact}  & \makecell{$(nd\cmin^{-1})^{2/3}(\log M)^{1/3}$\\w.h.p.} \\[0.8ex] 
 \hline
\citeauthor{jang2022popart} & $\infty$ & \cmark & \xmark & \xmark & ETC & \makecell{Hard\\Thresh.} & \makecell{$\calA \subset [-1,1]^{Md}$\\+ spans $\sR^{Md}$} & \makecell{$(nd)^{2/3}(\cmin^{-1}\log M)^{1/3}$ \\w.h.p.} \\[0.8ex] 
 \hline
\alexp (Ours) & $\infty$ & \cmark &  \cmark  & \cmark & \makecell{Greedy\\or UCB} & \makecell{\textsc{EXP4} on\\ reward est.} & \makecell{$\mathrm{Im}(\vphi_j)$ spans $\sR^{d}$\\$\calA$ is compact} & \makecell{$\sqrt{n\log M}(n^{1/4}+\cmin^{-1}\log M)$\\ w.h.p}
\end{tabular}%
}
\end{table}

