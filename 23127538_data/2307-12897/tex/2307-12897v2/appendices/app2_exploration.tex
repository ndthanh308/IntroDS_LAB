\section{Results on Exploration} \label{app:explore}
In this section we present lower-bounds on the eigenvalues of the covariance matrix $\Phi_t\Phi_t^\top$, as it is later used in our regret analysis. 
In particular, we show that the feature matrix $\Phi_t$ satisfies the restricted eigenvalue condition (\cref{ass:compatibility}) required for valid Lasso confidence set (\cref{thm:anytime_lasso}), and calculate a lower bound on $\kappa(\Phi_t, 2)$. The lower bound is later used by \cref{lem:anytime_CI_main} and \cref{lem:anytime_variance_bound} to develop the model selection regret. We show this bound in three steps.

Equivalent to \cref{cond:compatibility}, we write \smash{$\kappa(\Phi_t, s) = \inf_{b \in \Xi_s} \|\Phi_t \vb\|_2/\sqrt{t}$} where 
\begin{equation}\label{eq:xi_def}
\resizebox{0.92\hsize}{!}{%
    $\Xi_s \coloneqq \Big\{ \vb \in \sR^d\backslash \{0\} \Big\vert  \sum_{j \notin J} \|\vb_j\|_2 \leq  3 \sum_{j \in J} \|\vb_j\|_2, \,\sqrt{\sum_{j \in J} \|\vb_j\|^2_2} \leq 1 \text{ s.t. }J \subset \{1, \dots, M\}, \ \vert J \vert \leq s.\Big\}.
    $}
\end{equation}
For simplicity in notation, we further define
\begin{equation} \label{eq:def_tilde_kappa}
    \tilde \kappa(A, s) \coloneqq \min_{b \in \Xi_s} b^\top A b.
\end{equation}
since \smash{$\tilde \kappa(\tfrac{\Phi_t^\top\Phi_t}{t}, s) = \kappa^2(\Phi_t,s)$}.

\textbf{Step \textsc{I}.} Consider the exploratory steps at which $\alpha_t = 1$.
Let $\Phi_{\pi,t}$ be a sub-matrix of $\Phi_t$ where only rows from exploratory steps are included. Note that $\Phi_{\pi,t} \in \sR^{t'\times dM}$ is a random matrix, where the number of rows $t'$ are also random. We show that $\kappa^2(\Phi_t, s)$ is lower bounded by $\kappa^2(\Phi_{t,\pi}, s)$.

\begin{lemma}\label{lem:kappa_exp_to_kappa}
        Suppose $\Phi_{\pi, t}$ has $t'$ rows. Then,
        \[
        \kappa^2(\Phi_t, s) \geq \frac{t'}{t} \kappa^2(\Phi_{\pi, t}, s)
        \]
\end{lemma}
\begin{proof}[Proof of \cref{lem:kappa_exp_to_kappa}]
Let $\Psi^{(t)} = \vphi(\vx_t)\vphi^\top(\vx_t) \in \sR^{dM\times dM}$ for all $t=1, \dots, n$. Note that $\Psi^{(t)}$ is positive semi-definite by construction. We have,
\begin{align*}
    \norm{\Phi_t b}_2^2 = \sum_{s=1}^t b^\top \Psi^{(s)} b = \sum_{s \in T_\pi}^t b^\top \Psi^{(s)} b + \sum_{s \notin T_\pi}^t b^\top \Psi^{(s)} b 
\end{align*}
where the set $T_\pi$ contains the indices of the exploratory steps at which the action is selected according to $\pi$. Therefore,
\begin{align*}
    \kappa^2(\Phi_t, s)  & = \frac{1}{t}\min_{b \in \Xi_s} \|\Phi_t \vb\|_2^2 \\
    & = \min_{b \in \Xi_s} \frac{1}{t}\sum_{s \in T_\pi} b^\top \Psi^{(s)} b + \frac{1}{t}\sum_{s \notin T_\pi} b^\top \Psi^{(s)} b \\
    % & \geq \min_{b, b' \in \Xi_s}  \frac{1}{t}\sum_{s \in T_\pi} b^\top \Psi^{(s)} b + \frac{1}{t}\sum_{s \notin T_\pi} b'^\top \Psi^{(s)} b'  \\
    % & =  \frac{1}{t}\min_{b \in \Xi_s} \sum_{s \in T_\pi} b^\top \Psi^{(s)} b + \frac{1}{t}\min_{b' \in \Xi_s} \sum_{s \notin T_\pi} b'^\top \Psi^{(s)} b' +\\
    & \geq \min_{b \in \Xi_s} \frac{1}{t}\sum_{s \in T_\pi} b^\top \Psi^{(s)} b
\end{align*}
 where the last inequality holds due to $\Psi^{(s)}$ being PSD. 
 Then we have,
 \begin{align*}
    \kappa^2(\Phi_t, s) \geq \min_{b \in \Xi_s} b^\top\left( \frac{1}{t}\sum_{s \in T_\pi}^t \Psi^{(s)}\right) b  = \frac{\vert T_\pi \vert}{t} \kappa^2(\Phi_{\pi, t}, s)
\end{align*}
\end{proof}
%================================================
%================================================

 While the number of rows of $\Phi_{\pi,t}$ is a random variable, we continue to condition on the event that $\Phi_{\pi,t}$ has $t'$ rows, and investigate the distribution of its restricted eigenvalues.
 
\textbf{Step \textsc{II}.} The restricted eigenvalues of the {\em exploratory} submatrix are well bounded away from zero. 
\begin{lemma}\label{lem:kappa_com}
Let $\pi$ be the solution to \eqref{eq:cmin_def}, and $s \in \sN$. Suppose $\Phi_{\pi, t}$ has $t'$ rows. Then for all $\delta>0$,
\[
\sP \left(\forall t':\,  \kappa^2(\Phi_{\pi, t},s)  \geq \tilde \kappa(\Sigma, s) - \frac{80s}{\sqrt{t'}} \sqrt{ \log(2Md/ \delta) + (\log \log 4t')_+ } \right) \geq 1-\delta
\]
where \smash{$\Sigma = \Sigma(\pi, \vphi) \coloneqq \E_{\vx \sim \pi}\vphi(\vx)\vphi^\top(\vx)$} and $\tilde \kappa$ is defined in \eqref{eq:def_tilde_kappa}.
\end{lemma}

\textbf{Step \textsc{III}.} 
Remains to combine the two above lemmas and incorporate a high probability bound on $t'$, showing that it is close to $\sum_{s=1}^t \gamma_s$.

\begin{lemma}\label{lem:kappa_true_emp}
 There exist absolute constants $C_1, C_2$ which satisfy,
    \[
        \sP\left(\forall t\geq 1: \,\, \kappa^2(\Phi_t,2) \geq C_1\tilde \kappa(\Sigma, 2) t^{-1/4} - C_2t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} \right) \geq 1-\delta
        \]
if $\gamma_t = \calO(t^{-1/4})$. Let $\pi$ be the solution to \eqref{eq:cmin_def}, then it further holds that
    \[
        \sP\left(\forall t\geq 1: \,\, \kappa^2(\Phi_t,2) \geq C_1\cmin t^{-1/4} - C_2t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} \right) \geq 1-\delta
        \]
\end{lemma}
The regret analysis of \citet{hao2020high} also relies on connecting $\kappa(\Phi_t, s)$ to $\cmin$,  
and for this, they use Theorem 2.4 of \citet{javanmard2014confidence}.  
This theorem states that there exists a problem-dependent constant $C_1$ for which $\kappa^2(\Phi_t, s) \geq C_1\cmin$ with high probability, if $t \geq n_0$ and roughly \smash{$n_0 = \calO(\sqrt[3]{n^2\log M})$}. 
We highlight that \cref{lem:kappa_true_emp}, presents a lower-bound which holds for all $t \geq 1$, however this comes at the cost of getting a looser lower bound than the result of \citet{javanmard2014confidence} for the larger time steps $t$. In fact, due to the sub-optimal dependency of \cref{lem:kappa_true_emp} on $t$, we later obtain sub-optimal dependency on the horizon for the case when $n \gg M$.
It is unclear to us if this rate can be improved without assuming knowledge of $n$, or that $n \geq n_0$.

For the last lemma in this section we show that the empirical sub-matrices $\Phi_{t,j}$ are also bounded away from zero. This will be required later to prove \cref{thm:virtual_regret_rigorous}.
\begin{lemma}[Base Model $\lambda_{\mathrm{min}}$ Bound]\label{lem:lambda_min_virtual}
    Assume $\pi$ is the maximizer of \cref{eq:cmin_def}.  Then, with probability greater than $1-\delta$, simultaneously for all $j =1, \dots, M$ and $t \geq 1$, 
        \[
        \lambda_{\mathrm{min}}(\Phi_{t,j}^\top \Phi_{t,j}) \geq C_1\cmin t^{3/4} - C_2t^{3/8} \sqrt{\log(Md/\delta) + (\log\log t)_+}
        \]
if $\gamma_t = \calO(t^{-1/4})$.
\end{lemma}


\subsection{\alexp with Uniform Exploration} \label{app:cmin_bound}

We presented our main regret bound (\cref{thm:regret_main}) in terms of $\cmin$, which only depends on properties of the feature maps and the action domain. 
We give a lower-bound on $\cmin$ for a toy scenario which corresponds to the problem of linear feature selection over convex action sets.

\begin{proposition}[Invertible Features]\label{prop:conv_body}
Suppose $\vphi(\vx)  \coloneqq A\vx: \sR^{d}\rightarrow \sR^{d}$ is an invertible linear map, and $\calX \in \sR^{d}$ is a convex body. Then,
\[
\cmin \geq \frac{\lambda_{\min}(A)}{\lambda^2_{\max} (T)} > 0
\]
where $T$ is the transformation which maps $\calX$ to an isotropic body.
\end{proposition}

The lower-bound of \cref{prop:conv_body} is achieved by simply exploring via $\pi = \mathrm{Unif}(\calX)$. Inspired by \citet[][Lemma E.13]{schur2022lifelong}, we show that even for non-convex action domains and orthogonal feature maps, the uniform exploration yields a constant lower-bound on restricted eigenvalues.

\begin{proposition}[Orthonormal Features]\label{prop:othon_feat}
Suppose $\vphi_j: \calX \rightarrow \sR$ are chosen from an orthogonal basis of $L^2(\calX)$, and satisfy $\|\bm{\phi}_i\|_{L_{\mu}^2(\calX)} / \text{Vol}(\calX) \geq 1$. Then there exist absolute constants $C_1$ and $C_2$ for which the exploration distribution $\pi = \mathrm{Unif}(\calX)$ satisfies
    \[
        \sP\left(\forall t\geq 1: \,\, \kappa^2(\Phi_t,2) \geq  C_1 t^{-1/4} - C_2t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} \right) \geq 1-\delta.
        \]
\end{proposition}
The $d=1$ condition is met without loss of generality, by splitting the higher dimensional feature maps and introducing more base features, which will increase $M$. Moreover, the orthonormality condition is met by orthogonalizing and re-scaling the feature maps. Basis functions such as Legendre polynomials and Fourier features \citep{rahimi2007random} satisfy these conditions. 


    By invoking \cref{prop:othon_feat}, instead of \cref{lem:kappa_true_emp} in the proof of \cref{thm:regret_main}, we obtain the regret of \alexp with uniform exploration.
\begin{corollary}[\alexp with Uniform Exploration]\label{cor:alexp_unif}
  Let $\delta \in (0, 1]$. Suppose $\vphi_j: \calX \rightarrow \sR$ are chosen from an orthogonal basis of $L^2(\calX)$, and satisfy $\|\bm{\phi}_i\|_{L_{\mu}^2(\calX)} / \text{Vol}(\calX) \geq 1$. 
    Assume the oracle agent employs a UCB or a Greedy policy, as laid out in \cref{sec:main_result}.
Choose $\eta_t = \calO(1/\sqrt{t}C(M,\delta,d))$ and $\gamma_t = \calO(t^{-1/4})$ and
$\lambda_t  = \calO(C(M, \delta, d)/\sqrt{t})$,
     then \algo with uniform exploration $\pi = \mathrm{Unif}(\calX)$ attains the regret 
\begin{align*}
R(n)  = \calO\Big(  Bn^{3/4}  & + \sqrt{n} C(M, \delta, d)\log M + B^2 \sqrt{n} + B\sqrt{ n\left( (\log \log nB^2)_+ + \log(1/\delta)\right)}\\
                         & \quad  +(n^{3/4} + \log n )C(M, \delta, d)+ 
                         n^{5/8}\sqrt{d\log n + \log(1/\delta) + B^2}\Big)
\end{align*}
with probability greater than $1-\delta$, simultaneously for all $n \geq 1$. Here,
       \[
C(M, \delta, d) = \calO \left( \sqrt{ 1 + \sqrt{d \left(\log(M/\delta) + (\log\log d)_+ \right)} + \left( \log(M/\delta) + (\log\log d)_+ \right)}\right).
\]  
\end{corollary}


\subsection{Proof of Results on Exploration}
As an intermediate step, we consider the restricted eigenvalue property of the empirical covariance matrix. Given $t'$ samples, the empirical estimate of $\Sigma$ is
\begin{equation} \label{eq:emp_cov_matrix}
\hat \Sigma_{t'} \coloneqq  \frac{1}{t'} \sum_{s=1}^{t'} \vphi(\vx_s)\vphi^\top(\vx_s)    
\end{equation}
where $\vx_s$ are sampled according to $\pi$.  We show that every entry of $\hat \Sigma_{t'}$ is close to the corresponding entry in $\Sigma$, and later use it in the proofs of eigenvalue lemmas.
\begin{lemma}[Anytime Bound for The Entries of Empirical Covariance Matrix]\label{lem:elementwise_com}
Let $\hat \Sigma_{t'}$ be the empirical covariance matrix corresponding to $\Sigma(\pi, \vphi)$ given $t'$ samples. Then,
            \begin{equation*}
            \sP\left(\exists t':\,\, d_\infty(\Sigma, \hat\Sigma_{t'} ) \geq \frac{5}{\sqrt{t'}} \sqrt{\left( (\log \log 4t')_+ + \log(2Md/ \delta)\right)} \right) \leq \delta
        \end{equation*}       
        where $d_\infty(A, B) \coloneqq \max_{i,j} \abs{A_{i,j} - B_{i,j}}$.
\end{lemma}
\begin{proof}[Proof of \cref{lem:elementwise_com}]
We show the element-wise convergence of $\Sigma$ to \smash{$\hat \Sigma_t$} for the $(i,j)$ entry where $i,j = 1, \dots, dM$. 
    Consider the random sequence $X_s \coloneqq \Sigma_{i,j}- \phi_i(\vx_s)\phi_j(\vx_s)$. We show that $X_1, \dots, X_n$ satisfies conditions of \cref{lem:anytime_azuma}. We first observe that
    \[
    \E [X_s \vert X_{1:s-1}] = \E X_s = \Sigma(i,j) - \E_{\vx \sim \pi} \phi_i(\vx)\phi_j(\vx) = 0
    \]
    since by definition $\Sigma_{i,j} = \E_{\vx \sim \pi} \phi_i(\vx)^\top\phi_j(\vx)$. Moreover, we have normalized features $\norm{\vphi(\cdot)} \leq 1$, therefore, each entry $\phi_i(\cdot)\phi_j(\cdot)$ is also bounded, yielding $\abs{X_s}\leq 2$. Then \cref{lem:anytime_azuma} implies that for all $\tilde \delta>0$, 
    \[
    \sP\left(\exists t':\,\, \frac{1}{t'}\sum_{s=1}^{t'} X_s \geq \frac{5}{\sqrt{t'}} \sqrt{\left( (\log \log 4t')_+ + \log(2/\tilde \delta)\right)} \right) \leq \tilde \delta.
    \]
    Setting $\tilde \delta = \delta/(dM)$ and taking a union bound over all indices concludes the proof.
\end{proof}

%==============================================
%==============================================

We are now ready to present the proofs to the lemmas in \cref{app:explore}.
\begin{proof}[Proof of \cref{lem:kappa_com}]
By \eqref{eq:emp_cov_matrix} we have $\hat \Sigma_{t'} = \tfrac{\Phi_{\pi, t}^\top\Phi_{\pi, t}}{t'}$, and thereby
\begin{equation*}
    \kappa^2(\Phi_{\pi, t},s) = \min_{b \in \Xi_s} b^\top \hat \Sigma_{t'} b = \tilde \kappa(\hat\Sigma_{t'}, s).
\end{equation*}
    Inspired by Lemma 10.1 in \citet{sara2009conditions}, we show that element-wise closeness of matrices $\Sigma$ and $\hat \Sigma_{t'}$ (c.f. \cref{lem:elementwise_com}) implies closeness in $\tilde \kappa$: 
    % Since $\tilde \kappa$ is by definition non-negative we have,
    \begin{align*}
       \abs{  \kappa^2(\Phi_{\pi, t},s) - \tilde \kappa(\Sigma, s)} & = \abs{ \tilde \kappa(\hat\Sigma_{t'}, s) - \tilde \kappa(\Sigma, s)}\\
       & = \abs{\tilde \kappa\left(\hat\Sigma_{t'}-\Sigma, s\right)}\\
        & \leq \min_{b \in \Xi_s} d_{\infty}(\Sigma, \hat\Sigma_{t'}) \norm{b}_1^2 
    \end{align*}
    where the last line holds due to H{\"o}lder's. Moreover, since $b \in \Xi_s$, for any $J \subset [dM]$ where $\abs{J} \leq s$ it additionally holds that $\norm{b_J}_2 \leq 1$ and 
    \[
    \norm{b}_1 \leq (1+3) \norm{b_J}_1 \leq 4\sqrt{s} \norm{b_J}_2 \leq 4\sqrt{s}
    \]
    which gives,
        \[
     \kappa^2(\Phi_{\pi, t},s) \geq\tilde \kappa(\Sigma, s) - 16sd_{\infty}(\Sigma,\hat \Sigma_{t'}).
    \]
    Therefore by \cref{lem:elementwise_com}, 
    \begin{align} \label{eq:kappa_phi_sigma}
     \kappa^2(\Phi_{\pi, t},s) \geq \tilde \kappa(\Sigma, s)  - \frac{80s}{\sqrt{t'}} \sqrt{\left( (\log \log 4t')_+ + \log(2Md/ \delta)\right)}
    \end{align}
    with probability greater than $1-\delta$, simultaneously for all $t' \geq 1$.
\end{proof}


\begin{proof}[Proof of \cref{lem:kappa_true_emp}]
    % We start by lower bounding $c_{\kappa, t} = \kappa(\Phi_t,2)$ which appears in the proof of \cref{thm:anytime_regret_MS_formal}.
    In \cref{lem:kappa_exp_to_kappa} we showed that
 \begin{align*}
    \kappa^2(\Phi_t, s) \geq \frac{ t' }{t} \kappa^2(\Phi_{\pi, t}, s)
\end{align*}
    where $t'$ indicates the number of rows in the exploratory sub-matrix of $\Phi_t$. Recall that $t' = \sum_{s=1}^t\alpha_s$ where $\alpha_s$ are i.i.d~Bernoulli random variables with success probability of $\gamma_s$. Due to \cref{lem:anytime_bernoulli},
\begin{align}\label{eq:tprime}
 \sP\left(\forall t \geq1:\, \abs{ t' -  \Gamma_t } \leq \Delta_{t} \right) \geq 1-\delta/2
\end{align}
where
\[
\Delta_t \coloneqq \frac{5}{2}\sqrt{ \frac{(\log \log t)_+ + \log(8/\delta)}{t}}, \quad \Gamma_t \coloneqq \sum_{s=1}^t\gamma_s
\] 
    Due to \cref{lem:kappa_com}, with probability greater than $1-\delta/2$ the following holds for all $t\geq 1$
    \begin{align*}
         \kappa^2(\Phi_{t},2) &  \geq \frac{t'}{t} \tilde \kappa(\Sigma, 2)- \frac{160\sqrt{t'}}{t}  \sqrt{(\log \log 4t')_+ + \log(4Md/ \delta)} \\
         & \geq  \frac{\Gamma_t-\Delta_t}{t}\tilde \kappa(\Sigma, 2) - 160 \sqrt{\frac{\Gamma_t+\Delta_t}{t^2}}\sqrt{\left(\log \log \left(4\Gamma_t + \Delta_t\right)\right)_+ + \log(4Md/ \delta)} 
         \end{align*}
         where the second inequality holds with probability $1-\delta$, by incorporating \eqref{eq:tprime} and taking a union bound.
             For the rest of the proof and to keep the calculations simple, we ignore the values of the absolute constants. We use the notation $g(t) = o(f(t))$ to show that $f(t)$ grows much faster than $g(t)$. More formally, if for every constant $c$ there exists $t_0$, where $g(t) \leq c \abs{f(t)}$ for all $t \geq t_0$.
    % there exists an absolute constant $c$ where $g(t) \leq c \abs{f(t)}$ for all $t \geq 1$.
    Since $\gamma_s = \calO(s^{-1/4})$ there exists $C$ such that $\Gamma_t = C t^{3/4}$, then it is straightforward to observe that there exists absolute constants $\tilde C_i$ which satisfy,
         \begin{align*}
         \kappa^2(\Phi_{t},2) &  \geq \tilde C_1 t^{-1/4}\tilde \kappa(\Sigma, 2) - \frac{5t^{-3/2}\tilde \kappa(\Sigma, 2)}{2} \sqrt{ (\log \log t)_+ + \log(8/\delta)}\\
         & \quad -\tilde C_2 t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} - o\left(t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+}\right)\\
         & \geq \tilde C_1 t^{-1/4}\tilde \kappa(\Sigma, 2) - \tilde C_3 t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+}
    \end{align*}
        The last inequality holds since $t^{-3/2}\sqrt{\log\log t} = o(t^{-5/8}\sqrt{\log\log t})$.
        The above chain of inequalities imply that there exist absolute constants $C_1, C_2$, for which
        \[
        \sP\left(\forall t\geq 1: \,\, \kappa^2(\Phi_t,2) \geq C_1\tilde \kappa(\Sigma, 2) t^{-1/4} - C_2t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} \right) \geq 1-\delta.
        \]
        If $\pi$ is chosen according to \eqref{eq:cmin_def}, then $\tilde \kappa(\Sigma, 2) \geq \cmin$ yielding the lemma's second argument.
\end{proof}

%==============================================
%==============================================

\begin{proof}[Proof of \cref{lem:lambda_min_virtual}]
   Fix $j \in \{1, \dots, M\}$, and construct the set 
   \[
   \Xi_{1,j} = \left\{ \vb \in \sR^{d}\setminus \{0\} \Big\vert \vb = (\vb_1, \dots, \vb_M),\,\, \text{s.t. } \vb_j \in \sR^d, \norm{\vb_j}_2 \leq 1 \text{ and }\forall j'\neq j:\, \vb_{j'} = 0 \right\}.
   \]
   Note that $\Xi_{1,j} \subset \Xi_{s}$. Therefore,
   \[
   \inf_{b \in \Xi_{1,j}} \norm{\Phi_t \boldsymbol b}_2 \geq \inf_{b \in \Xi_s} \norm{\Phi_t \boldsymbol b}_2 = \sqrt{t}\kappa(\Phi_t, s).
   \]
   Moreover, by construction of $\Xi_{1,j}$ we have for all $\vb \in \Xi_{1,j}$ that $\Phi_t \vb = \Phi_{t,j}  \vb_j$, therefore,
   \[
   \inf_{b \in \Xi_{1,j}} \norm{\Phi_t \boldsymbol b}_2^2 = \inf_{\substack{\vb_j \in \sR^d\\\norm{\vb_j}_2^2 \leq 1}} \norm{\Phi_{t,j}\vb_j}_2^2 = \lambda_{\mathrm{min}}(\Phi_{t,j}^\top \Phi_{t,j}).
   \]
  From the above equations we conclude that 
   $\lambda_{\mathrm{min}}(\Phi^\top_{t,j}\Phi_{t,j}) \geq t\kappa^2({\Phi_t, s})$, for all $j = 1, \dots, M$.
   Therefore, using \cref{lem:kappa_true_emp} we obtain that there exists $C_1, C_2$ such that
   \begin{equation*}
           \resizebox{.99\hsize}{!}{$ \sP\left(\forall t\geq 1,\, j = 1, \dots, M: \,\,  \lambda_{\mathrm{min}}(\Phi_{t,j}^\top \Phi_{t,j})\geq C_1\cmin t^{3/4} - C_2t^{3/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} \right) \geq 1-\delta$}
   \end{equation*}
\end{proof}

%==============================================
%==============================================

\begin{proof}[Proof of \cref{prop:conv_body}]
Since $\calX$ is a convex body, then there exists an invertible map $T$, such that $T(\calX)$ is an isotropic body \citep[e.g. Proposition 1.1.1.,][]{apostolos2003notes}.
Then by definition, $\bar X \sim \mathrm{Unif}(T( \calX))$ is an isotropic distribution and $\mathrm{Cov}(\bar X) = I_d$ \citep[e.g., c.f. Chapter 3.3.5][]{vershynin2018high}.
% Since $\calX$ is a convex body, then $T$ is inverible, and we have, $\mathrm{Cov}(X) = (T^{-1})^2$. 
Since $\vphi$ is linear and invertible, it may be written is as $\vphi(\vx) = A\vx$, where $A$ is an invertible matrix.
Therefore, 
\[
\Sigma(\pi, \vphi) = \mathrm{Cov}(\vphi(X)) = A^\top \mathrm{Cov}(X) A = A^\top \mathrm{Cov}\left(T^{-1}\bar X\right) A =  A^\top (T^{-1})^2 A.
\]
As for the minimum eigenvalue, suppose $\vv \in \sR^d$ and $\norm{\vv} = 1$, then
\begin{align*}
\cmin & \geq \lambda_{\mathrm{min}}\left(\Sigma(\pi, \vphi)\right)\geq  \vv^\top A^\top (T^{-1})^2 A \vv \geq    \norm{A\vv}_2 \lambda_{\mathrm{min}}(T^{-2}) = \frac{\norm{A\vv}_2}{\lambda^2_{\max} (T)} \geq \frac{\lambda_{\min}(A)}{\lambda^2_{\max} (T)}.
\end{align*}
\end{proof}

%==============================================
%==============================================

\begin{proof}[Proof of \cref{prop:othon_feat}]
By the assumption of the proposition, for all $i \in M$
\[
[\Sigma(\pi, \vphi)]_{i,i} = \E_{\vx \sim \pi} \phi_i^2(\vx) = \frac{1}{\mathrm{Vol}(\calX)}\int_\calX \phi_i^2(\vx)\mathrm{d}\mu(\vx) \geq 1
\]
and for all $i\neq j$,
\[
[\Sigma(\pi, \vphi)]_{i,j} = \E_{\vx \sim \pi} \phi_i(\vx)\phi_j(\vx)  = \frac{1}{\mathrm{Vol}(\calX)}\int_\calX \phi_i(\vx)\phi_j(\vx)\mathrm{d}\mu(\vx) =0
\]
%==============
%==============
We use $\Sigma = \Sigma(\pi, \vphi)$. For any $\vb \in \sR^{Md}$ where $\norm{\vb} \leq 1$,
\begin{align*}
  \vb^\top \Sigma \vb = \sum_{i, j \in [M]}\vb_j^\top \Sigma_{i,j} \vb_i & =  \sum_{i \in [M]}\vb_i^\top \Sigma_{i,i} \vb_i + \sum_{i, j \in [M], i \neq j} \vb_j^\top \Sigma_{i,j} \vb_i \\
  & = \sum_{i \in [M]}\vb_i^\top \Sigma_{i,i} \vb_i \\
  &\geq 1 \sum_{i \in [M]}\norm{\vb_i}^2_2 \geq 1 .
\end{align*}
Which implies,
\[
\tilde \kappa (\Sigma, s) = \min_{\vb \in \Xi_s} \vb^\top \Sigma \vb \geq 1.
\]
By \cref{lem:kappa_true_emp}, there exist absolute constants $C_1$ and $C_2$ for which,
    \[
        \sP\left(\forall t\geq 1: \,\, \kappa^2(\Phi_t,2) \geq  \tilde \kappa(\Sigma, 2) C_1 t^{-1/4} - C_2t^{-5/8} \sqrt{\log(Md/\delta) + (\log\log t)_+} \right) \geq 1-\delta.
        \]
      concluding the proof.  
\end{proof}
