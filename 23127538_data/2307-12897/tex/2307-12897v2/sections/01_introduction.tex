\section{Introduction}

\looseness -1 When solving bandit problems or performing Bayesian optimization, we need to commit to a reward model {\em a priori}, based on which we estimate the reward function and build a policy for selecting the next action.
In practice, there are many ways to model the reward by considering different feature maps or hypothesis spaces, e.g., for optimizing gene knockouts \citep{gonzalez2015bayesian, pacchiano2022neural} or parameter estimation in biological dynamical systems \citep{ulmasov2016bayesian, imani2019mfbo}. 
It is not known a priori which model is going to yield the most sample efficient bandit algorithm, and we can only select the right model as we gather empirical evidence. 
 This leads us to ask, can we perform adaptive model selection, while simultaneously optimizing for reward?


\looseness -1  In an idealized setting with no sampling limits, given a model class of size $M$, we could initialize $M$ bandit algorithms (a.k.a~ agents) in parallel, each using one of the available reward models.
Then, as the algorithms run, at every step we can select the most promising agent, according to the cumulative rewards that are obtained so far. Model selection can then be cast into an online optimization problem on a $M$-dimensional probability simplex, where the probability of selecting an agent is dependent on its cumulative reward, and the optimizer seeks to find the distribution with the best return in hindsight.
This approach is impractical for large model classes, since at every step, it requires drawing $M$ different samples in parallel from the environment so that the reward for each agent is realized.%, e.g. evaluating a certain cell property for $M$ different gene perturbations. 

\looseness -1 In realistic applications of bandit optimization, we can only draw {\em one} sample at a time, and so we need to design an algorithm which allocates more samples to the agents that are deemed more promising.
Prior work \citep[e.g.,][]{odalric2011adaptive, agarwal2017corralling} propose to run a single meta algorithm which interacts with the environment by first selecting an agent, and then selecting an action according to the suggestion of that agent. %Therefore, at every step only the queried agent receives direct feedback. 
The online model selection problem can still be emulated in this setup, however this time the optimizer receives partial feedback, coming from only one agent. %, and zero for everyone else.
Consequently, many agents need to be queried, and the overall regret scales with $\mathrm{poly} M$, again restricting this approach to small model classes. In fact, addressing the limited scope of such algorithms, \citet{agarwal2017corralling} raise an open problem on the feasibility of obtaining a $\log M$ dependency for the regret.

We show that this rate is achievable, in the particular case of linearly parametrizable rewards.
% Similar to prior work, we only observe the reward for one of the agents, however, 
\looseness -1 We develop a technique to ``hallucinate'' the reward for every agent that was not selected, and run the online optimizer with emulated full-information feedback. %, as if the reward for all agents was observed. 
This allows the optimizer to assess the quality of the agents, without ever having queried them. 
As a result, our algorithm \alexp, satisfies a regret of rate \smash{$\calO(\max\{ \sqrt{n\log^3 M}, n^{3/4}\sqrt{\log M}\})$}, with high probability, simultaneously for all $n\geq 1$ (\cref{thm:regret_main}). 
Our key idea, leading to $\log M$ dependency, is to employ the Lasso as a low-variance online regression oracle, and estimate the reward for the agents that were not chosen. 
%
This trick is made possible through our novel time-uniform analysis of online Lasso regression (\cref{thm:anytime_lasso}).
Consequently, \alexp is horizon-independent, and explores adaptively without requiring an initial exploration stage. Empirically we find that \alexp consistently outperforms prior work across a range of environments. 
%

  %an initial exploratory stage, as it is common within the literature. 



% A common approach is to perform black-box online model selection, without imposing any structure on the reward function, and by considering a finite class of black-box agents %, each of which uses a different reward model 
% \citep[e.g.,][]{odalric2011adaptive}. 
% In doing so, at every iteration only one agent (the one chosen by the master algorithm) can be updated given the new action-reward pair, and the rest of the agents do not receive feedback.
% This creates a challenging sampling task for model/agent selection, since starving an agent of data may result in sub-optimal model selection. 
% Consequently, such algorithms only work for a small number of models and their regret scales with $\mathrm{poly} M$.

% We observe that this rate is achievable, in the particular case of linearly parametrizable rewards.
% We recognize that in this setting, we can update all the agents at every iteration, and thereby cast the model selection problem, as online learning with full-information, which we solve with our low-variance online learner.
% We propose \alexp, and show that it satisfies a regret of rate \smash{$\max\{ \sqrt{n}\log M, \sqrt[4]{n^3\log M}\}$}, with high probability, simultaneously for all $n\geq 1$.
% Our regret guarantees are uniformly valid over an unbounded time horizon, allowing \alexp to be anytime and horizon-independent. In particular, we present a novel time-uniform analysis of adaptive Lasso regression, which may be of independent interest.
% Lastly, we empirically demonstrate that \alexp consistently outperforms prior work, across a range of problem instances.

