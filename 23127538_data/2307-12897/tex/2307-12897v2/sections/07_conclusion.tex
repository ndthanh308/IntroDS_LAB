\vspacecaption
\vspace{-.2cm}
\section{Conclusion}
\vspace{-.2cm}
\vspacecaption
\looseness -1 We proposed \alexp, an algorithm for simultaneous online model selection and bandit optimization. 
As a first, our approach leads to anytime valid guarantees for model selection and bandit regret, and does not rely on a priori determined exploration schedule. 
Further, we showed how the Lasso can be used together with the exponential weights algorithm to construct a low-variance online learner. 
This new connection between high-dimensional statistics and online learning opens up avenues for future research on high-dimensional online learning.
We established empirically that \alexp has favorable exploration--exploitation dynamics, and outperforms existing baselines. 
% There are two unresolved problems left for future work.
% One is to improve the dependency of \cref{thm:regret_main} on $n$, or prove that it is impossible to achieve an {\em anytime} guarantee of $\sqrt{n}\log M$ without restricting the action set, imposing assumptions on the agents, or changes to the problem statement.
We tackled the open problem of \citet{agarwal2017corralling}, and showed that $\log M$ dependency for regret is achievable for linearly parametrizable rewards. This problem remains open for more general, non-parametric reward classes.
% We conclude with reformulating the problem and instead asking on which classes can we perform online model selection with a regret of rate $\log M$?


