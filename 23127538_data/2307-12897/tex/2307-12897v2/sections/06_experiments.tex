 \vspacecaption
\section{Experiments} 
\label{sec:experiments}
 \vspacecaption

\textbf{Experiment Setup.} We create a synthetic dataset based on our data model (\cref{sec:setting}), and choose the domain to be $1$-dimensional $\calX= [-1,1]$. 
As a natural choice of features, we consider the set of degree-$p$ Legendre polynomials, since they form an orthonormal basis for $L^2(\mathcal{X})$ if $p$ grows unboundedly.
We construct each feature map, by choosing $s$ different polynomials from this set, and therefore obtaining \smash{$M = {p+1 \choose s}$} different models.
More formally, we let $\vphi_j(x) = (P_{j_1}(x), \dots, P_{j_s}(x)) \in \sR^s$ where $\{j_1, \dots, j_s\} \subset \{0, \dots, p\}$ and $P_{j'}$ denotes a degree $j'$ Legendre polynomial.
To construct the reward function, we randomly sample $j^\star$ from $[M]$, and draw $\vtheta_{j^\star}$ from an i.i.d.~standard gaussian distribution. We then normalize $\vert \vert \vtheta_{j^\star} \vert \vert = 1$.
% For every $j \neq j^\star$, $\vtheta_j$ is set to zero.
When sampling from the reward, we add Gaussian noise with standard deviation $\sigma = 0.01$.
Figure~\ref{fig:functions} in the appendix shows how the random reward functions may look. For all experiments we set $n=100$, and plot the cumulative regret $R(n)$ averaged over $20$ different random seeds, the shaded areas in all figures show the standard error across these runs. \looseness-1 

\textbf{Algorithms.} We perform experiments on two \textsc{UCB} algorithms, one with oracle knowledge of $j^\star$, and a naive one which takes into account all $M$ feature maps.
We run Explore-then-Commit (\textsc{ETC}) by \citet{hao2020high}, which explores for a horizon of $n_0$ steps, performs Lasso once, and then selects actions greedily for the remaining steps.
As another baseline, we introduce Explore-then-Select (\textsc{ETS}) that explores for $n_0$ steps, performs model selection using the sparsity pattern of the Lasso estimator. For the remaining steps, the policy switches to \textsc{UCB}, calculated based on the selected features.
Performance of \textsc{ETC} and \textsc{ETS} depends highly on $n_0$, so we tune this hyperparameter separately for each experiment.
We also run \textsc{Corral} as proposed by \citet{agarwal2017corralling}, with \textsc{UCB} agents similar to \alexp. We tune the hyper-parameters of \textsc{Corral} as well.
To initialize \alexp we set the rates of $\lambda_t, \gamma_t$ and $\eta_t$ according to \cref{thm:regret_main}, and perform a light hyper-parameter tuning to choose the scaling constants. 
We have included the details and results of our hyper-parameter tuning in \cref{app:hparams}.
To solve \eqref{eq:glasso}, we use \textsc{Celer}, a fast solver for the group Lasso \citep{celer2018}. 
Every time \textsc{UCB} policy is used, we set the exploration coefficient $\beta_t = 2$, and every time exploration is required, we sample according to $\pi = \mathrm{Unif}(\calX)$. \cref{app:exps} includes the pseudo-code for all baseline algorithms.\footnote{The \textsc{Python} code for reproducing the experiments is accessible on \href{https://github.com/lasgroup/ALEXP}{github.com/lasgroup/ALEXP}.} \looseness -1

% Figure environment removed
% Figure environment removed


\textbf{Easy vs. Hard Cases.} We construct an easy problem instance, where $s=2$, $p=10$, and thus $M=55$. Models are lightly correlated since each two model can have at most one Legendre polynomial in common. We also generate an instance with highly correlated feature maps where $s=8$ and $p=10$, which will be a harder problem, since out of the total $M=55$ models, there are $36$ models which have at least $6$ Legendre polynomials in common with the oracle model $j^\star$. Figure~\ref{fig:easy_vs_hard} shows that not only \alexp is not affected by the correlations between the models, but also it achieves a performance competitive to the oracle in both cases, implying that our exponential weights technique for model selection is robust to choice of features. \textsc{ETC} and \textsc{ETS} rely on Lasso for model selection, which performs poorly in the case of correlated features. \textsc{Corral} uses log-barrier-OMD with an importance-weighted estimator, which has a significantly high variance. The curve for \textsc{Corral} in Figures~\ref{fig:easy_vs_hard}~and~\ref{fig:benchmark} is cropped since the regret values get very large. Figure~\ref{fig:corral_full} shows the complete results.
We construct another hard instance (\cref{fig:benchmark}), where the model class is large ($s=3, p=10, M=165$). \alexp continues to outperform all baselines with a significant gap.
It is clear in the regret curves how explore-then-commit style algorithms are inherently horizon-dependent, and may exhibit linear regret, if stopped at an arbitrary time. This is not an issue with the other algorithms.\looseness-1

\textbf{Scaling with $\boldsymbol{\mathrm{M}}$.} Figure~\ref{fig:scaling_exp} shows how well the algorithms scale as $M$ grows. For this experiment we set $s=2$ and change $p \in \{9, \dots, 13\}$. While increasing $M$ hardly affects \alexp and Oracle \textsc{UCB}, other baselines become less and less sample efficient.

% Figure environment removed
\looseness-1 \textbf{Learning Dynamics of \alexp.}
Figure~\ref{fig:prob_curves} gives some insight into the dynamics of \alexp when $M = 165$. 
In particular, it shows how \alexp can rule out sub-optimal agents without ever having queried them.
Figure (a) shows the distribution $\vq_{t}$, at $t=20$ which is roughly equal to the optimal $n_0$ for \textsc{ETC} in this configuration. The oracle model $j^\star$ is annotated with a star, and has the highest probability of selection. 
We observe that, already at this time step, more than $80\%$ of the agents are practically ruled out, due to small probability of selection. However, according to Figure (b), which shows $M_t$ the total number of visited models, less than $10\%$ of the models are queried at $t=20$. This is the key practical benefit of \alexp compared to black-box algorithms such as \textsc{Corral}.
Lastly, Figure (c) shows how $q_{t, j^\star}$ the probability of selecting the oracle agent changes with time. While this probability is higher than that of the other agents, Figure (c) shows that $q_{t, j^\star}$ is not exceeding $0.25$, therefore there is always a probability of greater than $0.75$ that we sample another agent, making \alexp robust to hard problem instances where many agents perform efficiently. We conclude that \alexp seems to rapidly recognize the better performing agents, and select among them with high probability. 
