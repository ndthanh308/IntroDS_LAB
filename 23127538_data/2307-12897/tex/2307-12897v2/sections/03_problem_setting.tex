\section{Problem Setting}\label{sec:setting}


% \looseness -1  We consider a bandit problem where a meta algorithm aims to optimize an unknown reward function, by relying on a number of base agents (bandit algorithms). 
% To interact with the environment, we first select an agent, and then choose the action $\vx_t \in \calX$ suggested by that agent, where $\calX \subset \sR^{d_0}$ is a compact domain.  
% We observe a noisy reward $y_t = r(\vx_t) + \varepsilon_t$, where the observation noise $\varepsilon_t$ is an i.i.d.~zero-mean sub-Gaussian variable with parameter $\sigma^2$.

We consider a bandit problem where a learner interacts with the environment in rounds. At step $t$ the learner selects an action $\vx_t \in \calX$, where $\calX \subset \sR^{d_0}$ is a compact domain and observes a noisy reward $y_t = r(\vx_t) + \varepsilon_t$ such that $\varepsilon_t$ is an i.i.d.~zero-mean sub-Gaussian variable with parameter $\sigma^2$. We assume the reward function $r: \calX \rightarrow \sR$ is linearly parametrizable by some unknown feature map, and that the model class $\{\vphi_j:\sR^{d_0} \rightarrow \sR^d, j = 1, \dots, M\}$ contains the set of plausible feature maps. We consider the setting where $M$ can be very large, and while the set $\{ \vphi_j \}$ may include misspecificed feature maps, it contains at least one feature map that represents the reward function, i.e., there exists $j^\star \in [M]$ such that \smash{$r(\cdot) = \vtheta^\top_{j^\star} \vphi_{j^\star}(\cdot)$}. 
% We also impose some technical conditions on the feature map class \smash{$\{ \vphi_j \}_{j=1}^M$}. First 
We assume the image of $\vphi_j$ spans $\sR^{d}$, and no two feature maps are linearly dependent, i.e. for any $j, j' \in [M]$, there exists no $\alpha \in \sR$ such that $\vphi_j(\cdot) = \alpha\vphi_{j'}(\cdot)$. 
This assumption, which is satisfied by design in practice,
ensures that the features are not ill-posed and we can explore in all relevant directions. 
% This ensures the optimal feature map $j^\star$ is unique. 
We assume that the concatenated feature map $\vphi(\vx) \coloneqq (\vphi_1(\vx), \dots, \vphi_M(\vx))$ is normalized $\norm{\vphi(\vx)} \leq 1$ for all $\vx \in \calX$ and that $\norm{\vtheta_{j^\star}}\leq B$, which implies $|r(\vx)| \leq B$ for all $\vx \in \calX$.


% We then have $M$ different base agents for model selection. We assume that agents do not communicateand instruct each agent $j$ to use the feature map $\vphi_j$ for modeling the reward


\looseness -1 We will model this problem in the language of model selection where a meta algorithm aims to optimize the unknown reward function by relying on a number of base learners. In order to interact with the environment the meta algorithm selects an agent that in turn selects an action. In our setting we thinking of each of these $M$ feature maps as controlled by a base agent running its own algorithm. Base agent $j$ uses the feature map $\vphi_j$ for modeling the reward. At step $t$ of the bandit problem, each agent $j$ is given access to the full history $H_{t-1} \coloneqq \{(\vx_1, y_1), \dots, (\vx_{t-1}, y_{t-1})\}$, and uses it to locally estimate the reward as \smash{$\hat\vbeta^\top_{t-1,j}\vphi_j(\cdot)$}, where \smash{$\hat\vbeta_{t-1,j} \in \sR^d$} is the estimated coefficients vector. The agent then uses this estimate to develop its action selection policy $p_{t,j} \in \calM(\calX)$. 
Here, $\calM$ denotes the space of probability measures defined on $\calX$.
The condition on existence of $j^\star$ will ensure that there is at least one agent which is using a correct model for the reward, and thereby can solve the bandit problem if executed in isolation. We refer to agent $j^\star$ as the oracle agent.


\looseness -1 Our goal is to find a sample-efficient strategy for iterating over the agents, such that their suggested actions maximize the cumulative reward, achieved over any horizon $n \geq 1$. This is equivalent to minimizing the cumulative regret $R(n) = \sum_{t=1}^n r(\vx^*) - r(\vx_t)$, where $\vx^*$ is a global maximizer of the reward function. We neither fix $n$, nor assume knowledge of it.

% Suppose we have $M$ different agents, each of which uses a different model for the reward. The reward model affects the agent's policy when suggesting actions.
% To interact with the environment, we first select an agent, and then choose the action $\vx_t$ suggested by that agent.
% We observe a noisy reward $y_t = r(\vx_t) + \varepsilon_t$, which we report back to all $M$ agents. This will allow the agents to update their reward model, and in turn, their action selection policy. The agents do not communicate.
% We would like to choose actions $\vx_t$ which maximize the cumulative reward achieved over any horizon $n \geq 1$. This is equivalent to minimizing the cumulative regret $R(n) = \sum_{t=1}^n r(\vx^*) - r(\vx_t)$, where $\vx^*$ is a global maximizer of $r(\cdot)$. We neither fix $n$, nor assume knowledge of it.

% \looseness -1 Our goal is to find a sample-efficient strategy for iterating over the agents, such that we attain a small cumulative regret. 
% To this end, we impose some structure on the reward function.
% Suppose actions are selected from a domain $\calX \subset \sR^{d_0}$ which spans $\sR^{d_0}$. This condition is rather trivial, and is satisfied if the set has a non-empty interior. 
% We assume that $r: \calX \rightarrow \sR$ is linearly parametrizable and consider the model class $\{\vphi_j:\sR^{d_0} \rightarrow \sR^d, j = 1, \dots, M\}$ which describes the relevant feature maps.
% Suppose $\vphi_j$ are invertible, and without loss of generality, assume that $\norm{\vphi_j(\vx)} \leq 1$ for all $j= 1, \dots, M$ and all $\vx \in \calX$.
% We assume that $M$ is very large, and while the set $\{ \vphi_j \}$ may have misspecificed or redundantly repeated feature maps, there exists at least one feature map that spans the reward function, i.e. there exists $j^\star$ such that $r(\cdot) = \vtheta^\top_{j^\star} \vphi_{j^\star}(\cdot)$. 
% We further assume that $\norm{\vtheta_{j^\star}}\leq B$, which implies $r(\cdot)$ is uniformly bounded by $B$.
% Lastly, we require that the observation noise $\varepsilon_t$ are i.i.d. zero-mean sub-Gaussian variables with variance $\sigma^2$.

%  We assume that agent do not communicate and set each agent $j$ to use the model $\vbeta^\top_j\vphi_j(\cdot)$ for the reward function, where \smash{$\vbeta_j \in \sR^d$}. 
% The reward model affects the agent's policy when suggesting actions.
% The condition on existence of $j^\star$ will ensure that there is at least one agent, which is using a correct model for the reward, and thereby, can solve the BO problem if executed in isolation. We refer to $j^\star$ as the oracle agent.
% At step $t$ of the bandit problem, each agent $j$ is given access to the full history $H_{t-1} \coloneqq \{(\vx_1, y_1), \dots, (\vx_{t-1}, y_{t-1})\}$, and uses it to locally estimate $\vbeta$, when developing its action selection policy $p_{t,j} \in \calM(\calX)$. 