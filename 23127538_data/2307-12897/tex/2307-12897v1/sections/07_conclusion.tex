\vspacecaption
\section{Conclusion}
\vspacecaption
\looseness -1 We proposed \alexp, an algorithm for simultaneous online model selection and bandit optimization. 
As a first, our approach leads to anytime valid guarantees for model selection and bandit regret, and does not rely on a priori determined exploration schedule. 
Further, we showed how the Lasso can be used together with the exponential weights algorithm to construct a low-variance online learner. 
This new connection between high-dimensional statistics and online learning opens up avenues for future research on high-dimensional online learning.
We established empirically that \alexp has favorable exploration--exploitation dynamics, and outperforms existing baselines. 
We addressed the open problem of \citet{agarwal2017corralling}, and showed that $\log M$ dependency for regret is achievable for linearly parametrizable rewards. However, this problem remains open for more general, non-parametric reward classes.


