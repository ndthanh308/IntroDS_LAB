\vspaceparagraph
\begin{table}[ht]
\caption{Overview of literature on online model selection for bandit optimization}
\label{tab:general_overview}
\centering
\begin{tabular}{c| c| c |c |c | c }
&{\small \makecell{MS\\Technique}} & {\small Regret} & {\small \makecell{MS \\Guarantee}} & {\small\makecell{adaptive\\exploration}} &  {\small anytime} \\[0.7ex]
\hline 
{\small\makecell{Sparse Linear\\ Bandits}} &  {\small Lasso} & $\log  M$ & \xmark  & \xmark &\xmark \\
\hline
{\small\makecell{MS for Black-\\Box Bandits}}& {\small \makecell{\textsc{OMD} with\\bandit feedback}} & $\mathrm{poly} M$ & \cmark  &  \cmark & \xmark \\
\hline
{\small\makecell{MS for Linear\\ Bandits (Ours)} }& {\small \makecell{\textsc{EXP4} with\\full-information}} & $\log M$ & \cmark & \cmark & \cmark
\vspaceparagraph
\
\end{tabular}
\vspaceparagraph
\end{table}
\section{Related Work} \label{sec:lit_rev}
Online Model selection (MS) for bandits considers combining a number of agents in a master algorithm, with the goal of performing as well as the best agent \citep{odalric2011adaptive,agarwal2017corralling, pacchiano2020model, luo2022corralling}. This literature operates on black-box model classes of size $M$ and uses variants of Online Mirror Descent (\textsc{OMD}) to sequentially select the agents. The optimizer operates on importance-weighted estimates of the agents' rewards, which has high variance ($\mathrm{poly}M$) and is non-zero only for the selected agent. Effectively, the optimizer receives partial (a.k.a.~bandit) feedback and agents are at risk of starvation, since at every step only the selected agent gets the new data point. 
These works assume knowledge of the horizon, however, we suspect that this may be lifted with a finer analysis of OMD. 

\looseness -1 Sparse linear bandits use sparsity-inducing methods, often Lasso \citep{tibshirani1996regression}, for estimating the reward in presence of many features, as an alternative to model selection. 
Early results are in the data-rich regime where the stopping time $n$ is known and larger than the number of models, and some suffer from $\mathrm{poly} M$ regret dependency \citep[e.g.,][]{abbasi2012online}. 
Recent efforts often consider the contextual case, where at every step only a finite stochastic subset of the action domain is presented to the agent, and that the distribution of these points is i.i.d.\ and sufficiently diverse \citep{li2022simple, bastani2020online, kim2019doubly, oh2021sparsity, cella2021multi}. We do not rely on such assumptions.
Most sparse bandit algorithms either start with a purely exploratory phase \citep{kim2019doubly, li2022simple, hao2020high, jang2022popart}, or rely on a priori scheduled exploration \citep{bastani2020online}.
The exploration budget is set according to the horizon $n$. Therefore, such algorithms inherently require the knowledge of $n$ and can be made anytime only via the doubling trick \citep{auer1995gambling}. \cref{tab:sparse_bandits} presents an in-depth overview.

\looseness -1 \alexp inherits the best of both worlds (\cref{tab:general_overview}): its regret enjoys the $\log M$ dependency of sparse linear bandits even on compact domains, and it has adaptive probabilistic exploration with anytime guarantees. 
In contrast to prior literature, we perform model selection with an online optimizer (\textsc{EXP4}), which hallucinates full-information feedback using a low-variance Lasso estimator instead of importance-weighted estimates. 
Moreover, our anytime approach lifts the horizon dependence and the exploration requirement of sparse linear bandits.

Our work is inspired by and contributes to the field of Learning with Expert Advice, which analyzes incorporating the advice of $M$ oblivious (non-adaptive) experts, with bandit or full-information feedback \citep{haussler1998sequential,auer2002nonstochastic, mcmahan2009tighter}. 
The idea of employing an online optimizer for learning stems from this literature, and has been used in various applications of online learning \citep{foster2017parameter, singla2018learning, muthukumar2019best,  karimi2021online, liu2022cost}. 
In particular, we are inspired by \citet{foster2020beyond} and \citet{moradipari2022feature}, who apply \textsc{EXP4} to least squares estimates, for arm selection in $K$-armed contextual bandits. However, their algorithms are not anytime, and due to the choice of estimator, the corresponding regret scales with \smash{$\calO(\min(\sqrt{M}, \sqrt{K\log M}))$}. 


