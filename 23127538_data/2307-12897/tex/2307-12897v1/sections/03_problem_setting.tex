\section{Problem Setting}\label{sec:setting}

We consider a bandit problem where a learner interacts with the environment in rounds. At step $t$ the learner selects an action $\vx_t \in \calX$, where $\calX \subset \sR^{d_0}$ is a compact domain and observes a noisy reward $y_t = r(\vx_t) + \varepsilon_t$ such that $\varepsilon_t$ is an i.i.d.~zero-mean sub-Gaussian variable with parameter $\sigma^2$. We assume the reward function $r: \calX \rightarrow \sR$ is linearly parametrizable by some unknown feature map, and that the model class $\{\vphi_j:\sR^{d_0} \rightarrow \sR^d, j = 1, \dots, M\}$ contains the set of plausible feature maps. We consider the setting where $M$ can be very large, and while the set $\{ \vphi_j \}$ may include misspecificed feature maps, it contains at least one feature map that represents the reward function, i.e., there exists $j^\star \in [M]$ such that \smash{$r(\cdot) = \vtheta^\top_{j^\star} \vphi_{j^\star}(\cdot)$}. 
We assume the image of $\vphi_j$ spans $\sR^{d}$, and no two feature maps are linearly dependent, i.e. for any $j, j' \in [M]$, there exists no $\alpha \in \sR$ such that $\vphi_j(\cdot) = \alpha\vphi_{j'}(\cdot)$. 
This assumption, which is satisfied by design in practice,
ensures that the features are not ill-posed and we can explore in all relevant directions. 
Without loss of generality, we assume the feature maps are bounded $\norm{\vphi_j(\vx)} \leq 1$ for all $j= 1, \dots, M$ and $\vx \in \calX$ and that $\norm{\vtheta_{j^\star}}\leq B$, which implies $|r(\vx)| \leq B$ for all $\vx \in \calX$.


\looseness -1 We will model this problem in the language of model selection where a meta algorithm aims to optimize the unknown reward function by relying on a number of base learners. In order to interact with the environment the meta algorithm selects an agent that in turn selects an action. In our setting we thinking of each of these $M$ feature maps as controlled by a base agent running its own algorithm. Base agent $j$ uses the feature map $\vphi_j$ for modeling the reward. At step $t$ of the bandit problem, each agent $j$ is given access to the full history $H_{t-1} \coloneqq \{(\vx_1, y_1), \dots, (\vx_{t-1}, y_{t-1})\}$, and uses it to locally estimate the reward as \smash{$\hat\vbeta^\top_{t-1,j}\vphi_j(\cdot)$}, where \smash{$\hat\vbeta_{t-1,j} \in \sR^d$} is the estimated coefficients vector. The agent then uses this estimate to develop its action selection policy $p_{t,j} \in \calM(\calX)$. 
Here, $\calM$ denotes the space of probability measures defined on $\calX$.
The condition on existence of $j^\star$ will ensure that there is at least one agent which is using a correct model for the reward, and thereby can solve the bandit problem if executed in isolation. We refer to agent $j^\star$ as the oracle agent.


\looseness -1 Our goal is to find a sample-efficient strategy for iterating over the agents, such that their suggested actions maximize the cumulative reward, achieved over any horizon $n \geq 1$. This is equivalent to minimizing the cumulative regret $R(n) = \sum_{t=1}^n r(\vx^*) - r(\vx_t)$, where $\vx^*$ is a global maximizer of the reward function. We neither fix $n$, nor assume knowledge of it.
