\vspacecaption
\vspacecaption
\section{Main Results}\label{sec:main_result}
\looseness -1 For the regret guarantee, we consider specific choices of base agents and exploratory distribution. 
Our analysis may be extended to include other policies, since \alexp can be wrapped around any bandit agent that is described by some $p_{t,j}$, and allows for random exploration with any distribution $\pi$.

\textbf{Base Agents.}
\looseness -1 
We assume that the oracle agent has either a \textsc{UCB} \citep{abbasi2011improved} or a \textsc{Greedy} \citep{auer2002finite} policy, and all other agents are free to choose {\em any arbitrary} policy.
Similar treatment can be applied to cases where the oracle uses other (sublinear) polices for solving linear or contextual bandits \citep[e.g.,][]{thompson1933likelihood,kaufmann2012bayesian,agarwal2014taming}.
In either case, agent $j^\star$ calculates a ridge estimate of the coefficients vector based on the history $H_{t}$
\vspaceequation
\begin{align*}
\vspaceequation
\hat \vbeta_{t,j^\star} &\coloneqq \argmin_{\vbeta \in \sR^{d}} \norm{\vy_t - \Phi_{t, j^\star}\vbeta}_2^2 + \tilde \lambda \norm{\vbeta}_2^2 = \left(\Phi_{t,j^\star}^\top \Phi_{t,j^\star} +  \tilde \lambda \boldsymbol I\right)^{-1} \Phi_{t,j^\star}^\top \vy_t.
\vspaceequation
\end{align*}
\looseness -1 Here \smash{$\Phi_{t, j^\star} \in \sR^{t \times d}$} is the feature matrix, where each row $s$ is $\vphi_{j^\star}^\top(\vx_s)$ and $\tilde \lambda$ is the regularization constant. 
Then at step $t$, a \textsc{Greedy} oracle suggests the action which maximizes the reward estimate \smash{$\hat \vbeta^\top_{t-1,j^\star}\vphi_{j^\star}(\vx)$}, and a \textsc{UCB} oracle queries  $\argmax u_{t-1,j^\star}(\vx)$ where $u_{t-1,j^\star}(\cdot)$ is the upper confidence bound that this agent calculates for $r(\cdot)$. 
\cref{prop:CI_oracle} shows that the sequence $(u_{t,j^\star})_{t\geq 1}$ is in fact an anytime valid upper bound for $r$ over the entire domain. 

\textbf{Exploratory policy.}
Performance of \alexp depends on the quality of the samples that $\pi$ suggests. The eigenvalues of the covariance matrix $\Sigma(\pi, \vphi) \coloneqq \E_{\vx \sim \pi}\vphi(\vx)\vphi^\top(\vx)$ reflect how diverse the data is, and thus are a good indicator for data quality. \citet{sara2009conditions} present a survey on the notions of diversity defined based on these eigenvalues. Let $\lambda_{\mathrm{min}}(A)$ denote the minimium eigenvalue of a matrix $A$. Similar to \citet{hao2020high}, we assume that $\pi$ is the maximizer of the problem below and present our regret bounds in terms of
\begin{equation}\label{eq:cmin_def}
\cmin = \cmin(\calX, \vphi) \coloneqq \max_{\pi \in \calM(\calX)} \lambda_{\mathrm{min}}\left(\Sigma(\pi, \vphi)\right),
\end{equation}
which is greater than zero under the conditions specified in our problem setting. 
Prior works in the sparse bandit literature all require a similar or stronger assumption of this kind, and \cref{tab:sparse_bandits} gives an overview. 
Alternatively, one can work with an arbitrary $\pi$, e.g., $\mathrm{Unif}(\calX)$, as long as $\lambda_{\mathrm{min}}(\Sigma(\pi, \vphi))$ is bounded away from zero. 
\cref{app:cmin_bound} reviews some configurations of $(\vphi, \calX, \pi)$ that lead to a non-zero minimum eigenvalue, and \cref{cor:alexp_unif} bounds the regret of \alexp with uniform exploration. \looseness -1

For this choice of agents and exploratory distribution, \cref{thm:regret_main} presents an informal regret bound.
Here, we have used the $\calO$ notation, and only included the fastest growing terms. The inequality is made exact in \cref{thm:main_regret_rigorous}, up to constant multiplicative factors. 

\begin{theorem}[Cumulative Regret of \alexp, Informal]\label{thm:regret_main}
 Let $\delta \in (0, 1]$ and set $\pi$ to be the maximizer of \eqref{eq:cmin_def}.
Choose learning rate $\eta_t = \calO(\cmin t^{-1/2}/ C(M,\delta,d))$, exploration probability $\gamma_t = \calO(t^{-1/4})$ and Lasso regularization parameter $\lambda_t  = \calO(\cmin t^{-1/2} C(M, \delta, d))$, where
       \[
C(M, \delta, d) = \calO\left(\sqrt{ 1 + \log(M/\delta) + (\log\log d)_+ + \sqrt{d \left(\log(M/\delta) + (\log\log d)_+ \right)} } \right).
\]
    Then \alexp satisfies the cumulative regret
\begin{align*}
   \vspaceequation
R(n) = \calO \Big(n^{3/4}B + \,n^{3/4}C(M,\delta,d)& +\,\cmin^{-1}\sqrt{n} C(M,\delta,d)\log M\\
& + \cmin^{-1/2}n^{5/8}\sqrt{d\log\left(n\right) + \log(1/\delta)}\Big)
\vspaceequation
\end{align*}
 simultaneously for all $n \geq 1$, with probability greater than $1-\delta$. 
\end{theorem}
\looseness-1 
In this bound, the first term is the regret incurred at exploratory steps (when $\alpha_t=1$), the second term is due to the estimation error of Lasso (i.e., \smash{$||\vtheta-\hat\vtheta_t||$}), and the third term is the regret of the exponential weights sub-algorithm. The fourth term, is the regret bound for the oracle agent $j^\star$, when run within the \alexp framework. It does not depend on the agent's policy (greedy or optimistic), and is worse than the minimax optimal rate of $\sqrt{nd\log n}$.
This is because the oracle is suggesting actions based on the history $H_t$, which includes uninformative action-reward pairs queried by other, potentially misspecified, agents. 
In \cref{cor:alexp_unif}, we provide a regret bound independent of $\cmin$, for orthogonal feature maps, and show that the same \smash{$\calO(\max\{ \sqrt{n\log^3 M}, n^{3/4}\sqrt{\log M}\})$} rate may be achieved even with the simple choice $\pi = \mathrm{Unif}(\calX)$.


\subsection{Proof Sketch} \label{sec:regret_anals}
The regret is caused by two sources: selecting a sub-optimal agent, and an agent selecting a sub-optimal action. 
Accordingly, for any $j \in 1, \dots, M$, we decompose the regret as
\begin{equation}\label{eq:reg_decompose}
R(n) = \sum_{t=1}^n r(\vx^\star) - r(\vx_t) = \Big(\sum_{t=1}^n r(\vx^\star) -  r_{t,j}\Big) + \Big(\sum_{t=1}^n r_{t,j} - r(\vx_t)    \Big).
\end{equation}
\looseness-1 The first term shows the cumulative regret of agent $j$, when run within \alexp. 
The second term evaluates the received reward against the cumulative average reward of model $j$. 
We bound each term separately. 

\textbf{Virtual Regret.} The first term $\tilde R_{j}(n) \coloneqq \sum_{t=1}^n r(\vx^\star) -  r_{t,j}$ compares the suggestion of agent $j$, against the optimal action. We call it the virtual regret since the sequence \smash{$(\vx_{t,j})_{t \geq 1}$} of the actions suggested by model $j$ are not necessarily selected by the meta algorithm, unless $j_t = j$. This regret is merely a technical tool, and not actually realized when running \algo.
The virtual regret of the oracle agent may still be analyzed using standard techniques for linear bandits, e.g., \citet{abbasi2011improved}, however we need to adapt it to take into account a subtle difference:  
The confidence sequence of model $j^\star$ is constructed according to the true sequence of actions \smash{$(\vx_{t})_{t \geq 1}$}, while its virtual regret is calculated based on the virtual sequence \smash{$(\vx_{t,j^\star})_{t \geq 1}$}, which the model suggests. The two sequences only match at the steps when model $j^\star$ is selected. 
Adapting the analysis of \citet{abbasi2011improved} to this subtlety, we obtain in \cref{thm:virtual_regret_rigorous} that with probability greater than $1-\delta$, simultaneously for all $n\geq 1$, 
\[\tilde R_{j^\star}(n) = \calO\left(n^{5/8}\cmin^{-1/2}\sqrt{d\log\left(n\right) + \log(1/\delta)}\right).\]

\textbf{Model Selection Regret.} The second term in \eqref{eq:reg_decompose} is the model selection regret,  $R(n,j)  \coloneqq \sum_{t=1}^n r_{t,j} - r(\vx_t)$, which evaluates the chosen action by \algo against the suggestion of the $j$-th agent. 
Our analysis relies on a careful decomposition of $R(n,j)$, 
\begin{align*}
R(n,j) = \sum_{t=1}^n \Bigg[ \underbrace{r_{t,j} - \hat r_{t,j} }_{\text{(I)}} + \underbrace{ \hat r_{t,j} - \sum_{i = 1}^M  q_{t,i} \hat r_{t,i} }_{\text{(II)}}  + \underbrace{\sum_{i = 1}^M q_{t,i} (\hat r_{t,i} - r_{t,i})}_{\text{(III)}}  + \underbrace{ \sum_{i = 1}^M q_{t,i}r_{t,i} - r(\vx_t)}_{\text{(IV)}}\Bigg].
\end{align*}
We bound away each term in a modular manner, until we are left with the regret of the standard exponential weights algorithm.
The terms (I) and (III) are controlled by the bias of the Lasso estimator, and are \smash{$\calO(n^{3/4}C(M, \delta, d))$} (\cref{lem:anytime_CI_main}).
The last term (IV) is zero in expectation, and reflects the deviation of $r(\vx_t)$ from its mean. We observe that the summands form a bounded Martingale difference sequence, and their sum grows as $\calO(\sqrt{n})$ (\cref{lem:anytime_mother_exp4}).
Term (II) is the regret of our online optimizer, which depends on the variance of the Lasso estimator. We bound this term with $\calO(\sqrt{n}C(M, \delta, d)\log M)$, by first conducting a standard anytime analysis of exponential weights (\cref{lem:exp3_vanilla}), and then incorporating the anytime Lasso variance bound (\cref{lem:anytime_variance_bound}).
We highlight that neither of the above steps require assumptions about the base agents. Combining these steps, \cref{thm:anytime_regret_MS_formal} establishes the formal bound on the model selection regret. 

\textbf{Anytime Lasso.}  
We develop novel confidence intervals for Lasso with history-dependent data, which are uniformly valid over an unbounded time horizon. This result may be of independent interest in applications of Lasso for online learning or sequential decision making.
Here, we use these confidence intervals to bound the bias and variance terms that appear in our treatment of the model selection regret.
The width of the Lasso confidence intervals depends on the quality of feature matrix $\Phi_t$, often quantified by the restricted eigenvalue property \citep{bickel2009dantzig,sara2009conditions, javanmard2014confidence}:\looseness -1
\begin{definition}\label{cond:compatibility}
\label{ass:compatibility}
For the feature matrix  $\Phi_t\in \sR^{t \times dM}$ we define $\kappa(\Phi_t,s)$ for any $1 \leq s\leq M$ as
\begin{align*}
\kappa(\Phi_t, s) \coloneqq & \inf_{(J,\vb)} \frac{1}{\sqrt{t}} \frac{\|\Phi_t \vb\|_2}{ \sqrt{\sum_{j \in J} \|\vb_j\|^2_2}}\\
 \text{s.t. }&  \ \vb \in \sR^d\backslash \{0\},\ \sum_{j \notin J} \|\vb_j\|_2 \leq 3 \sum_{j \in J} \|\vb_j\|_2,\, J \subset \{1, \dots, M\}, \ \vert J \vert \leq s.
\end{align*}
\end{definition}
\looseness -1 Our analysis is in terms of this quantity, and \cref{lem:kappa_true_emp} explains the connection between $\kappa(\Phi_t, s)$ and $\cmin$ as defined in \eqref{eq:cmin_def}, particularly that $\kappa(\Phi_t, 2)$ is also positive with a high probability.
\begin{theorem}[Anytime Lasso Confidence Sequences] Consider the data model $y_t = \vtheta^\top \vphi(\vx_t) + \varepsilon_t$ for all $ t \geq 1$, where $\varepsilon_t$ is i.i.d.~zero-mean sub-Gaussian noise, and $\vx_t$ is $\calF_{t}$-measurable, where $\calF_{t}\coloneqq \left(\vx_1, \dots, \vx_t, \varepsilon_1, \dots, \varepsilon_{t-1}\right)$. \label{thm:anytime_lasso}
    Then the solution of \eqref{eq:glasso} guarantees
        \[
    \sP\left(\forall t \geq 1:\,\, \norm{\vtheta - \hat\vtheta_t}_2 \leq \frac{4\sqrt{10}\lambda_t}{\kappa^2(\Phi_t, 2) } \right)\geq 1-\delta 
    \]
    if the regularization parameter satisfies for all $t\geq 1$
 \[
\lambda_t \geq \frac{2\sigma}{\sqrt{t}}\sqrt{ 1 +  \frac{12}{\sqrt{2}}\left( \log(2M/\delta) + (\log\log d)_+ \right) + \frac{5}{\sqrt{2}}\sqrt{d \left(\log(2M/\delta) + (\log\log d)_+ \right)} }.
\]
\end{theorem}
\looseness -1 Our confidence bound enjoys the same rate as Lasso with fixed (offline) data, up to $\log\log d$ factors.
We prove this theorem by constructing a self-normalized martingale sequence based on the $\ell_2$-norm of the empirical process error (\smash{$\Phi_t^\top\vvarepsilon_t$}). We then apply the ``stitched'' time-uniform boundary of \citet{howard2021time}. \cref{app:lasso} elaborates on this technique. 
Previous work on sparse linear bandits also include analysis of Lasso in an online setting, when $\vx_t$ is $\calF_{t}$ measurable. 
\citet{cella2021multi} imitate offline analysis and then apply a union bound over the time steps, which multiplies the width by $\log n$ and requires knowledge of the horizon.
\citet{bastani2020online} also rely on knowledge of $n$ and employ a scalar-valued Bernstein inequality on $\ell_\infty$-norm of the empirical process error, which inflates the width of the confidence sets by a factor of \smash{$\sqrt{d}$}. 
We work directly on the $\ell_2$-norm, and use a curved boundary for sub-Gamma martingale sequences, which according to \citet{howard2021time} is uniformly tighter than a Bernstein bound, especially for small $t$. 

%============================================
%============================================
%============================================
%============================================
\vspacecaption
\subsection{Discussion} \label{sec:properties}
In light of \cref{thm:regret_main} and \cref{thm:anytime_lasso}, we discuss some properties of \alexp .

\looseness-1\textbf{Sparse \textsc{EXP4}.} Our approach presents a new connection between online learning and high-dimensional statistics. The rule for updating the probabilities in \alexp is inspired by the exponential weighting for Exploration and Exploitation with Experts (\textsc{EXP4}) algorithm, which was proposed by \citet{auer2002nonstochastic} and has been extensively studied in the adversarial bandit and learning with expert advice literature \citep[e.g.,][]{mcmahan2009tighter,beygelzimer2011contextual}.
\textsc{EXP4} classically uses importance-weighted (IW) or ordinary least squares  (LS) estimators to estimate the return $r_{t,j}$, both of which are unbiased but high-variance choices \citep{bubeck2012regret}.
In particular, in our linearly parametrized setting, the variance of IW and LS scales with $Md$, which will lead to a $\mathrm{poly}(M)$ regret.
However, it is known that introducing bias can be useful if it reduces the variance \citep{zimmert2022return}. For instance, EXP3-IX \citep{kocak2014efficient} and EXP4-IX \citep{neu2015explore} construct a biased IW estimator. Equivalently, others craft regularizers for the reward of the online optimizer, seeking to improve the bias-variance balance \citep[e.g.,][]{abernethy2008efficient, bartlett2008high, abernethy2009beating, bubeck2017kernel, lee2020bias, zimmert2022return}.
A key technical observation in this work is that our online Lasso estimator leads \textsc{EXP4} to achieve sublinear regret which depends logarithmically on $M$.
This is due to the fact that while the estimator itself is $Md$-dimensional, its bias squared and variance scale with $\sqrt{d\log M}$.
To the best of our knowledge, this work is first to instantiate the \textsc{EXP4} algorithm with a sparse low-variance estimator. 



\textbf{Adaptive and Anytime.} 
To estimate the reward, prior work on sparse bandits commonly emulate the Lasso analysis on offline data or on a martingale sequence with a known length \citep{ hao2020high, bastani2020online}.
These works require a long enough sequence of exploratory samples, and knowledge of the horizon to plan this sequence.
\alexp removes both of these constraints, and presents a fully adaptive algorithm. 
Crucially, we employ the elegant martingale bounds of \citet{howard2020time} to present the first time-uniform analysis of the Lasso with history-dependent data (\cref{thm:anytime_lasso}). This way we can use all the data points and explore with a probability which vanishes at a $\calO(t^{-1/4})$ rate.
Our anytime confidence bound for Lasso, together with the horizon-independent analysis of the exponential weights algorithm, also allows \alexp to be stopped at any time with valid guarantees.\looseness-1

\textbf{Rate Optimality.}
For $M\gg n$, we obtain a $\calO(\sqrt{n\log^3 M})$ regret, which matches the rate conjectured by \citet{agarwal2017corralling}. However, if $M$ is comparable to $n$ or smaller, our regret scales with \smash{$\calO(n^{3/4}\sqrt{\log M})$}, and while it is still sublinear and scales logarithmically with $M$, the dependency on $n$ is sub-optimal.
This may be due to the conservative nature of our model selection analysis, during which we do not make assumptions about the dynamics of the base agents.
Therefore, to ensure sufficiently diverse data for successful model selection, we need to occasionally choose exploratory actions with a vanishing probability of $\gamma_t$. 
We conjecture that this is avoidable, if we make more assumptions about the agents, e.g., that a sufficient number of agents can achieve sublinear regret if executed in isolation. \citet{banerjee2023exploration} show that the data collected by sublinear algorithms organically satisfies a minimum eigenvalue lowerbound, which may also be sufficient for model selection. We leave this as an open question for future work.

