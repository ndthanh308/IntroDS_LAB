\section{Method}

\looseness -1 As warm-up, consider an example with deterministic agents, i.e., when $p_{t,j}$ is a Dirac measure on a specific action $\vx_{t,j}$. 
Suppose it was practically feasible to draw the action suggested by every agent and observe the corresponding reward vector \smash{$\vr_t = (r_{t,j})_{j=1}^M$}.
In this case, model selection becomes a full-information online optimization problem, and we can design a minimax optimal algorithm as follows. We assign a probability distribution $\vq_{t} = (q_{t,j})_{j=1}^M$ to the models, and update it such that the overall average return \smash{$\sum_{t=1}^n \vq^\top_{t} \vr_t$} is competitive to the best agent's average return \smash{$\sum_{t=1}^n r_{t,j^\star}$}. 
At every step, we update \smash{$q_{t+1,j} \propto \exp(\sum_{s=1}^{t} r_{s,j})$}, since such exponential weighting is known to lead to an optimal solution for this classic online learning problem  \citep{cesa2006prediction}.
In our setting however, the agents are stochastic, and we do not have access to the full $\vr_t$ vector.


We propose the \textsc{\textbf{A}}nytime \textsc{\textbf{Exp}}onential weighting algorithm based on \textsc{\textbf{L}}asso reward estimates (\alexp),  summarized in \cref{alg:lassoexp}.
  At step $t$ we first sample an agent $j_t$, and then sample an action $\vx_t$ according to the agent's policy $p_{t, j_t}$. 
Let $\Delta_M$ denote the $M$-dimensional probability simplex.
We maintain a probability distribution $\vq_{t} \in \Delta_M$ over the agents, and update it sequentially as we accumulate evidence on the performance of each agent. 
Ideally, we would have adjusted $q_{t,j}$ according to the average return of model $j$, that is,  $\E_{\vx \sim p_{t,j}} r(\vx)$. However, since $r$ is unknown, we estimate the average reward with some $\hat r_{t,j}$. We then update $\vq_{t}$ for the next step via,
\[
q_{t+1, j} = \frac{\exp(\eta_t \sum_{s=1}^{t}\hat r_{s,j})}{\sum_{i=1}^M \exp(\eta_t \sum_{s=1}^t\hat r_{s,i})}
\]
for all $j = 1, \dots, M$, where $\eta_t$ is the learning rate, and controls the sensitivity of the updates. 
This rule allows us to imitate the full-information example that we mentioned above. By utilizing $\hat r_{t,j}$ and hallucinating feedback from all agents, we can reduce the probability of selecting a badly performing agent, without ever having sampled them (c.f. \cref{fig:prob_curves}). 
 It remains to design the estimator $\hat r_{t,j}$.
 We concatenate all feature maps, and, knowing that many features are redundant, use a sparsity inducing estimator over the resulting coefficients vector. Mainly, let $\vphi(\vx) \coloneqq (\vphi_1(\vx), \dots, \vphi_M(\vx))$, and $\vtheta = (\vtheta_1, \dots, \vtheta_M) \in \sR^{Md}$ be the concatenated coefficients vector. We then solve
\begin{equation}\label{eq:glasso} 
    \hat \vtheta_t = \argmin_{\vtheta \in \sR^{Md}} \calL(\vtheta; H_t, \lambda_t) = \argmin_{\vtheta \in \sR^{Md}} \frac{1}{t} \norm{ \vy_t- \Phi_t\vtheta}^2_2 + 2\lambda_t\sum_{j=1}^M  \norm{\vtheta_j}_2
\end{equation}
where $\Phi_t = [\vphi^\top(\vx_s)]_{s\leq t} \in \sR^{t \times Md}$ is the feature matrix, $\vy_t \in \sR^t$ is the concatenated reward vector, and $\lambda_t$ is an adaptive regularization parameter. 
Problem \eqref{eq:glasso} is the online variant of the group Lasso \citep{lounici2011oracle}, and enforced sparsity at group level. Therefore, the sub-vectors \smash{$\hat\vtheta_{t,j} \in \sR^{d}$} that correspond to redundant feature maps are expected to be $\boldsymbol{0}$, i.e. the null vector. 
We then estimate the average return of each model by simply taking an expectation \smash{$\hat r_{t,j} = \E_{\vx \sim p_{t+1,j} } [\hat\vtheta_t^\top\vphi(\vx)]$}. This quantity is the average return of the agent's policy $p_{t+1,j}$, according to our Lasso estimator.
In \cref{sec:properties} we explain why the particular choice of Lasso is crucial for obtaining a $\log M$ rate for the regret. \looseness -1

For action selection, with probability $\gamma_t$, we sample agent $j$ with probability $q_{t,j}$ and draw $\vx_t \sim p_{t,j}$ as per suggestion of the agent. With probability $1-\gamma_t$, we choose the action according to some exploratory distribution $\pi \in \calM(\calX)$ which aims to sample informative actions.
This can be any design where $\mathrm{supp}(\pi) = \calX$. 
We mix $p_{t,j}$ with $\pi$, to collect sufficiently diverse data for model selection. We are not restricting the agents' policy, and therefore can not rely on them to explore adequately. 
In \cref{thm:regret_main}, we choose a decreasing sequence of $(\gamma_t)_{t\geq 1}$ the probabilities of exploration at step $t\geq 1$, since less exploration will be required as data accumulates.
To conclude, the action selection policy of \alexp is formally described as the mixture
\vspaceequation
\[
p_t(\vx) = \gamma_t \pi(\vx) + (1-\gamma_t) \sum_{j=1}^M q_{t,j}p_{t,j}(\vx).
\]

\begin{algorithm}[t]
\caption{\alexp \label{alg:lassoexp}}
\begin{algorithmic}
\State Inputs: $\pi, (\gamma_t,\eta_t,\lambda_t)$ for $t \geq 1$
\State Let $\vq_1 \leftarrow \text{Unif}(M)$ and initialize base agents $(p_{1,1}, \dots, p_{1,M})$.
\For{$t \geq 1$}
\State Draw $\alpha_t\sim \mathrm{Bernoulli}(\gamma_t)$. \Comment{Decide to explore or exploit}
\If{$\alpha_t = 1$}
\State Choose action $\vx_t$ randomly according to $\pi$. \Comment{Explore}
\Else 
\State Draw $j_t\sim \vq_{t}$.  \Comment{Select an agent}
\State Draw $\vx_t \sim p_{t,j_t}$. \Comment{Select the action suggested by the agent}
\EndIf
\State Observe $y_t = r(\vx_t) + \varepsilon_t$. \Comment{Receive reward}
\State $H_t = H_{t-1} \cup \{(\vx_t, y_t)\}$. \Comment{Append history}
\State $\hat \vtheta_{t} \leftarrow \argmin \calL(\vtheta; H_t, \lambda_t)$. \Comment{Update the parameter estimate}
\State Report $H_t$ to all agents, and get updated policies $p_{t+1,1}, \dots, p_{t+1,M}$. \Comment{Update agents}
\State Update estimated average return of every agent
\[
\hat r_{t,j} \leftarrow \E_{\vx \sim p_{t+1,j}} [\hat \vtheta_{t}^\top\vphi(\vx)], \quad  j=1, \dots, M
\]
\State Update agent probabilities
\[
\vspaceequation
q_{t+1,j} \leftarrow \frac{\exp(\eta_t \sum_{s=1}^t\hat r_{s,j})}{\sum_{i=1}^M \exp(\eta_t \sum_{s=1}^t\hat r_{s,i})}
\vspaceequation
\]
\EndFor
\end{algorithmic}
\end{algorithm}
\vspacefigure
