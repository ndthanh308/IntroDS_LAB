\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage[backend=biber, style=numeric]{biblatex}
\usepackage{derivative}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{placeins}
\usepackage[ddmmyyyy]{datetime}
\usepackage{xcolor}
\usepackage{nicematrix}
\usepackage{booktabs}
\usepackage{lscape} 
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}
\usepackage{longtable}
\usepackage{geometry}
\usepackage{lscape}
\usepackage{caption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{array}
\usepackage{vcell}
\usepackage{comment}
\usepackage{authblk}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}


\begin{comment}
    
\newenvironment{changemargin}[2]{%
  \begin{list}{}{%
    \setlength{\topsep}{0pt}%
    \setlength{\leftmargin}{#1}%
    \setlength{\rightmargin}{#2}%
    \setlength{\listparindent}{\parindent}%
    \setlength{\itemindent}{\parindent}%
    \setlength{\parsep}{\parskip}%
  }%
  \item[]}{\end{list}}

\end{comment}
\bibliography{Literatur}

\title{Memory Effects, Multiple Time Scales and Local Stability in Langevin Models of the S\&P500 Market Correlation}




\author[1,2,$\dagger$,*]{Tobias Wand}
\author[1,2,$\dagger$]{Martin Heßler}
\author[2]{Oliver Kamps}

\affil[1]{\small Institut für Theoretische Physik, Westfälische Wilhelms-Universität Münster}
\affil[2]{Center for Nonlinear Science Münster, Westfälische Wilhelms-Universität Münster}
\affil[$\dagger$]{These authors contributed equally to this work.}
\affil[*]{Corresponding author's e-mail address: t\_wand01@uni-muenster.de}
\date{%
 %   $^1$WWU Münster, Institut für Theoretische Physik\\%
  %  $^2$Center for Nonlinear Science Münster\\[2ex]%
    \today
}
% Comment



\begin{document}

\maketitle


\begin{abstract}
The analysis of market correlations is crucial for optimal portfolio selection of correlated assets, but their memory effects have often been neglected. In this work, we analyse the mean market correlation of the S\&P500 which corresponds to the main market mode in principle component analysis. We fit a generalised Langevin equation (GLE) to the data whose memory kernel implies that there is a significant memory effect in the market correlation ranging back at least three trading weeks. The memory kernel improves the forecasting accuracy of the GLE compared to models without memory and hence, such a memory effect has to be taken into account for optimal portfolio selection to minimise risk or for predicting future correlations. Moreover, a Bayesian resilience estimation provides further evidence for Non-Markovianity in the data and suggests the existence of a hidden slow time scale that operates on much slower times than the observed daily market data. Assuming that such a slow time scale exists, our work supports previous research on the existence of locally stable market states.
\end{abstract}

\paragraph{Keywords:}%\mbox{}\\
Langevin Equation, Econophysics, Bayesian Estimation, Memory Effects, Non-Markovian Dynamics

\section{Introduction}

The S\&P500 is an aggregated index of the stocks of the 500 largest companies traded at US stock exchanges and therefore serves as an indicator of the overall US economic performance. Estimating and predicting the correlation between different assets is crucial for optimal portfolio selection and risk management and has been the focus of financial research since Markowitz's portfolio theory \cite{Markowitz1952}. \\

Because the economy is a system with a large number of interacting parts (traders and companies), it is amenable to the analysis tools from complex systems science \cite{MantegnaStanleyBook}. Due to the increasing availability of data for the economy, highly data-driven methods can be applied in this field of research, with many researchers choosing to focus on the correlations between the relative price changes of different stocks, i.e. the correlations of the stocks' returns \cite{BROWN1989, RandomMatrixLalouxPotters,MantegnaCorrelations,Markowitz1952,Review_CorrelationsHierarchiesClustering_FinancialMarkets,RandomMatrixStanley}. 
In \cite{Mnnix2012,Rinn2015DynamicsOQ,Stepanov2015} the authors identified states of the economy by analysing correlation matrices of daily S\&P 500 data and found eight clusters. These can be interpreted as economic states reflecting e.g. times of economic growth or crises \cite{Mnnix2012} and are found to be locally stable \cite{Rinn2015DynamicsOQ,Stepanov2015}. Further analyses showed that exogenous events, precursors for crises and collective market behaviour can also be identified via the correlation matrix approach \cite{Heckens_2020,Heckens_2022,Heckens_2022b}.\\

Ever since Bachelier's seminal work introduced the random walk model to describe stock prices \cite{Bachelier}, the inherent stochasticity of financial data has been taken into account by researchers and practitioners alike. The Langevin equation is a model for stochastic differential equations that includes a deterministic drift and a random diffusion and can be used to describe such stochastic data. Much research has been devoted to estimating Langevin equations from data \cite{FRIEDRICH2000217,SIEGERT1998275,PhysRevLett.87.254501,PhysRevLett.89.149401,PhysRevLett.89.149402,KLEINHANS2007194,Willers2021} and Langevin equations have found applications in various fields of research such as fluid dynamics \cite{FluidDynamics_DriftDiffusion}, molecular dynamics \cite{GLE_MolecularSimulation} and meteorology \cite{Langevin_Meteorology} (cf. \cite{FRIEDRICH201187} for a review of applications). The generalised Langevin equation (GLE) expands the Langevin model by introducing a kernel function to include memory effects \cite{MoriGLE}. Bayesian statistics includes a collection of methods that reverse the classical approach to statistics and focuses on calculating posterior distributions of model parameters as probabilities conditional on the observed data \cite{Sivia2006-hh,von2014bayesian}. This approach enables an efficient estimation of Non-Markovian Langevin equations \cite{GLE_Clemens,Langevin_CorrelatedNoise_Clemens}. Also, other time series analysis methods can be implemented in the Bayesian frame work to e.g. detect change points in time series data \cite{MartinHessler}.\\ 
We show that an estimated GLE model manages to achieve a high goodness-of-fit for the correlation time series and implies a strong memory effect which has to be taken into account when predicting future market correlations. Furthermore, we perform a detailed comparison of a Markovian mono-time scale and a Non-Markovian two-time scale model with a hidden slow time scale which confirms the GLE analysis results regarding Non-Markovian memory effects. Additionally, the analysis supports the local quasi-stationary economic state theory discussed in Stepanov et al. \cite{Stepanov2015} and provides some evidence that a hidden slow time scale might be involved in the mean market correlation dynamics. It is not far-fetched to assume that a complex system like the human economy involves a multitude of time scales and our findings coincide with such reasoning. The slow time scales might be connected to business and innovation cycles or similar economic dynamical mechanisms even though we could not extract a quantitatively reasonable magnitude of the hidden slow time scale.\\   
The remainder of this article is structured as follows: Section \ref{sec:Methods} explains the data gathering and preprocessing in \ref{sec:Data}, the Bayesian methodology in \ref{sec:Bayes}, the GLE estimation procedure in \ref{sec:GLE_Methods} and the resilience analysis in subsection \ref{subsec:resilience theory}. Section \ref{sec:Results} describes the goodness-of-fit and the estimated memory parameters for the GLE model in \ref{sec:GLE_results} as well as the resilience results including the two-time scale discussion in section \ref{subsec:resilience results}. Finally, the results of these analyses are interpreted and compared to the results from \cite{Stepanov2015} in section \ref{sec:Discussion}.




\section{Data and Methods}
\label{sec:Methods}

\subsection{Data Preparation}
\label{sec:Data}
The S\&P 500 is a stock index containing 500 large US companies  traded at American stock exchanges. Daily stock data from these companies were downloaded via the Python package \textit{yfinance} \cite{yfinance} for the time period between 1992 and 2012, which is the same time period as in \cite{Rinn2015DynamicsOQ}. Only stock data of companies that were part of the S\&P 500 index during at least 99.5\% of the time were used for this analysis. If a company's price time series $P_t$ is not available for the full time period, the remaining 0.5\% of the price time series data are interpolated linearly with the \textit{.interpolate()} method in \textit{pandas} \cite{reback2020pandas, mckinney-proc-scipy-2010}. Overall, there are 249 companies' time series for 5291 trading days with one observation per date. Each company's returns $R_t = (P_{t+1} - P_t)/P_t$ are locally normalised to remedy the impact of sudden changes in the drift of the time series with the method introduced in \cite{SCHAFER20103856} as
\begin{equation}
    r_t = \frac{R_t - \langle R_t\rangle_n}{\sqrt{\langle R^2_t\rangle_n - \langle R_t\rangle^2_n}}.
\end{equation}
Here, $\langle\dots\rangle_n$ denotes a local mean across the $n$ most recent data points, i.e. $r_t$ is subjected to a standard normalisation transformation with respect to the local mean and standard deviation (i.e. volatility $\sigma$). Following \cite{Mnnix2012}, $n=13$ was chosen for the daily data. For each time step $t$ and each pair of sectors $i,j$ the local correlation coefficients 
\begin{equation}
\label{eq:Correlation}
    C_{i,j} = \frac{\langle r^{(i)}_t r^{(j)}_t\rangle_\tau - \langle r^{(i)}_t \rangle_\tau  \langle r^{(j)}_t \rangle_\tau}{ \sigma^{(i)}_\tau \sigma^{(j)}_\tau  }
\end{equation}
are calculated over a time period of $\tau=42$ trading days like in \cite{Mnnix2012} (the 42 working days correspond to 2 trading months) with the local standard deviations $\sigma^{(i)}_\tau$. As shown via Principle Component Analysis in \cite{Stepanov2015}, the mean correlation
\begin{equation}
    \Bar{C} = \frac{1}{N} \sum_\textmd{i,j} C_{i,j}
\end{equation}
already describes much of the variability in the data. Hence, it makes sense to restrict the analysis to this one-dimensional time series $\Bar{C}(t)$ (shown in figure \ref{fig:5vs42}). The time $t$ is here selected as the central value of the time window of length $\tau$ in order to have a symmetrical window. The preprocessed time series is available via \cite{DataZenodo}.

% Figure environment removed
\subsection{Bayesian Statistics}
\label{sec:Bayes}

Bayes' theorem for the conditional probability distributions $f(\cdot|\cdot)$ of model parameters $\theta$ and observed data 
\begin{equation}
\label{eq:Bayes theorem}
f_{post}(\theta|x) \sim f_{prior}(\theta) f(x|\theta)
\end{equation}
connects the standard statistical likelihood $f(x|\theta)$ and prior knowledge about the model parameters $f_{prior}$ to a posterior distribution $f_{post}$ of the unknown model parameters conditional on the observed data \cite{BayesTheorem}. A bundle of methods have been derived from this approach and are collectively referred to as Bayesian Statistics \cite{Sivia2006-hh,von2014bayesian}. Markov chain Monte Carlo algorithms (MCMC) can be used to infer the posterior distribution by simulating several random walkers that explore the (potentially high-dimensional) posterior distribution  \cite{Metropolis-Hastings}. It generates samples of the model parameters which are uncorrelated after cutting off the first $n_{burn}$ exploration steps as a burn-in period and thinning the remaining samples. The resulting MCMC samples can also be used to integrate the posterior distribution over all but one model parameter $\theta_i$ to derive the marginal posterior distribution $f(\theta_i|x)$ of this single parameter. Parameter estimation can then be done via the mean of the posterior distribution or via its maximum (abbreviated as MAP for maximum a posteriori) and credible intervals (CIs) of e.g. $95\%$ credibility can be directly derived from the quantiles of the samples for $\theta_i$.

\subsection{Fitting a Generalised Langevin Equation with Memory Kernel}
\label{sec:GLE_Methods}
Sections 4 and 5 of \cite{Stepanov2015} analyse the one-dimensional mean correlation time series by fitting a Langevin model
\begin{equation}\label{eq:Langevin}
    \odv{\Bar{C}}{t} (t) = D^{(1)}\left(\Bar{C},t\right) +\sqrt{ D^{(2)}\left(\Bar{C}   \right)} \Gamma(t)
\end{equation}
with independent Gaussian noise $\Gamma$, a deterministic time-dependent drift function $D^{(1)}$ and a time-independent diffusion function $D^{(2)}$. Note that the Langevin equation is a Markovian model, i.e. it has no memory. As an alternative model, a Generalised Langevin Equation (GLE) includes previous values of the time series in a memory kernel $\mathcal{K}$, but has only time-independent parameters with a functional equation
\begin{equation}
   \odv{\Bar{C}}{t} (t)= D^{(1)}\left(\Bar{C}\right) + \int_{s=0}^t \mathcal{K}(s) \Bar{C}(t-s) \diff s + \sqrt{ D^{(2)}\left(\Bar{C}   \right)} \Gamma(t).
   \label{eq:GLE}
\end{equation}


A Bayesian estimation of the parameters of equation $\eqref{eq:GLE}$ is implemented in \cite{GLE_Clemens}. It discretises the memory kernel $\mathcal{K}(k)$ to $\mathcal{K}_k$ and discretises the observed data into $n_B$ different bins to significantly speed up the estimation procedure. Because an overlap of the windows used to calculate $\Bar{C}$ can lead to artefacts in the memory effects (cf. section \ref{app:OverlapGLE} in the appendix), we choose to calculate $\Bar{C}$ with different parameters than $\tau=42$ and $s=1$ from \cite{Stepanov2015} (every $s^{th}$ is retained for the analysis and hence for $s=1$, the full time series is used). Instead, we set $\tau = 5$ and $s=5$, meaning that the mean market correlation $\Bar{C}$ is calculated over one trading week for the end of the trading week in disjoint windows (i.e. we create a time series whose length is $1/s = 20\%$ of the original time series). Because using disjoint intervals automatically leads to a thinning of the time series, this seemed like a useful trade-off with a real-world interpretation as weekly correlation. Note that although only five trading days contribute to the calculation of $\Bar{C}$, there is still an order of magnitude of $10^4$ correlations $C_{i,j}$ whose mean value is the desired $\Bar{C}$. I.e. although only a short time window is used to calculate each of the $(i,j)$ correlation pairs, the sheer size of the ensemble of $C_{i,j}$ reaffirms our trust in the accuracy of their mean $\Bar{C}$. The resulting time series is shown in figure \ref{fig:5vs42} together with the time series used in \cite{Stepanov2015} and although it obviously becomes much noisier due to the shorter window size, it still retains some of the features of the less noisy time series like e.g. the position of spikes with high correlation. \\


As described in \cite{GLE_Clemens}, the memory effects are aggregated to a quantity $K_k$ (named $\kappa_k$ in \cite{GLE_Clemens}) that measures the strength of all memory effects from $1$ up to $k$ time steps ago. If $K_k$ saturates towards a plateau at step $k_0$, then $k_0$ is the maximum length of any reasonable memory kernel. This often also coincides with kernel values of $\mathcal{K}_{k_0}\approx 0$ at $k_0$. The estimated values for the model parameters are chosen by calculating the marginal posterior distribution and choosing the mean or MAP parameter estimation. The goodness-of-fit is also evaluated by using MCMC to simulate an artificial time series $\Bar{C}^{(a)}_t$ with the estimated model and comparing the autocorrelation function and the distributions of $\Bar{C}^{(a)}_t$ and the increments $\Bar{C}^{(a)}_t - \Bar{C}^{(a)}_{t-j}$ to those of the original data. Finally, the inferred model structure can be trained on only a subset of the data (training data) and be used to predict the remaining test data to evaluate its predictive performance.

\subsection{Resilience Estimation}\label{subsec:resilience theory}
By applying Bayes' theorem \eqref{eq:Bayes theorem} we can deduce a quantitative measure of resilience under given model assumptions. Therefore, we infer the parameters of two models of stochastic differential equations in a rolling window approach that allows for resolving the time evolution of the resilience and noise level and accounts for the quasi-stationary nature of the time series that is observed in Stepanov et al. \cite{Stepanov2015}. Following the quasi-stationarity argument we assume to be in a fixed state per window, i.e. the fixed point $\Bar{C}^*$ which is approximated by averaging over the mean market correlation data per window. First, we estimate a Markovian Langevin equation \eqref{eq:Langevin} with the Taylor-expanded drift
\begin{align}
D_{\Bar{C}}^{(1)}(\Bar{C}(t),t) &= \alpha_0(t) + \alpha_1(t) (\Bar{C} - \Bar{C}^*) + \alpha_2(t) (\Bar{C} - \Bar{C}^*)^2 + \alpha_3(t) (\Bar{C} - \Bar{C}^*)^3 + \mathcal{O}(\Bar{C}^4) \\
&= \theta_0 (t; \Bar{C}^*) + \theta_1 (t; \Bar{C}^*) \cdot \Bar{C} + \theta_2 (t; \Bar{C}^*) \cdot \Bar{C}^2 + \theta_3 (t; \Bar{C}^*) \cdot \Bar{C}^3 + \mathcal{O}(\Bar{C}^4)
\end{align}
with the drift parameters $\theta_ {0,...,3}(t;\Bar{C}^*)\equiv \theta_ {0,...,3}(\Bar{C}^*)$ per rolling window and the constant diffusion $D^{(2)}(\Bar{C})=\theta_4^2\equiv const. =: \sigma^2$. We choose uninformed priors which are given by 
\begin{align}
p_{\rm prior}(\theta_0,\theta_1) = \frac{1}{2\pi (1+\theta_1^2)^\frac{3}{2}}
\end{align}
for the linear part of the drift function and the Jeffreys scale prior \cite{von2014bayesian}
\begin{align}
p_{\rm prior}(\theta_4 ) = \frac{1}{\theta_4}
\end{align}
for the noise level $\theta_4$ with suitable prior ranges. The priors of higher order parameters are chosen to be
\begin{align}\label{eq:gaussian prior}
&p_{\rm prior}(\theta_2) = \mathcal{N}(\mu = 0, \tilde{\sigma} = 4) \text{    and   } \\ &p_ {\rm prior}(\theta_3) = \mathcal{N}(\mu = 0, \tilde{\sigma} = 8)
\end{align}
with Gaussian distributions $\mathcal{N}$ centred around the mean $\mu = 0$ with a standard deviation $\tilde{\sigma} = 4$ and $\tilde{\sigma} = 8$.\\ 
Since we consider economic systems to operate on multiple time scales which concurrently lead often to Non-Markovian time series, we additionally introduce a two-dimensional Non-Markovian model analogue to Willers and Kamps \cite{Willers2021}. The model takes the form
\begin{align}\label{eq:NonMarkov model}
    \odv{\Bar{C}(t)}{t} &= D_{\Bar{C}}^{(1)}(\Bar{C},t) + \sqrt{D_{\Bar{C}}^{(2)}(\Bar{C},t)} \cdot \lambda\\
    \odv{\lambda(t)}{t} &= D_{\lambda}^{(1)}(\lambda,t) + \sqrt{D_{\lambda}^{(2)}(\lambda,t)} \cdot \Gamma (t) 
\end{align}
with a hidden Ornstein-Uhlenbeck process (OU-process) $\lambda$, drift $D_{\lambda}^{(1)}(\lambda,t) = -\frac{1}{\theta_5^2} \cdot \lambda$ and diffusion $D_{\lambda}^{(2)}(\lambda,t) = \frac{1}{\theta_5^2}$. Drift $D_{\Bar{C}}^{(1)}(\Bar{C},t)$ and diffusion $D_{\Bar{C}}^{(2)}(\Bar{C},t)$ of the observed process $\Bar{C}(t)$ remain unchanged. The Non-Markovian analogue to the constant noise level $\sigma^2$ of the Langevin equation is given by the composite noise level
\begin{align}
    \Psi = \sqrt{D_{\Bar{C}}^{(2)}(\bar{C},t)}\cdot \sqrt{D_{\lambda}^{(2)}(\lambda, t)}\cdot h
\end{align}
with the small discrete sampling time step $h$.\\
For the OU-process, an invariant prior of a straight line and a scale prior for the diffusion are multiplied:
\begin{align}
    p_{\rm prior}(\theta_5) = \frac{\theta_5}{2\pi \left(1+\left(-\frac{1}{\theta_5}\right)^2\right)^\frac{3}{2}}.
\end{align}
Furthermore, via the prior we introduce a pre-defined time scale separation of the time scales $\tau_{\Bar{C}}$ and $\tau_\lambda$ of the observed and unobserved process, respectively, i.e. we require either $\tau_{\Bar{C}}>\gamma\cdot \tau_{\lambda}$ or $\tau_{\lambda}>\gamma\cdot \tau_{\Bar{C}}$ with a scale separation coefficient $\gamma$. The characteristic time scales \cite{b:strogatz} are approximated by
\begin{align}
    \tau_\nu = \left\vert\left.\left(\odv{D_\nu^{(1)}(\nu,t)}{\nu}\right)^{-1}\right\vert\right\vert_{\nu = \nu^*}\qquad \text{ with }\nu\in\lbrace\Bar{C},\lambda\rbrace.
\end{align}
The priors for the model of the observed data $\Bar{C}(t)$ remain unchanged as well apart from the term $D_{\Bar{C}}^{(2)}(\Bar{C},t)= \theta_4^2$ which corresponds to a coupling constant in the Non-Markovian model. We simply apply a Gaussian prior like the one in equation \eqref{eq:gaussian prior} to $\theta_4$ in this case.\\
Inspired by the formalism of linear stability analysis for both models we calculate the drift slope
\begin{equation}
\zeta = \left. \frac{\text{d}D_{\Bar{C}}^{(1)}(\Bar{C})}{\text{d}\Bar{C}}\right\vert_{\Bar{C} = \Bar{C}^*}
\end{equation}
per window as a Bayesian parametric resilience measure. For the Markovian model the calculations are performed with the open-source Python toolkit \textit{antiCPy} \cite{url:GitHessler2021, url:DocsHessler2021}. In this modelling framework, a stable state corresponds to a negative drift slope $\zeta$, whereas a changing sign indicates destabilisation via a bifurcation. More details on the procedure can be found in \cite{MartinHessler}.\\


\section{Results}
\label{sec:Results}


\subsection{Estimated GLE Model}
\label{sec:GLE_results}
The data is split into $n_B=10$ equally wide bins and an initial modelling attempt with a rather long kernel $k_{\max} = 10$ is tested, i.e. $\mathcal{K}_q = 0$ for all $q>k_{\max}$. It shows a plateau emerging at around $k\geq 6$ (cf. appendix \ref{app:GLE_Kernel}). Hence, a model with $k_{\max}=6$ is used as a reasonable length for the memory kernel and its goodness-of-fit is evaluated. We then use a very conservative estimation of the memory up to $k_{\max} = 3$ to evaluate the GLE's predictive power.

\subsubsection{Goodness-of-fit}
A time series of length $10^5$ is simulated via Euler-Maruyama integration of the GLE model to compute the autocorrelation function (ACF) and to compare it to the ACF of the original time series. The best model is chosen via MAP estimation in the Bayesian framework of \cite{GLE_Clemens}, but selecting the mean estimation yields almost identical results. Both ACFs are computed via the function \textit{statsmodels.tsa.stattools.acf} from the Python package \textit{statsmodels} \cite{seabold2010statsmodels}. Figure \ref{fig:ACF} shows that the two ACFs show very good agreement up to lags of 10 trading weeks and decent agreement up to lags of 20 weeks. The distributions of the time series and the first two increments for the original and the simulated data are shown in figure \ref{fig:IncrementDistribution} for the MAP estimation and also for the mean estimation. Figure \ref{fig:IncrementDistribution} shows an almost perfect overlap between the two estimation procedures and a good agreement between the estimated time series and the original time series, especially for the increment distributions. Overall, these diagnostics indicate that the estimated model with memory kernel length $6$ manages to reproduce these important statistical properties of the original time series.







% Figure environment removed




% Figure environment removed

Because the realised values of the time series $\Bar{C}$ are not distributed uniformly, we also use a modelling procedure with unequal bin widths so that each of the $10$ bins has the same amount of data. However, this barely changes the model diagnostics shown in figures \ref{fig:ACF} and \ref{fig:IncrementDistribution}. The only noticeable change is that for long kernels with length $\geq10$, the ACF seems to be captured a little worse in the shown range of figure \ref{fig:ACF}, but manages to fit more closely to the empirical ACF for large lags at around $r\approx 100$. While the regular LE without memory has a very similar increment distribution, the ACF is much worse than for the GLE.

\subsubsection{Estimated Memory Kernel}
\label{sc:EstimateKernelLength}
To make further inference on the memory kernel and to estimate its quantitative effect, the posterior distribution of the model with memory length 6 is sampled via MCMC. 100 walkers are simulated for $10^5$ time steps and after a burn-in period of 450 initial time steps is discarded, the chains are thinned by only keeping every $450^\textmd{th}$ step to obtain uncorrelated samples. Bayesian CIs can now be calculated at the $95\%$ level for each parameter in the kernel function and the results are shown in figure \ref{fig:Kernel6_CredIntervals}. \\
The CIs of $\mathcal{K}_4$ are already very close to the zero line and those of $\mathcal{K}_5$ include zero, meaning that there is no evidence for a memory term at five weeks distance. Interestingly, the CIs for $\mathcal{K}_6$ clearly exclude the value zero. Because it is difficult to exactly identify the beginning of the plateau in the memory aggregation in figure \ref{fig:Kappa_Kernel10}, it may be possible that the plateau already emerges at $k=5$ and that the nonzero memory kernel $\mathcal{K}_6$ is therefore unreliable. However, the results in figure \ref{fig:Kernel6_CredIntervals} clearly imply a nonzero memory effect for four time steps with a  clearly nonzero effect strength for memories up to $k=3$ trading weeks.


% Figure environment removed


\subsubsection{Prediction via the GLE with Kernel Length 3}
\label{sec:Predict}
With a conservative interpretation of the results in section \ref{sc:EstimateKernelLength}, a model with memory length $k=3$ is used to evaluate the GLE's power to predict a future value $y_{t+1}$ by forecasting an accurate prediction $\hat{y}_{t+1}$. It is tested against a regular Langevin equation (LE) model without any memory effects (corresponding to $k=0$ and also estimated with the code in \cite{GLE_Clemens}) and against the naive benchmark of predicting the next time step $y_{t+1}$ by simply setting it to the last previously known value: $\hat{y}_{t+1} = y_t$. To test these three methods, the Langevin models are trained on the first $\alpha\%$ of the time series (the training data) and the predictions of the GLE, the LE and the naive forecast are evaluated on both the training data (as in-sample predictions) and on the remaining $1-\alpha\%$ test data (as out-of-sample predictions). The coefficient of prediction $\rho^2$ is used to evaluate their  predictive accuracy. If $n$ observations $y_{1,\dots,n}$ are forecasted as $\hat{y}_{1,\dots,n}$, then the coefficient of prediction is given by
\begin{equation}
    \rho^2 = 1 - \frac{  \sum_{i=1}^n (\hat{y}_i - y_i)^2  }{ \sum_{i=1}^n(\Bar{y} - y_i)^2 }
\end{equation}
with the $\Bar{y}$ denoting the mean value. It takes the value of $\rho^2=1$, if the prediction is always exactly true, and $\rho^2 = 0$, if the prediction is only as accurate as always using the mean $\Bar{y}$, and $\rho^2 <0$, if it is less accurate than using the mean and is computed via the function \textit{sklearn.metrics.r2\_score} from \cite{scikit-learn}. The results in table \ref{tab:Predictions} show that the GLE model consistently achieves the highest accuracy on in-sample and out-of-sample predictions for the three chosen test data sizes. Notably, the negative $\rho^2$ of the naive method for the test data indicates that the out-of-sample prediction is by no means trivial, meaning that the low, but positive $\rho^2$ of the GLE on the test data is nevertheless a good performance. The GLE achieves slightly better results than the LE, indicating that the memory effect should be taken into account for prediction tasks. Figure \ref{fig:Predictions} shows the predictions of the LE and GLE for the in-sample and out-of-sample predictions with $\alpha = 90\%$. The same visual comparison between naive forecast and the GLE can be found in the appendix \ref{app:GLE_Predict}.% If the LE is only fitted to the last four trading years (corresponding to 20\% of the time frame) in the training data comparable to the procedure in \cite{Stepanov2015}, its $\rho^2$ on the remaining 20\% test data is also below 0. 



\begin{table}[h!]
    \centering
   
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{|p{25mm}|ccc|ccc|}
    \hline
    \multirow{2}{25mm}{\textbf{   \\ 
    Model\hspace{10mm}$\alpha$}} & \multicolumn{3}{c|}{\textbf{In-Sample}}  & \multicolumn{3}{c
    |}{\textbf{Out-Of-Sample}}  \\
    % \hline
    % \textbf{Inactive Modes} & \textbf{Description}\\
    \cline{2-7}
    & 80\% & 85\% & 90\% & 80\% & 85\% & 90\% \\
    %\hhline{~--}
    \hline
    Naive & 0.07 & 0.15 & 0.19 & -0.21 & -0.15 & -0.11 \\ \hline
    LE & 0.29 & 0.32  & 0.35 & -0.14 & -0.01 & 0.08 \\ \hline
    GLE & 0.39 & 0.42 & 0.45 & 0.01 & 0.08 & 0.10 \\ \hline
  \end{tabular}
    \caption{Comparing the predictions of the naive forecast $\hat{y}_{t+1} = y_t$, the Langevin equation (LE) and the GLE with memory kernel length $k=3$ via their $\rho^2$ score on in-sample training data and out-of-sample test data ($\alpha\%$ training data and the last $1-\alpha\%$ of the time series as test data). Note that the partially negative $\rho^2$ for the test data indicates that the test data is quite difficult to predict, which corresponds to the high fluctuations in the final part of the time series in figure \ref{fig:5vs42}.}
    \label{tab:Predictions}
\end{table}


% Figure environment removed

\subsection{Hidden Slow Time Scale and Non-Markovianity}\label{subsec:resilience results}

Applying the resilience analysis method, described in subsection \ref{subsec:resilience theory}, we can deduce some interesting evidence for multiple time scales and further confirmation of Non-Markovianity present in the considered economic time series. The results for both the simple Markovian and multi-scale Non-Markovian model (cf. \eqref{eq:Langevin} and \eqref{eq:NonMarkov model}, respectively) are compared in figure \ref{fig: markov nonmarkov comparison} (a-c). For better readability the parameters of the calculations are listed in table \ref{table: resilience params} of appendix \ref{sec: SI resilience}.\\


% Figure environment removed

Stepanov et al. \cite{Stepanov2015} argue for quasi-stationary economic states which are occupied over finite time periods, before they transition into another quasi-stationary economic state. The approximated state potentials from the mean market correlation in the article \cite{Stepanov2015} suggest that there might be shifts in the fixed point positions over time, but no bifurcation-induced tipping (B-tipping) is involved, i.e. no qualitative change of stable and unstable fixed points or attractors is observed. Instead of B-tipping mechanisms, the intrinsic economic stochasticity drives the jumps between alternative quasi-stationary states, which is a mechanism basically related to noise-induced tipping (N-tipping).\\
If we choose a model that captures the key features of the data generating process, we should thus be able to uncover generally negative drift slope estimates $\hat{\zeta}$, corresponding to data of a locally quasi-stationary state in each rolling window of the mean market correlation $\bar{C}(t)$ the data of which is shown again in figure \ref{fig: markov nonmarkov comparison} (a).\\ 
%These considerations taken seriously we should be able to apply the resilience analysis described in subsection \ref{subsec:resilience theory} and uncover generally negative drift slopes in each rolling window, corresponding to data of a locally quasi-stationary state, if we choose a model that captures the key features of the data generating process. The results of the study are presented in figure \ref{fig: markov nonmarkov comparison}.\\
%The drift slope estimates $\hat{\zeta}$ derived from the Markovian model are shown in blue with orange CBs in figure \ref{fig: markov nonmarkov comparison}. The noise level estimates $\hat{\sigma}$ almost agree with the Non-Markovian estimates $\hat{\Psi}$, but the expected local stability is not resolved employing the Markovian model. Instead of negative drift slopes $\hat{\zeta}$, the estimates suggest persistently latent instability with $\hat{\zeta}\approx 0$. %The parameters that apply for the calculation are the same as before apart from $15000$ MCMC steps per walker.
In this spirit, we find the Non-Markovian model with an unobserved slow time scale, i.e. $\tau_\lambda>\gamma\cdot\tau_{\Bar{C}}$ with $\gamma = 2$, to yield the expected result of negative drift slope estimates $\hat{\zeta}$ as presented in figure \ref{fig: markov nonmarkov comparison} (b) and indicated by the red solid line with green credibility bands (CBs).\\
This result is supported by rather intuitive qualitative considerations: Economic processes are well-known to operate on various fast and slow time scales, on the one hand e.g. high frequency trading, trading psychology and sudden external events, whether that might be political, sociological, severe weather or other impacts. These fast-scale processes could be termed as ``economic weather'' in a climatologist's metaphor. On the other hand, long-term ``economic climate evolution'' takes place on much slower time-scales. Examples could be innovation processes and technical revolutions like the invention of the steam engine or the internet, economic cycle theories like that of Marx ($\tau\sim\SIrange{2}{10}{a}$) \cite{b:Marx2014,HARVEY2007618}, Keynes, Schumpeter or Kondratjew \cite{Bernard2013, Kondratieff,Schumpeter}, cycles of fiscal and demographic developments \cite{TurchinBook}, cultural evolution \cite{wand2023characteristic} and generation changes influencing economic reasoning, adaptions to climate change and the scarcity of resources and much more. Keeping that in mind, the trading day resolution of the data is rather fine-grained and it might be reasonable to assume that a hidden slow time scale is present in the data. The slow time scale of the presented Non-Markov model is determined by the upper boundary of the prior range which is chosen to correspond roughly to \SI{6}{a} which is the time of an average business cycle and coincides with the first zero-crossing of the autocorrelation function of the mean market correlation $\bar{C}(t)$. If the prior is chosen broader the model converges to roughly $\SIrange{700}{4000}{a}$ which is not a plausible magnitude of the time scale of economic evolution (cf. Appendix \ref{sec: SI resilience}). However, the main results of local quasi-stationary economic states is not affected by the prior choice. The estimation of a more reasonable magnitude of the slow time scale without prior restriction might be prohibited by the very limited amount of data per window, since the window range does not even include one complete business cycle which would be the smallest proposed slow time scale candidate. Further note that the two-time scale Non-Markovian model in eq. \eqref{eq:NonMarkov model} incorporates noise in the slow non-observed variable $\lambda$. In that way it could formally reflect intermediate dynamics on a time scale $\tau_{\rm N}$ with $\tau_{\bar{C}}<\tau_{\rm N}< \tau_\lambda$. But since it is coupled to the mean market correlation $\bar{C}(t)$ on trading day resolution, we consider the noise operating on a time scale $\tau_{\rm N}< \tau_{\bar{C}}$.\\
In contrast to the multi-scale Non-Markovian model, the mono-scale Markovian model cannot reflect the local quasi-stationarity of the economic states, postulated in Stepanov et al. \cite{Stepanov2015}, but the drift slope estimates $\hat{\zeta}$, indicated by the blue solid line with orange CBs in figure \ref{fig: markov nonmarkov comparison} (b), suggest persistently latent instability with $\hat{\zeta}\approx 0$. The noise analogues $\sigma$ and $\Psi$ of the Markovian and Non-Markovian model, respectively, are almost identical as observable in figure \ref{fig: markov nonmarkov comparison} (c) following the same color-coding. The noise level seems to increase over the years with a clear increase in the periods of the global financial crisis and the Euro crisis which accounts for a higher N-tipping probability in this highly turbulent economic period. The noise plateau of roughly one window length around the end of the Asian and the beginning of the Russian financial crises around 1998 is probably the results of the outliers that are incorporated into the windows.\\

Since the discussed observations alone only allow for relatively weak qualitative deduction of the discussed features of multi-scaling and Non-Markovianity, we additionally perform an analogous analysis on a synthetic time series $x$ that shares the key features of a hidden slow time scale and Non-Markovianity with the original one. In figure \ref{fig: histogram comparison} we provide a comparison of the per definition stationary first differences, (a) of the original mean market correlation $\bar{C}(t)$ and (b), of the synthetic time series $x$. The noise level in $x$ is assumed to increase over time to mirror the noise level evolution of the mean market correlation $\bar{C}(t)$, suggested by the estimates in figure \ref{fig: markov nonmarkov comparison} (c), and is adjusted to cover almost the range of the first differences in $\bar{C}(t)$. Only the positive trend of the mean market correlation visible in figure \ref{fig: markov nonmarkov comparison} (a) is not included in the simulations of $x$. In that way the PDFs of the two time series' first differences are shaped similar apart from the fact that the highly centered probability mass of the mean market correlation $\bar{C}(t)$ with steep tails due to rare outliers is a bit more smeared out into flatter tails of the synthetic time series $x$. More simulation details can be found in the Appendix \ref{sec: SI resilience}.\\


% Figure environment removed

The resilience analyses on the synthetic time series $x$ are shown in figure \ref{fig: markov nonmarkov comparison} (d-f) and are in very good agreement to the original analyses results in figure \ref{fig: markov nonmarkov comparison} (a-c). That appears at least as an independent qualitative confirmation of our findings' interpretation.\\
Moreover, this interpretation is supported by two additional facts: First, the estimation of a Markovian model on the Gaussian kernel detrended version of the mean market correlation $\bar{C}(t)$ results in similar results to the multi-scale Non-Markovian model. This strengthens our previous findings, because the detrending subtracts a non-stationary slow process suspected to be present in the data. We notice that weaker detrending leads to positive trends in the drift slope estimates $\hat{\zeta}$ which could be due to increasing distortions due to incomplete detrending of the non-stationarity. Second, we fit a multi-scale Non-Markovian model with inverse time scale separation $\tau_{\bar{C}}>\gamma \cdot \tau_\lambda$ with $\gamma = 2$ (i.e. the observed trading day time scale of the mean market correlation $\bar{C}(t)$ is considered to be at least two times slower than the hidden time scale). This leads to results similar to the \textit{Markovian} model with and without detrending. This is an expected result, since the prior restriction of the time scale separation basically restricts the model to the Markovian case in which only the trading day time scale can be resolved apart from an even faster stochastic contribution. In other words, the prior assumption $\tau_{\bar{C}}>\gamma \cdot \tau_\lambda$ with $\gamma = 2$ prohibits the incorporation of an unobserved slower time scale even if it is present in the data. The discussed results are presented in more detail in Appendix \ref{sec: SI resilience}.\\

%These considerations taken seriously we should be able to apply the resilience analysis described in subsection \ref{subsec:resilience theory} and uncover generally negative drift slopes in each rolling window, corresponding to data of a locally quasi-stationary state, if we choose a model that captures the key features of the data generating process. The results of the study are presented in figure \ref{fig: markov nonmarkov comparison}.\\
%The drift slope estimates $\hat{\zeta}$ derived  Markovian model are shown in blue with orange CBs in figure \ref{fig: markov nonmarkov comparison}. The noise level estimates $\hat{\sigma}$ almost agree with the Non-Markovian estimates $\hat{\Psi}$, but the expected local stability is not resolved employing the Markovian model. Instead of negative drift slopes $\hat{\zeta}$, the estimates suggest persistently latent instability with $\hat{\zeta}\approx 0$. The parameters that apply for the calculation are the same as before apart from $15000$ MCMC steps per walker.
%Interestingly, we find the Non-Markovian model with an unobserved slow time scale, i.e. $\tau_\lambda>\gamma\cdot\tau_{\Bar{C}}$ with $\gamma = 2$, to yield the expected result of negative drift slope estimates $\hat{\zeta}$ as presented in figure \ref{fig: markov nonmarkov comparison}. The time series shown in figure \ref{fig:antiCPy} (a) is evaluated in windows of $500$ data points and the drift slopes $\hat{\zeta}$ and the noise level $\hat{\Psi}$ are indicated by the red lines with green CBs in the figures \ref{fig:antiCPy} (b) and (c), respectively. The prior ranges are chosen to be $\theta_{0,...,3}\in [-50,50]$ and $\theta_{4,5}\in [0,50]$, the windows are shifted about $25$ data points and the MCMC is initialized via $50$ walkers and $20000$ steps.\\
%The Markovian counterpart results are shown in blue with orange CBs. The noise level estimates $\hat{\sigma}$ almost agree with the Non-Markovian estimates $\hat{\Psi}$, but the expected local stability is not resolved employing the Markovian model. Instead of negative drift slopes $\hat{\zeta}$, the estimates suggest persistently latent instability with $\hat{\zeta}\approx 0$. The parameters that apply for the calculation are the same as before apart from $15000$ MCMC steps per walker.\\
%Even if at the first glance it might be a surprising result that assuming a hidden slow process reproduces the local quasi-stationarity of the fast time scale process, there are several arguments which provide evidence of such a model describing the observed data: First, the measured mean market correlation yields the approximate time scale $\tau_{\Bar{C}}$ in trading days which seems to be rather short regarding socio-economic dynamics. 

\section{Discussion and Conclusion}
\label{sec:Discussion}
The estimated GLE model manages to reproduce the statistical properties of the original data for the end-of-week correlations as shown in \cref{fig:ACF,fig:IncrementDistribution}. The estimated memory kernel parameters show that even with a highly conservative interpretation of the 95\% credible level, there are clearly nonzero memory effects for memory terms for all lags as far back as a lag of 3 weeks. Therefore, it is advised to use a model with memory to describe the correlation of the S\&P500 market, which is an improvement to the Markovian Langevin model estimated in \cite{Stepanov2015}. Moreover, the GLE estimation presented in this article achieves a high goodness-of-fit for the entire time series, whereas Stepanov et al. used a time-dependent Langevin model by splitting the time series into different intervals and estimating Markovian Langevin equations for each of them. Our work shows that this procedure can be circumvented by using a model with memory of at least 3 trading weeks. The major advantage of our method is its possible application in predicting future market correlation: The time-dependent drift estimation in \cite{Stepanov2015} has no clear or smooth functional dependence on time and therefore, little information can be inferred about future values of the correlation time series. Our method needs no time dependency, generalises over the entire time series and can be used to predict future correlation values which can be used for portfolio risk assessment. As shown in section \ref{sec:Predict}, the memory kernel helps the GLE to achieve better prediction accuracy than the regular Langevin equation and much better results than the naive forecasting method of using the last observation as the predicted value.\\
Notably, the existence of memory effects in the market's correlation structure can be interpreted in the context of volatility clustering. It is a well-known stylised fact from empirical research on financial markets that the volatility of a stock's returns tends to cluster: periods of high volatility are often followed by periods of high volatility and vice versa for low volatility \cite{Gaunersdorfer2008,VolaClustering2}. The correlation $\rho_{X,Y}$ between two asset returns $r_X$ and $r_Y$ with expectation values $\mu_X,\mu_Y$ and volatilities $\sigma_X,\sigma_Y$ with is defined as
\begin{equation}
\rho_{X,Y} = \frac{\mathbb{E}\left[ (X-\mu_X)(Y-\mu_Y) \right]}{\sigma_X \sigma_Y}
\end{equation}
and therefore directly includes the volatility values $\sigma_X$ and $\sigma_Y$. Because the time series of volatility estimators $\sigma_X(t)$ shows a well-known memory effect, it is not far-fetched to assume a similar memory effect in the correlations $\rho_{X,Y}$ between two assets or, as we have discussed in this article, in the mean correlation of the market as a whole.\\

These considerations are complemented by a resilience analysis that involves the estimation of a mono-time scale Markovian model and a two-time scale Non-Markovian model. In contrast to the memoryless Markovian model, only the Non-Markovian model exhibits the negative drift slopes which are in line with the hypothesis of locally quasi-stationary economic states postulated and observed in Stepanov et al. \cite{Stepanov2015}. An independent change point analysis approach also supports this view \cite{ChangePointsSP500}. Overall, these findings provide new evidence for the existence of such locally quasi-stationary economic states and for the presence of a significant non-Markovian memory effect. \\
Interestingly, the resilience analysis yields some evidence that a second time scale which is slower than the trading day time scale of the mean market correlation data, is involved in the underlying economic dynamics. Economic processes operate on
various fast and slow time scales, e.g. day trading, trading psychology and sudden
external events --- whether that might be political, sociologic, severe weather or other impacts --- may be incorporated in the fast trading day resolution of the mean market correlation data. We refer to these fast-scale processes in terms of “economic weather” to employ a metaphor from climatology (cf. also the distinction in climate-like and weather-like tasks in \cite{Modelland}). In contrast, the long-term evolution of the “economic climate” might involve innovation processes and technical revolutions like the invention of the steam engine or the internet, economic cycle theories like that of Marx ($\tau\sim\SIrange{2}{10}{a}$), Keynes, Schumpeter or Kondratjew \cite{b:Marx2014,HARVEY2007618,Bernard2013,Schumpeter,Kondratieff,TurchinBook}, cultural evolution \cite{wand2023characteristic} and generational changes influencing economic reasoning, adaptions to climate change and the scarcity of resources and much more. However, we were not able to derive an economically reasonable magnitude of the slow time scale which we would expect to lie in the range of decades up to hundred years corresponding to well-known economic cycle theories or cultural evolution processes. Instead, our applied MCMC model estimation without prior range restriction converges to a hidden slow time scale of roughly $\SIrange{700}{4000}{a}$. Nevertheless, our results suggest that there should be involved at least two time scales in the data-generating process which is an interesting starting point for future research. Notably, it is not particularly surprising that we could not quantify the hidden time scale, since we employ a very simple model parametrisation, have only access to one variable of the high-dimensional economic state space and perform our estimation on small windows that not even include the smallest economic cycle time scale of roughly $\SIrange{2}{10}{a}$ that typically correspond to business cycles. Against this background it might be a very interesting challenge of future research to develop more realistic models and estimation procedures that perform reliably under the circumstances of limited data per window and incomplete variable sets to uncover the manifold of hidden time scales in the complex system of human economy.\\




\newpage


\printbibliography

\newpage 



\begin{appendices}

\section{GLE: Overlapping Windows for the Mean Correlation}
\label{app:OverlapGLE}
If the mean correlation is calculated via a window size of $\tau = 42$ and a shift of $s=1$ like in \cite{Stepanov2015}, then the GLE fit indicates some problems: The estimated memory kernel shows very strong negative values at $\mathcal{K}_{42}$ and the doubled temporal distance $\mathcal{K}_{84}$. Although the memory kernel suggests that there is a strong memory effect, this is most probably only attributable to the overlap of the window ranges used to calculate $\Bar{C}$.

% Figure environment removed

\FloatBarrier
\newpage

\section{GLE: Selection of Reasonable Kernel Length}
\label{app:GLE_Kernel}
To select a model with a reasonable kernel length, we first fit a model with a rather large kernel length and evaluate its combined memory aggregation $K_k$. The $k_0$ for which $K_k$ reaches a plateau is then chosen as a memory kernel length for the subsequent model fitting.

% Figure environment removed

\FloatBarrier
\newpage

\section{LE: Increment Distribution}
\label{app:Increments}

% Figure environment removed


\section{GLE: Prediction}
\label{app:GLE_Predict}
The results of the naive forecasting method are compared to the GLE in this figure, showcasing the superior accuracy of the GLE, which can also be seen by comparing the $\rho^2$ values in table \ref{tab:Predictions}.


% Figure environment removed

\section{Supplementary Material: Resilience Analyses} \label{sec: SI resilience}

This section complements the interpretation of the resilience analysis which hints to a hidden slow time scale and non-Markovianity, discussed in subsection \ref{subsec:resilience results} of the main article. In subsection \ref{subsec: SI synthetic data} we describe the simulation of the synthetic time series $x$ of the main article in detail, before we provide additional calculations in \ref{sec: SI resilience additional} which fit in with our previous results. At last, we provide the analyses' parameters in subsection \ref{subsec: resilience params}.\\
\subsection{Synthetic Time Series}\label{subsec: SI synthetic data}
The synthetic model
\begin{align*}
\dot{x} &= 15+x-x^3 + q\cdot y \\
\dot{y} &= -0.1\cdot y + \sqrt{0.1}\cdot \Gamma(t)
\end{align*}
is simulated via the Euler-Maruyama scheme for $30000$ steps in the time interval $[0,2000]$. The X coupling parameter $q$ is linearly increased from $0.5$ to $4$ over the whole time interval to create a first differences PDF with a width comparable to the PDF of the original mean market correlation data $\bar{C}(t)$.\\



\subsection{Additional Results}
\label{sec: SI resilience additional}
% Figure environment removed
The evaluation of the resilience analysis of the mean market correlation $\bar{C}(t)$ without and with Gaussian kernel detrending is performed for the Markovian model and the multi-scale non-Markovian model with the inverse prior time scale separation $\tau_{\bar{C}}>\gamma\cdot\tau_\lambda$, whereby $\gamma = 2$. the results are shown in figure \ref{fig:additional resilience}. Since the chosen time scale separation of the non-Markovian model determines the slowest time scale to be the observed trading day scale which is identical to the Markovian one, the calculations of both models are almost the same. The suggested local stability in the detrended cases confirms the interpretation that a second slow time scale might be hidden in the mean market correlation data. Detrendings with broader kernel widths tend to exhibit more pronounced positive trends which could on the one hand indicate a destabilisation of the local potential landscape due to uprising bifurcations. On the other hand this behaviour might be triggered by remaining non-stationarity of the data due to more and more incomplete detrending of the slow trend if the kernel width is increased.\\
In order to guarantee that the overall qualitative behaviour of the non-Markovian model in the main article with an unobserved slow time scale, i.e. $\tau_{\lambda}>\gamma\cdot\tau_{\bar{C}}$ with $\gamma = 2$, remains unchanged if the Ornstein-Uhlenbeck (OU) parameter is not restricted by the upper bound of its prior range, we present the converging results of significantly broader prior ranges in figure \ref{fig:SI convergence}. The exact parameters are listed in table \ref{table: resilience params} of subsection \ref{subsec: resilience params}. The drift slope estimates indicate locally stable economic states with $\hat{\zeta}<0$ in agreement with the results of the main article. The range of the OU parameter $\theta_5$ is roughly between $420$ and $1000$ which does not correspond to any reasonable slow economic time scale. However, this might due to the low amount of data per window which does not capture at least the smallest suggested time scale of business cycles that operate on scale of roughly two up to ten years. Furthermore the assumed polynomial model might be too simple to estimate quantitatively reasonable time scales.\\
% Figure environment removed

\FloatBarrier

\subsection{Parameter Documentation}
\label{subsec: resilience params}
In this subsection we summarise the parameter sets involved in the resilience calculations of the main article and the appendices in order to guarantee reproducibility without affecting the readability of the main article. The parameters are listed in table \ref{table: resilience params} and prior configurations are given in table \ref{table: resilience priors}.\\



\begin{table}
\centering
\begin{tabular}{llrrrrr}\toprule[1.3pt]
\textbf{Figure} & \textbf{Model} & \multicolumn{2}{r}{\textbf{Rolling Window}} & \multicolumn{3}{c}{\textbf{MCMC Parameters}}\\
\cmidrule(lr{0.25em}){3-4}\cmidrule(lr{0.25em}){5-7}\\
& &\textbf{Size}&\textbf{Shift}&\textbf{Walkers}&\textbf{Steps}&\textbf{Burn In}\\\midrule
\ref{fig: markov nonmarkov comparison}(a-c)&Markov&500&15&50&15000&200\\
\ref{fig: markov nonmarkov comparison}(a-c)&Non-Markov&500&15&50&20000&200\\
\ref{fig: markov nonmarkov comparison}(d-f)&Markov&500&15&50&20000&200\\
\ref{fig: markov nonmarkov comparison}(d-f)&Non-Markov&500&15&50&30000&200\\
\ref{fig:additional resilience}(a-c)&Non-Markov&500&15&50&20000&200\\
\ref{fig:additional resilience}(d-f)&Markov&500&15&50&15000&200\\
\ref{fig:additional resilience}(d-f)&Non-Markov&500&15&50&20000&200\\
\ref{fig:SI convergence}(a-d)&Non-Markov&500&15&50&30000&200\\
\bottomrule
\end{tabular}
\caption{Rolling window and MCMC parameters of the resilience calculations.}
\label{table: resilience params}
\end{table}



\begin{table*}
\begin{tabular}{llrrrrrrr}\toprule[1.3pt]
\textbf{Figure} & \textbf{Model} & \multicolumn{7}{c}{\textbf{Prior (Range)}}\\
\cmidrule(lr{0.25em}){3-9}\\
& &$\boldsymbol\theta_0$&$\boldsymbol\theta_1$&$\boldsymbol\theta_2$&$\boldsymbol\theta_3$&$\boldsymbol\theta_4$&$\boldsymbol\theta_5$&\textbf{TSS}\\\midrule
\ref{fig: markov nonmarkov comparison}(a-c)&Markov&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[0,50]$&---&---\\
\ref{fig: markov nonmarkov comparison}(a-c)&Non-Markov&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[0,50]$&$[0,50]$&$\tau_{\lambda}>2\tau_{\Bar{C}}$\\
\ref{fig: markov nonmarkov comparison}(d-f)&Markov&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[0,50]$&---&---\\
\ref{fig: markov nonmarkov comparison}(d-f)&Non-Markov&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[0,50]$&$[0,50]$&$\tau_{\lambda}>2\tau_{\Bar{C}}$\\
\ref{fig:additional resilience}(a-c)&Non-Markov&$[-25,25]$&$[-25,25]$&$[-25,25]$&$[-25,25]$&$[0,5]$&$[0.01,5]$&$\tau_{\Bar{C}}>2\tau_{\lambda}$\\
\ref{fig:additional resilience}(d-f)&Markov&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[0,50]$&---&---\\
\ref{fig:additional resilience}(d-f)&Non-Markov&$[-25,25]$&$[-25,25]$&$[-25,25]$&$[-25,25]$&$[0,5]$&$[0.01,5]$&$\tau_{\Bar{C}}>2\tau_{\lambda}$\\
\ref{fig:SI convergence}(a-d)&Non-Markov&$[-100,100]$&$[-100,100]$&$[-100,100]$&$[-100,100]$&$[0,250]$&$[0.01,2000]$&$\tau_{\lambda}>2\tau_{\Bar{C}}$\\
\bottomrule
\end{tabular}
\caption{Prior configurations of the resilience calculations. TSS is an abbreviation for Time Scale Separation.}
\label{table: resilience priors}
\end{table*}


%\begin{landscape}
%\begin{tabular}{llrrrrrrrrrrrr}\toprule[1.3pt]
%\textbf{Figure} & \textbf{Model} & \multicolumn{2}{r}{\textbf{Rolling Window}} & \multicolumn{3}{c}{\textbf{MCMC Parameters}} & \multicolumn{7}{c}{\textbf{Prior (Range)}}\\
%\cmidrule(lr{0.25em}){3-4}\cmidrule(lr{0.25em}){5-7}\cmidrule(lr{0.25em}){8-14}\\
%& &\textbf{Size}&\textbf{Shift}&\textbf{Walkers}&\textbf{Steps}&\textbf{Burn In}&$\boldsymbol\theta_0$&$\boldsymbol\theta_1$&$\boldsymbol\theta_2$&$\boldsymbol\theta_3$&$\boldsymbol\theta_4$&$\boldsymbol\theta_5$&\textbf{Time Scale Separation}\\\midrule
%\ref{fig: markov nonmarkov comparison}(a-c)&Markov&500&15&50&15000&200&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[-50,50]$&$[0,50]$&---\\
%\ref{fig: markov nonmarkov comparison}(a-c)&Non-Markov&500&15&50&20000&200\\
%\ref{fig: markov nonmarkov comparison}(d-f)&Markov&500&15&50&20000&200\\
%\ref{fig: markov nonmarkov comparison}(d-f)&Non-Markov&500&15&50&30000&200\\
%\ref{fig:additional resilience}(a-c)&Non-Markov&500&15&50&20000&200\\
%\ref{fig:additional resilience}(d-f)&Markov&500&15&50&15000&200\\
%\ref{fig:additional resilience}(d-f)&Non-Markov&500&15&50&20000&200\\
%\ref{fig:SI convergence}(a-d)&Non-Markov&500&15&50&30000&200\\
%\bottomrule
%    \end{tabular}
%\end{landscape}


\begin{comment}
\section{Supplementary Material: Change Point Analysis}
The CP analysis results are complemented by the computations under the assumption of one and five CPs. In figure \ref{fig: SI CP} the corresponding PDFs (a,b) as well as a comparison between the raw time series and the thinned time series (c) are provided. The overall similarities between the PDF results from one up to five CPs underline the consistency of the results. The thinning of the original time series as shown in figure \ref{fig: SI CP} (c) keeps the original trend structue of the data quite good.
% Figure environment removed
\end{comment}


\end{appendices}
\end{document}



\section{Other Window Sizes for antiCPy}
\label{app:AntiCPy}

Alternative choices for the window size of the sliding window approach via \textit{antiCPy} and the resulting change point detections are shown here. The figures depict the (normalised) number of maximum likelihood (ML) predictions for the existence of a change point at time $t$ evaluated over all sliding windows, the kernel density estimation of these ML positions and the original data.

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed


% Figure environment removed

\FloatBarrier
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% END OF DOCUMENT %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Table of Contents}

\paragraph{Data}
\begin{itemize}
    \item S\&P500 from yfinance
    \item Correlation via rolling window (GENERAL FORMULATION: window size $\tau$ shifted by $s$), and in Stepanov et al: $\tau = 42$ and $s=1$ (I think so?)
\end{itemize}


\paragraph{Change Point Analysis}
\begin{itemize}
    \item Motivation: Replicate Stepanov's change points
    \item Technical implementation via antiCPy and difficulties (combinatorics)
    \item Results for different window sizes and "best looking" result
    \item Comparision to Stepanov and to real-world financial events
\end{itemize}

\paragraph{GLE Estimation}
\begin{itemize}
    \item Motivation: Check time-dependent Markovian model vs. time-independent memory model
    \item Implementation: GLE by Willers and compare to acf via statsmodels
    \item Results (1): $\tau = 42$ and $s=1$ shows strange results at Kernel $~42$
    \item Results (2): $\tau = 5$ and $s=5$ (end-of-week, "independent" correlation matrices) decently interpretable results
    \item Results (2): evenly distributed bins have good medium-range ACF
    \item Results (2): bins with equal number of instances have good long-range ACF
    \item Results (2): incremental distributions look fine
    \item Conclusion: Memory of $~5$ weeks
    \end{itemize}