\section{The GREC Shared Tasks}\label{sec:grec}

In this section, we summarise the GREC task, the corpora used by GREC, and its conclusions.

\subsection{The GREC Task and its Corpora} \label{subsec:greccorpora}

According to \citeauthor{belz2009generating}, ``\emph{the GREC tasks are about how to generate appropriate references to an entity in the context of a piece of discourse longer than a sentence}" \citeyearpar[p.~297]{belz2009generating}. The main task 
was to predict the referential form, namely whether to use a pronoun, proper name, description or an empty reference at a given point in discourse.

The GREC challenges use two corpora, both created from the introductory sections of Wikipedia articles: (1) GREC-2.0 (henceforth \msr, as it was used in the GREC-MSR shared tasks of 2008 and 2009) consists of 1941 introductory sections of the articles across five domains (people, river, mountain, city, and country); and (2) GREC-People (henceforth \negc as it was used in the GREC-NEG shared task in 2009) contains 1000 introductory sections from Wikipedia articles about composers, chefs, and inventors. Here is an example from \negc:
% 

\enumsentence{\underline{\textbf{David Chang}} (born 1977) is a noted American chef. \underline{\textbf{He}} is chef/owner of Momofuku Noodle Bar, Momofuku Ko and Momofuku Ss√§m Bar in New York City. \underline{\textbf{Chang}} attended Trinity College, where \underline{\textbf{he}} majored in religious studies. In 2003, \underline{\textbf{Chang}} opened \underline{\textbf{his}} first restaurant, Momofuku Noodle Bar, in the East Village.}\label{ex:davidchang}

A key difference between \msr and \negc lies in their RE annotation practices. In \msr, only those REs that refer to the main topic of the article are annotated, while in \negc, mentions of all \textit{human} referents are annotated. For instance, in a document about David Chang, \msr will only annotate REs referring to David Chang, while \negc will include annotations for all human referents, including David Chang and others.

\subsection{REG Algorithms Submitted to GREC} \label{subsec:systems}

Various REG algorithms were submitted to the GREC challenges. 
These consist of feature-based ML algorithms: \cnts~\citep{hendrickx-etal-2008-cnts}, 
\icsi~\citep{favre-bohnet-2009-icsi}, \isg~\citep{bohnet-2008-g}, \osu~\citet{jamison-mehay-2008-osu} and \udel~\citet{greenbacker-mccoy-2009-udel}, and an algorithm that mixes feature-based ML and rules: \texttt{JUNLG}~\citep{gupta2009junlg}.
Table \ref{tab:grecsystems} presents the details of each model, including the ML method, and the original reported accuracy on \msr (cf. \citet{belz2009generating} for details).

\input{tab/grec_results.tex}

\subsection{Feature Selection}

The GREC Tasks were designed to find out \emph{what kind of information is useful for making choices between different kinds of referring expressions in context} \citep[p. 297]{belz2009generating}. However, the original paper does not consider the factors that contributed to the RE choice in the systems submitted to GREC. In a follow-up study, \citet{greenbacker2009feature} conducted a feature selection study informed by psycholinguistics. They experimented with various feature subsets derived from their system, known as UDel, which had previously been submitted to the GREC. Additionally, they incorporated selected features from another REG system, CNTS \citep{hendrickx-etal-2008-cnts}, into their study. They show that features motivated by psycholinguistic studies and certain sentence construction features have a positive impact on the performance of REG models. Follow-up feature-selection studies including \citet{kibrik2016referential} and \citet{same-van-deemter-2020-linguistic} also emphasise the contribution of factors such as recency and grammatical role to the choice of RE form.

