\section{Discussion} \label{sec:discussion}

In this paper, we have conducted a series of reproductions, evaluations, and analyses to check whether the conclusions of GREC are still true after 15 years. Below, we summarise and discuss our findings in accordance with our three research questions in Section~\ref{sec:rq}. We also report our post-hoc observations on the choice of evaluation metric.

\paragraph{Performance of REG Algorithms.}

To answer research question $\mathcal{R}_2$, we extended the GREC by introducing a corpus of a different genre, \wsj, and two pre-trained (PLM-based) REG models. We found that, on \msr, PLM-based and ML-based models perform similarly, as confirmed by both the BF and per-class analyses.
With regards to \negc, PLM-based and ML-based models have similar accuracy scores, as confirmed by the BF analysis, but there are large differences when micro-F1 is used, as confirmed by the per-class evaluation (i.e., ML-based models have difficulty predicting descriptions). On \wsj, PLM-based models are the clear winners.

These results suggest that, in terms of explanatory power, PLM-based models have good performance and good ``direct support'', i.e., a good ability to generalise to different contexts (see \citet{van2022role} for further discussion). Whether they have good ``indirect support'' (e.g., whether their predictions are in line with linguistic theories) needs to be investigated in further probing studies. 
 
\paragraph{Impact of the Choice of Corpus.}

As our evaluations and analyses demonstrate, the choice of corpus plays a crucial role in assessing REG algorithms. This role is twofold. Firstly, the choice of corpus strongly influences the evaluation results, pertaining to the research question $\mathcal{R}_1$. Secondly, in addition to the score differences discussed in Section~\ref{sec:evaluation}, we found that: (1) the difference between PLM-based and ML-based models on \wsj is larger (and evidentially different) than on \msr and \negc models (as evidenced by the BF analysis); (2) the correlations of the evaluation results between \wsj and both \msr and \negc are not significant.

For $\mathcal{R}_3$, we conducted feature selection analyses across the three corpora, discovering that the importance of the features ranks differently for each corpus. This suggests that when investigating the ``indirect support'' for a model, one needs to aggregate findings from multiple corpora with different genres. 


\paragraph{The Use of Evaluation Metrics.}

As we discussed in Section~\ref{sec:protocol}, different metrics evaluate different aspects of a model. This was further ascertained by the inconsistency of the BF analysis and per-class analysis. 
One lesson we have learned is that it is not enough to report or do analyses on a single metric. Another lesson is that the evaluation results by macro-F1 are more reliable than other metrics because (1) they are consistent across corpora with similar genres (i.e., \msr and \negc; see the Correlation analysis results); (2) the differences identified by using macro-F1 can be confirmed by the per-class evaluation.

\section{Conclusion}

We are now in a position to address the question that we raised in the Introduction: Can the conclusions from the GREC shared tasks still be trusted? By examining a wider class of corpora, models, and evaluation metrics than before, we found that the answer to this question is essentially negative since the GREC conclusions are prone to drastic change once a different corpus or a different metric is employed.

Perhaps this should come as no surprise. According to a widely accepted view of scientific progress (e.g., \citet{jayn}; applied to NLP in \citep{van2022role}), theories should be updated again and again in light of new data (i.e., indirect Support), and when new models are proposed, the plausibility of existing models should be compared against the plausibility of these new models (as well as pre-existing ones). New metrics deserve a place in this story as well, even though they are often overlooked. 
In other words, what we have seen in the present study is nothing more than science in progress -- something we are bound to see more of as the enterprise called NLP-as-Science matures. 

