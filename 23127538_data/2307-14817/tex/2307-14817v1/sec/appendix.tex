\section{Ranking of the Models} \label{sec:app_rank}

\paragraph{Accuracy-based Ranking}
\mbox{}\\

\noindent \msr : \bert $>$ \icsi $>$ \roberta $>$ \cnts $>$ \osu $>$ \isg $>$ \udel 

\noindent\negc: \udel $=$ \roberta $>$ \icsi $>$ \osu $>$ \cnts $>$ \bert $>$ \isg

\noindent \wsj: \roberta $>$ \bert $>$ \osu $>$ \isg $>$ \icsi $>$ \cnts $>$ \udel

\paragraph{Macro-F1 Ranking}
\mbox{}\\

\noindent \msr : \roberta $>$ \bert  $>$ \icsi $>$ \cnts $>$ \osu $>$ \isg $>$ \udel 


\noindent\negc: \roberta $>$ \bert $>$ \icsi $>$ \cnts $>$ \isg $>$ \osu $>$ \udel


\noindent \wsj: \roberta $>$ \bert $>$ \osu $>$ \isg $>$ \cnts $>$ \udel $>$ \icsi

\paragraph{Macro-weighted F1 Ranking}
\mbox{}\\

\noindent \msr : \bert  $>$ \roberta $>$ \icsi $>$ \cnts $>$ \osu $>$ \isg $>$ \udel 

\noindent\negc: \roberta $>$ \icsi $>$ \udel $>$ \bert $>$ \cnts $>$ \osu $>$ \isg

\noindent \wsj: \roberta $>$ \bert $>$ \isg $>$ \osu $>$ \cnts  $>$ \icsi $>$ \udel
\mbox{}\\

\section{Implementation Details for ML-based Models} \label{sec:appendixML}

The R programming language was used mostly for running the classic ML  models. The specification of the models can be found below: 

\paragraph{Conditional Random Field [CRF].} The R Package CRF (\url{https://cran.r-project.org/web/packages/crfsuite/}) was used to train these models. The iterations are set to 3000, and the learning method is Stochastic Gradient Descent with L2 regularization term (l2sgd).

\paragraph{Decision Tree [C5.0].} The R Package C5.0 \citep{kuhn2018package} was used to build the decision trees. The number of boosting iterations (trials) is set to 3, and the splitting criterion is information gain (entropy).

\paragraph{Memory-Based Learning [MBL].} As mentioned before, we implemented the k-Nearest Neighbors [KNN] algorithm instead of MBL. The R package caret with the method KNN was used to implement this model.

\paragraph{Maximum Entropy [MaxEnt].} The multinom algorithm from the nnet R package was used to implement this model. 

\paragraph{Multi-Layer Perceptron [MLP].} The Keras package was used to implement MLP. The model consists of two hidden layers with 16 and 8 units, respectively. The hidden layers use the rectified linear activation function (ReLU), and the output layer uses the Sigmoid activation function. The model is fitted for 50 training epochs. In addition, 50 samples (batch size) are propagated through the network.

\paragraph{eXtreme Gradient Boosting [XGBoost].} XGBoost was used for the feature selection experiments. We used the R packages xgboost and DALEXtra for the analysis. We set the learning rate to 0.05, the minimum split loss to 0.01, the maximum depth of a tree to 5, and the sub-sample
ratio of the training instances to 0.5.

\section{Feature Importance Rankings} \label{sec:appendixrankings}
The graphs in Figure \ref{fig:allrankings} show the rankings across \msr, \wsj, and \wsj. A maximum number of eight features is depicted in the graphs.
% Figure environment removed






