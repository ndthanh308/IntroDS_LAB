\section{Analysis} \label{sec:analysis}

% Overview of analyses and their motivation
To further compare the different models and investigate the impact of the choice of corpus, we conduct (1) a Bayes Factor (BF) analysis to determine whether the accuracy rates reported in Section~\ref{sec:evaluation} come from similar or different distributions, (2) a per-class evaluation of predictions to assess the success of each model in predicting individual classes, (3) a correlation analysis to quantify how the evaluation results change with respect to the choice of a corpus, and (4) a feature selection study to check how the importance of each feature changes as a function of the choice of corpus.

\subsection{Bayes Factor Analysis} \label{sec:bf}

\input{tab/perclass.tex}

Given that the accuracy scores are provided for all GREC systems in \citet{belz2009generating}, we chose to focus our analysis on the raw distributions of these scores. Our aim is to determine if there are significant differences between the accuracies of our models by comparing these distributions.
We conduct a Bayes Factor analysis with a beta distribution of 0.01 (henceforth: the threshold). This analysis aims to assess, for each pair of accuracies, how strong the evidence is that they come from a common distribution, or from different ones.
A difference below the threshold indicates that accuracy rates come from similar distributions; whereas, a difference above the threshold indicates that they come from different distributions, thus signalling that they differ evidentially. 
We interpret the strength of the evidence in favour of/against similar/different distributions according to \citet{kass1995bayes}. Therefore, based on this approach, we expect that the raw accuracy distributions of the best- and worst-performing models for each corpus differ evidentially. 

For \msr, the comparison between the best- and worst-performing models, namely \bert and \udel, provides no evidence that their accuracy rates are evidentially different from each other (BF = 1.4). The same holds for \negc, where the comparison of the best (\udel and \roberta) and worst (\isg) models appear to have similar probability distributions; therefore, these models are not evidentially different from each other. Conversely, in the case of \wsj, the BF analysis provides strong evidence that the accuracy distributions of the top-performing models, \bert and \roberta, are different from those of the classic ML models.

To summarise, we only observed significant differences in the \wsj-based models; the GREC models show more or less the same accuracy distributions. A reason might be that the aggregated calculation of accuracy loses the specificity of the classes being calculated.

\subsection{Per-class Evaluation} \label{sec:perclass}

As mentioned earlier, the \negc models demonstrate high accuracy (e.g. the highest average accuracy), but we observe a sharp decline in their macro-F1 values. In this analysis, we want to investigate whether the accuracy scores reported in Table~\ref{tab:performance} truly reflect the success of these algorithms or if they are merely the by-product of over-generating the dominant label or under-generating the less frequent label. Table~\ref{tab:perclass} presents the {\em per-class} precision, recall, and F1 scores of these models.

Upon comparing the F1 scores for the class \textit{description} across the three corpora, we observe that the \wsj models consistently achieve the highest scores, with all algorithms exceeding an F1 score of 50. In contrast, the F1 scores for both \msr and \negc are considerably lower than those of \wsj. The F1 scores for \negc are particularly low, with two notable instances, \udel and \osu, scoring 0 and below 10 respectively.  The poor prediction of the class description by the classic ML \negc models is likely due to an insufficient number of instances in the training dataset, thereby hindering the proper training of the algorithms. In contrast, the two PLM models demonstrate acceptable performance in predicting the class description (\bert = 61.16 \& \roberta = 69.02). This could indicate that pre-trained language models are advantageous where there is a class imbalance.

Another interesting observation concerns the high recall of the ``pronoun" prediction in the \negc models. Four of the classic models have a recall of over 92. In the case of \osu, for example, the recall is 95, which means that of all the cases that are pronouns, 95\% are labelled correctly. This is possibly an indication that pronouns have been over-generated in this system. In the PLM models, the recall is below 84.

In sum, the results of our per-class evaluation show the difficulties that the classic ML-based \negc models had in predicting the class \emph{description}. The \msr models also had poor performance in predicting descriptions, yet they were more successful than \negc. These results tentatively suggest that feature-based classification models need to be trained on an adequate and relatively balanced number of instances to reliably predict all classes. The results of this study suggest that the PLM models are less dependent on the choice of corpus, and therefore predict classes more robustly. 

\subsection{Correlation Analysis}

\input{tab/correlation.tex}

To quantify how the evaluation results change with respect to corpora, we compute the Spearman correlation coefficient between every pair of corpora, indicating how the rank of the models changes. Table~\ref{tab:correlation} shows the computed coefficients along with the p-values of the tests. It is noteworthy that only the results evaluated by the macro-weighted F1 on \msr and \negc are significantly correlated ($p < .001$). 

The lack of correlation between the results on \msr/\wsj and those on \negc/\wsj suggests that using a corpus of a different genre could greatly influence the ranking of the models and, therefore, make the conclusions difficult to generalise.
Additionally, these results are in line with the fact that \msr and \negc are from the same source, both being the introductory part of Wikipedia articles, and a higher correlation is to be expected. Also, we may conclude that macro-averaged F1 is a more reliable evaluation metric (see the discussions in Section~\ref{sec:evaluation}, Section~\ref{sec:bf}, and Section~\ref{sec:perclass}). 

\subsection{Feature Selection Study}

We performed a feature importance analysis to check whether the contribution of linguistic factors changes depending on the choice of the corpus. We used XGBoost from the family of Gradient Boosting trees~\citep{xgboost2016} and then computed the permutated variable importance for each model. Data were analysed in two ways: firstly, we used the complete dataset, as outlined in Section \ref{sec:corpus}; secondly, we excluded first-mention REs to concentrate only on subsequent mentions. Considering that the choice of a referent' first mention is less context-dependent, we only report on the latter dataset below:
% Figure environment removed

As expected, the ranking of feature importance varies across different corpora.  However, a substantial overlap is observed when considering the most important features across the three corpora. An example is the semantic category of the REs that is used in various \msr and \wsj REG models.\footnote{Only human referents are annotated in \negc; therefore, this feature is not applicable.} In the case of \msr, the REs belong to five semantic categories: human, city, country, river, and mountain. In the case of \wsj, the REs are annotated for a wide range of categories including human, city, country, organisation, objects, etc.
Notably, in every model that employs semantic category information, this feature has either the highest or second-highest importance ranking. A plausible explanation could be that humans use different referencing strategies to refer to different categories of referents. 

In addition to the semantic category, the grammatical role of the RE and the categorical sentential distance to the antecedent consistently have a high importance ranking. The grammatical role marks the distinction between subject, object, and determiner roles. The categorical distance in the number of sentences provides information on how far an RE is to its nearest coreferential antecedent. For instance, whether they are both in the same sentence or are separated by one or more sentences. Figure \ref{fig:rankings} illustrates the importance rankings of the \osu features in the three corpora. Other importance ranking graphs are available in Appendix \ref{sec:appendixrankings}. For a comprehensive description of all features employed in classic ML models and the feature importance analysis, refer to \citet{same-van-deemter-2020-linguistic}.


