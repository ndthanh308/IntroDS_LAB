\section{Related work on REG in context}\label{sec:litreview}

Deciding about the form of a referring expression and determining its content are two different steps of the Referring Expression Generation (REG) task \citep{comreg2019}. In the current article, our focus is on the first step, namely determining the form of a referring expression. We will expand thiw work to the second task, namelz the content realisation of the REs in future work.
 

\subsection{Different RF categories} \label{subsec:refcateories}

As \citet{kibrik2016referential} put it, the \textit{basic} and binary referential choice is between the choice of a pronoun versus a variety of NPs. Studies addressing pronominalization, such as \citet{mccoy1999generating,poesio2004centering} and \citet{henschel2000pronominalization}, often focus on this binary distinction. More recent studies 
have looked at a wider range of referring expression (RE) types. For instance, the GREC shared tasks \citep{belz2009generating} exploited four RE types, namely pronoun, proper name, common noun and covert (empty) reference. \citet{kibrik2016referential} focused on a three-way distinction between the choice of a pronoun, proper name and common noun; while \citet{castro-ferreira-etal-2016-towards} classified REs into five categories of pronoun, proper name, common noun, demonstrative NP and empty reference.

\subsection{Different approaches to REG in context} \label{subsec:regapproach} 


Different methods are used to predict the referential choice in context. Rule-based approaches, such as \citet{passonneau1996using}, \citet{mccoy1999generating}, \citet{henschel2000pronominalization} and \citet{krahmer2002efficient}, employ different algorithms to predict RF choice-taking, for instance, centering rules or salience-based accounts into consideration. The GREC shared-task challenges, as one of the first systematic studies on the generation of REs in context, introduced new
feature-based Machine Learning (ML) solutions to this task (e.g. \citet{greenbacker-mccoy-2009-udel, hendrickx-etal-2008-cnts, bohnet-2008-g, favre-bohnet-2009-icsi}, among others). 
Following these shared tasks, \citet{kibrik2016referential} trained decision trees and regression models on the WSJ MoRA, a corpus of Wall Street Journal articles, using a large number of factors. In a more recent feature-based study, \citet{castro-ferreira-etal-2016-towards} trained Naive Bayes and Recurrent Neural Network (RNN) algorithms on the VaREG corpus, taking individual differences in the generation of REs into account. Over the past few years, deep learning approaches have been pre-dominantly used for an end-to-end generation of REs in context, predicting type and content of expressions altogether \citep{castro-ferreira-etal-2018-neuralreg,cao-cheung-2019-referring,cunha-etal-2020-referring,same-etal-2022-non}. \citet{chen-etal-2021-neural-referential} have used pre-trained language models for the choice of RF, but they only use the benchmark NLG dataset, namely WebNLG \citep{gardent-etal-2017-webnlg, castro-ferreira-etal-2018-enriching}, in their study and only use \bert. 
