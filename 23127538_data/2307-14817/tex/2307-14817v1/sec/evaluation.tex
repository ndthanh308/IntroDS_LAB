\section{Evaluation} \label{sec:evaluation}

\input{tab/performance.tex}

In this section, we introduce the evaluation protocol and report the performance of the models.

\subsection{Implementation Details} \label{sec:implementation}

For \bert and \roberta, we used \textit{bert-base-cased} and \textit{roberta-base}, both from Hugging Face. For fine-tuning, we set the batch size to 16, the learning rate to 1e-3, the dropout rate to 0.5, and the size of the output layer to 256. We ran each model for 20 epochs and used the one that achieved the highest F1 score on the development set. The implementation details of the classic ML-based models can be found in Appendix~\ref{sec:appendixML}.

\subsection{Evaluation Protocol} \label{sec:protocol}

The main evaluation metric in the GREC-MSR shared tasks was accuracy. 
In addition to accuracy, we also report macro-F1 and weighted-macro F1. We argue that different metrics evaluate algorithms from different perspectives and provide us with different meaningful insights. 
For pragmatic tasks like REG, it makes sense to ask how well an algorithm performs on naturally distributed data which is often imbalanced. For these cases, reporting accuracy and weighted F1 are logical. 
Furthermore, analogous to other classification tasks, minority categories should not be overlooked. Take as an example the class \emph{description} in the \negc corpus, which occurs only 4\%. If a model fails to produce this class, the produced document might sound unnatural. Therefore, it is important to ensure that an algorithm is not over- or under-generating certain classes. Looking into accuracy and macro-F1 together provides insights into such cases.

\subsection{Performance of the Models}\label{subsec:overallacc}

The overall accuracy of the models, their macro F1, and their weighted-macro F1 are presented in Table \ref{tab:performance}. 
We also present the ranking of the models based on these scores in Appendix~\ref{sec:app_rank}. 


\paragraph{PLM-based Models.} The best-performing models across all corpora and metrics are PLM-based models.  In six out of nine rankings, \bert and \roberta are ranked as the top two models. The sole exception is \negc, where \bert is the second worst model. The benefit of using PLMs is the largest on the \wsj corpus. For example, \roberta improves the macro F1 score from 69.63 (i.e., the performance of the best ML-based model) to 82.70.


\paragraph{ML-based Models.} In contrast to the robust performance of the PLM models, the performance of the classic ML models is more corpus-dependent. In the case of \msr and \negc, \icsi is the best-performing model, while in the case of \wsj, it is at the bottom section of the rankings. Another interesting observation is the performance of the \udel models. In terms of accuracy, \udel has the highest performance in \negc, while it has the lowest performance in both \msr and \wsj. In terms of macro-F1 rankings, the \negc \udel model dropped from first to last place, whereas \bert improved from penultimate place to second place. In general, our ML models yielded lower scores than the original models used in the GREC study \citep{belz2009generating}. This could be attributed to a variety of factors, including differences in feature engineering and model parameters.

\paragraph{Comparing Different Metrics.} 

Upon comparing average scores across the three metrics, we observe that for \msr and \negc, PLMs are clear winners only when macro-F1 is the metric in question. However, for \wsj, PLMs are winners on all three metrics. This may be because the distribution of categories in \wsj is much more balanced than in the other two corpora.