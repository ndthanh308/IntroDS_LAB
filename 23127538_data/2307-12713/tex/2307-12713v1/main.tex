\documentclass[10pt]{article}

\usepackage[utf8]{inputenc}  
\usepackage[english]{babel}                             
\usepackage{syntax}
\usepackage{tikz}
\usepackage{soul,hyperref}
\usepackage{xcolor,xspace}
\usepackage[framemethod=TikZ]{mdframed}
\usetikzlibrary{petri,positioning,calc,shapes,fit}
\usetikzlibrary {automata} 
\usepackage{listings}
\usepackage[inline]{enumitem}
\usepackage{amsfonts}
\usepackage{amsmath}


\newtheorem{definition}{\textbf{Definition}}
\newtheorem{property}{\textbf{Property}}
\newtheorem{example}{\textbf{Example}}
\newtheorem{remark}{\textbf{Remark}}
\newtheorem{semantic}{\textbf{Semantics}}
\newtheorem{translation}{\textbf{Translation}}


\mdfdefinestyle{MyFrame}{%
     outerlinewidth=-1pt,
    %roundcorner=20pt,
    innertopmargin=5pt,
    innerbottommargin=5pt,
    innerrightmargin=10pt,
    innerleftmargin=10pt,
        leftmargin = -1pt,
        rightmargin = -1pt
    %backgroundcolor=gray!50!white}
        }



\definecolor{navyblue}{rgb}{0.0, 0.0, 0.5}

\newcounter{stxcnt}
\newenvironment{mysyntax}[1][]{%
    \refstepcounter{stxcnt}%
    \mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,rounded corners,fill=navyblue, text=white]
        {\normalsize Syntax\thestxcnt \hspace{1cm}#1};},
    innertopmargin=-1pt,linecolor=navyblue,backgroundcolor=navyblue!10,%
    linewidth=2pt,topline=true,roundcorner=10pt,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax%
    }
    \vspace{0.08cm}
    \begin{mdframed}[style=MyFrame,nobreak=true]\relax\small}{
    \end{mdframed}
    \vspace{-0.3cm}
}



%\pagestyle{headings}

\newcommand{\petri}{Petri\xspace}
\newcommand{\scade}{{\sc scade}\xspace}
\newcommand{\nnef}{{\sc nnef}\xspace}
\newcommand{\xtratum}{{\sc xtratum}\xspace}
\newcommand{\reluplex}{{\sc reluplex}\xspace}
\newcommand{\onnx}{{\sc onnx}\xspace}
\newcommand{\cuda}{{\sc cuda}\xspace}
\newcommand{\cudnn}{{\sc cudnn}\xspace}
\newcommand{\lenet}{{\sc LeNet-5}\xspace}
\newcommand{\khronos}{{\sc Khronos}\xspace}
\newcommand{\arm}{{\sc arm}\xspace}
\newcommand{\gpu}{{\sc gpu}\xspace}
\newcommand{\gpus}{{\sc gpu}s\xspace}
\newcommand{\fpga}{{\sc fpga}\xspace}
\newcommand{\nvidia}{{\sc nvidia}\xspace}
\newcommand{\ultrascale}{{\sc UltraScale+}\xspace}
\newcommand{\xavier}{{\sc Xavier}\xspace}
\newcommand{\pytorch}{{\sc PyTorch}\xspace}
\newcommand{\python}{{\sc Python}\xspace}
\newcommand{\tensorflow}{{\sc TensorFlow}\xspace}
\newcommand{\tensorrt}{{\sc TensorRT}\xspace}
\newcommand{\posix}{{\sc posix}\xspace}
\newcommand{\keras}{{\sc Keras}\xspace}
\newcommand{\tvm}{{\sc tvm}\xspace}
\newcommand*{\equal}{=}

\usetikzlibrary{arrows.meta}


\lstset{breaklines=true,
                    % numbers=left,
                    % numberstyle=\tiny,
                    % ytopmargin=1cm,
                    frame=single,
                    captionpos=b,
                    % xleftmargin=\parindent,
                    basicstyle=\small\ttfamily,
                    morekeywords={external, conv, max_pool, concat, reshape, linear, variable, variablesync, get_sync, send_sync, tick, relu, flatten, gemm, softmax},
                    keywordstyle=\color{blue}
}


\usepackage[verbose,a4paper,lmargin=2.5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=3cm]{geometry}



\title{Formal description of ML models for unambiguous implementation\\
  Adrien Gauffriau and Claire Pagetti\\
  \textit{Airbus, France} --  \textit{ONERA, France}
}



\begin{document}


\maketitle

\begin{abstract}
  Implementing deep neural networks in safety critical systems,
  in particular in the aeronautical domain,
  will require to offer adequate specification paradigms to 
  preserve
  the semantics of the trained model on the final hardware platform.
  We propose to extend the \nnef language
  in order to allow traceable distribution
  and parallelisation optimizations of a trained model.
  We show how such a specification can be
  implemented in \cuda on a \xavier platform.
\end{abstract}

\section{Introduction}
%claire
Machine learning (ML) applications have been gaining considerable attention in the field of transportation. However, their use in real-life operational safety-critical products, in particular in the aeronautical domain subject to stringent certification, raises several issues regarding functional correctness, compliance with requirements, formal verification, safety or implementation.
In order to tackle those issues,
new guidelines -- named  AS6983 standard \cite{wg114} --
are currently drafted by the EUROCAE WG-114/SAE G-34 joint working group
that cover the whole spectrum of the system development including the data sets composition, the ML model design and its implementation. In this paper, we focus only on the implementation of the ML model.



\subsection{Context}
In the ML current practices, a \emph{training framework} is used to design an ML model and the resulting ML model
is then deployed on the target with a \emph{deployment framework}.
It is up to the training framework or a designer to export the trained model description
in an exchange format and up to the deployment
framework to import the ML model description.
%and ensure the semantic preservation of the execution.
The left part of figure \ref{fig:currentpractice} shows those practices.
The deployment framework is most of the time an ML model interpreter,
that can accommodate any type of ML model architecture,
and that allocates at runtime the execution on the different
available accelerators (e.g. \gpu, \fpga) of the target.
These ML frameworks have been designed 1) to ease as
much as possible the deployment of models for the users
and 2) to optimize
as much as possible the execution performance (usually expressed
in trillion operations per second -- TeraOp/s).
As a result, they are very impressive and
allow for complex deployments and optimizations. Those are
hard, if not impossible, to reproduce for a programmer without
using ML libraries (e.g. \cudnn on \nvidia).

% Figure environment removed

The counterpart of such an approach is
1) the absence or limited information of
internal computation and allocation;
2) small modifications and adaptations
of the ML model
(e.g. when exporting the ML model description
or
when quantizing on the fly some
matrix multiplication to execute on deep learning accelerator -- DLA).
If this grey/black box approach
is acceptable and suitable for mainstream applications,
it is a blocking point for highly safety-critical applications.
As a result, the main objective
of the AS6983 standard with respect to the implementation process
is the \emph{semantics preservation} of the off-line trained model on the final hardware platform.
This means that the execution of the ML model in the training
framework should be exactly reproduced on the embedded target during execution.
To reach that objective, an alternative development process is proposed
as illustrated in the right hand part of figure \ref{fig:currentpractice}.
The cost is the reduced performance and deployment capacities compared
to ML deployment approaches.

\subsection{Assurance development process to ensure semantic preservation}
%To ensure the semantic preservation, the idea is the rely to trustable
%development process (figure \ref{fig:currentpractice}).
The principle of this trustable development process (figure \ref{fig:currentpractice}) is to ensure the semantic preservation at each step of the development cycle (e.g. between training framework and description, between description and code, between code and executable).
This is guaranteed thanks to a series of requirements provided in the AS6983.

\textbf{Requirement 1.}
First, the trained model must be formally
and unambiguously defined in what we call an adequate \emph{format}.
Such a \emph{format}
must come with a formal syntax and semantic,
and should be agnostic of any (training and/or deployment) framework.

\textbf{Requirement 2.}
Second, the implementation process
should allow several deployments on hardware platforms
and it must be known beforehand how the ML model will be mapped
on the accelerators and when.
%Indeed not all designers will map a model in the same way
%or will choose the same hardware.
%But what we expect
%is the semantic preservation by construction
%thus we choose to determine off-line the mapping
%and this parallel specification.
This entails in particular that
the  \emph{format} should allow several types of deployment such as
distribution, parallelization or pipelining.
Thus, there is a step that consists in splitting the
ML model description (in the chosen format) as a series of \emph{item} descriptions.
Indeed, in the avionics domain,
a target processor is decomposed as a set of software (SW) and/or hardware (HW) \emph{items}. Let us consider for instance an \ultrascale (ZCU102) platform \cite{ultrascale}: it is composed of several \arm cores, a \gpu and an \fpga.
If the ML model is spread over the different components, in particular on one \arm and the \fpga, there will necessarily be several items.
Indeed, 
the \arm associated code will be considered as one SW item
and will go through the
ED-12C/DO-178C \cite{do178c} development process.
Whereas the hardware design of the \fpga will be considered as another item (HW) and will go through the ED-80/DO-254 \cite{do254} development process.
%% If we consider a \xavier platform \cite{nvidiajeston} composed of several \arm cores and \gpu,
%% the designer will probably split the software implementation into several software items, in order to distinguish the \gpu and the \arm.
%% Thus, each item implementation requires its own formal specification
%% that corresponds to the ML model part it will execute.
This is the reason why we need to be able to decompose a global ML model description into several pieces
%expressed themselves in the same
%format
%such that the composition of the pieces description is
equivalent to the global model description.

\textbf{Requirement 3.}
Third, the implementation has to follow the usual aeronautical development standards
(e.g. ED-12C/DO-178C \cite{do178c} for software). 
%Thus, the description of a model within the
%format must be compliant with a certified implementation process.

%% The last objective concerns the capability to implement an item description following standards such as ED-12C/DO-178C \cite{do178c} or ED-80/DO-254 \cite{do254}.
%% Among the requirements from those standards, two are related to the format.
%% First, it must offer full traceability:
%% looking at the generated code (e.g. C or \cuda), it is humanly possible to trace back to the original exported
%% model.
%% Second, the execution must be \emph{predictable}, meaning that it is possible to assess a WCET (Worst Case Execution Time) \cite{WCETpaper}.
%% This entails that the code is expected to be allocated statically on the resources, all the memory allocations are static and the schedule (here the sequence of operations) is also static.

\subsection{Contributions}
Our general objective is to define an approach  compatible with the AS6983 requirements presented in the prior section.
We focus on a representative subset of deep neural networks (DNNs)
that is feed-forward neural networks trained off-line (see section \ref{sub-sec-DNN}).
Our main goal is
the definition of an adequate format, with a formal syntax and semantics,
able to describe both 1) a global ML model,
and 2) any parallelized allocation on several items,
the behaviour of which is equivalent to the global model.

\textbf{For requirement 1.}
There are several initiatives to propose an intermediate
format between trained models and their implementation such as
\onnx \cite{onnx} (Open Neural Network Exchange).
After a thorough evaluation of different existing formats,
we identified 
\nnef (Neural Network Exchange Format) \cite{nnefformat}
as the most suited for our purpose:
syntax and semantics are public
and moreover the authors made a strong effort to provide a formal specification. 
%\nnef is highly interoperable as it is compatible with any ML framework.


Since \nnef provided a potential candidate, we decided to construct our format on top of it.
As it is now, \nnef describes formally the global ML model. Indeed, the semantics of \nnef
is almost fully defined (see section \ref{sec-nnef}). What is however missing in \nnef
is the clear formalization of the \emph{execution model},
that is the formal behaviour behind a series of \emph{\nnef instructions}.
To fix this missing element, we rely on
\petri nets, a usual  representation of program behaviours \cite{peterson1977petri}.
This choice
is also consistent with the need to express distributed behaviour on items, as
\petri nets also allow to model all combinations of execution:
sequence, pipeline, recursion or parallel \cite{olderog1986operational}.
%In order to ensure semantic preservation,
%the ML model description should precisely reflect the
%ation at export, at load and at execution.


\textbf{For requirement 2.}
We illustrate in  section \ref{sec-distribution}
why decomposing ML model into items is of interest.
The first contribution (see section \ref{sec-nnef-ext}) is the definition of an 
extension of \nnef to allow the distribution of an ML model onto different items. To that end, we extend the syntax and semantics of \nnef.
We rely in particular  on \emph{logical data exchange} among items to distribute the computation and express the semantics with \emph{coloured \petri nets}.
We formally show that the synchronization of the \petri nets behaves as the \emph{original} non distributed \petri net.

\textbf{For requirement 3.}
We do not propose a complete DO-178C compatible approach but
instead show a reasonable
 implementation approach 
 on the \xavier platform \cite{nvidiajeston}
 that we believe
could be with some effort compliant with the ED-12C/DO-178C \cite{do178c}. 
To that end, we define an automatic \cuda-based code generation from an ML description
in the extended \nnef format and  assess in our evaluation its code traceability, the deterministic behaviour on the platform
and the semantics preservation from the trained framework.
All of that is detailed in section \ref{sec-code-generation}.

We will discuss the related work in section \ref{sec-related-work} before concluding.


\section{Format of description -- \nnef}
\label{sec-nnef}
% Peut être ne pas tout définir, mais essayer de faire un truc un peu plus minimal.
% Rajouter le fait que le noeud s'active quand on recoit un nouvel input.
\khronos standardization group\footnote{\url{https://www.khronos.org/}} has defined the \nnef (Neural Network Exchange format)
format with a specification that provides a syntax and a semantic.
After outlining the definition of deep neural network, we will present \nnef.
We focus on the \nnef syntax and semantics elements needed for our purpose. The reader can refer
to \cite{nnefformat} for a complete description of \nnef.
We end the section by defining the execution model semantics
with a translation into \petri net.

\subsection{Brief Reminder on Neural Network}
\label{sub-sec-DNN}
There are multiple ways to define DNNs:
directed graphs, computational graphs
or simply the mathematical functions transforming the input
into the output.
\nnef relies on a computation graph approach.
In any case, the input of a neural network
can be seen as a multi-dimensional vector also called \emph{tensor}.
\begin{definition}[Tensor]
A 3D-tensor
$T$ is represented by its \emph{size} $(n_c, n_h, n_w) $ 
where $n_c$ the number of channels (or feature maps), $n_h$ is the height and $n_w$ the width.
We denote by $T_{x_1,x_2,x_3}$ the value of $T$ for the indices $x_1,x_2,x_3$.
\end{definition}
A feed-forward deep neural network consists of a series of layers executed without recursion.
\begin{definition}[Feed-forward Deep Neural Network]
  A \emph{feed-forward neural network} $N=< Q,V >$ is a directed graph where:
  \begin{itemize}
\item $Q=\{l_1, \ldots, l_n\}$  the nodes
  are the \emph{layers} $l_i$.
  A layer can be of  many types such as  an activation, a convolution or a pooling.
  A layer comes with a set of parameters (e.g. weights or stride);
  \item
    $Q_i\subseteq Q$ is the set of input layers and
    $Q_f\subseteq Q$ is  the set of final layers, in particular $Q_i\cap Q_f=\emptyset$;
\item $V\subseteq Q\times Q$ the arrows represent the data flow within the neural network.
  $\emph{Pre}(i)= \{j | (l_j,l_i) \in V\}$  is the set of layers $l_j$ such that
  \emph{output}($l_j$) is an input of $l_i$.
  An input layer $j\in Q_i$ only consumes input tensors,
  i.e  $\emph{Pre}(j)=\emptyset$;.
  \end{itemize}
  We can compute the ancestors of a layer,
  $\emph{Pre}^*(j)=\cup \emph{Pre}^n(j)$ where
  %$\emph{Pre}^2(j)=\cup_{j_i\in\emph{Pre}(j)} \emph{Pre}(j_i)$
  %and
  $\emph{Pre}^n(j)=\cup_{j_i\in\emph{Pre}^{n-1}(j)} \emph{Pre}(j_i)$ and $\emph{Pre}^{1}(j)$ = $\emph{Pre}(j)$
  
  Because the network is feed-forward, $j\not \in \emph{Pre}^*(j)$.
  
 %%  A path in $N$ is defined by a succession of valid transitions starting from $l_1$ and ending in $l_n$:
%%   $p:l_1 \longrightarrow l_{j_1} \longrightarrow l_{j_2} \ldots \longrightarrow l_{j_p} \longrightarrow l_n$
%%   such that $(l_1,l_{j_1})\in V$, $\forall k, (l_{j_k},l_{j_{k+1}})\in V$ and $(l_{j_p},l_n)\in V$.
%%   Because we consider forward NN, in all valid paths reachable from $l_1$, a node can be visited at most once,
%% i.e.  $\forall k,i$, we have $l_1\not=l_{j_k}$,  $l_n\not=l_{j_k}$ and  $l_{j_i}\not=l_{j_k}$.
\end{definition}

\begin{example}
  \label{ex-lenet}
 The \lenet \cite{LeCunBDHHHJ89} is a usual CNN
  developed for hand written digit images recognition.
  Its structure (see figure  \ref{fig:lenet})
  consists of 8 consecutive layers: convolution, pooling, flat and dense.
  $Q_i=\{\emph{conv1}\}$, $Q_f=\{\emph{softmax}\}$,
  and for all layers $i\geq 2$, $\emph{Pre}(i)=\{i-1\}$.

  
  
% Figure environment removed

The size of the input / output tensors are shown on the figure.
   The first 2D-convolution \emph{conv1} which takes inputs of size $1\times 32\times 32$,
   is composed of 6 kernels of size $1\times 5\times 5$
   and a stride of $(1,1)$. Because the sizes of the input and the kernel are well balanced taking into account the stride, no padding is needed.
   The activation function \emph{ReLu} is applied to the outputs.
   The first pooling layer \emph{pool1} is a max pooling with stride $(2,2)$ and window $(2,2)$.
   The second 2D-convolution \emph{conv2} is  composed of 16 kernels of size $6\times 5\times 5$
   and of a stride $(1,1)$. The activation function \emph{ReLu} is applied to the outputs.
   The second pooling layer \emph{pool2} is a max pooling with stride $(2,2)$ and window $(2,2)$.
   The 3D-tensor of size  $16\times 5\times 5$   is flattened in a 1D-tensor of size $400$.
   Three dense layers are then applied, each followed by a \emph{ReLu}.
   A dense layer consists in applying a linear transformation
   $W\cdot I + B$ where $W$ are the weights and $B$ the bias.
   Finally, the post-processing is a \emph{softmax}.
\end{example}

\begin{remark}
  In this article we are only focusing on the inference topic. Thus, whereas \nnef provides the possibility to express the batch size (number of inputs computing in one inference), we only consider a batch size of 1. The possibility to increase the batch is motivated by the learning phase to improve the optimization convergence (stochastic gradient).
\end{remark}

\begin{definition}[Function associated to a DNN]
  \label{def-sem-DNN}
  The function $f_N$ computed by a DNN
 $N=< Q,V >$ 
  is the composition of the functions computed by each layer. In case of a \emph{directed DNN} -- i.e. where $Q_i=\{l_1\}$, $Q_f=\{l_n\}$ and $\forall i\geq 2$, $\emph{Pre}(i)=\{i-1\}$ -- the function is simply given by
  $f_N = f_{l_n} \circ \ldots \circ f_{l_1}$ where $\circ$ is the composition operator.
  Otherwise,
  $f_N = (f_{l_{f_1}}, \ldots, f_{l_{f_p}})$
  where $Q_f=\{l_{f_1}, \ldots, l_{f_p}\}$ and 
  the functions are defined as follows:
  $\forall i$ such that $l_i\not\in Q_i$, $f_{l_i}(f_{l_{j_1}}, \ldots, f_{l_{j_n}})$ where $\{j_1,\ldots,j_n\} = \emph{Pre(i)}$.
  The semantics of each elementary function (e.g. a 2D-convolution)
  is given for instance in \nnef by providing its mathematical function.
%  which can be used for an implementation.
\end{definition}

\subsection{Neural Network Description in \nnef}
\label{sec-nn-nnef}
Our first goal concerns the definition of a format,
with a formal syntax and semantics, able to describe any ML model
such as the \lenet of example \ref{ex-lenet}.
Let us explain why \nnef fulfils this goal.
An \nnef description is composed of two parts:
1) A computation graph described in a human readable text file;
and 2) the parameters provided in multiple raw data file.
The fact that the description is textual is important for the traceability between the specification (output of the training framework) and the embedded code.

The computation graph file describes all parameters needed and operations to be done. More precisely, 
each line of the computation graph is an elementary instruction (\nnef \emph{compound fragment}) that may be split
into several atomic operations (\nnef \emph{primitives}).
The result of each instruction is stored in a named variable, that represents a tensor,
and which can be used as input for other instruction(s).
To compute an operation all its inputs variables shall be available.
%Such a description indeed defines a computation graph where nodes are operations and vertices variables.

\begin{remark}
  \nnef description follows a SSA (static single assignment) form \cite{cytron1989efficient}
  which helps and simplifies the implementation process.
  It is usual to translate a program in its associated
  SSA form before compilation or optimization passes \cite{BourkeBDLPR17}.
\end{remark}


\begin{example}
Let us illustrate how to specify a neural network with an example.
  The \nnef textual specification of the \lenet detailed in example \ref{ex-lenet}
  is given in the listing \ref{list-nnef-lenet}. First, all parameters are declared and stored as variables. \emph{e1} is the input tensor of size $1\times 32\times 32$,
  \emph{v1} contains the 6 kernels of size $1\times 5\times 5$ of the first convolution and
  \emph{v2} is the bias applied at the end of the  first convolution.
  The parameters needed by the layers should all be defined
  as variables in the description file. 

\lstinputlisting[ caption={\lenet with \nnef syntax},
                  label=list-nnef-lenet]
 {MLMID-lenetshort.tex}

 After the variables declaration, comes the computation graph itself.
 The output of the first convolution is stored in the variable \emph{o1}.
 When calling the function / compound fragment \emph{conv}, the user must instantiate
 the full set of parameters for this type of layer:
 input tensor, kernels, bias,  stride, dilation, padding and groups.
 Every parameter appears explicitly in the definition and there is no ambiguity.
 For instance,
 the way to declare the padding explicitly states how the padding applies on top / bottom / right / left.
 After the convolution, the activation function has to be explicitly applied to \emph{o1}
 and thus is not hidden in the convolution, ensuring again an unambiguous description.
 The first pooling layer results in variable \emph{o3}.
% and the parameters corresponds to those expressed in the example \ref{ex-lenet}.
 Reading the description, we recognize the \lenet detailed before. 
 The flat layer is encoded with a more expressive function \emph{reshape} that allows several reshaping.
 The \emph{dense} layer is called \emph{linear}. The post-processing \emph{softmax} is also explcit.
\end{example}

%From the previous syntactic description, the structure of the ML model can be easily described.
The \nnef specification also
provides the link between instructions (e.g. \emph{conv}) appearing in the file
and their associated mathematical functions.
%Subsequently,
%we will not use \nnef terms, because the \nnef standard uses the generic term \emph{fragment} for both. 
We illustrate this with the max pooling layer only.



\subsection{Max Pooling Layer Semantics}
\label{sec:maxpool}
In order to illustrate issues issues that we may without having a formal description we propose to use the Max Pooling example.
Let us first remind  the functional semantics of a max pooling
layer where a padding is applied and no dilatation is applied.
Thus, the function is defined by $\mathcal{P}\emph{ool}_{k,s} \circ \mathcal{P}_{p,v}$
where each function is defined below.
\begin{definition}[Padding associated function -- $\mathcal{P}_{p,v}$]
  \label{def-pad}
  Let  $p=(p_t,p_b,p_l,p_r)$ be a 4-tuple of integers representing the padding to be applied on each border of a 3D-tensor and $v$ the float value to be used for the padding.
  The padding function  $\mathcal{P}_{p,v}$ applied on a 3D-tensor $I$ of size $(n_h,n_w,n_c)$
  outputs a 3D-tensor $O = \mathcal{P}_{p,v}(I)$ of size $(o_h,o_w,o_c)$ with
  $o_h = n_h + p_t + p_b$, $o_w = n_w + p_l + p_r$ and $o_c = n_c$ such that\\
  $O_{x,y,z}=
    \begin{cases}
            v  & \textbf{if }( x \leq p_t)  \emph{ or } (x > n_h + p_t)  \emph{ or }\\
            &   \ \ \   (y \leq p_l)  \emph{ or } (y > n_w + p_l)\\
            I_{x-p_t,y-p_l,z}  &\textbf{otherwise}
    \end{cases}
  $
\end{definition}

\begin{definition}[Pooling layer associated function --  $\mathcal{P}\emph{ool}_{k,s}$]
  Let $s=(s_h,s_w)$ be the \emph{stride} parameters and
   $k=(k_h,k_w)$ be the height and width of the \emph{window}.
  The pooling applied on a 3D-tensor $I$ of size $(n_h,n_w,n_c)$
  outputs a 3D-tensor $O=\mathcal{P}\emph{ool}_{k,s}(I)$ of size $(o_h,o_w,o_c)$
  with $o_h = \left\lfloor \frac{n_h - k_h}{s_h} + 1\right\rfloor$,
  $o_w = \left\lfloor \frac{n_w - k_w}{s_w} + 1 \right\rfloor$ and
  $o_c = n_c$ with
  $O_{x,y,z} = \emph{max}(I[s_h \cdot (x-1)+1:s_h \cdot (x-1)+k_h+1][s_w \cdot (y-1)+1:s_w \cdot (y-1)+k_w+1][z])$.
  Here, $I[s_{11}:s_{21},...,s_{1k}:s_{2k}]$ represents the \emph{slice} of $I$
of all the values $I_{s_{11}+x_1,...,s_{1k}+x_k}$ with $i \in [1, k]$ and $x_i \in [1,s_{2i}-s_{1i}]$.
 \end{definition}

The \nnef syntax of a max pooling layer describes the padding values with an enumerate string. Ignoring the border for a max pooling layer is equivalent to pad with the minimum float value (\emph{MIN_F}). This corresponds to the neutral operator of the max function.
 Looking now at the \emph{max_pool} elementary instruction according to \nnef documentation \cite{nnefformat}, it is defined in the pseudo-code
  by 2 atomic operations. 
  \begin{enumerate*}
    \item \emph{argmax_pool} that computes an array of index (corresponding to the max in each pool);
    \item \emph{sample} that returns for an array of index, an array with corresponding values.
  \end{enumerate*}
  This pseudo-code indeed encodes the expected function $\mathcal{P}\emph{ool}_{k,s} \circ \mathcal{P}_{p,v}$.
  
 \textbf{Of the importance of unambiguous description.} 
  We propose to highlight the importance of making unambiguous textual descriptions of ML models
  by comparing two state-of-the-art training frameworks, namely \pytorch and \keras (with \tensorflow).
  More specifically, we compare their
  way of encoding max pooling.
  A first important remark is that it is not possible to pad the \emph{borders} (top / bottom / left / right) with any combination.
%And let us try to implement it in the training framework.
\begin{itemize}
  \item In  \keras, a padding inside a max pooling layer can only be declared by a string $\in \{$"valid", "same"$\}$.
    "valid" means no elements to add,
    while "same" means that padding is added on the right and on the bottom borders only to fit  the size of the pool.

    \begin{lstlisting}[ %caption={\keras syntax for MaxPool},
      label=list-torch, language=Python]
  # KERAS SYNTAX for pool1
      MaxPool = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding = 'same')

  # NNEF for KERAS SYNTAX
      max_pool_keras = max_pool(input, size = [1, 1, 2, 2], stride = [1, 1, 2, 2], dilation = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 1), (0, 1)], border = 'ignore');
    \end{lstlisting}
This corresponds to
$  \mathcal{P}ool_{[2,2],[2,2]} \circ \mathcal{P}_{[0,1,0,1],\textrm{MIN\_F}}$.
\item In
  \pytorch, a padding inside a max pooling layer can only be declared by one or two integers. In case of one integer, this defines the number of elements to add to each border (top, bottom, left and right). In presence of 2 integers,
  the first  gives the number of elements to add at the top and  bottom, and the second at the left and right.
   \begin{lstlisting}[ %caption={\pytorch syntax for MaxPool},
      label=list-torch, language=Python]
  #PYTORCH SYNTAX for pool1
     MaxPool = NN.MaxPool2d(2,              2,       1)
                         # Kernel Size,  Stride, Padding

  #NNEF for PYTORCH SYNTAX
     max_pool_torch = max_pool(input, size = [1, 1, 2, 2], stride = [1, 1, 2, 2], dilation = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (1, 1), (1, 1)], border = 'ignore');
   \end{lstlisting}
 This corresponds to $\mathcal{P}ool_{[2,2],[2,2]} \circ \mathcal{P}_{[1,1,1,1],\textrm{MIN\_F}}$
\end{itemize}
The semantics of \keras and \pytorch are not equivalent,
and there is no possibility to convert one into another at once.
The only way to make a valid conversion is  to explicitly add a padding layer before the max pooling.
This difference of semantic between the two frameworks leads to issues when we want to convert a model from one framework to the other.

\subsection{\nnef Execution Model Semantics}
\label{subsec-petrinet-sem}
The semantics of the \emph{execution model}, that is the formal behaviour behind a series of \emph{\nnef instructions},
is not explicitly given by the standard. It assumes that one instruction can be executed
when all its inputs are computed and available.
Thus, executing the instructions in sequence following
the order of the \nnef guarantees a correct execution.
There are two types of instructions: those reading parameters from binary files or input tensors
and  layer-associated instructions based on one or several atomic operations.
An instruction is always of the form
\[
\emph{var} = \emph{operation} (v_1, \ldots, v_k, \emph{cst}_1,\ldots, \emph{cst}_j);
\]
where \emph{var} is a variable computed by the \emph{operation} (any \nnef \emph{fragment}),
$v_i$ are either variables computed beforehand, the input tensor or fixed parameters (e.g. kernels), and
\emph{cst}$_i$ are constant (e.g. stride).
The \nnef execution model can be formally expressed using the \petri net formalism \cite{peterson1977petri}.

\begin{translation}
  \label{trans-nnef-petri}
A \nnef description, composed of $n$ instructions, generates a \petri net $(P,T,V)$
where:
\begin{itemize}
\item the set of places $P$ corresponds to all variables appearing in the \nnef description
  (i.e. $n$ places for a description of $n$ instructions);
  \begin{itemize}
  \item a token in a place means that the associated variable is available for computation;
  \item initially there are as many tokens in each parameter-associated place as the parameter is needed in the instructions
    and as many tokens in the input tensor place as the input tensor is used by the instructions;
    \item there is a unique final place corresponding to the last variable computed in the \nnef file;
\end{itemize}
\item the set of transition names $T$ corresponds to all instructions appearing in the \nnef description;
\item $V\subseteq 2^P \times T \times \mathbb{N} \times P$ defines the set of transitions.
  A transition can be fired if there is a token in all input places.
  When it fires, the transition removes a token from each of these places and generates as many tokens as defined on the edge in the unique output place.
  \begin{itemize}
  \item each instruction $\emph{var} = \emph{op} (v_1, \ldots, v_k,\emph{cst}_1,\ldots, \emph{cst}_j)$ generates the transition given in figure \ref{fig:trans-instruction}
    where $p$ is the number of time \emph{var} will be consumed by other instructions;
    \item when only one token is produced by a transition, we omit the integer.
    \end{itemize}
  \end{itemize}
\end{translation}

% Figure environment removed

The semantic of the \petri net clarifies the execution order that is unclear in the \nnef formalism.
Especially, the order of the textual file should not impose a unique order when several valid ones may exist.
%For example, it is possible to read all weights and biases variables before the sequential execution of the layers but, these variables may also be read just before the execution of the associated convolution. 
\begin{example}
  The \lenet model  of figure \ref{fig:lenet} with its associated \nnef description in listing \ref{list-nnef-lenet} has the associated \petri net given in figure \ref{fig:sem}. We recognize the instructions sequence that describes the computation of the neural network graph.
  %All variables can be read before scheduling layers instructions (order of the \nnef file), but we can also imagine to schedule reading of variables just before scheduling the layer that is using these inputs.
  There is a unique possible schedule for this \nnef description,
  but we will see later other \nnef models that accept several schedules (see section \ref{sec-distribution}).
\end{example}

\begin{remark}
  The \petri net of figure \ref{fig:sem} only defines the semantics of a single inference pass. It is usual to repeat the inference pass
  in order to process new inputs (e.g. in a periodic manner).
  This can also be represented using the \petri formalism by sending back tokens to $e1$ and $v_k$ places.
  %For weights and biases variables, because they are static, we can just sending back a token after the execution of the associated instructions.
  For demonstration and clarity, subsequently in the paper, we always consider the semantics of a single inference.
\end{remark}


A way to define the semantics of \petri nets is to compute the set of reachable \emph{markings}, where a marking
 defines the number of tokens in each place at a given instant.
\begin{definition}[Marking]
  Considering a \petri net with $n$ places, a marking is defined as a vector $v\in\mathbb{N}^n$
  giving the number of tokens $v[i]$ in the i-th place.
 This initial location of tokens is called the \emph{initial marking},
 this represents the starting state of the system.
 A \emph{final marking} is a marking such that there is one token
 only in each final place
 and from which no transition can be fired any more.
\end{definition}
\begin{example}
  In the \petri net of figure \ref{fig:sem}, there are 24 states. There are 11 tokens in the initial making and there is a single token in the unique final marking
  (in the place \emph{out}).
\end{example}


\begin{property}[Initial marking and unique final marking]
  The unique initial marking is defined by the translation
  and consists of token(s) in the input tensor variable place and parameters-associated places.
  Because we consider feed-forward neural networks,
  there is unique \emph{final marking}.
\end{property}

\begin{definition}[Paths and semantics]
  A \emph{valid path} starts from the initial marking $m_i$,
  lists a series of fireable transitions 
  and ends in a final marking $m_f$, i.e.
    $m_i \longrightarrow^{t_1} m_1 \longrightarrow^{t_2} \ldots \longrightarrow^{t_l} m_f$.
  The semantics of a \petri net is the set of valid paths.
\end{definition}

\begin{property}[Possible executions of an \nnef description]
  Because we consider feed forward neural networks, the number of valid paths is finite and
  the valid paths correspond to all possible execution orders
  respecting the semantics of the ML model.
\end{property}
Semantics preserving code generation could lead to
any implementation the path of which is valid. Full sequential code following the order of instructions of the \nnef file is one of them.

\section{Is distribution needed?}
\label{sec-distribution}
Because we consider highly distributed platforms, a designer may choose to split the ML model into several parts
in order to accelerate the execution and reduce the execution time.
In particular, it could lead to developing parts independently and on different \emph{items} following the aeronautics standards \cite{arp4754}.
In such a case,  there should be a formal description for each item that will be the input
specification for HW/SW item implementation.
\begin{remark}
Note that if the designer considers its platform as a unique item, the \nnef description will be the specification.
\end{remark}

Looking at the \lenet model or other classical models, it is unclear 
why distributing across  into items would be beneficial.

\begin{example}[Off-loading convolution]
  Let us consider for instance the implementation of the \lenet
  on an \ultrascale (ZCU102) platform \cite{ultrascale}
composed of several \arm cores, a \gpu and an \fpga.
Let us assume that we choose to execute the convolution on the \fpga and all other layers on the \arm,
there will necessarily be several items.
The idea will be to offload the input tensor of each convolution on the \fpga
and retrieve the feature maps from the \fpga (see figure \ref{fig:lenet-fpga}).

% Figure environment removed

\end{example}

A second type of distribution could be to refine the layers and exhibit more parallelism by distributing the computation of a layer across several items.
It will be up to the designer to show the semantic preservation at this refinement level.
\begin{example}[Parallelizing the layers]
  % Looking again at the \lenet, because the number channel in the input tensor is $1$ and because the stride of the \emph{conv1} and of \emph{pool1} are $(2,2)$, the two first layers could be duplicated and applied on two distinct parts of the input tensor as shown in figure below.
  Looking again at the \lenet, we can split the computation of the first two layers on two different items. Each item
  will do the convolution+maxpool on a part of the input image. An idea is to split the input image along the height
  and  execute the two sub-parts on two different items (see figure below).
  In order to keep the semantics for the convolution, some overlap exists between the two sub-images. 

  % Figure environment removed
\end{example}




\begin{example}[Pipelining the computation]
  A third type of distribution could be the pipelining of the DNN execution.
  In this case, each item is in charge of computing a specific layer (or a group of layers).
  The first item computes the first layer(s) on the first tensor input and sends its output to the second item that will compute the second group of layer(s)
  while the first item starts processing the second input tensor. 
  This classical mechanism enables to reduce (e.g. for video processing) the frame rate while degrading the latency. 
  The depth of the pipeline is the number of inputs that can be handled at the same time among the pipeline.
  Taking into account the pipelining should be done in the \petri net by restoring the tokens in the initial places at the end of item 1 execution.

  % Figure environment removed

\end{example}

\section{\nnef extension for multiple items}
\label{sec-nnef-ext}
The purpose of this section is to propose a manner to separate the specification of each item  so that any execution of the items respecting the specification will properly encode the global ML model.
To that extent, we propose first to extend the syntax
of \nnef to allow explicit parallelization and then to express the associated semantics with colored \petri nets.


\subsection{Extension for item splitting}
We first need to specify the item on which the description will be implemented. To that end,
we enrich the graph definition with the keyword \emph{graphitem}
to provide the logical id of the HW or SW item. 

\begin{mysyntax}[GraphItem]
  \begin{verbatim}
<graph-definition> ::= <graph-declaration> <graph-declaration-item> <body>

<graph-declaration-item> ::= "graphitem" <identifier> <identifier>  
      "(" <identifier-list> ")" "-$>$" "(" <identifier-list> ")"
\end{verbatim}
\end{mysyntax}

\begin{semantic}
  The first \emph{$<$identifier$>$} refers to the item id,
  the second \emph{$<$identifier$>$} is the name of the local node
  and
  elements of the \emph{$<$identifier-list$>$} will be input or output of the graphitem. 
The semantic of the \emph{$<$graph-declaration-item$>$}
is such that all instructions within the 
\emph{$<$body$>$} are executed by the item.
%We introduce the possibility to have input or output exchanged between different \emph{$<$graph-declaration-item$>$}.
\end{semantic}


%\subsection{Extension to allow synchronization between items}
We also need to \emph{exchange data} between several items and ensure that those exchanged data are available before computation.
To that end, we introduce a new type of variable, namely \emph{variablesync}.
This references a variable that could be read from or write to another \emph{graphitem}. We use a \emph{fragment} to define this new type.

\begin{mysyntax}[VariableSync]
\begin{verbatim}
fragment variablesync<? = scalar>(shape: integer[])  -> ( output: tensor<?> )
\end{verbatim}
\end{mysyntax}

\begin{semantic}
  Each \emph{variablesync} is a shared variable with a unique writer and possibly multiple readers.
  Writer is in charge to transmit the variable via the instruction \emph{send\_var}
  and each reader can access this data via \emph{get\_var} instruction.
\end{semantic}
    
We then define new \nnef instructions to send or get data between several \emph{graphitem}. 

\begin{mysyntax}[get_var]
\begin{verbatim}
fragment get_var<? = scalar>( source : graphitem, data  : variablesync)
            -> ( output: tensor<?> )
\end{verbatim}
\end{mysyntax}

\begin{semantic} \emph{get_var} appears in each reader description.
  The output of this instruction is a local
  variable that gets the content of the shared variable
  and which is available for the caller item.
\emph{Source} is the item that writes and provides the shared variable the name of which is given by
\emph{data}.
%The output is the name of the local tensor that will get the content of the share variable.
\end{semantic}

Similarly, the writer must define the instruction to send a shared variable to other items.
\begin{mysyntax}[send_var]
\begin{verbatim}
fragment send_var<? = scalar>(  dest  : graphitem[], data  : scalar) 
            -> ( output: variablesync )
\end{verbatim}
\end{mysyntax}

\begin{semantic} \emph{send\_var} appears in the writer description only.
  It takes as input the list of reader items and the name of the tensor that shall be sent.
  The output tensor is a global \emph{variablesync}
  that will support the \emph{synchronization}.
\end{semantic}

The rest of the \nnef syntax remains unchanged.


\subsection{Splitting an \nnef description into multi-item descriptions}
\label{subsec-split-nnef}
Initially, the DNN is described in a unique \nnef
description as shown in section \ref{sec-nnef}.
Such a description contains 3 types of instructions:
\begin{itemize}
\item definition of input tensor(s);
\item definition of fixed parameters;
\item variables computed by each layer.
\end{itemize}
A splitting consists in partitioning the last type of instructions among the items,
add the adequate definition(s) of tensors / fixed parameters
and add the adequate \emph{send_var} / \emph{get_var}.
The composition of item descriptions  shall respect the semantics
of the full \nnef description. 



\begin{example}
  \label{ex-multiple-nnef}
Let us consider the following DNN with its associated \petri net (figure \ref{fig-net-complete}).

% Figure environment removed

 Let us assume that the DNN is allocated on 3 items such that $o_1$, $o_6$, $o_7$ and $out$ are computed on item 1;
  $o_2$, $o_3$ are computed on item 2 and  $o_4$, $o_5$ are computed on item 3.
Thus, the description on the items are given as follows:

%\noindent
%\begin{minipage}{.42\linewidth}
    \lstinputlisting[caption=\nnef for all items]
    {MLMIDcoshort-1.tex}
%\end{minipage}\hfill
%\begin{minipage}{.42\linewidth}
%  \lstinputlisting[ caption=\nnef for ITEM2]
%  {../MLMID/MLMIDcoshort-2.txt}
%\end{minipage}

The union of the instructions of each item corresponds to the complete \nnef description with the additional \emph{variablesync}
and the communication instructions.
Locally in the item, the pointers to the input tensor and fixed parameters must also be declared.
\end{example}


\subsection{\petri-based semantic}
We define the execution model semantics of  multi-item descriptions
using coloured \petri nets \cite{colorpetri}.
We associate a colour to each item where the colour is set to the tokens
and edges (on which the coloured tokens transit).

\begin{translation}
  Let assume there are $N$ items.
  We first apply the translation \ref{trans-nnef-petri} for each item leading to $N$
  independent \petri nets $(P_i,T_i,V_i)$, each with a unique and distinct colour.
For the new instruction, the translation is extended as follows:  
\begin{itemize}
\item a \emph{varsync} does not generate any place;
\item a \emph{get\_var} does not generate any transition;
\item a \emph{send\_var} produces a transition \emph{sync}
  with an incoming edge from the variable the content of which is transmitted.
\end{itemize}

The set of \nnef item descriptions generates a \petri net $(P,T,V)$
which is roughly speaking the union of the $N$ \petri nets $(P_i,T_i,V_i)$.
More precisely,
\begin{itemize}
\item any input tensor or fixed parameter that is duplicated in the \nnef files appears in the \petri net of the item.
  Those duplicated places are merged. Because we use the same naming convention,
  $P=\cup P_i$;
\item the initial tokens in the places are also merged leading to places with possibly multiple tokens and multiple colours;  
\item $T=\cup T_i \cup T'$ where $T'$ are the transitions connecting places of one item to other items thanks to the \emph{sync} transition. More precisely,
\begin{itemize}
\item for each writer, there are $k$ edges back from the \emph{sync} label where $k$ is the number of readers. The colour of the each arrow is the one of the reader and the number of tokens sent back corresponds to the number of time the shared variable appears in the reader item instructions;
\item for each reader, there is an edge from the writer place before \emph{sync} to each transition requiring the shared data;
\end{itemize}
  \item As before, when a coloured token is present in a place, it means that the associated variable is available for the item identified by the colour
and can be used by transition.
\end{itemize}
%A \emph{sync} is the only transition that can have different edges colour for inputs or outputs.
\end{translation}

\begin{example}
The \petri net associated to the example \ref{ex-multiple-nnef} is given in figure \ref{fig:sync}. We present the initial marking with colored tokens. Each color represents the state of one item. Compare to the figure \ref{fig:sem}, we expressend here the multi items semantics with synchronizations.
\end{example}

% Figure environment removed
  
\begin{property}[Equivalence between \petri nets]
  The semantics of the multi-items behaviour is equivalent to the complete original ML model.
\end{property}


\begin{remark}
  It is important to explicitly describe the \emph{send_var} and \emph{get_var}
  either in the \nnef files but also in the \petri net
  because items are supposed to be independent
  and segregated.
  In particular, an item X is not allowed to access the memory space of an item Y and interfere with its execution.
 % Thus, the implementation must respect this requirement which is traced in the description files.
  This is classical in aeronautics, see Arinc 653 specification.
  The \xtratum hypervisor \cite{xtratum} is an example of \emph{time and space partitioning} hypervisor that provides communication with \emph{sampling} and \emph{queuing} ports (close to Arinc 653 requirements).
\end{remark}

\section{Implementation and experiments}
\label{sec-code-generation}
The previous sections showed how to fulfill the requirements 1 and 2 of the introduction.
The purpose of this section is to give some hint of
how a DO-178C compatible implementation process, as required per requirement 3,
could be defined taking as input an extended \nnef specification.
The considered target, 
a Jetson \xavier TX system-on-chip, %using \emph{C++} and \cuda.
%This target %(see figure \ref{fig:jetson})
is composed of
6 Carmel \arm cores, a \gpu, 2 deep learning accelerators (NVDLA) and
other dedicated circuits.
The use of a \nvidia platform is mainly motivated by its availability
and the ease to quickly deploy neural networks application.
We will not discuss
the adequation of \gpu and \cuda implementation with DO-178C objectives because it is an open problem. 


%% % Figure environment removed


% Figure environment removed

The proposed implementation process workflow
is summarized in figure \ref{fig:implemworkflow}. 
The \emph{sync} implementation relies on barrier mechanism
and each item \nnef description is manually coded.
In order to validate the semantic preservation,
we made some instrumentation to show that: 
\begin{enumerate*}[label=(\roman*)]
  \item the execution trace is included in all possible execution traces defined by the \petri net;
  \item the numerical precision is kept;
  \item the measured execution time does not vary.
\end{enumerate*}
We will use the multi-items example presented in example \ref{ex-multiple-nnef} as the specification.
For our experiments, each item is allocated
to one CPU and all the \cuda cores of the \gpu (grouped in a \cuda stream).
%Thus one item correspond to one CPU and one \cuda stream.
As a consequence, we do not guarantee a segregation between items (as they share the GPU)
%
%and this will be the purpose of future work.
we instead focus on a way to implement
parallel operations of neural networks
with a static code scheduling while preserving the semantics.
As for the \petri semantics, we only developed a code for a single inference.
Nevertheless, it is easy to slightly modify the code by adding loops to handle several input tensors.
%The code generation is not fully automatic yet and supports only the layer types of the example.

%% % Figure environment removed



%% \subsection{Example for experiments}
%% \label{subsec-toyex}
%% Let us consider a toy neural network deployed on 3
%% different items of Figure \ref{fig:modelitem}.
%% %The model contains one input (convolution + maxpool) layer followed by two parallel branches that contains 2 (convolution / maxpool) layer. Finally branches are concatenate to fill the output fully connected layer.
%% %The input convolution and output fully connected layers are allocated to the first item.
%% %The parallel branches are parallelized on items 2 and 3. 
%% The complete network has been trained and exported with \pytorch into \nnef description.
%% The decomposition into 3 item descriptions is done following the rules of section \ref{subsec-split-nnef}.
%% %The full code is given for the artifact and reviews in an additional material.
%% %% We need to exchange data between all 3 items:
%% %% the output of the first (convolution + max pooling) computed by item1 is sent to item2 and item3. Then, layers of item2 and item3 can be computed in parallel. They finally both send their output to the item1 that finishes the computation. We give below the description for items 1 and 2.
%% Due to lack of space, we do not detail the \nnef descriptions. %but they will be available for artifact if any.
%% %Again, the code are available for artifact and review and we simply show part of item 1 description here.
%% %% \begin{minipage}{.42\linewidth}
%% %% \lstinputlisting[caption={Item1 description},
%% %%                   label=list-nnef-item1]
%% %%                 {../MLMID/MLMID-item1-extract.txt}
%% %% \end{minipage}\hfill
%% %% \begin{minipage}{.42\linewidth}
%% %%   \lstinputlisting[  caption={Item2 description},
%% %%     label=list-nnef-item2]
%% %%                   {../MLMID/MLMID-item2-extract.txt}
%% %% \end{minipage}
%% %% The item1 description starts by listed the variables and in particular the 3 \emph{variablesync}
%% %% that allow the communication across items.
%% %% A fragment $g_1$ has been defined that contains the convolution and max pooling.
%% %% Then the variable \emph{oitem1} is sent to the items 2 and 3.
%% %% The item 1 then waits for two variables, one from item 2 on \emph{oitem2}
%% %% and one from item 3 in \emph{oitem3}. After, item 1 ends with fragment $g_2$ which contains the concat, the flatten and the gemm.
%% %% Similarly, item2 description starts by listed the 2 s\emph{variablesync} that  allow the communication with the item2. It starts by waiting for item1 input data.
%% %% Fragment $g_3$ contains the two convolutions and max poolings done in the upper branch.
%% %% Then item2 sends to item1 its output.
%% %% The item3 is very similar to the item2 and is not presented here.
%% The figure \ref{fig:toymulti} presents the multi-items semantic of the toy example using a coloured \petri net. 

%% % Figure environment removed



 


\subsection{Get/Send specification}
%The example relies on 3 items that communicate thanks to the \emph{get_var} and \emph{send_var} fragments. 
We chose to implement \emph{get_var} and \emph{send_var}
with
1) global variables stored in the SRAM of the \xavier
and
2) the \posix barrier mechanism of the \emph{pthread} library.
%for the synchronization part and global variables for data exchanges. 
A barrier $b$, shared among several processes,
will block them as long as not all of them reach $b$.
Such a behavior is strictly included in the semantics of the \emph{sync} transition within the \petri net.
However, it is not the most efficient as it prevents
the sending item to proceed until
all the receiving items reach $b$,
whereas
the semantics of \emph{sync} transition only
requires a receiver to wait for the sender (not the sender
 to wait for all receivers).
Nonetheless,
the barrier mechanism is optimal for our  example
because no sender has to process
any instruction before a further \emph{get\_var} or stop execution.

%####################################################################@
\subsection{Manual code generation}
There are
C and \python interpreters of the \nnef format \cite{nneftools}
but only for traditional CPU target.
Consequently, no existing tool supports our syntax extension nor state-of-the-art \gpu.
Thus we developed the code for each item using C++ and
\cuda using the \cudnn library. Basically the C++ code is executed by the \arm processor whereas  \cuda  allows the definition of kernels that are executed synchronously by all \cuda cores.
The \cudnn library is built on top of \cuda for executing common neural networks layers.
%We focus subsequently on the principles of our software architecture and the scheduling.

\subsubsection{Software architecture}
Practically,
%each layer is represented by a C++ object that contains all the necessary code for initialization and execution.
each type of layer is implemented using a dedicated C++ class that inherits from the abstract \emph{Layer} class
that defines common attributes and methods to be implemented (\emph{init()} and \emph{forward()}) by child classes.
%Adding new layers is easy and just requires to implement their two abstract methods.
In effect,  \emph{init} statically allocates tensors and \cudnn descriptors while
\emph{forward} launches the layer computation based on \cudnn for Convolution and max pooling layers.
Each item contains one object implementing a static scheduler.
%that calls in sequence the \emph{forward()}
%method of each layer.
More precisely, during the \emph{init} phase, each item  creates an object for each layer
which are stored in ordered lists.
Thus, items 2 and 3 need one single ordered list whereas item 1 needs two ordered lists (one  for the first part and one for the second part).
During \emph{forward},
the scheduler  calls in order the \emph{forward} of objects stored in ordered lists.





\subsubsection{Scheduling}
%The implementation of the synchronization that we defined with \petri net  is based on the \emph{pthread} \posix library.
We define one separate thread for each item allocated to one CPU + \cuda cores.
% Communication between items are implemented with \emph{synchronizations}.
More precisely, synchronizations between threads use \texttt{pthread_barrier_t} and associated APIs (\texttt{barrier_init} and \texttt{barrier_wait}). 
Barriers synchronize accesses to shared variables. %that are stored in the SRAM.
%Other synchronization mechanisms such as spin-lock or semaphore may be closest
%to a non blocking behavior but are more complex to handle.
%when multi-items synchronization is needed.
%Our main purpose is to exhibit one possible implementation that respects the semantics.

\begin{center}
%\begin{figure}[hbt]
    \resizebox{.8\linewidth}{!}{\input{sched}}
%      \caption{Global scheduling\label{fig-scheduling}}
\end{center}


The execution sequence 
starts with the 3 threads creation on the CPUs
and then reaches the first synchronization barrier.
Then Item1 thread calls the \emph{forward} method of layers of the head (until \emph{send\_var})
while Items 2 and 3 threads wait for the second synchronization barrier.
After,
the second synchronization barrier,
Items 2 and 3 threads  call \emph{forward} method of their layers
while Item1 thread waits for the third synchronization barrier.
After the third synchronization barrier,
Item 1 thread calls \emph{forward} method of layers of the tail.
At last, threads join and exit.




%####################################################################@
\subsection{Semantic preservation of the \petri net}
\label{sec-petriprese}
%We developed 3 instrumentation codes to validate the semantic preservation.
The first analysis
aims at verifying that all observed scheduling of layers on the \xavier
respects the \petri net semantics.
Because we use a static scheduler, all schedules should
behave as shown in section \ref{sec-petriprese}
which is included in the semantics of coloured \petri net
of figure \ref{fig:sync}.
% Thus we need to monitor the start and end of each branch/items. In each item, we need to monitor the execution of layers.
For that, we logged each start/end of branches and layers
and we stressed the robustness of the implementation by addind some temporal noises (sleep in the code).

All observed traces respected the schedule of section \ref{sec-petriprese}
with some timing variations.
When observing the implementation with no noise,
execution traces of Item 2 and 3 are interleaved on the \gpu.
When adding a wait of 1s at the beginning of Item 2
(just after barrier1), all layers of Item 3 were executed before those of Item 2. 

%% % Figure environment removed



\subsection{Semantics preservation of the function}
The second instrumentation mechanism aims at checking
that the functional semantics of the DNN is preserved.
We achieve this by re-implementing the \nnef specification in \pytorch. Then we define 100 random vectors that we run
both on the \pytorch implementation and on the C++ implementation on the \nvidia target. Finally, we compute the overall average error mean between both executions for the 100 runs. 

%We do not have access to the source code of convolution algorithms used by \cudnn
%or to convolution provided by \pytorch.
We were not able to find the exact convolution algorithm of \pytorch. We think that it exists a non documented heuristic that calls the best algorithm (considering execution time) depending of the convolution parameters and available hardware. 
According to the \cudnn documentation \cite{nvidiacudnn},
it is possible to select the convolution algorithm among a list, but details of the implementation are not given.
Thus, there may be a discrepancy between convolutions that we cannot fix.
The average error mean
is extremely small $1.10^{-7}$ for FLOAT32
using 3 \cudnn algorithms (namely gemm, Winograd and direct).
Nevertheless numeric precision results for this experiment are in an acceptable range that is very close to the available numeric precision of floating point representation
and
this also is observed by other frameworks \cite{SilvaCGP22}. 


\subsection{Measured Execution Time (MET)}
One objective of the DO-178C that we did not mention until now
is the capacity to estimate the Worst Case Execution Time (WCET). 
Due to the complexity of \nvidia target, a formal demonstration using static analysis %such as \otawa
may be difficult. But at least, a good property is a low variation of the measured execution time among several executions.
In our case, the generated code does not contain any IF-THEN-ELSE patterns
or dynamic loop conditions. Thus, the variability is only linked to the hardware behavior. 
We measured the MET of the complete DNN and of the first convolution of Item 1 over 10 runs.
We rely on the \emph{nsys} tool from \nvidia to get timing measurements.

\begin{center}
% \noindent\resizebox{.5\linewidth}{!}{
  \begin{tabular}{|l|c|c|c|c|}
      \hline
       & \textbf{Mean(MET)} & \textbf{MIN(MET)} & \textbf{(MAX(MET))} & \textbf{STD(MET)} \\
      \hline
      \textbf{First Conv}  & 324 2976 ns  & 322 688 ns & 326 656 ns  & 1.45 ns \\
      \textbf{DNN}  &  24 257 $\mu$s &  16 285 $\mu$s &  53 950 $\mu$s & 13 526 $\mu$s\\
      \hline
  \end{tabular}
\end{center}

%Results show  mean, min, max and std measurements of the execution time for the first convolution and the whole pipeline.
The MET of the first convolution is very stable with a very low jitter.
The MET distribution of the DNN is large
and to understand why, we need to investigate the low level behaviour.
\nvidia\ \gpus are  black-boxes processors on which we cannot guarantee worst-case execution time \cite{AmertA21,carle-erts22}.
%This perfectly illustrate issues that we may encounter when using complex hardware whereas software algorithms are without any branches.
%For a compliant DO178C this behavior need to be understood and an effort shall be put in limiting this global jitter.

% Whereas the error is extremely 
% https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionFwdAlgo_t



% Rajouter des expes sur le profiling avec nvprob

\section{Related work}
\label{sec-related-work}
%
%\subsection{About Format Syntax and Semantics}
%As the domain of machine learning and more precisely of neural network
%is still in a research exploration phase,
There are plenty of different formats
but no consensus within the community.
%of which one to adopt,
%in particular for implementation and verification purposes.
State-of-the-art frameworks like \pytorch, or \tensorflow %(including \keras)
rely
on custom black-box formats built on top of \emph{protocol buffers}
\cite{protobuf} developed by Google.
A \emph{protocol buffer} is a structured binary format that is not human readable and
requires conversion tools and template files to be interpreted.
Thus, the syntax is not formally defined and specification of layers are only available through documentation website.
For example, \tensorflow proposes the \emph{.h5} \cite{h5format} format
and  \keras format \cite{kerasformat} builds on top of protocol buffer.
Moreover the way to save a neural network is not unified among frameworks
and may evolve every updated version with poor backward compatibility.
When moving from caffe to caffe2 (known today as \pytorch),  the caffe \cite{caffe} format was not supported anymore.
%and required users to convert it to the new format before utilization. 


% ONNX and NNEF. Exchange format between frameworks
All previous formats were developed specifically
for training frameworks (open source or proprietary) without any objective for sharing models.
Their main purpose was to allow saving and reloading
previous trained models without too much consideration on syntax and semantics.
%In other words, the semantics is the semantics of the layers implemented in the associated framework.
\onnx \cite{onnx} and \nnef \cite{nnefformat} were developed with the objective to be independent from frameworks. \onnx is still based on binary protocol buffers \cite{protobuf} (so without a textual syntax) but is proposing a functional semantics through its github site. On the contrary \nnef is proposing a textual format with a syntax and a semantics that is formally defined in a specification.
%The \nnef format is actually up to our knowledge the most adapted one to avionics and aerospace certification constraints developed in SAE G34/EUROCAE WG114 \cite{wg114} standardization group.
Unfortunately, the \nnef format suffers from a small community and tools supporting the format.
%Nevertheless, because semantic preservation is vital,
%we consider that the most important is the formal definition of the format in order to be able to demonstrate that an implementation has kept a neural networks semantics. 
%This was the main motivation of this paper in proposing a syntax extension in order to support multi items implementation.


NNet \cite{nnefformat} format is an example of ad-hoc format.
\reluplex neural networks examples \cite{KatzBDJK17} are in NNet.
It is based on textual files but without definition of syntax and semantics. Import and export tools are provided, but its utilization for sharing neural networks between teams remains supremely painful. 


% Internal representation
Some other formats like \cite{n2d2,tvm,MLIR}  tackle the need for intermediate representation between a neural network description and an implementation on a target. Especially this supports different optimizations passes like layers folding or low level tensor manipulation description like in LLVM \cite{llvm}. We consider that we are closest to programming language than from a neural network description format. Most of the time \onnx or \nnef are used as input like in \cite{LattnerAB0DPRSV21,pompougnac:hal-03043623,Jin2020}.

Because DNN are data-flow, it is natural to translate them into \emph{synchronous languages}.
There are some works such as
\cite{9094536} that proposed to encode them as Synchronous Dataflow Graphs
or
\scade tool suite \cite{ColacoPPP18} which is currently
developing a DNN libraries.
Once the translation is done, it is then possible
to reuse all the qualified code generation tools.
This is complementary to our work as we could use the \nnef description to generate the \scade program for instance.
To the best of our knowledge, none of actual available neural network description formats propose solution for describing multi items implementation with concerns on sharing variables among them.

% Need to include following ???
% MLIR (Multi-Level Intermediate Representation Overview) \cite{LattnerAB0DPRSV21} is a LLVM intermediate representation which was developed with the idea to use the same IR for all compiler optimizations (hence the ``Multi-Level''). It contains particular features that target machine learning applications, in particular it is possible to represent computation graphs in MLIR. MLIR can be instantiated into dialects that allow to put the focus on particular aspects of the code, to specify constraints or apply specific optimizations. An example of MLIR dialect that is particularly relevant to critical embedded applications such as the ones we target is described in~\cite{pompougnac:hal-03043623}: it enables the semantics of synchronous reactive applications inside an MLIR description.

%\subsection{Runtime}
% \onnx runtime ...
% MLIR
%% Associated to each format described in the previous section, we often find dedicated runtime that may be implemented in python like for \pytorch or \tensorflow or developed in C like \tensorflow Lite or $\mu$TVM.
%% These runtimes always rely on a dynamic exploration of the computation graph described by the previous format. The memory is also managed dynamically. It is associated to layers libraries that contain all supported layers of the framework. This approach is not compatible with the embedded aerospace domain that requires to develop software compatible with the DO178C standard.


%% \cite{SilvaCGP22} has developed the ACETONE code generator that claims to be compatible with critical avionics constrains. The approach is completely different and relies on an automatic C code generator that analyse the computation graph offline, do optimization and generate static C code with static memory allocation.







% \subsection{Towards code generation}
% C code generation ...

% a composition of functions as proposed by De Albuquerque et al. \cite{SilvaCGP22}.

% \cuda code generation ...


% Dynamic computation graph.

\section{Conclusion}
We have proposed a formal extension of \nnef that takes
into account the \emph{execution model} of a description and
allows for the modification of a description of a trained model
to define  traceable distribution
and parallelisation optimizations that preserves the semantics
while improving the execution time compared to a pure sequential aproach.
We have also proposed a code generation strategy based on barriers
for exchanging data between items.

As a future work, we would like to extend the \nnef tools to parse our extensions
and define an automatic code generation in \cuda.
We will also improve the \gpu utilization to ensure more predictable behaviour.
We will in particular reuse works from
\cite{AmertA21,AmertTVBSA21}.



%\begin{acks}
%TBD.
%\end{acks}


\section{Acknowledgement}
This work has benefited from the AI Interdisciplinary Institute ANITI, which is funded by the French ``Investing for the Future – PIA3'' program under the Grant agreement ANR-19-P3IA-0004. The authors gratefully acknowledge the support of the PHYLOG 2 project.


\bibliographystyle{alpha}
\bibliography{bib}


\end{document}
