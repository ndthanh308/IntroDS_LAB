\section{Related work}
\label{sec-related-work}

%
%\subsection{About Format Syntax and Semantics}
%As the domain of machine learning and more precisely of neural network
%is still in a research exploration phase,
There are plenty of different formats
but no consensus within the community.
%of which one to adopt,
%in particular for implementation and verification purposes.
State-of-the-art frameworks like \pytorch, or \tensorflow %(including \keras)
rely
on custom black-box formats built on top of \emph{protocol buffers}
\cite{protobuf} developed by Google.
A \emph{protocol buffer} is a structured binary format that is not human readable and
requires conversion tools and template files to be interpreted.
Thus, the syntax is not formally defined and specification of layers are only available through documentation website.
For example, \tensorflow proposes the \emph{.h5} \cite{h5format} format
and  \keras format \cite{kerasformat} builds on top of protocol buffer.
Moreover the way to save a neural network is not unified among frameworks
and may evolve every updated version with poor backward compatibility.
When moving from caffe to caffe2 (known today as \pytorch),  the caffe \cite{caffe} format was not supported anymore.
%and required users to convert it to the new format before utilization. 


% ONNX and NNEF. Exchange format between frameworks
All previous formats were developed specifically
for training frameworks (open source or proprietary) without any objective for sharing models.
Their main purpose was to allow saving and reloading
previous trained models without too much consideration on syntax and semantics.
%In other words, the semantics is the semantics of the layers implemented in the associated framework.
\onnx \cite{onnx} and \nnef \cite{nnefformat} were developed with the objective to be independent from frameworks. \onnx is still based on binary protocol buffers \cite{protobuf} (so without a textual syntax) but is proposing a functional semantics through its github site. On the contrary \nnef is proposing a textual format with a syntax and a semantics that is formally defined in a specification.
%The \nnef format is actually up to our knowledge the most adapted one to avionics and aerospace certification constraints developed in SAE G34/EUROCAE WG114 \cite{wg114} standardization group.
Unfortunately, the \nnef format suffers from a small community and tools supporting the format.
%Nevertheless, because semantic preservation is vital,
%we consider that the most important is the formal definition of the format in order to be able to demonstrate that an implementation has kept a neural networks semantics. 
%This was the main motivation of this paper in proposing a syntax extension in order to support multi items implementation.


NNet \cite{nnefformat} format is an example of ad-hoc format.
\reluplex neural networks examples \cite{KatzBDJK17} are in NNet.
It is based on textual files but without definition of syntax and semantics. Import and export tools are provided, but its utilization for sharing neural networks between teams remains supremely painful. 


% Internal representation
Some other formats like \cite{n2d2,tvm,MLIR}  tackle the need for intermediate representation between a neural network description and an implementation on a target. Especially this supports different optimizations passes like layers folding or low level tensor manipulation description like in LLVM \cite{llvm}. We consider that we are closest to programming language than from a neural network description format. Most of the time \onnx or \nnef are used as input like in \cite{LattnerAB0DPRSV21,pompougnac:hal-03043623,Jin2020}.

Because DNN are data-flow, it is natural to translate them into \emph{synchronous languages}.
There are some works such as
\cite{9094536} that proposed to encode them as Synchronous Dataflow Graphs
or
\scade tool suite \cite{ColacoPPP18} which is currently
developing a DNN libraries.
Once the translation is done, it is then possible
to reuse all the qualified code generation tools.
This is complementary to our work as we could use the \nnef description to generate the \scade program for instance.
To the best of our knowledge, none of actual available neural network description formats propose solution for describing multi items implementation with concerns on sharing variables among them.

% Need to include following ???
% MLIR (Multi-Level Intermediate Representation Overview) \cite{LattnerAB0DPRSV21} is a LLVM intermediate representation which was developed with the idea to use the same IR for all compiler optimizations (hence the ``Multi-Level''). It contains particular features that target machine learning applications, in particular it is possible to represent computation graphs in MLIR. MLIR can be instantiated into dialects that allow to put the focus on particular aspects of the code, to specify constraints or apply specific optimizations. An example of MLIR dialect that is particularly relevant to critical embedded applications such as the ones we target is described in~\cite{pompougnac:hal-03043623}: it enables the semantics of synchronous reactive applications inside an MLIR description.

%\subsection{Runtime}
% \onnx runtime ...
% MLIR
%% Associated to each format described in the previous section, we often find dedicated runtime that may be implemented in python like for \pytorch or \tensorflow or developed in C like \tensorflow Lite or $\mu$TVM.
%% These runtimes always rely on a dynamic exploration of the computation graph described by the previous format. The memory is also managed dynamically. It is associated to layers libraries that contain all supported layers of the framework. This approach is not compatible with the embedded aerospace domain that requires to develop software compatible with the DO178C standard.


%% \cite{SilvaCGP22} has developed the ACETONE code generator that claims to be compatible with critical avionics constrains. The approach is completely different and relies on an automatic C code generator that analyse the computation graph offline, do optimization and generate static C code with static memory allocation.







% \subsection{Towards code generation}
% C code generation ...

% a composition of functions as proposed by De Albuquerque et al. \cite{SilvaCGP22}.

% \cuda code generation ...


% Dynamic computation graph.

