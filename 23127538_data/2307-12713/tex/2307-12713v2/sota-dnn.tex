% !TEX root = main.tex
\chapter{Feed-forward deep neural networks} \label{ch:dnn}
% small history
%In the beginning of the chapter say that DNNs are only a part of machine learning that itself is part of AI. 
% There are two types of neural networks: feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don't.

This chapter presents the necessary background on \emph{feed-forward deep neural networks} to support the discussion of the following chapters. It is organized as follows: Section \ref{sec:concepts-dnn} introduces preliminary concepts on this class of functions together with a formal definition. Section {sec:train-inf} highlights the difference between the development and deployment phases in the life cycle of a feed-forward deep neural network model. Finally, Section \ref{sec:sota-dnn-models} presents the state-of-the-art of feed-forward deep neural network models targeted in this thesis.



% \section{Preliminary concepts} \label{sec:concepts-dnn}
\section{Formal definition of neural networks} \label{sec:concepts-dnn}
% DNNs are only a part of machine learning that itself is part of AI. 
% There are two types of neural networks: feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don't.

% When talking about neural networks we refer to feed-forward deep neural networks
% artificial neuron ? (notion of weights, bias, layer)
% notion of approximation
% notion of learning (updating) parameters
% non linearity is common for all NN, right?
%\begin{itemize}
%	\item Artificial intelligence (AI) is the science and engineering of creating intelligent machines that have the ability to achieve goals like humans do. 
%	\item Machine learning is a subset of AI, that does something.
%	\item Neural networks are a type of machine learning algorithm.
%	\item A neural network is a function f that aims at approximating f*.
%	\item A neural network deï¬nes a mappingy=f(x;$\theta$) and learns the value of the parameters $\theta$ that result in the best function approximation.
%	\item There are two types of neural networks: feed-forward neural networks (FNN) and recurrent neural networks (RNN). RNN have cycles in their connectivity structure, FNNs do not.
%	\item They are called networks because they are typically represented by composing together many different functions. The model is associated with a directed acyclic graph describing how the functions are composed together.
%	\item Each function is a layer. Each hidden layer of the network is typically vector valued.
%	\item Different layers may perform different transformations on their inputs. 
%	\item Rather than thinking of the layer as representing a single vector-to-vector function,we can also think of the layer as consisting of many units that act in parallel, each representing a vector-to-scalar function.
%	\item Deep neural networks are profound neural networks, with multiple hidden layers.  
%\end{itemize}


The field of artificial intelligence (AI) has gained much research attention in the last recent years. Its power reside in the capacity of solving problems that can be described by a list of formal, mathematical rules. In contrast, tasks that rely on intuitive reasoning, which can easily achieved by humans but that are hard to describe formally, represent a real challenge. Examples are tasks such as detecting a runway in a image for aircraft landing or yet understanding spoken commands \cite{Goodfellow-et-al-2016}. 
Within artificial intelligence there is an important domain called machine learning. It describes the study and development of statistical algorithms that are able to efficiently generalize on unseen data after the extraction of patterns from a similar, and representative, data set. 
The capability of generalizing -- or \emph{inferring} -- well derives from a \emph{learning} -- or \emph{training} -- process, which is an optimization problem \cite{Mitchell1997-ux}.

Neural networks are a class of machine learning algorithms. A neural network implements a mathematical function \Fn that aims at approximating a target function $h^*$ \cite{Hornik1989, Schafer2006}. The objective is that trainable parameters of function \Fn be recursively updated resulting in an accurate function approximation. The network terminology comes from the fact that \Fn is actually composed of many different mathematical functions. Each composing function is associated to a \emph{layer}, wherein it is possible to differentiate input, output and intermediate (hidden) layers. The number of layers in a neural network defines the \emph{depth} of function \Fn. Deep neural networks are then neural networks with multiple hidden layers. The neural notation is linked to the notion that each layer itself is a cluster of \emph{units}, where a unit is roughly inspired by a biological neuron. Thus, each unit implements a mathematical function as well. 
%The number of units acting in parallel in each layer define the neural network \emph{topology}.
Hence, the function performed by a layer is of the form $f_l:\mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$, wherein $m$ and $n$ represent respectively the input and output dimensions of the given layer's function. 

There are two types of deep neural networks: feed-forward neural networks and recurrent neural networks. Recurrent neural networks have cycles in their connectivity structure, the feed-forward variants do not.
A common representation of a feed-forward deep neural network (FDNN) is in the form of a \emph{directed acyclic graph} defining how its layers, and units, are connected together. Then to this directed graph we associate a mathematical function \Fn describing the mathematical rules of the neural network. 

%\begin{definition}[Neural network]
%A neural network is a \Fn defined by a set of parameters $\mathcal{N} = (L, \mathcal{T}, \mathcal{W})$ wherein:
%\begin{itemize}
%\item $ L > 2 $ is an integer, called the depth;
%\item $ \mathcal{T} = (k_1,\ldots,k_L) \in \mathbb{N}^{L} $ is a tuple of $L$ integers, called the topology, where $k_l$ represents the dimension of layer $l$;
%\item $\mathcal{W} = (\textbf{W}_{1},\ldots,\textbf{W}_{L}) \in \mathbb{R}^{k_1 \times k_2} \times \ldots \times \mathbb{R}^{k_{L-1} \times k_L}$ is a tuple of $L-1$ matrices, called the weights parameters. 
%\end{itemize}
%\noindent The neural network function $F_\mathcal{N}$ defined by these parameters is the composition function:
%\begin{equation}
%	F_\mathcal{N} : 
%	\begin{cases}
%      \mathbb{R}^{k_1} \rightarrow \mathbb{R}^{k_L}\\
%      \mathbf{x} \mapsto F_{L} \circ F_{L-1} \circ \ldots \circ F_{2}(\mathbf{x}) 
%    \end{cases}       
%\end{equation}
%wherein $F_l$, $\forall l \in  \ldbrack 2, L-1 \rdbrack$, is a non-linear function. 
%\end{definition}

\begin{definition}[Feed-forward Deep Neural Network] \label{def:dnn}
  A \emph{feed-forward deep neural network} $\mathcal{N}=(V,E)$ is a directed acyclic graph wherein:
  \begin{itemize}
  \item $V$ is the set of nodes of the graph, wherein $l \in V$ represents a layer;
%  A layer can be of many types such as an activation, a convolution or a pooling.
%  A layer comes with a set of parameters (e.g. weights or stride);
  \item $E\subseteq V\times V$ is the set of arrows of the graph, representing the data flow within the neural network.
  \end{itemize} 
\end{definition}

We represent $V_I\subseteq V$ as the set of input layers and $V_F\subseteq V$ as the set of final layers. Note that $V_I\cap V_F=\emptyset$.

Following Definition \ref{def:dnn}, we identify the direct ancestors of a layer $l$ as the elements of the set $Pre(l) = \{l' \in V \mid (l', l) \in E\}$. Additionally, all the ancestors of a layer $l$ are represented by $\emph{Pre}^*(l)=\bigcup \emph{Pre}^n(l)$, wherein
\begin{itemize}[label={}]
  \item $\emph{Pre}^n(l)=\bigcup_{l' \in\emph{Pre}^{n-1}(l)} \emph{Pre}(l')$ and $\emph{Pre}^{1}(l)$ = $\emph{Pre}(l)$
\end{itemize}

We precise that an input layer $l \in V_I$ only consumes input data, i.e., $Pre(l) = \emptyset$.  
Finally, as a straight consequence of a directed acyclic graph, we note that in a feed-forward neural network $l\not \in \emph{Pre}^*(l)$.

Figure \ref{fig:graph-dnn} illustrates a feed-forward neural network as described in Definition \ref{def:dnn}, where each layer (node) is represented within its associated function $f_l$. In this example we have $V_I=\{l_1,l_2\}$ and ${V_F=\{l_9, l_{10}, l_{11}\}}$. All the ancestors layers of layer $l_6$, for example, are defined by $Pre^*(l_6)=\{l_3, l_4, l_5, l_1\}$. We observe that skip connections are authorised, as in between layers $l_2$ and $l_7$, but there are no cycles, or feedback connections.   

% Figure environment removed



\begin{definition}[Function associated to a FDNN]
  \label{def:func-DNN}
  Let $\mathcal{N}=(V,E)$ be a feed-forward deep neural network and $V_F\subseteq V$ be the set of final layers, we note $\Fn = F_{V_F}$ the function associated to a feed-forward deep neural network, wherein:
\begin{equation}
\forall U \subseteq V, \quad F_{U} = (F_{l_{1}}, \ldots, F_{l_{n}}) \mid \forall j :\ \{1, \ldots,n\}: l_{{j}} \in U.
\end{equation}
wherein:
\begin{equation}
	 \forall l \in V, \quad F_{l}=f_{l}(F_{Pre(l)})   
\end{equation}
\end{definition}
Note that $F_{ \emptyset}$ is an empty tuple.

\begin{example}[\emph{Entirely} feed-forward deep neural network]
  In the particular case of a feed-forward deep neural network, wherein: 
\begin{itemize}
		\item $V = \{l_1, \ldots, l_n\}$;
		\item $\forall i\geq 2$, $\emph{Pre}(l_i)=\{l_{i-1}\}$.
\end{itemize}	


Therefore, $V_I=\{l_1\}$, $V_F=\{l_n\}$. 


According to Definition \ref{def:func-DNN} the function associated to $\mathcal{N}$ is the composition function:
\begin{equation}
	F_\mathcal{N} : 
	\begin{cases}
      \mathbb{R}^{m} \rightarrow \mathbb{R}^{p}\\
      \mathbf{x} \mapsto f_{l_n} \circ f_{l_{n-1}} \circ \ldots \circ f_{l_1}(\mathbf{x}) 
    \end{cases}       
\end{equation}
wherein $m$ in the input dimension of $f_{l_1}$ and $p$ is the output dimension of $f_{l_n}$. 


An illustration of an \emph{entirely} feed-forward deep neural network is shown in Figure \ref{fig:graph-entire-fdnn}, wherein we observe that $\emph{Pre}^*(l_i)=\emph{Pre}(l_i)=\{l_{i-1}\}$.

% Figure environment removed

\end{example}

\begin{example}[\emph{General} feed-forward deep neural network] In the case of a general feed-forward deep neural network as the one presented in Figure \ref{fig:graph-dnn}, according to Definition \ref{def:func-DNN}, the function associated to $\mathcal{N}$ is a tuple of functions:
\begin{align*}
	F_\mathcal{N} : ~& 
      \mathbb{R}^{m} \times \mathbb{R}^{m'} \rightarrow \mathbb{R}^{p} \times \mathbb{R}^{p'} \times \mathbb{R}^{p''}\\
	 F_\mathcal{N} = &~(F_{l_9}, F_{l_{10}}, F_{l_{11}})
\end{align*}
%      \mathbf{x} \mapsto f_{l_n} \circ f_{l_{n-1}} \circ \ldots \circ f_{l_1}(\mathbf{x}) 	
wherein,
%\begin{equation*}
%\begin{aligned}
%F_{l_9}= & f_{l_9} (F_{l_6}, F_{l_8}) \\
%F_{l_{10}}= & f_{l_{10}} (F_{l_8}) \\
%F_{l_{11}}= &  f_{l_{11}} (F_{l_8})\\
%F_{l_{8}} = &f_{l_8} (F_{l_6}, F_{l_7}) \\
%F_{l_6} = & f_{l_6} (F_{l_3}, F_{l_4}, F_{l_5}) \\
%F_{l_7} = & f_{l_7} \circ f_{l_2} \\
%F_{l_3} = & f_{l_3} \circ f_{l_1} \\
%F_{l_4} = & f_{l_4} \circ f_{l_1} \\
%F_{l_5} = & f_{l_5} \circ f_{l_1} \\
%\end{aligned}
%\end{equation*}
\begin{equation*}
\begin{array}{l|l|l|l}
F_{l_9}= f_{l_9} (F_{l_6}, F_{l_8}) & F_{l_{8}} = f_{l_8} (F_{l_6}, F_{l_7})        & F_{l_3} =  f_{l_3} (F_{l_{1}}) & F_{l_{1}} = f_{l_1}(\mathbf{x_1})\\
F_{l_{10}}= f_{l_{10}} (F_{l_8})    & F_{l_6} = f_{l_6} (F_{l_3}, F_{l_4}, F_{l_5}) & F_{l_4} =  f_{l_4} (F_{l_{1}}) & F_{l_{2}} = f_{l_2}(\mathbf{x_2})\\
F_{l_{11}}=  f_{l_{11}} (F_{l_8})   & F_{l_7} = f_{l_7} (F_{l_2})                   & F_{l_5} =  f_{l_5} (F_{l_{1}})\\ 
\end{array}
\end{equation*}


and $m$, $m'$ are the input dimensions of, respectively, $f_{l_1}$, $f_{l_2}$ and $p$, $p'$, $p''$ are the output dimensions of, respectively, $f_{l_9}$, $f_{l_{10}}$, $f_{l_{11}}$. 

\end{example}


In the remainder of this work, unless otherwise stated, neural networks refer to \emph{feed-forward deep neural networks}. 
In next section we mention different existing type of layers, however the formal semantics of the mathematical functions implemented by each layer considered in this work are detailed in Chapter \ref{ch:semantics}.


\section{Development versus deployment} \label{sec:train-inf}

The lifecycle of a neural network consists essentially in two parts: the development and the deployment. 
The development phase includes the design of the neural network model, i.e., the definition of the directed acyclic graph structure, followed by a training, or learning, stage. In this work we consider only \emph{offline learning}, meaning that, after trained, the neural network does not change during runtime and is a determinist function. The result of this phase is called the inference model. Afterwards, the deployment consists in implementing, i.e., coding the inference model in a low-level programming language and porting the code on the target hardware. Once implemented, the trained function is executed to perform the previously learned task, on what is referred to as inference phase. Figure \ref{fig:train-inf} illustrates the explained lifecycle.

% Figure environment removed


The design of the neural network model is highly dependent on the type of application targeted. It includes the choice of the number of layers as well as the function associated to each layer and their parameters. For example, convolutional neural networks are powerful to process images while multi-layer perceptrons perform well in classification tasks, as detailed in next section. Prior to the training phase one must compose the data set from which the neural network will learn the representative features needed for generalizing afterwards. In addition, performance metrics allowing to evaluate the learning process must be defined. Note that the neural network structure is susceptible to adapt during the training process. Finally, the learning approach itself can be categorized as
\begin{enumerate*}[(i)]
\item \emph{supervised learning}, where the network is trained from a labeled data set, meaning that to each input a label with the desired output is present,
\item \emph{unsupervised learning}, where the network is trained from a unlabeled data set and has to define its own rules to describe a given input and 
\item \emph{reinforcement learning}, which consists on maximizing a cumulative reward to learn what is the best action for a given situation.
\end{enumerate*}
As in this work we only consider the implementation of the inference model, we do not detail the training process and the associated techniques and methods. Moreover, we precise that, as long as it is performed offline, we are indifferent to the learning paradigm employed. 

% Talk about targets used.
With respect to the deployment phase, we made the research choice of exploring general purpose multi-core, commercial off-the-shelf (COTS), targets. We did not investigate accelerators, FPGAs or GPUs in this work. Classic cores may be less performant but have other advantages such as low energy consumption and mastered techniques to answer safety-critical certification requirements, as detailed in Chapter \ref{ch:bg-certif}. Our objective is to have a standalone application to be deployed in any target that supports C code, a standard low-level language, without runtime interpretation and relying on static memory allocation. Thus, in our experiments we focused on a single-core and bare-metal implementation. However, we aim to be agnostic towards the runtime environment as long as it provides real-time guarantees.

%We instead focus on general purpose multi-core, commercial off-the-shelf (COTS), hardware such as the Coolidge (Kalray, 2021) or the keystone (Texas Instruments, 2013) 

%Our goal is then to identify what are the limits in terms of size of models that can
%be deployed and performance that can be achieved.
%
%
%We aim to be agnostic towards the runtime environment as long as it provides real-time guarantees. 
%We do not desire to depend on a software stack that we do not master and which can potentially include
%unpredictable elements, temporally or functionally. In our experiments we used a bare-metal implementation.
%
%inference execution on a single-core bare-metal target is very determinist.
%
%Generate DO-178C compliant and efficient C code for inference models in floating-point precision is an
%important step.


\section{State-of-the-art on NN models} \label{sec:sota-dnn-models}
In this section we present examples of real-world neural network models as defined in Section \ref{sec:concepts-dnn}. These were later used to test and validate our approach for the deployment of inference models as detailed in Chapter \ref{ch:val-acetone}. 

%\subparagraph*{Fully-connected networks -- \acas experience}
%The first models correspond to the classical fully-connected networks as shown in the example. We rely in particular on the airborne collision avoidance system for unmanned aircraft (\acas) \cite{9081758}.
%The \acas system takes five input variables, i.e., information from sensors
%measurements, and computes five action advisories, represented by scores.
%The advisories indicate for example if the current 
%situation is clear-of-conflict in which case the aircraft can continue its pathway or whether it should turn to avoid collision.
%The original design relies on a set of off-line computed lookup tables (LUT) to make avoidance decisions. Some work \cite{DBLP:conf/cav/KatzBDJK17, Damour21} proposed
%to replace those LUT with some surrogate neural networks in order to reduce the memory footprint 
%and thus to improve the execution time.
%We  consider several DNN models with various structures,
%all with a ReLU activation function in hidden layers, linear activation for output layer and floating-point single precision (FP32) data type:
%\begin{itemize}
%\item regular structures with the same number of neurons per layer. We consider
%   7 hidden layers with \textit{reg50} (50 neurons per layer),
%    \textit{reg100} (100 neurons per layer) and \textit{reg200} (200 neurons per layer);
%  \item decreasing structures with
%    \textit{decr128} (5 hidden layers of size (128, 128, 64, 32, 16))
%      and \textit{decr256} (6 hidden layers of size (256, 256, 128, 64, 32, 16));
%\end{itemize} 


\subsection{ACAS-Xu surrogate models}
%Damour21 regular et decreasing
We study the approximation using neural networks of the \acas system, which stands for airborne collision avoidance system for unmanned aircraft. The \acas system takes five input variables, i.e., information from sensors measurements, and computes five action advisories, represented by scores.
The advisories indicate for example if the current situation is clear-of-conflict in which case the aircraft can continue its pathway or whether it should turn, to the left or to the right, to avoid collision. The original design relies on a set of offline computed lookup tables (LUT) to make avoidance decisions. Then, some works \cite{DBLP:conf/cav/KatzBDJK17, Damour21} proposed to replace these LUT with surrogate neural networks in order to reduce the memory footprint and thus to improve the execution time.

%figure system acas

The \acas neural networks have a variable number of layers. A common property they have is that all hidden layers are of the type \emph{dense} and between each one of them there is a layer implementing an activation, or non-linear function, of the type \emph{ReLU}. We consider regular structures, which have the same number of units per layer, as well as decreasing structures, with a number of units that decreases as we advance in the depth of the neural network. We examine the following models:
\begin{itemize}
\item  \acas \textit{reg50}: seven hidden layers with 50 units per layer,
\item \acas  \textit{reg100}: seven hidden layers with 100 units per layer
\item \acas  \textit{reg200}: seven hidden layers with 200 units per layer,
\item \acas  \textit{decr128}: five hidden layers with respectively $(128, 128, 64, 32, 16)$ units per layer,
\item \acas \textit{decr256}: six hidden layers with respectively $(256, 256, 128, 64, 32, 16)$ units per layer.
\end{itemize} 

%figure one of nn 
\subsection{LeNet-5}
The \lenet model \cite{DBLP:journals/neco/LeCunBDHHHJ89} is a feed-forward convolutional neural network developed for handwritten digits images recognition. It is one of the earliest models of this type and is known for promoting the development of machine learning with the introduction of the back-propagation algorithm in the training phase. Although simple, this model contains the main basic type of hidden layers: \emph{convolutional}, \emph{pooling} and \emph{dense} layers. The layers that implement an activation function between the aforementioned hidden layers are of the type \emph{hyperbolic tangent}. 
Finally, the output layer is followed by an activation layer of type \emph{softmax}. Figure \ref{fig:lenet} illustrates the structure of the \lenet model, with a representation that highlights the multidimensionality of the layers functions. 


The size of the input / output tensors are shown on the figure.
   The first 2D-convolution \emph{conv1} takes inputs of size $28\times 28\times 1$,
   is composed of 6 kernels $K^i$ of size $5\times 5\times 1$
   and of a stride $s=(1,1)$. The activation function \emph{tanh} is applied to the outputs. 
   The first pooling layer \emph{pool1} is an average pooling with stride $s=(2,2)$ and window $k=(2,2)$. 
   The second 2D-convolution \emph{conv2} is  composed of 16 kernels $K^i$ of size $5\times 5\times 6$
   and of a stride $s=(1,1)$. The activation function \emph{tanh} is applied to the outputs.
   The second pooling layer \emph{pool2} is an average pooling with stride $s=(2,2)$ and window $k=(2,2)$.
   The 3D-tensor of size  $6\times 6\times 4$
   is flattened in a 1D-tensor of size $96$.
   There are three dense layers with respectively
   $(n_i,n_o)=(96,120)$,
   $(n_i,n_o)=(120,84)$ and  $(n_i,n_o)=(84,10)$.
   The two first dense layers apply the activation function \emph{tanh}
   and the last one a \emph{softmax}.
   Thus the function associated to the \lenet model is:
   $F_\mathcal{N}=f_{\emph{softmax}} \circ f_{\emph{dense3}} \circ f_{\emph{tanh}} \circ f_{\emph{dense2}} \circ f_{\emph{tanh}} \circ f_{\emph{dense1}}\circ f_{\emph{flat}} \circ f_{\emph{pool2}} \circ f_{\emph{tanh}} \circ f_{\emph{conv2}} \circ f_{\emph{pool1}} \circ f_{\emph{tanh}} \circ f_{\emph{conv1}}$.

  % Figure environment removed


This model has 44,426 trainable parameters to stock and an inference pass executes 572,504 floating-point operations (FLOPs).


%\subparagraph*{CifarNet}
%\cifarnet was first introduced in \cite{Krizhevsky09learningmultiple} and was for a long time the state-of-the-art model used to solve the object classification problem on the Cifar-10 data set, which consists of 32 x 32 RGB images of 10 classes. \cifarnet is composed of three convolutional layers, and its pooling layers, followed by two dense layers (see figure \ref{fig:cifarnet}).
%The ReLu activation function is applied to all the layers. 
%The main difference with \lenet is that it has a three-dimensional input and the convolutional layers have additional parameters such as padding and a non equal to $1$ stride, which adds some complexity in terms of computation. With this configuration the number of trainable parameters increases to 122,570 alongside with 9,18 million FLOPs for inference.
%%\vspace{0.2cm}
%
%  % Figure environment removed
%
%\subparagraph*{AlexNet}
%The \alexnet architecture was first defined in \cite{DBLP:conf/nips/KrizhevskySH12} and is considered as one of the most influential works in computer vision. Indeed, thanks to the use of convolution layers and GPUs to accelerate deep learning, it achieved a considerably improved performance over other methods in the \imagenet Large Scale Visual Recognition Challenge (ILSVRC) of 2012. The \imagenet data set \cite{DBLP:conf/cvpr/DengDSLL009} is composed of 256 x 256 RGB images categorized under 1000 object class categories. 
%\alexnet has five convolution layers, three pooling layers and three dense layers (see figure \ref{fig:alexnet}\footnote{There is a pre-processing that consists of a \emph{scaling} from 256 x 256 to 228 x 228 of the input})
%with approximately 61 million tunable parameters and 1,64 billion FLOPs. Additionally, this model uses the ReLU activation function, which was presented as novelty and proved to be more efficient in learning phase than the, at the time, standard hyperbolic tangent \cite{DBLP:conf/nips/KrizhevskySH12}. 
%%\vspace{0.2cm}
%
%  % Figure environment removed


%\section{Fully-connected neural network}
%\begin{definition}[Fully-connected neural networks]
%A fully-connected neural network is a particular type of neural network where all nodes of a layer $l, \forall l \in  \ldbrack 2, L \rdbrack$, are connected to the nodes of layer $l-1$, wherein:
%\begin{equation}
%	\forall l \in  \ldbrack 2, L \rdbrack, \quad F_l: 
%	\begin{cases}
%      \mathbb{R}^{k_{l-1}} \rightarrow \mathbb{R}^{k_l}\\
%      \mathbf{x} \mapsto \sigma(\mathbf{W}_l\cdot\mathbf{x}) 
%    \end{cases}       
%\end{equation}
%\end{definition}
%
%\begin{example}
%
%\end{example}
%
%\section{Convolutional neural networks}
%%high level (-> why are they used, why are they powerful), formalization of functions will come later
%% computational complexity and memory requirements
%% uses convolution-based layer
%% and also a pooling layer
%%(the neuron in a CNN is only applied to a local spatial region of the input (i.e. its receptive field), not to the entire input as in an MLP. )
%% the neuron is "applied", because it is a mathematical function.
%% in cnns -> However, as we will see the number of effective connections is significantly greater due to parameter sharing.
%%Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity.  They still compute a dot product of their weights with the input followed by a non-linearity, but their connectivity is now restricted to be local spatially. the only difference between FC and CONV layers is that the neurons in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical.






















