\section{Introduction}
%claire
Machine learning (ML) applications have been gaining considerable attention in the field of transportation. However, their use in real-life operational safety-critical products, in particular in the aeronautical domain subject to stringent certification, raises several issues regarding functional correctness, compliance with requirements, formal verification, safety or implementation.
In order to tackle those issues,
new guidelines -- named  ED 324/ ARP 6983 standard \cite{wg114} --
are currently drafted by the EUROCAE WG-114/SAE G-34 joint working group
that cover the whole spectrum of the system development including the data sets composition, the ML model design and its implementation. In this paper, we focus only on the implementation of the ML model.



\subsection{Context}
In the ML current practices, a \emph{training framework} is used to design an ML model and the resulting ML model
is then deployed on the target with a \emph{deployment framework}.
It is up to the training framework or a designer to export the trained model description
in an exchange format and up to the deployment
framework to import the ML model description.
%and ensure the semantic preservation of the execution.
The left part of figure \ref{fig:currentpractice} shows those practices.
The deployment framework is most of the time an ML model interpreter,
that can accommodate any type of ML model architecture,
and that allocates at runtime the execution on the different
available accelerators (e.g. \gpu, \fpga) of the target.
These ML frameworks have been designed 1) to ease as
much as possible the deployment of models for the users
and 2) to optimize
as much as possible the execution performance (usually expressed
in trillion operations per second -- TeraOp/s).
As a result, they are very impressive and
allow for complex deployments and optimizations. Those are
hard, if not impossible, to reproduce for a programmer without
using ML libraries (e.g. \cudnn on \nvidia).

% Figure environment removed

The counterpart of such an approach is
1) the absence or limited information of
internal computation and allocation;
2) small modifications and adaptations
of the ML model
(e.g. when exporting the ML model description
or
when quantizing on the fly some
matrix multiplication to execute on deep learning accelerator -- DLA).
If this grey/black box approach
is acceptable and suitable for mainstream applications,
it is a blocking point for highly safety-critical applications.
As a result, the main objective
of the ARP 6983 standard with respect to the implementation process
is the \emph{semantics preservation} of the off-line trained model on the final hardware platform.
This means that the execution of the ML model in the training
framework should be exactly reproduced on the embedded target during execution.
To reach that objective, an alternative development process is proposed
as illustrated in the right hand part of figure \ref{fig:currentpractice}.
%The cost is the reduced performance and deployment capacities compared
%to ML deployment approaches.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Assurance development process}
%To ensure the semantic preservation, the idea is the rely to trustable
%development process (figure \ref{fig:currentpractice}).
The principle of this trustable development process (figure \ref{fig:currentpractice}) is to ensure the semantic preservation at each step of the development cycle (e.g. between training framework and description, between description and code, between code and executable).
This is guaranteed thanks to a series of requirements provided in the ARP 6983.

\textbf{Requirement 1.}
First, the trained model must be formally
and unambiguously defined in what we call an adequate \emph{format}.
Such a \emph{format}
must come with a formal syntax and semantic,
and should be agnostic of any (training and/or deployment) framework.

\textbf{Requirement 2.}
Second, the implementation process
should allow several deployments on hardware platforms
and it must be known beforehand how the ML model will be mapped
on the accelerators and when.
%Indeed not all designers will map a model in the same way
%or will choose the same hardware.
%But what we expect
%is the semantic preservation by construction
%thus we choose to determine off-line the mapping
%and this parallel specification.
This entails in particular that
the  \emph{format} should allow several types of deployment such as
distribution, parallelization or pipelining.
Thus, there is a step that consists in splitting the
ML model description (in the chosen format) as a series of \emph{item} descriptions.
Indeed, in the avionics domain,
a target processor is decomposed as a set of software (SW) and/or hardware (HW) \emph{items}. Let us consider for instance an \ultrascale (ZCU102) platform \cite{ultrascale}: it is composed of several \arm cores, a \gpu and an \fpga.
If the ML model is spread over the different components, in particular on one \arm and the \fpga, there will necessarily be several items.
Indeed, 
the \arm associated code will be considered as one SW item
and will go through the
ED-12C/DO-178C \cite{do178c} development process.
Whereas the hardware design of the \fpga will be considered as another item (HW) and will go through the ED-80/DO-254 \cite{do254} development process.
%% If we consider a \xavier platform \cite{nvidiajeston} composed of several \arm cores and \gpu,
%% the designer will probably split the software implementation into several software items, in order to distinguish the \gpu and the \arm.
%% Thus, each item implementation requires its own formal specification
%% that corresponds to the ML model part it will execute.
%This is the reason why we need to be able to decompose a global ML model description into several pieces
%expressed themselves in the same
%format
%such that the composition of the pieces description is
%equivalent to the global model description.

\textbf{Requirement 3.}
Third, the implementation has to follow the usual aeronautical development standards
(e.g. ED-12C/DO-178C \cite{do178c} for software). 
Thus, the description of a model within the
format must be compliant with a certified implementation process.
This last objective concerns the capability to implement an item description following standards such as ED-12C/DO-178C \cite{do178c} or ED-80/DO-254 \cite{do254}.
Among the requirements from those standards, two are related to the format.
First, it must offer full traceability:
looking at the generated code (e.g. C or \cuda), it is humanly possible to trace back to the original exported
 model.
 Second, the execution must be \emph{predictable}, meaning that it is possible to assess a WCET (Worst Case Execution Time) \cite{WCETpaper}.
 This entails that the code is expected to be allocated statically on the resources, all the memory allocations are static and the schedule (here the sequence of operations) is also static.

\vspace{-.2cm}

\subsection{Contributions}
Our general objective is to define an approach  compatible with the ARP 6983 requirements presented in the prior section.
We focus on a representative subset of deep neural networks (DNNs)
that is feed-forward neural networks trained off-line.
%(see section \ref{sub-sec-DNN}).
Our main goal is
the definition of an adequate format, with a formal syntax and semantics,
able to describe both 1) a global ML model,
and 2) any parallelized allocation on several items,
the behaviour of which is equivalent to the global model.

\textbf{For requirement 1.}
There are several initiatives to propose an intermediate
format between trained models and their implementation such as
\onnx \cite{onnx} (Open Neural Network Exchange).
After a thorough evaluation of different existing formats,
we identified 
\nnef (Neural Network Exchange Format) \cite{nnefformat}
as the most suited for our purpose:
syntax and semantics are public
and moreover the authors made a strong effort to provide a formal specification. 
%\nnef is highly interoperable as it is compatible with any ML framework.


Since \nnef provided a potential candidate, we decided to construct our format on top of it.
As it is now, \nnef describes formally the global ML model. Indeed, the semantics of \nnef
is almost fully defined (see section \ref{sec-nnef}). What is however missing in \nnef
is the clear formalization of the \emph{execution model},
that is the formal behaviour behind a series of \emph{\nnef instructions}.
To fix this missing element, we rely on
\petri nets, a usual  representation of program behaviours \cite{peterson1977petri}.
This choice
is also consistent with the need to express distributed behaviour on items, as
\petri nets also allow to model all combinations of execution:
sequence, pipeline, recursion or parallel \cite{olderog1986operational}.
%In order to ensure semantic preservation,
%the ML model description should precisely reflect the
%ation at export, at load and at execution.


\textbf{For requirement 2.}
We illustrate in  section \ref{sec-distribution}
why decomposing ML model into items is of interest.
%The first contribution, that will be detailed in the final paper and which can be found in the unpublished arXiv paper \cite{gauffriau2023formal},
%is the definition of an 
%extension of \nnef to allow the distribution of an ML model onto different items.
To that end, we extend the syntax and semantics of \nnef.
We rely in particular  on \emph{logical data exchange} among items to distribute the computation and express the semantics with \emph{coloured \petri nets}.
 We formally show that the synchronization of the \petri nets behaves as the \emph{original} non distributed \petri net.

\textbf{For requirement 3.}
 We do not propose a complete DO-178C compatible approach but
 instead show a reasonable
  implementation approach 
  on the \xavier platform \cite{nvidiajeston} %This will also be shown in the final paper.
  that we believe
 could be with some effort compliant with the ED-12C/DO-178C \cite{do178c}. 
%% To that end, we define an automatic \cuda-based code generation from an ML description
%% in the extended \nnef format and  assess in our evaluation its code traceability, the deterministic behaviour on the platform
%% and the semantics preservation from the trained framework.
%% All of that will be detailed in the final paper.
%% %section \ref{sec-code-generation}.

%We propose to detail answers to previous requirements in the final paper.
%For this extended abstract, we are given high level proposals on the requirements.



