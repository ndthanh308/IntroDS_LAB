\section{Implementation and experiments}
\label{sec-code-generation}
The previous sections showed how to fulfill the requirements 1 and 2 of the introduction.
The purpose of this section is to give some hint of
how a DO-178C compatible implementation process, as required per requirement 3,
could be defined taking as input an extended \nnef specification.
The considered target, 
a Jetson \xavier TX system-on-chip, %using \emph{C++} and \cuda.
%This target %(see figure \ref{fig:jetson})
is composed of
6 Carmel \arm cores, a \gpu, 2 deep learning accelerators (NVDLA) and
other dedicated circuits.
The use of a \nvidia platform is mainly motivated by its availability
and the ease to quickly deploy neural networks application.
We will not discuss
the adequation of \gpu and \cuda implementation with DO-178C objectives because it is an open problem. 


%% % Figure environment removed


% % Figure environment removed

%The proposed implementation process workflow
%is summarized in figure \ref{fig:implemworkflow}. 
The \emph{sync} implementation relies on barrier mechanism
and each item \nnef description is manually coded.
In order to validate the semantic preservation,
we made some instrumentation to show that: 
\begin{enumerate*}[label=(\roman*)]
  \item the execution trace is included in all possible execution traces defined by the \petri net;
  \item the numerical precision is kept;
  \item the measured execution time does not vary.
\end{enumerate*}
We will use the multi-items example presented in example \ref{ex-multiple-nnef} as the specification.
For our experiments, each item is allocated
to one CPU and all the \cuda cores of the \gpu (grouped in a \cuda stream).
%Thus one item correspond to one CPU and one \cuda stream.
As a consequence, we do not guarantee a segregation between items (as they share the GPU)
%
%and this will be the purpose of future work.
we instead focus on a way to implement
parallel operations of neural networks
with a static code scheduling while preserving the semantics.
As for the \petri semantics, we only developed a code for a single inference.
Nevertheless, it is easy to slightly modify the code by adding loops to handle several input tensors.
%The code generation is not fully automatic yet and supports only the layer types of the example.

%% % Figure environment removed



%% \subsection{Example for experiments}
%% \label{subsec-toyex}
%% Let us consider a toy neural network deployed on 3
%% different items of Figure \ref{fig:modelitem}.
%% %The model contains one input (convolution + maxpool) layer followed by two parallel branches that contains 2 (convolution / maxpool) layer. Finally branches are concatenate to fill the output fully connected layer.
%% %The input convolution and output fully connected layers are allocated to the first item.
%% %The parallel branches are parallelized on items 2 and 3. 
%% The complete network has been trained and exported with \pytorch into \nnef description.
%% The decomposition into 3 item descriptions is done following the rules of section \ref{subsec-split-nnef}.
%% %The full code is given for the artifact and reviews in an additional material.
%% %% We need to exchange data between all 3 items:
%% %% the output of the first (convolution + max pooling) computed by item1 is sent to item2 and item3. Then, layers of item2 and item3 can be computed in parallel. They finally both send their output to the item1 that finishes the computation. We give below the description for items 1 and 2.
%% Due to lack of space, we do not detail the \nnef descriptions. %but they will be available for artifact if any.
%% %Again, the code are available for artifact and review and we simply show part of item 1 description here.
%% %% \begin{minipage}{.42\linewidth}
%% %% \lstinputlisting[caption={Item1 description},
%% %%                   label=list-nnef-item1]
%% %%                 {../MLMID/MLMID-item1-extract.txt}
%% %% \end{minipage}\hfill
%% %% \begin{minipage}{.42\linewidth}
%% %%   \lstinputlisting[  caption={Item2 description},
%% %%     label=list-nnef-item2]
%% %%                   {../MLMID/MLMID-item2-extract.txt}
%% %% \end{minipage}
%% %% The item1 description starts by listed the variables and in particular the 3 \emph{variablesync}
%% %% that allow the communication across items.
%% %% A fragment $g_1$ has been defined that contains the convolution and max pooling.
%% %% Then the variable \emph{oitem1} is sent to the items 2 and 3.
%% %% The item 1 then waits for two variables, one from item 2 on \emph{oitem2}
%% %% and one from item 3 in \emph{oitem3}. After, item 1 ends with fragment $g_2$ which contains the concat, the flatten and the gemm.
%% %% Similarly, item2 description starts by listed the 2 s\emph{variablesync} that  allow the communication with the item2. It starts by waiting for item1 input data.
%% %% Fragment $g_3$ contains the two convolutions and max poolings done in the upper branch.
%% %% Then item2 sends to item1 its output.
%% %% The item3 is very similar to the item2 and is not presented here.
%% The figure \ref{fig:toymulti} presents the multi-items semantic of the toy example using a coloured \petri net. 

%% % Figure environment removed



 


\subsection{Get/Send specification}
%The example relies on 3 items that communicate thanks to the \emph{get_var} and \emph{send_var} fragments. 
We chose to implement \emph{get_var} and \emph{send_var}
with
1) global variables stored in the SRAM of the \xavier
and
2) the \posix barrier mechanism of the \emph{pthread} library.
%for the synchronization part and global variables for data exchanges. 
A barrier $b$, shared among several processes,
will block them as long as not all of them reach $b$.
Such a behavior is strictly included in the semantics of the \emph{sync} transition within the \petri net.
However, it is not the most efficient as it prevents
the sending item to proceed until
all the receiving items reach $b$,
whereas
the semantics of \emph{sync} transition only
requires a receiver to wait for the sender (not the sender
 to wait for all receivers).
Nonetheless,
the barrier mechanism is optimal for our  example
because no sender has to process
any instruction before a further \emph{get\_var} or stop execution.

%####################################################################@
\subsection{Manual code generation}
There are
C and \python interpreters of the \nnef format \cite{nneftools}
but only for traditional CPU target.
Consequently, no existing tool supports our syntax extension nor state-of-the-art \gpu.
Thus we developed the code for each item using C++ and
\cuda using the \cudnn library. Basically the C++ code is executed by the \arm processor whereas  \cuda  allows the definition of kernels that are executed synchronously by all \cuda cores.
The \cudnn library is built on top of \cuda for executing common neural networks layers.
%We focus subsequently on the principles of our software architecture and the scheduling.

\subsubsection{Software architecture}
Practically,
%each layer is represented by a C++ object that contains all the necessary code for initialization and execution.
each type of layer is implemented using a dedicated C++ class that inherits from the abstract \emph{Layer} class
that defines common attributes and methods to be implemented (\emph{init()} and \emph{forward()}) by child classes.
%Adding new layers is easy and just requires to implement their two abstract methods.
In effect,  \emph{init} statically allocates tensors and \cudnn descriptors while
\emph{forward} launches the layer computation based on \cudnn for Convolution and max pooling layers.
Each item contains one object implementing a static scheduler.
%that calls in sequence the \emph{forward()}
%method of each layer.
More precisely, during the \emph{init} phase, each item  creates an object for each layer
which are stored in ordered lists.
Thus, items 2 and 3 need one single ordered list whereas item 1 needs two ordered lists (one  for the first part and one for the second part).
During \emph{forward},
the scheduler  calls in order the \emph{forward} of objects stored in ordered lists.





\subsubsection{Scheduling}
%The implementation of the synchronization that we defined with \petri net  is based on the \emph{pthread} \posix library.
We define one separate thread for each item allocated to one CPU + \cuda cores.
% Communication between items are implemented with \emph{synchronizations}.
More precisely, synchronizations between threads use \texttt{pthread_barrier_t} and associated APIs (\texttt{barrier_init} and \texttt{barrier_wait}). 
Barriers synchronize accesses to shared variables. %that are stored in the SRAM.
%Other synchronization mechanisms such as spin-lock or semaphore may be closest
%to a non blocking behavior but are more complex to handle.
%when multi-items synchronization is needed.
%Our main purpose is to exhibit one possible implementation that respects the semantics.

\begin{center}
%\begin{figure}[hbt]
    \resizebox{.8\linewidth}{!}{\input{sched}}
%      \caption{Global scheduling\label{fig-scheduling}}
\end{center}


The execution sequence 
starts with the 3 threads creation on the CPUs
and then reaches the first synchronization barrier.
Then Item1 thread calls the \emph{forward} method of layers of the head (until \emph{send\_var})
while Items 2 and 3 threads wait for the second synchronization barrier.
After,
the second synchronization barrier,
Items 2 and 3 threads  call \emph{forward} method of their layers
while Item1 thread waits for the third synchronization barrier.
After the third synchronization barrier,
Item 1 thread calls \emph{forward} method of layers of the tail.
At last, threads join and exit.




%####################################################################@
\subsection{Semantic preservation of the \petri net}
\label{sec-petriprese}
%We developed 3 instrumentation codes to validate the semantic preservation.
The first analysis
aims at verifying that all observed scheduling of layers on the \xavier
respects the \petri net semantics.
Because we use a static scheduler, all schedules should
behave as shown in section \ref{sec-petriprese}
which is included in the semantics of coloured \petri net
of figure \ref{fig:sync}.
% Thus we need to monitor the start and end of each branch/items. In each item, we need to monitor the execution of layers.
For that, we logged each start/end of branches and layers
and we stressed the robustness of the implementation by addind some temporal noises (sleep in the code).

All observed traces respected the schedule of section \ref{sec-petriprese}
with some timing variations.
When observing the implementation with no noise,
execution traces of Item 2 and 3 are interleaved on the \gpu.
When adding a wait of 1s at the beginning of Item 2
(just after barrier1), all layers of Item 3 were executed before those of Item 2. 

%% % Figure environment removed



\subsection{Semantics preservation of the function}
The second instrumentation mechanism aims at checking
that the functional semantics of the DNN is preserved.
We achieve this by re-implementing the \nnef specification in \pytorch. Then we define 100 random vectors that we run
both on the \pytorch implementation and on the C++ implementation on the \nvidia target. Finally, we compute the overall average error mean between both executions for the 100 runs. 

%We do not have access to the source code of convolution algorithms used by \cudnn
%or to convolution provided by \pytorch.
We were not able to find the exact convolution algorithm of \pytorch. We think that it exists a non documented heuristic that calls the best algorithm (considering execution time) depending of the convolution parameters and available hardware. 
According to the \cudnn documentation \cite{nvidiacudnn},
it is possible to select the convolution algorithm among a list, but details of the implementation are not given.
Thus, there may be a discrepancy between convolutions that we cannot fix.
The average error mean
is extremely small $1.10^{-7}$ for FLOAT32
using 3 \cudnn algorithms (namely gemm, Winograd and direct).
Nevertheless numeric precision results for this experiment are in an acceptable range that is very close to the available numeric precision of floating point representation
and
this also is observed by other frameworks \cite{SilvaCGP22}. 


\subsection{Measured Execution Time (MET)}
One objective of the DO-178C that we did not mention until now
is the capacity to estimate the Worst Case Execution Time (WCET). 
Due to the complexity of \nvidia target, a formal demonstration using static analysis %such as \otawa
may be difficult. But at least, a good property is a low variation of the measured execution time among several executions.
In our case, the generated code does not contain any IF-THEN-ELSE patterns
or dynamic loop conditions. Thus, the variability is only linked to the hardware behavior. 
We measured the MET of the complete DNN and of the first convolution of Item 1 over 10 runs.
We rely on the \emph{nsys} tool from \nvidia to get timing measurements.

\begin{center}
% \noindent\resizebox{.5\linewidth}{!}{
  \footnotesize
  \scriptsize

\begin{tabular}{|l|c|c|c|c|}
      \hline
       & \textbf{Mean(MET)} & \textbf{MIN(MET)} & \textbf{(MAX(MET))} & \textbf{STD(MET)} \\
      \hline
      \textbf{First Conv}  & 324 2976 ns  & 322 688 ns & 326 656 ns  & 1.45 ns \\
      \textbf{DNN}  &  24 257 $\mu$s &  16 285 $\mu$s &  53 950 $\mu$s & 13 526 $\mu$s\\
      \hline
  \end{tabular}
\end{center}

%Results show  mean, min, max and std measurements of the execution time for the first convolution and the whole pipeline.
The MET of the first convolution is very stable with a very low jitter.
The MET distribution of the DNN is large
and to understand why, we need to investigate the low level behaviour.
\nvidia\ \gpus are  black-boxes processors on which we cannot guarantee worst-case execution time \cite{AmertA21,carle-erts22}.
%This perfectly illustrate issues that we may encounter when using complex hardware whereas software algorithms are without any branches.
%For a compliant DO178C this behavior need to be understood and an effort shall be put in limiting this global jitter.

% Whereas the error is extremely 
% https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionFwdAlgo_t



% Rajouter des expes sur le profiling avec nvprob
