\section{Is distribution needed?}
\label{sec-distribution}
Because we consider highly distributed platforms, a designer may choose to split the ML model into several parts
in order to accelerate the execution and reduce the execution time.
In particular, it could lead to developing parts independently and on different \emph{items} following the aeronautics standards \cite{arp4754}.
In such a case,  there should be a formal description for each item that becomes the input
specification for HW/SW item implementation. 
%Looking at the \lenet model or other classical models, it is unclear 
%why distributing across  into items would be beneficial.
We identified 3 different needs for distribution to be addressed
%by our syntax and semantics extension
%of the \nnef.
that we illustrate on the \lenet example.

\begin{remark}
Note that if the designer considers its platform as a unique item, the \nnef description will be the specification.
\end{remark}


\noindent \textbf{Off-loading computation.}
%\begin{example}[Off-loading convolution]
  Let us consider for instance the implementation of the \lenet
  on an \ultrascale (ZCU102) platform \cite{ultrascale}
composed of several \arm cores, a \gpu and an \fpga.
Let us assume that we choose to execute the convolution on the \fpga and all other layers on one \arm.
%there will necessarily be several items.
The idea will be to offload the input tensor of each convolution on the \fpga
and retrieve the feature maps from the \fpga (see figure below).

% Figure environment removed

%\end{example}

\noindent \textbf{Parallelizing the layers.}
A second type of distribution could be to refine the layers and exhibit more parallelism by distributing the computation of a layer across several items.
It will be up to the designer to show the semantic preservation at this refinement level.
%\begin{example}[Parallelizing the layers]
  % Looking again at the \lenet, because the number channel in the input tensor is $1$ and because the stride of the \emph{conv1} and of \emph{pool1} are $(2,2)$, the two first layers could be duplicated and applied on two distinct parts of the input tensor as shown in figure below.
  Looking again at the \lenet, we can split the computation of the first two layers on two different items. Each item
  will do the convolution+maxpool on a part of the input image.
  To do so, the input image is split along the height
  and two sub-parts are executed on two different items (see figure below).
  In order to keep the semantics for the convolution, some overlap exists between the two sub-images. 


% Figure environment removed
%\end{example}


\noindent \textbf{Pipelining the computation.}
  A third type of distribution is the pipelining of the DNN execution.
  In this case, each item is in charge of computing a specific layer (or a group of layers).
  The first item computes the first layer(s) on the first tensor input and sends its output to the second item that will compute the second group of layer(s)
  while the first item starts processing the second input tensor. 
  This classical mechanism enables to reduce (e.g. for video processing) the frame rate while degrading the latency. 
  The depth of the pipeline is the number of inputs that can be handled at the same time among the pipeline.
 % Taking into account the pipelining should be done in the \petri net by restoring the tokens in the initial places at the end of item 1 execution.


  % Figure environment removed


