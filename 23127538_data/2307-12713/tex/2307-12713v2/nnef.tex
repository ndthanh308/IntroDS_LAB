\section{Format of description -- \nnef}
\label{sec-nnef}
% Peut être ne pas tout définir, mais essayer de faire un truc un peu plus minimal.
% Rajouter le fait que le noeud s'active quand on recoit un nouvel input.
\khronos standardization group\footnote{\url{https://www.khronos.org/}} has defined the \nnef (Neural Network Exchange format)
format with a specification that provides a syntax and a semantic.
%After outlining the definition of deep neural network, we will present \nnef.
We focus on the \nnef syntax and semantics elements needed for our purpose. The reader can refer
to \cite{nnefformat} for a complete description of \nnef.
%We end the section by defining the execution model semantics
%with a translation into \petri net.

\subsection{Brief Reminder on Neural Network}

% From Iryna proposal, MAJ 12/02

The field of artificial intelligence has gained much research attention in the past years.
The power of AI resides in the capacity of solving highly complex problems \cite{Goodfellow-et-al-2016}. 
%In contrast, tasks that rely on intuitive reasoning, which can easily achieved by humans but that are hard to describe formally, represent a real challenge.
%Within artificial intelligence,
Machine learning domain describes the study and development of statistical algorithms that are able to efficiently generalize on unknown input data after the extraction of patterns from a similar, and representative, data set.
%Examples of tasks where ML succeeds are detecting a runway in an image for aircraft landing \cite{aerospace9110634}, supporting predictive maintenance \cite{Carvalho2019-pi}, helping air traffic control \cite{Yousefzadeh2021} or yet understanding spoken commands \cite{Nassif2019}. 
%The capability of generalizing -- or \emph{inferring} -- well derives from a \emph{learning} -- or \emph{training} -- process.
%, which is an optimization problem \cite{Mitchell1997-ux}.
Neural networks are a class of ML algorithms. A neural network implements a mathematical function \Fn that aims at approximating a continuous real-valued function \cite{Hornik1989, Schafer2006}. 
%The term network comes from the fact that
\Fn is  composed of  different mathematical functions called \emph{layers}.
%Each function composing \Fn is associated to a \emph{layer}.
 %The number of layers in a neural network defines its \emph{depth}.
%Deep neural networks are then neural networks with multiple hidden layers. 
%The number of units acting in parallel in each layer define the neural network \emph{topology}.
%The objective is that some parameters of function \Fn be recursively updated resulting in an accurate function approximation. This optimization process is called \emph{training}. 


There are two types of deep neural networks: feed-forward neural networks and recurrent neural networks. 
Recurrent neural networks (RNN) feature layers that take as input some of their output (or the output of a successor layer), thus creating cycles. In feed-forward variants it is not true. We are not addressing RNN in this paper.
A common representation of a feed-forward deep neural network (FDNN) is in the form of a \emph{directed acyclic graph} (DAG) defining how its layers are connected together.
%Then we associate a mathematical function \Fn to this directed graph, describing the mathematical rules of the neural network. 

\begin{definition}[Feed-forward Deep Neural Network] \label{def:dnn}
  \it
  A \emph{feed-forward deep neural network} $\mathcal{N}=(V,E)$ is a directed acyclic graph, wherein:
  \begin{itemize}
  \item $V$ is the finite set of vertices of the graph, which represent its layers $l \in V$;
  \item $E\subseteq V\times V$ is the set of edges of the graph, representing the data flow within the neural network.
  \end{itemize} 
\end{definition}

In order to construct the possible flows of data within the feed-forward deep neural network, it is necessary to define what are the predecessors and successors of a vertex, or layer. 

\begin{definition}[Predecessors / successors of a layer] \label{def:pre-layer}
  \it
The direct predecessors (resp. successors) of a layer $l$ are defined as the layers of the set 
$Pre(l) = \{l' \in V \mid (l', l) \in E\}$ (resp.
$Succ(l) = \{l' \in V \mid (l, l') \in E\}$).
The predecessor transitive closure of $l$ is the set of all its predecessors layers noted
 $\emph{Pre}^*(l)=\bigcup_{n=1}^{k} \emph{Pre}^n(l)$, wherein:
 
% \vspace{0.5cm}
    $\emph{Pre}^n(l) = 
\begin{cases}
    \emph{Pre}(l),& \text{if } n = 1\\
    \bigcup_{l' \in\emph{Pre}^{n-1}(l)} (\emph{Pre}(l')\medcup\{l'\}),              & \text{otherwise}
\end{cases}
$
\end{definition}

A layer can be classified into input, output and intermediate, or hidden.
An input layer $l$ only consumes input data, i.e., ${Pre(l) = \emptyset}$. Similarly, a final layer $l$ only produces output data, i.e., $Succ(l) = \emptyset$. We represent $V_I\subseteq V$ as the set of input layers and $V_O\subseteq V$ as the set of output layers. Note that $V_I\medcap V_O=\emptyset$. The remaining layers, $l\not \in V_I \medcup V_O$, are the hidden layers.
Finally, as a straight consequence of directed acyclic graphs properties, %we note that in a feed-forward neural network
$l\not \in \emph{Pre}^*(l)$.

% ADD BELOW figure? I do not have the PDF
% Figure \ref{fig:graph-dnn} illustrates a feed-forward neural network as described in Definition \ref{def:dnn}, where each layer (node) is represented within its associated function $f_l$. In this example we have $V_I=\{l_1,l_2\}$ and ${V_F=\{l_9, l_{10}, l_{11}\}}$. All the ancestors layers of layer $l_6$, for example, are defined by $Pre^*(l_6)=\{l_3, l_4, l_5, l_1\}$. We observe that skip connections are authorised, as in between layers $l_2$ and $l_7$, but there are no cycles, or feedback connections.   

% % Figure environment removed



The function performed by a layer is of the form $f_l:\mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$, wherein $m$ and $n$ represent respectively the input and output dimensions of the given layer's function. 
%The neural notation is linked to the notion that each layer itself is a collection of \emph{units}, wherein a unit is roughly inspired by a biological neuron. Thus, each unit implements a mathematical function as well. 
%
%The hidden layers can implement linear or non-linear functions.
%The existence of non-linearity in \Fn is an essential hypothesis in the theorem that claims that neural networks are universal approximators \cite{Hornik1989}.
\begin{definition}[Function associated to a FDNN]
  \label{def:func-DNN}
  \it
  Let $\mathcal{N}=(V,E)$ be a feed-forward deep neural network and $V_O\subseteq V$ be the set of output layers. Let us denote the function associated to a set of layers such that:
%F_{U} = (F_{l_{1}}, \ldots, F_{l_{n}}) \mid \forall j \in \{1, \ldots,n\}: l_{{j}} \in U \\
\begin{equation}
\forall U \subseteq V, F_{U} =
\begin{cases}
(F_{l_{1}}, \ldots, F_{l_{n}}), & \text{if } U = \{l_1, ...,l_n\} \\
 F_{ \emptyset},                & \text{if } U = \emptyset.
\end{cases}
\end{equation}
% \forall j :\ \{1, \ldots,n\}: l_{{j}} \in U \\
wherein:
\begin{equation}
	 \forall l \in V, \quad F_{l}=f_{l}(F_{Pre(l)})   
\end{equation}
\end{definition}


We note $\Fn = F_{V_O}$ the function associated to a feed-forward deep neural network.


\begin{example}[\emph{Single-path} feed-forward deep neural network]
  \label{ex-lenet}
  \it
  It corresponds to the particular case of a feed-forward deep neural network, wherein:
 $V = \{l_1, \ldots, l_n\}$ and  $\forall i\geq 2$, $\emph{Pre}(l_i)=\{l_{i-1}\}$.
Therefore, $V_I=\{l_1\}$, $V_F=\{l_n\}$. 
Such an example is the \lenet shown in figure \ref{fig:lenet}.

% Figure environment removed


According to Definition \ref{def:func-DNN} the function associated to a single-path DNN
is the composition function:
$  F_\mathcal{N}(x) = \circ f_{l_{n-1}} \circ \ldots \circ f_{l_1}(x)$
wherein $m_{l_1}$ in the input dimension of $f_{l_1}$ and $p_{l_n}$ is the output dimension of $f_{l_n}$. 
\end{example}

% \label{sub-sec-DNN}
% There are multiple ways to define DNNs:
% directed graphs, computational graphs
% or simply the mathematical functions transforming the input
% into the output.
% \nnef relies on a computation graph approach.
% In any case, the input of a neural network
% can be seen as a multi-dimensional vector also called \emph{tensor}.
% \begin{definition}[Tensor]
% A 3D-tensor
% $T$ is represented by its \emph{size} $(n_c, n_h, n_w) $ 
% where $n_c$ the number of channels (or feature maps), $n_h$ is the height and $n_w$ the width.
% We denote by $T_{x_1,x_2,x_3}$ the value of $T$ for the indices $x_1,x_2,x_3$.
% \end{definition}
% A feed-forward deep neural network consists of a series of layers executed without recursion.
% \begin{definition}[Feed-forward Deep Neural Network]
%   A \emph{feed-forward neural network} $N=< Q,V >$ is a directed graph where:
%   \begin{itemize}
% \item $Q=\{l_1, \ldots, l_n\}$  the nodes
%   are the \emph{layers} $l_i$.
%   A layer can be of  many types such as  an activation, a convolution or a pooling.
%   A layer comes with a set of parameters (e.g. weights or stride);
%   \item
%     $Q_i\subseteq Q$ is the set of input layers and
%     $Q_f\subseteq Q$ is  the set of final layers, in particular $Q_i\cap Q_f=\emptyset$;
% \item $V\subseteq Q\times Q$ the arrows represent the data flow within the neural network.
%   $\emph{Pre}(i)= \{j | (l_j,l_i) \in V\}$  is the set of layers $l_j$ such that
%   \emph{output}($l_j$) is an input of $l_i$.
%   An input layer $j\in Q_i$ only consumes input tensors,
%   i.e  $\emph{Pre}(j)=\emptyset$;.
%   \end{itemize}
%   We can compute the ancestors of a layer,
%   $\emph{Pre}^*(j)=\cup \emph{Pre}^n(j)$ where
%   %$\emph{Pre}^2(j)=\cup_{j_i\in\emph{Pre}(j)} \emph{Pre}(j_i)$
%   %and
%   $\emph{Pre}^n(j)=\cup_{j_i\in\emph{Pre}^{n-1}(j)} \emph{Pre}(j_i)$ and $\emph{Pre}^{1}(j)$ = $\emph{Pre}(j)$
  
%   Because the network is feed-forward, $j\not \in \emph{Pre}^*(j)$.
  
%  %%  A path in $N$ is defined by a succession of valid transitions starting from $l_1$ and ending in $l_n$:
% %%   $p:l_1 \longrightarrow l_{j_1} \longrightarrow l_{j_2} \ldots \longrightarrow l_{j_p} \longrightarrow l_n$
% %%   such that $(l_1,l_{j_1})\in V$, $\forall k, (l_{j_k},l_{j_{k+1}})\in V$ and $(l_{j_p},l_n)\in V$.
% %%   Because we consider forward NN, in all valid paths reachable from $l_1$, a node can be visited at most once,
% %% i.e.  $\forall k,i$, we have $l_1\not=l_{j_k}$,  $l_n\not=l_{j_k}$ and  $l_{j_i}\not=l_{j_k}$.
% \end{definition}

% \begin{example}
%   \label{ex-lenet}
%  The \lenet \cite{LeCunBDHHHJ89} is a usual CNN
%   developed for hand written digit images recognition.
%   Its structure (see figure  \ref{fig:lenet})
%   consists of 8 consecutive layers: convolution, pooling, flat and dense.
%   $Q_i=\{\emph{conv1}\}$, $Q_f=\{\emph{softmax}\}$,
%   and for all layers $i\geq 2$, $\emph{Pre}(i)=\{i-1\}$.

  
  
% % Figure environment removed

% The size of the input / output tensors are shown on the figure.
%    The first 2D-convolution \emph{conv1} which takes inputs of size $1\times 32\times 32$,
%    is composed of 6 kernels of size $1\times 5\times 5$
%    and a stride of $(1,1)$. Because the sizes of the input and the kernel are well balanced taking into account the stride, no padding is needed.
%    The activation function \emph{ReLu} is applied to the outputs.
%    The first pooling layer \emph{pool1} is a max pooling with stride $(2,2)$ and window $(2,2)$.
%    The second 2D-convolution \emph{conv2} is  composed of 16 kernels of size $6\times 5\times 5$
%    and of a stride $(1,1)$. The activation function \emph{ReLu} is applied to the outputs.
%    The second pooling layer \emph{pool2} is a max pooling with stride $(2,2)$ and window $(2,2)$.
%    The 3D-tensor of size  $16\times 5\times 5$   is flattened in a 1D-tensor of size $400$.
%    Three dense layers are then applied, each followed by a \emph{ReLu}.
%    A dense layer consists in applying a linear transformation
%    $W\cdot I + B$ where $W$ are the weights and $B$ the bias.
%    Finally, the post-processing is a \emph{softmax}.
% \end{example}

% \begin{remark}
%   In this article we are only focusing on the inference topic. Thus, whereas \nnef provides the possibility to express the batch size (number of inputs computing in one inference), we only consider a batch size of 1. The possibility to increase the batch is motivated by the learning phase to improve the optimization convergence (stochastic gradient).
% \end{remark}

% \begin{definition}[Function associated to a DNN]
%   \label{def-sem-DNN}
%   The function $f_N$ computed by a DNN
%  $N=< Q,V >$ 
%   is the composition of the functions computed by each layer. In case of a \emph{directed DNN} -- i.e. where $Q_i=\{l_1\}$, $Q_f=\{l_n\}$ and $\forall i\geq 2$, $\emph{Pre}(i)=\{i-1\}$ -- the function is simply given by
%   $f_N = f_{l_n} \circ \ldots \circ f_{l_1}$ where $\circ$ is the composition operator.
%   Otherwise,
%   $f_N = (f_{l_{f_1}}, \ldots, f_{l_{f_p}})$
%   where $Q_f=\{l_{f_1}, \ldots, l_{f_p}\}$ and 
%   the functions are defined as follows:
%   $\forall i$ such that $l_i\not\in Q_i$, $f_{l_i}(f_{l_{j_1}}, \ldots, f_{l_{j_n}})$ where $\{j_1,\ldots,j_n\} = \emph{Pre(i)}$.
%   The semantics of each elementary function (e.g. a 2D-convolution)
%   is given for instance in \nnef by providing its mathematical function.
% %  which can be used for an implementation.
% \end{definition}

\subsection{Neural Network Description in \nnef}
\label{sec-nn-nnef}
Our first goal concerns the definition of a format,
with a formal syntax and semantics, able to describe any ML model
such as the \lenet of example \ref{ex-lenet}.
Let us explain why \nnef fulfils this goal.
An \nnef description is composed of two parts:
1) A computation graph described in a human readable text file;
and 2) the parameters provided in multiple raw data file.
The fact that the description is textual is important for the traceability between the specification (output of the training framework) and the embedded code.

% #################################################
% Keep for the final paper
The computation graph file describes all parameters needed and operations to be done. More precisely, 
each line of the computation graph is an elementary instruction (\nnef \emph{compound fragment}) that may be split
into several atomic operations (\nnef \emph{primitives}).
The result of each instruction is stored in a named variable, that represents a tensor,
and which can be used as input for other instruction(s).
To compute an operation all its inputs variables shall be available.
%Such a description indeed defines a computation graph where nodes are operations and vertices variables.

\begin{remark}
  \nnef description follows a SSA (static single assignment) form \cite{cytron1989efficient}
  which helps the implementation process.
  It is usual to translate a program in its %associated
  SSA form before compilation or optimization passes \cite{BourkeBDLPR17}.
\end{remark}
% #################################################

\begin{example}
  \it
Let us illustrate how to specify a neural network with an example.
  The \nnef textual specification of the \lenet detailed in example \ref{ex-lenet}
  is given in the listing \ref{list-nnef-lenet}. First, all parameters are declared and stored as variables. \emph{e1} is the input tensor of size $1\times 32\times 32$,
  \emph{v1} contains the 6 kernels of size $1\times 5\times 5$ of the first convolution and
  \emph{v2} is the bias applied at the end of the  first convolution.
  The parameters needed by the layers should all be defined
  as variables in the description file. 

 % Figure environment removed

 % Keep for the final paper
 After the variables declaration, comes the computation graph itself.
 The output of the first convolution is stored in the variable \emph{o1}.
 When calling the function / compound fragment \emph{conv}, the user must instantiate
 the full set of parameters for this type of layer:
 input tensor, kernels, bias,  stride, dilation, padding and groups.
 Every parameter appears explicitly in the definition and there is no ambiguity.
 For instance,
 the way to declare the padding explicitly states how the padding applies on top / bottom / right / left.
 After the convolution, the activation function has to be explicitly applied to \emph{o1}
 and thus is not hidden in the convolution, ensuring again an unambiguous description.
 The first pooling layer results in variable \emph{o3}.
% and the parameters corresponds to those expressed in the example \ref{ex-lenet}.
 Reading the description, we recognize the \lenet detailed before. 
 The flat layer is encoded with a more expressive function \emph{reshape} that allows several reshaping.
 The \emph{dense} layer is called \emph{linear}. The post-processing \emph{softmax} is also explicit.
\end{example}

%From the previous syntactic description, the structure of the ML model can be easily described.
The \nnef specification also
provides the link between instructions (e.g. \emph{conv}) appearing in the file
and their associated mathematical functions.
Subsequently,
we will not use \nnef terms, because the \nnef standard uses the generic term \emph{fragment} for both. 
We illustrate this with the max pooling layer only.





\subsection{Max Pooling Layer Semantics}
\label{sec:maxpool}
Let us illustrate issues that may arise without a formal description.
Let us first remind  the functional semantics of a max pooling
layer where a padding (and no dilatation) is applied.
Thus, the function is defined by $\mathcal{P}\emph{ool}_{k,s} \circ \mathcal{P}_{p,v}$
where each function is defined below.
\begin{definition}[Padding associated function -- $\mathcal{P}_{p,v}$]
  \label{def-pad}
  \it
  Let  $p=(p_t,p_b,p_l,p_r)$ be a 4-tuple of integers representing the padding to be applied on each border of a 3D-tensor and $v$ the float value to be used for the padding.
  The padding function  $\mathcal{P}_{p,v}$ applied on a 3D-tensor $I$ of size $(n_h,n_w,n_c)$
  outputs a 3D-tensor $O = \mathcal{P}_{p,v}(I)$ of size $(o_h,o_w,o_c)$ with
  $o_h = n_h + p_t + p_b$, $o_w = n_w + p_l + p_r$ and $o_c = n_c$ such that\\
  $O_{x,y,z}=
    \begin{cases}
            v  & \textbf{if }( x \leq p_t)  \emph{ or } (x > n_h + p_t)  \emph{ or }\\
            &   \ \ \   (y \leq p_l)  \emph{ or } (y > n_w + p_l)\\
            I_{x-p_t,y-p_l,z}  &\textbf{otherwise}
    \end{cases}
  $
\end{definition}

\begin{definition}[Pooling layer associated function --  $\mathcal{P}\emph{ool}_{k,s}$]
  Let $s=(s_h,s_w)$ be the \emph{stride} parameters and
   $k=(k_h,k_w)$ be the height and width of the \emph{window}.
  The pooling applied on a 3D-tensor $I$ of size $(n_h,n_w,n_c)$
  outputs a 3D-tensor $O=\mathcal{P}\emph{ool}_{k,s}(I)$ of size $(o_h,o_w,o_c)$
  with $o_h = \left\lfloor \frac{n_h - k_h}{s_h} + 1\right\rfloor$,
  $o_w = \left\lfloor \frac{n_w - k_w}{s_w} + 1 \right\rfloor$ and
  $o_c = n_c$ with
  $O_{x,y,z} = \emph{max}(I[s_h \cdot (x-1)+1:s_h \cdot (x-1)+k_h+1][s_w \cdot (y-1)+1:s_w \cdot (y-1)+k_w+1][z])$.
  Here, $I[s_{11}:s_{21},...,s_{1k}:s_{2k}]$ represents the \emph{slice} of $I$
of all the values $I_{s_{11}+x_1,...,s_{1k}+x_k}$ with $i \in [1, k]$ and $x_i \in [1,s_{2i}-s_{1i}]$.
 \end{definition}

The \nnef syntax of a max pooling layer describes the padding values with an enumerate string. Ignoring the border for a max pooling layer is equivalent to pad with the minimum float value (\emph{MIN_F}). This corresponds to the neutral operator of the max function.
 Looking now at the \emph{max_pool} elementary instruction according to \nnef documentation \cite{nnefformat}, it is defined in the pseudo-code
  by 2 atomic operations. 
  \begin{enumerate*}
    \item \emph{argmax_pool} that computes an array of index (corresponding to the max in each pool);
    \item \emph{sample} that returns for an array of index, an array with corresponding values.
  \end{enumerate*}
  This pseudo-code indeed encodes the expected function $\mathcal{P}\emph{ool}_{k,s} \circ \mathcal{P}_{p,v}$.
  
 \textbf{Of the importance of unambiguous description.} 
  We propose to highlight the importance of making unambiguous textual descriptions of ML models
  by comparing two state-of-the-art training frameworks, namely \pytorch and \keras (with \tensorflow).
%  More specifically, we compare their
%  way of encoding max pooling.
%  A first important remark is that it is not possible to pad the \emph{borders} (top / bottom / left / right) with any combination.
%And let us try to implement it in the training framework.
\begin{itemize}
  \item In  \keras, a padding inside a max pooling layer can only be declared by a string $\in \{$"valid", "same"$\}$.
    "valid" means no elements to add,
    while "same" means that padding is added on the right and on the bottom borders only to fit  the size of the pool.

    \begin{lstlisting}[ %caption={\keras syntax for MaxPool},
      label=list-torch, language=Python]
  # KERAS SYNTAX for pool1
      MaxPool = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding = 'same')

  # NNEF for KERAS SYNTAX
      max_pool_keras = max_pool(input, size = [1, 1, 2, 2], stride = [1, 1, 2, 2], dilation = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 1), (0, 1)], border = 'ignore');
    \end{lstlisting}
This corresponds to
$  \mathcal{P}ool_{[2,2],[2,2]} \circ \mathcal{P}_{[0,1,0,1],\textrm{MIN\_F}}$.
\item In
  \pytorch, a padding inside a max pooling layer can only be declared by one or two integers. In case of one integer, this defines the number of elements to add to each border (top, bottom, left and right). In presence of 2 integers,
  the first  gives the number of elements to add at the top and  bottom, and the second at the left and right.
   \begin{lstlisting}[ %caption={\pytorch syntax for MaxPool},
      label=list-torch, language=Python]
  #PYTORCH SYNTAX for pool1
     MaxPool = NN.MaxPool2d(2,              2,       1)
                         # Kernel Size,  Stride, Padding

  #NNEF for PYTORCH SYNTAX
     max_pool_torch = max_pool(input, size = [1, 1, 2, 2], stride = [1, 1, 2, 2], dilation = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (1, 1), (1, 1)], border = 'ignore');
   \end{lstlisting}
 This corresponds to $\mathcal{P}ool_{[2,2],[2,2]} \circ \mathcal{P}_{[1,1,1,1],\textrm{MIN\_F}}$
\end{itemize}
The semantics of \keras and \pytorch are not equivalent,
and there is no possibility to convert one into another at once.
The only way to make a valid conversion is  to explicitly add a padding layer before the max pooling.
%This difference of semantic between the two frameworks leads to issues when we want to convert a model from one framework to the other.

\subsection{\nnef Execution Model Semantics}
\label{subsec-petrinet-sem}
The semantics of the \emph{execution model}, that is the formal behaviour behind a series of \emph{\nnef instructions},
is not explicitly given by the standard. It assumes that one instruction can be executed
when all its inputs are computed and available.
Thus, executing the instructions in sequence following
the order of the \nnef guarantees a correct execution.
There are two types of instructions: those reading parameters from binary files or input tensors
and  layer-associated instructions based on one or several atomic operations.
An instruction is always of the form
\[
 \emph{var} = \emph{operation} (v_1, \ldots, v_k, \emph{cst}_1,\ldots, \emph{cst}_j);
 \]
 where \emph{var} is a variable computed by the \emph{operation} (any \nnef \emph{fragment}),
 $v_i$ are either variables computed beforehand, the input tensor or fixed parameters (e.g. kernels), and
 \emph{cst}$_i$ are constant (e.g. stride).
 The \nnef execution model can be formally expressed using the \petri net formalism \cite{peterson1977petri}.
%We will detail the translation algorithm in the final paper. We however show hereafter the \petri net associated to the \lenet example.
 
\begin{translation}
  \label{trans-nnef-petri}
A \nnef description, composed of $n$ instructions, generates a \petri net $(P,T,V)$
where:
\begin{itemize}
\item the set of places $P$ corresponds to all variables appearing in the \nnef description
  (i.e. $n$ places for a description of $n$ instructions);
  \begin{itemize}
  \item a token in a place means that the associated variable is available for computation;
  \item initially there are as many tokens in each parameter-associated place as the parameter is needed in the instructions
    and as many tokens in the input tensor place as the input tensor is used by the instructions;
    \item there is a unique final place corresponding to the last variable computed in the \nnef file;
\end{itemize}
\item the set of transition names $T$ corresponds to all instructions appearing in the \nnef description;
\item $V\subseteq 2^P \times T \times \mathbb{N} \times P$ defines the set of transitions.
  A transition can be fired if there is a token in all input places.
  When it fires, the transition removes a token from each of these places and generates as many tokens as defined on the edge in the unique output place.
  \begin{itemize}
  \item each instruction $\emph{var} = \emph{op} (v_1, \ldots, v_k,\emph{cst}_1,\ldots, \emph{cst}_j)$ generates the transition given in figure \ref{fig:trans-instruction}
    where $p$ is the number of time \emph{var} will be consumed by other instructions;
    \item when only one token is produced by a transition, we omit the integer.
    \end{itemize}
  \end{itemize}
\end{translation}

% Figure environment removed

The semantic of the \petri net clarifies the execution order that is unclear in the \nnef formalism.
Especially, the order of the textual file should not impose a unique order when several valid ones may exist.
%For example, it is possible to read all weights and biases variables before the sequential execution of the layers but, these variables may also be read just before the execution of the associated convolution. 
\begin{example}
  \it
  The \lenet model  of figure \ref{fig:lenet} with its associated \nnef description in listing \ref{list-nnef-lenet} has the associated \petri net given in figure \ref{fig:sem}. We recognize the instructions sequence that describes the computation of the neural network graph.
  %All variables can be read before scheduling layers instructions (order of the \nnef file), but we can also imagine to schedule reading of variables just before scheduling the layer that is using these inputs.
  There is a unique possible schedule for this \nnef description,
  but we will see later other \nnef models that accept several schedules (see section \ref{sec-distribution}).
\end{example}

\begin{remark}
  The \petri net of figure \ref{fig:sem} only defines the semantics of a single inference pass. It is usual to repeat the inference pass
  in order to process new inputs (e.g. in a periodic manner).
  This can also be represented using the \petri formalism by sending back tokens to $e1$ and $v_k$ places.
  %For weights and biases variables, because they are static, we can just sending back a token after the execution of the associated instructions.
  For demonstration and clarity, subsequently in the paper, we always consider the semantics of a single inference.
\end{remark}


A way to define the semantics of \petri nets is to compute the set of reachable \emph{markings}, where a marking
 defines the number of tokens in each place at a given instant.
 \begin{definition}[Marking]
   \it
  Considering a \petri net with $n$ places, a marking is defined as a vector $v\in\mathbb{N}^n$
  giving the number of tokens $v[i]$ in the i-th place.
 This initial location of tokens is called the \emph{initial marking},
 this represents the starting state of the system.
 A \emph{final marking} is a marking such that there is one token
 only in each final place
 and from which no transition can be fired any more.
\end{definition}
 \begin{example}
   \it
  In the \petri net of figure \ref{fig:sem}, there are 24 states. There are 11 tokens in the initial making and there is a single token in the unique final marking
  (in the place \emph{out}).
\end{example}


\begin{property}[Initial marking and unique final marking]
  The unique initial marking is defined by the translation
  and consists of token(s) in the input tensor variable place and parameters-associated places.
  Because we consider feed-forward neural networks,
  there is unique \emph{final marking}.
\end{property}

\begin{definition}[Paths and semantics]
  \it
  A \emph{valid path} starts from the initial marking $m_i$,
  lists a series of fireable transitions 
  and ends in a final marking $m_f$, i.e.
    $m_i \longrightarrow^{t_1} m_1 \longrightarrow^{t_2} \ldots \longrightarrow^{t_l} m_f$.
  The semantics of a \petri net is the set of valid paths.
\end{definition}

\begin{property}[Possible executions of an \nnef description]
  Because we consider feed forward neural networks, the number of valid paths is finite and
  the valid paths correspond to all possible execution orders
  respecting the semantics of the ML model.
\end{property}
Semantics preserving code generation could lead to
any implementation the path of which is valid. Full sequential code following the order of instructions of the \nnef file is one of them.
