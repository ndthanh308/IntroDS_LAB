
%\clearpage
\section{AML use case}
\label{Experiment}

%In this section, we compare the performance of our HMPNN model to a rich set of existing GNNs using the AML-graph described in Section \ref{data}. 
In this section we test our HMPNN model on the AML scenario and data described in Section \ref{data}. In particular, we are interested in the ability of the method to predict the label (suspicious or not) on nodes where the label is unknown to the model.
To better understand how well suited our HMPNN model is for this use case, we compare its performance to a rich set of existing GNNs.

We start by describing the technical modeling regime and the benchmark methods. We then present the training/testing regime and the evaluation measures, before the results are presented and discussed.

\subsection{Technical Circumstances}
Our HMPNN model is implemented in Python, using the library PyTorch Geometric (PyG) \citep{Fey2019FastGR}. 
PyG is an open source python library built upon \textit{PyTorch}
\footnote{PyTorch is an open source deep learning library used for applications such as image analysis and natural language processing. It achieves high computational performance of tensor computing by utilizing graphics processing units (GPUs) through \textit{Compute Unified Device Architecture} (CUDA).}
\citep{NEURIPS2019_9015}, which specialises in  utilities for representing and training GNNs. 
PyG is the most popular library for working with GNNs, and most of the methods we benchmark against (see Section \ref{sec:benchmark_methods}) are also implemented in PyG, making it a convenient comparison framework for our experiment.

All modelling in this section were carried out in \textit{python 3.7.0}. 
The computer had 8 CPUs of the type \textit{High frequency Intel Xeon E5-2686 v4 (Broadwell) processors}, with 61GB shared memory, and one GPU of type \textit{NVIDIA Tesla V100} with 16GB memory. The GPU has 5,120 CUDA Cores and 640 Tensor Cores.


\subsection{Benchmark methods}
\label{sec:benchmark_methods}

We compare our HMPNN model against the following five methods: Basic 2-layer neural network with graph features,
Heterogeneous GraphSage,
HAN,
MAGNN, and % - Not done in thesis but would be interesting....
GINEConv. % It is homogeneous, but can utilize edge features. https://arxiv.org/abs/1905.12265 
% "Explicit Message-Passing Heterogeneous Graph Neural Network" Benchmarks against GCN, GAT, GIN, RGCN, HAN. I'm not sure how they apply the first four models to a heterogeneous graph, given that they don't allow message passing between nodes with different dimension of feature vectors. One approach could be to first map all feature vectors to a vector of the same length, and proceed in the same manner as done with graphsage in the thesis. 

The specifics of these methods are described below. 

\subsubsection{2-layer neural network with graph features}

To get a feeling of the performance improvement due to the use of methods utilizing the graph structure of the data, we include a non-GNN baseline method in our comparison. 
We used a purely entity-based two-layer neural network classifier, where the graph is only used upstream to create additional network features for each node which is added to its intrinsic entity level features. 

We created four types of network features: Degree summary, centrality metrics, DeepWalk embeddings
and Metapath2Vec embeddings. Combined, they are an attempt to incorporate high-quality features
on the node-level that summarize information about their role in the heterogeneous network. In
total, this results in 79 additional features. Combined with the entity features
we end up with 87 and 86 features for individuals and organisations, respectively. Precise specification of these network features are provided in \ref{app:network_features}.
This model was trained with PyTorch.

\subsubsection{GraphSage}
The second benchmark model is GraphSage. 
Because this message operator does not utilize the edge features, it does not initially have any information about the quantity of money that flows between the nodes. 
Therefore, there is reason to believe that the model will benefit from adding the additional degree features.

\subsubsection{HAN}
The (second/third ??) benchmark model is HAN. 
We used the implementation of HAN in PyG. 

Here, we set the hidden representation vector dimension to 8 for each embedding and for each node type in the hidden layers. We set the number of heads to 1. 
The resulting messages from neighboring nodes go through an additional SLP in each layer (with separate parameters for each node type) which outputs the new embeddings.

\subsubsection{MAGNN}


\subsubsection{GINEConv}



\subsection{Experiment setup}
The goal of this use case and benchmark is to compare the different methods ability to predict the label on nodes where the label is unknown to the model. To mimic such a scenario, we split the nodes into a training set and a test set. The whole graph is available for each of the models, but the labels are only available for the nodes in the training set. Then at testing time, the each method attempts to predict the (unknown) nodes in the test set, and their performance is compare using different performance measures.
In this experiment, we applied a 70-30 train-test split, where the splitting was performed using stratified random sampling with allocation proportional to the original class balance, such that the class balance is preserved in the two sets. The same split was used for all methods.

All models where trained with the Adam optimiser \citep{Kingma2015Adam}, using the Binary Cross Entropy
\[
\text{Loss}(\hat{y}_v, y_v) = y_v\cdot \log(\hat{y}_v) + (1-y_v)\cdot \log(1-\hat{y}_v).
\]
%

The hyperparameters were tuned using 5-fold cross-validation on the training set. Here, three hyperparameters were determined: (1) The regularization strength. (2) The learning rate. (3) The Number of training-iterations, i.e. by early stopping. 
For all experiments carried out, the learning rate was in the range $[10^{-4},10^{-1}]$. As for regularization, the $L_2$-constraint was used, and was in the range $[10^{-8},10^{-1}]$. The optimal value for the $L_2$-constraint was highly dependent on the complexity of the model to be trained.  


\subsection{Performance measures}

\subsection{Results}

\clearpage

\begin{table}[t]
    \centering
    \small
    \caption{Results logistic regression and neural network with 1 and 2 hidden layers. Here, 1 is logistic regression, and 2/3 are neural network with 1/2 hidden layers, respectively. Totally 98 node features, where 11 are intrinsic, 15 are summary of node-neighborhoods (counts number of different meta-steps), 8 are weighted summary of edges where the weight is the transaction-sums, and 64 are metapath2vec embeddings (8x8, dimension 8 for each of the 8 meta-steps an individual node is part of.)}
    \label{tab:...}
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{c}{Number}   & Degree   && \multicolumn{4}{c}{Recall(\%)}  &  \multicolumn{1}{c}{PR} & \multicolumn{1}{c}{ROC} \\ \cline{4-7}
\multicolumn{1}{c}{of Layers}  & Features&& 1  & 5  & 10 & 50 & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{AUC} \\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{1}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 61.54 & 36.28 & 28.55 & 5.53   &  \multirow{2}{*}{ 0.1075 } &  \multirow{2}{*}{ 0.8547 } \\
                                                                &                                           &Lift               & 135.96 & 80.16 & 63.07 & 12.23  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{2}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 64.00 & 43.82 & 30.51 & 5.89   &  \multirow{2}{*}{ 0.1173 } &  \multirow{2}{*}{ 0.8613 } \\
                                                                &                                           &Lift               & 141.40 & 96.82 & 67.41 & 13.01  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{3}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 61.54 & 47.56 & 34.68 &5.88   &  \multirow{2}{*}{ 0.1237} &  \multirow{2}{*}{ 0.8612 } \\
                                                                &                                           &Lift               & 135.96 & 105.08 & 76.61 & 13.00  &                           &\\ 
\hline
\end{tabular}
\end{table}


\begin{table}[t]
    \centering
    \small
    \caption{Results from graphSage-models }
    \label{tab:...}
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{c}{Number}   & Degree   && \multicolumn{4}{c}{Recall(\%)}  &  \multicolumn{1}{c}{PR} & \multicolumn{1}{c}{ROC} \\ \cline{4-7}
\multicolumn{1}{c}{of Layers}  & Features&& 1  & 5  & 10 & 50 & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{AUC} \\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{1}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 59.26 & 33.62 & 20.58 & 4.01   &  \multirow{2}{*}{ 0.0836 } &  \multirow{2}{*}{ 0.8329 } \\
                                                                &                                           &Lift               & 130.93 & 74.28 & 45.48 & 8.86  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{2}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 61.54 & 48.15 & 38.56 & 6.51   &  \multirow{2}{*}{ 0.1280 } &  \multirow{2}{*}{ 0.8879 } \\
                                                                &                                           &Lift               & 135.96 & 106.38 & 85.19 & 14.39  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{3}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 59.26 & 60.47 & 42.47 &7.19   &  \multirow{2}{*}{ 0.1452} &  \multirow{2}{*}{ 0.8960 } \\
                                                                &                                           &Lift               & 130.93 & 133.59 & 93.82 & 15.88  &                           &\\ 
\hline
\end{tabular}
\end{table}



\begin{table}[t]
    \centering
    \small
    \caption{Results from graphSage-models with additional node-features. The nodes representing Individual, LegalPerson and UnknownEntity have gotten 8, 8 and 6 extra node features, respectively. The features are the weighted in/out degrees for the transaction-edges, and the weight is the transaction-amount on the edges.}
    \label{tab:...}
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{c}{Number}   & Degree   && \multicolumn{4}{c}{Recall(\%)}  &  \multicolumn{1}{c}{PR} & \multicolumn{1}{c}{ROC} \\ \cline{4-7}
\multicolumn{1}{c}{of Layers}  & Features&& 1  & 5  & 10 & 50 & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{AUC} \\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{1}}  &\multicolumn{1}{c}{\multirow{2}{*}{Yes}}    & Precision(\%)     & 48.48 & 34.21 & 21.29 & 4.24   &  \multirow{2}{*}{ 0.0858 } &  \multirow{2}{*}{ 0.8405 } \\
                                                                &                                           &Lift               & 107.12 & 75.59 & 47.04 & 9.36  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{2}}  &\multicolumn{1}{c}{\multirow{2}{*}{Yes}}    & Precision(\%)     & 64.00 & 50.65 & 35.39 & 7.41   &  \multirow{2}{*}{ 0.1368 } &  \multirow{2}{*}{ 0.8882 } \\
                                                                &                                           &Lift               & 141.40 & 111.91 & 78.19 & 16.38  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{3}}  &\multicolumn{1}{c}{\multirow{2}{*}{Yes}}    & Precision(\%)     & 69.57 & 50.32 & 38.27 &7.68   &  \multirow{2}{*}{ 0.1424} &  \multirow{2}{*}{ 0.8915 } \\
                                                                &                                           &Lift               & 153.70 & 111.18 & 84.56 & 16.98  &                           &\\ 
\hline
\end{tabular}
\end{table}



\begin{table}[t]
    \centering
    \small
    \caption{Results from mpnn-models }
    \label{tab:...}
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{c}{Number}   & Degree   && \multicolumn{4}{c}{Recall(\%)}  &  \multicolumn{1}{c}{PR} & \multicolumn{1}{c}{ROC} \\ \cline{4-7}
\multicolumn{1}{c}{of Layers}  & Features&& 1  & 5  & 10 & 50 & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{AUC} \\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{1}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 64.00 & 38.24 & 29.75 & 5.34   &  \multirow{2}{*}{ 0.1090 } &  \multirow{2}{*}{ 0.8401 } \\
                                                                &                                           &Lift               & 141.40 & 84.48 & 65.73 & 11.80  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{2}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 69.57 & 42.62 & 36.38 & 7.76   &  \multirow{2}{*}{ 0.1359 } &  \multirow{2}{*}{ 0.8863 } \\
                                                                &                                           &Lift               & 153.70 & 94.17 & 80.39 & 17.14  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{3}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 84.21 & 53.79 & 38.27 &8.67   &  \multirow{2}{*}{ 0.1532} &  \multirow{2}{*}{ 0.8955 } \\
                                                                &                                           &Lift               & 186.06 & 118.85 & 84.56 & 19.16  &                           &\\ 
\hline
\end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \small
    \caption{Results from hmct-models }
    \label{tab:...}
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{c}{Number}   & Degree   && \multicolumn{4}{c}{Recall(\%)}  &  \multicolumn{1}{c}{PR} & \multicolumn{1}{c}{ROC} \\ \cline{4-7}
\multicolumn{1}{c}{of Layers}  & Features&& 1  & 5  & 10 & 50 & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{AUC} \\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{1}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 76.19 & 48.75 & 38.18 & 6.92   &  \multirow{2}{*}{ 0.1418 } &  \multirow{2}{*}{ 0.8801 } \\
                                                                &                                           &Lift               & 168.34 & 107.71 & 84.35 & 15.28  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{2}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 61.54 & 50.65 & 43.66 & 8.96   &  \multirow{2}{*}{ 0.1555 } &  \multirow{2}{*}{ 0.8989 } \\
                                                                &                                           &Lift               & 135.96 & 111.91 & 96.47 & 19.80  &                           &\\ 
\hline
\multicolumn{1}{c}{\multirow{2}{*}{3}}  &\multicolumn{1}{c}{\multirow{2}{*}{No}}    & Precision(\%)     & 66.67 & 58.21 & 50.99 &10.25   &  \multirow{2}{*}{ 0.1800} &  \multirow{2}{*}{ 0.9083 } \\
                                                                &                                           &Lift               & 147.29 & 128.61 & 112.65 & 22.65  &                           &\\ 
\hline
\end{tabular}
\end{table}


\clearpage
Number of model parameters for modelLogisticRegression: 99\\
Number of model parameters for modelNeuralNetwork\_1: 9,801\\
Number of model parameters for modelNeuralNetwork\_2: 19,503\\

Number of model parameters for modelSageConv1Layer: 189\\
Number of model parameters for modelSageConv2Layer: 3,999\\
Number of model parameters for modelSageConv3Layer: 7,809\\
Number of model parameters for modelNNConv1Layer: 296\\
Number of model parameters for modelNNConv2Layer: 6,536\\
Number of model parameters for modelNNConv3Layer: 12,776\\
Number of model parameters for modelHmct1Layer: 3,071\\
Number of model parameters for modelHmct2Layer: 4,487\\
Number of model parameters for modelHmct3Layer: 6,303\\

Number of model parameters for modelSageConv1Layer with additional node-features: 329\\
Number of model parameters for modelSageConv2Layer with additional node-features: 13,045\\
Number of model parameters for modelSageConv3Layer with additional node-features: 25,761\\
Number of model parameters for modelSageConv3Layer with additional node-features: 25,761\\
Number of model parameters for modelSageConv3Layer with additional node-features: 25,761\\