\begin{comment}
\subsection{Heterogeneous MPNN - OLD}
NOTES: \\
Expressing this using $v^{\nu}$ doesn't work, because one can't have a symbol with super/sub-script in a super/sub-script: \text{A\_A\_A} does not work!!!!!
\\
I don't think $s$ works... 
\\

Our heterogeneous formulation of the MPNN method can now be expressed.

The representation of all nodes are initialized as their feature vector:
\[
\b{h}_v^{(0)} = \b{x}_v^{\nu}, \; \forall \; \b{x}_v^{\nu} \in \b{X}^{\nu}, \nu \in Q^V.
\]

In each iteration $k = 1,\dots, K$, MPNN message passing will be performed for each meta-step that exists in the graph $s = (\mu, \varepsilon, \nu) \in S$.

For each meta-step $s = (\mu, \varepsilon, \nu) \in S$, we perform MPNN message passing for all nodes of type $\nu$:

\begin{align*}
    \b{m}_v^{(s,k)} &= \sum_{u\in N_s(v)} M_k^s(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}),\; \forall v \in V^{\nu} \\
    \b{h}_v^{(s,k)} &= U_k^s( \b{h}_v^{(k-1)}, \b{m}_v^{(s,k)})
\end{align*}

For each node $v$, aggregate the representations over the meta-steps $s \in S$:
\[
    \b{h}_v^{(k)} = \underset{s \in S}{\text{AGG}} ( \b{h}_v^{(s,k)} )
\]
\end{comment}

\begin{comment}
\subsubsection{MPNN}
\textbf{From \cite{WU2020}, but modified}: 

Message Passing Neural Network (MPNN) \citep{GILMER2017} outlines a general framework of spatial-based ConvGNNs. 
It treats graph convolutions as a message passing process in which information can be passed from one node to another along edges directly. MPNN runs $k$-step message passing iterations to let information propagate further. 
The message passing function is defined as
\begin{align*}
    \b{m}_v^{(k)} &= \sum_{u\in N(v)} M_k(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}), \\
    \b{h}_v^{(k)} &= U_k( \b{h}_v^{(k-1)}, \b{m}_v^{(k)})
\end{align*}
where $\b{h}_v^{0} = \b{x}_v$, $U_k(\cdot)$ and $M_k(\cdot)$ are functions with learnable parameters, and are called the \textit{node updated functions} and the \textit{message functions}, respectively.
After deriving the hidden representations of each node, $\b{h}_v^{(K)}$ can be passed to an output layer to perform node-level prediction tasks.
MPNN cover many existing GNNs by assuming different forms of $U_k(\cdot)$ and $M_k(\cdot)$, such as \cite{KIPF2016}.
\end{comment}

\begin{comment}
\begin{definition}(Adjacency matrix)
The adjacency matrix $\boldsymbol{A}$ of a graph $G = (V, E)$ is an $n\times n$ matrix with $\boldsymbol{A}_{ij}=1$ if $e_{ij}\in E$ and $\boldsymbol{A}_{ij} = 0$ if $e_{ij} \neq E$. 
\end{definition}

\begin{definition}(Node neighborhood (homogeneous))
\label{def:neighborhood}
The neighborhood of a node $v$ is defined as $N(v) = \{u \in V|(v,u) \in E\}$. 
\end{definition}
\end{comment}

\begin{comment}
\begin{definition}(Heterogeneous graph - OLD)
\label{def:heteroGraph}
A heterogeneous graph is represented as $G = (V, E, \boldsymbol{X}, \boldsymbol{R}, \phi, \psi)$ where $V$ is the set of nodes, and $E$ is the set of edges. 
Let $v\in V$ denote a node and $e_{uv}=(u,v)\in E$ an edge pointing from $v$ to $u$. 
Let $\phi : V \rightarrow Q^V$ represent a node type mapping function and $\psi : E \rightarrow Q^E$ an edge type mapping function.
Here, $Q^V$ and $Q^E$ denote finite sets of predefined node types and edge types, respectively.
Each node $v$ of type $\nu = \phi(v)$ is also associated with features $\boldsymbol{X}_v^{\nu}$, while each edge $e_{uv}$ of type $\varepsilon = \psi(e_{uv})$ with feature $\boldsymbol{R}_{uv}^{\varepsilon}$.\\
Note: This needs to change: $\psi(e_{uv})$ returns a subset of edge types in $Q^E$, which are are edge types that point from node $u$ to node $v$.
\end{definition}
\end{comment}

\begin{comment}
Another example with particular relevance for this paper is graphSage \citep{HAMILTON2017}. This paper considers a few different approaches, but one of them can be expressed as follows:
\begin{align*}
    M_k(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}) &= \b{W}_k \b{h}_u^{(k-1)}, \\
    U_k\big(\b{h}_v^{(k-1)}, \b{m}_v^{(k)}\big) &= \sigma\big(\b{m}_v^{(k)} + \b{B}_k \b{h}_v^{(k-1)} \big).
\end{align*}
 Here, $\sigma(\cdot)$ is the sigmoid function. 
\end{comment}


\begin{comment}
\subsection{Heterogeneous MPNN - OLD}

For a specific meta-step $s = (\mu, \varepsilon, \nu)$, all nodes $v\in V$ with type $\phi(v) = \nu$ will receive messages from the neighborhood that corresponds to the meta-step: $N_{\mu}^{\varepsilon}(v)$.
%
Each meta-step will have their own message function and node update function with parameters that will be learned for that specific meta-step.
We denote these functions $M_k^s(\cdot)$ and $U_k^s(\cdot)$ for iteration $k$ and meta-step $s$.
%
The message passing equation \eqref{eq_mpnn} can be expressed for a specific meta-step as follows:
\begin{align*}
    \b{m}_v^{(s,k)} &= \sum_{u\in N_{\mu}^{\varepsilon}(v)} M_k^s(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}),\;\forall\; v\in V, s = (\mu, \varepsilon, \phi(v)) \in S, \\
    \b{h}_v^{(s,k)} &= U_k^s( \b{h}_v^{(k-1)}, \b{m}_v^{(s,k)}).
\end{align*}

After the above equations have been performed for each meta-step at a certain iteration, a node $v$ will have multiple meta-step-specific representations $\b{h}_v^{(s,k)}$. These will be aggregated to produce the new representation vector $\b{h}_v^{(k)}$:
\[
    \b{h}_v^{(k)} = A\big( 
    \{
    \b{h}_v^{(s,k)} | s = (\mu, \varepsilon, \phi(v)) \in S
    \}  
    \big)
\]
\end{comment}


\begin{comment}
Our study has two contributions:
\begin{enumerate}
    \item Exploring the potential of applying supervised learning methods on a heterogeneous transaction-network to detect money laundering activities. 
    The dataset is realistic and created from real-world data, which makes the results representative to the real application. 
    \item Exploring various graph neural network architectures on a large real-world heterogeneous network, where the task is node classification. We expand upon methods and architectures in the literature, and construct a model especially suitable for the presented data. 
\end{enumerate}
\end{comment}


\begin{comment}
\clearpage
\begin{algorithm}
\caption{Heterogeneous MPNN }\label{alg:HMPNN}
\begin{algorithmic}
\State Initialize the representation of each node as its feature vector,
\[
\b{h}_v^{(0)} = \b{x}_v^{\nu}, \;\forall\; \b{x}_v^{\nu} \in \b{X}^{\nu}, \nu \in Q^V.
\]

\For{$k = 1,\dots, K$} \Comment{For each iteration}
\For{$\nu \in Q^V$} \Comment{For each node type in the graph}
\For{$v \in V,\; \phi(v) = \nu $} \Comment{For each node with node type $\nu$}
\For{$s = (\mu, \varepsilon, \nu)\in S$} \Comment{For each meta-step ending at $\nu$}

\State Compute the new representation for node $v$ for the specific
\State meta-step,
\[
\begin{aligned}
    \b{m}_v^{(s,k)} &= \sum_{u\in N_{\mu}^{\varepsilon}(v)} M_k^s(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}), \\
    \b{h}_v^{(s,k)} &= U_k^s( \b{h}_v^{(k-1)}, \b{m}_v^{(s,k)}).
\end{aligned}
\]
\EndFor
\State Aggregate the representations from the multiple meta-steps into a 
\State single new representation for that node,
\State
\[
    \b{h}_v^{(k)} = A_k^{\nu}\big( 
    \{
    \b{h}_v^{(s,k)} \mid s = (\mu, \varepsilon, \phi(v)) \in S
    \}  
    \big)
\]
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
\clearpage
\end{comment}



\begin{comment}
\clearpage
\begin{algorithm}
\caption{Heterogeneous MPNN II }\label{alg:HMPNN}
\footnotesize
\begin{algorithmic}
\State Initialize the representation of each node as its feature vector,
\[
\b{h}_v^{(0)} = \b{x}_v^{\nu}, \;\forall\; \b{x}_v^{\nu} \in \b{X}^{\nu}, \nu \in Q^V.
\]

\For{$k = 1,\dots, K$} \Comment{For each iteration}
\For{$\nu \in Q^V$} \Comment{For each node type in the graph}
\For{$v \in V$, such that $\phi(v) = \nu $} \Comment{For each node with node type $\nu$}
\For{$\mu \in Q^V, \varepsilon \in Q^E$ such that $N_{\mu}^{\varepsilon}(v) \neq \emptyset$} \Comment{For each meta-step ending at $\nu$}

\State Compute the new representation for node $v$ for the specific
\State meta-step,
\[
\begin{aligned}
    \b{m}_v^{(\mu, \varepsilon,k)} &= \sum_{u\in N_{\mu}^{\varepsilon}(v)} M_k^{\mu, \varepsilon}(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}), \\
    \b{h}_v^{(\mu, \varepsilon,k)} &= U_k^{\mu, \varepsilon}( \b{h}_v^{(k-1)}, \b{m}_v^{(\mu, \varepsilon,k)}).
\end{aligned}
\]
\EndFor
\State Aggregate the representations from the multiple meta-steps into a 
\State single new representation for that node,
\State
\[
    \b{h}_v^{(k)} = A_k^{\nu}\big( 
    \{
    \b{h}_v^{(\mu, \varepsilon,k)} \mid \mu \in Q^V, \varepsilon \in Q^E
    \}  
    \big)
\]
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
\end{comment}

\begin{comment}
\clearpage
\begin{algorithm}
\caption{Heterogeneous MPNN II MJ}\label{alg:HMPNN}
\footnotesize
\begin{algorithmic}
\State Initialize the representation of each node as its feature vector,
\[
\b{h}_v^{(0)} = \b{x}_v^{\nu}, \;\forall\; \b{x}_v^{\nu} \in \b{X}^{\nu}, \nu \in Q^V.
\]

\For{$k = 1,\dots, K$} \Comment{For each iteration}
\For{$v \in V$} \Comment{For each node}
\For{$\mu \in Q^V, \varepsilon \in Q^E$ such that $N_{\mu}^{\varepsilon}(v) \neq \emptyset$} \Comment{For each meta-step ending at $\phi(v)$}

\State Compute the new representation for node $v$ for the specific
\State meta-step,
\[
\begin{aligned}
    \b{m}_v^{(\mu, \varepsilon,k)} &= \sum_{u\in N_{\mu}^{\varepsilon}(v)} M_k^{\mu, \varepsilon}(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}), \\
    \b{h}_v^{(\mu, \varepsilon,k)} &= U_k^{\mu, \varepsilon}( \b{h}_v^{(k-1)}, \b{m}_v^{(\mu, \varepsilon,k)}).
\end{aligned}
\]
\EndFor
\State Aggregate the representations from the multiple meta-steps into a 
\State single new representation for that node,
\State using the aggregation function, $A_k^{\nu}$ of node type $\nu=\phi(v)$
\State
\[
    \b{h}_v^{(k)} = A_k^{\nu}\big( 
    \{
    \b{h}_v^{(\mu, \varepsilon,k)} \mid \mu \in Q^V, \varepsilon \in Q^E
    \}  
    \big)
\]
\EndFor

\EndFor
\end{algorithmic}
\end{algorithm}
\clearpage
\end{comment}





\begin{comment}
\clearpage
Using this framework, multiple different GNNs can be defined by different choices of  $U_k(\cdot)$ and $M_k(\cdot)$.
For example, GCN \citep{KIPF2016} can be expressed as 
\begin{align*}
M_k(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}) &=\frac{\b{r}_{uv}}{\sqrt{\hat{d}_u\hat{d}_v}}\b{h}_u^{(k-1)},\\   
U_k(\b{h}_v^{(k)}, \b{m}_v^{(k+1)}) &=\text{ReLU}(W_k \b{m}_v^{(k+1)}),
\end{align*}
where $\hat{d}_v = 1 + \sum_{u\in V}\b{r}_{uv}$. Note that $\b{r}_{uv}$ is here restricted to being a scalar rather than a vector, resulting from that GCN is defined on (possibly) weighted graphs, but not on graphs with multiple edge features\footnote{From \citep{KIPF2016}: "Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted or unweighted)."}.

\cite{GILMER2017} use 
$U_t = \text{GRU}(h_v^t, m_v^{t+1})$
as node update function, where $\text{GRU()}$ is the gated recurrent unit from \cite{CHO2014}. $\text{GRU()}$ is also used in \textit{Gated Graph Neural Networks} \citep{LI2016}.
%
As for the message function, they experiment with a few alternatives, one of which being 
\[
    M_k(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}) = g_k(\b{r}_{uv}) \b{h}_u^{(k-1)}.
\]
Here, $g_k(\cdot)$ is a neural network which maps $\b{r}_{uv}$ to a $d\times d$ matrix, where $d$ is the dimension of the representation vectors. 
From the viewpoint of the present work, this message passing function is essential, because it defines a learned message function that utilize edge features. 
Many other proposed GNNs such as GCN \citep{KIPF2016}, graphSage \citep{HAMILTON2017}, GAT \citep{VELIVCKOVIC2017}, RGCN \citep{SCHLICHTKRULL2018} and HAN \citep{WANG2019} do not incorporate edge features, and they therefore falls short on our graph. 
\clearpage
\end{comment}


\begin{comment}
However, these definitions do not allow multiple edges with the same direction between the same two nodes. We require a definition that incorporates a directed graph with multiple node types and multiple edge types, which also can have multiple edges between the same two nodes as long as the edges have different types. In addition, both nodes and edges have features which depends on their types. Here is our definition:
\end{comment}





