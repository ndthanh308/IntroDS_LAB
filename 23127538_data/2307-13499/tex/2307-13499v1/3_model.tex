\section{Model}
\label{sec:model}
In this section we define and describe our proposed heterogeneous GNN model, which is based on the generic framework from Message Passing Neural Network (MPNN) introduced in \cite{GILMER2017}.
We start by introducing the original (homogeneous) MPNN model and algorithm before we move on to give precise definitions for our heterogeneous graph setup and present our novel extension of the MPNN model for heterogeneous networks.

\subsection{Message Passing Neural Network (MPNN)}
\label{sec:MPNN}

\cite{GILMER2017} introduces the generic MPNN framework which is able to express a large group of different GNN models, including GCN, GraphSage, and GAT. 
This is done by formulating the message passing with two learned functions, $M_k(\cdot)$, called the \textit{message functions}, and $U_k(\cdot)$, called the \textit{node updated functions}, with forms to be specified later.
The framework runs $K$ message passing iterations $k=1,\ldots,K$ between nodes along the edges that connect them. 
The node representation vectors are initialized as their feature vectors, $\b{h}_v^{0} = \b{x}_v$, and the previous representation $\b{h}_v^{(k-1)}$ is the message that is being sent in each iteration $k$.
After $K$ iterations, the final representation $\b{h}_v^{(K)}$ is passed on to an output layer to perform e.g.~node-level prediction tasks.
The message-passing function is defined as 
\begin{equation}\label{eq_mpnn}
\begin{aligned}
    \b{m}_v^{(k)} &= \sum_{u\in N(v)} M_k(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}), \\
    \b{h}_v^{(k)} &= U_k( \b{h}_v^{(k-1)}, \b{m}_v^{(k)}),
\end{aligned}
\end{equation}
where $N(v)$ denotes the neighborhood of node $v$, and $\b{r}_{uv}$ represents the edge features for the edge between node $u$ and $v$.
By defining specific forms of $U_k(\cdot)$ and $M_k(\cdot)$, a distinct GNN method is formulated. 
From the viewpoint of this paper, an essential attribute of the MPNN framework is that the learned message-passing function utilizes edge features.
Not many other proposed GNNs incorporate edge features into their model. 
\cite{GILMER2017} emphasize the importance of edge features in the dataset they experiment on. 
For our dataset, the edge features contain essential information related to transactions and are required to be utilized in an expressive model. 



\subsection{Formal definitions}

Before we can formulate our \textit{heterogeneous} MPNN framework, we need to establish a precise definition of a heterogeneous graph, as well as a couple of additional concepts. 

We largely adopt the commonly used graph notation from \cite{WU2020}, 
and use a heterogeneous graph definition which is a slight modification to those in \cite{YANG2020} and \cite{WANG2019} in order to allow for multiple edges of different types between the same two nodes.
% A list of the most important quantities are provided in Table \ref{tbl:notation}.

% \begin{table}[!ht]\label{tbl:notation}
%     \centering
%     \footnotesize
%     \caption{Graph notation}
%     \begin{tabular}{lp{5.5cm}}
%         \hline
%         Notation & Description \\
%         \hline
%         $n$ & The number of nodes. \\
%         $m$ & The number of edges.\\
%         $d$  & The dimension of a node feature vector. \\
%         $c$ & The dimension of an edge feature vector. \\
%         \hline
%         $G$ & A (heterogeneous) graph. \\
%         $\boldsymbol{A}$ & The adjacency matrix of a graph. \\
%         \hline
%         $V$ & The set of nodes in a graph. \\
%         $u, v$ & A node $u,v\in V$.\\
%         $\boldsymbol{X}\in \mathbb{R}^{n\times d}$ & The node feature matrix of a graph.\\
%         $E$ & The set of edges in a graph. \\
%         $e \in E$ & An edge in the graph. \\
%         $e_{ij} \in E$ & An edge from node $v_i$ to $v_j$. \\
%         $e_{uv} \in E$ & An edge from node $u$ to node $v$. \\
%         $\boldsymbol{R}\in \mathbb{R}^{m\times c}$ & The edge feature matrix of a graph.\\
%         $N(v) = $ & The neighbors of a node $v$. \\ 
%         $\{v \in V \mid (v,u) \in E\}$ & \\
%         \hline
%         $\boldsymbol{W}, \boldsymbol{\Theta}, w, \theta$ & Learnable model parameters. \\
%         \hline
%         $M(\cdot)$& Message function. \\
%         $U(\cdot)$& Node update function. \\
%         $A(\cdot)$& Aggregation function. \\
%         $\b{h}_v$& Representation of node $v$.\\
%         $\b{m}_v$& Aggregated messages received by node $v$ from its neighborhood.\\
%         $k$ & Iteration index. \\
%         \hline
%         $Q^V$ & The set of node types in a graph.\\
%         $Q^E$ & The set of edge types in a graph.\\
%         $\phi(v)$ & The node type mapping function $\phi : V \rightarrow Q^V$. \\
%         $\psi(e)$ & The edge type mapping function $\psi : E \rightarrow Q^E$. \\
%          $\nu, \mu \in Q^V$ & A specific node type. \\
%          $\varepsilon \in Q^E$ & A specific  edge type. \\
%         $S$ & The set of meta-steps in the graph. \\
%         $s = \{(\mu,\varepsilon,\nu) \in S\}$ & A specific meta-step.\\
%         \hline
%     \end{tabular}
%     \label{tab:notation}
% \end{table}

\begin{definition}(Heterogeneous graph)
\label{def:heteroGraph}
A heterogeneous graph is represented as $G = (V, E, \boldsymbol{X}, \boldsymbol{R}, Q^V, Q^E, \phi)$ where each node $v\in V$ and each edge $e\in E$ has a type, and $Q^V$ and $Q^E$ denote finite sets of predefined node types and edge types, respectively.
%
Each node $v\in V$ has a node type $\phi(v) = \nu \in Q^V$, where $\phi(\cdot)$ is a node type mapping function. 
%
Further, for $\phi(v) = \nu$, $v$ has features $\b{x}_v^{\nu} \in \b{X}^{\nu}$, 
where 
$\b{X}^{\nu} = \{ \b{x}_v^{\nu}\mid v \in V,\, \phi(v) = \nu \}$ and
$\b{X} = \{\b{X}^{\nu} \mid \nu \in Q^V\}$.
The dimension and specifications of the node feature $\b{x}_v^{\nu}$ may be different for different node types $\nu$.
%
Further, let us denote by $e_{uv}^{\varepsilon}$ an edge of type $\varepsilon \in Q^E$ pointing from node $u$ to $v$. 
%There may exist multiple edges from node $u$ to $v$, but no more than one of each edge type. 
Each edge $e_{uv}^{\varepsilon}$ has features 
$\b{r}_{uv}^{\varepsilon} \in \b{R}^{\varepsilon}$, where 
$\b{R}^{\varepsilon} = \{\b{r}_{uv}^{\varepsilon} \mid u,v\in V \}$ and
$\b{R} = \{ \b{R}^{\varepsilon} \mid  \varepsilon \in Q^E\}$. 
Just like for nodes, the edge features may have different dimensions for different edge types.
\end{definition}

To formulate heterogeneous message passing, we will use the concept of \textit{meta-paths}. Meta-paths are commonly used to extend methods from a homogeneous to a heterogeneous graph. For example, \cite{DONG2017} use meta-paths when introducing \textit{metapath2vec}, which extends \textit{DeepWalk} \citep{PEROZZI2014} and the closely related \textit{node2vec} \citep{GROVER2016} to a method applicable on heterogeneous graphs.
\cite{WANG2019} use meta-paths to generalize the approach of graph attention networks \citep{VELIVCKOVIC2017} to that of heterogeneous graphs when formulating \textit{heterogeneous graph attention network} (HAN).
%
In addition to meta-path, the below definition introduces our own term, \textit{meta-steps}, which we use when formulating our model. 

\begin{definition}(Meta-path, meta-step)
\label{def:meta_path_step}
A \textit{Meta-path} belonging to a heterogeneous graph $G$ is a sequence of specific edge types between specific node types, 
\[
    (\nu_0, \varepsilon_1, \nu_1, \varepsilon_2, \dots,\nu_{k-1}, \varepsilon_k, \nu_{k}), \quad \nu_i \in Q^V , \varepsilon_i \in Q^E.
\]
Here, $k$ is the length of the meta-path.
%We say that the meta-path defines a \textit{composite relationship} between $\nu_0$ and $\nu_{k}$.
%To ease the readability, we will express a meta-path as follows,
%\[
%    \mathlarger{
%    \nu_0
%    \overset{\varepsilon_1}{\xrightarrow{\hspace*{0.5cm}}} 
%    \nu_1
%    \overset{\varepsilon_2}{\xrightarrow{\hspace*{0.5cm}}} 
%    \dots
%    \overset{\varepsilon_k}{\xrightarrow{\hspace*{0.5cm}}} 
%    \nu_{k}.
%    }
%\]
Let $S$ be the set of meta-paths of length 1, 
\[
    S = \{ s=(\mu,\varepsilon,\nu) \mid  \mu,\nu \in Q^V, \varepsilon \in Q^E \},
\]
and refer to the elements $s\in S$ as \textit{meta-steps}.
\end{definition}

Finally, we introduce a definition of node neighborhood over a specific meta-step:
\begin{definition}(Meta-step specific node neighborhood)
Let $N_{\mu}^{\varepsilon}(v)$ be the set of nodes of type $\mu$ which is connected to node $v$ by an edge of type $\varepsilon$ pointing to $v$:
\[
N_{\mu}^{\varepsilon}(v) = 
\{ u\in V \mid \phi(u) = \mu, e_{uv}^{\varepsilon}\in E   \}.
\]
We call $N_{\mu}^{\varepsilon}(v)$ the (incoming) node neighborhood to node $v$ with respect to the meta-step $s = (\mu, \varepsilon, \phi(v)) \in S$.
\end{definition}


\subsection{Heterogeneous MPNN}
\label{sec:HMPNN}
We are now ready to formulate our heterogeneous version of the MPNN method (HMPNN). 
The complete algorithm is provided in Algorithm \ref{alg:HMPNN}.

Our approach for extending a homogeneous GNN to a heterogeneous one is, essentially, the same as used by \cite{SCHLICHTKRULL2018}, where they generalize GCN to the method \textit{Relational Graph Convolutional Network} (RGCN) applicable on graphs with multiple edge types. 

The algorithm performs (at each iteration) multiple MPNN message passing operations, one for each meta-step $s \in S$ in the graph. Each of these has its separate learned functions $M_k^s(\cdot) = M_k^{(\mu,\varepsilon,\nu)}(\cdot)$ and $U_k^s(\cdot) = U_k^{(\mu,\varepsilon,\nu)}(\cdot)$, which allows the method to learn the context of each meta-step, and also allow message passing between nodes and across edges with varying numbers of features. 
%
The intermediate output of this process is multiple representation vectors for each node. To reduce these to a single vector, they are aggregated by a learned \text{aggregation function} $A^{\nu}(\cdot)$ which is specific to each node type. In line with the generic formulation of MPNN we do not specify a specific form of the aggregation function in Algorithm \ref{alg:HMPNN}. During the experiments, we have assessed two alternative options, which we will discuss shortly.

Our HMPNN model is implemented in Python, using the library PyTorch Geometric (PyG) \citep{Fey2019FastGR} allowing for high-performance computing by utilizing massive parallelization through GPUs. 
Source code is available here: \url{https://github.com/fredjo89/heterogeneous-mpnn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
\begin{algorithm}[ht!]
\caption{Heterogeneous MPNN}\label{alg:HMPNN}
\footnotesize
\begin{algorithmic}
\State Initialize the representation of each node as its feature vector,
\[
\b{h}_v^{(0)} = \b{x}_v^{\nu}, \;\forall\; v \in V \text{ of type } \nu = \phi(v).
\]

\For{$k = 1,\dots, K$} \Comment{For each iteration}
\For{$v \in V$} \Comment{For each node}
\For{$\mu \in Q^V, \varepsilon \in Q^E$ such that $N_{\mu}^{\varepsilon}(v) \neq \emptyset$} \Comment{For each meta-step ending at $\phi(v)$}

\State Compute the new representation for node $v$ of type $\nu=\phi(v)$, for the specific
\State meta-step,
\[
\begin{aligned}
    \b{m}_v^{(\mu, \varepsilon,k)} &= \sum_{u\in N_{\mu}^{\varepsilon}(v)} M_k^{(\mu, \varepsilon, \nu)}(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}), \\
    \b{h}_v^{(\mu, \varepsilon,k)} &= U_k^{(\mu, \varepsilon, \nu)}( \b{h}_v^{(k-1)}, \b{m}_v^{(\mu, \varepsilon,k)}).
\end{aligned}
\]
\EndFor
\State Aggregate the representations from the multiple meta-steps into a 
\State single new representation for that node,
\State
\[
    \b{h}_v^{(k)} = A_k^{(\nu)}\big( 
    \{
    \b{h}_v^{(\mu, \varepsilon,k)} \mid \mu \in Q^V, \varepsilon \in Q^E
    \}  
    \big)
\]
\EndFor

\EndFor
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Choosing specific forms of the functions}

As our setup is very general, the algorithm in Algorithm \ref{alg:HMPNN} gives rise to a whole range of new heterogeneous GNN methods. 
By defining specific forms of $U_k^{(\mu, \varepsilon, \nu)}(\cdot)$, $M_k^{(\mu, \varepsilon, \nu)}(\cdot)$ and $A_k^{(\nu)}(\cdot)$, a distinct GNN method is formulated. 
Note that there is nothing that prevents us from choosing different forms of these functions for different meta-steps or iterations. 
However, for the AML use case in Section \ref{sec:usecase}  we have limited the scope to a single form for each of the three functions, respectively. These are described in the following.

As message function, we use the same as \cite{GILMER2017}:
\[
M_k^{(\mu, \varepsilon, \nu)}(\b{h}_v^{(k-1)}, \b{h}_u^{(k-1)}, \b{r}_{uv}^{\varepsilon}) = g_k^{(\mu, \varepsilon, \nu)}(\b{r}_{uv}^{\varepsilon}) \b{h}_u^{(k-1)}.
\]
Here, $g_k^{(\mu, \varepsilon, \nu)}(\cdot)$ is a single layer neural network which maps the edge feature vector $\b{r}_{uv}^{\varepsilon}$ to a $d^v\times d^u$ matrix, where $d^u$, and $d^v$ are the number of features for the sending and receiving node type, respectively. 

As update function we use 
\[
U_k^{(\mu, \varepsilon, \nu)}\big(\b{h}_v^{(k-1)}, \b{m}_v^{(\mu, \varepsilon, \nu,k)}\big) = \sigma\Big(\b{m}_v^{(\mu, \varepsilon,k)} + \b{B}_k^{(\mu, \varepsilon, \nu)} \b{h}_v^{(k-1)}\Big),
\]
where $\b{B}_k^{(\mu, \varepsilon, \nu)}$ is a matrix.
Note that for a homogeneous graph, our choices of $M(\cdot)$ and $U(\cdot)$ are similar to those in \cite{HAMILTON2017}, except that the matrix applied in the message function is conditioned on the edge features rather than being the same across all edges. 

For the aggregation function, we consider two alternatives. The first is to take the sum of the vectors from each meta-step before performing a nonlinear transformation, in the same fashion as \citep{SCHLICHTKRULL2018}:
\begin{equation}\label{eq_mpnn_agg_sum}
A_k^{(\nu)}\big( 
    \{
    \b{h}_v^{(\mu, \varepsilon,k)} \mid \mu \in Q^V, \varepsilon \in Q^E
    \}  
    \big)
=
\sigma\bigg(
\underset{\mu \in Q^V, \varepsilon \in Q^E}{\sum} 
\b{h}_v^{(\mu, \varepsilon,k)}
\bigg).
\end{equation}
Here, $\sigma(\cdot)$ is the sigmoid function.
In the second aggregation method, the vectors $\b{h}_v^{(\mu,\varepsilon,k)}$ 
%for meta-steps $s=(\mu,\varepsilon,\phi(v))$ with node type $\phi(v)$ at the end-point, 
are concatenated into a single vector. A single-layer perceptron (neural network) is then applied to it, and outputs the new representation:
\begin{equation}\label{eq_mpnn_agg_ct}
A_k^{(\nu)}\big( 
    \{
    \b{h}_v^{(\mu, \varepsilon,k)} \mid \mu \in Q^V, \varepsilon \in Q^E
    \}  
    \big)
=
\sigma\Big(
W_k^{(\nu)}
\underset{\mu \in Q^V, \varepsilon \in Q^E}{||} 
\sigma \big(\b{h}_v^{(\mu, \varepsilon,k)}\big)
\Big).
\end{equation}
We denote the two resulting models \textit{HMPNN-sum} and \textit{HMPNN-ct}, respectively.
Figure \ref{fig:MPNN-architecture} illustrates the architectural extension of the homogeneous MPNN model to our HMPNN model (HMPNN-ct) as messages are passed to one of the node types, where \eqref{eq_mpnn_agg_ct} is used as aggregation function. 

% Figure environment removed