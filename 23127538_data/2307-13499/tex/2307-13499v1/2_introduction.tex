\section{Introduction}
\label{introduction}

Money laundering is the activity of securing proceeds of a criminal act by concealing where the proceeds come from. The ultimate goal is to make it look like the proceeds originated from legitimate sources. 
Money laundering is a vast global problem, as it enables all types of crime where the goal is to make a profit. 
%Therefore, fighting money laundering is in the interest of the global society.  
Because most money laundering goes undetected, it is difficult to quantify its effect on the global economy. 
However, a research report by \cite{UNODC} estimates that 1-2 trillion US dollars are being laundered each year, which corresponds to 2-5\% of global gross domestic product.
Both national and international anti-money laundering (AML) laws regulate electronic surveillance and reporting of suspicious transaction activities in financial institutions.
The purpose of the surveillance is to detect \textit{suspicious activities} with a high probability of being related to money laundering, such that they can be manually investigated.
The manual investigation is performed by experienced investigators, which inspect several aspects of the case, often involving multiple implicated customers, to then decide whether the behavior is suspicious enough to be reported to the authorities.
See Section \ref{sec:background_related_work} for more details about this process.
The present paper concerns the electronic surveillance process, which ought to identify a relatively small number of suspicious activities in a vast ocean of legitimate ones.

% Traditionally rule based etc
In the past few decades, the electronic surveillance systems in banks have typically consisted of several simple rules, created by domain experts, that use fixed thresholds and a moderate number of \texttt{if/else} statements that determine whether an alert is generated.
Such rules are still in large parts what makes up the surveillance systems \citep{CHEN2018}.  
However, these rules fall short of providing efficiency \citep{REUTERS}, for at least three key reasons: 
a) They rely on manual work to create and keep the rules up to date with the dynamically changing data \citep{CHEN2018}. This work increases with the complexity and number of rules. 
b) They are typically too simple to be able to detect money laundering with high precision, resulting in many low-quality alerts, i.e. \textit{false positives}\footnote{Hiring more workers to inspect each alert manually is often the resolution to compensate for this shortcoming.}.
c) Their simplicity makes them easy to circumvent for sophisticated money launderers, resulting in the possibility that severe crimes go undetected. 
The consequence of these three shortcomings is that banks spend huge resources on a very inefficient approach, and only a tiny fraction of illegal proceeds is being recovered \citep{POL2020}. 

Driven by multiple money laundering scandals in recent years\footnote{
As an example, it was revealed in 2018 that the Estonian branch of Danske Bank, Denmark's largest bank, carried out suspicious transactions for over \euro200 billion during 2007-2015 \citep{BJERREGAARD2019}. 
Subsequent lawsuit claims have amounted to over \euro2 billion.
%This resulted in lawsuits with total claims amounting to \$2 billion.
%One of the largest money laundering scandals in modern history involves Danske Bank, Denmark's largest bank.
%In 2018, it was revealed that its branch in Estonia carried out suspicious transactions for over \euro200 billion during %2007-2015 \citep{BJERREGAARD2019}, resulting in lawsuits with total claims of more than \$2 billion.
%Shortly thereafter, Danske Bank admitted to have had severe deficiencies in their money laundering controls in Estonia. 
%This has resulted in several lawsuits against Danske Bank, with total claims reaching \$2 billion. 
%Its stock price plummeted as a result, and was in 2020 at merely 30\% of its value before the scandal broke. 
}, 
the shortcomings of the rule-based system are high on the agenda for regulators and financial institutions alike, and considerable resources are devoted to developing more effective electronic surveillance systems. 
One avenue that is explored is to use machine learning (ML) to automatically learn when to generate alerts (see e.g.~\cite{JULLUM2020}, \cite{ROCHA2021}). 
Compared to human capabilities, ML is superior at detecting complicated patterns in vast volumes of data. 
As a consequence, ML-based systems have the potential to provide detection systems with increased accuracy and the ability to identify more sophisticated ways of laundering money.

Money laundering is a social phenomenon, where groups of organized criminals often collaborate to launder their criminally obtained proceeds. 
In the networks relevant for AML, the nodes may consist of customers, while the edges between the nodes represent money transfers, shared address, or joint ownership, to name a few possibilities. Analyses of such networks can uncover circumstances that would be impossible to detect through a purely entity-based approach, simply because the necessary information would not be present. 
Therefore, ML methods that leverage these relational data have a substantial advantage over those that do not. 
%As the suspicious behavior often lie in the relations between the customers, ML methods that utilize these relational data have a substantial advantage over those that do not. [alternative to the two sentence before.] 
The simplest way to incorporate network characteristics into ML methods is
%to use e.g.~network centrality metrics 
%or more sophisticated local network summaries based on spectral graph theory \citep{chung1997spectral} or the skip-gram based Node2Vec \citep{GROVER2016}
to create node features that capture information about the node's role in the network, e.g.~network centrality metrics  or characteristics of its neighborhood.
%Network science methods include e.g.~spectral graph theory \citep{chung1997spectral}, network centrality metrics, and skip-gram based methods such as Node2Vec \citep{GROVER2016}. 
%Such methods can be used to create node features that capture information about the node's role in the network, e.g.~its importance or characteristics of its neighborhood. 
The features can then be used in a downstream machine learning task. 
This approach is, however, suboptimal as the generated features might not be the ones most informative for the subsequent classification, and the full richness of the relational data will in any case not be passed over to the entity-based machine learning task.
%be captured by such network aggregation techniques

%This decoupling of the feature generation and the downstream machine learning task will often result in suboptimal learning, as the generated features may not be the ones most informative for the subsequent classification.
%More fundamentally, information is lost in the process of embedding network data into features on the entity level, and the downstream task will therefore not receive the full richness of information present in the network. 
\textit{Graph Neural Network} (GNN) is a class of methods that overcome this drawback by applying the machine learning task directly on the network data through a neural network. 
GNNs are able to solve various graph-related tasks such as node classification \citep{KIPF2016}, link prediction \citep{ZHANG2018}, graph clustering \citep{WANG2017}, graph similarity \citep{BAI2019} as well as unsupervised embedding \citep{KIPF2016variational}. 
The primary idea behind GNNs is to extend the modern and successful techniques of \textit{artificial neural networks} (ANNs) from regular tabular data to that of networks. 
In addition to their ability to incorporate both entity features and network features into a single, simultaneously trained model, most GNNs scale linearly with the number of edges in the network, making them applicable to large networks.
A huge benefit of GNNs in practical use cases is that they are inductive rather than transductive:  While transductive models (e.g.~Node2Vec \citep{GROVER2016}) can only be used on the specific data that was present during training, inductive models can be applied to entirely new data without having to be retrained. 
This is crucial for the AML application where new transactions (edges) appear continuously, and customers (nodes) enter and leave on a daily basis. 

Most GNNs are developed for \textit{homogeneous} networks with a single type of entity (node) and a single type of relation (edge).
In the present AML use case, the relevant network is \textit{heterogeneous} in both nodes and edges: The nodes represent both private customers, companies, and external accounts, while the edges represent both financial transactions and professional business ties between the nodes. 
%The field of developing GNNs able to take advantage of heterogeneous graphs is still in its infancy \citep{WU2020}.
There exist some GNNs in the literature that are able to handle heterogeneous networks, such as:
%Still, some heterogeneous GNNs have been proposed in the literature: 
RGCN \citep{SCHLICHTKRULL2018},
% Expand on the framework in GCN to graphs with multiple types of edges
HAN \citep{WANG2019},
% generalizes the approach in GAT to that of heterogeneous graphs. 
MAGNN \citep{FU2020MAGNN},
% Metapath Aggregated Graph Neural Network (MAGNN) builds on the approach of HAN by also considering the neighbors along each meta-path M. 
HGT \citep{HU2020HGT}, and 
% Heterogeneous Graph Transformer (HGT) (Hu et al. 2020) is inspired by the application of the Transformer architecture  in Vaswani et al. 2017 and Devlin et al. 2018
HetGNN \citep{ZHANG2019HetGNN}.
%These will be briefly described in Section \ref{sec:hetGNN}.
% ??
A common issue with all these methods is that they are not designed to incorporate edge features. 
In our AML use case, this corresponds to properties of the financial transactions (or the business ties) and is crucial information for an effective learning task.
%What all of these methods have in common is that they are not designed to incorporate edge features in their model.
Thus, based on our current knowledge and research, there exists no directly applicable GNN method for our AML use case.

The present paper proposes a heterogeneous GNN method that utilizes the edge features in the graph, which we denote \textit{Heterogeneous Message Passing Neural Network} (HMPNN). 
The HMPNN method is an expansion of the MPNN method \citep{GILMER2017} to a heterogeneous setting.
The extension essentially connects, and simultaneously trains multiple MPNN architectures, each working for different combinations of node and edge types.
We investigate two distinct approaches to aggregate the embeddings of node-edge combinations in the final step: 
The first approach applies a simple summation of the embedding vectors derived from the various combinations. 
The second approach is novel and concatenates the embeddings before applying an additional single-layer neural network to enhance the aggregation process.

The HMPNN is developed for and applied to detect money laundering activities in a large heterogeneous network created from real-world bank transactions and business role data which belongs to Norway's largest bank, DNB. 
The nodes represent bank customers and transaction counterparties, while the edges represent bank transactions and business ties.
The network has a total of more than 5 million nodes and almost 10 million edges. 
Among the bank customers, some are known to conduct suspicious behavior related to money laundering. While the rest are assumed not fraudulent, there could be some undetected suspicious behavior also there. Thus, this is actually a semi-labeled dataset. 
Finally, no oversampling or undersampling was performed, which makes the class imbalance realistic to what one encounters in practical situations. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are primarily two categories of bank customers: individual (retail) customers and organization (corporate) customers. 
Due to the fundamental differences between these two groups and their fraudulent behaviors, it is common practice to study and model their fraudulent behavior separately. 
In our setting, we, therefore, treat them as two distinct node types, each with its unique set of features.
In this paper, we limit the scope to modeling, predicting, and detection of fraudulent \textit{individual} customers. 
I.e.~all fraudulent customers in our dataset are individual customers. 
We chose to restrict our analysis to individuals because there is a significantly larger number of known fraudulent individuals compared to organizations, providing the model with more fraudulent behavior to learn from.
Furthermore, individuals are a more homogeneous group with simpler customer relationships compared to organizations, making them a more suitable group to apply our methodology to. 
Nevertheless, note that organizations may very well be involved and utilized by fraudulent individuals, and possess key connections in the graph that the model learns from.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In addition to network data, the nodes and edges have sets of features that depend on what type of node and edge they are. 
In this paper, HMPNN is compared to other state-of-the-art GNN methods and achieves superior results. 
To the best of our knowledge, there exists no prior published work on applying graph neural networks on a large real-world heterogeneous network for the purpose of AML.

The rest of this article is organized as follows: 
Section \ref{sec:background_related_work} provides some background and an overview of related work within the AML domain, as well as the GNN domain.
In Section \ref{sec:model} we formulate our HMPNN model after introducing the necessary mathematical framework and notation. 
Section \ref{sec:usecase} presents the data for our AML use case, the setup of our experiment, and presents and discusses the results we obtain.
Finally, Section \ref{sec:conclusion} summarizes our contribution, and provides some concluding remarks and directions for further work.
Additional model details are provided in Appendix\ref{app:network_features} and \ref{app:parameters}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and related work}
\label{sec:background_related_work}

In this section, we provide some background on the AML process and give an overview of earlier, related work in the domain of AML. 
We also briefly describe the state-of-the-art of homogeneous and heterogeneous GNNs in the general case.

\subsection{AML process}

As mentioned in the Introduction, financial institutions are required by law to have effective AML systems in place. 
Figure \ref{fig:alert_workflow} illustrates a typical AML process in a bank.
% Figure environment removed
The electronic surveillance system generates what is called \textit{alerts} (1) on some of the transactions, or transaction patterns, that go through the bank.  
An alert is first subject to a brief initial inspection, where those that can easily be identified as legitimate are picked out and marked as closed.  
Otherwise, the alert is upgraded to a \textit{case} (2).  
At this stage, multiple alerts might be merged into a single case, which regularly involves multiple implicated customers. 
The case is then inspected thoroughly by experienced investigators.  
If money laundering is ruled out, the case will be marked as closed. 
Otherwise, the case will be upgraded to a \textit{report} (3), and the revealed circumstances are reported in detail to the national \textit{Financial Intelligence Unit} (FIU). 
From here on, the FIU oversees further action and will determine whether to start a criminal investigation. 

The manual investigation part of the process is difficult to set aside for two reasons: a) The process is hard to automate, as the investigators sit on crucial, yearlong experience and often use non-quantifiable information sources to build their cases. b) AML laws do typically not allow automated reporting of suspicious behavior. 
The electronic surveillance generating the alerts is suitable for automation, and the benefits of better and more targeted alerts with fewer false positives, are huge for financial institutions. 
That is also why this paper is concerned with the electronic surveillance aspect of the AML process.

\subsection{AML literature}

In AML literature, there are very few papers that have datasets with real money laundering cases, and the majority of articles are validated on small datasets consisting of less than 10,000 observations \citep{CHEN2018}. 

% Traditional network science + AML
Both supervised (\cite{LIU2008}, \cite{DENG2009}, \cite{ZHANG2019}, \cite{JULLUM2020}) and unsupervised  (\cite{LORENZ2020}, \cite{ROCHA2021}) machine learning have been applied to AML use cases. However, the literature is quite limited, particularly for papers utilizing relational data. 

Some AML papers apply the aforementioned strategy of generating network features used in a down-stream machine learning task:
\cite{SAVAGE2016} create a graph from cash- and international transactions reported to the Australian FIU (AUSTRAC), from which they extract communities defined by filtered $k$-step neighborhoods around each node, to then create summarizing features used to classify community suspiciousness using supervised learning.
\cite{COLLADON2017} construct multiple graphs (each of which aims to reveal unique aspects) from customer and transaction data belonging to an Italian factoring company. On these graphs, they report significant correlation between traditional centrality metrics (e.g. betweenness centrality) and known high-risk entities.
\cite{ELLIOTT2019} uses a combination of network comparison and spectral analysis to create features that are applied in a downstream learning algorithm to classify anomalies. 

% GNNs + AML
There are only a few papers applying GNNs to money laundering detection:
In a brief paper, \cite{weber2018} discuss the use of GNN on the AML use case.
The paper provides some initial results of the scalability of GCN \citep{KIPF2016} and FastGCN \citep{chen2018fastgcn} for a synthetic data set. 
However, no results on the performance of the methods were provided. 
\cite{weber2019} compare GCN to other non-relational ML methods on a real-world graph generated from bitcoin transactions, with 203k nodes and 234k edges. 
The authors highlight the usefulness of the graph data and find that GCN outperforms logistic regression, but it is still outperformed by random forest. 
The dataset, called the Elliptic data, is also released with the paper and has later been utilized by several others: 
\cite{alarab2020competence} apply GCN extended with linear layers and improve significantly on the performance of the GCN model in \cite{weber2019}. 
\cite{lo2023inspection} apply a self-supervised GNN approach to create node embedding subsequently used as input in a random forest model, and reported good performance.
Others \citep{alarab2022graph, pareja2020evolvegcn} exploited the temporal aspect of the graph to increase the performance on the same data set. 

It is unknown to what extent results from synthetic or bitcoin transaction networks are transferable to the real-life application of transaction monitoring of bank transactions. Our graph is also about 25 times larger than the Elliptic dataset.
Moreover, as we will review immediately below, much has happened in the field of GNN since the introduction of GCN in 2018. 
Finally, to the best of our knowledge, there are no papers applying GNN (or other methodology) to an AML use case with a \textit{heterogeneous} graph. 

\subsection{General case GNN literature}

The history of GNNs can be traced back about twenty years and GNNs have during the past few years surged in popularity. 
This was kicked off by \citet{KIPF2016} who introduced the popular method \textit{Graph Convolutional Network} (GCN). 
For an excellent survey of GNNs including their history, we refer to \cite{WU2020}.
The core dynamics in GNNs is an iterative approach where each node receives information from its neighbors, and combines it with its own representation to create a new representation, which will be forwarded to its neighbors in the next iteration. 
We call this \textit{message passing}.
After a few iterations, these node representations are used to make inference about the node. 

Concentrating on today's most popular group of GNNs, \textit{Spatial-based convolutional GNNs}, we briefly review some relevant homogeneous and heterogeneous GNN methods below.

\subsubsection{Homogeneous GNN literature}
\label{sec:homGNN}

The GCN method \citep{KIPF2016} is motivated by spectral convolution and was originally formulated for the transductive setting operating directly on the adjacency matrix of the graph. However, the method can be reformulated in the inductive and spatial-based GNN setting using a message-passing formulation:  The message received by a node from its neighbors is a weighted linear transformation of the neighboring node representations, followed by aggregating the result by taking their sum.

GraphSage \citep{HAMILTON2017} expands on GCN in two ways: It uses a) a neighborhood sampling strategy to increase efficiency, and b) a neutral network, a pooling layer, and an LSTM \citep{hochreiter1997long} to aggregate the incoming messages instead of a simple sum. A drawback of GraphSage is, however, that it does not incorporate edge weights.

The Graph attention network (GAT) of \citet{VELIVCKOVIC2017} introduced the attention mechanism into the GNN framework. This mechanism learns the relative importance of a node's neighbors, to assign more weight to the most important ones.
% "Adopts attention mechanism to learn the relative weights between two connected nodes"
% Introduces the attention mechanism into the GNN framework. 
% The relative importance of a nodes neighbors is learned
% The relative importance of a nodes neighbors is learned through the attention mechanisms, and the incoming message is weighted accordingly.  
% The core idea is to learn which neighboring nodes are more important, and add a higher weight on the information from these in the computation.

\citet{GILMER2017} presented the Message Passing Neural Network (MPNN) framework, unifying a large group of different GNN models.
Apart from the unifying framework, the most essential contribution of this model is in our view that it applies a learned message-passing function that utilizes edge features.

% formulate a framework called Message Passing Neural Network (MPNN) that can express a large group of different GNN models.
% From the viewpoint of this paper, the most essential contribution of this model is that it introduces a learned message passing function that utilize edge features.


\subsubsection{Heterogeneous GNN literature}
\label{sec:hetGNN}
Heterogeneous graph neural networks are commonly defined as extensions of existing homogeneous graph neural networks. For instance, the Relational Graph Convolution Network (RGCN) \citep{SCHLICHTKRULL2018} extends the GCN framework to support graphs with multiple types of edges. RGCN achieves this by breaking down the heterogeneous graph into multiple homogeneous ones, one for each edge type. In each layer, GCN is applied to each homogeneous graph, and the resulting node embeddings are element-wise summed to form the final output. A drawback of RGCN is that it does not take node heterogeneity into account.
%
Heterogeneous Graph Attention Networks (HAN) \citep{WANG2019} generalize the Graph Attention Network (GAT) approach to heterogeneous graphs by considering messages between nodes connected by so-called meta-paths. Meta-paths (see the formal definition in Definition \ref{def:meta_path_step}) are composite relationships between nodes that help to capture the rich structural information of heterogeneous graphs. 
HAN defines two sets of attention mechanisms. The first is between two different nodes, which is analogous to GAT. The second set of attention mechanisms is performed at the level of meta-paths, which computes the importance score of different composite relationships.
%
Metapath Aggregated Graph Neural Network (MAGNN) \citep{FU2020MAGNN} extends the approach of HAN by also considering the intermediary nodes along each meta-path \citep{DONG2017}. While HAN computes node-wise attention coefficients by only considering the features of the nodes at each end of the meta-path, MAGNN transforms all node features along the path into a single vector. 
%
%HetGNN \citep{ZHANG2019HetGNN} utilizes a sampling strategy to select nodes from the heterogeneous neighborhood of each node and then aggregates their feature information using a neural network. 
%
%Heterogeneous Graph Transformer (HGT) 
%\citep{HU2020HGT}, 
%is inspired by the application of the transformer architecture  in Vaswani et al. 2017 and Devlin et al. 2018, and ...

The interest in GNNs is rapidly growing, and advancements in this field are consistently being made.
There are several other methods available that haven't been discussed here. 
For a more comprehensive understanding, we once again refer to the survey conducted by \cite{WU2020}, which provides an overview of GNNs in general. Additionally, for insights specifically on Heterogeneous Network Representation Learning, including Heterogeneous GNNs, we refer to \cite{YANG2020} for a good overview.