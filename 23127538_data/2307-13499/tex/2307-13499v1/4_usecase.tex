\section{AML use case}
\label{sec:usecase}

In this section, we first describe the heterogeneous graph data. Then we lay out the setup of the experiments we have performed on this dataset before we provide the results.  

\subsection{Data}

The graph is established based on customer and transaction data from Norway's largest bank, DNB, in the period from February 1, 2022, to January 31, 2023.
The nodes in the graph represent entities that are senders and/or recipients of financial transactions.
If two entities participate in a transaction with each other, this is represented by an edge (of type transaction) between the two, where the direction of the edge points from the sender to the recipient.
There are in total 5 million nodes and 9 million such edges in the graph. 

There are three types of nodes in the graph. 
The first one is called \textit{individual} and represents a human individual’s customer relationship in the bank. 
It includes all of the individual’s accounts in the bank\footnote{ 
Transactions made to/from any of the accounts in the bank belonging to the customer will result in an edge to/from the node representing the individual. 
}.
The second type of node is called \textit{organization}, and represents an organization’s or company’s customer relationship in the bank in the same manner as a node representing an individual. 
The third type of node is called \textit{external} and represents a sender or recipient of a transaction that is outside of the bank. 

The majority of the edges in the graph represent presence of a financial transaction between different individuals/organizations/external entities in the edge direction. 
In addition to this edge type, the graph includes role as a second edge type.
That edge points from an individual to an organization if the individual occupies a position on the board, is the CEO, or holds ownership in the organization. 
%
The resulting graph is directed and heterogeneous with respect to both nodes and edges. 
Figure \ref{fig:graph_schema} shows the schema of the graph, including the nine possible meta-steps. 
As shown in the schema, there are no edges between different external nodes,
since the bank does not have access to transactions not involving their customers. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The nodes that represent individuals are assigned a binary class (0 for regular individuals, and 1 for individuals known to conduct suspicious behavior). 
As mentioned in the introduction, the data only contains labels for individual nodes.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Suspicious individuals are defined as those that have been subject of an AML case (stage 2 
in Figure \ref{fig:alert_workflow}) during a certain time window. 
Note that customers implicated in cases that were not reported to the FIU are still defined as suspicious. This decision was made because our objective is to model suspicious activity, which these customers certainly have conducted, even though the suspiciousness was diminished by a close manual inspection.
Less than 0.5\% of the individuals belong to class 1 (suspicious).  


% Figure environment removed

Due to the sensitive nature of these data, containing both personal and possibly competition sensitive information for the bank, the data are not shareable. 
We are neither allowed to reveal the exact details of the graphs nor the features associated with the different nodes/edges in our model. 
Below, we give a broad overview of the characteristics and features of the graph, within our permission restrictions. 
To get a feeling of the local characteristics of the graph, Figure \ref{fig:egonet} shows egonets of four random nodes, with the starting node enlarged. 
The upper and lower panels show, respectively, the 3-hop and 9-hop egonets of the (undirected) transaction and role edges. The number of shown hops was chosen to balance presentability and amount of detail.
Moreover, Figure \ref{fig:degree_hist} shows histograms of the degree centrality for the three different node types.
%
% Figure environment removed
%
% Figure environment removed
%
Some nodes have a large number of neighbors, while others have few. 
While the degree distribution of individuals and organizations is similar, the distribution for organizations has a thicker tail, indicating that it's more common for organizations to exhibit a higher degree.
As we don't have knowledge of edges between external nodes, this node type typically has much fewer neighbors. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The three node types have separate sets of features that contain basic information about the entity.
There are eleven, eight, and two node features for, respectively, individuals, organizations, and external nodes.
The transaction edges have as features the number and monetary amount of transactions made within the one-year period. 
The role edges have two features, the first denoting the role type and the second the ownership percentage (provided the role represents ownership in the organization).


\subsection{Experiment setup}

The goal of this use case and experiment is to see to what extent our HMPNN method is able to predict the label (suspicious/regular) on nodes where the label is unknown to the model.
As mentioned in the Introduction, we concentrate on building models for detecting fraudulent \textit{individual} customers.
%Since individuals and organizations have different behavior, it makes sense to build different models for predicting the label of individuals and organizations. We concentrate on the largest of these, individuals. 
This means that even if we use the entire graph for message passing, only individual nodes (in the training set) are assigned a label to learn from, and only individual nodes (in the test set) are subsequently predicted and evaluated. 
%We created a model for one specific node type, namely either Individuals or Organizations.
%Due to time constraints, the decision was made to prioritize the development of models specifically for nodes of type Individual. As a consequence, this article only contains results for predictions made on Individuals

To evaluate the performance of our methods, we benchmark and compare their performance to a set of alternative models/methods. 
To mimic a scenario with unknown labels, we split the nodes into a training set and a test set. 
The whole graph is available for each of the models when training them, but the labels are only available for the nodes in the training set. 
At testing time, each method attempts to predict the (unknown) nodes in the test set, and their performance is compared using different performance measures.
We used a 70-30 train-test split, where the splitting was performed using stratified random sampling with allocation proportional to the original class balance, such that the class balance is preserved in the two sets. 
The same split was used for all methods.

We compare the result of four model frameworks: two non-graph methods supplied with additional node features, and two GNN methods. 
The models are applied with multiple model complexity configurations. 
To enhance the non-graph methods, we generated 83 additional node features that accompany the original node features. These include 11 summaries of the node-neighborhoods, i.e.~the number of neighbors of different types, both incoming and outgoing. We also computed 8 weighted summaries of node-neighborhoods, specifically for transaction edges and the monetary amount node feature. 
Additionally, we added 64 features generated using metapath2vec. These were assembled from embeddings of dimension 8 generated for each meta-path of length 2 starting and ending at a node of type individual. 
Together with the 11 intrinsic node features, this results in a total of 94 features. For more detailed information on these additional node features, please refer to Appendix \ref{app:network_features}.
The four methods are briefly described below:
\begin{description}
\item[Logistic regression] This classic method serves as a very basic benchmark not directly utilizing the network information and with a basic and inflexible parametric form. The method has access to the additional network-generated node features.
\item[Regular Neural Network] This method, applied with both 1 and 2 hidden layers, is much more flexible than the logistic regression model, but neither utilizes the network directly. The method has access to the additional network-generated node features.
\item[HGraphSage] GraphSage \citep{HAMILTON2017}  is a well-known homogeneous GNN method and is applied to our heterogeneous graph in the same fashion as HMPNN, as described below. In contrast to MPNN, GraphSage does not utilize edge features. We, therefore, test its performance both with and without additional node features that hold the weighted in/out degrees for the transaction edges, and the weight is the transaction amount on the edges. This results in 6 additional node features for nodes of type \textit{individual} and \textit{organization}, and 4 on nodes of type \textit{external}. We test the method with the aggregation function in \eqref{eq_mpnn_agg_sum} (HMPNN-sum). We applied the method with both one, two, and three hidden layers. 
\item[HMPNN] This GNN method, described in Section \ref{sec:HMPNN}, is our extension of MPNN to heterogeneous graphs. We test the method with the two aggregation functions in \eqref{eq_mpnn_agg_sum} (HMPNN-sum) and \eqref{eq_mpnn_agg_ct} (HMPNN-ct).
We applied the method with both one, two, and three hidden layers for each of the aggregation methods.
\end{description}
In total, our experiment contains 15 different models/method variants. 
The number of parameters involved in each of these is listed in Table \ref{tab:num-parameters} in the Appendix. 


%\subsubsection{2-layer neural network with graph features}
%
%To get a feeling of the performance improvement due to the use of methods utilizing the graph structure of the data, we include a non-GNN baseline method in our comparison. 
%We used a purely entity-based two-layer neural network classifier, where the graph is only used upstream to create additional network features for each node which is added to its intrinsic entity level features. 

%We created four types of network features: Degree summary, centrality metrics, DeepWalk embeddings
%and Metapath2Vec embeddings. Combined, they are an attempt to incorporate high-quality features
%on the node-level that summarize information about their role in the heterogeneous network. In
%total, this results in 79 additional features. Combined with the entity features
%we end up with 87 and 86 features for individuals and organisations, respectively. Precise specification of these network features are provided in \ref{app:network_features}.
%This model was trained with PyTorch.

%\subsubsection{GraphSage}
%The second benchmark model is GraphSage. 
%Because this message operator does not utilize the edge features, it does not initially have any information about the quantity of money that flows between the nodes. 
%Therefore, there is reason to believe that the model will benefit from adding the additional degree features.


The logistic regression and Regular Neural Network models were trained using the open source deep learning library Pytorch \citep{NEURIPS2019_9015}. 
HMPNN and HGraphSage were trained using the open source library Pytorch Geometric (PyG), which expands Pytorch with utilities for representing and training GNNs. 

All models were trained with the Adam optimiser \citep{Kingma2015Adam}, using the Binary Cross Entropy loss function:
\[
\text{Loss}(\hat{y}_v, y_v) = y_v\cdot \log(\hat{y}_v) + (1-y_v)\cdot \log(1-\hat{y}_v).
\]
%
The hyperparameters for each of the methods were tuned using 5-fold cross-validation on the training set. 
Here, three hyperparameters were determined: (1) The regularization strength,  (2) the learning rate, and (3) the number of training iterations, i.e., by early stopping. 
For all methods, the learning rate was in the range $[10^{-4},10^{-1}]$. 
As for regularization, the $L_2$-constraint was used, and was in the range $[10^{-8},10^{-1}]$. 
The optimal value for the $L_2$ constraint was highly dependent on the complexity of the model to be trained.

The experiments were carried out using  \textit{python 3.7.0}, with PyTorch 1.12.1 and PyG 2.2.0.
The computer used to run the experiments had 8 CPUs of the type \textit{High-frequency Intel Xeon E5-2686 v4 (Broadwell) processors}, with 61GB shared memory, and one GPU of type \textit{NVIDIA Tesla V100} with 16GB memory. 
This GPU has 5,120 CUDA Cores and 640 Tensor Cores.

\subsection{Results}

For each node in the test set, all the different methods output a score between 0 and 1, reflecting the probability that the node is a suspicious customer. 
To measure the overall performance of different methods on the test set we rely on the area under the precision/recall curve (PR AUC) and the area under the receiver operator curve (ROC AUC). 
PR AUC computes the area under the curve obtained by plotting the \textit{precision} (TP/(TP+FP)) as a function of the \textit{recall} (TP/(TP+FN)), and PR ROC computes the area under the curve obtained by plotting the recall as a function of \textit{False Positive Rate} (FP/(FP+TN)). Here TP/FP/TN/FN represents the number of classified nodes which are, respectively, true positives, false positives, true negatives, and false negatives. 
Figure \ref{fig:AUCres} displays the ROC AUC and PR AUC on the test set for all different methods and number of neural network layers used by the respective methods. 

% Figure environment removed

All methods benefit from including more layers. 
HMPNN-ct with 3 layers is the best method in terms of both PR AUC and ROC AUC.
While HMPNN-ct does quite well when applied with a single hidden layer, this is not the case for the other network models, at least when compared to the basic logistic regression (regular neural network with one layer). 
In terms of ROC AUC, the logistic regression model is actually better than all the other network models, and in terms of PR AUC, it is better than the HGraphSage models and comparable to HMPNN-sum. 
This is quite remarkable as the logistic regression model does not know anything about the network, except for additional network summary features, and has the simplest form of architecture. 
Note, however, that the Regular Neural Networks with 2 and 3 layers only do slightly better than Logistic Regression. This indicates that the simple architecture of the Logistic Regression model is not a significant downside for that limited data set.
 Moreover, the large performance gap to the HMPNN-ct model also shows that it is certainly possible to get more out of the network structure than the other network models manage. Thus, we believe that the lack of performance for the other network models is related to an inappropriate and inefficient architecture compared to that of HMPNN-ct. 
 The fact that overall, HMPNN-sum does not perform on par with HMPNN-ct further indicates that the performance boost in HMPNN-ct is mainly due to the architectural trick of the last single-layer neural network. 

%- HGraphSage seems to only benefit slightly from including the additional network features when it has one or two layers.

\begin{table}[t]
\centering
\small
\caption{Precision at specific values of Recall, in addition to PR AUC and ROC AUC for the different models}
\label{tab:precision-recall}
\begin{tabular}{p{2.5cm}ccccccc}
\toprule
\multirowcell{2}{Model}&Number & \multicolumn{4}{c}{Recall(\%)} & PR & ROC \\ \cline{3-6}
&of Layers & 1 & 5 & 10 & 50 & AUC & AUC \\ 
\hline
\multirowcell{3}{Regular\\Neural Network}
&1 & 61.54 & 36.28 & 28.55 & 5.53 & 0.1075 & 0.8547 \\
&2 & 64.00 & 43.82 & 30.51 & 5.89 & 0.1173& 0.8613\\
&3 & 61.54 & 47.56 & 34.68 &5.88 & 0.1237 & 0.8612\\
\hline
\multirowcell{3}{HGraphSage}
&1& 59.26 & 33.62 & 20.58 & 4.01 & 0.0836 & 0.8329 \\
&2& 61.54 & 48.15 & 38.56 & 6.51 & 0.1280 & 0.8879 \\
&3& 59.26 & 60.47 & 42.47 &7.19 & 0.1452 & 0.8960 \\
\hline
\multirowcell{3}{HGraphSage\\(extra features)}
&1& 48.48 & 34.21 & 21.29 & 4.24 & 0.0858 & 0.8405 \\
&2& 64.00 & 50.65 & 35.39 & 7.41 & 0.1368 & 0.8882 \\
&3& 69.57 & 50.32 & 38.27 &7.68 & 0.1424 & 0.8915 \\
\hline
\multirowcell{3}{HMPNN-sum}
&1& 64.00 & 38.24 & 29.75 & 5.34 & 0.1090 & 0.8401 \\
&2& 69.57 & 42.62 & 36.38 & 7.76 & 0.1359 & 0.8863 \\
&3& 84.21 & 53.79 & 38.27 &8.67 & 0.1532 & 0.8955 \\
\hline
\multirowcell{3}{HMPNN-ct}
&1& 76.19 & 48.75 & 38.18 & 6.92 & 0.1418 & 0.8801 \\
&2& 61.54 & 50.65 & 43.66 & 8.96 & 0.1555 & 0.8989 \\
&3& 66.67 & 58.21 & 50.99 &10.25 & 0.1800 & 0.9083 \\
\bottomrule
\end{tabular}
\end{table}

Considering the limitations in resources faced by banks, conducting thorough examinations of a substantial volume of suspicious cases is typically unfeasible.
Therefore, the primary purpose of the model is to generate a limited set of  high-quality predictions where money laundering is likely to occur, meaning that the precision at small to medium-sized recall levels is more relevant than the higher ones.
Table \ref{tab:precision-recall} shows the precision corresponding to recall levels of 1\%, 5\%, 10\% and 50\%, respectively, and allows studying the performance of the methods in greater depth and at a wider range.

Focusing on HMPNN-ct, we see that when the classification threshold is set such that we identify 1\% of the suspicious customers (recall = 1\%) two-thirds of those classified as suspicious \textit{are} actually suspicious (precision $\approx 67\%$). 
Increasing the classification threshold to 5\% or 10\% gives precisions of about 58\% and 51\%.
Moreover, if we decrease the threshold such that half of the suspicious customers (recall = 50\%) are identified, 90\% of the customers classified as suspicious are not really suspicious. 
These rates may not seem impressive at first glance. 
Considering the severe class imbalance in the data (less than 0.5\% of the total number of observations are suspicious), and the fact that detection of money laundering is a notoriously difficult problem, these performance scores are, actually, very promising. 

Finally, note that even though the 3-layer HMPNN-sum model performs worse than HMPNN-ct overall and for the larger recalls, it obtains a significantly better precision (84\% vs 67\%) at recall 1\%. In essence, this model is better at detecting the most evident instances of money laundering. 
This aspect is crucial to consider when selecting a model, particularly if resource limitations restrict the investigation to a small number of customers for potential money laundering.



