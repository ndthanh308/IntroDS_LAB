\section{Experiment}

\subsection{Experimental Setup}

% \begin{table}[]
% \caption{  Dataset Statistics }
% \begin{tabular}{cccc}
% \hline
% Dataset     & Users & Items  & Avg. Length of Sequence \\ \hline
% Electronics & 22685 & 20712  & 15.26                   \\
% Movies      & 26933 & 18855  & 28.97                   \\
% Beauty      & 82659 & 119365 & 28.97                   \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[]
\caption{  Dataset Statistics }
\label{table:statistics}
\begin{tabular}{cccc}
\hline
Dataset     & Users & Items  & Avg. Length of Sequence \\ \hline
Electronics & 29710 & 20712  & 13.51                   \\
Movies      & 199435 & 155527  & 10.87                   \\
Beauty      & 82659 & 124859 & 6.96                   \\ \hline
\end{tabular}
\end{table}

\subsubsection{Datasets}
%\textbf{\textcolor{red}{@Bardia: please write about each dataset and its preprocessing that we used + statistics table}}

From Amazon, we have adopted three widely used real-world datasets, which are shown in Table \ref{table:statistics}. The Electronics dataset is derived from the public Amazon review dataset. This includes reviews of Amazon products belonging to the "Electronics" category from May 1996 to July 2014. Both The Movies and The Beauty are drawn from the same "Movie" and "Beauty" Amazon review categories. User reviews are treated as an interaction between them. These interactions are treated equally on all items. The $K$ parameter specifies the minimum number of transactions a user must keep. In addition, we delete users with fewer interactions in the system. We sort the data in order of the first transaction time, user ID, and transaction time. We then assign a new label to the items and users according to their appearance time, so that the first user is one, the second user is two, etc. Lastly, we will separate the test data from the test items. We will only keep the test data that contains interactions that use items from the test items in the test data. For each user node in the test and validation sets, we take each observed edge as a positive sample of the user. We then randomly select 100 items that did not interact with the current user as negative samples. Then based on the rank of the positive sample's score among negative samples, evaluation metrics will be calculated as in 


\cite{wang2021sequential,wei2020fast,kang2018self}.


\subsubsection{Baselines}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
% \caption{ Comparison of Different Models }
% \begin{tabular}{|c|cc|cc|cc|}
% \hline
% \multirow{}{}{Methods} & \multicolumn{2}{c|}{Electronics}   & \multicolumn{2}{c|}{Movie}         & \multicolumn{2}{c|}{Beauty}        \\ \cline{2-7} 
%                          & \multicolumn{1}{c|}{Hit@1} & MRR   & \multicolumn{1}{c|}{Hit@1} & MRR   & \multicolumn{1}{c|}{Hit@1} & MRR   \\ \hline
% BERT4Rec                 & \multicolumn{1}{c|}{0.200} & 0.323 & \multicolumn{1}{c|}{0.220} & 0.351 & \multicolumn{1}{c|}{0.214} & 0.341 \\ \hline
% MeLU                     & \multicolumn{1}{c|}{0.136} & 0.243 & \multicolumn{1}{c|}{0.168} & 0.289 & \multicolumn{1}{c|}{0.160} & 0.279 \\ \hline
% MAMO                     & \multicolumn{1}{c|}{0.127} & 0.296 & \multicolumn{1}{c|}{0.194} & 0.320 & \multicolumn{1}{c|}{0.195} & 0.310 \\ \hline
% MetaTL                   & \multicolumn{1}{c|}{0.241} & 0.320 & \multicolumn{1}{c|}{0.267} & 0.337 & \multicolumn{1}{c|}{0.231} & 0.328 \\ \hline
% MetaCF                   & \multicolumn{1}{c|}{0.210} & 0.330 & \multicolumn{1}{c|}{0.234} & 0.365 & \multicolumn{1}{c|}{0.220} & 0.340 \\ \hline
% \textbf{Our Model}                & \multicolumn{1}{c|}{}      &       & \multicolumn{1}{c|}{}      &       & \multicolumn{1}{c|}{}      &       \\ \hline
% \end{tabular}
% \end{table}


% \usepackage{tabularray}
% \begin{table}
% \centering
% \caption{Baselines}
% \label{table:evaltable}
% \begin{tblr}{
%   cells = {c},
%   cell{1}{1} = {r=2}{},
%   cell{1}{2} = {c=2}{},
%   cell{1}{4} = {c=2}{},
%   cell{1}{6} = {c=2}{},
%   % vline{2-3,5} = {1}{},
%   % vline{4,6} = {2}{},
%   % vline{2,4,6} = {3-8}{},
%   hline{1,3-9} = {-}{},
%   hline{2} = {2-7}{},
% }
% Methods            & Electronics &         & Movie   &         & Beauty  &         \\
%                    & $Hit@1$     & $MRR$   & $Hit@1$ & $MRR$   & $Hit@1$ & $MRR$   \\
% BERT4Rec           & $0.200$     & $0.323$ & $0.220$ & $0.351$ & $0.214$ & $0.341$ \\
% MeLU               & $0.136$     & $0.243$ & $0.168$ & $0.289$ & $0.160$ & $0.279$ \\
% MAMO               & $0.127$     & $0.296$ & $0.194$ & $0.320$ & $0.195$ & $0.310$ \\
% MetaTL             & $0.241$     & $0.320$ & $0.267$ & $0.337$ & $0.231$ & $0.328$ \\
% MetaCF             & $0.210$     & $0.330$ & $0.234$ & $0.365$ & $0.220$ & $0.340$ \\
% \textbf{Our Model} & \textbf{0.271} & \textbf{0.383} & \textbf{0.251} & \textbf{0.371} & \textbf{0.251} & \textbf{0.360} 
% \end{tblr}
% \end{table}





%\textbf{\textcolor{red}{@Bardia: please write about baselines and their corresponding adaptations}}

We compare the proposed model with the following methods: 

(i) Sequential recommendation baselines utilize different methods to capture the sequential patterns in the interaction sequences of users:

\begin{itemize}
    % \item SASRec:  Rely on Gated Recurrent Units, the simple convolutional generative network, and the self-attention layers to learn sequential user behaviors, respectively.
    \item SASRec: presents a self-attentive sequential recommendation model that utilizes Gated Recurrent Units, a simple convolutional generative network, and a self-attention mechanism to capture sequential patterns in user behavior and improve recommendation accuracy. The model is trained using a modified BPR loss function. 

    % \item BERT4Rec: adopts the bi-directional transformer to extract the sequential patterns, which is state-of-the-art for the sequential recommendation.
    \item BERT4Rec: proposes a novel recommendation model that uses the BERT architecture to capture sequential patterns in user behavior and improve recommendation accuracy. The model is trained using the bi-directional transformer to extract sequential patterns, outperforming other state-of-the-art models in accuracy and robustness to cold-start and long-tail item problems. The paper acknowledges some BERT4Rec limitations, such as its computational complexity and data requirements. However, it argues that the model's benefits justify the additional computational resources. 

\end{itemize}

(ii) Cold-start baselines include methods that provide accurate recommendations for customers with limited information. We modify these cold-start baselines to fit the case without auxiliary information. To deal with this issue, in the no side-information setting, for the datasets, we convert them into implicit recommendations by setting rated items to 1 and others to 0, and we utilize the Marginal Ranking loss function, which is the same as in our model, as we make implicit recommendations for binary signals. We just use the ID embedding of users and items as a feature (some methods like NGCF and LightGCN use this kind of embedding). In the training phase of recommendations, we sample data from the user and corresponding positive and negative items to calculate the loss at each step.

\begin{itemize}
    \item MeLU: Resolve the cold-start problem faced by existing recommender systems. The MeLU method uses meta-learning to estimate new users' preferences based on items they have consumed in the past. Moreover, the system provides a strategy to select evidence candidates to estimate customized preferences. It is shown that MeLU has a lower mean absolute error than two comparative models when tested on two benchmark datasets. In addition, the evidence selection strategy is tested in a user study. It aims to overcome the limitations of previous recommendation studies. These studies provided poor recommendations for users who consumed few items and inadequate evidence for candidates to identify user preferences.

    \item MetaTL: For cold-start users with minimal logged interactions, capturing sequential patterns of users for sequential recommenders is challenging. Models with limited interactions lose their predictive power due to difficulties in learning sequential patterns. Using meta-learning, the method proposes an innovative MetaTL framework that models users' transition patterns. A translation-based architecture extracts dynamic transition patterns from sequential recommendations in MetaTL, and meta-transitional learning facilitates fast learning for cold-start users with limited interaction. Meta-learning can improve sequential recommendations for cold-start users by inferring accurate sequential interactions.

    \item MAMO: Two memory matrices are used to store task-specific and feature-specific memories to support personalized parameter initialization and fast user preference prediction. 
    
    \item MetaCF: Discusses the cold-start problem in Collaborative Filtering (CF), where limited data is available for new users in the system. Previous approaches use user profiles, but these are not always available due to privacy concerns. MetaCF is a novel learning paradigm that leverages meta-learning to enable fast adaptation for new users. MetaCF learns a suitable initialization model for rapidly adapting to a new user. Adaptation rates are optimized in a fine-grained manner using Dynamic Subgraph Sampling to account for the dynamic arrival of new users. The proposed framework outperforms state-of-the-art baselines by a large margin in the cold-start scenario with limited user-item interactions.

\end{itemize}

\subsubsection{Evaluation Metrics}
%\textbf{\textcolor{red}{@Bardia: please write about them like metaTL with different sentences.}}

% In the experiment, each user only has one positive and true item for testing. With the predicted scores, we take each observed edge as a positive sample of the user and then randomly select 100 items that did not interact with the current user as the negative samples. This method has been widely used in many other works. Then we rank the list of positive and 100 negative items. We use Hit Ratio at rank 10 (HR@10)as the evaluation metric to measure the ranking performance. Mean Reciprocal Rank ($MRR$) indicates the rankings of the positive items. We also evaluate the Hit Rate ($Hit$) for the top-1 prediction. $Hit$@1 = 1 if the positive item is ranked top-1, otherwise $HR$@1 = 0. Also, note that $HR$@1 equals the recall or NDCG for top-1 prediction.

Each user was tested on only one positive and true item during the experiment. Based on the predicted scores, observed edges were taken as positive samples for the user. 100 items without interaction with the user were randomly selected as negative samples. This method is commonly used in other works. The list of 100 negative and positive items was ranked, and Hit Ratio at rank 10 (HR@10) was applied as the evaluation metric to measure ranking performance. Mean Reciprocal Rank (MRR) was used to indicate the ranking of positive items, and Hit Rate (Hit) was evaluated for the top-1 prediction. If the positive item was ranked top-1, Hit@1 was equal to 1; otherwise, it was 0. It should be noted that HR@1 is equivalent to recall or NDCG for top-1 prediction.

\subsection{Overall Performance}

\begin{table*}[!t]
\centering
\caption{Experimental results of different methods under K=3 on three data sets}
\label{table:evaltable}
\begin{tblr}{
  cells = {c},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{},
  cell{1}{5} = {c=3}{},
  cell{1}{8} = {c=3}{},
  hline{1,3-13} = {-}{},
  hline{2} = {2-11}{},
}
Methods            & Electronics    &                &                & Movies         &                &                & Beauty         &                &                \\
                   & $MRR$          & $Hit@1$        & $NDCG@5$       & $MRR$          & $Hit@1$        & $NDCG@5$       & $MRR$          & $Hit@1$        & $NDCG@5$       \\
BERT4Rec           & $0.323$        & $0.200$        & $0.319$        & $0.421$        & $0.220$        & $0.357$        & $0.341$        & $0.214$        & $0.338$        \\
MeLU               & $0.243$        & $0.136$        & $0.265$        & $0.336$        & $0.168$        & $0.302$        & $0.279$        & $0.160$        & $0.292$        \\
MAMO               & $0.296$        & $0.127$        & $0.313$        & $0.384$        & $0.194$        & $0.345$        & $0.310$        & $0.195$        & $0.336$        \\
MetaTL             & $0.320$        & $0.241$        & $0.324$        & $0.438$        & $0.319$        & $0.412$        & $0.328$        & $0.231$        & $0.335$        \\
MetaCF             & $0.330$        & $0.210$        & $0.313$        & $0.474$        & $0.276$        & $0.397$        & $0.340$        & $0.220$        & $0.322$        \\
\textbf{ClusterSeq} & $\mathbf{0.383}$ & $\mathbf{0.262}$ & $\mathbf{0.391}$ & $\mathbf{0.660}$ & $\mathbf{0.542}$ & $\mathbf{0.685}$ & $\mathbf{0.443}$ & $\mathbf{0.254}$ & $\mathbf{0.341}$ \\


\end{tblr}
\end{table*}

In this study, we evaluated the performance of ClusterSeq and state-of-the-art models under K = 3 on several datasets. The results are presented in Table \ref{table:evaltable}. The best-performing method in each column is highlighted in bold. The findings indicate that ClusterSeq outperforms the competing models in all datasets, demonstrating its effectiveness in providing accurate recommendations for cold-start users with limited interactions.

We started with basic neural models for sequential recommendations. We discovered that BERT4Rec performed poorly due to its inability to capture patterns in user interaction sequences and learn effective embeddings for cold-start users. However, utilizing transformers to extract sequential patterns proved more effective as they aggregate items with attention scores. This leads to more informative representations for users with limited interactions.

MeLU, MAMO, MetaCF, and MetaTL are meta-learning-based methods that provide cold-start recommendations. As MeLU and MAMO require side information about users and items, we used their historical interactions as side information. However, MeLU and MAMO failed to produce satisfying results, as they are designed for scenarios with abundant auxiliary user/item information, which is not the case here. On the other hand, MetaCF and MetaTL performed well in the sequential recommendation, highlighting the importance of fast adaptation in cold-start scenarios. Nevertheless, they still fell short of ClusterSeq's proposed clustering patterns for a cold-start sequential recommendation.

\subsection{Ablation Study}


We compare the proposed model with its variants and some baselines under different K values (i.e., how many interactions are initially present) to evaluate its effectiveness. Our original experiment demonstrated that BERT4Rec is the state-of-the-art sequential recommendation method, and MetaTL is one of the strongest cold-start baselines (and illustrates meta-transitional learning). Despite its high prediction power, BERT4Rec performs poorly on cold-start sequential recommendation tasks with a limited number of items. In sequential and cold-start user recommendations with different numbers of initial interactions, the proposed model can outperform state-of-the-art methods due to the well-designed optimization steps and clustering of users within the graph.

% Figure environment removed

We compare the performance of our entire model (with the clustering module) to several model variants that do not include the clustering module. We evaluate these models on a standard benchmark dataset and report the results regarding our evaluation metrics.
Figure \ref{fig:ablation_results} shows the results of the ablation study. Clearly, the entire model achieves the highest evaluation metrics, indicating that the clustering module is necessary for the model to achieve its best performance. Performance is significantly reduced when the clustering module is removed.
% \textcolor{red}{k=3 with clustering module: mrr:35.2 with clustering module:38.3}


\subsection{Parameter Analysis}
In this section, we investigate the impact of model parameters on the recommendation performance of our proposed model under cold-start scenarios. We examined how cluster number affects performance, followed by the impact of dimensions of user representations and learning rates.

\subsubsection{Number of clusters}
To study the effect of the number of clusters, we vary the number of clusters and plot the performance of the proposed method in terms of MRR in Figure \ref{fig:num_clusters}. We observed that the performance of the proposed method is generally stable for different number of clusters. In particular, we find that the proposed model is robust against this hyperparameter which is hard to estimate.

% Figure environment removed

\subsubsection{Impact of Embedding Dimensions}
Next, we explore the influence of the dimension of user embeddings on the recommendation performance of the proposed model. We vary the embedding sizes from 32 to 512 and plot the resulting performance in terms of MRR in Figure \ref{fig:embedding_dim}. Our model achieves optimal performance when the embedding dimension is set to 256. Our model is not generally stable around the optimal setting, indicating that it is important to set the embedding dimensions carefully.

% Figure environment removed


\subsubsection{Impact of Batch size}
Lastly, we analyzed the impact of batch size on the performance of our model. We conducted experiments by varying the batch size from 64 to 4096 and evaluated the resulting performance in terms of MRR, as presented in Figure \ref{fig:batch_size}. Our observations show that the optimal performance of our model is achieved when the batch size is set to 1024. Additionally, we note that the training process can converge even with smaller batch sizes like 512 or 256. Overall, this analysis highlights the importance of selecting an appropriate batch size to achieve high performance in recommendation tasks.

% Figure environment removed

To sum up, our analysis of the model parameters indicates that the recommendation performance of our model is stable for a reasonable range of hyperparameter values. However, some parameters are found to be critical for achieving optimal performance. Therefore, our findings emphasize the significance of meticulously tuning the model parameters to attain high recommendation performance in cold-start scenarios.

% Tables are considered illustrations containing data. Therefore, they should also appear floated to the top (preferably) or bottom of the page, and with the captions below them.

% \begin{table}
%     \centering
%     \begin{tabular}{lll}
%         \hline
%         Scenario  & $\delta$ & Runtime \\
%         \hline
%         Paris     & 0.1s     & 13.65ms \\
%         Paris     & 0.2s     & 0.01ms  \\
%         New York  & 0.1s     & 92.50ms \\
%         Singapore & 0.1s     & 33.33ms \\
%         Singapore & 0.2s     & 23.01ms \\
%         \hline
%     \end{tabular}
%     \caption{Latex default table}
%     \label{tab:plain}
% \end{table}

% \begin{table}
%     \centering
%     \begin{tabular}{lrr}
%         \toprule
%         Scenario  & $\delta$ (s) & Runtime (ms) \\
%         \midrule
%         Paris     & 0.1          & 13.65        \\
%                   & 0.2          & 0.01         \\
%         New York  & 0.1          & 92.50        \\
%         Singapore & 0.1          & 33.33        \\
%                   & 0.2          & 23.01        \\
%         \bottomrule
%     \end{tabular}
%     \caption{Booktabs table}
%     \label{tab:booktabs}
% \end{table}

% If you are using \LaTeX, you should use the {\tt booktabs} package, because it produces better tables than the standard ones. Compare Tables \ref{tab:plain} and~\ref{tab:booktabs}. The latter is clearly more readable for three reasons:

% \begin{enumerate}
%     \item The styling is better thanks to using the {\tt booktabs} rulers instead of the default ones.
%     \item Numeric columns are right-aligned, making it easier to compare the numbers. Make sure to also right-align the corresponding headers, and to use the same precision for all numbers.
%     \item We avoid unnecessary repetition, both between lines (no need to repeat the scenario name in this case) as well as in the content (units can be shown in the column header).
% \end{enumerate}

