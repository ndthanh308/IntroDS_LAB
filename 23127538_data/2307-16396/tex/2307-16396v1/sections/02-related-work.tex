\section{Related Work}
Prior research relating to search systems in the context of visual analysis generally falls into three main categories: (1) semantic web search systems, (2) natural language interfaces (NLIs) for visual analysis, and (3) search interfaces for visualizations.

\subsection{Semantic Web Search Systems}
Semantic search was initiated as a document search technique to improve searching precision by understanding the purpose
of the search (i.e., intent) and the contextual significance of words as they appear in the searchable data space to generate more relevant results~\cite{Guha2003SemanticS}. Approaches to semantic web search can roughly be divided into those systems based on structured query languages~\cite{corby:2004,swoogle,Heflin2003SHOEAB,Kasneci2008NAGAHS,oren:2008,Thomas2007ONTOSEARCH2SO}, keyword-based approaches~\cite{falcons,Harth2007SWSEAB,Lei2006SemSearchAS,Tran2007OntologyBasedIO,zenz:2009}, where queries consist of lists of keywords, and natural-language-based approaches~\cite{cimiano:2008, Damljanovic2010NaturalLI,fernandez:2008,Lpez2005AquaLogAO,lopez:2006}. Our work is inspired by the body of semantic search techniques where we explore how we can support various search scenarios (i.e., Q\&A, exploratory, and design search) for exploring data repositories.

Early research focused on the problem of augmenting traditional text search with additional metadata using ontological techniques to increase recall and precision~\cite{moldovan:2000,Buscaldi2005AWQ,grubar:1993,ciri}. Our work explores semantic augmentation in the context of data repository search by considering additional metadata pertaining to attributes in curated data sources, such as synonyms and related concepts, as well as metadata that describes the pre-authored content, such as the visualization type, data attributes, and author name.

To support targeted Q\&A in semantic search, systems have explored ways for accurately detecting NL patterns and phrases that represent temporal intents such as "in the 20th century" or spatial intents such as "in Europe''~\cite{KOLOMIYETS20115412}. Typical approaches in this direction involve a combination of statistical techniques (syntactic parsing) and semantic operations to identify ontology concepts in the user's input. For instance,
QUERIX~\cite{kaufmann:2006} combines the Stanford CoreNLP parser with WordNet to recognize salient phrases from NL user queries~\cite{klein-manning-2003-accurate}. Other Q\&A  systems apply linguistic processing to the question, identifying named entities and other query-relevant phrases~\cite{Srihari1999InformationES,chu-carroll:2006,panto}. \olio~identifies a set of analytical intents (e.g., trends, location, groupings, aggregations, filters) in the queries for supporting Q\&A in a data-oriented semantic search context.

More recently, web search engines blend complementary search experiences of machine-generated results with pre-authored documents and web pages~\cite{ding:2005}. Search platforms~\cite{google,Bing} have made updates to their search algorithms that place greater emphasis on search queries, considering overall context and meaning over individual keywords. The algorithm employs form-based or `template' queries to answer questions at scale in real-time such as the weather, flight status, or the current score of a basketball game. The premise of our research is to explore a similar search paradigm, specifically in the context of data repositories, where we explore the interpretation of queries containing bespoke analytical intents in addition to keyword search. 

Traditional information retrieval methods rely on large amounts of searchable text content. However, multimedia repositories that include videos and images, have limited searchable text content. To this end, research in multimedia retrieval has explored metadata extraction techniques to improve the precision and recall of the search algorithms. Techniques include constructing bag-of-word image descriptors from the associated text in documents referring to other similar images~\cite{vaca-castano:2015}, analyzing visual features in images~\cite{visualrank}, parsing XML descriptors in MPEG video files~\cite{hammiche:2004}, and object and scene retrieval in videos~\cite{videogoogle}, to name a few. Our work addresses an analogous problem when searching data repositories, given the sparseness of searchable text content. We include additional semantics for both data sources and visualizations using ontological enrichment from external corpora, along with properties extracted from the XML properties in the visualizations.

\subsection{NLIs for Visual Analysis}  
NLIs for visual analysis specifically support dynamic Q\&A in the larger context of semantic search experience. Systems like DataTone~\cite{datatone} support analytical Q\&A, producing a chart according to that inference and then providing ambiguity widgets through which the user could adjust the system's default choice. Eviza~\cite{eviza} and Analyza~\cite{analyza} extend that premise through contextual inferencing. Evizeon~\cite{hoque2017applying} and Orko~\cite{orko} explore the notion of pragmatics in analytical conversation by using the knowledge of data attributes, values, and data-related expressions. 

Commercial visualization Q\&A systems~\cite{thoughtspot,ibmwatson,powerbi} have evolved over the years to better understand a user's analytical intent expressed in NL and provide reasonable visualization responses. The forms of inferring intent typically rely on explicitly named data attributes, values, and chart types in the user's input queries. Ask Data~\cite{setlur2019inferencing} handles various analytical expressions in NL form, such as grouping of attributes, aggregations, filters, and sorts. The system also handles impreciseness around vague numerical concepts such as `cheap' and `high' by inferring a range based on the underlying statistical properties of the data. 

However, these systems assume that the data source or dashboard is already preselected before interpreting the queries. Further, they tend to focus on a subset of semantic search (primarily Q\&A). Our work explores how analytical search intent can be interpreted to support the various flavors of search across multiple repositories of data sources and visualizations.


\subsection{Search interfaces for Visualizations}
% \begin{itemize}
%     \item \href{https://observablehq.com/@observablehq/searching-on-observable\#attributes}{https://observablehq.com/@observablehq/searching-on-observable\#attributes} (lists metadata supported on observable)
% \end{itemize}

Large-scale search platforms for visualizations have focused on experiences to help users reason and analyze data sets of interest. ManyEyes, a web-based service, combined public data sharing with interactive visualizations~\cite{viegas2007manyeyes}. Users could upload and visualize data on the web, facilitating the sharing
and discussion of visualizations. Morton et al.~\cite{morton2014public} used Tableau Public as a platform to analyze the use of online visual analysis systems and point out that there is a need for improvement of web-based visualization analytics systems to better support both search and content diversity of visualization designs. 

Past research also highlights a need for search tools and interfaces to be better integrated into users' authoring workflows. Battle et al. suggest new user experiences, such as design search where the visualization community could easily find D3 content based on chart types, visual style, and structure to help translate their ideas into often complex and bespoke visualizations~\cite{battle2021exploring}. Hoque and Agrawala~\cite{hoque2019searching} present a search engine for D3 visualizations collected from the web that allows queries based on their visual style and underlying structure. Their search engine indexes the marks and encoding, along with visual style and layout, to support the exploration of D3 charts with specific design characteristics. SightLine is a web portal that passively collects and organizes visualizations to explore the design space of visualizations on the web~\cite{sechler2017sightline}. By preserving the context of each visualization visit, the tool enables personal provenance through the discovery and exploration of trending visualizations, as well as a more targeted search by querying the metadata collected for each visualization. Along these lines, Observable has the provision for specifying search tags to restrict and combine search terms~\cite{Observable:searchtags}. The tags stem from metadata properties for these notebooks, including author, title, collection name, etc.~\cite{Observable:metadata}.

Building on prior research, our work recognizes the various scenarios for search in the context of data repositories and explores a semantic search user experience for supporting these scenarios within a \emph{unified} interface. \olio~serves as a research probe to explore the interpretation of search intent against data sources and visualizations by utilizing their underlying metadata. 
