\section{\olio}

\olio~is designed as an interface that supports semantic search behavior by dynamically generating visualization responses and pre-authored visualizations from data repositories. Below, we describe \olio{'s} interface through a brief usage scenario and subsequently detail the key system components and implementation.

\subsection{Interface}


% Figure environment removed

% Figure environment removed


The interface initially shows a landing screen that displays a sampling of data sources available for Q\&A search (Figure~\ref{fig:start-screen}). A user can hover over a data source thumbnail and view its corresponding metadata information (\textbf{DC3}). The user then types a search query, \textit{``housing prices usa''} in the input text box (Figure~\ref{fig:interface}A). The system detects that token `usa' is a geographic location and searches for a relevant data source in its data repository. \olio~finds the housing data source to be a match, and a map is dynamically generated as a Q\&A response to the query (Figure~\ref{fig:interface}B). In addition, as part of exploratory search, the query tokens are used as keywords to match any pre-authored visualizations (\textbf{DC1}). A grid of thumbnails is displayed to serve as a preview to the user for browsing and exploration (Figure~\ref{fig:interface}C). Each thumbnail is hyperlinked to its corresponding visualization file that the user can choose to peruse in more detail or download to their local machine. The title, author name, and creation date of the visualization are displayed below each thumbnail to provide additional context. Scented widgets~\cite{willett2007scented} appear on the right side of the exploratory search panel to support faceted browsing of the pre-authored visualizations (Figure~\ref{fig:interface}D). The user can narrow down the search results by simultaneously applying one or more filters, namely, author name, visualization type, and the creation date (\textbf{DC4}).



\subsection{System Overview}
% Figure environment removed

\olio~is implemented as a web-based application using Python and a Flask backend connected to a Node.js frontend. We leverage Elasticsearch~\cite{elasticsearch}, an open-source Java search engine that is designed to be distributive, scalable, and with near real-time query execution performance. \change{As a result, \olio{} can scale to a large number of data repositories for indexing and search.} Figure~\ref{fig:overview} illustrates a high-level depiction of the system’s architecture, with the following main components: query classifier, parser, semantic search framework, Q\&A module, and the general search module.

A repository of curated data sources is included in the system for Q\&A search. The data sources could be any tabular CSV file, but for the purpose of this prototype, we include \change{eight} data sources across a variety of familiar topics such as sales~\cite{superstore,coffeesales}, sports~\cite{rgriffin_2018}, world events~\cite{covid-ca}, entertainment~\cite{bansal_2021}, and civic issues~\cite{us-crimes, housing}. \change{The datasources varied in the number of attributes as well as their cardinality, including 4-20 columns and $\sim$300-28,000 rows.}

% COVID-19~\cite{covid-ca}, college admissions, movies~\cite{bansal_2021}, coffee sales~\cite{coffeesales}, Superstore~\cite{superstore}, Olympics~\cite{rgriffin_2018}, housing~\cite{housing}, and crimes in the U.S.~\cite{us-crimes}.


\subsection{Data Repositories and Metadata}
\label{sec:metadata}
Unlike traditional document search, data sources and visualizations tend to be text-sparse, with limited searchable text content. Hence, \olio~augments the data repositories with additional metadata and semantics that helps the system's understanding and interpretation of the search queries. Specifically, attributes and values in the data sources are linked to ontological concepts, including synonyms (e.g., `film' and `movie')~\cite{thesaurus} and related terms (e.g., `theft,' `burglary,' and `crime')~\cite{word2vec}. The system includes a small hierarchy of hypernyms and hyponyms, from Wordnet~\cite{wordnet}, whose depth typically ranges up or down to two hierarchical levels (e.g., $[`beverage,' `drink'] \rightarrow [`espresso,' `cappuccino']$).
The metadata also includes data types (i.e., `text,' `date,' `Boolean,' `geospatial,' `temporal,' and ‘numeric’) and attribute semantics, such as currency type (e.g., United States Dollar). This information could also be inferred using existing data pattern matching techniques~\cite{pytheus,adelfio:2013,potterswheel}. The metadata also identifies attributes that are measures (i.e., attributes that can be measured, aggregated, or used for mathematical operations) and dimensions (i.e., fields that cannot be aggregated except as count). This final set of metadata information is then added to the semantic search framework.

The pre-authored content is a set of $75,000$ visualizations sourced from Tableau Public~\cite{tableau2023public}, a free community-based platform. The topics of the visualizations are reflective of that demographic of users and include themes such as natural calamities, health, world events, financial news, entertainment, and sports, for example.

Given the XML visual specification of the Tableau workbooks, the system traverses the DOM structure and indexes any text metadata that can be extracted from the visualizations, similar to techniques described in~\cite{d3deconstruction}. Extracted metadata includes the visualization title, caption, tags, description, author name, and profile, the visualization marks encoded in the visualization, and the visualization type. To support design search for recognizing visualization types mentioned in the search query (\textbf{DC2}), we include a general list of visualization types and their linguistic variants in the semantic search framework, as shown in Figure~\ref{fig:viztypes}.

% Figure environment removed


While we focused on CSV data sources and Tableau visualizations, the architecture for \olio~is extensible to include any new or additional data repositories, including D3 and Vega-lite charts, and knowledgebase articles, for example.

\noindent We now describe the rest of \olio's system components in detail.


\subsection{Query Classifier}
 \olio~takes as input an NL search query that is passed to the \textit{query classifier}. The classifier supports federated query search~\cite{Shokouhi2011FederatedS}, which is the process of distributing a query to multiple search repositories and combining results into a single, consolidated search result. Thus, for users, it appears as if they were interacting with a single search instance (\textbf{DC1}). In this context, a user can search \olio~over heterogeneous data repositories (i.e., both data sources and visualizations) without having to change or modify how they structure the query input. The query classifier passes the search tokens to a \emph{parser} and the \emph{data source search index} (which is part of the semantic search framework) and determines if ~\olio~needs to generate a Q\&A search to dynamically generate visualization responses, or simply general search that supports both exploratory and design searches. Algorithm 1 describes the query classification process. At a high level, the query classifier passes the query tokens to the parser (line 7) to determine if the query contains any analytic intents such as aggregation, correlation, temporal, or geospatial expression (refer to Section \ref{sec:parser} for more details). The query classifier also passes the query tokens to the semantic search framework (refer to Section \ref{sec:semanticsearch} for more details) to determine if the query tokens match fields in any of the data sources (e.g., `prices' $\rightarrow$ \texttt{Price} in the housing data source) and the normalized match score is greater than a predetermined threshold (line 10). In practice, we found that $fieldMatch = 2$ and $normMatch = .3$ provided a reasonable threshold for relevant data source matches. If both conditions, i.e., the presence of an analytical intent and the match score meets the threshold criteria, then Q\&A search is first invoked to dynamically generate visualization responses to the given query (line 13); else, general search is invoked to return pre-authored content from the data repository (line 16).

\begin{algorithm}
\caption{Classifies the search behavior based on whether the query contains an analytical intent and there is a match on one or more of the curated data sources in \olio.}
 \flushleft
\begin{algorithmic}[1]
\Function{QueryClassifier}{$query$}
    \LeftComment*{Boolean to check if there is an analytical intent in query}
    \State $hasAnalyticalIntent \leftarrow False$
    \LeftComment*{Boolean to check if there is a data source match}
    \State $hasDSMatch \leftarrow False$
    \LeftComment*{Contains the match scores for $query$ and each data source, $ds$}
    \State $dsScores \leftarrow getDSScores(query, ds)$
    \LeftComment*{Contains the normalized match scores for $query$ and each data source, $ds$}
    \State $normScores \leftarrow norm(dsScores)$ 
    \LeftComment*{Predetermined thresholds set for field match in $ds$ and $normScores$}
    \State $fieldMatch$, $normMatch$
     \LeftComment{Check if the parsed query contains an analytical intent}    
    \If{$(parseForAnalyticalIntent(query)$}
        \State $hasAnalyticalIntent \leftarrow True$  
    \EndIf
    \LeftComment{Check if the query tokens match fields in $ds$ and normalized match score to $ds$ is greater than a pre-determined thresold}     
    \If{$(dsScores['fields'] > fieldMatch)$ \textbf{and} $(normScores > normMatch))$}
        \State $hasDSMatch \leftarrow True$  
    \EndIf
    \LeftComment{If $query$ has an analytical intent and contains tokens matching a $ds$, invoke Q\&A search before general search, else just invoke general search.}
    \If{($hasAnalyticalIntent$ \textbf{and} $hasDSMatch$)}
        \State $invokeQ\&ASearch(query, ds)$
    \EndIf
        \State $invokeGeneralSearch(query)$
\EndFunction
\end{algorithmic}
\end{algorithm}

% Figure environment removed

\subsection{Parser}
\label{sec:parser}
The parser removes stopwords (e.g., `a', `the') and conjunctions / disjunctions (e.g., `and,' `or') from the search query and extracts a list of n-gram tokens (e.g., ``\textit{Seattle house prices}'' $\rightarrow$ [Seattle], [house], [prices], [house prices], [Seattle house prices], etc.). The parser employs a Cocke-Kasami-Younger (CKY) parsing algorithm ~\cite{cocke1969programming,kasami1966efficient,younger1967recognition} and generates a dependency tree to understand relationships between words in the query. 

The input to the underlying CKY parser is a context-free grammar with production rules augmented with both syntactic and semantic predicates to detect the following analytical intents in the search query:

\begin{tight_itemize}
\item \textbf{Grouping}. Partition the data into categories. E.g., `by' a data attribute.
\item \textbf{Aggregation}. Group values of multiple rows of data together to form a single value
based on a mathematical operation. E.g., `average,' `median,' `count,' `distinct count.'
\item \textbf{Correlation}. Statistical measure of the strength of the relationship between two data attributes (measures). E.g., `correlate,' `relate.'
\item \textbf{Filters and limits}. Finite sets of operators that return a subset of the data attribute's domain. E.g., `filter to,' ‘at least,’ ‘between,’ ‘at most.’ Limits are also a finite set of operators akin to filters that return a subset of the attribute’s domain, restricting up to n rows. E.g., `top,' `bottom.'
\item \textbf{Temporal}. Time and date expressions containing temporal tokens and phrases. E.g., `over time,' `year,` `in 2020', `when.'
\item \textbf{Geospatial}. Geospatial expressions referring to location and place. E.g., `in Canada,'  `by location,' `where.'
\end{tight_itemize}

To help with detecting data attributes and values along with the intents, the parser has access to the set of curated data sources and their metadata. The parser then compares the n-grams to available data attributes looking for both syntactic (e.g., misspellings) and semantic similarities (e.g., synonyms) using the Levenshtein distance~\cite{yujian2007normalized} and the Wu-Palmer similarity score~\cite{wu1994verbs}, respectively (\textbf{DC2}). If the parser detects one or more of the aforementioned analytical intents, it returns the intent(s) along with its corresponding data attributes and values to the query classifier. 


\subsection{Semantic Search Framework}
\label{sec:semanticsearch}
The semantic search framework primarily comprises two phases: indexing and searching content and metadata in the data repositories. This two-phase process applies to content in the data repositories, i.e., both the curated data sources and visualization content. Figure~\ref{fig:semanticsearchframework} illustrates the pipeline of the semantic search framework.

\subsubsection{Indexing}
The indexing phase creates indices for each of the data repositories (data sources and visualization content) along with their metadata to support federated search in \olio~(\textbf{DC1}).

Given a data source and visualization content with associated metadata (i.e., attributes, data values, chart type, author name), each file is represented as a document vector, $x_i$, where:
\useshortskip
\begin{equation}
    \mathcal{X} = \{x_1, x_2, ..., x_n\} 
\end{equation}

We also store n-gram string tokens from these document vectors to support partial and exact matches in the system (\textbf{DC2}):
\useshortskip
\begin{equation}
\mathcal{S} = \{s_1, s_2, ...s_n\}
\end{equation}

where $s_i = \varepsilon(x_i)$ for some encoder, $\varepsilon$ that converts the document vectors into a collection of string tokens of cardinality $n$. The original vectors $\mathcal{X}$ and encoded tokens $\mathcal{S}$ are stored in the semantic search engine index by specifying the \emph{mapping} of the content, i.e., defining the type and format of the fields in the index. \olio~stores the text as keywords in the index, supporting exact-value search, fuzzy matching to handle typos and spelling variations, and n-n-grams for phrasal matching. A scoring algorithm, tokenizers, and filters are specified as part of the search index \emph{settings} to determine how the matched documents are scored with respect to the input query and the handling of tokens, such as the adding of synonyms from a thesaurus, removal of stopwords (e.g., `a,' 'the,' for') and duplicate tokens, and converting tokens to lowercase. The complete configuration specification is provided in supplementary material.
%Figure~\ref{fig:indexmappings} shows a JSON snippet for how we specify the mappings and settings for the search index.  

% % Figure environment removed

\subsubsection{Search}
\label{sec:search}
Conceptually, the search phase has two steps: retrieval and ranking. Given an input query, $q$, that is represented as a query vector, $\hat{q}$ with query tokens $q_1, q_2, ..., q_j$; we encode the vector into string tokens, $\hat{s} = \varepsilon(\hat{q})$ using the same encoder, $\varepsilon$ from the indexing phase. The search process retrieves the most relevant $r$ document vectors, $\mathcal{R} = \{x_{1}, x_{2}, ... x_{r}\}$ as candidates based on the amount of overlap between the query string token set $\hat{s}$ and the document string tokens in $\{s_1, s_2, ..., s_n\}$. More specifically, the scoring function $r_{max}$ maximizes search relevance by computing:
\useshortskip
\begin{equation}
    \{x_1, x_2, ..., x_r\} = {r_{max}}_{i \in \{1, 2, ..., n\}} |\hat{s} \cap s_i |
\end{equation}

\noindent\olio~then ranks the vectors in the candidate search result set, $\mathcal{R}$ based on $BM25$ scoring~\cite{manning2008introduction} with respect to the query vector, $\hat{q}$. BM25 is essentially a bag-of-words retrieval scoring function that ranks documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a preferred metric for computing similarities between vectors as the method corrects for variations in vector magnitudes resulting from uneven-length documents~\cite{manning2008introduction}. Given  $\hat{q}$, the BM25 score of a document vector, $x_i$ is:

\useshortskip
\begin{equation}
     {BM25}(\hat{q}, x_i) = \Sigma^{n}_{i=1}IDF(q_j).\frac{f(q_j, x_i).(k_1 + 1)}{f(q_j, x_i) + k_1. (1 - b + b . \frac{|x_i|}{avgdl})}
\end{equation}

where $f(q_j, x_i)$ is the number of times that  $q_{j}$ occurs in the document vector, $x_i$ and $avgdl$ is the average document vector length in the search index. $k_{1}$ and $b$ are constants to further optimize the scoring function. In practice, we have found that $k_1 \in [1.2,2.0]$ and $b = 0.75$ tend to provide reasonable ranking behavior. The Inverse Document Frequency, $IDF$, measures how often a term occurs in all of the documents and ranks unique terms in documents higher. It is computed as:

\useshortskip
\begin{equation}
IDF = ln(1 + \frac{(docCnt - f(q_j) + 0.5)}{f(q_j) + 0.5}
\end{equation}

where $docCnt$ is the total number of documents that have a value for the given query token, $q_j$ and $f(q_j)$ is the number of documents that contains the $i^{th}$ query term.

The $BM25$ scoring function sorts the vectors in descending order of normalized $BM25$ scores, $b \in [0,1]$, i.e., the higher the score, the higher the rank, creating the final ranked search result set,  $\mathcal{T}$, ranked based on the minimum difference between the query and each of the document vectors:

\useshortskip
\begin{equation}
    \mathcal{T}_{i \in \{i_i, i_2,...,i_r\}} = min(|\hat{q} - x_i|)
\end{equation}

% % Figure environment removed


The search request is then passed to the Elasticsearch server to compute Equations 3 and 4 and the system returns a ranked result set of either data sources (used for Q\&A) or visualization content used for both exploratory and design search scenarios.


\subsection{Q\&A Module}


% Figure environment removed


The \emph{Q\&A module} interprets the analytical intent expressed in the input search queries and dynamically generates visualization responses based on the list of top-matched data source(s) returned from the semantic search framework, as described in Section~\ref{sec:semanticsearch}. The module accepts tabular CSV datasets for the top-matched data source(s) as input, and all the visualizations in the tool are created using Vega-Lite~\cite{satyanarayan2016vega} and D3~\cite{2011-d3}. 

The interface and functionality for Q\&A search in \olio~is similar to that of NLIs for visual analysis~\cite{datatone,eviza,orko} with a few extensions that are inherent to the Q\&A behavior in the context of semantic search. For instance, the interface displays text showing a match (if any), to one or more data sources, along with a drop-down menu of the matched data sources  (\textbf{DC3}). A visualization is rendered based on attributes, values, and the analytical intent in the query, along with a text summary describing the visualization (refer to Figure~\ref{fig:teaser}A). A user can peruse the drop-down list of other data source alternatives, along with their corresponding percentage match scores (as computed in Section~\ref{sec:search}), and choose to switch to another data source in the drop-down list as shown in Figure~\ref{fig:qa-special-cases}A. In cases where there is a match to a data source for the query, but the tokens in the query do not resolve to valid attributes and values within the data source, \olio~displays suggested queries for the data source (\textbf{DC3}), shown in Figure~\ref{fig:qa-special-cases}B. These query suggestions are generated using a template-based approach presented by Srinivasan and Setlur~\cite{srinivasan2021snowy} that is based on a combination of attributes from the data source and data interestingness metrics.

The visualization generation process for Q\&A search supports three encoding channels (\texttt{x}, \texttt{y}, \texttt{color}) and four mark types (\texttt{bar}, \texttt{line}, \texttt{point}, and \texttt{geoshape}). These marks and encodings support the dynamic generation of bar charts, line charts, scatterplots, and maps that cover the range of analytic intents described in Section~\ref{sec:parser}. \olio~selects the default visualization using a simplified version of the Show Me system~\cite{mackinlay2007show}, employing similar rules to determine mark types based on the mappings between the visual encodings and attribute data types (e.g., showing a scatterplot if two quantitative attributes are mapped to the \texttt{xy}-channels and showing a line chart if a temporal attribute is visualized on the \texttt{x}-axis with a quantitative attribute on the \texttt{y}-axis).

Finally, \olio~displays a dynamic text summary describing the generated visualization (\textbf{DC3}). While template-based approaches~\cite{kim:2021,fasciano-lapalme-1996-postgraphe,mittal:1995} are viable options for the summary generation process, we chose to employ a large language model (LLM)-based approach~\cite{chatgpt} to explore its capabilities and better understand its limitations.
We initially attempted to pass the chart data as-is to ChatGPT to generate a description. However, we found the model was oftentimes generating wrong statistics or even hallucinating depending on the data domain context. To overcome these challenges but still provide an eloquent description, we instead opted for a combined approach of using both basic statistical computations and an LLM.

% The input to the LLM model is a prompt containing a statistical description returned with the generated visualization specification.
Specifically, the input to ChatGPT is a prompt containing a statistical description that is extracted from the generated visualization using a set of heuristics defined in prior data insight recommendation tools~\cite{demiralp2017foresight,cui2019datasite,srinivasan2018augmenting}.
For instance, for bar charts, we identify the min/max and average values; for scatterplots, we compute the Pearson's correlation coefficient~\cite{freedman2007statistics}, and so on. Consider the search query, \textit{``sales by region,''} which results in a bar chart displaying \texttt{Sales} across four \texttt{Region}s. An example of the statistical description, $keyStats$ from this bar chart is:
\begin{verbatim}
Region: Central has a minimum value of $220 for Sales
Region: South has the maximum value of $240 for Sales
Average Sales across Region is: $230
\end{verbatim}

\noindent The corresponding prompt to ChatGPT then becomes \textit{Rephrase the following input more eloquently: \escape{n}`\$\{keyStats\}\escape{n}'}, which ultimately generates the text summary: \textit{``The Sales in Central Region had the lowest value of \$220,  while South Region had the highest value of \$240. The average Sales across all Regions was \$230.}

\subsection{General Search Module}
% Figure environment removed

The \emph{general search module} displays thumbnails of pre-authored visualization content along with information such as title and date. The thumbnail images are hyperlinked to the corresponding Tableau Public workbook URLs if users choose to download or analyze the visualization in more detail. The module enables two types of searches: exploratory and design (\textbf{DC1}). Exploratory search returns visualization results based on keyword matches (\textbf{DC2}) in the input search query (e.g., \textit{``world population''} in Figure~\ref{fig:exploratory-search-examples}). Design search is a special form of exploratory search that returns visualization results specifically for keywords containing tokens referring to visualization types, their synonyms, and related concepts (e.g., \textit{``covid correlations''}) (\textbf{DC2}). Figure~\ref{fig:design-search-examples} shows examples of design search results in \olio.

% Figure environment removed