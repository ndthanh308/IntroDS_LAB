\section{Preliminaries~\label{sec:pre}}

\subsection{Notations}
\noindent Let $\mathcal{G},\mathcal{Y}$ represent input and label space respectively. $f_\phi(\cdot)=p_\omega\circ g_\theta(\cdot)$ represents graph predictor which consists of a GNN encoder $g_\theta(\cdot)$ and a classifier $p_\omega(\cdot)$. The graph predictor $f_\phi: \mathcal{G}\rightarrow\mathcal{Y}$ maps instance $G=(A,X) \in \mathcal{G}$ to label $Y \in \mathcal{Y}$ where $A \in \mathbb{R}^{N \times N}$ is the adjacent matrix and $X \in \mathbb{R}^{N \times D}$ is the node attribute matrix. Here, $N$, $D$ denote the number of nodes and attributes, respectively. To measure the discrepancy between the prediction and the ground-truth label, a loss function $\ell_{\text{sup}}$ is used (\eg, cross-entropy loss, mean square error). For unsupervised learning, a pretext loss $\ell_{\text{unsup}}$ is applied (\eg, InfoNCE loss~\cite{cpc}).
\subsection{Problem Statement}
\textbf{Graph OOD Generalization:} Given training set $\mathcal{G}^{\text{train}}={(G_i,Y_i)}_{i=1}^N$ containing $N$ instances that are drawn from training distribution $P_{\text{train}}(G,Y)$. In the supervised setting, it aims to learn an optimal graph predictor $f^*$ that can exhibit the best generalization performance on the data sampled from the test distribution:
% It has to note that in OOD scenario $P_{\text {test }}(\mathcal{G}, Y) \neq P_{\text {train }}(\mathcal{G}, Y)$ which is different to IID assumption:
\begin{equation}
f_\phi^*=\arg \min _{f_\phi} \mathbb{E}_{G, Y \sim P_{\text {test }}}\left[\ell_{\text{sup}}\left(f_\phi(G), Y\right)\right],
\end{equation}
where $P_{\text {test }}(G, Y) \neq P_{\text {train }}(G, Y)$ means there exists a distribution shift between training and testing sets. Such a shift may lead to the optimal predictor trained on the training set (\ie, minimizing $\mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(f_\phi(G), Y\right)\right]$) can not generalize on the testing set.
% \paragraph{Unsupervised graph OOD generalization} Given an unlabelled training data set $\mathcal{G}^{train}=\{(G_i)\}_{i=1}^N$ containing N instances from training distribution $P_{\text{train}}(\mathcal{G})$. In the unsupervised setting, to learn an optimal graph encoder $g^*$ that performs well on the pretext task, for instance, $\mathbb{E}_{\mathcal{G}, Y \sim P_{\text {train }}}\left[\ell_{\text{unsup}}\left(g_\theta(\mathcal{G}), g(\mathcal{G}^-), g(\mathcal{G}^+)\right)\right]$ where $\mathcal{G},\mathcal{G}^+$ are sampled from joint distribution and $\mathcal{G},\mathcal{G}^-$ are sampled from the product of marginal distributions. In order to evaluate the pre-trained model, it is common to use linear evaluation which we fix the pre-trained model and additionally train a linear classifier $p$ with labeled data set $D~P_{\text {train }}(\mathcal{G}, Y)$. And lastly, we test the graph predictor  $p\circ g$ on the test data set $P_{\text {test }}(\mathcal{G}, Y)$, where $P_{\text {test }}(\mathcal{G}, Y) \neq P_{\text {train }}(\mathcal{G}, Y)$. 
%\footnote{For notation uncluttered, we assume only one graph in the training set which is commonly found in the node classification task.}

\noindent\textbf{Self-Supervised Graph Learning:} 
The most representative self-supervised graph learning method is graph contrastive learning (GCL) recently~\cite{rosa,grace,graphcl,costa,bgrl,dgi,mvgrl}. In this subsection, we focus on introducing the main components and formulations used in GCL. GCL hopes to learn a generic feature extractor without human annotations through a pre-defined pre-text task (\eg, instance discrimination). Most of them~\cite{rosa,grace,graphcl,costa,bgrl,dgi,mvgrl} consist of three main components: (1) view generation, (2) view encoding and (3) representation contrasting. The pipeline is depicted in Figure~\ref{fig:contrast}. Firstly, given an input graph $G$, in the phase of view generation, two graph augmentation $\tau_{\alpha}(\cdot|G),\tau_{\beta}(\cdot|G)\sim\mathcal{T}$\footnote{$\mathcal{T}$ is an augmentation pool which can consist of various augmentation techniques like edge dropping, attribute masking, subgraph and etc~\cite{graphcl}.} will be applied on the original graph to generate the augmented views $G_\alpha, G_\beta$. Secondly, these augmented graphs will be fed into GNN-based encoding models~\cite{gcn,gat,gin,sage} to obtain node representations $U, V \in \mathbb{R}^{N \times D}$.
% We use $u,v \in \mathbb{R}^{1 \times D}$ to represent the random variables of node representation. 
Lastly, an unsupervised loss function will be applied to these representations. Taking the commonly used InfoNCE loss~\cite{cpc} as an example:
\begin{equation}
\begin{aligned}
\mathcal{L} _ {MI} & \left(U, V\right)=-\mathbb{E}_{u,v \sim P_{U,V}}\left[\operatorname{sim}\left(u, v\right)\right] \\
+ & \mathbb{E}_{u \sim P_U} \log \mathbb{E}_{v^- \sim P_V} \left[  e^{\operatorname{sim}\left(u, v^-\right)}\right],
\end{aligned}
\label{equ:con}
\end{equation}
where $\operatorname{sim}(u,v)$ is a score function (\eg,  cosine similarity), $u, v=g_{\theta}(G_{\alpha}),g_{\theta}(G_{\beta})$. More specifically, it can be formulated:
\begin{equation}
\begin{aligned}
\mathcal{L} _ {MI} & \left(g_\theta;\mathcal{G},\pi\right)=-\underset{G \in \mathcal{G}}{\mathbb{E}} \mathbb{E} _{\tau_\alpha, \tau_\beta \in \mathcal{T}}\left\|g_\theta(\tau_\alpha(G))-g_\theta\left(\tau_\beta(G)\right)\right\|^2 \\
+ & \underset{G \in \mathcal{G}}{\mathbb{E}} \log \underset{G^{\prime} \in \mathcal{G}}{\mathbb{E}}\mathbb{E}_{\tau^{\prime} \in \mathcal{T}}\left[  e^{\left\|g_\theta\left(\tau_\alpha\left(G\right)\right)-g_\theta\left(\tau^{\prime}(G^\prime)\right)\right\|^2}\right]
\end{aligned}
\label{equ:con}
\end{equation}
For simplicity, we assume that $\|g_\theta(G)\|=1$ for every $G \in \mathcal{G}$. By minimizing this loss, the former term (aka alignment loss $\mathcal{L}_{\text{align}}$~\cite{wang2020understanding}) pulls positive pairs ($u,v \sim P_{U,V}$) together by encouraging their similarity, and the latter term (aka uniformity loss $\mathcal{L}_{\text{uniform}}$~\cite{wang2020understanding}) pushes negative pairs ($u,v^- \sim P_U \otimes P_V$) apart. In order to evaluate the pre-trained model, an additional trainable linear classifier (\ie, one-layer MLP) will be built on top of the frozen encoder:

\begin{equation}
p_\omega^*=\arg \min _{p_\omega} \mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(p_\omega \circ g_\theta^*(G), Y\right)\right],
\end{equation}
where $g_\theta^*(\cdot)$ is obtained by minimizing Equation~\ref{equ:con} without labels. For evaluating pre-trained model, the optimal graph predictor $f_\phi^* =  p_\omega^* \circ g_\theta^*$ will be applied to testing data. In the previous works, they assume training data and testing data are sampled from an identical distribution. So some spurious patterns extracted by pre-trained models can benefit downstream dataset.

\zy{\noindent\textbf{Self-Supervised OOD Generalization on Graphs:}}
% The design of most existing GCL methods does not consider the OOD scenario. In other words, few works explore the self-supervised OOD generalization on graphs. For achieving this aim, we hope to learn a feature extractor $g_\theta^\star$ that can dig invariant patterns rather than spurious features (\eg, variant features with environments) which is more strict than in the last subsection. So, we need to modify the main components in current GCL methods which will be discussed in Section~\ref{sec:method}.
Existing GCL methods generally lack consideration of the OOD scenario. In other words, few works have specifically explored self-supervised OOD generalization on graphs. To address this limitation, our goal is to learn a feature extractor $g_\theta^\star$ that can effectively capture invariant rationales while avoiding spurious features, such as those influenced by varying environments, aiming for downstream node classification task. 
% For instance, taking the molecule property prediction as downstream task, we hope the encoder can recognize the functional group which will decide the property of molecule.  
We assume $\Phi(G)$ is invariant rationales of input instance $G$ which is stable in different environments (augmentations) following invariance assumption~\cite{ood-survey,good-survey}. We hope the pre-trained model can achieve this objective\footnote{We assume the augmentation function will not change the semantic labels of the original input here.}:
\begin{equation}
    g_\theta^{\star}(G_e)=g_\theta^{\star}(G_{e^\prime}) = \Phi(G),  \quad \forall e,e^\prime \in \operatorname{supp}\left(\mathcal{E}_{t r}\right)
\label{equ:goal}
\end{equation}
where $\mathcal{E}_{t r}$ denotes the set of training environments and $\mathbb{E}\left[Y \mid \Phi\left(G_e\right)\right]=\mathbb{E}\left[Y \mid \Phi\left(G_{e^{\prime}}\right)\right]$ which means invariant rationales exhibiting the stable correlation with semantic labels across different environments. However, during pre-training, we have no access to labels under self-supervised setting. Here, we build a connection between pre-text loss $\mathcal{L}_{M I}\left(g_\theta ; \mathcal{G}, \pi\right)$ and downstream loss $\mathcal{R}\left(g_\theta ; \mathcal{G}^{\operatorname{tar}}\right)$~\cite{arcl,huang2023towards}:
\begin{equation}
\begin{aligned}
\mathcal{R}\left(p_\omega \circ g_\theta ; \mathcal{G}_\pi\right) \leq 
& c\|p_\omega\| \sqrt{K \sigma}\left(\mathcal{L}_{\text {align }}(g_\theta ; \mathcal{G}, \pi)\right)^{\frac{1}{4}} \\ 
& +\|p_\omega\| \zeta (\sigma, \delta) \\
& +\sum_{k=1}^K \mathcal{G}_\pi\left(C_k\right)\left\|e_k-p_\omega \circ \mu_k\left(g_\theta ; \mathcal{G}_\pi\right)\right\|
\end{aligned}
\label{equ:cl_down}
\end{equation}
where $c$ is an absolute constant, $\zeta (\sigma, \delta)$ is of a constant which only depends on $(\sigma, \delta)$-augmentation~\cite{huang2023towards}, $C_k \subseteq \mathcal{G}$ is the set of all points in class $k$, $\mu_k(g_\theta ; \mathcal{G}):=\mathbb{E}_{G \sim \mathcal{G}} g_\theta(G) \text { for } k \in[K]$, $\pi$ is a certain distribution for augmentation $\tau \sim \mathcal{T}(\pi)$, the complete deduction and more illustrations can be found in Appendix. 
% The first term in Equation~\ref{equ:cl_down} is the alignment loss which is optimized during pre-training on $\mathcal{G}$. The second term is determined solely by $(\sigma,\delta)$ quantity of the data augmentation. Larger $\sigma$ and smaller $\delta$ induce smaller $\zeta(\sigma,\delta)$. The third term is related to the linear layer $p$ on top of $g$, and will be minimized in downstream training, where each class center is converted to the corresponding ground-truth label. If the regularization term $L_{reg}$ is appropriately chosen, the class centers can be distinguished from each other, and the third term can be reduced to 0 by $h$.
The first term of Equation~\ref{equ:cl_down} is the alignment loss optimized during pre-training on $\mathcal{G}$. 
The second term is determined by the $(\sigma,\delta)$ quantity of the data augmentation, with larger $\sigma$ and smaller $\delta$ resulting in smaller $\zeta(\sigma,\delta)$.
The third term is associated with the linear layer $p$ and is minimized in downstream training. The class centers can be distinguished by choosing an appropriate regularization term $\mathcal{L}_{\text{uniform}}$, leading to the third term becoming 0 via $h$.
In short, Equation~\ref{equ:cl_down} implies that contrastive learning on distribution $\mathcal{G}$ with augmentation function $\tau \sim \mathcal{T}(\pi)$ essentially optimizes the supervised risk on the augmented distribution $\mathcal{G}_\tau$. So, even without labels, we can approach the goal formulated as Equation~\ref{equ:goal} during pre-training to some extent, through modifying the main components in current GCL methods which will be discussed in Section~\ref{sec:method}.

% The above equation means $g_\theta^{\star}$ can extract invariant rationales across different environments. In order to achieve this, it is necessary to modify the main components in current GCL methods which will be discussed in Section~\ref{sec:method}.

%The linear classifier has risks to learn spurious correlation which may destroy the node embeddings. In order to avoid this, \cite{shi2022robust} adopts partial test data rather than training data for training linear classifier, but it is unfair to compare with supervised methods. So in this work, we still use training data to learn the linear head. 
For evaluating the pre-trained models, the linear evaluation protocol mentioned in the last subsection will be used: 
\begin{equation}
p_\omega^*=\arg \min _{p_\omega} \mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(p_\omega \circ g_\theta^\star(G), Y\right)\right],
\end{equation}
where pre-trained model $g_\theta^\star$ is frozen, and the linear classifier $p_\omega$ is trained with training data\footnote{To maintain fairness in the evaluation, we choose to train the linear head using the training data, rather than using partial test data as done in \cite{shi2022robust}. Although the linear classifier has risks of learning spurious correlations that could impact the node embeddings.}. The difference with the last subsection is $P_{\text {test }}(G, Y) \neq P_{\text {train }}(G, Y)$ which means there exist distribution shifts between training and testing data. This implies that the feature extractor must extract more invariant features to mitigate the risk of overfitting on the training data and ensure better generalization performance under distribution shifts.

% This requirement is more stringent compared to the previous subsection. To achieve this, we propose modifications to the main components of current GCL methods, which will be discussed in detail in Section~\ref{sec:method}.
% The design of existing GCL methods does not consider the OOD scenario. In other words, few works explore the self-supervised OOD generalization on graphs. In this subsection, we will simply analyze the drawbacks of current GCL methods while facing OOD scenario based on two main components and build a connection between GCL methods and OOD methods. For the view generation, inspired by invariant learning~\cite{irm,groupdro,arcl}, the alignment loss can be reformulated as:
% \begin{equation}
% \mathcal{L}_{\mathrm{align}^*}(g_{\theta} ; \mathcal{G}):=\underset{G \in \mathcal{G}}{\mathbb{E}} \sup _{\tau, \tau^{\prime} \in \mathcal{T}}\left\|g_{\theta}(\tau(G))-g_{\theta}\left(\tau^{\prime}(G)\right)\right\|^2,
% \end{equation}
% which replaces the expectation operator in Equation~\ref{equ:con} with supremum operator. This objective is a uniform version of original alignment loss which can make the representation varies less under different augmentation functions and result in more invariant features. In order to achieve this goal, we can adjust the view generation to approximately approach the supremum operator which will be further discussed in Section~\ref{sec:aug}. 

% Regarding the representation contrasting component, the original contrastive loss aims to maximize the lower bound of mutual information between positive pairs which will contain redundant information. Motivated by Information Bottleneck~\cite{dib,gib}, we reformulate Equation~\ref{equ:con} as Equation~\ref{equ:cmi_tmp} which hopes to exclude redundant information and find minimal sufficient representation\footnote{For self-supervised methods, their aim is to find a generic feature extractor that is agnostic to downstream tasks. But in this work, we focus on node-level tasks. If given label information, there exists explicit redundant information can be excluded.}:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_{\mathrm{rob}}&=\argmax I_{\theta}(U;V) - \gamma \underbrace{I_{\theta}(U;V \mid \hat{Y})}_{\text{Redundant info}}
% \end{aligned}
% \label{equ:cmi_tmp}
% \end{equation}
% where $I_\theta(\cdot)$ represents mutual information estimator, $\gamma > 0$ controls the trade-off between compression and performance, and $\hat{Y}$ denotes label\footnote{For unsupervised setting, it denotes pseudo label or protocols.} information of the downstream task.
% By adopting this learning paradigm, the trained model can effectively mitigate overfitting and exhibit increased resilience against distribution shifts. More illustrations can be found in Section~\ref{sec:cmi}.

% As for evaluation, we also adopt the linear evaluation protocol~\cite{simclr}. Using the frozen pre-trained model to extract node embeddings, and using training data is train an additional linear classifier to evaluate the performance:
% \begin{equation}
% p_\omega^*=\arg \min _{p_\omega} \mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(p_\omega \circ g_\theta^\star(G), Y\right)\right],
% \end{equation}
% The difference with the last subsection is $P_{\text {test }}(G, Y) \neq P_{\text {train }}(G, Y)$ which means there exist distribution shifts between training and testing data. This implies that the feature extractor must extract more invariant features to mitigate the risk of overfitting on the training data and ensure better generalization performance under distribution shifts.

% Contrastive learning seems to have better generalization on OOD test data by maximizing the lower bound of mutual information of positive pairs. Specifically, considering the augmented data from different domains, contrastive learning will align positive pairs from different domains, leading to extracting more casual patterns. Take the citation network as an example, where the categorization of scientific publications should not depend on the node degree. By employing augmentation techniques such as edge dropping, if there is a degree shift in the downstream test set, contrastive learning methods can achieve favorable generalization performance. Empirically, \cite{shi2022robust} found contrastive learning methods are more robust to distribution shift compared to supervised method~\cite{erm} in the computer vision domain. In this work, we will investigate how current graph contrastive learning methods perform on the graph OOD test set and provide a model-agnostic recipe for improving OOD generalization of the current contrastive methods which is coined as MARIO.
% By minimizing this loss, we can maximize the mutual information between positive pairs meanwhile maximize the mutual information between the input graph and representations~\cite{}. A detailed illustration can be found in Appendix. 


% Figure environment removed