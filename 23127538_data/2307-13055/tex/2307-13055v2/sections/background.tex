\section{Background and Problem Formulation~\label{sec:pre}}
In this section, we will start with the notations we use throughout the rest of the paper~(Sec.~\ref{sec:notations}); then we introduce the problem definition and background of graph OOD generalization~(Sec.~\ref{sec:graph-ood}) and graph self-supervised learning methods~(Sec.~\ref{sec:ssl-graph}); finally, we formalize the problem of graph contrastive learning for OOD generalization~(GCL-OOD) in Sec.~\ref{sec:gcl-ood}. 

\subsection{Notations}\label{sec:notations}
\noindent Let $\mathcal{G},\mathcal{Y}$ represent input and label space respectively. $f_\phi(\cdot)=p_\omega\circ g_\theta(\cdot)$ represents graph predictor which consists of a GNN encoder $g_\theta(\cdot)$ and a classifier $p_\omega(\cdot)$. The graph predictor $f_\phi: \mathcal{G}\rightarrow\mathcal{Y}$ maps instance $G=(A,X) \in \mathcal{G}$ to label $Y \in \mathcal{Y}$ where $A \in \mathbb{R}^{N \times N}$ is the adjacent matrix and $X \in \mathbb{R}^{N \times D}$ is the node attribute matrix. Here, $N$, $D$ denote the number of nodes and attributes, respectively. To measure the discrepancy between the prediction and the ground-truth label, a loss function $\ell_{\text{sup}}$ is used (\eg, cross-entropy loss, mean square error). For unsupervised learning, a pretext loss $\ell_{\text{unsup}}$ is applied (\eg, InfoNCE loss~\cite{cpc}). And we use $\mathcal{T}$ as augmentation pool, the augmentation function $\tau$ is randomly selected from $\mathcal{T}$ according to some distribution $\pi$. Let $\mathcal{G}^{\text{tar}}$ denote downstream dataset. 

\subsection{Graph OOD Generalization}\label{sec:graph-ood}
\textbf{Problem definition.} Given training set $\mathcal{G}^{\text{train}}={(G_i,Y_i)}_{i=1}^N$ containing $N$ instances that are drawn from training distribution $P_{\text{train}}(G,Y)$. In the supervised setting, it aims to learn an optimal graph predictor $f^*$ that can exhibit the best generalization performance on the data sampled from the test distribution:
\begin{equation}
f_\phi^*=\arg \min _{f_\phi} \mathbb{E}_{G, Y \sim P_{\text {test }}}\left[\ell_{\text{sup}}\left(f_\phi(G), Y\right)\right],
\end{equation}
where $P_{\text {test }}(G, Y) \neq P_{\text {train }}(G, Y)$ means there exists a distribution shift between training and testing sets. Such a shift may lead to the optimal predictor trained on the training set (\ie, minimizing $\mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(f_\phi(G), Y\right)\right]$) can not generalize on the testing set.

\noindent\textbf{Related works.} 
Out-of-distribution generalization (aka domain generalization) algorithms~\cite{9782500,8496795,ood-survey,good-survey}, developed to handle unknown distribution shifts, have gained significant attention due to the growing need for handling unseen data in real-world scenarios\footnote{There are similar topics like domain adaptation~\cite{zhu2021shift,wu2020unsupervised} and transfer learning~\cite{zhu2021transfer,5288526}. They typically assume access to part of test domains to adapt GNNs. In contrast, domain generalization does not use any samples from test domains, setting it apart from these approaches.}. Specifically, robust optimization~\cite{groupdro,hu2018does}, invariant representation/predictor learning~\cite{irm,sparseirm} and causal approaches~\cite{peters2016causal,heinze2018causal} are proposed to deal with such problems. In this subsection, we will put an emphasis on \emph{invariant representation learning} on graphs because of its more practical assumption and theory.

Graph invariant learning methods extend invariant learning on graph domain which are widely investigated recently~\cite{eerm,gil,dir,gsat}. EERM \cite{eerm}, GIL\cite{gil}, DIR~\cite{dir} rely on environment generators to find invariant predictive patterns with labels. GSAT~\cite{gsat} leverages the attention mechanism and the information bottleneck principle~\cite{dib} to construct interpretable GNNs for learning invariant subgraphs under distribution shifts. But most works focus on graph-level tasks under supervised setting. 

Dealing with node-level tasks without labels is more challenging due to the interconnected graph structure and the large scale of individual graphs as well as the lack of supervision. Training a model with good generalization in this scenario remains under-explored. In this work, we aim to develop an OOD algorithm for node-level tasks without labels, addressing this challenging problem.

\subsection{Graph Contrastive Learning}\label{sec:ssl-graph}
\textbf{Problem definition.} 
Graph contrastive learning (GCL) is a representative self-supervised graph learning method~\cite{rosa,grace,graphcl,costa,bgrl,dgi,mvgrl,9764632}. It consists of three main components: view generation, view encoding, and representation contrasting (Figure~\ref{fig:contrast}). Given an input graph $G$, two graph augmentations $\tau_{\alpha}$ and $\tau_{\beta}$ are used to generate two augmented views $G_\alpha=\tau_\alpha(G)$ and $G_\beta=\tau_\beta(G)$, respectively. A GNN model $g_\theta$~\cite{gcn,gat,gin,sage} is then applied to the augmented views to produce node representations $g_\theta(\tau_{\alpha}(G))\in\mathbb{R}^{N \times D}$.
Lastly, a contrastive loss function is applied to the representations, pulling together the positive pairs  while pushing apart negative pairs. Taking the commonly used InfoNCE loss~\cite{cpc} as an example, the formulation follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{MI}} & \left(g_\theta;\mathcal{G},\pi\right)=-\underset{G \in \mathcal{G}}{\mathbb{E}} \mathbb{E} _{\tau_\alpha, \tau_\beta \sim \pi^2}\left\|g_\theta(\tau_\alpha(G))-g_\theta\left(\tau_\beta(G)\right)\right\|^2 \\
+ & \underset{G \in \mathcal{G}}{\mathbb{E}} \log \underset{G^{\prime} \in \mathcal{G}}{\mathbb{E}}\mathbb{E}_{\tau^{\prime} \sim \pi}\left[  e^{\left\|g_\theta\left(\tau_\alpha\left(G\right)\right)-g_\theta\left(\tau^{\prime}(G^\prime)\right)\right\|^2}\right],
\end{aligned}
\label{equ:con}
\end{equation}
where in the second term, $G^\prime$ denotes a randomly sampled graph from the graph data distribution $\mathcal{G}$, serving as the constraint for non-collapsing representations.
For simplicity, the representation produced by encoder is automatically normalized to a unit sphere, \ie,  $\|g_\theta(G)\|=1, \forall G \in \mathcal{G}$. By minimizing this loss, the former term (aka alignment loss $\mathcal{L}_{\text{align}}$~\cite{wang2020understanding}) pulls positive pairs together by encouraging their similarity, and the latter term (aka uniformity loss $\mathcal{L}_{\text{uniform}}$~\cite{wang2020understanding}) pushes negative pairs apart. 
The quality of the pre-trained graph encoder is then evaluated by the linear separability of the final representations. Namely, an additional trainable linear classifier $p_\omega$ is built on top of the frozen encoder:
\begin{equation}
p_\omega^*=\arg \min _{p_\omega} \mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(p_\omega \circ g_\theta^*(G), Y\right)\right],
\end{equation}
where $g_\theta^*(\cdot)$ is obtained by minimizing Equation~\ref{equ:con} without labels. For evaluating pre-trained model, the optimal graph predictor $f_\phi^* =  p_\omega^* \circ g_\theta^*$ will be applied to testing data. 

\noindent\textbf{Related work.} Recently, contrastive methods in the graph domain have shown remarkable progress, surpassing supervised methods even without human annotations in certain cases~\cite{gmae,dgi,graphcl,grace,rosa,mvgrl,bgrl,zhu2023sgl,zhang2023structure,raft,9770382}. These self-supervised methods mostly assume an in-distribution (ID) setting, where the training and test data are sampled from the identical distribution. However, in real-world scenarios, where there are distribution shifts between training and test sets, the efficacy of existing methods remains a question. To solve this, RGCL~\cite{rgcl} employs a rationale generator to find casual subgraph for each instance~(graph) as graph augmentation in contrastive learning for further improving the OOD generalization performance. However, RGCL cannot be applied to node-level tasks because the rationale generator cannot be applied to large-scale graphs due to memory consumption and each instance~(node) is intractable to find its rational subgraph.

In this work, we are the first to investigate the robustness of graph self-supervised methods in the face of distribution shifts on node-level tasks. We provide a model-agnostic recipe for improving the OOD generalization of graph contrastive learning methods which does not rely on the choice of models. By addressing this research question, we provide valuable insights and practical guidelines for improving the performance of unsupervised graph contrastive learning with distribution shifts. In the next chapter, we will explicitly formulate the problem of graph contrastive learning for OOD generalization~(GCL-OOD) and pinpoint the corresponding challenges.

\subsection{GCL-OOD: Graph Contrastive Learning for OOD Generalization}\label{sec:gcl-ood}
Suppose $\Phi(G)$ is invariant rationales of input instance $G$ which is stable in different environments (augmentations) following invariance assumption~\cite{ood-survey,good-survey}: 
\begin{equation}
    \mathbb{E}\left[Y \mid \Phi\left(G_e\right)\right]=\mathbb{E}\left[Y \mid \Phi\left(G_{e^{\prime}}\right)\right], \quad \forall e,e^\prime \in \operatorname{supp}\left(\mathcal{E}_{t r}\right),
\end{equation}
where $\mathcal{E}_{t r}$ denotes the set of training environments\footnote{Data in different environments has different data distributions.} and the above equation represents that invariant rationales exhibit predictive invariant (stable) correlations with semantic labels across different environments.

The optimal~(invariant) graph encoder $g_\theta^\star$ achieves the invariant rationales $\Phi(G)$ across all the environments: \footnote{We assume the augmentation function will not change the semantic labels of the original input here.}:
\begin{equation}
    g_\theta^{\star}(G_e)=g_\theta^{\star}(G_{e^\prime}) = \Phi(G),  \quad \forall e,e^\prime \in \operatorname{supp}\left(\mathcal{E}_{t r}\right).
\label{equ:goal}
\end{equation}
% \begin{equation}
%     % g_\theta^{\star}(G_e)=g_\theta^{\star}(G_{e^\prime}) = \Phi(G),  \quad \forall e,e^\prime \in \operatorname{supp}\left(\mathcal{E}_{t r}\right).
%     \mathbb{E}\left[Y \mid g_\theta^{\star}(G_e)\right]=\mathbb{E}\left[Y \mid g_\theta^{\star}(G_{e^\prime})\right]=\mathbb{E}\left[Y \mid \Phi\left(G\right)\right].
% \label{equ:goal}
% \end{equation}
However, during the pre-training of GCL, we have no access to labels under self-supervised setting. Here, we build a connection between pre-text loss $\mathcal{L}_{\text{MI}}\left(g_\theta ; \mathcal{G}, \pi\right)$ and downstream loss $\mathcal{R}\left(g_\theta ; \mathcal{G}^{\operatorname{tar}}\right)$ by upper-bounding referring to~\cite{arcl,huang2023towards}:
\begin{equation}
\begin{aligned}
\mathcal{R}\left(p_\omega \circ g_\theta ; \mathcal{G}_\pi\right) \leq 
& c\|p_\omega\| \sqrt{K \sigma}\left(\mathcal{L}_{\text {align }}(g_\theta ; \mathcal{G}, \pi)\right)^{\frac{1}{4}} \\ 
& +\|p_\omega\| \zeta (\sigma, \delta) \\
& +\sum_{k=1}^K \mathcal{G}_\pi\left(C_k\right)\left\|e_k-p_\omega \circ \mu_k\left(g_\theta ; \mathcal{G}_\pi\right)\right\|,
\end{aligned}
\label{equ:cl_down}
\end{equation}
where $c$ is a positive constant, $\zeta (\sigma, \delta)$ is a set of constants that only depends on $(\sigma, \delta)$-augmentation~\cite{huang2023towards}, $C_k \subseteq \mathcal{G}$ is the set of the data points in class $k$, $\mu_k(g_\theta ; \mathcal{G}):=\mathbb{E}_{G \sim \mathcal{G}}[ g_\theta(G)] \text { for } k \in[K]$. The complete derivation and more illustrations can be found in Appendix~\ref{app:proof_recipe1}. 

The first term in Equation~\ref{equ:cl_down} is the alignment loss optimized during pre-training on $\mathcal{G}$. 
The second term is determined by the $(\sigma,\delta)$ quantity of the data augmentation, with larger $\sigma$ and smaller $\delta$ resulting in smaller $\zeta(\sigma,\delta)$.
The third term is associated with the linear layer $p$ and is minimized in downstream training. The class centers can be distinguished by choosing an appropriate regularization term $\mathcal{L}_{\text{uniform}}$, leading to the third term becoming 0 via $h$.
In short, Equation~\ref{equ:cl_down} implies that contrastive learning on distribution $\mathcal{G}$ with augmentation function $\tau$ essentially optimizes the upper-bound of supervised risk on the augmented distribution $\mathcal{G}_\tau$ resulting in a lower supervised risk. So, even without labels, we can approach the goal formulated as Equation~\ref{equ:goal} during pre-training to some extent, through modifying the main components in current GCL methods which will be discussed in Section~\ref{sec:method}.

For evaluating the pre-trained models, the linear evaluation protocol mentioned in the last subsection will be used: 
\begin{equation}
p_\omega^*=\arg \min _{p_\omega} \mathbb{E}_{G, Y \sim P_{\text {train }}}\left[\ell_{\text{sup}}\left(p_\omega \circ g_\theta^\star(G), Y\right)\right],
\end{equation}
where pre-trained model $g_\theta^\star$ is frozen, and the linear classifier $p_\omega$ is trained with training data\footnote{We train the linear head using the training data which is more appropriate to OOD generalization setting, rather than using partial test data as in \cite{shi2022robust}.}. The difference with the last subsection is $P_{\text {test }}(G, Y) \neq P_{\text {train }}(G, Y)$ which means there exist distribution shifts between training and testing data. A good performance on the test data distribution implies that the feature extractor must have extracted invariant features, mitigating the risk of overfitting on the training data and leading to improved generalization under distribution shifts.

% Figure environment removed

% We train the linear head using the training data to ensure fairness with the supervised setting, rather than using partial test data as in \cite{shi2022robust}. Although the linear classifier has risks of learning spurious correlations that could impact the node embeddings.