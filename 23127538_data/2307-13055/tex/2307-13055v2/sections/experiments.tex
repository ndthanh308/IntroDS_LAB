\section{Experiments}
% \haizhou{Follow the same way of introduction as we did in Section2.}
% \noindent In this section, we will introduce datasets and experimental setups that we used. Then we evaluate our method, other self-supervised methods, and supervised methods under different distribution shifts (\ie, concept shifts and covariate shifts) under common settings (\ie, transductive, inductive settings). It has to note that we focus on node-level tasks (\eg, node classification) in this work. As for graph-level tasks, we leave it as our future work and some simple experiments can be found in Appendix~\ref{app:graph_classification}. 
In this section, we first introduce the experimental setup including datasets, training, and evaluation protocol in Section~\ref{sec:dataset}~and~\ref{sec:unsupervised}. 
% Next, we present our experimental setup and conduct extensive experiments to evaluate our method in Section~\ref{sec:unsupervised}. 
We then perform an ablation study to demonstrate the effectiveness of each proposed component in Section~\ref{sec:ablation}. 
Additionally, we analyze the impact of important hyper-parameters in Section~\ref{sec:sensitivity}. 
Subsequently, we integrate our method with various encoding models, showcasing the model-agnostic nature of our recipe in Section~\ref{sec:other_models}. 
Finally, we provide some qualitative results such as feature visualization in Section~\ref{sec:vis}.
It is important to note that we focus on node-level tasks (\eg, node classification) in this work. As for graph-level tasks, we leave it as our future work, while some simple experiments are also provided in Appendix~\ref{app:graph_classification}.

\subsection{Datasets}\label{sec:dataset}
There exist some benchmarks for evaluating graph out-of-distribution generalization~\cite{good,ji2022drugood,gds}. 
Among them, GOOD~\cite{good} is the most representative and comprehensive benchmark that curates more diverse graph datasets with diverse tasks, including single/multi-task graph classification, graph regression, and node classification involving more distribution shifts (\ie, concept shifts and covariate shifts). Hence in this work, we follow the evaluation protocol proposed in \cite{good}. Furthermore, we validate the effectiveness of our method in the datasets (\ie, Amazon-Photo, Elliptic) that are used in EERM~\cite{eerm}. The statistics and detailed introduction to these datasets can be found in Table~\ref{tab:dataset} and Appendix~\ref{app:datasets}.

\begin{table*}[htp]
\caption{The descriptions of datasets. ``Domain-Level'' means splitting by graphs, ``Time-Aware'' denotes splitting according to chronological order.``Word'' and ``Degree'' represent splitting according to word diversity and node degree respectively. ``Language'' means splitting by user language, suggesting the prediction should not be impacted by the language the user use. ``University'' denotes splitting according to the domain university, implying that the prediction of webpages should be based on word contents and link connections rather than university features. ``Color'' means that nodes are split according to node differences in covariate shift and color-label correlations in concept shift.}
\label{tab:dataset}
\centering
\begin{tabular}{cccccccc}
\toprule
Datasets     & Network Type        & \#Nodes & \#Edges & \#Attributes &\#Classes& Train/Val/Test Split     & Metric   \\
% Cora         & Artificial Transformation & 2,703   &         &              &         &                      & Accuracy \\
Amazon-Photo\footnotemark
             & Co-purchasing network      & 7,650   & 119,081   & 755          & 10      & Domain-Level         & Accuracy \\
Elliptic\footnotemark  
             & Bitcoin transactions       & 203,769 & 234,355   & 165          & 2       & Time-Aware           & F1-Score \\
GOOD-Cora    & Scientific publications    & 19,793  & 126,842   & 8,710         & 70      & Word/Degree          & Accuracy \\
% GOOD-Arxiv   & arXiv papers               & 169,343 & 2,315,598 & 128          & 40      & Time/Degree          & Accuracy \\
GOOD-Twitch  & Gamer network              & 34,120  & 892,346   & 128          & 2       & Language             & ROC-AUC  \\
GOOD-CBAS    & A BA-house graph           & 700     & 3,962     & 4             & 4       & Color                & Accuracy \\
GOOD-WebKB   & Webpage network            & 617     & 1,138     & 1,703         & 5       & University           & Accuracy \\
\bottomrule
\end{tabular}
\end{table*}
\footnotetext[5]{This dataset is adopted from~\cite{yang2016revisiting}. \cite{eerm} constructs ten graphs with different environment id’s for each graph.} 
\footnotetext[6]{The original is available on \hyperlink{https://www.kaggle.com/ellipticco/elliptic-data-set}{https://www.kaggle.com/ellipticco/elliptic-data-set}}

\subsection{Unsupervised Representation Learning}\label{sec:unsupervised}
\subsubsection{Transductive Setting}~\label{sec:trans}
% \noindent\textbf{Baselines.}\quad We conduct experiments with 12 baselines which consist of three categories: supervised methods and self-supervised generative methods, self-supervised contrastive methods. Specifically, we compare with three supervised baselines: empirical risk minimization~(ERM)~\cite{erm}, invariant risk minimization (IRM)~\cite{irm}, and a recent proposed graph OOD method dubbed EERM~\cite{eerm}. We also compare various unsupervised node-level representation learning methods: three self-supervised generative methods including GAE~\cite{gae}, VGAE~\cite{gae}, GraphMAE~\cite{gmae} and seven self-supervised contrastive methods: DGI~\cite{dgi}, MVGRL~\cite{mvgrl}, GRACE~\cite{grace}, RoSA~\cite{rosa}, BGRL~\cite{bgrl}, COSTA~\cite{costa}, SwAV~\cite{swav}. The descriptions of these methods can be found in Appendix~\ref{app:baselines}.
In this subsection, we focus on validating our proposed algorithm under the transductive setting, where the test nodes will participate in message passing~\cite{gilmer2017neural} during training following~\cite{good}. 

\noindent\textbf{Baselines.} We conduct experiments with 12 baselines from three categories: (i)~supervised methods, including empirical risk minimization~(\textbf{ERM})~\cite{erm}, invariant risk minimization (\textbf{IRM})~\cite{irm}, and a recent proposed graph OOD method \textbf{EERM}~\cite{eerm}; (ii)~self-supervised generative methods including Graph Autoencoder (\textbf{GAE})~\cite{gae}, Variational Graph Autoencoder (\textbf{VGAE})~\cite{gae}, Self-Supervised Masked Graph Autoencoders (\textbf{GraphMAE})~\cite{gmae}; (iii)~self-supervised contrastive methods including Deep Graph Infomax (\textbf{DGI})~\cite{dgi}, Contrastive Multi-View Representation Learning on Graphs (\textbf{MVGRL})~\cite{mvgrl}, Deep Graph Contrastive Representation Learning (\textbf{GRACE})~\cite{grace}, A Robust Self-Aligned Framework for Node-Node Graph Contrastive Learning (\textbf{RoSA})~\cite{rosa}, Bootstrapped Representation Learning on Graphs (\textbf{BGRL})~\cite{bgrl}, Covariance-Preserving Feature Augmentation for Graph Contrastive Learning (\textbf{COSTA})~\cite{costa}, Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (\textbf{SwAV})~\cite{swav}. The detailed descriptions of these baselines can be found in Appendix~\ref{app:baselines}.

\noindent\textbf{Experimental setup.} We use the same graph encoder across different datasets for a fair comparison following~\cite{good}. We use grid search to find other hyper-parameters (\eg, learning rate, epochs) for different methods. For all experiments, we select the best checkpoints for ID and OOD tests according to results on ID and OOD validation sets following~\cite{good}, respectively. Experimental details and hyper-parameter selections are provided in Appendix~\ref{app:hyper}. For evaluating unsupervised methods, a linear classifier will be built on the frozen trained encoder after finishing pre-training. The reported results are the mean performance with standard deviation after 10 runs following~\cite{good}.

\noindent\textbf{Analysis.}\quad Based on the experimental results listed in Table~\ref{tab:trans_concept} and \ref{tab:trans_covariate}, we can draw the following conclusions: firstly, we find strong self-supervised methods (\eg, GRACE, BGRL, COSTA) are more robust to distribution shifts (concept shift in Table~\ref{tab:trans_concept} and covariate shift in Table~\ref{tab:trans_covariate}) compared to supervised methods. For instance, on GOOD-CBAS and GOOD-WebKB datasets, GRACE surpasses the best supervised method by large margins (over 6\% absolute improvement). Interestingly, we find the methods designed for OOD generalization (\ie, IRM) and graph OOD generalization (\ie, EERM) do not attain superior performance than the standard ERM on most of the datasets. For example, EERM shows superior OOD performance compared to ERM in only one experiment, and IRM outperforms ERM in four out of ten experiments across the conducted evaluations. This phenomenon is also observed in \cite{good,ahuja2020empirical,rosenfeld2021risks}, showcasing the challenge of achieving invariant prediction in non-Euclidean graph settings. 

Furthermore, our method surpasses other SOTA self-supervised methods on the OOD test set of all datasets by a considerable margin while achieving comparable performance in the in-distribution test set. For instance, on small datasets such as GOOD-CBAS and GOOD-WebKB, our method outperforms GRACE\footnote{MARIO is built up on GRACE according to our recipe. So, we make a comparison with GRACE here.} by over 2\% absolute accuracy on the OOD test set. On larger datasets such as GOOD-Cora and GOOD-Twitch, our method still outperforms other methods which shows its superiority. For instance, under covariate shift, MARIO surpasses other methods by over 7\% absolute accuracy on the GOOD-Twitch OOD test set. These statistics prove the effectiveness of our design.


\begin{table*}[htp]
\caption{Experimental results of all methods under concept shift. The bold font means the top-1 performance and the underline represents the second performance across the unsupervised methods. 'ID' represents in-distribution test performance and 'OOD' means out-of-distribution test performance. (OOM: out-of-memory on a GPU with 24GB memory)}
\label{tab:trans_concept}
\centering
\scalebox{0.95}{
\begin{tabular}{l|cc|cc|cc|cc|cc}
\toprule
\toprule
\multirow{3}{*}{concept shift} & \multicolumn{4}{c|}{GOOD-Cora}                   & \multicolumn{2}{c|}{GOOD-CBAS} & \multicolumn{2}{c|}{GOOD-Twitch} & \multicolumn{2}{c}{GOOD-WebKB} \\
                           & \multicolumn{2}{c}{word} & \multicolumn{2}{c|}{degree}& \multicolumn{2}{c|}{color}    & \multicolumn{2}{c|}{language}   & \multicolumn{2}{c}{university} \\
                           & ID         & OOD         & ID          & OOD          & ID            & OOD           & ID             & OOD            & ID            & OOD            \\
\midrule
ERM                        & 66.38±0.45 & 64.44±0.18  & 68.60±0.40  & 60.76±0.34   & 89.79±1.39    & 83.43±1.19    & 80.80±1.00     & 56.92±0.92     & 62.67±1.53    & 26.33±1.09     \\
IRM                        & 66.42±0.41 & 64.29±0.31  & 68.57±0.35  & 61.45±0.24   & 89.64±1.21    & 82.29±1.14    & 78.87±1.04     & 59.30±1.79     & 62.67±1.10    & 26.88±1.42     \\
EERM                       & 65.10±0.44 & 62.45±0.19  & 66.95±0.44  & 56.58±0.25   & 79.07±2.12    & 64.50±1.01    & OOM            & OOM            & 62.50±2.01    & 28.07±3.23      \\
\midrule
% Random-Init                & 37.53±1.74 & 32.12±1.24  & 37.82±1.71  & 27.74±1.14   &               &               &                &                & 60.33±2.21    & 27.07±1.70     \\
GAE                        & 60.65±0.89 & 58.00±0.55  & 62.59±1.11  & 53.44±0.80   & 75.28±1.36    & 68.07±2.05    & 81.25±0.81     & 51.51±1.05     & 62.17±3.34    & 25.78±1.85     \\
VGAE                       & 63.19±0.53 & 60.35±0.47  & 61.65±0.66  & 54.28±0.28   & 76.50±0.50    & 59.07±0.56    & 80.46±0.53     & 55.56±4.53     & 62.50±2.38    & 24.40±2.57     \\
GraphMAE                   & \underline{66.44±0.46} & \underline{64.87±0.30}  & 67.95±0.46  & 59.41±0.39   & 89.14±0.89    & 82.93±0.93    & 80.05±0.64     & 59.38±1.49     & 61.83±3.37    & 29.27±2.15     \\
DGI                        & 63.33±0.56 & 60.71±0.49  & 65.93±1.02  & 55.83±0.53   & 91.22±1.47    & 85.00±1.66    & 80.05±0.87     & 59.16±1.88     & 61.83±2.83    & 28.63±1.92      \\
MVGRL                      & OOM        & OOM         & OOM         & OOM          & 88.57±1.15    & 76.50±1.17    & OOM            & OOM            & 62.00±3.79    & 28.26±4.20     \\
GRACE                      & 65.61±0.61 & 63.92±0.44  & \textbf{68.59±0.35}  & 60.15±0.45   & 92.00±1.39    & 88.64±0.67    & \textbf{83.43±0.63}     & \underline{60.45±1.46}     & 64.00±3.43    & \underline{34.86±3.43}  \\
RoSA                       & 64.06±0.67 & 62.44±0.39  & 67.07±0.65  & 57.68±0.44   & 90.78±2.27    & 85.93±2.14    & 82.39±0.42     & 57.45±2.16     & 64.17±4.10    & 32.20±2.15     \\
BGRL                       & 65.18±0.43 & 63.43±0.45  & 66.83±0.80  & 59.63±0.38   & 92.36±1.16    & 87.14±1.60    & 82.52±0.60     & 55.48±1.48     & 63.67±2.33    & 31.47±3.43     \\
COSTA                      & 65.05±0.80 & 62.37±0.45  & 66.76±0.87  & 55.73±0.36   & \underline{93.50±2.62}    & \underline{89.29±3.11}    & 83.15±0.30 & 55.03±3.22     & 61.66±2.58    & 32.39±2.13 \\
% ArCL                       &            &             & 67.64±0.57  & 59.71±0.44   &               &               &                &                & 65.00±3.94    & 35.41±1.97 \\      
SwAV                       & 62.22±0.53 & 59.79±0.53  & 64.65±0.94  & 55.06±0.39   & 89.00±0.79    & 81.72±0.66    & \underline{83.32±0.15}     & 59.69±1.97     & \underline{65.17±3.76}    & 29.36±2.01    \\
\midrule
MARIO                       & \textbf{67.11±0.46} & \textbf{65.28±0.34}  & \underline{68.46±0.40}  & \textbf{61.30±0.28}   & \textbf{94.36±1.21}    & \textbf{91.28±1.10}    & 82.31±0.54     & \textbf{63.33±1.72}     & \textbf{65.67±2.81}    & \textbf{37.15±2.37}     \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[htp]
\caption{Experimental results of all methods under covariate shift. The bold font means the top-1 performance and the underline represents the second performance across the unsupervised methods. 'ID' represents in-distribution test performance and 'OOD' means out-of-distribution test performance. (OOM: out-of-memory on a GPU with 24GB memory)}
\label{tab:trans_covariate}
\centering
\scalebox{0.95}{
\begin{tabular}{l|cc|cc|cc|cc|cc}
\toprule
\toprule
\multirow{3}{*}{covariate shift} & \multicolumn{4}{c|}{GOOD-Cora}                                   & \multicolumn{2}{c|}{GOOD-CBAS} & \multicolumn{2}{c|}{GOOD-Twitch} & \multicolumn{2}{c}{GOOD-WebKB} \\
                           & \multicolumn{2}{c}{word} & \multicolumn{2}{c|}{degree}& \multicolumn{2}{c|}{color}    & \multicolumn{2}{c|}{language}   & \multicolumn{2}{c}{university} \\
                           & ID         & OOD         & ID          & OOD          & ID            & OOD           & ID             & OOD            & ID            & OOD            \\
\midrule
ERM                        & 70.50±0.41 & 64.69±0.33  & 72.46±0.49  & 55.53±0.50   & 92.00±3.08    & 77.57±1.29    & 70.98±0.41     & 49.35±5.09     & 39.34±1.79    & 14.52±3.14   \\
IRM                        & 70.48±0.26 & 64.53±0.57  & 71.98±0.34  & 53.72±0.46   & 90.86±2.41    & 78.86±1.67    & 69.81±0.95     & 49.11±2.82     & 38.52±3.30    & 13.97±2.80     \\
EERM                       & OOM        & OOM         & OOM         & OOM          & 65.00±2.57    & 57.43±3.60    & OOM            & OOM            & 46.07±4.55    & 27.40±7.65     \\
\midrule
GAE                        & 56.63±0.79 & 48.93±0.93  & 66.30±0.88  & 34.01±0.87   & 73.00±2.16    & 60.86±3.01    & 67.24±1.23     & 47.65±2.49     & 45.08±6.32    & 28.02±6.29    \\
VGAE                       & 62.02±0.66 & 54.12±0.86  & 69.41±0.57  & 44.20±1.29   & 62.29±2.04    & 63.29±1.11    & 66.99±1.43     & \underline{50.48±4.58}     & 48.85±4.68    & 20.87±6.69     \\
GraphMAE                   & 68.14±0.43 & 64.00±0.33  & \textbf{73.36±0.56}  & 53.75±0.55   & 67.28±3.03    & 67.28±1.49    & 68.84±1.20     & 48.02±2.79     & 48.03±4.34    & 30.00±8.09     \\
DGI                        & 60.85±0.75 & 57.03±0.67  & 68.97±0.41  & 41.75±0.88   & 69.57±4.09    & 59.71±3.43    & 68.43±1.05     & 44.83±1.61     & 48.52±5.04    & 21.11±7.50     \\
MVGRL                      & OOM        & OOM         & OOM         & OOM          & 65.00±1.94    & 64.15±0.77    & OOM            & OOM           & \textbf{54.10±5.39}    & 16.59±6.51     \\
GRACE                      & \underline{68.77±0.33} & \underline{64.21±0.41}  & 72.69±0.34  & \underline{56.10±0.63}   & \underline{93.57±1.83}    & \underline{89.29±3.40}    & \underline{71.12±0.87} & 46.21±1.54 & 49.67±5.82    & 28.10±4.68    \\
RoSA                       & 68.19±0.56 & 62.48±0.61  & 71.04±0.62  & 52.72±0.79   & 84.71±4.14    &79.14±3.51     & 70.58±0.36     & 45.83±1.72     & 52.30±4.24    & \underline{34.24±7.92}     \\
BGRL                       & 67.23±0.43 & 61.33±0.36  & 72.11±0.39  & 49.15±0.73   & 89.00±2.56    & 79.86±3.29    & \textbf{71.43±0.53}     & 43.86±0.94     & 51.80±5.55    & 30.32±7.61    \\
COSTA                      & 65.28±0.60 & 60.33±0.53  & 70.65±0.62  & 54.03±0.28   & 92.29±1.59    & 82.71±2.74    & 69.29±1.37     & 49.07±2.13     & 50.49±3.01    & 29.84±4.75   \\
SwAV                       & 63.29±1.01 & 56.98±0.94  & 70.27±0.73  & 43.00±0.52   & 89.57±1.12    & 81.43±1.69    & 69.19±0.93     & 49.37±2.96     & 49.84±4.82    & 30.55±6.72   \\
\midrule
MARIO                       & \textbf{69.99±0.54} & \textbf{65.06±0.34}  & \underline{72.73±0.43}  & \textbf{57.73±0.45}  & \textbf{94.57±2.46}    & \textbf{91.00±2.48}     & 68.31±0.78 & \textbf{57.37±1.37}     & \underline{53.94±3.23}    & \textbf{35.24±4.98}   \\
\bottomrule
\end{tabular}}

\end{table*}

\subsubsection{Inductive Setting}
In this subsection, we conduct experiments under the inductive settings, where the test nodes are kept unseen during training. This setting is more suitable for domain generalization.
% But we think it is more convincing that conduct experiments under inductive settings which means test nodes are unseen during training. This setting is more appropriate for domain generalization.

\noindent\textbf{Baselines:} For GOOD-WebKB and GOOD-CBAS datasets, we adopt ERM, IRM, GraphMAE, and GRACE as our baselines. And for Amazon-Photo and Elliptic datasets, we select ERM, EERM, and GRACE as our baselines.

\noindent\textbf{Experimental setup:} For GOOD-WebKB and GOOD-CBAS datasets, we use the same model configuration in Section~\ref{sec:trans}.
% Besides, we add experiments on Amazon-Photo dataset~\cite{yang2016revisiting} and Elliptic~\cite{elliptic} dataset in this subsection. 
For Amazon-Photo dataset~\cite{yang2016revisiting} and Elliptic~\cite{elliptic} dataset, they consist of many snapshots (training data and testing data use different snapshots) which are naturally inductive. For Amazon-Photo dataset, we use 2-layer GCN~\cite{gcn} as the encoder and for elliptic dataset, we use 5-layer GraphSAGE~\cite{sage} as encoder following~\cite{eerm}.

% Figure environment removed

\noindent\textbf{Analysis:}
According to Figure~\ref{fig:amazon},\ref{fig:elliptic},\ref{fig:ind_con},\ref{fig:ind_cov}, we can draw following conclusions:
firstly, based on Figure~\ref{fig:amazon}, it is evident that our method outperforms other representative supervised and self-supervised methods on all test graphs (T1$\sim$T8). This superiority is reflected in the larger median value of our method compared to others. For instance, MARIO achieves over a 3\% absolute improvement compared to ERM in terms of the mean value of eight median values. Additionally, our method demonstrates higher stability across different random initializations, as indicated by the closer proximity of the first and third quartile values to the median value~(\eg, the difference of first and third quartile values of ERM, EERM, GRACE and MARIO are 4.2, 3.3, 6.7 and 1.0 on T8 respectively which indicates MARIO is much more stable than other methods). Furthermore, our method exhibits consistent performance across different graphs (\eg, The standard deviation of median values on T1$\sim$T8 for ERM, EERM, GRACE, and MARIO are 0.4, 1.1, 1.2, and 0.3, respectively.), indicating its robustness to environmental variations and its ability to extract invariant features: $g(G^e) \approx g(G^{e'})$ for all $e, e' \in \mathcal{E}^\text{train}$. In summary, our method showcases enhanced OOD generalization capabilities.
% $g(G^e)g(G^e^\prime)$ where $any e, e^\prime in \mathcal{E}^{train}$

Secondly, from the results presented in Figure~\ref{fig:elliptic}, we can observe that our method averagely harvests 10.9\% absolute improvement over GRACE and 12.5\% absolute improvement over EERM in terms of F1 scores on Elliptic dataset. This demonstrates the effectiveness of our method in handling distribution shifts and improving performance compared to existing approaches. It is worth noting that GRACE's performance worsens over time, indicating its inability to handle distribution shifts effectively. In contrast, our method consistently achieves better F1 scores, except for T9, which is caused by the dark market shutdown occurred after T7~\cite{elliptic}. The emergence of such an event introduces significant variations in data distributions, which subsequently results in performance degradation for all methods. Indeed, this event serves as an unpredictable external factor that introduces significant challenges for models trained on limited training data. The results indicate that the performance heavily depends on available training data. Nonetheless, our approach outperforms other methods even in such an extreme case. This highlights the effectiveness of our method in addressing distribution shifts and improving generalization performance.

Finally, based on the observations from Figure~\ref{fig:ind_con} and Figure~\ref{fig:ind_cov} MARIO demonstrates the best performances on both ID and OOD test sets for GOOD-WebKB and GOOD-CBAS datasets, under both concept shift and covariate shift. Notably, MARIO outperforms other methods by more than 3\% and 10\% absolute improvement on GOOD-WebKB and GOOD-CBAS, respectively, under covariate shift. We can draw similar conclusions as discussed in Section~\ref{sec:trans}. Even under the inductive setting, our method continues to demonstrate excellent OOD generalization capabilities and achieves comparable or even improved in-distribution test performance. These statistical results further validate the effectiveness of our method in handling distribution shifts and enhancing generalization performance.

Overall, the observations we have made provide strong evidence of the great capacity of our method for handling distribution shifts, validating its effectiveness and potential for real-world applications.



% Figure environment removed

% Figure environment removed


% Figure environment removed


\subsection{Ablation Studies}\label{sec:ablation}
\noindent Table~\ref{tab:aba} provides a detailed analysis of the effect of each component according to our proposed recipe for improving OOD generalization in graph contrastive learning. Let's examine the different variants of our method and their impact on performance.
Specifically, MARIO~(w/o ad) represents MARIO without  adversarial augmentation. MARIO~(w/o cmi) denotes we only maximize the mutual information between positive pairs without considering conditional mutual information. MARIO~(w/o cmi, ad) means a vanilla graph contrastive method that is similar to GRACE. 

From Table~\ref{tab:aba}, we can find MARIO~(w/o cmi) lags far behind MARIO on OOD test set which demonstrates appropriately minimizing the redundant information (\ie, conditional mutual information) is essential to improve OOD generalization of GCL methods. And adversarial augmentation can also boost OOD generalization because it can approximately serve as a supermum operator to learn more invariant features  discussed in Section~\ref{sec:aug}. Based on the analysis of these variants, it is evident that the proposed improvements on data augmentation and contrastive loss in the recipe are both effective in enhancing graph OOD generalization. Each component contributes to the overall performance improvement, and their combination leads to a stronger self-supervised graph learner in terms of graph OOD generalization. 

In short, the findings from Table~\ref{tab:aba} support the rationale behind your proposed recipe and provide empirical evidence of the effectiveness of each proposed component. By incorporating these enhancements, our method achieves superior performance in handling distribution shifts and improving graph OOD generalization in graph contrastive learning.
\begin{table*}[htp]
\caption{Ablation studies for MARIO by masking each component.}
\label{tab:aba}
\centering
\scalebox{0.9}{
\begin{tabular}{l|cc|cc|cc|cc|cc}
\toprule
\toprule
\multirow{3}{*}{concept shift} & \multicolumn{4}{c|}{GOOD-Cora}                       & \multicolumn{2}{c|}{GOOD-CBAS} & \multicolumn{2}{c|}{GOOD-Twitch} & \multicolumn{2}{c}{GOOD-WebKB} \\
                           & \multicolumn{2}{c}{word} & \multicolumn{2}{c|}{degree}& \multicolumn{2}{c|}{color}    & \multicolumn{2}{c|}{language}   & \multicolumn{2}{c}{university} \\
                           & ID         & OOD         & ID          & OOD          & ID            & OOD           & ID             & OOD            & ID            & OOD            \\
\midrule
MARIO                      & \textbf{67.11±0.46} & \textbf{65.28±0.34}  & \textbf{68.46±0.40}  & \textbf{61.30±0.28}      & \textbf{94.36±1.21}  & \textbf{91.28±1.10}    & 82.31±0.54     & \textbf{63.33±1.72}     & \textbf{65.67±2.81}    & \textbf{37.15±2.37}     \\
MARIO(w/o ad)              & 66.23±0.53 & 64.02±0.18  & 67.88±0.38  & 60.46±0.29   & 93.21±1.25    & 90.29±0.91    & 82.42±0.73     & 60.50±1.02     & 64.83±2.83    & 36.51±3.25    \\
MARIO(w/o cmi)             & 65.32±0.60 & 63.51±0.32  & 68.14±0.32  & 61.19±0.34   & 94.15±1.23    & 90.57±1.96    & \textbf{82.51±0.56}     & 61.41±2.63     & 64.50±4.35    & 35.78±2.53     \\
MARIO(w/o cmi, ad)         & 64.67±0.55 & 63.11±0.32  & 67.95±0.65  & 60.01±0.57   & 93.36±1.66    & 89.64±1.73    & 81.90±0.75     & 60.12±1.60     & 64.17±3.67    & 34.13±2.38     \\
\bottomrule
\end{tabular}}
\end{table*}
% & 65.32±0.60 & 63.51±0.32 exchange 64.67±0.55 & 63.11±0.32
% 68.14±0.32       id ood test: 60.95±0.43       ood ood test: 61.19±0.34


\subsection{Sensitivity Analysis}\label{sec:sensitivity}
\noindent In this subsection, we will analyze some important hyper-parameters of our method. We conduct sensitivity analysis on GOOD-WebKB dataset with concept shift, we chose two sensitive hyper-parameters (\ie, the coefficient $\gamma$ of condition mutual information in Equation~\ref{equ:cmi} and the number of prototypes $|C|$ in Equation~\ref{equ:pq}). The coefficient of CMI range in $[0.001, 0.01, 0.1, 0.5, 1]$ and the number of prototypes $|C|$ ranges in $[10, 50, 100, 200, 300]$. From Figure~\ref{fig:sensitivity}, we can observe that $\gamma$ reaches 0.1 and $|C|$ reaches 100 or 200 can achieve the best OOD test accuracy. Both higher and lower values of $\gamma$ result in suboptimal performance. This finding aligns with previous research such as DIB~\cite{dib}, indicating that an appropriate compression level is crucial for achieving optimal performance. Extremely high or low compression values are not ideal. 

Regarding the number of prototypes $|C|$, based on the results shown in Figure~\ref{fig:sensitivity}, it is found that setting $|C|=100$ leads to the best performance in terms of OOD test accuracy. This choice provides a moderate number of pseudo labels, which is beneficial for the learning process. 

Based on the sensitivity analysis, we determined that setting $\gamma=0.1$ and $|C|=100$ on most datasets. These hyperparameter values strike a balance between compression level and the number of prototypes, resulting in improved graph OOD generalization.
% Figure environment removed


\subsection{Integrated with Other Models}\label{sec:other_models}
% Figure environment removed

\begin{table}[htp]
\caption{Results of different learning approaches with different encoding models (\ie, GCN, GraphSAGE, GAT).}
\label{tab:others}
\centering
\scalebox{0.9}{
\begin{tabular}{cc|cc|cc}
\toprule
\toprule
\multirow{3}{*}{Model}& \multirow{3}{*}{Method} & \multicolumn{2}{c|}{GOOD-CBAS} & \multicolumn{2}{c}{GOOD-WebKB} \\
                & & \multicolumn{2}{c|}{color}    & \multicolumn{2}{c}{university} \\
                &   & ID          & OOD         & ID          & OOD            \\
\midrule
\multirow{3}{*}{GCN} 
&ERM               & 89.79±1.39 & 83.43±1.19  &  62.67±1.53 & 26.33±1.09         \\
&GRACE             & 92.00±1.39 & 88.64±0.67  &  64.00±3.43 & 34.86±3.43        \\
&MARIO             & 94.36±1.21 & 91.28±1.10  &  65.67±2.81 & 37.15±2.37        \\ \bottomrule
\multirow{3}{*}{SAGE} 
&ERM               & 95.07±1.51 & 75.14±1.19  & 73.67±2.08  & 46.33±3.42       \\
&GRACE             & 95.29±1.11 & 74.43±2.36  & 70.50±5.06  & 49.54±3.83        \\
&MARIO             & 96.00±1.07 & 76.29±3.01  & 71.00±3.82  & 51.74±4.63        \\ \bottomrule
\multirow{3}{*}{GAT} 
&ERM               & 78.64±3.63 & 72.93±2.64  & 61.33±3.71  & 28.99±2.63        \\
&GRACE             & 84.57±1.79 & 78.36±1.60  & 59.50±2.36  & 35.78±3.26        \\
&MARIO             & 84.93±1.95 & 80.43±1.89  & 62.17±4.78  & 38.17±3.10        \\
\bottomrule
\end{tabular}}
\end{table}



\noindent In the subsection, we demonstrate the model-agnostic nature of the recipe by integrating it with various graph neural network (GNN) models, including GCN, GraphSAGE, and GAT.

From Table~\ref{tab:others}, it can be observed that regardless of the specific GNN model used as the encoder, our method consistently achieves the best performance on the OOD test set. This indicates the effectiveness and robustness of our method across different GNN models.
By achieving superior performance across different GNN models, MARIO demonstrates its versatility and ability to improve the OOD generalization of various graph neural models. This highlights the broad applicability and effectiveness of our recipe in enhancing the performance of different GNN encoders.

Furthermore, we integrate our recipe with other GCL methods in Appendix~\ref{app:other_methods}. The results demonstrate our recipe can boost the OOD generalization ability of various GCL methods which means our recipe can serve as a plug-in for many current classical GCL methods.

% Figure environment removed

\subsection{Visualization}\label{sec:vis}
\subsubsection{Metric Score Curves}
We present metric score curves for ERM and MARIO, including training, ID validation, ID testing, OOD validation, and OOD testing accuracy, in Figure~\ref{fig:curve2}. Notably, MARIO demonstrates superior convergence with approximately 10\% absolute improvement on the OOD test set compared to ERM. Furthermore, MARIO effectively narrows the performance gap between in-distribution and out-of-distribution performance, showcasing its efficacy in enhancing OOD generalization for graph data. More metric score curves can be found in Appendix~\ref{app:curves}.


\subsubsection{Feature Visualization}
In order to assess the quality of learned embeddings, we adopt t-SNE~\cite{tsne} to visualize the node embedding on GOOD-Cora dataset (concept shift in word domain) using random-init of GCN, EERM, GRACE, and MARIO, where different classes have different colors in Figure~\ref{fig:vis}. For clarity, we select eight classes with the largest number of nodes to enhance the informativeness and interpretability of the visualization. We can observe that the 2D projection of node embeddings learned by MARIO has a better separation of clusters, which indicates the model can help learn representative features for downstream tasks. It has to note that we depict both ID nodes and OOD nodes in the same figure. 

Besides, we also separately visualize ID nodes and OOD nodes in the different figures in the Appendix~\ref{app:feature}. And we can find MARIO performs a clearer separation of clusters whether on ID nodes or OOD nodes compared to other methods.


