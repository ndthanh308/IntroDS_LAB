% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{community}
M.~E. Newman and M.~Girvan, ``Finding and evaluating community structure in
  networks,'' \emph{Physical review E}, vol.~69, no.~2, p. 026113, 2004.

\bibitem{citation}
V.~Batagelj, ``Efficient algorithms for citation network analysis,''
  \emph{arXiv preprint cs/0309023}, 2003.

\bibitem{molecule}
C.~Chen, W.~Ye, Y.~Zuo, C.~Zheng, and S.~P. Ong, ``Graph networks as a
  universal machine learning framework for molecules and crystals,''
  \emph{Chemistry of Materials}, vol.~31, no.~9, pp. 3564--3572, 2019.

\bibitem{gcn}
T.~N. Kipf and M.~Welling, ``Semi-supervised classification with graph
  convolutional networks,'' in \emph{International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem{sage}
W.~Hamilton, Z.~Ying, and J.~Leskovec, ``Inductive representation learning on
  large graphs,'' \emph{Advances in neural information processing systems},
  vol.~30, 2017.

\bibitem{gin}
\BIBentryALTinterwordspacing
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka, ``How powerful are graph neural
  networks?'' in \emph{International Conference on Learning Representations},
  2019. [Online]. Available: \url{https://openreview.net/forum?id=ryGs6iA5Km}
\BIBentrySTDinterwordspacing

\bibitem{gat}
P.~Veli{\v{c}}kovi{\'c}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Li{\`o}, and
  Y.~Bengio, ``Graph attention networks,'' in \emph{International Conference on
  Learning Representations}.

\bibitem{erm}
V.~Vapnik, ``Principles of risk minimization for learning theory,''
  \emph{Advances in neural information processing systems}, vol.~4, 1991.

\bibitem{ermxxx}
V.~N. Vapnik, ``An overview of statistical learning theory,'' \emph{IEEE
  transactions on neural networks}, vol.~10, no.~5, pp. 988--999, 1999.

\bibitem{ogb}
W.~Hu, M.~Fey, M.~Zitnik, Y.~Dong, H.~Ren, B.~Liu, M.~Catasta, and J.~Leskovec,
  ``Open graph benchmark: Datasets for machine learning on graphs,''
  \emph{Advances in neural information processing systems}, vol.~33, pp.
  22\,118--22\,133, 2020.

\bibitem{eerm}
Q.~Wu, H.~Zhang, J.~Yan, and D.~Wipf, ``Handling distribution shifts on graphs:
  An invariance perspective,'' in \emph{International Conference on Learning
  Representations}.

\bibitem{good}
S.~Gui, X.~Li, L.~Wang, and S.~Ji, ``Good: A graph out-of-distribution
  benchmark,'' in \emph{Thirty-sixth Conference on Neural Information
  Processing Systems Datasets and Benchmarks Track}.

\bibitem{gil}
H.~Li, Z.~Zhang, X.~Wang, and W.~Zhu, ``Learning invariant graph
  representations for out-of-distribution generalization,'' in \emph{Advances
  in Neural Information Processing Systems}, 2022.

\bibitem{dir}
Y.~Wu, X.~Wang, A.~Zhang, X.~He, and T.-S. Chua, ``Discovering invariant
  rationales for graph neural networks,'' in \emph{International Conference on
  Learning Representations}.

\bibitem{rosa}
\BIBentryALTinterwordspacing
Y.~Zhu, J.~Guo, F.~Wu, and S.~Tang, ``Rosa: A robust self-aligned framework for
  node-node graph contrastive learning,'' in \emph{Proceedings of the
  Thirty-First International Joint Conference on Artificial Intelligence,
  {IJCAI-22}}, L.~D. Raedt, Ed.\hskip 1em plus 0.5em minus 0.4em\relax
  International Joint Conferences on Artificial Intelligence Organization, 7
  2022, pp. 3795--3801, main Track. [Online]. Available:
  \url{https://doi.org/10.24963/ijcai.2022/527}
\BIBentrySTDinterwordspacing

\bibitem{grace}
Y.~Zhu, Y.~Xu, F.~Yu, Q.~Liu, S.~Wu, and L.~Wang, ``{Deep Graph Contrastive
  Representation Learning},'' in \emph{ICML Workshop on Graph Representation
  Learning and Beyond}, 2020.

\bibitem{graphcl}
Y.~You, T.~Chen, Y.~Sui, T.~Chen, Z.~Wang, and Y.~Shen, ``Graph contrastive
  learning with augmentations,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~33, pp. 5812--5823, 2020.

\bibitem{bgrl}
S.~Thakoor, C.~Tallec, M.~G. Azar, R.~Munos, P.~Veli{\v{c}}kovi{\'c}, and
  M.~Valko, ``Bootstrapped representation learning on graphs,'' in \emph{ICLR
  2021 Workshop on Geometrical and Topological Representation Learning}, 2021.

\bibitem{dgi}
P.~Velickovic, W.~Fedus, W.~L. Hamilton, P.~Li{\`{o}}, Y.~Bengio, and R.~D.
  Hjelm, ``Deep graph infomax,'' in \emph{Proc. of ICLR}, 2019.

\bibitem{mvgrl}
K.~Hassani and A.~H. Khasahmadi, ``Contrastive multi-view representation
  learning on graphs,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 4116--4126.

\bibitem{gmae}
\BIBentryALTinterwordspacing
Z.~Hou, X.~Liu, Y.~Cen, Y.~Dong, H.~Yang, C.~Wang, and J.~Tang, ``Graphmae:
  Self-supervised masked graph autoencoders,'' ser. KDD '22.\hskip 1em plus
  0.5em minus 0.4em\relax New York, NY, USA: Association for Computing
  Machinery, 2022, p. 594â€“604. [Online]. Available:
  \url{https://doi.org/10.1145/3534678.3539321}
\BIBentrySTDinterwordspacing

\bibitem{costa}
Y.~Zhang, H.~Zhu, Z.~Song, P.~Koniusz, and I.~King, ``Costa:
  Covariance-preserving feature augmentation for graph contrastive learning,''
  in \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining}, 2022, pp. 2524--2534.

\bibitem{cpc}
A.~v.~d. Oord, Y.~Li, and O.~Vinyals, ``Representation learning with
  contrastive predictive coding,'' \emph{arXiv preprint arXiv:1807.03748},
  2018.

\bibitem{groupdro}
S.~Sagawa, P.~W. Koh, T.~B. Hashimoto, and P.~Liang, ``Distributionally robust
  neural networks,'' in \emph{International Conference on Learning
  Representations}.

\bibitem{hu2018does}
W.~Hu, G.~Niu, I.~Sato, and M.~Sugiyama, ``Does distributionally robust
  supervised learning give robust classifiers?'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2018, pp. 2029--2037.

\bibitem{irm}
M.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz, ``Invariant risk
  minimization,'' \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem{sparseirm}
X.~Zhou, Y.~Lin, W.~Zhang, and T.~Zhang, ``Sparse invariant risk
  minimization,'' in \emph{International Conference on Machine Learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 27\,222--27\,244.

\bibitem{peters2016causal}
J.~Peters, P.~B{\"u}hlmann, and N.~Meinshausen, ``Causal inference by using
  invariant prediction: identification and confidence intervals,''
  \emph{Journal of the Royal Statistical Society Series B: Statistical
  Methodology}, vol.~78, no.~5, pp. 947--1012, 2016.

\bibitem{heinze2018causal}
C.~Heinze-Deml, J.~Peters, and N.~Meinshausen, ``Invariant causal prediction
  for nonlinear models,'' \emph{Journal of Causal Inference}, vol.~6, no.~2, p.
  20170016, 2018.

\bibitem{gsat}
S.~Miao, M.~Liu, and P.~Li, ``Interpretable and generalizable graph learning
  via stochastic attention mechanism,'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp.
  15\,524--15\,543.

\bibitem{dib}
A.~A. Alemi, I.~Fischer, J.~V. Dillon, and K.~Murphy, ``Deep variational
  information bottleneck,'' \emph{arXiv preprint arXiv:1612.00410}, 2016.

\bibitem{wang2020understanding}
T.~Wang and P.~Isola, ``Understanding contrastive representation learning
  through alignment and uniformity on the hypersphere,'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2020, pp. 9929--9939.

\bibitem{rgcl}
S.~Li, X.~Wang, A.~Zhang, Y.~Wu, X.~He, and T.-S. Chua, ``Let invariant
  rationale discovery inspire graph contrastive learning,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2022, pp. 13\,052--13\,065.

\bibitem{ood-survey}
Z.~Shen, J.~Liu, Y.~He, X.~Zhang, R.~Xu, H.~Yu, and P.~Cui, ``Towards
  out-of-distribution generalization: A survey,'' \emph{arXiv preprint
  arXiv:2108.13624}, 2021.

\bibitem{good-survey}
H.~Li, X.~Wang, Z.~Zhang, and W.~Zhu, ``Out-of-distribution generalization on
  graphs: A survey,'' \emph{arXiv preprint arXiv:2202.07987}, 2022.

\bibitem{arcl}
X.~Zhao, T.~Du, Y.~Wang, J.~Yao, and W.~Huang, ``Arcl: Enhancing contrastive
  learning with augmentation-robust representations,'' in \emph{The Eleventh
  International Conference on Learning Representations}.

\bibitem{huang2023towards}
\BIBentryALTinterwordspacing
W.~Huang, M.~Yi, X.~Zhao, and Z.~Jiang, ``Towards the generalization of
  contrastive self-supervised learning,'' in \emph{The Eleventh International
  Conference on Learning Representations}, 2023. [Online]. Available:
  \url{https://openreview.net/forum?id=XDJwuEYHhme}
\BIBentrySTDinterwordspacing

\bibitem{shi2022robust}
Y.~Shi, I.~Daunhawer, J.~E. Vogt, P.~H. Torr, and A.~Sanyal, ``How robust are
  pre-trained models to distribution shift?'' \emph{arXiv preprint
  arXiv:2206.08871}, 2022.

\bibitem{simclr}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{International
  conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2020, pp. 1597--1607.

\bibitem{tian2020makes}
Y.~Tian, C.~Sun, B.~Poole, D.~Krishnan, C.~Schmid, and P.~Isola, ``What makes
  for good views for contrastive learning?'' \emph{Advances in neural
  information processing systems}, vol.~33, pp. 6827--6839, 2020.

\bibitem{ahuja2021invariance}
K.~Ahuja, E.~Caballero, D.~Zhang, J.-C. Gagnon-Audet, Y.~Bengio, I.~Mitliagkas,
  and I.~Rish, ``Invariance principle meets information bottleneck for
  out-of-distribution generalization,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 3438--3450, 2021.

\bibitem{li2022invariant}
B.~Li, Y.~Shen, Y.~Wang, W.~Zhu, D.~Li, K.~Keutzer, and H.~Zhao, ``Invariant
  information bottleneck for domain generalization,'' in \emph{Proceedings of
  the AAAI Conference on Artificial Intelligence}, vol.~36, no.~7, 2022, pp.
  7399--7407.

\bibitem{shafahi2019adversarial}
A.~Shafahi, M.~Najibi, M.~A. Ghiasi, Z.~Xu, J.~Dickerson, C.~Studer, L.~S.
  Davis, G.~Taylor, and T.~Goldstein, ``Adversarial training for free!''
  \emph{Advances in Neural Information Processing Systems}, vol.~32, 2019.

\bibitem{flag}
K.~Kong, G.~Li, M.~Ding, Z.~Wu, C.~Zhu, B.~Ghanem, G.~Taylor, and T.~Goldstein,
  ``Robust optimization as data augmentation for large-scale graphs,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2022, pp. 60--69.

\bibitem{kim2020adversarial}
M.~Kim, J.~Tack, and S.~J. Hwang, ``Adversarial self-supervised contrastive
  learning,'' \emph{Advances in Neural Information Processing Systems},
  vol.~33, pp. 2983--2994, 2020.

\bibitem{suresh2021adversarial}
S.~Suresh, P.~Li, C.~Hao, and J.~Neville, ``Adversarial graph augmentation to
  improve graph contrastive learning,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 15\,920--15\,933, 2021.

\bibitem{jiang2020robust}
Z.~Jiang, T.~Chen, T.~Chen, and Z.~Wang, ``Robust pre-training by adversarial
  contrastive learning,'' \emph{Advances in neural information processing
  systems}, vol.~33, pp. 16\,199--16\,210, 2020.

\bibitem{tishby_ib}
N.~Tishby and N.~Zaslavsky, ``Deep learning and the information bottleneck
  principle,'' in \emph{2015 ieee information theory workshop (itw)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2015, pp. 1--5.

\bibitem{gib}
T.~Wu, H.~Ren, P.~Li, and J.~Leskovec, ``Graph information bottleneck,''
  \emph{Advances in Neural Information Processing Systems}, vol.~33, pp.
  20\,437--20\,448, 2020.

\bibitem{donsker1975asymptotic}
M.~D. Donsker and S.~S. Varadhan, ``Asymptotic evaluation of certain markov
  process expectations for large time, i,'' \emph{Communications on Pure and
  Applied Mathematics}, vol.~28, no.~1, pp. 1--47, 1975.

\bibitem{mine}
M.~I. Belghazi, A.~Baratin, S.~Rajeshwar, S.~Ozair, Y.~Bengio, A.~Courville,
  and D.~Hjelm, ``Mutual information neural estimation,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2018, pp. 531--540.

\bibitem{fgan}
S.~Nowozin, B.~Cseke, and R.~Tomioka, ``f-gan: Training generative neural
  samplers using variational divergence minimization,'' \emph{Advances in
  neural information processing systems}, vol.~29, 2016.

\bibitem{nce}
M.~Gutmann and A.~Hyv{\"a}rinen, ``Noise-contrastive estimation: A new
  estimation principle for unnormalized statistical models,'' in
  \emph{Proceedings of the thirteenth international conference on artificial
  intelligence and statistics}.\hskip 1em plus 0.5em minus 0.4em\relax JMLR
  Workshop and Conference Proceedings, 2010, pp. 297--304.

\bibitem{pcl}
J.~Li, P.~Zhou, C.~Xiong, and S.~Hoi, ``Prototypical contrastive learning of
  unsupervised representations,'' in \emph{International Conference on Learning
  Representations}.

\bibitem{swav}
M.~Caron, I.~Misra, J.~Mairal, P.~Goyal, P.~Bojanowski, and A.~Joulin,
  ``Unsupervised learning of visual features by contrasting cluster
  assignments,'' \emph{Advances in neural information processing systems},
  vol.~33, pp. 9912--9924, 2020.

\bibitem{cuturi2013sinkhorn}
M.~Cuturi, ``Sinkhorn distances: Lightspeed computation of optimal transport,''
  \emph{Advances in neural information processing systems}, vol.~26, 2013.

\bibitem{bilevel}
R.~Liu, J.~Gao, J.~Zhang, D.~Meng, and Z.~Lin, ``Investigating bi-level
  optimization for learning and vision from a unified perspective: A survey and
  beyond,'' \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, vol.~44, no.~12, pp. 10\,045--10\,067, 2021.

\bibitem{ji2022drugood}
Y.~Ji, L.~Zhang, J.~Wu, B.~Wu, L.-K. Huang, T.~Xu, Y.~Rong, L.~Li, J.~Ren,
  D.~Xue \emph{et~al.}, ``Drugood: Out-of-distribution (ood) dataset curator
  and benchmark for ai-aided drug discovery--a focus on affinity prediction
  problems with noise annotations,'' \emph{arXiv preprint arXiv:2201.09637},
  2022.

\bibitem{gds}
\BIBentryALTinterwordspacing
M.~Ding, K.~Kong, J.~Chen, J.~Kirchenbauer, M.~Goldblum, D.~Wipf, F.~Huang, and
  T.~Goldstein, ``A closer look at distribution shifts and out-of-distribution
  generalization on graphs,'' in \emph{NeurIPS 2021 Workshop on Distribution
  Shifts: Connecting Methods and Applications}, 2021. [Online]. Available:
  \url{https://openreview.net/forum?id=XvgPGWazqRH}
\BIBentrySTDinterwordspacing

\bibitem{yang2016revisiting}
Z.~Yang, W.~Cohen, and R.~Salakhudinov, ``Revisiting semi-supervised learning
  with graph embeddings,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2016, pp. 40--48.

\bibitem{gae}
T.~N. Kipf and M.~Welling, ``Variational graph auto-encoders,'' \emph{NIPS
  Workshop on Bayesian Deep Learning}, 2016.

\bibitem{ahuja2020empirical}
K.~Ahuja, J.~Wang, A.~Dhurandhar, K.~Shanmugam, and K.~R. Varshney, ``Empirical
  or invariant risk minimization? a sample complexity perspective,''
  \emph{arXiv preprint arXiv:2010.16412}, 2020.

\bibitem{rosenfeld2021risks}
E.~Rosenfeld, P.~Ravikumar, and A.~Risteski, ``The risks of invariant risk
  minimization,'' in \emph{International Conference on Learning
  Representations}, vol.~9, 2021.

\bibitem{gilmer2017neural}
J.~Gilmer, S.~S. Schoenholz, P.~F. Riley, O.~Vinyals, and G.~E. Dahl, ``Neural
  message passing for quantum chemistry,'' in \emph{International conference on
  machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp.
  1263--1272.

\bibitem{elliptic}
A.~Pareja, G.~Domeniconi, J.~Chen, T.~Ma, T.~Suzumura, H.~Kanezashi, T.~Kaler,
  T.~Schardl, and C.~Leiserson, ``Evolvegcn: Evolving graph convolutional
  networks for dynamic graphs,'' in \emph{Proceedings of the AAAI conference on
  artificial intelligence}, vol.~34, no.~04, 2020, pp. 5363--5370.

\bibitem{tsne}
L.~Van~der Maaten and G.~Hinton, ``Visualizing data using t-sne.''
  \emph{Journal of machine learning research}, vol.~9, no.~11, 2008.

\bibitem{ma2021conditional}
M.~Q. Ma, Y.-H.~H. Tsai, P.~P. Liang, H.~Zhao, K.~Zhang, R.~Salakhutdinov, and
  L.-P. Morency, ``Conditional contrastive learning for improving fairness in
  self-supervised learning,'' \emph{arXiv preprint arXiv:2106.02866}, 2021.

\bibitem{nguyen2010estimating}
X.~Nguyen, M.~J. Wainwright, and M.~I. Jordan, ``Estimating divergence
  functionals and the likelihood ratio by convex risk minimization,''
  \emph{IEEE Transactions on Information Theory}, vol.~56, no.~11, pp.
  5847--5861, 2010.

\bibitem{byol}
J.-B. Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar \emph{et~al.},
  ``Bootstrap your own latent-a new approach to self-supervised learning,''
  \emph{Advances in neural information processing systems}, vol.~33, pp.
  21\,271--21\,284, 2020.

\bibitem{bojchevskideep}
A.~Bojchevski and S.~G{\"u}nnemann, ``Deep gaussian embedding of graphs:
  Unsupervised inductive learning via ranking,'' in \emph{International
  Conference on Learning Representations}.

\bibitem{ying2019gnnexplainer}
Z.~Ying, D.~Bourgeois, J.~You, M.~Zitnik, and J.~Leskovec, ``Gnnexplainer:
  Generating explanations for graph neural networks,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{moco}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for
  unsupervised visual representation learning,'' in \emph{Proceedings of the
  IEEE/CVF conference on computer vision and pattern recognition}, 2020, pp.
  9729--9738.

\bibitem{fey2019fast}
M.~Fey and J.~E. Lenssen, ``Fast graph representation learning with pytorch
  geometric,'' \emph{ArXiv preprint}, 2019.

\bibitem{torch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~K{\"{o}}pf, E.~Yang,
  Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang,
  J.~Bai, and S.~Chintala, ``Pytorch: An imperative style, high-performance
  deep learning library,'' in \emph{Proc. of NeurIPS}, 2019.

\bibitem{pedregosa2011scikit}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg \emph{et~al.},
  ``Scikit-learn: Machine learning in python,'' \emph{the Journal of machine
  Learning research}, vol.~12, pp. 2825--2830, 2011.

\end{thebibliography}
