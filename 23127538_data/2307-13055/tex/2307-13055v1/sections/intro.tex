\section{Introduction}
\IEEEPARstart{G}{raph-structured} data is pervasive in various aspects of our lives, such as social networks~\cite{community}, citation networks~\cite{citation}, and molecules~\cite{molecule}. Most existing graph learning algorithms~\cite{gcn,sage,gin,gat} work under the statistical assumption~\cite{erm,ermxxx} that the training and testing data are drawn from the same distribution. However, in real-world scenarios, this assumption does not often hold. For example, in citation networks, as time progresses, new topics emerge and citation distributions change~\cite{ogb,eerm,good}, which consequently causes a distribution shift between the training and testing domains. Apart from this challenge of training-test distributional shift, how to effectively utilize the massive unannotated graph data emerging every day also remains to be an intriguing problem to solve. Hence, the primary goal of this paper is to seek for a set of principles that achieves superior OOD generalization performance with unlabeled graph data. To this end, two primary challenges need to be addressed:

\emph{Challenge 1:} Non-Euclidean data structure of graphs causes complex distributional shifts (feature-level and topology-level) and lack of environment labels (due to the inherent abstraction of graph), which in consequence severely qualifies the direct application of existing OOD generalization methods.

\emph{Challenge 2:} Most existing OOD generalization methods heavily rely on label information. It remains a practical challenge how to elicit invariant representations when no access to labels is provided.

Many efforts have been made towards the resolution of the challenges above. To address \emph{Challenge~1}, EERM~\cite{eerm}, GIL~\cite{gil}, and DIR~\cite{dir} employ environment generators to simulate diverse distributional shifts in graph data. By minimizing the mean and variance of risks across multiple graphs and environments, these methods manage to capture invariant features that generalize well on unseen domains. However, these approaches heavily rely on the label information, which cannot be deployed in unsupervised settings, as \emph{Challenge~2} suggests. Regarding the second challenge, graph contrastive learning (GCL) has recently emerged as a prominent unsupervised graph learning framework. Although some of the GCL methods have demonstrated superior performance under in-distribution tests, their efficacy under out-of-distribution tests is still unclear, as they do not explicitly target on improving the OOD generalization ability. In summary, current methods struggle to effectively address both challenges simultaneously in the field of unsupervised OOD generalization for graph data.

In this work, we for the first time systematically study the robustness of current unsupervised graph learning methods~\cite{rosa,grace,graphcl,bgrl,dgi,mvgrl,gmae,costa} while facing distribution shifts.
By analyzing the common drawbacks of GCL methods, we propose a \textbf{M}odel-\textbf{A}gnostic \textbf{R}ecipe for \textbf{I}mproving \textbf{O}OD generalization of GCL methods (MARIO\footnote{Based on this recipe, we provide a shift-robust graph contrastive framework coined as MARIO}). To solve the above challenges, MARIO works on the two crucial components of a typical GCL method, \ie, view generation and representation contrasting, as depicted in Figure~\ref{fig:contrast}, and leaves the encoding models as an open choice for existing and future works for the sake of universal application. 
Concretely, MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations (solving \emph{Challenge 2}) and (ii) Invariance principle that incorporates adversarial data augmentation to acquire invariant representations (solving \emph{Challenge 1}).
Throughout extensive experiments, we observe that some graph contrastive methods are more robust to distribution shift, especially in datasets with artificial spurious features (\eg, GOOD-CBAS). Furthermore, our proposed model-agnostic recipe MARIO reaches comparable performance on the in-distribution test domain but shows superior performance on out-of-distribution test domain, regardless of what model is deployed for view encoding.

As the first work that investigates the efficacy of unsupervised graph learning methods while facing distribution shifts~\footnote{In this work, we specifically focus on node-level downstream tasks, which are more challenging than graph-level tasks due to the interconnected nature of instances within a graph.}, our paper's main contributions are  summarized as follows:
\begin{itemize}
    \item Through extensive experiments, we observe that some GCL methods are more robust to OOD tests than their supervised counterparts, providing insights for solving the challenge of graph OOD generalization.
    \item Motivated by invariant learning and information bottleneck, we analyze the limitations of the main components in current GCL frameworks for OOD generalization, and we further propose a \textbf{M}odel-\textbf{A}gnostic \textbf{R}ecipe for \textbf{I}mproving \textbf{O}OD generalization of GCL methods (MARIO). 
    \item The proposed model-agnostic recipe MARIO can be seamlessly deployed for various graph encoding models, achieving SOTA performance under the out-of-distribution test while reaching comparable performance under the in-distribution test.
\end{itemize}
