\section{Shift-Robust Graph Contrastive Learning~\label{sec:method}}
\noindent In this section, we will provide a \textbf{M}odel-\textbf{A}gnostic \textbf{R}ecipe for \textbf{I}mproving \textbf{O}OD generalizability of GCL methods, dubbed MARIO. 
A GCL training pipeline can be typically decomposed into three components: (i)~view generation, (ii)~view encoding, and (iii)~representation contrasting, as illustrated in Figure~\ref{fig:contrast}. 
MARIO works on the first~(view generation) and the last component~(representation contrasting), leaving the view encoding as an orthogonal design choice for GCL methods. Therefore it can be agnostically applied to different graph encoding models such as GCN~\cite{gcn}, GAT~\cite{gat}, GraphSAGE~\cite{sage} and etc. 

In the remaining content, we will first analyze the drawbacks of the two components in existing GCL methods for OOD generalization and introduce our proposed recipe correspondingly in Section~\ref{sec:aug} and Section~\ref{sec:cmi}. Finally, we will formulate the complete training scheme for graph OOD generalization problem in Section~\ref{sec:training}. \textbf{The complete derivation and more detailed illustration of all lemmas, theorems and corollaries can be found in the Appendix.}

\subsection{Recipe 1: Revisiting Graph Augmentation~\label{sec:aug}}
\noindent Data augmentation plays a crucial role in the transferability and generalization ability of contrastive learning~\cite{simclr,tian2020makes,huang2023towards,arcl}. It is proved that contrastive learning on distribution $\mathcal{G}$ with augmentation function $\tau$ essentially optimizes the supervised risk on the augmented distribution $\mathcal{G}_\tau$ instead of the original distribution $\mathcal{G}$~\cite{arcl}.
Consequently, if the downstream distribution $\mathcal{G}^{\mathrm{tar}}$ is similar to training distribution $\mathcal{G}$, the encoder obtained by contrastive learning shall perform well on it. 
Although the alignment loss in the contrastive learning achieves certain level of generalization, the learned representation distribution lacks domain invariance since it only takes expectation over the augmentation distribution $\pi$~\cite{arcl}. 
This limitation will severely hinder the OOD generalization ability of models~\cite{arcl,irm}. 

Our first improvement of graph contrastive learning based on graph augmentation is motivated by invariant learning~\cite{irm,arcl} which aims to learn domain-invariant features across $\{\mathcal{G}_{\tau}\}_{\tau \in \mathcal{T}}$ to solve \emph{Challenge 1}. Firstly, let us retrospect invariant risk minimization~\cite{irm}.

\begin{definition}[Invariant risk minimization,~IRM] If there is a classifier $p_{\omega^*}$ simultaneously optimal for all domains in $\mathcal{B}$, we will say that a data representation $g_\theta$ elicits an invariant predictor $p_{\omega^*} \circ g_\theta$ across a domain set $\mathcal{B}$:
\begin{equation}
p_{\omega^*} \in \arg \min _{p_\omega} \mathcal{R}(p_\omega \circ g_\theta ; \mathcal{G}) \text { for all } \mathcal{G} \in \mathcal{B},
\end{equation}
\label{def:irm}
where $\mathcal{R}$ denotes the risk of the predictor $p_\omega \circ g_\theta$ measured on domain $\mathcal{G}$.
\end{definition}


Definition~\ref{def:irm} yields the features that exhibit stable correlations with the target variable. It has been empirically and theoretically demonstrated that such features can enhance the generalization of models across distribution shifts in supervised learning~\cite{irm,ahuja2021invariance,li2022invariant}.
By setting $\mathcal{B}$ to the set of augmented graphs $\{\mathcal{G}_\tau\}_{\tau \in \mathcal{T}}$, this concept can be readily applied to graph contrastive learning methods with~\cite{groupdro,arcl}. The following definition of invariant alignment loss is the proposed objective for GCL-OOD problem, and we will draw the connection between Definition~\ref{def:irm} and Definition~\ref{def:invariant-alignment} in Theorem~\ref{thm:upper-bound}.

\begin{definition}[Invariant Alignment Loss]\label{def:invariant-alignment}
The invariant alignment loss $\mathcal{L}_{\mathrm{align}^*}$ of the graph encoder $g_\theta$ over the graph distribution $\mathcal{G}$ is defined as
\begin{equation}
\mathcal{L}_{\mathrm{align}^*}(g_{\theta} ; \mathcal{G}):=\underset{G \in \mathcal{G}}{\mathbb{E}} \sup _{\tau, \tau^{\prime} \in \mathcal{T}}\left\|g_{\theta}(\tau(G))-g_{\theta}\left(\tau^{\prime}(G)\right)\right\|^2.
\end{equation}
\end{definition}

The invariant alignment loss measures the difference between two representations under the most ``challenging'' two augmentations, rather than the trivial expectation as in Equation~\ref{equ:con}. Intuitively, it avoids the situation where the encoder behaves extremely differently in different $\mathcal{G}_\tau$. A special case of binary classification problem is analysed in Appendix \ref{app:case} to substantiate it.
Then we will discuss why the supremum operator can solve such a dilemma.
\begin{theorem}[Upper bound of variation across different domains~\cite{arcl}]\label{thm:upper-bound}
    For two augmentation functions $\tau$ and $\tau^{\prime}$, linear predictor $p$ and representation $g$, the variation across different domains is upper-bounded by
    \begin{equation}
        \sup _{\tau, \tau^{\prime} \in \mathcal{T}}\left|\mathcal{R}\left(p \circ g ; \mathcal{G}_\tau\right)-\mathcal{R}\left(p \circ g ; \mathcal{G}_{\tau^{\prime}}\right)\right| \leq c \cdot\|p\| \mathcal{L}_{\mathrm{align}^*}(f, \mathcal{G}) .
    \end{equation}
    Furthermore, fix $g$ and let $p_\tau \in \arg \min _p \mathcal{R}\left(p \circ g, \mathcal{G}_\tau\right)$. Then we have
    \begin{equation}
    \begin{aligned}
        |\mathcal{R}\left(p_\tau \circ g ; \mathcal{G}_{\tau^{\prime}}\right) & - \mathcal{R}\left(p_{\tau^{\prime}} \circ g ; \mathcal{G}_{\tau^{\prime}}\right)| \leq \\ & 2 c \cdot\left(\left\|p_\tau\right\|+\left\|p_{\tau^{\prime}}\right\|\right) \mathcal{L}_{\mathrm{align^*}}(g, \mathcal{G}).
    \end{aligned}
    \label{equ:proposition}
    \end{equation}
    \label{theorem:align}
\end{theorem}
The complete deduction and the connection between contrastive loss and downstream risk $\mathcal{R}$ can be found in Appendix~\ref{app:proof_recipe1}.
$\mathcal{L}_{\text{align}^*}$ replace the expectation operator over $\mathcal{T}$ with the supremum operator in $\mathcal{L}_{\text{align}}$ of Equation~\ref{equ:con}
resulting in $\mathcal{L}_{\text {align }}(g ; \mathcal{G}, \pi) \leq \mathcal{L}_{\text{align}^*}(g ; \mathcal{G})$ for all $g$ and $\pi$, and the augmentation function $\tau$ is randomly selected from the augmentation pool $\mathcal{T}$ according to a certain distribution $\pi$. 
When $\mathcal{L}_{\text {align}^*}$ is optimized to a small value, it indicates that $\mathcal{R}\left(p \circ g ; \mathcal{G}_\tau\right)$ varies a little under different augmentation functions $\tau$ resulting in the optimal representation for $\mathcal{G}_\tau$ is close to $\mathcal{G}_{\tau^\prime}$. That is, representation with smaller $\mathcal{L}_{\text {align}^*}$ tends to elicit the same linear optimal predictors across different domains, a property that does not hold for original alignment loss $\mathcal{L}_{\text {align}}$. In addition to pulling positive pairs together, $\mathcal{L}_{\text {align}^*}$ can ensure that this alignment is consistent and uniform across $\{\mathcal{G_\tau}\}_{\tau \in \mathcal{T}}$~\cite{arcl}.
 
\textbf{Adversarial augmentation.}
One problem with replacing $\mathcal{L}_{\text {align}}$ with  $\mathcal{L}_{\text {align}^*}$ is that it is intractable to estimate $\sup _{\tau, \tau^{\prime} \in \mathcal{T}}\left\|g(\tau(G))-g\left(\tau^{\prime}(G)\right)\right\|^2$, since it requires us to iterate over all augmentation methods. Previous work\cite{arcl} adopts multiple views and selects the worst pair to approximate the supermum operator. However, this strategy is straightforward and the quality is heavily dependent on the number of views. In order to find the worst case in the continuous space efficiently, we turn to the adversarial training~\cite{shafahi2019adversarial,flag,kim2020adversarial,suresh2021adversarial} to approximate the supermum operator:
% Therefore, we turn to the adversarial training~\cite{shafahi2019adversarial,flag,kim2020adversarial,suresh2021adversarial} to approximate the supermum operator: 
\begin{equation}
\min _{\theta} \mathbb{E}_{(G, Y) \sim \mathcal{G}}\left[\max _{\|{\delta}\|_p \leq \epsilon} L\left(g_{\theta}(X+{\delta}, A), Y\right)\right],
\end{equation}
where the inner loop maximizes the loss to approximate the most challenging perturbation, whose strength $\|\delta\|\leq \epsilon$ is strictly controlled so that it does not change the semantic labels of the original view, \eg, $\epsilon=1e-3$.
Considering the training efficiency, in this paper, we follow and further modify the supervised graph adversarial training framework FLAG \cite{flag} to accommodate unsupervised graph contrastive learning as follows:
\begin{equation}
\min _{\theta} \mathbb{E}_{(G_\alpha, G_\beta) \sim \mathcal{G}}\left[\max _{\|{\delta}\|_p \leq \epsilon} L\left(g_{\theta}(X_\alpha+{\delta}, A_\alpha), g_{\theta}(X_\beta, A_\beta)\right)\right].
\label{equ:unsup_ad}
\end{equation}

Note that it is not new for graph learning models to employ adversarial augmentation~\cite{kim2020adversarial,jiang2020robust,rosa}, our proposed recipe significantly differs from the previous works in terms of the objective: they focus on enhancing the robustness of models against adversarial attacks, while we leverage adversarial augmentation to specifically improve the OOD generalization of contrastive learning methods. By providing theoretical justifications and further elucidating the underlying principles, we contribute to a deeper understanding of the benefits of adversarial augmentation for OOD generalization in the contrastive learning paradigm.

\subsection{Recipe 2: Revisiting Representations Contrasting~\label{sec:cmi}} 
\noindent The vanilla contrastive loss like Equation~\ref{equ:con} aims to maximize the lower bound of the mutual information between positive pairs. However, there exists some redundant information (\ie, conditional mutual information) that can impede the generalization of graph contrastive learning. 
Our objective is to learn minimal sufficient representation related to downstream task which can effectively mitigate overfitting and demonstrate robustness against distribution shifts.
In this subsection, we introduce a recipe for representation contrasting to improve the generalization of GCL methods, motivated by the principle of information bottleneck~\cite{tishby_ib} to solve \emph{Challenge 2}. In short, we refer to the modified contrastive loss to get rid of supervision signals as well as learning generalized representations.
\begin{definition}[Information Bottleneck, IB]
    Let $X, Z, Y$ represent random variables of inputs, embeddings, and labels respectively. The formulation of information bottleneck's training objective is 
    \begin{equation}
    \arg\max_{\theta} R_{I B}(\theta)=I_{{\theta}}(Z; Y)-\beta I_{{\theta}}(Z; X),
    \label{equ:ib2}
    \end{equation}
    where $I_\theta$ represents mutual information estimator with parameters $\theta$, and $\beta > 0$ controls the trade-off between compression and the downstream task performance (larger $\beta$ leads to lower compression rate but high MI between the embedding $Z$ and the label $Y$). 
    \end{definition}
The definition above implies that the IB principle~\cite{dib,tishby_ib} aims to learn the minimal sufficient representation for the given task by maximizing the mutual information between the representation and the target (\emph{sufficiency}), and simultaneously constraining the mutual information between the representation and the input data (\emph{minimality}) as depicted in Figure~\ref{fig:mi}a.
By adopting this learning paradigm, the trained model can effectively mitigate overfitting and exhibit improved resilience against distribution shifts~\cite{gib,li2022invariant,ahuja2021invariance}. 

Motivated by this principle, we modify the vanilla contrastive loss~\cite{grace,rosa,costa} as Equation~\ref{equ:cmi}. 
The current graph contrastive learning methods aim to maximize the mutual information between positive pairs as depicted in Figure~\ref{fig:mi}b. Nevertheless, in scenarios where training labels are accessible, some shared information in the vanilla contrastive loss is redundant. In Figure~\ref{fig:mi}b, $V_1, V_2$ represents two augmented views from the same sample $\mathcal{G}$ and $U, V$ represent the representations of $V_1, V_2$, the redundant information which should be eliminated based on the IB principle. 
To precisely describe the redundant information here, let us introduce the concept of conditional mutual information~(CMI).

% ~(\ie, conditional mutual information~\cite{mackay2003information} in Definition~\ref{def:cmi}) 

% includes conditional mutual information $I(U;V \mid Y)$ 
% Figure environment removed
\begin{definition}[Conditional Mutual Information, CMI] 
The conditional mutual information $I(U; V\mid Y)$ measures the expected value of mutual information between $U$ and $V$ given $Y$ which can be formulated as 
\label{def:cmi}
\begin{equation}
\begin{aligned}
\operatorname{I}(U ; V \mid Y):&=\mathbb{E}_{y \sim Y}\left[D_{\mathrm{KL}}\left(P_{U, V \mid Y=y} \| P_{U \mid Y=y} P_{V \mid Y=y}\right)\right]\\&=\int_{\mathcal{Y}} D_{\mathrm{KL}}\left(P_{U, V \mid Y} \| P_{U \mid Y} P_{V \mid Y}\right) \mathrm{d} P_Y.
\end{aligned}
\end{equation}
\end{definition}
% the averaged shared information by X and Y but excludes the effect from Z [40]. This is because conditioning Z = z means taking Z = z as known a
To reduce the redundant information and hence improve the OOD generalization ability, we need to minimize the CMI between two views $U$ and $V$. However, it is intractable to estimate the equation above. In this work, we appeal to mutual information estimators~(\eg, Donsker-Varadhan estimator~\cite{donsker1975asymptotic,mine}, Jensen-Shannon estimator~\cite{fgan,mine}, InfoNCE~\cite{nce,cpc}) to estimate the lower bound of conditional mutual information.
Taking InfoNCE~\cite{cpc} as an example, the CMI objective can be approximated as
\begin{equation}
\begin{aligned}
\mathcal{L} _ {CMI} & \left(U, V\right)=-\mathbb{E}_{y \sim P_Y}\Big[\mathbb{E}_{u,v \sim P_{U,V\mid y}}\left[\operatorname{sim}\left(u, v\right)\right] \\
+ & \mathbb{E}_{u \sim P_{U \mid y}} \log \mathbb{E}_{v^- \sim P_{V\mid y}} \left[  e^{\operatorname{sim}\left(u, v^-\right)}\right]\Big]
\end{aligned}
\label{equ:con_cmi}
\end{equation}
where $\operatorname{sim}(x,y)$ is the cosine similarity function, the positive pairs are drawn from the conditional joint distribution, and negative pairs are drawn from the product of conditional marginal distribution. In short, we first sample $y \sim Y$, and then we sample positive and negative pairs from $P_{U,V \mid y}$ and $P_{U \mid y} P_{V \mid y}$\footnote{A classifier is required to determine which category the representations of the augmented inputs belong to. The true labels can be used for training the classifier.}.
The negative format of Equation~\ref{equ:con} is a lower bound of the conditional mutual information $I(U;V \mid Y)$. The proof can be found in Appendix~\ref{app:cmi}.

\textbf{Online Clustering.} The main challenge of applying the above approximation to our unsupervised pre-training lies in lack of labels~$Y$. To address this issue, we utilize online clustering techniques to obtain pseudo labels. These pseudo labels are iteratively refined during training, ensuring an increase in their mutual information with the ground-truth labels~\cite{pcl}. In order to incorporate the clustering into our pre-text task, we employ similar strategies as in \cite{swav,pcl}. We will initialize learnable prototypes $\boldsymbol{c}_i$ for each cluster $i$ and matrix $C=\left[c_1,c_2,\cdots,c_K\right]$ collects all column prototype vectors. For clustering, we can simply calculate the similarity between $K$ prototypes and node representations $u_i$ and $v_i$ for node~$i$:
\begin{equation}
\begin{aligned}
    &p_{u_i}\left(\hat{y} \mid u_i\right)=\operatorname{softmax}\left(C^T \cdot u_i\right),\\
    &q_{v_i}\left(\hat{y} \mid v_i\right)=\operatorname{softmax}\left(C^T \cdot v_i\right),
    \label{equ:pq}
\end{aligned}
\end{equation}
where the prototypes $C$ are updated by solving the problem of swapped prediction~\cite{swav}:
\begin{equation}
\begin{aligned}
    &\mathcal{L}_{\mathrm{clu}}\left(U,V\right)=\sum_i^B\left[\ell\left(p_{u_i},q_{v_i}\right)+\ell\left(q_{u_i}, p_{v_i}\right)\right], \\
    &\text{where} \quad \ell\left(p_{u_i}, q_{v_i}\right)=-\sum_k q_{v_i}^{(k)} \log p_{u_i}^{(k)}.
\end{aligned}
\label{equ:clu}
\end{equation}
The clustering loss focuses on contrasting nodes from different views by comparing cluster assignments rather than their representations.
% For assigning each node sample to its corresponding cluster, we will calculate the dot products between prototypes and representations $CZ^T$ where $\mathbf{C}=[c_1,c_2, ..., c_K], z=g_\theta\left(\mathbf{x}_{n t}\right) /\left\|g_\theta\left(\mathbf{x}_{n t}\right)\right\|_2$. And the node will be assigned to the cluster which has the largest value. 
However, there exists a trivial solution that all data samples are allocated to the same cluster. This problem can be solved by introducing another constraint of equal partition of the prototype assignment~\cite{swav}. Please refer to Appendix~\ref{app:online} for details.

For stable training, we use bi-level optimization~\cite{bilevel} for updating the encoder and prototypes (more details in Section~\ref{sec:training}). 
With these prototypes, we can infer the pseudo labels of node representations:
\begin{equation}
    \hat{Y} = \argmax C^TU.
\end{equation}
Hence our final shift-robust contrastive loss can be formulated as
\begin{equation}
\begin{aligned}
    \min_{g_\theta}\mathcal{L}_{\mathrm{rob}}&=\argmax I_{\theta}(U;V) - \gamma I_{\theta}(U;V \mid \hat{Y}) \\
                              &=\argmin_{g_\theta} \mathcal{L}_{\text{MI}} - \gamma \mathcal{L}_{\text{CMI}},
\end{aligned}
\label{equ:cmi}
\end{equation}
where $I_{\boldsymbol{\theta}}(U;V), I_{\boldsymbol{\theta}}(U;V \mid \hat{Y})$ can be instantiated as Equation~\ref{equ:con} and Equation~\ref{equ:cmi} respectively; $\gamma \ge 0$ controls the trade-off between compression and pre-text task's performance similar in Equation~\ref{equ:ib2}.
Intuitively, if the positive pairs have already shared the same semantic labels in the feature space (\ie, belong to the same cluster), the objective will reduce their shared information to avoid learning redundant information and overfitting~\cite{gib,dib} during training, which will bring performance gain in OOD generalization.
% It is a trade-off between compression and pre-text performance.


\subsection{Model Training~\label{sec:training}}\noindent Our shift-robust contrastive loss is formulated as Equation~\ref{equ:cmi} which involves maximizing the mutual information and minimizing the conditional mutual information at the same time. As mentioned in Section~\ref{sec:cmi}, bi-level optimization~\cite{bilevel} is used for updating the prototypes $C$ and other parameters in Equation~\ref{equ:cmi}:

\begin{equation}
\begin{aligned}
g^{k+1} &=\arg\min_g \mathcal{L}_{\text{rob}}(g^k, C^{k+1}; \mathcal{G}, \mathcal{T}), \\
C^{k+1} &=\arg\min_C \mathcal{L}_{\text{clu}}(g^k,C; \mathcal{G}, \mathcal{T}),
\end{aligned}
\end{equation}
where the parameter of the graph encoder $g_\theta$ is omitted for uncluttered notations. 
Concretely, we first fix the encoder $g$ and update the prototypes with $l$ steps of SGD to approximate the $\arg\min$ optimization, and then with the near-optimal prototypes $C$, we update the parameters of the encoder for $1$ step of SGD.
% \begin{equation}
% \begin{array}{lr}
% C^{k+1} \approx \underset{C}{\operatorname{argmin}} \mathcal{L}_{\text{clu}}\left(g^k, C ; \mathcal{G}, \mathcal{T}\right) & l\text{ steps of SGD},\\
% g^{k+1}=g^k-\eta \nabla_g \mathcal{L}_{\text{rob}}\left(g^k, C^{k+1} ; \mathcal{G}, \mathcal{T}\right) &\text{1 step SGD}.
% \end{array}
% \end{equation}
% Therefore, for computational efficiency, we resort to a stochastic optimization strategy where for each iteration k a mini-batch of data Dk in D is used instead of the whole dataset. However, this could lead to g quickly overfitting to the small mini-batch, leading to sub-optimal results. To alleviate such overfitting and allow stability, we resort to a truncated optimization with a proximal term by fixing the number of iterations to the inner optimization and use momentum based gradients. Such an approximation is common in deep learning (e.g., [11]). Our final iterative updates can be written as:
% gk+1
With the adversarial augmentation, our final optimization objective $\mathcal{L}_{\text{rob}}$ for the graph encoder is replaced by:
\begin{gather}
 % C \approx \underset{C}{\operatorname{argmin}} \mathcal{L}_{\text{clu}}\left(g, C ; \mathcal{G}, \mathcal{T}\right), \nonumber \\
 \min _{g} \mathbb{E}_{(G_\alpha, G_\beta) \sim \mathcal{G}}\left[\max _{\|{\delta}\|_p \leq \epsilon} \mathcal{L}_{\text{rob}}\left(g(X_\alpha+{\delta}, A_\alpha), g(X_\beta, A_\beta), C\right)\right].
\end{gather}
% Firstly, we optimize the prototypes through Equation~\ref{equ:clu}, and then we will update other parameters by Equation~\ref{equ:unsup_ad}. 
The algorithm can be summarized as in Algorithm~\ref{alg1}. The combination of the Invariant principle (Recipe 1) and the Information Bottleneck principle (Recipe 2) results in learned representations with enhanced OOD generalization, as demonstrated in the supervised setting~\cite{li2022invariant}. The ablation study in Section~\ref{sec:ablation} further confirms the effectiveness of this combination, leading to a synergistic effect where the whole is greater than the sum of its parts.
\begin{algorithm}[t]
    \textbf{Input}: Augmentation pool $\mathcal{T}$, ascent steps $M$, ascent step size $\epsilon$, encoder $g_\theta$, projector $p_\omega$, and training graph $G=(A, X)$ \\
    % \textbf{Output}:  \TODO{}\\
    \begin{algorithmic}[1] %[1] enables line numbers
        \WHILE{not converge}
        \STATE $\tau_{\alpha}, \tau_{\beta} \sim \mathcal{T}$
        \STATE $G_\alpha, G_\beta = \tau_{\alpha}(G),\tau_{\beta}(G)$
        \STATE $C=\arg\min_C\mathcal{L}_{\text{clu}} \left(g(G_\alpha), g(G_\beta), C\right)$
        
        \STATE $\delta_{0} \leftarrow U(-\epsilon, \epsilon)$
        \STATE $\hbar_{0} \leftarrow 0$
        \FOR{$\mathrm{t}=1 \ldots M$} 
        \STATE $Z_\alpha=p_{{\omega}} \circ g_{\theta}\left(X_\alpha+\delta_{t-1} , A_\alpha\right)$
        \STATE $Z_\beta = p_\omega \circ g_\theta(X_\beta, A_\beta)$
        \STATE $\hbar_{t} \leftarrow \hbar_{t-1}+\frac{1}{M} \cdot \nabla_{\theta, \omega} \mathcal{L}_{\text{rob}}\left(Z_\alpha, Z_\beta, C\right)$
        \STATE $\hbar_{\delta} \leftarrow \nabla_{\delta} \mathcal{L}_{\text{rob}}\left(Z_\alpha, Z_\beta, C\right)$
        \STATE $\delta_{t} \leftarrow \delta_{t-1}+\epsilon \cdot \hbar_{\delta} /\left\|\hbar_{\delta}\right\|_{F}$
        \ENDFOR
        \STATE $\theta \leftarrow \theta-\eta \cdot \hbar_{M, \theta}$
        \STATE $\omega \leftarrow \omega-\eta \cdot \hbar_{M, \omega}$
        \ENDWHILE
        % \STATE \textbf{return} \TODO{}
    \end{algorithmic}
    \caption{Algorithm for a shift-robust framework for graph contrastive learning.}
    \label{alg1}
\end{algorithm}

It is important to note that our optimization approach remains efficient. The optimization process of prototypes involves only a few steps (\eg, 10 steps in our experiments) of gradient descent, and the number of parameters involved is relatively small. Regarding adversarial augmentation, the inner loop iterates a moderate number of times, such as 3 iterations in our experiments, to approximate the optimal perturbation. During this process, we accumulate gradients in the inner loop to update the parameters in the outer loop. The time complexity of our method is linearly proportional to that of GRACE~\cite{grace}. Therefore, the additional computational overhead introduced by our optimization approach is acceptable.