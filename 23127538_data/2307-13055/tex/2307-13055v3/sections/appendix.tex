% \appendix
\appendices{
\input{sections/appendix_fold/proof_goal}


% \subsection{Proof of CMI lower bound}
% Formally, the  InfoNCE~\cite{cpc} loss is to maximize $\text{MI}(U;V)$ as:
% % \begin{equation}
% % \text { InfoNCE }=\sup _f \mathbb{E}_{\left(x_i, y_i\right) \sim P_{X, Y}}\left[\frac{1}{n} \sum_{i=1}^n \log \frac{e^{f\left(x_i, y_i\right)}}{\frac{1}{n} \sum_{j=1}^n e^{f\left(x_i, y_j\right)}}\right] \leq D_{\mathrm{KL}}\left(P_{X, Y} \| P_X P_Y\right)=\operatorname{MI}(X ; Y),
% % \end{equation}

% \begin{equation}
% \begin{aligned}
%     \text { InfoNCE }& =\sup _f \mathbb{E}_{\left(x_i, y_i\right) \sim P_{X, Y}}\left[\frac{1}{n} \sum_{i=1}^n \log \frac{e^{f\left(x_i, y_i\right)}}{\frac{1}{n} \sum_{j=1}^n e^{f\left(x_i, y_j\right)}}\right] \\
%                     & \leq D_{\mathrm{KL}}\left(P_{X, Y} \| P_X P_Y\right) =\operatorname{MI}(X ; Y) \\
% \end{aligned}
% \end{equation}

% \paragraph{CMI lower bound}
% \begin{equation}
% \mathrm{CCL} \leq D_{\mathrm{KL}}\left(P_{X, Y} \| \mathbb{E}_{P_Z}\left[P_{X \mid Z} P_{Y \mid Z}\right]\right)=\text { Weak-CMI }(U ; V \mid Y) \leq \operatorname{CMI}(U ; V \mid Y)
% \end{equation}
% \paragraph{}

\section{Special Case\label{app:case}}
The case adopted from~\cite{arcl} is used to prove the encoders learned through contrastive learning could behave extremely differently in different $\mathcal{G}_{\tau}$.
\begin{proposition}
    Consider a binary classification problem with data $(X_1, X_2) \sim \mathcal{N}(0, I_2)$. If $X_1 \geq 0$, the label $Y=1$, and the data augmentation is to multiply $X_2$ by standard normal noise:
    \begin{equation}
\begin{aligned}
\tau_\theta(X) & =\left(X_1, \theta \cdot X_2\right) \\
\theta & \sim \mathcal{N}(0,1)
\end{aligned}
\end{equation}
The transformation-induced domain set is $\mathcal{B}=\left\{\mathcal{G}_c: \mathcal{G}_c=\left(X_1, c \cdot X_2\right) \text { for } c \in \mathbb{R}\right\}$. Considering the 0-1 loss, $\forall \varepsilon \ge 0$, there holds representation $g$ and two domains $\mathcal{G}_c$ and $\mathcal{G}_{c^\prime}$ such that
\begin{equation}
\mathcal{L}_{\text {align }}(g ; \mathcal{G}, \pi)<\varepsilon
\end{equation}
but $g$ behaves extremely differently in different domains  $\mathcal{G}_c$ and $\mathcal{G}_{c^\prime}$:
\begin{equation}
\left|\mathcal{R}\left(g ; \mathcal{G}_c\right)-\mathcal{R}\left(g ; \mathcal{G}_{c^{\prime}}\right)\right| \geq \frac{1}{4}
\end{equation}
This instance\footnote{For simplicity, we assume the adjacent matrix is an identity matrix here.} illustrates that the obtained representation with small contrastive loss will still exhibit significantly varied performance over different augmentation-induced domains. The underlying idea behind this example lies in achieving a small $\mathcal{L}_{\text {align}}$ by aligning different augmentation-induced domains in an average sense, rather than a uniform one. Consequently, the representation might still encounter large alignment losses on certain infrequently chosen augmented domains.
\end{proposition}
\emph{Proof.} For $\varepsilon \ge 0$, let $t=\sqrt{\varepsilon}/2$ and $g(x_1, x_2)=x1+tx_2.$ Then, the alignment loss of $g$ satisfies:
\begin{equation}
\mathcal{L}_{\text {align }}(g ; \mathcal{G}, \pi)=t^2 \mathbb{E} X_2^2 \underset{\left(\theta_1, \theta_2\right) \sim \mathcal{N}(0,1)^2}{\mathbb{E}}\left(\theta_1-\theta_2\right)^2=2 t^2<\varepsilon .
\end{equation}
Set $c$ as 0 and $c^{\prime}$ as $1/t$, it is obviously that:
\begin{equation}
\mathcal{R}\left(g ; \mathcal{G}_c\right)=0
\end{equation}
but
\begin{equation}
\begin{aligned}
&\mathcal{R}\left(g ; \mathcal{G}_{c^{\prime}}\right)= \\ & P\left(X_1<0, X_1+X_2 \geq 0\right)+P\left(X_1 \geq 0, X_1+X_2 \leq 0\right)=\frac{1}{4}
\end{aligned}
\end{equation}
\section{Proof of Theorem~\ref{theorem:align}}\label{app:align}
\emph{Proof of Theorem~\ref{theorem:align}}:
\begin{equation}
\begin{aligned}
& \mathcal{R}\left(p \circ g ; \mathcal{G}_\tau\right)-\mathcal{R}\left(p \circ g ; \mathcal{G}_{\tau^{\prime}}\right) \\ 
& \quad =\underset{(G, Y) \sim \mathcal{G}}{\mathbb{E}}\left(|p \circ g(\tau(G))-Y|^2-\left|p \circ g\left(\tau^{\prime} (G)\right)-Y\right|^2\right) \\
& \quad =\underset{(G, Y) \sim \mathcal{G}}{\mathbb{E}}\left(p \circ g(\tau( G))-p \circ g\left(\tau^{\prime} (G)\right)\right)\left(\left(p \circ g(\tau (G))+ \right.\right.\\  & \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \left.\left. p \circ g\left(\tau^{\prime} (G)\right)\right)+2 Y\right) \\
& \quad \leq c \underset{(G, Y) \sim \mathcal{G}}{\mathbb{E}}\left\|p \circ g(\tau (G))-p \circ g\left(\tau^{\prime} (G)\right)\right\| \\
& \quad \leq c\|p\| \underset{(G, Y) \sim \mathcal{G}}{\mathbb{E}}\left\|g(\tau (G))-g\left(\tau^{\prime} (G)\right)\right\| \\
& \quad \leq c\|p\| \mathcal{L}_{\text{align}^*}(g)
\end{aligned}
\end{equation}
% For simplicity, we assume the loss function as MSE loss, resulting in the first equation. By rearranging the first equation, we can obtain the second equation. And

\input{sections/appendix_fold/proof_cmi}

\section{Experimental Details}
\subsection{Baselines\label{app:baselines}}
We consider empirical risk minimization (ERM), one OOD algorithm IRM and one graph-specific OOD algorithm EERM as supervised baselines. And we include 9 self-supervised methods as unsupervised baselines:
\begin{itemize}
    \item Invariant Risk Minimization (IRM~\cite{irm}) is an algorithm that seeks to learn data representations that are robust and generalize well across different environments by penalizing feature distributions that have different optimal linear classifiers for each environment
    \item EERM~\cite{eerm}  generates multiple graphs by environment generators and minimizes the mean and variance of risks from multiple environments to capture invariant features.
    \item Graph Autoencoder (GAE~\cite{gae}) is an encoder-decoder structure model. Given node attributes and structures, the encoder will compress node attributes into low-dimension latent space, and the decoder (dot-product) hopes to reconstruct existing links with compact node features.
    \item Variational Graph Autoencoder (VGAE~\cite{gae}) is similar to GAE but the node features are re-sampled from a normal distribution through a re-parameterization trick.
    \item GraphMAE~\cite{gmae} is a masked autoencoder. Different to GAE and VGAE, it will mask partial input node attributes firstly and then the encoder will compress the masked graph into latent space, finally a decoder aims to reconstruct the masked attributes. 
    % Compared with simple binary classification in GAE and VGAE, masked attributes reconstruction is more challenging, so GraphMAE can ach 
    \item Deep Graph Infomax (DGI~\cite{dgi}) is a node-graph contrastive method which contrasts the node representations and graph representation. First, it will apply the corrupt function to obtain a negative graph and two graphs will be fed into a shared GNN model to generate node embeddings. And a readout function will be applied on the original node embeddings to obtain graph-level representation. Corrupted embeddings and readout graph representation are considered as positive pairs, original node representations and readout graph representation are considered as positive pairs. 
    \item MVGRL~\cite{mvgrl} is similar to DGI but utilizes the information of multi-views. Firstly, it will use edge diffusion function to generate an augmented graph. And asymmetric encoders will be applied on the original graph and diffusion graph to acquire node embeddings. Next, a readout function is employed to derive graph-level representations. Original node representations and augmented graph-level representation are regarded positive pairs. Additionally, the augmented node representations and original graph-level representation are also considered as positive pairs. The negative pairs are constructed following \cite{dgi}.
    \item RoSA~\cite{rosa} is a robust self-aligned graph contrastive framework which does not require the explicit alignment of nodes in the positive pairs so that allows more flexible graph augmentation. It proposes the graph earth move distance (g-EMD) to calculate the distance between unaligned views to achieve self-alignment. Furthermore, it will use adversarial training to realize robust alignment.
    \item GRACE~\cite{grace} is node-node graph contrastive learning method. It designs two augmentation functions (\ie, removing edges and masking node features) to generate two augmented views. Then a shared graph model will be applied on augmented views to generate node embedding matrices. The node representations augmented from the same original node are regarded as positive pairs, otherwise are negative pairs. Lastly, pairwise loss (\eg, InfoNCE~\cite{cpc}) will be applied on these node matrices.
    \item BGRL~\cite{bgrl} is similar to GRACE but without negative samples which is motivated by BYOL~\cite{byol}.
    \item COSTA~\cite{costa} proposes feature augmentation to decrease the bias introduced by graph augmentation.
    \item SwAV~\cite{swav} is an unsupervised online clustering method which incorporates prototypes for clustering and employs swapped prediction for model training. It is originally designed for the computer vision domain, we adopt it into graph domain.
\end{itemize}


\subsection{Datasets\label{app:datasets}}
For GOOD-Cora, GOOD-Twitch, GOOD-CBAS and GOOD-WebKB datasets, they are all adopted from GOOD\cite{good} which is a comprehensive Graph OOD benchmark. These datasets contain both concept shift and covariate shift splits, for more details of splitting, please refer to Appendix A in \cite{good}. 

GOOD-Cora is a citation network that is derived from the full Cora dataset~\cite{bojchevskideep}. In the network, each node represents a scientific publication, and edges between nodes denote citation links. The task is to predict publication types (70-classification) of testing nodes. The data splits are generated based on two domain selections (\ie, word, degree). 
The word diversity selection is based on the count of selected words within a publication and is independent of the publication's label. On the other hand, the node degree selection ensures that the popularity of a paper does not influence its assigned class.

GOOD-Twitch is a gamer network dataset. In this dataset, each node represents a gamer, and the node features correspond to the games played by each gamer. The edges between nodes represent friendship connections among gamers. The binary classification task associated with this dataset involves predicting whether a user streams mature content or not. The data splits for GOOD-Twitch are based on the user language, ensuring that the prediction target is not biased by the specific language used by a user.

GOOD-CBAS is a synthetic dataset that is modified from the BA-Shapes dataset~\cite{ying2019gnnexplainer}. It involves a graph where 80 house-like motifs are attached to a base graph following the Barabási–Albert model, resulting in a graph with 300 nodes. The task associated with this dataset is to predict the role of each node within the graph. The roles can be classified into four classes, which include identifying whether a node belongs to the top, middle, or bottom of a house-like motif, or if it belongs to the base graph itself.
In contrast to using constant node features, the GOOD-CBAS dataset introduces colored features. This modification poses challenges for out-of-distribution (OOD) algorithms, as they need to handle differences in node colors within covariate splits and consider the correlations between node color and node labels within concept splits.

GOODWebKB is a network dataset that focuses on university webpages. Each node in the network represents a webpage, and the node features are derived from the words that appear on the webpage. The edges between nodes represent hyperlinks between webpages. The task associated with this dataset is a 5-class prediction task, where the goal is to predict the class of each webpage. The data splits for GOOD-WebKB are based on the domain of the university, ensuring that the classification of webpages is based on their word contents and link connections rather than any specific university features.

Amazon-Photo is a co-purchasing network that is widely used for evaluating the design of GNN models. In this network, each node corresponds to a specific product, and the presence of an edge between two products indicates that they are frequently purchased together by customers. In the original dataset, it is observed that the node features exhibit a significant correlation with the corresponding node labels. In order to evaluate the model's ability to generalize to out-of-distribution scenarios, it is necessary to introduce distribution shifts into the training and testing data. To achieve this, we adopt the strategies employed in the EERM~\cite{eerm}. Specifically, we leverage the available node features $X_1$ to create node labels $Y$ and spurious environment-sensitive features $X_2$. To elaborate, a randomly initialized GNN takes $X_1$ and the adjacency matrix as inputs and employs an argmax operation in the output layer to obtain one-hot vectors as node labels. Additionally, we employ another randomly initialized GNN that takes the concatenation of $Y$ and an environment id as input to generate spurious node features $X_2$. By combining these two sets of features, we obtain the input node features, $X = [X_1, X_2]$, which are used for both training and evaluation. This process is repeated to create ten graphs with distinct environment id's. Such a shift between different graphs can be considered as a concept shift~\cite{ood-survey}. Finally, one graph is allocated for training, another for validation, and the remaining graphs are used for evaluating the OOD generalization of the trained model.

Elliptic is a financial network that records the payment flows among transactions as time goes by. It consists of 49 graph snapshots which are collected at different times. Each graph snapshot is a network of Bitcoin transactions where each node represents one transaction and each edge denotes a payment flow. Partial nodes (approximately 20\%) are labeled as licit or illicit transactions and we hope to identify illicit transactions in the future. For data preprocessing, we adopt the same strategies in EERM~\cite{eerm}: removing extremely imbalanced snapshots and using the 7th-11th/12th-17th/17th-49th snapshots for training/validation/testing data. And 33 testing graph snapshots will be split into 9 test sets according to chronological order. In Figure~\ref{fig:rate}, we depict the label rate and positive label rate for training/validation/testing sets. It is evident that the varying positive label rates across different data sets are apparent. Indeed, the model needs to deal with the label distribution shifts from training to testing data.
% Figure environment removed
\subsection{Hyper-parameters~\label{app:hyper}}
For GOOD datasets, we adopt GraphSAINT~\cite{} as subsampling technique, while utilizing a 3-layer GCN~\cite{gcn} with 300 hidden units as the backbone following~\cite{good}. For the supervised baselines (\ie, ERM, IRM, EERM), we use the identical hyper-parameters specified in \cite{good}. For other unsupervised baselines, we conduct a grid search to find the best performance. Specifically, max training epoch ranges in \{50, 100, 200, 500, 600\} and learning rate ranges in \{1e-1,1e-2,1e-3,1e-4,1e-5\}, augmentation ratio range in $[0.1, 0.6]$. Regarding GraphMAE, the masking ratio ranges in \{0.25,0.5,0.75\}, and we use a one-layer GCN as the decoder. For MARIO, the specific hyper-parameters are listed in Table~\ref{tab:hyper}. For the adversarial augmentation, we set ascent steps $M$ as 3 and the ascent step size $\epsilon$ as 1e-3. For detailed hyper-parameters of all algorithms, please refer to \hyperlink{https://github.com/ZhuYun97/MARIO}{https://github.com/ZhuYun97/MARIO}.

For Amazon-Photo, we utilize 2-layer GCN with 128 hidden units as encoder, and we set $\tau, p_{f,1}, p_{f,2}, p_{e,1}, p_{e,2}, \gamma, |C|$ as 0.2, 0.2, 0.3, 0.2, 0.3, 0.1, 100 respectively and learning rate as 1e-4. Other hyper-parameters remain the same in EERM~\cite{eerm}.
Regarding Elliptic datasets, we employ 5-layer GraphSAGE~\cite{sage} with 32 hidden units as encoder following EERM~\cite{eerm}. And we set $\tau, p_{f,1}, p_{f,2}, p_{e,1}, p_{e,2}, \gamma, |C|$ as 0.8, 0.2, 0.3, 0.2, 0.3, 0.5, 120 respectively. The remaining hyper-parameters are consistent with EERM~\cite{eerm}.
% We list the specific hyper-parameters we used to report results:
\begin{table*}[htp]
\caption{Hyperparameters specifications for MARIO}
    \label{tab:hyper}
    \centering
    \begin{tabular}{c|cc|cc|cc|cc|cc}
    \toprule
              & \multicolumn{4}{c|}{GOOD-Cora}                                     & \multicolumn{2}{c|}{GOOD-CBAS}  & \multicolumn{2}{c|}{GOOD-Twitch} & \multicolumn{2}{c}{GOOD-WebKB} \\
              & \multicolumn{2}{c}{word}         & \multicolumn{2}{c|}{degree}     & \multicolumn{2}{c|}{color}      & \multicolumn{2}{c|}{language}    & \multicolumn{2}{c}{university}  \\
              & concept      & covariate         & concept      & covariate       & concept      & covariate       & concept      & covariate        & concept      & covariate       \\     \midrule                  
Model         & GCN          & GCN               & GCN          & GCN             & GCN          & GCN             & GCN          & GCN              & GCN          & GCN             \\
\# Layers     & 3            & 3                 & 3            & 3               & 3            & 3               & 3            & 3                & 3            & 3                \\
\# Hidden size & 300         & 300               & 300          & 300             & 300          & 300             & 300          & 300              & 300          & 300              \\
Epochs        & 100          & 150               & 100          & 100             & 200          & 500             & 600          & 200              & 100          & 500               \\
Learning rate & 1e-3         & 1e-3              & 1e-3         & 1e-3            & 1e-1         & 1e-2            & 1e-1         & 1e-1             & 1e-2         & 1e-2               \\ \midrule
$\tau$        & 0.2          & 0.2               & 0.2          & 0.2             & 0.2          & 0.2             & 0.2          & 0.2              & 0.5          & 0.5                 \\
$p_{f,1}$     & 0.3          & 0.3               & 0.0          & 0.3             & 0.2          & 0.2             & 0.2          & 0.2              & 0.5          & 0.2                \\
$p_{f,2}$     & 0.4          & 0.3               & 0.0          & 0.3             & 0.3          & 0.3             & 0.3          & 0.3              & 0.5          & 0.3                   \\
$p_{e,1}$     & 0.4          & 0.4               & 0.6          & 0.6             & 0.2          & 0.2             & 0.2          & 0.2              & 0.5          & 0.2                \\
$p_{e,2}$     & 0.5          & 0.4               & 0.6          & 0.6             & 0.3          & 0.3             & 0.3          & 0.3              & 0.5          & 0.3                \\
$|C|$         & 100          & 100               & 150          & 150             & 100          & 100             & 100          & 100              & 100          & 100               \\
$\gamma$      & 0.2          & 0.5               & 0.8          & 0.8             & 0.1          & 0.1             & 0.2          & 0.2              & 0.1          & 0.2                  \\ 
Pro. LR       & 1e-5         & 1e-5              & 1e-5         & 1e-5            & 1e-4         & 1e-3            & 1e-5         & 1e-3             & 1e-3         & 1e-3                \\\midrule
Epochs for LC & 500          & 200               & 100          & 100             & 100          & 2000            & 100          & 100              & 50           & 50                 \\
LR for LC     & 1e-4         & 1e-3              & 1e-3         & 1e-3            & 1e-2         & 1e-3            & 1e-1         & 1e-1             & 1e-1         & 1e-1                 \\
\bottomrule
    \end{tabular} 
    
\end{table*}

\subsection{Evaluation metrics}
In order to evaluate the pre-trained models, we adopt the linear evaluation protocol which is commonly used in self-supervised methods~\cite{simclr,moco,byol}. That is, we will train a linear classifier (\ie, one-layer MLP) on top of (frozen) representations learned by self-supervised methods. The training epochs (Epochs for LC in Table~\ref{tab:hyper}) and learning rate (LR for LC in Table~\ref{tab:hyper}) of the linear classifier are obtained by grid search.

\subsection{Computer infrastructures specifications}
For hardware, all experiments are conducted on a computer server with eight GeForce RTX 3090 GPUs with 24GB memory and 64 AMD EPYC 7302 CPUs. And our models are implemented by Pytorch Geometric 2.0.4~\cite{fey2019fast} and Pytorch 1.11.0~\cite{torch}. All datasets used in our
work are available on \hyperlink{https://github.com/divelab/GOOD}{https://github.com/divelab/GOOD} and \hyperlink{https://github.com/qitianwu/GraphOOD-EERM}{https://github.com/qitianwu/GraphOOD-EERM}.

\section{Additional Experiments}
\subsection{Graph classification~\label{app:graph_classification}}

\subsubsection{Datasets}
The PROTEINS dataset comprises protein data. During training, we use graphs ranging from 4 to 25 nodes, while during testing, we evaluate on graphs spanning from 6 to 620 nodes. 
The D\&D dataset is also protein-based and involves two distinct splitting methods, namely $\text{D\&D}_{200}$ and $\text{D\&D}_{300}$. 
For the $\text{D\&D}_{200}$ split, training is conducted on graphs containing 30 to 200 nodes, and testing is performed on graphs consisting of 201 to 5,748 nodes. As for the $\text{D\&D}_{300}$ split, training is carried out on 500 graphs ranging from 30 to 300 nodes, while testing is conducted on other graphs comprising 30 to 5,748 nodes.
\subsubsection{Experimental setups \& Baselines}
\textbf{Experimental setups.} For all graph classification datasets, we utilize 2-layer GCN containing 64 hidden units as graph encoder, and we choose global max pooling as the readout function. For other hyper-parameters, we set $\text{lr}, \tau, p_{f,1}, p_{f,2}, p_{e,1}, p_{e,2}, \gamma, |C|$ as 0.01, 0.5, 0.2, 0.3, 0.2, 0.3, 0.5, 40 respectively for self-supervised methods. For ERM, the learning rate ranges in \{1e-2, 1e-3, 5e-3, 1e-4\} and the number of training epochs is selected in \{50, 100, 200\}.
For evaluating pre-trained models, we use an off-the-shelf $\ell_2$-regularized LogisticRegression classifier from Scikit-Learn ~\cite{pedregosa2011scikit} using the 'liblinear' solver with a small hyperparameter search over the regularization strength to be between $\left\{2^{-10}, 2^{-9}, \ldots 2^9, 2^{10}\right\}$.

\noindent\textbf{Baselines.} GraphCL~\cite{graphcl} investigated the impact of different graph augmentations (\ie, node dropping, edge perturbation, attribute masking and subgraph sampling) on graph classification datasets. The framework is similar to  SimCLR~\cite{simclr} but specified for graph domain.

\subsubsection{Results}
From Table~\ref{tab:graph_class}, we can find GraphCL~\cite{graphcl} can achieve comparable performance with ERM even without labels. And our recipe can boost the OOD generalization ability of unsupervised methods and even surpasses ERM which means our recipe is also effective for graph classification task.
\begin{table}[htp]
\caption{Results of different methods on OOD graph classification tasks. We report the mean of Accuracy with standard deviation after 10 runs.}
    \label{tab:graph_class}
    \centering
    \begin{tabular}{c|ccc}
    \toprule
                    & $\text{PROTEIN}_{25}$ & $\text{D\&D}_{200}$ & $\text{D\&D}_{300}$\\ \midrule
\#Train/Test Graphs &  500/613              & 462/716         & 400/678        \\
\#Nodes Train       & 4-25                   & 30-200          & 30-300         \\
\#Nodes Test        & 6-620                  & 201-5748        & 30-5748        \\ \midrule
    ERM             & 77.24±0.95             & 44.25±5.16      & 67.91±1.60       \\
    GraphCL         & 76.92±0.91             & 48.12±6.43      & 67.82±1.29     \\
    GraphCL(+MARIO) & 78.08±0.97             & 51.62±5.47      & 69.13±1.23     \\ \bottomrule
    \end{tabular}
    
\end{table}

\subsection{Integrated with other methods~\label{app:other_methods}}
Our recipe is not only model-agnostic but also an add-on training scheme that can be adopted on most graph contrastive learning (GCL) methods. In Table~\ref{tab:other_methods}, we use our recipe to guide various GCL methods (GRACE, COSTA). MARIO can further boost these methods on both ID and OOD test performance.
\begin{table}[htp]
\caption{Results of various methods integrated with MARIO. We report the mean and standard deviation of Accuracy after 10 runs.}
    \label{tab:other_methods}
\scalebox{0.9}{
    \centering
    \begin{tabular}{c|cc|cc}
    \toprule
        \multirow{3}{*}{concept shift}  & \multicolumn{2}{c|}{GOOD-WebKB}  &  \multicolumn{2}{c}{GOOD-CBAS} \\
                 & \multicolumn{2}{c|}{university}  & \multicolumn{2}{c}{color}      \\ 
                 &        ID  & OOD                &    ID        & OOD             \\ \midrule
        GRACE    & 64.00±3.43 & 34.86±3.43         &  92.00±1.39  & 88.64±0.67       \\
GRACE (+MARIO)   & 65.67±2.81 & 37.15±2.37         &  94.36±1.21  & 91.28±1.10      \\ \midrule
        COSTA    & 61.66±2.58 & 32.39±2.13         &  93.50±2.62  & 89.29±3.11      \\
COSTA(+MARIO)    & 62.33±2.60 & 35.32±3.46         &  98.00±1.31  & 94.36±1.51      \\ \bottomrule
    \end{tabular} }
    
\end{table}
\subsection{Metric scores curves\label{app:curves}}
In the main text, we draw the metric scores curves of GOOD-CBAS datasets, here we draw more metric scores curves. We can draw the same conclusions described in the main text from Figure~\ref{fig:curve_cora1},\ref{fig:curve_cora2},\ref{fig:curve_twitch},\ref{fig:curve_webkb}.
% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed

\subsection{Feature Visualization\label{app:feature}}
In the main text, we visualize all node (including ID and OOD nodes) embeddings in the same figure, we can find our method can map ID and OOD nodes in a similar feature space and the margin of each cluster is larger which can prove the superiority of our method dealing with OOD data. In this subsection, we separately draw ID and OOD nodes in different figures. In Figure~\ref{fig:tsne_id_ood}, the first the depict the ID node embeddings and the second line draws the OOD node embeddings. We can also observe that MARIO performs better clustering compared to other methods.
% Figure environment removed
\footnotetext{Different from the main text, we select seven informative classes. And all models are trained under concept shift in degree domain.}

}