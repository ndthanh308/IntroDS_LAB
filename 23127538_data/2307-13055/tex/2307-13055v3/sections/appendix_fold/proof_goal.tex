\section{Proofs in Section~\ref{sec:pre}}\label{app:proof_recipe1}
In this section, we will illustrate some notations mentioned in Equation~\ref{equ:cl_down} firstly, and then we will provide a proof of Equation~\ref{equ:cl_down}.
\begin{definition}[$(\sigma,\delta)$-augmentation\cite{huang2023towards}] Let $C_k \subseteq \mathcal{G}$ denote the set of all points in class $k$. A graph augmentation set $\mathcal{T}$ can be referred to as a $(\sigma,\delta)$-augmentation on $\mathcal{G}$, where $\sigma \in (0,1]$ and $\delta > 0$. This is the case if, for every $k \in [K]$, there exists a subset $C_k^0 \subseteq C_k$ such that the following conditions hold:

\begin{equation}
\begin{aligned}
& P_{G \sim \mathcal{G}}\left(G \in C_k^0\right) \geq \sigma {P}_{G \sim \mathcal{G}}\left(G \in C_k\right), \\ 
& \text {and } \sup _{G_1, G_2 \in C_k^0} d_A\left(G_1, G_2\right) \leq \delta , \\
\end{aligned}
\end{equation}
where $d_{\mathcal{A}}\left(G_1, G_2\right):=\inf _{\tau_1, \tau_2 \in \mathcal{T}} d\left(\tau_1\left(G\right), \tau_2\left(G\right)\right)$ for some distance $d(\cdot, \cdot)$.

\end{definition}
This definition quantifies the concentration of augmented data. An augmentation set with a smaller value of $\delta$ and a larger value of $\sigma$ results in a more clustered arrangement of the original data. In other words, samples from the same class are closer to each other after augmentation. Consequently, one can anticipate that the learned representation $g_\theta$ will exhibit improved cluster performance. This principle was proposed in \cite{huang2023towards} and modified to a more practical scenario by \cite{arcl}.

\noindent\textbf{Proof of Equation~\ref{equ:cl_down}.} For simplicity, we omit the notations $\mathcal{G}$ and $\pi$ here and omit the parameters $\theta,\omega$ of $g_\theta$ and $p_\omega$, and use $G_1$ to denote the augmented data, use $t_k$ to represent $\mathcal{G}_{\pi}(C_k)$ and use $\mu_k$ to denote $\mu_k(g;\mathcal{G}_{\pi})$. Based on Theorem 2, Lemma B.1 in \cite{huang2023towards}, and Appendix B in \cite{arcl}, we have
\begin{equation}
\underset{G \in C_k}{\mathbb{E}}\left\|g\left(G_1\right)-\mu_k\right\| \leq c \sqrt{\frac{1}{t_k}} \mathcal{L}_{\text {align }}^{\frac{1}{4}}(g)+\zeta(\sigma, \delta)
\end{equation}
for some constant $c$, where
\begin{equation}
\zeta (\sigma, \delta):=4\left(1-\sigma\left(1-\frac{L \delta}{4}\right)\right)
\end{equation}
We can find $\zeta$ is decreasing with $\sigma$ and increasing with $\delta$. So, we can obtain:
\begin{equation}
\begin{aligned}
& c\left(\sum_{k=1}^K \sqrt{t_k}\right) \mathcal{L}_{\mathrm{align}}^{\frac{1}{4}}(f)+\zeta(\sigma, \delta) \\
& \geq \sum_{k=1}^K t_k \underset{G \in C_k}{\mathbb{E}}\left\|g\left(G_1\right)-\mu_k\right\| \\
& \geq \sum_{k=1}^K \frac{t_k}{\|p\|} \underset{G \in C_k}{\mathbb{E}} \underset{G_1 \in \tau(G)}{\mathbb{E}}\left\|p \circ g\left(G_1\right)-p \circ \mu_k\right\| \\
& \geq \sum_{k=1}^K \frac{t_k}{\|p\|} \underset{G \in C_k}{\mathbb{E}} \underset{G_1  \in \tau(G)}{\mathbb{E}}\left\|p \circ g\left(G_1\right)-e_k\right\| \\ 
&\quad\quad\quad-\frac{1}{\|p\|} \sum_{k=1}^K t_k\left\|e_k-p \circ \mu_k\right\| \\
& =\frac{1}{\|p\|} \mathcal{R}(p \circ g)-\frac{1}{\|p\|} \sum_{k=1}^K t_k\left\|e_k-p \circ \mu_k\right\|
\end{aligned}
\end{equation}
for all linear layer $p \in \mathcal{R}^{K \times d_1}$. Therefore, we obtain
\begin{equation}
\begin{aligned}
\mathcal{R}(p \circ g) & \leq c\|p\| \mathcal{L}_{\text {align }}^{\frac{1}{4}} \sum_{k=1}^K \sqrt{t_k}+\|p\| \zeta(\sigma, \delta)\\
&\quad\quad+\sum_{k=1}^K t_k\left\|e_k-p \circ \mu_k\right\| \\
& \leq c\|p\| \sqrt{K} \mathcal{L}_{\text {align}}^{\frac{1}{4}}+\|p\| \zeta(\sigma, \delta) \\ 
& \quad\quad +\sum_{k=1}^K t_k\left\|e_k-p \circ \mu_k\right\|.
\end{aligned}
\end{equation}