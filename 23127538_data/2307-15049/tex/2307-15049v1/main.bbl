\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{bengio2013estimating}
Yoshua Bengio.
\newblock Estimating or propagating gradients through stochastic neurons.
\newblock {\em arXiv preprint arXiv:1305.2982}, 2013.

\bibitem{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc~Van Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In {\em Eur. Conf. Comput. Vis.}, pages 446--461, 2014.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Adv. Neural Inform. Process. Syst.}, pages 1877--1901, 2020.

\bibitem{chen2022prompt}
Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun
  Zhang.
\newblock Prompt learning with optimal transport for vision-language models.
\newblock {\em arXiv preprint arXiv:2210.01253}, 2022.

\bibitem{chen2020just}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock {\em Adv. Neural Inform. Process. Syst.}, pages 2039--2050, 2020.

\bibitem{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 3606--3613,
  2014.

\bibitem{csordas2020neural}
R{\'o}bert Csord{\'a}s, Sjoerd van Steenkiste, and J{\"u}rgen Schmidhuber.
\newblock Are neural nets modular? inspecting functional modularity through
  differentiable weight masks.
\newblock {\em Int. Conf. Learn. Represent.}, 2020.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 248--255,
  2009.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{du2022learning}
Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
\newblock Learning to prompt for open-vocabulary object detection with
  vision-language model.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 14084--14093,
  2022.

\bibitem{engel1997retinotopic}
Stephen~A Engel, Gary~H Glover, and Brian~A Wandell.
\newblock Retinotopic organization in human visual cortex and the spatial
  precision of functional mri.
\newblock {\em Cerebral cortex (New York, NY: 1991)}, pages 181--192, 1997.

\bibitem{fei2004learning}
Li Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog. Worksh.}, pages
  178--178, 2004.

\bibitem{frome2013devise}
Andrea Frome, Greg~S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio
  Ranzato, and Tomas Mikolov.
\newblock Devise: A deep visual-semantic embedding model.
\newblock {\em Adv. Neural Inform. Process. Syst.}, 26, 2013.

\bibitem{gao2021clip}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang,
  Hongsheng Li, and Yu Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock {\em arXiv preprint arXiv:2110.04544}, 2021.

\bibitem{gao2020making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock {\em arXiv preprint arXiv:2012.15723}, 2020.

\bibitem{gao2018dynamic}
Xitong Gao, Yiren Zhao, {\L}ukasz Dudziak, Robert Mullins, and Cheng-zhong Xu.
\newblock Dynamic channel pruning: Feature boosting and suppression.
\newblock In {\em Int. Conf. Learn. Represent.}, 2018.

\bibitem{guo2022calip}
Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and
  Bin Cui.
\newblock Calip: Zero-shot enhancement of clip with parameter-free attention.
\newblock {\em arXiv preprint arXiv:2209.14169}, 2022.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 770--778,
  2016.

\bibitem{he2019filter}
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 4340--4349,
  2019.

\bibitem{helber2019eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and
  land cover classification.
\newblock {\em IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing}, pages 2217--2226, 2019.

\bibitem{hubel1962receptive}
David~H Hubel and Torsten~N Wiesel.
\newblock Receptive fields, binocular interaction and functional architecture
  in the cat's visual cortex.
\newblock {\em The Journal of physiology}, page 106, 1962.

\bibitem{jia2021scaling}
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em Int. Conf. Mach. Learn.}, pages 4904--4916, 2021.

\bibitem{jia2022visual}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath
  Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock {\em arXiv preprint arXiv:2203.12119}, 2022.

\bibitem{Krause_2013_ICCV_Workshops}
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em Int. Conf. Comput. Vis. Worksh.}, June 2013.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{liebenwein2019provable}
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus.
\newblock Provable filter pruning for efficient neural networks.
\newblock In {\em Int. Conf. Learn. Represent.}, 2019.

\bibitem{lin2020dynamic}
Tao Lin, Sebastian~U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi.
\newblock Dynamic model pruning with feedback.
\newblock {\em arXiv preprint arXiv:2006.07253}, 2020.

\bibitem{liu2017learning}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In {\em Int. Conf. Comput. Vis.}, pages 2736--2744, 2017.

\bibitem{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock {\em arXiv preprint arXiv:1810.05270}, 2018.

\bibitem{lu2022prompt}
Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian.
\newblock Prompt distribution learning.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 5206--5215,
  2022.

\bibitem{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock {\em arXiv preprint arXiv:1306.5151}, 2013.

\bibitem{mallya2018piggyback}
Arun Mallya, Dillon Davis, and Svetlana Lazebnik.
\newblock Piggyback: Adapting a single network to multiple tasks by learning to
  mask weights.
\newblock In {\em Eur. Conf. Comput. Vis.}, pages 67--82, 2018.

\bibitem{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 7765--7773,
  2018.

\bibitem{mansilla2021domain}
Lucas Mansilla, Rodrigo Echeveste, Diego~H Milone, and Enzo Ferrante.
\newblock Domain generalization via gradient surgery.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 6630--6638,
  2021.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em Indian Conference on Computer Vision, Graphics \& Image
  Processing}, pages 722--729, 2008.

\bibitem{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar.
\newblock Cats and dogs.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 3498--3505,
  2012.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em Int. Conf. Mach. Learn.}, pages 8748--8763, 2021.

\bibitem{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em Eur. Conf. Comput. Vis.}, pages 525--542, 2016.

\bibitem{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em Int. Conf. Mach. Learn.}, pages 5389--5400. PMLR, 2019.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{soomro2012ucf101}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock Ucf101: A dataset of 101 human actions classes from videos in the
  wild.
\newblock {\em arXiv preprint arXiv:1212.0402}, 2012.

\bibitem{sui2021chip}
Yang Sui, Miao Yin, Yi Xie, Huy Phan, Saman Aliari~Zonouz, and Bo Yuan.
\newblock Chip: Channel independence-based pruning for compact neural networks.
\newblock {\em Adv. Neural Inform. Process. Syst.}, 34, 2021.

\bibitem{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock {\em Adv. Neural Inform. Process. Syst.}, 2019.

\bibitem{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In {\em IEEE computer society conference on computer vision and
  pattern recognition}, pages 3485--3492, 2010.

\bibitem{yang2021nvit}
Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and Jan Kautz.
\newblock Nvit: Vision transformer compression and parameter redistribution.
\newblock {\em arXiv preprint arXiv:2110.04869}, 2021.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em Adv. Neural Inform. Process. Syst.}, pages 5824--5836, 2020.

\bibitem{zang2022unified}
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen~Change Loy.
\newblock Unified vision and language prompt learning.
\newblock {\em arXiv preprint arXiv:2210.07225}, 2022.

\bibitem{zeki1988functional}
Semir Zeki and Stewart Shipp.
\newblock The functional logic of cortical connections.
\newblock {\em Nature}, pages 311--317, 1988.

\bibitem{zhang2021can}
Dinghuai Zhang, Kartik Ahuja, Yilun Xu, Yisen Wang, and Aaron Courville.
\newblock Can subnetwork structure be the key to out-of-distribution
  generalization?
\newblock In {\em Int. Conf. Mach. Learn.}, pages 12356--12367, 2021.

\bibitem{zhang2023prompt}
Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li,
  Yu Qiao, and Peng Gao.
\newblock Prompt, generate, then cache: Cascade of foundation models makes
  strong few-shot learners.
\newblock {\em arXiv preprint arXiv:2303.02151}, 2023.

\bibitem{zhang2022tip}
Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu
  Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free adaption of clip for few-shot
  classification.
\newblock In {\em Eur. Conf. Comput. Vis.}, pages 493--510, 2022.

\bibitem{zhao2020masking}
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch{\"u}tze.
\newblock Masking as an efficient alternative to finetuning for pretrained
  language models.
\newblock {\em arXiv preprint arXiv:2004.12406}, 2020.

\bibitem{zhou2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock {\em Adv. Neural Inform. Process. Syst.}, 32, 2019.

\bibitem{zhou2022conditional}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In {\em IEEE Conf. Comput. Vis. Pattern Recog.}, pages 16816--16825,
  2022.

\bibitem{zhou2022learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock {\em Int. J. Comput. Vis.}, pages 2337--2348, 2022.

\bibitem{zhu2022prompt}
Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang.
\newblock Prompt-aligned gradient for prompt tuning.
\newblock {\em arXiv preprint arXiv:2205.14865}, 2022.

\end{thebibliography}
