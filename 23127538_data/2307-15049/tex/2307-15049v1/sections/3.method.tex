\section{Method}\label{sec:method}

% Figure environment removed


In this section, we introduce the Regularized Mask
Tuning (RMT) method in detail, which aims to better adapt CLIP to downstream tasks.  
%
% In~\cref{sec:method_clip}, we first revisit CLIP and existing parameter-efficient adaptation methods for few-shot transfer tasks. 
%
% In~\cref{sec:method_parameter_prompt}, we elaborate on the proposed parameter prompt. In~\cref{sec:method_collaborate}, we introduce the way to combine the proposed prompt with other parameter-efficient adaptation methods, \eg prompt-based CoOP~\cite{zhou2022learning} and adapter-based TIP-Adapter~\cite{zhang2022tip}.


\subsection{Preliminaries of CLIP}
\label{sec:method_clip}
CLIP~\cite{radford2021learning} mainly consists of two components: image encoder $\mG_{I}(\vtheta)$ and text encoder $\mG_{T}(\bm{\beta})$, which are designed to project image and text into the same feature embedding space. 
% Concretely, the text encoder is built with the transformer for extracting text features. Meanwhile, the image encoder is used to extract image features that have the same channel dimension as the text features.
% The architecture of the image encoder can be ResNet \cite{he2016deep} or ViT \cite{dosovitskiy2020image}. 
% Cosine similarity between text and image features is utilized for alignment in CLIP.
% The CLIP is trained with a large dataset, which contains 400 million image and text pairs collected from the internet.
% The CLIP, benefiting from the 400 million text-image pairs from the web and the multi-modality structure, achieves exceptional zero-shot transfer capacity in the downstream tasks. 
Specifically, given the images $\{\boldsymbol{x}_1, \boldsymbol{x}_2,\cdots,\boldsymbol{x}_m\}$ and a set of corresponding categories, the image recognition task aims to classify an image to a specific category. Here, the $m$ denotes the number of images in the dataset. 
% In general, to evaluate the transferability of the pre-trained V\&L model, we follow the few-shot evaluation protocol. In other words, we use 1, 2, 4, 8, and 16 shots (\textit{i.e.,} $m=S\times k$, $S$ is the number of shots) for training respectively, and deploying models in the full test sets.
To zero-shot adapt CLIP to the image recognition task, the name of category $\boldsymbol{y}_i$ is filled into a set of words, \textit{e.g.} ``a photo of a [class]'', to construct a hand-craft text prompt $\boldsymbol{t}_i$ as the input of text encoder. 
The possibility of an image $\boldsymbol{x}_j$ being assigned to class $\boldsymbol{y}_i$ is formulated as following:
\begin{gather}
    \boldsymbol{g}_i = G_T(\boldsymbol{t}_i;\vtheta), \boldsymbol{f}_j = G_I(\boldsymbol{x}_j;\bm{\beta}), \\
    p(\boldsymbol{y}=i \mid \boldsymbol{x}_j)=\frac{\exp \left(\cos \left(\boldsymbol{g}_i, \boldsymbol{f}_j\right) / \tau\right)}{\sum_{n=1}^k \exp \left(\cos \left(\boldsymbol{g}_n, \boldsymbol{f}_j\right) / \tau\right)},
    \label{eq.possibility}
\end{gather}
where the $\text{cos}(\cdot, \cdot)$ denotes the cosine similarity between two inputs and $\tau$ is a learnable temperature parameter. 

% \subsection{Binary Mask Tuning}
% \label{sec:method_parameter_prompt}
% Different from the common prompt tuning methods that adopt the image/text prompts, we design a new type of prompt, termed parameter prompt, which masks the network parameters under a learnable selection. Specifically, we apply binary masks on CLIP to search a subset of pre-trained parameters, which are relevant to downstream tasks. 
% % For CLIP, the parameter smaller than 1e-8 takes 50.31\% in the weight metrics of all fully connected layer in the text encoder. Considering the NLP model is sparse,
% %(why only image encoder)
% % We only apply the binary mask on the image encoder.

% In this way, to better understand the proposed parameter prompt, let's take a fully connected layer as an example (other layers, such as convolution and attention, are the same operation). Specifically, given a fully connected layer, the input and output of which are $\boldsymbol{x}\in \mathcal{R}^{c_{in}}$ and $\boldsymbol{y}\in \mathcal{R}^{c_{out}}$, respectively. The $c_{in}$ denotes the channel dimension of input, and the $c_{out}$ refers to the channel dimension of output. The weight matrix of the fully connected layer is $\boldsymbol{W}= \mathcal{R}^{c_{out}\times c_{in}} $, which can be expanded as:
% % $\{\mathbf{w}_{1,1},\mathbf{w}_{1,2},\mathbf{w}_{i,j},\dots,\mathbf{w}_{c_{in}, c_{out}}\} \in$
% \begin{align}
%     \label{eq.weight_matrix}
%     \boldsymbol{W} & = \left[\begin{array}{ccc}
% {\boldsymbol{w}_{1,1}} & \cdots & {\boldsymbol{w}_{1, c_{in}}} \\
% \vdots & \ddots & \vdots \\
% {\boldsymbol{w}_{c_{out}, 1}} & \cdots & {\boldsymbol{w}_{c_{out}, c_{in}}}
% \end{array}\right].
% \end{align}
% The fully connected layer can be formulated as follows:
% \begin{equation}
%     \label{eq.linear}
%     \boldsymbol{y} = \boldsymbol{W}\boldsymbol{x}+\boldsymbol{b},
% \end{equation}
% where $\boldsymbol{b} \in \mathcal{R}^{c_{out}} $ is the bias vector. For each weight matrix, we employ a learnable matrix $\boldsymbol{M}$ with initializing value $\pi$, which has the same shape as the weight matrix $\boldsymbol{W}$. We set a hard threshold $\alpha$ to binarized the learnable matrix $\boldsymbol{M}$ as follows:
% \begin{equation}
%     \label{eq.mask}
%     \boldsymbol{m}^b_{i,j} = \begin{cases}
%     1, & \text{if  } \boldsymbol{m}_{i,j}\geq \alpha \\
%     0, & \text{if  } \boldsymbol{m}_{i,j}< \alpha
%     \end{cases}
%     ,
% \end{equation}
% where the $\boldsymbol{m}_{i,j}$ denotes the parameter in the $i$-th row and $j$-th column of learnable matrix $\boldsymbol{M}$, and $\boldsymbol{m}^b_{i,j}$ denotes the corresponding parameter in binary mask $\boldsymbol{M}^b$. Then the updated weight matrix $\boldsymbol{W}'$ is obtained as following:
% \begin{align}
%     \label{eq.weight_matrix}
%     \boldsymbol{W}' = \boldsymbol{W}\odot \boldsymbol{M}^b, & 
% %     = \left[\begin{array}{ccc}
% % {\boldsymbol{w}_{1,1}\boldsymbol{m}^b_{1,1}} & \cdots & {\boldsymbol{w}_{1, c_{in}}\boldsymbol{m}^b_{1, c_{in}}} \\
% % \vdots & \ddots & \vdots \\
% % {\boldsymbol{w}_{c_{out}, 1}\boldsymbol{m}^b_{c_{out}, 1}} & \cdots & {\boldsymbol{w}_{c_{out}, c_{in}}\boldsymbol{m}^b_{c_{out}, c_{in}}}
% % \end{array}\right],
% \end{align}
% where the $\odot$ refers to Hadamard product.

% Since previous works \cite{zhao2020masking,mallya2018packnet} have shown that training task-specific bias has not a significant improvement for the downstream tasks, we only apply the binary mask on the weight matrix to lower the computation cost. Thus, the updated fully connected layer can be formulated as $\boldsymbol{y} = \boldsymbol{W}'\boldsymbol{x}+\boldsymbol{b}$. This method can be easily extended to convolutional layers, where we also only apply binary masks on the weight matrix.
% The binary mask is optimized with the cross-entropy classification loss.
% Importantly, since the binarized function shown in \cref{eq.mask} is non-differentiable, we use the gradient of $\boldsymbol{M}^b$ as a noisy estimator to update the learnable matrix $\boldsymbol{M}$, following the previous work \cite{zhao2020masking,lin2020dynamic}. The optimization can be formulated as:
% \begin{equation}
%     \label{eq.optimize}
%     \boldsymbol{m}_{i,j} \gets  \boldsymbol{m}_{i,j}-\gamma \frac{\partial \mathcal{L}_{\text{ce}}}{\partial\boldsymbol{m}^b_{i,j}},
% \end{equation}
% where the $\gamma$ denotes the learning rate that controls the sparsity of mask, and the $ \mathcal{L}_{\text{ce}}$ denotes the loss value obtained from the cross entropy classification loss function.


\subsection{Mask Tuning}
Although CLIP has strong zero-shot performance, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but may get concealed by some unnecessary information emerging in the upstream pre-training. To uncover the hidden valuable knowledge in pre-trained weights, we aim to identify the weights required by a downstream task, termed as a neural pathway, to facilitate few-shot learning~\cite{zhou2022learning}. 
%
Concretely, we take the parameters $\vtheta$ of the image encoder as an example for analysis. Given $\vtheta=(\vtheta_1,\cdots,\vtheta_n)^T\in\para\subset\sR^N$ where $N$ is the parameter volume, our aim is to learn a binary mask matrix $ \mM^{bin}$ as a downstream-related neural pathway:
\begin{align}
    \label{eq.weight_matrix}
    \vtheta_{\gM} := \vtheta\odot \mM^{bin}, & 
%     = \left[\begin{array}{ccc}
% {\boldsymbol{w}_{1,1}\boldsymbol{m}^b_{1,1}} & \cdots & {\boldsymbol{w}_{1, c_{in}}\boldsymbol{m}^b_{1, c_{in}}} \\
% \vdots & \ddots & \vdots \\
% {\boldsymbol{w}_{c_{out}, 1}\boldsymbol{m}^b_{c_{out}, 1}} & \cdots & {\boldsymbol{w}_{c_out}, c_{in}}\boldsymbol{m}^b_{c_{out}, c_{in}}}
% \end{array}\right],
\end{align}
where the $\odot$ refers to Hadamard product and $\vtheta_{\gN}^T\in\para_{\gN}\subset\sR^n, n\ll N,$ refers to a small subset of pre-trained weights. By utilizing solely the parameters of the subset of pre-trained weights, it is adequate to transfer to the downstream domain. Since the binarized function shown in \cref{eq.weight_matrix} is non-differentiable, we conduct a real-valued mask weight $\boldsymbol{M}$ and pass it through an element-wise thresholding binary function to obtain
$\boldsymbol{M}^{bin}$. Meanwhile, we use the gradient $\frac{\partial \mathcal{L}_{\text{ce}}}{\partial\boldsymbol{M}^{bin}}$ as a noisy estimator of $\frac{\partial \mathcal{L}_{\text{ce}}}{\partial\boldsymbol{M}}$ to update the $\boldsymbol{M}$, following the previous work \cite{zhao2020masking,lin2020dynamic}. The optimization can be formulated as:
\begin{equation}
    \label{eq.optimize}
    \boldsymbol{M} \gets \boldsymbol{M}-\gamma \frac{\partial \mathcal{L}_{\text{ce}}}{\partial\boldsymbol{M}^{bin}},
\end{equation}
where the $\gamma$ denotes the learning rate that controls the sparsity of mask, and $\mathcal{L}_{\text{ce}}$ denotes the Cross-Entropy (CE) loss. The binary mask $\mM^{bin}=\mathcal{I}[\mM>\alpha]$, where $\alpha$ is a hard threshold.
An astonishing discovery is that setting \textbf{0.16\%}
% \textbf{0.58\%}
parameters of CLIP image encoder to 0 results in a performance improvement of \textbf{44.40\%} compared to the zero-shot performance in the EuroSAT. This discovery supports the notion that certain parameters contain valuable knowledge for downstream tasks, which are also duplicated in redundant parameters. Consequently, selecting an efficient neural pathway from pre-trained weights significantly influences performance.


% Different from the common prompt/adapter tuning methods that add the image/text prompts or extra architecture, we design a new type of tuning technology, termed binary tuning, which masks the network parameters under a learnable selection. Specifically, we apply binary masks on CLIP to search a subset of pre-trained parameters, which are relevant to downstream tasks. 
% For CLIP, the parameter smaller than 1e-8 takes 50.31\% in the weight metrics of all fully connected layer in the text encoder. Considering the NLP model is sparse,
% %(why only image encoder)




\paragraph{Which layers to apply binary mask?} 

% Figure environment removed
While this method is capable of efficiently identifying the parameter pathway that is most suitable for the downstream task, the sheer number of mask parameters to be trained can be overwhelming. As a result, it is crucial to devise a means of assessing parameter importance, by identifying the relevant neural pathway based on these significant parameters to mask. This method represents a more balanced approach, one that strikes a delicate balance between computational effort and overall performance. Our goal is to identify a subset of weights that can be effectively transferred to the downstream task while retaining important general information about the model. 
% To achieve this, we seek to scale down the absolute value of these weights, allowing us to selectively retain the most critical parameters while discarding the less relevant ones. 

% Let $\theta=\vtheta_h$ denote the $i$-element of the whole parameter vector $\vtheta$ for simplicity. Scaling it by factor $\alpha$ will produce cross-entropy loss $\ce(\alpha\theta)$. Let $\rho=(1-\alpha)(\theta\frac{\partial\ce}{\partial\theta})^{-1}$, we can rewrite it as
% \begin{equation}\label{eq:scale}
%     \ce(\alpha\theta)=\ce(\theta(1-\rho\theta\frac{\partial\ce}{\partial\theta})).
% \end{equation}
% Through Taylor expansions~\tocite{}, we know that 
% \begin{equation}\label{eq:taylor}
%     \begin{aligned}
%     \ce(\alpha\theta)\approx\ce(\theta)-\rho\theta^2\frac{\partial\ce}{\partial\theta}^2<\ce(\theta)
%     \end{aligned}
% \end{equation}
% as long as $0<\rho\ll1$. To make~\cref{eq:scale} is a scaling down, we need (when $0<\rho\ll1$)
% \begin{equation}\label{eq:equiv}
%     0<\alpha=1-\rho\theta\frac{\partial\ce}{\partial\theta}<1\Leftrightarrow \theta\frac{\partial\ce}{\partial\theta}>0.
% \end{equation}
% In conclusion, $\theta\frac{\ce}{\partial\theta}>0$ will indicate whether scaling down the $h$-th parameter will decrease the cross-entropy loss and thus identify whether $\theta_h$ is an important weight for the given domain. 
% in the fine-tuned model 
To achieve this, we analyze the change in mask weight $\mM$ for each layer after training on the target dataset with the CE loss, \ie, $\Delta = \sum \gamma* \frac{\partial\ce}{\partial\mM}$.
% $\Delta = \sum \gamma* \theta\frac{\ce}{\partial\theta}$.
These parameters come from two types of layers -- (1) multi-head self-attention (MSHA) layers and (2) multilayer perception (MLP) layers. We present the mean $\Delta$ when the mask weight is training on the 11 datasets in~\cref{fig:change}.
% \cref{fig:change} shows the mean $\Delta$ when the model is fine-tuned on the 11 datasets. 
As we see, the MSHA layer parameters have relatively higher $\Delta$ compared to the MLP layer. Moreover, MSHA layers are $20\%$ of the total parameter count in the model and achieve the same performance (\eg, 83.96\% vs. 83.96\% on average over 11 datasets). This suggests binary attention mask $\boldsymbol{M}^{bin}_a$ plays a significant role during fine-tuning, and we leverage that in our method as shown in~\cref{fig:arch}. We name this method \textbf{Attention Mask Tuning} (AMT). Moreover, we term binary mask on all layers as Parameter Mask Tuning (PMT) and MLP layers as Multilayer perception Mask Tuning (MMT), respectively, for distinction.


% \begin{theorem}[Identification of Concept Neurons]\label{th}
% For a given parameter $\theta\in\vtheta$, slightly scaling down it can decrease the concept-implanting loss, which is equivalent to
% \begin{equation}
%     \theta\frac{\partial\ce}{\partial\theta}>0,
% \end{equation}
% and the decreasing value is proportional to $(\theta\frac{\partial\ce}{\partial\theta})^2$. Thus $\theta$ is a concept neuron if and only if $\theta\frac{\partial\ce}{\partial\theta}>0$.
% \end{theorem}
% Following this theorem, a naive method to detect whether $\theta\in\vtheta$ is a concept neuron can be that we sample $K$ different values $\theta^1,\cdots,\theta^K$ ranging from zero to $\theta$, and if 
% \begin{equation}
%     \theta^1\frac{\partial\ce}{\partial\theta}(\theta^1)+\cdots\theta^K\frac{\partial\ce}{\partial\theta}(\theta^K)>\tau>0,
% \end{equation}
% where $\frac{\partial\ce}{\partial\theta}(\theta^k)$ is the gradient at point $\theta=\theta^i,k\in[K]$, $\tau$ is a constant hyper-parameter, then $\theta$ is a concept neuron. 

% We deduce a self-adaptive sampling method for the choices of $\theta^1,\cdots,\theta^K$. We set $\theta^1=\theta$, and 
% \begin{equation}\label{eq:sampling}
%     \theta^{k+1}=\theta^k(1-\rho\theta^k\frac{\partial\ce}{\partial\theta}(\theta^k)),k=1,\cdots,K-1.
% \end{equation}
% It will sample more densely in the neighborhood where $\vert\theta\frac{\partial\ce}{\partial\theta}\vert$ is small, thus ambiguous to indicate the valuence, while sparsely when the valuence is obvious. Parameters with more ambiguous regions will tend to be excluded from concept neurons. Thus the identification of concept neurons will be more cautious and robust.

% For all the parameters $\vtheta$ of the diffusion model $\diff$, we can use~\cref{algorithm:CN} to compute a concept neuron mask parallelistically to indicate whether each neuron is or not a concept neuron. The main computation~\cref{eq:main} can be further accelerated using the Newton-Leibniz Law of the Calculus~\cite{rudin1976principles}. We use this accelerated version in practice. See appendix for detail.



% \begin{equation}
% \label{eq.classification_loss}
%   \mathcal{L}_{\text{ce}}=\frac{1}{N}\sum,
% \end{equation}

% \begin{algorithm}[t]
% 	%\textsl{}\setstretch{1.8}
% 	\caption{Parameter Prompt Learning}
% 	\label{alg.PPL}
% 	\KwIn{The image encoder $\boldsymbol{G}_I$ and text encoder $\boldsymbol{G}_T$ of CLIP, data $\mathcal{D}_{train}$ for downstream task, init value $\pi$, hard threshold $\alpha$.}
% 	\KwResult{Binary mask $\boldsymbol{M}^b$ for image encoder}
% 	Construct hand-craft text prompt set $\mathcal{T}=\{\boldsymbol{t}_c\}^{\mathcal{C}}_{c=1}$ with the label set of $\mathcal{D}_{train}$
	
% 	Extract text features $\boldsymbol{g}_c=\boldsymbol{G}_T(\boldsymbol{t}_c), c=1,2,\cdots,\mathcal{C}$
	
% 	Initialize the learnable matrix $\boldsymbol{M}$ with init value $\pi$
	
% 	\For{$t \in [1, T]$}{
% 	Sample a mini-batch $\{\boldsymbol{x}_i,\boldsymbol{y}_i\}^{N}_{i=1}$ from $\mathcal{D}_{train}$
	
% 	Apply hard threshold $\alpha$ on the learnable matrix $\boldsymbol{M}$ to get binary matrix $\boldsymbol{M}^b$ according to \cref{eq.mask}
	
% % 	\For{each weight matrix $\boldsymbol{W}$ in $\boldsymbol{G}_I$}{
% 	Update $\boldsymbol{W}$ to $\boldsymbol{W}'$ according to \cref{eq.weight_matrix}
% % 	}
	
% 	Extract image features $\boldsymbol{f}_i $%=\boldsymbol{G}_I'(x_i)$
	
% 	Calculate the affinity between image and text features $\boldsymbol{s}_{i,c}=sim(\boldsymbol{f}_i,\boldsymbol{g}_c), c=1,2,\cdots,\mathcal{C}$
	
% 	Compute cross entropy classification loss with  $\mathcal{L}_{\text{ce}}(\boldsymbol{s}_{i}, \boldsymbol{y}_i)$
	
% 	Optimize the learnable matrix $\boldsymbol{M}$ by gradient descent according to \cref{eq.optimize}
	
% 	}
% \end{algorithm}



\subsection{Gradient Dropout Regularity}

Stochastic Gradient Descent (SGD)~\cite{robbins1951stochastic} is a popular optimization algorithm used in machine learning to minimize a loss function during training. SGD works by randomly selecting a small subset of training examples to compute the gradient of the loss function. It can help to avoid overfitting, as it adds some level of randomness to the gradient updates. This helps to prevent the algorithm from getting stuck in local minima and encourages exploration of the solution space. But in few-shot learning scenarios, particularly in 1-shot or 2-shot learning, the mini-batch data is typically derived from the entire training set to compute the gradient. Thus, this approach lacks the stochastic property of traditional SGD, which can lead to the overfitting of the model to the training data.

In order to make our binary tuning method better suited for few-shot scenarios, we develop Gradient Dropout Regularity formalism that randomly introduces the gradient regularity to reduce the amount of overfitting that occurs and help the model generalize better to new domains. 
We deem the zero-shot CLIP predictions as the general knowledge and the label from the downstream task as the target-specific knowledge.
Then we introduce the Kullback-Leibler (KL) divergence between them to regularize the gradient.
To implement Gradient Dropout Regularity, we first define the Gradient Retaining Purity $\mathcal{P}$ as follows 
\begin{equation} 
\label{eq:psp} 
\mathcal{P} = \frac{1}{2}\left(1+\frac{\text{sgn}(\gce)\left(\gce + \gkl\right)}{|\gce| + |\gkl|}\right),
\end{equation}
where $\mathcal{P}$ is bounded by $[0,1]$. There are two ways to describe the relationship between $\gce$ and $\gkl$. 
% Firstly, their angle is less than 90 degrees, implying that the optimization direction of few-shot downstream knowledge is compatible with general knowledge. Thus, we can safely update the gradient direction as $\gce$. Secondly, their angle is greater than 90 degrees, indicating that optimizing the context vectors with $\gce$ will result in the loss of pre-trained general knowledge. This implies that few-shot downstream knowledge conflicts with general knowledge. 
Firstly, their sign is the same target-specific, implying that the optimization direction of few-shot downstream knowledge is compatible with general knowledge. Thus, we can safely update the gradient as $\gce$.
Secondly, their signs are different at the updated position, indicating that optimizing the binary mask with $\gce$ will result in the loss of pre-trained general knowledge. This implies that few-shot downstream knowledge conflicts with general knowledge. 
In this case, we regularize the $\gce$ via random gradient dropout strategy under the guidance of $\gkl$ to optimize the model for classification. We mathematically rewrite the~\cref{eq:psp} formulated as:
\begin{equation}\label{eq:projgrad}
\mathcal{P}= 
\left\{
             \begin{array}{ll}
             1, & \text{if}\ \gce \cdot\gkl \geq 0 \\
             (1+\frac{\gce + \gkl}{|\gce| + |\gkl|})/2,& \text{if}\ \gce>0\ \text{and}\ \gkl<0 \\
             (1-\frac{\gce + \gkl}{|\gce| + |\gkl|})/2,& \text{if}\ \gce<0\ \text{and}\ \gkl>0.
             \end{array}
\right.
\end{equation}
Thus, $\mathcal{P}$ is a measure of the agreement of general and target-specific knowledge at the updated position.
% how many positive gradients are present at any given value. 
We the formulate a gradient dropout function $\mathcal{M}_{\text{ce}}$ as: $\mathcal{M}_{\text{ce}} = \mathcal{I}[\mathcal{P}>U]$,
where $\mathcal{I}$ the standard indicator function. 
$U$ is a tensor composed of i.i.d $U(0,1)$ random variables. 
The optimization can be formulated as:
\begin{equation}
    \label{eq.optimize_final}
    \boldsymbol{M}_a \gets  \boldsymbol{M}_a-\gamma * (1-l+l*\mathcal{I}[\mathcal{P}>U]) * \frac{\partial \mathcal{L}_{\text{ce}}}{\partial\boldsymbol{M}_a^{bin}},
\end{equation}
where $l \in [0,1]$ is a leak parameter. $l<1$ means we allow $\gce$ leak through.
The complete Gradient Dropout Regularity technique involves computing the purity metric $\mathcal{P}$ at each gradient point and building a gradient consistency framework for cross-entropy loss with the help of KL divergence guidance. The steps for this are outlined in~\cref{alg.PPL}. We name the AMT with the Gradient Dropout Regularity technique as R-AMT.
Similarly, PMT and MMT with the Gradient Dropout Regularity technique are named R-PMT and R-MMT, respectively.



\begin{algorithm}[t]
	%\textsl{}\setstretch{1.8}
	\caption{Regularized Mask Tuning}
	\label{alg.PPL}
	\KwIn{The image encoder $\boldsymbol{G}_I$ and text encoder $\boldsymbol{G}_T$ of CLIP, data $\mathcal{D}_{train}$ for downstream task, hard threshold $\alpha$, and leak parameter $l$.}
	\KwResult{Mask $\boldsymbol{M}_a$ for image encoder}
	Construct hand-craft text prompt set $\mathcal{T}=\{\boldsymbol{t}_c\}^{\mathcal{C}}_{c=1}$ with the label set of $\mathcal{D}_{train}$
	
	Extract text features $\boldsymbol{g}_c=\boldsymbol{G}_T(\boldsymbol{t}_c), c=1,2,\cdots,\mathcal{C}$
	
	Initialize the learnable mask weight $\boldsymbol{M}_a$ according the the weight choices
	
	\For{$e \in [1, epoch]$}{
	Sample a mini-batch $\{\boldsymbol{x}_i,\boldsymbol{y}_i\}^{N}_{i=1}$ from $\mathcal{D}_{train}$
	
	Apply hard threshold $\alpha$ on the mask weight $\boldsymbol{M}_a$ to calculate CE loss $\mathcal{L}_{\text{ce}}$ and KL loss $\mathcal{L}_{\text{kl}}$
 % to get binary matrix $\boldsymbol{M}_a^{bin}$
	
% 	\For{each weight matrix $\boldsymbol{W}$ in $\boldsymbol{G}_I$}{
	% Generate masked image encoder $\boldsymbol{G}_I^m$ by updating weight matrix $\boldsymbol{W}$ to $\boldsymbol{W}'$ according to \cref{eq.weight_matrix}
% 	}
	
	% Extract image features $\boldsymbol{f}_i^m = \boldsymbol{G}_I(\vx_i;\vtheta_{\gM})$  and $\boldsymbol{f}_i= \boldsymbol{G}_I(\vx_i;\vtheta)$ with masked image encoder $\boldsymbol{G}_I^m$ and input image encoder $\boldsymbol{G}_I$, respectively
 %        % Extract CLIP image features $\boldsymbol{f}_i$ with image encoder $\boldsymbol{G}_I$
	
	% Calculate the affinity between image and text features $\boldsymbol{s}_{i,c}^m=sim(\boldsymbol{f}_i^m,\boldsymbol{g}_c)$ and $\boldsymbol{s}_{i,c}=sim(\boldsymbol{f}_i,\boldsymbol{g}_c), c=1,2,\cdots,\mathcal{C}$

        % Calculate the affinity between CLIP image and text features $\boldsymbol{s}_{i,c}=sim(\boldsymbol{f}_i,\boldsymbol{g}_c), c=1,2,\cdots,\mathcal{C}$
	
	% Compute cross-entropy loss with  $\mathcal{L}_{\text{ce}}(\boldsymbol{s}_{i}^m, \boldsymbol{y}_i)$

 %        Compute KL divergence with  $\mathcal{L}_{\text{kl}}(\boldsymbol{s}_{i}, \boldsymbol{s}_{i}^m)$

        % Adopt $\boldsymbol{M}_a$ to calculate cross-entropy loss $\mathcal{L}_{\text{ce}}$ and kl loss $\mathcal{L}_{\text{kl}}$

        Calculate $\mathcal{P} = \frac{1}{2}\left(1+\frac{\text{sgn}(\gce)\left(\gce + \gkl\right)}{|\gce| + |\gkl|}\right)$

        Sample $U$ from Uniform Distribution $U(0,1)$

        % Calculate $\mathcal{M}_i = \mathcal{I}[f(\mathcal{P})>U]$

        Set final gradient \\$\nabla_{final} = (1-l*(1-\mathcal{I}[\mathcal{P}>U]))\gce$
	
	Optimize the learnable matrix $\boldsymbol{M}_a$ with $\nabla_{final}$ by gradient descent:  $\boldsymbol{M}_a \gets  \boldsymbol{M}_a-\gamma \nabla_{final}$
	
	}
	
\end{algorithm}




% \todo{explain l}

% As an example, suppose $\sum_i(\nabla_w L_i) = 3$ at some scalar $w$ for two error functions $L_1$ and $L_2$. Now suppose that $\nabla_w L_1 = 1$ and $\nabla_w L_2 = 2$. In this case, all gradients are positive and therefore sign consistent, and so we keep all gradients during this training step. However, if $\nabla_w L_1 = -197$ and $\nabla_w L_2 = 200$, the gradients are highly inconsistent in sign. In this case, we would choose positive/negative from close to a 50/50 distribution.

% A simple example of a GradDrop step is given in Figure \ref{fig:graddrop} for the trivial activation $f(x)=x$. 

% \subsection{Extension to Transfer Learning and other Batch-Separated Gradient Signals}
% \label{sec:batchseparation}

% A complication arises when different gradients correspond to different examples, e.g. in mixed-batch transfer learning where transfer and source examples connect to separate losses. The different gradients at an activation layer would then not interact, which makes GradDrop the trivial transformation.

% We also cannot just blindly add gradients along the batch dimension, as the information present in each gradient is conditional on that gradient's particular inputs. Generally, deep nets consolidate information across a batch by summing gradient contributions at a trainable weight layer. To correctly extend GradDrop to batch-separated gradients, we will do the same.

% For a given layer of activations $A$ of shape $(B, F)$, we imagine there exists an additional weight layer $W^{(A)}$ of shape $(F)$ composed of 1.0s, and consider the forward pass $A \mapsto W^{(A)} \circ A$. $W^{(A)}$ is a virtual layer and is not actually allocated memory during training; we only use it to derive meaningful mathematical properties. Namely, we can then calculate the gradient via the chain rule to arrive at \begin{equation}\label{eq:marginalize} \nabla_{W^{(A)}}L_i = \sum_{\textrm{batch}}(A \circ \nabla_{A}L_i)\end{equation} where the final sum is taken over the batch dimension\footnote{The initialization of the virtual layer is not only meant to keep the forward logic trivial. It is relevant also in the derivation of Equation \ref{eq:marginalize}, as it gives us that $\nabla_AL_i = W^{(A)}\circ \nabla_{W^{(A)}\circ A}L_i = \nabla_{W^{(A)}\circ A}L_i$}. In other words, premultiplying the gradient values by the input allows us to meaningfully sum over the batch dimension to calculate $\mathcal{P}$ and the $\mathcal{M}_i$s. In practice, because we are only interested in $\nabla_{W^{(A)}}$ insofar as it changes the sign content of $\nabla_A$, we will only premultiply by the \textit{sign} of the input.


% \begin{algorithm}
% \caption{Gradient Sign Dropout Layer (GradDrop Layer)}\label{alg:graddrop}
% \begin{algorithmic}[1]
% \State \textbf{choose} monotonic activation function $f$ \Comment{Usually just $f(p)=p$}
% \State \textbf{choose} input layer of activations $A$ \Comment{Usually the last shared layer}
% \State \textbf{choose} leak parameters $\{\ell_1, \ldots, \ell_n\} \in [0,1]$ \Comment{For pure GradDrop set all to 0}
% \State \textbf{choose} final loss functions $L_1, \ldots, L_n$
% %\item[]
% %\Function{FORWARD}{$A$} \\
% %\Return A
% %\EndFunction
% \item[]
% \Function{BACKWARD}{$A$, $L_1, \ldots, L_n$}\Comment{returns total gradient after GradDrop layer}

% \For{$i$ in $\{1, \ldots, n\}$}
% \State \textbf{calculate} $G_i = \texttt{sgn}(A)\circ\nabla_A L_i$ \Comment{\texttt{sgn}(A) inspired by Equation}
% \If {$G_i$ is batch separated}
% \State $G_i \gets \sum_{\texttt{batchdim}} G_i$
% \EndIf
% \EndFor
% \State \textbf{calculate} $\mathcal{P} = \frac{1}{2}\left(1+\frac{\sum_iG_i}{\sum_i |G_i|}\right)$ \Comment{$\mathcal{P}$ has the same shape as $G_1$}
% \State \textbf{sample} $U$, a tensor with the same shape as $\mathcal{P}$ and $U[i, j, \ldots] \sim \texttt{Uniform}(0,1)$
% \For{$i$ in $\{1, \ldots, n\}$}
% \State \textbf{calculate} $\mathcal{M}_i = \mathcal{I}[f(\mathcal{P})>U]\circ \mathcal{I}[G_i >0] + \mathcal{I}[f(\mathcal{P})<U]\circ\mathcal{I}[G_i <0]$
% \EndFor
% \State \textbf{set} $\textrm{newgrad} = \sum_i (\ell_i + (1-\ell_i)*\mathcal{M}_i)\circ\nabla_AL_i$\\
% \Return{newgrad}
% %\hspace{2em}\Return A
% \EndFunction
% \end{algorithmic}
% \end{algorithm}

% For many of our experiments, we renormalize the final gradients so that $||\nabla||_2$ remains constant throughout the GradDrop process. Although not practically required, this ensures that GradDrop does not alter the global learning rate and thus observed benefits result purely from GradDrop masking.

% Note also the introduction of the leak parameters $\ell_i$. Setting $\ell_i>0$ allows some original gradient to leak through, which is useful when losses have different priorities -- for example, in transfer learning, we prioritize performance on the transfer set. 




% \subsection{Collaboration with Other Methods}
% \label{sec:method_collaborate}
% After training the parameter prompt (\ie, binary mask), we only save the binarized values $\boldsymbol{M}^b$ to 
% reduce the storage space occupied by the parameter prompt. 
% Since our method is synergistic to most existing parameter-efficient adaption methods, we further combine our methods with existing parameter-efficient methods, \eg prompt-base CoOP\cite{zhou2022learning} and adapter-based TIP-Adapter\cite{zhang2022tip}. Concretely, given the learned parameter prompt $\boldsymbol{M}^b$, we directly apply Hadamard product in \cref{eq.weight_matrix} to get an updated weight matrix $\boldsymbol{W}'$ for the image encoder. 
% To combine with CoOP, we freeze the feature encoders and optimize the text prompt (CoOP) with cross-entropy classification loss. The possibility of an image $\boldsymbol{x}_j$ being assigned to class $\boldsymbol{y}_i$ can be formulated as following:
% \begin{equation}
% \label{eq.mask_possibility}
%     p(\boldsymbol{y}=i \mid \boldsymbol{x}_j)=\frac{\exp \left(\cos \left(\boldsymbol{w}'_i, \boldsymbol{f}'_j\right) / \tau\right)}{\sum_{n=1}^k \exp \left(\cos \left(\boldsymbol{w}'_n, \boldsymbol{f}'_j\right) / \tau\right)},
% \end{equation}
% where the $\boldsymbol{f}'_j$ denotes the image feature extracted by the image encoder with parameter prompt and $\boldsymbol{w}'_i$ refers to the text features of learned text prompts.

% To combine with TIP-Adapter, we use the image features $\boldsymbol{F}'_I$ extracted by the updated image encoder to initialize the adapter. It can be formulated as:
% \begin{gather}
%     \label{eq.mask_tip}
%     \boldsymbol{A}'(\boldsymbol{f}_j) = exp(-\beta(1-\boldsymbol{f}_j\boldsymbol{F}^{'\mathrm{T}}_I)).
%     % \boldsymbol{f}_j\boldsymbol{F}^\mathrm{T}_{I_i})
% \end{gather}
% The adapter is further optimized with cross-entropy classification loss toward better performance.

