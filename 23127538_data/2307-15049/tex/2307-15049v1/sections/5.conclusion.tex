\section{Conclusion}\label{sec:conclusion}
In this work, we design a new type of tuning method, termed regularized mask tuning, that masks the network parameters under a learnable selection. 
Specifically, we first identify a set of parameters that are key to a given downstream task, then attach a binary mask to this parameter, and finally optimize these masks on the downstream data with the parameters frozen.
When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, to prevent the model from forgetting and overfitting.
% Meanwhile, our method is synergistic with most existing prompt tuning methods and provides the capacity to customize parameter settings based on downstream needs.
Extensive experiments demonstrate that our method consistently outperforms existing methods and is synergistic with them.
Future work will explore applying mask tuning to other visual tasks such as segmentation.
% \todo{why not add the mask in the text encoder?}