\section{Experiments}\label{sec:exp}
\subsection{Experimental Settings}

\noindent\textbf{Datasets}.
We conduct experiments on 11 publicly available image classification datasets following CoOP~\cite{zhou2022learning}. The datasets including ImageNet~\cite{deng2009imagenet}, FGVCAircraft~\cite{maji2013fine}, StanfordCars~\cite{Krause_2013_ICCV_Workshops}, Flowers102~\cite{nilsback2008automated}, Caltech101~\cite{fei2004learning}, DTD~\cite{cimpoi2014describing}, EuroSAT~\cite{helber2019eurosat}, Food101~\cite{bossard2014food}, UCF101~\cite{soomro2012ucf101}, OxfordPets~\cite{parkhi2012cats}, and SUN397~\cite{xiao2010sun}. 
% The detailed statistics of these datasets are illustrated in \cref{table:dataset}. 
% These datasets cover a wide range of image classification tasks, \eg, classification of objects, actions, textures, satellite images, and fine-grained classification. 
% Thus, these datasets compose a comprehensive benchmark for evaluating the transfer capacity of the V\&L model.

% \begin{table}[tb]
% 	\caption{The statistics of the 11  image classification datasets}
	
% 	\centering
% 	% \vspace{-0.20cm}
% 	\resizebox{\linewidth}{!}{
% 		\begin{tabular}{c|c|c|c}
% 		\hline
% 		\multirow{1}{*}{Dataset} & \multirow{1}{*}{\# of class} &
% 		\multirow{1}{*}{\# of testing data} &
% 		\multirow{1}{*}{Task} \\ 
% 		\hline 
		
% 		ImageNet~\cite{deng2009imagenet} & 1,000 & 50,000  & Object \\
% 		Caltech101~\cite{fei2004learning} & 101 & 2,465 & Object \\
% 		FGVCAircraft~\cite{maji2013fine} & 100 & 3,333  & Fine-grained \\
% 		StanfordCars~\cite{Krause_2013_ICCV_Workshops} & 196 & 8,041 & Fine-grained \\
% 		Flowers102~\cite{nilsback2008automated} & 102 & 2,463 & Fine-grained \\
%         OxfordPets~\cite{parkhi2012cats} & 37 & 3,669 & Fine-grained \\
%         Food101~\cite{bossard2014food} & 101 & 30,300 & Fine-grained \\
% 		DTD~\cite{cimpoi2014describing} & 47 & 1,692 & Textures \\
% 		EuroSAT~\cite{helber2019eurosat} & 10 & 8,100 & Satellite Images \\
%         UCF101~\cite{soomro2012ucf101} & 101 & 3,783 & Actions \\
% 		SUN397~\cite{xiao2010sun} & 397 & 19,850 & Scenes \\
		
% 		\hline
% 		\end{tabular}
% 		}    
% 		\label{table:dataset}
% 		\begin{tablenotes}
% 		\scriptsize
% 		\item[*] We use these datasets for few shot image classification task. The number of training data and validation data is set to be a specific multiple of the number of classes in training period. Thus, we only demonstrate the number of testing data.
% 		\end{tablenotes}
		
% 	% \vspace{-.15in}
% 	\end{table}


% Figure environment removed


\iffalse
% Figure environment removed
\fi

\noindent\textbf{Implementation Details}.
We transfer CLIP to the few-shot image classification task with AMT and R-AMT. Specifically, we use 1, 2, 4, 8, and 16-shot training sets to optimize the model and evaluate it on the full test set, following~\cite{radford2021learning}. For $n$-shot image classification, we random sample $n$ images per category for training. All results reported below are the average of three runs with different random seeds. All images are resized to $224\times 224$. Random cropping, resizing, and random horizontal flipping strategy are used for data augmentation.
We utilize ViT-B/16 as the visual backbone of CLIP. 
For a fair comparison, all experiments only use single text prompt, except learnable text prompt methods, \eg, for ImageNet and SUN397, the text prompt is set to be ``a photo of a [class].''
% , while a task-relevant sentence is added for fine-grained classification datasets, \eg, for Flowers102, the text prompt is ``a photo of a [class], a type of flower.'' For other datasets, the text prompt is set to be a task-related context, \eg, for UCF101, the text prompt is ``a photo of a person doing [class].''
We adopt Adam optimizer for optimization.
% For ImageNet, the maximum epoch is set to 10, and the learning rate is set to 3e-5.
% For other datasets, the maximum epoch is set to 30, and the learning rate is set to 8e-5.
The mask weights are initialized element-wise with $10^{-2}$. The threshold $\alpha$ is set to be $5\times10^{-3}$.
The $l$ in~\cref{eq.optimize_final} is set to 0.3 for datasets except for ImageNet, SUN397, and Food101 in 16-shot experiments. And $l$ is set to 1.0 in other experiments.
% The parameter prompts are applied on all layers for 16/8/4-shot, while the parameter prompts are only applied on the last self-attention layer for 2/1-shot (except for EuroSAT where the  parameter prompts are applied on the last layer for all settings).
% The initial learning rate is set to be 5e-5, which is decayed by 0.1 at half of the maximum epoch.




\subsection{Comparison to State-of-the-Art Methods}
\noindent
\textbf{Main Results on 11 Datasets.} We compare AMT and R-AMT with Zero-shot CLIP and five state-of-the-art methods on the 11 datasets as mentioned above, demonstrated in~\cref{fig:main_results_vit}. 
% It is mentioned that although the VPT and UPT adopt the visual/unified prompt, they are only for transformer architecture to design methods and are not universal. Thus, we do not include the VPT and UPT in this setting of 11 Datasets.
% The baseline methods include Zero-shot CLIP and Linear probe CLIP: 1) the former directly transfers to the downstream task without training; 2) the latter trains a linear classifier for downstream tasks based on the pretrained CLIP. 
Zero-shot CLIP directly transfers to the downstream task without training.
The state-of-the-art methods include prompt tuning methods, \ie, CoOP~\cite{zhou2022learning}, VPT~\cite{jia2022visual}, UPT~\cite{zang2022unified}, ProGrad~\cite{zhu2022prompt}, and adapter tuning method TIP-Adapter~\cite{zhang2022tip}.
% The prompt-based methods are CoOP~\cite{zhou2022learning} and ProGrad~\cite{zhu2022prompt}. Both of them tend to learn better text prompts for adaptation to downstream tasks. 
According to \cref{fig:Average}, the AMT and R-AMT outperform these methods on average over 11 datasets, which approves the ability of AMT and R-AMT to transfer CLIP to the downstream tasks. R-AMT achieves better performance compared with AMT. It indicates the gradient dropout regularity formalism is able to enhance the transfer ability of mask tuning in few-shot scenarios.
% Moreover, we find that the AMT and R-AMT achieve superior performance on the FGVCAircarft dataset (\cref{fig:FGVCAircraft}). It is because \todo{reason}.

% Concretely, the PPL improves the second-best method ProGrad by 3.85\% on the 16-shot setting, while 33.50\% of parameters in the image encoder are left out on average.
% Moreover, the PPL achieves the best accuracy in most of the datasets on the 16/8/4/2-shot setting. From \cref{fig:FGVCAircraft}, we find the improvement in terms of accuracy is more significant as the training few-shot data increase for the FGVCAircraft dataset.
% However, given the 1-shot data for training, there is performance degradation of PPL on some datasets, \eg Food101. This shows the reliability of the parameter prompt depends on the number of training data. A small quantity of training data for fine-grained classification tasks may lead to overfitting.
% to select preferred knowledge within the pretrained model for downstream tasks.


\noindent
\textbf{Results on base-to-new generalization setting.}
Following CoCoOP~\cite{zhou2022conditional}, we conduct experiments on base-to-new generalization setting. Concretely, the classes are split equally into the base and new classes on each dataset. The base classes are used for training. The $l$ is set to 1 in all base-to-new generalization experiments. The averaged results over 11 datasets are shown in~\cref{tab:Average_b2n}. The numerical experimental results on each dataset are shown in Supplementary.
Overall, R-AMT reaches the best performance, which surpasses the second best method CLIP-Adapter~\cite{gao2021clip} 2.00\% on the harmonic mean on average.
Notably, the AMT achieves quite high performance on the base classes. But the accuracy has significantly degraded (5.11\% on average) on the new classes compared with Zero-shot CLIP. 
We deem the degradation to be the result of overfitting since the amount of training data is too small for some datasets, \eg, Eurosat. 
The R-AMT achieves competitive results with AMT on base classes. However, the performance of R-AMT improves AMT by 3.04\% on average in new classes, which demonstrates the anti-overfitting ability of the proposed gradient dropout regularity formalism.

\begin{table}[t]
    \caption{\textbf{Comparison on the base-to-new generalization setting on the average over 11 datasets with 16 shots.} ``H'' denotes the harmonic mean of the accuracy on base and new classes. Thanks to gradient dropout regularity, R-AMT can efficiently maintain the knowledge of new classes while improving the anti-overfitting ability of the model to base classes. We report the average accuracy over three runs. The  error bar and performance of each dataset are provided in the supplementary materials.}
    \vspace{2pt}
    % \resizebox{0.7\linewidth}{!}{
    \centering\small
    \setlength{\tabcolsep}{13pt}
    \begin{tabular}{ccc|c}
    \toprule
        Method & Base & New & H \\
         \hline
       Zero-shot CLIP  &  69.34 & \textbf{74.22} & 71.70 \\
       CoCoOP~\cite{zhou2022conditional}  & 80.47 & 71.69 & 75.83 \\
       ProGrad~\cite{zhu2022prompt}  & 82.79 & 68.55 & 75.00 \\
       CLIP-adapter~\cite{gao2021clip}  & 82.62 & 70.97 & 76.35 \\
       \hline
       \multirow{1}{*}{AMT}  & \textbf{86.17} & 69.11 & \underline{76.70}  \\
       % & - & - &  - \\
       \multirow{1}{*}{R-AMT} & \underline{85.71} &\underline{72.15} & \textbf{78.35} \\
       % & - & - & - \\
    \bottomrule
    \end{tabular}
    % }
    \label{tab:Average_b2n}
    \vspace{-8pt}
\end{table}



\noindent
\textbf{The robustness to distribution shift.}
We evaluate the out-of-distribution (OOD) ability of AMT and R-AMT by training them on ImageNet and evaluating on ImageNet-V2~\cite{recht2019imagenet} and Imagenet-Sketch~\cite{wang2019learning}, following~\cite{zhang2022tip}. The evaluating datasets have compatible categories with the training set. But the three datasets are different in semantics. The OOD experimental results are shown in~\cref{table:distribution_shift}. R-AMT achieves the best performance, which surpasses TIP-Adapter~\cite{zhang2022tip} 1.06\% on ImageNet-V2 and surpasses CoOP~\cite{zhou2022learning} 0.04\% on Imagenet-Sketch. This indicates the R-AMT is also capable of OOD tasks. Moreover, R-AMT boosts AMT 0.47\%, 0.94\%, and 0.91\% on ImageNet, ImageNet-V2, and Imagenet-Sketch, respectively. It further proves that R-AMT benefits from the gradient dropout regularity technique in terms of enhancing transfer and anti-overfitting ability.

\subsection{Combination with State-of-the-Art Methods}
\noindent
To prove the R-AMT is synergistic to existing parameter-efficient methods, we combine it with CoOP~\cite{zhou2022learning} and TIP-Adapter~\cite{zhang2022tip} on 11 datasets with 16 shots, as shown in~\cref{table:combine_sota_vit}. 
Concretely, we first load the binary masks trained with R-AMT  and multiply them with the original parameters of the image encoder of CLIP. Then we train the learnable contextual prompt or adapter following CoOP and TIP-Adapter.
Particularly, the few-shot training set for R-AMT, CoOP+R-AMT, and TIP-Adapter+R-AMT is the same.
For CoOP+R-AMT, the learned text prompt is randomly initialized and the length of the text prompt is set to 16, using the same training details as CoOP~\cite{zhou2022learning}. 
% The results of CoOP reported in~\cref{table:combine_sota_vit} are obtained with the same experimental setting for a fair comparison.
The CoOP+R-AMT boosts the performance of CoOP by 3.26\% on average.
This indicates the R-AMT provides a more reliable image encoder for learning better text prompts using CoOP.
In addition, this combination approach directly uses a mask that is optimized by hand-craft text and does not update this mask for the learnable text prompts from CoOP, resulting in not completely unleashing the potential of the mask for downstream tasks (\ie, not surpass the R-AMT).
For TIP-Adapter+R-AMT, the training details are also the same as the TIP-Adapter~\cite{zhang2022tip}.
% the $\alpha$ and $\beta$ are set to be 1.0 and 5.5, respectively, following TIP-Adapter~\cite{zhang2022tip}. The adapter is training for 20 epochs. The initial learning rate is set to be 0.001. The hyper-parameter setting for training is the same for TIP-Adapter. 
The TIP-Adapter+R-AMT improves TIP-Adapter 3.13\% on average with 16 shots.
This verifies the ability of R-AMT to endow existing parameter-efficient methods with the ability to better adapt to the downstream task.

\begin{table}[t]
	\caption{\textbf{Comparison on robustness to distribution shift.}}
	\vspace{2pt}
	\centering\small
        \setlength{\tabcolsep}{2.2pt}
	% \resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|cc|c}
		\toprule
            \multirow{2}{*}{Method}  & Source & \multicolumn{2}{c|}{Target}  & \multirow{2}{*}{Average} \\
		% \multirow{1}{*}{Method} & \multirow{1}{*}{Average}& Source & \multicolumn{1}{*}{Target} & \multicolumn{1}{*}{Target} \\
            & ImageNet & -V2 & -Sketch &  \\
            \hline
            
		Zero-shot CLIP & 66.73 & 60.83 & 46.15 & 57.90 \\
            Linear probe & 65.85 & 56.26 & 34.77 & 52.29 \\
            CoOP & 71.73 & 64.56 & 47.89 & 61.39 \\
            CLIP-adapter & 71.77 & 63.97 & 46.27 & 60.67 \\
            TIP-adapter & \textbf{73.08} &64.85 & 46.76 & \underline{61.56} \\
            \hline
            AMT & 72.60\std{0.12} & \underline{64.97}\std{0.11} & \underline{47.02}\std{0.13} & 61.53 \\
            R-AMT &\underline{73.07}\std{0.10} & \textbf{65.91}\std{0.34} & \textbf{47.93}\std{0.26} & \textbf{62.30}\\
            % \hline
            % R-MMT &\textbf{73.52}\std{0} & {66.09}\std{0} & {47.68}\std{0} & \textbf{62.43}\\
            % R-PMT &73.48\std{0} & \textbf{66.19}\std{0} & {47.60}\std{0} & {62.42}\\
            
		\bottomrule
		\end{tabular}
            % }
		% \begin{tablenotes}
  %   		\scriptsize
  %   		\item[*] For TIP-Adapter, the results reported by Zhang \etal~\cite{zhang2022tip} on ImageNet are based on using prompt ensembling as text input. For a fair comparison, we conduct experiments with a single text prompt for TIP-Adapter and report the results here.
		% \end{tablenotes}
		\label{table:distribution_shift}
		
	\vspace{-8pt}
\end{table}

% \begin{table}[tb]
% 	\caption{Combining with state-of-the-art methods on 16-shot ImageNet.}
% 	\centering
% 	\vspace{-0.20cm}
% 	\resizebox{0.5\linewidth}{!}{
% 		\begin{tabular}{l|c}
% 		\hline
% 		\multirow{1}{*}{Methods} & \multirow{1}{*}{ImageNet} \\ 
% 		\hline 
		
% 		Zero-shot CLIP & 66.73 \\
% 		PMT & \textbf{73.07~\textcolor{red}{+ 6.67}} \\ \hline
% 		CoOP~\cite{zhou2022learning} & 72.01  \\
% 		PMT+CoOP & \textbf{73.35~\textcolor{red}{+ 1.34}} \\ \hline
% 		TIP-Adapter~\cite{zhang2022tip} & 73.08 \\
% 		PMT+TIP-Adapter &\textbf{74.28 ~\textcolor{red}{+ 1.20}} \\
		
% 		\hline
% 		\end{tabular}
% 		} 
% 		\label{table:combine_sota}
		
% 	\vspace{-.1in}
% 	\end{table}

\begin{table*}[t]
	\caption{\textbf{Combining with the state-of-the-art methods on 16 shots.} Our mask tuning is synergistic with most existing parameter-efficient tuning methods (\eg, adapter tuning~\cite{zhang2022tip} and prompt tuning~\cite{zhou2022learning}) and can boost about $3\%$ performance on top of them.}
        \vspace{2pt}
	\centering\small
        \setlength{\tabcolsep}{3pt}
	% \vspace{-0.20cm}
	% \resizebox{\linewidth}{!}{
		\begin{tabular}{c|ccccccccccccc}
		\toprule
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Method}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{ImageNet}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Caltech101}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{FGVCAircraft}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{StanfordCars}} & 
            \makebox[0.05\textwidth][c]{\rotatebox{45}{Flowers102}} & 
            \makebox[0.05\textwidth][c]{\rotatebox{45}{OxfordPets}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Food101}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{DTD}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{EuroSAT}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{UCF101}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{SUN397}} & 
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Average}}& 
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Gain}}\\
 
		\hline 
		Zero-shot CLIP & 66.73 & 92.94 & 24.72 & 65.32 & 71.34 & 89.21 & 86.06 & 44.39 & 47.60 & 66.75 & 62.50 & 65.23 &- \\
		R-AMT & 73.07 & 97.00 & 58.47 & 85.93 & 98.17 & 93.80 & 87.47 & 74.57 & 91.80 & 86.93 & 76.40 & \textbf{83.96}&{\textcolor{red}{+18.73}}  \\
            \hline
		CoOP~\cite{zhou2022learning} & 72.01 & 95.47 & 43.29 & 82.91 & 96.93 & 91.92 & 84.33 & 69.21 & 86.05 & 82.25 & 74.58 & 79.90 &- \\
		CoOP+R-AMT& {73.35} & 96.70 & 56.37 & 85.63 & 97.83 & 93.20 & 86.13 & 73.03 & 90.20 & 86.87 & 75.45 &\textbf{83.16}&{~\textcolor{red}{+3.26}} \\ 
            \hline
		TIP-Adapter~\cite{zhang2022tip} & 73.08 & 95.63 & 45.20 & 83.04 & 96.15 & 92.66 & 87.31 & 71.57 & 88.53 & 84.24 & 76.21 & 81.24 &- \\
		TIP-Adapter+R-AMT &{74.28} & 96.97 & 61.07 & 86.27 & 97.80 & 94.07 & 87.43 & 74.77 & 91.50 & 86.93 & 76.97 & \textbf{84.37}&{~\textcolor{red}{+3.13}}\\
		
		\bottomrule
		\end{tabular}
		% }
		\label{table:combine_sota_vit}
% 		\begin{tablenotes}
% 		\scriptsize
% 		\item[*] We use these datasets for few shot image classification task. The number of training data and validation data is set to be a specific multiple of the number of classes in training period. Thus, we only demonstrate the number of testing data.
% 		\end{tablenotes}
		
	\vspace{-5pt}
	\end{table*}

\begin{table*}[t]
	\caption{\textbf{Effect of performing masking on different layers.} Attaching a binary mask to the multi-head self-attention layer (\ie, R-AMT) achieves the same performance as R-PMT but with lower computational effort.}
        \vspace{2pt}
	\centering\small
        \setlength{\tabcolsep}{4.8pt}
	% \vspace{-0.20cm}
	% \resizebox{\linewidth}{!}{
		\begin{tabular}{c|ccccccccccccc}
		\toprule
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Method}} &
            % Method &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{ImageNet}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Caltech101}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{FGVCAircraft}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{StanfordCars}} & 
            \makebox[0.05\textwidth][c]{\rotatebox{45}{Flowers102}} & 
            \makebox[0.05\textwidth][c]{\rotatebox{45}{OxfordPets}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Food101}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{DTD}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{EuroSAT}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{UCF101}} &
		\makebox[0.05\textwidth][c]{\rotatebox{45}{SUN397}} & 
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Average}}& 
		\makebox[0.05\textwidth][c]{\rotatebox{45}{Storage Space}}\\
 
		\hline 
		% OPPL         & 72.76 & 96.93 & 57.79 & 85.51 & 98.05 & 93.29 & 86.20 & 74.09 & 90.60 & 86.50 & 74.78 & 83.32 & 7.67M \\
		% OR-AMT& 72.79 & 96.99 & 57.19 & 85.58 & 97.82 & 93.61 & 87.36 & 74.17 & 90.16 & 86.48 & 75.39 & 83.41 & 7.67M \\
		% PMT          & 72.60 & 97.10 & 59.43 & 85.70 & 98.07 & 93.43 & 85.93 & 74.53 & 92.00 & 87.00 & 72.27 & 83.46 & 28.90M \\
		R-AMT & 73.07 & \textbf{97.00} & 58.47 & 85.93 & 98.17 & 93.80 & 87.47 & 74.57 & \textbf{91.80} & 86.93 & \textbf{76.40} & \textbf{83.96} & 6.7M \\
            R-MMT & \textbf{73.52} & 96.77 & 59.57 & \textbf{86.43} & 98.07 & \textbf{93.83} & 87.40 & 75.73 & 84.07 & 87.70 & 74.23 & 83.39 & 14M \\
		% PPL          & 72.70 & 96.80 & 59.40 & 86.57 & 97.97 & 93.60 & 86.70 & 75.33 & 88.00 & 87.27 & 74.53 & 83.63 & 85.52M \\
            R-PMT & 73.48 & 96.63 & \textbf{60.30} & 86.33 & \textbf{98.27} & 93.77 & \textbf{87.50} & \textbf{75.60} & 88.20 & \textbf{87.33} & 76.12 & \textbf{83.96} & 19M \\
		
		\bottomrule
		\end{tabular}
		% }
		\label{table:different_layes_vit}
% 		\begin{tablenotes}
% 		\scriptsize
% 		\item[*] We use these datasets for few shot image classification task. The number of training data and validation data is set to be a specific multiple of the number of classes in training period. Thus, we only demonstrate the number of testing data.
% 		\end{tablenotes}
		
	% \vspace{-.15in}
        \vspace{-.1in}
	\end{table*}

\subsection{Ablation Studies}
\label{sec:ab}
\noindent
\textbf{Analysis of Masking different layers.}
We conduct ablation studies on masking different layers of the image encoder.~\cref{table:different_layes_vit} shows the results on 16 shots over 11 datasets. Concretely, we apply the binary masks on all weight matrices of convolutional layers and fully connected layers when training R-PMT. For R-AMT, the binary masks are applied on the multi-head self-attention (MHSA) layers, while for R-MMT, the binary masks are applied on the multilayer perceptron (MLP) layers.
R-AMT achieves equal performance with R-PMT on the average of 11 datasets, which surpasses R-MMT 0.57\%. 
But the R-AMT only uses 6.7M for storing the trained model, which is 12.3M less than R-PMT.
Moreover, we find the R-AMT achieves superior performance when there are limited training classes, \eg, EuroSAT.
Thus, we deem the R-AMT to be a more practical method.
 


\noindent
\textbf{Influence of gradient dropout regularity.}
We explore the influence of gradient dropout regularity with 16 shots ImageNet. The experimental results are shown in~\cref{table:graddrop}.
The proposed gradient dropout regularity requires the guidance of KL divergence. Thus, we conduct an ablation study by directly adding the KL loss $\mathcal{L}_{kl}$ with the cross-entropy loss $\mathcal{L}_{ce}$ to training the binary mask, which is termed as AMT+KL loss. 
It shows that if we directly add these two losses, the accuracy drops by 0.68\% on 16-shot ImageNet. Because the $\mathcal{L}_{ce}$ aims to transfer the model to downstream tasks, while the $\mathcal{L}_{kl}$ requires the disparity between the classification logits of AMT and CLIP is not large.
Directly adding $\mathcal{L}_{kl}$ with $\mathcal{L}_{ce}$ limits the transfer ability of AMT.
AgreeGrad~\cite{mansilla2021domain} adopts gradient surgery to solve the domain conflict, but it excessively believes in previous knowledge from KL loss, resulting in performance degradation.
Recently, ProGrad~\cite{zhu2022prompt} proposes a gradient projection method for training text prompts. This gradient projection method and our gradient dropout regularity technique both require the guidance of KL divergence.
Thus, we employ the gradient projection method to train our AMT for comparison, named AMT+ProGrad. AMT+ProGrad surpasses AMT by 0.10\%, but is 0.37\% lower than R-AMT ($l$=1.0).
It indicates the gradient projection technique can help mask tuning. But the improvement is limited since all conflict gradients are forced to be projected in the vertical direction.
The gradient dropout regularity adds some level of randomness to the gradient guided by the KL divergence, which helps the model generalize better to downstream tasks.
% the transfer ability of mask tuning since the conflicts gradients are all forced to be projected in the vertical direction. But the gradient dropout regularity technique adds some level of randomness to the gradient guided by the KL divergence, which helps the model generalize better to downstream tasks.
Moreover, we analyze the influence of $l$ in the gradient dropout regularity technique. 
A smaller $l$ implies a higher probability of CE-related gradient maintenance, which divers the binary masks more sparse.
The R-AMT achieves the best performance on 16-shot ImageNet when $l=1.0$. When $l$ is small than 1.0, The performance degradation is caused by the leak through gradients of $\mathcal{L}_{ce}$, which conflicts with the general knowledge of CLIP.


 \begin{table}[t]
	\caption{\textbf{Ablation studies on gradient dropout regularity strategy.} The proposed gradient dropout regularity can make better use of general knowledge of KL loss while exploring the knowledge of downstream data. ``$l$'' controls the level of agreement in CE Loss.
 % Analysis of the influence of gradient dropout regularity technique on 16-shot ImageNet.
 }
        \vspace{2pt}
	\centering\small
        \setlength{\tabcolsep}{5pt}
	% \vspace{-0.20cm}
	% \resizebox{0.95\linewidth}{!}{
		\begin{tabular}{c|c|c|c|c}
		\toprule
		% \makebox[0.05\textwidth][c]{\rotatebox{0}{Method}}  &
  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{Method}} &
  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{$l$}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{ImageNet}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{Caltech101}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{FGVCAircraft}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{StanfordCars}} & 
  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{Flowers102}} & 
  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{OxfordPets}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{Food101}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{DTD}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{EuroSAT}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{UCF101}} &
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{SUN397}} & 
		% \makebox[0.05\textwidth][c]{\rotatebox{45}{Average}}\\
 
  %        % R-AMT & 1.0/0.3 & 73.07  & 97.00 & 58.47 & 85.93 & 98.17 & 93.80 & 87.47 & 74.57 & 91.80 & 86.93 & 76.40  & 83.96 \\
		% \hline 
		% AMT    & -  & 72.60 & 97.10 & 59.43 & 85.70 & 98.07 & 93.43 & 85.93 & 74.53 & 92.00 & 87.00 & 72.27 & 83.46 \\
  %      R-AMTKL loss& -  & 71.92 & 95.23 & 47.09 & 80.12 & 91.09 & 92.69 & 87.06 & 67.10 & 86.02 & 79.51 & 71.38 & 79.02 \\
  %      R-AMTPCGrad & -  & 72.70 & 96.53 & 53.30 & 84.43 & 96.10 & 93.60 & 87.57 & 73.23 & 91.10 & 85.37 & 75.67 & 82.69 \\
		% R-AMT & 1.0 & 73.07 & 96.89 & 55.81 & 85.90 & 97.07 & 93.97 & 87.47 & 74.13 & 89.65 & 86.07 & 76.40 & 83.31 \\
  %           R-AMT & 0.5 & 72.95 & 97.00 & 57.83 & 85.90 & 98.12 & 94.03 & 86.71 & 74.65 & 91.34 & 86.69 & 75.96 & 83.74 \\
  %           R-AMT & 0.3 & 72.87 & 97.00 & 58.47 & 85.93 & 98.17 & 93.80 & 86.50 & 74.57 & 91.80 & 86.93 & 75.87 & 83.81 \\
  %           R-AMT & 0.1 & 72.67 & 96.87 & 59.00 & 85.90 & 98.07 & 93.57 & 86.04 & 74.53 & 91.97 & 86.84 & 75.38 & 83.71 \\

            Method & $l$ & Accuracy & Gain& Sparsity\\
            \hline 
            Zero-shot CLIP & - & 66.73 & - & -  \\
            \hline 
            AMT         & - & 72.60\std{0.12} & - & 2.64  \\
            AMT+KL loss & - & 71.92\std{0.06} & \textcolor{green}{-0.68} & 2.58 \\
            AMT+AgreeGrad~\cite{mansilla2021domain} & - & 68.82\std{0.09} & \textcolor{green}{-3.78} & 1.73 \\
            AMT+ProGrad~\cite{zhu2022prompt} & - & 72.70\std{0.22} & \textcolor{red}{+0.10} & 2.67  \\
            \hline
            R-AMT & 1.0 & \textbf{73.07}\std{0.10} & \textcolor{red}{+0.47}& 2.45 \\
            R-AMT & 0.8 & 72.97\std{0.13} & \textcolor{red}{+0.37}& 2.50 \\
            R-AMT & 0.5 & 72.95\std{0.19} & \textcolor{red}{+0.35} & 2.56 \\
            R-AMT & 0.3 & 72.87\std{0.05} & \textcolor{red}{+0.27} & 2.59 \\
            R-AMT & 0.1 & 72.67\std{0.12} & \textcolor{red}{+0.07} & 2.61 \\
            
            
		
		\bottomrule
		\end{tabular}
		% }
		\label{table:graddrop}
  \vspace{-10pt}
  \end{table}


 %   \begin{table}[ht]
	% \caption{Analysis on Gradient Drop on ImageNet on 16-shot base-to-new generalization setting.}

	% \centering
	% % \vspace{-0.20cm}
	% \resizebox{0.7\linewidth}{!}{
	% 	\begin{tabular}{c|c|cc|c}
	% 	\toprule
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{0}{Method}}  &
 %  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{Method}} &
 %  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{$l$}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{ImageNet}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{Caltech101}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{FGVCAircraft}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{StanfordCars}} & 
 %  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{Flowers102}} & 
 %  %           \makebox[0.05\textwidth][c]{\rotatebox{45}{OxfordPets}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{Food101}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{DTD}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{EuroSAT}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{UCF101}} &
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{SUN397}} & 
	% 	% \makebox[0.05\textwidth][c]{\rotatebox{45}{Average}}\\
 
	% 	% \hline 
	% 	% AMT         & - & 72.60 & 97.10 & 59.43 & 85.70 & 98.07 & 93.43 & 85.93 & 74.53 & 92.00 & 87.00 & 72.27 & 83.46 \\
 %  %           % R-AMT & 1.0/0.3 & 73.07  & 97.00 & 58.47 & 85.93 & 98.17 & 93.80 & 87.47 & 74.57 & 91.80 & 86.93 & 76.40  & 83.96 \\
 %  %           R-AMTKL loss & - & 71.92 & 95.23 & 47.09 & 80.12 & 91.09 & 92.69 & 87.06 & 67.10 & 86.02 & 79.51 & 71.38 & 79.02 \\
 %  %           % \hline
 %  %           R-AMTPCGrad & - & 72.70 & 96.53 & 53.30 & 84.43 & 96.10 & 93.60 & 87.57 & 73.23 & 91.10 & 85.37 & 75.67 & 82.69 \\
 %  %           \hline
	% 	% R-AMT & 1.0 & 73.07 & 96.89& 55.81 & 85.90 & 97.07 & 93.97 & 87.47 & 74.13 & 89.65 & 86.07 & 76.40 & 83.31 \\
 %  %           R-AMT & 0.5 & 72.95 & 97.00 & 57.83 & 85.90 & 98.12 & 94.03 & 86.71 & 74.65 & 91.34 &86.69 & 75.96 & 83.74 \\
 %  %           R-AMT & 0.3 & 72.87 & 97.00 & 58.47 & 85.93 & 98.17 & 93.80 & 86.50 & 74.57 & 91.80 & 86.93 & 75.87 & 83.81 \\
 %  %           R-AMT & 0.1 & 72.67 & 96.87 & 59.00 & 85.90 & 98.07 & 93.57 & 86.04 & 74.53 & 91.97 & 86.84 & 75.38 & 83.71 \\

 %            Method & $l$ & Base & New &H \\
 %            \hline 
 %            AMT         & - & 72.60\\
 %            R-AMTKL loss & - & 71.92 \\
 %            R-AMTPCGrad & - & 72.70\\
 %            R-AMT & 1.0 & 73.07\\
 %            R-AMT & 0.5 & 72.95 \\
 %            R-AMT & 0.3 & 72.87\\
 %            R-AMT & 0.1 & 72.67\\
            
            
		
	% 	\bottomrule
	% 	\end{tabular}
	% 	}
	% 	\label{table:graddrop_b2n}
	% \end{table}

\noindent
\textbf{Analysis of hard threshold $\alpha$.} We conduct ablation study on the hard threshold $\alpha$ with the initial value of mask weights fixed in~\cref{table:ablation_alpha}. It shows the binary masks are sparser as $\alpha$ gets larger. The R-AMT achieves the best accuracy when $\alpha=5\times10^{-3}$. We deem that some redundant information still has not been masked when $\alpha=4\times10^{-3}$. Thus, this information still influences the performance of the model in the downstream task. When $\alpha=6\times10^{-3}$, some valuable parameters are moved out by the binary masks. It caused performance degradation.
% \textbf{More ablation studies are included in the Supplementary.}
	
 \begin{table}[t]
	\caption{\textbf{Effect of hard threshold $\alpha$ on 16-shot ImageNet}. The threshold determines the sparsity of model.
 % Analysis of the influence of hard threshold $\alpha$ on 16-shot ImageNet.
 }
        \vspace{2pt}
	\centering\small
        \setlength{\tabcolsep}{10pt}
	% \vspace{-0.20cm}
	% \resizebox{0.9\linewidth}{!}{
		\begin{tabular}{c|c|c|c}
		\toprule

            $\alpha$  & $4\times10^{-3}$ &  $5\times10^{-3}$ & $6\times10^{-3}$ \\
            \hline 
            Accuracy      & 72.87\std{0.06}  & \textbf{73.07}\std{0.10}& 72.91\std{0.14}  \\
            Sparsity  & 1.99 & 2.45 & 3.12\\
		\bottomrule
		\end{tabular}
		% }
		\label{table:ablation_alpha}
\vspace{-8pt}
  \end{table}
  
  

% \subsubsection{Analysis of Parameter Prompts}
% \label{sec.analysis}
% \noindent
% \textbf{More ablation studies are included in the Supplementary.}
