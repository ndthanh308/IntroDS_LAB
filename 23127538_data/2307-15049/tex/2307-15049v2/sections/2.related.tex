
\section{Related Work}\label{sec:related-work}

\noindent\textbf{Vision-language models} achieve cross-modality alignment by learning a joint embedding space for text and image representation. A typical VLM consists of three components: text encoder, image encoder, and alignment function. The text and image encoder is trained separately before being connected by the alignment function in the early stage~\cite{frome2013devise}.
Recent VLMs such as CLIP \cite{radford2021learning} and Align~\cite{jia2021scaling} jointly optimize text and image encoder through contrastive learning. Benefiting from the millions of text-image pairs from the web and the multi-modality structure, these VLMs achieve exceptional zero-shot transfer capacity in the downstream tasks. Toward better transfer ability, researchers propose a series of parameter-efficient methods to adapt CLIP to downstream tasks, such as image recognition~\cite{zhou2022learning,zang2022unified,gao2021clip,zhang2022tip,guo2022calip}.
% In this work, we propose a novel mask-tuning strategy to efficiently mine the important parameters in CLIP for few-shot and visual recognition tasks.

\noindent\textbf{Parameter-efficient adaption methods} for CLIP can be coarsely divided into two categories: prompt tuning~\cite{zhou2022learning,zhang2023prompt,zang2022unified,chen2022prompt} and adapter tuning~\cite{gao2021clip,zhang2022tip}. 
Inspired by the success of prompt learning in NLP~\cite{brown2020language,li2021prefix,gao2020making}, some researchers involve prompt learning methods in CLIP to improve the few-shot transfer capacity. Zhou \etal~\cite{zhou2022learning} first introduce learnable text prompts to adapt CLIP to visual recognition tasks, which brings a great improvement over Zero-shot CLIP. Zang \etal~\cite{zang2022unified} propose a unified prompt learning strategy for text and image encoder, which simultaneously refine the text and image representation for adapting to downstream tasks.
Adapter modules consist of a small set of learnable parameters, which are further inserted into the frozen pre-trained model for adaptation. Gao \etal~\cite{gao2021clip} add adapters after text and image branches through residual connection. Zhang \etal~\cite{zhang2022tip} employ a training-free adapter module following the image encoder, which is initialized using the knowledge extracted from the downstream training set. 
However, existing methods mainly focus on changing the input of CLIP (\ie, text prompt tuning~\cite{zhou2022learning} and visual prompt tuning~\cite{zang2022unified}) or adding extra modules out of CLIP (\ie, adapter tuning~\cite{gao2021clip,zhang2022tip}), which neglects to excavate the inner power of CLIP.

\noindent\textbf{Binary mask} is commonly used to find a subnetwork structure from the model, which can be viewed as a way of network pruning. It can be achieved through a straight-through estimator~\cite{bengio2013estimating,rastegari2016xnor}.
Csord√°s \etal~\cite{csordas2020neural} learn binary masks to identify subnetworks responsible for different tasks. Zhang \etal~\cite{zhang2021can} search subnetworks with binary mask to achieve better out-of-distribution (OOD) performance. These works focus on finding a functional subpart of weights inside a given pre-trained neural network, which can be retrained for new tasks. 
However, Zhou~\etal~\cite{zhou2019deconstructing} find that applying binary mask with a model is also a way to train the model by investigating the lottery ticket hypothesis of network pruning.  Recently, researchers~\cite{liu2018rethinking,zhao2020masking} propose that training binary mask for a pre-trained language model is similar to finetuning and is more parameter-efficient.
Moreover, Mallya \etal~\cite{mallya2018piggyback} train binary mask with fixed convolutional neural network for image classification, which achieves good performance. These works demonstrate the capacity of binary masks in parameter-efficient training in natural language processing and computer vision. 
Different from these methods, we propose a regularized mask tuning to search an important subset of weights in the image encoder of fixed CLIP for downstream tasks. Moreover, the regularized mask tuning can be further combined with other parameter-efficient methods presuming better few-shot performance.
% Inspired by this, we propose parameter prompt learning to adapt CLIP to downstream tasks, where the binary masks serve as parameter prompts. We train the parameter prompt for the image encoder of CLIP and justify that there is redundant information within the CLIP, even though the zero-shot performance is impressive.
% Inspired by this, we propose regularized mask tuning to adapt CLIP to downstream tasks, where the binary masks serve as parameter prompts. We train the parameter prompt for the image encoder of CLIP and justify that there is redundant information within the CLIP, even though the zero-shot performance is impressive.
