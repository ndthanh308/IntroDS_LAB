\section{Introduction}\label{sec:intro}


\definecolor{flamecolor}{RGB}{233,161,72}
\definecolor{snowflakecolor}{RGB}{91,157,219}
% Figure environment removed


The advent of large-scale pre-trained vision-language models (VLMs)~\cite{radford2021learning} has ushered in a new era of incorporating language features to supervise the image encoder for a wide range of downstream visual tasks, such as few-shot learning~\cite{zhou2022learning} and open-world detection~\cite{du2022learning}. 
Thanks to the multimodal architecture and millions of text-image pairs from the web, VLMs exhibit exceptional zero-shot transferability in downstream tasks. 
To further enhance the transferability of VLMs, researchers have proposed efficient tuning methods, such as adapter tuning~\cite{gao2021clip,zhang2022tip} or prompt tuning~\cite{zhou2022learning,zang2022unified,lu2022prompt}. 
These techniques incorporate a small number of task-specific parameters and train them solely on the downstream task, thus significantly improving the performance and reducing computational requirements.


The essence of efficient tuning methods lies in two fundamental components, \ie leveraging the well-learned knowledge structure of VLMs and efficiently exploring the task-specific knowledge given few-shot data. 
Despite its potential, however, most existing efficient transfer learning approaches direct utilize all parameters of pre-trained VLMs and do not consider further unleashing the potential of the well-learned knowledge of VLMs. 
Specifically, prompt tuning methods~\cite{zhou2022learning} use the frozen CLIP model and add the extra learnable parameters from the input side as shown in \cref{fig:moti}a. 
Adapter modules~\cite{gao2021clip,zhang2022tip} consist of a small set of the learnable module, further inserted into the frozen pre-trained model for adaptation as in~\cref{fig:moti}b.
Despite the considerable efforts in efficient tuning methods from the prompt or adapter side, these methods do not explore the frozen CLIP parameters at all, choosing instead to add additional modules to learn task-specific knowledge.
Thus, as shown in~\cref{fig:moti}c, we adopt \textbf{\textit{mask tuning}} to explore the well-learned knowledge structure of VLMs and uncover the hidden knowledge in them for task-specific domains. 


In the field of neural physiology~\cite{
hubel1962receptive,engel1997retinotopic,zeki1988functional}, it has been discovered that neurons in the brain cortex exhibit diverse knowledge of various visual features such as shape, color, and depth. 
The knowledge is distributed in distinct neurons that have specific functions and work in conjunction with one another, termed neural pathways.
When there is knowledge of a new environment coming, the neurons will compare it with the old knowledge learned in the past and then pick new conjunctions (\ie, neural pathways) to adapt to the new environment.
Analogous to VLMs, parameters act as a manifestation of neurons and are responsible for memorizing knowledge from data. Thus, selecting suitable parameters as parameter pathways is beneficial for uncovering the key knowledge of downstream tasks.
% The pre-trained weights already contain the knowledge in part required by a downstream task, but this knowledge may be obscured by irrelevant information emerging during upstream pre-training.
% It has been discovered that selecting parameters given few-shot downstream data can assist the model in better grasping the preferred knowledge that is applicable to each downstream domain. 
% As a result, selecting appropriate parameter pathways is critical for transferring pre-trained VLMs to a variety of downstream tasks.


Inspired by the neural pathways, we propose an efficient \textit{Regularized Mask Tuning (R-MT)} method to mask the parameters of the pre-trained VLMs under a learnable selection.
Specifically, we first identify a subset of the parameters (\eg, multi-head self-attentive layer) based on the magnitude of the gradient changes as sensitive network parameters for downstream tasks.
Then, we introduce a binary mask equipped with gradient dropout regularization to the selected parameters.
Because few-shot training tends to cause overfitting, we introduce the logits from pre-trained VLMs as the general knowledge to prevent mask tuning from forgetting.
Concretely, the gradient dropout regularity as an effective regularizer introduces the probabilistic masking strategy that samples gradients based on the level of consistency of the downstream-related knowledge and the general knowledge, which can reject weak loss minima that may lead to overfitting.
% Intrinsic evaluations reveal that representations generated by our binary masked vision-language models encode knowledge preferences for specific downstream tasks. 
Our findings indicate that selecting well-placed parameters is crucial for achieving successful transfer settings.
Moreover, our method is orthogonal to most existing parameter-efficient adaption methods (\eg, adapter and prompt) and endows them the ability to customization on downstream needs.
Extensive experiments on 11 datasets demonstrate the effectiveness of the proposed method.

