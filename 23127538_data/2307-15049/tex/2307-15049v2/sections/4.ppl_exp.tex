\section{Experiments}\label{sec:exp}
\subsection{Experimental Settings}

\noindent\textbf{Datasets}.
We conduct experiments on 11 publicly available image classification datasets following CoOP~\cite{zhou2022learning}. The datasets including ImageNet~\cite{deng2009imagenet}, FGVCAircraft~\cite{maji2013fine}, StanfordCars~\cite{Krause_2013_ICCV_Workshops}, Flowers102~\cite{nilsback2008automated}, Caltech101~\cite{fei2004learning}, DTD~\cite{cimpoi2014describing}, EuroSAT~\cite{helber2019eurosat}, Food101~\cite{bossard2014food}, UCF101~\cite{soomro2012ucf101}, OxfordPets~\cite{parkhi2012cats}, and SUN397~\cite{xiao2010sun}. 
% The detailed statistics of these datasets are illustrated in \cref{table:dataset}. 
These datasets cover diverse image classification tasks, \eg, classification of objects, actions, textures, satellite images, and fine-grained classification. Thus, these datasets compose a comprehensive benchmark for evaluating the transfer capacity of the V\&L model.

% \begin{table}[tb]
% 	\caption{The statistics of the 11  image classification datasets}
	
% 	\centering
% 	% \vspace{-0.20cm}
% 	\resizebox{\linewidth}{!}{
% 		\begin{tabular}{c|c|c|c}
% 		\hline
% 		\multirow{1}{*}{Dataset} & \multirow{1}{*}{\# of class} &
% 		\multirow{1}{*}{\# of testing data} &
% 		\multirow{1}{*}{Task} \\ 
% 		\hline 
		
% 		ImageNet~\cite{deng2009imagenet} & 1,000 & 50,000  & Object \\
% 		Caltech101~\cite{fei2004learning} & 101 & 2,465 & Object \\
% 		FGVCAircraft~\cite{maji2013fine} & 100 & 3,333  & Fine-grained \\
% 		StanfordCars~\cite{Krause_2013_ICCV_Workshops} & 196 & 8,041 & Fine-grained \\
% 		Flowers102~\cite{nilsback2008automated} & 102 & 2,463 & Fine-grained \\
%         OxfordPets~\cite{parkhi2012cats} & 37 & 3,669 & Fine-grained \\
%         Food101~\cite{bossard2014food} & 101 & 30,300 & Fine-grained \\
% 		DTD~\cite{cimpoi2014describing} & 47 & 1,692 & Textures \\
% 		EuroSAT~\cite{helber2019eurosat} & 10 & 8,100 & Satellite Images \\
%         UCF101~\cite{soomro2012ucf101} & 101 & 3,783 & Actions \\
% 		SUN397~\cite{xiao2010sun} & 397 & 19,850 & Scenes \\
		
% 		\hline
% 		\end{tabular}
% 		}    
% 		\label{table:dataset}
% 		\begin{tablenotes}
% 		\scriptsize
% 		\item[*] We use these datasets for few shot image classification task. The number of training data and validation data is set to be a specific multiple of the number of classes in training period. Thus, we only demonstrate the number of testing data.
% 		\end{tablenotes}
		
% 	% \vspace{-.15in}
% 	\end{table}
\noindent\textbf{Implementation Details}.
We transfer CLIP to the few-shot image classification task with PPL. Specifically, we use 1, 2, 4, 8, and 16-shot training sets to optimize the model and evaluate it on the full test set, following~\cite{radford2021learning}. For $n$-shot image classification, we random sample $n$ images per category for training. The results reported below is the average of three runs with different random seeds. All images are resized to $224\times 224$. Random cropping, resizing, and random horizontal flipping strategy are used for data augmentation.
We utilize ResNet-50~\cite{he2016deep} followed by a self-attention layer as the image encoder of CLIP. 
For a fair comparison, all experiments only use single text prompt, except learnable text prompt methods. For ImageNet and SUN397, the text prompt is set to be ``a photo of a [class].'', while a task-relevant sentence is added for fine-grained classification datasets, \eg, for Flowers102, the text prompt is ``a photo of a [class], a type of flower.'' For other datasets, the text prompt is set to be a task-related context, \eg, for UCF101, the text prompt is ``a photo of a person doing [class].''
We adopt Adam optimizer for optimization. By adjusting the weight decay hyper-parameter of the Adam optimizer, we can change the sparsity of the parameter prompts, which will be discussed in \cref{sec.analysis}.
For ImageNet, the maximum epoch is set to be 10, the weight decay is set to be 1e-5 and the parameter prompts are applied on all layers.
For other datasets, the weight decay is set to be 1e-4, and the maximum epoch is set to be 30 for 16/8/4-shot, and 60 for 2/1-shot. The parameter prompts are applied on all layers for 16/8/4-shot, while the parameter prompts are only applied on the last self-attention layer for 2/1-shot (except for EuroSAT where the  parameter prompts are applied on the last layer for all settings).
The init value $\pi$ and threshold $\alpha$ are set to be 1e-2 and 5e-3, respectively.
% The initial learning rate is set to be 5e-5, which is decayed by 0.1 at half of the maximum epoch.



\subsection{Comparison to State-of-the-Art Methods}

% Figure environment removed

\noindent
\textbf{Main Results on 11 Datasets.} We compare PPL with two baseline methods and two state-of-the-art prompt-based methods on the aforementioned 11 datasets, which are demonstrated in \cref{fig:main_results}. 
% It is mentioned that although the VPT and UPT adopt the visual/unified prompt, they are only for transformer architecture to design methods and are not universal. Thus, we do not include the VPT and UPT in this setting of 11 Datasets.
The baseline methods include Zero-shot CLIP and Linear probe CLIP: 1) the former directly transfers to the downstream task without training; 2) the latter trains a linear classifier for downstream tasks based on the pretrained CLIP. The prompt-based methods are CoOP~\cite{zhou2022learning} and ProGrad~\cite{zhu2022prompt}. Both of them tend to learn better text prompts for adaptation to downstream tasks. 
According to~\cref{fig:Average}, the PPL outperforms these methods on average over 11 datasets, which approves the ability of PPL to transfer CLIP to the downstream tasks with few-shot data. Concretely, the PPL improves the second-best method ProGrad by 3.85\% on the 16-shot setting, while 33.50\% of parameters in the image encoder are left out on average.
Moreover, the PPL achieves the best accuracy in most of the datasets on the 16/8/4/2-shot setting. From \cref{fig:FGVCAircraft}, we find the improvement in terms of accuracy is more significant as the training few-shot data increase for the FGVCAircraft dataset.
However, given the 1-shot data for training, there is performance degradation of PPL on some datasets, \eg Food101. This shows the reliability of the parameter prompt depends on the number of training data. A small quantity of training data for fine-grained classification tasks may lead to overfitting.
% to select preferred knowledge within the pretrained model for downstream tasks.

\noindent\textbf{Comparison with Zero-shot CLIP.}
In \cref{fig:compare_zs}, we demonstrate the absolute improvement of PPL compared with Zero-shot CLIP and the sparsity of parameter prompts on the 16-shot setting. The PPL boosts the performance of Zero-shot CLIP on all datasets.  Significant improvements were achieved on EuroSAT and FGVCAircraft dataset, which reach 49.03\% and 35.35\%, respectively. For FGVCAircraft, the sparsity of parameter prompts reaches 73.47\% (\ie, the 73.47 percentage of zeros in binary masks). It proves that the pre-trained weights contain some unnecessary information for the downstream task, which may harm the transfer ability of the pre-trained model. 


% Figure environment removed

\noindent
\textbf{Different Vision Backbones.} In~\cref{table:vision_backbone}, we report the results of implementing PPL on different vision backbones of CLIP on 16-shot ImageNet, including ResNet50, ResNet101, ViT-B/16, and ViT-B-32. We can observe that the PPL outperforms all parameter-efficient methods on all kinds of vision backbones by leaving out 9.97\% of weight parameters on average. When utilizing ResNet50, ResNet101, and ViT-B-32 as vision backbones, the PPL surpasses the second-best method TIP-Adapter~\cite{zhang2022tip} by 0.68\%, 1.31\%, and 0.79\%, respectively. And PPL exceeds UPT~\cite{zang2022unified} by 0.66\% while using ViT-B-16 as the vision backbone. These results demonstrate that the proposed parameter prompt is superior to the visual or text prompts.
\subsection{Combination with State-of-the-Art Methods}
\begin{table}[tb]
	\caption{Comparison with the state-of-the-art methods with different vision backbones on 16-shot ImageNet.}
	
	\centering
	\vspace{-0.20cm}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|c|c|c}
		\hline
		\multirow{1}{*}{Methods} & \multirow{1}{*}{ResNet50} &
		\multirow{1}{*}{ResNet101}  &
		\multirow{1}{*}{ViT-B/16}&
		\multirow{1}{*}{ViT-B/32}\\ 
		\hline 
		
		Zero-shot CLIP & 58.18  & 61.62   & 66.73 &  62.05 \\
		VPT~\cite{jia2022visual} & - & - & 70.57 &- \\
		CoOP~\cite{zhou2022learning} & 62.90 & 66.60 & 71.92& 66.85  \\
		CLIP-Adapter~\cite{gao2021clip} & 63.59  & 65.39  & 71.13 & 66.19  \\
		TIP-Adapter~\cite{zhang2022tip} & 64.17* & 66.42*  & 72.36* &  67.12* \\
		UPT~\cite{zang2022unified} & - & - & 72.63 &- \\
		\hline
		PPL & \textbf{64.85~\textcolor{red}{+0.68}} & \textbf{67.73~\textcolor{red}{+1.31}}  & \textbf{73.29~\textcolor{red}{+0.66}} & \textbf{67.91~\textcolor{red}{+0.79}}  \\
		Sparisty & 5.32 & 9.80  & 12.97   & 11.80  \\
		\hline
		\end{tabular}
		}  
		\begin{tablenotes}
    		\scriptsize
    		\item[*] For TIP-Adapter, the results reported by Zhang \etal~\cite{zhang2022tip} on ImageNet are based on using prompt ensembling as text input. For a fair comparison, we conduct experiments with a single text prompt for TIP-Adapter and report the results here.
		\end{tablenotes}
		\label{table:vision_backbone}
		
	\vspace{-.1in}
\end{table}

\noindent
To prove the PPL is synergistic to existing parameter-efficient methods, we combine it with CoOP~\cite{zhou2022learning} and TIP-Adapter~\cite{zhang2022tip} on 16-shot ImageNet and FGVCAircraft, as shown in~\cref{table:combine_sota}. Particularly, the few-shot training set for PPL, PPL+CoOP, and PPL+TIP-Adapter is the same.
For PPL+CoOP, the learned text prompt is initialized as ``a photo of a [class]''. The length of the text prompt is set to 4. The maximum training epoch is set to be 50 for ImageNet, and 200 for FGCVAircraft, following CoOP~\cite{zhou2022learning}. The results of CoOP reported in~\cref{table:combine_sota} are obtained with the same experimental setting for a fair comparison. The PPL+CoOP boosts the performance of CoOP on ImageNet and FGVCAircraft by 1.79\% and 22.32\%, respectively. The performance degradation of PPL+CoOP compared with PPL may due to overfitting.
For PPL+TIP-Adapter, the $\alpha$ and $\beta$ are set to be 1.0 and 5.5, respectively, following TIP-Adapter~\cite{zhang2022tip}. The adapter is training for 20 epochs. The initial learning rate is set to be 0.001. The hyper-parameter setting for training is the same for TIP-Adapter. The PPL+TIP-Adapter improves TIP-Adapter on 16-shot ImageNet and FGCVAircraft by 1.61\% and 17.38\%, respectively.
This verifies the ability of PPL to endow existing parameter-efficient methods with the ability to better adapt to the downstream task.




\subsection{Ablation Study}

\subsubsection{Applying PPL on Text/Image Encoders of CLIP}
% \todo{\cref{table:different_encoder}}

We conduct experiments to analyze the influence of applying PPL on different encoders in CLIP. \cref{table:different_encoder} shows the results on 16-shot FGVCAircraft. The PPL indicates we apply parameter prompts on the image encoder $\boldsymbol{G}_I$. The PPL-T denotes we only train parameter prompts for the text encoder $\boldsymbol{G}_T$ of CLIP, while the PPL-A means we apply parameter prompts on both the image and text encoders. 
However, training parameter prompts for the two encoders simultaneously is hard to converge. We deem this may destroy the affinity between text and image features encoded by CLIP.
Thus, we first train parameter prompts for the image encoder and freeze them for training parameter prompts for the text encoder in PPL-A.
The sparsity of PPL and PPL-T is comparative. But the accuracy improvement is much more significant with PPL. This indicates the unnecessary parameters in the text encoder have less impact on the performance of CLIP compared with the image encoder. Thus, selecting useful parameters in the image encoder for the downstream few-shot classification task makes more sense.
Moreover, the sparsity of parameter prompts for the text encoder is high for PPL-A, but the accuracy decreases. Thus, we only apply parameter prompts on the image encoder of CLIP.

\begin{table}[tb]
	\caption{Combining with state-of-the-art methods on 16-shot ImageNet and FGVCAircraft.}
	\centering
	\vspace{-0.20cm}
	\resizebox{2.6in}{!}{
		\begin{tabular}{l|c|c}
		\hline
		\multirow{1}{*}{Methods} & \multirow{1}{*}{ImageNet} & \multirow{1}{*}{FGVCAircraft}\\ 
		\hline 
		
		Zero-shot CLIP & 58.18 & 17.01 \\
		PPL & \textbf{64.85~\textcolor{red}{+ 6.67}} & \textbf{52.36~\textcolor{red}{+ 35.35}}   \\ \hline
		CoOP~\cite{zhou2022learning} & 63.33 & 28.47  \\
		PPL+CoOP & \textbf{65.12~\textcolor{red}{+ 1.79}} & \textbf{50.79 ~\textcolor{red}{+ 22.32}} \\ \hline
		TIP-Adapter~\cite{zhang2022tip} & 64.17* & 35.55 \\
		PPL+TIP-Adapter &\textbf{65.78 ~\textcolor{red}{+ 1.61}} & \textbf{52.93 ~\textcolor{red}{+ 17.38}} \\
		
		\hline
		\end{tabular}
		} 
		\label{table:combine_sota}
		
	\vspace{-.1in}
	\end{table}

\begin{table}[tb]
	\caption{Influence of applying the mask on different encoders of CLIP on 16-shot FVGCAircraft.}
	
	\centering
	\vspace{-0.20cm}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|c|c|c}
		\hline
		\multirow{1}{*}{Methods} & \multirow{1}{*}{Zero-shot CLIP} &
		\multirow{1}{*}{PPL} &
		\multirow{1}{*}{PPL-T}&
		\multirow{1}{*}{PPL-A}\\ 
		\hline 
	
		\hline
		Sparsity ($\boldsymbol{G}_T$)  & 0 & 0 &  72.62 & 87.95 \\
		Sparsity ($\boldsymbol{G}_I$)  & 0 & 73.47 &  0 & 73.47 \\
		\hline
		Accuracy & 17.01  & \textbf{52.36}& 39.48 & 50.93 \\
		
		\hline
		\end{tabular}
		}    
		\label{table:different_encoder}
		
	\vspace{-.15in}
	\end{table}


\subsubsection{Analysis of Parameter Prompts}
\label{sec.analysis}
	
\noindent
\textbf{Sparsity Control.}	
For the Adam optimizer, the weight decay $\lambda$ is generally set to be 0. When weight decay is set to be large than 0, it equals adding an L2-regularization term to the loss, which can be formulated as $\lambda\sum_{i,j}m_{i,j}^2$. Thus, we assume that when we set the $\lambda$ to be a large value, the parameters in the learnable matrix $\boldsymbol{M}$ will be fairly small, which results in high sparsity. But if we set the $\lambda$ to be 0, we set no restrictions to the sparsity of the learnable matrix. In this case, the parameter prompts can find a small number of unnecessary parameters within the image encoder. 
Thus, we conduct an ablation study on the weight decay $\lambda$ as shown in \cref{table:wd}. The best performance is obtained when $\lambda=1e-5$.
We observe that when we set $\lambda=1e-4$, the sparsity of parameter prompts increases a lot. The gain diminishes may be caused by leaving out important parameters.
When $\lambda=1e-6$, the sparsity of parameter prompts decreases, and there are still unnecessary parameters that have not been removed in the image encoder.
Thus, the weight decay hyper-parameter $\lambda$ is important for PPL. A suitable weight decay can lead the parameter prompts to find an appropriate sparsity of binary mask, which can leave out unnecessary parameters and keep the important ones.


% \begin{table}[tb]
% 	\caption{Sparsity of parameter prompt on different layers of image encoder on 16-shot ImageNet}
% 	\centering
% 	\vspace{-0.20cm}
% 	\resizebox{\linewidth}{!}{
% 		\begin{tabular}{c|c|c|c|c|c}
% 		\hline
% 		\multirow{1}{*}{Dataset} &\multirow{1}{*}{Layer 1} &\multirow{1}{*}{Layer 2} &\multirow{1}{*}{Layer 3} &\multirow{1}{*}{Layer 4} &\multirow{1}{*}{SA} \\ 
% 		\hline 
% 		ImageNet &0.20 & 0.59 & 1.42 & 4.02 & 8.36 \\
% 		FVGCAircraft & 7.83 & 25.48 & 50.97 & 74.81 & 87.35 \\
% 		\hline
% 		\end{tabular}
% 		} 
% 		\label{table:layer_sparsity}
		
% 	\vspace{-.15in}
% 	\end{table}
	
\begin{table}[tb]
	\caption{Influence of weight decay of optimizer on 16-shot ImageNet.}
        % \renewcommand{\tabcolsep}{10.pt}
	\centering
	\vspace{-0.20cm}
	\resizebox{1.7in}{!}{
		\begin{tabular}{c|c|c}
		\hline
		\multirow{1}{*}{$\lambda$} & \multirow{1}{*}{Accuracy} &
		\multirow{1}{*}{Sparsity} \\ 
		\hline
		1e-6 & 64.78 & 1.02 \\
		1e-5 & 64.85 & 5.32   \\
		1e-4 & 64.45 &  23.64 \\
		\hline
		\end{tabular}
		} 
		\label{table:wd}
		
	\vspace{-.1in}
	\end{table}
	
\noindent
\textbf{Parameter Prompts on Different Layers.} 
% In \cref{table:layer_sparsity}, we present the sparsity of the parameter prompt on each layer of the image encoder. The models are trained with 16-shots ImageNet and FVGCAircraft. Layer 1-4 denotes the 4 layers in ResNet50 from bottom to up, and SA denotes the last multi-head attention layer. We can observe that the sparsity is relatively higher in the top layers, \eg multi-head attention layer and Layer 4 of ResNet, on both of the datasets. This suggests that knowledge that exists in lower layers is more general than the knowledge in top layers, which is consistent with previous findings~\cite{howard2018universal,mallya2018piggyback,zhao2020masking}.
We consider applying parameter prompts on different layers in the image encoder from top to bottom. The results are demonstrated in \cref{table:layer_ppl}. Layer 1-4 denotes the 4 layers in ResNet50 from bottom to up, and SA denotes the last self-attention layer.
As the number of masked layers increases, the sparsity of parameter prompts become higher. The best accuracy is obtained when parameter prompts are applied on Layer3,4+SA. But it only surpasses applying parameter prompts on all layers by 0.01\%. Thus, we recommend applying parameter prompts on all layers unless the number of training data is too small.
In \cref{fig:grad_cam}, we visualize the image feature maps of Zero-shot CLIP, PPL-SA, and PPL by Grad-CAM~\cite{selvaraju2017grad}. The PPL-SA denotes the parameter prompt is only used on the last self-attention layer. We observe that the parameter prompts lead the model to focus on more regions of the foreground, which demonstrate that the parameter prompts can suppress redundant information.
% and the influence of background is the least when parameter prompts are applied to the whole image encoder.

\begin{table}[tb]
% \renewcommand{\tabcolsep}{10.pt}
	\caption{Applying the parameter prompt on different layers of image encoder on 16-shot ImageNet.}
	\centering
	\vspace{-0.20cm}
	\resizebox{2.1in}{!}{
		\begin{tabular}{c|c|c}
		\hline
		\multirow{1}{*}{Layers} & \multirow{1}{*}{Accuracy} &
		\multirow{1}{*}{Sparsity} \\ 
		\hline 
		SA &  64.57  & 3.28   \\
		Layer 4+SA &   64.67  &  4.84  \\
		Layer 3,4+SA &  \textbf{64.86} &  4.99  \\
		Layer 2,3,4+SA &  64.79 & 5.05  \\
		All &  64.85 & \textbf{5.32}  \\
		\hline
		\end{tabular}
		} 
		\label{table:layer_ppl}
		
	\vspace{-.15in}
	\end{table}



% (intersection over union)	
\noindent
\textbf{Parameter Prompts for Different Datasets.} As shown in~\cref{fig:cm}, we present the IoU of parameter prompts between two arbitrary datasets on the 16-shot setting. Since we random sample 16 images per class to train the parameter prompts three times with different seeds, the parameter prompts within one dataset are not always the same. This result demonstrates that for downstream tasks, the knowledge of pre-trained weight is not invariable.
% the average of IoU between parameter prompts trained in two arbitrary runs. 
From \cref{fig:cm}, we also observe that for each dataset the maximum IoU is always itself, which indicates the parameter prompts can find task-specific parameters within CLIP. However, the IoU between some two datasets is relatively high, like ImageNet and Food101. This might result from the similarity in data. The two datasets contain some same categories, \eg, hot dogs and ice cream. Future work may explore the relationship between the parameter prompts and the attribute of datasets.

% Figure environment removed


% Figure environment removed