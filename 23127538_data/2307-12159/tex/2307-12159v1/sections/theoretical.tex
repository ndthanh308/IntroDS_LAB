\section{Related Works}
\label{s.related_works}

Facial expression is a significant part of human nonverbal contact, is more effective than words in face-to-face communication~\cite{mehrabian1968some}, and serves as a distinctive universal means of transmission. Very often, impaired facial expressions  manifest as symptomatic indications across countless medical conditions~\cite{yolcu2019facial}.

Bandini et al.~\cite{bandini2018automatic} introduced a novel approach for automatically detecting bulbar ALS. Their method involves analyzing facial movement features extracted from video recordings. The dataset comprises ten ALS patients (six male and four female) and eight age-matched healthy control subjects (six male and two female), which were asked to perform specific actions during recordings. Initially, each individual was recorded at rest (REST) with a neutral facial expression for 20 seconds. It is worth noting that this task was not used for analysis but only as a reference for extracting the geometric characteristics during the tasks.

Next, each participant was asked to perform the following actions: open their jaw to the maximum extent, repeated five times (OPEN); lip puckering (as if kissing a baby) a total of four times (KISS); pretend to blow out a candle, five times (BLOW); smile with closed lips, five times (SPREAD); repeat the syllable /pa/ in a single breath as fast as possible (PA); repeat the word /pataka/ as quickly as possible (PATAKA); repeat the sentence "Buy Bobby a puppy" (BBP) ten times in their usual tone and speaking speed.

Furthermore, the image pre-processing step was performed using the supervised descent method~\cite{xiong2013supervised}, which extracts corresponding facial landmarks for eyebrows, eyelids,  and nose, as likewise outer and inner lip contours for each frame. Also, a third coordinate was estimated for these landmarks based on intrinsic camera parameters. In this regard, feature extraction was carried out considering the points in the mouth region, as they demonstrated greater sensitivity to ALS. Considering aspects of lip movement such as range and speed of motion, symmetry, and shape, two different algorithms were used for classification: SVM and Logistic Regression. Last but not least, the best classification result was achieved in the BBP task, with an accuracy of 88.9\%.

% Ademais, o pré-processamento das imagens foi feito com auxílio do supervised descent method (SDM)~\cite{xiong2013supervised} que para cada frame extrai pontos da face correspondentes as sobrancelhas, pálpebras, nariz, contornos externos e internos aos lábios. Vale ressaltar que, foi estimado uma terceira coordenada para esses pontos com base em parâmetros intrinsecos da câmera. Neste sentido, a extração de características foi feita considerando os pontos da região da boca, visto que eles demostravam maior sensitividade para ALS. Considerando aspectos do movimento dos lábios como range of motion, speed of motion, symmetry, e shape, utilizou-se de dois algoritmos distintos para a classificação: a SVM e a regressão logística. Assim, o melhor resultado para a classificação foi atingido na task BBP, alcançando uma acurácia de 88.9\%.

Xu et al.~\cite{xu2020facial} conducted a study on classifying expressions using facial landmarks. Their approach used a Graph Convolutional Network (GCN)~\cite{kipf2016semi} to classify facial expressions in images. They employed the Dlib-ml machine learning algorithm~\cite{king2009dlib} to estimate the positions of 64 facial landmarks, which are employed to construct a graph along with their two-dimensional coordinates. The training process incorporated three different databases: JAFFE~\cite{lyons1998coding}, FER2013~\cite{goodfellow2013challenges}, and CK+~\cite{lucey2010extended}. The classes considered in this study included Anger, Disgust, Fear, Happiness, Sadness, and Surprise, achieving an accuracy of 95.85\%.

% A classificação de expressões faciais baseadas em pontos da face foi realizada by Xu, Xu and Ruan, Zhou and Yang, Lei~\cite{xu2020facial}. Nesta abordagem o intuíto foi avaliar a classificação da expressão facial em imagens com um um tipo de GNN, denominado Graph Convolutional Network (GCN)~\cite{kipf2016semi}. First, the positions of landmarks were estimated using a machine learning algorithm called Dlib-ml~\cite{king2009dlib} que extrai 64 pontos da face na imagem. Neste aspecto, os pontos juntos a suas coordenadas no plano bidimensional foram utilizados para formar um grafo que, por fim entra no modelo de GCN proposto. Vale ressaltar que, o treinamento do modelo foi realizado com 3 bancos de dados distintos: JAFFE~\cite{lyons1998coding}, FER2013~\cite{goodfellow2013challenges} e CK+~\cite{lucey2010extended}. Neste caso, as classes de emoções disponíveis foram Anger, Disgust, Fear, Happiness, Sadness, e Surprise atingindo uma acurácia de 95.85\% com o algoritmo proposto. Entretanto, uma abordagem para diagnóstico de doenças com base na expressão facial e utilizando GNNs ainda não havia sido utilizada.

\section{Theoretical Background}
\label{s.theoretical}

Graph Neural Networks bring the problem of learning patterns in a dataset to the graph domain. Formally, a graph ${\cal G} = ({\cal V}, {\cal E})$ is defined as a set of nodes $V{\cal }$ and a set of edges ${\cal E}$ between them, aka the adjacency relation. During the iteration process, each node (receiver) receives a set of aggregated messages from its neighbors, applying an aggregation function and an update function. It is worth noting that each node forwards information to its neighbors before its features are updated. In the next iteration, it forwards the new information (message) to its neighbors once more, as illustrated in Figure~\ref{fig:GNN_explanation}. 

For each iteration $k$, a hidden vector ${\textbf{h}_u}^{(k)}\in\mathbb{R}^n$ incorporates the features of node $u \in {\cal V}$, where $n$ stands for the number of input features. It is worth noting that the hidden vector ${\textbf{h}_u}^{(0)}$ encodes the features before training, i.e., at the initial stage. Firstly, a node-order invariant function is used to aggregate features from the neighborhood $\EuScript{N}(u)$ of node $u$. Secondly, the aggregated features are used to update the node information, described as follows:

\begin{equation}
\label{Eq1}
\textbf{h}_u^{k+1} = U^{k}~\left(\textbf{h}_u^{k},~A^{k}_u \left(\{\textbf{h}_v^{k}, ~\forall v \in \EuScript{N}(u)\}\right)\right),
\end{equation}
where $U^{(k)}$ and $A^{(k)}$ stand for the updating and aggregating functions, respectively. One can use distinct models for these functions, but this paper employs a formulation based on an attention mechanism, described further,

% Figure environment removed



\subsection{Graph Attention Networks}
\label{ss.GAT}

Graph Attention Networks (GATs) are a strategy for improving the aggregation function. In this network, the message gives different priorities to the information from the neighborhood. The first application of this concept in a model was described by Veličković et al.~\cite{velivckovic2017graph} and crafted as follows:
\begin{equation}
\textbf{h}_u^{k+1} = \sigma\left(\sum_{v \in \EuScript{N}(u)}{ \alpha_{v\rightarrow u}^{k}\textbf{W}^{k}\textbf{h}_v^{k}}\right),
\end{equation}
where $\textbf{W}\in\mathbb{R}^{ n'\times n}$ is a trainable parameter known as the weight matrix, $n'$ and $\sigma$ stand for the number of output features and the sigmoid function, respectively. In addition, $\alpha_{v\rightarrow u}\in\mathbb{R}$ indicates the attention given from $v$ to the node $u$, i.e., the degree of influence $v$ has on updating the features of node $u$. A higher value of $\alpha_{v\rightarrow u}$ implies a stronger impact of $v$ on the feature update process of $u$. Formally, its definition is represented as follows:
\begin{equation}
\label{e.alpha}
    \alpha_{v\rightarrow u}^{k} = \frac{exp\left(\lambda\left(\left[\textbf{a}_u^{k}\right]^T\left[\textbf{W}^{k}\textbf{h}_u^{k} \mathbin\Vert \textbf{W}^{k}\textbf{h}_v^{k}\right]\right) \right)}{\sum_{v' \in \EuScript{N}(u)}{ exp\left(\lambda\left(\left[\textbf{a}_u^{k}\right]^T\left[\textbf{W}^{k}\textbf{h}_u^{k} \mathbin\Vert \textbf{W}^{k}\textbf{h}_{v'}^{k}\right]\right)\right)}},
\end{equation}
where $\textbf{a}_u\in\mathbb{R}^{2*n'}$ defines a trainable parameter known as the attention vector. The symbol $\mathbin\Vert$ denotes the concatenation operator, and $\lambda$ represents the LeakyReLU non-linearity function (with negative input slope $\beta = 0.2$). 

In addition, this particular GNN has proven to be more effective in accurately identifying the healthy state of patients by analyzing the facial landmarks extracted from their expressions during task performance.

