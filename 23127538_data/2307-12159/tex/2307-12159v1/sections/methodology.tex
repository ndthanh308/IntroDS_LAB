\section{Experimental Methodology}
\label{s.methodology}

\subsection{Dataset}
\label{ss.dataset}

Established by Bandini et al. \cite{bandini2020new}, Toronto NeuroFace is the first public dataset with videos of oro-facial gestures performed by individuals with oro-facial impairments, including post-stroke (PS), ALS, and healthy control (HC). The dataset comprises 261 colored (RGB) videos of thirty-six participants: 11 patients with ALS, 14 patients with PS, and 11 HC. This work focuses on distinguishing ALS from healthy individuals, for we are primarily interested in the former. Therefore, we concentrated on a subset containing ALS and HC groups only. Each video captures a participant performing one of the subtasks from a set of speech and non-speech tasks commonly used during the clinical oro-facial examination. After manually segmenting the videos, we divided the dataset into 921 videos of repetitions. Table~\ref{t.subtask_repetition} presents the distribution of the number of repetitions for each subtask used in the experiments.

\begin{table}[ht]
\caption{Number of repetitions for each subtask.}
\begin{tabular}{|c|l|c|c|}
\hline
\textbf{Subtask}                                 & \multicolumn{1}{c|}{\textbf{Description}}                                                                                                 & \textbf{ALS}          & \textbf{HC}           \\ \hline
\cellcolor[HTML]{FFFFFF}                         &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}SPREAD} & \multirow{-2}{*}{Pretending to smile with tight lips}                                                                                     & \multirow{-2}{*}{55}  & \multirow{-2}{*}{59}  \\ \hline
                                                 &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{KISS}                           & \multirow{-2}{*}{Pretend to kiss a baby}                                                                                                  & \multirow{-2}{*}{59}  & \multirow{-2}{*}{57}  \\ \hline
\cellcolor[HTML]{FFFFFF}                         &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}OPEN}   & \multirow{-2}{*}{Maximum opening of the jaw}                                                                                              & \multirow{-2}{*}{54}  & \multirow{-2}{*}{55}  \\ \hline
                                                 &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{BLOW}                           & \multirow{-2}{*}{Pretend to blow a candle}                                                                                                & \multirow{-2}{*}{31}  & \multirow{-2}{*}{39}  \\ \hline
                                                 &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{BBP}                            & \multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Repetitions of the sentence \\ “Buy Bobby a Puppy”\end{tabular}}                              & \multirow{-2}{*}{95}  & \multirow{-2}{*}{111} \\ \hline
                                                 &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{PA}                             & \multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Repetitions of the syllables /pa/ \\ as fast as possible in a single breath\end{tabular}}     & \multirow{-2}{*}{100} & \multirow{-2}{*}{110} \\ \hline
                                                 &                                                                                                                                           &                       &                       \\
\multirow{-2}{*}{PATAKA}                         & \multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Repetitions of the syllables /pataka/ \\ as fast as possible in a single breath\end{tabular}} & \multirow{-2}{*}{88}  & \multirow{-2}{*}{108} \\ \hline
\end{tabular}
\label{t.subtask_repetition}
\end{table}

\subsection{Pre-processing}
\label{ss.pre}

To eliminate visual elements outside the subject's face and ensure consistency in the dataset, we use OpenFace 2.0 tool \cite{baltrusaitis2018openface} during the preprocessing stage. This tool first detects the main face, then perform a transformation based on head pose estimation and a crop operation on all frames. The resulting output ends up in $200\times200$ grayscale images centered on the facial region, as illustrated in Figure \ref{f.pre-processing}. 

% Figure environment removed

\subsection{Feature extraction}
\label{ss.pre}

In this work, we used the Facial Alignment Network (FAN)~\cite{bulat2017far}, a deep learning model, to identify the frame-by-frame face geometric structure of each video in the dataset. As a state-of-the-art approach, FAN employs heatmap regression to accurately detect facial landmark points following the MULTI-PIE 2D 68-point format~\cite{gross2010multi}, enabling alignment in two and three dimensions. Since the dataset contains videos recorded with the frontal face position, we considered the alignment in two dimensions only.

Previous studies show that patients with ALS exhibit significant sensitivity in lip and jaw movements \cite{langmore1994physiologic, bandini2018kinematic}. Therefore, we selected 26 points from the landmarks extracted by FAN that represent such regions (Figure~\ref{f.feature}a). To establish connections between these landmark nodes, we employed the Delaunay triangulation ~\cite{delaunay1934sphere}, which involves creating a triangular mesh by connecting the specific landmarks (Figure~\ref{f.feature}b).

To enhance information communication among graph nodes during the learning process, we strategically use point 31 (According to the 68-point format)~\cite{gross2010multi}, which corresponds to the nose tip, as a central node. This key node serves as a hub, connecting all other nodes independently of the Delaunay triangulation calculation (Figure~\ref{f.feature}c). Lastly, as the final step of the feature extraction process, we set the edge's weight as the Euclidean distance between its corresponding nodes. 

% Figure environment removed

\subsection{Classification and evaluation}

The classification performance was evaluated using a leave-one-subject-out cross-validation (LOSO-CV) approach, following the method proposed by Bandini et al.~\cite{bandini2018automatic}. Furthermore, to enhance the reliability of predictions in real-world scenarios and mitigate issues like overfitting or memorizing training data, we employ separate sets for training, validation, and testing in each interaction. Concerning the validation sets, we randomly select two subjects, one categorized as HC and the other as ALS, ensuring a balanced representation of both classes in this stage. 

The evaluation was conducted in two modes, i.e., repetition- and subject-based classification:

\subsubsection{Repetition classification}
\label{sss.rep}

For each iteration of the LOSO-CV, the repetitions produced by one participant were treated as individual samples in the test set. At the same time, the remaining data was split into validation and training sets. This approach ensures that every participant, both HC and those with ALS, and their respective repetitions were considered in separate test sets. During this trial, individuals' speech and non-speech repetitions were classified as belonging to the HC or ALS group. Figure~\ref{f.repetition} illustrates the process mentioned above for a given individual\footnote{In this experiment, we count the hit/miss over each repetition (for each individual) to compose the final classification accuracy. Basically, we are labeling repetitions and not individuals.}.

% Figure environment removed

\subsubsection{Subject classification}
\label{sss.sub}

At each iteration of the LOSO-CV, each subject was treated as a test case and classified as either HC or ALS. The classification was determined through a majority vote among its predicted repetitions; in tie cases, the subject was considered HC to generate a more conservative prediction according to Bandini et al.~\cite{bandini2018automatic}. Figure~\ref{f.subject} depicts an overview of the subject classification process.

% Figure environment removed

In both repetition- and subject-based classification, the validation set was used to prevent bias in the model's hyperparameters and to facilitate the implementation of the early stopping technique. The number of epochs for training was determined by monitoring the learning progress on the validation set\footnote{The maximum number of epochs is set to $100$, the batch-size comprises $64$ samples, the learning rates are set to $10^{-4}$ and $10^{-5}$ considering the GAT and linear layers, respectively. The number of hidden layers was set to $17$. These values were empirically chosen  based on the results over the validation set.}.

Considering Bandini et al.~\cite{bandini2018automatic} as the benchmark to our work, the experiments were also performed considering two other classification models for comparison purposes: SVM with linear and radial basis function (RBF) and Logistic Regression. Both models use 11 geometric and kinematic features extracted from speech and non-speech tasks. A grid search was used to find proper values for SVM parameters, i.e., the confidence value $C$ and the RBF kernel scale parameter $\gamma$.

\subsection{Proposed model}
\label{s.proposed}

Initially, the proposed model uses 15 equally spaced frames for each repetition performed by the patient. FPG receives a graph of twenty-six nodes representing the face landmarks, where each node encodes a feature vector with the $x$ and $y$ coordinates of its related landmark. In addition, each graph edge stores its length determined by the Euclidean distance between its two corresponding nodes.

Further, each frame proceeds through six GAT and two linear layers. Before the information is forwarded to the linear layers, an average pooling is performed using the nodes, i.e., all information encoded in the graph is mapped into a single vector. Figure~\ref{f.proposed_model} illustrates such a process.

% Figure environment removed

The result obtained after pooling goes through two linear layers, which generate the model's output. Nonetheless, the error is calculated based on the frame label, and the mode among frames represents the outcomes concerning the repetition experiment. In other words, classifying an individual's repetition is based on the majority consensus among the frames. Likewise, when classifying the subject, the majority mode derived from the classifications of each repetition determines whether the patient has ALS or not.
