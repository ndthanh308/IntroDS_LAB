%%% ====================================================================
%%%  BibTeX-file{
%%%     author          = "Gerry Murray",
%%%     version         = "1.2",
%%%     date            = "2 April 2012",
%%%     filename        = "acmsmall-sample-bibfile.bib",
%%%     address         = "ACM, NY",
%%%     email           = "murray at hq.acm.org",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "ACM Reference Format, bibliography, citation, references",
%%%     supported       = "yes",
%%%     docstring       = "This BibTeX database file contains 'bibdata' entries
%%%                        that 'match' the examples provided in the Specifications Document
%%%                        AND, also, 'legacy'-type bibs. It should assist authors in 
%%%                        choosing the 'correct' at-bibtype and necessary bib-fields
%%%                        so as to obtain the appropriate ACM Reference Format output. 
%%%			   It also contains many 'Standard Abbreviations'. "
%%%  }
%%% ====================================================================

% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

@article{go2009twitter,
  title={Twitter sentiment classification using distant supervision},
  author={Go, Alec and Bhayani, Richa and Huang, Lei},
  journal={CS224N project report, Stanford},
  volume={1},
  number={12},
  pages={2009},
  year={2009}
}

@inproceedings{NIPS2015_250cf8b5,
 author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Character-level Convolutional Networks for Text Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
 volume = {28},
 year = {2015}
}

@inproceedings{
zhang2018mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@inproceedings{marivate2020improving,
  title={Improving short text classification through global augmentation methods},
  author={Marivate, Vukosi and Sefara, Tshephisho},
  booktitle={Machine Learning and Knowledge Extraction: 4th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2020, Dublin, Ireland, August 25--28, 2020, Proceedings 4},
  pages={385--399},
  year={2020},
  organization={Springer}
}

@article{bayer2022survey,
  title={A survey on data augmentation for text classification},
  author={Bayer, Markus and Kaufhold, Marc-Andr{\'e} and Reuter, Christian},
  journal={ACM Computing Surveys},
  volume={55},
  number={7},
  pages={1--39},
  year={2022},
  publisher={ACM New York, NY}
}

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries
@INPROCEEDINGS{imbalancedtoxic,
  author={Ibrahim, Mai and Torki, Marwan and El-Makky, Nagwa},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Imbalanced Toxic Comments Classification Using Data Augmentation and Deep Learning}, 
  year={2018},
  volume={},
  number={},
  pages={875-878},
  doi={10.1109/ICMLA.2018.00141}}


@inproceedings{longpre-etal-2020-effective,
    title = "How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?",
    author = "Longpre, Shayne  and
      Wang, Yu  and
      DuBois, Chris",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.394",
    doi = "10.18653/v1/2020.findings-emnlp.394",
    pages = "4401--4411",
    abstract = "Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei andZou, 2019) and Back-Translation (Sennrichet al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XLNet, and RoBERTa. We observe a negative result, finding that techniques which previously reported strong improvements for non-pretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.",
}

@inproceedings{najafi2019,
 author = {Najafi, Amir and Maeda, Shin-ichi and Koyama, Masanori and Miyato, Takeru},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Robustness to Adversarial Perturbations in Learning from Incomplete Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/60ad83801910ec976590f69f638e0d6d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{wallach2019,
 author = {Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Labels Required for Improving Adversarial Robustness?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bea6cfd50b4f5e3c735a972cf0eb8450-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{carmon2019,
 author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unlabeled Data Improves Adversarial Robustness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{tyukavina2022,
author = {A. Tyukavina and P. Potapov and M. C. Hansen and A. H. Pickens and S. V. Stehman and S. Turubanova and D. Parker and V. Zalles and Andr{\'e} Lima and I. Kommareddy and X. Song and L. Wang and N. Harris},
journal = {Frontiers in Remote Sensing},
title = {Global Trends of Forest Loss Due to Fire From 2001 to 2019},
volume = {3},
year = {2022},
}

@article{luo2018,
author = {Y. Luo and Q. Lu and J. Liu and Q. Fu and J. Harkin and L. McDaid and J. Mart{\'\i}nez-Corral and G. Biot-Mar{\'\i}},
journal = {Association for Computing Machinery},
pages = {371--375},
publisher = {15th ACM International Conference on Computing Frontiers (CF' 18)},
title = {Forest fire detection using spiking neural networks},
year = {2018},
}

@article{al-zebda2021,
author = {A. K. Al-Zebda and M. M. Al-Kahlout and A. M. A. Ghaly and D. Z. Mudawah},
journal = {International Journal of Academic Information Systems Research (IJAISR)},
pages = {51-57},
title = {Predicting Forest Fires using Meteorological Data: an {ANN} Approach},
volume = {5},
year = {2021},
}

@book{yang2021,
author = {S. Yang and M. Lupascu and K. S. Meel},
publisher = {Computer Vision and Pattern Recognition (CoRR)},
title = {Predicting Forest Fire Using Remote Sensing Data And Machine Learning},
year = {2021},
}

@incollection{hochreiter1997,
author = {S. Hochreiter. and J. Schmidhuber},
booktitle = {Neural computation, vol. 9, number 8},
pages = {1735-1780},
title = {Long short-term memory},
year = {1997},
}

@inproceedings{cho2014,
author = {K. Cho and B. V. Merri{\"e}nboer and C. Gulcehre and D. Bahdanau and F. Bougares and H. Schwenk and Y. Bengio},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
title = {Learning phrase representations using RNN encoder-decoder for statistical machine translation},
year = {2014},
}

@article{albawi2017,
author = {S. Albawi and T. A. Mohammed and S. Al-Zawi},
booktitle = {International Conference on Engineering and Technology (ICET)},
pages = {1-6},
title = {Understanding of a convolutional neural network},
year = {2017},
}

@book{breiman1984,
author = {L. Breiman and J. H. Friedman and R. A. Olshen and C. J. Stone},
publisher = {The Wadsworth Statistics/Probability Series},
title = {Classification and regression trees},
year = {1984},
}

@article{breiman2001,
author = {L. Breiman},
journal = {Machine Learning},
number = {1},
pages = {5-32},
title = {Random forests},
volume = {45},
year = {2001},
}

@inproceedings{chen2016,
author = {T. Chen. and C. Guestrin},
pages = {785-794},
publisher = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
}

@article{ke2017,
author = {G. Ke and Q. Meng and T. Finley and T. Wang and W. Chen and W. Ma and Q. Ye and T. Liu and Tie-Yan},
journal = {Advances in neural information processing systems},
pages = {3146-3154},
title = {Lightgbm: A highly efficient gradient boosting decision tree},
volume = {30},
year = {2017},
}

@article{prokhorenkova2017,
author = {L. Prokhorenkova and G. Gusev and A. Vorobev and A. V. Dorogush and A. Gulin},
publisher = {Conference on Neural Information Processing Systems (NeurIPS)},
title = {CatBoost: unbiased boosting with categorical features},
year = {2018},
}

@article{lewis1998,
author = {D. D. Lewis},
journal = {European Conference on Machine Learning (ECML)},
pages = {4-15},
title = {Naive Bayes at forty: The independence assumption in information retrieval},
year = {1998},
}

@article{altman1992,
author = {N. S. Altman},
journal = {The American Statistician},
number = {3},
pages = {175-185},
title = {An introduction to kernel and nearest-neighbor nonparametric regression},
volume = {46},
year = {1992},
}

@book{haykin1994,
author = {S. Haykin},
publisher = {Prentice Hall},
title = {Neural networks: a comprehensive foundation},
year = {1994},
}

@article{cortes1995,
author = {C. Cortes and V. Vapnik},
journal = {Machine learning},
number = {3},
pages = {273-297},
title = {Support-vector networks},
volume = {20},
year = {1995},
}

@article{parmezan2019,
author = {A. R. Parmezan and V. M. A. Souza and G. E. A. P. A. Batista},
journal = {Information Sciences},
pages = {302-337},
title = {Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-art and the best conditions for the use of each model},
volume = {484},
year = {2019},
}

@article{akter2021,
author = {R. Akter and J. -M. Lee and D. -S. Kim},
pages = {732-734},
journal = {International Conference on Information Networking (ICOIN)},
title = {Analysis and Prediction of Hourly Energy Consumption Based on Long Short-Term Memory Neural Network},
year = {2021},
}

@inproceedings{vanitha2019,
author = {Prabha P. P and V. Vanitha and Resmi R},
booktitle = {2nd International Conference on Intelligent Computing},
pages = {1310-1314},
publisher = {Instrumentation and Control Technologies (ICICICT)},
title = {Wind Speed Forecasting using Long Short Term Memory Networks},
year = {2019},
}

@article{xu2021,
author = {J. Xu and K. Wang and C. Lin and L. Xiao and X. Huang and Y. Zhang},
journal = {Water},
pages = {8},
title = {FM-GRU: A Time Series Prediction Method for Water Quality Based on seq2seq Framework},
volume = {13},
year = {2021},
}

@article{zainuddin2021,
author = {Z. Zainuddin and P. Akhir E. A. and Hasan M. H.},
journal = {Bulletin of Electrical Engineering and Informatics},
number = {2},
pages = {870-878},
title = {Predicting machine failure using recurrent neural network-gated recurrent unit (RNN-GRU) through time series data},
volume = {10},
year = {2021},
}

@inproceedings{mehtab2020,
author = {S. Mehtab and J. Sen and S. Dasgupta},
booktitle = {4th International Conference on Electronics},
publisher = {Communication and Aerospace Technology (ICECA},
title = {Robust Analysis of Stock Price Time Series Using {CNN} and {LSTM}-Based Deep Learning Models},
year = {2020},
}

@inproceedings{koprinska2018,
author = {I. Koprinska and D. Wu and Z. Wang},
booktitle = {International Joint Conference on Neural Networks (IJCNN},
pages = {1-8},
title = {Convolutional Neural Networks for Energy Time Series Forecasting},
year = {2018},
}

@article{soriano2015,
author = {B. M. A. Soriano and O. Daniel and S. A. Santos},
journal = {Ci{\^e}ncia Florestal},
number = {4},
pages = {809-816},
title = {Efficiency of fire risk indices for the Pantanal Sul-Mato-Grossense (in Portuguese)},
volume = {25},
year = {2015},
}

@book{vigano2017,
author = {H. H. da G. Vigan{\'o} and C. C. de Souza and M. F. Cristaldo and L. de Jesus},
publisher = {Revista Brasileira de Geografia F{\'\i}sica},
title = {Artificial Neural Networks in Prediction of Forest Fires and Burns in the Pantanal (in Portuguese)},
year = {2017},
}

@article{alho2019,
author = {C. J. R. Alho and S. B. Mamede and M. Benites and B. S. Andrade and J. J. O. Sep{\'u}lveda},
journal = {Ambiente \& Sociedade},
title = {Threats to the Biodiversity of the Brazilian Pantanal by Land Use and Occupation (in Portuguese)},
volume = {22},
year = {2019},
}

@book{inpe2022,
author = {National Institute for Space Research (INPE)},
publisher = {Available at \url{https://queimadas.dgi.inpe.br/queimadas/bdqueimadas}},
title = {Fires: monitoring of hotspots (in Portuguese)},
}

@book{soares1972,
address = {Inter-American Institute of Agricultural Sciences, Costa Rica},
author = {R. V. Soares},
publisher = {Tropical Teaching and Research Center},
title = {Determination of a fire danger index for the central region of Paran{\'a}, Brazil (in Portuguese)},
year = {1972},
}

@book{soares2006,
author = {J. R. S. Nunes and R. V. Soares and A. C. Batista},
publisher = {Revista Floresta},
title = {FMA+ - a new wildfire danger index for the State of Paran{\'a}, Brazil (in Portuguese)},
year = {2006},
}

@article{telicyn1970,
author = {G. P. Telicyn},
journal = {Lesnoe Khozyaistvo},
number = {1},
pages = {1-58},
title = {Logarithmic index of fire weather danger for forests},
volume = {11},
year = {1970},
}

@article{casavecchia2019,
author = {B. H. Casavecchia and A. P de Souza and D. M. Stangerlin and E. M. Uliana and R. R. Melo},
journal = {Revista de Ci{\^e}ncias Agr{\'a}rias},
pages = {3},
title = {Fire danger indices in the transition area of Cerrado-Amazonia (in Portuguese)},
volume = {42},
year = {2019},
}

@unpublished{soares2007,
author = {R. V. Soares and A. C. Batista},
title = {Forest fires: control, effects and use of fire (in Portuguese)},
year = {2007},
}

@article{ziccardi2020,
author = {L. G. Ziccardi and C. R. Thiersch and A. M. Yanai and P. M. Fearnside and P. J. Ferreira-Filho},
journal = {Journal of Forestry Research},
number = {2},
pages = {581-590},
title = {Forest-fire risk indices and zoning of hazardous areas in Sorocaba, S{\=a}o Paulo state, Brazil},
volume = {31},
year = {2020},
}

@book{nesterov1949,
author = {V. G. Nesterov},
publisher = {USSR State Industry Press},
title = {Combustibility of the forest and methods for its determination (in Russian)},
year = {1949},
}

@article{angstrom1942,
author = {A. Angstrom},
journal = {Svenska Skogsvirdsforeningens Tidskrift},
pages = {323-343},
title = {The risks for forest fires and their relation to weather and climate (in Swedish)},
volume = {40},
year = {1942},
}

@book{torres2008,
author = {F. T. P. Torres and G. A. Ribeiro},
publisher = {Floresta e Ambiente},
title = {Efficiency of indices of risk of forest fire in Juiz de Fora/MG (in Portuguese)},
year = {2008},
}

@book{miranda2021,
author = {G. J. D. Miranda and B. A. F. de Mendon\c{c}a and E. R. S. de Oliveira and K. A. de Oliveira and J. M. N. Romeiro and F. T. P. Torres},
publisher = {Floresta},
title = {Large Fires and Fire Danger Indices in 'Governador' Indigenous Territory, Maranh{\=a}o State},
year = {2021},
}

@article{shahriyari2019,
author = {L. Shahriyari},
journal = {Brief Bioinform},
number = {3},
pages = {985-994},
title = {Effect of normalization methods on the performance of supervised learning algorithms applied to HTSeq-FPKM-UQ data sets: 7SK RNA expression as a predictor of survival in patients with colon adenocarcinoma},
volume = {20},
year = {2019},
}

@article{ahsan2021,
author = {M. M. Ahsan and M. A. P. Mahmud and P. K. Saha and K. D. Gupta and Z. Siddique},
journal = {Technologies},
pages = {3},
title = {Effect of Data Scaling Methods on Machine Learning Algorithms and Model Performance},
volume = {9},
year = {2021},
}

@article{hamad2020,
author = {R. A. Hamad and L. Yang and W. L. Woo and B. Wei},
journal = {Applied Sciences},
pages = {15},
title = {Joint Learning of Temporal Models to Handle Imbalanced Data for Human Activity Recognition},
volume = {10},
year = {2020},
}

@article{wu2020,
author = {S. Wu and X. Xiao and Q. Ding and P. Zhao and Y. Wei and J. Huang},
publisher = {Conference on Neural Information Processing Systems (NeurIPS},
title = {Adversarial Sparse Transformer for Time Series Forecasting},
year = {2020},
}

@article{nguyen2021,
author = {N. Nguyen and B. Quanz},
publisher = {The Thirty-Fifth AAAI Conference on Artificial Intelligence},
title = {Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate Time Series Forecasting},
year = {2021},
}

@book{narciso2019,
author = {M. Narciso and B. Soriano},
publisher = {Congresso Brasileiro de Agroinform{\'a}tica},
title = {SARIPAN - Fire Risk Alert System for the Pantanal (in Portuguese)},
year = {2019},
}

@inproceedings{skolova2006,
author = {M. Sokolova and N. Japkowicz and S. Szpakowicz},
year = {2006},
pages = {1015-1021},
title = {Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation},
volume = {4304},
journal = {AI 2006: Advances in Artificial Intelligence, Lecture Notes in Computer Science}
}

@article{willmott2005,
author={C. J. Willmott and K. Matsuura},
title={Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance},
journal={Climate Research},
year={2005},
volume={30},
number={1},
pages={79-82}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{convabusedataset,
    title = "{C}onv{A}buse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational {AI}",
    author = "Cercas Curry, Amanda  and
      Abercrombie, Gavin  and
      Rieser, Verena",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.587",
    doi = "10.18653/v1/2021.emnlp-main.587",
    pages = "7388--7403",
    abstract = "We present the first English corpus study on abusive language towards three conversational AI systems gathered {`}in the wild{'}: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more {`}nuanced{'} approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90{\%}.",
}

@article{amini2022self,
  title={Self-Training: A Survey},
  author={Amini, Massih-Reza and Feofanov, Vasilii and Pauletto, Loic and Devijver, Emilie and Maximov, Yury},
  journal={arXiv preprint arXiv:2202.12040},
  year={2022}
}

@article{mhsdataset,
  author    = {Chris J. Kennedy and
               Geoff Bacon and
               Alexander Sahn and
               Claudia von Vacano},
  title     = {Constructing interval variables via faceted Rasch measurement and
               multitask deep learning: a hate speech application},
  journal   = {CoRR},
  volume    = {abs/2009.10277},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.10277},
  eprinttype = {arXiv},
  eprint    = {2009.10277},
  timestamp = {Thu, 14 Oct 2021 09:15:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-10277.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{oliddataset,
    title = "Predicting the Type and Target of Offensive Posts in Social Media",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Farra, Noura  and
      Kumar, Ritesh",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1144",
    doi = "10.18653/v1/N19-1144",
    pages = "1415--1420",
    abstract = "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.",
}

@INPROCEEDINGS {imagenet,
author = {J. Deng and W. Dong and R. Socher and L. Li and Kai Li and Li Fei-Fei},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)},
title = {ImageNet: A large-scale hierarchical image database},
year = {2009},
volume = {},
issn = {1063-6919},
pages = {248-255},
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called &amp;#x201C;ImageNet&amp;#x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
keywords = {large-scale systems;image databases;explosions;internet;robustness;information retrieval;image retrieval;multimedia databases;ontologies;spine},
doi = {10.1109/CVPR.2009.5206848},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2009.5206848},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@ARTICLE{recognizepatternswithoutteacher,  author={Fralick, S.},  journal={IEEE Transactions on Information Theory},   title={Learning to recognize patterns without a teacher},   year={1967},  volume={13},  number={1},  pages={57-64},  doi={10.1109/TIT.1967.1053952}}

@inproceedings{cotraining,
author = {Blum, Avrim and Mitchell, Tom},
title = {Combining Labeled and Unlabeled Data with Co-Training},
year = {1998},
isbn = {1581130570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/279943.279962},
doi = {10.1145/279943.279962},
booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
pages = {92–100},
numpages = {9},
location = {Madison, Wisconsin, USA},
series = {COLT' 98}
}

@inproceedings{selftrainingweaksupervision,
author = {Karamanolakis, Giannis and Mukherjee, Subhabrata (Subho) and Zheng, Guoqing and Awadallah, Ahmed H.},
title = {Self-training with Weak Supervision},
booktitle = {NAACL 2021},
year = {2021},
month = {May},
abstract = {State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domainspecific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.},
publisher = {NAACL 2021},
url = {https://www.microsoft.com/en-us/research/publication/self-training-weak-supervision-astra/},
}


@INPROCEEDINGS {crosspseudosupervision,
author = {X. Chen and Y. Yuan and G. Zeng and J. Wang},
booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision},
year = {2021},
volume = {},
issn = {},
pages = {2613-2622},
keywords = {training;image segmentation;computer vision;semantics;training data;pattern recognition;standards},
doi = {10.1109/CVPR46437.2021.00264},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00264},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@ARTICLE{selftrainingseq,
       author = {{He}, Junxian and {Gu}, Jiatao and {Shen}, Jiajun and {Ranzato}, Marc'Aurelio},
        title = "{Revisiting Self-Training for Neural Sequence Generation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
         year = 2019,
        month = sep,
          eid = {arXiv:1909.13788},
        pages = {arXiv:1909.13788},
archivePrefix = {arXiv},
       eprint = {1909.13788},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190913788H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS {randaugment,
author = {E. D. Cubuk and B. Zoph and J. Shlens and Q. V. Le},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
title = {Randaugment: Practical automated data augmentation with a reduced search space},
year = {2020},
volume = {},
issn = {},
pages = {3008-3017},
abstract = {Recent work on automated augmentation strategies has led to state-of-the-art results in image classification and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated augmentation strategies. We find that while previous work required a search for both magnitude and probability of each operation independently, it is sufficient to only search for a single distortion magnitude that jointly controls all operations. We hence propose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task. Despite the simplifications, our method achieves equal or better performance over previous automated augmentation strategies on on CIFAR-10/100, SVHN, ImageNet and COCO datasets. EfficientNet-B7, we achieve 85.0% accuracy, a 1.0% increase over baseline augmentation, a 0.6% improvement over AutoAugment on the ImageNet dataset. With EfficientNet-B8, we achieve 85.4% accuracy on ImageNet, which matches a previous result that used 3.5B extra images. On object detection, the same method as classification leads to 1.0-1.3% improvement over baseline augmentation. Code will be made available online.},
keywords = {task analysis;distortion;data models;training;noise measurement;market research;computational modeling},
doi = {10.1109/CVPRW50498.2020.00359},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPRW50498.2020.00359},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}


@inproceedings{xie2020unsupervised,
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
title = {Unsupervised Data Augmentation for Consistency Training},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAug-ment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {525},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{backtranslation,
    title = "Understanding Back-Translation at Scale",
    author = "Edunov, Sergey  and
      Ott, Myle  and
      Auli, Michael  and
      Grangier, David",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1045",
    doi = "10.18653/v1/D18-1045",
    pages = "489--500",
    abstract = "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT{'}14 English-German test set.",
}

@inproceedings{pham-hong-chokshi-2020-pgsg,
    title = "{PGSG} at {S}em{E}val-2020 Task 12: {BERT}-{LSTM} with Tweets{'} Pretrained Model and Noisy Student Training Method",
    author = "Pham-Hong, Bao-Tran  and
      Chokshi, Setu",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.280",
    doi = "10.18653/v1/2020.semeval-1.280",
    pages = "2111--2116",
    abstract = "The paper presents a system developed for the SemEval-2020 competition Task 12 (OffensEval-2): Multilingual Offensive Language Identification in Social Media. We achieve the second place (2nd) in sub-task B: Automatic categorization of offense types and are ranked 55th with a macro F1-score of 90.59 in sub-task A: Offensive language identification. Our solution is using a stack of BERT and LSTM layers, training with the Noisy Student method. Since the tweets data contains a large number of noisy words and slang, we update the vocabulary of the BERT large model pre-trained by the Google AI Language team. We fine-tune the model with tweet sentences provided in the challenge.",
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{noisystudent,
  title={Self-training with noisy student improves imagenet classification},
  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10687--10698},
  month = {June},
  year={2020}
}

@article{embeddingnoise,
  title={Adversarial Training Methods for Semi-Supervised Text Classification},
  author={Takeru Miyato and Andrew M. Dai and Ian J. Goodfellow},
  journal={arXiv: Machine Learning},
  year={2017}
}

@inproceedings{temporal_ensembling,
  author    = {Samuli Laine and
               Timo Aila},
  title     = {Temporal Ensembling for Semi-Supervised Learning},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=BJ6oOfqge},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LaineA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{miyato2018virtual,
  title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1979--1993},
  year={2018},
  publisher={IEEE}
}

@inproceedings{10.5555/2969442.2969635,
author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
title = {Semi-Supervised Learning with Ladder Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3546–3554},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}


@misc{https://doi.org/10.48550/arxiv.1909.13788,
  doi = {10.48550/ARXIV.1909.13788},
  
  url = {https://arxiv.org/abs/1909.13788},
  
  author = {He, Junxian and Gu, Jiatao and Shen, Jiajun and Ranzato, Marc'Aurelio},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Revisiting Self-Training for Neural Sequence Generation},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{10.5555/295240.295806,
author = {Nigam, Kamal and McCallum, Andrew and Thrun, Sebastian and Mitchell, Tom},
title = {Learning to Classify Text from Labeled and Unlabeled Documents},
year = {1998},
isbn = {0262510987},
publisher = {American Association for Artificial Intelligence},
address = {USA},
abstract = {In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%.},
booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {792–799},
numpages = {8},
location = {Madison, Wisconsin, USA},
series = {AAAI '98/IAAI '98}
}

@misc{https://doi.org/10.48550/arxiv.1904.07305,
  doi = {10.48550/ARXIV.1904.07305},
  
  url = {https://arxiv.org/abs/1904.07305},
  
  author = {RoyChowdhury, Aruni and Chakrabarty, Prithvijit and Singh, Ashish and Jin, SouYoung and Jiang, Huaizu and Cao, Liangliang and Learned-Miller, Erik},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Automatic adaptation of object detectors to new domains using self-training},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{dopido2013semisupervised,
  title={Semisupervised self-learning for hyperspectral image classification},
  author={D{\'o}pido, Inmaculada and Li, Jun and Marpu, Prashanth Reddy and Plaza, Antonio and Dias, Jos{\'e} M Bioucas and Benediktsson, Jon Atli},
  journal={IEEE transactions on geoscience and remote sensing},
  volume={51},
  number={7},
  pages={4032--4044},
  year={2013},
  publisher={IEEE}
}

@INPROCEEDINGS{4129456,  author={Rosenberg, Chuck and Hebert, Martial and Schneiderman, Henry},  booktitle={2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1},   title={Semi-Supervised Self-Training of Object Detection Models},   year={2005},  volume={1},  number={},  pages={29-36},  doi={10.1109/ACVMOT.2005.107}}

@inproceedings{10.3115/981658.981684,
author = {Yarowsky, David},
title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
year = {1995},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/981658.981684},
doi = {10.3115/981658.981684},
abstract = {This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.},
booktitle = {Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics},
pages = {189–196},
numpages = {8},
location = {Cambridge, Massachusetts},
series = {ACL '95}
}

@ARTICLE{4787647,  author={Chapelle, O. and Scholkopf, B. and Zien, Eds., A.},  journal={IEEE Transactions on Neural Networks},   title={Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]},   year={2009},  volume={20},  number={3},  pages={542-542},  doi={10.1109/TNN.2009.2015974}}

@inproceedings{zampieri-etal-2020-semeval,
    title = "{S}em{E}val-2020 Task 12: Multilingual Offensive Language Identification in Social Media ({O}ffens{E}val 2020)",
    author = {Zampieri, Marcos  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Atanasova, Pepa  and
      Karadzhov, Georgi  and
      Mubarak, Hamdy  and
      Derczynski, Leon  and
      Pitenis, Zeses  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.188",
    doi = "10.18653/v1/2020.semeval-1.188",
    pages = "1425--1447",
    abstract = "We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.",
}

@inproceedings{zampieri-etal-2019-semeval,
    title = "{S}em{E}val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media ({O}ffens{E}val)",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Farra, Noura  and
      Kumar, Ritesh",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-2010",
    doi = "10.18653/v1/S19-2010",
    pages = "75--86",
    abstract = "We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.",
}

@inproceedings{10.1145/3041021.3054223,
author = {Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
title = {Deep Learning for Hate Speech Detection in Tweets},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054223},
doi = {10.1145/3041021.3054223},
abstract = {Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {759–760},
numpages = {2},
keywords = {hate speech detection, lstm, cnn, deep learning applications, twitter},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{davidson2017automated, title={Automated Hate Speech Detection and the Problem of Offensive Language}, volume={11}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14955}, abstractNote={ &lt;p&gt; A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, year={2017}, month={May}, pages={512-515} }

@inproceedings{schmidt-wiegand-2017-survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1101",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
}

@inproceedings{vanhee,
author = {Van Hee, Cynthia and Lefever, Els and Verhoeven, Ben and Mennes, Julie and Desmet, Bart and Pauw, Guy and Daelemans, Walter and Hoste, Véronique},
year = {2015},
month = {09},
pages = {672–680},
booktitle={Proceedings of Recent Advances in Natural Language Processing},
title = {Detection and fine-grained classification of cyberbullying events}
}

@article{njagi,
author = {Njagi, Dennis and Zuping, Z. and Hanyurwimfura, Damien and Long, Jun},
year = {2015},
month = {04},
pages = {215-230},
title = {A Lexicon-based Approach for Hate Speech Detection},
volume = {10},
journal = {International Journal of Multimedia and Ubiquitous Engineering},
doi = {10.14257/ijmue.2015.10.4.21}
}

@inproceedings{10.1145/2872427.2883062,
author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
title = {Abusive Language Detection in Online User Content},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883062},
doi = {10.1145/2872427.2883062},
abstract = {Detection of abusive language in user generated online content has become an issue of increasing importance in recent years. Most current commercial methods make use of blacklists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted examples of hate speech. In this work, we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. We also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, we use our detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {145–153},
numpages = {9},
keywords = {hate speech, natural language processing, abusive language, nlp, discourse classification, stylistic classification},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{waseemhovyhate,
    title = "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak  and
      Hovy, Dirk",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-2013",
    doi = "10.18653/v1/N16-2013",
    pages = "88--93",
}

@article{10.1002/asi.21690,
author = {Sood, Sara Owsley and Churchill, Elizabeth F. and Antin, Judd},
title = {Automatic Identification of Personal Insults on Social News Sites},
year = {2012},
issue_date = {February 2012},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {63},
number = {2},
issn = {1532-2882},
url = {https://doi.org/10.1002/asi.21690},
doi = {10.1002/asi.21690},
abstract = {As online communities grow and the volume of user-generated content increases, the need for community management also rises. Community management has three main purposes: to create a positive experience for existing participants, to promote appropriate, socionormative behaviors, and to encourage potential participants to make contributions. Research indicates that the quality of content a potential participant sees on a site is highly influential; off-topic, negative comments with malicious intent are a particularly strong boundary to participation or set the tone for encouraging similar contributions. A problem for community managers, therefore, is the detection and elimination of such undesirable content. As a community grows, this undertaking becomes more daunting. Can an automated system aid community managers in this task? In this paper, we address this question through a machine learning approach to automatic detection of inappropriate negative user contributions. Our training corpus is a set of comments from a news commenting site that we tasked Amazon Mechanical Turk workers with labeling. Each comment is labeled for the presence of profanity, insults, and the object of the insults. Support vector machines trained on these data are combined with relevance and valence analysis systems in a multistep approach to the detection of inappropriate negative user contributions. The system shows great potential for semiautomated community management. © 2012 Wiley Periodicals, Inc.},
journal = {J. Am. Soc. Inf. Sci. Technol.},
month = {feb},
pages = {270–285},
numpages = {16},
keywords = {computer mediated communications, machine learning, user generated content, automatic classification, sentiment analysis}
}

@inproceedings{10.5555/2382029.2382139,
author = {Xu, Jun-Ming and Jun, Kwang-Sung and Zhu, Xiaojin and Bellmore, Amy},
title = {Learning from Bullying Traces in Social Media},
year = {2012},
isbn = {9781937284206},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We introduce the social study of bullying to the NLP community. Bullying, in both physical and cyber worlds (the latter known as cyberbullying), has been recognized as a serious national health issue among adolescents. However, previous social studies of bullying are handicapped by data scarcity, while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes. Our main contribution is to present evidence that social media, with appropriate natural language processing techniques, can be a valuable and abundant data source for the study of bullying in both worlds. We identify several key problems in using such data sources and formulate them as NLP tasks, including text classification, role labeling, sentiment analysis, and topic modeling. Since this is an introductory paper, we present baseline results on these tasks using off-the-shelf NLP solutions, and encourage the NLP community to contribute better models in the future.},
booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {656–666},
numpages = {11},
location = {Montreal, Canada},
series = {NAACL HLT '12}
}

@INPROCEEDINGS {6406271,
author = {Y. Chen and Y. Zhou and S. Zhu and H. Xu},
booktitle = {2012 International Conference on Privacy, Security, Risk and Trust (PASSAT)},
title = {Detecting Offensive Language in Social Media to Protect Adolescent Online Safety},
year = {2012},
volume = {},
issn = {},
pages = {71-80},
abstract = {Since the textual contents on online social media are highly unstructured, informal, and often misspelled, existing research on message-level offensive language detection cannot accurately detect offensive content. Meanwhile, user-level offensiveness detection seems a more feasible approach but it is an under researched area. To bridge this gap, we propose the Lexical Syntactic Feature (LSF) architecture to detect offensive content and identify potential offensive users in social media. We distinguish the contribution of pejoratives/profanities and obscenities in determining offensive content, and introduce hand-authoring syntactic rules in identifying name-calling harassments. In particular, we incorporate a user&#x27;s writing style, structure and specific cyber bullying content as features to predict the user&#x27;s potentiality to send out offensive content. Results from experiments showed that our LSF framework performed significantly better than existing methods in offensive content detection. It achieves precision of 98.24% and recall of 94.34% in sentence offensive detection, as well as precision of 77.9% and recall of 77.8% in user offensive detection. Meanwhile, the processing speed of LSF is approximately 10msec per sentence, suggesting the potential for effective deployment in social media.},
keywords = {feature extraction;media;syntactics;context;text mining;educational institutions;history},
doi = {10.1109/SocialCom-PASSAT.2012.55},
url = {https://doi.ieeecomputersociety.org/10.1109/SocialCom-PASSAT.2012.55},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep}
}

@InProceedings{Yim_2017_CVPR,
author = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
title = {A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@InProceedings{pmlr-v80-furlanello18a,
  title = 	 {Born Again Neural Networks},
  author =       {Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1607--1616},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/furlanello18a.html},
  abstract = 	 {Knowledge Distillation (KD) consists of transferring “knowledge” from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student’s compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and non-predicted classes.}
}


@inproceedings{liu2019knowledge,
  title={Knowledge distillation via instance relationship graph},
  author={Liu, Yufan and Cao, Jiajiong and Li, Bing and Yuan, Chunfeng and Hu, Weiming and Li, Yangxi and Duan, Yunqiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7096--7104},
  year={2019}
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3967--3976},
  year={2019}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@article{zagoruyko2016paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1612.03928},
  year={2016}
}

@inproceedings{heo2019knowledge,
  title={Knowledge transfer via distillation of activation boundaries formed by hidden neurons},
  author={Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3779--3787},
  year={2019}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{10.1145/1150402.1150464,
    author = {Buciluundefined, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
    title = {Model Compression},
    year = {2006},
    isbn = {1595933395},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1150402.1150464},
    doi = {10.1145/1150402.1150464},
    abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
    booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {535–541},
    numpages = {7},
    keywords = {model compression, supervised learning},
    location = {Philadelphia, PA, USA},
    series = {KDD '06}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{BEDDIAR2021100153,
title = {Data expansion using back translation and paraphrasing for hate speech detection},
journal = {Online Social Networks and Media},
volume = {24},
pages = {100153},
year = {2021},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2021.100153},
url = {https://www.sciencedirect.com/science/article/pii/S2468696421000355},
author = {Djamila Romaissa Beddiar and Md Saroar Jahan and Mourad Oussalah},
keywords = {Cyberbullying detection, Hate speech, Back translation, Paraphrasing, NLP transformers, Encoder–decoder},
abstract = {With proliferation of user generated contents in social media platforms, establishing mechanisms to automatically identify toxic and abusive content becomes a prime concern for regulators, researchers, and society. Keeping the balance between freedom of speech and respecting each other dignity is a major concern of social media platform regulators. Although, automatic detection of offensive content using deep learning approaches seems to provide encouraging results, training deep learning-based models requires large amounts of high-quality labeled data, which is often missing. In this regard, we present in this paper a new deep learning-based method that fuses a Back Translation method, and a Paraphrasing technique for data augmentation. Our pipeline investigates different word-embedding-based architectures for classification of hate speech. The back translation technique relies on an encoder–decoder architecture pre-trained on a large corpus and mostly used for machine translation. In addition, paraphrasing exploits the transformer model and the mixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are compared to seek enhanced classification results. We evaluate our proposal on five publicly available datasets; namely, AskFm corpus, Formspring dataset, Warner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The performance of the proposal together with comparison to some related state-of-art results demonstrate the effectiveness and soundness of our proposal.}
}

@inproceedings{10.1145/3078714.3078723,
author = {Mondal, Mainack and Silva, Leandro Ara\'{u}jo and Benevenuto, Fabr\'{\i}cio},
title = {A Measurement Study of Hate Speech in Social Media},
year = {2017},
isbn = {9781450347082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078714.3078723},
doi = {10.1145/3078714.3078723},
abstract = {Social media platforms provide an inexpensive communication medium that allows anyone to quickly reach millions of users. Consequently, in these platforms anyone can publish content and anyone interested in the content can obtain it, representing a transformative revolution in our society. However, this same potential of social media systems brings together an important challenge---these systems provide space for discourses that are harmful to certain groups of people. This challenge manifests itself with a number of variations, including bullying, offensive content, and hate speech. Specifically, authorities of many countries today are rapidly recognizing hate speech as a serious problem, specially because it is hard to create barriers on the Internet to prevent the dissemination of hate across countries or minorities. In this paper, we provide the first of a kind systematic large scale measurement and analysis study of hate speech in online social media. We aim to understand the abundance of hate speech in online social media, the most common hate expressions, the effect of anonymity on hate speech and the most hated groups across regions. In order to achieve our objectives, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both of these systems. Our results identify hate speech forms and unveil a set of important patterns, providing not only a broader understanding of online hate speech, but also offering directions for detection and prevention approaches.},
booktitle = {Proceedings of the 28th ACM Conference on Hypertext and Social Media},
pages = {85–94},
numpages = {10},
keywords = {twitter, whisper, pattern recognition, anonymity, social media, hate speech},
location = {Prague, Czech Republic},
series = {HT '17}
}

@inproceedings{10.1145/3357384.3358040,
author = {Rizos, Georgios and Hemker, Konstantin and Schuller, Bj\"{o}rn},
title = {Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate-Speech Classification},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358040},
doi = {10.1145/3357384.3358040},
abstract = {In this paper, we address the issue of augmenting text data in supervised Natural Language Processing problems, exemplified by deep online hate speech classification. A great challenge in this domain is that although the presence of hate speech can be deleterious to the quality of service provided by social platforms, it still comprises only a tiny fraction of the content that can be found online, which can lead to performance deterioration due to majority class overfitting. To this end, we perform a thorough study on the application of deep learning to the hate speech detection problem: a) we propose three text-based data augmentation techniques aimed at reducing the degree of class imbalance and to maximise the amount of information we can extract from our limited resources and b) we apply them on a selection of top-performing deep architectures and hate speech databases in order to showcase their generalisation properties. The data augmentation techniques are based on a) synonym replacement based on word embedding vector closeness, b) warping of the word tokens along the padded sequence or c) class-conditional, recurrent neural language generation. Our proposed framework yields a significant increase in multi-class hate speech detection, outperforming the baseline in the largest online hate speech database by an absolute 5.7\% increase in Macro-F1 score and 30\% in hate speech class recall.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {991–1000},
numpages = {10},
keywords = {class imbalance, online hate speech detection, short text data augmentation},
location = {Beijing, China},
series = {CIKM '19}
}

@article{mosolova2018text,
  title={Text Augmentation for Neural Networks.},
  author={Mosolova, Anna and Fomin, Vadim and Bondarenko, Ivan},
  journal={AIST (Supplement)},
  volume={2268},
  pages={104--109},
  year={2018}
}

@article{doi:10.1080/08839514.2021.1988443,
author = {Safa Alsafari and Samira Sadaoui},
title = {Semi-Supervised Self-Training of Hate and Offensive Speech from Social Media},
journal = {Applied Artificial Intelligence},
volume = {35},
number = {15},
pages = {1621-1645},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/08839514.2021.1988443},
URL = { 
        https://doi.org/10.1080/08839514.2021.1988443
},
eprint = { 
        https://doi.org/10.1080/08839514.2021.1988443
    
}
}

@inproceedings{li-2020-lee,
  author    = {Junyi Li and
               Tianzi Zhao},
  editor    = {Parth Mehta and
               Thomas Mandl and
               Prasenjit Majumder and
               Mandar Mitra},
  title     = {Lee@HASOC2020: ALBERT-based Max Ensemble with Self-training for Identifying
               Hate Speech and Offensive Content in Indo-European Languages},
  booktitle = {Working Notes of {FIRE} 2020 - Forum for Information Retrieval Evaluation,
               Hyderabad, India, December 16-20, 2020},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {2826},
  pages     = {210--216},
  publisher = {CEUR-WS.org},
  year      = {2020},
  url       = {http://ceur-ws.org/Vol-2826/T2-15.pdf},
  timestamp = {Mon, 22 Mar 2021 17:21:07 +0100},
  biburl    = {https://dblp.org/rec/conf/fire/LiZ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jaiswal2020survey,
  title={A survey on contrastive self-supervised learning},
  author={Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
  journal={Technologies},
  volume={9},
  number={1},
  pages={2},
  year={2020},
  publisher={MDPI}
}

@article{LIU2022100616,
title = {Audio self-supervised learning: A survey},
journal = {Patterns},
volume = {3},
number = {12},
pages = {100616},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100616},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002410},
author = {Shuo Liu and Adria Mallol-Ragolta and Emilia Parada-Cabaleiro and Kun Qian and Xin Jing and Alexander Kathan and Bin Hu and Björn W. Schuller},
keywords = {self-supervised learning, audio and speech processing, multi-modal SSL, representation learning, unsupervised learning},
abstract = {Summary
Similar to humans’ cognitive ability to generalize knowledge and skills, self-supervised learning (SSL) targets discovering general representations from large-scale data. This, through the use of pre-trained SSL models for downstream tasks, alleviates the need for human annotation, which is an expensive and time-consuming task. Its success in the fields of computer vision and natural language processing have prompted its recent adoption into the field of audio and speech processing. Comprehensive reviews summarizing the knowledge in audio SSL are currently missing. To fill this gap, we provide an overview of the SSL methods used for audio and speech processing applications. Herein, we also summarize the empirical works that exploit audio modality in multi-modal SSL frameworks and the existing suitable benchmarks to evaluate the power of SSL in the computer audition domain. Finally, we discuss some open problems and point out the future directions in the development of audio SSL.}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}

@article{10.1145/3577925,
author = {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah, Mubarak},
title = {Self-Supervised Learning for Videos: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3577925},
doi = {10.1145/3577925},
abstract = {The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning which does not require annotations and has shown promise in both image and video domains. Different from the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domain. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: 1) pretext tasks, 2) generative learning, 3) contrastive learning, and 4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {dec},
keywords = {visual-language models, multimodal learning, video understanding, representation learning, deep learning, self-supervised learning, zero-shot learning}
}

@inproceedings{richardson2022semi,
  title={Semi-Supervised Machine Learning for Analyzing COVID-19 Related Twitter Data for Asian Hate Speech},
  author={Richardson, Caitlin and Shah, Sandeep and Yuan, Xiaohong},
  booktitle={2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={1643--1648},
  year={2022},
  organization={IEEE}
}

@InProceedings{santos_et_al:OASIcs.SLATE.2022.11,
  author =	{Santos, Raquel Bento and Matos, Bernardo Cunha and Carvalho, Paula and Batista, Fernando and Ribeiro, Ricardo},
  title =	{{Semi-Supervised Annotation of Portuguese Hate Speech Across Social Media Domains}},
  booktitle =	{11th Symposium on Languages, Applications and Technologies (SLATE 2022)},
  pages =	{11:1--11:14},
  series =	{Open Access Series in Informatics (OASIcs)},
  ISBN =	{978-3-95977-245-7},
  ISSN =	{2190-6807},
  year =	{2022},
  volume =	{104},
  editor =	{Cordeiro, Jo\~{a}o and Pereira, Maria Jo\~{a}o and Rodrigues, Nuno F. and Pais, Sebasti\~{a}o},
  publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{https://drops.dagstuhl.de/opus/volltexte/2022/16757},
  URN =		{urn:nbn:de:0030-drops-167570},
  doi =		{10.4230/OASIcs.SLATE.2022.11},
  annote =	{Keywords: Hate Speech, Semi-Supervised Learning, Semi-Automatic Annotation}
}

@article{fortunahatespeechsurvey,
author = {Fortuna, Paula and Nunes, S\'{e}rgio},
title = {A Survey on Automatic Detection of Hate Speech in Text},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3232676},
doi = {10.1145/3232676},
abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {85},
numpages = {30},
keywords = {opinion mining, literature review, text mining, Hate speech, natural language processing}
}

@inproceedings{waseem-2016-racist,
    title = "Are You a Racist or Am {I} Seeing Things? Annotator Influence on Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak",
    booktitle = "Proceedings of the First Workshop on {NLP} and Computational Social Science",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-5618",
    doi = "10.18653/v1/W16-5618",
    pages = "138--142",
}

@inproceedings{leonardelli2020dh,
  title={DH-FBK@ HaSpeeDe2: Italian Hate Speech Detection via Self-Training and Oversampling.},
  author={Leonardelli, Elisa and Menini, Stefano and Tonelli, Sara},
  booktitle={Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020)},
  volume={2765},
  year={2020}
}

@InProceedings{waseem-hovy:2016:N16-2,
  author    = {Waseem, Zeerak  and  Hovy, Dirk},
  title     = {Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter},
  booktitle = {Proceedings of the NAACL Student Research Workshop},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {88--93},
  url       = {http://www.aclweb.org/anthology/N16-2013}
}

@inproceedings{ng-etal-2019-facebook,
    title = "{F}acebook {FAIR}{'}s {WMT}19 News Translation Task Submission",
    author = "Ng, Nathan  and
      Yee, Kyra  and
      Baevski, Alexei  and
      Ott, Myle  and
      Auli, Michael  and
      Edunov, Sergey",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5333",
    doi = "10.18653/v1/W19-5333",
    pages = "314--319",
    abstract = "This paper describes Facebook FAIR{'}s submission to the WMT19 shared news translation task. We participate in four language directions, English {\textless}-{\textgreater} German and English {\textless}-{\textgreater} Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system{'}s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian.",
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}