\subsection{Related works}
\paragraph{Selective Sampling.}
Numerous studies have been conducted on selective sampling across various settings \citep{cesa2005minimizing,dekel2012selective,agarwal2013selective,hanneke2015minimax,hanneke2021toward,zhu2022efficient}, with the work of \citet{sekhari2023selective} being closest to ours. \citet{sekhari2023selective} presented a suite of provably efficient algorithms that are applicable to settings including contextual bandits and imitation learning. The primary distinction between our setting and the prior works lies in the feedback modality--we assume preference-based feedback, whereas they assume direct label feedback or reward signals. 

\paragraph{Contextual bandits with preference feedback.}
\citet{dudik2015contextual} is the first to consider contextual dueling bandits, and one of their algorithms achieves the optimal regret rate. \citet{saha2022efficient} studied contextual dueling bandits using a value function class and proposed an algorithm based on a reduction to online regression, which also achieves an optimal worst-case regret bound. 
In this paper, we mainly follow the setting of the latter and make notable improvements in two aspects: (1) in addition to the $O(\sqrt{AT})$ optimal regret rate where $A$ is the number of actions and $T$ is the number of interaction rounds, we established an instance-dependent regret upper bound that can be significantly smaller when the bandit exhibits a favorable structure; (2) our algorithm has an instance-dependent upper bound on the number of queries, and thus when the underlying instance is well behaved (has small eluder dimension and large gap), we will make significantly fewer queries.

Another related work is \citet{saha2022versatile} which achieves the best-of-both-worlds regret for non-contextual dueling bandits. We note that our setting is more general due to the existence of context and general function approximation, enabling us to leverage function class beyond linear and tabular cases. 


\paragraph{RL with preference feedback.} RL with preference feedback has been widely employed in recent advancements in AI \citep{ouyang2022training,openai2023gpt}. According to \citet{wirth2017survey}, there are generally three types of preference feedback: action preferences \citep{furnkranz2012preference}, state preferences \citep{wirth2014learning}, and trajectory preferences \citep{busa2014preference,novoseller2020dueling,xu2020preference,lee2021b,chen2022human,saha2023dueling,pacchiano2021dueling,biyik2018batch,taranovic2022adversarial,sadigh2017active}. We focus on the action preference modality with the goal of achieving tight regret bounds and query complexities.
 
The concurrent work from \citet{zhan2023query} investigates the experimental design in both the trajectories-based and action-based preference settings, for which they decouple the process of collecting trajectories from querying for human feedback. Their action-based setting is the same as ours, but they mainly focus on linear parameterization, while our approach is a reduction to online regression and can leverage general function approximation beyond linear function classes. 




\paragraph{Imitation learning.} In imitation learning, two common feedback modalities are typically considered: demonstrations that contain experts' actions, and preferences. The former involves directly acquiring expert actions (e.g., \citet{ross2011reduction,ross2014reinforcement,sun2017deeply, chang2015learning, sekhari2023selective}), while the latter focuses on obtaining preferences between selected options \citep{chu2005preference,lee2021pebble,zhu2023principled}.  \citet{brown2019extrapolating,brown2020better} leveraged both demonstrations and preference-based information and empirically showed that their algorithm can learn to outperform experts. Our imitation learning setting belongs to the second category, and we established bounds on the regret and the query complexity for our algorithm. We show that our algorithm can learn a policy that can provably outperform the expert (when it is suboptimal for the underlying environment). 

