\section{Preliminaries}



In this section, we introduce the setup for contextual bandits and imitation learning with preference-based feedback. 
We denote $[N]$ as the set of integers \(\crl{1, \dots, N}\). The set of all distributions over a set $\+S$ is denoted by $\Delta(\+S)$.

\subsection{Contextual Bandits with Preference-Based Feedback}
In this section, we introduce the contextual dueling bandits setting. We assume a context set $\+X$ and an action space $\+A=[A]$. At each round $t\in[T]$, a context $x_t$ is drawn \textit{adversarially}, and the learner's task is to decide whether to make a query \asedit{to the expert}. If the learner makes a query, it needs to select a pair of actions $(a_t,b_t)\in\+A\times\+A$, upon which a noisy feedback $y_t\in\{-1,1\}$ is revealed to the learner regarding whether $a_t$ or $b_t$ is better. Specifically, we assume \asedit{that the expert} relies on a preference function $f^\star:\+X\times\+A\times\+A\rightarrow[-1,1]$ \asedit{based on which, it samples its feedback $y_t$ as} 
\begin{align*}
\Pr(a_t\text{ is preferred to }b_t\given x_t)
\coloneqq 
\Pr( y_t = 1 \given x_t, a_t,b_t )
=
\phi\big(f^\star(x_t,a_t,b_t)\big)
\end{align*}
where $\phi(d):[-1,1]\rightarrow[0,1]$ is the link function, which satisfies $\phi(d)+\phi(-d)=1$ for any $d$. If the learner does not make a query, it should still select a pair of actions $(a_t,b_t)\in\+A\times\+A$ but will not receive any feedback.
Let $Z_t\in\{0,1\}$ indicate whether the learner makes a query at round $t$.

We assume that the learner has access to a function class $\+F\subseteq\+X\times\+A\times\+A\rightarrow[-1,1]$ that realizes \(f^\star\). Furthermore, we assume that $f^\star$, as well as the functions in $\+F$, is transitive and anti-symmetric.
\begin{assumption}\label{asm:properties}
	We assume $f^\star\in\+F$ and any functions $f\in\+F$ satisfies the following two properties:
		(1) transitivity: for any $x\in\+X$ and $a,b,c\in\+A$, if $f(x,a,b)>0$ and $f(x,b,c)>0$, then we must have $f(x,a,c)>0$;
		(2) anti-symmetry: $f(x,a,b)=-f(x,b,a)$ for any $x\in\+X$ and any $a,b\in\+A$.
\end{assumption}  
We provide an example below for which \Cref{asm:properties} is satisfied.
\begin{example}\label{lem:ex-r-r}
Assume there exists a function $r^\star:\+X\times\+A\rightarrow[0,1]$ such that $f^\star(x,a,b)=r^\star(x,a)-r^\star(x,b)$ for any $x\in\+X$ and $a,b\in\+A$. Typically, such a function $r^\star$ represents the ``reward function'' of the contextual bandit. In such a scenario, we can first parameterize a reward class $\+R\subseteq\+X\times\+A\rightarrow[0,1]$ and define $\+F=\{f:f(x,a,b)=r(x,a)-r(x,b), r\in\+R\}$. Moreover, it is common to have $\phi(d)\coloneqq 1/(1+\exp(-d))$ in this setting, which recovers the Bradley-Terry-Luce (BTL) model \citep{bradley1952rank}---a commonly used model in practice for learning reward models \citep{christiano2017deep}.
\end{example}
\Cref{asm:properties} ensures the existence of an optimal arm, as stated below.
\begin{lemma}\label{lem:optimal-action-exist}
	 Under \Cref{asm:properties}, for any function $f\in\+F$ and any context $x\in\+X$, there exists an arm $a\in\+A$ such that $f(x,a,b)\geq0$ for any arm $b\in\+A$. We denote this best arm by $\pi_f(x):=a$.\footnote{When the best arms is not unique, the ties are broken arbitrarily but consistently.}

\end{lemma}
The learner's goal is to minimize the regret while minimizing the number of queries, which are defined as:
\begin{align*}
		\@{Regret}_T^{\@{CB}}\coloneqq\sum_{t=1}^{T}\big(
		f^\star(x_t,\pi_{f^\star}(x_t),a_t)+
				f^\star(x_t,\pi_{f^\star}(x_t),b_t)\big)
            ,\quad
		\@{Queries}_T^{\@{CB}}\coloneqq\sum_{t=1}^T Z_t.
\end{align*}

It is worth noting that when $f^\star$ is the difference in rewards (as in \Cref{lem:ex-r-r}), the regret defined above reduces to the standard regret of a contextual bandit.
We also remark that our feedback generation generalizes that of \citet{saha2022efficient} in that we assume an additional link function $\phi$, while they assume the feedback is sampled from $\Pr(y=1\given x,a,b)=(P_t[a_t,b_t]+1)/2$, which is captured in our setting (see \Cref{ex:sq-loss}). However, \cite{saha2022efficient} do not assume transitivity. 




\subsection{Imitation Learning with Preference-Based Feedback}

In our imitation learning setup, we consider that the learner operates in a finite-horizon Markov decision process (MDP), which is a tuple $M(\+X,\+A,r,P,H)$ where $\+X$ is the state space, $\+A$ is the action space, $P$ is the transition kernel, $r:\+X\times\+A\rightarrow[0,1]$ is the reward function, and $H$ is the length of each episode. The interaction between the learner and the environment proceeds as follows: at each episode $t\in[T]$, the learner receives an initial state $x_{t,0}$ which could be chosen adversarially. Then, the learner interacts with the environment for $H$ steps. At each step $h$, the learner first decides whether to make a query. If making a query, the learner needs to select a pair of actions $(a_{t,h},b_{t,h})\in\+A\times\+A$, upon which a feedback $y_{t,h}\in\{-1,1\}$ is revealed to the learner regarding which action is preferred from the expert's perspective. Here the feedback is sampled according to
\begin{align*}
\Pr(a_{t,h}\text{ is preferred to }b_{t,h}\given x_{t,h}, h)
\coloneqq
\Pr( y_{t,h} = 1 \given x_{t,h}, a_{t,h}, b_{t,h}, h )
=
\phi\big(f_h^\star(x,a_{t,h},b_{t,h})\big).
\end{align*}
Irrespective of whether the learner made a query, it then picks a single action from $a_{t,h},b_{t,h}$ and transit to the next step (our algorithm will just pick an action uniformly at random from $a_{t,h},b_{t,h}$). After $H$ steps, the next episode starts. Let $Z_{t,h}\in\{0,1\}$ indicate whether the learner decided to query at step \(h\) in episode $t$. We assume that the function class $\+F$ is a product of $H$ classes, i.e., $\+F=\+F_0\times\cdots\+F_{H-1}$ where, for each $h$, we use $\+F_h=\{f:\+X\times\+A\times\+A\rightarrow[-1,1]\}$ to model $f^\star_h$ and assume that $\+F_h$ satisfies \Cref{asm:properties}. 

A policy is a mapping $\pi:\+X\rightarrow\Delta(\+A)$. For a policy $\pi$, the state value function for a state $x$ at step $h$ is defined as $V^\pi_h(x)\coloneqq\E[\sum_{i=h}^{H-1} r_i\given x_h=x]$ and the state-action value function for a state-action pair $(x,a)$ is $Q^\pi_h(x,a)\coloneqq\E[\sum_{i=h}^{H-1} r_i\given x_h=x,a_h=a]$, where the expectations are taken w.r.t.~the trajectories sampled by $\pi$ in the underlying MDP. 

\asedit{In the imitation learning setting, we assume that the expert (who gives the preference-based feedback) is equipped with a markovian policy \(\pi_e\), and that the preference of the expert is dependent on the reward-to-go under \(\pi_e\) (i.e. on a state \(x\), actions with higher values of \(Q^{\pi_e}(s, a)\) will be preferred by the expert). Formalizing this intuition, we assume that $f^\star_h$ is defined such that as $f_h^\star(x,a,b)\coloneqq Q_h^{\pi_e}(x,a)-Q_h^{\pi_e}(x,b)$.  The goal of the learner is still to minimize the regret and number of queries:} 

\begin{align*}
		\@{Regret}_T^{\@{IL}}
		\coloneqq\sum_{t=1}^T \big(V^{\pi_e}_0(x_{t,0})-V^{\pi_t}_0(x_{t,0})\big),\quad
		\@{Queries}_T^{\@{IL}}\coloneqq\sum_{t=1}^T\sum_{h=0}^{H-1} Z_{t,h}.
\end{align*}
Here $\pi_t$ is the strategy the learner uses to select actions at episode $t$.











\subsection{Link Function and Online Regression Oracle}
Following the standard practice in the literature \citep{agarwal2013selective}, we assume $\phi$ is the derivative of some $\alpha$-strongly convex function (see \Cref{def:strong-conv}) $\Phi:[-1,1]\rightarrow\=R$ and define the associated loss function as $\ell_\phi(d,y)=\Phi(d)-d(y+1)/2$. Additionally, in line with prior works in the literature \citep{foster2020instance,foster2020beyond, simchi2022bypassing,foster2018practical,sekhari2023selective}, our algorithm utilizes an online regression oracle, which is assumed to have a sublinear regret guarantee w.r.t.~\(\+F\) on arbitrary data sequences. 
\begin{assumption}\label{asm:oracle-regret}
We assume the learner has access to an online regression oracle pertaining to the loss $\ell_\phi$ such that for any sequence $\{(x_1,a_1,b_1,y_1),\dots,(x_T,a_T,b_T,y_T)\}$ where the label $y_t$ is generated by $y_t\sim\phi(f^\star(x_t,a_t,b_t))$, we have
$$
	\sum_{t=1}^T\ell_\phi\big(f_t(x_t,a_t,b_t),y_t\big)-\inf_{f\in\+F}\ell_\phi\big(f(x_t,a_t,b_t),y_t\big)\leq\Upsilon(\+F,T)
$$
for some $\Upsilon(\+F,T)$ that grows sublinearly with respect to $T$.\footnote{The online regression oracle updates as follows: in each iteration, after seeing $x_t,a_t,b_t$, it proposes a decision $f_t$, then $y_t$ is revealed and the online regression oracle incurs loss $\ell_\phi(f_t(x_t,a_t,b_t), y_t)$.} For notational simplicity, whenever clear from the context, we define  $\Upsilon\coloneqq\Upsilon(\+F,T)$.  
\end{assumption}
Here $\Upsilon$ represents the regret upper bound and is typically of logarithmic order in $T$ or the cardinality of the function class $\+F$ in many cases (here we drop the dependence on $T$ in notation for simplicity). We provide a few examples below: \looseness=-1
\begin{example}[Squared loss]\label{ex:sq-loss}
If we consider $\Phi(d)=d^2/4+d/2+1/4$, which is $1/4$-strongly convex, then we obtain $\phi(d)=(d+1)/2$ and $\ell_\phi(d,y)=(d-y)^2/4$, thereby recovering the squared loss, which has been widely studied in prior works. For example, \citet{rakhlin2014online} characterized the minimax rates for online square loss regression in terms of the offset sequential Rademacher complexity, resulting in favorable bounds for the regret. Specifically, we have $\Upsilon= O(\log|\+F|)$ assuming the function class $\+F$ is finite, and $\Upsilon= O(d\log(T))$ assuming $\+F$ is a $d$-dimensional linear class. We also kindly refer the readers to \citet{krishnamurthy2017active,foster2018practical} for efficient implementations.
\end{example}
\begin{example}[Logistic loss]\label{ex:log-loss}
When $\Phi(d)=\log(1+\exp(d))$ which is strongly convex at $[-1,1]$, we have $\phi(d)=1/(1+\exp(-d))$ and $\ell_\phi(d,y)=\log(1+\exp(-yd))$. Thus, we recover the logistic regression loss, which allows us to use online logistic regression and achieve $\Upsilon=O(\log|\+F|)$ assuming finite $\+F$. There have been numerous endeavors in minimizing the log loss, such as \citet{foster2018logistic} and \citet[Chapter 9]{cesa2006prediction}.
\end{example}
