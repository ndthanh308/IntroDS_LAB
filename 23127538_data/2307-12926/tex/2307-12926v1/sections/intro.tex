\section{Introduction}


Human feedback for training machine learning models has been widely used in many scenarios, including robotics \citep{ross2011reduction,ross2013learning,jain2015learning,laskey2016shiv,christiano2017deep} and natural language processing \citep{stiennon2020learning,ouyang2022training}. By integrating human feedback into the training process,  these prior works provide techniques to align machine-learning models with human intention and enable high-quality human-machine interaction (e.g., ChatGPT). 

Existing methods generally leverage two types of human feedback. The first is the action from human experts, which is the dominant feedback mode used in the literature of imitation learning or learning from demonstrations \citep{abbeel2004apprenticeship,ziebart2008maximum,daume2009search,ross2011reduction,ross2014reinforcement,sun2017deeply,osa2018algorithmic,li2023reinforcement}. 
The second type of feedback is preference-based feedback, which involves comparing pairs of actions. In this approach, the expert provides feedback by indicating their preference between two options selected by the learner. While both types of feedback have their applications, our focus in this work is on preference-based feedback, which is particularly suitable for scenarios where it is challenging for human experts to recommend the exact optimal action while making pairwise comparisons is much easier. 

Learning via preference-based feedback has been extensively studied, particularly in the field of \textit{dueling bandits} \citep{yue2011beat,yue2012k,zoghi2014relative,ailon2014reducing,komiyama2015regret,wu2016double,saha2021dueling,bengs2021preference,saha2022versatile} and \textit{contextual dueling bandits} \citep{dudik2015contextual,saha2022efficient,wu2023borda}.  Different from the standard bandit setting, the learner proposes two actions in dueling bandits and only gets noisy preference feedback from the human expert. Follow-up works extend the preference-based learning model from the one-step bandit setting to the multi-step decision-making (e.g., IL and RL) setting \citep{chu2005preference,sadigh2017active,christiano2017deep,lee2021pebble,chen2022human,saha2023dueling}. These studies mainly focus on how to learn a high-quality policy from human feedback, without concerning the question of active query in order to minimize the query complexity.  

However, query complexity is an important metric to optimize when learning from human feedback, as human feedback is expensive to collect \citep{lightman2023let}. For instance, InstructGPT \citep{ouyang2022training} is trained only on around 30K pieces of human feedback, which is significantly fewer than the internet-scale dataset used for pre-training the base model GPT3, indicating the challenge of scaling up the size of human feedback datasets. In other areas, such as robotics, learning from human feedback is also not easy, and prior studies (e.g., \citet{cohn2011comparing,zhang2022time,myers2023active}) have explored this issue from various perspectives. \citet{ross2013learning,laskey2016shiv} pointed out that querying human feedback in the learning loop is challenging, and extensively querying for feedback puts too much burden on the human experts. 

In this work, we design \emph{principled algorithms that can learn from preference-based feedback while at the same time minimizing query complexity}, 
under the settings of contextual bandits \citep{auer2002nonstochastic,langford2007epoch} and imitation learning \citep{ross2011reduction}. Our main contributions can be summarized as follows.


\begin{itemize}[leftmargin=*]
	\item In the contextual dueling bandits setting, the stochastic preference feedback is generated based on some preference matrix \citep{saha2022efficient}. We propose an algorithm (named \textsc{AURORA} -- in short of \textit{Active preference qUeRy fOR contextual bAndits}) that can achieve a best-of-both-worlds regret bound (i.e., achieves the minimum of the worst-case regret and an instance dependent regret), while at the same providing an instance-dependent query complexity bound. For benign instances with small eluder dimension and large gap,  our regret and query complexity bounds both scale with $\ln(T)$ where $T$ is the total number of interactions in contextual bandits. 
	\item In imitation learning, the stochastic preference feedback is generated based on the underlying reward-to-go of the expert's policy (e.g., the expert prefers actions that lead to higher reward-to-go). We propose an algorithm named \textsc{AURORAE}, in short of \textit{Active preference qUeRy fOR imitAtion lEarning}, which instantiates $H$ instances of \textsc{AURORA}, one per each time step for the finite horizon Markov Decision Process (MDP), where $H$ is the horizon. By leveraging preference-based feedback, we show that, interestingly, our algorithm can learn to outperform the expert when the expert is suboptimal. Such a result is beyond the scope of the classic imitation learning algorithm \textsc{DAgger}, and previously can only be achieved by algorithms like \textsc{AggreVaTe(d)} \citep{ross2014reinforcement,sun2017deeply,cheng2018convergence} and \textsc{LOLS} \citep{chang2015learning} which require direct access to expert's actions and also reward signal -- a much stronger feedback mode than ours.
\end{itemize}

To the best of our knowledge, for both contextual bandit and imitation learning with preference-based feedback, our algorithms are the first to achieve best-of-both-worlds regret bounds via active querying. %












