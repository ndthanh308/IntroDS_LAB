\section{Imitation Learning with Preference-Based Active Queries}

\begin{algorithm}[t]
\begin{algorithmic}[1]
\caption{Active preference qUeRy fOR imitAtion lEarning (AURORAE)}\label{alg:il}
\REQUIRE Function class  $\+F_0,\+F_1,\dots,\+F_{H-1}$, confidence parameter $\beta$.
\STATE Learner creates $H$ instances of \Cref{alg:cb}: $\textsc{AURORA}_h(\+F_h,\beta)$ for $h=0,1,\dots,H-1$.
   	\FOR{$t=1,2,\dots,T$}
   		\STATE Learner receive initial state $x_{t,0}$.
		\FOR{$h=0,1,\dots,H-1$}
			\STATE Learner feeds $x_{t,h}$ to $\textsc{AURORA}_h(\+F_h,\beta)$, and receives back 
		 $a_{t,h}$, $b_{t,h}$, $Z_{t,h}$.
			\IF{$Z_{t,h}=1$}
				\STATE Learner receives feedback $y_{t,h}$.
				\STATE Learner feeds $((x_{t,h},a_{t,h},b_{t,h}), y_{t,h})$
            to $\textsc{AURORA}_h(\+F_h,\beta)$ \asedit{to update its online regression oracle and local variables.} 
			\ENDIF
			\STATE Learner executes $a\sim \text{Uniform}(\{a_{t,h},b_{t,h}\})$ and transits to $x_{t,h+1}$. 
		\ENDFOR 
   	\ENDFOR 
\end{algorithmic}
\end{algorithm}

 In this section, we introduce our second algorithm, which is presented in \Cref{alg:il} for imitation learning. In essence, the learner treats the MDP as a concatenation of $H$ contextual bandits and runs an instance of \textsc{AURORA} (\Cref{alg:cb}) for each time step. Specifically, the learner first creates $H$ instances of \textsc{AURORA}, denoted by $\textsc{AURORA}_h$ (for $h=0,\dots,H-1$). Here, \textsc{AURORA}\(_h\) should be thought of as an interactive program that takes the context $x$ as input and outputs $a$, $b$, and $Z$. At each episode $t$, and each step $h$ therein, the learner first feeds the current state $x_{t,h}$ to $\textsc{AURORA}_h$ as the context; then, $\textsc{AURORA}_h$ decides whether to query (i.e.~\(Z_{t, h}\)) and returns the actions \(a_{t,h}\) and \(b_{t,h}\). If it decides to make a query, the learner will ask for the feedback $y_{t,h}$ on the proposed actions $a_{t,h}, b_{t,h}$, and provide the information $((x_{t,h},a_{t,h},b_{t,h}), y_{t,h})$ back to $\textsc{AURORA}_h$ to update its online regression oracle (and other local variables). We recall that the noisy binary feedback $y_{t,h}$ is sampled as  $y_{t,h}\sim \phi(Q^{\pi_e}_h(x_{t,h},a_{t,h})- Q^{\pi_e}_h(x_{t,h},b_{t,h}))$, and also emphasize that the learner neither has access to  $a\sim \pi_e(x_{t,h})$ like in \textsc{DAgger} \citep{ross2011reduction}  nor reward-to-go like in  \textsc{AggreVaTe(D)} \citep{ross2014reinforcement,sun2017deeply}. Finally, the learner chooses one of the two actions uniformly at random, executes it in the underlying MDP, and transits to the next state $x_{t,h+1}$ in the episode. The above process is then repeated with $\textsc{AURORA}_{h+1}$ till the episode ends. We name this algorithm \textsc{AURORAE}, the plural form of \textsc{AURORA}, which signifies that the algorithm is essentially a stack of multiple \textsc{AURORA} instances.

\subsection{Theoretical Analysis}

As \Cref{alg:il} is essentially a stack of \Cref{alg:cb}, we can inherit many of the theoretical guarantees from the previous section. To state the results, we first extend \Cref{asm:uniform-gap} into imitation learning. 
\begin{assumption}[Uniform Gap]\label{asm:uniform-gap-il} \asedit{Let \(f_h^\star\) be defined such that for any \(x \in \+X\), \(a, b \in \+A^2\), \(f_h^\star(x, a, b) = Q_h^{\pi_e}(x, a) -  Q_h^{\pi_e}(x, b)\).}
For all $h$, we assume the optimal action for $f_h^\star$ under any state $x\in\+X$ is unique. Further, we assume a uniform gap $\Delta:=\inf_h \inf_x \inf_{a\neq\pi_{f_h^\star}(x)} f_h^\star(x,\pi_{f_h^\star}(x),a)>0$.
\end{assumption}

This assumption essentially says that $Q^{\pi_e}_h$ has a gap in actions. 
We remark that, just as \Cref{asm:uniform-gap} is a common condition in the bandit literature, \Cref{asm:uniform-gap-il} is also common in MDPs \citep{du2019provably,foster2020instance,simchowitz2019non,jin2020simultaneously,lykouris2021corruption, he2021logarithmic}. The theoretical guarantee for \Cref{alg:il} is presented in \Cref{thm:il-regret}. We note a technical difference between this result and \Cref{thm:cb-regret}: although we treat the MDP as a concatenation of $H$ contextual bandits, the instantaneous regret of imitation learning is defined as the performance gap between the combined policy $\pi_t$ derived from the $H$ instances as a cohesive unit and the expert policy. This necessitates the use of performance difference lemma (\Cref{lem:pdl}) to get a unified result.

\begin{theorem}\label{thm:il-regret}
Under \Cref{asm:properties,asm:uniform-gap-il,asm:oracle-regret}, \Cref{alg:il} guarantees the following upper bounds of the regret and the number of queries:
\begin{align*}
	&\@{Regret}_T^{\@{IL}}\leq
	\widetilde{O}\left(
	H\cdot\min\left\{
	\sqrt{AT\beta}
	,\;
	\frac{A^2\beta^2\@{dim}_E\left(\+F,\Delta\right)}{\Delta}
	\right\}
	\right)-\@{Adv}_T,\\
	&\@{Queries}_T^{\@{IL}}\leq
	\widetilde{O}\left(
	H\cdot\min\left\{T
	,\;
	\frac{A^3\beta^3 \@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}
	\right\}
	\right)
\end{align*}
with probability at least $1-\delta$. Here
$
\@{Adv}_T\coloneqq\sum_{t=1}^T \sum_{h=0}^{H-1} \E_{x_{t,h}\sim d^{\pi_t}_{x_{t,0},h}}[\max_a A^{\pi_e}_h(x_{t,h},a)]
$ is non-negative,
and $d^{\pi_t}_{x_{t,0},h}(x)$ denotes the probability of $\pi_t$ \footnote{Policy $\pi_t$ consists of $H$ time-dependent policies $\pi_{t,1},\dots, \pi_{t,H}$, where each $\pi_{t,h}$ is defined implicitly via $\textsc{AURORA}_h$, i.e., $\pi_{t,h}$ generates action as follows: given $x_{t,h}$, $\textsc{AURORA}_h$ recommends $a_{t,h},b_{t,h}$,  followed by uniformly sampling an action from $\{a_{t,h},b_{t,h}\}$. } reaching the state $x$ at time step $h$ starting from inital state $x_{t,0}$. In the above,  $\beta=O(\alpha^{-1}\Upsilon+\alpha^{-2}\log(H\delta^{-1}\log(T)))$ and $\alpha$ denotes the coefficient of strong convexity of $\Phi$.
\end{theorem}


Compared to \Cref{thm:cb-regret}, the main terms of the upper bounds for imitation learning are precisely the bounds in \Cref{thm:cb-regret} multiplied by $H$. In the proof presented in \Cref{sec:pf-thm-il-regret}, we use the performance difference lemma to reduce the regret of imitation learning to the sum of the regret of $H$ contextual dueling bandits, which explains this additional factor of $H$. 

Another interesting point is that the main term of the regret upper bound is subtracted by a non-negative term $\@{Adv}_T$, which measures the degree to which we can \textit{outperform} the expert policy. This means that our algorithm not only competes with the expert policy but can also surpass it to some extent. This guarantee is stronger than that of \textsc{DAgger} \citep{ross2011reduction} in that \textsc{DAgger} cannot ensure the learned policy is better than the expert policy regardless of how suboptimal the expert may be. While this may look surprising at first glance since we are operating under a somewhat weaker query mode than that of \textsc{DAgger}, we note that by querying experts for comparisons on pairs of actions with feedback sampling as $y\sim \phi( Q^{\pi_e}(x,a) - Q^{\pi_e}(x,b))$, it is possible to identify the action that maximizes $Q^{\pi_e}(x,a)$ (even if we cannot identify the value $Q^{\pi_e}(x,a)$). 
Finally, we remark that our worst-case regret bound is similar to that of \citet{ross2014reinforcement,sun2017deeply}, which can also outperform a suboptimal expert but require access to both expert's actions and reward signals---a much stronger query model than ours. \looseness=-1 





