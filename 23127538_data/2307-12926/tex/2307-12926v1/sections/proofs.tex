\section{Missing Proofs}\label{sec:missing-pf}

\subsection{Supporting Lemmas}

\begin{definition}[Strong convexity]\label{def:strong-conv}
	A function $\Phi:[-1,1]\rightarrow\=R$ is $\alpha$-strongly-convex if for all $u,u'\in\=R$, we have
	\begin{align*}
		\frac{\alpha}{2}(u'-u)^2\leq \Phi(u') - \Phi(u) - \nabla\Phi(u)(u'-u).
	\end{align*}
	where $\nabla\Phi$ means the derivative of $\Phi$.
\end{definition}

\begin{lemma}\label{lem:width-lower-bound}
	For any $t\in[T]$, if $f^\star\in\+F_t$, then we have $w_t\geq\Delta$ whenever $|\+A_t|>1$.
\end{lemma}
\begin{proof}[Proof of \cref{lem:width-lower-bound}]
	When $|\+A_t|>1$, we know there exists a function $f'\in\+F_t$ satisfying
	\begin{align*}
		a':=\pi_{f'}(x_t)\neq \pi_{f^\star}(x_t)=:a^\star_t.
	\end{align*}
	Then we have $\Delta
	\leq f^\star(x_t,a^\star_t,a')\leq f^\star(x_t,a^\star_t,a')-f'(x_t,a^\star_t,a')\leq w_t$ where the second inequality holds since $f'(x_t,a^\star_t,a')\leq0$.
\end{proof}


\begin{lemma}\label{lem:regret-bounded-by-w}
	For any $t\in[T]$ and any arm $a\in\+A_t$, we have $f^\star(x_t,\pi_{f^\star}(x_t),a)\leq w_t$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:regret-bounded-by-w}]
For any $a\in\+A_t$, by the definition of $\+A_t$, there must exists a function $f$ for which $a=\pi_{f}(x_t)$. Hence,
	\begin{align*}
		f^\star(x_t,\pi_{f^\star}(x_t),a)
		\leq
		f^\star(x_t,\pi_{f^\star}(x_t),a)-f(x_t,\pi_{f^\star}(x_t),a)\leq w_t,
	\end{align*}
	where the first inequality holds since $f(x_t,\pi_{f^\star}(x_t),a)\leq0$.
\end{proof}

The following lemma is adapted from \citet[Lemma 2]{agarwal2013selective}.
\begin{lemma}\label{lem:pointwise-bound}
	The following holds with probability at least $1-\delta$ for any $T>3$,
	\begin{align*}
	\sum_{t=1}^TZ_t\big(f^\star(x_t,a_t,b_t)-f_t(x_t,a_t,b_t)\big)^2
	\leq\frac{4\Upsilon}{\alpha}+\frac{16+24\alpha}{\alpha^2}\log\big(4\delta^{-1}\log(T)\big).
	\end{align*} 
\end{lemma}

\begin{proof}[Proof of \Cref{lem:pointwise-bound}]
Throughout the proof, we denote $z_t:=(x_t,a_t,b_t)$ for notational simplicity. We define $D_\Phi$ as the Bregman divergence of the function $\Phi$:
\begin{align*}
	D_\Phi(u,v)=\Phi(u)-\Phi(v)-\phi(v)(u-v)
\end{align*}
where we recall that $\phi=\Phi'$ is the derivative of $\Phi$. Since $\Phi$ is $\alpha$-strong convex, we have $\alpha(u-v)^2/2\leq D_\Phi(u,v)$, and hence,
\begin{align}\label{eq:square-bregman}
	\sum_{t=1}^T Z_t\big(f^\star(z_t)-f_t(z_t)\big)^2\leq\frac{2}{\alpha}\sum_{t=1}^T Z_tD_\Phi(f_t(z_t),f^\star(z_t)).
\end{align}
Hence, it suffice to derive an upper bound for the Bregman divergence in the right hand side above. Define $\nu_t$ as below:
\begin{align*} 
	\nu_t :=&Z_t\Big[D_\Phi\left(f_t(z_t), f^\star(z_t)\right)-\left(\ell_\phi\left(f_t(z_t),y_t\right)-\ell_\phi\left(f^\star(z_t), y_t\right)\right)\Big] \\
	 =&Z_t\Big[D_\Phi\left(f_t(z_t), f^\star(z_t)\right)-\left(\Phi\left(f_t(z_t)\right)-(y_t+1)f_t(z_t)/2-\Phi\left(f^\star(z_t)\right)+(y_t+1)f^\star(z_t)/2\right)\Big] \\
	 =&Z_t\Big[\Phi\left(f_t(z_t)\right)-\Phi\left(f^\star(z_t)\right)-\phi\left(f^\star(z_t)\right)\left(f_t(z_t)-f^\star(z_t)\right)\\
    &\quad-\left(\Phi\left(f_t(z_t)\right)-(y_t+1)f_t(z_t)/2-\Phi\left(f^\star(z_t)\right)+(y_t+1)f^\star(z_t)/2\right)\Big] \\
	 =&Z_t\big(f_t(z_t)-f^\star(z_t)\big)\big((y_t+1)/2-\phi(f^\star(z_t))\big)
	\end{align*}
We note that $\E_t[(y_t+1)/2]=\phi(f^\star(z_t))$, and thus $\E_t[\nu_t]=0$, which means $\nu_t$ is a martingale difference sequence. Now we bound the value and the conditional variance of $\nu_t$ in order to derive concentration results.
\begin{enumerate}
	\item Bound the value of $\nu_t$: 
	\begin{align*}
		|\nu_t|\leq| (y_t+1)/2-\phi\left(f^\star(z_t)\right)|\cdot|f_t(z_t)-f^\star(z_t)|\leq1\cdot2 =2.
	\end{align*}
	\item Bound the conditional variance of $\nu_t$: 
	\begin{align*}
		\E_t[\nu_t^2]
		=& Z_t\E_t\left[\left( (y_t+1)/2-\phi\left(f^\star(z_t)\right)\right)^2\left(f_t(z_t)-f^\star(z_t)\right)^2\right]\\
		\leq& Z_t\E_t\left[\left(f_t(z_t)-f^\star(z_t)\right)^2\right]\\
		\leq& Z_t\E_t\left[\frac{2}{\alpha}\cdot D_\Phi(f_t(z_t),f^\star(z_t))\right]\\
		\leq& \frac{2Z_t}{\alpha}D_\Phi(f_t(z_t),f^\star(z_t))
	\end{align*}
	where for the last line we note that $x_t,g_t$ are measurable at $t$.
\end{enumerate}
Now we apply \Cref{lem:free2}, which yields for any $\delta<1/e$ and $T>3$, with probability at least $1-4\delta\log(T)$,
\begin{align*}
	\sum_{t=1}^T\nu_t
	\leq&\max\left\{
	2\sqrt{\sum_{t=1}^T\frac{2Z_t}{\alpha}D_\Phi(f_t(z_t),f^\star(z_t))},6\sqrt{\log(1/\delta)}
	\right\}\sqrt{\log(1/\delta)}\\
	\leq&
	2\sqrt{{\sum_{t=1}^T\frac{2 Z_t}{{\alpha}}D_\Phi(f_t(z_t),f^\star(z_t))}{\log(1/\delta)}}+6{\log(1/\delta)}\tag{since $\max(a,b)\leq a+b$}\\
	\leq&
	{\sum_{t=1}^T\frac{1}{2}Z_tD_\Phi(f_t(z_t),f^\star(z_t))}+{\frac{4\log(1/\delta)}{\alpha}}+6{\log(1/\delta)}\tag{AM-GM}
\end{align*}
Recall the definition of $\nu_t$, and we conclude that
\begin{align*}
	\sum_{t=1}^TZ_t{D}_{\Phi}\left(f_t(z_t), f^\star(z_t)\right)-\sum_{t=1}^TZ_t\Big(\ell_\phi\big(f_t(z_t), y_t\big)-\ell_\phi\big(f^\star(z_t), y_t\big)\Big)\leq\\\sum_{t=1}^T\frac{1}{2}Z_tD_\Phi(f_t(z_t),f^\star(z_t))+\frac{4\log(1/\delta)}{\alpha}+6{\log(1/\delta)},
\end{align*}
which implies
\begin{align*}
	\frac{1}{2}\sum_{t=1}^TZ_t {D}_{\Phi}\left(f_t(z_t), f^\star(z_t)\right)\leq\sum_{t=1}^TZ_t\Big(\ell_\phi\big(f_t(z_t), y_t\big)-\ell_\phi\big(f^\star(z_t), y_t\big)\Big)+\frac{4\log(1/\delta)}{\alpha}+6{\log(1/\delta)}.
\end{align*}
Plugging this upper bound of Bregman divergence into \eqref{eq:square-bregman}, we obtain that, with probability at least $1-4\delta\log(T)$, for any $\delta<1/e$ and $T>3$, we have
	\begin{align*}
	\sum_{t=1}^TZ_t\big(f^\star(z_t)-f_t(z_t)\big)^2
	\leq\frac{4}{\alpha}\Upsilon+\left(\frac{16}{\alpha^2}+\frac{24}{\alpha}\right)\log(\delta^{-1})=:\beta
	\end{align*}
Finally, we finish the proof by adjusting the coefficient $\delta$ and taking a union bound to obtain the desired result.
\end{proof}









The following lemma is a variant of \citet[Proposition 3]{russo2013eluder}, with the main difference being that (1) the version space is established using the function produced by the oracle instead of the least squares estimator, and (2) the extra multiplicative factor $Z_t$.
\begin{lemma}\label{lem:w-eluder}
	For \Cref{alg:cb}, it holds that
	\begin{align}\label{eq:w-eluder-eq}
		\sum_{t=1}^T Z_t \indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a_t,b_t)-f'(x_t,a_t,b_t)> \epsilon\right\}\leq\left(\frac{4\beta}{\epsilon^2}+1\right)\@{dim}_E(\+F,\epsilon)
	\end{align}
	for any constant $\epsilon>0$,
\end{lemma}
\begin{proof}[Proof of \Cref{lem:w-eluder}]
We first define a subsequence consisting only of the elements for which we made a query in that round. Specifically, we define $((x_{i_1}, a_{i_1}, b_{i_1}), (x_{i_2}, a_{i_2}, b_{i_2}), \dots, (x_{i_k}, a_{i_k}, b_{i_k}))$ where $1\leq i_1 < i_2 < \dots < i_k\leq T$ and $(x_t, a_t, b_t)$ belongs to the subsequence if and only if $Z_t=1$. We further simplify the notation by defining $z_j:=(x_{i_j},a_{i_j},b_{i_j})$ and $f(z_j):=f(x_{i_j},a_{i_j},b_{i_j})$. Then we note that the left-hand side of \eqref{eq:w-eluder-eq} is equivalent to
	\begin{align}\label{eq:equal1}
		\sum_{j=1}^k  \indic\left\{\sup_{f,f'\in\+F_j}f(z_j)-f'(z_j)> \epsilon\right\},
	\end{align}
	and the version space in \Cref{alg:cb} is equal to
	\begin{align}\label{eq:equal2}
		\+F_j=\left\{f\in\+F:\sum_{s=1}^{j-1} \Big(f(z_s)-f_t(z_s)\Big)^2\leq \beta\right\}.
	\end{align}
	
	Hence, it suffice to establish the lower bound for \eqref{eq:equal1} under the version space of \eqref{eq:equal2}. To that end, we make one more simplicication in notation: we denote 
\begin{align*}
	w'_j:=\sup_{f,f'\in\+F_j}f(z_j)-f'(z_j)
\end{align*}

We begin by showing that if $w'_j>\epsilon$ for some $j\in[k]$, then $z_j$ is $\epsilon$-dependent on at most $4\beta/\epsilon^2$ disjoint subsequence of its predecessors. To see this, we note that when $w'_j>\epsilon$, there must exist two function $f,f'\in\+F_j$ such that $f(z_j)-f'(z_j)>\epsilon$. If $z_j$ is $\epsilon$-dependent on a subsequence $(z_{i_1},z_{i_2},\dots,z_{i_n})$ of its predecessors, we must have
\begin{align*}
\sum_{s=1}^{n} \big(f(z_{i_s})-f'(z_{i_s})\big)^2>\epsilon^2.
\end{align*}
Hence, if $z_j$ is $\epsilon$-dependent on $l$ disjoint subsequences, we have 
\begin{align}\label{eq:eluder1}
	\sum_{s=1}^{j-1} \big(f(z_s)-f'(z_s)\big)^2
	>
	l\epsilon^2.
\end{align}
For the left-hand side above, we also have
\begin{align*}
	\sum_{s=1}^{j-1} \big(f(z_s)-f'(z_s)\big)^2
	\leq
	2\sum_{s=1}^{j-1} \big(f(z_s)-f_t(z_s)\big)^2
	+
	2\sum_{s=1}^{j-1} \big(f_t(z_s)-f'(z_s)\big)^2
	\leq4\beta\numberthis\label{eq:eluder2}
\end{align*}
where the first inequality holds since $(a+b)^2\leq2(a^2+b^2)$ for any $a,b$, and the second inequality holds by \eqref{eq:equal2}. Combining \eqref{eq:eluder1} and \eqref{eq:eluder2}, we get that $l\leq4\beta/\epsilon^2$.
	
	Next, we show that for any sequence $(z'_1,\dots,z'_\tau)$, there is at least one element that is $\epsilon$-dependent on at least $\tau/d-1$ disjoint subsequence of its predecessors, where $d:=\@{dim}_E(\+F,\epsilon)$. To show this, let $m$ be the integer satisfying $md+1\leq \tau\leq md+d$. We will construct $m$ disjoint subsequences, $B_1,\dots,B_m$. At the beginning, let $B_i=(z'_i)$ for $i\in[m]$. If $z'_{m+1}$ is $\epsilon$-dependent on each subsequence $B_1,\dots,B_m$, then we are done. Otherwise, we select a subsequence $B_i$ which $z'_{m+1}$ is $\epsilon$-independent of and append $z'_{m+1}$ to $B_i$. We repeat this process for all elements with indices $j>m+1$ until either $z'_j$ is $\epsilon$-dependent on each subsequence or $j=\tau$. For the latter, we have $\sum_{i=1}^m |B_i|\geq md$, and since each element of a subsequence $B_i$ is $\epsilon$-independent of its predecesors, we must have $|B_i|=d$ for all $i$. Then, $z_\tau$ must be $\epsilon$-dependent on each subsequence by the definition of eluder dimension.
	
	Finally, let's take the sequence $(z'_1,\dots z'_\tau)$ to be the subsequence of $(z_1,\dots,z_k)$ consisting of elements $z_j$ for which $w_j'>\epsilon$. As we have established, we have (1) each $z'_j$ is $\epsilon$-dependent on at most $4\beta/\epsilon^2$ disjoint subsequences, and (2) some $z'_j$ is $\epsilon$-dependent on at least $\tau/d-1$ disjoint subsequences. Therefore, we must have $\tau/d-1\leq 4\beta/\epsilon^2$, implying that $\tau\leq(4\beta/\epsilon^2+1)d$.
\end{proof}



The following lemma is adopted from \citet[Lemma 3]{saha2022efficient}.
\begin{lemma}\label{lem:igw}
	For any function $f\in\+F$ and any context $x\in\+X$, the following convex program of $p\in\Delta(\+A)$ is always feasible:
	\begin{align*}
		\forall a\in\+A:
		\sum_{b}f(x,a,b)p(b)+\frac{2}{\gamma p(a)}\leq\frac{5A}{\gamma}.
	\end{align*}
	Furthermore, any solution $p$ satisfies:
	\begin{align*}
		\E_{a\sim p}\Big[f^\star(x,\pi_{f^\star}(x),a)\Big]
		\leq 
		\frac{\gamma}{4}\E_{a,b\sim p}\Big[\big(f(x,a,b)-f^\star(x,a,b)\big)^2\Big]+\frac{5A}{\gamma}
	\end{align*}
	whenever $\gamma\geq 2A$.
\end{lemma}



\begin{lemma}\label{lem:igw-r-version}
	Assume that for each $f\in\+F$, there exists an associated function $r:\+X\times\+A\rightarrow[0,1]$ such that $f(x,a,b)=r(x,a)-r(x,b)$ for any $x\in\+X$ and $a,b\in\+A$. In this case, for any context $x\in\+X$, if we define $p$ as
	\begin{align*}
		p(a)=
		\begin{cases}
			\frac{1}{A+\gamma\big(r(x,\pi_f(x))-r(x,a)\big)} & a\neq\pi_f(x)\\
			1-\sum_{a\neq\pi_f(x)}p(a) & a=\pi_f(x)
		\end{cases},
	\end{align*}
	then we have
	\begin{align*}
		\E_{a\sim p}\Big[f^\star(x,\pi_{f^\star}(x),a)\Big]
		\leq
		\gamma\E_{a,b\sim p}\Big[\Big(f(x,a,b)-f^\star(x,a,b)\Big)^2\Big]+\frac{A}{\gamma}
	\end{align*}
\end{lemma}
\begin{proof}[Proof of \cref{lem:igw-r-version}]
	Fix any $b\in\+A$. Then, the distribution $p$ can be rewritten as
	\begin{align*}
		p(a)=
		\begin{cases}
			\left(
			A+2\gamma
			\left(
			\frac{r(x,\pi_f(x))-r(x,b)+1}{2}
			-
			\frac{r(x,a)-r(x,b)+1}{2}
			\right)
			\right)^{-1} 
			& a\neq\pi_f(x)\\
			1-\sum_{a\neq\pi_f(x)}p(a) 
			& a=\pi_f(x)
		\end{cases}.
	\end{align*}
	Therefore, denoting $f^\star(x,a,b)=r^\star(x,a)-r^\star(x,b)$ for some function $r^\star$, we have
	\begin{align*}
		\E_{a\sim p}\Big[f^\star(x,\pi_{f^\star}(x),a)\Big]
		=&\E_{a\sim p}\Big[r^\star(x,\pi_{f^\star}(x))-r^\star(x,a)\Big]\\
		=&2\E_{a\sim p}\left[\frac{r^\star(x,\pi_{f^\star}(x))-r^\star(x,b)+1}{2}-\frac{r^\star(x,a)-r^\star(x,b)+1}{2}\right]\\
		\leq&
		2\cdot2\gamma\E_{a\sim p}\left[\left(\frac{r(x,a)-r(x,b)+1}{2}-\frac{r^\star(x,a)-r^\star(x,b)+1}{2}\right)^2\right]+\frac{A}{\gamma}\\
		=&
		\gamma\E_{a\sim p}\Big[\Big(f(x,a,b)-f^\star(x,a,b)\Big)^2\Big]+\frac{A}{\gamma}
	\end{align*}
	where for the inequality above we invoked \Cref{lem:igw-general} with $\^y(a)=(r(x,a)-r(x,b)+1)/2$ and $y^\star(a)=(r^\star(x,a)-r^\star(x,b)+1)/2$. We note that the above holds for any $b\in\+A$. Hence, we complete the proof by sampling $b\sim p$.
\end{proof}


\begin{lemma}\label{lem:sum-zw}
	Assume $f^\star\in\+F_t$ for all $t\in[T]$. Suppose there exists some $t'\in[T]$ such that $\lambda_t=0$ for all $t\leq t'$. Then we have 
	\begin{align*}
	\sum_{t=1}^{t'} Z_tw_t\leq
	56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))
	\end{align*}
\end{lemma}
with probability at least $1-\delta$.
\begin{proof}
Since $f^\star\in\+F_t$, we always have $\pi_{f^\star}(x_t)\in\+A_t$ for all $t\in[T]$. Hence, whenever $Z_t$ is zero, we have $\+A_t=\{\pi_{f^\star}(x_t)\}$ and thus we do not incur any regret. Hence, we know $Z_tw_t$ is either 0 or at least $\Delta$ by \Cref{lem:width-lower-bound}. Let us fix an integer $m>1/\Delta$, whose value will be specified later. We divide the interval $[\Delta,1]$ into bins of width $1/m$ and conduct a refined study of the sum of $Z_t w_t$:
\begin{align*}
	\sum_{t=1}^{t'} Z_tw_t
	\leq&\sum_{t=1}^{t'} \sum_{j=0}^{\left(1-\Delta\right)m-1}Z_tw_t\cdot\indic\left\{Z_tw_t\in\left[\Delta+\frac{j}{m},\,\Delta+\frac{j+1}{m}\right]\right\}\\
	\leq&\sum_{j=0}^{\left(1-\Delta\right)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{t=1}^{t'} Z_t\indic\left\{w_t\geq\Delta+\frac{j}{m}\right\}\\
	=&\sum_{j=0}^{\left(1-\Delta\right)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{t=1}^{t'} Z_t\indic\left\{\sup_{a,b\in\+A_t}\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\Delta+\frac{j}{m}\right\}\\
	=&\sum_{j=0}^{\left(1-\Delta\right)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{t=1}^{t'} Z_t\sup_{a,b\in\+A_t}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\Delta+\frac{j}{m}\right\}\\
	\leq&\sum_{j=0}^{\left(1-\Delta\right)m-1}\left(\Delta+\frac{j+1}{m}\right)\sum_{t=1}^{t'} Z_t\sum_{a,b}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\left(\Delta+\frac{j}{m}\right)\right\}\\
	\leq&\sum_{j=0}^{\left(1-\Delta\right)m-1}\left(\Delta+\frac{j+1}{m}\right)A^2 \underbrace{\sum_{t=1}^{t'} Z_t \E_{a,b\sim p_t}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\left(\Delta+\frac{j}{m}\right)\right\}}_{(*)}
\end{align*}
where in the third inequality we replace the supremum over $a,b$ by the summation over $a,b$, and in the last inequality we further replace it by the expectation. Here recall that $p_t(a)$ is uniform when $\lambda_t=0$, leading to the extra $A^2$ factor. To deal with $(*)$, we first apply \Cref{lem:freedman} to recover the empirical $a_t$ and $b_t$, and then apply \Cref{lem:w-eluder} to get an upper bound via the eluder dimension:
\begin{align*}
	(*)\leq&2\sum_{t=1}^{t'} Z_t \indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a_t,b_t)-f'(x_t,a_t,b_t)\geq\left(\Delta+\frac{j}{m}\right)\right\}+8\log(\delta^{-1})\\
	\leq&2\left(\frac{4\beta}{\left(\Delta+\frac{j}{m}\right)^2}+1\right)\@{dim}_E\left(\+F;\Delta\right)+8\log(\delta^{-1})\\
	\leq&\frac{10\beta}{\left(\Delta+\frac{j}{m}\right)^2}\cdot\@{dim}_E\left(\+F;\Delta\right)+8\log(\delta^{-1})
\end{align*}
with probability at least $1-\delta$. Plugging $(*)$ back, we obtain
\begin{align*}
	\sum_{t=1}^{t'} Z_tw_t
	\leq&\sum_{j=0}^{\left(1-\Delta\right)m-1}\left(\Delta+\frac{j+1}{m}\right)\cdot\frac{10A^2\beta}{\left(\Delta+\frac{j}{m}\right)^2}\cdot\@{dim}_E\left(\+F;\Delta\right)+8mA^2\log(\delta^{-1})\\
	=&10A^2\beta\cdot \@{dim}_E\left(\+F,\Delta\right)\sum_{j=0}^{\left(1-\Delta\right)m-1}\frac{\Delta+\frac{j+1}{m}}{\left(\Delta+\frac{j}{m}\right)^2}+8mA^2\log(\delta^{-1})\\
	\leq&10A^2\beta\cdot \@{dim}_E\left(\+F,\Delta\right)\left(\frac{\Delta+1/m}{\Delta^2}+\sum_{j=1}^{\left(1-\Delta\right)m-1}\frac{2}{\Delta+\frac{j}{m}}\right)+8mA^2\log(\delta^{-1})\\
	\leq&10A^2\beta\cdot \@{dim}_E\left(\+F,\Delta\right)\sum_{j=0}^{\left(1-\Delta\right)m-1}\frac{2}{\Delta+\frac{j}{m}}+8mA^2\log(\delta^{-1})\\
	\leq&20A^2\beta\cdot \@{dim}_E\left(\+F,\Delta\right)\sum_{j=0}^{\left(1-\Delta\right)m-1}\int_{j-1}^j\frac{1}{\Delta+\frac{x}{m}} \d x+8mA^2\log(\delta^{-1})\\
	=&20A^2\beta\cdot \@{dim}_E\left(\+F,\Delta\right)\int_{-1}^{(1-\Delta)m-1}\frac{1}{\Delta+\frac{x}{m}} \d x+8mA^2\log(\delta^{-1})\\
	=&20A^2\beta\cdot \@{dim}_E\left(\+F,\Delta\right)\cdot m\log\left(\frac{1}{\Delta-m^{-1}}\right)+8mA^2\log(\delta^{-1})
\end{align*}
where for the second inequality, we use the fact that $(j+1)/m\leq 2j/m$ for any $j\geq 1$; for the third inequality, we assume $m>1/\Delta$. Setting $m=2/\Delta$, we arrive at
\begin{align*}
	\sum_{t=1}^{t'} Z_tw_t
	\leq&
	40A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/\Delta)+16A^2\log(\delta^{-1})/\Delta\\
	\leq&
	56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta)),
\end{align*}
which completes the proof.
\end{proof}


\begin{lemma}\label{lem:lambda-all-0}
Whenever 
$$
56A^2\beta\cdot\@{dim}_E\left(\+F,\Delta\right)\cdot\log(2/(\delta\Delta))/\Delta<\sqrt{AT/\beta},
$$
we have $\lambda_1=\lambda_2=\dots=\lambda_T=0$ with probability at least $1-\delta$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:lambda-all-0}]
	We prove it via contradiction. Assume the inequality holds but there exists $t'$ for which $\lambda_{t'}=1$. Without loss of generality, we assume that $\lambda_t=0$ for all $t<t'$, namely that $t'$ is the first time that $\lambda_t$ is 1. Then by definition of $\lambda_{t'}$, we have
	\begin{align*}
		\sum_{s=1}^{t'-1} Z_sw_s\geq\sqrt{AT/\beta}.
	\end{align*}
	On the other hand, by \Cref{lem:sum-zw}, we have
	\begin{align*}
		\sum_{s=1}^{t'-1} Z_s w_s\leq
	56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta)).
	\end{align*}
	The combination of the above two inequalities contradicts with the conditions.
\end{proof}




\subsection{Proof of Lemma~\ref{lem:optimal-action-exist}}
\begin{proof}[Proof of \Cref{lem:optimal-action-exist}]
	We prove it via contradiction. If no such arm exists, meaning that for any arm $a$, there exists an arm $b$ such that $f^\star(x,a,b)<0$. Then we can find a sequence of arms $(a_1,a_2,\dots,a_k)$ such that $f^\star(x,a_i,a_{i+1})<0$ for any $i=1,\dots,k-1$ and $f^\star(x,a_k,a_1)<0$, which contradicts with the transitivity (\Cref{asm:properties}).
\end{proof}














\subsection{Proof of Theorem~\ref{thm:cb-regret}}\label{sec:pf-thm-cb-regret}

We begin by showing the worst-case regret upper bound.

\begin{lemma}[Worst-case regret upper bound]\label{lem:worst-case-regret-ub}
	For \Cref{alg:cb}, assume $f^\star\in\+F_t$ for all $t\in[T]$. Then, we have
	\begin{align*}
		\@{Regret}^{\@{CB}}_T\leq68\sqrt{AT\beta}\cdot\log(4\delta^{-1})
	\end{align*}
	with probability at least $1-\delta$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:worst-case-regret-ub}]

We recall that the regret is defined as
\begin{align*}
		\@{Regret}^{\@{CB}}_T=\sum_{t=1}^{T}\big(
		f^\star(x_t,\pi_{f^\star}(x_t),a_t)+
				f^\star(x_t,\pi_{f^\star}(x_t),b_t)\big).
\end{align*}
Since $a_t$ and $b_t$ are always drawn independently from the same distribution in \Cref{alg:cb}, we only need to consider the regret of the $a_t$ part in the following proof for brevity --- multiplying the result by two would yield the overall regret.

	We first observe the definition of $\lambda_t$ in \Cref{alg:cb}: the left term $\sum_{s=1}^{t-1}Z_s w_s$ in the indicator is non-decreasing in $t$ while the right term remains constant. This means that there exists a particular time step $t'\in[T]$ dividing the time horizon into two phases: $\lambda_t=0$ for all $t\leq t'$ and $\lambda_t=1$ for all $t>t'$. Now, we proceed to examine these two phases individually.
	
	For all rounds before or on $t'$, we can compute the expected partial regret as
	\begin{align*}
		\sum_{t=1}^{t'}\E_{a\sim p_t}\big[f^\star(x_t,\pi_{f^\star}(x_t),a)\big]
		=
		\sum_{t=1}^{t'}Z_t\E_{a\sim p_t}\big[f^\star(x_t,\pi_{f^\star}(x_t),a)\big]
		\leq&\sum_{t=1}^{t'}Z_tw_t\leq\sqrt{AT\beta},\numberthis\label{eq:worst-regret-1}
	\end{align*}
	where the equality holds since we have $\+A_t=\{\pi_{f^\star}(x_t)\}$ whenever $Z_t=0$ under the condition that $f^\star\in\+F_t$, and thus we don't incur regret in this case. The first inequality is \Cref{lem:regret-bounded-by-w}, and the second inequality holds by the definition of $\lambda_t$ and the condition that $\lambda_{t}=0$.
	
	On the other hand, for all rounds after $t'$, we have 
	\begin{align*}
		&\sum_{t=t'+1}^{T}\E_{a\sim p_t}\big[f^\star(x_t,\pi_{f^\star}(x_t),a)\big]\\
		=&\sum_{t=t'+1}^{T}Z_t\E_{a\sim p_t}\big[f^\star(x_t,\pi_{f^\star}(x_t),a)\big]\\
		\leq&\sum_{t=t'+1}^{T}Z_t\left(\frac{5A}{\gamma_t}+\frac{\gamma_t}{4}\E_{a,b\sim p_t}\Big[\big(f^\star(x_t,a,b)-f_t(x_t,a,b)\big)^2\Big]\right)\\
		=&\sum_{t=t'+1}^{T}Z_t\left(\frac{5A}{\sqrt{AT/\beta}}+\frac{\sqrt{AT/\beta}}{4}\E_{a,b\sim p_t}\Big[\big(f^\star(x_t,a,b)-f_t(x_t,a,b)\big)^2\Big]\right)\\
		\leq&5\sqrt{AT\beta}+\frac{\sqrt{AT/\beta}}{4}\sum_{t=t'+1}^{T} Z_t \E_{a,b\sim p_t}\Big[\big(f^\star(x_t,a,b)-f_t(x_t,a,b)\big)^2\Big]\\
		\leq&5\sqrt{AT\beta}+\frac{\sqrt{AT/\beta}}{2}\sum_{t=t'+1}^{T} Z_t \big(f^\star(x_t,a_t,b_t)-f_t(x_t,a_t,b_t)\big)^2+8\sqrt{AT/\beta}\cdot\log(4\delta^{-1})\\
		\leq&5\sqrt{AT\beta}+\frac{\sqrt{AT\beta}}{2}+8\sqrt{AT/\beta}\cdot\log(4\delta^{-1}).\numberthis\label{eq:worst-regret-2}
	\end{align*}
	where the first inequality holds by \Cref{lem:igw} (or \Cref{lem:igw-r-version} for specific function classes), the second equality is by the definition of $\gamma_t$, the third inequality is by \Cref{lem:freedman}, and the fourth inequality holds by \Cref{lem:pointwise-bound}.
	
	Putting the two parts, \eqref{eq:worst-regret-1} and \eqref{eq:worst-regret-2}, together, we arrive at
	\begin{align*}
		\sum_{t=1}^{T}\E_{a\sim p_t}\big[f^\star(x_t,\pi_{f^\star}(x_t),a)\big]
		\leq7\sqrt{AT\beta}+8\sqrt{AT/\beta}\cdot\log(4\delta^{-1})
		\leq15\sqrt{AT\beta}\cdot\log(4\delta^{-1}).
	\end{align*}
	Now we apply \Cref{lem:freedman} again. The following holds with probability at least $1-\delta/2$,
	\begin{align*}
		\sum_{t=1}^{T}f^\star(x_t,\pi_{f^\star}(x_t),a_t)
		\leq
		2\sum_{t=1}^{T}\E_{a\sim p_t}\big[f^\star(x_t,\pi_{f^\star}(x_t),a)\big]
		+4\log(4\delta^{-1})
		\leq
		34\sqrt{AT\beta}\cdot\log(4\delta^{-1}).
	\end{align*}
	The above concludes the regret of the $a_t$ part. The regret of the $b_t$ can be shown in the same way. Adding them together, we conclude that
	\begin{align*}
		\@{Regret}^{\@{CB}}_T=\sum_{t=1}^{T}\big(
		f^\star(x_t,\pi_{f^\star}(x_t),a_t)+
				f^\star(x_t,\pi_{f^\star}(x_t),b_t)\big)
		\leq68\sqrt{AT\beta}\cdot\log(4\delta^{-1}).
	\end{align*}
\end{proof}



\begin{lemma}[Instance-dependent regret upper bound]\label{lem:ins-depend-regret-ub}
	For \Cref{alg:cb}, assume $f^\star\in\+F_t$ for all $t\in[T]$. Then, we have
	\begin{align*}
		\@{Regret}^{\@{CB}}_T\leq
		3808 A^2\beta^2\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log^2(4/(\delta\Delta)) 
	\end{align*}
	with probability at least $1-\delta$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:ins-depend-regret-ub}]
	We consider two cases. First, when 
	\begin{equation}\label{eq:regret-cases1}
	56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))<\sqrt{AT/\beta},
	\end{equation}
	we invoke \Cref{lem:lambda-all-0} and get that $\lambda_t=0$ for all $t\in[T]$. Hence, we have
	\begin{align*}
		\@{Regret}^{\@{CB}}_T
		=&\sum_{t=1}^{T}\big(
		f^\star(x_t,\pi_{f^\star}(x_t),a_t)+
				f^\star(x_t,\pi_{f^\star}(x_t),b_t)\big)\\
		\leq&2\sum_{t=1}^T Z_t w_t\\
		\leq&112A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))\\
		\leq&3808 A^2\beta^2\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log^2(4/(\delta\Delta))
	\end{align*}
	where the first inequality is by \Cref{lem:regret-bounded-by-w} and the fact that we incur no regret when $Z_t=0$ since $f^\star\in\+F_t$. The second inequality is by \Cref{lem:sum-zw}.
	
	On the other hand, when the contrary of \eqref{eq:regret-cases1} holds, i.e., 
	\begin{equation}\label{eq:regret-cases2}
	56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))\geq\sqrt{AT/\beta},
	\end{equation}
	applying \Cref{lem:worst-case-regret-ub}, we have
	\begin{align*}
		\@{Regret}^{\@{CB}}_T
		\leq&68\sqrt{AT\beta}\cdot\log(4\delta^{-1})\\
		=&68\beta\cdot\log(4\delta^{-1})\cdot\sqrt{AT/\beta}\\
		\leq&68\beta\cdot\log(4\delta^{-1})\cdot
	56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))\\
		\leq& 3808 A^2\beta^2\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log^2(4/(\delta\Delta))
	\end{align*}
	where we apply the condition \eqref{eq:regret-cases2} in the second inequality.
\end{proof}


\begin{lemma}[Query complexity]\label{lem:query-bound}
	For \Cref{alg:cb}, assume $f^\star\in\+F_t$ for all $t\in[T]$. Then, we have
		\begin{align*}
			\@{Queries}^{\@{CB}}_T\leq \min\left\{T,\, 3136 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}\cdot\log^2(2/(\delta\Delta))\right\}
		\end{align*}
		with probability at least $1-\delta$.
	\end{lemma}
	\begin{proof}[Proof of \Cref{lem:query-bound}]
	
	We consider two cases. First, when
	\begin{align}\label{eq:query-case}
	56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))<\sqrt{AT/\beta}	
	\end{align}
	we can invoke \Cref{lem:lambda-all-0} and get that $\lambda_t=0$ for all $t\in[T]$. Hence,
	\begin{align*}
		\@{Queries}^{\@{CB}}_T
		=&\sum_{t=1}^T Z_t\\
		=&\sum_{t=1}^T Z_t\indic\{w_t\geq \Delta\}\\
		=&\sum_{t=1}^TZ_t\sup_{a,b\in\+A_t}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\Delta\right\}\\
		\leq&\sum_{t=1}^TZ_t\sum_{a,b}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\Delta\right\}\\
		\leq&A^2\underbrace{\sum_{t=1}^TZ_t \E_{a,b\sim p_t}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\Delta\right\}}_{(*)}
	\end{align*}
	where the second equality is by \Cref{lem:width-lower-bound}, the second inequality holds as $p_t(a)$ is uniform for any $a,b$ when $\lambda_t=0$. We apply \Cref{lem:freedman} and \Cref{lem:w-eluder} to $(*)$ and obtain
	\begin{align*}
		(*)\leq& 2\sum_{t=1}^TZ_t \indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a_t,b_t)-f'(x_t,a_t,b_t)\geq\Delta\right\}+8\log(\delta^{-1})\\
		\leq& 2\left(\frac{4\beta}{\Delta^2}+1\right)\@{dim}_E(\+F;\Delta)+8\log(\delta^{-1})\\
		\leq& \frac{10\beta}{\Delta^2}\cdot\@{dim}_E(\+F;\Delta)+8\log(\delta^{-1}).
	\end{align*}
	Plugging this back, we obtain
	\begin{align*}
		\@{Queries}^{\@{CB}}_T
		\leq&\frac{10A^2\beta}{\Delta^2}\cdot\@{dim}_E(\+F;\Delta)+8A^2\log(\delta^{-1})\\
		\leq&3136 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}\cdot\log^2(2/(\delta\Delta)).
	\end{align*}
	
	On the other hand, when the contrary of \eqref{eq:query-case} holds, i.e., 
	\begin{align*}
	56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\Delta\right)}{\Delta}\cdot\log(2/(\delta\Delta))\geq\sqrt{AT/\beta}.
	\end{align*}
	Squaring both sides, we obtain
	\begin{align*}
	3136 A^4 \beta^2 \frac{\@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}\cdot\log^2(2/(\delta\Delta))
	\geq AT/\beta
	\end{align*}
	which leads to
	\begin{equation*}
	T
	\leq 3136 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}\cdot\log^2(2/(\delta\Delta)).
	\end{equation*}
	We note that we always have $\@{Queries}^{\@{CB}}_T\leq T$, and thus,
	\begin{equation*}
	\@{Queries}^{\@{CB}}_T\leq
	T
	\leq 3136 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}\cdot\log^2(2/(\delta\Delta)).
	\end{equation*}
	Hence, we complete the proof.
	\end{proof}
	
	Having established the aforementioned lemmas, we are now able to advance towards the proof of Theorem 1.
	
	\begin{proof}[Proof of Theorem~\ref{thm:cb-regret}]
	By \Cref{lem:pointwise-bound} and the construction of version spaces $\+F_t$ in \Cref{alg:cb}, we have $f^\star\in\+F_t$ for all $t\in[T]$ with probability at least $1-\delta$. Then, the rest of the proof follows from \Cref{lem:worst-case-regret-ub,lem:ins-depend-regret-ub,lem:query-bound}.
	\end{proof}



\subsection{Proof of Theorem~\ref{thm:lower-bound}}

In this section, we will prove the following theorem, which is stronger than \Cref{thm:lower-bound}.
\begin{theorem}[Lower bounds]\label{thm:lower-bound-stronger}
The following two claims hold:
\begin{enumerate}
	\item[(1)] for any algorithm, there exists an instance that leads to $\@{Regret}^{\@{CB}}_T=\Omega(\sqrt{AT})$;
	\item[(2)] for any algorithm achieving a worse-case expected regret upper bound in the form of $\E[\@{Regret}^{\@{CB}}_T]= O(\sqrt{A}\cdot T^{1-\beta})$ for some $\beta>0$, there exists an instance with gap $\Delta=\sqrt{A}\cdot T^{-\beta}$ that results in $\E[\@{Regret}^{\@{CB}}_T]=\Omega(A/\Delta)=\Omega(\sqrt{A}\cdot T^{\beta})$ and $\E[\@{Queries}^{\@{CB}}_T]=\Omega(A/\Delta^2)=\Omega(T^{2\beta})$.
\end{enumerate}
\end{theorem}
We observe that \Cref{thm:lower-bound} can be considered as a corollary of the above theorem when setting $\beta=1/2$.

In what follows, we will first demonstrate lower bounds in the setting of \textit{multi-armed bandits (MAB) with active queries} and subsequently establish a reduction from it to contextual dueling bandits in order to achieve these lower bounds.  We start by formally defining the setting of MAB with active queries below. 

\textbf{Multi-armed bandits with active queries.}
We consider a scenario where there exist $A$ arms. Each arm $a$ is assumed to yield a binary reward (0 or 1), which is sampled from a Bernoulli distribution $\text{Bern}(\-r_a)$, where $\-r_a$ denotes the mean reward associated with arm $a$.The arm with the highest mean reward is denoted by $a^\star\coloneqq\argmax_a \-r_a$. Let $\Delta_a\coloneqq \-r_{a^\star}-\-r_{a}$ denote the gap of arm $a\in[A]$. The interaction proceeds as follows: at each round $t\in[T]$, we need to pull an arm but can choose whether to receive the reward signal (denote this choice by $Z_t$). The objective is to minimize two quantities: the regret and the number of queries,
\begin{equation}\label{eq:mab-regret-query}
	\@{Regret}_T=\sum_{t=1}^T \Delta_{a_t},\quad\@{Queries}_T=\sum_{t=1}^T Z_t.
\end{equation}

Towards the lower bounds, we will start with a bound on the KL divergence over distributions of runs under two different bandits. This result is a variant of standard results which can be found in many bandit literature (e.g., \citet{lattimore2020bandit}).


\begin{lemma}\label{lem:kl-decompose}
Let $I_1$ and $I_2$ be two instances of MAB. We define $p_1$ and $p_2$ as their respective distributions over the outcomes of all pulled arms and reward signals when a query is made. Concretely, $p_1$ and $p_2$ are measuring the probability of outcomes (denoted by $O$) in the following form:
	\begin{align*}
		O=\big(Z_1,a_1,(r_1),\dots,Z_T,a_T,(r_T)\big)
	\end{align*}
	where the reward $r_t$ is included only when $Z_t=1$, and we added parentheses above to indicate this point. We denote $\Pr_1$ (resp. $\Pr_2$) as the reward distribution of $I_1$ (resp. $I_2$).  We define $\-n_a=\sum_{t=1}^T Z_t \indic\{a_t=a\}$ as the number of times arm $a$ is pulled when making a query. Then, given any algorithm $\#A$, the Kullbackâ€“Leibler divergence between $p_1$ and $p_2$ can be decomposed in the following way
	\begin{align*}
		\@{KL}(p_1,p_2)&=\sum_{a=1}^A \E_{p_1}[\-n_{a}] \cdot \@{KL}\big(\Pr_1(r\given a),\Pr_2(r\given a)\big).
	\end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lem:kl-decompose}]
	We define the conditional distribution
	\begin{align*}
		\overline\Pr_1(r_t\given Z_t,a_t)
		\begin{cases}
			\Pr_1(r_t\given a_t) & \text{if }Z_t=1\\
			1 & \text{if }Z_t=0
		\end{cases},
	\end{align*}
	and similarly for $\overline\Pr_2$. Additionally, we denote $\Pr_{\#A}$ as the probability associated with algorithm $\#A$. Then, for any outcome $O$, we have
	\begin{align*}
		p_1(O)=\prod_{t=1}^T \Pr_{\#A}\big(Z_t,a_t\given Z_1,a_1,(r_1),\dots,Z_{t-1},a_{t-1},(r_{t-1})\big) \overline\Pr_1(r_t\given Z_t,a_t),
	\end{align*}
	and we can write $p_2(O)$ in a similar manner. Hence,
	\begin{align*}
		\@{KL}(p_1,p_2)&=\E_{O\sim p_1}\left[
		\log\left(
		\frac{\prod_{t=1}^T \Pr_{\#A}\big(Z_t,a_t\given Z_1,a_1,(r_1),\dots,Z_{t-1},a_{t-1},(r_{t-1})\big) \overline\Pr_1(r_t\given Z_t,a_t)}{\prod_{t=1}^T \Pr_{\#A}\big(Z_t,a_t\given Z_1,a_1,(r_1),\dots,Z_{t-1},a_{t-1},(r_{t-1})\big) \overline\Pr_2(r_t\given Z_t,a_t)}
		\right)
		\right]\\
		&=\E_{O\sim p_1}\left[
		\sum_{t=1}^T\log\left(
		\frac{\overline\Pr_1(r_t\given Z_t,a_t)}{\overline\Pr_2(r_t\given Z_t,a_t)}
		\right)
		\right]\\
		&=\E_{O\sim p_1}\left[
		\sum_{t=1}^T Z_t\log\left(
		\frac{\Pr_1(r_t\given a_t)}{\Pr_2(r_t\given a_t)}
		\right)
		\right]\\
		&=\E_{O\sim p_1}\left[
		\sum_{t=1}^T Z_t\E_{r_t\sim \Pr_1(\cdot\given a_t)}\left[\log\left(
		\frac{\Pr_1(r_t\given a_t)}{\Pr_2(r_t\given a_t)}
		\right)
		\right]
		\right]\\
		&=\E_{O\sim p_1}\left[
		\sum_{t=1}^T Z_t \cdot \@{KL}\big(\Pr_1(\cdot\given a_t),\Pr_2(\cdot\given a_t)\big)
		\right]\\
		&=\sum_{a=1}^A \E_{O\sim p_1}[\-n_{a}] \cdot \@{KL}\big(\Pr_1(\cdot\given a_t),\Pr_2(\cdot\given a_t)\big)
	\end{align*}
	where the third equality holds by the definition of $\overline\Pr_1$ and $\overline\Pr_2$.
\end{proof}

The following lemma establishes lower bounds for MAB with active queries. It presents a trade-off between the regret and the number of queries.


\begin{lemma}\label{lem:mab-tradeoff}
	Let $\+I$ denote the set of all MAB instances. Assume \Alg{} is an algorithm that achieves the following worst-case regret upper bound for some $C$ and $\beta$:
	\begin{equation*}
		\E\big[\@{Regret}_T\big]\leq C T^{1-\beta},
	\end{equation*}
	for all $I\in\+I$. 
	Then, for any MAB instance $I\in\+I$, the regret and the number of queries made by algorithm \Alg{} are lower bounded:
	\begin{equation*}
		\E\big[\@{Regret}_T\big]\geq 
		\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}}\log\left(\frac{\Delta_{a}}{4 C T^{-\beta}}\right)
		,\quad
		\E\big[\@{Queries}_T\big]\geq 
		\sum_{a\neq a^\star}\frac{\zeta}{\Delta^2_{a}}\log\left(\frac{\Delta_{a}}{4 C T^{-\beta}}\right)
	\end{equation*}
	where the coefficient $\zeta=\min_a\min\{\-r_a,1-\-r_a\}$ depends on the instance $I$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:mab-tradeoff}]
For any MAB instance $I$ and any arm $a^\dagger$, we define a corresponding MAB instance $I'$ as follows. Denote $\-r$ and $\-r'$ as the mean reward of $I$ and $I'$, respectively. For $I'$, we set the mean reward $\-r'_a=\-r_a$ for any $a\neq a^\dagger$ and $\-r'_{a^\dagger}=\-r_{a^\dagger}+2\Delta_{a^\dagger}$. Consequently, the optimal arm of $I'$ is $a^\dagger$ with margin $\Delta_{a^\dagger}$. Let $n_a$ denote the number of times that arm $a$ is pulled. We define the event
	\begin{align*}
		E=\{n_{a^\dagger}> T/2\}.
	\end{align*}
	Then, we have
	\begin{align*}
		\E_p\big[\@{Regret}_T\big]\geq \frac{T\Delta_{a^\dagger}}{2}\cdot p(E)
		,\quad
		\E_{p'}\big[\@{Regret}_T\big]\geq \frac{T\Delta_{a^\dagger}}{2}\cdot p'(E^\complement).
	\end{align*}
	Hence,
	\begin{align*}
		2CT^{1-\beta}
		\geq&\E_p\big[\@{Regret}_T\big]+\E_{p'}\big[\@{Regret}_T\big]\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \big(p(E)+p'(E^\complement)\big)\\
		=&\frac{T\Delta_{a^\dagger}}{2} \Big(1-\big(p'(E)-p(E)\big)\Big)\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \Big(1-\@{TV}\big(p,p'\big)\Big)\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \Big(1-\sqrt{1-\exp\big(-\@{KL}(p,p')\big)}\Big)\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \exp\left(-\frac{1}{2}\cdot\@{KL}(p,p')\right).
	\end{align*}
	By \Cref{lem:kl-decompose}, we have
	\begin{align*}
		\@{KL}(p,p')
		=&\sum_{a=1}^A\E_{p}[\-n_a]\cdot\@{KL}\big(\Pr(r\given a),\Pr'(r\given a)\big)\\
		=&\E_{p}[\-n_{a^\dagger}]\cdot\@{KL}\big(\Pr(r\given a^\dagger),\Pr'(r\given a^\dagger)\big)\\
		\leq&\E_{p}[\-n_{a^\dagger}]\cdot\Delta^2_{a^\dagger} \cdot 2/\zeta
	\end{align*}
	where  the last inequality is by \Cref{lem:kl-bern}. Putting the above two inequality together, we arrive at
	\begin{align*}
		\E_{p}[\-n_{a^\dagger}]\geq\frac{\zeta}{\Delta^2_{a^\dagger}}\log\left(\frac{\Delta_{a^\dagger}}{4 C T^{-\beta}}\right).
	\end{align*}
	This establishes a query lower bound for arm $a^\dagger$. Consequently, we have
	\begin{align*}
		\E[\@{Regret}_T]
		\geq 
		\sum_{a\neq a^\star} \E_{p}[\-n_{a}]\cdot\Delta_a
		\geq\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}}\log\left(\frac{\Delta_{a}}{4 C T^{-\beta}}\right),
	\end{align*}
	and similarly,
	\begin{align*}
		\E[\@{Queries}_T]
		\geq 
		\sum_{a\neq a^\star} \E_{p}[\-n_{a}]
		\geq\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}^2}\log\left(\frac{\Delta_{a}}{4 C T^{-\beta}}\right).
	\end{align*}
\end{proof}

Now we can proceed with the proof of \Cref{thm:lower-bound-stronger}.


\begin{proof}[Proof of \Cref{thm:lower-bound-stronger}]
	We provide a reduction from the multi-armed bandits with active queries to the contextual dueling bandits. Our desired lower bound for the contextual dueling bandit setting thus follows from the above lower bound for Multi-Armed Bandits (MABs). Let \Alg{} denote any algorithm for contextual dueling bandits. 
	
	\textbf{Reduction.}
	Since we focus on the multi-armed bandit where no context is involved, we just ignore the notation of context everywhere for brevity. We will start from an MAB instance, and then simulate a binary feedback and feed it to a dueling bandit algorithm \Alg{} which is used to solve the original MAB instance. Particularly, consider the MAB instance with A-many actions each with an expected reward denoted as $\bar r_a$. 

    At the beginning of iteration $t$ in the MAB instance, the learner calls the dueling algorithm \Alg{} to generate two actions $a_t$ and $b_t$. The learner plays $a_t$ at iteration $t$ to receive a reward $y_{a_t}$; the learner then moves to iteration $t+1$ to play $b_t$, and receives reward $y_{b_t}$. At the end of iteration $t+1$, the learner simulates a binary feedback by setting $o = 1$ if $y_{a_t} > y_{b_t}$; $o=-1$ if $y_{a_t} < y_{b_t}$; $o$ being $1$ or $-1$ uniform randomly if $y_{a_t}= y_{b_t}$. Then, the learner sends $(a_t, b_t, o)$ to the dueling algorithm \Alg{} to query for two actions which will be played at iterations $t+2$ and $t+3$, respectively.
 
 
	From the dueling algorithm \Alg{}'s perspective, given two actions $a$ and $b$, we can verify that the probability of seeing label 1 is $(\-r_a-\-r_b+1)/2$. So we can just specify the link function to be $\phi(d)=(d+1)/2$. As we verified earlier, the corresponding $\Phi$ is strongly convex~(\Cref{ex:sq-loss}). Moreover, since $f^\star(a,b) = \bar r_{a} - \bar r_{b}$, if we define the gap of the MAB instance as $\-\Delta\coloneqq\min_{a\neq a^\star}(\-r_{a^\star}-\-r_a)$ where $a^\star\coloneqq\argmax_i\-r_i$, then we have $\-\Delta=\Delta$ in this reduction where $\Delta$ is the definition of the gap in the dueling setting. We further note that the regret of the MAB instance is 
	$$
	\sum_{t=1}^T (\-r_{a^\star} -\-r_{a_t})+\sum_{t=1}^T (\-r_{a^\star} -\-r_{b_t}),
	$$
	which, by our definition of $f^\star$, is equivalent to the preference-based regret that occurred to the dueling algorithm \Alg{}. The number of queries is clearly equivalent as well. Thus, the regret and the query complexity of the dueling algorithm \Alg{} can be directly translated to the regret and the query complexity of the MAB instance. 
	
	Now, we are ready to prove the two claims in our statement.
	
	\textbf{Proof of the first claim.}
	We refer the reader to \citet[Theorem 15.2]{lattimore2020bandit} for a proof of the minimax regret lower bound of $\Omega(\sqrt{AT})$ for the MAB. Through the reduction outlined above, that lower bound naturally extends to the dueling bandits setting, yielding $\@{Regret}^{\@{CB}}_T\geq\Omega(\sqrt{AT})$ (otherwise, via the above reduction, we would have achieved an approach that breaks the lower bound of MAB). 

	
	\textbf{Proof of the second claim.}
We choose an arbitrary MAB for which $\zeta=\min_a\min\{\-r_a,1-\-r_a\}>0.2$ and the gaps of all arms are equal to $\Delta$. Invoking~\Cref{lem:mab-tradeoff}, we have
	\begin{align*}
		\E\big[\@{Regret}_T\big]&\geq 
		\frac{0.2(A-1)}{\Delta}\log\left(\frac{\Delta}{4CT^{-\beta}}\right)\geq \Omega\left(\frac{A}{\Delta} \right),\\
		\E\big[\@{Queries}_T\big]&\geq 
		\frac{0.2(A-1)}{\Delta^2}\log\left(\frac{\Delta}{4CT^{-\beta}}\right)\geq\Omega\left(\frac{A}{\Delta^2}\right).
	\end{align*}
	We further choose $\Delta=40CT^{-\beta}$ and $C=\sqrt{A}$, leading to
	\begin{align*}
		\E\big[\@{Regret}_T\big]
		&\geq 
		\frac{0.2(A-1)}{40\sqrt{A}}\cdot T^{\beta}
		=
		\Omega\left(\sqrt{A}\cdot T^\beta\right)
            ,\\
		\E\big[\@{Queries}_T\big]
		&\geq 
		\frac{0.2(A-1)}{1600A}\cdot T^{2\beta}
		=\Omega\left(T^{2\beta}\right).
	\end{align*}
	
	Via the reduction we have shown above, these lower bounds naturally extend to the contextual dueling bandit setting, thereby completing the proof.
\end{proof}

\subsubsection{Alternative Lower Bounds Conditioning on the Limit of Regret}\label{sec:lb-2}

In this section, we establish an analogue of \Cref{thm:lower-bound-stronger} but under a different condition. We first introduce the concept of \textit{diminishing regret}.

\begin{definition}
	We say that an algorithm guarantees a diminishing regret if for all contextual dueling bandit instances and $p>0$, it holds that $$\lim_{T\rightarrow\infty}\frac{\E[\@{Regret}^{\@{CB}}_T]}{T^p}=0.$$
\end{definition}

The lower bounds under the assumption of diminishing regret guarantees are stated as follows.

\begin{theorem}[Lower bounds]\label{thm:lower-bound2}
The following two claims hold:
\begin{enumerate}
	\item[(1)] for any algorithm, there exists an instance that leads to $\@{Regret}^{\@{CB}}_T\geq\Omega(\sqrt{AT})$;
	\item[(2)] for any gap $\Delta$ and any algorithm achieving diminishing regret, there exists an instance with gap $\Delta$ that results in $\E[\@{Regret}^{\@{CB}}_T]\geq\Omega(A/\Delta)$ and $\E[\@{Queries}^{\@{CB}}_T]\geq\Omega(A/\Delta^2)$ for sufficiently large $T$.
\end{enumerate}
\end{theorem}

We should highlight that the condition of diminishing regret (\Cref{thm:lower-bound2}) and the worst-case regret upper bounds (\Cref{thm:lower-bound,thm:lower-bound-stronger}) are not comparable in general. However, \Cref{thm:lower-bound2} is also applicable to our algorithm (\Cref{alg:cb}) since our algorithm possesses an instance-dependent regret upper bound that is clearly diminishing.



To prove \Cref{thm:lower-bound2}, we first show the following lemma, which is a variant of \Cref{lem:mab-tradeoff}.
\begin{lemma}\label{lem:mab-tradeoff2}
	Let $\+I$ denote the set of all MAB instances. Assume \Alg{} is an algorithm that achieves diminishing regret for all MAB instances in $\+I$, i.e., for any $I\in\+I$ and $p>0$, it holds that
	\begin{equation*}
		\lim_{T\rightarrow\infty}\frac{\E[\@{Regret}_T]}{T^p}=0.
	\end{equation*}
	 Then, for any MAB instance $I\in\+I$, the regret and the number of queries made by algorithm \Alg{} are lower bounded in the following manner:
	\begin{equation*}
		\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\E\big[\@{Regret}_T\big]}{\log T}\geq\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}}
		,\quad
		\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\E\big[\@{Queries}_T\big]}{\log T}\geq 
		\sum_{a\neq a^\star}\frac{\zeta}{\Delta^2_{a}}
	\end{equation*}
	where the coefficient $\zeta\coloneqq\min_a\min\{\-r_a,1-\-r_a\}$ depends on the instance $I$. Recall that $\@{Regret}_T$ and $\@{Queries}_T$ are defined in \eqref{eq:mab-regret-query}.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:mab-tradeoff2}]
The proof is similar to \Cref{lem:mab-tradeoff}. 
For any MAB instance $I\in\+I$ and any arm $a^\dagger$, we define a corresponding MAB instance $I'$ as follows. Denote $\-r$ and $\-r'$ as the mean reward of $I$ and $I'$, respectively. For $I'$, we set the mean reward $\-r'_a=\-r_a$ for any $a\neq a^\dagger$ and $\-r'_{a^\dagger}=\-r_{a^\dagger}+2\Delta_{a^\dagger}$. Consequently, the optimal arm of $I'$ is $a^\dagger$ with margin $\Delta_{a^\dagger}$. Let $n_a$ denote the number of times that arm $a$ is pulled. We define the event
	\begin{align*}
		E=\{n_{a^\dagger}> T/2\}.
	\end{align*}
	Let $p$ and $p'$ denote the probability of $I$ and $I'$, respectively. Then, we have
	\begin{align*}
		\E_p\big[\@{Regret}_T\big]\geq \frac{T\Delta_{a^\dagger}}{2}\cdot p(E)
		,\quad
		\E_{p'}\big[\@{Regret}_T\big]\geq \frac{T\Delta_{a^\dagger}}{2}\cdot p'(E^\complement)
	\end{align*}
	where $E^\complement$ means the complement of event $E$. Hence,
	\begin{align*}
		\E_p\big[\@{Regret}_T\big]+\E_{p'}\big[\@{Regret}_T\big]
		\geq&\frac{T\Delta_{a^\dagger}}{2} \big(p(E)+p'(E^\complement)\big)\\
		=&\frac{T\Delta_{a^\dagger}}{2} \Big(1-\big(p'(E)-p(E)\big)\Big)\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \Big(1-\@{TV}\big(p,p'\big)\Big)\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \Big(1-\sqrt{1-\exp\big(-\@{KL}(p,p')\big)}\Big)\\
		\geq&\frac{T\Delta_{a^\dagger}}{2} \exp\left(-\frac{1}{2}\cdot\@{KL}(p,p')\right).
	\end{align*}
	Here $\@{TV}$ denotes the total variation distance. By \Cref{lem:kl-decompose}, we have
	\begin{align*}
		\@{KL}(p,p')
		=&\sum_{a=1}^A\E_{p}[\-n_a]\cdot\@{KL}\big(\Pr(r\given a),\Pr'(r\given a)\big)\\
		=&\E_{p}[\-n_{a^\dagger}]\cdot\@{KL}\big(\Pr(r\given a^\dagger),\Pr'(r\given a^\dagger)\big)\\
		\leq&\E_{p}[\-n_{a^\dagger}]\cdot\Delta^2_{a^\dagger} \cdot 2/\zeta
	\end{align*}
	where  the last inequality is by \Cref{lem:kl-bern}. Putting it all together, we arrive at
	\begin{align*}
		\E_{p}[\-n_{a^\dagger}]\geq\frac{\zeta}{\Delta^2_{a^\dagger}}\log\left(\frac{T\Delta_{a^\dagger}}{2\Big(\E_p\big[\@{Regret}_T\big]+\E_{p'}\big[\@{Regret}_T\big]\Big)}\right).
	\end{align*}
	Taking the limit on both sides yields
	\begin{align*}
		\mathop{\lim\inf}_{T\rightarrow\infty}
		\frac{\E_{p}[\-n_{a^\dagger}]}{\log T}
		\geq&\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\zeta}{\Delta^2_{a^\dagger}}\cdot\frac{\log\left(\frac{T\Delta_{a^\dagger}}{2\Big(\E_p\big[\@{Regret}_T\big]+\E_{p'}\big[\@{Regret}_T\big]\Big)}\right)}{\log T}\\
		=&\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\zeta}{\Delta^2_{a^\dagger}}\cdot\left(1+\underbrace{\frac{\log(\Delta_{a^\dagger}/2)}{\log T}}_{\rm(i)}-\underbrace{\frac{\log\Big(\E_p\big[\@{Regret}_T\big]+\E_{p'}\big[\@{Regret}_T\big]\Big)}{\log T}}_{\rm(ii)}\right).
	\end{align*}
	Here the limit of $\rm(i)$ is clearly 0. For the limit of $\rm(ii)$, we note that by the definition of diminishing regret, for any $C>0$, there exists a $T'$ such that $\E[\@{Regret}_{T}]/T^p\leq C$ for any $T>T'$. This implies
	\begin{align*}
		\frac{\log\Big(\E_p\big[\@{Regret}_T\big]+\E_{p'}\big[\@{Regret}_T\big]\Big)}{\log T}
		\leq
		\frac{\log\Big( 2CT^p\Big)}{\log T}=\frac{\log(2C)}{\log T}+p
	\end{align*}
	for any $p>0$. Therefore, the limit of $\rm(ii)$ is also 0. Plugging these back, we obtain
	\begin{align*}
		\mathop{\lim\inf}_{T\rightarrow\infty}
		\frac{\E_{p}[\-n_{a^\dagger}]}{\log T}\geq\frac{\zeta}{\Delta^2_{a^\dagger}}.
	\end{align*}
	This establishes a query lower bound for arm $a^\dagger$. Consequently, we have
	\begin{align*}
		\mathop{\lim\inf}_{T\rightarrow\infty}
		\frac{\E[\@{Regret}_T]}{\log T}
		\geq 
		\mathop{\lim\inf}_{T\rightarrow\infty}
		\sum_{a\neq a^\star} \frac{\E_{p}[\-n_{a}]\cdot\Delta_a}{\log T}
		\geq\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}},
	\end{align*}
	and similarly,
	\begin{align*}
		\mathop{\lim\inf}_{T\rightarrow\infty}
		\frac{\E[\@{Queries}_T]}{\log T}
		\geq 
		\mathop{\lim\inf}_{T\rightarrow\infty}
		\sum_{a\neq a^\star} \frac{\E_{p}[\-n_{a}]}{\log T}
		\geq\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}^2}.
	\end{align*}
\end{proof}


Now, we proceed with the proof of \Cref{thm:lower-bound2}.

\begin{proof}[Proof of \Cref{thm:lower-bound2}]
The proof of the first claim is the same as \Cref{thm:lower-bound-stronger}, so we will omit it here. Let us now focus on the proof of the second claim. By \Cref{lem:mab-tradeoff2}, for any algorithm achieving diminishing regret, the following is true for any MAB instance:
	\begin{equation*}
		\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\E\big[\@{Regret}_T\big]}{\log T}\geq 
		\sum_{a\neq a^\star}\frac{\zeta}{\Delta_{a}},\quad
		\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\E\big[\@{Queries}_T\big]}{\log T}\geq 
		\sum_{a\neq a^\star}\frac{\zeta}{\Delta^2_{a}}.
	\end{equation*}
	We choose an arbitrary MAB for which $\zeta\geq 0.2$ and the gaps of all suboptimal arms are equal to $\Delta$. Then, for this instance, we have
	\begin{equation*}
		\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\E\big[\@{Regret}_T\big]}{\log T}\geq 
		\frac{0.2(A-1)}{\Delta},\quad
		\mathop{\lim\inf}_{T\rightarrow\infty}\frac{\E\big[\@{Queries}_T\big]}{\log T}\geq 
		\frac{0.2(A-1)}{\Delta^2}.
	\end{equation*}
	By the definition of limit, when $T$ is large enough (exceeding a certain threshold), we have
	\begin{equation*}
		\frac{\E\big[\@{Regret}_T\big]}{\log T}\geq 
		\frac{0.1(A-1)}{\Delta},\quad
		\frac{\E\big[\@{Queries}_T\big]}{\log T}\geq 
		\frac{0.1(A-1)}{\Delta^2}.
	\end{equation*}
	Via the reduction we have shown in the proof of \Cref{thm:lower-bound-stronger}, these lower bounds naturally extend to the contextual dueling bandit setting, thereby completing the proof.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:cb-general-regret}}\label{sec:pf-thm-cb-general-regret}
\begin{proof}[Proof of \Cref{thm:cb-general-regret}]

We establish the bounds for regret and the number of queries, consecutively. First, we set an arbitrary gap threshold $\epsilon > 0$. Since our algorithm is independent of $\epsilon$, we can later choose any $\epsilon$ that minimizes the upper bounds.

\textbf{Proof of regret.} 
We start with the regret upper bound. By definition, we have
\begin{align*}
	\@{Regret}^{\@{CB}}_T=\sum_{t=1}^{T}\big(
	f^\star(x_t,\pi_{f^\star}(x_t),a_t)+
				f^\star(x_t,\pi_{f^\star}(x_t),b_t)\big)	.
\end{align*}
Since $a_t$ and $b_t$ are always drawn independently from the same distribution in \Cref{alg:cb}, we only need to consider the regret of the $a_t$ part in the following proof for brevity --- multiplying the result by two would yield the overall regret.

The worst-case regret upper bound presented in  \Cref{lem:worst-case-regret-ub} doesn't reply on the gap assumption and thus remains applicable in this setting. Hence, we only need to prove the instance-dependent regret upper bound. To that end, we first need an analogue of \Cref{lem:lambda-all-0}.

\begin{lemma}\label{lem:lambda-all-0-general}
	Fix any $\epsilon > 0$. Whenever 
	$$
	2T_\epsilon + 56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))
	<
	\sqrt{AT/\beta},
	$$
	we have $\lambda_1=\lambda_2=\dots=\lambda_T=0$ with probability at least $1-\delta$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:lambda-all-0-general}]
	The proof is similar to \Cref{lem:lambda-all-0} and is via contradiction. Assume the inequality holds but there exists $t'$ for which $\lambda_{t'}=1$. Without loss of generality, we assume that $\lambda_t=0$ for all $t<t'$, namely that $t'$ is the first time that $\lambda_t$ is 1. Then by definition of $\lambda_{t'}$, we have
	\begin{align*}
		\sum_{s=1}^{t'-1} Z_sw_s\geq\sqrt{AT/\beta}.
	\end{align*}
	On the other hand, we have
	\begin{align*}
	\sum_{s=1}^{t'-1} Z_s w_s
	= &
	\sum_{s=1}^{t'-1} \indic\{\@{Gap}(x_t)\leq\epsilon\} Z_s w_s + 	\sum_{s=1}^{t'-1} \indic\{\@{Gap}(x_t)>\epsilon\} Z_s w_s\\
	\leq &
	2T_\epsilon + 56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))
	\end{align*}
	where the inequality is by \Cref{lem:sum-zw}. The above two inequalities contradicts with the conditions.
\end{proof}
Towards an instance-dependent regret upper bound, we adapt the proof of \Cref{lem:ins-depend-regret-ub} to this setting. We consider two cases. First, when 
	\begin{equation}\label{eq:regret-cases1-general}
	2T_\epsilon + 56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))
	<
	\sqrt{AT/\beta},
	\end{equation}
	we invoke \Cref{lem:lambda-all-0-general} and get that $\lambda_t=0$ for all $t\in[T]$. Hence, we have
	\begin{align*}
		\@{Regret}^{\@{CB}}_T
		=&\sum_{t=1}^{T}\big(
		f^\star(x_t,\pi_{f^\star}(x_t),a_t)+
				f^\star(x_t,\pi_{f^\star}(x_t),b_t)\big)\\
		\leq&2\sum_{t=1}^T  \indic\{\@{Gap}(x_t)\leq\epsilon\} Z_t w_t + 2 \sum_{t=1}^T  \indic\{\@{Gap}(x_t)>\epsilon\} Z_t w_t\\
		\leq& 4 T_\epsilon + 112A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))\\
		\leq& 136\beta\cdot\log(4\delta^{-1})\cdot T_\epsilon + 3808 A^2\beta^2\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log^2(4/(\delta\epsilon))
	\end{align*}
	where the first inequality is by \Cref{lem:regret-bounded-by-w} and the fact that we incur no regret when $Z_t=0$ since $f^\star\in\+F_t$. The second inequality is by \Cref{lem:sum-zw}.
	
	On the other hand, when the contrary of \eqref{eq:regret-cases1-general} holds, i.e., 
	\begin{equation}\label{eq:regret-cases2-general}
	2T_\epsilon + 56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))
	\geq
	\sqrt{AT/\beta},
	\end{equation}
	applying \Cref{lem:worst-case-regret-ub}, we have
	\begin{align*}
		\@{Regret}^{\@{CB}}_T
		\leq&68\sqrt{AT\beta}\cdot\log(4\delta^{-1})\\
		=&68\beta\cdot\log(4\delta^{-1})\cdot\sqrt{AT/\beta}\\
		\leq&68\beta\cdot\log(4\delta^{-1})\cdot
	\left(2 T_\epsilon + 56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))\right)\\
		\leq& 136\beta\cdot\log(4\delta^{-1})\cdot T_\epsilon + 3808 A^2\beta^2\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log^2(4/(\delta\epsilon))
	\end{align*}
	where we apply the condition \eqref{eq:regret-cases2-general} in the second inequality.

\textbf{Proof of the number of queries.}
To show an upper bound for the number of queries, we also consider two cases. First, when
\begin{align}\label{eq:query-case-general}
	2T_\epsilon + 56A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))
	<
	\sqrt{AT/\beta},
\end{align}
we can invoke \Cref{lem:lambda-all-0-general} and get that $\lambda_t=0$ for all $t\in[T]$. Hence, similar to the proof of \Cref{lem:query-bound}, we have
\begin{align*}
	\@{Queries}^{\@{CB}}_T
	=&\sum_{t=1}^T Z_t\\
	=&\sum_{t=1}^T Z_t\indic\{\@{Gap}(x_t) < \epsilon\}+\sum_{t=1}^T Z_t\indic\{\@{Gap}(x_t) \geq \epsilon\}\\
	=&T_\epsilon + \sum_{t=1}^TZ_t\sup_{a,b\in\+A_t}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\epsilon\right\}\\
	\leq& T_\epsilon +\sum_{t=1}^TZ_t\sum_{a,b}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\epsilon\right\}\\
	\leq& T_\epsilon +A^2\underbrace{\sum_{t=1}^TZ_t \E_{a,b\sim p_t}\indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a,b)-f'(x_t,a,b)\geq\epsilon\right\}}_{(*)}
\end{align*}
where the second inequality holds as $p_t(a)$ is uniform for any $a,b$ when $\lambda_t=0$. We apply \Cref{lem:freedman} and \Cref{lem:w-eluder} to $(*)$ and obtain
\begin{align*}
	(*)\leq& 2\sum_{t=1}^TZ_t \indic\left\{\sup_{f,f'\in\+F_t}f(x_t,a_t,b_t)-f'(x_t,a_t,b_t)\geq\epsilon\right\}+8\log(\delta^{-1})\\
	\leq& 2\left(\frac{4\beta}{\epsilon^2}+1\right)\@{dim}_E(\+F;\epsilon)+8\log(\delta^{-1})\\
	\leq& \frac{10\beta}{\epsilon^2}\cdot\@{dim}_E(\+F;\epsilon)+8\log(\delta^{-1}).
\end{align*}
Plugging this back, we obtain
\begin{align*}
	\@{Queries}^{\@{CB}}_T
	\leq& T_\epsilon + \frac{10A^2\beta}{\epsilon^2}\cdot\@{dim}_E(\+F;\epsilon)+8A^2\log(\delta^{-1})\\
	\leq& 8 T^2_\epsilon\beta/A + 6272 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\epsilon\right)}{\epsilon^2}\cdot\log^2(2/(\delta\epsilon))
\end{align*}
where the second line corresponds to the upper bound derived from the alternative case, which is shown below.

When the contrary of \eqref{eq:query-case-general} holds, i.e., 
\begin{align*}
	2T_\epsilon + 56 A^2\beta\cdot\frac{\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}\cdot\log(2/(\delta\epsilon))
	\geq
	\sqrt{AT/\beta}.
\end{align*}
Squaring both sides and leveraging the inequality $(a+b)^2\leq 2a^2+2b^2$, we obtain
\begin{align*}
8T^2_\epsilon + 6272 A^4 \beta^2 \frac{\@{dim}^2_E\left(\+F,\epsilon\right)}{\epsilon^2}\cdot\log^2(2/(\delta\epsilon))
\geq AT/\beta
\end{align*}
which leads to
\begin{equation*}
T
\leq 8 T^2_\epsilon\beta/A + 6272 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\epsilon\right)}{\epsilon^2}\cdot\log^2(2/(\delta\epsilon)).
\end{equation*}
We note that we always have $\@{Queries}^{\@{CB}}_T\leq T$ and thus 
\begin{equation*}
\@{Queries}^{\@{CB}}_T
\leq
T
\leq 8 T^2_\epsilon\beta/A + 6272 A^3 \beta^3 \frac{\@{dim}^2_E\left(\+F,\epsilon\right)}{\epsilon^2}\cdot\log^2(2/(\delta\epsilon)).
\end{equation*}

\textbf{Minimizing on $\epsilon$.}
Given that the aforementioned proofs hold for any threshold $\epsilon$, we can select the specific value of $\epsilon$ that minimizes the upper bounds. Hence, we deduce the desired result.
\end{proof}







\subsection{Proof of Theorem~\ref{thm:il-regret}}\label{sec:pf-thm-il-regret}
\begin{proof}[Proof of Theorem~\ref{thm:il-regret}]
The upper bound of the number of queries is straightforward: \Cref{alg:il} is simply running $H$ instances of \Cref{alg:cb}, so the total number of queries is simply the sum of these $H$ instances. For bounding the regret, we have
	\begin{align*}
		\@{Regret}^{\@{IL}}_T
		=&\sum_{t=1}^T V^{\pi_e}_0(x_{t,0})-V^{\pi_t}_0(x_{t,0})\\
		\leq&\sum_{h=0}^{H-1} \sum_{t=1}^T \E_{x_{t,h},a_{t,h}\sim d^{\pi_t}_{x_{t,0},h}}\Big[Q^{\pi_e}_h(x_{t,h},\pi^{\pi_e}_h(x_{t,h}))-Q^{\pi_e}_h(x_{t,h},a_{t,h})\Big]\\
		\leq&\sum_{h=0}^{H-1} \sum_{t=1}^T \E_{x_{t,h},a_{t,h}\sim d^{\pi_t}_{x_{t,0},h}}\Big[Q^{\pi_e}_h(x_{t,h},\pi_h^+(x_{t,h}))-Q^{\pi_e}_h(x_{t,h},a_{t,h})\Big]\\
		&\quad-\sum_{h=0}^{H-1} \sum_{t=1}^T \E_{x_{t,h}\sim d^{\pi_t}_{x_{t,0},h}}\Big[A^{\pi_e}_h(x_{t,h},\pi^+_h(x_{t,h}))\Big]\\
		\leq&H\cdot\E\left[\@{Regret}^{\@{CB}}_T\right]-\@{Adv}_T.
	\end{align*}
	where the first inequality holds by \Cref{lem:pdl}, and we denote $\pi^+_h(x_{t,h})=\argmax_a Q^{\pi_e}_h(x_{t,h},a)$ in the second inequality. Then, we can plug the upper bound of $\@{Regret}^{\@{CB}}_T$ (\Cref{thm:cb-regret}). Moreover, we need to take a union bound over all $h \in [H]$.
\end{proof}



















