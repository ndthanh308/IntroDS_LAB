\section{Contextual Bandits with Preference-Based Active Queries} 



\begin{algorithm}[t]
\begin{algorithmic}[1]
   \caption{Active preference qUeRy fOR contextual bAndits (AURORA)}
   \label{alg:cb}
	\REQUIRE Function class $\+F$, confidence parameter $\beta=\frac{4\Upsilon}{\alpha}+\frac{16+24\alpha}{\alpha^2}\log\big(4\delta^{-1}\log(T)\big)$.
	\STATE Online regression oracle produces $f_1$.
   	\FOR{$t=1,2,\dots,T$}
   		\STATE Learner receives context $x_t$, and computes the version space 
   		\begin{align*}
   			\+F_t\gets\Bigg\{f\in\+F:\sum_{s=1}^{t-1} Z_s\Big(f(x_s,a_s,b_s)-f_s(x_s,a_s,b_s)\Big)^2\leq \beta\Bigg\}.
   		\end{align*}
   		and the candidate arm set $\+A_t\gets\{\pi_f(x_t):\forall f\in\+F_t\}$.
   		\STATE Learner decides whether to query $Z_t\gets\indic\{|\+A_t|>1\}$.
   		\IF{$Z_t=1$}  
   			\STATE Set $w_t\gets\sup_{a,b\in\+A_t}\sup_{f,f'\in\+F_t} f(x_t,a,b)-f'
   			(x_t,a,b)$, and $\lambda_t\gets\indic\{\sum_{s=1}^{t-1}Z_s w_s\geq\sqrt{AT/\beta}\}$.
   			\IF{$\lambda_t=0$}
   				\STATE $p_t\gets\@{Uniform}(\+A_t)$.
   			\ELSE
   				\STATE $\gamma_t\gets\sqrt{AT/\beta}$.
   				\STATE Let $p_t$ be a solution of $\max_{a\in\+A_t}\sum_b f_t(x_t,a,b)p_t(b)+\frac{2}{\gamma_t p_t(a)}\leq\frac{5A}{\gamma_t}$.\alglinelabel{line:compute-p}
   			\ENDIF
   			\STATE Learner samples $a_t,b_t\sim p_t$ independently and receives the feedback $y_t$. 
			\STATE Learner feeds $((x_{t},a_{t},b_{t}),y_{t})$ to the online regression oracle which returns $f_{t+1}$.
   		\ELSE
   			\STATE Learner sets $a_t$ and $b_t$ to be the only action in $\+A_t$, and plays them. \alglinelabel{line:no-query-play-only-arm}
                \STATE $f_{t+1}\gets f_t$.
   		\ENDIF	
   	\ENDFOR 
\end{algorithmic} 
\end{algorithm}


We first present our algorithm, named \textsc{AURORA}, for contextual dueling bandits, as shown in \Cref{alg:cb}. At each round $t\in[T]$, the online regression oracle outputs a predictor $f_t$, using which the learner constructs a version space $\+F_t$ containing all functions close to past predictors on observed data. Here, the threshold 
$	
\beta$ set to $4\Upsilon/\alpha+(16+24\alpha)\log\big(4\delta^{-1}\log(T)\big)/\alpha^2
$ ensures that $f^\star\in\+F_t$ for any $t\in[T]$ with probability at least $1-\delta$ (\Cref{lem:pointwise-bound}). Thus, $\+A_t$ is non-empty for all \(t \in [T]\) and correspondingly  Line~\ref{line:no-query-play-only-arm} is well defined. The learner then forms a candidate arm set $\+A_t$ consisting of greedy arms induced by all functions in the version space. When $|\+A_t|=1$, the only arm in the set is the optimal arm since $f^\star\in\+F_t$, and thus no query is needed ($Z_t=0$). However, when $|\+A_t|>1$, any arm in $\+A_t$ could potentially be the optimal arm, and thus the learner needs to make a comparison query to obtain more information. 

Next, we explain the strategy used by the learner for making a query. Firstly, the learner computes $w_t$, which represents the ``width'' of the version space. Specifically, $w_t$ overestimates the instantaneous regret for playing any arm in $\+A_t$ (\Cref{lem:regret-bounded-by-w}). Then, the learner defines $\lambda_t$ that indicates if the estimated cumulative regret  $\sum_{s=1}^{t-1}Z_w w_s$ has exceeded $\sqrt{AT/\beta}$. Note that $Z_t$ is multiplied to $w_t$ since no regret is incurred when $Z_t=0$. The strategy to choose the actions (to be queried) for different values of $\lambda_t$ are as follows: 

\begin{itemize}[leftmargin=*]
	\item If $\lambda_t=0$, the cumulative reward has not yet exceeded $\sqrt{AT/\beta}=O(\sqrt{T})$, so the learner will explore as much as possible by uniform sampling from $\+A_t$.
	\item If $\lambda_t=1$, the regret may have reached $O(\sqrt{T})$, and therefore the learner uses a technique similar to inverse gap weighting (IGW), as inspired by \cite{saha2022efficient}, to achieve a better balance between exploration and exploitation. Specifically, the learner solves the  convex program\footnote{It is convex as it can be written as $|\+A_t|$ convex constraints: $\sum_b f_t(x_t,a,b)p_t(b)+\frac{2}{\gamma_t p_t(a)}\leq\frac{5A}{\gamma_t},\forall a\in\+A_t$.} in Line~\ref{line:compute-p}, which is feasible and whose solution $p_t$ satisfies (see \Cref{lem:igw})
	\begin{align*}
		\E_{a\sim p_t}\Big[f^\star(x_t,\pi_{f^\star}(x),a)\Big]
		=
		O\left(
		\gamma_t\E_{a,b\sim p_t}\Big[\big(f_t(x_t,a,b)-f^\star(x_t,a,b)\big)^2\Big]+\frac{A}{\gamma_t}\numberthis\label{eq:informal-igw}
		\right).
	\end{align*}
As a result of the above relation, we note that one can convert the instantaneous regret to the point-wise error between the predictor $f_t$ and the truth $f^\star$ plus an additive $A/\gamma_t$. This allows us to bound the cumulative point-wise error by the regret of the online regression oracle. In the special case, when there exists a ``reward function'' $r:\+X\times\+A\rightarrow[0,1]$ for each $f\in\+F$ such that $f(x,a,b)=r(x,a)-r(x,b)$ (\Cref{lem:ex-r-r}), the solution $p_t$ can be directly written as 
	\begin{align*}
		p_t(a)=
		\begin{cases} 
			\frac{1}{A+\gamma_t\big(r_t(x_t,\pi_{f_t}(x_t))-r_t(x_t,a)\big)} & a\neq\pi_{f_t}(x_t)\\
			1-\sum_{a'\neq\pi_{f_t}(x_t)}p_t(a') & a=\pi_{f_t}(x_t)
		\end{cases},
	\end{align*}
	where $r_t$ is the reward function associated with $f_t$, i.e., $f_t(x,a,b)=r_t(x,a)-r_t(x,b)$.
	This is the standard IGW exploration strategy \citep{foster2020beyond} and leads to the same guarantee as \eqref{eq:informal-igw} (see \Cref{lem:igw-r-version}).
\end{itemize}
\subsection{Theoretical Analysis}

Towards the theoretical guarantees of \Cref{alg:cb}, we employ two quantities to characterize a contextual bandit instance: the uniform gap and the eluder dimension, which are introduced below.
\begin{assumption}[Uniform gap]\label{asm:uniform-gap}
We assume the optimal arm $\pi_{f^\star}(x)$ induced by $f^\star$ under any context $x\in\+X$ is unique. Further, we assume a uniform gap $\Delta:=\inf_x \inf_{a\neq\pi_{f^\star}(x)} f^\star(x,\pi_{f^\star}(x),a)>0$.
\end{assumption}

We note that the existence of a uniform gap is a standard assumption in the literature of contextual bandits \citep{dani2008stochastic,abbasi2011improved,audibert2010best,garivier2019explore,foster2020beyond,foster2020instance}. Next, we introduce the eluder dimension \citep{russo2013eluder} and begin by defining ``$\epsilon$-dependence''.
\begin{definition}[$\epsilon$-dependence]
Let $\+G\subseteq\+X\rightarrow\=R$ be any function class. We say an element $x\in\+X$ is $\epsilon$-dependent on $\{x_1,x_2,\dots,x_n\}\subseteq\+X$ with respect to $\+G$ if any pair of functions $g,g'\in\+G$ satisfying $\sum_{i=1}^n(g(x_i)-g'(x_i))\leq\epsilon^2$ also satisfies $g(x)-g'(x)\leq\epsilon$. Otherwise, we say $x$ is $\epsilon$-independent of $\{x_1,x_2,\dots,x_n\}$.
\end{definition}

\begin{definition}[Eluder dimension]
	The $\epsilon$-eluder dimension of a function class $\+G\subseteq\+X\rightarrow\=R$, denoted by $\@{dim}_E(\+G,\epsilon)$, is the length $d$ of the longest sequence of elements in $\+X$ satisfying that there exists some $\epsilon'\geq\epsilon$ such that every element in the sequence is $\epsilon'$-independent of its predecessors.
\end{definition}
Eluder dimension is a standard complexity measure for function classes and has been used in the literature of bandits and RL extensively \citep{chen2022human,osband2014model,wang2020reinforcement,foster2020instance,wen2013efficient,jain2015learning,ayoub2020model,ishfaq2021randomized,huang2021towards}. Examples where the eluder dimension is small include linear functions, generalized linear models, and functions in Reproducing Kernel Hilbert Space (RKHS).  

Given these quantities, we are ready to state our main results. The proofs are provided in \Cref{sec:missing-pf}.

\begin{theorem}\label{thm:cb-regret}
Under \Cref{asm:properties,asm:oracle-regret,asm:uniform-gap}, \Cref{alg:cb} guarantees the following upper bounds of the regret and the number of queries:
\begin{align*}
	&\@{Regret}_T^{\@{CB}}=
	\widetilde{O}\left(
	\min\left\{
	\sqrt{AT\beta}
	,\;
	\frac{A^2\beta^2\@{dim}_E\left(\+F,\Delta\right)}{\Delta}
	\right\}
	\right),\\
	&\@{Queries}_T^{\@{CB}}=
	\widetilde{O}\left(
	\min\left\{T
	,\;
	\frac{A^3\beta^3 \@{dim}^2_E\left(\+F,\Delta\right)}{\Delta^2}
	\right\}
	\right)
\end{align*}
with probability at least $1-\delta$. We recall that $\beta=O(\alpha^{-1}\Upsilon+\alpha^{-2}\log(\delta^{-1}\log(T)))$, and $\alpha$ denotes the coefficient of strong convexity of $\Phi$. We have hidden logarithmic terms in the upper bounds for brevity. 
\end{theorem}
When the loss $\ell_\phi$ is either square loss or logistic loss (\Cref{ex:sq-loss,ex:log-loss}), the parameter $\beta$ is logarithmic in $T$. In such cases, the regret is $\~O(\min\{\sqrt{T},\@{dim}_E\left(\+F,\Delta\right)/\Delta\})$ and the number of queries is $\~O(\min\{T,\@{dim}^2_E(\+F,\Delta)/\Delta^2\})$, ignoring $A$ and logarithmic terms. Both consist of two components: the worst-case and the instance-dependent upper bounds. The worst-case bound provides a guarantee under all circumstances, while the instance-dependent one may significantly improve the upper bound when the underlying problem is well-behaved (i.e., has a small eluder dimension and a large gap).


\paragraph{Intuition of proofs.} We next provide intuition for why our algorithm has the aforementioned theoretical guarantees. First, we observe that from the definition of $\lambda_t$, the left term inside the indicator is non-decreasing, which allows us to divide rounds into two phases. In the first phase, $\lambda_t$ is always 0, and then at some point, it changes to 1 and remains 1 for the rest rounds. After realizing this, we first explain the intuition of the worst-case regret. In the first phase, as $w_t$ is an overestimate of the instantaneous regret (see \Cref{lem:regret-bounded-by-w}), the accumulated regret in this phase cannot exceed $O(\sqrt{T})$. In the second phase, we adapt the analysis of IGW to this scenario to obtain an $O(\sqrt{T})$ upper bound. A similar technique has been used in ~\cite{saha2022efficient,foster2020instance}. As the regret in both phases is at most $O(\sqrt{T})$, the total regret cannot exceed $O(\sqrt{T})$. Next, we explain the intuition of instance-dependent regret. Due to the existence of a uniform gap $\Delta$, we can first prove that as long as $|\+A_t|>1$, we must have $w_t\geq\Delta$ (see \Cref{lem:width-lower-bound}). This means that for all rounds that may incur regret, the corresponding width is at least $\Delta$. However, this cannot happen too many times as this frequency is bounded by the eluder dimension, which leads to an instance-dependent regret upper bound. Leveraging a similar technique, we can also obtain an upper bound on the number of queries. \looseness=-1

\paragraph{Comparion to \textsc{MinMaxDB} \citep{saha2022efficient}.}
In this prior work, the authors assume that $\Pr(y=1\given x,a,b)=(f^\star(x,a,b)+1)/2$, which is a specification of our feedback model (\Cref{ex:sq-loss}). 
While our worst-case regret bound matches their regret bound, our paper improves upon their results by having an additional instance-dependent regret bound that depends on the eluder dimension and gap. Furthermore, we also provide bounds on the query complexity which could be small for benign instances while \textsc{MinMaxDB} simply queries on every round. 

\paragraph{Comparion to \textsc{AdaCB} \citep{foster2020instance}.}
Our method shares some similarities with \citet{foster2020instance}, especially in terms of theoretical results, but differs in two aspects: (1) they assume regular contextual bandits where the learner observes the reward directly, while we assume preference feedback, and (2) they assume a stochastic setting where contexts are drawn i.i.d., but we assume that the context is adversarially chosen. While these two settings may not be directly comparable, it should be noted that \citep{foster2020instance} do not aim to minimize query complexity. 

\paragraph{Lower bounds.}
To understand whether our algorithm attains tight upper bounds, we provide the following lower bound which follows from a reduction from regular multi-armed bandits to contextual dueling bandits.  
\begin{theorem}[Lower bounds]\label{thm:lower-bound}
The following two claims hold:
\begin{enumerate}
\item[(1)] For any algorithm, there exists an instance that leads to $\@{Regret}^{\@{CB}}_T=\Omega(\sqrt{AT})$;
\item[(2)] For any algorithm achieving a worse-case expected regret upper bound in the form of $\E[\@{Regret}^{\@{CB}}_T]= O(\sqrt{AT})$, there exists an instance with gap $\Delta=\sqrt{A/T}$ that results in $\E[\@{Regret}^{\@{CB}}_T]=\Omega(A/\Delta)$ and $\E[\@{Queries}^{\@{CB}}_T]=\Omega(A/\Delta^2)=\Omega(T)$.
\end{enumerate}
\end{theorem}

By relating these lower bounds to \Cref{thm:cb-regret}, we conclude that our algorithm achieves a tight dependence on the gap $\Delta$ and $T$, up to logarithmic factors, in both the regret and query complexity upper bounds. Furthermore, as an additional contribution, we establish an alternative lower bound in Section \ref{sec:lb-2} by conditioning on the limit of regret, rather than the worst-case regret as assumed in Theorem \ref{thm:lower-bound}. %



\paragraph{Results without the uniform gap assumption.}
We highlight that \Cref{thm:cb-regret} can naturally extend to scenarios where a uniform gap does not exist (i.e., when \Cref{asm:uniform-gap} is not satisfied) without any modifications to the algorithm. The result is stated below, which is analogous to \Cref{thm:cb-regret}.
\begin{theorem}\label{thm:cb-general-regret}
Under \Cref{asm:properties,asm:oracle-regret}, \Cref{alg:cb} guarantees the following upper bounds of the regret and the number of queries:
\begin{align*}
	&\@{Regret}_T^{\@{CB}}=
	\widetilde{O}\left(
	\min\left\{
	\sqrt{AT\beta}
	,\,
	\min_{\epsilon>0}\left\{
	T_\epsilon\beta+\frac{A^2\beta^2\@{dim}_E\left(\+F,\epsilon\right)}{\epsilon}
	\right\}\right\}
	\right),\\
	&\@{Queries}_T^{\@{CB}}=
	\widetilde{O}\left(
	\min\left\{
	T
	,\,
	\min_{\epsilon>0}\left\{
	T_\epsilon^2\beta/A+\frac{A^3\beta^3 \@{dim}^2_E\left(\+F,\epsilon\right)}{\epsilon^2}
	\right\}	\right\}
	\right)
\end{align*}
with probability at least $1-\delta$. Here we define the gap of context $x$ as $\@{Gap}(x)\coloneqq\min_{a\neq\pi_{f^\star}(x)} f^\star(x,\pi_{f^\star}(x), a)$ and the number of rounds where contexts have small gap as $T_\epsilon\coloneqq\sum_{t=1}^T \indic\{\@{Gap}(x_t)\leq\epsilon\}$. We also recall that $\beta=O(\alpha^{-1}\Upsilon+\alpha^{-2}\log(\delta^{-1}\log(T)))$, and $\alpha$ denotes the coefficient of strong convexity of $\Phi$. %
\end{theorem}
Compared to \Cref{thm:cb-regret}, the above result has an extra gap-dependent term defined as $T_\epsilon$. Here $\epsilon$ denotes a gap threshold, and $T_\epsilon$ measures how many times the context falls into a small-gap region. We highlight that $T_\epsilon$ is small under certain conditions such as the Tsybakov noise condition \citep{tsybakov2004optimal}. It is also worth mentioning that our algorithm is agnostic to $\epsilon$, thus allowing us to take the minimum over all $\epsilon>0$. 

\paragraph{Comparion to \textsc{SAGE-Bandit} \citep{sekhari2023selective}.}
\Cref{thm:cb-general-regret} bears similarity to Theorem 4 in \citet{sekhari2023selective}, which examines active queries in contextual bandits with standard reward signal (\(0-1\) reward). It is worth noting that although our result looks slightly worse in terms of the factor $A$ (the number of actions), we believe that this inferiority is reasonable since our approach requires two actions to form a query, thus analytically expanding the action space to $\+A^2$. Whether this dependency can be improved remains a question for future investigation. 



























