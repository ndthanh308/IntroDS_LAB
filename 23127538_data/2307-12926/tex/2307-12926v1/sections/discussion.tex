\section{Discussion and Future Work} 

We presented interactive decision-making algorithms that learn from preference-based feedback while minimizing query complexity. Our algorithms for contextual bandits and imitation learning share worst-case regret bounds similar to the bounds of the state-of-art algorithms in standard settings while maintaining instance-dependent regret bounds and query complexity bounds. Notably, our imitation learning algorithm can outperform suboptimal experts, matching the result of \citep{ross2014reinforcement,sun2017deeply}, which operates under much stronger feedback. \looseness=-1


In terms of future work,  we believe our result on contextual dueling bandits can be extended to the stochastic setting where we may replace the eluder dimension with the value function disagreement coefficient \citep{foster2020instance}, which is typically smaller than the eluder dimension, and replace the online regression oracle by a supervised-learning batch regression oracle. We also conjecture that the dependence on the eluder dimension in the query complexity bound can be improved. Finally, another interesting direction is to develop practical implementations of our proposed algorithms. \looseness=-1

\subsection*{Acknowledgements} 
AS acknowledges support from the Simons Foundation and NSF through award DMS-2031883, as well as from the DOE through award DE-SC0022199. KS acknowledges support from NSF CAREER Award 1750575, and LinkedIn-Cornell grant. 

