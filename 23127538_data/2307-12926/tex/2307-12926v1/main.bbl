\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~1, 2004.

\bibitem[Agarwal(2013)]{agarwal2013selective}
Alekh Agarwal.
\newblock Selective sampling algorithms for cost-sensitive multiclass
  prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  1220--1228. PMLR, 2013.

\bibitem[Agarwal et~al.(2019)Agarwal, Jiang, Kakade, and
  Sun]{agarwal2019reinforcement}
Alekh Agarwal, Nan Jiang, Sham~M Kakade, and Wen Sun.
\newblock Reinforcement learning: Theory and algorithms.
\newblock \emph{CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, pages
  10--4, 2019.

\bibitem[Ailon et~al.(2014)Ailon, Karnin, and Joachims]{ailon2014reducing}
Nir Ailon, Zohar Karnin, and Thorsten Joachims.
\newblock Reducing dueling bandits to cardinal bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  856--864. PMLR, 2014.

\bibitem[Audibert et~al.(2010)Audibert, Bubeck, and Munos]{audibert2010best}
Jean-Yves Audibert, S{\'e}bastien Bubeck, and R{\'e}mi Munos.
\newblock Best arm identification in multi-armed bandits.
\newblock In \emph{COLT}, pages 41--53, 2010.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  463--474. PMLR, 2020.

\bibitem[Bengs et~al.(2021)Bengs, Busa-Fekete, El~Mesaoudi-Paul, and
  H{\"u}llermeier]{bengs2021preference}
Viktor Bengs, R{\'o}bert Busa-Fekete, Adil El~Mesaoudi-Paul, and Eyke
  H{\"u}llermeier.
\newblock Preference-based online learning with dueling bandits: A survey.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 278--385, 2021.

\bibitem[Biyik and Sadigh(2018)]{biyik2018batch}
Erdem Biyik and Dorsa Sadigh.
\newblock Batch active preference-based learning of reward functions.
\newblock In \emph{Conference on robot learning}, pages 519--528. PMLR, 2018.

\bibitem[Bradley and Terry(1952)]{bradley1952rank}
Ralph~Allan Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and
  Niekum]{brown2019extrapolating}
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{International conference on machine learning}, pages
  783--792. PMLR, 2019.

\bibitem[Brown et~al.(2020)Brown, Goo, and Niekum]{brown2020better}
Daniel~S Brown, Wonjoon Goo, and Scott Niekum.
\newblock Better-than-demonstrator imitation learning via automatically-ranked
  demonstrations.
\newblock In \emph{Conference on robot learning}, pages 330--359. PMLR, 2020.

\bibitem[Busa-Fekete et~al.(2014)Busa-Fekete, Sz{\"o}r{\'e}nyi, Weng, Cheng,
  and H{\"u}llermeier]{busa2014preference}
R{\'o}bert Busa-Fekete, Bal{\'a}zs Sz{\"o}r{\'e}nyi, Paul Weng, Weiwei Cheng,
  and Eyke H{\"u}llermeier.
\newblock Preference-based reinforcement learning: evolutionary direct policy
  search using a preference-based racing algorithm.
\newblock \emph{Machine learning}, 97:\penalty0 327--351, 2014.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Cesa-Bianchi et~al.(2005)Cesa-Bianchi, Lugosi, and
  Stoltz]{cesa2005minimizing}
Nicolo Cesa-Bianchi, G{\'a}bor Lugosi, and Gilles Stoltz.
\newblock Minimizing regret with label efficient prediction.
\newblock \emph{IEEE Transactions on Information Theory}, 51\penalty0
  (6):\penalty0 2152--2162, 2005.

\bibitem[Chang et~al.(2015)Chang, Krishnamurthy, Agarwal, Daum{\'e}~III, and
  Langford]{chang2015learning}
Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum{\'e}~III, and John
  Langford.
\newblock Learning to search better than your teacher.
\newblock In \emph{International Conference on Machine Learning}, pages
  2058--2066. PMLR, 2015.

\bibitem[Chen et~al.(2022)Chen, Zhong, Yang, Wang, and Wang]{chen2022human}
Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang.
\newblock Human-in-the-loop: Provably efficient preference-based reinforcement
  learning with general function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  3773--3793. PMLR, 2022.

\bibitem[Cheng and Boots(2018)]{cheng2018convergence}
Ching-An Cheng and Byron Boots.
\newblock Convergence of value aggregation for imitation learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1801--1809. PMLR, 2018.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chu and Ghahramani(2005)]{chu2005preference}
Wei Chu and Zoubin Ghahramani.
\newblock Preference learning with gaussian processes.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 137--144, 2005.

\bibitem[Cohn et~al.(2011)Cohn, Durfee, and Singh]{cohn2011comparing}
Robert Cohn, Edmund Durfee, and Satinder Singh.
\newblock Comparing action-query strategies in semi-autonomous agents.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~25, pages 1102--1107, 2011.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \emph{21st Annual Conference on Learning Theory}, pages 355--366,
  2008.

\bibitem[Daum{\'e} et~al.(2009)Daum{\'e}, Langford, and Marcu]{daume2009search}
Hal Daum{\'e}, John Langford, and Daniel Marcu.
\newblock Search-based structured prediction.
\newblock \emph{Machine learning}, 75:\penalty0 297--325, 2009.

\bibitem[Dekel et~al.(2012)Dekel, Gentile, and Sridharan]{dekel2012selective}
Ofer Dekel, Claudio Gentile, and Karthik Sridharan.
\newblock Selective sampling and active learning from single and multiple
  {{experts}}.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 2655--2697, 2012.

\bibitem[Du et~al.(2019)Du, Luo, Wang, and Zhang]{du2019provably}
Simon~S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Dud{\'\i}k et~al.(2015)Dud{\'\i}k, Hofmann, Schapire, Slivkins, and
  Zoghi]{dudik2015contextual}
Miroslav Dud{\'\i}k, Katja Hofmann, Robert~E Schapire, Aleksandrs Slivkins, and
  Masrour Zoghi.
\newblock Contextual dueling bandits.
\newblock In \emph{Conference on Learning Theory}, pages 563--587. PMLR, 2015.

\bibitem[Foster and Rakhlin(2020)]{foster2020beyond}
Dylan Foster and Alexander Rakhlin.
\newblock Beyond ucb: Optimal and efficient contextual bandits with regression
  oracles.
\newblock In \emph{International Conference on Machine Learning}, pages
  3199--3210. PMLR, 2020.

\bibitem[Foster et~al.(2018{\natexlab{a}})Foster, Agarwal, Dud{\'\i}k, Luo, and
  Schapire]{foster2018practical}
Dylan Foster, Alekh Agarwal, Miroslav Dud{\'\i}k, Haipeng Luo, and Robert
  Schapire.
\newblock Practical contextual bandits with regression oracles.
\newblock In \emph{International Conference on Machine Learning}, pages
  1539--1548. PMLR, 2018{\natexlab{a}}.

\bibitem[Foster et~al.(2021)Foster, Rakhlin, Simchi-Levi, and
  Xu]{foster2020instance}
Dylan Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu.
\newblock Instance-dependent complexity of contextual bandits and reinforcement
  learning: A disagreement-based perspective.
\newblock In \emph{Conference on Learning Theory}, pages 2059--2059. PMLR,
  2021.

\bibitem[Foster et~al.(2018{\natexlab{b}})Foster, Kale, Luo, Mohri, and
  Sridharan]{foster2018logistic}
Dylan~J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan.
\newblock Logistic regression: The importance of being improper.
\newblock In \emph{Conference On Learning Theory}, pages 167--208. PMLR,
  2018{\natexlab{b}}.

\bibitem[F{\"u}rnkranz et~al.(2012)F{\"u}rnkranz, H{\"u}llermeier, Cheng, and
  Park]{furnkranz2012preference}
Johannes F{\"u}rnkranz, Eyke H{\"u}llermeier, Weiwei Cheng, and Sang-Hyeun
  Park.
\newblock Preference-based reinforcement learning: a formal framework and a
  policy iteration algorithm.
\newblock \emph{Machine learning}, 89:\penalty0 123--156, 2012.

\bibitem[Garivier et~al.(2019)Garivier, M{\'e}nard, and
  Stoltz]{garivier2019explore}
Aur{\'e}lien Garivier, Pierre M{\'e}nard, and Gilles Stoltz.
\newblock Explore first, exploit next: The true shape of regret in bandit
  problems.
\newblock \emph{Mathematics of Operations Research}, 44\penalty0 (2):\penalty0
  377--399, 2019.

\bibitem[Hanneke and Yang(2015)]{hanneke2015minimax}
Steve Hanneke and Liu Yang.
\newblock Minimax analysis of active learning.
\newblock \emph{J. Mach. Learn. Res.}, 16\penalty0 (1):\penalty0 3487--3602,
  2015.

\bibitem[Hanneke and Yang(2021)]{hanneke2021toward}
Steve Hanneke and Liu Yang.
\newblock Toward a general theory of online selective sampling: Trading off
  mistakes and queries.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3997--4005. PMLR, 2021.

\bibitem[He et~al.(2021)He, Zhou, and Gu]{he2021logarithmic}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4171--4180. PMLR, 2021.

\bibitem[Huang et~al.(2022)Huang, Lee, Wang, and Yang]{huang2021towards}
Baihe Huang, Jason~D Lee, Zhaoran Wang, and Zhuoran Yang.
\newblock Towards general function approximation in zero-sum markov games.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Ishfaq et~al.(2021)Ishfaq, Cui, Nguyen, Ayoub, Yang, Wang, Precup, and
  Yang]{ishfaq2021randomized}
Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang,
  Doina Precup, and Lin Yang.
\newblock Randomized exploration in reinforcement learning with general value
  function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4607--4616. PMLR, 2021.

\bibitem[Jain et~al.(2015)Jain, Sharma, Joachims, and Saxena]{jain2015learning}
Ashesh Jain, Shikhar Sharma, Thorsten Joachims, and Ashutosh Saxena.
\newblock Learning preferences for manipulation tasks from online coactive
  feedback.
\newblock \emph{The International Journal of Robotics Research}, 34\penalty0
  (10):\penalty0 1296--1313, 2015.

\bibitem[Jin and Luo(2020)]{jin2020simultaneously}
Tiancheng Jin and Haipeng Luo.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 16557--16566, 2020.

\bibitem[Kakade and Tewari(2008)]{kakade2008generalization}
Sham~M Kakade and Ambuj Tewari.
\newblock On the generalization ability of online strongly convex programming
  algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem[Komiyama et~al.(2015)Komiyama, Honda, Kashima, and
  Nakagawa]{komiyama2015regret}
Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa.
\newblock Regret lower bound and optimal algorithm in dueling bandit problem.
\newblock In \emph{Conference on learning theory}, pages 1141--1154. PMLR,
  2015.

\bibitem[Krishnamurthy et~al.(2017)Krishnamurthy, Agarwal, Huang,
  Daum{\'e}~III, and Langford]{krishnamurthy2017active}
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daum{\'e}~III, and John
  Langford.
\newblock Active learning for cost-sensitive classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  1915--1924. PMLR, 2017.

\bibitem[Langford and Zhang(2007)]{langford2007epoch}
John Langford and Tong Zhang.
\newblock The epoch-greedy algorithm for multi-armed bandits with side
  information.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Laskey et~al.(2016)Laskey, Staszak, Hsieh, Mahler, Pokorny, Dragan,
  and Goldberg]{laskey2016shiv}
Michael Laskey, Sam Staszak, Wesley Yu-Shu Hsieh, Jeffrey Mahler, Florian~T
  Pokorny, Anca~D Dragan, and Ken Goldberg.
\newblock Shiv: Reducing supervisor burden in dagger using support vectors for
  efficient learning from demonstrations in high dimensional state spaces.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 462--469. IEEE, 2016.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2021{\natexlab{a}})Lee, Smith, Dragan, and
  Abbeel]{lee2021b}
Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel.
\newblock B-pref: Benchmarking preference-based reinforcement learning.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021{\natexlab{a}}.

\bibitem[Lee et~al.(2021{\natexlab{b}})Lee, Smith, and Abbeel]{lee2021pebble}
Kimin Lee, Laura~M Smith, and Pieter Abbeel.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock In \emph{International Conference on Machine Learning}, pages
  6152--6163. PMLR, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Yang, and Wang]{li2023reinforcement}
Zihao Li, Zhuoran Yang, and Mengdi Wang.
\newblock Reinforcement learning with human feedback: Learning dynamic choices
  via pessimism.
\newblock \emph{arXiv preprint arXiv:2305.18438}, 2023.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Lykouris et~al.(2021)Lykouris, Simchowitz, Slivkins, and
  Sun]{lykouris2021corruption}
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun.
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In \emph{Conference on Learning Theory}, pages 3242--3245. PMLR,
  2021.

\bibitem[Myers et~al.(2023)Myers, B{\i}y{\i}k, and Sadigh]{myers2023active}
Vivek Myers, Erdem B{\i}y{\i}k, and Dorsa Sadigh.
\newblock Active reward learning from online preferences.
\newblock \emph{arXiv preprint arXiv:2302.13507}, 2023.

\bibitem[Novoseller et~al.(2020)Novoseller, Wei, Sui, Yue, and
  Burdick]{novoseller2020dueling}
Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick.
\newblock Dueling posterior sampling for preference-based reinforcement
  learning.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  1029--1038. PMLR, 2020.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv}, 2023.

\bibitem[Osa et~al.(2018)Osa, Pajarinen, Neumann, Bagnell, Abbeel, Peters,
  et~al.]{osa2018algorithmic}
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J~Andrew Bagnell, Pieter Abbeel,
  Jan Peters, et~al.
\newblock An algorithmic perspective on imitation learning.
\newblock \emph{Foundations and Trends{\textregistered} in Robotics},
  7\penalty0 (1-2):\penalty0 1--179, 2018.

\bibitem[Osband and Van~Roy(2014)]{osband2014model}
Ian Osband and Benjamin Van~Roy.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Pacchiano et~al.(2021)Pacchiano, Saha, and Lee]{pacchiano2021dueling}
Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee.
\newblock Dueling rl: reinforcement learning with trajectory preferences.
\newblock \emph{arXiv preprint arXiv:2111.04850}, 2021.

\bibitem[Rakhlin and Sridharan(2014)]{rakhlin2014online}
Alexander Rakhlin and Karthik Sridharan.
\newblock Online non-parametric regression.
\newblock In \emph{Conference on Learning Theory}, pages 1232--1264. PMLR,
  2014.

\bibitem[Ross and Bagnell(2014)]{ross2014reinforcement}
Stephane Ross and J~Andrew Bagnell.
\newblock Reinforcement and imitation learning via interactive no-regret
  learning.
\newblock \emph{arXiv preprint arXiv:1406.5979}, 2014.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Ross et~al.(2013)Ross, Melik-Barkhudarov, Shankar, Wendel, Dey,
  Bagnell, and Hebert]{ross2013learning}
St{\'e}phane Ross, Narek Melik-Barkhudarov, Kumar~Shaurya Shankar, Andreas
  Wendel, Debadeepta Dey, J~Andrew Bagnell, and Martial Hebert.
\newblock Learning monocular reactive uav control in cluttered natural
  environments.
\newblock In \emph{2013 IEEE international conference on robotics and
  automation}, pages 1765--1772. IEEE, 2013.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Sadigh et~al.(2017)Sadigh, Dragan, Sastry, and
  Seshia]{sadigh2017active}
Dorsa Sadigh, Anca~D Dragan, Shankar Sastry, and Sanjit~A Seshia.
\newblock \emph{Active preference-based learning of reward functions}.
\newblock 2017.

\bibitem[Saha and Gaillard(2021)]{saha2021dueling}
Aadirupa Saha and Pierre Gaillard.
\newblock Dueling bandits with adversarial sleeping.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27761--27771, 2021.

\bibitem[Saha and Gaillard(2022)]{saha2022versatile}
Aadirupa Saha and Pierre Gaillard.
\newblock Versatile dueling bandits: Best-of-both world analyses for learning
  from relative preferences.
\newblock In \emph{International Conference on Machine Learning}, pages
  19011--19026. PMLR, 2022.

\bibitem[Saha and Krishnamurthy(2022)]{saha2022efficient}
Aadirupa Saha and Akshay Krishnamurthy.
\newblock Efficient and optimal algorithms for contextual dueling bandits under
  realizability.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 968--994. PMLR, 2022.

\bibitem[Saha et~al.(2023)Saha, Pacchiano, and Lee]{saha2023dueling}
Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee.
\newblock Dueling rl: Reinforcement learning with trajectory preferences.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 6263--6289. PMLR, 2023.

\bibitem[Sekhari et~al.(2023)Sekhari, Sridharan, Sun, and
  Wu]{sekhari2023selective}
Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu.
\newblock Selective sampling and imitation learning via online regression.
\newblock \emph{arXiv preprint arXiv:2307.04998}, 2023.

\bibitem[Simchi-Levi and Xu(2022)]{simchi2022bypassing}
David Simchi-Levi and Yunzong Xu.
\newblock Bypassing the monster: A faster and simpler optimal algorithm for
  contextual bandits under realizability.
\newblock \emph{Mathematics of Operations Research}, 47\penalty0 (3):\penalty0
  1904--1931, 2022.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3008--3021, 2020.

\bibitem[Sun et~al.(2017)Sun, Venkatraman, Gordon, Boots, and
  Bagnell]{sun2017deeply}
Wen Sun, Arun Venkatraman, Geoffrey~J Gordon, Byron Boots, and J~Andrew
  Bagnell.
\newblock Deeply aggrevated: Differentiable imitation learning for sequential
  prediction.
\newblock In \emph{International conference on machine learning}, pages
  3309--3318. PMLR, 2017.

\bibitem[Taranovic et~al.(2022)Taranovic, Kupcsik, Freymuth, and
  Neumann]{taranovic2022adversarial}
Aleksandar Taranovic, Andras~Gabor Kupcsik, Niklas Freymuth, and Gerhard
  Neumann.
\newblock Adversarial imitation learning with preferences.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Tsybakov(2004)]{tsybakov2004optimal}
Alexander~B Tsybakov.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 135--166,
  2004.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6123--6135, 2020.

\bibitem[Wen and Van~Roy(2013)]{wen2013efficient}
Zheng Wen and Benjamin Van~Roy.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Wirth and F{\"u}rnkranz(2014)]{wirth2014learning}
Christian Wirth and Johannes F{\"u}rnkranz.
\newblock On learning from game annotations.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in
  Games}, 7\penalty0 (3):\penalty0 304--316, 2014.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz,
  et~al.]{wirth2017survey}
Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes F{\"u}rnkranz, et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.

\bibitem[Wu and Liu(2016)]{wu2016double}
Huasen Wu and Xin Liu.
\newblock Double thompson sampling for dueling bandits.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Wu et~al.(2023)Wu, Jin, Lou, Farnoud, and Gu]{wu2023borda}
Yue Wu, Tao Jin, Hao Lou, Farzad Farnoud, and Quanquan Gu.
\newblock Borda regret minimization for generalized linear dueling bandits.
\newblock \emph{arXiv preprint arXiv:2303.08816}, 2023.

\bibitem[Xu et~al.(2020)Xu, Wang, Yang, Singh, and Dubrawski]{xu2020preference}
Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski.
\newblock Preference-based reinforcement learning with finite-time guarantees.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18784--18794, 2020.

\bibitem[Yue and Joachims(2011)]{yue2011beat}
Yisong Yue and Thorsten Joachims.
\newblock Beat the mean bandit.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 241--248. Citeseer, 2011.

\bibitem[Yue et~al.(2012)Yue, Broder, Kleinberg, and Joachims]{yue2012k}
Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims.
\newblock The k-armed dueling bandits problem.
\newblock \emph{Journal of Computer and System Sciences}, 78\penalty0
  (5):\penalty0 1538--1556, 2012.

\bibitem[Zhan et~al.(2023)Zhan, Uehara, Sun, and Lee]{zhan2023query}
Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason~D Lee.
\newblock How to query human feedback efficiently in rl?
\newblock \emph{arXiv preprint arXiv:2305.18505}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Carroll, Bobu, and Dragan]{zhang2022time}
David Zhang, Micah Carroll, Andreea Bobu, and Anca Dragan.
\newblock Time-efficient reward learning via visually assisted cluster ranking.
\newblock \emph{arXiv preprint arXiv:2212.00169}, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Jiao, and Jordan]{zhu2023principled}
Banghua Zhu, Jiantao Jiao, and Michael~I Jordan.
\newblock Principled reinforcement learning with human feedback from pairwise
  or $ k $-wise comparisons.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2023.

\bibitem[Zhu and Nowak(2022)]{zhu2022efficient}
Yinglun Zhu and Robert Nowak.
\newblock Efficient active learning with abstention.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 35379--35391, 2022.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Brian~D Ziebart, Andrew Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the 23rd national conference on Artificial
  intelligence-Volume 3}, pages 1433--1438, 2008.

\bibitem[Zoghi et~al.(2014)Zoghi, Whiteson, Munos, and
  Rijke]{zoghi2014relative}
Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke.
\newblock Relative upper confidence bound for the k-armed dueling bandit
  problem.
\newblock In \emph{International conference on machine learning}, pages 10--18.
  PMLR, 2014.

\end{thebibliography}
