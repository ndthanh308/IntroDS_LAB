\documentclass[11pt, letterpaper]{article} %
\usepackage{times}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xspace}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color, colortbl}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{bm}
\usepackage{fullpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage[colorlinks=true, linkcolor=red, citecolor=blue, urlcolor=WildStrawberry]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{natbib}
\usepackage{mathrsfs}
\usepackage{parskip}

\usepackage{mkolar_definitions}

\input{rwstyle.tex}


\newcommand{\alglinelabel}{
  \addtocounter{ALC@line}{-1}
  \refstepcounter{ALC@line}
  \label
}


\Crefname{assumption}{Assumption}{Assumptions}

\author{%
Ayush Sekhari$^{1}$ \quad Karthik Sridharan$^{2}$ \quad Wen Sun$^{2}$ \quad Runzhe Wu$^{2}$
\vspace{5pt}
\\
\normalsize{$^1$MIT \quad $^2$Cornell University}
\vspace{5pt}
\\
\small{\texttt{sekhari@mit.edu \quad \{ks999,ws455,rw646\}@cornell.edu}
}}


\date{}


\title{Contextual Bandits and Imitation Learning via Preference-Based Active Queries\thanks{Authors are listed in alphabetical order of their last names.}} 

\bibpunct{(}{)}{;}{a}{,}{,}

\usepackage[suppress]{color-edits} 
\addauthor{as}{red}
\addauthor{rw}{orange}

\begin{document}

\sloppy 

\maketitle 

\begin{abstract}
We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class \asedit{for choosing its actions and deciding when to query}. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as \(O(\min\{\sqrt{T}, d/\Delta\})\), where \(T\) represents the number of interactions, \(d\) represents the eluder dimension of the function class, and \(\Delta\) represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of \(\Delta\), and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only \(O(\min\{T, d^2/\Delta^2\})\) queries to the expert. 

We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length \(H\) each, and provide similar guarantees for regret and query complexity. \asedit{The regret bound for our imitation learning algorithm, which relies on preference-based feedback, matches the prior results in interactive imitation learning \citep{ross2014reinforcement} that require access to the expert's actions as well as reward signals. Furthermore, we show that our algorithm enjoys improved query complexity bounds. Interestingly, in some cases, our algorithm for imitation learning via preference-feedback can even learn to outperform the underlying expert thus highlighting a practical benefit of considering preference-based feedback in imitation learning.}  
 
\end{abstract} 

\input{sections/intro}
\input{sections/related_work}
\input{sections/prelim}
\input{sections/duel_CB}
\input{sections/duel_IL}
\input{sections/discussion}





\bibliography{references.bib}
\bibliographystyle{plainnat}


\newpage
\appendix


\input{sections/supp_lems}
\input{sections/proofs.tex}


\end{document}
