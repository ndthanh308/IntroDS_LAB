\documentclass[sigconf, review, anonymous]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}

\usepackage{subcaption}
\usepackage{multirow}
\usepackage{indentfirst}
\renewcommand\footnotetextcopyrightpermission[1]{}
\settopmatter{printacmref=false} %remove ACM reference format
\usepackage{makecell}
\usepackage{bbding}
\usepackage{diagbox}
\usepackage{color}
% \usepackage{floatrow}
\usepackage{wrapfig}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\revised}[1]{{\color{red} #1}}
\newcommand{\wjb}[1]{{\color{green} #1}}
\acmConference[ACM MM 23]{ACM Conference}{October 2023}{Ottawa, Canada}


\acmSubmissionID{830}

\begin{document}

\section{supplementary}

\subsection{Difference between DRAEM and EasyNet}\label{sec:difference-DRAEM-EasyNet}
As stated in EasyNet, we learn from DRAEM's reconstruction and abnormal image generation methods, but the architecture between EasyNet and DRAEM is quite different. Firstly, the reconstruction network of EasyNet employs the multi-layer and multi-scale feature information before and after reconstruction, which is also highlighted in the ablation studies, while the counterpart of DREAM can only be segmented by the images before and after reconstruction. Secondly, our experiments find that 2-layer MLP usage is sufficient to effectively segment the abnormal region without the need to use a large U-Net like DREAM. Finally, EasyNet pays more attention to the fusion and segmentation of multi-modal features, while DREAM only focuses on RGB features.

\subsection{Datasets}
\textbf{MVTec 3D-AD~\cite{Bergmann2021TheM3}} includes ten categories and a total of 2,656 training samples along with 1,137 testing samples. The 3D scans in this dataset were acquired via a structured-light-powered industrial scanner that captured the x, y, and z coordinates of the target object. Additionally, RGB data is also collected at the same time for each point in the cloud. To process the 3D data accurately, it is crucial to remove all the background noise. A RANSAC algorithm is employed to estimate the background plane, ensuring that points within 0.005 distances were eliminated without disturbing the RGB data. However, their corresponding pixels in the RGB image were set to zero. This step minimized disturbances while enhancing the accuracy of anomaly detection. 

\textbf{Eyescandies~\cite{bonfiglioli2022eyecandies}} is a novel synthetic dataset comprising ten different categories of candies rendered in a controlled environment. Bonfiglioli \textit{et al.}~\cite{Bonfiglioli2022TheED} generated item instances through modeling software and collected relevant data. The dataset provides depth and RGB images in an industrial conveyor scenario. The ten categories of candies show different challenges, such as complex textures, self-occlusions, and specularities. By controlling the lighting conditions and parameters of a procedural rendering pipeline in the modeling software, the authors of the dataset produced datasets containing complex instances with varying conditions. Similar to MVTec 3D-AD, the training dataset only consists of normal samples, while the testing dataset consists of normal and abnormal samples.

\subsection{I-AUROC and P-AUROC}
I-AUROC notes Image-level AUROC and P-AUROC notes Pixel-level AUROC. Image-level AUROC is based on the area under the ROC curve of the entire image, where the horizontal axis represents a false positive rate and the vertical axis represents a true positive rate. Each point represents the performance of a model under different classification thresholds. Different points can be obtained by changing the image classification threshold to plot the entire ROC curve. Image-level AUROC is used to evaluate the classification quality of the overall image. Pixel-level AUROC reflects the segmentation accuracy of a model at the pixel level based on the area under the ROC curve of each pixel.

\subsection{Implementation Details}

This section presents the implementation details of our experiments. 

MRN uses the "UNet-like" structure as the primary network with intermediate skip operations subtracted primarily from the original UNet. The input image is resized to $256\times256$, and the abnormal and normal images are allocated according to a 1:1 ratio. The abnormal images are applied with Berlin noise~\cite{Zavrtanik2021DRMA} added on top of normal images.

For MSN, the two-layer MLP network is used to fuse different scale features of RGB and depth features. In the experiment of Section 4.2.3, the two layers MLPs network are employed to fuse different modal features. The input and output features of all the MLPs have the same size of $256 \times 256$. And a SE block~\cite{Hu2017SqueezeandExcitationN} is utilized for Attention-based Information Entroy Fusion Module to score channel attention for both modes.

The training process adopted the Adam optimizer with a learning rate of 0.002, which is dynamically adjusted twice, at $0.8 \times $ epochs and $ 0.9 \times $ epochs, with a multiplier factor of 0.2. The batch size is set to 8. Finally, we report the best anomaly detection results obtained after 800 training steps of MRN.

\subsection{Memory of Mainstream Methods}
Memory-based methods are also deployed in real-world applications, so the use of memory banks should not be restricted. Therefore, we calculate in Table 1 the memory size required by the current mainstream methods to use the RGBD method in the MVTec 3D-AD dataset.


\begin{table}[!ht]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{ c|c c c c c c c c c c c}
    \hline
        \textbf{Method} & \textbf{Bagel} & \textbf{Cable Gland} & \textbf{Carrot} & \textbf{Cookie} & \textbf{Dowel} & \textbf{Foam} & \textbf{Peach} & \textbf{Potato} & \textbf{Rope} & \textbf{Tire} & \textbf{Mean} \\ \hline
        PatchCore+FPFH~\cite{Horwitz2022AnEI}(MB) & 228.984 & 209.281 & 268.403 & 197.083 & 270.282 & 221.479 & 338.790 & 281.547 & 279.667 & 197.083 & 249.260 \\ 
        M3DM~\cite{Wang2023MultimodalIA}(GB) & 5.580 & 5.122 & 6.569 & 4.824 & 6.615 & 5.421 & 8.292 & 6.891 & 6.845 & 4.824 & 6.098 \\ \hline
        
    \end{tabular}
    }
    \caption{The size of the memory of mainstream methods using memory bank on the MVTec 3D-AD.}
    \label{tab:memory_bank_size}
\end{table}

The M3DM method occupies a large amount of memory, resulting in a large amount of data transmission and loading time in the actual reasoning process. Although PatchCore+FPFH occupies a smaller memory, its performance is worse than EasyNet. The size of the memory bank is mainly affected by the training set, which indicates that they are using large memory sizes and hindering their practical application.

\subsection{AUPRO Score of MVTec 3D-AD}
In addition to I-AUROC, we also calculated the EasyNet model's AUPRO performance on the MvTec 3D-AD dataset, and Table~\ref{table:aupro_easynet} clearly shows that EasyNet achieves state-of-the-art performance on MvTec 3D-AD without the use of pre-trained models and memory banks and is slightly worse performance than 3D-ST using pre-trained models. In the experimental settings of Pure RGB and Pure Depth, our model does not take priority, but it also proves to a certain extent that our Attention-based Information Entropy Fusion Module plays a role, which blends the information of the two modes well.

\begin{table*}[!ht]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{c c | c c c c c c c c c c | c | c | c }
    \toprule[0.8mm]
        ~ & \textbf{Method} & \textbf{Bagel} & \textbf{Cable Gland} & \textbf{Carrot} & \textbf{Cookie} & \textbf{Dowel} & \textbf{Foam} & \textbf{Peach} & \textbf{Potato} & \textbf{Rope} & \textbf{Tire} & \textbf{Mean} & \textbf{\thead{memeory \\bank use}} & \textbf{\thead{pretrain\\model use}}\\ \hline
        \multirow{9}{*}{\thead{Pure\\Depth}} & Depth GAN~\cite{Bergmann2021TheM3}  & 0.111  & 0.072  & 0.212  & 0.174  & 0.160  & 0.128  & 0.003  & 0.042  & 0.446  & 0.075  & 0.143 & &\\ 
        ~ & Depth AE~\cite{Bergmann2021TheM3} & 0.147  & 0.069  & 0.293  & 0.217  & 0.207  & 0.181  & 0.164  & 0.066  & 0.545  & 0.142  & 0.203 & &\\ 
        ~ & Depth VM~\cite{Bergmann2021TheM3}  & 0.280  & 0.374  & 0.243  & 0.526  & 0.485  & 0.314  & 0.199  & 0.388  & 0.543  & 0.385  & 0.374 & &\\ 
        ~ & Voxel GAN~\cite{Bergmann2021TheM3} & 0.440  & 0.453  & 0.875  & 0.755  & 0.782  & 0.378  & 0.392  & 0.639  & 0.775  & 0.389  & 0.583  & &\\ 
        ~ & Voxel AE~\cite{Bergmann2021TheM3}  &  0.260  & 0.341  & 0.581  & 0.351  & 0.502  & 0.234  & 0.351  & 0.658  & 0.015  & 0.185  & 0.348  & & \\ 
        ~ & Voxel VM~\cite{Bergmann2021TheM3}  & 0.453  & 0.343  & 0.521  & 0.697  & 0.680  & 0.284  & 0.349  & 0.634  & 0.616  & 0.346  & 0.492 & &\\ 
        ~ & FPFH~\cite{Horwitz2022AnEI} & \red{0.973}  & \red{0.879}  & \red{0.982}  & \red{0.906}  & \red{0.892}  & \blue{0.735}  & \red{0.977}  & \red{0.982}  & \red{0.956}  & \red{0.961}  & \red{0.924} &\Checkmark &    \\ 
        ~ & M3DM~\cite{Wang2023MultimodalIA} &  \blue{0.943}  & \blue{0.818}  & \blue{0.977}  & \blue{0.882}  & \blue{0.881}  & \red{0.743}  & \blue{0.958}  & \blue{0.974}  & \blue{0.950}  & \blue{0.929}  & \blue{0.906} &\Checkmark &\Checkmark \\ 
        ~ & \textbf{EasyNet(ours)} & 0.160  & 0.030  & 0.680  & 0.759  & 0.758  & 0.069  & 0.225  & 0.734  & 0.797  & 0.509  & 0.472 &   &   \\ 
        \hline
        \multirow{3}{*}{\thead{Pure\\RGB}} & PatchCore~\cite{Roth2021TowardsTR}  & \blue{0.901}  & \red{0.949}  & \red{0.928}  & \red{0.877}  & \blue{0.892}  & 0.563  & 0.904  & \red{0.932}  & \blue{0.908}  & \red{0.906}  & \red{0.876}  &\Checkmark &\Checkmark\\ 
        ~ & M3DM~\cite{Wang2023MultimodalIA} & \red{0.944}  & \blue{0.918}  & 0.896  & \blue{0.749}  & \red{0.959}  & \red{0.767}  & \red{0.919}  & 0.648  & \red{0.938}  & \blue{0.767}  & \blue{0.850} &\Checkmark &\Checkmark \\ 
        ~ & \textbf{EasyNet(ours)} & 0.751  & 0.825  & \blue{0.916}  & 0.599  & 0.698  & \blue{0.699}  & \blue{0.917}  & \blue{0.827}  & 0.887  & 0.636  & 0.776  &   &   \\ 
        \hline
        \multirow{8}{*}{\thead{RGB+\\Depth}} & Depth GAN~\cite{Bergmann2021TheM3}  & 0.421  & 0.422  & 0.778  & 0.696  & 0.494  & 0.252  & 0.285  & 0.362  & 0.402  & \red{0.631}  & 0.474 &   &   \\ 
        ~ & Depth AE~\cite{Bergmann2021TheM3} & 0.432  & 0.158  & 0.808  & 0.491  & \blue{0.841}  & 0.406  & 0.262  & 0.216  & 0.716  & 0.478  & 0.481 &   &  \\ 
        ~ & Depth VM~\cite{Bergmann2021TheM3}   & 0.388  & 0.321  & 0.194  & 0.570  & 0.408  & 0.282  & 0.244  & 0.349  & 0.268  & 0.331  & 0.335 &  &  \\ 
        ~ & Voxel GAN~\cite{Bergmann2021TheM3} & 0.664  & 0.620  & 0.766  & \blue{0.740}  & 0.783  & 0.332  & 0.582  & 0.790  & 0.633  & 0.483  & 0.639 &  &  \\ 
        ~ & Voxel AE~\cite{Bergmann2021TheM3}  & 0.467  & \blue{0.750}  & 0.808  & 0.550  & 0.765  & 0.473  & 0.721  & \blue{0.918}  & 0.019  & 0.170  & 0.564 &  &  \\ 
        ~ & Voxel VM~\cite{Bergmann2021TheM3}  & 0.510  & 0.331  & 0.413  & 0.715  & 0.680  & 0.279  & 0.300  & 0.507  & 0.611  & 0.366  & 0.471 &  &  \\ 
        ~ & 3D-ST~\cite{Bergmann2022AnomalyDI} & \red{0.950}  & 0.483  & \red{0.986}  & \red{0.921}  & \red{0.905}  & \blue{0.632}  & \red{0.945}  & \red{0.988}  & \red{0.976}  & \blue{0.542}  & \red{0.833} & & \Checkmark \\ 
        ~ & \textbf{EasyNet(ours)} & \blue{0.839}  & \red{0.864}  & \blue{0.951}  & 0.618  & 0.828  & \red{0.836}  & \blue{0.942}  & 0.889  & \blue{0.911}  & 0.528  & \blue{0.821} & &  \\ 
         \bottomrule[0.8mm]
    \end{tabular}
    }
    \caption{AUPRO score for anomaly detection of all categories of MVTec 3D-AD.}
    \label{tab:aupro_easynet}
\end{table*}

\subsection{Visualizations on Eyescandies}
Figure~\ref{fig:result_of_eyescandies} visualizes the performance of EasyNet on Eyescandies dataset, proving the effectiveness of the proposed method. In Figure~\ref{fig:result_of_eyescandies}, in the EasyNet dataset, although RGB images are interfered with by various lighting conditions, the anomaly heat map result of RGB image generated by EasyNet has a high coincidence degree with the ground truth. Compared with only RGB images, the contour of the anomaly heat map generated by EasyNet fusion method is clearer.

% Figure environment removed


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}