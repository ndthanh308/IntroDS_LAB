 
\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algpseudocode}

% \setcopyright{acmcopyright}
% \copyrightyear{2022}
% \acmYear{2022}
% \acmDOI{3503161.3547762}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{indentfirst}
\renewcommand\footnotetextcopyrightpermission[1]{}
\settopmatter{printacmref=false} %remove ACM reference format

% \usepackage[ruled,linesnumbered]{algorithm2e}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}


\usepackage{makecell}
\usepackage{bbding}
\usepackage{diagbox}
\usepackage{color}
% \usepackage{floatrow}
\usepackage{wrapfig}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\revised}[1]{{\color{black} #1}}
\newcommand{\wjb}[1]{{\color{green} #1}}
\acmConference[ACM MM 23]{ACM Conference}{October 2023}{Ottawa, Canada}
% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[MM '22]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% \acmConference[MM '22]{ACM International Conference on Multimedia}{Oct 10--14,
%  2022}{Lisbon, Portugal}
% \acmBooktitle{Proceedings of the 30th ACM International Conference on Multimedia (MM '22), October 10--14, 2022, Lisbon, Portugal}
% \acmPrice{15.00}

% % https://doi.org/10.1145/3503161.3547762

\acmSubmissionID{830}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

% \copyrightyear{2022}
% \acmYear{2022}
% \setcopyright{acmcopyright}\acmConference[MM '22]{Proceedings of the 30th ACM
% International Conference on Multimedia}{October 10--14, 2022}{Lisboa, Portugal}
% \acmBooktitle{Proceedings of the 30th ACM International Conference on Multimedia
% (MM '22), October 10--14, 2022, Lisboa, Portugal}
% \acmPrice{15.00}
% \acmDOI{10.1145/3503161.3547762}
% \acmISBN{978-1-4503-9203-7/22/10}


\begin{document}

\title{EasyNet: An Easy Network for 3D Industrial Anomaly Detection}
\author{Ruitao Chen}
\authornote{Equally contribute to this work}
 \affiliation{%
  \institution{Southern University of Science and Technology}
  \country{Shenzhen, China}}
 \email{chenrt2022@mail.sustech.edu.cn}
 
 \author{Guoyang Xie}
 \authornotemark[1]
 \affiliation{%
  \institution{Southern University of Science and Technology}
  \country{Shenzhen, China}}
      \affiliation{%
  \institution{University of Surrey}
  \country{Guildford GU2 7XH, UK}
  }
 \email{guoyang.xie@surrey.ac.uk}

 \author{Jiaqi Liu}
\authornotemark[1]
 \affiliation{%
  \institution{Southern University of Science and Technology}
  \country{Shenzhen, China}}
 \email{liujq32021@mail.sustech.edu.cn}
 
 
 \author{Jinbao Wang}
\authornote{Corresponding author}
 \affiliation{%
  \institution{Southern University of Science and Technology}
  \country{Shenzhen, China}}
 \email{linkingring@163.com}
 
 
  \author{Ziqi Luo}
 \affiliation{%
  \institution{Southern University of Science and Technology}
  \country{Shenzhen, China}}
 \email{12233217@mail.sustech.edu.cn}

 \author{Jinfan Wang}
 \affiliation{%
  \institution{Southern University of Science and Technology}
  \country{Shenzhen, China}}
 \email{wangjf@sustech.edu.cn}
 
 \author{Feng Zheng}
 %\authornote{Corresponding author}
 \authornotemark[2]
 \affiliation{%
  \institution{CSE and RITAS, Southern University of Science and Technology}
  \country{Shenzhen, China}}
 \email{f.zheng@ieee.org}
 
% \email{linkingring@163.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

% \begin{CCSXML}
% <ccs2012>
%   <concept>
%       <concept_id>10010147.10010178.10010224.10010225</concept_id>
%       <concept_desc>Computing methodologies~Computer vision tasks</concept_desc>
%       <concept_significance>500</concept_significance>
%       </concept>
%  </ccs2012>
% \end{CCSXML}

%\ccsdesc[500]{Computing methodologies~Computer vision tasks}

% Jinbao Wang, Guoyang Xie, Yawen Huang, Yefeng Zheng, Yaochu Jin, Feng Zheng


% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}
% \renewcommand{\shortauthors}{Jinbao Wang et al.}

\begin{abstract}
 %The topic of 2D-based industrial anomaly detection has been extensively researched. However, the field of multi-mode industrial anomaly detection, which is based on the fusion of RGB images and depth information, remains largely unexplored. To address this gap, we have proposed Sim3DNet, a novel network that is simple and easy to implement, for detecting and localizing anomalies in industrial settings.
 %Sim3DNet comprises four primary components: (1) an encoder that extracts different depth features, (2) a decoder that reconstructs normal images, (3) an anomaly discriminator for feature fusion and segmentation, and (4) a gated switch for feature fusion. By leveraging a reconstruction task, Sim3DNet is able to effectively learn the feature residuals of different scales and sizes between abnormal and normal patterns, ultimately leading to the accurate reconstruction of the segmentation map of abnormal regions.
%Through a series of comprehensive experiments, we have demonstrated that Sim3DNet achieves excellent performance on the MVTec 3D AD benchmark test, achieving a 90\% anomaly detection AUROC that is comparable to state-of-the-art methods based on transformer pre-training large models and memory banks. Moreover, Sim3DNet is not only highly effective, but also significantly faster than existing methods, achieving a remarkable frame rate of 83 FPS on a Tesla V100 GPU.

%CN: 首先第一句先表达一下3D anomaly detection的necessity. 然后直奔主题，说现有3D模型有什么坏处 1) 依赖预处理模型 2) 工业制造需要大量的memory bank存储. 而我们新propose出来的easynet是不需要依赖预处理模型，同时也不需要依赖memory bank, 而且inference speed达到了非常快的速度.
3D anomaly detection is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but most of them cannot meet the needs of IM. There are several disadvantages: i) difficult to deploy on production lines since their algorithms heavily rely on large pre-trained models; ii) hugely increase storage overhead due to overuse of memory banks; iii) the inference speed cannot be achieved in real-time. To overcome these issues, we propose an easy and deployment-friendly network (called EasyNet) without using pre-trained models and memory banks: firstly, we design a multi-scale multi-modality feature encoder-decoder to accurately reconstruct the segmentation maps of anomalous regions and encourage the interaction between RGB images and depth images; secondly, we adopt a multi-modality anomaly segmentation network to achieve a precise anomaly map; thirdly, we propose an attention-based information entropy fusion module for feature fusion during inference, making it suitable for real-time deployment. Extensive experiments show that EasyNet achieves an anomaly detection AUROC of 92.6\% without using pre-trained models and memory banks. In addition, EasyNet is faster than existing methods, with a high frame rate of 94.55 FPS on a Tesla V100 GPU.
\end{abstract}


\keywords{3D anomaly detection, multi-modality fusion, unsupervised learning, industrial manufacturing}

\maketitle

\section{Introduction}\label{sec:introduction}

% % Figure environment removed

% % Figure environment removed
% % Figure environment removed

%CN: 首先第一段 简单回顾一下3D AD的必要性. 必要性的说法（首先现在绝大部分的模型里面暂时都是2D模型，而不是3D模型。这时候需要用到figure 1讲明白3D-AD的必要性） 然后讲出现在3D AD现在真在的困难境地 (需要大模型预训练和memory bank)，导致极其困难能够正式在工业制造中落地, 然后说面对这些现有的困难 我们是如何实现这些问题的, TODO, 3D Motivation image.
There is a strong need to propose a deployment-friendly 3D unsupervised anomaly detection (3D-AD) model to tap the gap, which brings 3D-AD's capabilities into the factory floor. Currently, most of anomaly detection methods~\cite{li2022towards, xie2023pushing, Xie2023IMIADII, liu2023deep} are based on 2D images. But in the quality inspection of industrial products, human inspectors utilize both color (RGB) characteristics and depth information to determine whether it is a defective product, where depth information is essential for anomaly detection. As shown in Figure~\ref{fig:3DAD-motivation}, for foam and peach, it is difficult to identify the anomalies from the RGB image alone. Though 3D-AD algorithms~\cite{Wang2023MultimodalIA, Rudolph2022AsymmetricSN, Bergmann2022AnomalyDI} are attracting interest from the academy, most of them are far from satisfactory for industrial manufacturing (IM). According to Figure~\ref{fig:easynet-motivation}, there are several issues: i) The cutting-edge 3D-AD methods steadily rely on the representational abilities of large pre-trained models, leading to slow inference speed and huge storage overhead. ii) Feature embedding-based 3D-AD methods excessively use memory banks, leading to huge memory bank costs in real-world applications. Because of this, it is important and urgent to build an application-oriented 3D-AD model to meet the demands of IM. 
%3D anomaly detection, i.e., detecting abnormal points that deviate from normal pattern for 3D structures, is attracting interest from the academy.



% Figure environment removed

%CN: 第二段直接说明我们为了避免应用预训练模型和memory banks，是如何使用重构模型来RGB和depth之间的融合信息, 
To avoid using large pre-trained models and memory banks, we propose an easy but effective multi-modality anomaly detection and localization network, called \textbf{EasyNet}. Specifically, EasyNet consists of two parts, the Multi-modality Reconstruction Network (MRN) and the Multi-modality Segmentation Network (MSN). First, instead of directly using pre-trained features, we generate synthesized anomalies on RGB images and depth images and reconstruct the anomalies with semantically plausible free contents, while keeping the non-anomalous region of the multi-modality inputs (RGB images and depth images) unchanged. Simultaneously, to simplify the anomalous detection procedure by memory bank-based methods, we feed the reconstructed RGB image and depth image into a simple MSN to obtain the anomaly map. As shown in Figure~\ref{fig:easynet-total-arch}, the whole architecture, including both MRN and MSN, significantly encourage the interaction between RGB and depth features.  

%CN: 第三段主要是写我们使用了轻量级的fusion scheme, 来达到inference speed sota的结局.
To reduce the disturbance between RGB and depth images, we propose an efficient information entropy-based feature fusion scheme. We find that some 3D-AD methods, like AST~\cite{rudolph2022asymmetric} and BTF~\cite{horwitz2022back} cannot fully utilize the advantage of multi-modality fusion, i.e. RGB-D performance is not competitive than RGB performance. The main reason is that there are no uniform abnormal patterns in RGB or depth images. For example, some anomalies can be detected by pure RGB images and depth information works as the noise and may degrade the overall anomaly detection performance. While certain types of anomalies can be easily detected by depth images, RGB and depth feature fusion can greatly enhance anomaly detection performance. Hence, we propose a dynamic multi-modality fusion scheme to make use of RGB and depth features. The architecture of the fusion scheme is shown in Figure~\ref{fig:information_entropy_scheme}. Moreover, as shown in Table~\ref{tab:inference-accuracy}, our proposed fusion scheme is simple and much more computationally efficient than the aforementioned 3D-AD models~\cite{horwitz2022back, rudolph2022asymmetric, Wang2023MultimodalIA}. Our proposed information entropy-based fusion scheme is easy to train and apply, with outstanding performance and inference speed. As a result, EasyNet can achieve 92.6\% on MVTec 3D-AD and 86.9\% on Eyescandies in I-AUROC while running at 94.55 FPS, surpassing the previous best-published 3D-AD methods on accuracy and efficiency. 



Our contributions can be summarized as follows:
\begin{itemize}
    \item EasyNet is easy to implement and deploy for 3D unsupervised anomaly detection, i.e., eliminating the usage of pre-trained models and memory banks. 
    \item We propose an Attention-based Information Fusion Module that achieves the fastest inference speed than the existing methods, with a high frame rate of 94.55 FPS on a Tesla V100 GPU.
    \item We present a Multi-modality Reconstruction Network(MRN) to accurately reconstruct the anomalous region and encourage the interaction of RGB and depth.
    \item We propose a Multi-modality Segmentation Network(MSN) to output the anomaly map precisely.
    \item EasyNet obtains the state-of-the-art result in Pure RGB. Note that EasyNet obtains the best anomaly detection AUROC of 92.6\%, reducing the error by 40\% compared to the next best performing.
\end{itemize} 


\section{Related Work}
Anomaly detection (AD) is a classical topic, which aims to distinguish normal samples and abnormal samples. Existing experimental settings usually only take normal samples as the training set, and evaluate the ability of the model to distinguish abnormal samples in the test set. The current unsupervised AD can be mainly divided into feature extraction-based methods and image reconstruction-based methods. The former is restricted by the pre-trained model, while the latter is free from this limitation. Based on this idea, we designed a reconstructive AD algorithm for RGB-D data, removing the restrictions on pre-trained models and memory banks.

\subsection{2D Anomaly Detection}
Since the emergence of the MVTec AD dataset~\cite{bergmann2019mvtec}, research on AD in industrial 2D images has received more attention. Most existing research is based on this set for unsupervised AD tasks. 

There is more research on feature embedding-based methods than reconstruction-based methods. The most basic idea is to regard AD as a one-class classification problem and turn the AD problem into a problem of finding boundaries for classification. CutPaste~\cite{li2021cutpaste} and SimpleNet~\cite{liu2023simplenet} are representative methods. They make abnormal samples and change unsupervised AD datasets into supervised datasets. Teacher-student architecture is another useful approach. The teacher network distills knowledge to the student network by extracting features from normal samples. While the teacher network and the student network perform differently when producing abnormal samples and they detect anomalies through this characteristic~\cite{bergmann2020uninformed, Deng2022AnomalyDV}. Normalizing flow methods map samples into a Gaussian distribution, while abnormal samples deviate from this distribution~\cite{rudolph2021same, gudovskiy2022cflow}. Methods based on memory banks are simple but effective, whose ideas come from the k-nearest neighbors (KNN) algorithm. They store features of normal samples and calculate the distance between the features of test samples and features of normal samples during testing to determine whether the samples are abnormal~\cite{defard2021padim, roth2022towards}.
As for reconstruction-based methods, most of them are similar in structure. They synthesize abnormal samples and restore abnormal samples to normal samples.  For example, DRAEM~\cite{zavrtanik2021draem} and NSA~\cite{schluter2022natural} synthesize abnormal samples in image level, while DSR~\cite{zavrtanik2022dsr} and UniAD~\cite{you2022unified} synthesize abnormal samples in feature level.

% However, most of the 2D AD methods directly use the pre-trained model of RGB natural images, which cannot be directly used to process depth information, so it is difficult to apply to 3D AD directly. There is a certain gap between the two, and our method tries to get rid of this dependence so that 2D AD can smoothly transition to 3D AD.
Generally, most of the 2D-AD methods use the pre-trained model of natural images to extract RGB's features while they don't process depth information, so it is difficult to apply to 3D-AD directly. There is a certain gap between the two, and our method tries to get rid of this dependence so that 2D-AD can smoothly transition to 3D-AD.

% Figure environment removed

% easynet total architecture comprises of three main components:anomaly generator: adds berlin noise to multi-modal images for simulating abnormal positive and negative enhanced rgb and depth images.multi-modality reconstruction network: a reconstruction task that uses two layers of multi-modal feature information to restore enhanced anomaly images to rgb and depth images without anomalies.multi-modality segmentation network: utilizes a gate module for fusing extracted multi-modal features. during training, the gate module is entirely open, but in the testing phase, it calculates the self-attention information entropy score to guide the network in integrating feature information. we use ssim loss and mse loss to calculate rgb image's reconstruction loss while only using mse loss for depth images. moreover, focal loss is used for calculating pixel classification task loss.
% Figure environment removed

\subsection{3D Anomaly Detection}
Different from 2D-AD, 3D-AD is a new research topic since the publication of MVTec 3D-AD~\cite{Bergmann2021TheM3}. As shown in Figure~\ref{fig:3DAD-motivation}, 3D-AD is a more challenging but also more promising research direction. The effective use of depth information can greatly improve detection accuracy in specific scenarios. On the other hand, how to integrate depth information and prevent it from interfering with RGB information is the current difficulty.

Bergmann~\textit{et al.}~\cite{Bergmann2022AnomalyDI} introduce a point cloud feature extraction network of the teacher-student model. During training, the features extracted by the student network and the teacher network are forced to be consistent. During the test, the differences between the features extracted by the teacher-student model are compared to locate anomalies. Horwitz~\textit{et al.}~\cite{horwitz2022back} combine hand-crafted 3D descriptors with the KNN framework, a classic AD approach. Those two methods are efficient but with poor performance. AST~\cite{Rudolph2022AsymmetricSN} gets a better result in MVTec 3D-AD. However, it only uses depth information to remove the background. AST still uses the 2D-AD method to detect anomalies and the depth information about items is ignored. Similar to BTF, but M3DM~\cite{Wang2023MultimodalIA} extracts features from the point cloud and RGB images respectively and fuses them to make a decision, which has a better performance than treating RGB and depth as six-layer images as BTF. The visualization effect of M3DM is given in the fourth row of Figure~\ref{fig:3DAD-motivation}. CPMF~\cite{cao2023complementary} also adopts the KNN paradigm, the difference is that the author projects the point cloud into 2D images from different angles, and fuses the obtained 2D image information for detection.

To summarize, the existing 3D-AD models either have poor performance or are greatly affected by pre-trained models and memory banks. Differently, we design a simple and effective 3D-AD model without relying on pre-trained models and memory banks. Our method achieves the SOTA performance outperforming all previous methods that learn without additional pre-training.

%\textbf{Contrastive Learning}

\section{Approach}\label{sec:method}

\subsection{Problem Definition and Challenges}
\label{subsec:challenges}

Our 3D-AD setting is similar to M3DM~\cite{Wang2023MultimodalIA} and AST~\cite{rudolph2022asymmetric} and can be formally stated as follows. Given a set of training examples $\mathcal{T} = \left\{ t_{i}\right\}_{i=1}^{N}$, in which $\left\{ t_{1}, t_{2}, \cdots, t_{N}\right\}$ are the normal samples and each of them consists of a paired images, RGB image $I_{rgb}$ and depth image $I_{depth}$. In addition, $\mathcal{T}_{n}$ belongs to one certain category, $c_{j}$, $c_{j} \in \mathcal{C}$, where $\mathcal{C}$ denotes the set of all categories. During test time, given a normal or abnormal sample from a target category $c_{j}$, the AD model should predict whether or not the test 3D object is anomalous and localize the anomaly region if the prediction result is anomalous. 
%
The following are the main challenges. (1) Less information on normal samples can be used. Each category's training dataset contains only normal samples, i.e., no pixel-level annotations of $I_{rgb}$ and $I_{depth}$. (2) It is difficult to find an effectively multi-modality fusion way. Because anomalies may appear in RGB, depth or in both. If we simply fuse both of these features, it may degrade the overall AD performance. (3) Real-world applications have limited storage space. So it is impractical to build a model that uses large pre-trained models and memory banks.

% Due to limited storage in real-world applications, designing a model that eliminates the usage of large pre-trained models and memory banks but also achieves competitive performance is another challenging issue. 

\subsection{EasyNet}
This section provides a comprehensive description of EasyNet. As illustrated in Figure~\ref{fig:easynet-total-arch}, the proposed model comprises a multi-scale Multi-modality Reconstruction Network (MRN), a multi-scale Multi-modality Segmentation Network (MSN) and an attention-based information entropy fusion module, with the fusion network being exclusively applied during reasoning stages. The subsequent sections elaborate on the design and functionality of each module.

\subsubsection{Multi-modality Reconstruction Network (MRN)}
The multi-modal reconstruction network establishes a task of image reconstruction. In this task, the network reconstructs the original image from an artificially corrupted image obtained from the simulator. The network is designed as an encoder-decoder structure to transform the local features of the input image into a mode that more closely resembles the normal sample distribution.

The framework of the simulator is depicted in Figure~\ref{fig:anomaly_generation}. We generate a foreground mask on the original depth image and apply a mask operation on the randomly generated Berlin noise figure. Our empirical evaluation reveals that only adding foreground noise exclusively assists the network in recognizing the noise on the foreground object rapidly. Next, the Berlin noise map undergoes binarization to produce positive and negative mask maps. Both random and original RGB images undergo weighting, alongside the Berlin noise map and depth image. Finally, the resulting outputs include RGB and depth images with anomalies and masks.
% The framework of the simulator is depicted in Figure~\ref{fig:anomaly_generation}，the input data and output data are circled by dotted red lines. We generate a foreground mask on the original depth image and apply a mask operation on the randomly generated Berlin noise figure. Our empirical evaluation reveals that only adding foreground noise exclusively assists the network in recognizing the noise on the foreground object rapidly. Next, the Berlin noise map undergoes binarization to produce positive and negative mask maps. Both random and original RGB images undergo weighting, alongside the Berlin noise map and depth image. Finally, the resulting outputs include RGB and depth images with anomalies and masks.
% Figure environment removed
Traditionally, the $L_{2}$ loss function is used in image reconstruction tasks, but it ignores the differences in structure and perception, resulting in the lack of spatial variation and multi-scale characteristics of the reconstructed images. The use of SSIM loss function (~\ref{con:ssim_loss} ) can not be a problem, so we use $L_{2}$ loss function and SSIM loss function in the reconstruction of network RGB images. In our experiment, it is also found that spatial variation and multi-scale features have limited and even negative effects on depth images, so only $L_{2}$ loss function is used in depth image reconstruction. We have:
\begin{equation}
\begin{aligned}
L_{SSIM}(I,I_r) = \frac{1}{N_p}\sum\limits_{i = 1}^{H}{\sum\limits_{j = 1}^{W} {1-SSIM(I,I_r)_{(i,j)}}},
\end{aligned}
\label{con:ssim_loss}
\end{equation}
where $H$ and $W$ are the height and width of image $I$, respectively. $N_p$ equals the number of pixels in $I$. $I_r$ represents the reconstruction output of the network.


Therefore, the final image reconstruction loss function should be:
\begin{equation}
\begin{aligned}
L_{rec}(I,I_r) &= L_{rec}^{RGB}(I,I_r) + L_{rec}^{depth}(I,I_r) \\
&= \lambda_1L_{SSIM}^{RGB}(I,I_r)+\lambda_2 l_2^{RGB}(I,I_r)+\lambda_3 l_2^{depth}(I,I_r),
\end{aligned}
\end{equation}
where $\lambda_1$, $\lambda_2$, $\lambda_3$ are loss balancing hyper-parameters, All are set to 1 in our experimental setting.

%主要写特征提取和和柏林噪音
% % Figure environment removed

\subsubsection{Multi-modality Segmentation Network (MSN)}
\label{subsubsec:multi_seg_net}


% % Figure environment removed
The MSN evaluates the normality of each time slot $(H, W)$. Similar to DRAEM~\cite{zavrtanik2021draem}, the training set samples are processed by the simulator and the discriminator performs mask identification by identifying the input of enhanced images and reconstructed images. EasyNet extracts multi-layer features evaluated by discriminators through an MRN. MSN utilizes multi-layer reconstruction features and enhanced image features, which come from our assumption that some features that deviate from the normal distribution will be removed gradually with the deepening of the multi-modality reconstruction network. By comparing the difference of eigenvalues before and after removal, the locations of outliers can be obtained.

When extracting reconstruction features and enhancing image features of multiple layers, we mainly adopt the first three layers of shallow networks of MRN and the last three layers of reconstructed features and carry out up-sampling operations to adapt for features of multiple layers. Moreover, we conduct ablation experiments. As shown in Section~\ref{subsubsec:ablation_study_number}, experiments show that when two-layer features are adopted, both the accuracy and computing cost of the network are optimized.
We use a two-layer multilayer perceptron structure (MLP) to process multi-layer scale features extracted from RGB and depth images respectively. Finally, we use another two-layer MLP structure to combine the features of the two modes and perform positive and negative discriminations for each pixel in the image. As shown in Section~\ref{subsec:main_result}, the proposed straightforward strategy is successful in reaching its goal.

% Figure environment removed

\subsubsection{Attention-based Information Fusion Module}

As noted in Section~\ref{subsec:challenges}, exceptions may occur solely in pure RGB or depth images, or both. The combination of both features may diminish the overall performance of AD, where blending RGB and depth led to an inverse outcome. To address this issue, as shown in Figure~\ref{fig:information_entropy_scheme}, we generate multi-channel self-attention scores from input features in the input layer of MSN's multi-layer perception module. 
We then compare the information entropy of the channel that integrates RGB and depth features with that of the channel integrating only pure RGB features. We hypothesize that the greater the information entropy of the channel attention score, the richer the feature knowledge it contains. If fusion features enhance the information gain beyond RGB features, it could have a positive effect on the performance of the results. The experimental results presented in Section~\ref{subsubsec:Information_Entropy} provide support for our theory. The mathematical representation of this process is shown in Formula~\ref{con:f_fusion}.

\begin{equation}
% \small
\footnotesize
\begin{aligned}
F_{fusion} = 
\begin{cases}
F_{RGB}+F_{depth},&f_{IE}(F_{RGB}+F_{depth})>f_{IE}(F_{RGB})+\alpha\\
F_{RGB},&f_{IE}(F_{RGB}+f_{depth}) \leq f_{IE}(F_{RGB})+\alpha
\end{cases}
\end{aligned}
\label{con:f_fusion}
\end{equation}
where $F_{fusion}$ represents the features after fusion, $F_{RGB}$ represents the features of RGB, $F_{depth}$ represents the features of depth, $f_{IE}( \cdot)$ represents the function of calculating information entropy, and $\alpha$ represents the threshold adjustment factor.

When calculating the loss between the predicted mask and the ground truth mask, we use the Focal Loss~\cite{Lin2017FocalLF} function, which could well solve the problem of sample imbalance in the single-class classification of pixels. The formula is expressed as shown in Formula~\ref{con:focal_loss}:
\begin{equation}
\begin{aligned}
L_{focal}(M,M_{out}) = -\alpha_t(1-p_t)^\gamma log(p_t),
\end{aligned}
\label{con:focal_loss}
\end{equation}
where $\alpha_t$ is a scaling factor related to class $t$, $\gamma$ is an adjustable parameter, $p_t$ corresponds to the predicted classification of pixel points, the abnormal category is 1, and the normal category is 0.





\subsubsection{Total Loss Function}

To sum up, training losses of networks mainly come from MRN and MSN. The main optimization objectives and tasks are reconstruction losses and classification losses. Finally, the overall loss of the network during training is as follows:
\begin{equation}
\begin{aligned}
L_{total}(I,I_r) =& L_{rec}^{RGB}(I,I_r) + L_{rec}^{depth}(I,I_r) + L_{focal}(M,M_{out})\\
=& \lambda_1L_{SSIM}^{RGB}(I,I_r)+\lambda_2 l_2^{RGB}(I,I_r)\\
& +\lambda_3 l_2^{depth}(I,I_r)+ \lambda_4 L_{focal}(M,M_{out}),
\end{aligned}
\end{equation}
where $\lambda_1$, $\lambda_2$, $\lambda_3$, and $\lambda_4$ are loss balancing hyper-parameters. \revised{Easynet aims to meet the objectives of optimizing anomaly detection and reconstruction tasks while training, so we expect to optimize the above objectives by assigning weights to different losses. All four $\lambda$ are set to 1 in our experimental setting.}

\subsubsection{Algorithms}
The EasyNet is implemented as Algorithm~\ref{algorithm_easynet}. Firstly, images $I_{rgb}$ and $I_{depth}$ are enhanced by an anomaly generator $\Phi_{ag}$ to produce augmented images $Aug_{rgb}$ and $Aug_{depth}$ respectively. The reconstruction network $\Phi_{rec}$ then extracts depth and RGB features at different scales from these augmented images and origin images. Finally, network $\Phi_{seg}$ generates an anomaly score maps $M$ and $M_{rgb}$ corresponding to the original images. Additionally, the function $\Phi_{ai}$ generates corresponding self-attention information entropy scores for both RGB and RGB-D channels.
%
To achieve optimal fusion information effect, both reconstruction loss and classification loss are calculated, followed by returning the loss gradient. this process allows the network to learn how to better combine RGB and depth features in a complementary and informative way.
\begin{algorithm}
	% \textsl{}\setstretch{1.8}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{EasyNet pseudo-code}
	\label{algorithm_easynet}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: train dataloader $D_{train}$, test dataloader $D_{test}$, epochs
		\STATE \textbf{Output}: trained $\Phi_{rec}$ and $\Phi_{seg}$, $M$
        \STATE \textbf{Initialization ramdomly}:$\Phi_{rec}$ and $\Phi_{seg}$
        \STATE /*Training time*/
        \FOR{$i = 0$ to epochs}
        \FOR{$I_{rgb}, I_{depth}, M_{gt} \leftarrow D_{train}$}
        % \tcc{\Phi_{ag}:Anomaly genaration}
        \STATE $Aug_{rgb},Aug_{depth} = \Phi_{ag}(I_{rgb},I_{depth})$
        % \Comment \Phi_{rec}：Reconstruction Network\\
        \STATE $F_{rgb}, F_{depth}, Rec_{rgb}, Rec_{depth} = \Phi_{rec}(Aug_{rgb}, Aug_{depth})$
        % \Comment{\Phi_{ai}:function of calculating channel self attention score}
        \STATE $F_{fusion} = Concat(F_{rgb}, F_{depth})$
        \STATE $M_{rgb} = \Phi_{seg}(F_{rgb})$
        \STATE $M = \Phi_{seg}(F_{fusion})$
        \STATE$L_{rgb} = \Phi_{loss}(Aug_{rgb}, Aug_{depth}, I_{rgb}, I_{depth}, M_{rgb}, M_{gt})$
        \STATE$L_{total} = \Phi_{loss}(Aug_{rgb},Aug_{depth}, I_{rgb},I_{depth}, M, M_{gt})$
        \STATE$L_{rgb}.backward $ \\ \STATE$L_{total}.backward$
        \ENDFOR
        \ENDFOR
        \STATE /*Inference time*/
        \FOR{$I_{rgb}, I_{depth}, M_{gt} \leftarrow D_{test}$}
        \STATE $Aug_{rgb}, Aug_{depth} = \Phi_{ag}(I_{rgb}, I_{depth})$
        \STATE $F_{rgb}, F_{depth}, Rec_{rgb}, Rec_{depth} = \Phi_{rec}(Aug_{rgb}, Aug_{depth})$
        \STATE $S_{rgb} = \Phi_{ai}(Feature_{rgb})$
        \STATE $S_{fusion} = \Phi_{ai}(F_{rgb}, F_{depth})$
        \IF{$S_{fusion}-S_{rgb} > \alpha$}
        \STATE $F_{fusion} = Concat(F_{rgb}, F_{depth})$
        \ELSE
        \STATE $F_{fusion} = F_{rgb}$
        \ENDIF
        \STATE $M = \Phi_{seg}(F_{fusion})$
        \STATE \textbf{return} $M$
        \ENDFOR
	\end{algorithmic}  
\end{algorithm}




\begin{table*}[t]
    \centering
    \caption{I-AUROC score for anomaly detection of all categories of MVTec-3D AD.}
    \vspace{-10pt}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c c | c c c c c c c c c c | c | c | c }
    \toprule[0.5mm]
        ~ & \textbf{Method} & \textbf{Bagel} & \textbf{Cable Gland} & \textbf{Carrot} & \textbf{Cookie} & \textbf{Dowel} & \textbf{Foam} & \textbf{Peach} & \textbf{Potato} & \textbf{Rope} & \textbf{Tire} & \textbf{Mean} & \thead{\textbf{Memory} \\\textbf{Bank Usage}} & \thead{\textbf{Pre-trained}\\\textbf{Model Usage}}\\ \hline
        \multirow{11}{*}{\textbf{\thead{Pure\\Depth}}} & Depth GAN~\cite{Bergmann2021TheM3}  & 0.530  & 0.376  & 0.607  & 0.603  & 0.497  & 0.484  & 0.595  & 0.489  & 0.536  & 0.521  & 0.523 & & \\ 
        ~ & Depth AE~\cite{Bergmann2021TheM3} & 0.468  & \blue{0.731}  & 0.497  & 0.673  & 0.534  & 0.417  & 0.485  & 0.549  & 0.564  & 0.546  & 0.546  & & \\ 
        ~ & Depth VM~\cite{Bergmann2021TheM3}  & 0.510  & 0.542  & 0.469  & 0.576  & 0.609  & 0.699  & 0.450  & 0.419  & 0.668  & 0.520  & 0.546 & &  \\ 
        ~ & Voxel GAN~\cite{Bergmann2021TheM3} & 0.383  & 0.623  & 0.474  & 0.639  & 0.564  & 0.409  & 0.617  & 0.427  & 0.663  & 0.577  & 0.537 & &  \\ 
        ~ & Voxel AE~\cite{Bergmann2021TheM3}  & 0.693  & 0.425  & 0.515  & 0.790  & 0.494  & 0.558  & 0.537  & 0.484  & 0.639  & 0.583  & 0.571 & &  \\ 
        ~ & Voxel VM~\cite{Bergmann2021TheM3}  & 0.750  & \red{0.747}  & 0.613  & 0.738  & 0.823  & 0.693  & 0.679  & 0.652  & 0.609  & \blue{0.690}  & 0.699  & & \\ 
        ~ & 3D-ST~\cite{Bergmann2022AnomalyDI} & 0.862  & 0.484  & 0.832  & 0.894  & 0.848  & 0.663  & 0.763  & 0.687  & \red{0.958}  & 0.486  & 0.748 &  &\Checkmark   \\ 
        ~ & PatchCore+FPFH~\cite{Horwitz2022AnEI} & 0.825  & 0.551  & \blue{0.952}  & 0.797  & \blue{0.883}  & 0.582  & 0.758  & 0.889  & 0.929  & 0.653  & 0.782 &\Checkmark &    \\ 
        ~ & AST~\cite{rudolph2022asymmetric} & \blue{0.881}  & 0.576  & \red{0.965}  & \blue{0.957}  & 0.679  & \red{0.797}  & \red{0.990}  & \blue{0.915}  & \blue{0.956}  & 0.611  & \blue{0.833} &  &\Checkmark \\ 
        ~ & M3DM~\cite{Wang2023MultimodalIA} & \red{0.941}  & 0.651  & \red{0.965}  & \red{0.969}  & \red{0.905}  & \blue{0.760}  & \blue{0.880}  & \red{0.974}  & 0.926  & \red{0.765}  & \red{0.874} &\Checkmark &\Checkmark \\ 
        ~ & \textbf{EasyNet(ours)} & 0.735  & 0.678  & 0.747  & 0.864  & 0.719  & 0.716  & 0.713  & 0.725  & 0.885  & 0.687  & 0.747 &   &   \\ 
        \hline
        \multirow{8}{*}{\textbf{\thead{Pure\\RGB}}} & DifferNet~\cite{Rudolph2020SameSB} & 0.859  & 0.703  & 0.643  & 0.435  & 0.797  & 0.790  & 0.787  & 0.643  & 0.715  & 0.590  & 0.696 &  &\Checkmark \\ 
        ~ & PADiM~\cite{Defard2020PaDiMAP}  & \blue{0.975}  & 0.775  & 0.698  & 0.582  & 0.959  & 0.663  & 0.858  & 0.535  & 0.832  & 0.760  & 0.764  &\Checkmark  &\Checkmark \\ 
        ~ & PatchCore~\cite{Roth2021TowardsTR}  & 0.876  & 0.880  & 0.791  & 0.682  & 0.912  & 0.701  & 0.695  & 0.618  & 0.841  & 0.702  & 0.770 &\Checkmark &\Checkmark \\ 
        ~ & STEPM~\cite{Wang2021StudentTeacherFP}  & 0.930  & 0.847  & 0.890  & 0.575  & 0.947  & 0.766  & 0.710  & 0.598  & 0.965  & 0.701  & 0.793 &  &\Checkmark \\ 
        ~ & CS-Flow~\cite{Gudovskiy2021CFLOWADRU}  & 0.941  & 0.930  & 0.827  & 0.795  & \blue{0.990}  & 0.886  & 0.731  & 0.471  & \blue{0.986}  & 0.745  & 0.830  &  &\Checkmark \\ 
        ~ & AST~\cite{rudolph2022asymmetric} & 0.947  & 0.928  & 0.851  & \blue{0.825}  & 0.981  & \red{0.951}  & \blue{0.895}  & 0.613  & \red{0.992}  & \blue{0.821}  & \blue{0.880} &   &\Checkmark \\ 
        ~ & M3DM~\cite{Wang2023MultimodalIA} & 0.944  & 0.918  & 0.896  & 0.749  & 0.959  & 0.767  & \red{0.919}  & 0.648  & 0.938  & 0.767  & 0.850 &\Checkmark &\Checkmark \\ 
        ~ & SPADE~\cite{Cohen2020SubImageAD} & 0.771 & 0.793 & 0.760 & 0.531 & 0.848 & 0.683 & 0.646 & 0.460 & 0.879 & 0.502 & 0.687 &\Checkmark &\Checkmark \\ 
        ~ & FastFlow~\cite{Yu12021FastFlowUA} & 0.624 & 0.472 & 0.654 & 0.694 & 0.501 & 0.667 & 0.595 & 0.632 & 0.816 & 0.731 & 0.639 &  &\Checkmark \\ 
        ~ & RD4AD~\cite{Deng2022AnomalyDV} & \blue{0.975} & \blue{0.987} & \red{0.943} & 0.575 & \red{0.999} & 0.830 & 0.863 & 0.618 & 0.984 & \red{0.899} & 0.867 &  &\Checkmark \\ 
        ~ & STPM~\cite{Wang2021StudentTeacherFP} & 0.899 & 0.706 & 0.796 & 0.486 & 0.512 & 0.678 & 0.502 & \blue{0.666} & 0.962 & 0.581 & 0.679 &  &\Checkmark \\ 
        ~ & \textbf{EasyNet(ours)} & \red{0.982}  & \red{0.992}  & \blue{0.917}  & \red{0.953}  & 0.919  & \blue{0.923 } & 0.840  & \red{0.785}  & \blue{0.986}  & 0.742  & \red{0.904} &   &   \\ 
        \hline
        \multirow{11}{*}{\textbf{\thead{RGB+\\Depth}}} & Depth GAN~\cite{Bergmann2021TheM3}  & 0.538  & 0.372  & 0.580  & 0.603  & 0.430  & 0.534  & 0.642  & 0.601  & 0.443  & 0.577  & 0.532 &   &   \\ 
        ~ & Depth AE~\cite{Bergmann2021TheM3} & 0.648  & 0.502  & 0.650  & 0.488  & 0.805  & 0.522  & 0.712  & 0.529  & 0.540  & 0.552  & 0.595 &   &  \\ 
        ~ & Depth VM~\cite{Bergmann2021TheM3}  & 0.513  & 0.551  & 0.477  & 0.581  & 0.617  & 0.716  & 0.450  & 0.421  & 0.598  & 0.623  & 0.555 &  &  \\ 
        ~ & Voxel GAN~\cite{Bergmann2021TheM3} & 0.680  & 0.324  & 0.565  & 0.399  & 0.497  & 0.482  & 0.566  & 0.579  & 0.601  & 0.482  & 0.517 &  &  \\ 
        ~ & Voxel AE~\cite{Bergmann2021TheM3}  & 0.510  & 0.540  & 0.384  & 0.693  & 0.446  & 0.632  & 0.550  & 0.494  & 0.721  & 0.413  & 0.538 &  &  \\ 
        ~ & Voxel VM~\cite{Bergmann2021TheM3}  & 0.553  & 0.772  & 0.484  & 0.701  & 0.751  & 0.578  & 0.480  & 0.466  & 0.689  & 0.611  & 0.609 &  &  \\ 
        ~ & 3D-ST~\cite{Bergmann2022AnomalyDI} & 0.950  & 0.483  & \red{0.986}  & 0.921  & 0.905  & 0.632  & 0.945  & \red{0.988}  & 0.976  & 0.542  & 0.833 & & \Checkmark \\ 
        ~ & PatchCore+FPFH~\cite{Horwitz2022AnEI} & 0.918  & 0.748  & 0.967  & 0.883  & 0.932  & 0.582  & 0.896  & 0.912  & 0.921  & \red{0.886}  & 0.865 &\Checkmark & \\ 
        ~ & AST~\cite{rudolph2022asymmetric} & 0.983  & 0.873  & \blue{0.976}  & \blue{0.971}  & 0.932  & 0.885  & \red{0.974}  & \blue{0.981}  & \red{1.000}  & 0.797  & \blue{0.937} &  &\Checkmark \\ 
        ~ & M3DM~\cite{Wang2023MultimodalIA} & \red{0.994}  & \blue{0.909}  & 0.972  & \red{0.976}  & \red{0.960}  & \blue{0.942}  & \blue{0.973}  & 0.899  & 0.972  & \blue{0.850}  & \red{0.945} &\Checkmark &\Checkmark \\ 
        ~ & \textbf{EasyNet(ours)} & \blue{0.991}  & \red{0.998}  & 0.918  & 0.968  & \blue{0.945}  & \red{0.945}  & 0.905  & 0.807  & \blue{0.994}  & 0.793  & 0.926 & &  \\ 
         \bottomrule[0.5mm]
    \end{tabular}
    }
    \label{tab:mvtec-3d-benchmark}
\end{table*}



\section{Experiments}
\subsection{Experimental Details}
\subsubsection{Datasets}
\textbf{MVTec 3D-AD~\cite{Bergmann2021TheM3}} includes ten categories and a total of 2,656 training samples along with 1,137 testing samples. The 3D scans in this dataset were acquired via a structured-light-powered industrial scanner that captured the x, y, and z coordinates of the target object. Additionally, RGB data is also collected at the same time for each point in the cloud. To process the 3D data accurately, it is crucial to remove all the background noise. A RANSAC algorithm is employed to estimate the background plane, ensuring that points within 0.005 distances were eliminated without disturbing the RGB data. However, their corresponding pixels in the RGB image were set to zero. This step minimized disturbances while enhancing the accuracy of anomaly detection. 
% Finally, the position tensor and RGB images underwent a resizing operation to match the feature extractor's input size of 256 × 256.

\textbf{Eyescandies~\cite{bonfiglioli2022eyecandies}} is a novel synthetic dataset comprising ten different categories of candies rendered in a controlled environment. Bonfiglioli \textit{et al.}~\cite{Bonfiglioli2022TheED} generated item instances through modeling software and collected relevant data. The data set provides depth and RGB images in an industrial conveyor scenario. The ten categories of candies show different challenges, such as complex textures, self-occlusions, and specularities. By controlling the lighting conditions and parameters of a procedural rendering pipeline in the modeling software, the authors of the dataset produced datasets containing complex instances with varying conditions. Similar to MVTec 3D-AD, the training dataset only consists of normal samples, while the testing dataset consists of normal and abnormal samples.

\subsubsection{Evaluation Metrics}

\revised{Due to the unsupervised experimental setting, accuracy is usually not used to evaluate the performance of anomaly detection methods, but rather the Area Under the Curve (AUC) metric, we use the I-AUROC and P-AUROC in our method. Besides, the common evaluation metrics we used include Area Under the Receiver Operating Characteristic Curve (AUROC) and the Area Under the Precision-Recall curve (AUPR/AP), and the explanation of I-AUROC and P-AUROC please refer to the \textit{supplementary materials}.}


% AUROC is a common evaluation metric for data classification problems, also regarded as one of the fundamental metrics for anomaly detection issues. 
% Table~\ref{tab:confusion_matrix} displays the correlation between predicted results and actual results. $TPR=\frac{TP}{TP+FN}$ and $FPR=\frac{FP}{FP+TN}$ can construct a curve with varying x- and y-axis values for different thresholds. AUROC measures how well a model can distinguish between various classes.

% \begin{table}[ht]
% \centering
% \caption{Confusion Matrix}{
% \scalebox{0.85}{\begin{tabular}{c|cc}
% \hline
% \diagbox{\textbf{Actual}}{\textbf{Predicted}}  & \textbf{Normal} & \textbf{Abnormal} \\ \hline
%     \textbf{Normal}  & True Positive (TP) &  False Negative (FN)  \\
%   \textbf{Abnormal}  & False Positive (FP)   &  True Negative (TN) \\ \hline
% \end{tabular}}
% }
% \label{tab:confusion_matrix}
% \end{table}

% AUPR/AP is more dependable than AUROC when the TN samples are more than other samples. $Precision=\frac{TP}{TP+FP}$ and  $Recall=\frac{TP}{TP+FN}$ can also form a curve, and AP is the area under the curve. Since AP doesn't consider TN, it won't be impacted when TN samples make up a significant amount. Industrial photos usually have a tiny aberrant area and a larger normal area, which results in a high pixel-level AUROC without distinction. Therefore, using AP as the index for pixel-level evaluation makes more sense.

\subsubsection{Implementation Details}
% Training 具体说明使用的网络层数mlp设置，训练的学习率和epochs等。具体地可以分开两个 branch来写，怎么写得有逻辑就怎么写

This section presents the implementation details of our experiments. 

MRN uses the "UNet-like" structure as the primary network with intermediate skip operations subtracted primarily from the original UNet. The input image is resized to $256\times256$, and the abnormal and normal images are allocated according to a 1:1 ratio. The abnormal images are applied with Berlin noise~\cite{Zavrtanik2021DRMA} added on top of normal images.
%
For MSN, the two-layer MLP network is used to fuse different scale features of RGB and depth features. In experiment~\ref{subsubsec:Information_Entropy}, the two layers MLPs network are employed to fuse different modal features. The input and output features of all the MLPs have the same size of $256 \times 256$. And a SE block~\cite{Hu2017SqueezeandExcitationN} is utilized for Attention-based Information Fusion Scheme to score channel attention for both modes.

The training process adopted the Adam optimizer with a learning rate of 0.002, which is dynamically adjusted twice, at $0.8 \times $ epochs and $ 0.9 \times $ epochs, with a multiplier factor of 0.2. The batch size is set to 8. Finally, we report the best anomaly detection results obtained after 800 training steps of MRN.


\begin{table*}[th]
    \centering
    \caption{I-AUROC score for anomaly detection of all categories of Eyescandies.}
        \vspace{-10pt}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c c | c c c c c c c c c c | c | c | c }
    \toprule[0.5mm]
        ~ & \textbf{Method} & \textbf{\thead{Candy \\Cane}} & \textbf{\thead{Chocolate\\Cookie}} & \textbf{\thead{Chocolate\\Cookie}} & \textbf{Confetto} & \textbf{\thead{Gummy\\Bear}} & \textbf{\thead{Hazelnut\\Truffle}} & \textbf{\thead{Licorice\\Sandwich}} &\textbf{ Lollipop} & \textbf{\thead{Marsh-\\mallow}} & \textbf{\thead{Peppermint\\Candy}} & \textbf{Mean} & \textbf{\thead{Memory \\Bank usage}} & \textbf{\thead{Pre-trained\\Model Usage}}\\ \hline
        \multirow{5}{*}{\textbf{\thead{Pure\\Depth}}} & Raw~\cite{Horwitz2022AnEI} & \blue{0.654} & 0.510 & 0.563 & 0.451 & 0.433 & 0.454 & 0.472 & 0.515 & 0.626 & 0.366 & 0.504 &\Checkmark & \\ 
         ~ &HoG~\cite{Horwitz2022AnEI} & 0.653 & 0.510 & 0.470 & 0.723 & \blue{0.728} & \blue{0.520} & 0.717 & 0.667 & 0.699 & 0.742 & 0.643 &\Checkmark &\\ 
         ~ &SIFT~\cite{Horwitz2022AnEI} & 0.589 & 0.582 & 0.683 & \red{0.885} & 0.663 & 0.480 & \blue{0.778} & 0.702 & \blue{0.746} & \red{0.790} & 0.690 &\Checkmark &\\ 
         ~ &FPFH~\cite{Horwitz2022AnEI} & \red{0.670} & \blue{0.710} & \red{0.805} & \blue{0.806} & \red{0.748} & 0.515 & \red{0.794} & \red{0.757} & \red{0.765} & \blue{0.757} & \red{0.733} &\Checkmark &\\ 
        ~ & \textbf{EasyNet(ours)} & 0.629  & \red{0.716}  & \blue{0.768}  & 0.731  & 0.660  & \red{0.710}  & 0.712  & \blue{0.711}  & 0.688 & 0.731 & \blue{0.706} & &\\ 
        \hline
        \multirow{7}{*}{\textbf{\thead{Pure\\RGB}}}
         ~ & GANomaly~\cite{Akay2018GANomalySA} & 0.485  & 0.512  & 0.532  & 0.504  & 0.558  & 0.486  & 0.467  & 0.511  & 0.481  & 0.528  & 0.507 & & \\ 
        ~ & DFKDE~\cite{anomalib} & 0.539  & 0.577  & 0.482  & 0.548  & 0.541  & 0.492  & 0.524  & 0.602  & 0.658  & 0.591  & 0.555 &  &\Checkmark \\ 
         ~ & DFM~\cite{Ahuja2019ProbabilisticMO} & 0.532  & 0.776  & 0.624  & 0.675  & 0.681  & 0.596  & 0.685  & 0.618  & 0.964  & 0.770  & 0.692 &  &\Checkmark \\ 
         ~ & STEPM~\cite{Wang2021StudentTeacherFP}  & \blue{0.551}  & 0.654  & 0.576  & 0.784  & \blue{0.737}  & \blue{0.790}  & 0.778  & 0.620  & 0.840  & 0.749  & 0.708  &  &\Checkmark \\ 
         ~ & PaDiM~\cite{Defard2020PaDiMAP} & 0.531  & 0.816  & \blue{0.821}  & \blue{0.856}  & \red{0.826}  & 0.727  & \blue{0.784}  & 0.665  & \red{0.987}  & \blue{0.924}  & \blue{0.794} & \Checkmark  &\Checkmark\\ 
        ~ & AutoEncoder~\cite{Bonfiglioli2022TheED} &0.527  & \blue{0.848}  & 0.772  & 0.734  & 0.590  & 0.508  & 0.693  & \blue{0.760}  & 0.851  & 0.730  & 0.701  & & \\ 
        ~ & \textbf{EasyNet(ours)} & \red{0.723}  & \red{0.925}  & \red{0.849}  & \red{0.966}  & 0.705  & \red{0.815}  & \red{0.806}  & \red{0.851}  & \blue{0.975}  & \red{0.960}  & \red{0.858} & & \\ \hline
        \multirow{3}{*}{\textbf{\thead{RGB+\\Depth}}} & AutoenEoder~\cite{Bonfiglioli2022TheED} &0.529  & 0.861  & 0.739  & 0.752  & 0.594  & 0.498  & 0.679  & 0.651  & 0.838  & 0.750  & 0.689 & &\\
        ~ & PatchCore+FPFH~\cite{Horwitz2022AnEI} & \blue{0.606}  & \blue{0.904}  & \blue{0.792}  & \blue{0.939}  & \blue{0.720}  & \blue{0.563}  & \blue{0.867}  & \blue{0.860}  & \red{0.992}  & \blue{0.842}  & \blue{0.809} &\Checkmark &\Checkmark\\
        ~ & \textbf{EasyNet(ours)} & \red{0.737}  &  \red{0.934}  &  \red{0.866}  &  \red{0.966} &  \red{0.717}  &  \red{0.822}  &  \red{0.847}  &  \red{0.863} & \blue{0.977}  &  \red{0.960}  & \red{0.869}  & &  \\  
         \bottomrule[0.5mm]
    \end{tabular}
    }
    \label{tab:eyescandies-benchmark}
\end{table*}

\begin{table*}[th]
    \centering
    \caption{Ablation studies on attention-based information entropy fusion module. The best is in red and the second best is in blue. }
        \vspace{-10pt}
    \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{c|c | c c c c c c c c c c c}
    \toprule[0.5mm]
    \multirow{4}{*}{\textbf{MVTec 3D-AD}} & \textbf{Evaluation} & \textbf{Bagel} & \textbf{Cable gland} & \textbf{Carrot} & \textbf{Cookie} & \textbf{Dowel} & \textbf{Foam} & \textbf{Peach} & \textbf{Potato} & \textbf{Rope} & \textbf{Tire} & \textbf{Mean} \\ 
    \cline{2-13}
         & Gate Close & \blue{0.982}  & 0.992  & \blue{0.917}  & 0.953  & 0.919  & 0.923  & 0.840  & \blue{0.785}  & \blue{0.986}  & \blue{0.742}  & 0.904  \\ 
        & Gate Open & 0.974  & \blue{0.996}  & 0.914  & \blue{0.968}  & \blue{0.941}  & \blue{0.938}  & \blue{0.882}  & 0.781  & 0.982  & \red{0.793}  & \blue{0.917}  \\ 
        &Gate Control & \red{0.991}  & \red{0.998}  & \red{0.918}  & \red{0.968}  & \red{0.945}  & \red{0.945}  & \red{0.905}  & \red{0.807}  & \red{0.994}  & \red{0.793}  & \red{0.926}  \\ 
        \Xhline{1.5pt}
   \multirow{4}{*}{\textbf{Eyescandies}} & \textbf{Evaluation} & \textbf{\thead{Candy \\Cane}} & \textbf{\thead{Chocolate\\Cookie}} & \textbf{\thead{Chocolate\\Cookie}} & \textbf{Confetto} & \textbf{\thead{Gummy\\Bear}} & \textbf{\thead{Hazelnut\\Truffle}} & \textbf{\thead{Licorice\\Sandwich}} &\textbf{ Lollipop} & \textbf{\thead{Marsh-\\mallow}} & \textbf{\thead{Peppermint\\Candy}} & \textbf{Mean} \\ 
   \cline{2-13}
         & Gate Close & \blue{0.723}  & \blue{0.925}  & \blue{0.849}  & \blue{0.966 } & \blue{0.705}  & \blue{0.815}  & 0.806  & \blue{0.851}  & \red{0.975}  & \red{0.960}  & \blue{0.857}  \\  
        & Gate Open & 0.722  & 0.919  & 0.827  & 0.945  & 0.685  & 0.813  & \blue{0.846}  & 0.850  & \red{0.975}  & \blue{0.959}  & 0.854  \\ 
        & Gate Control & \red{0.737}  & \red{0.934} & \red{0.866}  & \red{0.966}  & \red{0.717}  & \red{0.822}  & \red{0.847}  & \red{0.863}  & \red{0.977}  & \red{0.960}  & \red{0.869}\\ 
        \bottomrule[0.5mm]
    \end{tabular}
    }   
    \label{tab:gate-abalation-study}
\end{table*}

\begin{table*}[th]
    \centering
    \caption{The ablation study for the number of fusion layers. The best is in red and the second best is in blue. }
        \vspace{-10pt}
    \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{{c|c c c c c c c c c c c c}}
    \toprule[0.5mm]
        \textbf{RGB+Depth} & \textbf{Evaluation} & \textbf{Bagel} & \textbf{Cable gland} & \textbf{Carrot} & \textbf{Cookie} & \textbf{Dowel} & \textbf{Foam} & \textbf{Peach} & \textbf{Potato} & \textbf{Rope} & \textbf{Tire} & \textbf{Mean} \\ \hline
        
        \multirow{3}{*}{Image AUC} & 1 layer & \blue{0.967} & 0.981 & \blue{0.912} & \blue{0.890} & 0.901 & 0.908 & \blue{0.763} & \blue{0.717} & \red{1.000} & 0.731 & 0.877 \\ 
        ~& 2 layers & \red{0.974} & \blue{0.996} & \red{0.914} & \red{0.968} & \blue{0.941} & \red{0.938} & \red{0.882} & \red{0.781} & \blue{0.982} & \blue{0.793} & \red{0.917} \\
        ~& 3 layers & 0.964 & \red{1.000} & 0.884 & 0.889 & \red{0.967} & \blue{0.932} & 0.734 & 0.697 & \red{1.000} & \red{0.866} & \blue{0.893} \\ \hline

        \multirow{3}{*}{Image AP} & 1 layer & \blue{0.992} & 0.995 & \red{0.982} & \blue{0.969} & 0.976 & 0.975 & \blue{0.915} & 0.873 & \red{1.000} & 0.906 & 0.958 \\ 
        ~& 2 layers & \red{0.994} & \blue{0.999} & \blue{0.981} & \red{0.991} & \blue{0.984} & \red{0.984} & \red{0.966} & \red{0.930} & \blue{0.992} & \blue{0.927} & \red{0.975} \\ 
        ~& 3 layers & 0.991 & \red{1.000} & 0.975 & 0.966 & \red{0.992} & \blue{0.982} & 0.903 & \blue{0.916} & \red{1.000} & \red{0.963} & \blue{0.969} \\ \hline

        \multirow{3}{*}{Pixel AUC} & 1 layer & 0.875 & \blue{0.861} & \blue{0.963} & \blue{0.678} & 0.716 & \red{0.998} & \blue{0.969} & \blue{0.921} & 0.928 & \blue{0.861} & \blue{0.877} \\ 
        ~& 2 layers & \red{0.935} & \red{0.941} & \red{0.971} & \red{0.897} & \red{0.885} & \blue{0.997} & \red{0.992} & 0.888 & \blue{0.955} & 0.728 & \red{0.919} \\ 
        ~& 3 layers & \blue{0.904} & 0.717 & 0.836 & 0.651 & \blue{0.809} & \blue{0.997} & 0.914 & \red{0.942} & \red{0.986} & \red{0.97} & 0.873 \\ \hline

        \multirow{3}{*}{Pixel AP} & 1 layer & 0.020 & 0.025 & \blue{0.097} & \blue{0.015} & 0.026 & \red{0.803} & \blue{0.081} & \blue{0.032} & 0.133 & \red{0.163} & \blue{0.139} \\ 
        ~& 2 layers & \blue{0.039} & \red{0.062} & \red{0.188} & \red{0.025} & \blue{0.034} & 0.562 & \red{0.298} & \red{0.034} & \blue{0.144} & 0.031 & \red{0.142} \\ 
        ~& 3 layers & \red{0.041} & \blue{0.043} & 0.084 & 0.008 & \red{0.121} & \blue{0.648} & 0.017 & 0.014 & \red{0.249} & \blue{0.134} & 0.136 \\ 
        \bottomrule[0.5mm]
    \end{tabular}
    }
    \label{table:ablation_number_layer}
\end{table*}


\subsection{Experimental Results and Analysis}
\label{subsec:main_result}

\subsubsection{RGB+Depth on MVTec 3D-AD and Eyescandies}

Table~\ref{tab:mvtec-3d-benchmark} and Table~\ref{tab:eyescandies-benchmark} clearly demonstrate that EasyNet achieves the state-of-the-art performance on MVTec 3D-AD and Eyescandies without using pre-trained model and memory bank. Specifically, EasyNet outperforms AutoEncoder~\cite{Bonfiglioli2022TheED} and PatchCore+FPFH~\cite{Horwitz2022AnEI} in RGB+Depth setting of Eyescandies by a large margin, 20.7\% and 6.9\%, respectively. Even though PatchCore+FPFH~\cite{Horwitz2022AnEI} uses large pre-trained models, i.e., WideResNet-50 and huge memory banks for the features of each point, it still cannot compete with EasyNet. For MVTec 3D-AD, AST~\cite{rudolph2022asymmetric} and M3DM~\cite{Wang2023MultimodalIA} are the cutting-edge models in MVTec 3D-AD. However, both of them use large pre-trained models or memory banks. In specific, M3DM uses two pre-trained models (Point Transformer and Vision Transformer) to extract the features from depth images and RGB images. In addition, M3DM employs two large memory banks (average 6.098 GB) to store the features from depth images and RGB images. Due to strict storage limitations in practice, M3DM cannot be perfectly fit in real-world applications. Although PatchCore+FPFH has a relatively small memory footprint (249.260 MB on average), the actual performance is not as good as Easynet. The memory bank size of M3DM and PatchCore+FPFH on the MVTec 3D-AD can be found in \textit{supplementary materials}. AST also adopts two EfficientNet-B5 as the feature extractor for depth image and RGB image, which violate the storage limitation in IM. Moreover, according to Table~\ref{tab:inference-accuracy}, massive usage of pre-trained models will slow down the inference speed, which cannot meet the requirement of IM. Furthermore, the performance gap among EasyNet, AST and M3DM is very small, 2.1\% and 1.2\%. Hence, EasyNet is the best 3D-AD model to meet all the demands of IM.



\subsubsection{Pure RGB Performance}
% EasyNet模型在RGB上优势很大，但是为什么只讲RGB其实需要一个理由。我认为比较好的理由就是在实际生产当中 depth sensor中的limitation是非常大的，深度信息的精度是只有2-3米以内才比较精准，而且很容易受环境影响。所以我们尝试在只使用 RGB image的情况下来使用. EasyNet在 RGB 上的优势就是在证明即使depth sensor坏了的情况之下发 EasyNet仍然具备极大的优势
In real-world applications, the limitation of the depth sensor is very large since the effective distance range of depth sensor is 3 meters. In addition, most of the depth sensors are easily affected by the lighting condition. Hence, as for simulating the failure of the depth sensor, we conduct the experiment by only using RGB images as the input. In pure RGB branch of Table~\ref{tab:mvtec-3d-benchmark} and Table~\ref{tab:eyescandies-benchmark}, EasyNet achieves state-of-the-art performance in I-AUROC in pure RGB track. In specific, EasyNet outperforms in pure RGB of Eyescandies with a large margin, 15.7\% to AutoEncoder~\cite{Bonfiglioli2022TheED} and 6.4\% to PaDiM~\cite{Defard2020PaDiMAP}. Moreover, EasyNet gets 5.97\% better I-AUROC score than M3DM and 2.65\% better I-AUROC score than AST. In total, The performance of EasyNet is robust even though the depth sensor is a failure. 

% Figure environment removed

% % Figure environment removed


\subsubsection{Attention-based Information Entropy Fusion Module}
\label{subsubsec:Information_Entropy}
Table~\ref{tab:gate-abalation-study} clearly illustrates the effectiveness of our proposed attention-based information entropy fusion module in EasyNet. \revised{The gate network is the key to control multi-feature fusion.} We conduct the ablation studies on three options, Gate Close, Gate Open and Gate Control, respectively. Gate Close means that EasyNet only utilizes RGB images as the input and ignores depth information. Gate Open denotes that EasyNet uses both the RGB image and the depth image and combines their features for evaluation. Gate Control means that EasyNet adopts an attention-based information entropy fusion module to select the depth features during the inference case. In Gate Open, we discover that depth information degrades the total performance if we select both RGB features and depth features during inference. In some case, we find that only utilizing RGB features are enough to detect the anomalies. Then the depth information may work as the noise to interfere with the final result. Therefore, we design an attention-based information entropy fusion module to select the feature for fusion, which can enhance the performance of all classes in MVTec 3D-AD and Eyescandies. 
% 列出信息熵门控公式，给出图标，说明结果的好处，重点分析foam和tire这两个没有提升，是因为网络都选择了融合depth，可以给出具体的可视化图，说明这两个类单靠rgb不行，然后再重点分析cookie和potato，这两个使用门控后都提升了，比单纯使用rgb或者单纯rgbd更好，可以给出具体的可视化图，说明cookie或者potato有些图片是纯rgb是看不来的，所以得融depth。
% 综上说明一个观点，使用rgb加上depth来融合，depth因为只有深度信息，有些是会对结果产生不利的影响，所以需要在合适的时候再融合进去，而且rgb也不能完全没有depth，所以基于信息熵的gate方法是有效的，而且比纯rgbd提升了性能。



%\subsection{Ablation Study}
\subsubsection{Ablation study on the number of fusion layers}
\label{subsubsec:ablation_study_number}
% As described in Section ~\ref{subsubsec:multi_seg_net}, Multi-modality Segmentation Network segments outliers by fusing multi-layer enhanced image features and reconstructed features. Table ~\ref{table:ablation_number_layer} shows that EasyNet achieves optimal performance when using the features of 2 layers. In the index of I-AUROC, the effect of using two layers is 4.56\% higher than that of using one layer, 2.69\% higher than that of using 3 layers, and 4.79\% higher than that of using 1 layer, 5.27\% higher than that of using one layer in the index of P-AUROC. The effect is 1\% higher than that of using three layers, indicating that the effect of using two layers of features is optimal in both anomaly detection and location performance. The use of three-layer features will lead to performance degradation, which also verifies our hypothesis. With the deepening of the multi-mode reconstruction network, some feature parts deviating from the normal distribution will be gradually removed. The use of three-layer features will introduce more features removing abnormal parts, leading to the performance degradation of the discriminator.

As described in Section~\ref{subsubsec:multi_seg_net}, the MSN is utilized for segmenting outliers by fusing multi-layer enhanced image features and reconstructed ones. Table~\ref{table:ablation_number_layer} presents that EasyNet performs optimally when incorporating features from only two layers. Specifically, the performance metrics of I-AUROC and P-AUROC are respectively improved by 4.36\% and 4.57\% with two layers compared to one layer; and they are further increased by 2.62\% and 5.27\% with three layers compared to one layer. These results indicate that the employment of two feature layers can achieve the best performance both in anomaly detection and localization. In contrast, using three feature layers leads to performance degradation, which verifies our hypothesis. The deepening of the multi-modality reconstruction network can gradually eliminate some abnormal feature parts, whereas using three feature layers introduces more feature removal of abnormal parts, thereby leading to poor discriminator performance.

%给出表格，说明使用2层是权衡的结果
% \subsubsection{Ablation Study on Light-weight Fusion Scheme}

\subsubsection{Accuracy VS Inference Speed} \label{sec:accuracy_vs_inference}

\begin{wraptable}{r}{0.23\textwidth}
    \centering
    \caption{Inference abilities.}
        \vspace{-10pt}
    \scalebox{0.8}{
    \begin{tabular}{c| c c}
    \toprule[0.3mm]
        \textbf{Method} & \textbf{I-AUROC} & \textbf{FPS} \\ \hline
        BTF~\cite{horwitz2022back} & 0.865 & 27.92 \\ 
        AST~\cite{rudolph2022asymmetric} & 0.937 & 41.94 \\ 
       M3DM~\cite{Wang2023MultimodalIA} & 0.945 & 0.10 \\ 
        \textbf{EasyNet(ours)} & 0.926 & 94.55 \\ 
        \bottomrule[0.3mm]
    \end{tabular}}
    \label{tab:inference-accuracy}
\end{wraptable}
As we previously described in Section~\ref{sec:introduction}, inference speed is one of the important factors to be considered in IM. Since the real production line needs to check each product in real time. Table~\ref{tab:inference-accuracy} shows EasyNet obtains the fastest speed among the cutting-edge anomaly detection methods but without sacrificing the performance. In specific, EasyNet gets 125\% FPS better than AST and 93900\% FPS than M3DM. As for performance, the performance gap among EasyNet, AST and M3DM is very small, 2.1\% and 1.2\%. Therefore, EasyNet is the most deployment-friendly 3D-AD method for IM. 

% \begin{table}[th]
%     \centering
%     \caption{Accuracy VS Inference}
%     \scalebox{0.85}{ \begin{tabular}{c| c c}
%     \toprule[0.5mm]
%         \textbf{Method} & \textbf{I-AUROC} & \textbf{FPS} \\ \hline
%         BTF~\cite{horwitz2022back} & 0.865 & 27.92 \\ 
%         AST~\cite{rudolph2022asymmetric} & 0.937 & 41.94 \\ 
%        M3DM~\cite{Wang2023MultimodalIA} & 0.945 & 0.10 \\ 
%         EasyNet(ours) & 0.926 & 94.55 \\ 
%         \bottomrule[0.5mm]
%     \end{tabular}}
%     \label{tab:inference-accuracy}
% \end{table}




\subsection{Visualization}
% Figure~\ref{fig:result_of_mvtec_3d_ad} and Figure~\ref{fig:result_of_eyescandies} visualize the performance of EasyNet method in two data sets of MVTec-3D AD and EyesCandies. In FIG. 1, compared with AST, which extracted the features of the middle layer of the pre-training network, our method positioned the anomalies more clearly. In addition, compared with the effect of using only rgb images, the false positive probability of anomalies, such as cable gland, is lower after using the Fusion method. Moreover, just like peach and potato, the abnormal places show more obvious performance on depth graphs, and the anomaly detection effect after fusion is also better. This also demonstrates the importance of using depth image information in industrial anomaly detection. For the data set of EyesCandies, because the data is generated in simulation, RGB features are affected more by care, and the Fusion method can also identify the abnormal position well.

% Figure~\ref{fig:result_of_mvtec_3d_ad} visualizes the performance of EasyNet on MVTec 3D-AD, demonstrating the effectiveness of the proposed method.
% In Figure~\ref{fig:result_of_mvtec_3d_ad}, our method outperforms AST~\cite{rudolph2022asymmetric}, which extracts features from the middle layers of a pre-trained network, by providing clearer positioning of anomalies. Furthermore, the fusion method reduces false positives for anomalies such as \textit{cable glands}, compared to using only RGB images. Abnormalities in \textit{peach} and \textit{potato} are also more clearly visible on depth graphs, indicating the importance of using depth image information in industrial AD. Note that we put the visualization results of EasyNet on Eyescandies in \textit{supplementary materials}.

% Regarding the EyesCandies dataset, since the data is generated through simulation, illumination significantly affects RGB images. In this context, EasyNet using only RGB images produces better results than other methods, and the fusion method identifies abnormal positions reliably.
\revised{Figure~\ref{fig:result_of_mvtec_3d_ad} visualizes the performance of EasyNet on MVTec 3D-AD, demonstrating the effectiveness of the proposed method.
In Figure~\ref{fig:result_of_mvtec_3d_ad}, Compared to existing 3D anomaly detection methods (AST~\cite{rudolph2022asymmetric}), EasyNet can reduce false positive rates significantly and achieve higher segmentation accuracy. Furthermore, the fusion method reduces false positives for anomalies such as \textit{cable glands}, compared to using only RGB images. Abnormalities in \textit{peach} and \textit{potato} are also more clearly visible on depth graphs, indicating the importance of using depth image information in industrial AD. In addition, EasyNet is not affected by domain gap between natural and industrial images and has a higher inference speed than existing methods. Note that we put the visualization results of EasyNet on Eyescandies in \textit{supplementary materials}.}

\section{Conclusions}
This paper addresses a promising and challenging task, i.e., deployment-friendly 3D-AD and proposes an easy but effective neural network (termed as EasyNet) to achieve competitive performance without using large pre-trained models and memory banks. Specifically, as for getting rid of large pre-trained models and memory banks, EasyNet employs MRN to implicitly detect and reconstruct the anomalies with semantically plausible anomaly-free content, while keeping the non-anomalous regions of the input image unchanged. Meanwhile, EasyNet proposes an MSN to produce an accurate anomaly segmentation map from the concatenated reconstructed RGB images and depth images and their original appearances. In the test phase, EasyNet adopts multi-head self-query scores in the early fusion stage to select the informative depth features before fusing with RGB features. To this end, EasyNet achieves the fastest inference speed without sacrificing performance. 

% \begin{table*}[!ht]
%     \centering
%     \scalebox{0.9}{
%     \begin{tabular}{c c | c c c c c c c c c c | c | c | c }
%     \toprule[0.8mm]
%         ~ & \textbf{Method} & \textbf{Bagel} & \textbf{Cable Gland} & \textbf{Carrot} & \textbf{Cookie} & \textbf{Dowel} & \textbf{Foam} & \textbf{Peach} & \textbf{Potato} & \textbf{Rope} & \textbf{Tire} & \textbf{Mean} & \textbf{\thead{memeory \\bank use}} & \textbf{\thead{pretrain\\model use}}\\ \hline
%         \multirow{9}{*}{\thead{Pure\\Depth}} & Depth GAN~\cite{Bergmann2021TheM3}  & 0.111  & 0.072  & 0.212  & 0.174  & 0.160  & 0.128  & 0.003  & 0.042  & 0.446  & 0.075  & 0.143 & &\\ 
%         ~ & Depth AE~\cite{Bergmann2021TheM3} & 0.147  & 0.069  & 0.293  & 0.217  & 0.207  & 0.181  & 0.164  & 0.066  & 0.545  & 0.142  & 0.203 & &\\ 
%         ~ & Depth VM~\cite{Bergmann2021TheM3}  & 0.280  & 0.374  & 0.243  & 0.526  & 0.485  & 0.314  & 0.199  & 0.388  & 0.543  & 0.385  & 0.374 & &\\ 
%         ~ & Voxel GAN~\cite{Bergmann2021TheM3} & 0.440  & 0.453  & 0.875  & 0.755  & 0.782  & 0.378  & 0.392  & 0.639  & 0.775  & 0.389  & 0.583  & &\\ 
%         ~ & Voxel AE~\cite{Bergmann2021TheM3}  &  0.260  & 0.341  & 0.581  & 0.351  & 0.502  & 0.234  & 0.351  & 0.658  & 0.015  & 0.185  & 0.348  & & \\ 
%         ~ & Voxel VM~\cite{Bergmann2021TheM3}  & 0.453  & 0.343  & 0.521  & 0.697  & 0.680  & 0.284  & 0.349  & 0.634  & 0.616  & 0.346  & 0.492 & &\\ 
%         ~ & FPFH~\cite{Horwitz2022AnEI} & \red{0.973}  & \red{0.879}  & \red{0.982}  & \red{0.906}  & \red{0.892}  & \blue{0.735}  & \red{0.977}  & \red{0.982}  & \red{0.956}  & \red{0.961}  & \red{0.924} &\Checkmark &    \\ 
%         ~ & M3DM~\cite{Wang2023MultimodalIA} &  \blue{0.943}  & \blue{0.818}  & \blue{0.977}  & \blue{0.882}  & \blue{0.881}  & \red{0.743}  & \blue{0.958}  & \blue{0.974}  & \blue{0.950}  & \blue{0.929}  & \blue{0.906} &\Checkmark &\Checkmark \\ 
%         ~ & \textbf{EasyNet(ours)} & 0.160  & 0.030  & 0.680  & 0.759  & 0.758  & 0.069  & 0.225  & 0.734  & 0.797  & 0.509  & 0.472 &   &   \\ 
%         \hline
%         \multirow{3}{*}{\thead{Pure\\RGB}} & PatchCore~\cite{Roth2021TowardsTR}  & \blue{0.901}  & \red{0.949}  & \red{0.928}  & \red{0.877}  & \blue{0.892}  & 0.563  & 0.904  & \red{0.932}  & \blue{0.908}  & \red{0.906}  & \red{0.876}  &\Checkmark &\Checkmark\\ 
%         ~ & M3DM~\cite{Wang2023MultimodalIA} & \red{0.944}  & \blue{0.918}  & 0.896  & \blue{0.749}  & \red{0.959}  & \red{0.767}  & \red{0.919}  & 0.648  & \red{0.938}  & \blue{0.767}  & \blue{0.850} &\Checkmark &\Checkmark \\ 
%         ~ & \textbf{EasyNet(ours)} & 0.751  & 0.825  & \blue{0.916}  & 0.599  & 0.698  & \blue{0.699}  & \blue{0.917}  & \blue{0.827}  & 0.887  & 0.636  & 0.776  &   &   \\ 
%         \hline
%         \multirow{8}{*}{\thead{RGB+\\Depth}} & Depth GAN~\cite{Bergmann2021TheM3}  & 0.421  & 0.422  & 0.778  & 0.696  & 0.494  & 0.252  & 0.285  & 0.362  & 0.402  & \red{0.631}  & 0.474 &   &   \\ 
%         ~ & Depth AE~\cite{Bergmann2021TheM3} & 0.432  & 0.158  & 0.808  & 0.491  & \blue{0.841}  & 0.406  & 0.262  & 0.216  & 0.716  & 0.478  & 0.481 &   &  \\ 
%         ~ & Depth VM~\cite{Bergmann2021TheM3}   & 0.388  & 0.321  & 0.194  & 0.570  & 0.408  & 0.282  & 0.244  & 0.349  & 0.268  & 0.331  & 0.335 &  &  \\ 
%         ~ & Voxel GAN~\cite{Bergmann2021TheM3} & 0.664  & 0.620  & 0.766  & \blue{0.740}  & 0.783  & 0.332  & 0.582  & 0.790  & 0.633  & 0.483  & 0.639 &  &  \\ 
%         ~ & Voxel AE~\cite{Bergmann2021TheM3}  & 0.467  & \blue{0.750}  & 0.808  & 0.550  & 0.765  & 0.473  & 0.721  & \blue{0.918}  & 0.019  & 0.170  & 0.564 &  &  \\ 
%         ~ & Voxel VM~\cite{Bergmann2021TheM3}  & 0.510  & 0.331  & 0.413  & 0.715  & 0.680  & 0.279  & 0.300  & 0.507  & 0.611  & 0.366  & 0.471 &  &  \\ 
%         ~ & 3D-ST~\cite{Bergmann2022AnomalyDI} & \red{0.950}  & 0.483  & \red{0.986}  & \red{0.921}  & \red{0.905}  & \blue{0.632}  & \red{0.945}  & \red{0.988}  & \red{0.976}  & \blue{0.542}  & \red{0.833} & & \Checkmark \\ 
%         ~ & \textbf{EasyNet(ours)} & \blue{0.839}  & \red{0.864}  & \blue{0.951}  & 0.618  & 0.828  & \red{0.836}  & \blue{0.942}  & 0.889  & \blue{0.911}  & 0.528  & \blue{0.821} & &  \\ 
%          \bottomrule[0.8mm]
%     \end{tabular}
%     }
%     \caption{AUPRO score for anomaly detection of all categories of MVTec-3D AD.}
    
% \end{table*}

% \clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
