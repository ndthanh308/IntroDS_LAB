\documentclass[10pt,conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage[frozencache=true,cachedir=minted-cache]{minted} 
\usepackage{pgf-pie}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
\usepackage[hyphens]{url}
\usepackage{algpseudocode}
\usepackage{algorithm}


% Ensure letter paper
\pdfpagewidth=8.5in
\pdfpageheight=11in

\newcommand{\hpcayear}{2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% -- UPDATE -- %%%%%%%%%%%%%%%
\newcommand{\hpcasubmissionnumber}{NaN}
% \title{MASE: Efficient and Scalable Software and Hardware Co-Exploration for New ML Models}
\title{Fast Prototyping Next-Generation Accelerators for New ML Models using MASE: ML Accelerator System Exploration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{jcred}{HTML}{e31a1c}
\definecolor{jcgreen}{HTML}{33a02c}
\definecolor{jcblue}{HTML}{1f78b4}
\definecolor{jcorange}{HTML}{ff7f00}
\definecolor{jcpurple}{HTML}{6a3d9a}
\definecolor{jclightred}{HTML}{fb8072}
\definecolor{jclightgreen}{HTML}{b3de69}
\definecolor{jclightblue}{HTML}{80b1d3}
\definecolor{jclightorange}{HTML}{fdb462}
\definecolor{jclightpurple}{HTML}{bebada}
\newcommand\jc[1]{\textcolor{red}{\bf #1}}
\newcommand\jcp[1]{\textit{#1}}
\newcommand\az[1]{\textcolor{blue}{{\bf [AZ:} #1{\bf]}}}
\newcommand\best[1]{\textcolor{jcgreen}{\bf #1}}
\newcommand\worst[1]{\textcolor{jcred}{\bf #1}}
\newcommand\change[1]{{\color{black} #1}}
\newcommand{\emphbold}[1]{\textbf{\textit{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% -- ONLY FOR CAMERA READY -- %%%%%%%%
% \def\hpcacameraready{} % Uncomment to build camera-ready version
\newcommand{\hpcapubid}{0000--0000/00\$00.00}
\newcommand\hpcaauthors{Jianyi Cheng, Cheng Zhang, Zhewen Yu, Alex Montgomerie-Corcoran, Can Xiao, \\ Christos-Savvas Bouganis and 
Yiren Zhao}
\newcommand\hpcaaffiliation{Imperial College London, UK}
\newcommand\hpcaemail{\{jianyi.cheng17, cheng.zhang122, zhewen.yu18, alexander.montgomerie-corcoran15, can.xiao22, \\ christos-savvas.bouganis, a.zhao\}@imperial.ac.uk}

%%%%% -- ARTEFACT EVALUATION RESULTS -- %%%%%%
% Uncomment the following based on the badges that were awarded to this paper
%\def\aeopen{}           % The artifact is publically available
%\def\aereviewed{}     % The artefact has been reviewed
%\def\aereproduced{} % The results have been reproduced
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{hpca-template}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% -- PAPER CONTENT STARTS -- %%%%%%%%%

\begin{abstract}

Machine learning (ML) accelerators have been studied and used extensively to compute ML models with high performance and low power. However, designing such accelerators normally takes a long time and requires significant effort.

Unfortunately, the pace of development of ML software models is much faster than the accelerator design cycle, leading to frequent and drastic modifications in the model architecture, thus rendering many accelerators obsolete. Existing design tools and frameworks can provide quick accelerator prototyping, but only for a limited range of models that can fit into a single hardware device, such as an FPGA. Furthermore, with the emergence of large language models, such as GPT-3, there is an increased need for hardware prototyping of these large models within a many-accelerator system to ensure the hardware can scale with the ever-growing model sizes.

In this paper, we propose an efficient and scalable approach for exploring accelerator systems to compute large ML models. We developed a tool named MASE that can directly map large ML models onto an efficient streaming accelerator system. Over a set of ML models, we show that MASE can achieve better energy efficiency to GPUs when computing inference for recent transformer models. Our tool will open-sourced upon publication.
\end{abstract}


%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%
\input{sections/intro}
\input{sections/overview}
\input{sections/background}
\input{sections/method}
\input{sections/experiments}
\input{sections/conclusions}

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{IEEEtranS}
\bibliography{refs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

