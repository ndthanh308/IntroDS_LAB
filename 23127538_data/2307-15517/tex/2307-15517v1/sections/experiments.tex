\section{Experiments}
\label{sec:experiments}

We evaluated MASE on ten ML models, including classic CNN models and new transformer models. The detailed specifications of these models are shown in Table~\ref{tab:model_specs}. 
% The reported fine-tuned accuracy is not optimal as we did not use techniques such as knowledge distillation (KD). \change{In this evaluation, our focus is on the MASE framework itself. While these techniques can be added to MASE, they are considered outside the scope of this work. To ensure fairness, both the baselines and our results were not optimised using any of these techniques.} 
We obtained the total clock cycles from the MASE co-simulator, and the area results were extracted from the Post Synthesis report in Vivado. The FPGA family we used for the result measurements is Alveo U250, and the version of Xilinx software employed is 2020.2.

\subsection{Mixed-Precision Quantization Co-design}
\label{sec:experiments:search}

As mentioned in Section \ref{sec:methodology:modify_sw}, MASE explores model quantization before deploying the models on hardware. We adopt the 8-bit fixed-point representation \cite{choukroun2019low} as the default setup, referred to as ``int8'' in the rest of this paper. We report the accuracy of the quantized model in Table~\ref{tab:model_specs}, including both post-training and fine-tuning results.

It is well-known that applying a uniform bit width to the entire network may not be optimal. Previous literature \cite{wang2019haq} has highlighted that the dynamic range of data varies significantly across different layers of the same network. This observation motivates the exploration of mixed-precision arithmetic. MASE provides users with the ability to investigate mixed-precision arithmetic through our efficient IR.

In this example, we demonstrate how MASE can be utilized to automatically search for the optimal bit width of each layer in a hardware-aware manner. We employ the Tree-structured Parzen Estimator (TPE) \cite{bergstra2011algorithms} as the searching algorithm. For each explored design point, MASE automatically collects information on both the model accuracy and hardware resource utilisation.

% Figure environment removed


\change{Traditional mixed-precision quantization searches for small total numbers of bits with minimum loss. For this example, the analyser returns 23 optimal-like designs at minimal loss. However, these designs might not be all hardware efficient.} Figure~\ref{fig:search} illustrates the search results for {\tt facebook/opt-125m}. From the figure, we observe the following:
\begin{itemize}
    \item The loss of our work is worse than GPU because we only do standard quantization without fine-tuning.
    \item \change{The total bits of the model might be the same for all the searched results but they have shown different energy efficiency.}
    \item \change{The best design point at the top right of the figure has better energy efficiency than the GPU.}
\end{itemize}
\change{Particularly, Figure~\ref{fig:bitmap} shows the distribution of the bits over different layers in the top-right design point in Figure~\ref{fig:search}. The layers are repeated because there are 12 building blocks. More than half of the bit widths have been quantized to less than 4 bits, which significantly improves energy efficiency.}

\subsection{Exploring Efficient Custom Arithmetic}
\label{sec:experiments:msfloat}
\input{imgs/mix_precision}

Here we show how to use custom arithmetic operations as user-defined blocks and integrate them into MASE for hardware architecture exploration. We use Microsoft Floating Point (MSFP) \cite{darvish2020pushing} in this section. It is a floating point representation sharing an exponent across vector elements. An MSFP vector takes the form as follows:
\begin{align}
    2^{e_{shared}} [ (-1)^{s_0}m_0, (-1)^{s_1}m_1, \dots, (-1)^{s_{n-1}}m_{n-1} ]
\end{align}
where $n$ signed mantissa $(-1)^{s_i}m_{i}$, $i=0,1,\dots, n-1$ scaled by the same exponent ${e_{shared}}$ 
($n$ is the so-called bounding box size) \cite{darvish2020pushing}. 
The shared exponent enables efficient data size by reducing the average bit width. Additionally, it reduces the number of 
bits when computing matrix multiplication, leading to potentially better energy efficiency. 
An example of an 8-bit exponent 
and 4-bit value MSFP value is shown in Figure~\ref{fig:msfloat_op}.

In this example, we seek to exploit MSFP operations in BERT using MASE. 
We provide similar tunable parameters as shown in Figure~\ref{fig:linear_example} so it can be 
explored in the MASE DSE. 
The sharing constraint is set to 16, and the bit width is configured to 4. That means every 16 4-bit values share an 8-bit exponent.

% Figure environment removed

\input{imgs/transformer_results}
\input{imgs/cnn_results}

The results of MSFP integration for BERT are shown in Table~\ref{tab:related_works_transformer}. 
From the table, we made the following observations:

\begin{itemize}
    \item Quantizing BERT to fixed-point has lost significant accuracy. This is because the variation of the data is huge and cannot fit into the 8-bit range.
    \item The accuracy of using MSFP is comparable with the floating point version. That is because the MSFP still preserves a large range of data values benefiting from its floating point representation, although the average bit widths are smaller than 8 bits.
    \item The performance of the MSPF implementation is significantly higher than the GPU results and has shown the best energy efficiency. That is because the low bit widths enable more data transfer to the FPGA, which was the performance bottleneck of the hardware system. Also, the arbitrary precision enables more power efficiency compared to the fixed architecture of GPUs.
\end{itemize}

As shown above, MASE enables users to fast prototype new arithmetic components and integrate them into a large model for systematical
exploration. This eases the complexity in large hardware system design and significantly improves the hardware development process.


\subsection{Overall Results}

Table~\ref{tab:related_works_transformer} shows the overall results for transformers. From the table, we made the following observations:

\begin{itemize}
    \item The accuracy of quantized {\tt facebook/opt-1.3b} is significantly worse than the floating point results. This is because the accuracy was not fine-tuned because of out-of-memory \change{errors} in our GPU server.
    \item Overall, MSFP has shown the best performance and energy efficiency. This could improve hardware performance if integrated into the next generation of ML accelerators.
    \item The results produced by vanilla MASE \change{show} high throughput with extremely large resources. This shows that the many accelerator systems generated by MASE can scale well with the number of devices as the cross-device communication does not have interconnect restrictions.
    \item The bit width used by vanilla MASE is 8 bits, which is inefficient for some models. This reduces the energy efficiency of the hardware. Quantization search could be used for further improvements, but it takes a long time for large models.
\end{itemize}

Table~\ref{tab:related_works_cnn} shows the overall results for CNN models. From the table, we made the following observations:

\begin{itemize}
    \item The ML accelerators for CNN have been widely studied and have shown competitive results. Here we can show that MASE can still generate efficient hardware with competitive performance.
    \item The results for {\tt mobilenetv3} models are not as competitive as related works, \change{because} the model is relatively small, and there are fewer opportunities to exploit model parallelism. 
\end{itemize}