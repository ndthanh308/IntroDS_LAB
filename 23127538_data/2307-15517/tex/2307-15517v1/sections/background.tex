
\section{Background}
\label{sec:background}

In this section, we will first introduce the automated toolflows for ML hardware accelerators. Secondly, we compare the existing IR designs in the ML community. Finally, we review the related works on software and hardware co-design.

\subsection{HLS for Hardware Accelerators}

Early research on hardware accelerators manually translated software models to HLS code~\cite{stratus, catapult}. The results are often promising, but it takes a significant amount of effort and time to interpret the software model and perform manual hardware translation. Such engineering effort is usually repeated when accelerating a new model architecture. 

There have been interests in fast prototyping hardware accelerators using HLS tools. High-level synthesis is a process that automatically translates high-level software programs into low-level hardware descriptions~\cite{hls}. This enables software engineers without hardware background to \change{customize} their own hardware accelerators, or hardware engineers to efficiently and rapidly prototype hardware accelerators. 

HLS4ML~\cite{fahim2021hls4ml} is proposed for translating ML models into HLS code, resulting in custom hardware designs. AMD Xilinx FINN~\cite{blott_trets2018} supports hardware acceleration of CNN models. However, these tools keep the ML model unchanged and only exploit parallelism for hardware designs that produce the same results as the software ML model.

MASE aims to solve these problems as an efficient and scalable \change{system-level} HLS tool for ML. First, MASE utilises an efficient abstraction that enables software and hardware co-optimization. This allows the software model to be modified during hardware synthesis to generate better-quality designs. Second, MASE supports direct lowering from PyTorch to hardware accelerator systems, significantly reducing the development time for accelerator design.

% Figure environment removed

\change{

\subsection{Machine Learning and HLS IR}

There are a variety of tools for ML training and inference. These tools often use an intermediate representation (IR) to represent a model architecture. For instance, various IRs are widely used in software ML communities, including TorchScript~\cite{devito2022torchscript}, ONNX~\cite{bai2019onnx} and FX Torch~\cite{reed2022torch}. Such an IR represents the model at the module level, where each operation is a predefined module or function in PyTorch. However, these IRs are designed specifically for processors and cannot describe custom hardware designs. MASE IR extends these IRs and enables custom hardware modelling for efficient hardware synthesis.

In hardware acceleration, the software model is often represented in a target-independent software IR, such as MLIR~\cite{lattner2020mlir} or LLVM~\cite{lattner2004llvm}. This IR is then translated into a hardware design. MLIR is an IR that can express a program at different abstractions, also known as dialects, enabling efficient optimization at the appropriate level of abstraction. It supports both software and hardware abstractions~\cite{circt}. LLVM, originally designed as a target-independent IR for software programs, has also been used to express HLS programs~\cite{vivado_hls}. However, these IRs represent an ML model at a lower granularity than the module level and cannot be easily transformed back to the original model. This poses challenges for training models at this level of abstraction. In contrast, MASE IR retains the model trainability, which creates opportunities for software and hardware co-design. A detailed comparison between MASE IR and existing IRs is presented in Table~\ref{tab:mase_ir}.

\subsection{Software and Hardware Co-Design}
}

The field of Automated Machine Learning (AutoML) has made significant progress in recent years, aiming to achieve high accuracy and hardware performance. From a software perspective, this involves discovering state-of-the-art models through Neural Architecture Search (NAS) \cite{liu2018darts, zoph2016neural}. On the hardware side, the NAS process incorporates performance metrics like latency and power, and it also integrates the hardware metrics with the model architecture for a joint search \cite{abdelfattah2020best}.

\change{

HW-NAS-BENCH \cite{li2021hw} investigated co-design for small datasets such as CIFAR-10 and CIFAR-100. The study measured hardware performance on edge devices and developed device-specific performance models. EDD \cite{li2020edd} focused on searching for the optimal combination of parallelism, loop-tiling factors, and quantization strategies for a given model. The authors formulated the co-design problem as a differentiable loss function, which was then optimized using gradient descent. Similar works in this field include Auto-NBA \cite{fu2021auto}, HAO \cite{dong2021hao}, and Co-Exploration \cite{jiang2020hardware}.

These related works primarily focused on mapping neural networks to a single device, and their evaluations often involved small or outdated models. In contrast, MASE offers node-level hardware generation and integration with arbitrary user-defined components across a multi-accelerator system. This capability enables MASE to support the deployment of large and state-of-the-art models. In Section~\ref{sec:experiments:search}, we provide a case study to demonstrate how MASE efficiently searches for both efficient quantization and hardware mapping for these models. In the future, the MASE tool flow can be easily extended to other use cases, such as zero-shot NAS \cite{lin2021zen}.
}

\input{imgs/mase_ir_attr}