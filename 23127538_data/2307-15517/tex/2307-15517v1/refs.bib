@article{duarte_arxiv2018,
  title={Fast inference of deep neural networks in FPGAs for particle physics},
  author={Duarte, Javier and Han, Song and Harris, Philip and Jindariani, Sergo and Kreinar, Edward and Kreis, Benjamin and Ngadiuba, Jennifer and Pierini, Maurizio and Rivera, Ryan and Tran, Nhan and others},
  journal={Journal of Instrumentation},
  volume={13},
  number={07},
  pages={P07027},
  year={2018},
  publisher={IOP Publishing}
}

@misc{circt,
  title={CIRCT-based HLS compilation flows, debugging, and cosimulation tools.},
  author={CIRCT contributors},
  howpublished = {\url{https://github.com/circt-hls/circt-hls}},
  year={2023}
}

@article{blott_trets2018,
author = {Blott, Michaela and Preu\ss{}er, Thomas B. and Fraser, Nicholas J. and Gambardella, Giulio and O’brien, Kenneth and Umuroglu, Yaman and Leeser, Miriam and Vissers, Kees},
title = {FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3242897},
doi = {10.1145/3242897},
abstract = {Convolutional Neural Networks have rapidly become the most successful machine-learning algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing systems. While the underlying arithmetic is structurally simple, compute and memory requirements are challenging. One of the promising opportunities is leveraging reduced-precision representations for inputs, activations, and model parameters. The resulting scalability in performance, power efficiency, and storage footprint provides interesting design compromises in exchange for a small reduction in accuracy. FPGAs are ideal for exploiting low-precision inference engines leveraging custom precisions to achieve the required numerical accuracy for a given application. In this article, we describe the second generation of the FINN framework, an end-to-end tool that enables design-space exploration and automates the creation of fully customized inference engines on FPGAs. Given a neural network description, the tool optimizes for given platforms, design targets, and a specific precision. We introduce formalizations of resource cost functions and performance predictions and elaborate on the optimization algorithms. Finally, we evaluate a selection of reduced precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS&nbsp;F1, demonstrating new unprecedented measured throughput at 50&nbsp;TOp/s on AWS&nbsp;F1 and 5&nbsp;TOp/s on embedded devices.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {dec},
articleno = {16},
numpages = {23},
keywords = {Neural network, quantized neural networks, FINN, convolutional neural networks, artificial intelligence, inference, FPGA, hardware accellerator}
}

@article{fahim2021hls4ml,
  title={hls4ml: An open-source codesign workflow to empower scientific low-power machine learning devices},
  author={Fahim, Farah and Hawks, Benjamin and Herwig, Christian and Hirschauer, James and Jindariani, Sergo and Tran, Nhan and Carloni, Luca P and Di Guglielmo, Giuseppe and Harris, Philip and Krupa, Jeffrey and others},
  journal={arXiv preprint arXiv:2103.05579},
  year={2021}
}

@ARTICLE{hls,
author={Philippe Coussy and Daniel D. Gajski and Michael Meredith and Andres Takach},
journal={IEEE Design Test of Computers},
title={An Introduction to High-Level Synthesis},
year={2009},
volume={26},
number={4},
pages={8-17},
ISSN={0740-7475},
month={July},
publisher = {IEEE},}

@article{tarafdar_trets2022,
author = {Tarafdar, Naif and Di Guglielmo, Giuseppe and Harris, Philip C. and Krupa, Jeffrey D. and Loncar, Vladimir and Rankin, Dylan S. and Tran, Nhan and Wu, Zhenbin and Shen, Qianfeng and Chow, Paul},
title = {AIgean: An Open Framework for Deploying Machine Learning on Heterogeneous Clusters},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3482854},
doi = {10.1145/3482854},
abstract = {&nbsp;AIgean, pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos, for multi-FPGA deployment, and hls4ml, for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-to-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment &amp; Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean. We demonstrate the effectiveness of AIgean with two use cases: an autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA’s strength in low-latency computing, as our implementations target batch-1 implementations.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {dec},
articleno = {23},
numpages = {32},
keywords = {FPGAs, hardware/software co-design, data center}
}

@INPROCEEDINGS{niu_micro2022,
  author={Niu, Wei and Guan, Jiexiong and Shen, Xipeng and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={GCD2: A Globally Optimizing Compiler for Mapping DNNs to Mobile DSPs}, 
  year={2022},
  volume={},
  number={},
  pages={512-529},
  doi={10.1109/MICRO56248.2022.00044}}

@misc{stratus,
author = {Cadence},
title 	= {{Stratus High-Level Synthesis}},
url = {https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/synthesis/stratus-high-level-synthesis.html},
year	=	{2023}
}
@misc{torch_mlir,
author = {{LLVM}},
title 	= {{Torch-MLIR}},
url = {https://github.com/llvm/torch-mlir},
year	=	{2023}
}


@misc{vivado,
author = {{AMD Xilinx}},
title 	= {{Vivado}},
url = {https://www.xilinx.com/products/design-tools/vivado.html},
year	=	{2023}
}

@misc{catapult,
author = {Siemens},
title 	= {{Catapult High-Level Synthesis and Verification}},
url = {https://eda.sw.siemens.com/en-US/ic/catapult-high-level-synthesis/},
year	=	{2023}
}

@article{kouris2020approximate,
  title={{Approximate LSTMs for time-constrained inference: Enabling fast reaction in self-driving cars}},
  author={Kouris, Alexandros and Venieris, Stylianos I and Rizakis, Michail and Bouganis, Christos-Savvas},
  journal={IEEE Consumer Electronics Magazine},
  volume={9},
  number={4},
  pages={11--26},
  year={2020},
  publisher={IEEE}
}

@inproceedings{abdelfattah2020best,
  title={Best of both worlds: Automl codesign of a cnn and its hardware accelerator},
  author={Abdelfattah, Mohamed S and Dudziak, {\L}ukasz and Chau, Thomas and Lee, Royson and Kim, Hyeji and Lane, Nicholas D},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@inproceedings{li2021hw,
  title={HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark},
  author={Li, Chaojian and Yu, Zhongzhi and Fu, Yonggan and Zhang, Yongan and Zhao, Yang and You, Haoran and Yu, Qixuan and Wang, Yue and Lin, Yingyan},
  booktitle={The 9th International Conference on Learning Representations 2021 (ICLR 2021)},
  year={2021}
}

@inproceedings{xiao2022towards,
  title={Towards Agile DNN Accelerator Design Using Incremental Synthesis on FPGAs},
  author={Xiao, Qingcheng and Liang, Yun},
  booktitle={Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={42--48},
  year={2022}
}

@inproceedings{zheng2021se,
  title={{SE-SSD: Self-ensembling single-stage object detector from point cloud}},
  author={Zheng, Wu and Tang, Weiliang and Jiang, Li and Fu, Chi-Wing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14494--14503},
  year={2021}
}

@article{reed2022torch,
  title={torch. fx: Practical Program Capture and Transformation for Deep Learning in Python},
  author={Reed, James and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={638--651},
  year={2022}
}

@article{bai2019onnx,
  title={Onnx: Open neural network exchange},
  author={Bai, Junjie and Lu, Fang and Zhang, Ke and others},
  journal={GitHub repository},
  pages={54},
  year={2019}
}

@article{devito2022torchscript,
  title={Torchscript: Optimized execution of pytorch programs},
  author={DeVito, Zachary},
  journal={Retrieved January},
  year={2022}
}

@article{adolphs2022boosting,
  title={{Boosting Search Engines with Interactive Agents}},
  author={Adolphs, Leonard and Buck, Christian and Chen Huebscher, Michelle and Ciaramita, Massimiliano and Espeholt, Lasse and Hofmann, Thomas and Kilcher, Yannic and Rothe, Sascha and Sessa, Pier Giuseppe and Sestorain Saralegui, Lierni and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}
@article{wang2022pvt,
  title={Pvt v2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  volume={8},
  number={3},
  pages={415--424},
  year={2022},
  publisher={Springer}
}
@inproceedings{lattner2004llvm,
  title={LLVM: A compilation framework for lifelong program analysis \& transformation},
  author={Lattner, Chris and Adve, Vikram},
  booktitle={International symposium on code generation and optimization, 2004. CGO 2004.},
  pages={75--86},
  year={2004},
  organization={IEEE}
}
@article{lattner2020mlir,
  title={MLIR: A compiler infrastructure for the end of Moore's law},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  journal={arXiv preprint arXiv:2002.11054},
  year={2020}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{korngiebel2021considering,
  title={Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery},
  author={Korngiebel, Diane M and Mooney, Sean D},
  journal={NPJ Digital Medicine},
  volume={4},
  number={1},
  pages={93},
  year={2021},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{fu2021auto,
  title={Auto-NBA: Efficient and effective search over the joint space of networks, bitwidths, and accelerators},
  author={Fu, Yonggan and Zhang, Yongan and Zhang, Yang and Cox, David and Lin, Yingyan},
  booktitle={International Conference on Machine Learning},
  pages={3505--3517},
  year={2021},
  organization={PMLR}
}
@INPROCEEDINGS{wang2021fpt,
  author={Gao, Jingbo and Qian, Yu and Hu, Yihan and Fan, Xitian and Luk, Wai-Shing and Cao, Wei and Wang, Lingli},
  booktitle={2021 International Conference on Field-Programmable Technology (ICFPT)}, 
  title={LETA: A lightweight exchangeable-track accelerator for efficientnet based on FPGA}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/ICFPT52863.2021.9609919}}

@inproceedings{sun2022fpga,
author = {Sun, Mengshu and Li, Zhengang and Lu, Alec and Li, Yanyu and Chang, Sung-En and Ma, Xiaolong and Lin, Xue and Fang, Zhenman},
title = {FILM-QNN: Efficient FPGA Acceleration of Deep Neural Networks with Intra-Layer, Mixed-Precision Quantization},
year = {2022},
isbn = {9781450391498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490422.3502364},
doi = {10.1145/3490422.3502364},
abstract = {With the trend to deploy Deep Neural Network (DNN) inference models on edge devices with limited resources, quantization techniques have been widely used to reduce on-chip storage and improve computation throughput. However, existing DNN quantization work deploying quantization below 8-bit may be either suffering from evident accuracy loss or facing a big gap between the theoretical improvement of computation throughput and the practical inference speedup. In this work, we propose a general framework, called FILM-QNN, to quantize and accelerate multiple DNN models across different embedded FPGA devices. First, we propose the novel intra-layer, mixed-precision quantization algorithm that assigns different precisions onto the filters of each layer. The candidate precision levels and assignment granularity are determined from our empirical study with the capability of preserving accuracy and improving hardware parallelism. Second, we apply multiple optimization techniques for the FPGA accelerator architecture in support of quantized computations, including DSP packing, weight reordering, and data packing, to enhance the overall throughput with the available resources. Moreover, a comprehensive resource model is developed to balance the allocation of FPGA computation resources (LUTs and DSPs) as well as data transfer and on-chip storage resources (BRAMs) to accelerate the computations in mixed precisions within each layer. Finally, to improve the portability of FILM-QNN, we implement it using Vivado High-Level Synthesis (HLS) on Xilinx PYNQ-Z2 and ZCU102 FPGA boards. Our experimental results of ResNet-18, ResNet-50, and MobileNet-V2 demonstrate that the implementations with intra-layer, mixed-precision (95% of 4-bit weights and 5% of 8-bit weights, and all 5-bit activations) can achieve comparable accuracy (70.47%, 77.25%, and 65.67% for the three models) as the 8-bit (and 32-bit) versions and comparable throughput (214.8 FPS, 109.1 FPS, and 537.9 FPS on ZCU102) as the 4-bit designs.},
booktitle = {Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {134–145},
numpages = {12},
keywords = {fpga, model compression, hardware acceleration, deep learning, mixed-precision quantization},
location = {Virtual Event, USA},
series = {FPGA '22}
}
@inproceedings{choi2021dance,
  title={Dance: Differentiable accelerator/network co-exploration},
  author={Choi, Kanghyun and Hong, Deokki and Yoon, Hojae and Yu, Joonsang and Kim, Youngsok and Lee, Jinho},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)},
  pages={337--342},
  year={2021},
  organization={IEEE}
}
@INPROCEEDINGS{venieris2016fccm,
  author={Venieris, Stylianos I. and Bouganis, Christos-Savvas},
  booktitle={2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)}, 
  title={fpgaConvNet: A Framework for Mapping Convolutional Neural Networks on FPGAs}, 
  year={2016},
  volume={},
  number={},
  pages={40-47},
  doi={10.1109/FCCM.2016.22}}


@INPROCEEDINGS{zhao2022fpl,
  author={Zhao, Ruizhe and Cheng, Jianyi and Luk, Wayne and Constantinides, George A.},
  booktitle={2022 32nd International Conference on Field-Programmable Logic and Applications (FPL)}, 
  title={POLSCA: Polyhedral High-Level Synthesis with Compiler Transformations}, 
  year={2022},
  volume={},
  number={},
  pages={235-242},
  doi={10.1109/FPL57034.2022.00044}}
@inproceedings{xu2022iccad,
author = {Xu, Ruifan and Xiao, Youwei and Luo, Jin and Liang, Yun},
title = {HECTOR: A Multi-Level Intermediate Representation for Hardware Synthesis Methodologies},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549370},
doi = {10.1145/3508352.3549370},
abstract = {Hardware synthesis requires a complicated process to generate synthesizable register transfer level (RTL) code. High-level synthesis tools can automatically transform a high-level description into hardware design, while hardware generators adopt domain specific languages and synthesis flows for specific applications. The implementation of these tools generally requires substantial engineering efforts due to RTL's weak expressivity and low level of abstraction. Furthermore, different synthesis tools adopt different levels of intermediate representations (IR) and transformations. A unified IR obviously is a good way to lower the engineering cost and get competitive hardware design rapidly by exploring different synthesis methodologies.In this paper, we propose Hector, a two-level IR providing a unified intermediate representation for hardware synthesis methodologies. The high-level IR binds computation with a control graph annotated with timing information, while the low-level IR provides a concise way to describe hardware modules and elastic interconnections among them. Implemented based on the multi-level compiler infrastructure (MLIR), Hector's IRs can be converted to synthesizable RTL designs. To demonstrate the expressivity and versatility, we implement three synthesis approaches based on Hector: a high-level synthesis (HLS) tool, a systolic array generator, and a hardware accelerator. The hardware generated by Hector's HLS approach is comparable to that generated by the state-of-the-art HLS tools, and the other two cases outperform HLS implementations in performance and productivity.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {54},
numpages = {9},
keywords = {intermediate representation, hardware synthesis},
location = {San Diego, California},
series = {ICCAD '22}
}
@misc{vivado_hls,
author 	= {{Xilinx Vivado HLS}},
url 	= {https://www.xilinx.com/support/documentation-navigation/design-hubs/dh0012-vivado-high-level-synthesis-hub.html},
year	=	{2022}
}
@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{krizhevsky2017imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}
@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}
@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}
@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{liu2018darts,
  title={Darts: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv preprint arXiv:1806.09055},
  year={2018}
}
@article{conneau2016very,
  title={Very deep convolutional networks for text classification},
  author={Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"\i}c and Lecun, Yann},
  journal={arXiv preprint arXiv:1606.01781},
  year={2016}
}
@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}
@inproceedings{dey2017gate,
  title={Gate-variants of gated recurrent unit (GRU) neural networks},
  author={Dey, Rahul and Salem, Fathi M},
  booktitle={2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)},
  pages={1597--1600},
  year={2017},
  organization={IEEE}
}
@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}
@inproceedings{touvron2022deit,
  title={Deit iii: Revenge of the vit},
  author={Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIV},
  pages={516--533},
  year={2022},
  organization={Springer}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}
@inproceedings{wu2019fbnet,
  title={Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search},
  author={Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10734--10742},
  year={2019}
}
@article{cai2019once,
  title={Once-for-all: Train one network and specialize it for efficient deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  journal={arXiv preprint arXiv:1908.09791},
  year={2019}
}
@article{zhao2022rapid,
  title={Rapid model architecture adaption for meta-learning},
  author={Zhao, Yiren and Gao, Xitong and Shumailov, Ilia and Fusi, Nicolo and Mullins, Robert},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18721--18732},
  year={2022}
}
@article{lee2021hardware,
  title={Hardware-adaptive efficient latency prediction for nas via meta-learning},
  author={Lee, Hayeon and Lee, Sewoong and Chong, Song and Hwang, Sung Ju},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27016--27028},
  year={2021}
}
@inproceedings{dong2021hao,
  title={Hao: Hardware-aware neural architecture optimization for efficient inference},
  author={Dong, Zhen and Gao, Yizhao and Huang, Qijing and Wawrzynek, John and So, Hayden KH and Keutzer, Kurt},
  booktitle={2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={50--59},
  year={2021},
  organization={IEEE}
}
@article{jiang2020hardware,
  title={Hardware/software co-exploration of neural architectures},
  author={Jiang, Weiwen and Yang, Lei and Sha, Edwin Hsing-Mean and Zhuge, Qingfeng and Gu, Shouzhen and Dasgupta, Sakyasingha and Shi, Yiyu and Hu, Jingtong},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={39},
  number={12},
  pages={4805--4815},
  year={2020},
  publisher={IEEE}
}
@article{darvish2020pushing,
  title={Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point},
  author={Darvish Rouhani, Bita and Lo, Daniel and Zhao, Ritchie and Liu, Ming and Fowers, Jeremy and Ovtcharov, Kalin and Vinogradsky, Anna and Massengill, Sarah and Yang, Lita and Bittner, Ray and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={10271--10281},
  year={2020}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@inproceedings{choukroun2019low,
  title={Low-bit quantization of neural networks for efficient inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  pages={3009--3018},
  year={2019},
  organization={IEEE}
}

@inproceedings{wang2019haq,
  title={Haq: Hardware-aware automated quantization with mixed precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8612--8620},
  year={2019}
}

@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{li2020edd,
  title={Edd: Efficient differentiable dnn architecture and implementation co-search for embedded ai solutions},
  author={Li, Yuhong and Hao, Cong and Zhang, Xiaofan and Liu, Xinheng and Chen, Yao and Xiong, Jinjun and Hwu, Wen-mei and Chen, Deming},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@inproceedings{lin2021zen,
  title={Zen-nas: A zero-shot nas for high-performance image recognition},
  author={Lin, Ming and Wang, Pichao and Sun, Zhenhong and Chen, Hesen and Sun, Xiuyu and Qian, Qi and Li, Hao and Jin, Rong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={347--356},
  year={2021}
}