% Generated by IEEEtranS.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranS.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{abdelfattah2020best}
M.~S. Abdelfattah, {\L}.~Dudziak, T.~Chau, R.~Lee, H.~Kim, and N.~D. Lane,
  ``Best of both worlds: Automl codesign of a cnn and its hardware
  accelerator,'' in \emph{2020 57th ACM/IEEE Design Automation Conference
  (DAC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1--6.

\bibitem{adolphs2022boosting}
L.~Adolphs, C.~Buck, M.~Chen~Huebscher, M.~Ciaramita, L.~Espeholt, T.~Hofmann,
  Y.~Kilcher, S.~Rothe, P.~G. Sessa, L.~Sestorain~Saralegui \emph{et~al.},
  ``{Boosting Search Engines with Interactive Agents},'' \emph{Transactions on
  Machine Learning Research}, 2022.

\bibitem{vivado}
\BIBentryALTinterwordspacing
{AMD Xilinx}, ``{Vivado},'' 2023. [Online]. Available:
  \url{https://www.xilinx.com/products/design-tools/vivado.html}
\BIBentrySTDinterwordspacing

\bibitem{bai2019onnx}
J.~Bai, F.~Lu, K.~Zhang \emph{et~al.}, ``Onnx: Open neural network exchange,''
  \emph{GitHub repository}, p.~54, 2019.

\bibitem{bergstra2011algorithms}
J.~Bergstra, R.~Bardenet, Y.~Bengio, and B.~K{\'e}gl, ``Algorithms for
  hyper-parameter optimization,'' \emph{Advances in neural information
  processing systems}, vol.~24, 2011.

\bibitem{blott_trets2018}
\BIBentryALTinterwordspacing
M.~Blott, T.~B. Preu\ss{}er, N.~J. Fraser, G.~Gambardella, K.~O’brien,
  Y.~Umuroglu, M.~Leeser, and K.~Vissers, ``Finn-r: An end-to-end deep-learning
  framework for fast exploration of quantized neural networks,'' \emph{ACM
  Trans. Reconfigurable Technol. Syst.}, vol.~11, no.~3, dec 2018. [Online].
  Available: \url{https://doi.org/10.1145/3242897}
\BIBentrySTDinterwordspacing

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{stratus}
\BIBentryALTinterwordspacing
Cadence, ``{Stratus High-Level Synthesis},'' 2023. [Online]. Available:
  \url{https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/synthesis/stratus-high-level-synthesis.html}
\BIBentrySTDinterwordspacing

\bibitem{choi2021dance}
K.~Choi, D.~Hong, H.~Yoon, J.~Yu, Y.~Kim, and J.~Lee, ``Dance: Differentiable
  accelerator/network co-exploration,'' in \emph{2021 58th ACM/IEEE Design
  Automation Conference (DAC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 337--342.

\bibitem{choukroun2019low}
Y.~Choukroun, E.~Kravchik, F.~Yang, and P.~Kisilev, ``Low-bit quantization of
  neural networks for efficient inference,'' in \emph{2019 IEEE/CVF
  International Conference on Computer Vision Workshop (ICCVW)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2019, pp. 3009--3018.

\bibitem{circt}
C.~contributors, ``Circt-based hls compilation flows, debugging, and
  cosimulation tools.'' \url{https://github.com/circt-hls/circt-hls}, 2023.

\bibitem{hls}
P.~Coussy, D.~D. Gajski, M.~Meredith, and A.~Takach, ``An introduction to
  high-level synthesis,'' \emph{IEEE Design Test of Computers}, vol.~26, no.~4,
  pp. 8--17, July 2009.

\bibitem{darvish2020pushing}
B.~Darvish~Rouhani, D.~Lo, R.~Zhao, M.~Liu, J.~Fowers, K.~Ovtcharov,
  A.~Vinogradsky, S.~Massengill, L.~Yang, R.~Bittner \emph{et~al.}, ``Pushing
  the limits of narrow precision inferencing at cloud scale with microsoft
  floating point,'' \emph{Advances in neural information processing systems},
  vol.~33, pp. 10\,271--10\,281, 2020.

\bibitem{devito2022torchscript}
Z.~DeVito, ``Torchscript: Optimized execution of pytorch programs,''
  \emph{Retrieved January}, 2022.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{dong2021hao}
Z.~Dong, Y.~Gao, Q.~Huang, J.~Wawrzynek, H.~K. So, and K.~Keutzer, ``Hao:
  Hardware-aware neural architecture optimization for efficient inference,'' in
  \emph{2021 IEEE 29th Annual International Symposium on Field-Programmable
  Custom Computing Machines (FCCM)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2021, pp. 50--59.

\bibitem{duarte_arxiv2018}
J.~Duarte, S.~Han, P.~Harris, S.~Jindariani, E.~Kreinar, B.~Kreis, J.~Ngadiuba,
  M.~Pierini, R.~Rivera, N.~Tran \emph{et~al.}, ``Fast inference of deep neural
  networks in fpgas for particle physics,'' \emph{Journal of Instrumentation},
  vol.~13, no.~07, p. P07027, 2018.

\bibitem{fahim2021hls4ml}
F.~Fahim, B.~Hawks, C.~Herwig, J.~Hirschauer, S.~Jindariani, N.~Tran, L.~P.
  Carloni, G.~Di~Guglielmo, P.~Harris, J.~Krupa \emph{et~al.}, ``hls4ml: An
  open-source codesign workflow to empower scientific low-power machine
  learning devices,'' \emph{arXiv preprint arXiv:2103.05579}, 2021.

\bibitem{fu2021auto}
Y.~Fu, Y.~Zhang, Y.~Zhang, D.~Cox, and Y.~Lin, ``Auto-nba: Efficient and
  effective search over the joint space of networks, bitwidths, and
  accelerators,'' in \emph{International Conference on Machine Learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 3505--3517.

\bibitem{wang2021fpt}
J.~Gao, Y.~Qian, Y.~Hu, X.~Fan, W.-S. Luk, W.~Cao, and L.~Wang, ``Leta: A
  lightweight exchangeable-track accelerator for efficientnet based on fpga,''
  in \emph{2021 International Conference on Field-Programmable Technology
  (ICFPT)}, 2021, pp. 1--9.

\bibitem{jiang2020hardware}
W.~Jiang, L.~Yang, E.~H.-M. Sha, Q.~Zhuge, S.~Gu, S.~Dasgupta, Y.~Shi, and
  J.~Hu, ``Hardware/software co-exploration of neural architectures,''
  \emph{IEEE Transactions on Computer-Aided Design of Integrated Circuits and
  Systems}, vol.~39, no.~12, pp. 4805--4815, 2020.

\bibitem{korngiebel2021considering}
D.~M. Korngiebel and S.~D. Mooney, ``Considering the possibilities and pitfalls
  of generative pre-trained transformer 3 (gpt-3) in healthcare delivery,''
  \emph{NPJ Digital Medicine}, vol.~4, no.~1, p.~93, 2021.

\bibitem{kouris2020approximate}
A.~Kouris, S.~I. Venieris, M.~Rizakis, and C.-S. Bouganis, ``{Approximate LSTMs
  for time-constrained inference: Enabling fast reaction in self-driving
  cars},'' \emph{IEEE Consumer Electronics Magazine}, vol.~9, no.~4, pp.
  11--26, 2020.

\bibitem{lattner2004llvm}
C.~Lattner and V.~Adve, ``Llvm: A compilation framework for lifelong program
  analysis \& transformation,'' in \emph{International symposium on code
  generation and optimization, 2004. CGO 2004.}\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2004, pp. 75--86.

\bibitem{lattner2020mlir}
C.~Lattner, M.~Amini, U.~Bondhugula, A.~Cohen, A.~Davis, J.~Pienaar, R.~Riddle,
  T.~Shpeisman, N.~Vasilache, and O.~Zinenko, ``Mlir: A compiler infrastructure
  for the end of moore's law,'' \emph{arXiv preprint arXiv:2002.11054}, 2020.

\bibitem{li2021hw}
C.~Li, Z.~Yu, Y.~Fu, Y.~Zhang, Y.~Zhao, H.~You, Q.~Yu, Y.~Wang, and Y.~Lin,
  ``Hw-nas-bench: Hardware-aware neural architecture search benchmark,'' in
  \emph{The 9th International Conference on Learning Representations 2021 (ICLR
  2021)}, 2021.

\bibitem{li2020edd}
Y.~Li, C.~Hao, X.~Zhang, X.~Liu, Y.~Chen, J.~Xiong, W.-m. Hwu, and D.~Chen,
  ``Edd: Efficient differentiable dnn architecture and implementation co-search
  for embedded ai solutions,'' in \emph{2020 57th ACM/IEEE Design Automation
  Conference (DAC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  1--6.

\bibitem{lin2021zen}
M.~Lin, P.~Wang, Z.~Sun, H.~Chen, X.~Sun, Q.~Qian, H.~Li, and R.~Jin,
  ``Zen-nas: A zero-shot nas for high-performance image recognition,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, pp. 347--356.

\bibitem{liu2018darts}
H.~Liu, K.~Simonyan, and Y.~Yang, ``Darts: Differentiable architecture
  search,'' \emph{arXiv preprint arXiv:1806.09055}, 2018.

\bibitem{torch_mlir}
\BIBentryALTinterwordspacing
{LLVM}, ``{Torch-MLIR},'' 2023. [Online]. Available:
  \url{https://github.com/llvm/torch-mlir}
\BIBentrySTDinterwordspacing

\bibitem{niu_micro2022}
W.~Niu, J.~Guan, X.~Shen, Y.~Wang, G.~Agrawal, and B.~Ren, ``Gcd2: A globally
  optimizing compiler for mapping dnns to mobile dsps,'' in \emph{2022 55th
  IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 2022, pp.
  512--529.

\bibitem{reed2022torch}
J.~Reed, Z.~DeVito, H.~He, A.~Ussery, and J.~Ansel, ``torch. fx: Practical
  program capture and transformation for deep learning in python,''
  \emph{Proceedings of Machine Learning and Systems}, vol.~4, pp. 638--651,
  2022.

\bibitem{catapult}
\BIBentryALTinterwordspacing
Siemens, ``{Catapult High-Level Synthesis and Verification},'' 2023. [Online].
  Available:
  \url{https://eda.sw.siemens.com/en-US/ic/catapult-high-level-synthesis/}
\BIBentrySTDinterwordspacing

\bibitem{sun2022fpga}
\BIBentryALTinterwordspacing
M.~Sun, Z.~Li, A.~Lu, Y.~Li, S.-E. Chang, X.~Ma, X.~Lin, and Z.~Fang,
  ``Film-qnn: Efficient fpga acceleration of deep neural networks with
  intra-layer, mixed-precision quantization,'' in \emph{Proceedings of the 2022
  ACM/SIGDA International Symposium on Field-Programmable Gate Arrays}, ser.
  FPGA '22.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA:
  Association for Computing Machinery, 2022, p. 134–145. [Online]. Available:
  \url{https://doi.org/10.1145/3490422.3502364}
\BIBentrySTDinterwordspacing

\bibitem{tan2019efficientnet}
M.~Tan and Q.~Le, ``Efficientnet: Rethinking model scaling for convolutional
  neural networks,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 6105--6114.

\bibitem{venieris2016fccm}
S.~I. Venieris and C.-S. Bouganis, ``fpgaconvnet: A framework for mapping
  convolutional neural networks on fpgas,'' in \emph{2016 IEEE 24th Annual
  International Symposium on Field-Programmable Custom Computing Machines
  (FCCM)}, 2016, pp. 40--47.

\bibitem{wang2019haq}
K.~Wang, Z.~Liu, Y.~Lin, J.~Lin, and S.~Han, ``Haq: Hardware-aware automated
  quantization with mixed precision,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2019, pp. 8612--8620.

\bibitem{wang2022pvt}
W.~Wang, E.~Xie, X.~Li, D.-P. Fan, K.~Song, D.~Liang, T.~Lu, P.~Luo, and
  L.~Shao, ``Pvt v2: Improved baselines with pyramid vision transformer,''
  \emph{Computational Visual Media}, vol.~8, no.~3, pp. 415--424, 2022.

\bibitem{xiao2022towards}
Q.~Xiao and Y.~Liang, ``Towards agile dnn accelerator design using incremental
  synthesis on fpgas,'' in \emph{Proceedings of the 2022 ACM/SIGDA
  International Symposium on Field-Programmable Gate Arrays}, 2022, pp. 42--48.

\bibitem{vivado_hls}
\BIBentryALTinterwordspacing
{Xilinx Vivado HLS}, 2022. [Online]. Available:
  \url{https://www.xilinx.com/support/documentation-navigation/design-hubs/dh0012-vivado-high-level-synthesis-hub.html}
\BIBentrySTDinterwordspacing

\bibitem{xu2022iccad}
\BIBentryALTinterwordspacing
R.~Xu, Y.~Xiao, J.~Luo, and Y.~Liang, ``Hector: A multi-level intermediate
  representation for hardware synthesis methodologies,'' in \emph{Proceedings
  of the 41st IEEE/ACM International Conference on Computer-Aided Design}, ser.
  ICCAD '22.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA:
  Association for Computing Machinery, 2022. [Online]. Available:
  \url{https://doi.org/10.1145/3508352.3549370}
\BIBentrySTDinterwordspacing

\bibitem{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster, D.~Simig,
  P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer, ``Opt: Open pre-trained
  transformer language models,'' 2022.

\bibitem{zhao2022fpl}
R.~Zhao, J.~Cheng, W.~Luk, and G.~A. Constantinides, ``Polsca: Polyhedral
  high-level synthesis with compiler transformations,'' in \emph{2022 32nd
  International Conference on Field-Programmable Logic and Applications (FPL)},
  2022, pp. 235--242.

\bibitem{zheng2021se}
W.~Zheng, W.~Tang, L.~Jiang, and C.-W. Fu, ``{SE-SSD: Self-ensembling
  single-stage object detector from point cloud},'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp.
  14\,494--14\,503.

\bibitem{zoph2016neural}
B.~Zoph and Q.~V. Le, ``Neural architecture search with reinforcement
  learning,'' \emph{arXiv preprint arXiv:1611.01578}, 2016.

\end{thebibliography}
