@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}
@article{fahim2021hls4ml,
  title={hls4ml: An open-source codesign workflow to empower scientific low-power machine learning devices},
  author={Fahim, Farah and Hawks, Benjamin and Herwig, Christian and Hirschauer, James and Jindariani, Sergo and Tran, Nhan and Carloni, Luca P and Di Guglielmo, Giuseppe and Harris, Philip and Krupa, Jeffrey and others},
  journal={arXiv preprint arXiv:2103.05579},
  year={2021}
}

@inproceedings{coward2023automating,
  title={Automating constraint-aware datapath optimization using e-graphs},
  author={Coward, Samuel and Constantinides, George A and Drane, Theo},
  booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@article{cheng2023seer,
  title={SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR},
  author={Cheng, Jianyi and Coward, Samuel and Chelini, Lorenzo and Barbalho, Rafael and Drane, Theo},
  journal={arXiv preprint arXiv:2308.07654},
  year={2023}
}

@inproceedings{zhao2022polsca,
  title={Polsca: Polyhedral high-level synthesis with compiler transformations},
  author={Zhao, Ruizhe and Cheng, Jianyi and Luk, Wayne and Constantinides, George A},
  booktitle={2022 32nd International Conference on Field-Programmable Logic and Applications (FPL)},
  pages={235--242},
  year={2022},
  organization={IEEE}
}

@inproceedings{lattner2004llvm,
  title={LLVM: A compilation framework for lifelong program analysis \& transformation},
  author={Lattner, Chris and Adve, Vikram},
  booktitle={International symposium on code generation and optimization, 2004. CGO 2004.},
  pages={75--86},
  year={2004},
  organization={IEEE}
}

@inproceedings{lattner2021mlir,
  title={MLIR: Scaling compiler infrastructure for domain specific computation},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={2--14},
  year={2021},
  organization={IEEE}
}

@inproceedings{xu2023eliminating,
  title={Eliminating excessive dynamism of dataflow circuits using model checking},
  author={Xu, Jiahui and Murphy, Emmet and Cortadella, Jordi and Josipovic, Lana},
  booktitle={Proceedings of the 2023 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
  pages={27--37},
  year={2023}
}
@inproceedings{umuroglu2017finn,
  title={Finn: A framework for fast, scalable binarized neural network inference},
  author={Umuroglu, Yaman and Fraser, Nicholas J and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
  booktitle={Proceedings of the 2017 ACM/SIGDA international symposium on field-programmable gate arrays},
  pages={65--74},
  year={2017}
}
@inproceedings{venieris2016fpgaconvnet,
  title={fpgaConvNet: A framework for mapping convolutional neural networks on FPGAs},
  author={Venieris, Stylianos I and Bouganis, Christos-Savvas},
  booktitle={2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={40--47},
  year={2016},
  organization={IEEE}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@inproceedings{ye2022scalehls,
  title={Scalehls: A new scalable high-level synthesis framework on multi-level intermediate representation},
  author={Ye, Hanchen and Hao, Cong and Cheng, Jianyi and Jeong, Hyunmin and Huang, Jack and Neuendorffer, Stephen and Chen, Deming},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={741--755},
  year={2022},
  organization={IEEE}
}

@article{sohrabizadeh2022autodse,
  title={AutoDSE: Enabling software programmers to design efficient FPGA accelerators},
  author={Sohrabizadeh, Atefeh and Yu, Cody Hao and Gao, Min and Cong, Jason},
  journal={ACM Transactions on Design Automation of Electronic Systems (TODAES)},
  volume={27},
  number={4},
  pages={1--27},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

@article{devito2022torchscript,
  title={Torchscript: Optimized execution of pytorch programs},
  author={DeVito, Zachary},
  journal={Retrieved January},
  year={2022}
}

@article{reed2022torch,
  title={torch. fx: Practical Program Capture and Transformation for Deep Learning in Python},
  author={Reed, James and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={638--651},
  year={2022}
}

@inproceedings{fan2022adaptable,
  title={Adaptable butterfly accelerator for attention-based NNs via hardware and algorithm co-design},
  author={Fan, Hongxiang and Chau, Thomas and Venieris, Stylianos I and Lee, Royson and Kouris, Alexandros and Luk, Wayne and Lane, Nicholas D and Abdelfattah, Mohamed S},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={599--615},
  year={2022},
  organization={IEEE}
}
@inproceedings{ham2021elsa,
  title={ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks},
  author={Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={692--705},
  year={2021},
  organization={IEEE}
}

@inproceedings{lu2021sanger,
  title={Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture},
  author={Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={977--991},
  year={2021}
}

@inproceedings{li2020ftrans,
  title={Ftrans: energy-efficient acceleration of transformers using fpga},
  author={Li, Bingbing and Pandey, Santosh and Fang, Haowen and Lyv, Yanjun and Li, Ji and Chen, Jieyang and Xie, Mimi and Wan, Lipeng and Liu, Hang and Ding, Caiwen},
  booktitle={Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
  pages={175--180},
  year={2020}
}

@inproceedings{hong2022dfx,
  title={DFX: A low-latency multi-FPGA appliance for accelerating transformer-based text generation},
  author={Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={616--630},
  year={2022},
  organization={IEEE}
}

@inproceedings{ham20203,
  title={A\^{} 3: Accelerating attention mechanisms in neural networks with approximation},
  author={Ham, Tae Jun and Jung, Sung Jun and Kim, Seonghak and Oh, Young H and Park, Yeonhong and Song, Yoonho and Park, Jung-Hun and Lee, Sanghee and Park, Kyoung and Lee, Jae W and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={328--341},
  year={2020},
  organization={IEEE}
}

@article{bai2019onnx,
  title={Onnx: Open neural network exchange},
  author={Bai, Junjie and Lu, Fang and Zhang, Ke and others},
  journal={GitHub repository},
  pages={54},
  year={2019}
}

@inproceedings{qin2023fact,
  title={FACT: FFN-attention Co-optimized transformer architecture with eager correlation prediction},
  author={Qin, Yubin and Wang, Yang and Deng, Dazheng and Zhao, Zhiren and Yang, Xiaolong and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--14},
  year={2023}
}

@article{kim2023squeezellm,
  title={Squeezellm: Dense-and-sparse quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}

@article{zeng2024flightllm,
  title={FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGA},
  author={Zeng, Shulin and Liu, Jun and Dai, Guohao and Yang, Xinhao and Fu, Tianyu and Wang, Hongyi and Ma, Wenheng and Sun, Hanbo and Li, Shiyao and Huang, Zixiao and others},
  journal={arXiv preprint arXiv:2401.03868},
  year={2024}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{wang2019glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  note={In the Proceedings of ICLR.},
  year={2019}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{ye2023hida,
  title={HIDA: A Hierarchical Dataflow Compiler for High-Level Synthesis},
  author={Ye, Hanchen and Jun, Hyegang and Chen, Deming},
  journal={arXiv preprint arXiv:2311.03379},
  year={2023}
}

@inproceedings{zhang2018dnnbuilder,
  title={DNNBuilder: An automated tool for building high-performance DNN hardware accelerators for FPGAs},
  author={Zhang, Xiaofan and Wang, Junsong and Zhu, Chao and Lin, Yonghua and Xiong, Jinjun and Hwu, Wen-mei and Chen, Deming},
  booktitle={2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}

@inproceedings{zhao2023sigma,
  title={Sigma: Compiling einstein summations to locality-aware dataflow},
  author={Zhao, Tian and Rucker, Alexander and Olukotun, Kunle},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={718--732},
  year={2023}
}

@inproceedings{rucker2024revet,
  title={Revet: A language and compiler for dataflow threads},
  author={Rucker, Alexander C and Sundram, Shiv and Smith, Coleman and Vilim, Matthew and Prabhakar, Raghu and Kj{\o}lstad, Fredrik and Olukotun, Kunle},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1--14},
  year={2024},
  organization={IEEE}
}

@article{cytron1991efficiently,
  title={Efficiently computing static single assignment form and the control dependence graph},
  author={Cytron, Ron and Ferrante, Jeanne and Rosen, Barry K and Wegman, Mark N and Zadeck, F Kenneth},
  journal={ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume={13},
  number={4},
  pages={451--490},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@article{carloni2001theory,
  title={Theory of latency-insensitive design},
  author={Carloni, Luca P and McMillan, Kenneth L and Sangiovanni-Vincentelli, Alberto L},
  journal={IEEE Transactions on computer-aided design of integrated circuits and systems},
  volume={20},
  number={9},
  pages={1059--1076},
  year={2001},
  publisher={IEEE}
}
@article{miao2023specinfer,
  title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Wong, Rae Ying Yee and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
  journal={arXiv preprint arXiv:2305.09781},
  year={2023}
}
@inproceedings{rosser2018cocotb,
  title={Cocotb: a Python-based digital logic verification framework},
  author={Rosser, Benjamin John},
  booktitle={Micro-electronics Section seminar},
  year={2018},
  organization={CERN, Geneva, Switzerland}
}
@inproceedings{filer2017demonstration,
  title={Demonstration and performance analysis of 4 Tb/s DWDM metro-DCI system with 100G PAM4 QSFP28 modules},
  author={Filer, Mark and Searcy, Steven and Fu, Yang and Nagarajan, Radhakrishnan and Tibuleac, Sorin},
  booktitle={Optical Fiber Communication Conference},
  pages={W4D--4},
  year={2017},
  organization={Optica Publishing Group}
}
@inproceedings{fox2021block,
  title={A block minifloat representation for training deep neural networks},
  author={Fox, Sean and Rasoulinezhad, Seyedramin and Faraone, Julian and Leong, Philip and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@proceedings{minifloat,
    author = {Cermelli, C.A. and Roddier, D.G. and Busso, C.C.},
    title = "{MINIFLOAT: A Novel Concept of Minimal Floating Platform For Marginal Field Development}",
    volume = {All Days},
    series = {International Ocean and Polar Engineering Conference},
    year = {2004},
    month = {05},
    abstract = "{This paper describes a new type of minimal floating offshore platform, named MINIFLOAT, suitable for developing deepwater marginal fields. The paper covers the design methodology and hydrodynamic analysis of the platform. The numerical method used to determine the floater dynamics is presented. Results of the platform response are given for a specific case. The platform described in this paper can be used in a number of ways to support various equipment used in the production of oil and gas from deep and ultra-deep water hydrocarbon fields: - support and control to a subsea wellhead, including chemical injection, hydraulic power, telecommunication - host water-injection equipment to assist field production - host first stage separation equipment to allow long offsets to hub/shore - provide power supply to heated flowlines, multiphase pumps or other subsea installations. This new concept enables safe and cost-efficient development of marginal fields in deep and ultra deep water. It allows application of advanced subsea technologies to reduce field development cost, while maintaining a surface "presence" for reduced cost, enhanced reliability and ease of maintenance.As the demand for hydrocarbons in the next 10 years is expected to increase by as much as 5 percent a year, and as very large oil and gas fields are less likely to be found, oil and gas companies are looking into means of developing their proven smaller reserves. Significant exploration activities have taken place in recent years in the deep waters of the Gulf of Mexico resulting in numerous discoveries. However, many of these fields do not contain sufficiently large amount of oil or gas to justify the expenses of a stand-alone field development, such as a production platform and pipeline infrastructure.}",
    note = {ISOPE-I-04-038},
    eprint = {https://onepetro.org/ISOPEIOPEC/proceedings-pdf/ISOPE04/All-ISOPE04/ISOPE-I-04-038/1859875/isope-i-04-038.pdf},
}
@misc{hacc,
author 	= {{Heterogeneous Accelerated Compute Cluster (HACC) Program}},
url 	= {https://www.xilinx.com/support/university/xup-hacc.html},
year	=	{2023}
}
@misc{vivado,
author 	= {{AMD Xilinx Vivado}},
url 	= {https://www.xilinx.com/support/university/vivado.html},
year	=	{2023}
}
@misc{zhang2023revisiting,
      title={Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?}, 
      author={Cheng Zhang and Jianyi Cheng and Ilia Shumailov and George A. Constantinides and Yiren Zhao},
      year={2023},
      eprint={2310.05079},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{mxfp,
author 	= {{OCP Microscaling Formats (MX) Specification}},
url 	= {https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf},
year	=	{2023}
}
@misc{ms_catapult,
author 	= {{Microsoft Project Catapult}},
url 	= {https://www.microsoft.com/en-us/research/project/project-catapult/},
year	=	{2023}
}
@INPROCEEDINGS{9798375,
  author={Handagala, Suranga and Leeser, Miriam and Patle, Kalyani and Zink, Michael},
  booktitle={IEEE INFOCOM 2022 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={Network Attached FPGAs in the Open Cloud Testbed (OCT)}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/INFOCOMWKSHPS54753.2022.9798375}}

@misc{pynvml,
author 	= {{pyNVML}},
url 	= {https://pypi.org/project/nvidia-ml-py/},
year	=	{2023}
}
@misc{huggingface,
author 	= {{Hugging Face}},
url 	= {https://huggingface.co/},
year	=	{2023}
}
@article{kahan1996ieee,
  title={IEEE standard 754 for binary floating-point arithmetic},
  author={Kahan, William},
  journal={Lecture Notes on the Status of IEEE},
  volume={754},
  number={94720-1776},
  pages={11},
  year={1996}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@misc{zhang2020ternarybert,
      title={TernaryBERT: Distillation-aware Ultra-low Bit BERT}, 
      author={Wei Zhang and Lu Hou and Yichun Yin and Lifeng Shang and Xiao Chen and Xin Jiang and Qun Liu},
      year={2020},
      eprint={2009.12812},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bai2021binarybert,
      title={BinaryBERT: Pushing the Limit of BERT Quantization}, 
      author={Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and Xin Jiang and Qun Liu and Michael Lyu and Irwin King},
      year={2021},
      eprint={2012.15701},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{Zafrir_2019,
	doi = {10.1109/emc2-nips53020.2019.00016},
	url = {https://doi.org/10.1109%2Femc2-nips53020.2019.00016},
	year = 2019,
	month = {dec},
	publisher = {{IEEE}},
	author = {Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
	title = {Q8BERT: Quantized 8Bit {BERT}},
	booktitle = {2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - {NeurIPS} Edition ({EMC}2-{NIPS})}
}
@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{shen2019qbert,
      title={Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT}, 
      author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
      year={2019},
      eprint={1909.05840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}
@article{yao2022zeroquant,
  title={ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}
@article{xiao2022smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2022}
}
@misc{lin2017accurate,
      title={Towards Accurate Binary Convolutional Neural Network}, 
      author={Xiaofan Lin and Cong Zhao and Wei Pan},
      year={2017},
      eprint={1711.11294},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhang2018lqnets,
      title={LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks}, 
      author={Dongqing Zhang and Jiaolong Yang and Dongqiangzi Ye and Gang Hua},
      year={2018},
      eprint={1807.10029},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{sun2019hybrid,
  title={Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks},
  author={Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{drumond2018training,
  title={Training dnns with hybrid block floating point},
  author={Drumond, Mario and Lin, Tao and Jaggi, Martin and Falsafi, Babak},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{kalliojarvi1996roundoff,
  title={Roundoff errors in block-floating-point systems},
  author={Kalliojarvi, Kari and Astola, Jaakko},
  journal={IEEE transactions on signal processing},
  volume={44},
  number={4},
  pages={783--790},
  year={1996},
  publisher={IEEE}
}
@article{miyashita2016convolutional,
  title={Convolutional neural networks using logarithmic data representation},
  author={Miyashita, Daisuke and Lee, Edward H and Murmann, Boris},
  journal={arXiv preprint arXiv:1603.01025},
  year={2016}
}
@article{dai2021vs,
  title={Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference},
  author={Dai, Steve and Venkatesan, Rangha and Ren, Mark and Zimmer, Brian and Dally, William and Khailany, Brucek},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={873--884},
  year={2021}
}
@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1325--1334},
  year={2019}
}
@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}
@article{darvish2020pushing,
  title={Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point},
  author={Darvish Rouhani, Bita and Lo, Daniel and Zhao, Ritchie and Liu, Ming and Fowers, Jeremy and Ovtcharov, Kalin and Vinogradsky, Anna and Massengill, Sarah and Yang, Lita and Bittner, Ray and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={10271--10281},
  year={2020}
}
@article{rouhani2023shared,
  title={Shared Microexponents: A Little Shifting Goes a Long Way},
  author={Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and others},
  journal={arXiv preprint arXiv:2302.08007},
  year={2023}
}
@article{wu2018training,
  title={Training and inference with integers in deep neural networks},
  author={Wu, Shuang and Li, Guoqi and Chen, Feng and Shi, Luping},
  journal={arXiv preprint arXiv:1802.04680},
  year={2018}
}
@inproceedings{dong2019hawq,
  title={Hawq: Hessian aware quantization of neural networks with mixed-precision},
  author={Dong, Zhen and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={293--302},
  year={2019}
}
@inproceedings{habi2020hmq,
  title={Hmq: Hardware friendly mixed precision quantization block for cnns},
  author={Habi, Hai Victor and Jennings, Roy H and Netzer, Arnon},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXVI 16},
  pages={448--463},
  year={2020},
  organization={Springer}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}
@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@misc{gpt-neo,
  author = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  title = {{GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}},
  month  = mar,
  year  = 2021,
  note  = {{If you use this software, please cite it using these metadata.}},
  publisher  = {Zenodo},
  version = {1.0},
  doi = {10.5281/zenodo.5297715},
  url = {https://doi.org/10.5281/zenodo.5297715}
}
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@misc{2023-dettmers-bitsandbytes,
  title = {Bitsandbytes},
  author = {Dettmers, Tim},
  year = 2023,
  month = jun,
  urldate = {2023-06-19},
  abstract = {8-bit CUDA functions for PyTorch},
  copyright = {MIT},
  keywords = {notion},
  version = {7c65101},
  url = {https://github.com/TimDettmers/bitsandbytes}
}
@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}
@article{wu2018mixed,
  title={Mixed precision quantization of convnets via differentiable neural architecture search},
  author={Wu, Bichen and Wang, Yanghan and Zhang, Peizhao and Tian, Yuandong and Vajda, Peter and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1812.00090},
  year={2018}
}
@inproceedings{wang2019haq,
  title={Haq: Hardware-aware automated quantization with mixed precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8612--8620},
  year={2019}
}
@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}
@inproceedings{Wolf_Transformers_State-of-the-Art_Natural_2020,
author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
month = oct,
pages = {38--45},
publisher = {Association for Computational Linguistics},
title = {{Transformers: State-of-the-Art Natural Language Processing}},
url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
year = {2020}
}
@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}
% wikitext
@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
% arc easy
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
% lambada
@inproceedings{paperno-etal-2016-lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}
% piqa
@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}
% glue: qnli
@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}
% sst2
@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}
% mrpc
@inproceedings{dolan-brockett-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}
% cola
@article{warstadt-etal-2019-neural,
    title = "Neural Network Acceptability Judgments",
    author = "Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1040",
    doi = "10.1162/tacl_a_00290",
    pages = "625--641",
    abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.",
}
@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}
@misc{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}
@inproceedings{Paszke_PyTorch_An_Imperative_2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alch√©-Buc, F. and Fox, E. and Garnett, R.},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}
@article{wei2022outlier,
  title={Outlier suppression: Pushing the limit of low-bit transformer language models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  journal={arXiv preprint arXiv:2209.13325},
  year={2022}
}
@article{bondarenko2021understanding,
  title={Understanding and overcoming the challenges of efficient transformer quantization},
  author={Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2109.12948},
  year={2021}
}
@inproceedings{roemmele2011choice,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI spring symposium: logical formalizations of commonsense reasoning},
  pages={90--95},
  year={2011}
}
@misc{chatgpt,   
    title = {Introducing ChatGPT},   
    url = {//https://openai.com/blog/chatgpt}, 
    author = {OpenAI},    
    year = {2022},   
    note = {Accessed on June, 23th, 2023} 
}
@article{hur2023fast,
  title={A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks},
  author={Hur, Suyeon and Na, Seongmin and Kwon, Dongup and Kim, Joonsung and Boutros, Andrew and Nurvitadhi, Eriko and Kim, Jangwoo},
  journal={ACM Transactions on Architecture and Code Optimization},
  volume={20},
  number={1},
  pages={1--24},
  year={2023},
  publisher={ACM New York, NY}
}
@inproceedings{liu2021hardware,
  title={Hardware acceleration of fully quantized bert for efficient natural language processing},
  author={Liu, Zejian and Li, Gang and Cheng, Jian},
  booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={513--516},
  year={2021},
  organization={IEEE}
}
@inproceedings{peng2022length,
  title={A length adaptive algorithm-hardware co-design of transformer on fpga through sparse attention and dynamic pipelining},
  author={Peng, Hongwu and Huang, Shaoyi and Chen, Shiyang and Li, Bingbing and Geng, Tong and Li, Ang and Jiang, Weiwen and Wen, Wujie and Bi, Jinbo and Liu, Hang and others},
  booktitle={Proceedings of the 59th ACM/IEEE Design Automation Conference},
  pages={1135--1140},
  year={2022}
}
@inproceedings{plagwitz2022trac,
  title={TRAC: Compilation-Based Design of Transformer Accelerators for FPGAs},
  author={Plagwitz, Patrick and Hannig, Frank and Teich, J{\"u}rgen},
  booktitle={2022 32nd International Conference on Field-Programmable Logic and Applications (FPL)},
  pages={17--23},
  year={2022},
  organization={IEEE}
}
@inproceedings{khataei2023approximate,
  title={Approximate Hybrid Binary-Unary Computing with Applications in BERT Language Model and Image Processing},
  author={Khataei, Alireza and Singh, Gaurav and Bazargan, Kia},
  booktitle={Proceedings of the 2023 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
  pages={165--175},
  year={2023}
}
@inproceedings{bai2023transformer,
  title={Transformer-OPU: An FPGA-based Overlay Processor for Transformer Networks},
  author={Bai, Yueyin and Zhou, Hao and Zhao, Keqing and Chen, Jianli and Yu, Jun and Wang, Kun},
  booktitle={2023 IEEE 31st Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={221--221},
  year={2023},
  organization={IEEE}
}
@inproceedings{guo2023olive,
  title={OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization},
  author={Guo, Cong and Tang, Jiaming and Hu, Weiming and Leng, Jingwen and Zhang, Chen and Yang, Fan and Liu, Yunxin and Guo, Minyi and Zhu, Yuhao},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}
@article{feng2020language,
  title={Language-agnostic BERT sentence embedding},
  author={Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
  journal={arXiv preprint arXiv:2007.01852},
  year={2020}
}
@inproceedings{yang2020bert,
  title={Bert representations for video question answering},
  author={Yang, Zekun and Garcia, Noa and Chu, Chenhui and Otani, Mayu and Nakashima, Yuta and Takemura, Haruo},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1556--1565},
  year={2020}
}
@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}
@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}
@article{jia2019beyond,
  title={Beyond Data and Model Parallelism for Deep Neural Networks.},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={1--13},
  year={2019}
}
@article{deb2002fast,
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II},
  author={Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal={IEEE transactions on evolutionary computation},
  volume={6},
  number={2},
  pages={182--197},
  year={2002},
  publisher={IEEE}
}
@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}
@article{alonso2021elastic,
  title={Elastic-df: Scaling performance of dnn inference in fpga clouds through automatic partitioning},
  author={Alonso, Tobias and Petrica, Lucian and Ruiz, Mario and Petri-Koenig, Jakoba and Umuroglu, Yaman and Stamelos, Ioannis and Koromilas, Elias and Blott, Michaela and Vissers, Kees},
  journal={ACM Transactions on Reconfigurable Technology and Systems (TRETS)},
  volume={15},
  number={2},
  pages={1--34},
  year={2021},
  publisher={ACM New York, NY}
}

@inproceedings{darvish2023with,
author = {Darvish Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and Shao, Lai and Kolhe, Gaurav and Melts, Dimitry and Klar, Jasmine and L'Heureux, Renee and Perry, Matt and Burger, Doug and Chung, Eric and Deng, Zhaoxia (Summer) and Naghshineh, Sam and Park, Jongsoo and Naumov, Maxim},
title = {With Shared Microexponents, A Little Shifting Goes a Long Way},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589351},
doi = {10.1145/3579371.3589351},
abstract = {This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {83},
numpages = {13},
keywords = {AI data types, compute efficiency, artificial intelligence},
location = {Orlando, FL, USA},
series = {ISCA '23}
}



@inproceedings{jouppi2023tpu,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5\% of system cost and <3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {energy, GPU, IPU, carbon emissions, supercomputer, CO2 equivalent emissions, domain specific architecture, power usage effectiveness, TPU, warehouse scale computer, optical interconnect, machine learning, embeddings, large language model, reconfigurable},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@article{tarafdar2021aigean,
  title={AIgean: An open framework for deploying machine learning on heterogeneous clusters},
  author={Tarafdar, Naif and Di Guglielmo, Giuseppe and Harris, Philip C and Krupa, Jeffrey D and Loncar, Vladimir and Rankin, Dylan S and Tran, Nhan and Wu, Zhenbin and Shen, Qianfeng and Chow, Paul},
  journal={ACM Transactions on Reconfigurable Technology and Systems (TRETS)},
  volume={15},
  number={3},
  pages={1--32},
  year={2021},
  publisher={ACM New York, NY}
}

@article{elangovan2022ax,
  title={Ax-BxP: Approximate blocked computation for precision-reconfigurable deep neural network acceleration},
  author={Elangovan, Reena and Jain, Shubham and Raghunathan, Anand},
  journal={ACM Transactions on Design Automation of Electronic Systems (TODAES)},
  volume={27},
  number={3},
  pages={1--20},
  year={2022},
  publisher={ACM New York, NY}
}

@article{tarafdar2018galapagos,
  title={Galapagos: A full stack approach to FPGA integration in the cloud},
  author={Tarafdar, Naif and Eskandari, Nariman and Sharma, Varun and Lo, Charles and Chow, Paul},
  journal={IEEE Micro},
  volume={38},
  number={6},
  pages={18--24},
  year={2018},
  publisher={IEEE}
}



@inproceedings{park2017scale,
  title={Scale-out acceleration for machine learning},
  author={Park, Jongse and Sharma, Hardik and Mahajan, Divya and Kim, Joon Kyung and Olds, Preston and Esmaeilzadeh, Hadi},
  booktitle={Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={367--381},
  year={2017}
}

@inproceedings{zadeh2020gobo,
  title={Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference},
  author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={811--824},
  year={2020},
  organization={IEEE}
}

@inproceedings{song2020drq,
  title={Drq: dynamic region-based quantization for deep neural network acceleration},
  author={Song, Zhuoran and Fu, Bangqi and Wu, Feiyang and Jiang, Zhaoming and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={1010--1021},
  year={2020},
  organization={IEEE}
}

@inproceedings{park2018energy,
  title={Energy-efficient neural network accelerator based on outlier-aware low-precision computation},
  author={Park, Eunhyeok and Kim, Dongyoung and Yoo, Sungjoo},
  booktitle={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  pages={688--698},
  year={2018},
  organization={IEEE}
}

@inproceedings{fan2023taskfusion,
author = {Fan, Zichen and Zhang, Qirui and Abillama, Pierre and Shoouri, Sara and Lee, Changwoo and Blaauw, David and Kim, Hun-Seok and Sylvester, Dennis},
title = {TaskFusion: An Efficient Transfer Learning Architecture with Dual Delta Sparsity for Multi-Task Natural Language Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589040},
doi = {10.1145/3579371.3589040},
abstract = {The combination of pre-trained models and task-specific fine-tuning schemes, such as BERT, has achieved great success in various natural language processing (NLP) tasks. However, the large memory and computation costs of such models make it challenging to deploy them in edge devices. Moreover, in real-world applications like chatbots, multiple NLP tasks need to be processed together to achieve higher response credibility. Running multiple NLP tasks with specialized models for each task increases the latency and memory cost latency linearly with the number of tasks. Though there have been recent works on parameter-shared tuning that aim to reduce the total parameter size by partially sharing weights among multiple tasks, computation remains intensive and redundant despite different tasks using the same input. In this work, we identify that a significant portion of activations and weights can be reused among different tasks, to reduce cost and latency for efficient multi-task NLP. Specifically, we propose TaskFusion, an efficient transfer learning software-hardware co-design that exploits delta sparsity in both weights and activations to boost data sharing among tasks. For training, TaskFusion uses ‚Ñì1 regularization on delta activation to learn inter-task data redundancies. A novel hardware-aware sub-task inference algorithm is proposed to exploit the dual delta sparsity. We then designed a dedicated heterogeneous architecture to accelerate multi-task inference with an optimized scheduling to increase hardware utilization and reduce off-chip memory access. Extensive experiments demonstrate that TaskFusion can reduce the number of floating point operations (FLOPs) by over 73\% in multi-task NLP with negligible accuracy loss, while adding a new task at the cost of only < 2\% parameter size increase. With the proposed architecture and optimized scheduling, Task-Fusion can achieve 1.48--2.43\texttimes{} performance and 1.62--3.77\texttimes{} energy efficiency than those using state-of-the-art single-task accelerators for multi-task NLP applications.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {5},
numpages = {14},
keywords = {accelerator, heterogeneous architecture, deep learning, sparsity, transfer learning, natural language processing, transformer, multi-task},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{tambe2021edgebert,
author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
title = {EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480095},
doi = {10.1145/3466752.3480095},
abstract = {Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 \texttimes{}, 2.5 \texttimes{}, and 53 \texttimes{} lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {830‚Äì844},
numpages = {15},
keywords = {embedded non-volatile memories, natural language processing, latency-aware, software and hardware co-design},
location = {Virtual Event, Greece},
series = {MICRO '21}
}



@inproceedings{hegde2021mind,
  title={Mind mappings: enabling efficient algorithm-accelerator mapping space search},
  author={Hegde, Kartik and Tsai, Po-An and Huang, Sitao and Chandra, Vikas and Parashar, Angshuman and Fletcher, Christopher W},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={943--958},
  year={2021}
}

@inproceedings{zhang2022full,
  title={A full-stack search technique for domain optimized deep learning accelerators},
  author={Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={27--42},
  year={2022}
}
@inproceedings{zheng2022astitch,
  title={AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures},
  author={Zheng, Zhen and Yang, Xuanda and Zhao, Pengzhan and Long, Guoping and Zhu, Kai and Zhu, Feiwen and Zhao, Wenyi and Liu, Xiaoyong and Yang, Jun and Zhai, Jidong and others},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={359--373},
  year={2022}
}

@inproceedings{wang2022overlap,
  title={Overlap communication with dependent computation via decomposition in large deep learning models},
  author={Wang, Shibo and Wei, Jinliang and Sabne, Amit and Davis, Andy and Ilbeyi, Berkin and Hechtman, Blake and Chen, Dehao and Murthy, Karthik Srinivasa and Maggioni, Marcello and Zhang, Qiao and others},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={93--106},
  year={2022}
}

@inproceedings{kao2023flat,
  title={FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks},
  author={Kao, Sheng-Chun and Subramanian, Suvinay and Agrawal, Gaurav and Yazdanbakhsh, Amir and Krishna, Tushar},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={295--310},
  year={2023}
}

@inproceedings{song2023optimus,
  title={Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression},
  author={Song, Jaeyong and Yim, Jinkyu and Jung, Jaewon and Jang, Hongsun and Kim, Hyung-Jin and Kim, Youngsok and Lee, Jinho},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={560--573},
  year={2023}
}

@inproceedings{guo2023sti,
  title={STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining},
  author={Guo, Liwei and Choe, Wonkyo and Lin, Felix Xiaozhu},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={791--803},
  year={2023}
}

@article{kalamkar2019study,
  title={A study of BFLOAT16 for deep learning training},
  author={Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and others},
  journal={arXiv preprint arXiv:1905.12322},
  year={2019}
}

@inproceedings{darvish2023shared,
  title={With Shared Microexponents, A Little Shifting Goes a Long Way},
  author={Darvish Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--13},
  year={2023}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{micikevicius2022fp8,
  title={FP8 formats for deep learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@article{harma2022accuracy,
  title={Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training},
  author={Harma, Simla Burcu and S{\"o}nmez, Canberk and Falsafi, Babak and Jaggi, Martin and Oh, Yunho},
  journal={arXiv preprint arXiv:2211.10737},
  year={2022}
}

@article{reddy2019coqa,
  title={Coqa: A conversational question answering challenge},
  author={Reddy, Siva and Chen, Danqi and Manning, Christopher D},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={249--266},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~‚Ä¶}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}
@inproceedings{ozaki2020multiobjective,
  title={Multiobjective tree-structured parzen estimator for computationally expensive optimization problems},
  author={Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
  booktitle={Proceedings of the 2020 genetic and evolutionary computation conference},
  pages={533--541},
  year={2020}
}