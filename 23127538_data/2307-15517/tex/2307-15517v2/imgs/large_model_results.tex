\begin{table*}
\caption{Comparison with four GPU devices for 7B LLMs. MASE adopts a custom type named MSFP6 (block size of 16, 8-bit exponent, and 6-bit mantissa), which outperforms all other works in energy efficiency. The state-of-the-art {\tt LLM.int8} on GPUs uses {\em tensor core hardware with manually implemented CUDA kernels} for the ``gather" and ``scatter" operations, and performs floating point multiplications, while our FPGA solution by MASE achieves high energy efficiency by automatically deploying new arithmetic types.
}
\label{tab:overall_results}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrrrrr}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Models}} & \multicolumn{2}{c}{4 GPUs ({\tt Float32})} & \multicolumn{3}{c}{4 GPUs ({\tt LLM.int8}~\cite{dettmers2022llmint8})} & \multicolumn{3}{c}{$<$30 FPGAs ({\tt int8})} & \multicolumn{3}{c}{$<$30 FPGAs ({\tt MSFP6} \cite{darvish2020pushing})} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-6}
\cmidrule(lr){7-9}
\cmidrule(lr){10-12}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{FPS/W} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{FPS/W} & \multicolumn{1}{c}{$\times$} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{FPS/W} & \multicolumn{1}{c}{$\times$} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{FPS/W} & \multicolumn{1}{c}{$\times$} \\ \midrule
OPT-2.7B & 0.51 & 0.051 & 0.51 & 0.051 & 1.0 & \best{0.58} & 0.030 & 0.6 & 0.53 & 0.135 & \best{2.6} \\
OPT-6.7B & 0.76 & 0.008 & 0.76 & 0.029 & 3.6 & 0.49 & 0.008 & 1.0 & \best{0.77} & 0.053 & \best{6.6} \\
LLaMA-7B & \best{0.83} & 0.008 & \best{0.83} & 0.036 & 4.5 & 0.53 & 0.008 & 1.0 & 0.79 & 0.039 & \best{4.8} \\
Vicuna-7B & \best{0.84} & 0.008 & \best{0.84} & 0.036 & 4.5 & 0.56 & 0.008 & 1.0 & 0.73 & 0.039 & \best{4.8} \\
Alpaca-7B & \best{0.89} & 0.008 & 0.85 & 0.036 & 4.5 & 0.56 & 0.008 & 1.0 & 0.83 & 0.039 & \best{4.8} \\
\bottomrule
\end{tabular}
% }
\end{table*}

