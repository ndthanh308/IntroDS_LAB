\begin{table*}
    \centering
    \caption{Evaluation of MASE on large language models. $\times$ = energy efficiency improvement compared to Float32. We highlight the \best{best} results in comparison. TPS = tokens per second. TPS/W = tokens per Joule.}
    \label{tab:overall_results}
\resizebox{\textwidth}{!}{%

\begin{tabular}{lrrrrrrrrrrrrrrrrr}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Models}} & \multicolumn{3}{c}{PyTorch (Float32) on 4 GPUs} & \multicolumn{4}{c}{{\tt LLM.int8()}~\cite{dettmers2022llm} on 4 GPUs} & \multicolumn{5}{c}{MASE (MSFP4)} & \multicolumn{5}{c}{MASE (MSFP6)} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-8}
\cmidrule(lr){9-13}
\cmidrule(lr){14-18}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} & \multicolumn{1}{c}{$\times$} & \multicolumn{1}{c}{\#device} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} & \multicolumn{1}{c}{$\times$} & \multicolumn{1}{c}{\#device} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} & \multicolumn{1}{c}{$\times$} \\
\midrule
{\tt BERT-BASE } & \best{0.93} & 211k & 155.68 & 0.02 & 298k & 214.16 & 1.38 & 25 & 0.91 & \best{781k} & \best{407.04} & \best{2.63} & 25 & 0.92 & 587k & 313.60 & 2.02 \\
{\tt BERT-LARGE} & \best{0.92} & 65.2k & 47.36 & \best{0.92} & 116k & 83.20 & 1.76 & 31 & \best{0.92} & \best{294k} & \best{117.76} & \best{2.49} &  19 & \best{0.92} & 146k & 96.00 & 2.03 \\
{\tt OPT-125M  } & \best{0.92} & 218k & 160.00 & 0.02 & 303k & 218.88 & 1.37 & 25 & 0.91 & \best{781k} & \best{414.72} & \best{2.59} & 25 & \best{0.92} & 587k & 313.60 & 1.96 \\
{\tt OPT-350M  } & \best{0.94} & 66.1k & 48.64 & 0.93 & 118k & 86.08 & 1.76 & 31 & 0.93 & \best{294k} & \best{115.20} & \best{2.37} & 19 & 0.93 & 146k & 94.72 & 1.95 \\
{\tt OPT-1.3B  } & \best{0.82} & 18.6k & 14.08 & 0.81 & \best{54.7k} & \best{39.68} & \best{2.81} & 17 & 0.60 & 36.6k & 26.88 & 1.91 & 20 & 0.80 & 36.6k & 23.04 & 1.63 \\
\midrule
{\tt OPT-2.7B  } & 0.51 & 8.83k & 6.53 & 0.51 & 4.35k & 6.14 & 0.94 & 23 & \best{0.53} & \best{23.4k} & \best{12.54} & \best{1.92} & 28 & \best{0.53} & \best{23.4k} & 10.62 & 1.63 \\
{\tt OPT-6.7B  } & 0.76 & 768 & 1.02 & 0.76 & 2.82k & 3.71 & 3.63 & 22 & 0.59 & \best{9.22k} & \best{5.38} & \best{5.25} & 26 & \best{0.77} & \best{9.22k} & 4.61 & 4.50 \\
{\tt LLaMA-7B  } & \best{0.83} & 896 & 1.02 & \best{0.83} & 3.58k & 4.61 & 4.52 & 22 & 0.69 & \best{9.22k} & \best{5.38} & \best{5.25} & 26 & 0.79 & \best{9.22k} & 4.48  & 4.38 \\
{\tt Vicuna-7B } & \best{0.84} & 896 & 1.02 & \best{0.84} & 3.71k & 4.61 & 4.52 & 22 & 0.63 & \best{9.22k} & \best{5.38} & \best{5.25} & 26 & 0.77 & \best{9.22k} & 4.48  & 4.38 \\
{\tt Alpaca-7B } & \best{0.89} & 896 & 1.02 & 0.85 & 3.71k & 4.61 & 4.52 & 22 & 0.78 & \best{9.22k} & \best{5.38} & \best{5.25} & 26 & 0.83 & \best{9.22k} & 4.48  & 4.38 \\
\bottomrule
\end{tabular}
}
\end{table*}