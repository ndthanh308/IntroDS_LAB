\begin{table*}[]
\caption{Comparison between the quantized model into mixed-precision {\tt MXInt} on many-FPGA systems (connected using QSFP28) and the vanilla model in {\tt LLM.int8()}~\cite{dettmers2022llm} on multi-GPU systems (connected using PCIE). 
% The upper part of the table runs the whole model on a many-FPGA system. The lower part of the table runs a few blocks of the model on the many-FPGA system due to limited FPGA resources, and iterates computation for running the whole model. 
Our approach shows significant improvement in both throughput and energy efficiency compared to GPUs. TPS = token per second. TPS/W = token per Joule.}
\label{tab:overall_results}
\centering
\resizebox{\textwidth}{!}{%
% {\footnotesize
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Models}} & \multicolumn{7}{c}{Our Work} & \multicolumn{3}{c}{GPU} & \multicolumn{2}{c}{Normalized to GPU} \\
\cmidrule(lr){2-8}
\cmidrule(lr){9-11}
\cmidrule(lr){12-13}
& \multicolumn{1}{c}{Devices} & \multicolumn{1}{c}{LUTs} & \multicolumn{1}{c}{DSPs} & \multicolumn{1}{c}{BRAM} & \multicolumn{1}{c}{URAM} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} & \multicolumn{1}{c}{Devices} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} & \multicolumn{1}{c}{TPS} & \multicolumn{1}{c}{TPS/W} \\
\midrule
{\tt LLaMA-160M} & 30 & 43.6M & 75.5k & 7.4k & 1.8k & \best{781k} & \best{339.20} & 4 & 315k & 227.37 & 2.27$\times$ & 1.49$\times$ \\
{\tt BERT-BASE } & 25 & 34.8M & 49.4k & 5.2k & 1.3k & \best{781k} & \best{414.98} & 4 & 298k & 214.16 & 2.62$\times$ & 1.94$\times$ \\
{\tt BERT-LARGE} & 16 & 24.8M & 34.0k & 15.8k & 3.9k & \best{146k} & \best{114.56} & 4 & 116k & 83.20 & 1.26$\times$ & 1.38$\times$\\
{\tt OPT-125M  } & 26 & 38.9M & 57.2k & 6.0k & 1.5k & \best{586k} & \best{293.76} & 4 & 303k & 218.88 & 1.93$\times$ & 1.34$\times$ \\
{\tt OPT-350M  } & 19 & 28.4M & 38.7k & 15.3k & 3.7k & \best{145k} & \best{97.41} & 4 & 118k & 86.08 & 1.23$\times$ & 1.13$\times$ \\
{\tt OPT-1.3B  } & 19 & 29.1M & 27.3k & 73.7k & 17.7k & 36.6k & 21.59 & 4 & \best{54.7k} & \best{39.68} & 0.67$\times$ & 0.54$\times$ \\
{\tt OPT-2.7B }& 27 & 41.7M & 46.7k & 65.3k & 15.7k & \best{29.3k} & \best{9.72} & 4 & 4.35k & 6.14 & 6.74$\times$ & 1.58$\times$ \\
\midrule
{\bf Geom. Mean} & 22 & 33.3M & 46.5k & 17.0k & 5.3k & \best{290k} & \best{164.58} & 4 & 133k & 98.65 & 2.04$\times$ & 1.29$\times$ \\
\midrule
{\tt OPT-6.7B }& 29 & 39.1M & 50.9k & 40.3k & 9.9k & \best{9.37k} & \best{6.16} & 4 & 2.82k & 3.71 & 3.32$\times$ & 1.66$\times$\\
{\tt LLaMA-7B }& 28 & 36.3M  & 75.3k & 41.6k & 10.0k & \best{9.15k} & \best{5.64} & 4 & 3.58k & 4.61 & 2.56$\times$ & 1.23$\times$ \\
{\tt Vicuna-7B} & 29 & 37.7M & 70.8k & 40.4k & 9.8k & \best{9.15k} & \best{5.45} & 4 & 3.71k & 4.61 & 2.47$\times$ & 1.18$\times$\\
{\tt Alpaca-7B} & 29 & 37.9M & 70.7k & 41.6k & 10.0k & \best{9.15k} & \best{5.44} & 4 & 3.71k & 4.61 & 2.47$\times$ & 1.18$\times$\\
\midrule
{\bf Geom. Mean} & 29 & 37.8M & 66.4k & 41.0k & 9.9k & \best{9.20k} & \best{5.67} & 4 & 3.43k & 4.37 & 2.73$\times$ & 1.33$\times$ \\
\bottomrule
\end{tabular}}
\end{table*}