Let us now delineate the ideal behaviour of a revision policy for an incremental sequence labelling model. A utopian model would always output the correct label and thus never need to produce edits or revisions \citep{tapir}.\footnote{That is indeed the case for strictly monotonic models if we use their final output as gold standard.} But due to the incremental nature of language processing, models should not be penalised for building hypotheses that are \textit{locally valid}, as long as a revision is timely triggered. That is, however, complex to know in raw textual input where local ambiguities are not identified. Instead, we can characterise an outlook according to desirable principles and available resources. In scenarios with an infinite time budget, we can simply wait for the input to be complete. If computation budget can be afforded, restart-incrementality is a good fit. But the constraints are not always so loose.

An ideal revision policy should thus revise as rarely as possible for stability. If a prefix/label is correct, the policy should avoid revising it, whereas an incorrect prefix/label should be revised (maybe not immediately, but eventually).
It should always trigger effective, convenient, and definite revisions, preferably in earlier time steps.\footnote{In the beginning, the absence of both right and left context makes prediction harder. Towards the end, the availability of more left context should lead to less, and better, revisions.} Recurrent or oscillating revisions cause more instability and should be avoided. Innovative edits are preferable (as long as they are effective), and short range is better to be combined with delay strategies. Connectedness is a relevant dimension for BIO labelling schemes: If, for instance, the beginning label is edited, ideally the middle labels should change simultaneously. Finally, accompanied edits can be further evaluated in their relation to each other and the linguistic input. A good recomputation policy should, additionally, always result in active revisions.  

In terms of metrics, R-Pertinence and A-Appropriateness should be exactly $1$, \textit{i.e.}~all revisions should occur upon incorrect prefixes and all correct prefixes should not be revised. A-Pertinence and R-Appropriateness should be as high as possible, but cannot be expected to be exactly $1$ because it may take some time steps until the input that actually resolves the ambiguity or mistake is observed.
