\input{tables/metrics-table}

Traditional sequence labelling evaluation metrics like accuracy or F1 can be computed on label, sequence or dataset level. The incremental dimension requires its own metrics, some of which we discussed in $\S$\ref{sec:litreview}. Here, we propose specific metrics to evaluate revision and/or recomputation policies. For each time step $t$ in a sequence, either a revision ($R$) occurred, which is sometimes effective ($R_{e}$), or only an addition ($A$). Assuming we have established a metric for prefix correctness,\footnote{A binary variable or a continuous variable, like accuracy, with a defined threshold for tolerated incorrectness.} we know whether the prefix at $t-1$ was correct ($C$) or incorrect ($I$). That results in a distribution of $N$ actions in $\{R, A\} \times \{C, I\}$. From these counts, we derive the metrics in Table \ref{table:metrics}, computed either per sequence or over the whole dataset. Models that have the option to \textit{recompute} ($R'$) can also be evaluated in $\{R', \neg R'\} \times \{C, I\}$ with two additional metrics. 

Since only \textit{effective} revisions are actually desired, the $R$ in the numerators can be replaced by $R_{e}$ for a more focused evaluation. Revisions can be further weighted by how often and how far in the sentence processing they happen. Similarly, edits can be assessed by their correction time and survival time \citep{baumann2013:phd}.
