Incremental natural language processing\footnote{For a review, see \citet{kohn-2018-incremental}. In other contexts, also referred to as real-time processing \citep{pozzan2015revise} or streaming \citep{kaushal-etal-2023-efficient}.} has \textit{time} at front line, being pivotal for interactive settings. At each time step, models must operate on partial input to deliver partial output, but sometimes previous decisions have to be revised. For example, at time step $4$ in Figure \ref{fig:contructed-example}, the labels for the input tokens $2$ and $3$ were edited into new states. With regard to revisions, at least three types of incremental processors exist, as summarised in Table \ref{table:types}:

\begin{enumerate}
    \item Inherently incremental but monotonic models. They keep an internal state that is updated and used to extend the output at each time step, but cannot revise previous outputs. 

    \item Non-incremental models used with a \textit{restart-incremental} interface, being forced to perform a full recomputation at each time step. Such models revise the output as a by-product of their recomputations.

    \item Incremental models with a dedicated policy to detect the need to perform revisions only when deemed necessary and, more specifically, deciding which parts of the output prefix need to be revised and how.
\end{enumerate}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{@{}m{0.05cm} m{0.2cm} m{2.8cm} m{2.5cm}@{}} 
    \toprule
     &  & \textbf{non-incremental} &  \textbf{incremental}   \\
     \cmidrule{3-4} 
     \multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{revisions}}}    & no & n/a &  strictly monotonic outputs \\
     \cmidrule{3-4} 
                                   &  yes  & recomputation policy doing revisions as a by-product  & revision policy     \\              
    \bottomrule
    \end{tabular}
    \caption{Types of incremental processors.}
    \label{table:types}
\end{table}

Monotonicity avoids instability in the output, allowing subprocesses to start immediately, as it is certain that the outputs will not change. However, they never recover from mistakes, which is one of the drawbacks of employing vanilla RNNs and LSTMs \citep{lstm}.

Models that depend on the availability of full sentences at once can be ``incrementalised'' with the \textit{restart-incremental} paradigm \citep{iu-restart}, causing revisions to occur via recomputations.\footnote{Also called \textit{incremental interface} \citep{beuck-etal-2011-decision} or \textit{beat-driven approach} 
\citep{baumann-incremental}.} 

Cutting-edge NLP models currently rely on Transformers \citep{vaswani2017attention}, which are non-incremental. Using them in a \textit{restart-incremental} fashion requires recomputing from scratch at every time step, which we hereby name the \textit{naive recomputation policy}. It is a very expensive policy because, for a sequence of $n$ tokens, the complexity is $\sum_{i=1}^{n}i^2$ (\textit{i.e.}~the $n$-th square pyramidal number). Besides, this naive approach wastes computational budget, because not all recomputations cause revisions. The results reported by \citet{tapir}, for example, show that only around 25\% of the recomputations actually changed the output prefix. The disadvantages of the naive policy can be alleviated by a smarter policy that cuts down the number of time steps with recomputations.   

Still, beyond deciding when to \textit{recompute}, a revision policy par excellence should directly guide the more specific decision of when (and what) to actually \textit{revise}, and must be evaluated accordingly.
