We begin by formalising incremental sequence labelling tasks, extending the similar definition of streaming sequence tagging by \citet{kaushal-etal-2023-efficient} with \textit{edits} and \textit{revisions}. Like them, we assume an idealised format where incremental units are well-defined, fixed and complete input tokens, and a model that produces a label for every new input token, so that the output is necessarily extended at every time step. Note, however, that incremental processors may have to operate at sub-token level or with transitional input, which requires the capability of retracting decisions and adjusting to varying length in real-time. In some models, outputs may not have an immediate one-to-one correspondence to the input (\textit{e.g.}~due to a delay strategy \citep{baumann-incremental}, or to techniques like opportunistic decoding \citep{zheng-etal-2020-opportunistic}) and parallel hypotheses can be kept in memory. See \citet{iu-restart} for details.

Let $L=\{L_1, \ldots, L_M\}$ be a set of labels. In standard sequence labelling, the task is to map an input sequence of $n$ tokens $(w_i)_{i=1}^n$ to an output sequence of $n$ labels $(l_i)_{i=1}^n$, $l_i \in L$. Each output label $l_i$ classifies its corresponding token $w_i$. The task is more complex than plain token-level classification because the sequential nature of the input and the output need to be taken into account when predicting labels. If available, a gold-standard sequence $(g_i)_{i=1}^n$, with $g_i \in L$, is used to evaluate the correctness of the predicted output sequence.

\input{tables/charac-table}

In an incremental setting, the input is provided in a piecemeal fashion, one token at a time. At each time step $t=1, 2, \ldots, n$, an increasing input prefix $(w_i)_{i=1}^t$ is available to the model and an output prefix $(l_i)_{i=1}^t$ is predicted. Therefore, an input sequence with $n$ tokens will result in $n$ output prefixes $p_1, p_2, \ldots, p_n$, which we consider to be partial hypotheses for the final output. Each $p_i$ is a sequence of $i$ labels, containing one additional label at the right in relation to $p_{i-1}$. The last hypothesis $p_n$ is the final decision of the model, having observed the full input. The complete sequence of prefixes can be represented as a lower triangular matrix, whose cells $c_i^j$ contain the label assigned to $w_i$ at time $j$ and each row $i$ contains $p_i$. We can represent the incremental input and output in an \textit{incremental chart} (IC) as follows:

\vspace{0.2cm}
\begin{center} 
    $\begin{NiceArray}{r|r|ccccc} 

        \toprule    
        w_1                         & p_1=              & l_1^1     &           &           &           &         \\ 
        w_1, w_2                    & p_2=              & l_1^2     &   l_2^2   &           &           &         \\ 
        w_1, w_2, w_3               & p_3=              & l_1^3     &   l_2^3   &  l_3^3    &           &         \\ 
        \vdots                      & \vdots            & \vdots    &   \vdots  &  \vdots   &  \ddots   &         \\
        w_1, w_2, \ldots, w_n       & p_n=              & l_1^n     & l_2^n     &  l_3^n    & \cdots    & l_n^{n} \\ \cmidrule{2-7}
                                    & \text{gold} =     & g_1       & g_2       &  g_3      & \cdots    & g_n     \\  \bottomrule
    \end{NiceArray}$
\end{center}
\vspace{0.2cm}

At each time step $t$, the observation of the new input token $w_t$ causes the model to i) extend the output sequence with one label for $w_t$ (an addition) and ii) optionally also change its current hypotheses $l_1, \ldots, l_{t-1}$ for previous tokens (substitutions). 

An \textit{edit} occurs at time $t$ for label $i$ if $l_{i}^{t} \neq l_{i}^{t-1}$, meaning that the model's prediction for $w_i$'s label changed. A \textit{revision} occurs when, apart from the compulsory addition, a prefix changes at time $t$ in relation to the previous prefix, \textit{i.e.}~when at least one label is edited.\footnote{The addition is not taken into account here, as it has no precedent label to be compared to at this point. The first time step is by definition not a revision, since there is no prefix yet.}  In Figure \ref{fig:contructed-example}, revisions occur at time steps $2, 4, 5, 6, 8, 9$ and $10$. Highlighted labels in the prefixes are edits. 

% Figure environment removed

\paragraph{\textbf{Gold Standard}} Evaluation can be done with respect to incremental or non-incremental gold standards \citep{baumann-incremental}. Often, only the non-incremental version is available, \textit{i.e.}~the labels on the complete sequence, assigned having all left and right context taken into account. A genuinely incremental gold standard contains step-by-step gold prefixes encoding interpretations that are \textit{locally valid} until right context renders it invalid, as illustrated in Figure \ref{fig:locally-valid}.\footnote{For existing examples, see \citet{hrycyk-etal-2021-fast}, \citet{rawat2022real} and \citet{beuck-inc-parsing}.} Since it is usually not available, we can instead ``incrementalise'' the final gold standard by deriving all its prefixes as hard labels. But this approach somewhat unfairly expects that, even at steps with multiple locally valid interpretations, the model commits to the final decision without observing the input that actually induces that interpretation as correct and the others as wrong. Moreover, using an independent gold standard conflates the external overall performance of the model with the quality of its internal incrementality; an alternative is to consider the final output of the model as a silver standard \citep{baumann-incremental}. The correctness of labels and prefixes is then measured with a metric $M$ with respect to the defined target.

