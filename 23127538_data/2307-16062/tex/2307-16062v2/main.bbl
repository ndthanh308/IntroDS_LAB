% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{enayati2022methodical}
A.~M.~S. Enayati, Z.~Zhang, and H.~Najjaran, ``A methodical interpretation of adaptive robotics: Study and reformulation,'' \emph{Neurocomputing}, vol. 512, pp. 381--397, 2022.

\bibitem{wang2021survey}
J.~Wang, T.~Zhang, N.~Ma, Z.~Li, H.~Ma, F.~Meng, and M.~Q.-H. Meng, ``A survey of learning-based robot motion planning,'' \emph{IET Cyber-Systems and Robotics}, vol.~3, no.~4, pp. 302--314, 2021.

\bibitem{kim2015trajectory}
J.-J. Kim and J.-J. Lee, ``Trajectory optimization with particle swarm optimization for manipulator motion planning,'' \emph{IEEE transactions on industrial informatics}, vol.~11, no.~3, pp. 620--631, 2015.

\bibitem{schulman2014motion}
J.~Schulman, Y.~Duan, J.~Ho, A.~Lee, I.~Awwal, H.~Bradlow, J.~Pan, S.~Patil, K.~Goldberg, and P.~Abbeel, ``Motion planning with sequential convex optimization and convex collision checking,'' \emph{The International Journal of Robotics Research}, vol.~33, no.~9, pp. 1251--1270, 2014.

\bibitem{perez2020membrane}
I.~P{\'e}rez-Hurtado, M.~{\'A}. Mart{\'\i}nez-del Amor, G.~Zhang, F.~Neri, and M.~J. P{\'e}rez-Jim{\'e}nez, ``A membrane parallel rapidly-exploring random tree algorithm for robotic motion planning,'' \emph{Integrated Computer-Aided Engineering}, vol.~27, no.~2, pp. 121--138, 2020.

\bibitem{ichter2020learned}
B.~Ichter, E.~Schmerling, T.-W.~E. Lee, and A.~Faust, ``Learned critical probabilistic roadmaps for robotic motion planning,'' in \emph{2020 IEEE International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 9535--9541.

\bibitem{luis2020online}
C.~E. Luis, M.~Vukosavljev, and A.~P. Schoellig, ``Online trajectory generation with distributed model predictive control for multi-robot motion planning,'' \emph{IEEE Robotics and Automation Letters}, vol.~5, no.~2, pp. 604--611, 2020.

\bibitem{wang2022memory}
Y.~Wang and X.~Guo, ``Memory-based stochastic trajectory optimization for manipulator obstacle avoiding motion planning,'' in \emph{2022 7th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 188--194.

\bibitem{yang2019survey}
Y.~Yang, J.~Pan, and W.~Wan, ``Survey of optimal motion planning,'' \emph{IET Cyber-Systems and Robotics}, vol.~1, no.~1, pp. 13--19, 2019.

\bibitem{arulkumaran2017deep}
K.~Arulkumaran, M.~P. Deisenroth, M.~Brundage, and A.~A. Bharath, ``Deep reinforcement learning: A brief survey,'' \emph{IEEE Signal Processing Magazine}, vol.~34, no.~6, pp. 26--38, 2017.

\bibitem{bao2022learn}
J.~Bao, G.~Zhang, Y.~Peng, Z.~Shao, and A.~Song, ``Learn multi-step object sorting tasks through deep reinforcement learning,'' \emph{Robotica}, pp. 1--17, 2022.

\bibitem{kulhanek2021visual}
J.~Kulh{\'a}nek, E.~Derner, and R.~Babu{\v{s}}ka, ``Visual navigation in real-world indoor environments using end-to-end deep reinforcement learning,'' \emph{IEEE Robotics and Automation Letters}, vol.~6, no.~3, pp. 4345--4352, 2021.

\bibitem{zhang2022high}
Z.~Zhang, R.~Dershan, A.~M.~S. Enayati, M.~Yaghoubi, D.~Richert, and H.~Najjaran, ``A high-fidelity simulation platform for industrial manufacturing by incorporating robotic dynamics into an industrial simulation tool,'' \emph{IEEE Robotics and Automation Letters}, vol.~7, no.~4, pp. 9123--9128, 2022.

\bibitem{zhao2020sim}
W.~Zhao, J.~P. Queralta, and T.~Westerlund, ``Sim-to-real transfer in deep reinforcement learning for robotics: a survey,'' in \emph{2020 IEEE Symposium Series on Computational Intelligence (SSCI)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 737--744.

\bibitem{tsurumine2019deep}
Y.~Tsurumine, Y.~Cui, E.~Uchibe, and T.~Matsubara, ``Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation,'' \emph{Robotics and Autonomous Systems}, vol. 112, pp. 72--83, 2019.

\bibitem{cai2021modular}
M.~Cai, M.~Hasanbeig, S.~Xiao, A.~Abate, and Z.~Kan, ``Modular deep reinforcement learning for continuous motion planning with temporal logic,'' \emph{IEEE Robotics and Automation Letters}, vol.~6, no.~4, pp. 7973--7980, 2021.

\bibitem{yang2021hierarchical}
X.~Yang, Z.~Ji, J.~Wu, Y.-K. Lai, C.~Wei, G.~Liu, and R.~Setchi, ``Hierarchical reinforcement learning with universal policies for multistep robotic manipulation,'' \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2021.

\bibitem{xiong2020comparison}
H.~Xiong, T.~Ma, L.~Zhang, and X.~Diao, ``Comparison of end-to-end and hybrid deep reinforcement learning strategies for controlling cable-driven parallel robots,'' \emph{Neurocomputing}, vol. 377, pp. 73--84, 2020.

\bibitem{voigt2020multi}
F.~Voigt, L.~Johannsmeier, and S.~Haddadin, ``Multi-level structure vs. end-to-end-learning in high-performance tactile robotic manipulation.'' in \emph{CoRL}, 2020, pp. 2306--2316.

\bibitem{da2019survey}
F.~L. Da~Silva and A.~H.~R. Costa, ``A survey on transfer learning for multiagent reinforcement learning systems,'' \emph{Journal of Artificial Intelligence Research}, vol.~64, pp. 645--703, 2019.

\bibitem{stulp2011learning}
F.~Stulp, E.~Theodorou, M.~Kalakrishnan, P.~Pastor, L.~Righetti, and S.~Schaal, ``Learning motion primitive goals for robust manipulation,'' in \emph{2011 IEEE/RSJ International Conference on Intelligent Robots and Systems}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 325--331.

\bibitem{ly2020learning}
A.~O. Ly and M.~Akhloufi, ``Learning to drive by imitation: An overview of deep behavior cloning methods,'' \emph{IEEE Transactions on Intelligent Vehicles}, vol.~6, no.~2, pp. 195--209, 2020.

\bibitem{fang2019survey}
B.~Fang, S.~Jia, D.~Guo, M.~Xu, S.~Wen, and F.~Sun, ``Survey of imitation learning for robotic manipulation,'' \emph{International Journal of Intelligent Robotics and Applications}, vol.~3, no.~4, pp. 362--369, 2019.

\bibitem{ravichandar2020recent}
H.~Ravichandar, A.~S. Polydoros, S.~Chernova, and A.~Billard, ``Recent advances in robot learning from demonstration,'' \emph{Annual review of control, robotics, and autonomous systems}, vol.~3, pp. 297--330, 2020.

\bibitem{rajeswaran2017learning}
A.~Rajeswaran, V.~Kumar, A.~Gupta, G.~Vezzani, J.~Schulman, E.~Todorov, and S.~Levine, ``Learning complex dexterous manipulation with deep reinforcement learning and demonstrations,'' \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem{tian2021learning}
Y.~Tian, X.~Cao, K.~Huang, C.~Fei, Z.~Zheng, and X.~Ji, ``Learning to drive like human beings: A method based on deep reinforcement learning,'' \emph{IEEE Transactions on Intelligent Transportation Systems}, 2021.

\bibitem{nair2018overcoming}
A.~Nair, B.~McGrew, M.~Andrychowicz, W.~Zaremba, and P.~Abbeel, ``Overcoming exploration in reinforcement learning with demonstrations,'' in \emph{2018 IEEE international conference on robotics and automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 6292--6299.

\bibitem{gupta2021reinforcement}
K.~Gupta, ``Reinforcement learning in complex environments with locally trained na{\"\i}ve agents,'' Ph.D. dissertation, University of British Columbia, 2021.

\bibitem{florence2022implicit}
P.~Florence, C.~Lynch, A.~Zeng, O.~A. Ramirez, A.~Wahid, L.~Downs, A.~Wong, J.~Lee, I.~Mordatch, and J.~Tompson, ``Implicit behavioral cloning,'' in \emph{Conference on Robot Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 158--168.

\bibitem{stulp2012reinforcement}
F.~Stulp, E.~A. Theodorou, and S.~Schaal, ``Reinforcement learning with sequences of motion primitives for robust manipulation,'' \emph{IEEE Transactions on robotics}, vol.~28, no.~6, pp. 1360--1370, 2012.

\bibitem{schaal2006dynamic}
S.~Schaal, ``Dynamic movement primitives-a framework for motor control in humans and humanoid robotics,'' in \emph{Adaptive motion of animals and machines}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2006, pp. 261--280.

\bibitem{stulp2011hierarchical}
F.~Stulp and S.~Schaal, ``Hierarchical reinforcement learning with movement primitives,'' in \emph{2011 11th IEEE-RAS International Conference on Humanoid Robots}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 231--238.

\bibitem{cohen2021motion}
Y.~Cohen, O.~Bar-Shira, and S.~Berman, ``Motion adaptation based on learning the manifold of task and dynamic movement primitive parameters,'' \emph{Robotica}, vol.~39, no.~7, pp. 1299--1315, 2021.

\bibitem{kulvicius2011joining}
T.~Kulvicius, K.~Ning, M.~Tamosiunaite, and F.~Worg{\"o}tter, ``Joining movement sequences: Modified dynamic movement primitives for robotics applications exemplified on handwriting,'' \emph{IEEE Transactions on Robotics}, vol.~28, no.~1, pp. 145--157, 2011.

\bibitem{hogan2012dynamic}
N.~Hogan and D.~Sternad, ``Dynamic primitives of motor behavior,'' \emph{Biological cybernetics}, vol. 106, no.~11, pp. 727--739, 2012.

\bibitem{li2021reinforcement}
A.~Li, Z.~Liu, W.~Wang, M.~Zhu, Y.~Li, Q.~Huo, and M.~Dai, ``Reinforcement learning with dynamic movement primitives for obstacle avoidance,'' \emph{Applied Sciences}, vol.~11, no.~23, p. 11184, 2021.

\bibitem{liang2021dynamic}
Y.~Liang, W.~Li, Y.~Wang, R.~Xiong, Y.~Mao, and J.~Zhang, ``Dynamic movement primitive based motion retargeting for dual-arm sign language motions,'' in \emph{2021 IEEE International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 8195--8201.

\bibitem{yuan2022hierarchical}
Y.~Yuan, Z.~L. Yu, L.~Hua, Y.~Cheng, J.~Li, and X.~Sang, ``Hierarchical dynamic movement primitive for the smooth movement of robots based on deep reinforcement learning,'' \emph{Applied Intelligence}, pp. 1--18, 2022.

\bibitem{saveriano2023dynamic}
M.~Saveriano, F.~J. Abu-Dakka, A.~Kramberger, and L.~Peternel, ``Dynamic movement primitives in robotics: A tutorial survey,'' \emph{The International Journal of Robotics Research}, vol.~42, no.~13, pp. 1133--1184, 2023.

\bibitem{sun2021motion}
H.~Sun, W.~Zhang, R.~Yu, and Y.~Zhang, ``Motion planning for mobile robotsâ€”focusing on deep reinforcement learning: A systematic review,'' \emph{IEEE Access}, vol.~9, pp. 69\,061--69\,081, 2021.

\bibitem{zhou2022review}
C.~Zhou, B.~Huang, and P.~Fr{\"a}nti, ``A review of motion planning algorithms for intelligent robots,'' \emph{Journal of Intelligent Manufacturing}, vol.~33, no.~2, pp. 387--424, 2022.

\bibitem{yu2022intelligent}
L.~Yu, J.~Luo, and K.~Zhou, ``An intelligent robot motion planning method and application via lppo in unknown environment,'' in \emph{2022 12th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 265--270.

\bibitem{ying2022trajectory}
F.~Ying, H.~Liu, R.~Jiang, and X.~Yin, ``Trajectory generation for multiprocess robotic tasks based on nested dual-memory deep deterministic policy gradient,'' \emph{IEEE/ASME Transactions on Mechatronics}, 2022.

\bibitem{fan2018surreal}
L.~Fan, Y.~Zhu, J.~Zhu, Z.~Liu, O.~Zeng, A.~Gupta, J.~Creus-Costa, S.~Savarese, and L.~Fei-Fei, ``Surreal: Open-source reinforcement learning framework and robot manipulation benchmark,'' in \emph{Conference on Robot Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 767--782.

\bibitem{naughton2021elastica}
N.~Naughton, J.~Sun, A.~Tekinalp, T.~Parthasarathy, G.~Chowdhary, and M.~Gazzola, ``Elastica: A compliant mechanics environment for soft robotic control,'' \emph{IEEE Robotics and Automation Letters}, vol.~6, no.~2, pp. 3389--3396, 2021.

\bibitem{hou2017novel}
Y.~Hou, L.~Liu, Q.~Wei, X.~Xu, and C.~Chen, ``A novel ddpg method with prioritized experience replay,'' in \emph{2017 IEEE international conference on systems, man, and cybernetics (SMC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 316--321.

\bibitem{luo2020dynamic}
J.~Luo and H.~Li, ``Dynamic experience replay,'' in \emph{Conference on robot learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 1191--1200.

\bibitem{choi2020robotic}
J.~Choi, H.~Kim, Y.~Son, C.-W. Park, and J.~H. Park, ``Robotic behavioral cloning through task building,'' in \emph{2020 International Conference on Information and Communication Technology Convergence (ICTC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1279--1281.

\bibitem{tamizi2024end}
M.~G. Tamizi, H.~Honari, A.~Nozdryn-Plotnicki, and H.~Najjaran, ``End-to-end deep learning-based framework for path planning and collision checking: bin-picking application,'' \emph{Robotica}, vol.~42, no.~4, pp. 1094--1112, 2024.

\bibitem{farag2018behavior}
W.~Farag and Z.~Saleh, ``Behavior cloning for autonomous driving using convolutional neural networks,'' in \emph{2018 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 1--7.

\bibitem{chen2017robot}
C.~Chen, C.~Yang, C.~Zeng, N.~Wang, and Z.~Li, ``Robot learning from multiple demonstrations with dynamic movement primitive,'' in \emph{2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 523--528.

\bibitem{li2023prodmp}
G.~Li, Z.~Jin, M.~Volpp, F.~Otto, R.~Lioutikov, and G.~Neumann, ``Prodmp: A unified perspective on dynamic and probabilistic movement primitives,'' \emph{IEEE Robotics and Automation Letters}, vol.~8, no.~4, pp. 2325--2332, 2023.

\bibitem{gupta2022exploiting}
K.~Gupta and H.~Najjaran, ``Exploiting abstract symmetries in reinforcement learning for complex environments,'' in \emph{2022 International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 3631--3637.

\bibitem{kumar2021should}
A.~Kumar, J.~Hong, A.~Singh, and S.~Levine, ``{Should} {I} run offline reinforcement learning or behavioral cloning?'' in \emph{International Conference on Learning Representations}, 2021.

\bibitem{wang2023diffusion}
Z.~Wang, J.~J. Hunt, and M.~Zhou, ``Diffusion policies as an expressive policy class for offline reinforcement learning,'' \emph{arXiv/2208.06193}, 2023.

\bibitem{shafiullah2022behavior}
N.~M. Shafiullah, Z.~Cui, A.~A. Altanzaya, and L.~Pinto, ``Behavior transformers: Cloning $ k $ modes with one stone,'' \emph{Advances in neural information processing systems}, vol.~35, pp. 22\,955--22\,968, 2022.

\bibitem{paranjape2015motion}
A.~A. Paranjape, K.~C. Meier, X.~Shi, S.-J. Chung, and S.~Hutchinson, ``Motion primitives and 3d path planning for fast flight through a forest,'' \emph{The International Journal of Robotics Research}, vol.~34, no.~3, pp. 357--377, 2015.

\bibitem{SpinningUp2018}
J.~Achiam, ``{Spinning Up in Deep Reinforcement Learning},'' 2018.

\bibitem{Jocher_YOLO_by_Ultralytics_2023}
\BIBentryALTinterwordspacing
G.~Jocher, A.~Chaurasia, and J.~Qiu, ``{YOLO by Ultralytics},'' Jan. 2023. [Online]. Available: \url{https://github.com/ultralytics/ultralytics}
\BIBentrySTDinterwordspacing

\bibitem{opencv_library}
G.~Bradski, ``{The OpenCV Library},'' \emph{Dr. Dobb's Journal of Software Tools}, 2000.

\end{thebibliography}
