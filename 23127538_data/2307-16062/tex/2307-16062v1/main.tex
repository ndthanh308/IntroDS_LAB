\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage[english]{babel}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\usepackage{enumitem}
%\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary {arrows.meta}
\usepackage{amssymb}
\usepackage{multirow}
%\usepackage{algorithm2e}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor,colortbl}


\graphicspath{{./figures}}

\newcommand{\qed}{$\hfill\square$}
\newcommand{\x}{\mathbf{x}}
\newtheorem{problem}{Problem}


\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother


\definecolor{shadow_red}{rgb}{1, 0.9, 0.9}
\definecolor{dark_red}{rgb}{0.8, 0, 0}
\definecolor{mid_red}{rgb}{1, 0.7, 0.7}

\definecolor{shadow_blue}{rgb}{0.9, 0.95, 1}
\definecolor{dark_blue}{rgb}{0, 0.2, 0.8}
\definecolor{mid_blue}{rgb}{0.6, 0.7, 1}

\definecolor{shadow_yellow}{rgb}{1, 0.9, 0.8}
\definecolor{dark_yellow}{rgb}{0.8, 0.5, 0}
\definecolor{mid_yellow}{rgb}{1, 0.8, 0.5}

\begin{document}

\title{Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning}

\author{Zengjie Zhang$^{1}$,~\IEEEmembership{Member,~IEEE},
    Jayden Hong$^{2}$,
    Amir Soufi Enayati$^{2}$,~\IEEEmembership{Student Member~IEEE},\\
    Homayoun Najjaran$^{2*}$,~\IEEEmembership{Member,~IEEE}
\thanks{This work receives financial support from Kinova\textregistered~Inc. and Natural Sciences and Engineering Research Council (NSERC) Canada under the Grant CRDPJ 543881-19. We also benefit from the hardware resources for the experimental setup provided by Kinova\textregistered~Inc.}% <-this % stops a space
\thanks{$^{1}$Zengjie Zhang is with the Department of Electrical Engineering, Eindhoven University of Technology, Netherlands,  {\tt\small \{z.zhang3\}@tue.nl}.}
\thanks{$^{2}$Jayden Hong, Amir Soufi Enayati, and Homayoun Najjaran are with the Faculty of Engineering and Computer Science, University of Victoria, Canada, {\tt\small \{jaydenh, amsoufi, najjaran\}@uvic.ca}.}
\thanks{*Corresponding author.}
}

% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel perspective on using motion primitives and human demonstration to leverage the performance of RL for robot applications.
\end{abstract}

\begin{IEEEkeywords}
reinforcement learning, robot motion planning, learning from demonstration, behavior cloning, motion primitive, heuristic method, human motion
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{T}{he} next-generation manufacturing is expected to have a higher level of automation and involve less human power. Intelligent robots are needed to actively learn skills instead of being programmed by experts explicitly~\cite{zhang2020online, enayati2022methodical}. 
Reinforcement learning (RL) is a powerful approach that enables robots to automatically learn an ideal manipulation policy via trial and error. A typical application of RL is robot motion planning which requires the robot to move from an initial position to a goal position without colliding with the obstacles in the environment~\cite{wang2021survey}. As illustrated in Fig.~\ref{fig:intro}, motion planning is an essential problem for more complicated tasks such as %pushing~\cite{deng2019deep}, 
grasping~\cite{joshi2020robotic}, assembly~\cite{haarnoja2018composable}, and manipulation~\cite{nemec2017door}. The conventional approaches used for robot motion planning include optimization-based methods, such as trajectory optimization~\cite{kim2015trajectory} and sequential convex optimization~\cite{schulman2014motion}, and the sampling-based methods, including exploration trees~\cite{perez2020membrane}, probabilistic roadmaps~\cite{ichter2020learned}, and model predictive control~\cite{luis2020online} which highly depend on the precise model of the environment. More recent methods, such as stochastic optimization-based planning (STOMP)~\cite{wang2022memory}, attempt to combine the advantages of both types of approaches. A review of the conventional robot motion planning methods can be referred to in~\cite{mohanan2018survey, yang2019survey}. Compared to them, RL does not directly solve an optimization problem based on the precise model of the environment. Instead, an optimal planner, or an RL agent, can be trained automatically during interactions with the environment. 

%An RL agent uses an artificial neural network to approximate the optimal motion planner for a robot. The parameters of the policy approximator are iteratively updated using the interaction data until the optimal reward is steadily achieved. 
% Typically, RL agents with larger or more complex neural networks are likely to solve more complicated tasks. On the other hand, they tend to spend longer training times and are easier to cause the overfitting problem. 

% Figure environment removed
\IEEEpubidadjcol

A typical case of RL is deep reinforcement learning (DRL) which adopts an end-to-end learning scheme using deep neural networks~\cite{arulkumaran2017deep}. It aims at constructing an all-in-one planner for highly-coupled and complicated robot tasks, especially the ones that depend on computer vision, such as grasping~\cite{bao2022learn} and autonomous navigation~\cite{kulhanek2021visual}.
%such as grasping~\cite{kalashnikov2018scalable, julian2020efficient}, object sorting~\cite{bao2022learn, an2022sorting}, autonomous navigation~\cite{shi2019end, kulhanek2021visual}, and autonomous driving~\cite{chen2021interpretable}.
%It is frequently adopted for vision-based robot manipulation tasks by directly mapping the raw perception data to robot actions~\cite{tampuu2020survey}.  
Nevertheless, end-to-end learning suffers from the slow convergence rate of the training process and the sensitivity to environmental changes~\cite{enayati2022methodical}. This is due to the large scale of the deep neural network and the high likelihood of overfitting. Efforts to resolve these issues include developing high-fidelity simulation platforms~\cite{zhang2022high} or designing sim-to-real schemes to improve its adaptability and robustness~\cite{zhao2020sim}. 
Compared to end-to-end learning, exploiting heuristics can effectively speed up the training of RL agents and avoid overfitting by splitting a big RL problem into several smaller learning problems. Effective heuristic methods for RL include feature extraction layers~\cite{tsurumine2019deep}, modularized neural networks~\cite{cai2021modular}, and hierarchical structures~\cite{yang2021hierarchical}. In~\cite{xiong2020comparison, voigt2020multi}, experimental studies are used to address that hierarchical-RL ensures faster convergence and better robustness than end-to-end learning. Besides, transfer learning facilitated by heuristic mappings is used to transfer a simple-task policy to a complicated task without additional training~\cite{da2019survey}. Heuristic models, such as motion primitives~\cite{stulp2011learning, stulp2012reinforcement} are also widely used to simplify an RL problem by transforming a complex decision-making problem into a simpler domain. %Dynamic Movement Primitive (DMP) is a common type of motion primitive used for motion planning of robot manipulators.

Behavior cloning (BC) from demonstration is another technology to promote the training performance of RL agents. It is used by imitation learning (IL) and programming by demonstration (PbD) to learn human-like policies for robot manipulation tasks~\cite{fang2019survey, ravichandar2020recent}. The main idea of BC is to duplicate the human policy encoded in the demonstration data to a robot motion planner by supervised learning methods~\cite{ly2020learning}. 
%It provides an indirect manner to incorporate expert knowledge in RL agent training. Human demonstrations can be encoded in manual command signals, physical contact signals~\cite{de2005pd}, movement trajectories~\cite{turnwald2019human}, and video streams. Common approaches used to generate robot planners from human demonstrations include imitation learning (IL) and programming by demonstration (PbD)~\cite{fang2019survey, ravichandar2020recent}. The critical technology of IL is \textit{behavior cloning} (BC) which is aimed to duplicate the human motion pattern encoded in the demonstration data to a robot motion planner using regression and supervised learning methods~\cite{ly2020learning}. 
%Besides IL, 
For RL, a common approach is to use BC to obtain a decent human-like policy which then serves as the initial policy for the agent training~\cite{rajeswaran2017learning, tian2021learning, nair2018overcoming}. The human-like policy is not necessarily to be the optimal one but is at least a decent policy for robot motion generation. This method, however, renders complete separation between BC and agent training, such that BC is not helpful in improving the performance of RL. A recent study proposed a novel method to integrate BC into the training process of an RL agent, which greatly improves the convergence speed~\cite{gupta2021reinforcement}. However, the demonstration used for BC is generated by a PID controller in a simulation environment, instead of real human data. Also, BC is still performed in a \textit{explicit} manner which directly penalizes the deviation between the cloned and the demonstration actions. This may lead to the overfitting of the demonstration policy. Recent work tries to solve this problem by proposing an \textit{implicit} BC (IBC) method that performs BC by penalizing a certain energy function of the cloned policy, such that the cloned policy is less sensitive to the action deviations~\cite{florence2022implicit}. %This method is also promising to be used for BC-integrated RL to solve the overfitting problem.

We believe that proper usage of both heuristics and human demonstration can leverage the training speed and the generalizability of RL agents. Out of this motivation, we propose a novel RL method for robot motion planning facilitated by DMP and IBC, which has not been investigated by existing work, to our best knowledge. The efficacy of the proposed method and its advantages over conventional RL agents are validated using simulation studies. We also use an experimental study to demonstrate its applicability to practical robotic tasks. Our detailed contributions are summarized as follows. 
\begin{itemize}
\item Firstly, we created a dataset of human demonstrations in a point-to-point reaching task. The dataset is published online and can be openly used for studies on demonstration-facilitated methods for robot motion planning. 
\item Secondly, we propose the framework for developing an IBC-DMP RL agent for the motion planning of a robot manipulator based on Multi-degree-of-freedom (DoF) DMP and IBC. The details of the framework are given. This framework provides a novel perspective for improving the learning performance and generalizability of RL agents using demonstrations and heuristic models.
\item Thirdly, we present a series of technical points that are important to ensuring decent training performance of an IBC-DMP RL agent, including DMP dimension extension, human action generation, IBC loss computation, and critical loss refinement. They can also inspire other RL-based methods with heuristic models and demonstrations.
\item Besides the simulation studies to validate the efficacy of the IBC-DMP RL method, we also conduct an experimental study to demonstrate how this method can be used to solve practical robotic problems.
%We use simulation and experimental studies to evaluate the efficacy of the proposed IBC-DMP RL method and compare it with the conventional RL method. The results indicate that the IBC-DMP RL outperforms conventional RL in terms of faster training convergence and higher test scores. 
\end{itemize}

The rest part of the paper is organized as follows. Sec.~\ref{sec:rw} introduces the related work and the preliminary knowledge of the main technologies used in this paper. Sec.~\ref{sec:frame} interprets the framework of the proposed method and formulates the problem. Sec.~\ref{sec:human_data} introduces how we generate human demonstrations from a hand trajectory recording experiment. Sec.~\ref{sec:training} presents the important technical details of the IBC-DMP agent. Sec.~\ref{sec:va_sim} and Sec.~\ref{sec:exp} present our simulation and experiment studies, respectively. Finally, Sec.~\ref{sec:con} concludes the paper.


\textit{Notation}: In this paper, we use $\mathbb{R}$, $\mathbb{R}_{\geq 0}$, and $\mathbb{R}^+$ to represent the sets of real numbers, non-negative numbers, and positive numbers, respectively. Also, we use $\mathbb{N}$, $\mathbb{N}_{\geq 0}$, and $\mathbb{N}^+$ to represent the sets of real integers, non-negative integers, and positive integers, respectively.


\section{Related Work and Preliminaries}\label{sec:rw}

In this section, we introduce the related work and the preliminary knowledge of the three important technologies used in this paper, namely DMP, off-policy RL, and BC.

\subsection{Dynamic Movement Primitive (DMP)}\label{sec:dmp}

A motion primitive is a heuristic model commonly used for robot motion generation. A special motion primitive is the dynamic movement primitive (DMP) designed for robot manipulators. The mathematical formulation of DMP is a virtual second-order linear dynamic model. The state of the DMP model is the position and velocity of the robot trajectory to be generated. Its input is an actuation function to be learned. 
%The main objective of solving a DMP is to find a proper actuation function such that the generated trajectory minimizes a predefined cost~\cite{stulp2011hierarchical}. 
The trajectories generated by DMP are ensured with inherent smoothness and stability due to the continuity of second-order dynamic models. DMP was originally proposed as an abstract model to describe human motions~\cite{schaal2006dynamic}. Then, it is applied to IL for the learning of human movement patterns~\cite{kulvicius2011joining} since DMP provides an elegant and simple model to depict human motions. %In this sense, IL is a supervised learning process where the DMP model is expected to fit human demonstrations.
%, where the DMP is usually used to learn a known reference trajectory denoted by $x^{\mathrm{r}}_t$, $\dot{x}^{\mathrm{r}}_t$, $\ddot{x}^{\mathrm{r}}_t \in \mathbb{R}$. In this sense, the optimized matric $\mathcal{J}(x_t, \dot{x}_t, \zeta_t)$ is used to depict the deviation between the reference trajectory and the one to be generated by the DMP. The main method used to achieve IL is supervised learning which requires that the reference trajectory is well labeled. %The labeled trajectory samples are usually obtained by recording human motions.
When human demonstrations are not available, DMP can be solved using RL with a predefined cost function~\cite{hogan2012dynamic}. It is very effective to simplify a complicated robot motion planning problem via hierarchical RL~\cite{stulp2011hierarchical}. DMP-facilitated RL has become a popular method for robot motion planning in recent work~\cite{liang2021dynamic, yuan2022hierarchical, cohen2021motion}. A survey on the application of DMP to robot manipulation problems is presented in~\cite{saveriano2021dynamic}. Nevertheless, DMP has been mainly solved using on-policy RL methods, which makes it difficult to promote the current policy using demonstration data. Using off-policy methods to train a DMP model is still an unsolved problem.

DMP is a virtual dynamic model used to generate desired trajectories for a moving point~\cite{stulp2011hierarchical, cohen2021motion}, defined as follows,
\begin{subequations}\label{eq:dmp}
\begin{alignat}{2}
\tau \ddot{x}_t = &\,  \alpha \!\left( \beta \!\left( x_{\mathrm{g}} - x_t \right)\! - \dot{x}_t \right)\! + \zeta_t (x_{\mathrm{g}}-x_0 ) f(\zeta_t), \label{eq:dmp_1} \\
\tau \dot{\zeta}_t = &\, - \omega \zeta_t, \label{eq:dmp_2}
\end{alignat}
\end{subequations}
where $x_t, \dot{x}_t, \ddot{x}_t \!\in\! \mathbb{R}$ are, respectively, the time-dependent position, velocity, and acceleration of the desired trajectory to be generated, $x_0, x_{\mathrm{g}} \!\in\! \mathbb{R}$ are the initial and the goal positions of the desired trajectory, $\zeta_t \!\in\! \mathbb{R}$ is a canonical dynamic variable with a non-zero initial value $\zeta_0 \!\in\! \mathbb{R}$, $\tau \!\in\! \mathbb{R}^+$ is a constant temporal scalar that depicts the inertia of DMP, $\alpha, \beta \!\in\! \mathbb{R}^+$ are constant parameters that determine the damping and stiffness of the DMP, $\omega \!\in\! \mathbb{R}$ is a constant parameter that controls the duration of the DMP, and $f:\mathbb{R}\rightarrow \mathbb{R}$ is the actuation function that determines the shape of the generated trajectory. With a proper actuation function $f$, the DMP model \eqref{eq:dmp} generates a trajectory $x_t$ which has the continuous position, velocity, and acceleration with respect to time. Therefore, it guarantees smoothness when applied to robot motion planning.
The actuation function $f$ is solved by minimizing a certain cost. Conventionally, DMP approximates the optimal actuation function using radius-basis-function (RBF) neural networks
%uses a radius-basis-function (RBF) neural network to approximate the optimal actuation function, i.e., $f(\zeta_t) \!=\! \left.\left({\sum_{j=1}^M \!\varphi_j(\zeta_t) \omega_j}\right) \right/\!{\sum_{j=1}^M \!\varphi_j(\zeta_t)}$, 
%where $M \!\in\! \mathbb{N}^+$ is the number of the RBFs, $\omega_j \!\in\! \mathbb{R}^+$ are constant weights, $\varphi_j:\mathbb{R} \!\rightarrow\! \mathbb{R}$ is a RBF $\varphi_j(x) \!=$ $\mathrm{exp}\!\left( -\frac{(x-c_j)^2}{2 \sigma_j^2} \right)$, $j\!=\!1$, $2$, $\cdots$, $M$, where $c_j, \sigma_j \in \mathbb{R}$ are constant parameters. The RBF-based actuation function 
which can be solved using on-policy RL methods~\cite{stulp2011hierarchical}. DMP is usually used to directly generate the desired trajectory for a robot end-effector in the Cartesian space, where an inverse-kinematic (IK) algorithm is needed to map the trajectory to the joint space~\cite{stulp2012reinforcement}.



\subsection{Off-policy Reinforcement Learning}\label{sec:mdp}

RL methods can be categorized into on-policy and off-policy approaches depending on whether the policy to be promoted is the one that is interacting with the environment. Both types of approaches are widely used for robot motion planning~\cite{wang2021survey, sun2021motion, zhou2022review}. A typical on-policy method is proximal policy optimization (PPO) which updates the policy using the policy gradient calculated from the perturbed system trajectories~\cite{yu2022intelligent}. On-policy methods typically promote policies only after complete episodes. Also, historical policies are no longer used for policy promotion. On the contrary, the policy update of off-policy RL methods, such as deep deterministic policy gradient (DDPG), can be performed at any time. Off-policy methods can also utilize the historical data stored in the experience replay buffer to promote the current policy~\cite{ying2022trajectory}. This means that off-policy methods can learn from multiple policies encoded in the historical data, which is its main advantage over on-policy methods. Nevertheless, the main shortcomings of off-policy methods are long training times and unstable training behaviors due to the bootstrapping effect in the initial stage of the training process. In fact, the training performance of an off-policy agent is highly dependent on the quality of the experience data. In the application of robot motion planning, it is commonly witnessed that on-policy methods outperform off-policy methods when the quality of the experience data is not sufficiently good~\cite{fan2018surreal, naughton2021elastica}. Therefore, many efforts are devoted to improving the performance of off-policy agents by refining the experience data~\cite{hou2017novel, luo2020dynamic}. 

Markov Decision Process (MDP) is the essential mathematic model of RL. An MDP is defined as a tuple $\mathcal{M} \!=\! (\mathcal{S},\mathcal{A}, \mathscr{F}, \mathscr{R})$, where $\mathcal{S}$ and $\mathcal{A}$ are the state space and the action space of the agent, respectively, and $\mathscr{F}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ and $\mathscr{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ are the state transition and the reward function of the agent. RL is dedicated to solving the optimal policy $\pi:\mathcal{S} \rightarrow \mathcal{A}$ subject to the following optimization problem,
\begin{equation}\label{eq:optideter}
\textstyle \pi^* = \arg \max_{\pi} \sum_{t=0}^{T} \gamma^{t} \mathscr{R} \!\left(s_t, \pi(s_t) \right),
\end{equation}
where $T \in \mathbb{N}^+$ is the length of a trajectory $s_0 s_1 \cdots s_T$, $0 \!<\!\gamma \!<\!1$ is a discount factor, and $s_t \in \mathcal{S}$ is the state of the MDP at time $t = 0,1,\cdots,T$. The solution $\pi^*$ to \eqref{eq:optideter} is referred to as the optimal policy.

The optimal policy $\pi^*$ can be solved either with an on-policy method, such as proximal policy optimization (PPO), or an off-policy approach, like deep deterministic policy gradient (DDPG). In this paper, our main focus is on off-policy RL and we use DDPG as the baseline model of our approach since it allows for improving the current policy using the experience data. A basic DDPG agent typically consists of two neural networks, namely an actor $\pi_{\theta}$ and a critic $Q_{w}$ which are respectively used to approximate the optimal policy $\pi^*$ as shown in \eqref{eq:optideter} and the value function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, where $\theta$ and $w$ denote the parameters of the neural networks. Details about the definition of the value function can be found in~\cite{hou2017novel}. The main objective of the training process for a DDPG agent is to constantly update the values of the parameters $\theta$ and $w$, such that the approximations $\pi_{\theta} \rightarrow \pi^*$ and $Q_{w} \rightarrow Q$ are as close as possible. Another two neural networks $\pi_{\theta'}: \mathcal{S} \rightarrow \mathcal{A}$ and $Q_{w'}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ with parameters $\theta'$ and $w'$, referred to as \textit{target neural networks} are typically used to smooth out the approximation process. The parameters $\theta'$ and $w'$ are iteratively updated following the updates of $\theta$ and $w$,
\begin{equation}\label{eq:tac_grad}
\theta' \leftarrow \lambda \theta' + (1- \lambda) \theta,~w' \leftarrow \lambda w' + (1-\lambda) w,
\end{equation}
where $0 \!<\! \lambda \!<\! 1$ is an interpolation factor used to average the updates of the target networks and stabilize the approximation. 

The training of the critic network $Q_w$ is a supervised learning process. The samples used to train the network are from an \textit{experience replay buffer} $\mathcal{B}$ sized $n \!\in\! \mathbb{N}^+$, which is randomly sampled from an experience replay buffer $\mathcal{B}$. Each sample in the buffer $\mathcal{B}$ is organized as the format $\{\,s_j, a_j, s_j', r_j, d_j\}$, where $s_j \in \mathcal{S}$ and $a_j \in \mathcal{A}$ are the state and the action of the agent at a certain history instant $j=1$, $2$, $\cdots$, $n$, $s_j' \!=\! \mathscr{F}(s_j, a_j)$ is the \textit{successive state}, $r_j \!=\! \mathscr{R}(s_j, a_j)$ is the corresponding instant reward, and $d_j$ is a boolean termination flag that determines whether an episode terminates or not. The critic loss function of the buffer $\mathcal{B}$ is computed as 
\begin{equation}\label{eq:conv_critic_loss}
\textstyle \mathcal{L}_C(\mathcal{B}) = \frac{1}{n} \sum_{j=1}^n \left(l_j - Q_{w}(s_j, a_j) \right)^2
\end{equation}
where $l_j = r_j + \gamma (1-d_j) Q_{w'}(s'_j, \pi_{\theta'}(s'_j))$ is the label of sample $(s_j, a_j)$. The actor network $\pi_{\theta}$ is also trained using the buffer $\mathcal{B}$ with the following loss function,
\begin{equation}\label{eq:conv_actor_loss}
\textstyle \mathcal{L}_A(\mathcal{B}) =- \frac{1}{n} \sum_{j=1}^n Q_w(s_j, \pi_{\theta}(s_j)).
\end{equation}
Given the computed losses $\mathcal{L}_A$ and $\mathcal{L}_C$, the parameters of the actor and the critic networks are updated with the following gradient-based law, in an iterative and alternative manner,
\begin{equation}\label{eq:ac_grad}
\Delta \theta =- \alpha_{\theta} \nabla_{\theta} \mathcal{L}_A,~\Delta w =- \alpha_{w} \nabla_{w} \mathcal{L}_C,
\end{equation}
where $\Delta \theta$ and $\Delta w$ are the parameter increments at each iteration,  $\alpha_{\theta}, \alpha_{w} \in \mathbb{R}^+$ are the learning rates of the actor and the critic networks, and $\nabla_{\theta} \mathcal{L}_A$ and $\nabla_{w} \mathcal{L}_C$ are the gradients of the loss functions to the network parameters.


%The main advantage of an off-policy RL agent, such as DDPG, is the exploitation of historical policies. This allows the exploitation of diverse policies for value promotion, which improves the exploration capability of the agent. Similar techniques are also used to propose multi-gradient-based approaches~\cite{li2021adaptive}. Nevertheless, the lack of sample labels leads to a \textit{bootstrapping} process for the training of the critic network. The bootstrapping process makes the training performance very sensitive to the initial conditions, leading to low learning stability. In this paper, we present a novel method to improve the training performance of the off-policy agents using demonstration data. We will show that, by properly embedding the demonstration policy in the loss functions, both the stability and convergence speed of the agent training are improved.



\subsection{Behavior Cloning (BC)}\label{sec:bc}

BC is an important technology of IL to duplicate human policy from demonstrations. It has been widely applied to robot motion planning and autonomous driving~\cite{choi2020robotic, ly2020learning}. Here, behavior refers to what \textit{actions} humans tend to take under certain \textit{states}. Cloning means learning a new policy to fit human behaviors. BC formulates a supervised learning problem where human actions serve as the ground truth labels of the states. Then, the policy to be cloned can be trained using deep neural networks (DNN)~\cite{farag2018behavior} or Gaussian mixture models (GMM)~\cite{chen2017robot}. When human demonstrations are recorded as movement trajectories, states refer to the position and velocity of human trajectories at a certain time and action is the corresponding acceleration~\cite{fang2019survey}. DMP has also been used to simplify the BC process~\cite{li2023prodmp}. Nevertheless, calculating acceleration from human demonstrations requires the second-order derivative operation which brings up differential noise to the human demonstration samples. Additional procedures such as locally weighted regression (LWR) are often used to mitigate the effects of the noise~\cite{zhou2017task}. 

Another issue of BC is that the pre-trained policy might overfit human behaviors. Note that human likeness does not necessarily indicate the best planning policy. Thus, overfitting human behaviors may degrade the performance of robot motion planning with respect to the predefined reward. It is addressed that BC performs worse than nominal RL agents in many cases due to overfitting~\cite{kumar2021should}. This motivates us to combine BC and nominal RL to develop a better robot motion planner, instead of directly using BC to generate human-like trajectories. The conventional BC-facilitated RL methods usually perform a fully separate scheme, i.e., to learn an initial policy with BC before training the agent using nominal RL methods~\cite{rajeswaran2017learning, tian2021learning, nair2018overcoming}. % where DMP is commonly used~\cite{chen2017robot, matsubara2010learning, tan2011potential}. 
Nevertheless, in such a separate scheme, BC is not fully exploited to promote the training of the RL agent. It also lacks the flexibility of achieving a balance between human likeness and the predefined reward. Recent studies made some attempts to integrate BC into the off-policy RL training using a dual-buffer structure~\cite{gupta2021reinforcement, gupta2022exploiting}, which inspires us to use BC from human demonstration to leverage the training performance of RL.


%Behavior cloning has also been used to improve the training performance of an RL agent. Specifically, it is mainly used as an auxiliary approach to supervising the training of the actor network. For on-policy agents, behavior cloning is usually performed before the RL training to generate a decent, but not necessarily an optimal policy~\cite{}. This renders a separate behavior cloning process from the RL learning phase, which is often very effective since a good initial policy is very helpful to improve the training performance of an on-policy agent. 


BC refers to the process of training a target policy from a source policy encoded in a demonstration. Given a set of demonstrations that are stored in a buffer $\mathcal{B}$ as defined in Sec.~\ref{sec:mdp}, the source pattern refers to the policy $\tilde{\pi}$ that fits the data $(s_j, a_j)$ for all $j=1,2,\cdots,n$. The objective of BC is to train a parameterized policy $\pi_{\theta}$ to approximate $\tilde{\pi}$. Conventionally, BC renders a supervised learning problem, where the parameter $\theta$ is updated via a gradient-based law $\Delta \theta = -\alpha_{\theta} \nabla_{\theta} \mathcal{L}_I$,
where $\Delta \theta$ is the parameter increment, $\alpha_{\theta} \!\in\! \mathbb{R}^+$ is the learning rate, and $\nabla_{\theta} \mathcal{L}_I$ is the gradient of a loss function $\mathcal{L}_I$ defined as
\begin{equation}\label{eq:bc_loss_old}
\textstyle \mathcal{L}_I(\mathcal{B}) = \sum_{j=1}^n \mathscr{M}^2\!\left( a_j - \pi_{\theta}(s_j) \right)
\end{equation}
where $\mathscr{M}: \mathcal{A} \!\rightarrow\! \mathbb{R}_{\geq 0}$ is a non-negative function that evaluates the deviation between the two policies, which is commonly the 2-norm of vectors. Such an approach is also referred to as \textit{explicit BC (EBC)} since the loss function \eqref{eq:bc_loss_old} explicitly penalizes the deviation between actions of the source and the target policies. EBC is very likely to cause overfitting since it is obsessed with the fitting of two policies. It may not be successful when the demonstration data is noisy and subject to large variance. Recent study proposes an \textit{implicit BC (IBC)} method which suggests the following loss function~\cite{florence2022implicit},
\begin{equation}\label{eq:ibc_loss_old}
\textstyle \mathcal{L}_I(\mathcal{B}) = \sum_{j=1}^n  E_{\theta}(s_j, a_j), 
\end{equation}
where $E_{\theta}: \mathcal{S} \times \mathcal{A}\!\rightarrow\! \mathbb{R}_{\geq 0}$ is a non-negative energy function of demo data $(s_j, a_j)$ parameterized by $\theta$. Thus, IBC generates the target policy through an implicit energy function, which can effectively avoid the overfitting problem. It also provides a flexible interface to combine BC with an off-policy RL agent. A technical question to be answered in this paper is how to design a proper energy function to facilitate RL with IBC.


%In this sense, the target policy $\pi_{\mu}$ is supervised to fit the source data $\mathcal{B}$. Note that, when the variance of the buffer $\mathcal{B}$ is large, training the target network $\pi_{\mu}$ is challenging due to the over-fitting problem. In this paper, we perform behavior cloning in an alternative way, i.e., encoding the demonstration data into the experience replay buffer of an off-policy RL agent in a compatible sample format $\{\,s_j, a_j, s_j', r_j, d_j\}$, as introduced in Sec.~\ref{sec:mdp}. In this sense, behavior cloning is integrated into the training procedure of the agent, which can effectively mitigate the over-fitting problem, %although the generation of $s_j', r_j, d_j$ is difficult. Fortunately, 
%The application of dynamic movement primitives (DMP) provides a straightforward approach for this encoding.





\section{Framework and Problem Statement}\label{sec:frame}

In this section, we first introduce the overall framework of the proposed IBC-DMP RL method. Then, we present a novel Multi-DoF DMP model for motion planning of a high degree-of-freedom (DoF) robot. Finally, based on the proposed framework, we clarify the problem to be solved in this paper.

\subsection{Overall Framework}

The overall framework of IBC-DMP RL is illustrated in Fig.~\ref{fig:framework}. The basic model used to generate robot trajectories is \textit{Multi-DoF DMP}, an adapted version of the conventional DMP model introduced in Sec.~\ref{sec:dmp}. It also serves as the environment in the robot motion planning problem. The human demonstration encodes a demonstration policy $\tilde{\mathbf{f}}$ which reflects how humans behave in a motion planning task. An \textit{IBC-DMP agent} is used to generate the desired motion planning policy $\mathbf{f}$ for the task. The agent is trained by a dual-buffer structure which is composed of a \textit{demonstration buffer} and an \textit{interaction buffer}. These two buffers store the demonstration data generated from the human demonstrations and the interaction data during the learning process, respectively.
%The interaction between the recorded \textit{human demonstration} and the \textit{Multi-DoF DMP} generates the state and action data to be stored in the \textit{demo buffer}. Besides, the interaction data between the \textit{IBC-DMP agent} and the \textit{Multi-DoF DMP} are stored in the interaction buffer. Then, the IBC-DMP agent is trained using the data stored in the experience replay buffer.

% Figure environment removed




\subsection{Motion Planning Using Multi-DoF DMP}\label{sec:multi_dmp}

The conventional DMP in Sec.~\ref{sec:dmp} is defined for a one-dimensional point and has been used for robot motion planning in a decoupled manner, i.e., one DMP is used to generate the trajectory for each DoF of the robot~\cite{stulp2012reinforcement}. This leads to the lack of coupling among different DoFs, which restricts the flexibility of solving the optimal planning policy. To resolve this issue, in this paper, we propose the following Multi-DoF DMP model for the motion planning of a robot end-effector in the Cartesian space,
\begin{equation}\label{eq:dmp_multi_dof}
\tau\ddot{\x}_t \!=\! K_{\alpha} \!\left( K_{\beta} \!\left( \x_{\mathrm{g}} \!-\! \x_t \right)\! -\! \dot{\x}_t \right)\! + \zeta_t \|\x_{\mathrm{g}}\!-\!\x_{\mathrm{i}} \| \mathbf{f}(\mathcal{X}_t),
\end{equation}
where $\x_t$, $\dot{\x}_t$, $\ddot{\x}_t \!\in\! \mathbb{R}^3$ are the position, linear velocity, and acceleration of the end-effector in the Cartesian space, $\x_{\mathrm{i}}$, $\x_{\mathrm{g}} \!\in\! \mathbb{R}^3$ are the initial and goal positions of the end-effector, $K_{\alpha}$, $K_{\beta} \!\in\! \mathbb{R}^{3 \times 3}$ are parametric matrices, $\tau \!\in\! \mathbb{R}$ and $\zeta_t \in \mathbb{R}$ are the temporal scalar and the canonical variable, the same as \eqref{eq:dmp}, $\mathcal{X}_t\!=\!\{\,\x_t, \dot{\x}_t, \x_{\mathrm{b}}, \zeta_t \}$ is the state vector of the Multi-DoF DMP, where $\x_{\mathrm{b}}$ is a vector that describes the configuration of the obstacle, and $\mathbf{f}:\mathbb{R}^{10} \!\rightarrow\! \mathbb{R}^3$ is a multi-dimension actuation function to be determined. The initial state of model \eqref{eq:dmp_multi_dof} is set as $\x_0 \!=\! \x_{\mathrm{i}}$ and $\dot{\x}_0 \!=\! 0$.
In this paper, we only consider a \textit{single static obstacle} for brevity. Also, we use a vertically positioned \textit{cylinder} model to represent the obstacle. Then, the position of the obstacle is represented as a three-dimensional constant vector $\x_{\mathrm{b}} \!\in\! \mathbb{R}^3$ assigned as the Cartesian coordinate of the top surface center of the cylinder.
In this sense, the DMP state can be represented as a ten-dimensional vector $\mathcal{X}_t\!=\![\,\x_t^{\top}, \dot{\x}_t^{\top}, \x_t^{\top}-\x_{\mathrm{b}}^{\top}, \zeta_t]^{\!\top\!} \!\in\!\mathbb{R}^{10}$.
Here, we encode a time-variant vector $\x_t - \x_{\mathrm{b}}$ into the DMP state instead of the constant vector $\x_{\mathrm{b}}$ since the former provides larger diversity to the data. The incorporation of more complicated environments with multiple dynamic obstacles is beyond the scope of this paper and will be considered in future work.
%Note that $\x_{\mathrm{b}}$ does not necessarily represent one obstacle but can be the overall information of multiple obstacles. This provides the possibility of motion planning for .
The multi-DoF DMP is different from the conventional DMP~\eqref{eq:dmp} in three aspects. 
\begin{itemize}
\item The actuation function $\mathbf{f}$ does not only depend on the canonical variable $\zeta_t$ but also on the internal states of the DMP, namely $\x_t$ and $\dot{\x}_t$, and the obstacle position $\x_{\mathrm{b}}$. Different from the conventional DMP models, this may change the poles or the closed-loop dynamics of the Multi-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state-dependent actuation function $\mathbf{f}$ can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters $K_{\alpha}$ and $K_{\beta}$. This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits $\underline{f} \leq \mathbf{f}(\mathcal{X}_t) \leq \overline{f}$ to avoid this.
\item The gain of the actuation function is the absolute distance between the initial position $\x_{\mathrm{i}}$ and the goal position $\x_{\mathrm{g}}$, $\|\x_{\mathrm{g}}-\x_{\mathrm{i}}\|$, instead of the element-wise distance used by the conventional DMP~\eqref{eq:dmp}. Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.
\item The obstacle position $\x_{\mathrm{b}}$ is included in the state $\mathcal{X}_t$ of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.
\end{itemize}



%This design is intended to fully incorporate the coupling among different dimensions of the DMP to improve its flexibility. 

\subsection{Cost Function of Motion Planning}

This paper solves a general robot motion planning problem. Specifically, given an initial position $\x_{\mathrm{i}} \!\in\! \mathbb{R}^3$ and a goal position $\x_{\mathrm{g}} \!\in\! \mathbb{R}^3$ in the Cartesian space, generate a smooth trajectory $\x_t \!\in\! \mathbb{R}^3$ for $0 \!<\! t \!\leq\! T$ using the Multi-DoF DMP model in \eqref{eq:dmp_multi_dof}, such that $\x_T$ is sufficiently close to $\x_{\mathrm{g}}$, where $T \!\in\! \mathbb{R}^+$ is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in $\x_{\mathrm{b}} \!\in\! \mathbb{R}^2$. These requirements are encoded in the following cost function, 
\begin{equation}\label{eq:cost}
\mathcal{J}_t = \left\{ \begin{array}{ll}
\sum_{i=1}^4 \alpha_i \mathcal{J}_t^{(i)},  & 0 \leq t < T, \\
\alpha_5 \mathcal{J}_t^{(5)}, & t=T,
\end{array} \right.
\end{equation}
where $\alpha_i \!\in\! \mathbb{R}^+$ are constant parameters, $i=1,2,\cdots,5$, and $\mathcal{J}_t^{(i)} \!\in\! \mathbb{R}$ are instant costs at time $t$, defined as
\begin{equation*}
\begin{split}
&\mathcal{J}_t^{(1)} \!=\! \!\left\|\ddot{\x}_t \right\|^2,~\mathcal{J}_t^{(2)} \!=\! \left\|\x_t \!-\!\x_{\mathrm{g}} \right\|^2, \\
&\mathcal{J}_t^{(4)} \!=\! \left\{ \begin{array}{ll} \!\!\!\eta\!\left( \left\|\x_t^{(1,2)} \!-\! \x_{\mathrm{b}}^{(1,2)} \right\| \!-\! r_{\mathrm{b}},\varepsilon_0^{\mathrm{b}},\varepsilon_1^{\mathrm{b}} \right) &\mathrm{if}~0\!<\!\x_t^{(3)}\!<\!\x_{\mathrm{b}}^{(3)} \\
\!\!\!0 & \mathrm{otherwise}, \\
\end{array} \right. \\
&\mathcal{J}_t^{(3)} = \eta\!\left(\x_t^{(3)},\varepsilon_0^{\mathrm{d}},\varepsilon_1^{\mathrm{d}}\right),~ \mathcal{J}_t^{(5)} = \xi(\|\x_t \!-\! \x_{\mathrm{g}}\|, \varepsilon_T),
\end{split}
\end{equation*}
where $\x_t^{(1,2)} \!\in\!\mathbb{R}^2$, $\x_t^{(3)} \!\in\!\mathbb{R}$, $\x_{\mathrm{b}}^{(1,2)} \!\in\!\mathbb{R}^2$, $\x_{\mathrm{b}}^{(3)} \!\in\!\mathbb{R}$ are the slides of vectors $\x_t$ and $\x_{\mathrm{b}}$, $r_{\mathrm{b}} \in \mathbb{R}^+$ is the radius of the cylinder, $\eta:\mathbb{R}\!\rightarrow\!\mathbb{R}_{\geq 0}$ with constant parameters $0 \!<\! \varepsilon_0 \!<\! \varepsilon_1$ is an artificial potential field function defined as
\begin{equation}
\eta(x, \varepsilon_0, \varepsilon_1) \!=\! \left\{ \begin{array}{ll}
\!\!\!(x\!-\!\varepsilon_0)^{-2} \!-\! (\varepsilon_1 \!-\! \varepsilon_0)^{-2}, &\!\! \varepsilon_0 \!<\! x \!<\! \varepsilon_1, \\
\!\!\!0, &\!\! x \!\geq\! \varepsilon_1,
\end{array} \right.
\end{equation}
$0 \!<\! \varepsilon_0^{\mathrm{b}} \!<\! \varepsilon_1^{\mathrm{b}}$ and $0 \!<\! \varepsilon_0^{\mathrm{d}} \!<\! \varepsilon_1^{\mathrm{d}}$ are potential field parameters for the obstacle and the ground, and $\xi:\mathbb{R}\!\rightarrow\!\mathbb{R}_{\geq 0}$ is a squared dead-zone function defined as
\begin{equation}
\xi(x, \varepsilon) = \left\{ \begin{array}{ll}
0, & 0 \leq x \leq \varepsilon, \\
x - \varepsilon, & x > \varepsilon,
\end{array} \right.
\end{equation}
where $\varepsilon \in \mathbb{R}^+$ is the dead-zone scalar. Here, $\eta(x)$ serves as an artificial potential field function that penalizes $x$ if it gets close to $\varepsilon_0$ from the positive direction. Another parameter $\varepsilon_1$ is the upper limit of $x$ such that $\eta(x)$ has an effect. To avoid undefinition, we assign $\eta(x)$ with a large number when $x \!\leq\! \varepsilon_0$.


In the comprehensive cost function~\eqref{eq:cost}, term $\mathcal{J}_t^{(1)}$ penalizes the value of the acceleration to ensure sufficient smoothness. Term $\mathcal{J}_t^{(2)}$ penalizes the distance between the current position $\x_t$ and the goal position $\x_{\mathrm{g}}$, aiming at fast and straightforward goal reaching. Terms $\mathcal{J}_t^{(3)}$ and $\mathcal{J}_t^{(4)}$ attempt to keep the position $\x_t$ away from the ground ($z=0$ plane) and the cylinder obstacle, respectively. The usage of artificial potential field functions ensures a very large cost when $\x_t$ runs into the cylinder obstacle or the ground. The last term $\mathcal{J}_T^{(5)}$ penalizes the distance between the ultimate position $\x_T$ and the goal position $\x_{\mathrm{g}}$. A dead-zone scalar $\varepsilon_T$ is involved to prescribe the tolerable level of the ultimate error. Specifically, the ultimate cost is exerted only when the ultimate error exceeds the threshold $\varepsilon_T$.
By minimizing the comprehensive cost~\eqref{eq:cost} for the Multi-DoF DMP model~\eqref{eq:dmp_multi_dof}, one can get a smooth trajectory $\x_t$ that is able to reach the goal position $\x_{\mathrm{g}}$ at the ending time $t=T$ while avoiding collision with the ground and the obstacle. 

%the cost~\eqref{eq:cost} only considers one obstacle. It can be extended to the case with more obstacles by adding more terms similar to $\mathcal{J}_t^{(4)}$ but with different values of $\x_{\mathrm{b}}$.


\subsection{Decision-Making Problem Statement}\label{sec:statement}

Consider that the Multi-DoF DMP model \eqref{eq:dmp_multi_dof} is discretized in time in a zero-order hold manner with a discrete sampling time $\Delta t \!\in\! \mathbb{R}^+$, as follows, 
\begin{equation}\label{eq:dmp_multi_dof_dis}
\begin{split}
&\x_{t+\Delta t} \!=\! \x_t + \Delta t \dot{\x}_t ,~\dot{\x}_{t+\Delta t} \!=\! \dot{\x}_{t} + \Delta t\ddot{\x}_{t}, \\
&\zeta_{t+\Delta t} \!=\! \zeta_t - \frac{\Delta t}{\tau} \omega \zeta_t,
\end{split}
\end{equation}
where $\ddot{\x}_t$ is given by \eqref{eq:dmp_multi_dof}.
Then, the generated trajectory $\x_t$ is a set of waypoints at the sampled timing points $\mathbf{t}=\{0, \Delta t, 2\Delta t, \cdots, T\}$. In this sense, the objective of motion planning is to solve the optimal actuation function $\mathbf{f}$ in \eqref{eq:dmp_multi_dof} such that the accumulated cost $\sum_{t=0}^T \mathcal{J}_t$ is minimized. 
This problem renders decision-making over an MDP $\mathcal{M} \!=\! (\mathcal{S},\mathcal{A}, \mathscr{F}, \mathscr{R})$ defined in Sec.~\ref{sec:mdp}, where $\mathcal{S} \!=\!\{\mathcal{X}_t|\x_{\mathrm{i}} \!\in\! \mathbb{R}^3, \, 0\!\leq\! t \!<\! T\}$ is the state space, $\mathcal{A} \!=\!\left\{\mathbf{f}(s) \left| \forall\, s \in \mathcal{S} \right. \right\}$ is the action space, $\mathscr{F}$ is the state transition characterized by \eqref{eq:dmp_multi_dof_dis}, and $\mathscr{R}:-\mathcal{J}_t$ is the instant reward. In this sense, the policy $\pi$ can be represented as a parameterized actuation function $\mathbf{f}_{\theta}$, modeled as a neural network and learned using an off-policy RL method introduced in Sec.~\ref{sec:mdp}. Besides, BC introduced in Sec.~\ref{sec:bc} can be used to promote the training of the RL agent with the demonstration policy $\tilde{\mathbf{f}}$, as addressed in Sec.~\ref{sec:bc}. Note that, different from the conventional imitation learning problems, this paper does not aim at using $\mathbf{f}_{\theta}$ to approximate the demonstration policy $\tilde{\mathbf{f}}$. Instead, $\tilde{\mathbf{f}}$ is only used to improve the training of $\mathbf{f}_{\theta}$.

Now, we are ready to state the two main objectives of this paper: (1). transforming human demonstration into data that are compatible with the demonstration buffer, corresponding to the left column of Fig.~\ref{fig:framework}, and (2). train the optimal policy $\mathbf{f}$ using the demonstration buffer, as the right column of Fig.~\ref{fig:framework}. Our solutions to these two problems will be presented in Sec.~\ref{sec:human_data} and Sec.~\ref{sec:training}, respectively.


\section{Human Demonstration Collection}\label{sec:human_data}

This section interprets how we collect the human demonstration data in a recording experiment and transform it into compatible data for the demonstration buffer. We have published the dataset at~\cite{dataset}. Note that the recorded human motion data are only for public educational purposes. No private information about the subject is disclosed. Also, the configuration of the data recording experiment does not cause any risk of harm to the subject. Therefore, this research is exempt from human-related ethic issues.

\subsection{Human Data Acquisition}\label{sec:hdr}

We use a data-recording experiment to collect human-hand motion in a point-to-point reaching (P2PR) task since it is a typical scenario of robot motion planning. The experiment is designed to capture how the human hand attempts to reach a goal position while avoiding collisions with a mid-way obstacle. An HTC VIVE tracking system, consisting of several motion trackers and a pair of virtual reality (VR) base stations %, as shown in Fig.~\ref{fig:tracking}, 
is used to record the position of any object attached to the trackers. 
%Fig.~\ref{fig:tracking_1} and Fig.~\ref{fig:tracking_2} respectively show one tracker and one base station as examples. 
The system is able to track the Cartesian pose of the rigid body in a stable sampling frequency $100\,$Hz. The tracking precision can be as high as $1\,$mm.

%% Figure environment removed
The top view of the experiment configuration is shown in Fig.~\ref{fig:human_exp}. A human subject sits in front of a plat desk. The desk is $0.35\,$m vertically higher than the seat such that the subject is seated in a comfortable manner. A rectangular planar motion region is marked on the desk with height $0.5\,$mm and width $0.6\,$m, shown as a thick-line gray box in Fig.~\ref{fig:human_exp}. The motion region is aligned with the desk edge towards the subject. The subject's seat is placed such that the hand can be comfortably positioned in the left-bottom corner of the motion region and can naturally reach any point within the motion region without standing up. Two VR base stations are placed on the diagonal corners of the workspace for superior tracking precision, as shown as camera symbols in Fig.~\ref{fig:human_exp}. The coordinate of the tracking system is calibrated such that its origin coincides with the left-bottom corner of the workspace and its $x$-, $y$-, and $z$-axis point to the right, front, and top of the subject, respectively. We set the initial position $\mathcal{I}$ as the one that the hand is comfortably placed at the origin. For a given goal $\mathcal{G}$ and an obstacle $\mathcal{O}$, the subject is required to move the hand from the initial position $\mathcal{I}$ to the goal $\mathcal{G}$ in a natural manner while avoiding the obstacle $\mathcal{O}$. A possible hand trajectory is illustrated as a dashed arrow in Fig.~\ref{fig:human_exp}.

% Figure environment removed

For the development of the IBC-DMP agent in this paper, we recorded a human demonstration dataset using the experimental setup described above.  Fig.~\ref{fig:human_exp_photo} illustrates the data recording process. As illustrated in the figure, we use three trackers to mark the positions of the obstacle, the human hand, and the goal, respectively. In the experiment, we use a cylinder bottle with a height of $66\,$mm and a diameter of $200\,$mm as the obstacle. We fix the tracker to the top of the obstacle, leading to a total height of $70\,$mm. Besides, we tie a tracker to the wrist of the subject to track the motion of the hand. The third tracker is directly placed on the desk to serve as the goal. During the entire recording process, the subject performs $600$ trials of P2PR motion repeatedly. For each trial, the goal $\mathcal{G}$ is placed at a random position in the motion region. It should be not too close to the initial position $\mathcal{I}$ to ensure the feasibility of the hand motion. The obstacle $\mathcal{O}$ is also positioned at a random point close to the mid-way between the initial position $\mathcal{I}$ and the goal $\mathcal{G}$. Note that the obstacle should be neither beside the goal nor too close to the initial position. Otherwise, it would be very difficult to produce a natural trajectory. %Also, the obstacle should be placed in the mid-way between the initial and the goal positions, such that it does affect the shape of the hand motion. 
The subject is asked to always perform the motion and avoid the obstacle in the most comfortable manner. Other than that, there are no restrictions from which direction the subject must avoid the obstacle. The subject is allowed to bend the torso but cannot stand up from the seat to reach a far point. 

% Figure environment removed

After removing the invalid motions, we finally obtain $544$ hand trajectories corresponding to various goal positions and obstacle positions, as illustrated in Fig.~\ref{fig:trials_2}. Each trajectory contains the hand position of one P2PR motion.
It can be seen that different trajectories show great variety in their shapes. Taller trajectories indicate that the hand goes over the obstacle and short ones pass around the obstacles. A common feature of these trajectories is that they all successfully avoid obstacles while maintaining the smoothness of the shape. Fig.~\ref{fig:trials_4} gives a more clear perspective of how a hand trajectory reaches the target and avoids the obstacle. 

% Figure environment removed


\subsection{Data Preprocessing: Normalization}

The diversity of human hand trajectories is reflected by both their shapes and speeds. The shape of a trajectory prescribes in which direction the hand should move to avoid the obstacle, and its speed indicates how fast the hand moves. The diversity of the shapes is beneficial to improve the robustness of the trained planning agent against various obstacles and goal positions. However, the speeds of the hand trajectories do not contribute to the training robustness but bring up disturbances. Thus, a preprocessing procedure is needed to normalize the trajectory speeds while retaining the diversity of the shapes. In this paper, we are concerned with the average speed of a trajectory. Let $\pmb{\tau}=\{0,\Delta \tau, 2\Delta \tau, \cdots, \bar{T} \}$ be the time stamps of a recorded hand trajectory $\mathbf{x}_{\tau}^H \in \mathbb{R}^3$, $\tau \in \pmb{\tau}$, where $\Delta \tau =0.01\,$s is the sampling time of the VIVE tracking system. Then, the average speed is defined as $\bar{V}=L/\bar{T}$ (m/s), where $\bar{T}$ and $L=\sum_{\tau=\Delta \tau}^{\bar{T}} \|\mathbf{x}^H_{\tau} \!-\! \mathbf{x}^H_{\tau - \Delta \tau}\|_2$ are the total time duration and the total length of the trajectory, respectively. 

We normalize the recorded trajectories to a uniform average speed $\bar{V}_*=0.2\,$m/s by assigning each trajectory a new time series $\pmb{\tau}'=\{0,\Delta \tau', 2\Delta \tau', \cdots, \bar{T}' \}$, where $\displaystyle \Delta \tau' = \frac{L}{\bar{V}_* \bar{T}} \Delta \tau$ and $\bar{T}' = {L}/{\bar{V}_*}$. Then, we approximate the linear velocity of the trajectory as $\displaystyle \dot{\mathbf{x}}^H_{\tau'} = \frac{\mathbf{x}^H_{\tau'}-\mathbf{x}^H_{\tau_{i}'- \Delta \tau'}}{\Delta \tau'}$, $\tau'=\Delta \tau'$, $2\Delta \tau'$, $\cdots$, $\bar{T}'$, and calculate the maximal speed $\hat{V} = \max_i \|\dot{\mathbf{x}}^H_{\tau_i'}\|_2$. Similar to the average speed, the maximal speed is an important index to describe the kinematic property of a trajectory. We analyze the statistical properties of the total length, the maximal speed, and the average speed of all trajectories to investigate how the normalization changes their diversities. Their mean values (Mean), the standard deviations (Std), and the extreme deviations (Max-Min) are shown in Tab.~\ref{tab:p2p-ca}. It can be seen that the average speeds of the trajectories are normalized and the maximal speeds are reduced. Also, the deviation values of the maximal speeds are also reduced, which indicates less influence of the diversity of the maximal speeds on the recorded trajectories. Meanwhile, the statistical properties of the total lengths are not changed due to the consistency of the shapes of the trajectories. Therefore, the disturbance of the speeds of the trajectories is reduced while the diversity of the shapes is reserved.

%The large Std and Max-Min values of the kinematic features indicate the large diversity of the recorded hand trajectories. 
%We scale all hand trajectories to a uniform total time $T \!=\!2.5\,$s by adjusting their velocities. Then, we interpolate the scaled trials using a fixed sampling time $\Delta t \!=\! 0.02\,$s. %As a result, the scaled trajectories have a common total time $T$ and sampling time $\Delta t$. 
%The kinematic diversity of the scaled trajectories is also shown in Tab.~\ref{tab:p2p-ca}. It is noticed that both the standard and the extreme deviations of the kinematic features are reduced after the normalization, indicating reduced kinematic diversity. Note that the shapes of the trajectories are preserved in this procedure.

\linespread{1.2}
\begin{table}[htbp]
\centering
\caption{The kinematic diversity of the recorded trajectories}
\begin{tabular}{c|c|c|c|c|c|c}
	\thickhline
	\rowcolor{dark_red} \multirow{2}{*}{} & \multicolumn{3}{c|}{\color{white} \textbf{Before Normalization}} & \multicolumn{3}{c}{\color{white} \textbf{After Normalization}} \\ 
\cline{2-7}\rowcolor{mid_red} 
  & {\color{black} \textbf{Mean}} & {\color{black} \textbf{Std}} & {\color{black} \textbf{\scriptsize{Max-Min}}} & {\color{black} \textbf{Mean}} & {\color{black} \textbf{Std}} & {\color{black} \textbf{\scriptsize{Max-Min}}} \\
    \hline 
%    $T$ & 0.7823 & 0.2635 & 1.920 & 4.5236 & 1.7808 & 11.706 \\
    $L$ & 0.4526 & 0.1779 & 1.1706 & 0.4526 & 0.1779 & 1.1706 \\
\rowcolor{shadow_red}    $\hat{V}$ & 1.1667 & 0.2787 & 1.6096 & 0.3998 & 0.0595 & 0.4807 \\
    $\bar{V}$ & 0.5793 & 0.1316 & 0.7842 & 0.2 & 0 & 0 \\
	\thickhline
\end{tabular}
\label{tab:p2p-ca}
\end{table}
\linespread{1}

%\linespread{1.2}
%\begin{table}[htbp]
%\centering
% \caption{The kinematic diversity of the recorded trajectories}
% \begin{tabular}{l|c|c|c}
% 	\hline
% 	Kinematic Features & Mean & Std & Max-Min \\
%     \hline
%     Total Time $T$ (s) & 0.7815 & 0.2630 & 1.9200 \\
%     Total Length $L$ (m) & 0.4521 & 0.1781 & 1.1706 \\
%     Maximum Speed $\hat{V}$ (m/s) & 1.2391 & 0.3553 & 4.9728 \\
%     Average Speed $L/T$ (m/s) & 0.5784 & 0.1332 & 0.9676 \\
% 	\hline
% \end{tabular}
% \label{tab:p2p-ca}
% \end{table}
% \linespread{1}

% \linespread{1.2}
% \begin{table}[htbp]
% \centering
% \caption{The kinematic diversity of the scaled trajectories}
% \begin{tabular}{l|c|c|c}
% 	\hline
% 	Kinematic Features & Mean & Std & Max-Min \\
%     \hline
%     Total Time $T$ (s) & 1.600 & 0 & 0 \\
%     Total Length $L$ (m) & 0.450 & 0.175 & 0.9170 \\
%     Maximum Speed $\hat{V}$ (m/s) & 0.606 & 0.281 & 2.0640 \\
%     Average Speed $L/T$ (m/s) & 0.281 & 0.110 & 0.5730 \\
% 	\hline
% \end{tabular}
% \label{tab:p2p-ca_scaled}
% \end{table}
% \linespread{1}

\subsection{Buffer Generation}\label{sec:data_gen}

After the normalization process, we transform the normalized hand trajectories into demonstration data that are compatible with the experience replay buffer using the Multi-DoF DMP model, as illustrated in the left column of Fig.~\ref{fig:framework}. To fit the sampling rate of the DMP model $\Delta t$, we use the time stamp series $\mathbf{t} = \{0, \Delta t, 2\Delta t, \cdots, T\}$ to interpolate the normalized trajectory $\x^H_{\tau}$, $\tau \in \pmb{\tau}$. The interpolated trajectory is denoted as $\x^H_{t}$, $t \in \bf{t}$.

As addressed in Sec.~\ref{sec:mdp}, the data for the experience replay buffer have the format $\{\,s_t, a_t, r_t, s'_t, d_t\,\}$, for a certain time $t\in \mathbb{N}^+$, where the state $s_t = \mathcal{X}_t$ is also the state of the Multi-DoF DMP model as introduced in Sec.~\ref{sec:multi_dmp}.%, with the variable $\x_t$ in $\mathcal{X}_t$ substituted by $\x_t^H$. 
The action $a_t \!=\! \tilde{\mathbf{f}}\!\left(\mathcal{X}_t \right)$ is the output of the demonstration policy $\tilde{\mathbf{f}}$ which allows $\x_t$ to fit the Multi-DoF DMP model. % Here, we use the notation $\tilde{\mathbf{f}}$ to indicate that it reflects the human demonstration policy, which is consistent with the notation in Fig.~\ref{fig:framework}.
Then, $s'\!=\!\mathcal{X}_{t+\Delta t}$ is the successive state of $s_t$ under the action $a_t$, $r_t\!=\!-\mathcal{J}_t$ is the instant reward, and $d_t$ is a binary value to determine whether $\x_t$ reaches the goal $\mathbf{x}_{\mathrm{g}}$. 

The determination of action $a_t$ is not trivial since the demonstration policy $\tilde{\mathbf{f}}$ is not previously known. 
In conventional work, each action sample $a_t=\tilde{\mathbf{f}}(\mathcal{X}_t)$ for a given state sample $\mathcal{X}_t$ is directly computed by inverting the DMP model~\eqref{eq:dmp_multi_dof}. This requires the acceleration $\ddot{\x}_t$ calculated via a twice-difference operation which brings differential noises to the action samples. As a result, the variance of the samples may be increased, which may disturb the learning process. In this paper, we use a PID-based approach to generate actions for the state samples. Specifically, by representing the positions and velocities of a human hand trajectory as $\x_t^H, \dot{\x}_t^H \!\in\! \mathbb{R}^3$, we use a Multi-DoF DMP model described by \eqref{eq:dmp_multi_dof} and \eqref{eq:dmp_multi_dof_dis} with the following actuation function to generate a trajectory $\x_t$ that fits the hand trajectory $\x^H_t$,
\begin{equation}
\tilde{\mathbf{f}}(\mathcal{X}_t) = K_{\mathrm{P}} (\x_t^H\!-\!\x_t) + K_{\mathrm{D}} (\dot{\x}_t^H\!-\!\dot{\x}_t),
\end{equation}
with initial and goal conditions $\x_{0}\!=\!\x_{0}^H$, $\x_{\mathrm{g}}\!=\!\x_{\mathrm{g}}^H$, and $\dot{\x}_{0}\!=\!0$, where $K_{\mathrm{P}}=1500 I$ and $K_{\mathrm{P}} =40 I$ are constant matrices, and $I\!\in\! \mathbb{R}^{3\times 3}$ is an identity matrix. The main advantage of the PID-based method is not requiring the acceleration $\ddot{\x}_t$. Thus, it can reduce the noise introduced to the samples. With proper parameters $K_{\mathrm{P}}$, $K_{\mathrm{P}}$, the generated trajectory $\x_t$ coincides with the recorded hand trajectory $\ddot{\x}_t$ with small errors $\x_t^H - \x_t$, which ensures the efficacy of the action samples.

Having determined the action sample $a_t$ for the state sample $s_t$, the successive state $s'_t$ can also be calculated using the Multi-DoF DMP model. The reward $r_t$ can be calculated using $-\mathcal{J}_t$ from the reward function \eqref{eq:cost}. Then, the termination flag $d_t$ is determined as follows,
\begin{equation}\label{eq:term_flag}
d_t = \left\{ \begin{array}{ll}
1 & \mathrm{if}\, t = \bar{T}\, \mathrm{or} \, \|\x_t - \x_{\mathrm{g}}\|_2 \leq \varepsilon_T, \\
0 & \mathrm{otherwise},
\end{array}\right.
\end{equation}
where $\bar{T}=5\,$s is the maximal time length of an episode.

We ultimately transform 544 recorded hand trajectories into 123171 samples. 
The parameters of the Multi-DoF DMP model and the cost function $\mathcal{J}_t$ are shown in Tab.~\ref{tab:hyper-param-dmp}. 

\linespread{1.2}
\begin{table}[htbp]
\centering
\caption{The Parameters of the Multi-DoF DMP Model and the Cost}
\begin{tabular}{c|c||c|c||c|c||c|c}
	\thickhline
	\rowcolor{dark_blue} {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} & {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} & {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} & {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} \\
        \hline
        $K_{\alpha}$ & $10 I$ & $K_{\beta}$ & $1.2 I$ & $\tau$ & $0.25$ & $\omega$ & $6$ \\
        \rowcolor{shadow_blue} $\underline{f}$ & $-5$ & $\overline{f}$ & $5$ & $\alpha_1$ & $0.001$ & $\alpha_2$ & $10$ \\
        $\alpha_3$ & $0.001$ & $\alpha_4$ & $0.001$ & $\alpha_5$ & $10^5$ & $\varepsilon_T$ & $0.01$ \\
        \rowcolor{shadow_blue} $\varepsilon_0^{\mathrm{b}}$ & $0.05$ & $\varepsilon_1^{\mathrm{b}}$ & $0.045$ & $\varepsilon_0^{\mathrm{d}}$ & $0.01$ & $\varepsilon_1^{\mathrm{d}}$ & $0.05$ \\
	\thickhline
\end{tabular}
\label{tab:hyper-param-dmp}
\end{table}
\linespread{1}



\section{Training of the IBC-DMP agent}\label{sec:training}

Having converted the recorded hand trajectories to the demonstration data, we propose the training method of the off-policy RL agent with demonstration-based BC. We first give an overview of the training method. Then, we specifically interpret two important technical points of the proposed method, namely actor loss reshaping and critic loss refinement. Finally, we give the algorithm for agent training.


\subsection{Overview of the Training Method for IBC-DMP RL}\label{sec:m_overview}

The training process of the IBC-DMP agent is organized as a flow chart illustrated in Fig.~\ref{fig:over_view}. Similar to a traditional DDPG agent, the IBC-DMP agent consists of four forward neural networks (FNN), namely a source actor network $\pi_{\theta}: \mathcal{S} \rightarrow \mathcal{A}$, a target actor network $\pi_{\theta'}: \mathcal{S} \rightarrow \mathcal{A}$, a source critic network $Q_w: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, and a target critic network $Q_{w'}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, where the subscripts $\theta$, $\theta'$, $w$ and $w'$ denote the parameters of these networks. The source actor and critic networks are constructed to approximate the optimal policy and the value function of the RL problem. Meanwhile, the target networks are used to improve the stability of this approximation. % Three loss functions are designed to update the parameters of the neural networks. 
Besides, the IBC-DMP agent is equipped with a dual-buffer structure, which is inspired by the previous work on off-policy RL~\cite{ying2022trajectory, gupta2022exploiting}. The \textit{demo buffer} is used to store the demonstration data of the human motion recorded in Sec.~\ref{sec:human_data} and the \textit{replay buffer} serves as a normal experience replay storage unit. The buffers are designed with fixed depths. Both buffers import the demonstration data from the recorded demo trials before the start of the training process. They also provide the batched data to calculate the losses for the FNNs. The training process is summarized as the following five steps.

% Figure environment removed

\subsubsection{Demonstration Data Importing}

The human demonstration data generated in Sec.~\ref{sec:human_data} are imported to the demonstration buffer and then shuffled. They are also imported to the interaction buffer to boot up the training process. The importing processes are denoted as solid arrows in Fig.~\ref{fig:over_view}. This step is executed only once at the very beginning of each training process.

\subsubsection{Batch Sampling}

During the training process, four data batches are regularly sampled from the two buffers, denoted as $\mathcal{B}_{\mathrm{D}}^{\pi}$, $\mathcal{B}_{\mathrm{D}}^{Q}$, $\mathcal{B}_{\mathrm{I}}^{\pi}$, and $\mathcal{B}_{I}^Q$, respectively. The subscript $\mathcal{D}$ indicates that the batches are sampled from the Demo Buffer and $\mathcal{I}$ means from the Replay Buffer. The superscripts $\pi$ and $Q$ denote that the batches are used to update the neural networks. The data in the batches are used to update the parameters of the actor and critic networks. The sampling is conducted individually and independently to eliminate the dependence between the samples. The sampling process is represented as double dashed arrows in Fig.~\ref{fig:over_view}. The sampling frequency of the data batches is usually the same as the update rate of the networks.

\subsubsection{Network Updates}

The sampled data batches are used to calculate the losses of the actor network and the critic network using loss functions $\mathcal{L}_A$ and $\mathcal{L}_C$, respectively. In this paper, we propose novel loss functions for the training of the neural networks of the IBC-DMP agent, which will be introduced in Sec.~\ref{sec:bc}. 
With the loss functions $\mathcal{L}_A$ and $\mathcal{L}_C$, the parameters of the source networks are updated using the gradient-based law in \eqref{eq:ac_grad}. Then, the target networks are updated using \eqref{eq:tac_grad}. This process is denoted by double arrows in Fig.~\ref{fig:over_view}.

%Before the training process starts, the demonstration data is copied to both the demo buffer and the replay buffer. During the training process, the actor and the critic are respectively updated using the gradients of the loss functions $\!\left(\mathcal{B}_{\mathrm{D}}^{\pi}, \mathcal{B}_{\mathrm{I}}^{\pi} \right)$ and $\mathcal{L}_C\!\left(\mathcal{B}_{\mathrm{D}}^{Q}, \mathcal{B}_{\mathrm{I}}^{Q} \right)$, where $\mathcal{B}_{\mathrm{D}}^{\pi}$ and $\mathcal{B}_{\mathrm{D}}^{Q}$ are data batches sampled from the demo buffer and $\mathcal{B}_{\mathrm{I}}^{\pi}$ and $\mathcal{B}_{Q}^{\mathrm{r}}$ are from the replay buffer. In this paper, the sizes of all these four batches are set as the same value $i \in \mathbb{N}^+$ and $i \ll ????????$, although they could be different. 

\subsubsection{Experience Storing and Forgetting}

Similar to all off-policy RL agents with experience replay, the IBC-DMP agent also stores its interaction data with the environment in the interaction buffer, at every sampling instant. As shown in Fig.~\ref{fig:over_view}, the latest interaction data is always added to the tail of the interaction buffer, right after the demonstration data. The storing process is represented as a dashed arrow. The interaction buffer has a fixed size such that the old data is forgotten and replaced by the new data. Thus, the demonstration data are ultimately purged from the interaction buffer and lose their impacts on the data batches $\mathcal{B}_{\mathrm{D}}^Q$ and $\mathcal{B}_{\mathrm{I}}^Q$ as the learning proceeds. 

\subsection{Reshaped Actor Loss Based on IBC}\label{sec:ibc}

One of the critical technical points of the proposed IBC-DMP agent is the proper design of the loss functions to train the networks. In this paper, we propose a reshaped loss function for the actor network,
\begin{equation}\label{eq:la}
\mathcal{L}_A \!\left(\mathcal{B}_{\mathrm{D}}^{\pi}, \mathcal{B}_{\mathrm{I}}^{\pi} \right) = \hat{\mathcal{L}}_{A}\!\left(\mathcal{B}_{\mathrm{I}}^{\pi} \right) + \lambda_{\mathrm{BC}} \mathcal{L}_{BC} \!\left(\mathcal{B}_{\mathrm{D}}^{\pi} \right),
\end{equation}
where $\hat{\mathcal{L}}_{A}\!\left(\mathcal{B}_{\mathrm{I}}^{\pi} \right)$ has the same form as a conventional actor loss function defined in \eqref{eq:conv_actor_loss}, $\mathcal{L}_{BC}\!\left(\mathcal{B}_{\mathrm{D}}^{\pi} \right)$ is a novel IBC loss function and $\lambda_{\mathrm{BC}} \in \mathbb{R}^+$ is the BC ratio to adjust the proportion of $\lambda_{\mathrm{BC}}$ in the overall actor loss $\mathcal{L}_A$. For any data buffer $\mathcal{B}$ sized $n \in \mathbb{N}^+$, the IBC loss is calculated as
\begin{equation}\label{eq:bc_loss}
\textstyle \mathcal{L}_{BC}\!\left(\mathcal{B} \right) \!=\! -\frac{1}{n} \!\sum_{j=1}^{n} E_w(\pi_{\theta}|s_j,a_j),
\end{equation}
where $E_w(\pi_{\theta}|s_j,a_j)$ is an energy function of the current policy $\pi_{\theta}$ defined as
\begin{equation}
E_w(\pi_{\theta}|s_j,a_j) \!=\! -\mathrm{ReLU}\! \left( Q_w(s_j, a_j)\!-\! Q_w(s_j, \pi_{\theta}(s_j)) \right),
\end{equation}
where $\{s_j, a_j\}$ are the $j$-th state and action samples of $\mathcal{B}$, $j = 0, 1, \cdots, n$, and $\mathrm{ReLU}(x) = \max(x,0)$, $x\in \mathbb{R}$, is a Rectified Linear Unit (ReLU) function.

The actor loss function in \eqref{eq:la} consists of two parts. The first part $\hat{\mathcal{L}}_A$ is defined on the interaction data batch $\mathcal{B}_{\mathrm{I}}^{\pi}$ and has the same form as the conventional actor loss function of an off-policy RL agent. The second part $\mathcal{L}_{BC}$ is a novel IBC-based loss function defined on the demonstration data batch defined on the interaction data batch $\mathcal{B}_{\mathrm{D}}^{\pi}$. It is used to penalize the current policy $\pi_{\theta}$ if produces a worse action $\pi_{\theta}(s_j)$ than the demonstration action $a_j$. As a result, it forces the policy $\pi_{\theta}$ to perform better than the demonstration policy during the training process. The extent of this effect is controlled by the BC ratio $\lambda_{\mathrm{BC}}$. In this sense, BC is seamlessly integrated into the training process of the actor network, which leads to a flexible manner of updating the policies.
%We note that the data batch $\mathcal{B}_{R}^{Q}$ used to calculate the loss $\hat{\mathcal{L}}_A$ is sampled from the experience Replay Buffer following a similar manner to the conventional DDPG agent. Especially when the demo data is completely eliminated from the Replay Buffer, the actor loss $\hat{\mathcal{L}}_A(\mathcal{B}_{\mathrm{I}}^{\pi})$ is exactly the same as the conventional DDPG. 

The form of the BC loss function \eqref{eq:bc_loss} is inspired by the EBC-based RL in previous work which penalizes the deviation between the current policy $\pi_{\theta}$ and the demonstration policy~\cite{gupta2021reinforcement}. %However, the IBC loss function is different from EBC. 
Nevertheless, \eqref{eq:bc_loss} adopts the IBC technology which penalizes a certain energy function of the current policy~\cite{florence2022implicit}. In this paper, the energy function is selected as the deviation between the value functions of the current policy and the demonstration policy. Meanwhile, a ReLU operation is exerted to the deviation since no penalty is needed if the current policy performs better than the demonstration policy. The IBC-based loss function is dedicated to improving the value of the current policy instead of its similarity to the demonstration policy. For example, the penalty may not be exerted even if the two policies $a_j$ and $\pi_{\theta}(s_j)$ are not the same, as long as $\pi_{\theta}(s_j)$ is superior to $a_j$ according to the current value $Q_w$. Therefore, the IBC-based loss function is expected to have a better capability of avoiding overfitting the demonstration policy compared to the EBC-based one.

% Similar to EBC, IBC also evaluates the similarity between the demonstration behavior $a_j$ and the current policy $\pi_{\theta}(s_j)$ and penalizes the deviation. Thus, the overall loss $\mathcal{L}_A$ does not only attempts to bring down the policy value loss depicted by $\hat{\mathcal{L}}_A$, but also drives the current policy towards the direction where it resembles the demonstration policy. Nevertheless, different from EBC which directly compares the actions, IBC performs in an Implicit manner by evaluating the difference between their values with respect to the current approximated value function $Q_w$. This means the penalty may not be exerted even if the two policies $a_j$ and $\pi_{\theta}(s_j)$ are not the same, as long as $\pi_{\theta}(s_j)$ is superior to $a_j$ according to the current value $Q_w$. Such an improvement can effectively avoid overfitting the demonstration policy. The value-oriented manner for policy evaluation also makes the BC process more suitable for off-policy RL.

\subsection{Refined Critic Loss}

The IBC loss in \eqref{eq:conv_actor_loss} depends on the parameter $w$ of the critic network $Q_w$. If the critic network $Q_w$ is not well trained, the BC loss $\mathcal{L}_{BC}$ may not be able to 
accurately capture the penalty that should be imposed on the current policy $\pi_{\theta}$, leading to an invalid loss that does not help the policy update. This is especially likely to occur in the initial stage of the learning process. % As a result, the effectiveness of BC to improve the agent's learning speed is counteracted and reduced. 
To solve this problem, we use a refined data batch $\mathcal{B} \!=\! \mathcal{B}_{\mathrm{D}}^{Q} \!\cup\! \mathcal{B}_{\mathrm{I}}^Q$ to compose the loss function for the critic network, $\mathcal{L}_C\!\left(\mathcal{B}_{\mathrm{D}}^{Q} \!\cup\! \mathcal{B}_{\mathrm{I}}^Q \right)$, where the form of $\mathcal{L}_C$ is defined in \eqref{eq:conv_critic_loss}.
% \begin{equation}\label{eq:loss_critic}
% \textstyle    \mathcal{L}_C\!\left(\mathcal{B}^{Q} \right) = \frac{1}{N^{Q}} \sum_{j=1}^{N^{Q}} \left(l_j  - Q_{w}(s_j, \pi_{\theta}(s_j))  \right)^2,
% \end{equation}
% where $\mathcal{B}^{Q} \!=\! \mathcal{B}_{\mathrm{D}}^{Q} \!\cup\! \mathcal{B}_{\mathrm{I}}^Q$ is the conjuncted batch sized $N^{Q} \!=\! N_{\mathrm{D}}^{Q} \!+\! N_{\mathrm{R}}^{Q}$, and $l_j \!=\! r_j \!+\! \gamma(1-d_j)Q_{w'}\!(s'_j, \pi_{\theta'}(s'_j))$ is the label of the data $s_j$ calculated using the reward $r_i$, the successive state $s'_j$, the termination index $d_j$, and the target networks $\pi_{\theta'}$ and $Q_{w'}$. All data $\{s_i, r_i, s'_i, d_i\}$ used to calculate the loss are from the overall batch $\mathcal{B}^Q$. The loss function $\mathcal{L}_C$ has a similar form to a conventional critic loss shown in~\eqref{eq:conv_critic_loss}. 
Here, we refer to $\mathcal{B}_{\mathrm{D}}^{Q} \!\cup\! \mathcal{B}_{\mathrm{I}}^Q$ as a refined data batch since it has a larger proportion of demonstration data compared to an interaction data batch $\mathcal{B}_{\mathrm{I}}^Q$ of the same size. The ratio between the sizes of $\mathcal{B}_{\mathrm{D}}^{Q}$ and $\mathcal{B}_{\mathrm{I}}^Q$ is referred to as the refining factor $\lambda_{\mathrm{RF}}$. Here, we assume that human demonstration data are very likely to have higher values than random data. Therefore, a refined data batch with a larger $\lambda_{\mathrm{RF}}$ is likely to better describe the true critic loss and more helpful to the booting up of the training of the critic network. However, an overlarge $\lambda_{\mathrm{RF}}$ may lead to overfitting of the human demonstration.
%Nevertheless, instead of purely using the experience replay data to compute the critic loss, $\mathcal{L}_C$ maintains a certain proportion of the demonstration data in the overall batch $\mathcal{B}^{Q}$. The value of the proportion is determined by the ratio between the batch sizes $n_{\mathrm{D}}^{Q}$ and $n_{\mathrm{R}}^{Q}$. Compared to $\mathcal{B}_{\mathrm{I}}^Q$, the data from the demo batch $\mathcal{B}_{\mathrm{D}}^Q$ are usually better labeled and can help the bootup of the learning of the value function $Q_w$. The most intuitive manner is to keep this proportion constant during the entire learning process. 
In this paper, we select $\lambda_{\mathrm{RF}}$ as a constant.
For better performance, the demo batch $\mathcal{B}_{\mathrm{D}}^{Q}$ can be gradually eliminated from $\mathcal{B}^{Q}$ as the learning proceeds, which renders a decreasing refining factor. This can be an interesting topic for future work.


%For off-policy agents, the BC process is usually embedded into the RL learning stage by adding the cloning loss \eqref{eq:bc_loss_old} to the actor loss function \eqref{eq:conv_actor_loss} as a correction term~\cite{}. As a result, the cloning loss exerts an additional penalty to drive the current policy toward the demonstration policy. Compared to on-policy agents, the effectiveness of BC to off-policy agents is relatively limited, since the critical issue of off-policy RL is the proper approximation of the value function, instead of the actor function. Also, the loss function used for imitation learning as shown in \eqref{eq:bc_loss_old} is not suitable for the training of RL agent since it may lead to the over-fitting problem when the variance of the data batch is large. In this paper, our method is designed to improve the training of both the actor and the critic networks using BC. 

\subsection{The Training Algorithm for An IBC-DMP Agent}

The training procedure of the IBC-DMP agent is formulated as Algorithm \ref{ag:demo}, where $N$ is the total number of episodes during the training process, $T$ is the ending time of an episode, and $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ is a random variable for a certain time $t = 0, 1, \cdots, T$. Algorithm \ref{ag:demo} inherits the following common points from the conventional off-policy RL method.
\begin{itemize}
\item Initialization of the four neural networks (lines 2 and 3).
\item The random sampling of actions (line 9).
\item Storing interaction data (line 13).
\item Sampling data from the interaction buffer (line 16).
\item The update methods for the networks (lines 19 and 20).
\end{itemize}
Meanwhile, Algorithm~\ref{ag:demo} is different from the conventional off-policy RL agent in the following aspects.
\begin{itemize}
\item Initialization of the buffers $\mathcal{B}_{\mathrm{D}}$ and $\mathcal{B}_{\mathrm{I}}$ (line 1).
\item A Multi-DoF DMP as the environment model (line 10).
\item Sampling data from the demonstration buffer (line 15).
\item The computation of loss functions (lines 17 and 18).
\end{itemize}


%\RestyleAlgo{ruled}
\begin{algorithm}[htbp]
\caption{The demo IBC-DMP DDPG algorithm}\label{ag:demo}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE demonstration buffer $\mathcal{B}_{\mathrm{D}}$ and interaction buffer $\mathcal{B}_{\mathrm{I}}$
\ENSURE trained policy parameter $\theta$
\STATE Import demonstration data to buffers $\mathcal{B}_{\mathrm{D}}$, $\mathcal{B}_{\mathrm{I}}$
\STATE Randomly initialize the source networks $\pi_{\theta}$, $Q_{w}$
\STATE Initialize the target networks $\pi_{\theta'}$, $Q_{w'}$ with $\theta' \!\leftarrow\! \theta$, $w' \!\leftarrow\! w$

\FOR{$i \leftarrow 1$ \TO $N$}
    \STATE Sample the initial position $\x_{\mathrm{i}}$, the obstacle position $\x_{\mathrm{b}}$, and the goal position $\x_{\mathrm{g}}$
    \STATE Initialize the state $s_0 \!=\! \mathcal{X}_{0}$
    \FOR{$t \leftarrow 0$ \TO $T$}
        \STATE Observe the state $s_t \!=\! \mathcal{X}_{t}$
        \STATE Sample an action $a_t=\pi_{\theta}(s_t) + \epsilon_t$
        \STATE Update the DMP model using \eqref{eq:dmp_multi_dof_dis}
        \STATE Observe the successive state $s_t'\!=\!\mathcal{X}_{t+\Delta t}$
        \STATE Calculate the instant reward $r_t$ and flag $d_t$
        \STATE Store $\{s_t, a_t, s_t', r_t, d_t\}$ to $\mathcal{B}_{\mathrm{I}}$
        \IF{\textit{UPDATE} is \textbf{true}}
        \STATE Sample random batches $\mathcal{B}_{\mathrm{D}}^{\pi}$, $\mathcal{B}_{\mathrm{D}}^Q$ from $\mathcal{B}_{\mathrm{D}}$
        \STATE Sample random batches $\mathcal{B}_{\mathrm{I}}^{\pi}$, $\mathcal{B}_{\mathrm{I}}^Q$ from $\mathcal{B}_{\mathrm{I}}$
        \STATE Calculate the actor loss $\mathcal{L}_A\!\left(\mathcal{B}_{\mathrm{D}}^{\pi}, \mathcal{B}_{\mathrm{I}}^{\pi} \right)$ using \eqref{eq:la}
        \STATE Calculate the critic loss $\mathcal{L}_B\!\left(\mathcal{B}_{\mathrm{D}}^{Q} \!\cup\! \mathcal{B}_{\mathrm{I}}^Q \right)$ using \eqref{eq:conv_critic_loss}
        \STATE Update the source networks $\pi_{\theta}$, $Q_w$ using~\eqref{eq:ac_grad}
        \STATE Update the target networks $\pi_{\theta'}$, $Q_{w'}$ using~\eqref{eq:tac_grad}
        \ENDIF
    \STATE $s_t \leftarrow s_t'$
    \ENDFOR

    
\ENDFOR

\end{algorithmic}
\end{algorithm}


\section{Evaluation in Simulation}\label{sec:va_sim}

In this section, we evaluate the efficacy of the proposed IBC-DMP RL agent using a simulation study. We consider a point-to-point reaching case in the three-dimensional task space, where the end-effector of a robot is expected to move from a fixed initial position $\x_{\mathrm{i}}\!=\!(0,\,0,\,0.05)\,$m to an arbitrary goal position $\x_{\mathrm{g}}$ while avoiding the collision with a cylinder obstacle placed at an arbitrary position $\x_{\mathrm{b}}^{(1,2)}$. The radius and height of the obstacle are $r_{\mathrm{b}}=0.035\,$m and $\x_{\mathrm{b}}^{(3)} = 0.16\,$m which are the same as the demonstration recording experiment in Sec.~\ref{sec:human_data}. Note that assigning a fixed initial position $\x_{\mathrm{i}}$ does not lose the generality since we can always use a coordinate transformation to transform any initial position in practice to $\x_{\mathrm{i}}$. Also, in this simulation study, we treat the robot end-effector as a point and only consider its position. The experimental study in Sec.~\ref{sec:exp} will show how to deploy the trained policy to the end-effector of a robot manipulator in a pick-and-place task. All length units are in meters. 
The code for the simulation study is implemented based on the Spinningup baseline programs~\cite{SpinningUp2018} and written in Python. All training and test processes are performed on a Thinkpad laptop workstation with Intel(R) Core(TM) i7-10750H CPU at $2.60\,$GHz.
The programs and data of the simulation studies are published at~\cite{code}.

\subsection{Agent Training}\label{sec:sim_training}

With the demonstration data collected in Sec.~\ref{sec:human_data}, we use Algorithm \ref{ag:demo} to train an IBC-DMP agent for the point-to-point reaching task. The size of the demo buffer $\mathcal{B}_{\mathrm{D}}$ is $N_{\mathrm{D}}\!=\!123171$, which contains all eligible demonstration samples. The size of the experience replay buffer $\mathcal{B}_{\mathrm{I}}$ is $N_{\mathrm{R}}\!=\!10^6$ which is sufficiently large to include both the demonstration samples and the history samples during the training process. Other hyper-parameters of training are listed in Tab.~\ref{tab:hyper-param}, where $N_{\mathrm{D}}^{\pi}$, $N_{\mathrm{D}}^{Q}$, $N_{\mathrm{I}}^{\pi}$, and $N_{\mathrm{I}}^{Q}$ are the sizes of the data batches $\mathcal{B}_{\mathrm{D}}^{\pi}$, $\mathcal{B}_{\mathrm{D}}^{Q}$, $\mathcal{B}_{\mathrm{I}}^{\pi}$, and $\mathcal{B}_{\mathrm{I}}^{Q}$. The configuration corresponds to a refining factor $\lambda_{\mathrm{RF}}=9$.
The actor and the critic are three-layer forward neural networks in which the neuron numbers are $64$, $128$, and $64$. The actuation functions of the networks are Rectified Linear Unit (ReLU) functions. The parameters of the networks are randomly initialized. 


\linespread{1.2}
\begin{table}[htbp]
\centering
\caption{The Hyper-Parameters of the Training of the IBC-DMP Agent}
\begin{tabular}{c|c||c|c||c|c||c|c}
	\thickhline
	\rowcolor{dark_yellow} {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} & {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} & {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} & {\color{white} \textbf{Par.}} & {\color{white} \textbf{Value}} \\
        \hline
        $\gamma$ & $0.99$ & $\sigma$ & $0.1$ & $T$ & $5$ & $\Delta t$ & $0.02$ \\
        \rowcolor{shadow_yellow} $N$ & $500$ & $\lambda$ & $0.995$ &
        $\alpha_{\theta}$ & $10^{-3}$ & $\alpha_{w}$ & $10^{-3}$ \\
        $N_D^Q$ & $450$ & $N_I^Q$ & $50$ &
        $N_D^{\pi}$ & $100$ & $N_I^{\pi}$ & $100$ \\
	\thickhline
\end{tabular}
\label{tab:hyper-param}
\end{table}
\linespread{1}

% For any time $0\leq t \leq T$ in a training episode, the sample data $(s_t, a_t, s_t', r_t, d_t)$ are determined using the same method as in Sec.~\ref{sec:data_gen} except that the actor being trained serves as the actuation function $\mathbf{f}$. 

Each training episode is a run of the Multi-DoF DMP model~\eqref{eq:dmp_multi_dof_dis} from the fixed initial position $\x_{\mathrm{i}}$ to a random goal position $\x_{\mathrm{g}}$ uniformly sampled from $\x_{\mathrm{g}} \!\in\! \mathcal{P}_{\mathrm{g}}$, where $\mathcal{P}_{\mathrm{g}}=\{P_{\mathrm{g}},\,\Delta P_{\mathrm{g}}\}$ is a polyhedral region with $P_{\mathrm{g}}=(0.30~0.35~0.08)\,$m being its center coordinate and $P_{\mathrm{g}} \!+\! \Delta P_{\mathrm{g}}$ being its vertexes, where $\Delta P_{\mathrm{g}}\!=\!(\pm 0.05~\pm 0.05~\pm 0.01\,)\,$m. The purpose of uniform sampling is to improve the robustness of the trained agent against the perturbation of the actual goal positions. 
% The time length of each episode is $5\,$s with a discrete-time sampling interval $0.02\,$s, resulting in $250$ steps per episode. Thus, the total number of interactions per training is $1.25 \times 10^5$.
We also randomly sample the obstacle position by $\x_{\mathrm{b}} \!\in\! \mathcal{P}_{\mathrm{b}}(\x_{\mathrm{i}}, \x_{\mathrm{g}})$ to improve the robustness of the agent to obstacle positions, where $\mathcal{P}_{\mathrm{b}}(\x_{\mathrm{i}}, \x_{\mathrm{g}}) \!=\! \{P_{\mathrm{b}}(\x_{\mathrm{i}}, \x_{\mathrm{g}}), \Delta P_{\mathrm{b}}\}$ is a polyhedral sampling region that depends on the initial position $\x_{\mathrm{i}}$ and the sampled goal position $\x_{\mathrm{g}}$, with $\displaystyle P_{\mathrm{b}} = \frac{\x_{\mathrm{i}} \!+\! \x_{\mathrm{g}}}{2}$ being the geometry center and $P_{\mathrm{b}} \!+\! \Delta P_{\mathrm{b}}$ being the vertexes, where $\Delta P_{\mathrm{b}}\!=\!(\pm 0.05~\pm 0.05~\pm 0.02)\,$m. We set the obstacle sampling space $\mathcal{P}_{\mathrm{b}}$ generally in the mid-way between the fixed initial position $\x_{\mathrm{i}}$ and the goal sampling space $\mathcal{P}_{\mathrm{g}}$ to intentionally create challenging environmental configurations for the agent training.


We train three IBC-DMP agents with different BC ratios $\lambda_{\mathrm{BC}}=1,1.5,2$ to evaluate the influence of IBC on agent training. To show the advantage of the IBC-DMP agent, we also conduct a comparison study with a conventional agent without IBC. The experience replay buffer of the no-IBC agent only has one interaction buffer. For a fair comparison, the interaction buffer is filled with random samples generated from the multi-DoF DMP model \eqref{eq:dmp_multi_dof_dis} using random actions. Apart from this, all other training parameters of the no-IBC agent are the same as the IBC-DMP agents. Moreover, for each agent, we repeatedly train $M=10$ policies with the same initial condition but with different random seeds. The evaluation of the training performance of the agents will be conducted on all $M$ policies to balance the effects of randomness. 


% we fill the demo batch $\mathcal{B}_{\mathrm{D}}$ with $N_{\mathrm{D}}\!=\!123171$ random samples generated from the multi-DoF DMP model \eqref{eq:dmp_multi_dof_dis} using random actions. All algorithms are implemented based on the Spinningup baseline programs~\cite{SpinningUp2018}. 

%\subsection{Training Performance}

\subsubsection{Convergence Performance}\label{sec:Lij}
We first evaluate the convergence performance of the agents during the training process. For each training episode $i=1,2,\cdots, N$ of each randomization $j=1,2,\cdots, M$, we use the accumulated reward per training episode (ARPE) score defined as $R_{ij}\!=\!\sum_{t=0}^Tr_t^{ij}$ to value the training performance of an episode, where $r_t^{ij}$ is the instant reward at step $t=0,1,\cdots, T$ of episode $i$ for random seed $j$. Then, the performance of an RL agent can be evaluated by whether the ARPE converges to a high value. Nevertheless, the ARPE values among different training episodes may vary greatly. On the other hand, all ARPE values are non-positive. Therefore, we use the logarithm of ARPE (L-ARPE) defined as $L_{ij}\!=\!-\mathrm{ln}(1\!-\!R_{ij})$ to denote the training performance, for better visualization. The lines indicating the change of the L-ARPEs of all agents as the episode increases are illustrated in Fig.~\ref{fig:bc_training}. %The $x$-axes of the figures indicate the number of episodes and the $y$-axes represent the L-ARPE values. 
In each subfigure, the solid line denotes the mean values $\mu(L_{i})=\frac{1}{M} \sum_{j=1}^M L_{ij}$ of the L-ARPE lines over the $M$ random seeds, and the shadow region represents the standard deviation $\sigma(L_{i}) = \frac{1}{M} \sum_{j=1}^M \left(L_{ij}-\mu(L_{i})\right)^2$ of the L-ARPE lines. We smooth out the lines in all subfigures with $10$ episodes for clear presentation. 

From Fig.~\ref{fig:bc_training}, we can see that the L-ARPE lines of all the IBC-DMP RL agents ultimately converge to steady values before $500$ episodes. Among these IBC-DMP agents, the one with $\lambda_{\mathrm{BC}}\!=\!2$ has the highest ultimate average L-ARPE $\mu(L_{N})$ and the smallest ultimate standard deviation $\sigma(L_{N})$. It also shows a more clear reduction of the standard derivation $\sigma(L_{i})$ as $i$ increases, compared to other BC ratios. Nevertheless, the no-IBC agent failed to achieve convergence. Even though it achieves a high score $\mu(L_{i})$ and low deviation $\sigma(L_{i})$ in the early stage of the training, its performance becomes worse as the training proceeds. This comparison study indicates the advantages of the IBC-DMP agents with faster convergence and higher stability to randomization than the conventional no-IBC agents.
%mean value of the ultimate training L-ARPE (the solid line) and the smallest standard deviation (the shadow region around the solid line). Compared to these IBC-DMP RL agents, the no-IBC RL agent in Fig.~\ref{fig:bc_training_no_BC} does not have converging curves. Specifically, the average L-ARPE decreases (the solid line), and the standard deviation increases (the shadow region) during training. These observations validate the fast convergence of the IBC-DMP RL agents. Also, the BC ratio $\lambda_{\mathrm{BC}}\!=\!2$ leads to the best training performance.

% Figure environment removed



\subsubsection{Trained Scores}
Apart from the L-ARPE lines, we also quantitatively evaluate the trained performance of the agents by analyzing their training scores. Tab.~\ref{tab:qua_metric} shows the ultimate L-ARPE scores $L_{Nj}$ of all four agents for all random seeds $j=1,2,\cdots,M$. The averaged score $\mu(L_{N})$ and the standard deviation $\sigma(L_{N})$ are also displayed. 
%We also define the increasing rate of L-ARPE in training $\displaystyle \mathrm{IR}\!=\!\frac{\bar{L}_{N} \!-\! \bar{L}_1 }{\bar{L}_1}$, where $\bar{L}_1$ is the mean value of $L_{ij}$ at episode $i=1$. The IR metric depicts the extent of performance increase of the agent during the training process. The three quantitative metrics of the four agents are shown in Tab.~\ref{tab:qua_metric}. 
We can observe that all IBC-DMP agents have higher ultimate average L-ARPE scores and smaller standard deviations than the no-IBC agent. %They also have higher increasing rates which denote performance promotion of RL agents during the training process. 
Specifically, all IBC-DMP agents have training scores that are higher than or approximately equal to $-6$, while the score of the no-IBC agent is lower than $-6$. In this sense, we can select the L-ARPE value -6 as a standard to judge whether a trajectory is of good performance or not.
The results indicate the superior training performance of the IBC-DMP agents over the conventional no-IBC agent. Tab.~\ref{tab:qua_metric} also shows that $\lambda_{\mathrm{BC}}\!=\!2$ is the best BC ratio among all configurations. %To verify the significance of the advantage of IBC-DMP agents in training, we also perform $t$-tests between the $L_{Nj}$ scores of each IBC-DMP agent and the no-IBC agent with $\alpha=0.05$. The resulting $p$-values of the three IBC-DMP agents are shown in the last row of Tab.~\ref{tab:qua_metric}, where a smaller $p$-value indicates a more significant difference between the averaged scores $\mu(L_{N})$ of different agents. The results show that the IBC-DMP agent with $\lambda_{\mathrm{BC}}=2$ has a significant advantage over the no-IBC agent, while the advantages of other BC ratios may not be that significant. 
The overall results indicate that IBC-DMP can improve the training performance of an RL agent given a proper BC ratio.

% provides clear evidence that the agent with BC ratio $\lambda_{\mathrm{BC}}\!=\!2$ has the highest mean value and the smallest standard deviation of the ultimate L-ARPE, which validates the efficacy of IBC.


\linespread{1.2}
\begin{table}[htbp]
\centering
\caption{The Quantitative Scores of Agent Training}
\begin{tabular}{c|c|c|c|c}
\thickhline
\rowcolor{dark_red}
{\color{white}\textbf{Scores}} & {\color{white}\textbf{no-IBC}} & {\color{white}$\pmb{\lambda_{\mathrm{BC}}\!=\!1}$} & {\color{white}$\pmb{\lambda_{\mathrm{BC}}\!=\!1.5}$} & {\color{white}$\pmb{\lambda_{\mathrm{BC}}\!=\!2}$}\\
\hline
%\multirow{10}{*}{}
$L_{N1}$ & -7.9646 & -8.0932 & -4.8913 & -5.0798 \\
\rowcolor{shadow_red} $L_{N2}$ & -9.2790 & -5.1169 & -8.2311 & -4.8871 \\
$L_{N3}$& -7.2851 & -5.1896 & -7.1078 & -5.1206 \\
\rowcolor{shadow_red}$L_{N4}$ & -6.9950 & -4.9594 & -5.1420 & -5.1225 \\
$L_{N5}$ & -4.7116 & -5.0356 & -4.9359 & -4.8457 \\
\rowcolor{shadow_red}$L_{N6}$ & -5.0261 & -4.7340 & -4.7847 & -5.4356 \\
$L_{N7}$& -6.4798 & -8.1941 & -6.7794 & -4.8920 \\
\rowcolor{shadow_red}$L_{N8}$ & -8.3686 & -5.4488 & -5.0069 & -5.3338 \\
$L_{N9}$& -7.9009 & -5.0293 & -5.5441 & -4.7767 \\
\rowcolor{shadow_red}$L_{N10}$ & -4.8554 & -5.2022 & -7.5939 & -7.8335 \\
\hline
$\mu(L_{N})$ & \textbf{-6.8866} & \textbf{-5.7003} & \textbf{-6.0017} & \textbf{-5.3328} \\
\rowcolor{mid_red} $\sigma(L_{N})$ & \textbf{1.5088} & \textbf{1.2342} & \textbf{1.2297} & \textbf{0.8577} \\
%\hline
%\rowcolor{mid_red} $p$-value & \textemdash & 0.0851 & $0.1901$ & $0.0175$ \\
\thickhline
\end{tabular}
\label{tab:qua_metric}
\end{table}
\linespread{1}

\subsection{Agent Test}\label{sec:fix_goal}

In this subsection, we evaluate the performance of the four trained agents in a test study. The Multi-DoF DMP model is required to start from a fixed initial position $\x_{\mathrm{i}}\!=\!(0~0~0.05)\,$m to a random goal position $\x_{\mathrm{g}}$ while avoiding a cylinder obstacle in a random position $\x_{\mathrm{b}}$. For each trained policy $j=1,2,\cdots,M$ of each agent, we perform $R=400$ test runs with different goal positions $\x_{\mathrm{g}}$ and obstacle positions $\x_{\mathrm{b}}$. The goal positions $\x_{\mathrm{g}}$ are sampled from a polyhedral region $\tilde{\mathcal{P}}_{\mathrm{g}}\!=\!\{\tilde{P}_{\mathrm{g}}, \Delta \tilde{P}_{\mathrm{g}}\}$, where $\tilde{P}_{\mathrm{g}}\!=\!(0.32~0.34~0.09)\,$m and $\Delta \tilde{P}_{\mathrm{g}}\!=\!(\pm 0.3~\pm 0.25~\pm 0.05)\,$m. Note that the sampling space for test $\tilde{P}_{\mathrm{g}}$ is larger than the one for training $P_{\mathrm{g}}$ as introduced in Sec.~\ref{sec:sim_training}, since we intend to test the generalizability of the IBC-DMP RL agents to the goal positions that are not used for training. During the sampling of the goal positions, we also eliminate the ones that are too close to the initial positions to guarantee the feasibility of trajectory generation. For each sampled goal position $\x_{\mathrm{g}}$, the obstacle position $\x_{\mathrm{b}}$ is also randomly sampled from a polyhedral region $\tilde{\mathcal{P}}_{\mathrm{b}}=\{\tilde{P}_{\mathrm{b}}, \Delta \tilde{P}_{\mathrm{b}}\}$ of which the center $\displaystyle \tilde{P}_{\mathrm{b}} \!=\! \frac{\x_{\mathrm{i}} \!+\! \tilde{P}_{\mathrm{g}}}{2}$ is the mid-point between the initial position and the goal position, and the vertexes are $\displaystyle \frac{\x_{\mathrm{i}} + \tilde{P}_{\mathrm{g}}}{2} \!+\! \Delta \tilde{P}_{\mathrm{b}}$, where $\Delta \tilde{P}_{\mathrm{b}}\!=\!(\pm 0.05,\,\pm 0.05,\,\pm 0.02)\,$m.

\subsubsection{Test Trajectories}
Fig.~\ref{fig:bc_test} shows the test trajectories of the trained agents for given goal and obstacle positions $\x_{\mathrm{g}}\!=\!(0.32~0.34~0.09)\,$m and $\x_{\mathrm{b}}\!=\!(0.13~0.14~0.16)\,$m. Each subfigure shows the trajectories of the corresponding agent generated by $M$ trained policies for the fixed $\x_{\mathrm{g}}$ and $\x_{\mathrm{b}}$, where the red trajectories are those colliding with the obstacle or with the ground. Here, we refer to a trajectory as \textit{with collisions} if it intersects with the obstacle or with the ground, i.e., if it has at least $1$ discrete-time samples that are within the cylinder obstacle domain or under the ground. It is noticed that
% Thus, the goal position and the obstacle position in the test study are the centers of the sampling regions used during the agent training in Sec.~\ref{sec:training}. For each IBC-DMP RL agent and the no-IBC RL agent, we trained $10$ policies using different random seeds. The test trajectories of the four agents generated using different policies are illustrated in Fig.~\ref{fig:bc_test}, where all safe test trajectories without collision with the obstacle nor with the ground are marked as blue. Otherwise, the trajectories with a collision are marked as red. % It is clearly seen that for the no-IBC agent in Fig.~\ref{fig:bc_test_0}, 
$5$ out of $10$ test trajectories of the conventional no-IBC agent are with collisions. On the contrary, each IBC-DMP agent only has at most $1$ colliding trajectory. This indicates the superior test performance of IBC-DMP agents in terms of collision avoidance, compared to the no-IBC agent.


% Figure environment removed

\subsubsection{Test Scores}

We also use test scores to quantify the test performance of the agents.
For each agent, we use the L-ARPE score $L_{rj}$ defined in Sec.~\ref{sec:Lij} to describe the test performance of a trajectory generated by policy $j=1,2,\cdots,M$ and position sample $r=1,2,\cdots,R$. Then, the averaged score $L_{\bar{r}j}\!=\!\frac{1}{R} \sum_{r=1}^R L_{rj}$ can be used to evaluate the general test performance of policy $j$. The test scores $L_{\bar{r}j}$ for all policies $j$ and all trained agents are listed in Tab.~\ref{tab:test_score}, where the mean values and standard deviations of the test scores, defined as $\mu(L_{\bar{r}})\!=\!\frac{1}{M} \sum_{j=1}^M L_{\bar{r}j}$ and $\sigma({L}_{\bar{r}})\!=\!\frac{1}{M} \sum_{j=1}^M \left(L_{\bar{r}j}-\mu(L_{\bar{r}})\right)^2$ are also presented.


Tab.~\ref{tab:test_score} clearly shows that all IBC-DMP agents have much higher average score $\mu(L_{\bar{r}})$ and smaller standard deviation $\sigma(L_{\bar{r}})$ than the no-IBC agent.
Specifically, all IBC-DMP agents are higher than or approximately equal to the performance standard -6, while the no-IBC agent has a lower test score. This indicates the advantage of the IBC-DMP agents in test performance.
Since we have far larger sampling spaces for goal and obstacle positions in the test than that in training, the superior test performance of the IBC-DMP agents also reflects their better generalizability to various goal and obstacle positions than the no-IBC agents. Also, among the IBC-DMP agents, the one with the largest BC ratio ($\lambda_{\mathrm{BC}}$) performs the best with the highest average score $\mu(L_{r}) =$-5.886 and the smallest standard deviation $\sigma(L_{r}) =$0.241. This indicates that it does not only have the best overall test performance but also has the best stability to the changes of different environmental conditions. The overall results validate the efficacy of the IBC-DMP RL framework in improving the generalizability and stability of RL agents.




\linespread{1.2}
\begin{table}[htbp]
\centering
\caption{The Quantitative Scores of Agent Test}
\begin{tabular}{c|c|c|c|c}
	\thickhline
\rowcolor{dark_blue}	{\color{white} \textbf{Scores}} & {\color{white} \textbf{no-IBC}} & {\color{white} \textbf{$\lambda_{\mathrm{BC}}\!=\!1$}} & {\color{white} \textbf{$\lambda_{\mathrm{BC}}\!=\!1.5$}} & {\color{white} \textbf{$\lambda_{\mathrm{BC}}\!=\!2$}} \\
        \hline
$L_{\bar{r}1}$ & -6.1022 & -6.8036 & -6.2833 & -6.0177 \\
\rowcolor{shadow_blue} $L_{\bar{r}2}$ & -9.7703 & -5.9426 & -5.1831 & -5.6973 \\
$L_{\bar{r}3}$ & -6.3339 & -5.9161 & -6.0908 & -5.8809 \\
\rowcolor{shadow_blue} $L_{\bar{r}4}$ & -5.6509 & -5.9897 & -5.9751 & -6.0104 \\
$L_{\bar{r}5}$ & -5.7340 & -5.9492 & -6.0357 & -5.5467 \\
\rowcolor{shadow_blue} $L_{\bar{r}6}$ & -5.6169 & -5.6672 & -6.8701 & -6.1443 \\
$L_{\bar{r}7}$ & -7.3525 & -6.5007 & -5.7318 & -5.5285 \\
\rowcolor{shadow_blue} $L_{\bar{r}8}$ & -8.8828 & -6.2207 & -5.8197 & -6.2114 \\
$L_{\bar{r}9}$ & -6.4230 & -5.9020 & -6.2411 & -5.8731 \\
\rowcolor{shadow_blue} $L_{\bar{r}10}$ & -5.6641 & -5.7333 & -6.2062 & -6.2656 \\
\hline
$\mu(L_{\bar{r}})$ & \textbf{-6.7530} & \textbf{-6.0625} & \textbf{-6.0437} & \textbf{-5.9176} \\
\rowcolor{mid_blue} $\sigma(L_{\bar{r}})$ & \textbf{1.3942} & \textbf{0.3333} & \textbf{0.4124} & \textbf{0.2487}  \\
	\thickhline
\end{tabular}
\label{tab:test_score}
\end{table}
\linespread{1}

\subsubsection{Collision Rates}

One of the main concerns of robot motion planning is the avoidance of collision with obstacles in the environment. To evaluate the performance of the agents with respect to collision avoidance in the test study, we calculate the collision rates $R_{\mathrm{c}j} \!=\! N_{\mathrm{cls}}^j/N_{\mathrm{ttl}}^j$ for all policies $j=1$, $2$, $\cdots$, $10$, where $N_{\mathrm{tll}}^j\!=\!400$ is the total number of test trajectories of policy $j$ and $N_{\mathrm{cls}}^j$ is the number of trajectories with collisions. Tab.~\ref{tab:collision_rates} lists the collision rates of all $10$ policies of the four agents. The overall average value and the standard deviation of the collision rates of each agent are also presented. It is clearly shown that the IBC-DMP agents have far smaller collision rates than the no-IBC agent. They also have smaller standard deviation values. 
The agent with $\lambda_{\mathrm{BC}}\!=\!2$ can be recognized as the safest and the most reliable one among the four agents with the lowest average collision rates and the smallest standard deviation. The overall results indicate that IBC-DMP agents have superior collision avoidance performance over the conventional no-IBC agent.


\linespread{1.2}
\begin{table}[htbp]
\centering
\caption{The Collision Rates of Agent Test}
\begin{tabular}{c|c|c|c|c}
	\thickhline
\rowcolor{dark_yellow}	{\color{white} \textbf{Scores}} & {\color{white} \textbf{no-IBC}} & {\color{white} \textbf{$\lambda_{\mathrm{BC}}\!=\!1$}} & {\color{white} \textbf{$\lambda_{\mathrm{BC}}\!=\!1.5$}} & {\color{white} \textbf{$\lambda_{\mathrm{BC}}\!=\!2$}} \\
        \hline
$R_{\mathrm{c}1}$ & 0.50\% & 34.75\% & 17.00\% & 0.50\% \\
\rowcolor{shadow_yellow} $R_{\mathrm{c}2}$ & 100\% & 0 & 1.00\% & 0 \\
$R_{\mathrm{c}3}$ & 47.75\% & 0 & 0.25\% & 0 \\
\rowcolor{shadow_yellow} $R_{\mathrm{c}4}$ & 0.25\% & 0 & 0.50\%\ & 0 \\
$R_{\mathrm{c}5}$ & 0 & 0.25\% & 0.25\% & 0.25\% \\
\rowcolor{shadow_yellow} $R_{\mathrm{c}6}$ & 0 & 0 & 56.00\% & 0 \\
$R_{\mathrm{c}7}$ & 57.50\% & 25.00\% & 16.75\% & 0 \\
\rowcolor{shadow_yellow} $R_{\mathrm{c}8}$ & 100\% & 0 & 0.25\% & 0 \\
$R_{\mathrm{c}9}$ & 18.50\% & 1.00\% & 0 & 0 \\
\rowcolor{shadow_yellow} $R_{\mathrm{c}10}$ & 0.50\% & 0 & 0 & 0.50\% \\
\hline
$\mu(R_{\mathrm{c}})$ & \textbf{32.50\%} & \textbf{6.10\%} & \textbf{9.20\%} & \textbf{0.13\%} \\
\rowcolor{mid_yellow} $\sigma(R_{\mathrm{c}})$ & \textbf{0.3920} & \textbf{0.1209} & \textbf{0.1691} & \textbf{0.0020}  \\
	\thickhline
\end{tabular}
\label{tab:collision_rates}
\end{table}
\linespread{1}

%\subsection{Agent Test: Various Goal and Obstacle Positions}

%The second test study in this subsection is to test the performance of the agents using various goal and obstacle positions. For this test study, we use the same initial position $P_0\!=\!(0,\,0,\,0.05)\,$m as Sec.~\ref{sec:fix_goal}. Differently, for each policy of each agent ($10$ in total), we sample $100$ goal positions $\tilde{P}_{\mathrm{g}}$ from a polyhedral $\tilde{P}_{\mathrm{g}}\!\in\!P_{\mathrm{g}}\!+\!\Delta P_{\mathrm{g}}$, where $P_{\mathrm{g}}\!=\!(0.32,\,0.34,\,0.09)\,$m and $\Delta P_{\mathrm{g}}\!=\!(\pm 0.3,\,\pm 0.25,\,\pm 0.05)\,$m. Note that this sampling region for goal positions is far larger than the one in the agent training in Sec.~\ref{sec:training}. This is for the purpose of testing the generalizability of the IBC-DMP RL agents to the goal positions that are not used for training. During the sampling of the goal positions, we also eliminate the ones that are too close to the initial positions to guarantee the feasibility of trajectory generation. For each goal position $\tilde{P}_{\mathrm{g}}$, the obstacle $P_{\mathrm{o}}$ is randomly sampled from a polyhedral region $\mathcal{P}_{\mathrm{o}}$ with the mid-point between the initial position and the goal position $\displaystyle \frac{P_0 + \tilde{P}_{\mathrm{g}}}{2}$ being its center, and $\displaystyle \frac{P_0 + \tilde{P}_{\mathrm{g}}}{2} \!+\! \Delta P_{\mathrm{g}}$ being its vertexes, where $\Delta P_{\mathrm{g}}\!=\!(\pm 0.05,\,\pm 0.05,\,\pm 0.02)\,$m. For each generated trajectory, we calculate the score $L_{\mathrm{test}}$ as defined in Sec.~\ref{sec:fix_goal}. The mean value and standard deviation of the test scores on all $10\!\times\!100$ generated trajectories, namely $\bar{L}_{\mathrm{test}}$ and $\sigma(L_{\mathrm{test}})$, of all four agents, are presented in Tab.~\ref{tab:test_score_various}.

% \linespread{1.2}
% \begin{table}[htbp]
% \caption{The Test Scores of the Second Test Study}
% \label{tab:test_score_various}
% \begin{center}
% \begin{tabular}{c|c|c|c|c}
% 	\hline
% 	Test Scores & no-IBC & $\lambda_{\mathrm{BC}}\!=\!1$ & $\lambda_{\mathrm{BC}}\!=\!1.5$ & $\lambda_{\mathrm{BC}}\!=\!2$ \\
%         \hline
%         $\bar{L}_{\mathrm{test}}$ & $-6.473$ & $-5.533$ & $-5.477$ & $-5.361$ \\
%         $\sigma(L_{\mathrm{test}})$ & $1.857$ & $0.8232$ & $0.8216$ & $0.6189$ \\
% 	\hline
% \end{tabular}
% \end{center}
% \end{table}
% \linespread{1}

% Similar to the fixed-goal test in Sec.~\ref{sec:fix_goal}, the IBC-DMP RL agents also show better test performance over the no-IBC agent, with higher mean scores and smaller standard deviations, in the test study of this subsection. The results show that the IBC-DMP RL agents have superior test performance even on the goal positions that are not included in the training stage, which indicates their decent generalizability. Among the IBC-DMP RL agents, the one with a larger BC ratio performs better. The one with $\lambda_{\mathrm{BC}}\!=\!2$ performs the best, which also validates the efficacy of IBC in agent learning.


\section{Experimental Validation}\label{sec:exp}

In this section, we use an experimental case study to evaluate the performance of the proposed IBC-DMP RL method. A Rubik's cube-stacking case is used to demonstrate how to use an IBC-DMP agent to accomplish a general pick-and-place-based assembly task. The robot used in this study is a 6-DoF Kinova\textregistered~Gen$\,$3 manipulator with a two-finger Robotiq$^\circledR$ 2F-85 gripper and an Omnivision OV5640 on-gripper camera as shown in Fig.~\ref{fig:kinova_robot}. The robot is connected to a desktop workstation which is equipped with an AMD\textregistered~Ryzen9 3950X CPU and an Intel\textregistered~RTX3090 GPU. The operating system of the desktop is Ubuntu 18.04. The robot is controlled by the Kinova$^\circledR$ Kortex API written in Python~\cite{kinova_kortex}. Another RGB camera is deployed in the front of the robot to provide vision from the third-person point of view, as shown in Fig.~\ref{fig:robot_2}.

% Figure environment removed

\subsection{Experiment Configuration}

As shown in Fig.~\ref{fig:robot_2}, the workspace of the cube-stacking scenario is marked as a rectangular region on the table in front of the Gen3 robot arm. We set the origin of the task coordinate at the robot base. The axes of the coordinate are shown as colored arrows in Fig.~\ref{fig:robot_2}. The coordinates of the vertexes of the workspace are (0.25,\,0.45,\,0)\,m, (0.25,\,-0.2,\,0)\,m, (0.6,\,0.45,\,0)\,m, and (0.6,\,-0.2,\,0)\,m. 
%(0.546, 0.472, 0), (0.235, 0.439, 0), (0.280, -0.205, 0), (0.600, -0.193, 0) (m) in the task space of the robot with respect to the base frame.
The size of the workspace is determined to cover the largest view of the on-gripper camera within the reach of the robot gripper. Three Rubik's cubes with different sizes (two cubes with a 57 mm edge and one cube with a 45 mm edge) and colors are randomly placed within the rectangular region manually. A paper cup with a radius of 90 mm and a height of 110 mm is also manually placed at a random position as an obstacle. The robot is commanded to start from the HOME position at (0.356, 0.106, 0.277)\,m, as shown in Fig.~\ref{fig:robot_2}, pick up the Rubik's cubes, and place them at the GOAL position (marked as a green light point) one by one from the largest to the smallest (red, orange, and then blue), until they are piled up in a column. The piling-up process can be recognized as a simple assembly task, during which the robot gripper should not collide with the paper cup. The vision of the Rubik's cubes and the paper cup obstacle is captured using the on-gripper camera and their positions are calculated using the pre-built object detection libraries in YOLOv8~\cite{Jocher_YOLO_by_Ultralytics_2023} and OpenCV~\cite{opencv_library}. %In this sense, we can assume that the robot has perfect knowledge about the positions of the obstacle $\mathbf{b}$ and the target $P_{\mathcal{G}}$ for each indivisual movement to pick or place an object.

For each experimental trial, we represent the initial positions of the three cubes with red, orange, and blue top surfaces as $P_{\mathrm{red}}$, $P_{\mathrm{org}}$, and $P_{\mathrm{blu}}$, respectively. We also set three via-points $\tilde{P}_{\mathrm{red}}$, $\tilde{P}_{\mathrm{org}}$, and $\tilde{P}_{\mathrm{blu}}$ 5\,cm right above them to ease the grasping of the cubes. In this sense, six trajectories need to be generated for the cube-stacking task, including HOME\,--\,$\tilde{P}_{\mathrm{red}}$\,--\,$G_{\mathrm{red}}$\,--\,$\tilde{P}_{\mathrm{org}}$\,--\,$G_{\mathrm{org}}$\,--\,$\tilde{P}_{\mathrm{blu}}$\,--\,$G_{\mathrm{blu}}$, where $G_{\mathrm{red}}$, $G_{\mathrm{org}}$, and $G_{\mathrm{blu}}$ are the stacking positions of the cubes in the ultimate stacking column. The procedure of the stacking task is described in Algorithm~\ref{ag:task}, where \textit{DMP} is a function used to generate the desired trajectories using the multi-DoF DMP model \eqref{eq:dmp_multi_dof} with given initial, goal, and obstacle positions $\x_{\mathrm{i}}$, $\x_{\mathrm{g}}$, and $\x_{\mathrm{b}}$, respectively, and the trained policy $\mathbf{f}$. In this experiment, we select policy \#5
% The process involves a total of six trajectories, consisting of alternating pick and place actions. It includes three trajectories for picking the Rubik's cube, and three trajectories for placing it in the goal position. 
We are not involving additional task-level complexities in this task since the main goal of this paper is to evaluate how an IBC-DMP agent efficiently produces safe trajectories. Also, for brevity, the multi-DoF DMP model is only used to generate the translational positions of the gripper. The orientations of the gripper are commanded in a very intuitive and simple manner with the following principles.
\begin{itemize}
\item The gripper always points down to the table.
\item The orientations about $x$- and $y$-axis are fixed for all generated trajectories.
\item The orientations about $z$-axis are linearly changed during the motion, in order to grasp the cube or place it with the proper orientation in the line \ref{code:gen_trj} of Algorithm~\ref{ag:task}.
\end{itemize}
Such a design ensures the successful execution of the cube-stacking task without causing singular configurations.
%Our objective is to generate a trajectory for the end-effector of the robot, such that it moves from an initial position to the target position without colliding with the obstacle. 
All trajectories are designed in the Cartesian space and are mapped to the joint space of the robot using the pre-built inverse kinematics (IK) library of the Kinova$^\circledR$ Gen$\,$3 robot.


 

\begin{algorithm}[htbp]
\caption{Cube stacking task procedure}\label{ag:task}
\begin{algorithmic}[1]
\STATE Initialize robot gripper at HOME position
\STATE Assign $\x_{\mathrm{b}}$ with the paper cup position
\FOR{$j$ \textbf{in} $\{\mathrm{red}, \mathrm{org}, \mathrm{blu}\}$}
    \STATE $\x_{\mathrm{i}} \leftarrow \x_t$, $\x_{\mathrm{g}} \leftarrow \tilde{P}_{j}$
    \STATE Generate trajectory $(\x_t, \dot{\x}_t)$\,=\,\textit{DMP}($\x_{\mathrm{i}}$, $\x_{\mathrm{g}}$, $\x_{\mathrm{b}}$, $\mathbf{f}$) \label{code:gen_trj}
    \label{code:start}
    \WHILE{$\|\x_t\!-\!\x_{\mathrm{g}}\|\!>\!\varepsilon_T$}
    \STATE Follow trajectory $(\x_t, \dot{\x}_t)$
    \ENDWHILE \label{code:end}
    \STATE Grasp cube at $P_j$ \label{code:grasp}
    \STATE Determine stacking position $\x_{\mathrm{i}} \leftarrow \x_t$, $\x_{\mathrm{g}} \leftarrow G_j$
    \STATE Repeat line \ref{code:start} to line \ref{code:end}
    \STATE Release cube
\ENDFOR

\end{algorithmic}
\end{algorithm}



\subsection{Experiment Results}\label{sec:experi}



% Figure environment removed




To evaluate the overall efficacy of the IBC-DMP RL framework, we select policy \#\,6 of $\lambda_{\mathrm{BC}}\!=\!1.5$, which performs the worst in the test study as shown in Tab.~\ref{tab:test_score}, for the experimental validation. If the selected policy shows a decent performance, we can claim the general validity of the IBC-DMP framework in practical applications. We perform 22 experimental trials to incorporate the influence of randomness. For each trial, the initial positions of the cubes and the cup are randomly placed by hand. Also, we intentionally create challenging situations where the robot gripper has a large chance to go around the paper cup. One example trial is shown in Fig.~\ref{fig:exp_results}, where the paper cup is in the mid-way between the red and the orange cubes, and the goal position. In this situation, the gripper must go around the cup to avoid collision with it, leading to nontrivial trajectories. Fig.~\ref{fig:exp_results} shows that the IBC-DMP policy can successfully generate collision-free trajectories for the robot gripper, which indicates the efficacy of the IBC-DMP motion planning framework.

Similar to Sec.~\ref{sec:fix_goal}, we also use the L-ARPE scores to quantitatively evaluate the performance of the IBC-DMP policy in the experimental study. We use the internal position sensor of the Gen 3 robot to record the actual executed paths of the gripper during the cube-stacking task. Fig.~\ref{fig:box_plot} is the box plot of the L-ARPE scores of these paths which are grouped according to trials. Each box denotes the distribution of the L-ARPE scores of the 6 trajectories of an experimental trial. The red bar in each box represents the median value of the L-ARPE scores which quantifies the overall performance of the policy in the corresponding trial. It is noticed that all red bars are higher than -7, which shows its advantage compared to the test scores of the no-IBC agent in Tab.~\ref{tab:test_score}. Also, the scores of 16 out of 22 trials are higher than the performance standard -6, which indicates the decent overall performance of the selected policy with respect to L-ARPE scores. 

% Figure environment removed

The distribution of the ultimate reaching errors is displayed in the box plot in Fig.~\ref{fig:box_plot_err}. It is noticed that all reaching errors are strictly restricted under 0.01\,m due to the selection of the error threshold $\varepsilon_T$. Meanwhile, the overall collision rate is $13.64\%$ which is higher than the testing collision rates of the IBC-DMP agents due to the uncertainties in the real experiment. However, it is far lower than the testing collision rate of the conventional no-IBC agent. Therefore, the experimental results indicate that the selected policy achieves the ideal ultimate reaching errors as prescribed by the error threshold and has advantages over the conventional no-IBC agent in terms of collision avoidance. 
Since the selected policy is of inferior test performance among other IBC-DMP policies, its decent performance implies the efficacy of the proposed IBC-DMP RL framework in resolving practical robot tasks.


% Figure environment removed

\section{Discussion}

In this paper, the two important technologies used to facilitate off-policy RL for robot motion planning are multi-DoF DMP and IBC. They both can be recognized as the proper encoding of expert knowledge. The multi-DoF DMP is adapted from conventional motion primitives of which the effectiveness has been verified by many experts and peers. From a mathematical point of view, DMP models can be seen as a class of heuristic models dedicated to reducing the dimensionality of a planning problem. The dynamic structure of a DMP model reflects how expert knowledge is used to restrict the smoothness and inherent stability of generated trajectories. IBC, as an adapted version of BC for imitation learning, provides a different manner to encode expert knowledge by cloning the expert's behaviors in the task. Note that these two different sources of expert knowledge might not always be beneficial to the given robot task. They may even conflict with each other in certain circumstances. A very important technical point of this paper is to flexibly combine the two types of expert knowledge using an off-policy RL framework, such that the resulting motion planning policy is not overfitting to any source of expert knowledge. Furthermore, the advantages of both types of knowledge are fully exploited to improve the training speed, generalizability, and reliability of the RL agent. The decent results of the simulation and experimental studies provide evidence that the performance of RL can be improved by properly encoding expert knowledge.

Although the general efficacy of the IBC-DMP framework is validated, it still has limitations. The multi-DoF DMP model has a higher flexibility than the conventional DMP since its actuation function is dependent on the state of the model. However, this may sacrifice the stability of the DMP model, possibly leading to odd-shaped trajectories. Besides, an IBC-DMP may be over-trained if the number of training episodes is too large, where the training score of the policy decreases or becomes unsteady as the training proceeds longer. This may also be due to the sacrifice of the inherent stability of the Multi-DoF DMP. Another limitation of IBC-DMP is that its performance tends to be sensitive to the exploring noise added to actions during the training stage. Further investigations on the correlation between the performance of IBC-DMP and the action noise are needed in future work.


\section{Conclusion}\label{sec:con}

In this paper, we propose a novel framework for developing RL agents for robot motion-planning tasks based on two promoted methods, namely multi-DoF DMP and IBC. An off-policy RL agent serves as a bridge to flexibly combine the expert knowledge encoded by the two methods, resulting in an advantageous agent with improved training speed, generalizability, and reliability. Its efficacy and advantage over the conventional no-IBC agent are validated by simulation and experimental studies. In future work, we will focus on improving the stability of the Multi-DoF DMP model and the sensitivity of IBC-DMP agents to exploring action noise.


%\section*{Acknowledgments}
%This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



%{\appendix[Proof of the Zonklar Equations]
%Use $\backslash${\tt{appendix}} if you have a single appendix:
%Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
%If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
%You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
% starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}

%In this paper, we show the general overview of the method proposed in this paper.


%\section{References Section}
%You can use a bibliography generated by BibTeX as a .bbl file.
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, reference.bib}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{% Figure removed}]{Zengjie Zhang}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




% \vfill

\end{document}


