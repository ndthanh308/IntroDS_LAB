\begin{thebibliography}{10}

\bibitem{mti}
Simon Vandenhende, Stamatios Georgoulis, and Luc Van~Gool.
\newblock Mti-net: Multi-scale task interaction networks for multi-task
  learning.
\newblock In {\em ECCV}, 2020.

\bibitem{atrc}
David Bruggemann, Menelaos Kanakis, Anton Obukhov, Stamatios Georgoulis, and
  Luc Van~Gool.
\newblock Exploring relational context for multi-task dense prediction.
\newblock In {\em ICCV}, 2021.

\bibitem{zamir2020robust}
Amir~R Zamir, Alexander Sax, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra
  Malik, and Leonidas~J Guibas.
\newblock Robust learning through cross-task consistency.
\newblock In {\em CVPR}, 2020.

\bibitem{kokkinos2017ubernet}
Iasonas Kokkinos.
\newblock Ubernet: Training a universal convolutional neural network for low-,
  mid-, and high-level vision using diverse datasets and limited memory.
\newblock In {\em CVPR}, 2017.

\bibitem{astmt}
Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos.
\newblock Attentive single-tasking of multiple tasks.
\newblock In {\em CVPR}, 2019.

\bibitem{kendall2018multi}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In {\em CVPR}, 2018.

\bibitem{invpt2022}
Hanrong Ye and Dan Xu.
\newblock Inverted pyramid multi-task transformer for dense scene
  understanding.
\newblock In {\em ECCV}, 2022.

\bibitem{padnet}
Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.
\newblock Pad-net: Multi-tasks guided prediction-and-distillation network for
  simultaneous depth estimation and scene parsing.
\newblock In {\em CVPR}, 2018.

\bibitem{lin2017feature}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em CVPR}, 2017.

\bibitem{zhang2021survey}
Yu~Zhang and Qiang Yang.
\newblock A survey on multi-task learning.
\newblock {\em TKDE}, 2021.

\bibitem{mtlsurvey}
Simon Vandenhende, Stamatios Georgoulis, Wouter Van~Gansbeke, Marc Proesmans,
  Dengxin Dai, and Luc Van~Gool.
\newblock Multi-task learning for dense prediction tasks: A survey.
\newblock {\em TPAMI}, 2021.

\bibitem{zamir2018taskonomy}
Amir~R Zamir, Alexander Sax, William Shen, Leonidas~J Guibas, Jitendra Malik,
  and Silvio Savarese.
\newblock Taskonomy: Disentangling task transfer learning.
\newblock In {\em CVPR}, 2018.

\bibitem{kanakis2023composite}
Menelaos Kanakis, Thomas~E Huang, David Br{\"u}ggemann, Fisher Yu, and Luc
  Van~Gool.
\newblock Composite learning for robust and effective dense predictions.
\newblock In {\em WACV}, 2023.

\bibitem{bachmann2022multimae}
Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir.
\newblock Multimae: Multi-modal multi-task masked autoencoders.
\newblock In {\em ECCV}, 2022.

\bibitem{hoyer2021three}
Lukas Hoyer, Dengxin Dai, Yuhua Chen, Adrian Koring, Suman Saha, and Luc
  Van~Gool.
\newblock Three ways to improve semantic segmentation with self-supervised
  depth estimation.
\newblock In {\em CVPR}, 2021.

\bibitem{liu2019MTAN}
Shikun Liu, Edward Johns, and Andrew~J Davison.
\newblock End-to-end multi-task learning with attention.
\newblock In {\em CVPR}, 2019.

\bibitem{liu2021conflict}
Bo~Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock In {\em NeurIPS}, 2021.

\bibitem{contrastiveMTL2023}
Siwei Yang, Hanrong Ye, and Dan Xu.
\newblock Contrastive multi-task dense prediction.
\newblock In {\em AAAI}, 2023.

\bibitem{li2022Learning}
Wei-Hong Li, Xialei Liu, and Hakan Bilen.
\newblock Learning multiple dense prediction tasks from partially annotated
  data.
\newblock In {\em CVPR}, 2022.

\bibitem{gradientsign}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock In {\em NeurIPS}, 2020.

\bibitem{gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em ICML}, 2018.

\bibitem{wang2020gradient}
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.
\newblock Gradient vaccine: Investigating and improving multi-task optimization
  in massively multilingual models.
\newblock In {\em ICLR}, 2020.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{crossstitch}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In {\em CVPR}, 2016.

\bibitem{nddr}
Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan~L Yuille.
\newblock Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural
  discriminative dimensionality reduction.
\newblock In {\em CVPR}, 2019.

\bibitem{zhang2021automtl}
Lijun Zhang, Xiao Liu, and Hui Guan.
\newblock Automtl: A programming framework for automating efficient multi-task
  learning.
\newblock In {\em NeurIPS}, 2021.

\bibitem{gao2020mtl}
Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu.
\newblock Mtl-nas: Task-agnostic neural architecture search towards
  general-purpose multi-task learning.
\newblock In {\em CVPR}, 2020.

\bibitem{papnet}
Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang.
\newblock Pattern-affinitive propagation across depth, surface normal and
  semantic segmentation.
\newblock In {\em CVPR}, 2019.

\bibitem{zhang2021transfer}
Xiaoya Zhang, Ling Zhou, Yong Li, Zhen Cui, Jin Xie, and Jian Yang.
\newblock Transfer vision patterns for multi-task pixel learning.
\newblock In {\em ACMMM}, 2021.

\bibitem{psd}
Ling Zhou, Zhen Cui, Chunyan Xu, Zhenyu Zhang, Chaoqun Wang, Tong Zhang, and
  Jian Yang.
\newblock Pattern-structure diffusion for multi-task learning.
\newblock In {\em CVPR}, 2020.

\bibitem{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{vandenhende2019branched}
Simon Vandenhende, Stamatios Georgoulis, Bert De~Brabandere, and Luc Van~Gool.
\newblock Branched multi-task networks: deciding what layers to share.
\newblock In {\em ECCV}, 2020.

\bibitem{lu2017fully}
Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu~Cheng, Tara Javidi, and Rogerio
  Feris.
\newblock Fully-adaptive feature sharing in multi-task networks with
  applications in person attribute classification.
\newblock In {\em CVPR}, 2017.

\bibitem{guo2020learning}
Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht.
\newblock Learning to branch for multi-task learning.
\newblock In {\em ICML}, 2020.

\bibitem{taskprompter2023}
Hanrong Ye and Dan Xu.
\newblock Taskprompter: Spatial-channel multi-task prompting for dense scene
  understanding.
\newblock In {\em ICLR}, 2023.

\bibitem{jordan1994hierarchical}
Michael~I Jordan and Robert~A Jacobs.
\newblock Hierarchical mixtures of experts and the em algorithm.
\newblock {\em Neural Computation}, 6(2):181--214, 1994.

\bibitem{jacobs1993learning}
Robert~A Jacobs and Michael~I Jordan.
\newblock Learning piecewise control strategies in a modular neural network
  architecture.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics},
  23(2):337--345, 1993.

\bibitem{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock {\em Neural Computation}, 3(1):79--87, 1991.

\bibitem{eigen2013learning}
David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever.
\newblock Learning factored representations in a deep mixture of experts.
\newblock {\em arXiv preprint arXiv:1312.4314}, 2013.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em JMLR}, 23:1--39, 2022.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock In {\em ICLR}, 2021.

\bibitem{ma2018multigate}
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed~H Chi.
\newblock Modeling task relationships in multi-task learning with multi-gate
  mixture-of-experts.
\newblock In {\em KDD}, 2018.

\bibitem{m3vit}
Hanxue Liang, Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou,
  Yu~Cheng, Cong Hao, and Zhangyang Wang.
\newblock M3vit: Mixture-of-experts vision transformer for efficient multi-task
  learning with model-accelerator co-design.
\newblock In {\em NeurIPS}, 2022.

\bibitem{chen2022mod}
Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik
  Learned-Miller, and Chuang Gan.
\newblock Mod-squad: Designing mixture of experts as modular multi-task
  learners.
\newblock {\em arXiv preprint arXiv:2212.08066}, 2022.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{chen2014detect}
Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and
  Alan Yuille.
\newblock Detect what you can: Detecting and representing objects using
  holistic models and body parts.
\newblock In {\em CVPR}, 2014.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em IJCV}, 2010.

\bibitem{silberman2012indoor}
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In {\em ECCV}, 2012.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\end{thebibliography}
