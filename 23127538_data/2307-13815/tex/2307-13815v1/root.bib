@article{xai360,
  author  = {Vijay Arya and Rachel K. E. Bellamy and Pin-Yu Chen and Amit Dhurandhar and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Q. Vera Liao and Ronny Luss and Aleksandra MojsiloviÄ‡ and Sami Mourad and Pablo Pedemonte and Ramya Raghavendra and John T. Richards and Prasanna Sattigeri and Karthikeyan Shanmugam and Moninder Singh and Kush R. Varshney and Dennis Wei and Yunfeng Zhang},
  title   = {{AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models}},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {130},
  pages   = {1--6},
  url     = {http://jmlr.org/papers/v21/19-1035.html}
}

@INPROCEEDINGS{protodash,
  author={Gurumoorthy, Karthik S. and Dhurandhar, Amit and Cecchi, Guillermo and Aggarwal, Charu},
  booktitle={{2019 IEEE International Conference on Data Mining (ICDM)}}, 
  title={Efficient Data Representation by Selecting Prototypes with Importance Weights}, 
  year={2019},
  volume={},
  number={},
  pages={260-269},
  doi={10.1109/ICDM.2019.00036}}

  @misc{dipvae,
      title={Variational Inference of Disentangled Latent Concepts from Unlabeled Observations}, 
      author={Abhishek Kumar and Prasanna Sattigeri and Avinash Balakrishnan},
      year={2018},
      eprint={1711.00848},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cem,
 author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives}},
 volume = {31},
 year = {2018}
}

@article{gce,
author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin},
title = {{Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation}},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {1},
pages = {44-65},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2014.907095},
URL = {  
        https://doi.org/10.1080/10618600.2014.907095
},
eprint = {   
        https://doi.org/10.1080/10618600.2014.907095
}
}

@inproceedings{lime,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {{``Why Should I Trust You?'': Explaining the Predictions of Any Classifier}},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, interpretability, interpretable machine learning, explaining machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{shap,
 author = {{Lundberg, Scott M and Lee, Su-In}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{A Unified Approach to Interpreting Model Predictions}},
 volume = {30},
 year = {2017}
}


@InProceedings{tssmig,
  title = 	 {{Axiomatic Attribution for Deep Networks}},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {{Proceedings of the 34th International Conference on Machine Learning}},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
  abstract = 	 {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}

@inproceedings{ted,
author = {Hind, Michael and Wei, Dennis and Campbell, Murray and Codella, Noel C. F. and Dhurandhar, Amit and Mojsilovi\'{c}, Aleksandra and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R.},
title = {{TED: Teaching AI to Explain Its Decisions}},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314273},
doi = {10.1145/3306618.3314273},
abstract = {Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {123–129},
numpages = {7},
keywords = {supervised classification, meaningful explanation, machine learning, explainable AI, elicitation, AI ethics},
location = {Honolulu, HI, USA},
series = {AIES '19}
}


@InProceedings{ocot,
  title = 	 {{Order Constraints in Optimal Transport}},
  author =       {Lim, Yu Chin Fabian and Wynter, Laura and Lim, Shiau Hong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13313--13333},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lim22b/lim22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lim22b.html},
  abstract = 	 {Optimal transport is a framework for comparing measures whereby a cost is incurred for transporting one measure to another. Recent works have aimed to improve optimal transport plans through the introduction of various forms of structure. We introduce novel order constraints into the optimal transport formulation to allow for the incorporation of structure. We define an efficient method for obtaining explainable solutions to the new formulation that scales far better than standard approaches. The theoretical properties of the method are provided. We demonstrate experimentally that order constraints improve explainability using the e-SNLI (Stanford Natural Language Inference) dataset that includes human-annotated rationales as well as on several image color transfer examples.}
}

@misc{imd,
      title={{Interpretable Differencing of Machine Learning Models}}, 
      author={Swagatam Haldar and Diptikalyan Saha and Dennis Wei and Rahul Nair and Elizabeth M. Daly},
      year={2023},
      eprint={2306.06473},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cfn,
 author = {Puri, Isha and Dhurandhar, Amit and Pedapati, Tejaswini and Shanmugam, Karthikeyan and Wei, Dennis and Varshney, Kush R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21668--21680},
 publisher = {Curran Associates, Inc.},
 title = {{CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions}},
 volume = {34},
 year = {2021}
}

@inproceedings{bdr,
 author = {Dash, Sanjeeb and Gunluk, Oktay and Wei, Dennis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Boolean Decision Rules via Column Generation}},
 volume = {31},
 year = {2018}
}


@InProceedings{glr,
  title = 	 {{Generalized Linear Rule Models}},
  author =       {Wei, Dennis and Dash, Sanjeeb and Gao, Tian and Gunluk, Oktay},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6687--6696},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wei19a/wei19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/wei19a.html},
  abstract = 	 {This paper considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and probabilistic classification. Rules facilitate model interpretation while also capturing nonlinear dependences and interactions. Our problem formulation accordingly trades off rule set complexity and prediction accuracy. Column generation is used to optimize over an exponentially large space of rules without pre-generating a large subset of candidates or greedily boosting rules one by one. The column generation subproblem is solved using either integer programming or a heuristic optimizing the same objective. In experiments involving logistic and linear regression, the proposed methods obtain better accuracy-complexity trade-offs than existing rule ensemble algorithms. At one end of the trade-off, the methods are competitive with less interpretable benchmark models.}
}

@incollection{feri,
title = {{Fast Effective Rule Induction}},
editor = {Armand Prieditis and Stuart Russell},
booktitle = {Machine Learning Proceedings 1995},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {115-123},
year = {1995},
isbn = {978-1-55860-377-6},
doi = {https://doi.org/10.1016/B978-1-55860-377-6.50023-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500232},
author = {William W. Cohen},
abstract = {Many existing rule learning systems are computationally expensive on large noisy datasets. In this paper we evaluate the recently-proposed rule learning algorithm IREP on a large and diverse collection of benchmark problems. We show that while IREP is extremely efficient, it frequently gives error rates higher than those of C4.5 and C4.5rules. We then propose a number of modifications resulting in an algorithm RIPPERk that is very competitive with C4.5rules with respect to error rates, but much more efficient on large samples. RIPPERk obtains error rates lower than or equivalent to C4.5rules on 22 of 37 benchmark problems, scales nearly linearly with the number of training examples, and can efficiently process noisy datasets containing hundreds of thousands of examples.}
}

@inproceedings{cp,
 author = {Dhurandhar, Amit and Shanmugam, Karthikeyan and Luss, Ronny and Olsen, Peder A},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Improving Simple Models with Confidence Profiles}},
 volume = {31},
 year = {2018}
}

@ARTICLE{XAI3602019,
       author = {{Arya}, Vijay and {Bellamy}, Rachel K.~E. and {Chen}, Pin-Yu and {Dhurandhar}, Amit and {Hind}, Michael and {Hoffman}, Samuel C. and {Houde}, Stephanie and {Liao}, Q. Vera and {Luss}, Ronny and {Mojsilovi{\'c}}, Aleksandra and {Mourad}, Sami and {Pedemonte}, Pablo and {Raghavendra}, Ramya and {Richards}, John and {Sattigeri}, Prasanna and {Shanmugam}, Karthikeyan and {Singh}, Moninder and {Varshney}, Kush R. and {Wei}, Dennis and {Zhang}, Yunfeng},
        title = "{One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
         year = 2019,
        month = sep,
          eid = {arXiv:1909.03012},
        pages = {arXiv:1909.03012},
          doi = {10.48550/arXiv.1909.03012},
archivePrefix = {arXiv},
       eprint = {1909.03012},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190903012A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{xai_survey,
title = {{Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence}},
journal = {Information Fusion},
pages = {101805},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101805},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
author = {Sajid Ali and Tamer Abuhmed and Shaker El-Sappagh and Khan Muhammad and Jose M. Alonso-Moral and Roberto Confalonieri and Riccardo Guidotti and Javier {Del Ser} and Natalia Díaz-Rodríguez and Francisco Herrera},
keywords = {Explainable Artificial Intelligence, Interpretable machine learning, Trustworthy AI, AI principles, Post-hoc explainability, XAI assessment, Data Fusion, Deep Learning}
}

@article{de1,
author = {Tilouche, Shaima and Partovi Nia, Vahid and Bassetto, Samuel},
title = {{Parallel coordinate order for high-dimensional data}},
journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
volume = {14},
number = {5},
pages = {501-515},
keywords = {computational complexity, divergence, high-dimensional data, information measure, visualization},
doi = {https://doi.org/10.1002/sam.11543},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11543},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11543},
abstract = {Abstract Visualization of high-dimensional data is counter-intuitive using conventional graphs. Parallel coordinates are proposed as an alternative to explore multivariate data more effectively. However, it is difficult to extract relevant information through the parallel coordinates when the data are high-dimensional with thousands of overlapping lines. The order of the axes determines the perception of information on parallel coordinates. Thus, the information between attributes remains hidden if coordinates are improperly ordered. Here we propose a general framework to reorder the coordinates. This framework is general enough to cover a wide range of data visualization objectives. It is also flexible enough to contain many conventional ordering measures. Consequently, we present the coordinate ordering binary optimization problem and enhance it to achieve a computationally efficient greedy approach that suits high-dimensional data. Our approach is applied to wine data and genetic data. The purpose of dimension reordering of wine data is to highlight attributes' dependence. Genetic data are reordered to enhance cluster detection. The proposed framework shows that it is able to adapt the criteria for the visualization objective.},
year = {2021}
}

@book{de2,
  title={{Interpretable machine learning}},
  author={Molnar, Christoph},
  year={2020},
  publisher={Lulu. com}
}

@misc{de3,
      title={{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{de4,
author = {W. James Murdoch  and Chandan Singh  and Karl Kumbier  and Reza Abbasi-Asl  and Bin Yu },
title = {{Definitions, methods, and applications in interpretable machine learning}},
journal = {Proceedings of the National Academy of Sciences},
volume = {116},
number = {44},
pages = {22071-22080},
year = {2019},
doi = {10.1073/pnas.1900654116},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1900654116},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1900654116},
abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.}}

@article{de5,
  author  = {Trevor Campbell and Tamara Broderick},
  title   = {{Automated Scalable Bayesian Inference via Hilbert Coresets}},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {15},
  pages   = {1-38},
  url     = {http://jmlr.org/papers/v20/17-613.html}
}
@misc{de6,
      title={{Towards a Definition of Disentangled Representations}}, 
      author={Irina Higgins and David Amos and David Pfau and Sebastien Racaniere and Loic Matthey and Danilo Rezende and Alexander Lerchner},
      year={2018},
      eprint={1812.02230},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{de7,
      title={{The Intriguing Properties of Model Explanations}}, 
      author={Maruan Al-Shedivat and Avinava Dubey and Eric P. Xing},
      year={2018},
      eprint={1801.09808},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{de8,
  title={{Facets: An open source visualization tool for machine learning training data}},
  author={Wexler, James},
  journal={Google Open Source Blog},
  year={2017}
}
@inproceedings{de9,
author = {Matejka, Justin and Fitzmaurice, George},
title = {{Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing}},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025912},
doi = {10.1145/3025453.3025912},
abstract = {Datasets which are identical over a number of statistical properties, yet produce dissimilar graphs, are frequently used to illustrate the importance of graphical representations when exploring data. This paper presents a novel method for generating such datasets, along with several examples. Our technique varies from previous approaches in that new datasets are iteratively generated from a seed dataset through random perturbations of individual data points, and can be directed towards a desired outcome through a simulated annealing optimization strategy. Our method has the benefit of being agnostic to the particular statistical properties that are to remain constant between the datasets, and allows for control over the graphical appearance of resulting output.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1290–1294},
numpages = {5},
keywords = {visualization, anscombe, scatter plots},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@misc{de10,
      title={{Embedding Projector: Interactive Visualization and Interpretation of Embeddings}}, 
      author={Daniel Smilkov and Nikhil Thorat and Charles Nicholson and Emily Reif and Fernanda B. Viégas and Martin Wattenberg},
      year={2016},
      eprint={1611.05469},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{de11,
 author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Examples are not enough, learn to criticize! Criticism for Interpretability}},
 volume = {29},
 year = {2016}
}
@article{de12,
author = {Jacob Bien and Robert Tibshirani},
title = {{Prototype selection for interpretable classification}},
volume = {5},
journal = {The Annals of Applied Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {2403 -- 2424},
keywords = {‎classification‎, integer program, nearest neighbors, prototypes, set cover},
year = {2011},
doi = {10.1214/11-AOAS495},
URL = {https://doi.org/10.1214/11-AOAS495}
}

@inproceedings{de13,
author = {Lin, Hui and Bilmes, Jeff},
title = {{A Class of Submodular Functions for Document Summarization}},
year = {2011},
isbn = {9781932432879},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
pages = {510–520},
numpages = {11},
location = {Portland, Oregon},
series = {HLT '11}
}
@article{de14,
  title={{Visualizing data using t-SNE.}},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}
@article{de15,
author = {Tao Shi and Bin Yu and Eugene E Clothiaux and Amy J Braverman},
title = {{Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies}},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {584-593},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214507000001283},

URL = { 
    
        https://doi.org/10.1198/016214507000001283
    
    

},
eprint = { 
    
        https://doi.org/10.1198/016214507000001283
    
    

}

}

@Inbook{de16,
author="DuMouchel, William",
editor="Abello, James
and Pardalos, Panos M.
and Resende, Mauricio G. C.",
title={{Data Squashing: Constructing Summary Data Sets",
bookTitle="Handbook of Massive Data Sets}},
year="2002",
publisher="Springer US",
address="Boston, MA",
pages="579--591",
abstract="A ``large dataset'' is here defined as one that cannot be analyzed using some particular desired combination of hardware and software because of computer memory constraints. DuMouchel et al. (1999) defined ``data squashing'' as the construction of a substitute smaller dataset that leads to approximately the same analysis results as the large dataset. Formally, data squashing is a type of lossy compression that attempts to preserve statistical information. To be efficient, squashing must improve upon the common strategy of taking a random sample from the large dataset. Three recent papers on data squashing are summarized and their results are compared.",
isbn="978-1-4615-0005-6",
doi="10.1007/978-1-4615-0005-6_16"
}

@article{me1,
author = {Al-Shedivat, Maruan and Dubey, Avinava and Xing, Eric},
title = {{Contextual Explanation Networks}},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CENs)-- a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction, valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {194},
numpages = {44}
}

@misc{me2,
      title={{Saliency Learning: Teaching the Model Where to Pay Attention}}, 
      author={Reza Ghaeini and Xiaoli Z. Fern and Hamed Shahbazi and Prasad Tadepalli},
      year={2019},
      eprint={1902.08649},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{me3,
 author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{This Looks Like That: Deep Learning for Interpretable Image Recognition}},
 volume = {32},
 year = {2019}
}

@InProceedings{me4,
author="Hu, Dichao",
editor="Bi, Yaxin
and Bhatia, Rahul
and Kapoor, Supriya",
title={{An Introductory Survey on Attention Mechanisms in NLP Problems}},
booktitle="Intelligent Systems and Applications",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="432--448",
abstract="First derived from human intuition, later adapted to machine translation for automatic token alignment, attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is assigned, has been widely applied to and attained significant improvement in various tasks in natural language processing, including sentiment classification, text summarization, question answering, dependency parsing, etc. In this paper, we survey through recent works and conduct an introductory summary of the attention mechanism in different NLP problems, aiming to provide our readers with basic knowledge on this widely used method, discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance.",
isbn="978-3-030-29513-4"
}

@inproceedings{me5,
author = {Card, Dallas and Zhang, Michael and Smith, Noah A.},
title = {{Deep Weighted Averaging Classifiers}},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287595},
doi = {10.1145/3287560.3287595},
abstract = {Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {369–378},
numpages = {10},
keywords = {conformal methods, interpretability credibility},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}
@misc{me6,
      title={{Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet}}, 
      author={Wieland Brendel and Matthias Bethge},
      year={2019},
      eprint={1904.00760},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{me7,
      title={{Attention is not Explanation}}, 
      author={Sarthak Jain and Byron C. Wallace},
      year={2019},
      eprint={1902.10186},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@InProceedings{me8,
author = {Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Anna and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
title = {{Multimodal Explanations: Justifying Decisions and Pointing to the Evidence}},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}
@inproceedings{me9,
 author = {Alvarez Melis, David and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Towards Robust Interpretability with Self-Explaining Neural Networks}},
 volume = {31},
 year = {2018}
}
@article{me10, title={{Beyond Sparsity: Tree Regularization of Deep Models for Interpretability}}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11501}, DOI={10.1609/aaai.v32i1.11501}, abstractNote={ &lt;p&gt; The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wu, Mike and Hughes, Michael and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale}, year={2018}, month={Apr.} }

@InProceedings{me11,
author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
title = {{Interpretable Convolutional Neural Networks}},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}
@inproceedings{me12,
author = {Angelino, Elaine and Larus-Stone, Nicholas and Alabi, Daniel and Seltzer, Margo and Rudin, Cynthia},
title = {{Learning Certifiably Optimal Rule Lists}},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098047},
doi = {10.1145/3097983.3098047},
abstract = {We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm provides the optimal solution, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. This framework is a novel alternative to CART and other decision tree methods.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {35–44},
numpages = {10},
keywords = {optimization, interpretable models, decision trees, rule lists},
location = {Halifax, NS, Canada},
series = {KDD '17}
}
@misc{me13,
      title={{Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning}}, 
      author={Nicolas Papernot and Patrick McDaniel},
      year={2018},
      eprint={1803.04765},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{me14,
      title={{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}}, 
      author={Andrew Slavin Ross and Michael C. Hughes and Finale Doshi-Velez},
      year={2017},
      eprint={1703.03717},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@INPROCEEDINGS{me15,
  author={Wojna, Zbigniew and Gorban, Alexander N. and Lee, Dar-Shyang and Murphy, Kevin and Yu, Qian and Li, Yeqing and Ibarz, Julian},
  booktitle={{2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)}}, 
  title={Attention-Based Extraction of Structured Information from Street View Imagery}, 
  year={2017},
  volume={01},
  number={},
  pages={844-850},
  doi={10.1109/ICDAR.2017.143}}

@inproceedings{me16,
 author = {Choi, Edward and Bahadori, Mohammad Taha and Sun, Jimeng and Kulas, Joshua and Schuetz, Andy and Stewart, Walter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism}},
 volume = {29},
 year = {2016}
}
@InProceedings{me17,
author="Hendricks, Lisa Anne
and Akata, Zeynep
and Rohrbach, Marcus
and Donahue, Jeff
and Schiele, Bernt
and Darrell, Trevor",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title={{Generating Visual Explanations}},
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="3--19",
abstract="Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.",
isbn="978-3-319-46493-0"
}

@misc{me18,
      title={{Rationalizing Neural Predictions}}, 
      author={Tao Lei and Regina Barzilay and Tommi Jaakkola},
      year={2016},
      eprint={1606.04155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{me19,
      title={{Neural Machine Translation by Jointly Learning to Align and Translate}}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{pe1,
author = {Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
title = {{Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks}},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}
@misc{pe2,
      title={{NormLime: A New Feature Importance Metric for Explaining Deep Neural Networks}}, 
      author={Isaac Ahern and Adam Noack and Luis Guzman-Nateras and Dejing Dou and Boyang Li and Jun Huan},
      year={2019},
      eprint={1909.04200},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{pe3,
    author = {Apley, Daniel W. and Zhu, Jingyu},
    title = {{Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models}},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {82},
    number = {4},
    pages = {1059-1086},
    year = {2020},
    month = {06},
    abstract = "{In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.}",
    issn = {1369-7412},
    doi = {10.1111/rssb.12377},
    url = {https://doi.org/10.1111/rssb.12377},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/82/4/1059/49323845/jrsssb\_82\_4\_1059.pdf},
}

@misc{pe4,
      title={{RISE: Randomized Input Sampling for Explanation of Black-box Models}}, 
      author={Vitali Petsiuk and Abir Das and Kate Saenko},
      year={2018},
      eprint={1806.07421},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@InProceedings{pe5,
  title = 	 {{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})}},
  author =       {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and sayres, Rory},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2668--2677},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18d/kim18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kim18d.html},
  abstract = 	 {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.}
}
@article{pe6, title={{Anchors: High-Precision Model-Agnostic Explanations}}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11491}, DOI={10.1609/aaai.v32i1.11491}, abstractNote={ &lt;p&gt; We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, &quot;sufficient&quot; conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos}, year={2018}, month={Apr.} }

@INPROCEEDINGS{pe7,
  author={Ramu, M. and Raj, Chinnakotla Jayanth and Nithish, Apthiri and Boggula, Chandhu and G, Ganesh Babu and K.I, Srikanth goud},
  booktitle={{2023 7th International Conference on Computing Methodologies and Communication (ICCMC)}}, 
  title={Applying Deep Learning Methods on Spam Review Detection}, 
  year={2023},
  volume={},
  number={},
  pages={162-169},
  doi={10.1109/ICCMC56507.2023.10083900}}

@misc{pe8,
      title={{SmoothGrad: removing noise by adding noise}}, 
      author={Daniel Smilkov and Nikhil Thorat and Been Kim and Fernanda Viégas and Martin Wattenberg},
      year={2017},
      eprint={1706.03825},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{pe9,
  title = 	 {{Learning Important Features Through Propagating Activation Differences}},
  author =       {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3145--3153},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/shrikumar17a.html},
  abstract = 	 {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH}
}

@InProceedings{pe10,
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title = {{Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization}},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{pe11,
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
title = {{Learning Deep Features for Discriminative Localization}},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{pe12,
    doi = {10.1371/journal.pone.0130140},
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {{On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation}},
    year = {2015},
    month = {07},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0130140},
    pages = {1-46},
    abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
    number = {7},

}

@InProceedings{pe13,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title={{Visualizing and Understanding Convolutional Networks}},
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}

@misc{pe14,
      title={{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}}, 
      author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
      year={2014},
      eprint={1312.6034},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@TECHREPORT{pe15,
title = {{The Taylor Decomposition: A Unified Generalization of the Oaxaca Method to Nonlinear Models}},
author = {Bazen, Stephen and Joutard, Xavier},
year = {2013},
institution = {Aix-Marseille School of Economics, France},
type = {AMSE Working Papers},
number = {1332},
abstract = {The widely used Oaxaca decomposition applies to linear models. Extending it to commonly used nonlinear models such as binary choice and duration models is not straightforward. This paper shows that the original decomposition using a linear model can be obtained as a first order Taylor expansion. This basis provides a means of obtaining a coherent and unified approach which applies to nonlinear models, which we refer to as a Taylor decomposition. Explicit formulae are provided for the Taylor decomposition for the main nonlinear models used in applied econometrics including the Probit binary choice and Weibull duration models. The detailed decomposition of the explained component is expressed in terms of what are usually referred to as marginal effects and a remainder. Given Jensen's inequality, the latter will always be present in nonlinear models unless an ad hoc or tautological basis for decomposition is used.},
keywords = {Oaxaca decomposotion; nonlinear models},
url = {https://EconPapers.repec.org/RePEc:aim:wpaimx:1332}
}

@Inbook{pe16,
author="Hyv{\"a}rinen, Aapo
and Hurri, Jarmo
and Hoyer, Patrik O.",
title={{Independent Component Analysis}},
bookTitle="Natural Image Statistics: A Probabilistic Approach to Early Computational Vision",
year="2009",
publisher="Springer London",
address="London",
pages="151--175",
abstract="In this chapter, we discuss a statistical generative model called independent component analysis. It is basically a proper probabilistic formulation of the ideas underpinning sparse coding. It shows how sparse coding can be interpreted as providing a Bayesian prior, and answers some questions which were not properly answered in the sparse coding framework.",
isbn="978-1-84882-491-1",
doi="10.1007/978-1-84882-491-1_7"
}


@ARTICLE{zhangair,
       author = {{Zhang}, Jiajun and {Cosma}, Georgina and {Bugby}, Sarah and {Finke}, Axel and {Watkins}, Jason},
        title = "{Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jul,
          eid = {arXiv:2307.11643},
        pages = {arXiv:2307.11643},
archivePrefix = {arXiv},
       eprint = {2307.11643},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230711643Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{scikit-learn,
 title={{Scikit-learn: Machine Learning in {P}ython}},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@Article{numpy,
 title         = {{Array programming with {NumPy}}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{tqdm, title={{tqdm: A fast, Extensible Progress Bar for Python and CLI}}, DOI={10.5281/zenodo.7697295}, abstractNote={<ul> <li>add Python 3.11 and drop Python 3.6 support (#1439, #1419, #502 &lt;- #720, #620)</li> <li>misc code &amp; docs tidy</li> <li>fix &amp; update CI workflows &amp; tests</li> </ul>}, journal={Zenodo}, author={Casper da Costa-Luis and Stephen Karl Larroque and Kyle Altendorf and Hadrien Mary and richardsheridan and Mikhail Korobov and Noam Raphael and Ivan Ivanov and Marcel Bargull and Nishant Rodrigues and et al.}, year={2023}, month={Mar} }

@Article{matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {{Matplotlib: A 2D graphics environment}},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

@Manual{shapely,
  title = {{{GEOS} coordinate transformation software library}},
  author = {{GEOS contributors}},
  organization = {Open Source Geospatial Foundation},
  year = {2021},
  url = {https://libgeos.org/},
}

@article {ccmn,
	author = {Ter-Sarkisov, Aram},
	title = {{COVID-CT-Mask-Net: Prediction of COVID-19 from CT Scans Using Regional Features}},
	year = {2022},
	doi = {10.1007/s10489-021-02731-6},
	journal = {Applied Intelligence},
	volume = {52},
	pages = {9664–9675}}

@ARTICLE{lfcn,
  author={Yang, Kaifeng and Liu, Yuliang and Zhang, Shiwen and Cao, Jiajian},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={{Surface Defect Detection of Heat Sink Based on Lightweight Fully Convolutional Network}}, 
  year={2022},
  volume={71},
  number={},
  pages={1-12},
  doi={10.1109/TIM.2022.3188033}}

@INPROCEEDINGS {run,
author = {D. Jha and P. H. Smedsrud and M. A. Riegler and D. Johansen and T. Lange and P. Halvorsen and H. D. Johansen},
booktitle = {2019 IEEE International Symposium on Multimedia (ISM)},
title = {{ResUNet++: An Advanced Architecture for Medical Image Segmentation}},
year = {2019},
volume = {},
issn = {},
pages = {225-2255},
abstract = {Accurate computer-aided polyp detection and segmentation during colonoscopy examinations can help endoscopists resect abnormal tissue and thereby decrease chances of polyps growing into cancer. Towards developing a fully automated model for pixel-wise polyp segmentation, we propose ResUNet++, which is an improved ResUNet architecture for colonoscopic image segmentation. Our experimental evaluations show that the suggested architecture produces good segmentation results on publicly available datasets. Furthermore, ResUNet++ significantly outperforms U-Net and ResUNet, two key state-of-the-art deep learning architectures, by achieving high evaluation scores with a dice coefficient of 81.33%, and a mean Intersection over Union (mIoU) of 79.27% for the Kvasir-SEG dataset and a dice coefficient of 79.55%, and a mIoU of 79.62% with CVC-612 dataset.},
keywords = {},
doi = {10.1109/ISM46123.2019.00049},
url = {https://doi.ieeecomputersociety.org/10.1109/ISM46123.2019.00049},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}


@Article{IEMRCNN,
AUTHOR = {Zhang, Jiajun and Cosma, Georgina and Watkins, Jason},
title = {{Image Enhanced Mask R-CNN: A Deep Learning Pipeline with New Evaluation Measures for Wind Turbine Blade Defect Detection and Classification}},
JOURNAL = {Journal of Imaging},
VOLUME = {7},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {46},
URL = {https://www.mdpi.com/2313-433X/7/3/46},
ISSN = {2313-433X},
ABSTRACT = {Demand for wind power has grown, and this has increased wind turbine blade (WTB) inspections and defect repairs. This paper empirically investigates the performance of state-of-the-art deep learning algorithms, namely, YOLOv3, YOLOv4, and Mask R-CNN for detecting and classifying defects by type. The paper proposes new performance evaluation measures suitable for defect detection tasks, and these are: Prediction Box Accuracy, Recognition Rate, and False Label Rate. Experiments were carried out using a dataset, provided by the industrial partner, that contains images from WTB inspections. Three variations of the dataset were constructed using different image augmentation settings. Results of the experiments revealed that on average, across all proposed evaluation measures, Mask R-CNN outperformed all other algorithms when transformation-based augmentations (i.e., rotation and flipping) were applied. In particular, when using the best dataset, the mean Weighted Average (mWA) values (i.e., mWA is the average of the proposed measures) achieved were: Mask R-CNN: 86.74\%, YOLOv3: 70.08\%, and YOLOv4: 78.28\%. The paper also proposes a new defect detection pipeline, called Image Enhanced Mask R-CNN (IE Mask R-CNN), that includes the best combination of image enhancement and augmentation techniques for pre-processing the dataset, and a Mask R-CNN model tuned for the task of WTB defect detection and classification.},
DOI = {10.3390/jimaging7030046}
}