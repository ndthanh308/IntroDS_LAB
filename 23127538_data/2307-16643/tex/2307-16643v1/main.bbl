% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{shen2018natural}
J.~Shen, R.~Pang, R.~J. Weiss, M.~Schuster, N.~Jaitly, Z.~Yang, Z.~Chen,
  Y.~Zhang, Y.~Wang, R.~Skerrv-Ryan \emph{et~al.}, ``Natural {TTS} synthesis by
  conditioning wavenet on mel spectrogram predictions,'' \emph{Proc. ICASSP},
  2018.

\bibitem{siuzdak2022wavthruvec}
H.~Siuzdak, P.~Dura, P.~van Rijn, and N.~Jacoby, ``{WavThruVec}: Latent speech
  representation as intermediate features for neural speech synthesis,''
  \emph{Proc. Interspeech}, 2022.

\bibitem{taylor2019analysis}
J.~Taylor and K.~Richmond, ``Analysis of pronunciation learning in end-to-end
  speech synthesis.'' \emph{Proc. Interspeech}, 2019.

\bibitem{fong2019comparison}
J.~Fong, J.~Taylor, K.~Richmond, and S.~King, ``A comparison between letters
  and phones as input to sequence-to-sequence models for speech synthesis,'' in
  \emph{10th ISCA Speech Synthesis Workshop}, 2019, pp. 223--227.

\bibitem{black1998issues}
A.~W. Black, K.~Lenzo, and V.~Pagel, ``Issues in building general letter to
  sound rules,'' in \emph{The third ESCA/COCOSDA workshop (ETRW) on speech
  synthesis}, 1998.

\bibitem{taylor2005hidden}
P.~Taylor, ``Hidden {Markov} models for grapheme to phoneme conversion,'' in
  \emph{Ninth European Conference on Speech Communication and Technology},
  2005.

\bibitem{galescu2002pronunciation}
L.~Galescu and J.~F. Allen, ``Pronunciation of proper names with a joint n-gram
  model for bi-directional grapheme-to-phoneme conversion,'' in \emph{Seventh
  International Conference on Spoken Language Processing}, 2002.

\bibitem{novak2012wfst}
J.~R. Novak, N.~Minematsu, and K.~Hirose, ``{WFST}-based grapheme-to-phoneme
  conversion: Open source tools for alignment, model-building and decoding,''
  in \emph{Proceedings of the 10th International Workshop on Finite State
  Methods and Natural Language Processing}, 2012, pp. 45--49.

\bibitem{damper1998comparison}
R.~Damper, Y.~Marchand, M.~Adamson, and K.~Gustafson, ``A comparison of
  letter-to-sound conversion techniques for {E}nglish text-to-speech
  synthesis,'' \emph{Proceedings of the Institute of Acoustics}, vol.~20,
  no.~6, pp. 245--254, 1998.

\bibitem{toshniwal2016jointly}
S.~Toshniwal and K.~Livescu, ``Jointly learning to align and convert graphemes
  to phonemes with neural attention models,'' in \emph{Proc. IEEE Spoken
  Language Technology Workshop (SLT)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2016, pp. 76--82.

\bibitem{rao2015grapheme}
K.~Rao, F.~Peng, H.~Sak, and F.~Beaufays, ``Grapheme-to-phoneme conversion
  using long short-term memory recurrent neural networks,'' in \emph{Proc.
  ICASSP}, 2015.

\bibitem{yolchuyeva2020transformer}
S.~Yolchuyeva, G.~N{\'e}meth, and B.~Gyires-T{\'o}th, ``Transformer based
  grapheme-to-phoneme conversion,'' \emph{Proc. Interspeech}, 2019.

\bibitem{milde2017multitask}
B.~Milde, C.~Schmidt, and J.~K{\"o}hler, ``Multitask sequence-to-sequence
  models for grapheme-to-phoneme conversion.'' \emph{Proc. Interspeech}, 2017.

\bibitem{peters2017massively}
B.~Peters, J.~Dehdari, and J.~van Genabith, ``Massively multilingual neural
  grapheme-to-phoneme conversion,'' \emph{Proc. First Workshop on Building
  Linguistically Generalizable NLP Systems}, 2017.

\bibitem{sokolov2020neural}
A.~Sokolov, T.~Rohlin, and A.~Rastrow, ``Neural machine translation for
  multilingual grapheme-to-phoneme conversion,'' \emph{Proc. Interspeech},
  2019.

\bibitem{elsaadany2020grapheme}
O.~ElSaadany and B.~Suter, ``Grapheme-to-phoneme conversion with a multilingual
  transformer model,'' in \emph{Proceedings of the 17th SIGMORPHON Workshop on
  Computational Research in Phonetics, Phonology, and Morphology}, 2020, pp.
  85--89.

\bibitem{yu2020multilingual}
M.~Yu, H.~D. Nguyen, A.~Sokolov, J.~Lepird, K.~M. Sathyendra, S.~Choudhary,
  A.~Mouchtaris, and S.~Kunzmann, ``Multilingual grapheme-to-phoneme conversion
  with byte representation,'' \emph{Proc. ICASSP}, 2020.

\bibitem{zhu2022byt5}
J.~Zhu, C.~Zhang, and D.~Jurgens, ``{ByT5} model for massively multilingual
  grapheme-to-phoneme conversion,'' \emph{Proc. Interspeech}, 2022.

\bibitem{engelhart2021grapheme}
E.~Engelhart, M.~Elyasi, and G.~Bharaj, ``Grapheme-to-phoneme transformer model
  for transfer learning dialects,'' \emph{arXiv preprint arXiv:2104.04091},
  2021.

\bibitem{dong2022neural}
L.~Dong, Z.-Q. Guo, C.-H. Tan, Y.-J. Hu, Y.~Jiang, and Z.-H. Ling, ``Neural
  grapheme-to-phoneme conversion with pre-trained grapheme models,''
  \emph{Proc. ICASSP}, 2022.

\bibitem{yu2020ensemble}
X.~Yu, N.~T. Vu, and J.~Kuhn, ``Ensemble self-training for low-resource
  languages: Grapheme-to-phoneme conversion and morphological inflection,'' in
  \emph{Proceedings of the 17th SIGMORPHON Workshop on Computational Research
  in Phonetics, Phonology, and Morphology}, 2020, pp. 70--78.

\bibitem{hauer2020low}
B.~Hauer, A.~A. Habibi, Y.~Luan, A.~Mallik, and G.~Kondrak, ``Low-resource
  {G2P} and {P2G} conversion with synthetic training data,'' in
  \emph{Proceedings of the 17th SIGMORPHON Workshop on Computational Research
  in Phonetics, Phonology, and Morphology}, 2020, pp. 117--122.

\bibitem{hammond2021data}
M.~Hammond, ``Data augmentation for low-resource grapheme-to-phoneme mapping,''
  in \emph{Proceedings of the 18th SIGMORPHON Workshop on Computational
  Research in Phonetics, Phonology, and Morphology}, 2021, pp. 126--130.

\bibitem{zelasko2022discovering}
P.~{\.Z}elasko, S.~Feng, L.~M. Vel{\'a}zquez, A.~Abavisani, S.~Bhati,
  O.~Scharenborg, M.~Hasegawa-Johnson, and N.~Dehak, ``Discovering phonetic
  inventories with crosslingual automatic speech recognition,'' \emph{Computer
  Speech \& Language}, vol.~74, p. 101358, 2022.

\bibitem{li2020universal}
X.~Li, S.~Dalmia, J.~Li, M.~Lee, P.~Littell, J.~Yao, A.~Anastasopoulos, D.~R.
  Mortensen, G.~Neubig, A.~W. Black \emph{et~al.}, ``Universal phone
  recognition with a multilingual allophone system,'' in \emph{Proc. ICASSP},
  2020.

\bibitem{klejch2021deciphering}
O.~Klejch, E.~Wallington, and P.~Bell, ``Deciphering speech: a zero-resource
  approach to cross-lingual transfer in {ASR},'' \emph{Proc. Interspeech},
  2021.

\bibitem{goel2010approaches}
N.~Goel, S.~Thomas, M.~Agarwal, P.~Akyazi, L.~Burget, K.~Feng, A.~Ghoshal,
  O.~Glembek, M.~Karafi{\'a}t, D.~Povey \emph{et~al.}, ``Approaches to
  automatic lexicon learning with limited training examples,'' in \emph{2010
  IEEE International Conference on Acoustics, Speech and Signal
  Processing}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010, pp.
  5094--5097.

\bibitem{route2019multimodal}
J.~Route, S.~Hillis, I.~C. Etinger, H.~Zhang, and A.~W. Black, ``Multimodal,
  multilingual grapheme-to-phoneme conversion for low-resource languages,'' in
  \emph{Proceedings of the 2nd Workshop on Deep Learning Approaches for
  Low-Resource NLP (DeepLo 2019)}, 2019, pp. 192--201.

\bibitem{aquino2019g2p}
A.~Aquino, J.~L. Tsang, C.~R. Lucas, and F.~de~Leon, ``{G2P} and {ASR}
  techniques for low-resource phonetic transcription of tagalog, cebuano, and
  hiligaynon,'' in \emph{2019 International Symposium on Multimedia and
  Communication Technology (ISMAC)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2019, pp. 1--5.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{klein2017opennmt}
G.~Klein, Y.~Kim, Y.~Deng, J.~Senellart, and A.~Rush, ``{O}pen{NMT}:
  Open-source toolkit for neural machine translation,'' in \emph{Proc. {ACL}},
  2017, pp. 67--72.

\bibitem{wells1995computer}
J.~C. Wells, ``Computer-coding the {IPA}: a proposed extension of {SAMPA},''
  \emph{Revised draft}, vol.~4, no.~28, p. 1995, 1995.

\bibitem{povey2011kaldi}
D.~Povey, A.~Ghoshal, G.~Boulianne, L.~Burget, O.~Glembek, N.~Goel,
  M.~Hannemann, P.~Motlicek, Y.~Qian, P.~Schwarz \emph{et~al.}, ``The kaldi
  speech recognition toolkit,'' in \emph{Proc. ASRU}, 2011.

\bibitem{rath2013improved}
S.~P. Rath, D.~Povey, K.~Vesel{\`y}, and J.~Cernock{\`y}, ``Improved feature
  processing for deep neural networks.'' \emph{Proc. Interspeech}, 2013.

\bibitem{peddinti2015time}
V.~Peddinti, D.~Povey, and S.~Khudanpur, ``A time delay neural network
  architecture for efficient modeling of long temporal contexts,'' \emph{Proc.
  Interspeech}, 2015.

\bibitem{schreiber2018pomegranate}
J.~Schreiber, ``Pomegranate: fast and flexible probabilistic modeling in
  {Python},'' \emph{Journal of Machine Learning Research}, vol.~18, no. 164,
  pp. 1--6, 2018.

\bibitem{xue-etal-2021-mt5}
L.~Xue, N.~Constant, A.~Roberts, M.~Kale, R.~Al-Rfou, A.~Siddhant, A.~Barua,
  and C.~Raffel, ``m{T}5: A massively multilingual pre-trained text-to-text
  transformer,'' in \emph{Proc. NAACL}, 2021, pp. 483--498.

\end{thebibliography}
