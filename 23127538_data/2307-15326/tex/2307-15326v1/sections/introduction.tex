\section{Introduction} \label{sec:introduction}
The choice of image for an online ad can have a significant impact on the online user exposed to the ad. If the ad image is enticing enough, it can not only create brand awareness among online users but also drive them to click the ad and make subsequent purchases (conversions) \cite{mappi_CIKM,gemx_kdd}. However, if the ad image is not properly designed to capture the user's attention, it would lead to poor user interactions and adversely affect the advertising platform (by lowering revenue) and the advertiser (by lowering conversion rate). In this context, a common observation \cite{cikm2020_createbetterads} is that ad images with products in a natural or real world setting (lifestyle images) tend to have better online performance. For example, an ad selling a chair is expected to perform better if the image shows a chair in a living room versus a chair against a solid (synthetic) background (as shown in Figure~\ref{fig:pull_figure}).
However, such staging of products may be expensive and time consuming, specially when a vendor is selling multiple products at the same time.
In DPA offerings from ad platforms (\emph{e.g.}, Yahoo), the catalog images from an e-commerce vendor (\emph{e.g.}, Walmart, Amazon) are typically used directly as ad images. As described later in our data analysis (based on data from an ad platform), a major fraction of such images are not staged, and hence there is a scope to enhance such images (\emph{e.g.}, by generating a suitable background for the product).
% Figure environment removed

Image generation has been an actively studied topic for the past few years. Diffusion models \cite{dalle} and GANs \cite{goodfellow2014generative,isola2017image} have been powering the state-of-the-art results in this area. Image in-painting \cite{nazeri2019edgeconnect} is a slightly easier version of the problem where only parts of the image need to be generated as opposed to the whole image. To the best of our knowledge, we are the first to study GAN based image generation approaches for enhancing product images to serve as ad images.
Our main contributions can be summarized as follows.
\begin{enumerate}
\item We study three tasks (as outlined below): (i) vanilla staging, (ii) copy-paste staging, and (iii) image-to-parallax animation.
\item In task 1, we aim to generate the entire background for a product. We use pix2pix \cite{isola2017image} to train a GAN model with pairs of images (input: segmented out product image, output: staged product image with ground truth background).
\item In task 2, our goal is to retrieve a similar product image (with staging) and copy-paste the background while filling in gaps (holes) created in the process of swapping products. We leverage GAN-based in-painting to fill in the gaps mentioned above. We also introduce a weighted boundary loss for in-painting to focus on the image generation quality at product boundaries. Through Frechet inception distance (FID) score, and human evaluations we show that copy-paste staging is significantly better than the vanilla staging baseline.

\item In task 3, we use GAN-based in-painting to create a sequence of images simulating the main product's movement against the staged background as in a parallax animation. The foreground and background both move, but at different speeds, creating the illusion of depth. This is to show how our approach can lead to video ads from product images.
\end{enumerate}
Our retrieval based approach (second task above) shares the intuition common in text generation: retrieval augmented generation (RAG) has better context understanding and generation quality. The remainder of this paper is organized as follows: related work in Section~\ref{sec:related}, problem formulation in Section~\ref{sec:formulation}, relevant data in Section~\ref{sec:data}, and our proposed approaches in Section~\ref{sec:method}. We go over our experimental results in Section~\ref{sec:results}, and end with a discussion in Section~\ref{sec:discussion}.