\section{Product staging using GANs} \label{sec:method}
In this section, we first explain the salient object detection model which is common to our proposed approaches for all the tasks outlined in Section~\ref{sec:formulation}.
Next, we cover our proposed approaches for tasks $1$, $2$, and $3$ (tasks explained in Section~\ref{sec:formulation}) in Sections~\ref{sec:method_task_1}, \ref{sec:method_task_2}, and \ref{sec:method_task_3} respectively. Our major modeling contributions are in the approaches for tasks 2 and 3; for task 1, although we define the task, we use existing approaches (pix2pix \cite{isola2017image}) to solve it. Task 2 is an easier version of task 1, and our proposed approach leads to more realistic staged product images compared to pix2pix for task 1.

\subsection{Product segmentation via saliency} \label{sec:method_saliency}
For tasks $1$, $2$ and $3$, we use U\textsuperscript{2}-Net \cite{qin2020u2} as our saliency object detector. The saliency object detector plays a crucial role in the first step of our approaches since it separates the main product(s) that will be replaced or copy-pasted versus the background in a product image. Once saliency probability maps are obtained from U\textsuperscript{2}-Net, we set the threshold at $0.5$ to generate binary masks and separate foreground pixels from the background pixels. For each task, we use product segmentation in a different manner as explained below.

\subsection{Vanilla staging} \label{sec:method_task_1}
For vanilla staging, we use the \verb|pix2pix| method to generate an image background for a product which is segmented by a saliency mask. The algorithm is a conditional GAN optimizing the loss combining 1) regular GAN objective, and 2) $\ell_1$ distance between original and restored images.  We use product segmentation to prepare pairs of images to train pix2pix for stage (background generation). In particular, given an image with a staged product, we remove the background via product segmentation and use this as the input image to pix2pix. Figure~\ref{fig:pix2pix_ex} shows an example for this approach (original image in the middle, segmented product on the left, and the version with generated staging on the right). 

% Figure environment removed

\subsection{Retrieval assisted copy-paste staging}  \label{sec:method_task_2}
For copy paste staging, we bypass the problem of generating the entire image background by using backgrounds from other relevant images. Our method consists of the following steps:
\begin{enumerate}
    \item For a given segmented product (Figure~\ref{fig:pull_figure}, left), retrieve top-$k$ similar products from a training collection (Fig.~\ref{fig:top_2_sim}). Similarity measure is a cosine distance between embeddings of corresponding product images provided by \verb|Inception-V3|~\cite{szegedy2016rethinking}.
    \item For the top-$k$ similar images, segment out the original products (Fig.~\ref{fig:top_2_masked_out}) and fill in the holes by inpainting using EdgeConnect \cite{nazeri2019edgeconnect} (GAN based model) and a new loss function that we introduce in Section~\ref{sec:wbl}.
    \item Copy-paste the original product image to the inpainted top-$k$ similar images, aligning shape and center mass for the corresponding product masks (Fig.~\ref{fig:top_2_res}). 
\end{enumerate}
The above algorithm is illustrated with examples in Figure \ref{fig:cp_flow}. We provide additional examples in Figure~\ref{fig:cp_example_0}.
% Figure environment removed
% Figure environment removed
After completing steps 1-3, we generate $k$ product images with various backgrounds, only small parts of which (holes around the product before/after) are generated by GANs, which makes the images look more real if comparing with vanilla staging. For better background generation we introduce a new loss function as described below.

\subsubsection{Weighted Boundary Loss} \label{sec:wbl}
Recent works \cite{nazeri2019edgeconnect} and \cite{wang2021image} explore coarse-to-fine inpainting approaches, since the structures of objects are complex and diverse, adding an intermediate step, like edge maps or monochromic images, can help models to learn progressively and eventually generate better final inpainted outputs. We propose a weighted boundary loss (WBL) to not only simplify the learning process (since the model needs to focus on lesser area), but also mimic the end application use case.  Following prior work \cite{nazeri2019edgeconnect}, our total generator loss consists of a conventional adversarial loss $L_{ADV}$ and a feature-matching loss $L_{FM}$. In addition to these two losses, since our goal is to make the model learn better at the boundary of the masked area, we add weighted boundary loss $L_{WBL}$ to amplify the loss penalty at the boundary area pixels. WBL is: \begin{equation}
L_{WBL}=W_{map}*L_{\ell_1-norm}(E_{GT}, E_{pred}),
\end{equation}
where $E_{GT}$ is ground truth edge map of input images, $E_{pred}$ is predicted edge map generated by the generator. The $W_{map}$ is a pixel-wise weighted map and has the same size as input masked images and ground truth. To be more specific, the $W_{map}$ has $\lambda_{boundary}$ for pixels around the boundary between masked area and unmasked area, and $\lambda_{non-boundary}$ for pixels away from the boundary, the pixel-wise $L_{l1-norm}$ will multiply the corresponding $\lambda$ as we calculate $L_{WBL}$. As Figure~\ref{fig:loss_mask_example} illustrates, for each training sample, we create free-form dense masks by the method proposed by \cite{yu2019free}. Then, we find the boundary area of the free-form mask and assign $\lambda_{boundary}$ (white area in Figure~\ref{fig:loss_weighted_mask}) and $\lambda_{non-boundary}$ (gray area in Figure~\ref{fig:loss_weighted_mask}). For experiments, we fixed $\lambda_{boundary}=0.9$ and $\lambda_{non-boundary}=0.1$.

% Figure environment removed

\subsection{Image-to-animation}  \label{sec:method_task_3}
Parallax effect happens when the background pixels move slower than foreground objects in an animation, thereby creating an illusion of depth in a two-dimensional image. Generally, parallax effect requires independent foreground images and background images, and proper technique to make transparent backgrounds. In our proposed approach, by leveraging the power of salient object detection and in-painting, a parallax effect animation can be generated from a 2D image. Practically, we run salient object detection to define foreground pixels, then gradually move the foreground object around creating empty gaps between the current position of the object and original position. To fill the empty gap, we then use image in-painting model to in-paint those pixels and create serial realistic images. We illustrate the sample results of the above approach in Figure~\ref{fig:parallax_example}.

% Figure environment removed