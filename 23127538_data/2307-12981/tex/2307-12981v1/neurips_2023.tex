\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}
\usepackage[pdftex]{graphicx}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{enumitem}
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{comment}
\definecolor{MyDarkBlue}{rgb}{0,0.08,1}
\definecolor{MyDarkGreen}{rgb}{0.02,0.6,0.02}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{MyDarkOrange}{rgb}{0.40,0.2,0.02}
\definecolor{MyPurple}{RGB}{111,0,255}
\definecolor{MyRed}{rgb}{1.0,0.0,0.0}
\definecolor{MyGold}{rgb}{0.75,0.6,0.12}
\definecolor{MyDarkgray}{rgb}{0.66, 0.66, 0.66}

\newcommand{\todo}[1]{\textcolor{MyRed}{todo: #1}}
\newcommand{\gc}[1]{\textcolor{green}{chuang: #1}}
\newcommand{\zf}[1]{\textcolor{MyDarkBlue}{[zf: #1]}}

\hypersetup{colorlinks=true,linkcolor=red,citecolor=brown,urlcolor=blue}
\usepackage[font={small}]{caption}
% \title{3D-LLM: Ground Everything \\from 3D in Large Language Models}
% \title{3D Grounded Instruction Tuning}
% \title{3D-LLM: Aligning 3D with Large Language Models}
% \title{Bridging 3D world with large large models}
\title{3D-LLM: Injecting the 3D World \\ into Large Language Models}
% \title{3D-Aware Large Language Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Yining Hong\\
  University of California, Los Angeles\\
  % examples of more authors
  \And
  Haoyu Zhen \\
  Shanghai Jiao Tong University \\
  % Address \\
  % \texttt{email} \\
  \And
  Peihao Chen\\
  South China University of Technology\\
  \And
  Shuhong Zheng\\
  University of Illinois Urbana-Champaign \\
  \And 
  Yilun Du\\
  Massachusetts Institute of Technology\\
  \And 
  Zhenfang Chen\\
  MIT-IBM Watson AI Lab\\
  \And
  Chuang Gan\\
  UMass Amherst and MIT-IBM Watson AI Lab
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
% Recent works utilize instruction tuning on large language models for better accommodation to all types of tasks. 
Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. 
Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D
grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. 
% align the features between 2D images and 3D point clouds to extract 3D features and utilize pretrained 2D VLMs for the training of 3D-LLMs. 
By introducing a 3D localization mechanism, 3D-LLMs can
better capture 3D spatial information. 
Experiments on ScanQA  show that our model outperforms state-of-the-art baselines by a large margin (\textit{e.g.}, the BLEU-1 score surpasses state-of-the-art score by 9\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : \url{https://vis-www.cs.umass.edu/3dllm/}.
\end{abstract}


\input{tex/introduction}
\input{tex/related}
\input{tex/data}
\input{tex/method}
\input{tex/exp}

\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
In this paper, we propose a new family of 3D-LLMs that can take 3D representations as inputs and generate responses. We introduce a series of 3D-language data generation pipelines to generate a dataset of 300K 3D-language pairs to train our 3D-LLMs, including dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Our 3D-LLMs leverage 2D pretrained VLMs as backbones and a novel 3D localization mechanism. Experiments show that our 3D-LLMs outperform state-of-the-art baseline models on ScanQA datasets, and could perform a diverse set of 3D-related tasks. A limitation is that the 3D feature extractor relies on 2D multi-view images, and thus all 3D scenes need to be rendered so that they can be trained in 3D-LLMs, which introduces an additional rendering process. 
% Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple tasks, such as commonsense reasoning. 
% Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D
% grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. 
% % align the features between 2D images and 3D point clouds to extract 3D features and utilize pretrained 2D VLMs for the training of 3D-LLMs. 
% By introducing a 3D localization mechanism, 3D-LLMs could
% better capture 3D spatial information. 
% Experiments on ScanQA  show that our model outperforms state-of-the-art baselines by a large margin (\textit{e.g.}, the BLEU-1 score surpasses state-of-the-art score by 9\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Additional experiments and qualitative examples show that our model could perform more tasks beyond the scope of existing LLMs and VLMs, such as 3D grounding, navigation, and so on \todo{To be rewritten by ChatGPT?}. 


{\small
\bibliographystyle{abbrv}  % abbrv, named
\bibliography{ref}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\input{supp_new}

\end{document}