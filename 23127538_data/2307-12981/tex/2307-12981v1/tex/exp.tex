\vspace{-5pt}
\section{Experiments}
\vspace{-1em}
We first introduce the architecture, and training and evaluation protocols. In Sec \ref{sec: held-out}, we analyze the held-out experiments on ScanQA Dataset. Sec \ref{sec: more} covers more analysis on held-in evaluation and qualitative examples. Due to page limit, \textbf{we put the following content into Appendix}: 1) Held-Out Experiments on 3DMV-VQA and object navigation; 2) Held-In Experiments on grounding and dense captioning; 3) More ablative studies; 4) More qualitative examples.

%\subsection
% \textbf{Implementation Details}
% \label{sec: details}

%\subsubsection
\textbf{Architecture}
We experiment on three backbone 2D VLMs for 3D-LLMs: Flamingo 9B, BLIP-2 Vit-g Opt2.7B, BLIP-2 Vit-g FlanT5-XL. For BLIP-2, during pre-training the 3D-LLMs, we initialize the model from BLIP-2 checkpoints released in LAVIS library \cite{li2022lavis}, and finetune the parameters for the QFormer. 3D features are 1408-dim features, same as EVA\_CLIP hidden feature dim used by BLIP-2. We keep most parts of the LLMs (\textit{i.e., } Opt and FlanT5) frozen, except the weights for the newly-added location tokens in the input and the output embeddings. For Flamingo, we initialize the model from the Flamingo9B checkpoint released in OpenFlamingo repository \cite{anas_awadalla_2023_7733589}. We finetune the parameters for perceiver, gated cross attention layers, and the weights for additional location tokens in the input and output embeddings. 3D features are 1024-dim features, same as CLIP hidden feature dim used by Flamingo. 

%\subsubsection{Training and Evaluation Datasets \& Protocols}
\textbf{Training \& Evaluation Datasets \& Protocols}
We split our datasets into two genres, held-in datasets and held-out datasets. 
Specifically, our 3D-language data generation pipeline generates the held-in datasets of multiple tasks. we split the datasets into train/val/test sets (8:1:1). We utilize training sets of held-in datasets for pre-training foundation 3D-LLMs, and their validation and test sets can be applied for held-in evaluation.
During pre-training, we mix the held-in datasets of all tasks. The models are trained with the standard language modeling loss to output responses. 
Held-out datasets, on the other hand, are not used in training the foundation 3D-LLMs. We use two held-out 3D question answering datasets for held-out evaluation: ScanQA and 3DMV-VQA. 
We put the analysis of experiments of 3DMV-VQA\cite{hong20233d} in the supplementary material. 

\vspace{-0.5em}
\subsection{Held-Out Evaluation}
\vspace{-0.5em}
\label{sec: held-out}
%\subsubsection{Experiments on ScanQA}
% \textbf{Experiments on ScanQA}

We finetune our pretrained 3D-LLMs on the ScanQA dataset and compare with baseline models.

\textbf{Baselines \& Evaluation Metrics}
We include representative baseline models on the benchmark. Particularly, \textbf{ScanQA} is the state-of-the-art method on the benchmark that uses VoteNet to obtain object proposals, and then fuse them with language embeddings. \textbf{ScanRefer+MCAN} is a baseline that identifies the referred object
and the MCAN model is applied to the image surrounding
the localized object. \textbf{VoteNet+MCAN} detects objects in a 3D space, extracts their features, and uses them in a standard VQA model. Notably, these baseline models all extract explicit object representations from a pretrained localization module. 
% And state-of-the-art ScanQA method uses three losses: object localization, object classification, and answer classification to train their model. SingleImage+MCAN is an image-based baseline that takes a single image as input.
In addition to these baselines, we also design several LLM-based baselines.
% \vspace{-2mm}
% \begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 
\textbf{LLaVA} is a visual instruction tuning that connects a vision encoder and LLM for general-purpose visual and language understanding. We use its pretrained model and do zero-shot evaluation on our dataset. We use a single random image as input. We use LLaVA 13B model. 
 \textbf{Single Image + Pretrained VLMs} use our 2D VLM backbones (\textit{i.e.}, flamingo and BLIP-2), replace the 3D inputs of 3D-LLMs with single image features to train the models, and then finetune on ScanQA dataset.
 \textbf{Multi-View Image + Pretrained VLMs} use our 2D VLM backbones, replace the 3D inputs of 3D-LLMs with concatenated features of multi-view images to train the models, and then finetune on ScanQA dataset.
% \textbf{Evaluation Metrics} 
We report BLEU, ROUGE-L, METEOR, CIDEr for robust answer matching. We also use exact match (EM) metric. 

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{l|llllllll}
    \hline
        ~ & B-1 & B-2 & B-3 & B-4 & METEOR & ROUHE-L & CIDER & EM \\ \hline
        % SingleImage+MCAN & 14.8 & 8.2 & 16.5 & 0.7 & 7.6 & 19.3 & 33.1 & 12.7 \\ 
        VoteNet+MCAN* & 28.0 & 16.7 & 10.8 & 6.2 & 11.4 & 29.8 & 54.7 & 17.3 \\ 
        ScanRefer+MCAN* & 26.9 & 16.6 & 11.6 & 7.9 & 11.5 & 30 & 55.4 & 18.6 \\ 
        ScanQA* & 30.2 & 20.4 & 15.1 & 10.1 & 13.1 & 33.3 & 64.9 & \textbf{21.0} \\ \hline
        LLaVA(zero-shot)  & 7.1 & 2.6 & 0.9 & 0.3 & 10.5 & 12.3 & 5.7 & 0.0\\
        flamingo-SingleImage & 23.8 & 14.5 & 9.2 & 8.5 & 10.7 & 29.6 & 52 & 16.9 \\ 
        flamingo-MultiView & 25.6 & 15.2 & 9.2 & 8.4 & 11.3 & 31.1 & 55 & 18.8 \\ 
        BLIP2-flant5-SingleImage & 28.6 & 15.1 & 9.0 & 5.1 & 10.6 & 25.8 & 42.6 & 13.3 \\ 
        BLIP2-flant5-MultiView & 29.7 & 16.2 & 9.8 & 5.9 & 11.3 & 26.6 & 45.7 & 13.6 \\ \hline
        3D-LLM (flamingo)  & 30.3 & 17.8 & 12.0 & 7.2 & 12.2 & 32.3 & 59.2 & 20.4 \\ 
        3D-LLM (BLIP2-opt) & 35.9 & 22.5 & 16.0 & 9.4 & 13.8 & 34.0 & 63.8 & 19.3 \\ 
        3D-LLM (BLIP2-flant5) & \textbf{39.3} & \textbf{25.2} & \textbf{18.4} & \textbf{12.0} & \textbf{14.5} & \textbf{35.7} & \textbf{69.4} & 20.5\\ \hline
    \end{tabular}
    \vspace{-5pt}
    \caption{Experimental results on ScanQA validation set. * Means the models use explicit object representations. B-1, B-2, B-3, B-4 denote BLEU-1, BLEU-2, BLEU-3, BLEU-4 respectively. Our model outperforms all baseline models for all evaluation metrics except for the EM metric.}
    \label{tab:scanqa-val}
    \vspace{-15pt}
\end{table*}

\begin{table*}[th]
    \centering
    \small
    \begin{tabular}{l|llllll}
        \hline
        ~ & BLEU-1 & BLEU-4 & METEOR & ROUHE-L & CIDER & EM \\ \hline
        SingleImage+MCAN & 16.5 & 0.0 & 8.4 & 21.5 & 38.6 & 15.8 \\ 
        VoteNet+MCAN* & 29.5 & 6.0 & 12.0 & 30.9 & 58.2 & 19.7 \\ 
        ScanRefer+MCAN* & 27.9 & 7.5 & 11.9 & 30.7 & 57.4 & 20.6 \\ 
        ScanQA* & 31.6 & \textbf{12.0} & 13.5 & 34.3 & 67.3 & \textbf{23.5} \\ \hline
        3D-LLM (flamingo) & 32.6 & 8.4 & 13.5 & 34.8 & 65.6 & 23.2 \\ 
        3D-LLM (BLIP2-opt) & 37.3 & 10.7 & 14.3 & 34.5 & 67.1 & 19.1 \\ 
        3D-LLM (BLIP2-flant5) & \textbf{38.3} & 11.6 & \textbf{14.9} & \textbf{35.3} & \textbf{69.6} & 19.1 \\ \hline
    \end{tabular}
    \caption{Experimental results on ScanQA test set. * Means the models use explicit object representations. Our model outperforms all baseline models for most of the evaluation metrics.}
    \label{tab:scanqa-test}
    \vspace{-1em}
\end{table*}

\textbf{Result Analysis} 
We report our results on ScanQA validation set in Table \ref{tab:scanqa-val}, and results on test set in Table \ref{tab:scanqa-test}. We observe a significant increase in the evaluation metrics. For example, for BLEU-1, our model outperforms the state-of-the-art ScanQA model by $\sim$9\% for validation set and $\sim$7\% for test set. For CIDER, we report a $\sim$5\% gain compared to ScanQA, and much higher than other 3D-based baselines. These results show that by injecting 3D into LLMs, the models can generate answers that are much more similar to the ground-truth answers. Furthermore, 3D-based baselines use object detectors like VoteNet to segment the objects, and then send per-object features into their models, while our inputs are holistic 3D features without explicit object representations. This shows that our model could perform visual reasoning about objects and their relationships even without explicit object representations. We then examine whether 2D VLMs have the same ability. We find that by taking single-view images or multi-view images as inputs, the performances drop much compared to 3D-LLMs. Specifically, multi-view images also contain information about the whole scene. However, they have much lower performances compared to 3D-LLMs, probably because features of multi-view images are disorganized, thus losing 3D-related information. 

% %\subsubsection{Experiments on 3DMV-VQA}
% \textbf{Experiments on 3DMV-VQA}

% We finetune our pretrained 3D-LLMs on the 3DMV-VQA dataset and compare with baseline models.

% \textbf{Baselines \& Evaluation Metrics} We include representative baseline models on the benchmark. 
% % Specifically, LSTM is a language-only baseline, and MAC is a state-of-the-art VQA baseline that takes in single images.
% NS-VQA is the neuro-symbolic method that first extracts object proposals and then perform neuro-symoblic reasoning. 3D-Feature + LSTM is a baseline that extracts 3D features first and then concatenates with LSTM to output final answers. 3D-CLR is the state-of-the-art method that extracts 3D feature fields first, and then perform neuro-symbolic reasoning. In addition to these baselines, we also add 2D VLMs baselines like we did for ScanQA.
% % \textbf{Evaluation Metrics} As in the original paper, we report per-category accuracy and overall accuracy.

% \begin{table*}[t]
% 	\begin{center}\small
 
% 	\begin{tabular}{lccccc}
% 	\hline 
%       Methods   & Concept & Counting & Relation& Comparison & Overall\\ 
     
%     \hline 
%         % LSTM           &53.4&15.3&24.0&55.2 &29.8\\
%         % MAC &62.4&19.7&47.8&62.3 &46.7\\    
%         NS-VQA* &59.8&21.5&33.4&61.6 &38.0\\ 
%         3D-Feature+LSTM  &61.2 &22.4 & 49.9 & 61.3 &48.2\\ 
%         3D-CLR*  & 66.1 & \textbf{41.3} & 57.6&  \textbf{72.3} & 57.7\\ 
%     \hline 
%        flamingo-SingleImage &58.7 & 18.5 & 38.4 &60.1 & 40.3\\
%        flamingo-MultiView &60.0 & 18.3 & 40.2 & 61.4 & 41.6\\
%        BLIP-SingleImage & 58.0 & 20.4 & 42.3 & 62.3 & 43.1\\
%        BLIP-MultiView & 61.9 & 21.1 & 48.0 & 62.3 & 47.1\\
%     \hline 
%         3D-LLM (flamingo) & \textbf{69.6} & 28.5 & \textbf{61.4} & 66.7 & \textbf{57.8}\\
%         3D-LLM (BLIP5-opt) & 63.4 & 30.7 & 57.6 & 65.2 & 54.9\\
%         3D-LLM (BLIP2-flanT5)  & 68.1 & 31.4 & 55.1 & 69.7 & 54.6\\
%     \hline 
% 	\end{tabular}
% 	\end{center}
%         \vspace{-1em}
% 	\caption{Experimental results on 3DMV-VQA dataset. * denotes using explicit object representations and neuro-symbolic reasoning. }
%     \label{tab:3dmv-vqa}
% \end{table*}

% \textbf{Result Analysis} Table \ref{tab:3dmv-vqa} shows the performances on 3DMV-VQA. We can see that 3D-LLMs outperform state-of-the-art baseline model in the question types of concept and relation, and also in the overall performance. Our model also outperforms 3D-Feature+LSTM, demonstrating the power of LLMs over vanilla language models with similar 3D features as inputs. Overall, 3D-based methods outshine 2D-based versions of the methods. Our 3D-LLMs outperforms their corresponding 2D VLMs with image input, further demonstrating the importance of 3D representations for 3D-LLMs. 

% %\subsubsection{Navigation}
% \textbf{Navigation}
% \zf{To be added.}
\vspace{-0.5em}
\subsection{More Extensive Evaluation}
\vspace{-0.5em}
\label{sec: more}
%\subsection{Held-In Evaluation}
\textbf{Held-In Evaluation}
%\vspace{-1em}
We carry out experiments on held-in datasets of three tasks: 3D captioning, 3D-assited dialog and task decomposition. The baselines include 2D VLMs as for the held-out evaluation. We add one language-only baseline: FlanT5, which examines LLMs' ability to complete these tasks without any visual input.
 To evaluate the quality of responses, we include BLEU, ROUGE-L, METEOR, CIDEr as our metrics.
 We report the held-in evaluation performances in Table \ref{tab:held-in}. From the table, we could see that 3D-LLMs could generate high-quality responses, outperforming both 2D VLMs and language-only LLMs. 
% Please refer to the Supplementary Material for more held-in evaluation.

\begin{comment}
\begin{table}[!ht]
    \centering
    \small
    \begin{tabular}{lllllll}
    \hline
         ~ & BLEU-1 & BLEU-2 & BLEU-3 & BLEU4 & METEOR & ROUGH-L  \\ \hline
        \textit{\textbf{3D Captioning}} & ~ & ~ & ~ & ~ & ~ & ~  \\
        flamingo-SingleImage & 24.0 & 12.5 & 8.1 & 4.6 &11.4 & 24.2\\
        flamingo-MultiView & 26.4 & 14.4 & 8.5 & 5.3  & 12.0 & 25.5\\
        BLIP-SingleImage & 24.3 & 12.3 & 8.5 & 5.1 & 11.9 & 24.0\\
        BLIP-MultiView & 26.4 & \textbf{14.6} & \textbf{9.3} & 5.3 & 12.1 & 25.5\\
        3D-LLM (flamingo) & 26.2 & 14.3 & 8.2 &  5.2 & 
        \textbf{12.6} &25.3\\
        3D-LLM (BLIP5-opt) & 25.1 & 14.1 & 7.9 & 5.1 & 11.3 & \textbf{26.1} \\
        3D-LLM (BLIP2-t5) & \textbf{26.5} & 14.5 & 9.1 & \textbf{5.5} & 12.2 & 26.0  \\ \hline
        \textit{\textbf{3D-assisted Dialog}}  & ~ & ~ & ~ & ~ & ~ & ~  \\ 
        FlanT5 & 27.4 & 16.5 & 11.1 & 8.7  & 9.5 & 27.5\\
        flamingo-SingleImage & 29.4 & 18.7 & 11.3 & 9.4 & 10.0 & 26.8\\
        flamingo-MultiView & 30.6 & 21.3 & 11.9 & 9.1  & 10.4 & 27.9\\
        BLIP-SingleImage & 28.4 & 17.3 & 10.6 & 9.1 & 10.2 & 27.4\\
        BLIP-MultiView & 32.4 & 20.9 & 12.1 & 9.5 & 11.0 & 29.5\\
        3D-LLM (flamingo) & 35.0 & 22.8 & 15.4 & 10.6 & 16.0 & 34.2 \\
        3D-LLM (BLIP5-opt) & \textbf{44.6} & \textbf{31.5} & \textbf{23.5} & \textbf{18.2} & 20.4 & 41.6 \\
        3D-LLM (BLIP2-t5) & 43.6 & 30.8 & 23.0 & 17.7 & \textbf{20.7} & \textbf{41.7}  \\ \hline
        \textit{\textbf{Task Decomposition}}  & ~ & ~ & ~ & ~ & ~ & ~  \\ 
        FlanT5 & 20.5 & 11.1 & 6.7 & 4.0 & 7.8 & 19.3 \\
        flamingo-SingleImage & 21.4 & 13.0 & 7.8 & 5.1 & 8.6 & 21.6\\
        flamingo-MultiView & 23.1 & 14.7 & 8.4 & 5.3  & 9.1 & 23.2\\
        BLIP-SingleImage & 22.2 & 12.3 & 8.2 & 5.9 & 8.5 & 21\\
        BLIP-MultiView & 24.2 & 15.2 & 8.9 & 4.8 & 9.7 & 24\\
        
        3D-LLM (flamingo) & 28.9 & 15.6 & 10.2 & 6.4 & 16.0 & 28.5 \\
         3D-LLM (BLIP5-opt) & \textbf{31.2} & \textbf{17.7} & 10.9 & \textbf{7.6} & \textbf{17.5} & \textbf{32.3} \\
         3D-LLM (BLIP2-t5) & 30.3 & 17.6 & \textbf{11.5} & 7.4 & 17.2 & 31.9  \\ \hline
    \end{tabular}
    %\vspace{-1em}
    \caption{Experimental Results on Held-In Datasets.}
    \label{tab:held-in}
\end{table}
\end{comment}

\input{tables/tab_held_in.tex}
%\input{tables/tab_held_in2.tex}
%\subsection{Ablative Study}
% \textbf{Ablative Study}
% %\vspace{-1em}
% To evaluate the benefits of our 3D localization mechanism. We add an ablative study where we add no or part of the 3D localization. We experiment with Flamingo-9B on the ScanQA dataset, and report the performances on the ScanQA dataset. We show the results on Table \ref{tab:loc}. We can see that, the performances decrease without the location tokens, and drop a lot without position embeddings. The performances of 3D-LLM without the localization mechanism are far worse than the original 3D-LLMs. This shows that adding the localization mechanism does improve the performance.

% \begin{table*}[t]
%     \centering
%     \small
%     \begin{tabular}{l|llllllll}
%     \hline
%         ~ & B-1 & B-2 & B-3 & B-4 & METEOR & ROUHE-L & CIDER & EM \\ \hline
%         3D-LLM wo/ position embed. & 27.4 & 16.9 & \textbf{12.1} & \textbf{7.6} & 11.9 & 31.1 & 53.6 & 18.8 \\ 
%         3D-LLM wo/ location tokens & 29.0 & 17.5 & 11.2 & 7.4 & 12.1 & 31.9 & 58.4 & 19.6 \\
%         3D-LLM wo/ localization &25.8 & 15.4 & 9.7 & 7.3 & 10.9 & 30.1 & 53.6 & 18.2 \\
%         3D-LLM  & \textbf{30.3} & \textbf{17.8} & 12.0 & 7.2 & \textbf{12.2} & \textbf{32.3} & \textbf{59.2} & \textbf{20.4} \\ 
%         \hline
%     \end{tabular}
%     \vspace{-0.5em}
%     \caption{Ablative Study on the 3D localization mechanism. Performances decrease without localization.}
%     \vspace{-0.3em}
%     \label{tab:loc}
% \end{table*}

%\subsection{Qualitative Examples}
\textbf{Qualitative Examples}
In Figure \ref{fig:qualitative}, we show qualitative examples of 3D-LLM's predictions. We can see that our 3D-LLM is able to perform a variety of tasks.
% Figure environment removed



