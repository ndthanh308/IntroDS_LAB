\section{Introduction}

% LLM, 2D LLM -> 3D LLM. 
% 1. grounded 2. instruction tuning
% Nowadays we have witnessed a surge of large language models (LLMs) that could communicate like human beings (\textit{e.g.,} ChatGPT). 
% Nowadays we have witnessed a 

% Recently, we have witnessed the success of the general-purpose large language model (LLM) ChatGPT which could respond to any instructions and communicate like human beings.
% To obtain such instruction following abilities, there's been a surge of work in instruction tuning on LLMs, including LLaMA, Alpaca, Vicuna 

% This has also triggered a  
In the past several years, we have witnessed a surge of large language models (LLMs)  (\textit{e.g.}, GPT4 \cite{openai2023gpt4}) that excel at multiple tasks, such as communication and commonsense reasoning.  Recent works have explored aligning images and videos with LLM for a new generation of multi-modal LLMs (\textit{e.g.}, Flamingo \cite{Alayrac2022Flamingo}, BLIP-2 \cite{li2023blip2}) that equip LLMs with the ability to understand and reason about 2D images.
However, as powerful as the models can be in communication and reasoning, they are not \textit{grounded in the real 3D physical world}, which involves richer concepts such as spatial relationships, affordances, physics and interaction so on. Therefore, such LLMs pale in comparison with the robots depicted in sci-fi movies - the assistants that could understand the 3D environments, as well as perform reasoning and planning based on the 3D understandings. 

To this end, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs that could take 3D representations (\textit{i.e.}, 3D point clouds with their features) as input, and perform a series of 3D-related tasks. By taking the 3D representations of scenes as input, LLMs are blessed with twofold advantages: \textbf{(1)} long-term memories about the entire scene can be stored in the holistic 3D representations, instead of episodic partial-view observations. \textbf{(2)} 3D properties such as affordances and spatial relationships can be reasoned from 3D representations, far beyond the scope of language-based or 2D image-based LLMs.

%\vspace{-10pt}
% Figure environment removed

% no data -> build data

One major challenge of training the proposed 3D-LLMs lies in data acquisition. Unlike the vast amount of paired 2D-images-and-text data on the Internet, the scarcity of 3D data hinders the development of 3D-based foundation models. 3D data paired with language descriptions are even harder to obtain. To address this, we propose a set of unique data generation pipelines that could generate large-scale 3D data paired with language. Specifically, we make use of ChatGPT \cite{openai2023gpt4} and devise three efficient prompting procedures for communication between 3D data and language. In this way, we are able to obtain 300k 3D-language data covering a diverse set of tasks, including but not limited to 3D captioning, dense captioning, 3D question answering, 3D task decomposition, 3D grounding, 3D-assisted dialog, navigation and so on, as shown in Figure \ref{fig:gpt-data}.

% data & computation efficient -> 3d instruction tuning on 2D LLM

% \gc{We can merge the following two parts.}
The next challenge resides in how to obtain meaningful 3D features that could align with language features for 3D-LLMs. One way is to train 3D encoders from scratch using a similar contrastive-learning paradigm for the alignment between 2D images and language (\textit{e.g.}, CLIP \cite{radford2021learning}). However, this paradigm consumes tremendous data, time, and GPU resources. From another perspective, there are numerous recent works that build 3D features from 2D multi-view images (\textit{e.g.}, concept fusion \cite{jatavallabhula2023conceptfusion}, 3D-CLR \cite{hong20233d}). Inspired by this, we also utilize a 3D feature extractor that constructs 3D features from the 2D pretrained features of rendered multi-view images. Recently, there are also quite a few visual-language models (\textit{e.g.}, BLIP-2 \cite{li2023blip2}, Flamingo \cite{Alayrac2022Flamingo}) utilizing the 2D pretrained CLIP features for training their VLMs. Since our extracted 3D features are mapped to the same feature space as 2D pretrained features, we can seamlessly use 2D VLMs as our backbones and input the 3D features for the efficient training of 3D-LLMs.
% Thus, we propose to obtain 3D features from 2D multi-view images using 2D-3D mapping, and leverage 2D pre-trained VLMs for the efficient training of 3D-LLMs.
% 3d grounding abilities -> grounding mechanism

One crucial aspect of 3D-LLMs, different from vanilla LLMs and 2D VLMs, is that 3D-LLMs are expected to have a underlying 3D spatial sense of information. Thus, we develop a 3D localization mechanism that bridges the gap between language and spatial locations. Specifically, we append 3D position embeddings to the extracted 3D features to better encode spatial information. In addition, we append a series of location tokens to the 3D-LLMs, and localization can be trained via outputting location tokens given the language descriptions of specific objects in the scenes. In this way, 3D-LLMs could better capture 3D spatial information.

To sum up, our paper has the following contributions:
\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 

\item We introduce a new family of 3D-based Large Language models (3D-LLMs) that can take 3D points with features and language prompts as input, and perform a variety of 3D-related tasks. We focus on tasks beyond the scope of vanilla LLMs or 2D-LLMs, such as tasks about holistic scene understanding, 3D spatial relationships, affordances and 3D planning.

\item We devise novel data collection pipelines that could generate large-scale 3D-language data. Based on the pipelines, we collect a dataset that has over 300k 3D-language data that cover a diverse set of 3D-related tasks, including but not limited to 3D captioning, dense captioning, 3D question answering, 
task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on.  

\item We use a 3D feature extractor that extracts meaningful 3D features from rendered multi-view images. We utilize 2D pretrained VLMs as our backbones for efficient training. We introduce a 3D localization mechanism for training the 3D-LLMs to better capture 3D spatial information.

\item Experiments on held-out evaluation dataset, ScanQA, outperform state-of-the-art baselines. In particular, 3D LLMs outperform baselines by a large margin on ScanQA (\textit{e.g.,} 9\% for BLEU-1). Experiments on held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. 
Qualitative studies further demonstrate that our model is able to handle a diverse set of tasks.
% \item Our model can be utilized in several applications, such as navigation and rendering of specific locations from language instructions (e.g., render an image of the bathroom, the robot is able to navigate to the bathroom and take a picture). We build demo websites for these applications. 

\item We plan to release our 3D-LLMs, the 3D-language dataset, and language-aligned 3D features of the dataset for future research development. 

\end{itemize}