\vspace{-1em}
\section{Related Works}
\vspace{-0.5em}
% \subsection{Large Language Models}
\textbf{Large Language Models.} Our work is closely related to large language models~\cite{brown2020language,devlin2018bert,radford2019language,chowdhery2022palm,OpenAI2023GPT4TR} (LLMs) like GPT-3~\cite{brown2020language} and PaLM~\cite{chowdhery2022palm}, which are able to handle different language tasks with a single model and show strong generalization abilities. These models are typically trained on massive textual data with self-supervised training targets like predicting the next tokens~\cite{brown2020language,radford2019language} or reconstructing the masked tokens~\cite{devlin2018bert,raffel2020exploring}. To better align these LLMs' predictions to human instructions, improve the models' generalization abilities on unseen tasks, a series of instruction tuning methods~\cite{ouyang2022training,sun2023principle} and datasets~\cite{chung2022scaling,dollyblog} have been proposed. In this work, we aim to inject the 3D world into large language models, understanding rich 3D concepts such as spatial relations, affordances, and physics. 

% \subsection{Instruction Tuning}
% \subsection{Vision-Language Pre-trained Models}
\textbf{Vision-Language Pre-trained Models.} Our work is also related to vision-language pre-trained models that connect images and natural language~\cite{li2023blip,liu2023visual,gong2023multimodal,radford2021learning,jia2021scaling}. Some research~\cite{radford2021learning,jia2021scaling} learn to train models from scratch with massive image-language pairs and apply them to downstream tasks like visual question answering~\cite{balanced_vqa_v2,balanced_binary_vqa}, captioning~\cite{chen2015microsoft}, and referring expression comprehension~\cite{yu2016modeling} with finetuning. Other researchers have connected pre-trained vision models and pre-trained LLMs with additional learnable neural modules like perceiver~\cite{anas_awadalla_2023_7733589} and QFormers~\cite{li2023blip}, leveraging perception abilities in pre-trained vision models, and reasoning and generalization capacities in LLMs. Inspired by these previous works, we plan to build an AI assistant that could understand the 3D world and perform corresponding 3D reasoning and planning. This is not trivial and we need to overcome obstacles like how to handle the problem of data sparsity, how to align the 3D world with 2D images, and how to capture 3D spatial information.


% \subsection{3D \& Language}
\textbf{3D \& Language.} Another line of research that is similar to ours is 3D and language~\cite{chen2020scanrefer,Ye20213DQA,chen2021scan2cap,hong20233d,Achlioptas2020ReferIt3DNL,Feng2021FreeformDG,Huang2021TextGuidedGN,Ye20213DQA,Azuma2022ScanQA3Q, hong2023threedclr, hong20223d}. ScanQA~\cite{Ye20213DQA} requires a model to answer questions related to the 3D world; ScanRefer~\cite{chen2020scanrefer} asks a model to localize a region that the text expression refer to;
3D captioning~\cite{chen2021scan2cap} tests models' abilities to generate captions describing the 3D scenes. However, these 3D tasks and their corresponding models are usually task-specific and could only handle cases within the same distribution of the training sets without generalization. Different from them, we aim to build a 3D model that could handle different tasks at the same time and enable new abilities like 3D-assistant dialog and task decomposition. 