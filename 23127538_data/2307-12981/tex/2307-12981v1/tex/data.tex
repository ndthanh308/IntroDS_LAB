\vspace{-8pt}
\section{3D-Language Data Generation}
\vspace{-3pt}

% Figure environment removed
% background
The community has witnessed the proliferation of multi-modal data thanks to easy access to a tremendous amount of 2D image and text pairs on the internet. However, when it comes to 3D-related data, obtaining multimodal resource is not easy,  
due to not only the scarcity of 3D assets, but also the difficulty of providing language data for 3D assets. There are some existing datasets that contain 3D-language data (\textit{e.g.}, ScanQA \cite{Ye20213DQA}, ScanRefer \cite{chen2020scanrefer}). However, they are limited with regard to both quantity and diversity, restricted to only one task per dataset. How to generate a 3D-language dataset that can be utilized for all kinds of 3D-related tasks is well worth delving into. 

% three ways for generating data
Inspired by the recent success of large language models like GPT \cite{openai2023gpt4}, we propose to leverage such models for 3D-language data collection. Specifically, as shown in Figure \ref{fig:data}, we have three ways to prompt a text-only GPT for generating data. 1) boxes-demonstration-instruction based prompting. 
We input the axis-aligned bounding boxes (AABB) of both the rooms and the objects in the 3D scenes, providing information about the semantics and spatial locations of the scene. We then provide specific instructions to the GPT model to generate diverse data. We give 0-3 few-shot demonstration examples of the GPT model showing what kind of data it is instructed to generate.  2) ChatCaptioner based prompting. We utilize techniques similar to \cite{zhu2023chatgpt}, in which ChatGPT is prompted to ask a series of informative questions about an image and BLIP-2~\cite{li2023blip2} answers the questions. In order to collect 3D-related data, we input images from different views to BLIP-2, and ChatGPT is instructed to ask questions and collect information of different regions to form a global 3D description of the entire scene. 3) Revision based prompting. It can be used for transfer one type of 3D data to another,. 


% what kind of data can chatgpt generate
Given the prompting pipelines, GPT is able to generate various types of 3D-language data as summarized in Figure \ref{fig:gpt-data}. We show detailed prompts to generate all types of data in the Appendix.



% Where do 3D assets come from?
We mainly establish our 3D-language dataset upon several 3D assets:

\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 
\item Objaverse is a universe of 800K 3D objects. However, since the language descriptions were extracted from online sources and not examined by humans, most objects have very noisy descriptions (\textit{e.g.,} with urls) or no descriptions. We utilize ChatCaptioner based prompting to generate high-quality 3D-related descriptions for the scenes. 
\item Scannet \cite{Dai2017ScanNetR3} is a richly-annotated dataset of approximately 1k 3D indoor scenes. It provides semantics and bounding boxes of the objects in the scenes. 
\item Habitat-Matterport (HM3D) \cite{ramakrishnan2021hm3d} is a dataset of 3D environments of embodied AI. HM3DSem \cite{yadav2022habitat} further adds semantic annotations and bounding boxes for more than 200 scenes of HM3D. 
\end{itemize}
