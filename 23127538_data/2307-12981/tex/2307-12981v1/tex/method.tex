\vspace{-10pt}
\section{3D-LLM}
\vspace{-6pt}
\subsection{Overview}
\vspace{-6pt}
In this section, we introduce how we train our 3D-LLMs. We argue that it's hard to train 3D-LLMs from scratch, since our collected 3D-language dataset is still not the size of billion-scale image-language dataset used to train 2D VLMs. Furthermore, for 3D scenes, there are no available pretrained encoders like those for 2D images (e.g., CLIP ViT encoders). Thus, retraining 3D-language models from scratch is data-inefficient and resource-heavy. Recently, researchers have proposed to extract 3D features from 2D multi-view images \cite{jatavallabhula2023conceptfusion, hong20233d}. Using these alignment methods, we could use pretrained image encoders to extract image features, and then map the features to the 3D data. Since the pretrained image features serve as inputs to 2D VLMs, the mapped 3d features of the same feature space can also be seamlessly fed into the pretrained 2D VLMs, which we use as our backbones to train 3D-LLMs. We also propose a 3D localization mechanism to boost the model's ability to capture 3D spatial information. Figure \ref{fig:method} shows our framework.

%\vspace{-10pt}
% Figure environment removed

% % Figure environment removed

% \subsection{2D-3D Feature Alignment}
\vspace{-10pt}
\subsection{3D Feature Extractor}
\vspace{-5pt}
%s how to build 3D features. why need to build from 2D: this zero-shot approach enables task-agnostic training and open-vocabulary queries. compute budget. 
The first step of training 3D-LLMs is to build meaningful 3D features that could be aligned with language features. For 2D images, there exist feature extractors like CLIP, which learn visual models from language supervision. The models are pretrained using billion-scale internet data of image-language pairs. It's hard to pre-train such feature learners from scratch, since there are no 3D-language assets comparable to internet-scale image-language pairs in terms of quantity and diversity. 

On the contrary, numerous methods have been proposed to extract 3D features from 2D multi-view images \cite{jatavallabhula2023conceptfusion, hong20233d, Gadre2022CLIPOW, huang2023visual}. Inspired by these works, we extract features for 3D points by rendering the 3D scenes in several different views, and construct 3D features from rendered image features. 

% We extract dense features for 2D images following \cite{jatavallabhula2023conceptfusion}. Specifically, we use MaskFormer\cite{DBLP:journals/corr/abs-2107-06278} to extract class-agnostic instances for each image. Then, we use pretrained image encoders (e.g., CLIP, EVA\_CLIP) to extract global feature for the image, and local features for all instances. Finally, we fuse global features with local features to get per-pixel final features for 2D images. 

We first extract pixel-aligned dense features for rendered images following \cite{jatavallabhula2023conceptfusion}. Then, we utilize three methods to construct 3D features from rendered image features. These methods are designed for different types of 3D data.
\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 
\item \textbf{Direct Reconstruction.} We directly reconstruct point cloud from rgbd images rendered from the 3D data using ground-truth camera matrixes. The features are directly mapped to the reconstructed 3D points. This method is suitable for rendered rgbd data with perfect camera poses and intrinsics.
\item \textbf{Feature Fusion.} Similar to \cite{jatavallabhula2023conceptfusion}, we fuse 2D features into 3D maps using gradslam \cite{gradslam}. Different from dense mapping methods, the features are fused in addition to depths and colors. This method is suitable for 3D data with noisy depth map renderings, or noisy camera poses and intrinsics.
\item \textbf{Neural Field.} We utilize \cite{hong20233d}, which constructs 3D compact representation using neural voxel field \cite{Sun_2022_CVPR}. Specifically, each voxel in the field has a feature in addition to density and color. Then we align 3D features in the rays and 2D features in the pixels using MSE loss. This method is for 3D data with RGB renderings but no depth data, and noisy camera poses and intrinsics. 
\end{itemize}

In this way, we are able to obtain the $<N, \mathcal{D}_v>$-dim 3D features of each 3D scene, where $N$ is the number of points in the point cloud, and $\mathcal{D}_v$ is the feature dimension. 
\vspace{-5pt}
\subsection{Training 3D-LLMs}
% \subsection{Pre-trained 2D Vision-Language Models for 3D-LLMs}
\vspace{-5pt}
\subsubsection{2D VLMs as backbones}
\vspace{-5pt}
In addition to the feature extractor, training 3D-LLMs from scratch is also non-trivial. In fact, 
according to \cite{li2023blip2, Alayrac2022Flamingo}, the training of 2D VLMs only begins to show "signs of life" after consuming half a billion images. They usually use frozen and pre-trained image encoders such as CLIP to extract features for 2D images. 
Considering that with 3D feature extractor, the 3D features can be mapped into the same feature space as 2D images, it's reasonable to use these 2D VLMs as our backbones.

The perceiver architecture proposed by \cite{Jaegle2021PerceiverGP} leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to handle very large inputs of arbitrary input sizes, thus can tackle different modalities. This architecture is utilized in VLMs like Flamingo \cite{Alayrac2022Flamingo}. BLIP-2 \cite{li2023blip2} also utilizes a similar structure called QFormer. The 2D image features, output from frozen image encoders, are flattened and sent to the perceiver to generate a fixed-sized input. Given that our 3D features are in the same feature space as the 2D features by the 3D feature extractor, and that perceiver is able to handle inputs of arbitrary input sizes of the same feature dimension, point cloud features with arbitrary sizes could also be fed into the perceiver. Therefore, we use the 3D feature extractor to extract the 3D features in the same feature space as the features of the frozen image encoders. Then, we use pretrained 2D VLMs as our backbones, input the aligned 3D features to train 3D-LLMs with our collected 3D-language dataset. 

\vspace{-5pt}
\subsubsection{3D Localization Mechanism}
\vspace{-5pt}
Apart from building 3D features, which can be aligned with language semantics, it's also essential to capture 
3D spatial information. To this end, we propose a 3D localization mechanism that boosts 3D LLMs' abilities to absorb spatial information. It consists of two parts:

\textbf{Augmenting 3D features with position embeddings}
Besides the 3D features aggregated from 2D multi-view features, we also add position embeddings to the features. Suppose the feature dim is $\mathcal{D}_v$. We generate sin/cos position embeddings of the three dimensions, each has an embedding size $\mathcal{D}_v / 3$. We concatenate the embeddings of all three dimensions, and concatenate them to the 3D features.

\textbf{Augmenting LLM vocabularies with location tokens}
In order to align 3D spatial locations with LLMs, we propose to embed 3D locations in the vocabularies, following \cite{Chen2021Pix2seqAL} and \cite{Wang2022UnifyingAT}. To be specific, the region to be grounded can be denoted as a sequence of discrete tokens representing the bounding box in the form of AABB. The continuous corner coordinates of the bounding boxes are uniformly discretized to voxel integers as location tokens $\left\langle x_{min}, y_{min}, z_{min}, x_{max}, y_{max}, z_{max}\right\rangle$. After adding these additional location tokens, we unfreeze the weights for these tokens in the input and output embeddings of language models.


