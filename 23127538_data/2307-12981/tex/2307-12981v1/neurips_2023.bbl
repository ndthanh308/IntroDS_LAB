\begin{thebibliography}{10}

\bibitem{Achlioptas2020ReferIt3DNL}
P.~Achlioptas, A.~Abdelreheem, F.~Xia, M.~Elhoseiny, and L.~J. Guibas.
\newblock {ReferIt3D}: Neural listeners for fine-grained {3D} object
  identification in real-world scenes.
\newblock In {\em ECCV}, 2020.

\bibitem{anas_awadalla_2023_7733589}
A.~Awadalla, I.~Gao, J.~Gardner, J.~Hessel, Y.~Hanafy, W.~Zhu, K.~Marathe,
  Y.~Bitton, S.~Gadre, J.~Jitsev, S.~Kornblith, P.~W. Koh, G.~Ilharco,
  M.~Wortsman, and L.~Schmidt.
\newblock Openflamingo, Mar. 2023.

\bibitem{Azuma2022ScanQA3Q}
D.~Azuma, T.~Miyanishi, S.~Kurita, and M.~Kawanabe.
\newblock {ScanQA}: {3D} question answering for spatial scene understanding.
\newblock {\em 2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 19107--19117, 2022.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, pages
  1877--1901, 2020.

\bibitem{chen2020scanrefer}
D.~Z. Chen, A.~X. Chang, and M.~Nie{\ss}ner.
\newblock {ScanRefer}: {3D} object localization in {RGB-D} scans using natural
  language.
\newblock {\em 16th European Conference on Computer Vision (ECCV)}, 2020.

\bibitem{Chen2021Pix2seqAL}
T.~Chen, S.~Saxena, L.~Li, D.~J. Fleet, and G.~E. Hinton.
\newblock Pix2seq: A language modeling framework for object detection.
\newblock {\em ArXiv}, abs/2109.10852, 2021.

\bibitem{chen2015microsoft}
X.~Chen, H.~Fang, T.-Y. Lin, R.~Vedantam, S.~Gupta, P.~Doll{\'a}r, and C.~L.
  Zitnick.
\newblock Microsoft coco captions: Data collection and evaluation server.
\newblock {\em arXiv preprint arXiv:1504.00325}, 2015.

\bibitem{chen2021scan2cap}
Z.~Chen, A.~Gholami, M.~Nie{\ss}ner, and A.~X. Chang.
\newblock Scan2cap: Context-aware dense captioning in rgb-d scans.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3193--3203, 2021.

\bibitem{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{Dai2017ScanNetR3}
A.~Dai, A.~X. Chang, M.~Savva, M.~Halber, T.~A. Funkhouser, and M.~Nie{\ss}ner.
\newblock {ScanNet}: Richly-annotated {3D} reconstructions of indoor scenes.
\newblock {\em 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 2432--2443, 2017.

\bibitem{dollyblog}
Databricks.
\newblock Free dolly: Introducing the world's first truly open
  instruction-tuned llm, 2023.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{Alayrac2022Flamingo}
J.-B.~A. et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock 2022.

\bibitem{Feng2021FreeformDG}
M.~Feng, Z.~Li, Q.~Li, L.~Zhang, X.~Zhang, G.~Zhu, H.~Zhang, Y.~Wang, and A.~S.
  Mian.
\newblock Free-form description guided 3d visual graph network for object
  grounding in point cloud.
\newblock {\em 2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 3702--3711, 2021.

\bibitem{Gadre2022CLIPOW}
S.~Y. Gadre, M.~Wortsman, G.~Ilharco, L.~Schmidt, and S.~Song.
\newblock {CLIP} on wheels: Zero-shot object navigation as object localization
  and exploration.
\newblock {\em ArXiv}, abs/2203.10421, 2022.

\bibitem{gong2023multimodal}
T.~Gong, C.~Lyu, S.~Zhang, Y.~Wang, M.~Zheng, Q.~Zhao, K.~Liu, W.~Zhang,
  P.~Luo, and K.~Chen.
\newblock {MultiModal-GPT}: A vision and language model for dialogue with
  humans.
\newblock {\em arXiv preprint arXiv:2305.04790}, 2023.

\bibitem{balanced_vqa_v2}
Y.~Goyal, T.~Khot, D.~Summers{-}Stay, D.~Batra, and D.~Parikh.
\newblock Making the {V} in {VQA} matter: Elevating the role of image
  understanding in {V}isual {Q}uestion {A}nswering.
\newblock In {\em Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem{hong20223d}
Y.~Hong, Y.~Du, C.~Lin, J.~Tenenbaum, and C.~Gan.
\newblock 3d concept grounding on neural fields.
\newblock {\em Advances in Neural Information Processing Systems},
  35:7769--7782, 2022.

\bibitem{hong20233d}
Y.~Hong, C.~Lin, Y.~Du, Z.~Chen, J.~B. Tenenbaum, and C.~Gan.
\newblock {3D} concept learning and reasoning from multi-view images, 2023.

\bibitem{hong2023threedclr}
Y.~Hong, C.~Lin, Y.~Du, Z.~Chen, J.~B. Tenenbaum, and C.~Gan.
\newblock 3d concept learning and reasoning from multi-view images.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, 2023.

\bibitem{hu2016natural}
R.~Hu, H.~Xu, M.~Rohrbach, J.~Feng, K.~Saenko, and T.~Darrell.
\newblock Natural language object retrieval, 2016.

\bibitem{huang2023visual}
C.~Huang, O.~Mees, A.~Zeng, and W.~Burgard.
\newblock Visual language maps for robot navigation, 2023.

\bibitem{Huang2021TextGuidedGN}
P.-H. Huang, H.-H. Lee, H.-T. Chen, and T.-L. Liu.
\newblock Text-guided graph neural networks for referring {3D} instance
  segmentation.
\newblock In {\em AAAI}, 2021.

\bibitem{Jaegle2021PerceiverGP}
A.~Jaegle, F.~Gimeno, A.~Brock, A.~Zisserman, O.~Vinyals, and J.~Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{jatavallabhula2023conceptfusion}
K.~M. Jatavallabhula, A.~Kuwajerwala, Q.~Gu, M.~Omama, T.~Chen, S.~Li, G.~Iyer,
  S.~Saryazdi, N.~Keetha, A.~Tewari, J.~B. Tenenbaum, C.~M. de~Melo,
  M.~Krishna, L.~Paull, F.~Shkurti, and A.~Torralba.
\newblock Conceptfusion: Open-set multimodal {3D} mapping, 2023.

\bibitem{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung,
  Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021.

\bibitem{gradslam}
J.~{Krishna Murthy}, S.~Saryazdi, G.~Iyer, and L.~Paull.
\newblock gradslam: Dense slam meets automatic differentiation.
\newblock {\em arXiv}, 2020.

\bibitem{li2022lavis}
D.~Li, J.~Li, H.~Le, G.~Wang, S.~Savarese, and S.~C.~H. Hoi.
\newblock {LAVIS}: A library for language-vision intelligence, 2022.

\bibitem{li2023blip2}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock {BLIP-2}: Bootstrapping language-image pre-training with frozen image
  encoders and large language models, 2023.

\bibitem{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock {BLIP-2}: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{liu2023visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2304.08485}, 2023.

\bibitem{openai2023gpt4}
OpenAI.
\newblock {GPT}-4 technical report, 2023.

\bibitem{OpenAI2023GPT4TR}
OpenAI.
\newblock {GPT-4} technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{Qi2019DeepHV}
C.~Qi, O.~Litany, K.~He, and L.~J. Guibas.
\newblock Deep hough voting for 3d object detection in point clouds.
\newblock {\em 2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 9276--9285, 2019.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551,
  2020.

\bibitem{ramakrishnan2021habitat}
S.~K. Ramakrishnan, A.~Gokaslan, E.~Wijmans, O.~Maksymets, A.~Clegg, J.~Turner,
  E.~Undersander, W.~Galuba, A.~Westbury, A.~X. Chang, et~al.
\newblock Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d
  environments for embodied ai.
\newblock {\em arXiv preprint arXiv:2109.08238}, 2021.

\bibitem{ramakrishnan2021hm3d}
S.~K. Ramakrishnan, A.~Gokaslan, E.~Wijmans, O.~Maksymets, A.~Clegg, J.~M.
  Turner, E.~Undersander, W.~Galuba, A.~Westbury, A.~X. Chang, M.~Savva,
  Y.~Zhao, and D.~Batra.
\newblock Habitat-matterport {3D} dataset ({HM3D}): 1000 large-scale {3D}
  environments for embodied {AI}.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2021.

\bibitem{savva2019habitat}
M.~Savva, A.~Kadian, O.~Maksymets, Y.~Zhao, E.~Wijmans, B.~Jain, J.~Straub,
  J.~Liu, V.~Koltun, J.~Malik, et~al.
\newblock Habitat: A platform for embodied ai research.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9339--9347, 2019.

\bibitem{Sun_2022_CVPR}
C.~Sun, M.~Sun, and H.-T. Chen.
\newblock Direct voxel grid optimization: Super-fast convergence for radiance
  fields reconstruction.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 5459--5469, June 2022.

\bibitem{sun2023principle}
Z.~Sun, Y.~Shen, Q.~Zhou, H.~Zhang, Z.~Chen, D.~Cox, Y.~Yang, and C.~Gan.
\newblock Principle-driven self-alignment of language models from scratch with
  minimal human supervision.
\newblock {\em arXiv e-prints}, pages arXiv--2305, 2023.

\bibitem{Wang2022UnifyingAT}
P.~Wang, A.~Yang, R.~Men, J.~Lin, S.~Bai, Z.~Li, J.~Ma, C.~Zhou, J.~Zhou, and
  H.~Yang.
\newblock Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock In {\em International Conference on Machine Learning}, 2022.

\bibitem{wijmans2019dd}
E.~Wijmans, A.~Kadian, A.~Morcos, S.~Lee, I.~Essa, D.~Parikh, M.~Savva, and
  D.~Batra.
\newblock Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion
  frames.
\newblock {\em arXiv preprint arXiv:1911.00357}, 2019.

\bibitem{yadav2022habitat}
K.~Yadav, R.~Ramrakhya, S.~K. Ramakrishnan, T.~Gervet, J.~Turner, A.~Gokaslan,
  N.~Maestre, A.~X. Chang, D.~Batra, M.~Savva, et~al.
\newblock Habitat-matterport {3D} semantics dataset.
\newblock {\em arXiv preprint arXiv:2210.05633}, 2022.

\bibitem{yang2019fast}
Z.~Yang, B.~Gong, L.~Wang, W.~Huang, D.~Yu, and J.~Luo.
\newblock A fast and accurate one-stage approach to visual grounding, 2019.

\bibitem{Ye20213DQA}
S.~Ye, D.~Chen, S.~Han, and J.~Liao.
\newblock {3D} question answering.
\newblock {\em IEEE transactions on visualization and computer graphics}, PP,
  2021.

\bibitem{yu2016modeling}
L.~Yu, P.~Poirson, S.~Yang, A.~C. Berg, and T.~L. Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 69--85. Springer, 2016.

\bibitem{balanced_binary_vqa}
P.~Zhang, Y.~Goyal, D.~Summers{-}Stay, D.~Batra, and D.~Parikh.
\newblock {Y}in and {Y}ang: Balancing and answering binary visual questions.
\newblock In {\em Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem{zhu2023chatgpt}
D.~Zhu, J.~Chen, K.~Haydarov, X.~Shen, W.~Zhang, and M.~Elhoseiny.
\newblock Chatgpt asks, blip-2 answers: Automatic questioning towards enriched
  visual descriptions, 2023.

\end{thebibliography}
