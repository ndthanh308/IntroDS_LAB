% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{Ji_2023,
	year = 2023,
	author = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
	title = {Survey of Hallucination in Natural Language Generation},
	journal = {{ACM} Computing Surveys}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{stahlberg2020neural,
  title={Neural machine translation: A review},
  author={Stahlberg, Felix},
  journal={Journal of Artificial Intelligence Research},
  volume={69},
  pages={343--418},
  year={2020}
}

@article{huang2020challenges,
  title={Challenges in building intelligent open-domain dialog systems},
  author={Huang, Minlie and Zhu, Xiaoyan and Gao, Jianfeng},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={38},
  number={3},
  pages={1--32},
  year={2020},
  publisher={ACM New York, NY, USA}
}


@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}


@article{yang2023watermarking,
  title={Watermarking Text Generated by Black-Box Language Models},
  author={Yang, Xi and Chen, Kejiang and Zhang, Weiming and Liu, Chang and Qi, Yuang and Zhang, Jie and Fang, Han and Yu, Nenghai},
  journal={arXiv preprint arXiv:2305.08883},
  year={2023}
}

@article{tan2023evaluation,
  title={Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions},
  author={Tan, Yiming and Min, Dehai and Li, Yu and Li, Wenbo and Hu, Nan and Chen, Yongrui and Qi, Guilin},
  journal={arXiv preprint arXiv:2303.07992},
  year={2023}
}

@article{dong2023self,
  title={Self-collaboration Code Generation via ChatGPT},
  author={Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
  journal={arXiv preprint arXiv:2304.07590},
  year={2023}
}

@article{liu2023your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={arXiv preprint arXiv:2305.01210},
  year={2023}
}

@article{bian2023chatgpt,
  title={Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models},
  author={Bian, Ning and Han, Xianpei and Sun, Le and Lin, Hongyu and Lu, Yaojie and He, Ben},
  journal={arXiv preprint arXiv:2303.16421},
  year={2023}
}

@article{alkaissi2023artificial,
  title={Artificial hallucinations in ChatGPT: implications in scientific writing},
  author={Alkaissi, Hussam and McFarlane, Samy I},
  journal={Cureus},
  volume={15},
  number={2},
  year={2023},
  publisher={Cureus}
}

@article{jawahar2020automatic,
   title={Automatic detection of machine generated text: A critical survey},
   author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
   journal={arXiv preprint arXiv:2011.01314},
   year={2020}
}

@article{mitchell2023detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2301.11305},
  year={2023}
}

@article{mitrovic2023chatgpt,
  title={Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text},
  author={Mitrovi{\'c}, Sandra and Andreoletti, Davide and Ayoub, Omran},
  journal={arXiv preprint arXiv:2301.13852},
  year={2023}
}

@article{solaiman2019release,
  title={Release strategies and the social impacts of language models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@inproceedings{uchendu2020authorship,
  title={Authorship attribution for neural text generation},
  author={Uchendu, Adaku and Le, Thai and Shu, Kai and Lee, Dongwon},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={8384--8395},
  year={2020}
}

@article{bakhtin2019real,
  title={Real or fake? learning to discriminate machine from human generated text},
  author={Bakhtin, Anton and Gross, Sam and Ott, Myle and Deng, Yuntian and Ranzato, Marc'Aurelio and Szlam, Arthur},
  journal={arXiv preprint arXiv:1906.03351},
  year={2019}
}



@article{sadasivan2023can,
  title={Can ai-generated text be reliably detected?},
  author={Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
  journal={arXiv preprint arXiv:2303.11156},
  year={2023}
}

@article{frohling2021feature,
  title={Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover},
  author={Fr{\"o}hling, Leon and Zubiaga, Arkaitz},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e443},
  year={2021},
  publisher={PeerJ Inc.}
}




@misc{openai2021chatgpt,
  author = {OpenAI},
  title = {{Chatgpt}: Optimizing language model for dialogue},
  howpublished = {\url{https://www.openai.com/blog/chatgpt/}},
  year = {2021},
  note = {Accessed: 2023-01-10}
}



@article{lazaridou2022internet,
  title={Internet-augmented language models through few-shot prompting for open-domain question answering},
  author={Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
  journal={arXiv preprint arXiv:2203.05115},
  year={2022}
}

@inproceedings{xu2022sequence,
  title={Sequence level contrastive learning for text summarization},
  author={Xu, Shusheng and Zhang, Xingxing and Wu, Yi and Wei, Furu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11556--11565},
  year={2022}
}

@article{goyal2022news,
  title={News summarization and evaluation in the era of gpt-3},
  author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
  journal={arXiv preprint arXiv:2209.12356},
  year={2022}
}



@inproceedings{keymanesh-etal-2022-makes,
    title = "What Makes Data-to-Text Generation Hard for Pretrained Language Models?",
    author = "Keymanesh, Moniba  and
      Benton, Adrian  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gem-1.50",
    pages = "539--554",
    abstract = "Expressing natural language descriptions of structured facts or relations {--} data-to-text generation (D2T) {--} increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models (PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T. In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how the data is incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.",
}

@inproceedings{vaithilingam2022expectation,
  title={Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models},
  author={Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L},
  booktitle={Chi conference on human factors in computing systems extended abstracts},
  pages={1--7},
  year={2022}
}


@article{zhang2023self,
  title={Self-Edit: Fault-Aware Code Editor for Code Generation},
  author={Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi},
  journal={arXiv preprint arXiv:2305.04087},
  year={2023}
}