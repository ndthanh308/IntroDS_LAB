\begin{thebibliography}{37}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Alkaissi and McFarlane(2023)}]{alkaissi2023artificial}
Hussam Alkaissi and Samy~I McFarlane. 2023.
\newblock Artificial hallucinations in chatgpt: implications in scientific
  writing.
\newblock \emph{Cureus}, 15(2).

\bibitem[{Bakhtin et~al.(2019)Bakhtin, Gross, Ott, Deng, Ranzato, and
  Szlam}]{bakhtin2019real}
Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and
  Arthur Szlam. 2019.
\newblock Real or fake? learning to discriminate machine from human generated
  text.
\newblock \emph{arXiv preprint arXiv:1906.03351}.

\bibitem[{Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell}]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell. 2021.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pages 610--623.

\bibitem[{Bengio et~al.(2000)Bengio, Ducharme, and Vincent}]{bengio2000neural}
Yoshua Bengio, R{\'e}jean Ducharme, and Pascal Vincent. 2000.
\newblock A neural probabilistic language model.
\newblock \emph{Advances in neural information processing systems}, 13.

\bibitem[{Bian et~al.(2023)Bian, Han, Sun, Lin, Lu, and He}]{bian2023chatgpt}
Ning Bian, Xianpei Han, Le~Sun, Hongyu Lin, Yaojie Lu, and Ben He. 2023.
\newblock Chatgpt is a knowledgeable but inexperienced solver: An investigation
  of commonsense problem in large language models.
\newblock \emph{arXiv preprint arXiv:2303.16421}.

\bibitem[{Chen et~al.(2020)Chen, Liu, Zhong, Dou, Wang, Qiu, and
  Huang}]{chen-etal-2020-cdevalsumm}
Yiran Chen, Pengfei Liu, Ming Zhong, Zi-Yi Dou, Danqing Wang, Xipeng Qiu, and
  Xuanjing Huang. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.findings-emnlp.329}
  {{CDE}val{S}umm: An empirical study of cross-dataset evaluation for neural
  summarization systems}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3679--3691, Online. Association for Computational
  Linguistics.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma et~al.}]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}.

\bibitem[{Dong et~al.(2023)Dong, Jiang, Jin, and Li}]{dong2023self}
Yihong Dong, Xue Jiang, Zhi Jin, and Ge~Li. 2023.
\newblock Self-collaboration code generation via chatgpt.
\newblock \emph{arXiv preprint arXiv:2304.07590}.

\bibitem[{Fr{\"o}hling and Zubiaga(2021)}]{frohling2021feature}
Leon Fr{\"o}hling and Arkaitz Zubiaga. 2021.
\newblock Feature-based detection of automated language models: tackling gpt-2,
  gpt-3 and grover.
\newblock \emph{PeerJ Computer Science}, 7:e443.

\bibitem[{Gardent et~al.(2017)Gardent, Shimorina, Narayan, and
  Perez-Beltrachini}]{gardent-etal-2017-webnlg}
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura
  Perez-Beltrachini. 2017.
\newblock \href {https://doi.org/10.18653/v1/W17-3518} {The {W}eb{NLG}
  challenge: Generating text from {RDF} data}.
\newblock In \emph{Proceedings of the 10th International Conference on Natural
  Language Generation}, pages 124--133, Santiago de Compostela, Spain.
  Association for Computational Linguistics.

\bibitem[{Goyal et~al.(2022)Goyal, Li, and Durrett}]{goyal2022news}
Tanya Goyal, Junyi~Jessy Li, and Greg Durrett. 2022.
\newblock News summarization and evaluation in the era of gpt-3.
\newblock \emph{arXiv preprint arXiv:2209.12356}.

\bibitem[{Huang et~al.(2020)Huang, Zhu, and Gao}]{huang2020challenges}
Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020.
\newblock Challenges in building intelligent open-domain dialog systems.
\newblock \emph{ACM Transactions on Information Systems (TOIS)}, 38(3):1--32.

\bibitem[{Jawahar et~al.(2020)Jawahar, Abdul-Mageed, and
  Lakshmanan}]{jawahar-etal-2020-automatic}
Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.coling-main.208} {Automatic
  detection of machine generated text: A critical survey}.
\newblock In \emph{Proceedings of the 28th International Conference on
  Computational Linguistics}, pages 2296--2309, Barcelona, Spain (Online).
  International Committee on Computational Linguistics.

\bibitem[{Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen,
  and Yih}]{karpukhin-etal-2020-dense}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
  Edunov, Danqi Chen, and Wen-tau Yih. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.550} {Dense
  passage retrieval for open-domain question answering}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6769--6781, Online. Association
  for Computational Linguistics.

\bibitem[{Keymanesh et~al.(2022)Keymanesh, Benton, and
  Dredze}]{keymanesh-etal-2022-makes}
Moniba Keymanesh, Adrian Benton, and Mark Dredze. 2022.
\newblock \href {https://aclanthology.org/2022.gem-1.50} {What makes
  data-to-text generation hard for pretrained language models?}
\newblock In \emph{Proceedings of the 2nd Workshop on Natural Language
  Generation, Evaluation, and Metrics (GEM)}, pages 539--554, Abu Dhabi, United
  Arab Emirates (Hybrid). Association for Computational Linguistics.

\bibitem[{Kirchenbauer et~al.(2023)Kirchenbauer, Geiping, Wen, Katz, Miers, and
  Goldstein}]{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom
  Goldstein. 2023.
\newblock A watermark for large language models.
\newblock \emph{arXiv preprint arXiv:2301.10226}.

\bibitem[{Lazaridou et~al.(2022)Lazaridou, Gribovskaya, Stokowiec, and
  Grigorev}]{lazaridou2022internet}
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai
  Grigorev. 2022.
\newblock Internet-augmented language models through few-shot prompting for
  open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2203.05115}.

\bibitem[{Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.703} {{BART}:
  Denoising sequence-to-sequence pre-training for natural language generation,
  translation, and comprehension}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7871--7880, Online. Association for
  Computational Linguistics.

\bibitem[{Lin(2004)}]{lin-2004-rouge}
Chin-Yew Lin. 2004.
\newblock \href {https://aclanthology.org/W04-1013} {{ROUGE}: A package for
  automatic evaluation of summaries}.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2023)Liu, Xia, Wang, and Zhang}]{liu2023your}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang. 2023.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation
  of large language models for code generation.
\newblock \emph{arXiv preprint arXiv:2305.01210}.

\bibitem[{Mitchell et~al.(2023)Mitchell, Lee, Khazatsky, Manning, and
  Finn}]{mitchell2023detectgpt}
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher~D Manning, and
  Chelsea Finn. 2023.
\newblock Detectgpt: Zero-shot machine-generated text detection using
  probability curvature.
\newblock \emph{arXiv preprint arXiv:2301.11305}.

\bibitem[{Mitrovi{\'c} et~al.(2023)Mitrovi{\'c}, Andreoletti, and
  Ayoub}]{mitrovic2023chatgpt}
Sandra Mitrovi{\'c}, Davide Andreoletti, and Omran Ayoub. 2023.
\newblock Chatgpt or human? detect and explain. explaining decisions of machine
  learning model for detecting short chatgpt-generated text.
\newblock \emph{arXiv preprint arXiv:2301.13852}.

\bibitem[{Nan et~al.(2021)Nan, Radev, Zhang, Rau, Sivaprasad, Hsieh, Tang,
  Vyas, Verma, Krishna, Liu, Irwanto, Pan, Rahman, Zaidi, Mutuma, Tarabar,
  Gupta, Yu, Tan, Lin, Xiong, Socher, and Rajani}]{nan-etal-2021-dart}
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad,
  Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,
  Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi,
  Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi~Chern Tan,
  Xi~Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen~Fatema Rajani.
  2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.37} {{DART}:
  Open-domain structured data record to text generation}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 432--447, Online. Association for Computational
  Linguistics.

\bibitem[{Narayan et~al.(2018)Narayan, Cohen, and
  Lapata}]{narayan-etal-2018-dont}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1206} {Don{'}t give me the
  details, just the summary! topic-aware convolutional neural networks for
  extreme summarization}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807, Brussels, Belgium.
  Association for Computational Linguistics.

\bibitem[{OpenAI(2021)}]{openai2021chatgpt}
OpenAI. 2021.
\newblock {Chatgpt}: Optimizing language model for dialogue.
\newblock \url{https://www.openai.com/blog/chatgpt/}.
\newblock Accessed: 2023-01-10.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock \href {http://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{papineni-etal-2002-bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock \href {https://doi.org/10.3115/1073083.1073135} {{B}leu: a method for
  automatic evaluation of machine translation}.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics}, pages 311--318, Philadelphia, Pennsylvania,
  USA. Association for Computational Linguistics.

\bibitem[{Sadasivan et~al.(2023)Sadasivan, Kumar, Balasubramanian, Wang, and
  Feizi}]{sadasivan2023can}
Vinu~Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and
  Soheil Feizi. 2023.
\newblock Can ai-generated text be reliably detected?
\newblock \emph{arXiv preprint arXiv:2303.11156}.

\bibitem[{Schuster et~al.(2020)Schuster, Schuster, Shah, and
  Barzilay}]{schuster-etal-2020-limitations}
Tal Schuster, Roei Schuster, Darsh~J. Shah, and Regina Barzilay. 2020.
\newblock \href {https://doi.org/10.1162/coli_a_00380} {The limitations of
  stylometry for detecting machine-generated fake news}.
\newblock \emph{Computational Linguistics}, 46(2):499--510.

\bibitem[{See et~al.(2017)See, Liu, and Manning}]{see-etal-2017-get}
Abigail See, Peter~J. Liu, and Christopher~D. Manning. 2017.
\newblock \href {https://doi.org/10.18653/v1/P17-1099} {Get to the point:
  Summarization with pointer-generator networks}.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1073--1083,
  Vancouver, Canada. Association for Computational Linguistics.

\bibitem[{Solaiman et~al.(2019)Solaiman, Brundage, Clark, Askell, Herbert-Voss,
  Wu, Radford, Krueger, Kim, Kreps et~al.}]{solaiman2019release}
Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss,
  Jeff Wu, Alec Radford, Gretchen Krueger, Jong~Wook Kim, Sarah Kreps, et~al.
  2019.
\newblock Release strategies and the social impacts of language models.
\newblock \emph{arXiv preprint arXiv:1908.09203}.

\bibitem[{Stahlberg(2020)}]{stahlberg2020neural}
Felix Stahlberg. 2020.
\newblock Neural machine translation: A review.
\newblock \emph{Journal of Artificial Intelligence Research}, 69:343--418.

\bibitem[{Tan et~al.(2023)Tan, Min, Li, Li, Hu, Chen, and
  Qi}]{tan2023evaluation}
Yiming Tan, Dehai Min, Yu~Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi.
  2023.
\newblock Evaluation of chatgpt as a question answering system for answering
  complex questions.
\newblock \emph{arXiv preprint arXiv:2303.07992}.

\bibitem[{Vaithilingam et~al.(2022)Vaithilingam, Zhang, and
  Glassman}]{vaithilingam2022expectation}
Priyan Vaithilingam, Tianyi Zhang, and Elena~L Glassman. 2022.
\newblock Expectation vs. experience: Evaluating the usability of code
  generation tools powered by large language models.
\newblock In \emph{Chi conference on human factors in computing systems
  extended abstracts}, pages 1--7.

\bibitem[{Xu et~al.(2022)Xu, Zhang, Wu, and Wei}]{xu2022sequence}
Shusheng Xu, Xingxing Zhang, Yi~Wu, and Furu Wei. 2022.
\newblock Sequence level contrastive learning for text summarization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 11556--11565.

\bibitem[{Yang et~al.(2023)Yang, Chen, Zhang, Liu, Qi, Zhang, Fang, and
  Yu}]{yang2023watermarking}
Xi~Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang,
  and Nenghai Yu. 2023.
\newblock Watermarking text generated by black-box language models.
\newblock \emph{arXiv preprint arXiv:2305.08883}.

\bibitem[{Zhang et~al.(2023)Zhang, Li, Li, Li, and Jin}]{zhang2023self}
Kechi Zhang, Zhuo Li, Jia Li, Ge~Li, and Zhi Jin. 2023.
\newblock Self-edit: Fault-aware code editor for code generation.
\newblock \emph{arXiv preprint arXiv:2305.04087}.

\end{thebibliography}
