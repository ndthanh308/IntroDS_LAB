% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{deshpande2023toxicity,
  title={Toxicity in chatgpt: Analyzing persona-assigned language models},
  author={Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2304.05335},
  year={2023}
}

@article{Ji_2023,
	year = 2023,
	author = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
	title = {Survey of Hallucination in Natural Language Generation},
	journal = {{ACM} Computing Surveys}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{stahlberg2020neural,
  title={Neural machine translation: A review},
  author={Stahlberg, Felix},
  journal={Journal of Artificial Intelligence Research},
  volume={69},
  pages={343--418},
  year={2020}
}

@article{huang2020challenges,
  title={Challenges in building intelligent open-domain dialog systems},
  author={Huang, Minlie and Zhu, Xiaoyan and Gao, Jianfeng},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={38},
  number={3},
  pages={1--32},
  year={2020},
  publisher={ACM New York, NY, USA}
}


@misc{chung2022scaling,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@misc{rasooli2015yara,
      title={Yara Parser: A Fast and Accurate Dependency Parser}, 
      author={Mohammad Sadegh Rasooli and Joel Tetreault},
      year={2015},
      eprint={1503.06733},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@misc{kirchenbauer2023watermark,
      title={A Watermark for Large Language Models}, 
      author={John Kirchenbauer and Jonas Geiping and Yuxin Wen and Jonathan Katz and Ian Miers and Tom Goldstein},
      year={2023},
      eprint={2301.10226},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yang2023watermarking,
      title={Watermarking Text Generated by Black-Box Language Models}, 
      author={Xi Yang and Kejiang Chen and Weiming Zhang and Chang Liu and Yuang Qi and Jie Zhang and Han Fang and Nenghai Yu},
      year={2023},
      eprint={2305.08883},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{tan2023chatgpt,
      title={Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of GPT family LLMs' Question Answering Performance}, 
      author={Yiming Tan and Dehai Min and Yu Li and Wenbo Li and Nan Hu and Yongrui Chen and Guilin Qi},
      year={2023},
      eprint={2303.07992},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{dong2023selfcollaboration,
      title={Self-collaboration Code Generation via ChatGPT}, 
      author={Yihong Dong and Xue Jiang and Zhi Jin and Ge Li},
      year={2023},
      eprint={2304.07590},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{liu2023code,
      title={Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation}, 
      author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and Lingming Zhang},
      year={2023},
      eprint={2305.01210},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{bian2023chatgpt,
  title={Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models},
  author={Bian, Ning and Han, Xianpei and Sun, Le and Lin, Hongyu and Lu, Yaojie and He, Ben},
  journal={arXiv preprint arXiv:2303.16421},
  year={2023}
}

@article{alkaissi2023artificial,
  title={Artificial hallucinations in ChatGPT: implications in scientific writing},
  author={Alkaissi, Hussam and McFarlane, Samy I},
  journal={Cureus},
  volume={15},
  number={2},
  year={2023},
  publisher={Cureus}
}




@misc{mitchell2023detectgpt,
      title={DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature}, 
      author={Eric Mitchell and Yoonho Lee and Alexander Khazatsky and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2301.11305},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mitrović2023chatgpt,
      title={ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text}, 
      author={Sandra Mitrović and Davide Andreoletti and Omran Ayoub},
      year={2023},
      eprint={2301.13852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{solaiman2019release,
      title={Release Strategies and the Social Impacts of Language Models}, 
      author={Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Gretchen Krueger and Jong Wook Kim and Sarah Kreps and Miles McCain and Alex Newhouse and Jason Blazakis and Kris McGuffie and Jasmine Wang},
      year={2019},
      eprint={1908.09203},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{uchendu2020authorship,
  title={Authorship attribution for neural text generation},
  author={Uchendu, Adaku and Le, Thai and Shu, Kai and Lee, Dongwon},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={8384--8395},
  year={2020}
}

@misc{bakhtin2019real,
      title={Real or Fake? Learning to Discriminate Machine from Human Generated Text}, 
      author={Anton Bakhtin and Sam Gross and Myle Ott and Yuntian Deng and Marc'Aurelio Ranzato and Arthur Szlam},
      year={2019},
      eprint={1906.03351},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{sadasivan2023aigenerated,
      title={Can AI-Generated Text be Reliably Detected?}, 
      author={Vinu Sankar Sadasivan and Aounon Kumar and Sriram Balasubramanian and Wenxiao Wang and Soheil Feizi},
      year={2023},
      eprint={2303.11156},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{frohling2021feature,
  title={Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover},
  author={Fr{\"o}hling, Leon and Zubiaga, Arkaitz},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e443},
  year={2021},
  publisher={PeerJ Inc.}
}




@misc{openai2021chatgpt,
  author = {OpenAI},
  title = {{Chatgpt}: Optimizing language model for dialogue},
  howpublished = {\url{https://www.openai.com/blog/chatgpt/}},
  year = {2021},
  note = {Accessed: 2023-01-10}
}


@misc{lazaridou2022internetaugmented,
      title={Internet-augmented language models through few-shot prompting for open-domain question answering}, 
      author={Angeliki Lazaridou and Elena Gribovskaya and Wojciech Stokowiec and Nikolai Grigorev},
      year={2022},
      eprint={2203.05115},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xu2022sequence,
  title={Sequence level contrastive learning for text summarization},
  author={Xu, Shusheng and Zhang, Xingxing and Wu, Yi and Wei, Furu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11556--11565},
  year={2022}
}

@misc{goyal2023news,
      title={News Summarization and Evaluation in the Era of GPT-3}, 
      author={Tanya Goyal and Junyi Jessy Li and Greg Durrett},
      year={2023},
      eprint={2209.12356},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings{keymanesh-etal-2022-makes,
    title = "What Makes Data-to-Text Generation Hard for Pretrained Language Models?",
    author = "Keymanesh, Moniba  and
      Benton, Adrian  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gem-1.50",
    pages = "539--554",
    abstract = "Expressing natural language descriptions of structured facts or relations {--} data-to-text generation (D2T) {--} increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models (PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T. In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how the data is incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.",
}

@inproceedings{vaithilingam2022expectation,
  title={Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models},
  author={Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L},
  booktitle={Chi conference on human factors in computing systems extended abstracts},
  pages={1--7},
  year={2022}
}


@misc{zhang2023selfedit,
      title={Self-Edit: Fault-Aware Code Editor for Code Generation}, 
      author={Kechi Zhang and Zhuo Li and Jia Li and Ge Li and Zhi Jin},
      year={2023},
      eprint={2305.04087},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lee2023wrote,
      title={Who Wrote this Code? Watermarking for Code Generation}, 
      author={Taehyun Lee and Seokhee Hong and Jaewoo Ahn and Ilgee Hong and Hwaran Lee and Sangdoo Yun and Jamin Shin and Gunhee Kim},
      year={2023},
      eprint={2305.15060},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yoo-etal-2023-robust,
    title = "Robust Multi-bit Natural Language Watermarking through Invariant Features",
    author = "Yoo, KiYoon  and
      Ahn, Wonhyuk  and
      Jang, Jiho  and
      Kwak, Nojun",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.117",
    doi = "10.18653/v1/2023.acl-long.117",
    pages = "2092--2115",
    abstract = "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8{\%} point on average on four datasets, three corruption types, and two corruption ratios",
}

@misc{zhao2023provable,
      title={Provable Robust Watermarking for AI-Generated Text}, 
      author={Xuandong Zhao and Prabhanjan Ananth and Lei Li and Yu-Xiang Wang},
      year={2023},
      eprint={2306.17439},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2023protecting,
      title={Protecting Language Generation Models via Invisible Watermarking}, 
      author={Xuandong Zhao and Yu-Xiang Wang and Lei Li},
      year={2023},
      eprint={2302.03162},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{kuditipudi2023robust,
      title={Robust Distortion-free Watermarks for Language Models}, 
      author={Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang},
      year={2023},
      eprint={2307.15593},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2023codable,
      title={Towards Codable Text Watermarking for Large Language Models}, 
      author={Lean Wang and Wenkai Yang and Deli Chen and Hao Zhou and Yankai Lin and Fandong Meng and Jie Zhou and Xu Sun},
      year={2023},
      eprint={2307.15992},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}