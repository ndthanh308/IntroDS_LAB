\begin{filecontents}{ex_article.aux}
\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
	\global\let\oldcontentsline\contentsline
	\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
	\global\let\oldnewlabel\newlabel
	\gdef\newlabel#1#2{\newlabelxx{#1}#2}
	\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
	\AtEndDocument{\ifx\hyper@anchor\@undefined
		\let\contentsline\oldcontentsline
		\let\newlabel\oldnewlabel
		\fi}
	\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ledoit2022power}
\citation{markowitz1952portfolio}
\citation{cressie2015statistics}
\citation{kaipio2006statistical}
\citation{evensenekf}
\citation{kalmanfilter}
\citation{iglesias2013ensemble}
\citation{ringner2008principal}
\citation{kulis2013metric}
\citation{ledoit2022power}
\citation{ledoit2012nonlinear}
\citation{donohogavishjohnstone}
\citation{bickellevina}
\citation{graphicallasso}
\citation{gasparicohn}
\citation{localizationEnKF}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{giles2015multilevel}
\citation{cliffe_multilevel_2011}
\citation{NME:NME4761}
\citation{PWK16MFMCAsymptotics}
\citation{GORODETSKY2020109257}
\citation{multifidelityReview}
\citation{schaden2020multilevel}
\citation{schaden2021asymptotic}
\citation{giles2015multilevel}
\citation{cliffe_multilevel_2011}
\citation{NME:NME4761}
\citation{bpkwmg}
\citation{schaden2020multilevel}
\citation{bierig2015convergence}
\citation{mycek2019multilevel}
\citation{qian2018multifidelity}
\citation{mlenkf}
\newlabel{sec:relatedWork}{{1.1}{2}{Multifidelity covariance estimation: literature review}{subsection.2}{}}
\newlabel{sec:relatedWork@cref}{{[subsection][1][1]1.1}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Multifidelity covariance estimation: literature review}{2}{subsection.2}\protected@file@percent }
\citation{maurais2023logEuclidean}
\citation{arsigny2006log}
\citation{dolzDataSparseMultilevel2023}
\citation{destouches2023multivariate}
\citation{schaden2020multilevel}
\citation{bhatiaposdef}
\citation{schaden2020multilevel}
\citation{maurais2023logEuclidean}
\citation{bhatiaposdef}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Contributions}{3}{subsection.3}\protected@file@percent }
\newlabel{sec:bg}{{2}{3}{Background}{section.4}{}}
\newlabel{sec:bg@cref}{{[section][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{section.4}\protected@file@percent }
\newlabel{sec:spdManifold}{{2.1}{3}{The manifold of SPD matrices}{subsection.5}{}}
\newlabel{sec:spdManifold@cref}{{[subsection][1][2]2.1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The manifold of SPD matrices}{3}{subsection.5}\protected@file@percent }
\newlabel{eq:spd_ip}{{2.1}{3}{The manifold of SPD matrices}{equation.6}{}}
\newlabel{eq:spd_ip@cref}{{[equation][1][2]2.1}{[1][3][]3}}
\citation{pennec2006intrinsic}
\citation{bhatiaposdef}
\citation{pennec2006intrinsic}
\newlabel{eq:logA}{{2.2}{4}{The manifold of SPD matrices}{equation.7}{}}
\newlabel{eq:logA@cref}{{[equation][2][2]2.2}{[1][4][]4}}
\newlabel{eq:expA}{{2.3}{4}{The manifold of SPD matrices}{equation.8}{}}
\newlabel{eq:expA@cref}{{[equation][3][2]2.3}{[1][4][]4}}
\newlabel{eq:geodesic}{{2.4}{4}{The manifold of SPD matrices}{equation.9}{}}
\newlabel{eq:geodesic@cref}{{[equation][4][2]2.4}{[1][4][]4}}
\newlabel{eq:intrinsicdist}{{2.5}{4}{The manifold of SPD matrices}{equation.10}{}}
\newlabel{eq:intrinsicdist@cref}{{[equation][5][2]2.5}{[1][4][]4}}
\newlabel{sec:intrinsicStatistics}{{2.2}{4}{Statistics on the manifold}{subsection.11}{}}
\newlabel{sec:intrinsicStatistics@cref}{{[subsection][2][2]2.2}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Statistics on the manifold}{4}{subsection.11}\protected@file@percent }
\newlabel{eq:frechet_mean}{{2.6}{4}{Statistics on the manifold}{equation.12}{}}
\newlabel{eq:frechet_mean@cref}{{[equation][6][2]2.6}{[1][4][]4}}
\citation{pennec2006intrinsic}
\citation{pennec2006riemannian}
\newlabel{eq:gammaS_def}{{2.7}{5}{Statistics on the manifold}{equation.13}{}}
\newlabel{eq:gammaS_def@cref}{{[equation][7][2]2.7}{[1][5][]5}}
\newlabel{eq:mdist_def}{{2.8}{5}{Statistics on the manifold}{equation.14}{}}
\newlabel{eq:mdist_def@cref}{{[equation][8][2]2.8}{[1][5][]5}}
\newlabel{sec:formulation}{{3}{5}{Estimator formulation}{section.15}{}}
\newlabel{sec:formulation@cref}{{[section][3][]3}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Estimator formulation}{5}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem setup}{6}{subsection.16}\protected@file@percent }
\newlabel{eq:Sdata}{{3.1}{6}{Problem setup}{equation.18}{}}
\newlabel{eq:Sdata@cref}{{[equation][1][3]3.1}{[1][6][]6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example data \cref  {eq:Sdata} corresponding to $L = 3$ with $F^1 = \{0, 1\}$, $F^2 = \{1, 2\}$, $F^3 = \{1, 2, 3\}$, and $F^4 = \{3\}$. Matrices within the same \textit  {column} of the table have the same mean, ${\bf  E}[S_i^{(k)}] = {\bf  E}[S_i^{(j)}] = \Sigma _i$, while matrices within the same \textit  {row} are statistically coupled with each other, $S_i^{(k)} \not \!\perp \!\!\!\perp S_\ell ^{(k)}$.\relax }}{6}{table.caption.19}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:dataStructure}{{1}{6}{Example data \cref {eq:Sdata} corresponding to $L = 3$ with $F^1 = \{0, 1\}$, $F^2 = \{1, 2\}$, $F^3 = \{1, 2, 3\}$, and $F^4 = \{3\}$. Matrices within the same \textit {column} of the table have the same mean, $\bfE [S_i^{(k)}] = \bfE [S_i^{(j)}] = \Sigma _i$, while matrices within the same \textit {row} are statistically coupled with each other, $S_i^{(k)} \not \!\perp \!\!\!\perp S_\ell ^{(k)}$.\relax }{table.caption.19}{}}
\newlabel{tab:dataStructure@cref}{{[table][1][]1}{[1][6][]6}}
\citation{schaden2020multilevel}
\newlabel{sec:sRV}{{3.2}{7}{Manifold regression estimator}{subsection.20}{}}
\newlabel{sec:sRV@cref}{{[subsection][2][3]3.2}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Manifold regression estimator}{7}{subsection.20}\protected@file@percent }
\newlabel{eq:S_rv}{{3.2}{7}{Manifold regression estimator}{equation.21}{}}
\newlabel{eq:S_rv@cref}{{[equation][2][3]3.2}{[1][7][]7}}
\newlabel{sec:covS}{{3.2.1}{7}{Covariance of $\bfS $}{subsubsection.22}{}}
\newlabel{sec:covS@cref}{{[subsubsection][1][3,2]3.2.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Covariance of ${\bf  S}$}{7}{subsubsection.22}\protected@file@percent }
\newlabel{eq:gammaS}{{3.3}{7}{Covariance of $\bfS $}{equation.23}{}}
\newlabel{eq:gammaS@cref}{{[equation][3][3]3.3}{[1][7][]7}}
\newlabel{eq:gamma_blockDiag}{{3.4}{7}{Covariance of $\bfS $}{equation.24}{}}
\newlabel{eq:gamma_blockDiag@cref}{{[equation][4][3]3.4}{[1][7][]7}}
\citation{smith2005covariance}
\@writefile{thm}{\contentsline {definition}{{Definition}{3.1}{Manifold Regression Multifidelity (MRMF) Covariance Estimator}}{8}{theorem.25}\protected@file@percent }
\newlabel{eq:mdist_min}{{3.5}{8}{Covariance of $\bfS $}{equation.26}{}}
\newlabel{eq:mdist_min@cref}{{[equation][5][3]3.5}{[1][7][]8}}
\newlabel{sec:mdistMin}{{3.1}{8}{Covariance of $\bfS $}{equation.26}{}}
\newlabel{sec:mdistMin@cref}{{[definition][1][3]3.1}{[1][8][]8}}
\newlabel{sec:example_s3}{{3.3}{8}{Running example}{subsection.27}{}}
\newlabel{sec:example_s3@cref}{{[subsection][3][3]3.3}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Running example}{8}{subsection.27}\protected@file@percent }
\newlabel{eq:exampleData}{{3.6}{8}{Running example}{equation.28}{}}
\newlabel{eq:exampleData@cref}{{[equation][6][3]3.6}{[1][8][]8}}
\newlabel{eq:threemat_model}{{3.7}{8}{Running example}{equation.30}{}}
\newlabel{eq:threemat_model@cref}{{[equation][7][3]3.7}{[1][8][]8}}
\newlabel{eq:gamma_blockdiag_ex}{{3.8}{9}{Running example}{equation.31}{}}
\newlabel{eq:gamma_blockdiag_ex@cref}{{[equation][8][3]3.8}{[1][9][]9}}
\newlabel{eq:mdist_min_ex}{{3.9}{9}{Running example}{equation.32}{}}
\newlabel{eq:mdist_min_ex@cref}{{[equation][9][3]3.9}{[1][9][]9}}
\newlabel{sec:analysis}{{4}{9}{Analysis, simplification, and interpretation of the manifold regression estimator}{section.33}{}}
\newlabel{sec:analysis@cref}{{[section][4][]4}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis, simplification, and interpretation of the manifold regression estimator}{9}{section.33}\protected@file@percent }
\citation{bhatiaposdef}
\newlabel{ss:mdistProperties}{{4.1}{10}{Properties of Mahalanobis distance}{subsection.34}{}}
\newlabel{ss:mdistProperties@cref}{{[subsection][1][4]4.1}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Properties of Mahalanobis distance}{10}{subsection.34}\protected@file@percent }
\newlabel{sec:tsAgnostic}{{4.1.1}{10}{Tangent space agnosticism}{subsubsection.35}{}}
\newlabel{sec:tsAgnostic@cref}{{[subsubsection][1][4,1]4.1.1}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Tangent space agnosticism}{10}{subsubsection.35}\protected@file@percent }
\@writefile{thm}{\contentsline {proposition}{{Proposition}{4.1}{}}{10}{theorem.36}\protected@file@percent }
\newlabel{thm:ts_agnostic}{{4.1}{10}{Tangent space agnosticism}{equation.37}{}}
\newlabel{thm:ts_agnostic@cref}{{[proposition][1][4]4.1}{[1][10][]10}}
\newlabel{ss:affine_invariance}{{4.1.2}{10}{Affine invariance}{subsubsection.38}{}}
\newlabel{ss:affine_invariance@cref}{{[subsubsection][2][4,1]4.1.2}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Affine invariance}{10}{subsubsection.38}\protected@file@percent }
\@writefile{thm}{\contentsline {proposition}{{Proposition}{4.2}{}}{10}{theorem.39}\protected@file@percent }
\citation{chevallier2022exponential}
\citation{schwartzmanLognormalDistributionsGeometric2016}
\newlabel{eq:mdist_thm}{{4.2}{11}{Affine invariance}{equation.40}{}}
\newlabel{eq:mdist_thm@cref}{{[equation][2][4]4.2}{[1][10][]11}}
\newlabel{thm:affineinvariance}{{4.2}{11}{Affine invariance}{equation.40}{}}
\newlabel{thm:affineinvariance@cref}{{[proposition][2][4]4.2}{[1][11][]11}}
\newlabel{sec:modelOnTS}{{4.1.3}{11}{Maximum likelihood interpretation}{subsubsection.41}{}}
\newlabel{sec:modelOnTS@cref}{{[subsubsection][3][4,1]4.1.3}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Maximum likelihood interpretation}{11}{subsubsection.41}\protected@file@percent }
\newlabel{eq:tangentspacemodel}{{4.3}{11}{Maximum likelihood interpretation}{equation.42}{}}
\newlabel{eq:tangentspacemodel@cref}{{[equation][3][4]4.3}{[1][11][]11}}
\newlabel{eq:dualmodels}{{4.4}{11}{Maximum likelihood interpretation}{equation.43}{}}
\newlabel{eq:dualmodels@cref}{{[equation][4][4]4.4}{[1][11][]11}}
\@writefile{thm}{\contentsline {proposition}{{Proposition}{4.3}{}}{12}{theorem.44}\protected@file@percent }
\newlabel{eq:gaussianE}{{4.5}{12}{Maximum likelihood interpretation}{equation.45}{}}
\newlabel{eq:gaussianE@cref}{{[equation][5][4]4.5}{[1][12][]12}}
\newlabel{prop:mle}{{4.3}{12}{Maximum likelihood interpretation}{equation.45}{}}
\newlabel{prop:mle@cref}{{[proposition][3][4]4.3}{[1][12][]12}}
\newlabel{sec:example_s4}{{4.2}{12}{Running example}{subsection.46}{}}
\newlabel{sec:example_s4@cref}{{[subsection][2][4]4.2}{[1][12][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Running example}{12}{subsection.46}\protected@file@percent }
\newlabel{eq:mdist_min_ex_tsa}{{4.6}{12}{Running example}{equation.47}{}}
\newlabel{eq:mdist_min_ex_tsa@cref}{{[equation][6][4]4.6}{[1][12][]12}}
\newlabel{eq:mdist_iddata}{{4.7}{12}{Running example}{equation.48}{}}
\newlabel{eq:mdist_iddata@cref}{{[equation][7][4]4.7}{[1][12][]12}}
\newlabel{eq:manifoldmodel}{{4.8}{13}{Running example}{equation.49}{}}
\newlabel{eq:manifoldmodel@cref}{{[equation][8][4]4.8}{[1][13][]13}}
\newlabel{sec:fixSigmalo}{{4.3}{13}{Fixed-$\Sigmalo $ simplification}{subsection.50}{}}
\newlabel{sec:fixSigmalo@cref}{{[subsection][3][4]4.3}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Fixed-${\Sigma _{\rm  lo}}$ simplification}{13}{subsection.50}\protected@file@percent }
\newlabel{eq:fixlo_three}{{4.9}{13}{Fixed-$\Sigmalo $ simplification}{equation.51}{}}
\newlabel{eq:fixlo_three@cref}{{[equation][9][4]4.9}{[1][13][]13}}
\newlabel{eq:twomat_mdist}{{4.10}{13}{Fixed-$\Sigmalo $ simplification}{equation.52}{}}
\newlabel{eq:twomat_mdist@cref}{{[equation][10][4]4.10}{[1][13][]13}}
\citation{pennec2006intrinsic}
\citation{maurais2023logEuclidean}
\newlabel{eq:twoMatrixRV}{{4.11}{14}{Fixed-$\Sigmalo $ simplification}{equation.53}{}}
\newlabel{eq:twoMatrixRV@cref}{{[equation][11][4]4.11}{[1][14][]14}}
\@writefile{thm}{\contentsline {proposition}{{Proposition}{4.4}{}}{14}{theorem.54}\protected@file@percent }
\newlabel{eq:fixlo_mdist}{{4.12}{14}{Fixed-$\Sigmalo $ simplification}{equation.55}{}}
\newlabel{eq:fixlo_mdist@cref}{{[equation][12][4]4.12}{[1][14][]14}}
\newlabel{prop:expectedMdist}{{4.4}{14}{Fixed-$\Sigmalo $ simplification}{equation.55}{}}
\newlabel{prop:expectedMdist@cref}{{[proposition][4][4]4.4}{[1][14][]14}}
\newlabel{sec:mfGeneralGeom}{{4.4}{14}{Multifidelity estimation in general geometries}{subsection.56}{}}
\newlabel{sec:mfGeneralGeom@cref}{{[subsection][4][4]4.4}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Multifidelity estimation in general geometries}{14}{subsection.56}\protected@file@percent }
\@writefile{thm}{\contentsline {proposition}{{Proposition}{4.5}{}}{14}{theorem.57}\protected@file@percent }
\citation{rubinstein1985efficiency}
\citation{kalmanfilter}
\citation{evensenbook}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\newlabel{eq:twomat_reg}{{4.13}{15}{Multifidelity estimation in general geometries}{equation.58}{}}
\newlabel{eq:twomat_reg@cref}{{[equation][13][4]4.13}{[1][14][]15}}
\newlabel{eq:regest_nleq}{{4.14}{15}{Multifidelity estimation in general geometries}{equation.59}{}}
\newlabel{eq:regest_nleq@cref}{{[equation][14][4]4.14}{[1][15][]15}}
\newlabel{prop:nleq}{{4.5}{15}{Multifidelity estimation in general geometries}{equation.59}{}}
\newlabel{prop:nleq@cref}{{[proposition][5][4]4.5}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Interpretation of fixed-${\Sigma _{\rm  lo}}$ estimator as control variates}{15}{subsubsection.60}\protected@file@percent }
\newlabel{eq:reg_as_cv}{{4.15}{15}{Interpretation of fixed-$\Sigmalo $ estimator as control variates}{equation.61}{}}
\newlabel{eq:reg_as_cv@cref}{{[equation][15][4]4.15}{[1][15][]15}}
\@writefile{toc}{\contentsline {paragraph}{Euclidean control variate estimator}{15}{section*.62}\protected@file@percent }
\newlabel{eq:sigmaLCV}{{4.16}{15}{Euclidean control variate estimator}{equation.63}{}}
\newlabel{eq:sigmaLCV@cref}{{[equation][16][4]4.16}{[1][15][]15}}
\newlabel{eq:LCV_rearr}{{4.17}{15}{Euclidean control variate estimator}{equation.64}{}}
\newlabel{eq:LCV_rearr@cref}{{[equation][17][4]4.17}{[1][15][]15}}
\citation{arsigny2006log}
\citation{smith2005covariance}
\citation{schaden2020multilevel}
\citation{schaden2021asymptotic}
\citation{MalagoEtAl2018}
\citation{bhatia2019bures}
\citation{Lin2019}
\@writefile{toc}{\contentsline {paragraph}{Log-linear control variate estimator}{16}{section*.65}\protected@file@percent }
\newlabel{eq:LEMF}{{4.18}{16}{Log-linear control variate estimator}{equation.66}{}}
\newlabel{eq:LEMF@cref}{{[equation][18][4]4.18}{[1][15][]16}}
\newlabel{eq:le_est}{{4.19}{16}{Log-linear control variate estimator}{equation.67}{}}
\newlabel{eq:le_est@cref}{{[equation][19][4]4.19}{[1][16][]16}}
\newlabel{eq:cv_euclidean}{{4.20}{16}{Log-linear control variate estimator}{equation.68}{}}
\newlabel{eq:cv_euclidean@cref}{{[equation][20][4]4.20}{[1][16][]16}}
\newlabel{eq:cv_logEuclidean}{{4.21}{16}{Log-linear control variate estimator}{equation.69}{}}
\newlabel{eq:cv_logEuclidean@cref}{{[equation][21][4]4.21}{[1][16][]16}}
\newlabel{eq:cv_affinvar}{{4.22}{16}{Log-linear control variate estimator}{equation.70}{}}
\newlabel{eq:cv_affinvar@cref}{{[equation][22][4]4.22}{[1][16][]16}}
\@writefile{thm}{\contentsline {theorem}{{Theorem}{4.6}{}}{16}{theorem.71}\protected@file@percent }
\newlabel{prop:BLUEs}{{4.6}{16}{Log-linear control variate estimator}{Item.74}{}}
\newlabel{prop:BLUEs@cref}{{[theorem][6][4]4.6}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Generality of the regression framework}{17}{subsubsection.75}\protected@file@percent }
\newlabel{sec:computation}{{5}{17}{Computational approaches}{section.76}{}}
\newlabel{sec:computation@cref}{{[section][5][]5}{[1][17][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Computational approaches}{17}{section.76}\protected@file@percent }
\newlabel{eq:mdist_min_I}{{5.1}{17}{Computational approaches}{equation.77}{}}
\newlabel{eq:mdist_min_I@cref}{{[equation][1][5]5.1}{[1][17][]17}}
\newlabel{sec:regularization}{{5.1}{17}{Regularization in the intrinsic metric}{subsection.78}{}}
\newlabel{sec:regularization@cref}{{[subsection][1][5]5.1}{[1][17][]17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Regularization in the intrinsic metric}{17}{subsection.78}\protected@file@percent }
\newlabel{eq:mdist_min_reg}{{5.2}{17}{Regularization in the intrinsic metric}{equation.79}{}}
\newlabel{eq:mdist_min_reg@cref}{{[equation][2][5]5.2}{[1][17][]17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Regularization parameter selection}{17}{subsubsection.80}\protected@file@percent }
\citation{absil2009optimization}
\citation{boumal2014manopt}
\citation{sra2015conic}
\citation{matrixcookbook}
\citation{minka2000old}
\newlabel{eq:fixlo_penalized}{{5.3}{18}{Regularization parameter selection}{equation.81}{}}
\newlabel{eq:fixlo_penalized@cref}{{[equation][3][5]5.3}{[1][17][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Intrinsic MSE of ${\hat  {\Sigma }_{\text  {hi}}}$ (red) and mean Mahalanobis distance at ${\hat  {\Sigma }_{\text  {hi}}}$ (teal) as a function of regularization parameter $\lambda $ in the fixed-${\Sigma _{\rm  lo}}$ setting. We vary the dimension $d \in \{3, 4, 5\}$ within a class of simple example problems. The $\lambda $ associated with the minimum of the MSE curves corresponds closely to that associated with mean Mahalanobis distance equal to $\frac  {d(d+1)}{2}$, plotted with dashed black lines.\relax }}{18}{figure.caption.82}\protected@file@percent }
\newlabel{fig:regParSelect}{{1}{18}{Intrinsic MSE of $\Sigmahihat $ (red) and mean Mahalanobis distance at $\Sigmahihat $ (teal) as a function of regularization parameter $\lambda $ in the fixed-$\Sigmalo $ setting. We vary the dimension $d \in \{3, 4, 5\}$ within a class of simple example problems. The $\lambda $ associated with the minimum of the MSE curves corresponds closely to that associated with mean Mahalanobis distance equal to $\frac {d(d+1)}{2}$, plotted with dashed black lines.\relax }{figure.caption.82}{}}
\newlabel{fig:regParSelect@cref}{{[figure][1][]1}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Square root parameterization}{18}{subsection.83}\protected@file@percent }
\newlabel{eq:sqrtOpt}{{5.4}{18}{Square root parameterization}{equation.84}{}}
\newlabel{eq:sqrtOpt@cref}{{[equation][4][5]5.4}{[1][18][]18}}
\citation{maurais2023logEuclidean}
\newlabel{sec:numerics}{{6}{19}{Numerical results}{section.85}{}}
\newlabel{sec:numerics@cref}{{[section][6][]6}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical results}{19}{section.85}\protected@file@percent }
\newlabel{sec:simpleGaussian}{{6.1}{19}{Simple Gaussian example}{subsection.86}{}}
\newlabel{sec:simpleGaussian@cref}{{[subsection][1][6]6.1}{[1][19][]19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Simple Gaussian example}{19}{subsection.86}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Simple Gaussian example: Regularization parameters selected by matching mean minimum Mahalanobis distance over 32 pilot trials to $\frac  {d(d+1)}{2}$ (left), resulting mean minimum Mahalanobis distance over 3000 trials using the selected regularization parameters (middle), and fraction of EMF estimators which were indefinite over 3000 repeated trials (right). All budgets except $B = 196$ resulted in at least one indefinite EMF estimator.\relax }}{19}{figure.caption.87}\protected@file@percent }
\newlabel{fig:diagnostics_gaussian}{{2}{19}{Simple Gaussian example: Regularization parameters selected by matching mean minimum Mahalanobis distance over 32 pilot trials to $\frac {d(d+1)}{2}$ (left), resulting mean minimum Mahalanobis distance over 3000 trials using the selected regularization parameters (middle), and fraction of EMF estimators which were indefinite over 3000 repeated trials (right). All budgets except $B = 196$ resulted in at least one indefinite EMF estimator.\relax }{figure.caption.87}{}}
\newlabel{fig:diagnostics_gaussian@cref}{{[figure][2][]2}{[1][19][]19}}
\citation{maurais2023logEuclidean}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Simple Gaussian example: Median squared error in the Frobenius norm (left) and intrinsic metric (right).\relax }}{20}{figure.caption.88}\protected@file@percent }
\newlabel{fig:se_gaussian}{{3}{20}{Simple Gaussian example: Median squared error in the Frobenius norm (left) and intrinsic metric (right).\relax }{figure.caption.88}{}}
\newlabel{fig:se_gaussian@cref}{{[figure][3][]3}{[1][20][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Simple Gaussian example: Frobenius squared error histograms of ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ compared to ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  HF}$ (top), ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LF}$ (middle), and ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LEMF}$ (bottom). ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ attains significantly lower error than ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  HF}$ at all budgets, intuitively because it obtains more information, via recourse to correlated low-fidelity samples, at the same cost. For small budgets ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LF}$ has lower squared error than ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ because its variability is small due to the large number of samples comprising it, but as the budget increases its bias becomes apparent and ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ yields estimates with lower error.\relax }}{21}{figure.caption.89}\protected@file@percent }
\newlabel{fig:SE_hists}{{4}{21}{Simple Gaussian example: Frobenius squared error histograms of $\Sigmahihat ^{\rm MRMF}$ compared to $\Sigmahihat ^{\rm HF}$ (top), $\Sigmahihat ^{\rm LF}$ (middle), and $\Sigmahihat ^{\rm LEMF}$ (bottom). $\Sigmahihat ^{\rm MRMF}$ attains significantly lower error than $\Sigmahihat ^{\rm HF}$ at all budgets, intuitively because it obtains more information, via recourse to correlated low-fidelity samples, at the same cost. For small budgets $\Sigmahihat ^{\rm LF}$ has lower squared error than $\Sigmahihat ^{\rm MRMF}$ because its variability is small due to the large number of samples comprising it, but as the budget increases its bias becomes apparent and $\Sigmahihat ^{\rm MRMF}$ yields estimates with lower error.\relax }{figure.caption.89}{}}
\newlabel{fig:SE_hists@cref}{{[figure][4][]4}{[1][20][]21}}
\citation{zadeh2016geometric}
\citation{xing2002distance}
\citation{weinberger2009distance}
\citation{kulis2013metric}
\citation{bellet2013survey}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Simple Gaussian example: Intrinsic squared error distributions of ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ as compared to ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  HF}$ (top), ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LF}$ (middle), and ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LEMF}$ (bottom). The advantages of ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ relative to ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  HF}$ and ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LF}$ are more pronounced in the intrinsic metric, which compares matrices as operators by examining their generalized eigenvalues, than in the Frobenius metric, which compares matrices as vectors. The intrinsic metric also reveals the poor performance of ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LEMF}$ at low budgets, though at higher budgets the performances of ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  MRMF}$ and ${\hat  {\Sigma }_{\text  {hi}}}^{\rm  LEMF}$ are comparable.\relax }}{22}{figure.caption.90}\protected@file@percent }
\newlabel{fig:SE_hists_intrinsic}{{5}{22}{Simple Gaussian example: Intrinsic squared error distributions of $\Sigmahihat ^{\rm MRMF}$ as compared to $\Sigmahihat ^{\rm HF}$ (top), $\Sigmahihat ^{\rm LF}$ (middle), and $\Sigmahihat ^{\rm LEMF}$ (bottom). The advantages of $\Sigmahihat ^{\rm MRMF}$ relative to $\Sigmahihat ^{\rm HF}$ and $\Sigmahihat ^{\rm LF}$ are more pronounced in the intrinsic metric, which compares matrices as operators by examining their generalized eigenvalues, than in the Frobenius metric, which compares matrices as vectors. The intrinsic metric also reveals the poor performance of $\Sigmahihat ^{\rm LEMF}$ at low budgets, though at higher budgets the performances of $\Sigmahihat ^{\rm MRMF}$ and $\Sigmahihat ^{\rm LEMF}$ are comparable.\relax }{figure.caption.90}{}}
\newlabel{fig:SE_hists_intrinsic@cref}{{[figure][5][]5}{[1][21][]22}}
\newlabel{sec:metricLearning}{{6.2}{22}{Metric learning with the surface quasi-geostrophic equation}{subsection.91}{}}
\newlabel{sec:metricLearning@cref}{{[subsection][2][6]6.2}{[1][22][]22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Metric learning with the surface quasi-geostrophic equation}{22}{subsection.91}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Geometric mean metric learning}{22}{subsubsection.92}\protected@file@percent }
\citation{zadeh2016geometric}
\citation{HeldEtAl1985}
\newlabel{eq:GMML}{{6.1}{23}{Geometric mean metric learning}{equation.93}{}}
\newlabel{eq:GMML@cref}{{[equation][1][6]6.1}{[1][23][]23}}
\newlabel{eq:TandD_covs}{{6.2}{23}{Geometric mean metric learning}{equation.94}{}}
\newlabel{eq:TandD_covs@cref}{{[equation][2][6]6.2}{[1][23][]23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Surface quasi-geostrophic equation}{23}{subsubsection.95}\protected@file@percent }
\newlabel{eq:SQG}{{6.3}{23}{Surface quasi-geostrophic equation}{equation.96}{}}
\newlabel{eq:SQG@cref}{{[equation][3][6]6.3}{[1][23][]23}}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\citation{bpkwmg}
\citation{zadeh2016geometric}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Multifidelity metric learning}{24}{subsubsection.97}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces SQG metric learning: Sample allocations for single- and multi-fidelity estimators of $\Gamma _0$ and $\Gamma _1$. Each allocation requires the same computational budget, and the multifidelity allocations differ between classes due to differing values of generalized correlation determining the allocations according to \cite  {maurais2023logEuclidean}.\relax }}{24}{table.caption.98}\protected@file@percent }
\newlabel{tab:samp_alloc}{{2}{24}{SQG metric learning: Sample allocations for single- and multi-fidelity estimators of $\Gamma _0$ and $\Gamma _1$. Each allocation requires the same computational budget, and the multifidelity allocations differ between classes due to differing values of generalized correlation determining the allocations according to \cite {maurais2023logEuclidean}.\relax }{table.caption.98}{}}
\newlabel{tab:samp_alloc@cref}{{[table][2][]2}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Results}{24}{subsubsection.99}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SQG metric learning: Squared error of $\hat  A^{\rm  LF}$ (left), $\hat  A^{\rm  LEMF}$, (center), and $\hat  A^{\rm  MRMF}$ (right), computed in the Frobenius norm (top) and intrinsic metric \cref  {eq:intrinsicdist} (bottom). The squared-error histograms of $\hat  A^{\rm  HF}$ are overlaid in cyan for comparison, and underneath each plot we note the percentage change in mean squared error (MSE) of each estimator relative to $\hat  A^{\rm  HF}$. Note that histogram horizontal axes are log-scaled.\relax }}{25}{figure.caption.100}\protected@file@percent }
\newlabel{fig:sqg_mse_HF}{{6}{25}{SQG metric learning: Squared error of $\hat A^{\rm LF}$ (left), $\hat A^{\rm LEMF}$, (center), and $\hat A^{\rm MRMF}$ (right), computed in the Frobenius norm (top) and intrinsic metric \cref {eq:intrinsicdist} (bottom). The squared-error histograms of $\hat A^{\rm HF}$ are overlaid in cyan for comparison, and underneath each plot we note the percentage change in mean squared error (MSE) of each estimator relative to $\hat A^{\rm HF}$. Note that histogram horizontal axes are log-scaled.\relax }{figure.caption.100}{}}
\newlabel{fig:sqg_mse_HF@cref}{{[figure][6][]6}{[1][24][]25}}
\citation{maurais2023logEuclidean}
\citation{maurais2023logEuclidean}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces SQG metric learning: Squared error distributions of $\hat  A^{\rm  LF}$ (left) and $\hat  A^{\rm  LEMF}$ compared to those of $\hat  A^{\rm  MRMF}$, overlaid in blue, with squared error computed in the Frobenius norm (top) and intrinsic distance (bottom). $\hat  A^{\rm  MRMF}$ achieves 80.\% lower Frobenius MSE than $\hat  A^{\rm  LF}$ and 60.\% lower Frobenius MSE than $\hat  A^{\rm  LEMF}$, and achieves 63\% lower intrinsic MSE than $\hat  A^{\rm  LF}$ and 78\% lower intrinsic MSE than $\hat  A^{\rm  LEMF}$. Note that histogram horizontal axes are log-scaled.\relax }}{26}{figure.caption.101}\protected@file@percent }
\newlabel{fig:sqg_mse_reg}{{7}{26}{SQG metric learning: Squared error distributions of $\hat A^{\rm LF}$ (left) and $\hat A^{\rm LEMF}$ compared to those of $\hat A^{\rm MRMF}$, overlaid in blue, with squared error computed in the Frobenius norm (top) and intrinsic distance (bottom). $\hat A^{\rm MRMF}$ achieves 80.\% lower Frobenius MSE than $\hat A^{\rm LF}$ and 60.\% lower Frobenius MSE than $\hat A^{\rm LEMF}$, and achieves 63\% lower intrinsic MSE than $\hat A^{\rm LF}$ and 78\% lower intrinsic MSE than $\hat A^{\rm LEMF}$. Note that histogram horizontal axes are log-scaled.\relax }{figure.caption.101}{}}
\newlabel{fig:sqg_mse_reg@cref}{{[figure][7][]7}{[1][25][]26}}
\@writefile{toc}{\contentsline {paragraph}{Efficacy in Estimating $A_{\rm  GMML}$}{26}{section*.102}\protected@file@percent }
\citation{zadeh2016geometric}
\citation{maurais2023logEuclidean}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Boxplot (left) and bar graph of $\mathrm  {MRE}(\hat  A)$ for $\hat  A \in \{\hat  A^{\rm  HF}, \hat  A^{\rm  LF}, \hat  A^{\rm  LEMF}, \hat  A^{\rm  MRMF}\}$. Empirical MRE was computed over 5000 test samples of ${\bf  y}$ \cref  {eq:MRE_mc} for 50 realizations of each $\hat  A$. Use of multifidelity regression to estimate $\Gamma _0$ and $\Gamma _1$ decreases average MRE by 53\% relative to when $\Gamma _0$ and $\Gamma _1$ are estimated from high-fidelity samples alone. Average MRE corresponding to the regression estimator is additionally 25\% lower than that corresponding to the low-fidelity-only estimator and 6.9\% lower than that corresponding to the LEMF estimator.\relax }}{27}{figure.caption.103}\protected@file@percent }
\newlabel{fig:mre}{{8}{27}{Boxplot (left) and bar graph of $\mathrm {MRE}(\hat A)$ for $\hat A \in \{\hat A^{\rm HF}, \hat A^{\rm LF}, \hat A^{\rm LEMF}, \hat A^{\rm MRMF}\}$. Empirical MRE was computed over 5000 test samples of $\bfy $ \cref {eq:MRE_mc} for 50 realizations of each $\hat A$. Use of multifidelity regression to estimate $\Gamma _0$ and $\Gamma _1$ decreases average MRE by 53\% relative to when $\Gamma _0$ and $\Gamma _1$ are estimated from high-fidelity samples alone. Average MRE corresponding to the regression estimator is additionally 25\% lower than that corresponding to the low-fidelity-only estimator and 6.9\% lower than that corresponding to the LEMF estimator.\relax }{figure.caption.103}{}}
\newlabel{fig:mre@cref}{{[figure][8][]8}{[1][26][]27}}
\@writefile{toc}{\contentsline {paragraph}{Downstream performance quantified by mean relative error}{27}{section*.104}\protected@file@percent }
\newlabel{eq:MRE_def}{{6.4}{27}{Downstream performance quantified by mean relative error}{equation.105}{}}
\newlabel{eq:MRE_def@cref}{{[equation][4][6]6.4}{[1][27][]27}}
\newlabel{eq:MRE_mc}{{6.5}{27}{Downstream performance quantified by mean relative error}{equation.106}{}}
\newlabel{eq:MRE_mc@cref}{{[equation][5][6]6.5}{[1][27][]27}}
\citation{pennec2006intrinsic}
\citation{moakher2002means}
\citation{gillis2014NMF}
\citation{amari2016information}
\citation{villani2009optimal}
\citation{hanApproximateControlVariates2023}
\bibstyle{siamplain}
\bibdata{references}
\bibcite{absil2009optimization}{1}
\bibcite{amari2016information}{2}
\bibcite{arsigny2006log}{3}
\bibcite{bellet2013survey}{4}
\bibcite{localizationEnKF}{5}
\bibcite{bhatiaposdef}{6}
\bibcite{bhatia2019bures}{7}
\bibcite{bickellevina}{8}
\bibcite{bierig2015convergence}{9}
\bibcite{boumal2014manopt}{10}
\bibcite{chevallier2022exponential}{11}
\bibcite{cliffe_multilevel_2011}{12}
\bibcite{cressie2015statistics}{13}
\newlabel{sec:conclusion}{{7}{28}{Conclusions}{section.107}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][27][]28}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{28}{section.107}\protected@file@percent }
\bibcite{destouches2023multivariate}{14}
\bibcite{dolzDataSparseMultilevel2023}{15}
\bibcite{donohogavishjohnstone}{16}
\bibcite{evensenekf}{17}
\bibcite{evensenbook}{18}
\bibcite{graphicallasso}{19}
\bibcite{gasparicohn}{20}
\bibcite{giles2015multilevel}{21}
\bibcite{gillis2014NMF}{22}
\bibcite{GORODETSKY2020109257}{23}
\bibcite{hanApproximateControlVariates2023}{24}
\bibcite{HeldEtAl1985}{25}
\bibcite{mlenkf}{26}
\bibcite{iglesias2013ensemble}{27}
\bibcite{kaipio2006statistical}{28}
\bibcite{kalmanfilter}{29}
\bibcite{kulis2013metric}{30}
\bibcite{ledoit2012nonlinear}{31}
\bibcite{ledoit2022power}{32}
\bibcite{Lin2019}{33}
\bibcite{MalagoEtAl2018}{34}
\bibcite{markowitz1952portfolio}{35}
\bibcite{maurais2023logEuclidean}{36}
\bibcite{minka2000old}{37}
\bibcite{moakher2002means}{38}
\bibcite{mycek2019multilevel}{39}
\bibcite{NME:NME4761}{40}
\bibcite{PWK16MFMCAsymptotics}{41}
\bibcite{bpkwmg}{42}
\bibcite{multifidelityReview}{43}
\bibcite{pennec2006intrinsic}{44}
\bibcite{pennec2006riemannian}{45}
\bibcite{matrixcookbook}{46}
\bibcite{qian2018multifidelity}{47}
\bibcite{ringner2008principal}{48}
\bibcite{rubinstein1985efficiency}{49}
\bibcite{schaden2020multilevel}{50}
\bibcite{schaden2021asymptotic}{51}
\bibcite{schwartzmanLognormalDistributionsGeometric2016}{52}
\bibcite{smith2005covariance}{53}
\bibcite{sra2015conic}{54}
\bibcite{villani2009optimal}{55}
\bibcite{weinberger2009distance}{56}
\bibcite{xing2002distance}{57}
\bibcite{zadeh2016geometric}{58}
\gdef \@abspage@last{30}

\end{filecontents}