\documentclass[final,supplement,onefignum,onetabnum]{siamonline220329}

\input{ex_shared}

\externaldocument[][nocite]{revision}

\ifpdf
\hypersetup{
  pdftitle={Supplementary Materials: Multifidelity Covariance Estimation via Regression},
  pdfauthor={A. Maurais, Y. Marzouk, B. Peherstorfer, and T. Alsup}
}
\fi

\makeatletter 
\@mparswitchfalse%
\makeatother
\normalmarginpar

\begin{document}

\maketitle
\section{Supplement to \cref{sec:metricLearning}: metric-learning with the surface quasi--\\geostrophic equation}
\label{app:sqg} 
% Figure environment removed
\subsection{Experimental setup}
The surface quasi-geostrophic equation as presented in \cite{HeldEtAl1985,CapetEtAl2008} describes the evolution of the surface buoyancy $b: \calX \times [0, \infty) \to \R$ on the periodic spatial domain $\calX = [-\pi, \pi] \times [-\pi, \pi]$ via 
\begin{equation} 
\begin{aligned}
\frac{\partial }{\partial t} b(\bfx, t; \bftheta) + J(\psi, b) = 0, \quad z = 0 \\ 
b = \frac{\partial}{\partial z} \psi \\ 
\Delta \psi = 0,\quad z < 0 \\ 
\psi \to 0, \quad z \to -\infty, 
\end{aligned}
\label{eq:sqg_system}
\end{equation} 
where $\bfx = (x_1,x_2)$ is the surface spatial coordinate, $\psi: \calX \times (-\infty, 0] \to \R $ is the stream-function, and $J(\psi, b)$ denotes the Jacobian determinant 
\[
J(\psi, b) = \left( \frac{\partial \psi}{\partial x_1} \right) \left( \frac{\partial b}{\partial x_2} \right) - \left(\frac{\partial b}{\partial x_1} \right)\left( \frac{\partial \psi}{\partial x_2} \right).  
\]
The parameters $\bftheta \in \R^5$ determine the initial condition $b_0$ and some aspects of the dynamics \cref{eq:sqg_system}; we set 
\[
    b_0(\bfx;\bftheta) = -\frac{1}{(2\pi/ |\theta_5|)^2} \exp\left( -x_1^2 - \exp(2\theta_1) x_2^2 \right),
\]
the contours of which form ellipses parametrized by the log aspect-ratio $\theta_1$ and the amplitude $\theta_5$. The gradient Coriolis parameter $\theta_2$, log buoyancy frequency $\theta_3$, and background zonal flow $\theta_4$ all determine aspects of the dynamics.  

In our metric learning experiment in \cref{sec:metricLearning} we draw the parameters $\bftheta$ from an equal two-component Gaussian mixture, i.e., 
\[
p(\bftheta \mid i) = \calN(\bfmu_i, C) = \pi_i, \quad i \sim \mathrm{Ber}(1/2),
\]
where $\bfmu_0$ and $\bfmu_1$ differ only in their first components,
\[
\bfmu_0 = \begin{bmatrix}
1 & 0 & 0 & 0 & 4 
\end{bmatrix}\t, \quad \bfmu_1 = \begin{bmatrix}
0.1 & 0 & 0 & 0 & 4 
\end{bmatrix}\t
\]
and the parameter covariance $C$ is given by 
\[
C = \begin{bmatrix}
    0.3^2 \\ 
    & 0.003^2 \\ 
    & & 0 \\ 
    & & & 0.08^2 \\ 
    & & & & 0.3^2
\end{bmatrix}.
\]
Note that this choice of $C$ indicates that the log buoyancy frequency $\theta_3 = 0$ is deterministic, but the observational covariances $\Gamma_0$ and $\Gamma_1$ which we learn in \cref{sec:metricLearning} are still full-rank. In \cref{fig:SQG_solution} we show examples of the initial buoyancy $b_0$ and final buoyancy $b$ at time $T = 24$ for samples of $\bftheta$ from both mixture components.  

\subsection{Additional results}
\label{app:sqg:addResults}
In the following subsections we display results pertaining to estimation of $\Gamma_0 = \Cov[\bfy \mid \bftheta \sim \pi_0]$ and $\Gamma_1 = \Cov[\bfy \mid \bftheta \sim \pi_1]$. We see in \cref{sec:metricLearning} that the best estimates of $A_{\rm GMML}$ in both the Frobenius and intrinsic metrics are obtained with $\Gamma_0$ and $\Gamma_1$ estimated via multifidelity regression, even though, as we show below, multifidelity regression is generally not the best-performing estimator for $\Gamma_0$ and $\Gamma_1$ in the Frobenius metric. This behavior is sensible when one considers that (a) $A_{\rm GMML}$ is defined as a point on a geodesic between two SPD matrices in the affine-invariant geometry, and the regression estimator, being constructed using the affine-invariant geometry, is thus the ``natural'' choice in this application, (b) while the LEMF estimator out-performs the regression estimator in the Frobenius metric for estimation of $\Gamma_1$, it does quite poorly in estimating $\Gamma_0$ and thus yields relatively poor estimates of $A_{\rm GMML}$, and (c) we are generally unable to construct $A_{\rm GMML}$ from estimates of $\Gamma_0$ and $\Gamma_1$ computed with the EMF estimator due to a high frequency (94\%) of indefiniteness of $\hat\Gamma_{0}^{\rm EMF}$ or $\hat\Gamma_1^{\rm EMF}$. 
% Figure environment removed
\subsubsection{Estimation of $\Gamma_0$}
\vskip -0.25cm
In \cref{fig:Gamma0_fro,fig:Gamma0_AI_LE} we show squared error histograms corresponding to $\hat\Gamma_0^{\rm HF}$, $\hat\Gamma_0^{\rm LF}$, $\hat\Gamma_0^{\rm EMF}$, $\hat\Gamma_0^{\rm LEMF}$, and $\hat\Gamma_0^{\rm MRMF}$. In general $\hat\Gamma_0^{\rm LF}$, $\hat\Gamma_0^{\rm EMF}$, and $\hat\Gamma_0^{\rm MRMF}$ all yield substantial decreases in squared error relative to $\hat\Gamma_0^{\rm HF}$, while interestingly $\hat\Gamma_0^{\rm LEMF}$ results in an \textit{increase} squared error relative to $\hat\Gamma_0^{\rm HF}$, perhaps due to amplification of error by the matrix exponential. As one might expect, $\hat\Gamma_0^{\rm MRMF}$ achieves the lowest MSE in the intrinsic metric, while $\hat\Gamma_0^{\rm EMF}$ achieves lowest MSE in the Frobenius metric. At the same time, 82.4\% of realizations of $\hat\Gamma_0^{\rm EMF}$ are indefinite and thus useless for construction of $\hat A_{\rm GMML}$. 
% Figure environment removed

\subsubsection{Estimation of $\Gamma_1$}
In \cref{fig:Gamma1_fro,fig:Gamma1_AI_LE} we show squared error histograms corresponding to $\hat\Gamma_1^{\rm HF}$, $\hat\Gamma_1^{\rm LF}$, $\hat\Gamma_1^{\rm EMF}$, and $\hat\Gamma_1^{\rm MRMF}$. In general $\hat\Gamma_1^{\rm LF}$, $\hat\Gamma_1^{\rm EMF}$, $\hat\Gamma_1^{\rm LEMF}$, and $\hat\Gamma_1^{\rm MRMF}$ all yield substantial decreases in squared error relative to $\hat\Gamma_1^{\rm HF}$. In contrast to $\hat\Gamma_0^{\rm LEMF}$, $\hat\Gamma_1^{\rm LEMF}$ results in decreases, rather than increases, in MSE, relative to the high-fidelity-only estimator, but good performance in estimation of $\Gamma_1$ alone is not enough to ensure good estimates of $A_{\rm GMML}$. 
In a similar vein, the frequency with which $\hat\Gamma_1^{\rm EMF}$ is indefinite was only 67\%, a moderate decrease from the 82\% of $\hat\Gamma_0^{\rm EMF}$.
% Figure environment removed
\clearpage 
% Figure environment removed

\clearpage 
\section{SPD Product Manifolds}
\label{app:prodMan}
The product $\bbP_d^K = \bbP_d \times \cdots \times \bbP_d$ ($K$ times, $K\in \Z^+$) is a Riemannian manifold when equipped with tangent spaces and metric derived from $\bbP_d$. In this appendix we provide the relevant geometric and statistical definitions for $\bbP_d^K$, which in most cases follow directly from the properties of $\bbP_d$ discussed in \cref{sec:bg}.

\subsection{Geometry}
\label{app:prodMan_geom}
Let $\bfA = (A_1, \dots, A_K) \in \bbP_d^K$. The tangent space to $\bbP_d^K$ at $\bfA$ is 
\begin{equation*}
\rmT_\bfA \bbP_d^K = \mathrm{T}_{(A_1, \dots, A_K)}\bbP_d^K = \bigotimes_{k=1}^K \rmT_{A_k}\bbP_d. 
\end{equation*}
With this definition of tangent space, the Riemannian metric or \textbf{inner product} on $\bbP_d^K$ can be decomposed as follows: let $\bfU, \bfV \in \rmT_\bfA \bbP_d^K \subseteq \bbH_d^K$. We define $g_{\bfA}: \mathrm{T}_{\bfA}\bbP_d^K\times \mathrm{T}_{\bfA}\bbP_d^K \to \R$ via 
\begin{equation*}
g_{ \bfA }(\bfU, \bfV) = \langle \bfU, \; \bfV \rangle_\bfA = \sum_{k=1}^K \langle U_k,\; V_k\rangle_{A_k} = \sum_{k=1}^K \trace{U_kA_k\inv V_kA_k\inv}.
\end{equation*}
Corresponding to this inner product we have an \textbf{outer product} on $\rmT_\bfA\bbP_d^K$, 
\begin{equation*}
\bfU \otimes_{\bfA}\bfV = \begin{bmatrix} U_1 \\ \vdots \\ U_K \end{bmatrix} \otimes \begin{bmatrix}A_1\inv V_1 A_1\inv \\ \vdots \\ A_K\inv V_K A_K\inv\end{bmatrix},
\end{equation*}
where $\otimes$ is the Kronecker product and the result defines a linear mapping from $\rmT_\bfA\bbP_d^K$ to $\rmT_\bfA\bbP_d^K$. As in the $\bbP_d^1$ case, the trace of the $\bfA$ outer-product is equal to the $\bfA$ inner-product,
\begin{equation*}
\trace{\bfU \otimes_{\bfA} \bfV} = \langle \bfU,\bfV \rangle_{\bfA}. 
\end{equation*}
Bridging between $\bbP_d^K$ and $\rmT_\bfA\bbP_d^K$ we have the \textbf{logarithmic and exponential mappings} 
\begin{equation*}
\begin{aligned}
	\log_{\bfA}: \bbP_d^K \to \mathrm{T}_{\bfA}\bbP_d^K, \quad & \log_{\bfA}(\bfB) = (\log_{A_1}B_1,\dots, \log_{A_K} B_K) \\
	\exp_{\bfA}: \mathrm{T}_{\bfA}\bbP_d^K \to \bbP_d^K, \quad & \exp_{\bfA}(\bfX) = (\exp_{A_1} X_1,\dots, \exp_{A_K} X_K),
\end{aligned}
\end{equation*}
which are simply the logarithmic and exponential mappings on $\bbP_d^1$ applied elementwise to $\bfB \in \bbP_d^K$ and $\bfX \in \rmT_\bfA \bbP_d^K$ with the corresponding entries of $\bfA$. 


Geodesics and distance on the product manifold $\bbP_d^K$ are given as follows:  
Let $\bfA, \bfB \in \bbP_d^K$.  If $\gamma_1(t), \dots, \gamma_K(t)$ are the geodesics from $A_1, \dots, A_K$ to $B_1, \dots, B_K$, respectively, on $\bbP_d$, then the \textbf{geodesic} from $\bfA $ to $\bfB$ on $\bbP_d^K$ is given by
\begin{equation}
	\begin{aligned}
		\gamma_{\bfA \to \bfB}(t) &= (\gamma_1(t),\dots, \gamma_K(t)) \\
		&= \left(A_1^{\frac{1}{2}}(A_1^{-\frac{1}{2}} B_1 A_1^{-\frac{1}{2}})^t A_1^{\frac{1}{2}},\dots, A_K^{\frac{1}{2}}(A_K^{-\frac{1}{2}} B_K A_K^{-\frac{1}{2}})^t A_K^{\frac{1}{2}} \right).
	\end{aligned}
	\label{eq:geodesicPnK}
\end{equation}

The \textbf{intrinsic distance} between $\bfA$ and $\bfB$ on $\bbP_d^K$ follows accordingly from \cref{eq:intrinsicdist},
\begin{equation*}
	d^2(\bfA, \bfB) = \sum_{k=1}^K d(A_k, B_k)^2
	= \sum_{k=1}^K \left\|\log(A_k^{-\frac{1}{2}}B_k A_k^{-\frac{1}{2}}) \right\|_F^2 
	= \sum_{k=1}^K \sum_{i=1}^d \log^2\lambda_i(A_k\inv B_k).
\end{equation*}

\subsection{Statistics}
\label{app:prodMan_stats}
Let $\bfS = (S_1, \dots, S_K)$ be a $\bbP_d^K$-valued random variable. In constructing our multifidelity covariance estimator we employ the following notions of statistics for $\bfS$ which are consistent with the general definitions in \cite{pennec2006intrinsic} but, due to the product manifold structure of $\bbP_d^K$ (\cref{app:prodMan_geom}), in many cases have nice decompositions to statistics on $\bbP_d^1$. 

To begin, the \textbf{expectation} of $\bfS$ is the element $\bfY \in \bbP_d^K$ which minimizes the expected squared distance to $\bfS$, 
\begin{equation}
\begin{aligned}
\bfE[S] &= \argmin_{\bfY \in \bbP_d^K} \E\left[d^2(\bfY, \bfS)\right]
= \argmin_{Y \in \bbP_d^K} \E\left[\sum_{k=1}^K d^2(Y_k, S_k) \right] 
= \argmin_{Y \in \bbP_d^K} \sum_{k=1}^K \E \left[d^2(Y_k, S_k)\right] \\
 &=  (\bfE[S_1], \dots, \bfE[S_K]) = (\Sigma_1, \dots, \Sigma_K) \equiv \bfSigma. 
\end{aligned}
\label{eq:prodman_mean}
\end{equation}
Because the squared distance between $\bfS$ and $\bfY$ decomposes into the sum of $K$ squared distances between the individual components of $\bfS$ and $\bfY$, to obtain the (Frechet) mean of $\bfS$ we simply take the Frechet mean of each component of $\bfS$. 

The \textbf{variance} of $\bfS$ is defined as the expected squared distance between $\bfS$ and its mean, 
\begin{equation*}
	\sigma^2_\bfS = \E[d^2(\bfS, \bfSigma)] = \E \left[ \sum_{k=1}^K d^2(S_k, \Sigma_k) \right] = \sum_{k=1}^K \sigma^2_{S_k},
\end{equation*}
where $\sigma^2_{S_k}$ is the variance of $S_k \in \bbP_d$, $k = 1, \dots, K$.  

As in the case of $\bbP_d^1$-valued random variables in \cref{sec:bg}, the \textbf{covariance} of $\bfS$ is the $\bfSigma$-outer-product of the ``vector difference'' $\log_\bfSigma \bfS$ with itself, 
\begin{equation*}
\Cov[\bfS] = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS] \equiv \Gamma_\bfS.
\end{equation*}
$\Gamma_\bfS$ is a symmetric positive semidefinite linear operator from $\bbH_d^K = \rmT_\bfSigma \bbP_d^K$ to $\rmT_\bfSigma \bbP_d^K = \bbH_d^k$. As on $\bbP_d^1$, we have $\trace{\Gamma_\bfS} = \sigma^2_\bfS$.  

Finally given $\bfS \sim (\Sigma, \Gamma_\bfS)$ \rev{with $\Gamma_\bfS$ invertible}, we define the \textbf{Mahalanobis distance} between $\bfS$ and a deterministic point $\bfY \in \bbP_d^K$
\begin{equation} 
d^2_\bfS(\bfY) = \langle \log_\bfSigma \bfY, \; \Gamma_\bfS\inv \log_\bfSigma \bfY \rangle_\bfSigma. 
\label{eq:prodMan}
\end{equation} 
The Mahalanobis distance is a $\Gamma_\bfS\inv$-weighted version of the intrinsic distance between $\bfSigma$ and $\bfY$, and, unless $\Gamma_\bfS$ is ``diagonal'' (in the appropriate sense), in general \textit{cannot} be decomposed into a sum of pairwise distances between $\Sigma_k$ and $Y_k$. The $\Gamma_\bfS\inv$ weighting introduces interaction between separate components of $\log_\bfSigma \bfY$. 



\bibliographystyle{siamplain}
\makeatletter\@input{xx.tex}\makeatother
\bibliography{references}


\end{document}
