\documentclass[final,supplement,onefignum,onetabnum]{siamonline220329}

\input{ex_shared}

\externaldocument[][nocite]{ex_article}

\ifpdf
\hypersetup{
  pdftitle={Supplementary Materials: Multifidelity Covariance Estimation via Regression},
  pdfauthor={A.\@ Maurais, T.\@ Alsup, B.\@ Peherstorfer, Y.\@ Marzouk}
}
\fi

\makeatletter 
\@mparswitchfalse%
\makeatother
\normalmarginpar

\begin{document}

\maketitle

\section{Proofs}
\label{app:proofs}

\subsection{Proof of \cref{thm:ts_agnostic}}
We begin with a lemma establishing symmetry of the operators weighing the inner- and outer-products of tangent spaces to $\bbP_d^{N}$.
\begin{lemma}
    Let $\bfY = (Y_1, \dots, Y_N) \in \bbP_d^N$, where $N \in \Z^+$, and define the linear operator $G_\bfY: \bbH_d^N \to \bbH_d^N$ by 
\[
\bfA = (A_1, \dots, A_N) \mapsto  (Y_1 \inv A_1 Y_1 \inv, \dots, Y_N \inv A_{N} Y_{N} \inv) = G_\bfY\bfA. 
\]
$G_\bfY$ is symmetric.
\begin{proof}
    Let $\bfA, \bfB \in \bbH_d^N$. Using the definition of the inner product for symmetric matrices and the cyclic property of trace, we quickly establish symmetry of $G_\bfY$,
    \begin{multline*}
    \langle G_\bfY \bfA, \, \bfB \rangle = \sum_{n=1}^N \langle Y_n\inv A_n Y_n\inv,\, B \rangle = \sum_{n=1}^N \trace{ Y_n\inv A_n Y_n\inv B } = 
    \sum_{n=1}^N \trace{  A_n Y_n\inv B Y_n\inv}\\ 
    =  \sum_{n=1}^N \langle  A_n,\, Y_n\inv B Y_n\inv \rangle = \langle \bfA,\, G_\bfY \bfB \rangle. 
    \end{multline*}
\end{proof}
\label{lemma:Gy_symmetric}
\end{lemma}


We next demonstrate that $\Gamma_\bfS$ can be factored in terms of the Riemannian transformation of $\rmT_\bfSigma \bbP_d^N$ in \cref{prop:gammaS_decomp}.
\begin{lemma}
	Consider $\Gamma_\bfS = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS]$ for the random variable $\bfS$ with mean $\bfSigma$ in \cref{eq:S_rv}. $\Gamma_\bfS$ can be written 
	\begin{equation*}
	\Gamma_\bfS = \Gamma_{\bfS, \bfI}G_{\bfSigma},
	\end{equation*}
	where $\Gamma_{\bfS, \bfI} = \E[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS]$, and $G_{\bfSigma}$ is the linear operator on $\bbH_d^{N}$ mapping 
 \[
\bfA = (A_1, \dots, A_{N}) \mapsto  \left ((\Sigma^1) \inv A_1 (\Sigma^1) \inv, \dots, (\Sigma^N) \inv A_{N} (\Sigma^N) \inv \right ) = G_{\bfSigma}\bfA,
 \]
where $\Sigma^1, \dots, \Sigma^N$ %
are the $N$ individual $\bbP_d$-valued elements of $\bfSigma = (\bfSigma^{(1)}, \dots, \bfSigma^{(K)})$. 
	\begin{proof} 
The covariance of $\bfS$ is given by
    \begin{equation*}
    \Gamma_\bfS = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS] = \E[\log_\bfSigma \bfS \otimes G_\bfSigma \log_\bfSigma \bfS],
    \end{equation*}
    by definition of the outer-product $\otimes_\bfSigma$. We factor the symmetric linear operator $G_\bfSigma$ out of the outer product and obtain 
    \begin{equation*}
    \Gamma_\bfS =\E[\log_\bfSigma \bfS \otimes G_\bfSigma\log_\bfSigma \bfS] = \E[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS]G_\bfSigma\t \equiv \Gamma_{\bfS, I} G_\bfSigma.
    \end{equation*}
	\end{proof}
	\label{prop:gammaS_decomp}
\end{lemma}

This decomposition of $\Gamma_\bfS$ allows us to quickly show the tangent-space-agnosticism in \cref{thm:ts_agnostic}. By \cref{prop:gammaS_decomp}, we can factor the covariance of $\bfS$ as $\Gamma_\bfS = \Gamma_{\bfS, \bfI}G_{\bfSigma}$, hence its inverse is given by 
\begin{equation*}
\Gamma_\bfS\inv = G_{\bfSigma}\inv \Gamma_{\bfS, \bfI}\inv. 
\end{equation*}
The Mahalanobis distance in \cref{eq:mdist_min} is defined in terms of the $\bfSigma$ inner product, which we can write
\begin{equation*}
d^2_S(\bfSigma) = \left\langle \log_\bfSigma \bfS, \; \Gamma_\bfS\inv \log_\bfSigma \bfS \right\rangle_\bfSigma = \left\langle \log_\bfSigma \bfS, \; G_\bfSigma (\Gamma_\bfS\inv \log_\bfSigma \bfS) \right\rangle,
\end{equation*}
with $G_\bfSigma$ as in \cref{prop:gammaS_decomp}. Substituting $\Gamma_\bfS\inv = G_\bfSigma\inv \Gamma_{\bfS, \bfI}\inv$ above, we obtain the desired result, 
\begin{equation*}
d^2_S(\bfSigma) = \left\langle \log_\bfSigma \bfS, \; G_\bfSigma (G_\bfSigma\inv\Gamma_{\bfS, \bfI}\inv \log_\bfSigma \bfS) \right\rangle = \left\langle \log_\bfSigma \bfS, \; \Gamma_{\bfS, \bfI} \inv \log_\bfSigma \bfS \right\rangle.
\end{equation*}

\subsection{Proof of \cref{thm:affineinvariance}}
\subsubsection{Preliminaries}
In order to show affine-invariance of the Mahalanobis distance, we require a result relating $\log_{\tilde A} \tilde B$ to $\log_A B$, where $\tilde A = Y\inv A Y\inv$ and $\tilde B = Y\inv B Y\inv$ for some $Y \in \bbP_d$, which we will then extend to $\bbP_d^N$. 

\begin{lemma}
	Let $A, B, Y \in \bbP_d$, and define $\tilde A = Y\inv A Y\inv$ and $\tilde B = Y\inv B Y\inv$. It holds that 
	\begin{equation}
	\log_{\tilde A} \tilde B = Y\inv (\log_A B) Y\inv,
	\label{eq:logequiv}
	\end{equation}
	i.e., affine transformations on $\bbP_d$ correspond to affine transformations on tangent spaces to $\bbP_d$. 
	
\begin{proof}
Recall the definition of $\log_A B$ for $A, B \in \bbP_d$, 
\begin{equation*}
\log_A B = A^{\frac{1}{2}}\log(A^{-\frac{1}{2}}B A^{-\frac{1}{2}})A^{\frac{1}{2}} = A\log(A\inv B). 
\end{equation*}	
$\log_{\tilde A} \tilde B$ is given by 
\begin{equation}
	\begin{aligned}
		\log_{\tilde A}\tilde B = \tilde A\log(\tilde A\inv \tilde B) &= Y\inv AY\inv\log(Y A\inv Y Y\inv BY\inv )  \\
		&= Y\inv AY\inv \log(Y A\inv B Y\inv ).
	\end{aligned}
	\label{eq:logtilde}
\end{equation} 
In \cite{bhatiaposdef} we find mentioned that for $T \in \bbM_d$ with no eigenvalues on $(\infty, 0]$ and $S\in\bbM_d$ invertible, with $\bbM_d$ denoting the set of real $d \times d$ matrices, $S\log(T)S\inv = \log(STS\inv)$. Applying this fact to the last line of \eqref{eq:logtilde}, we see that
\begin{equation*}
\log_{\tilde A}\tilde B = Y\inv A Y\inv\log(Y A\inv B Y\inv) = Y\inv A\log(A\inv B)Y\inv \equiv Y\inv \log_A(B) Y \inv,
\end{equation*}
yielding the desired equivalence \cref{eq:logequiv}. 
\end{proof}
\label{lemma:logequiv}
\end{lemma}

The extension of this relationship to an analogous one $\bbP_d^N$ is immediate:

\begin{corollary}
	Let $\bfA = (A_1, \dots, A_N) \in \bbP_d^N$, $\bfB = (B_1, \dots, B_N) \in \bbP_d^N$ and define 
 \[
 \tilde\bfA = (Y_1\inv A_1 Y_1\inv, \dots, Y_N\inv A_N Y_N\inv) = G_\bfY\bfA, \; \tilde\bfB = (Y_1\inv B_1 Y_1\inv, \dots, Y_N\inv B_N Y_N\inv) = G_\bfY\bfB
 \]
 where $\bfY \in \bbP_d^N$ and $G_\bfY: \bbH_d^N \to \bbH_d^N$ is the symmetric linear operator mapping 
 \[
\bfC = (C_1, \dots, C_N) \mapsto  (Y_1 \inv C_1 Y_1 \inv, \dots, Y_N \inv C_{N} Y_{N} \inv) = G_\bfY\bfC
 \]
 with $\bfC \in \bbH_d^N$. Then 
	\begin{align*}
		\log_{\tilde \bfA}\tilde \bfB = \log_{G_\bfY\bfA} (G_\bfY\bfB) = G_\bfY \log_\bfA \bfB.
	\end{align*}
	\begin{proof}
	Applying \cref{lemma:logequiv} elementwise to $\log_{\tilde \bfA} \tilde \bfB$, we see
	\begin{equation*}
		\log_{\tilde\bfA} \tilde \bfB = \begin{bmatrix} \log_{\tilde A_1}\tilde B_1 \\ \vdots \\ \log_{\tilde A_N} \tilde B_N \end{bmatrix} = \begin{bmatrix} Y_1\inv(\log_{A_1}B_1)Y_1\inv \\ \vdots \\ Y_N \inv(\log_{A_N}B_N)Y_N\inv \end{bmatrix} = G_\bfY \log_\bfA \bfB.
	\end{equation*}	
	\end{proof}
\label{corr:logequiv_K}
\end{corollary}

\subsubsection{Main Result}
With \cref{lemma:logequiv} and \cref{corr:logequiv_K}, we can now show that the Mahalanobis distance objective \cref{eq:mdist_min} is affine-invariant. %

By \cref{thm:ts_agnostic}, the Mahalanobis distance in \cref{eq:mdist_thm} is equal to 
		\begin{equation*}
		d^2_\bfS(\bfSigma) = \langle \log_\bfSigma \bfS, \; \Gamma_\bfS\inv \log_\bfSigma \bfS \rangle_\bfSigma = \langle \log_\bfSigma \bfS, \; \Gamma_{\bfS, \bfI}\inv \log_\bfSigma \bfS \rangle 
		\end{equation*}
		where $\Gamma_{\bfS, I} =\E[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS]$.  In computing the Mahalanobis distance for $d^2_{\tilde \bfS}(\tilde \bfSigma)$ we will similarly only concern ourselves with the unweighted inner product $\langle \cdot, \cdot \rangle$ and $\Gamma_{\tilde\bfS, \bfI} = \E \left[\log_{\tilde \bfSigma}\tilde\bfS \otimes \log_{\tilde \bfSigma}\tilde\bfS \right]$. Using \cref{corr:logequiv_K}, factoring linear operators out of the outer product, and noting that $G_\bfY$ is symmetric, we write $\Gamma_{\tilde\bfS, I}$ as 
		\begin{align*}
			\Gamma_{\tilde\bfS, I} &= \E \left[\log_{\tilde \bfSigma}\tilde\bfS \otimes \log_{\tilde \bfSigma}\tilde\bfS \right] \\
			&= \E \left[G_\bfY\log_{\bfSigma}\bfS \otimes G_\bfY\log_{\bfSigma} \bfS \right] \\
			&= G_\bfY\E \left[ \log_{\bfSigma}\bfS \otimes \log_{\bfSigma} \bfS  \right]G_\bfY\t \\
			&= G_\bfY\Gamma_{\bfS, I} G_\bfY.
		\end{align*}
	 Hence, $\Gamma_{\tilde\bfS, \bfI}\inv = G_\bfY\inv \Gamma_{\bfS, \bfI}\inv G_\bfY\inv$. The Mahalanobis distance $d^2_{\tilde \bfS}(\tilde \bfSigma)$ is thus 
	 \begin{align*}
	 	d^2_{\tilde \bfS}(\tilde \bfSigma) &= \left\langle \log_{\tilde \bfSigma}\tilde\bfS,\; \Gamma_{\tilde\bfS, \bfI}\inv \log_{\tilde\bfSigma}\tilde\bfS \right\rangle \\
	 	&= \left\langle G_\bfY\log_{ \bfSigma}\bfS,\; (G_\bfY\inv \Gamma_{\bfS, \bfI}\inv G_\bfY\inv) G_\bfY\log_{ \bfSigma} \bfS \right\rangle \\
	 	&= \left\langle \log_{ \bfSigma}\bfS,\; \Gamma_{\bfS, \bfI}\inv \log_{ \bfSigma} \bfS \right\rangle \\
	 	&= d^2_\bfS(\bfSigma).
	 \end{align*}

\subsection{Proof of \cref{prop:mle}}
\cref{thm:ts_agnostic} shows that instead of directly minimizing the Mahalanobis distance objective in \cref{eq:mdist_min}, which is rigorously defined using the inner- and outer-products specific to $\rmT_\bfSigma \bbP_d^N$, we can equivalently solve 
\begin{equation}
	(\hat\Sigma_0,\dots, \hat\Sigma_L) = \argmin_{\Sigma_0, \dots,\Sigma_L \in \bbP_d} \langle \log_{\bfSigma} \bfS,\; \Gamma_{\bfS, \bfI}\inv \log_{\bfSigma} \bfS \rangle \quad \text{s.t. } \bfSigma = \mu_\bfS(\Sigma_0, \dots, \Sigma_L),
	\label{eq:mdist_min_tsa}
\end{equation}
defined with the standard Euclidean products $\langle \cdot, \cdot \rangle$ and $\otimes$. 
   Because $\bbH_d^N$ is a Euclidean vector space, the density of $\bm\calE \sim \calN_{\bbH_d^N}(\mathbf{0}, \Gamma_{\bm \calE})$ can be written 
   \[
   p(\bm{\calE}) \propto \exp\left(-\ltfrac{1}{2}\langle \bm{\calE},\; \Gamma_{\bm{\calE}}\inv \bm{\calE} \rangle \right) = \exp \left(-\ltfrac{1}{2}\langle \log_\bfSigma \bfS,\; \Gamma_{\bfS, \bfI}\inv \log_\bfSigma \bfS \rangle \right),
   \]
   where we have used the fact that $\Gamma_{\bm{\calE}} = \Gamma_{\bfS, \bfI}$. Maximizing the above is equivalent to minimizing its logarithm, which is what occurs in \cref{eq:mdist_min_tsa} and equivalently \cref{eq:mdist_min}. 

\subsection{Proof of \cref{prop:nleq}}
\label{app:proof_nleq}
The inner product in \cref{eq:twomat_reg} can be decomposed into inner products between the individual components of $\log_\bfSigma \bfS$, 
\begin{equation} 
\begin{aligned}
	\Sigmahihat &= \argmin_{\Sigmahi \in \bbP_d} \left\langle \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Slobar(\Slo) \end{bmatrix}, \; \Gamma_{S} \inv \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Slobar(\Slo) \end{bmatrix}  \right\rangle  \\
	&= \argmin_{\Sigmahi \in \bbP_d} \langle \log_\Sigmahi(\Shi), \; C_{\rm hi} \log_\Sigmahi (\Shi) \rangle + 2 \langle \log_\Sigmahi (\Shi), \; C_{\rm lo, hi} \log_\Slobar (\Slo) \rangle \\
 &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad + \langle \log_\Slobar (\Slo), \; C_{\rm lo} \log_\Slobar (\Slo) \rangle,
\end{aligned}
\label{eq:opt_blocks}
\end{equation} 
where $C_{\rm hi}$, $C_{\rm lo, hi}$, and $C_{\rm lo}: \bbH_d \to \bbH_d$ are blocks of $\Gamma_\bfS\inv$, 
\[
\Gamma_{\bfS}\inv = 
\begin{bmatrix}
C_{\rm hi} & C_{\rm lo, hi} \\
C_{\rm lo, hi}\t & C_{\rm lo}
\end{bmatrix} .
\]

Denote $\calShi = \log_\Sigmahi(\Shi)$ and $\calSlo = \log_\Slobar(\Slo)$; $\calShi$ depends on $\Sigmahi$ in \eqref{eq:opt_blocks} whereas $\calSlo$ is fixed. Thus we have 
\begin{equation} 
\begin{aligned}
\Sigmahihat &= \argmin_{\Sigmahi \in \bbP_d}\; \langle \calShi, \; C_{\rm hi} \calShi \rangle + 2 \langle \calShi, \; C_{\rm lo, hi} \calSlo \rangle  + \langle  \calSlo, \; C_{\rm lo} \calSlo \rangle \quad \text{s.t. } \calShi = \log_\Sigmahi \Shi \\
&\equiv \argmin_{\Sigmahi \in \bbP_d}\; f(\calShi) \quad \text{s.t. } \calShi = \log_\Sigmahi \Shi 
\end{aligned}
\label{eq:insert_calS}
\end{equation} 
Since \eqref{eq:insert_calS} depends on $\Sigmahi$ only through $\calShi$, we may optimize the Mahalanobis distance with respect to $\calShi$, 
\[
\calShihat = \argmin_{\calShi \in \bbH_d} f(\calShi) 
\]
which in the end will leave us with a nonlinear equation for $\Sigmahihat$. %
The gradient of $f$ with respect to $\calShi$ is given by
\[
\nabla f(\calShi) = 2 C_{\rm hi} \calShi + 2 C_{\rm lo, hi} \calSlo,
\]
where we recall that $\langle \cdot, \cdot \rangle$ in \eqref{eq:insert_calS} denotes the Frobenius (trace) inner product between symmetric matrices. Setting this gradient equal to the zero matrix and solving the corresponding equation yields an optimum value of $\calShi = \log_\Sigmahi (\Shi)$,
\begin{equation*}
\calShihat = -C_{\rm hi}\inv C_{\rm lo, hi}\calSlo %
\end{equation*}
Because $C_{\rm hi}$ and $C_{\rm lo, hi}$ are linear operators on symmetric matrices, $\calShihat$ is indeed symmetric. Using relevant formulae for blockwise inversion of a square-partitioned symmetric linear operator \cite{lu2002inverses}, we express $C_{\rm hi}$, $C_{\rm lo}$, and $C_{\rm lo, hi}$ in terms of the blocks of $\Gamma_\bfS$,
\begin{equation}
\begin{gathered}
C_{\rm hi} = (\Gamma_{\rm hi} - \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \Gamma_{\rm lo, hi}\t)\inv, \quad C_{\rm lo} = \Gamma_{\rm lo}\inv + \Gamma_{\rm lo}\inv \Gamma_{\rm lo, hi}\t(\Gamma_{\rm hi} - \Gamma_{\rm lo, hi} \Gamma_{\rm lo}\inv \Gamma_{\rm lo, hi}\t)\inv \Gamma_{\rm lo, hi}\Gamma_{\rm  lo}\inv \\
C_{\rm lo, hi} = -(\Gamma_{\rm hi} - \Gamma_{\rm lo, hi} \Gamma_{\rm lo}\inv \Gamma_{\rm lo, hi}\t)\inv \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv .
\end{gathered}
\label{eq:Cinvs}
\end{equation} 
Notably, from \eqref{eq:Cinvs} we see 
\[
C_{\rm lo} = \Gamma_{\rm lo}\inv + \Gamma_{\rm lo}\inv  \Gamma_{\rm lo, hi}\t C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \quad \text{and} \quad  C_{\rm lo, hi} = - C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv. 
\]
Thus the optimum value of $\calShihat$ is given by 
\begin{equation} 
\calShihat = -C_{\rm hi}\inv C_{\rm lo, hi}\calSlo = \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \log_\Slobar(\Slo). 
\label{eq:nleq_calS}
\end{equation}
\Cref{eq:nleq_calS} is a nonlinear equation defining the regression estimate of $\Sigmahi$ when the value of $\Sigmalo$ is fixed at $\Slobar$, 
\begin{equation*}
\begin{aligned} 
\log_\Sigmahi(\Shi) &= \Gamma_{\rm lo, hi} \Gamma_{\rm lo}\inv \log_\Slobar(\Slo) \\
&\Updownarrow \\
\Sigmahi \log(\Sigmahi\inv \Shi) &= \Gamma_{\rm lo, hi} \Gamma_{\rm lo}\inv(\Slobar \log(\Slobar\inv \Slo)).
\end{aligned} 
\end{equation*}

\subsection{Proof of \cref{prop:expectedMdist}}
Notice, as before, that the squared Mahalanobis distance in \cref{eq:twomat_reg} depends on $\Sigmahi$ only through $\calShi \equiv \log_\Sigmahi \Shi$. With $\calSlo$ likewise denoting $\log_\Sigmalo \Slo$, we use $f(\cdot)$ to denote the value of the squared Mahalanobis distance objective at a particular $\calShi = \log_\Sigmahi \Shi \in \bbH_d$, 
\[
f(\calShi) = \left\langle \begin{bmatrix} \calShi \\ \calSlo \end{bmatrix}, \; \Gamma_{S} \inv \begin{bmatrix} \calShi \\ \calSlo \end{bmatrix}  \right\rangle.
\]
The value of $f(\cdot)$ at $\calShihat = \Gamma_{\rm lo, hi} \Gamma_{\rm lo}\inv \calS_{\rm lo} = -C_{\rm hi}\inv C_{\rm lo, hi}\calS_{\rm lo}$, is
\begin{align*}
f(\calShihat) &= \langle C_{\rm hi}\inv C_{\rm lo, hi}\calS_{\rm lo}, \; C_{\rm lo, hi}\calS_{\rm lo} \rangle + -2 \langle C_{\rm hi}\inv C_{\rm lo, hi}\calS_{\rm lo}, \; C_{\rm lo, hi} \calS_{\rm lo} \rangle  + \langle  \calS_{\rm lo}, \; C_{\rm lo} \calS_{\rm lo} \rangle \\
&= -\langle C_{\rm hi}\inv C_{\rm lo, hi}\calS_{\rm lo}, \; C_{\rm lo, hi}\calS_{\rm lo} \rangle + \langle  \calS_{\rm lo}, \; C_{\rm lo} \calS_{\rm lo} \rangle \\
&= -\langle C_{\rm hi}\inv (- C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv)\calS_{\rm lo}, \; - C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv\calS_{\rm lo} \rangle + \langle  \calS_{\rm lo}, \; (\Gamma_{\rm lo}\inv + \Gamma_{\rm lo}\inv  \Gamma_{\rm lo, hi}\t C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv) \calS_{\rm lo} \rangle \\
&= -\langle \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \calS_{\rm lo}, \; C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv\calS_{\rm lo} \rangle  + \langle  \calS_{\rm lo}, \; \Gamma_{\rm lo}\inv  \Gamma_{\rm lo, hi}\t C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \calS_{\rm lo} \rangle + \langle  \calS_{\rm lo}^*, \; \Gamma_{\rm lo}\inv \calS_{\rm lo} \rangle \\
&= -\langle \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \calS_{\rm lo}, \; C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv\calS_{\rm lo} \rangle  + \langle  \calS_{\rm lo} \Gamma_{\rm lo}\inv  \Gamma_{\rm lo, hi}\t, \;  C_{\rm hi} \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \calS_{\rm lo} \rangle + \langle  \calS_{\rm lo}, \; \Gamma_{\rm lo}\inv \calS_{\rm lo} \rangle \\
&= \langle  \calS_{\rm lo}, \; \Gamma_{\rm lo}\inv \calS_{\rm lo} \rangle \\
&= \langle \log_\Sigmalo \Slo, \; \Gamma_{\rm lo}\inv \log_\Sigmalo \Slo \rangle 
\end{align*}
We see that the value of the squared Mahalanobis distance associated with the minimizer $\Sigmahihat$ satisfying $\log\Sigmahihat \Shi = \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \log_\Sigmalo \Slo$ is exactly {the Mahalanobis distance between $\Slo$ and its marginal distribution}. As noted in \cite{pennec2006intrinsic}, this Mahalanobis distance has expectation $\frac{d(d+1)}{2}$,
\begin{equation} 
\begin{aligned}
	\E[f(\calShihat)] &= \E[\langle \log_\Sigmalo \Slo, \; \Gamma_{\rm lo}\inv \log_\Sigmalo \Slo \rangle] = \E[\trace{ (\log_\Sigmalo \Slo) \Gamma_{\rm lo}\inv (\log_\Sigmalo \Slo) }] \\
	&= \E[\trace{ (\log_\Sigmalo \Slo \otimes \log_\Sigmalo \Slo)\Gamma_{\rm lo}\inv}]] = \trace{\E[\log_\Sigmalo \Slo \otimes \log_\Sigmalo \Slo]\Gamma_{\rm lo}\inv } \\
	&= \trace{\Gamma_{\rm lo }\Gamma_{\rm lo }\inv} = \trace{I_{\bbH_d}}  = \ltfrac{d(d+1)}{2}.
\end{aligned}
\label{eq:mean_mdist_min}
\end{equation}

\subsection{Proof of \cref{prop:BLUEs}}
We first demonstrate that the estimator \cref{eq:cv_euclidean} is a BLUE in the Euclidean geometry for $\bbP_d$, and note that the fact that \cref{eq:cv_logEuclidean} is a BLUE in the log-Euclidean geometry for $\bbP_d$ follows by a directly analogous argument. 

In order to estimate $\Sigmahi = \E[\Shi]$ linearly in the Euclidean geometry from $\Shi$ and $\Slo$ we seek an estimator of the form
\begin{equation}
\Sigmahihat = \bfA\Shi + \bfB\Slo + \tilde C \equiv \bfA\Shi + \bfB(\Slo + C),
\label{eq:euclidean_blue}
\end{equation}
where $\bfA, \bfB: \bbH_d \to \bbH_d$ are linear and $\tilde C \in \bbH_d$ is fixed. For simplicity we assume that $\bfB$ is invertible and employ the change of variables $C = \bfB\inv \tilde C$. We want this estimator \eqref{eq:euclidean_blue} to be unbiased, 
\[
\E[\Sigmahihat] = \bfA\Sigmahi + \bfB(\Sigmalo + C) \equiv \Sigmahi,
\]
which results in the constraint 
\[
(\bfA - \bfI)\Sigmahi = \bfB(\Sigmalo + C) \iff C = \bfB\inv(\bfA - \bfI)\Sigmahi -\Sigmalo. 
\]
We substitute the constraint back into our estimator and obtain 
\begin{equation}
\Sigmahihat = \bfA\Shi + \bfB(\Slo + \bfB\inv(\bfA - \bfI)\Sigmahi -\Sigmalo) = \bfA\Shi + \bfB(\Slo - \Sigmalo) + (\bfA - \bfI)\Sigmahi. 
\label{eq:euclidean_constraint_blue}
\end{equation}
In order for our estimator \cref{eq:euclidean_blue} to be admissible, it cannot depend on $\Sigmahi$. Thus we see from \cref{eq:euclidean_constraint_blue} that we must have $\bfA \equiv \bfI$, simplifying our estimator to 
\begin{equation}
\Sigmahihat = \Shi + \bfB(\Slo - \Sigmalo). 
\label{eq:euclidean_cv_B}
\end{equation}
Now we want to choose the linear operator $\bfB: \bbH_d \to \bbH_d$ such that the Euclidean MSE of \cref{eq:euclidean_cv_B} is minimized. 
\begin{align*}
\bfB^* &= \argmin_{\bfB: \bbH_d \to \bbH_d} \E[||\Sigmahi - \Shi - \bfB(\Slo - \Sigmalo) ||_F^2] \\
 &= \argmin_{\bfB: \bbH_d \to \bbH_d} \E[\left\langle\Sigmahi - \Shi - \bfB(\Slo - \Sigmalo), \; \Sigmahi - \Shi - \bfB(\Slo - \Sigmalo)\right\rangle ] \\
 &= \argmin_{\bfB: \bbH_d \to \bbH_d} \E[\langle \Sigmahi - \Shi, \, -\bfB(\Slo - \Sigmalo) \rangle + \langle -\bfB(\Slo - \Sigmalo), \, \Sigmahi - \Shi \rangle + \langle \bfB(\Slo - \Sigmalo), \, \bfB(\Slo - \Sigmalo) \rangle ] \\
 &= \argmin_{\bfB: \bbH_d \to \bbH_d} \trace{\Psi_{\rm lo, hi}\bfB\t} + \trace{\bfB\Psi_{\rm hi, lo}} + \trace{\bfB \Psi_{\rm lo} \bfB\t},
\end{align*}
where $\Psi_{\rm lo, hi} = \E[(\Shi - \Sigmahi) \otimes (\Slo - \Sigmalo)]$, $\Psi_{\rm hi, lo} = \E[(\Slo - \Sigmalo) \otimes (\Shi - \Sigmahi)] = \Psi_{\rm lo, hi}]\t$, and $\Psi_{\rm lo} = \E[(\Slo - \Sigmalo) \otimes (\Slo - \Sigmalo)]$ are the two cross-covariances between $\Shi$ and $\Slo$ and the auto-covariance of $\Slo$. We solve for $\bfB^*$ by taking the gradient of the last line of the above with respect to $\bfB$ and setting it equal to zero, obtaining 
\[
0 = 2\Psi_{\rm lo, hi} + 2\bfB^*\Psi_{\rm lo} \iff \bfB^* = - \Psi_{\rm lo, hi}\Psi_{\rm lo}\inv.
\]
Substituting this choice of $\bfB$ into \cref{eq:euclidean_cv_B}, we see 
\[
\Sigmahihat = \Shi - \Psi_{\rm lo, hi}\Psi_{\rm lo}\inv(\Slo - \Sigmalo) = \Shi + \Psi_{\rm lo, hi}\Psi_{\rm lo}\inv(\Sigmalo - \Slo),
\]
which corresponds to the most general form of the EMF estimator \eqref{eq:cv_euclidean}. Thus, the EMF estimator is a BLUE.  The LEMF estimator \eqref{eq:cv_logEuclidean} can be shown to be a BLUE in the log-Euclidean geometry for $\bbP_d$ by a directly analogous argument.

A slight modification of our argument shows that the fixed-$\Sigmalo$ regression estimator \eqref{eq:cv_affinvar} can be thought of as a BLUE on tangent space. Because we know $\Slo$ and $\Sigmalo$, we can compute the ``difference'' $\log_\Sigmalo \Slo$. Suppose that we want to estimate $\log_\Sigmahi \Shi$ linearly from $\log_\Sigmalo \Slo$, meaning that we seek  
\[
\widehat{\log_\Sigmahi \Shi} = \bfB(\log_\Sigmalo \Slo + C),
\]
where $\bfB: \bbH_d \to \bbH_d$ is linear and $C \in \bbH_d$. Because we know $\Shi$, once we have obtained our estimate of $\log_\Sigmahi \Shi$ we can use it to solve for the corresponding estimate of $\Sigmahi$. We want our estimator to be unbiased, so we require that 
\[
\E[\widehat{\log_\Sigmahi \Shi}] = \E[\bfB(\log_\Sigmalo \Slo + C)] \equiv \E[\log_\Sigmahi \Shi].
\]
By definition, $\E[\log_\Sigmahi \Shi] = \E[\log_\Sigmalo \Slo] = 0$, which gives $C = 0$, yielding 
\begin{equation}
\widehat{\log_\Sigmahi \Shi} = \bfB \log_\Sigmalo \Slo.
\label{eq:affinvar_BLUE}
\end{equation}
We want to choose $\bfB$ such that we minimize the MSE of the estimator on $\bbH_d$, 
\begin{align*}
\bfB^* &= \argmin_{\bfB: \bbH_d \to \bbH_d} \E[\langle\bfB\log_\Sigmalo\Slo - \log_\Sigmahi \Shi, \; \bfB\log_\Sigmalo \Slo - \log_\Sigmahi \Shi \rangle] \\
&= \argmin_{\bfB: \bbH_d \to \bbH_d} \E[\langle \bfB\log_\Sigmalo \Slo, \; \bfB\log_\Sigmalo \Slo \rangle - \langle \log_\Sigmahi \Shi, \; \bfB \log_\Sigmalo \Slo \rangle - \langle \bfB\log_\Sigmalo \Slo, \; \log_\Sigmahi \Shi \rangle ] \\
&= \argmin_{\bfB: \bbH_d \to \bbH_d} \bfB\Gamma_{\rm lo}\bfB\t - \Gamma_{\rm lo, hi}\bfB\t - \bfB\Gamma_{\rm hi, lo}.
\end{align*}
The optimization objective on the last line of the above has the same form as we encountered for the Euclidean estimator, resulting in 
\[
\bfB^* = \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv 
\]
and giving the fixed-$\Sigmalo$ regression estimator 
\[
\log_{\Sigmahihat} \Shi = \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv\log_\Sigmalo \Slo,
\]
which is indeed a type of BLUE on tangent space $\bbH_d$. 

\section{Supplement to \cref{sec:metricLearning}: metric-learning with the surface quasi-geostrophic equation}
\label{app:sqg} 
\subsection{Experimental setup}
The surface quasi-geostrophic equation as presented in \cite{HeldEtAl1985,CapetEtAl2008} describes the evolution of the surface buoyancy $b: \calX \times [0, \infty) \to \R$ on the periodic spatial domain $\calX = [-\pi, \pi] \times [-\pi, \pi]$ via 
\begin{equation} 
\begin{aligned}
\frac{\partial }{\partial t} b(\bfx, t; \bftheta) + J(\psi, b) = 0, \quad z = 0 \\ 
b = \frac{\partial}{\partial z} \psi \\ 
\Delta \psi = 0,\quad z < 0 \\ 
\psi \to 0, \quad z \to -\infty, 
\end{aligned}
\label{eq:sqg_system}
\end{equation} 
where $\bfx = (x_1,x_2)$ is the surface spatial coordinate, $\psi: \calX \times (-\infty, 0] \to \R $ is the stream-function, and $J(\psi, b)$ denotes the Jacobian determinant 
\[
J(\psi, b) = \left( \frac{\partial \psi}{\partial x_1} \right) \left( \frac{\partial b}{\partial x_2} \right) - \left(\frac{\partial b}{\partial x_1} \right)\left( \frac{\partial \psi}{\partial x_2} \right).  
\]
The parameters $\bftheta \in \R^5$ determine the initial condition $b_0$ and some aspects of the dynamics \cref{eq:sqg_system}; we set 
\[
    b_0(\bfx;\bftheta) = -\frac{1}{(2\pi/ |\theta_5|)^2} \exp\left( -x_1^2 - \exp(2\theta_1) x_2^2 \right),
\]
the contours of which form ellipses parametrized by the log aspect-ratio $\theta_1$ and the amplitude $\theta_5$. The gradient Coriolis parameter $\theta_2$, log buoyancy frequency $\theta_3$, and background zonal flow $\theta_4$ all determine aspects of the dynamics.  

In our metric learning experiment in \cref{sec:metricLearning} we draw the parameters $\bftheta$ from an equal two-component Gaussian mixture, i.e., 
\[
p(\bftheta \mid i) = \calN(\bfmu_i, C) = \pi_i, \quad i \sim \mathrm{Ber}(1/2),
\]
where $\bfmu_0$ and $\bfmu_1$ differ only in their first components,
\[
\bfmu_0 = \begin{bmatrix}
1 & 0 & 0 & 0 & 4 
\end{bmatrix}\t, \quad \bfmu_1 = \begin{bmatrix}
0.1 & 0 & 0 & 0 & 4 
\end{bmatrix}\t
\]
and the parameter covariance $C$ is given by 
\[
C = \begin{bmatrix}
    0.3^2 \\ 
    & 0.003^2 \\ 
    & & 0 \\ 
    & & & 0.08^2 \\ 
    & & & & 0.3^2
\end{bmatrix}.
\]
Note that this choice of $C$ indicates that the log buoyancy frequency $\theta_3 = 0$ is deterministic, but the observational covariances $\Gamma_0$ and $\Gamma_1$ which we learn in \cref{sec:metricLearning} are still full-rank. In \cref{fig:SQG_solution} we show examples of the initial buoyancy $b_0$ and final buoyancy $b$ at time $T = 24$ for samples of $\bftheta$ from both mixture components.  
% Figure environment removed

\subsection{Additional results}
\label{app:sqg:addResults}
In the following subsections we display results pertaining to estimation of $\Gamma_0 = \Cov[\bfy \mid \bftheta \sim \pi_0]$ and $\Gamma_1 = \Cov[\bfy \mid \bftheta \sim \pi_1]$. We see in \cref{sec:metricLearning} that the best estimates of $A_{\rm GMML}$ in both the Frobenius and intrinsic metrics are obtained with $\Gamma_0$ and $\Gamma_1$ estimated via multifidelity regression, even though, as we show below, multifidelity regression is generally not the best-performing estimator for $\Gamma_0$ and $\Gamma_1$ in the Frobenius metric. This behavior is sensible when one considers that (a) $A_{\rm GMML}$ is defined as a point on a geodesic between two SPD matrices in the affine-invariant geometry, and the regression estimator, being constructed using the affine-invariant geometry, is thus the ``natural'' choice in this application, (b) while the LEMF estimator out-performs the regression estimator in the Frobenius metric for estimation of $\Gamma_1$, it does quite poorly in estimating $\Gamma_0$ and thus yields relatively poor estimates of $A_{\rm GMML}$, and (c) we are generally unable to construct $A_{\rm GMML}$ from estimates of $\Gamma_0$ and $\Gamma_1$ computed with the EMF estimator due to a high frequency (94\%) of indefiniteness of $\hat\Gamma_{0}^{\rm EMF}$ or $\hat\Gamma_1^{\rm EMF}$. 
% Figure environment removed
\subsubsection{Estimation of $\Gamma_0$}
In \cref{fig:Gamma0_fro,fig:Gamma0_AI_LE} we show squared error histograms corresponding to $\hat\Gamma_0^{\rm HF}$, $\hat\Gamma_0^{\rm LF}$, $\hat\Gamma_0^{\rm EMF}$, $\hat\Gamma_0^{\rm LEMF}$, and $\hat\Gamma_0^{\rm MRMF}$. In general $\hat\Gamma_0^{\rm LF}$, $\hat\Gamma_0^{\rm EMF}$, and $\hat\Gamma_0^{\rm MRMF}$ all yield substantial decreases in squared error relative to $\hat\Gamma_0^{\rm HF}$, while interestingly $\hat\Gamma_0^{\rm LEMF}$ results in an \textit{increase} squared error relative to $\hat\Gamma_0^{\rm HF}$, perhaps due to amplification of error by the matrix exponential. As one might expect, $\hat\Gamma_0^{\rm MRMF}$ achieves the lowest MSE in the intrinsic metric, while $\hat\Gamma_0^{\rm EMF}$ achieves lowest MSE in the Frobenius metric. At the same time, 82.4\% of realizations of $\hat\Gamma_0^{\rm EMF}$ are indefinite and thus useless for construction of $\hat A_{\rm GMML}$. 



% Figure environment removed



\subsubsection{Estimation of $\Gamma_1$}
In \cref{fig:Gamma1_fro,fig:Gamma1_AI_LE} we show squared error histograms corresponding to $\hat\Gamma_1^{\rm HF}$, $\hat\Gamma_1^{\rm LF}$, $\hat\Gamma_1^{\rm EMF}$, and $\hat\Gamma_1^{\rm MRMF}$. In general $\hat\Gamma_1^{\rm LF}$, $\hat\Gamma_1^{\rm EMF}$, $\hat\Gamma_1^{\rm LEMF}$, and $\hat\Gamma_1^{\rm MRMF}$ all yield substantial decreases in squared error relative to $\hat\Gamma_1^{\rm HF}$. In contrast to $\hat\Gamma_0^{\rm LEMF}$, $\hat\Gamma_1^{\rm LEMF}$ results in decreases, rather than increases, in MSE, relative to the high-fidelity-only estimator, but good performance in estimation of $\Gamma_1$ alone is not enough to ensure good estimates of $A_{\rm GMML}$. 
In a similar vein, the frequency with which $\hat\Gamma_1^{\rm EMF}$ is indefinite was only 67\%, a moderate decrease from the 82\% of $\hat\Gamma_0^{\rm EMF}$.
% Figure environment removed


% Figure environment removed


\section{SPD Product Manifolds}
\label{app:prodMan}
The product $\bbP_d^K = \bbP_d \times \cdots \times \bbP_d$ ($K$ times, $K\in \Z^+$) is a Riemannian manifold when equipped with tangent spaces and metric derived from $\bbP_d$. In this appendix we provide the relevant geometric and statistical definitions for $\bbP_d^K$, which in most cases follow directly from the properties of $\bbP_d$ discussed in \cref{sec:bg}.

\subsection{Geometry}
\label{app:prodMan_geom}
Let $\bfA = (A_1, \dots, A_K) \in \bbP_d^K$. The tangent space to $\bbP_d^K$ at $\bfA$ is %
\begin{equation*}
\rmT_\bfA \bbP_d^K = \mathrm{T}_{(A_1, \dots, A_K)}\bbP_d^K = \bigotimes_{k=1}^K \rmT_{A_k}\bbP_d. 
\end{equation*}
With this definition of tangent space, the Riemannian metric or \textbf{inner product} on $\bbP_d^K$ can be decomposed as follows: let $\bfU, \bfV \in \rmT_\bfA \bbP_d^K \subseteq \bbH_d^K$. We define $g_{\bfA}: \mathrm{T}_{\bfA}\bbP_d^K\times \mathrm{T}_{\bfA}\bbP_d^K \to \R$ via 
\begin{equation*}
g_{ \bfA }(\bfU, \bfV) = \langle \bfU, \; \bfV \rangle_\bfA = \sum_{k=1}^K \langle U_k,\; V_k\rangle_{A_k} = \sum_{k=1}^K \trace{U_kA_k\inv V_kA_k\inv}.
\end{equation*}
Corresponding to this inner product we have an \textbf{outer product} on $\rmT_\bfA\bbP_d^K$, 
\begin{equation*}
\bfU \otimes_{\bfA}\bfV = \begin{bmatrix} U_1 \\ \vdots \\ U_K \end{bmatrix} \otimes \begin{bmatrix}A_1\inv V_1 A_1\inv \\ \vdots \\ A_K\inv V_K A_K\inv\end{bmatrix},
\end{equation*}
where $\otimes$ is the Kronecker product and the result defines a linear mapping from $\rmT_\bfA\bbP_d^K$ to $\rmT_\bfA\bbP_d^K$. As in the $\bbP_d^1$ case, the trace of the $\bfA$ outer-product is equal to the $\bfA$ inner-product,
\begin{equation*}
\trace{\bfU \otimes_{\bfA} \bfV} = \langle \bfU,\bfV \rangle_{\bfA}. 
\end{equation*}
Bridging between $\bbP_d^K$ and $\rmT_\bfA\bbP_d^K$ we have the \textbf{logarithmic and exponential mappings} 
\begin{align*}
	\log_{\bfA}: \bbP_d^K \to \mathrm{T}_{\bfA}\bbP_d^K, \quad & \log_{\bfA}(\bfB) = (\log_{A_1}B_1,\dots, \log_{A_K} B_K) \\
	\exp_{\bfA}: \mathrm{T}_{\bfA}\bbP_d^K \to \bbP_d^K, \quad & \exp_{\bfA}(\bfX) = (\exp_{A_1} X_1,\dots, \exp_{A_K} X_K),
\end{align*}
which are simply the logarithmic and exponential mappings on $\bbP_d^1$ applied elementwise to $\bfB \in \bbP_d^K$ and $\bfX \in \rmT_\bfA \bbP_d^K$ with the corresponding entries of $\bfA$. 


Geodesics and distance on the product manifold $\bbP_d^K$ are given as follows: %
Let $\bfA, \bfB \in \bbP_d^K$.  If $\gamma_1(t), \dots, \gamma_K(t)$ are the geodesics from $A_1, \dots, A_K$ to $B_1, \dots, B_K$, respectively, on $\bbP_d$, then the \textbf{geodesic} from $\bfA $ to $\bfB$ on $\bbP_d^K$ is given by
\begin{equation}
	\begin{aligned}
		\gamma_{\bfA \to \bfB}(t) &= (\gamma_1(t),\dots, \gamma_K(t)) \\
		&= \left(A_1^{\frac{1}{2}}(A_1^{-\frac{1}{2}} B_1 A_1^{-\frac{1}{2}})^t A_1^{\frac{1}{2}},\dots, A_K^{\frac{1}{2}}(A_K^{-\frac{1}{2}} B_K A_K^{-\frac{1}{2}})^t A_K^{\frac{1}{2}} \right).
	\end{aligned}
	\label{eq:geodesicPnK}
\end{equation}

The \textbf{intrinsic distance} between $\bfA$ and $\bfB$ on $\bbP_d^K$ follows accordingly from \cref{eq:intrinsicdist},
\begin{align*}
	d^2(\bfA, \bfB) &= \sum_{k=1}^K d(A_k, B_k)^2
	= \sum_{k=1}^K \left\|\log(A_k^{-\frac{1}{2}}B_k A_k^{-\frac{1}{2}}) \right\|_F^2 
	= \sum_{k=1}^K \sum_{i=1}^d \log^2\lambda_i(A_k\inv B_k).
\end{align*}

\subsection{Statistics}
\label{app:prodMan_stats}
Let $\bfS = (S_1, \dots, S_K)$ be a $\bbP_d^K$-valued random variable. In constructing our multifidelity covariance estimator we employ the following notions of statistics for $\bfS$ which are consistent with the general definitions in \cite{pennec2006intrinsic} but, due to the product manifold structure of $\bbP_d^K$ (\cref{app:prodMan_geom}), in many cases have nice decompositions to statistics on $\bbP_d^1$. 

To begin, the \textbf{expectation} of $\bfS$ is the element $\bfY \in \bbP_d^K$ which minimizes the expected squared distance to $\bfS$, 
\begin{equation}
\begin{aligned}
\bfE[S] &= \argmin_{\bfY \in \bbP_d^K} \E\left[d^2(\bfY, \bfS)\right]
= \argmin_{Y \in \bbP_d^K} \E\left[\sum_{k=1}^K d^2(Y_k, S_k) \right] 
= \argmin_{Y \in \bbP_d^K} \sum_{k=1}^K \E \left[d^2(Y_k, S_k)\right] \\
 &=  (\bfE[S_1], \dots, \bfE[S_K]) = (\Sigma_1, \dots, \Sigma_K) \equiv \bfSigma. 
\end{aligned}
\label{eq:prodman_mean}
\end{equation}
Because the squared distance between $\bfS$ and $\bfY$ decomposes into the sum of $K$ squared distances between the individual components of $\bfS$ and $\bfY$, to obtain the (Frechet) mean of $\bfS$ we simply take the Frechet mean of each component of $\bfS$. 

The \textbf{variance} of $\bfS$ is defined as the expected squared distance between $\bfS$ and its mean, 
\begin{align*}
	\sigma^2_\bfS &= \E[d^2(\bfS, \bfSigma)] = \E \left[ \sum_{k=1}^K d^2(S_k, \Sigma_k) \right] = \sum_{k=1}^K \sigma^2_{S_k},
\end{align*}
where $\sigma^2_{S_k}$ is the variance of $S_k \in \bbP_d$, $k = 1, \dots, K$.  

As in the case of $\bbP_d^1$-valued random variables in \cref{sec:bg}, the \textbf{covariance} of $\bfS$ is the $\bfSigma$-outer-product of the ``vector difference'' $\log_\bfSigma \bfS$ with itself, 
\begin{equation*}
\Cov[\bfS] = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS] \equiv \Gamma_\bfS.
\end{equation*}
$\Gamma_\bfS$ is a symmetric positive semidefinite linear operator from $\bbH_d^K = \rmT_\bfSigma \bbP_d^K$ to $\rmT_\bfSigma \bbP_d^K = \bbH_d^k$. As on $\bbP_d^1$, we have $\trace{\Gamma_\bfS} = \sigma^2_\bfS$. %

Finally given $\bfS \sim (\Sigma, \Gamma_\bfS)$, we define the \textbf{Mahalanobis distance} between $\bfS$ and a deterministic point $\bfY \in \bbP_d^K$
\begin{equation} 
d^2_\bfS(\bfY) = \langle \log_\bfSigma \bfY, \; \Gamma_\bfS\inv \log_\bfSigma \bfY \rangle_\bfSigma. 
\label{eq:prodMan}
\end{equation} 
The Mahalanobis distance is a $\Gamma_\bfS\inv$-weighted version of the intrinsic distance between $\bfSigma$ and $\bfY$, and, unless $\Gamma_\bfS$ is ``diagonal'' (in the appropriate sense), in general \textit{cannot} be decomposed into a sum of pairwise distances between $\Sigma_k$ and $Y_k$. The $\Gamma_\bfS\inv$ weighting introduces interaction between separate components of $\log_\bfSigma \bfY$. 



\bibliographystyle{siamplain}
\bibliography{references}

\makeatletter\@input{xx.tex}\makeatother
\end{document}
