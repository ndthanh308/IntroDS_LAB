\documentclass[12pt, final]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} 
\usepackage{cite}
\usepackage{mauraisStyle}
\usepackage{cleveref}
\usepackage[normalem]{ulem}

\usepackage[textsize=tiny]{todonotes}
\setlength{\marginparwidth}{2.2cm}

\definecolor{darkgreen}{rgb}{.15,.55,0}
\newcommand{\ymm}[1]{\todo[linecolor=darkgreen,backgroundcolor=darkgreen!25]{#1}}

\newcommand{\rmT}{\mathrm{T}}

\newcommand{\Shi}{{S_{\rm hi}}}
\newcommand{\Slo}{{S_{\rm lo}}}

\newcommand{\ghi}{{g_{\rm hi}}}
\newcommand{\glo}{{g_{\rm lo}}}

\renewcommand{\chi}{{c_{\rm hi}}}
\newcommand{\clo}{{c_{\rm lo}}}

\newcommand{\Bhi}{{B_{\rm hi}}}
\newcommand{\Blo}{{B_{\rm lo}}}

\newcommand{\Xhi}{{X_{\rm hi}}}
\newcommand{\Xlo}{{X_{\rm lo}}}
\newcommand{\XloOne}{{X_{\mathrm{lo}, 1}}}
\newcommand{\XloL}{{X_{\mathrm{lo}, L}}}
\newcommand{\Xloell}{{X_{\mathrm{lo}, \ell}}}
\newcommand{\Xmf}{X_{\rm mf}}

\newcommand{\Xhii}{{X_{\rm hi}^{i}}}
\newcommand{\Xloi}{{X_{\rm lo}^{i}}}

\newcommand{\Xhij}{{X_{\rm hi}^{j}}}
\newcommand{\Xloj}{{X_{\rm lo}^{j}}}

\newcommand{\Xhini}{{X_{\rm hi}^{n, i}}}
\newcommand{\Xloni}{{X_{\rm lo}^{n, i}}}

\newcommand{\Sigmahi}{{\Sigma_{\rm hi}}}
\newcommand{\Sigmalo}{{\Sigma_{\rm lo}}}

\newcommand{\sigmahi}{{\sigma_{\rm hi}}}
\newcommand{\sigmalo}{{\sigma_{\rm lo}}}

\newcommand{\Gammahi}{{\Gamma_{\rm hi}}}
\newcommand{\Gammalo}{{\Gamma_{\rm lo}}}

\newcommand{\calShi}{{\calS_{\rm hi}}}
\newcommand{\calSlo}{{\calS_{\rm lo}}}

\newcommand{\ShiMnot}{{S_{\text{hi}}^{M_0}}}
\newcommand{\SloMnot}{{S_{\text{lo}}^{M_0}}}
\newcommand{\SloM}{{S_{\text{lo}}^{M}}}

\newcommand{\Sigmahihat}{{\hat{\Sigma}_{\text{hi}}}}
\newcommand{\Sigmalohat}{{\hat{\Sigma}_{\text{lo}}}}

\newcommand{\SloOne}{{S^1_{\text{lo}}}}
\newcommand{\SloTwo}{{S^2_{\text{lo}}}}

\newcommand{\Slon}{{S^n_{\text{lo}}}}
\newcommand{\Shin}{{S^n_{\text{hi}}}}

\newcommand{\kibitz}[2]{\textcolor{#1}{#2}}
\renewcommand{\aimee}[1]{\kibitz{violet}{[AM: #1]}}

\begin{document}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

\section{Available Content}
\subsection*{Theory and Computation}
\begin{enumerate}[I.]
    \item Regression estimator formulation
        \begin{enumerate}[(a)]
            \item Statistically coupled samples $\Rightarrow$ statistically coupled sample covariance matrices 
            \item Sample covariance matrices $\bfS$ as a random variable 
            \item Interpretation of Riemannian covariance
            \item Mahalanobis distance minimization 
            \item Resulting additive statistical model on tangent space 
        \end{enumerate}
    \item Properties of the Mahalanobis distance 
        \begin{enumerate}[(a)]
            \item Tangent-space agnostic 
            \item Affine-invariant 
        \end{enumerate}
    \item Computation 
        \begin{enumerate}[(a)]
        \item Square root formulation 
            \begin{itemize}
                \item Comment on why we don't use manifold optimization 
                \item Benefit: can use unconstrained methods, easy to implement 
                \item Drawback: can yield semidefinite estimates 
            \end{itemize}
        \item Regularization in the intrinsic distance 
            \begin{itemize}
                \item Enforces strict positive definiteness 
                \item \sout{Can be interpreted as a Bayesian posterior if we assume matrix-normal likelihood $\pi(\log_\Sigma(\bfS) \mid \bfSigma)$ and prior $\pi(\log_\bfA(\bfSigma))$, where $\bfA$ is the prior mean.} \aimee{Actually it cannot due to a gnarly Jacobian determinant term that the Mahalanobis distance doesn't capture - sad :( }
            \end{itemize}
        \item Fixed $\Sigma$-lo simplification
            \begin{itemize}
                \item ``Sensible'' thing to do when number of LF samples is high 
                \item Mahalanobis distance minimization reduces to solution of a nonlinear equation for $\Sigmahi$, which can be interpreted as a control variate equation.
                \item Provides an admissible way to pick regularization parameter $\lambda$ because we know what the expected Mahalanobis distance is in this case. 
            \end{itemize}
        \end{enumerate} 
    \item Link to control variates 
        \begin{enumerate}[(a)]
            \item Fixed $\Sigmalo$ simplification yields control variate equation 
            \item Comparison with Euclidean and Tangent space estimators (with general linear-operator valued weights): each minimizes squared error in its corresponding metric  
            \item Broader connections: scalar BLUEs of Schaden \& Ullmann, other scalar multifidelity estimators 
        \end{enumerate}
\end{enumerate}

\subsection*{Numerical Examples}
\begin{enumerate}[I.]
    \item ``Cooked up'' Gaussian example 
    \item Surface QGE model, assuming that we can find good settings 
    \item 1-D heat equation (Terrence) 
    \item PCA/classification (Terrence) 
    \item Metric learning (Terrence) 
    \item \aimee{Do we have an example in which the low-fidelity model is {not} a finer discretization of the high-fidelity model, but instead is a surrogate -- ROM, simplified physics, or data-fit/machine learning? Could consider the KL/PCE heat equation example from 16.940, because that gives a lot of knobs to turn} 
\end{enumerate}

\newpage 
\section{Paper Outline}

\subsection{Introduction}

Covariance estimation broadly useful (examples of where, including but not limited to DA). Huge range of work, different regimes. Undersampled regime: zoo of regularization methods (shrinkage, sparsity, localization/tapering). 

Need for \textit{multi-fidelity} covariance estimation.

Interesting/unique challenges of this problem. Note previous work: lack of definiteness. Underlying issue: estimator does not respect geometry of the underlying parameter.

This paper---contributions:
\begin{itemize}
    \item Formulate MF covariance estimation as regression on the SPD manifold, according to the intrinsic geometry
    \item Discuss and demonstrate numerical solution of this problem. Good performance, etc.
    \item Show that our framework is an instance of a general approach to MF estimation in different geometries. Yields CV-type estimators, also regression-type estimators. 
\end{itemize}

Outline, briefly.


\subsection{Background} 
\begin{enumerate}
    \item SPD manifold
    \item Intrinsic statistics \aimee{Definitions of mean, covariance. cf Pennec 2006.}
    \item Multifidelity estimation: basic idea and related previous work \aimee{Or should previous work go in the intro?}\ymm{Only brief mention in intro, but main review here?} 
    
    \item Previous work on multivariate covariance estimation specifically (see refs from AM thesis)
        \begin{itemize}
        \item ML variance estimation (EQ) + other paper in AM thesis
        \item ML EnKF (Law et al.\ paper)
        \item MF EnKF (covariance of a control variate)
    \end{itemize}
\end{enumerate}

\subsection{Estimator Formulation}
\begin{enumerate}
    \item Bifidelity sampling setup (coupled pairs plus independent set) 
    \item $\bfS$ as a random variable 
    \item Riemannian covariance \aimee{I previously had these last two items in the next section}
    \item Mahalanobis distance minimization 
    \item Resulting additive model on tangent space 
\end{enumerate}
\subsection{Analysis, Connections, Simplifications, etc.}
\begin{enumerate}
    \item Properties of Mahalanobis Distance 
        \begin{enumerate}
            \item Tangent-space agnostic 
            \item Affine-invariant 
        \end{enumerate}
    \item Fixed-$\Sigmalo$ simplification 
        \begin{enumerate}
            \item Nonlinear equation for $\Sigmahi$ 
            \item Analytical expected Mahalanobis distance 
        \end{enumerate}
    \item Multifidelity estimation in general geometries
    \begin{enumerate}
        \item Interpretation of fixed-$\Sigmalo$ estimator as control variates 
        \item Connection to other multifidelity estimators -- covariance (including our own) and otherwise  
    \end{enumerate}
\end{enumerate}

\subsection{Computational considerations}
\begin{enumerate}
    \item Square root parametrization (and why not manifold optimization) 
    \item Regularization in the intrinsic metric 
        \begin{enumerate}
            \item Enforces positive definiteness in square root parametrization 
            \item Regularization parameter in the fixed $\Sigmalo$ simplification can be picked by matching the mean objective function to $d(d+1)/2$
        \end{enumerate} 
\end{enumerate}
\subsection{Numerical Results}
\aimee{Whatever results we decide to include}
\subsection{Conclusion} 



\end{document}
