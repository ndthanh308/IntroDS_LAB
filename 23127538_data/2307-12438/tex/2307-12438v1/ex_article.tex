\documentclass[final,onefignum,onetabnum]{siamonline220329}


\input{ex_shared}

\graphicspath{{figures/}}

\ifpdf
\hypersetup{
  pdftitle={Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices},
  pdfauthor={A.\@ Maurais, T.\@ Alsup, B.\@ Peherstorfer, Y.\@ Marzouk}
}
\fi


\externaldocument[][nocite]{ex_supplement}


\makeatletter 
\@mparswitchfalse%
\makeatother
\normalmarginpar

\begin{document}

\maketitle
\begin{abstract} 
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates.  We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential. 
\end{abstract}

\begin{keywords}
covariance estimation, multifidelity methods, Riemannian geometry, statistical coupling, estimation on manifolds, Mahalanobis distance 
\end{keywords}

\begin{MSCcodes}
15B48, 15B57, 53Z50, 62J02, 65J10, 65J15, 65J20
\end{MSCcodes}

\section{Introduction}
In the words of \cite{ledoit2022power}, the covariance matrix is ``arguably the second most important object in all of statistics.'' Covariance matrices are key objects in portfolio theory \cite{markowitz1952portfolio} and spatial statistics \cite{cressie2015statistics}. In Bayesian inference, covariance matrices are the essential elements of prior distributions for inverse problems \cite{kaipio2006statistical}, and they dictate the extent to which predictive models are corrected by observations in Kalman-type data assimilation \cite{evensenekf, kalmanfilter} and inversion \cite{iglesias2013ensemble} schemes. In data science and machine learning, covariance matrices underlie principal component analysis \cite{ringner2008principal}, perhaps the most canonical method for dimension reduction, and arise in downstream tasks such as metric learning \cite{kulis2013metric}.
 
Covariance matrices are usually estimated from data, and often the biggest hurdle to doing so is insufficient sample information:
in many applications, data are expensive and the number of samples we can practically obtain may be on the order of the parameter dimension. For this reason there has been extensive development of regularization methods for covariance estimation in the small-sample regime, including 
shrinkage \cite{ledoit2022power,ledoit2012nonlinear,donohogavishjohnstone}, enforcement of sparsity in the precision matrix \cite{bickellevina,graphicallasso}, and localization/tapering \cite{gasparicohn,localizationEnKF}. 

Small-sample covariance estimators generally assume access to a low number of identically distributed samples, which in the context of computational and data science we might associate with the output of a single computational model. We refer to this model as the \textit{high fidelity} model and assume that, in addition to being costly to evaluate, it retains a high-level of veracity to the physical process it seeks to capture. For example, this high-fidelity model may correspond to computationally intensive dynamic simulations as encountered in numerical weather prediction and aerodynamic modeling. 
In such physically-driven applications, however, we do not usually have only \textit{one} model at our disposal. Rather, we may additionally have access to any number of \emph{lower-fidelity models} obtained, e.g., via coarser discretizations, machine learning surrogates, or reduced physics approximations of the high-fidelity model. Lower-fidelity models are generally much cheaper to sample but less accurate than the high-fidelity model. 

Scarcity of high-fidelity samples is an issue for many tasks in computational and data science, and for this reason a wide range of \textit{multifidelity} and \textit{multilevel} methods \cite{giles2015multilevel,cliffe_multilevel_2011,NME:NME4761,PWK16MFMCAsymptotics,GORODETSKY2020109257} have been developed to exploit the model hierarchies that exist in applications. The idea is simple: rather than devoting all of our computational resources to high-fidelity model evaluations, we judiciously allocate our budget among evaluations of high \textit{and} lower-fidelity models, and in doing so achieve better performance for the same cost. The scope and range of applications of such multifidelity methods are vast, and we do not attempt to summarize them here; see \cite{multifidelityReview} for a comprehensive review. Among current multifidelity approaches, the best linear unbiased estimator (BLUE) framework of \cite{schaden2020multilevel,schaden2021asymptotic} is an inspiration for our effort, as we shall describe below; it uses generalized linear regression to obtain multilevel estimates of scalar quantities of interest.

To our knowledge, however, multifidelity methods have only recently been brought to bear on covariance estimation. Multifidelity covariance estimation is particularly challenging because covariance matrices have geometric properties that an estimator must respect: namely, symmetry and positive semi-definiteness. 
Straightforward application of techniques designed for Euclidean data, including multilevel Monte Carlo \cite{giles2015multilevel,cliffe_multilevel_2011}, multifidelity Monte Carlo \cite{NME:NME4761,bpkwmg} and multilevel BLUEs \cite{schaden2020multilevel}, to covariance matrices may yield results which are not positive semidefinite, and therefore not covariance matrices. 

\subsection{Multifidelity covariance estimation: literature review} 
\label{sec:relatedWork}
Multifidelity and multilevel covariance estimators in the current literature are most often specialized to the one-dimensional case; i.e., they are multifidelity and multilevel estimators of \textit{scalar} variances and covariances. 
Convergence of multilevel Monte Carlo variance estimators is discussed in \cite{bierig2015convergence} and similar analysis concerning multilevel Monte Carlo estimation of scalar covariances 
can be found in \cite{mycek2019multilevel};  both employ control variates and typical multilevel assumptions on the rates of error decay and cost increase with increasing model ``level.'' Multifidelity control variate estimators of variance and Sobol sensitivity indices are developed in \cite{qian2018multifidelity}; the framework employed therein is rate-free, 
and the corresponding optimal estimators are formulated in terms of correlations between model fidelities.

The earliest approaches to multilevel and multifidelity covariance \textit{matrix} estimation are largely embedded in works on multifidelity and multilevel data assimilation, 
in which low-fidelity samples are used to improve an estimate of a quantity-of-interest depending on a high-fidelity covariance matrix, such as the Kalman gain operator. For instance, at each step of the multilevel EnKF (MLEnKF) \cite{mlenkf} a multilevel covariance estimate is constructed using the trademark ``telescoping sum'' of multilevel Monte Carlo, which, due to the presence of subtraction, can induce loss of positive-definiteness. %
Loss of definiteness in the MLEnKF is corrected in a post-hoc manner by rounding negative eigenvalues up to zero, but %
the authors note that ``it would be of independent interest to devise multilevel [covariance] estimators which preserve positivity without such an imposition.'' 

 


There has been some development to this end, namely the positive-definite multifidelity covariance estimators of \cite{maurais2023logEuclidean}, constructed using control variates in the log-Euclidean geometry \cite{arsigny2006log} for symmetric positive definite (SPD) matrices; we will compare to these estimators in the present work. 
Other recent approaches to multifidelity/multilevel covariance estimation, such as the data-sparse multilevel covariance estimation of \cite{dolzDataSparseMultilevel2023} and a multivariate generalization \cite{destouches2023multivariate} of the scalar multilevel BLUEs of \cite{schaden2020multilevel}, rely on the Euclidean geometry for symmetric matrices and hence do not ensure positive-definite results. %


\subsection{Contributions}
In this paper, we formulate multifidelity covariance estimation as a regression problem on the manifold of SPD matrices equipped with the affine-invariant geometry \cite{bhatiaposdef}. We take our inspiration from the regression framework of \cite{schaden2020multilevel} but operate within a {Riemannian}, rather than Euclidean, geometry for SPD matrices and thus obtain guaranteeably positive-definite results. Our manifold regression multifidelity (MRMF) estimator can furthermore be seen as a generalization of control-variate type multifidelity estimators, including those in \cite{maurais2023logEuclidean}; we show that such estimators can be obtained as simplifications of the regression framework we present here. 
We discuss the numerical implementation of our estimator, introducing regularization schemes and a parameterization enabling the use of unconstrained optimization methods. We show via numerical examples that our estimator can yield significant reductions in covariance estimation error and improved performance in downstream tasks, such as metric learning, relative to single-fidelity {and} existing multifidelity estimators.


\medskip

The rest of the paper is organized as follows. \Cref{sec:bg} reviews some necessary background. In \Cref{sec:formulation} we introduce our estimator. In \Cref{sec:analysis} we discuss its properties, connections to existing multifidelity estimators, and generalizations. We discuss computational considerations in \Cref{sec:computation} and demonstrate the estimator's performance in two numerical examples in \Cref{sec:numerics}; then we close and provide some outlook in  \Cref{sec:conclusion}. 

\section{Background}
\label{sec:bg}

\subsection{The manifold of SPD matrices}
\label{sec:spdManifold}
The set of $d \times d$ symmetric positive definite matrices, which we denote by $\bbP_d$, forms a \textit{Riemannian manifold} embedded in the vector space of $d \times d$ symmetric matrices $\bbH_d$. The manifold $\bbP_d$ is locally similar to $\bbH_d$ at each point $A \in \bbP_d$, and at each $A \in \bbP_d$ we define the \textit{tangent space} $\rmT_A\bbP_d  \subseteq \bbH_d$ with a unique inner product. In the development of our estimator (\Cref{sec:formulation}) we make use of this inner product along with its corresponding outer product, geodesics, and geodesic distance. We introduce these concepts here briefly and direct the reader interested in a more rigorous treatment to \cite{bhatiaposdef}. %

Let $A \in \bbP_d$, and $U, V \in \mathrm{T}_A\bbP_d \subseteq \bbH_d$. The \textbf{inner product} on $\rmT_A\bbP_d$, $g_A(\cdot, \cdot): \mathrm{T}_A\bbP_d\times \mathrm{T}_A\bbP_d \to \R$, is defined as a weighted Frobenius inner product $\langle \cdot, \cdot \rangle$ for symmetric matrices, 
\begin{equation}
g_A(U, V) = \langle U, \; V \rangle_A = \langle U, \; A\inv VA\inv \rangle = \trace{U A\inv V A\inv}.
\label{eq:spd_ip}
\end{equation}
This inner product gives rise to a corresponding \textbf{outer product} on $\rmT_A \bbP_d$, equivalent to the Euclidean outer product for symmetric matrices with the same transformation applied to the second argument, 
\begin{align*}
	U \otimes_A V& = U \otimes (A\inv V A\inv).
\end{align*}

In addition to inner- and outer-products, for given $A \in \bbP_d$ there exist diffeomorphic \textbf{logarithmic} and \textbf{exponential} mappings which connect $\bbP_d$ and $\rmT_A \bbP_d$. Let $A, B \in \bbP_d$. The mapping $\log_A: \bbP_d \to \mathrm{T}_A\bbP_d \subseteq \bbH_d$ which takes elements from $\bbP_d$ to the tangent space at $A$ is 
\begin{equation}
\log_A(B) = A^{\frac{1}{2}}\log(A^{-\frac{1}{2}}BA^{-\frac{1}{2}})A^{\frac{1}{2}} = A\log(A\inv B).
\label{eq:logA}
\end{equation}
Now let $X \in \mathrm{T}_A\bbP_d$. The mapping $\exp_A: \mathrm{T}_A\bbP_d \to \bbP_d$ which takes objects from the tangent space located at $A$ back to the manifold, is given by
\begin{equation}
\exp_A(X) = A^{\frac{1}{2}}\exp(A^{-\frac{1}{2}}XA^{-\frac{1}{2}})A^{\frac{1}{2}} = A\exp(A\inv X).
\label{eq:expA}
\end{equation}
The first forms of \cref{eq:logA} and \cref{eq:expA} make explicit the fact that $\log_A$ and $\exp_A$ produce symmetric outputs, while the second can be advantageous in analysis and computation.

The inner-product \eqref{eq:spd_ip} defines a {natural metric} on $\bbP_d$, giving rise to notions of geodesics and distance. For $A, B \in \bbP_d$, the \textbf{geodesic}, or shortest path, on $\bbP_d$ between $A$ and $B$ is
\begin{equation}
\gamma(t) = A^{1/2}(A^{-1/2}BA^{-1/2})^tA^{1/2}, \quad t\in [0,1].
\label{eq:geodesic}
\end{equation}
One can confirm that $\gamma(0) = A$ and $\gamma(1) = B$. The \textbf{intrinsic distance} between $A$ and $B$ is equal to the length of this geodesic and is %
\begin{equation}
	d(A,B) =  \sqrt{\langle \log_A B,\; \log_A B\rangle_A }
	= ||\log(A^{-1/2}BA^{-1/2})||_{\rm F} 
	= \left(\sum_{i=1}^d\log^2\lambda_i(A\inv B) \right)^{\frac{1}{2}}. 
	\label{eq:intrinsicdist}
\end{equation} 

In defining our multifidelity covariance estimator we primarily work with \textit{product manifolds} of SPD matrices, i.e., $\bbP_d^K = \bbP_d \times \cdots \times \bbP_d$ ($K$ times) where $K \in \Z^+$. $\bbP_d^K$ is itself a Riemannian manifold with geometry obtained by extension of the geometry of $\bbP_d$; see \cref{app:prodMan_geom} for details. 

\subsection{Statistics on the manifold}  
\label{sec:intrinsicStatistics}
Utilizing definitions in \cite{pennec2006intrinsic} with the geometry described above, we obtain notions of mean, variance, and covariance for a $\bbP_d$-valued random matrix $S$. As with the geometry, the extension of these statistics to product-manifold-valued random variables is straightforward and described in \cref{app:prodMan_stats}.

Let $S \in \bbP_d$ be random. We define the \textbf{expectation} of $S$ to be the \textit{Frechet mean} of $S$, that is, the point $\Sigma \in \bbP_d$ which minimizes the expected squared distance to $S$,
\begin{equation}
\begin{aligned}
	\bfE[S] &= \argmin_{Y \in \bbP_d} \E\left[d^2(Y, S)\right]
	= \argmin_{Y \in \bbP_d} \E\left[||\log(Y^{-1/2}SY^{-1/2})||_{\rm F}^2\right] \equiv \Sigma.
\end{aligned}
\label{eq:frechet_mean}
\end{equation}
Because $\bbP_d$ is a complete Riemannian manifold with nonpositive curvature \cite{bhatiaposdef}, this mean is unique \cite{pennec2006intrinsic}. 

The \textbf{variance} of $S$ is the expected squared distance between $S$ and its mean $\Sigma = \bfE[S]$,
\begin{align*}
	\sigma^2_{S} &= \E[d^2(\Sigma, S)]  = \E\left[||\log(\Sigma^{-1/2}S\Sigma^{-1/2})||_{\rm F}^2\right]. \\
\end{align*}
In other words, the variance $\sigma^2_S$ is the {minimum} over $Y \in \bbP_d$ of $\E\left[||\log(Y^{-1/2}SY^{-1/2})||_{\rm F}^2\right]$, while $\Sigma = \bfE[S]$ is the corresponding minimizer. %

Next we define a notion of covariance for $S \in \bbP_d$. Recall that for random $x \in \R^n$ with mean $\mu$, the covariance of $x$ is the expected outer product of the vector difference between $x$ and $\mu$ with itself, 
\begin{equation*}
\Cov[x] = \E[(x - \mu)(x - \mu)\t] = \E[(x - \mu) \otimes (x - \mu)]. 
\end{equation*}
Because $\R^n$ is a vector space, the vector difference $x - \mu$ is an element of $\R^n$ and $\Cov[x] \in \R^{n \times n}$ defines a symmetric positive definite linear operator from $\R^n$ to $\R^n$. 

The SPD manifold is \textit{not} a vector space, so the {covariance} of $S \in \bbP_d$ cannot be defined directly on $\bbP_d$. Thus we define the \textbf{covariance} of $S$ on the {tangent space}  to $\bbP_d$ at $\Sigma$, setting 
\begin{align}
	\Cov[S] = \E[\log_\Sigma S \otimes_\Sigma \log_\Sigma S] \equiv \Gamma_S.
	\label{eq:gammaS_def}
\end{align}
This covariance \cref{eq:gammaS_def} shares the structure of the traditional vector covariance in that it is an expected outer product of a function of $S$ and its mean $\Sigma$. This function, $\log_\Sigma S$, we interpret as the ``vector difference'' between $S$ and $\Sigma$ \cite{pennec2006intrinsic, pennec2006riemannian}. $\log_\Sigma S \in \rmT_\Sigma \bbP_d$ is the mapping of $S \in \bbP_d$ onto $\rmT_\Sigma \bbP_d$, the tangent space associated with $\Sigma$: if $S = \Sigma$ then $\log_\Sigma S = 0^{d\times d}$, and if $S \neq \Sigma$ then $S$ has a nonzero image under $\log_\Sigma(\cdot)$. 

$\Gamma_\bfS$ is a symmetric  positive semidefinite linear operator on $\rmT_\Sigma \bbP_d \subseteq \bbH_d$.
Note that the trace of $\Gamma_S$ is indeed the variance $\sigma^2_S$,
\begin{align*}
\trace{\Gamma_S} &= \trace{\E[\log_\Sigma S \otimes_\Sigma \log_\Sigma S ]} = \E[\trace{\log_\Sigma S \otimes_\Sigma \log_\Sigma S }] \\
 &= \E[\langle \log_\Sigma S,\; \log_\Sigma S \rangle_\Sigma] = \E[d^2(\Sigma, S)] \equiv \sigma^2_S,
\end{align*}
where we have used that the trace of the $\Sigma$-outer-product is equal to the $\Sigma$-inner-product. %

Using the covariance of $S$ we define a notion of (squared) \textbf{Mahalanobis distance} between $S$ and a deterministic point $Y \in \bbP_d$,
\begin{align}
	d^2_S(Y;\, \Sigma) = \langle \log_\Sigma Y,\; \Gamma_S\inv \log_\Sigma Y \rangle_\Sigma.
	\label{eq:mdist_def}
\end{align}
The Mahalanobis distance is a $\Gamma_S\inv$-weighted version of the intrinsic distance \eqref{eq:intrinsicdist} between $\Sigma$ and $Y$ and is analogous to the Mahalanobis distance for vector-valued random variables. In writing \cref{eq:mdist_def} we have chosen to explicitly highlight the dependence on $\bfE[S] = \Sigma$ because for the remainder of our development $\Sigma$ will generally be unknown.






\section{Estimator formulation}
\label{sec:formulation}
In this section we introduce the basic formulation of our estimator, encompassing assumptions on how data are sampled, a model for the data on SPD product manifolds, and the resulting optimization problem we solve to obtain multifidelity covariance estimates. %

\subsection{Problem setup}
Let $[S_0]$ denote an equivalence class of random matrices $S \in \bbP_d$ such that $\bfE[S] = \Sigma_0$, i.e., $[S_0] = \{S \in \bbP_d: \bfE[S] = \Sigma_0 \}$.\footnote{We introduce these classes because in practice it is common to employ covariance estimators which possess the same mean but have different distributions, e.g., due to different sample sizes. The key structure of our formulation involves grouping such estimators by their means, corresponding to different levels of fidelity.
} 
Suppose that we are able to sample elements of $[S_0]$ at high computational cost and would like to estimate the unknown mean matrix $\Sigma_0 \in \bbP_d$. At the same time we are able to sample from a number of related \textit{low-fidelity} equivalence classes, $[S_i] = \{S \in \bbP_d: \bfE[S] = \Sigma_i\}$, $i \in \{1, \dots, L\}$, at comparatively lower computational costs. The low-fidelity mean-matrices $\Sigma_1, \dots, \Sigma_L$ are also unknown and may be of some interest to estimate, but our primary objective is to estimate $\Sigma_0$. %

We assume that we can obtain \textit{statistically coupled} samples from any combination of the equivalence classes $[S_0], \dots, [S_L]$. Specifically, letting $F = (F^k)_{k=1}^K \subseteq 2^{\{0, \ldots, L\}}$ represent $K$ subsets of the indices $\{0, \dots, L\}$, our data consist of $K$ collections of samples from $[S_0], \dots, [S_L]$,
\begin{equation}
\left(S_i^{(k)}: i \in F^k \right) , \quad k = 1, \dots, K, 
\label{eq:Sdata}
\end{equation} 
which accordingly have expectations 
\[
\bfE \left[(S_i^{(k)}: i \in F^k)\right] = (\Sigma_i: i \in F^k), \quad k = 1, \dots, K.
\] 
The collections $(S_i^{(k)}: i \in F^k)$ are generated such that for each $k \in \{1, \dots, K\}$ the random matrices $S_i^{(k)}$, $i \in F^k$ are {correlated} with each other, but $S_i^{(k)}$ is independent of $S^{(\ell)}_j$ (written as $S_i^{(k)} \indep S^{(\ell)}_j$) for any $\ell \neq k$, $i, j \in \{1, \dots, L\}$. Note that for $i \in \{0, \dots, L\}$ and $j \neq k$ we do \textit{not} assume that $S_i^{(j)} \overset{d}{=} S_i^{(k)}$; rather we only assume equivalence of means $\bfE[S_i^{(j)}] = \bfE[S_i^{(k)}] = \Sigma_i$. A convenient way to visualize this equivalence-class/coupling structure is via a table, which we illustrate in \cref{tab:dataStructure} for an example with $L = 3$.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c}
         & $[S_0]$ & $[S_1]$ & $[S_2]$ & $[S_3]$  \\
         \hline
      $k = 1$   &  $S_0^{(1)}$ & $S_1^{(1)}$ & & \\ 
      $k = 2$   &  & $S_1^{(2)}$ &  $S_2^{(2)}$ &  \\
      $k = 3$   &  & $S_1^{(3)}$ & $S_2^{(3)}$ & $S_3^{(3)}$ \\
      $k = 4$   & & & & $S_3^{(4)}$ 
    \end{tabular}
    \caption{Example data \cref{eq:Sdata} corresponding to $L = 3$ with $F^1 = \{0, 1\}$, $F^2 = \{1, 2\}$, $F^3 = \{1, 2, 3\}$, and $F^4 = \{3\}$. Matrices within the same \textit{column} of the table have the same mean, $\bfE[S_i^{(k)}] = \bfE[S_i^{(j)}] = \Sigma_i$, while matrices within the same \textit{row} are statistically coupled with each other, $S_i^{(k)} \not\!\perp\!\!\!\perp S_\ell^{(k)}$.}
    \label{tab:dataStructure}
\end{table}



From the data $\{(S_i^{(k)}: i \in F^k)\}_{k = 1}^K$,
our goal is to compute a \textbf{multifidelity} estimate of $\Sigma_0$ which, in addition to involving samples from the high-fidelity class $[S_0] = \{S  \in \bbP_d: \bfE[S] = \Sigma_0\}$ (under the assumption that $0 \in F^k$ for at least one $k \in \{1, \dots, K\}$), incorporates correlated samples from one or more of $[S_1], \dots, [S_L]$. These correlations will serve to reduce the expected squared error of our estimator of $\Sigma_0$ while enabling us to exploit the relatively lower sampling costs associated with $[S_1], \dots, [S_L]$.











\subsection{Manifold regression estimator} \label{sec:sRV} 
In a similar vein to \cite{schaden2020multilevel}, we define our manifold regression multifidelity covariance estimator by interpreting the data in \cref{eq:Sdata} as a random variable. For $k \in \{1, \dots, K\}$ denote $\bfS^{(k)} = (S_i^{(k)}: i \in F^k)$ and $\bfSigma^{(k)} = (\Sigma_i: i \in F_k)$; it follows %
that $\bfE[\bfS^{(k)}] = \bfSigma^{(k)}$. We model our data \cref{eq:Sdata} by ``stacking'' $\bfS^{(1)}, \dots, \bfS^{(K)}$ into an $N$-vector of matrices, where $N = \sum_{k=1}^K \left| F^k \right| = \sum_{k=1}^K N_k$, writing 
\begin{equation}
\bfS = \begin{bmatrix} %
\bfS^{(1)} \\ \vdots \\ \bfS^{(K)}
\end{bmatrix} \sim \left(\mu_\bfS(\Sigma_0, \dots, \Sigma_L) =  \begin{bmatrix}
\bfSigma^{(1)} \\ \vdots \\ \bfSigma^{(K)}
\end{bmatrix} \equiv \bfSigma,\quad \Gamma_\bfS = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS ] \right)
\label{eq:S_rv}
\end{equation}
where $\mu_\bfS(\Sigma_0, \dots, \Sigma_L)$ is the mean and $\Gamma_\bfS$ is the Riemannian covariance of the $\bbP_d^{N}$-valued random variable $\bfS$. 

\subsubsection{Covariance of $\bfS$}
\label{sec:covS}
The covariance of $\bfS$ is 
\begin{align}
	\Gamma_\bfS = \E\left[ \begin{bmatrix}
		\log_{\bfSigma^{(1)}} \bfS^{(1)} \\ \vdots \\ \log_{\bfSigma^{(K)}} \bfS^{(K)}
	\end{bmatrix} \otimes_\bfSigma \begin{bmatrix}
		\log_{\bfSigma^{(1)}} \bfS^{(1)} \\ \vdots \\ \log_{\bfSigma^{(K)}} \bfS^{(K)}
	\end{bmatrix} \right] = \E\left[ \begin{bmatrix}
		\log_{\bfSigma^{(1)}} \bfS^{(1)} \\ \vdots \\ \log_{\bfSigma^{(K)}} \bfS^{(K)}
	\end{bmatrix} \otimes \begin{bmatrix}
		G_{\bfSigma^{(1)}}\log_{\bfSigma^{(1)}} \bfS^{(1)} \\ \vdots \\ G_{\bfSigma^{(K)}}\log_{\bfSigma^{(K)}} \bfS^{(K)}
	\end{bmatrix} \right],
\label{eq:gammaS}
\end{align}
where $G_{\bfSigma^{(k)}}$ is the linear transformation mapping on $\bbP_d^{N_k}$ mapping
\[
\bfA = \left(A_1, \dots, A_K \right) \mapsto \left(\Sigma_{k_1}\inv A_1\Sigma_{k_1}\inv, \dots, \Sigma_{k_{N_k}}\inv A_{N_k} \Sigma_{k_{N_k}}\inv \right) = G_{\bfSigma^{(k)}}\bfA.
\]
The transformations $G_{\bfSigma^{(1)}}, \dots, G_{\bfSigma^{(K)}}$ arise from the affine-invariant metric on $\bbP_d$.

$\Gamma_\bfS$ is a symmetric positive semidefinite linear operator on $\rmT_\bfSigma \bbP_d^{N} = \bbH_d^{N}$. Due to the coupling and independence structure in our data $\bfS$ \cref{eq:Sdata}, $\Gamma_\bfS$ has ``block diagonal'' structure which we represent in \cref{eq:gamma_blockDiag},
\begin{equation} 
\Gamma_\bfS = 
\begin{bmatrix}
\Gamma_\bfS^{(1)} \\
& \ddots \\
& & \Gamma_\bfS^{(K)}
\end{bmatrix}.
\label{eq:gamma_blockDiag}
\end{equation} 
where $\Gamma_{\bfS^{(k)}} = \E[\log_{\bfSigma^{(k)}} \bfS^{(k)} \otimes_{\bfSigma^{(k)}} \log_{\bfSigma^{(k)}} \bfS^{(k)}]$ is the Riemannian covariance of $\bfS^{(k)}$, $k\in \{1, \dots, K\}$. 
Each block of \cref{eq:gamma_blockDiag} is a symmetric positive semidefinite linear operator from $\bbH_d^{(N_k)}$ to $\bbH_d^{(N_k)}$, $k \in \{1, \dots, K\}$. 

Within this random variable model for $\bfS \sim (\bfSigma, \Gamma_{\bf S})$, we estimate the mean of $\bfS$ by \textit{minimizing Mahalanobis distance}.

\begin{definition}[Manifold Regression Multifidelity (MRMF) Covariance Estimator]
Given a realization of the random variable $\bfS \sim (\bfSigma, \Gamma_\bfS)$ \cref{eq:S_rv} we estimate the true covariance matrices $\Sigma_0, \dots, \Sigma_L$ which parameterize $\mu_\bfS(\Sigma_0, \dots, \Sigma_L) = \bfSigma$ by minimizing squared \textit{Mahalanobis distance} with respect to $\bfSigma$, %
\begin{equation}
	(\hat\Sigma_0,\dots, \hat\Sigma_L) = \argmin_{\Sigma_0, \dots,\Sigma_L \in \bbP_d} \langle \log_{\bfSigma} \bfS,\; \Gamma_\bfS\inv \log_{\bfSigma} \bfS \rangle_{\bfSigma} \quad \text{s.t. } \bfSigma = \mu_\bfS(\Sigma_0, \dots, \Sigma_L).
	\label{eq:mdist_min}
\end{equation}
\label{sec:mdistMin}
\end{definition}
\subsection{Running example} 
\label{sec:example_s3}
As a concrete illustration of the ideas in this section we consider an example with three data matrices.
Take the model of \cref{eq:S_rv} with $L = 1$ and $F = \{\{0, 1\}, \{1\}\} = \{F^1, F^2\}$  such that our data are 
\[
\bfS = (S_0^{(1)}, S_1^{(1)}, S_1^{(2)}) \equiv (\Shi, \SloOne, \SloTwo),
\]
where $\Shi$ and $\SloOne$ are correlated with each other but $\SloTwo \indep (\Shi, \SloOne)$. This structure may arise, for example, if $\Shi$ and $\SloOne$ are sample covariance matrices (SCMs) computed from statistically coupled realizations of random \textit{vectors} $\Xhi, \Xlo \in \R^d$ and $\SloTwo$ a sample covariance matrix computed from independent realizations of $\Xlo$. Specifically, suppose that we have at our disposal
\begin{equation} 
\begin{aligned}
\left\{(X_{\text{hi}}^i, X_{\text{lo}}^i)\right\}_{i=1}^{M_1}, & \quad \text{\textbf{statistically coupled} sample pairs of $X_{\rm hi}$, $X_{\rm lo} \in \R^d$} \\
\left\{X_{\text{lo}}^i \right\}_{i=M_1+1}^{M_1 + M_2}, & \quad \text{\textbf{independent} samples of $X_{\rm lo} \in \R^d$}.
\end{aligned}
\label{eq:exampleData}
\end{equation} 
The samples $X_{\text{hi}}^i$ and $X_{\text{lo}}^i$ are {correlated} for the same $i$, but the \textit{pairs} $\{(X_{\text{hi}}^i, X_{\text{lo}}^i)\}_{i=1}^{M_1}$ are independent and identically distributed (i.i.d.) for different $i$. Likewise, the additional low-fidelity samples $\{X_{\text{lo}}^i \}_{i=M_1+1}^{M_1 + M_2}$ are i.i.d.\ and independent of the pairs $\left\{(X_{\text{hi}}^i, X_{\text{lo}}^i)\right\}_{i=1}^{M_1}$. In this setting we take %
\begin{equation*}
		\Shi \equiv \widehat\Cov[\{X_{\rm hi}^i\}_{i=1}^{M_1} ], \quad \SloOne \equiv \widehat\Cov[\{X_{\rm lo}^i\}_{i=1}^{M_1} ], \quad  \SloTwo \equiv \widehat\Cov[\{X_{\rm lo}^i\}_{i=M_1 + 1}^{M} ],
\end{equation*}
where we have defined $M = M_1 + M_2$. Due to the coupling and independence structure in the data \cref{eq:exampleData}, $\Shi$ and $\SloOne$ are correlated with each other while $\SloTwo$ is independent of $(\Shi, \SloOne)$. Furthermore, $\bfE[\SloOne] = \bfE[\SloTwo] = \Sigmalo$ but $\SloOne \overset{d}{\neq} \SloTwo$ because $\SloOne$ and $\SloTwo$ are constructed from different numbers of samples of $\Xlo$.\footnote{As noted in \cite{smith2005covariance}, sample covariance matrices are only \textit{asymptotically} unbiased in the intrinsic metric, i.e., even though $\E[\SloOne] = \E[\SloTwo]$ in the Euclidean sense, in general $\bfE[\SloOne] \neq \bfE[\SloTwo]$. However, in the absence of intrinsically unbiased sample covariance estimators we make the modeling assumption that $\bfE[\SloOne] = \bfE[\SloTwo] = \Sigmalo$ and have obtained good results in practice from doing so; see \Cref{sec:numerics}.}


The variable $\bfS$ takes values in $\bbP_d^3$ with mean $\bfSigma$ and covariance $\Gamma_\bfS$, 
\begin{equation}
\bfS = \begin{bmatrix}
	\Shi \\ \SloOne \\ \SloTwo \end{bmatrix} \sim \left(\bfSigma= \begin{bmatrix}
	\Sigmahi \\ \Sigmalo \\ \Sigmalo
\end{bmatrix}, \Gamma_\bfS  = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS] \right),
\label{eq:threemat_model}
\end{equation}
where
\begin{align*}
	\Gamma_\bfS &= \E\left[ \begin{bmatrix}
		\log_\Sigmahi(\Shi) \\ \log_\Sigmalo(\SloOne) \\ \log_\Sigmalo(\SloTwo) \end{bmatrix} \otimes_\bfSigma \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Sigmalo(\SloOne) \\ \log_\Sigmalo(\SloTwo) \end{bmatrix} \right] 
\end{align*}
is the Riemannian covariance of $\bfS$, a symmetric positive semidefinite linear operator on $\rmT_\bfSigma \bbP_d^3 = \bbH_d^3$. Because $\SloTwo$ is independent of $\Shi$ and $\SloOne$, $\Gamma_\bfS$ has block structure 
\begin{equation} 
\Gamma_{\bfS} = 
\begin{bmatrix}
\Gamma_{\rm hi} & \Gamma_{\rm lo, hi} & \mathbf{0} \\
\Gamma_{\rm hi, lo} & \Gamma_{\rm lo, 1} & \mathbf{0} \\ 
\mathbf{0} & \mathbf{0} & \Gamma_{\rm lo, 2}
\end{bmatrix}.
\label{eq:gamma_blockdiag_ex}
\end{equation}
The nonzero blocks of $\Gamma_\bfS$ are the auto-covariance of $\Shi$,
\begin{equation*}
 \Gamma_{\rm hi} = \E[\log_\Sigmahi(\Shi) \otimes_\Sigmahi \log_\Sigmahi (\Shi)], 
 \end{equation*}
 the auto-covariances of $\SloOne$ and $\SloTwo$,
\begin{equation*}
 \Gamma_{\rm lo, 1} = \E[\log_\Sigmalo(\SloOne) \otimes_\Sigmalo \log_\Sigmalo(\SloOne)] \quad \text{and} \quad \Gamma_{\rm lo, 2} = \E[\log_\Sigmalo(\SloTwo) \otimes_\Sigmalo \log_\Sigmalo(\SloTwo)],
 \end{equation*}
 and the cross-covariances between $\Shi$ and $\SloOne$,
\begin{equation*}
\Gamma_{\rm lo, hi} = \E[\log_\Sigmahi \Shi \otimes_\Sigmalo \log_\Sigmalo \SloOne] \quad \text{and} \quad \Gamma_{\rm hi, lo} = \E[\log_\Sigmalo \SloOne \otimes_\Sigmahi \log_\Sigmahi \Shi]. 
\end{equation*}
The squared Mahalanobis distance minimization we solve to estimate $\Sigmahi$ and $\Sigmalo$ is %
 \begin{equation} 
	\begin{aligned}
		\left(\Sigmahihat, \Sigmalohat \right) 		& = \argmin_{\Sigmahi, \Sigmalo \in \bbP_d} \langle\log_\bfSigma \bfS,\; \Gamma_\bfS\inv \log_\bfSigma \bfS \rangle_\bfSigma &\text{s.t. } \bfSigma = (\Sigmahi, \Sigmalo, \Sigmalo)&  \\
        &= \argmin_{\Sigmahi, \Sigmalo \in \bbP_d} \left\langle \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Sigmalo(\SloOne) \\ \log_\Sigmalo(\SloTwo) \end{bmatrix}, \; \Gamma_\bfS\inv \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Sigmalo(\SloOne) \\ \log_\Sigmalo(\SloTwo) \end{bmatrix}  \right\rangle_{\bfSigma} &\text{s.t. } \bfSigma = (\Sigmahi, \Sigmalo, \Sigmalo)&  \\
	\end{aligned}
 \label{eq:mdist_min_ex}
 \end{equation} 

\section{Analysis, simplification, and interpretation of the manifold regression estimator}
\label{sec:analysis}
In this section we analyze the regression estimator \cref{eq:mdist_min}, demonstrating useful properties, simplifications, and interpretations. In \Cref{ss:mdistProperties} we show that the Mahalanobis distance we minimize is affine-invariant and agnostic to the tangent space on which it is defined; moreover, it can be endowed with a {maximum likelihood} interpretation. In \Cref{sec:example_s4} we demonstrate how these properties are useful in practice. In \Cref{sec:fixSigmalo} we present a simplification of our estimator wherein $\Sigmalo$ is fixed and find that, in addition to being computationally advantageous, this choice leads to greater analytical tractability. In particular, we show in \Cref{sec:mfGeneralGeom} that the fixed-$\Sigmalo$ simplification yields a surprising link to control variates, uniting our work here with many existing multifidelity estimators. Proofs of results in this section can be found in \cref{app:proofs}.
\subsection{Properties of Mahalanobis distance} 
\label{ss:mdistProperties}
In what follows here we demonstrate two mathematical properties of the Mahalanobis distance and show that the estimator \cref{eq:mdist_min} is a maximum likelihood estimator under a Gaussian noise model for $\log_\bfSigma \bfS$. The first property, tangent-space agnosticism (\cref{thm:ts_agnostic}), simplifies computation of the estimator by eliminating dependence on the $\bfSigma$-specific weightings defining $\langle \cdot, \cdot \rangle_\bfSigma$ and $\otimes_\bfSigma$, and the second, affine-invariance (\cref{thm:affineinvariance}), enables use of stabilizing preconditioners. The maximum likelihood interpretation (\cref{prop:mle}) grounds our estimator theoretically and opens the door to parametric modeling of $\log_\bfSigma \bfS = \bm{\calE} \in \bbH_d^N$. 

\subsubsection{Tangent space agnosticism} 
\label{sec:tsAgnostic}
As formulated in \cref{eq:mdist_min} the squared Mahalanobis distance between $\bfSigma$ and $\bfS$ depends {highly} non-trivially on $\bfSigma$: not only do we have to contend with the ``vector difference'' $\log_\bfSigma \bfS$, but the very {operators} $\otimes_\bfSigma$ and $\langle \cdot, \cdot \rangle_\bfSigma$ of $\rmT_\bfSigma \bbP_d^{N}$ defining the covariance and Mahalanobis distance depend on $\bfSigma$. %

While we could perhaps compute with \cref{eq:mdist_min} directly and find a way to estimate $\Gamma_\bfS$ as defined with $\otimes_\bfSigma$, it would be convenient to remove the dependence of $\Gamma_\bfS$ and the Mahalanobis distance on the $\bfSigma$-dependent weightings of $\rmT_\bfSigma \bbP_d^{N}$. Intuitively, we would like our estimator to behave ``the same'' independent of the particular tangent space in which it is realized. Since all tangent spaces to $\bbP_d^{N}$ are in some sense equal to $\bbH_d^{N}$, one would hope that the choice of inner- and outer-product operators in \cref{eq:mdist_min} does not affect the estimates of $\Sigma_0, \dots, \Sigma_L$. %

In this instance we indeed get our wish: Mahalanobis distance is \textit{tangent space agnostic}, meaning that we can compute the regression estimator \cref{eq:mdist_min} on any tangent space to $\bbP_d^{N}$ we want and obtain the same results.  

\begin{proposition}
Let $\bfS$ and $\bfSigma = \mu_\bfS(\Sigma_0,\dots, \Sigma_L)$ be as in \cref{eq:S_rv}. The squared Mahalanobis distance objective of \cref{eq:mdist_min} is independent of the tangent space in which it is evaluated, i.e., 
\begin{equation}
\mathfrak{D}^2_\bfS(\bfSigma) 	
  \coloneqq d^2_\bfS(\bfS;\, \bfSigma) = \langle \log_\bfSigma \bfS, \; \Gamma_\bfS\inv \log_\bfSigma \bfS \rangle_\bfSigma = \langle \log_\bfSigma \bfS, \; \Gamma_{\bfS, \bfI} \inv \log_\bfSigma \bfS \rangle,
\end{equation}
where $\Gamma_{\bfS, \bfI} = \E[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS]$ is the covariance of $\bfS$ computed using the standard (unweighted) outer product, and $\langle \cdot, \cdot \rangle$ denotes the unweighted Frobenius inner-product on $\bbH_d^{N}$. 
	\label{thm:ts_agnostic}
\end{proposition}
This result follows from the fact that the linear transformation used to define $\langle \cdot, \cdot \rangle_\bfSigma$ in the Mahalanobis distance is canceled by its own inverse when $\Gamma_\bfS\inv$ is applied as a weighting.

\subsubsection{Affine invariance} 
\label{ss:affine_invariance}
A salient property of the intrinsic metric on $\bbP_d$ is that it is \textit{affine-invariant} %
in the sense that if for $A, B \in \bbP_d$ we define $\tilde A = Y\inv AY\inv$ and $\tilde B = Y\inv B Y\inv$ with $Y \in \bbP_d$, then it holds that $d(\tilde A, \tilde B) = d(A, B)$ \cite{bhatiaposdef}.
Affine-invariance of the intrinsic metric on $\bbP_d$ immediately gives affine-invariance on $\bbP_d^N$: if $\bfA = (A_1, \dots, A_N)$, $\bfB = (B_0, \dots, B_N) \in \bbP_d^N$, and we define $\tilde\bfA = (Y_1\inv A_1 Y_1\inv, \dots, Y_N\inv A_N Y_N\inv)$ and $\tilde\bfB = (Y_1\inv B_0 Y_1\inv, \dots, Y_N\inv B_N Y_N\inv)$ for some $\bfY \in \bbP_d^N$, then one can easily show that %
\begin{equation*}
d^2(\tilde \bfA, \tilde \bfB) = d^2(\bfA, \bfB).
\end{equation*}

In this section we show that the affine-invariance property of the intrinsic metric on $\bbP_d^N$ extends to the Mahalanobis distance \cref{eq:mdist_min} defining our multifidelity covariance estimator. %

\begin{proposition}
	Consider the random variable $\bfS$ in \cref{eq:S_rv} and the Mahalanobis distance 
	\begin{equation}
	\frakD^2_\bfS(\bfSigma) = \langle \log_\bfSigma \bfS, \; \Gamma_\bfS\inv \log_\bfSigma \bfS \rangle_\bfSigma. 
	\label{eq:mdist_thm}
	\end{equation}
	Let $\tilde \bfS = G_\bfY \bfS$, where $\bfY \in \bbP_d^N$ and $G_\bfY: \bbH_d^N \to \bbH_d^N$ is the linear operator mapping 
 \[
\bfC = (C_1, \dots, C_N) \mapsto  (Y_1 \inv C_1 Y_1 \inv, \dots, Y_N \inv C_{N} Y_{N} \inv) = G_\bfY\bfC.
 \]
 $\tilde\bfS$ is a linear transformation of $\bfS$ with corresponding mean $\tilde \bfSigma = G_\bfY \bfSigma$ and covariance $\Gamma_{\tilde \bfS} = \E[\log_{\tilde \bfSigma} \tilde \bfS \otimes_{\tilde \bfSigma} \log_{\tilde \bfSigma} \tilde \bfS] $. It holds that 
	\begin{equation*}
	\frakD^2_\bfS(\bfSigma) = \langle \log_\bfSigma \bfS, \; \Gamma_\bfS\inv \log_\bfSigma \bfS \rangle_\bfSigma = \langle \log_{\tilde \bfSigma} \tilde \bfS, \; \Gamma_{\tilde \bfS}\inv \log_{\tilde \bfSigma} \tilde \bfS \rangle_{\tilde \bfSigma} = \frakD^2_{\tilde \bfS}(\tilde \bfSigma).
	\end{equation*}
	Furthermore, $\Gamma_{\tilde \bfS, \bfI} = G_\bfY \Gamma_{\bfS, \bfI} G_\bfY$. 
\label{thm:affineinvariance}
\end{proposition}

\cref{thm:affineinvariance} is useful in practice, as it allows us to apply stabilizing affine preconditioners in our computations. We can particularly transform the data $\bfS$ to a vector of identity matrices, significantly simplifying the form of $\log_\bfSigma \bfS$. We demonstrate this technique in \Cref{sec:example_s4}. 

\subsubsection{Maximum likelihood interpretation}
\label{sec:modelOnTS}
The Mahalanobis distance minimization in \cref{eq:mdist_min} can be viewed as nonlinear regression for $\Sigma_0, \dots, \Sigma_L$, corresponding to %
an additive noise model for $\log_\bfSigma \bfS$ on tangent space $\rmT_\bfSigma \bbP_d^{N}$: %
We have defined the random variable $\bfS$ via 
\begin{equation*}
\bfS \sim \left( \bfE[\bfS] = \bfSigma,\; \Cov[\bfS] = \Gamma_\bfS  \right),
\end{equation*}
with $\bfE[\bfS] = \bfSigma$ taking values on the manifold $\bbP_d^{N}$ and the covariance $\Gamma_\bfS$ defined on the \textit{tangent space} to the manifold $\rmT_\bfSigma \bbP_d^{N} \subseteq \bbH_d^{N}$. $\bbH_d^{N}$ is a Euclidean vector space, so we can interpret $\log_\bfSigma \bfS$ as a new random variable on that space and view
\begin{equation*}
\Gamma_{\bfS, \bfI} = \E[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS] = \E[(\log_\bfSigma \bfS  - \mathbf{0}) \otimes (\log_\bfSigma \bfS - \mathbf{0})]
 \end{equation*} 
as the Euclidean covariance of $\log_\bfSigma \bfS$. An additive noise model for the variation of $\log_\bfSigma \bfS$ on $\bbH_d^{N}$ corresponding to $\Gamma_\bfS$ would then be 
\begin{equation}
	\log_\bfSigma\bfS = \log_\bfSigma\bfSigma + \bm{\calE} = \bm{\calE}, 
	\label{eq:tangentspacemodel}
\end{equation}
where $\bm{\calE}\equiv \log_\bfSigma \bfS$ is a $\bbH_d^{N}$-valued, mean-zero random variable with covariance $\Gamma_{\bm{\calE}} = \Gamma_{\bfS, \bfI}$. 

The additive noise model \cref{eq:tangentspacemodel} on tangent space suggests an exponential model on the manifold, 
\begin{equation}
\begin{gathered}
	\log_\bfSigma \bfS = \bm{\calE} \\
	\Updownarrow\\
	\bfS = \exp_\bfSigma \bm{\calE},
\end{gathered}
\label{eq:dualmodels}
\end{equation}
wherein we see that the mean-zero, symmetric-matrix-valued perturbations in $\bm{\calE}$ are transformed by $\exp_\bfSigma(\cdot)$ to define an inherently positive definite $\bbP_d^{N}$-valued random variable. 

The relationship \cref{eq:dualmodels} is an example of an ``exponential-wrapped distribution'' \cite{chevallier2022exponential} for symmetric positive definite matrices. In the particular case where the elements of $\bm{\calE}$ are symmetric-matrix-Gaussian, one obtains ``canonical log-normal'' distributions for each element of $\bfS$ \cite{schwartzmanLognormalDistributionsGeometric2016}. %
In fact, solving \cref{eq:mdist_min} is equivalent to performing maximum likelihood estimation in the case that the elements of $\bm{\calE}$ have a centered Gaussian distribution on $\bbH_d^N$. 

\begin{proposition}
    Suppose that $\log_\bfSigma \bfS = \bm{\calE} \in \bbH_d^N$ has a Gaussian distribution on $\bbH_d^N$, 
    \begin{equation}
    \bm{\calE} \sim \calN_{\bbH_d^N}(\mathbf{0}, \Gamma_{\bm{\calE}}).
    \label{eq:gaussianE}
    \end{equation}
   Then the solution to \cref{eq:mdist_min} is a maximum likelihood estimate.
\label{prop:mle}
\end{proposition}
While the Gaussian model \cref{eq:gaussianE} for $\log_\bfSigma \bfS$ does lead to a satisfying statistical interpretation, this distributional assumption is \textit{not} a requirement. In the same way that ordinary least squares estimation (based only on first and second moments) is justified even when scalar data do not satisfy a Gaussian noise model, our Mahalanobis distance minimization estimator \cref{eq:mdist_min} 
is applicable
to data $\bfS$ possessing a variety of error distributions, as we demonstrate in our numerical examples (\Cref{sec:numerics}). 

\subsection{Running example} 
\label{sec:example_s4}
Continuing with the setup of \Cref{sec:example_s3} with $\bfS = (\Shi, \SloOne, \SloTwo)$ and $\bfE[\bfS] = \bfSigma = (\Sigmahi, \Sigmalo, \Sigmalo)$ we demonstrate here how the properties discussed so far %
apply to that particular model. For clarity we use $\bfsfS$ to denote the specific realization of the random variable $\bfS$ which appears in our estimator.
Owing to the tangent-space agnosticism of \cref{thm:ts_agnostic}, we can minimize the Mahalanobis distance \cref{eq:mdist_min_ex} by equivalently solving 
\begin{equation}
(\Sigmahihat, \Sigmalohat) = \argmin_{\Sigmahi, \Sigmalo \in \bbP_d} \left\langle\log_\bfSigma \bfsfS,\, \Gamma_{\bfS, \bfI}\inv \log_\bfSigma \bfsfS \right\rangle \quad \text{s.t. } \bfSigma = (\Sigmahi, \Sigmalo, \Sigmalo),
\label{eq:mdist_min_ex_tsa}
\end{equation}
which is formulated with the standard Euclidean inner- and outer-products, where $\Gamma_{\bfS, \bfI} = \E[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS]$.
Thanks to \cref{thm:affineinvariance} we can further simplify numerics by applying a preconditioning affine transformation: let $\bfY = (\sfS_{\rm hi}^{\frac{1}{2}},\, (\sfSloOne)^{\frac{1}{2}},\, (\sfSloTwo)^{\frac{1}{2}})$. We take $G_\bfY: \bbH_d^3 \to \bbH_d^3$ to be the mapping
\[
\bfA = (A_1, A_2, A_3) \mapsto \left(\sfS_{\rm hi}^{-\frac{1}{2}}A_1 \sfS_{\rm hi}^{-\frac{1}{2}},\; (\sfSloOne)^{-\frac{1}{2}}A_2(\sfSloOne)^{-\frac{1}{2}},\; (\sfSloTwo)^{-\frac{1}{2}}A_3(\sfSloTwo)^{-\frac{1}{2}} \right) = G_\bfY \bfA,
\]
with which we transform $\bfsfS$, $\bfSigma$ and $\Gamma_{\bfS, \bfI}$, obtaining
\begin{align*}
	\bfsfS &\mapsto \tilde\bfsfS = (\sfI,\, \sfI,\, \sfI)  = \bfsfI \\
	\bfSigma &\mapsto \tilde\bfSigma = \left(\sfS_{\rm hi}^{-\frac{1}{2}}\Sigmahi \sfS_{\rm hi}^{-\frac{1}{2}},\; (\sfSloOne)^{-\frac{1}{2}}\Sigmalo (\sfSloOne)^{-\frac{1}{2}},\; (\sfSloTwo)^{-\frac{1}{2}}\Sigmalo (\sfSloTwo)^{-\frac{1}{2}} \right) \\
	\Gamma_{\bfS, \bfI} &\mapsto \Gamma_{\tilde\bfS, \bfI} = G_\bfY \Gamma_{\bfS, \bfI} G_\bfY.
\end{align*}
Defining $B = (\sfSloTwo)^{-\frac{1}{2}}(\sfSloOne)^{\frac{1}{2}}$, instead of \cref{eq:mdist_min_ex_tsa} we can alternately solve %
\begin{equation}
	\left(\widehat{ \tilde{\Sigma}}_{\rm hi}, \widehat{ \tilde{\Sigma}}_{\rm lo} \right) = \argmin_{\tilde \Sigma_{\rm hi}, \tilde \Sigma_{\rm lo} \in \bbP_d} \langle\log_{\tilde\Sigma}\bfsfI,\; G_\bfY \inv\Gamma_{\bfS, \bfI}\inv G_\bfY \inv \log_{\tilde \Sigma } \bfsfI \rangle \quad \text{s.t. } \tilde\Sigma = (\tilde\Sigma_{\rm hi},\, \tilde\Sigma_{\rm lo},\, B\tilde\Sigma_{\rm lo} B\t )
	\label{eq:mdist_iddata}
\end{equation}
and transform the resulting minimizers to obtain 
\[
\Sigmahihat = \sfS_{\rm hi}^{\frac{1}{2}}\widehat{ \tilde{\Sigma}}_{\rm hi}\sfS_{\rm hi}^{\frac{1}{2}} \quad \text{and} \quad\Sigmalohat = (\sfSloOne)^{\frac{1}{2}}\widehat{\tilde\Sigma}_{\rm lo}(\sfSloOne)^{\frac{1}{2}}.
\]

The fact that $\tilde\bfsfS = \bfsfI$ simplifies the form of $\log_{\tilde\bfSigma} \bfsfI$ relative to that of $\log_\bfSigma \bfsfS$. Consider, for example, the first components of $\log_{\tilde\bfSigma} \bfsfI$ and $\log_\bfSigma \bfsfS$, involving $\Sigmahi$ and $\tilde\Sigma_{\rm hi}$. We have  
\begin{equation*}
\log_\Sigmahi \sfShi %
= \Sigmahi \log\left(\Sigma_{\rm hi}\inv \sfShi \right), \quad \text{while} \quad \log_{\tilde{\Sigma}_{\rm hi}} \mathrm{\sfI} = -\tilde{\Sigma}_{\rm hi} \log \left(\tilde{\Sigma}_{\rm hi} \right).
\end{equation*}

Within the framework of \Cref{sec:modelOnTS} the linear model \cref{eq:tangentspacemodel} for $\log_\bfSigma \bfS$ on $\rmT_\bfSigma \bbP_d^3$ suggests the following exponential model for the random variable $\bfS$ on $\bbP_d^3$, 
\begin{equation}
    \bfS = \exp_\bfSigma(\bm{\calE}) = \begin{bmatrix}
        \Sigma_{\rm hi}^{\frac{1}{2}}\exp(\Sigma_{\rm hi}^{-\frac{1}{2}}\calE_{\rm hi}^1 \Sigma_{\rm hi}^{-\frac{1}{2}})\Sigma_{\rm hi}^{\frac{1}{2}} \\[0.2cm]
\Sigma_{\rm lo}^{\frac{1}{2}}\exp(\Sigma_{\rm lo}^{-\frac{1}{2}}\calE_{\rm lo}^1 \Sigma_{\rm lo}^{-\frac{1}{2}})\Sigma_{\rm lo}^{\frac{1}{2}} \\[0.2cm] 
\Sigma_{\rm lo}^{\frac{1}{2}}\exp(\Sigma_{\rm lo}^{-\frac{1}{2}}\calE_{\rm lo}^2 \Sigma_{\rm lo}^{-\frac{1}{2}})\Sigma_{\rm lo}^{\frac{1}{2}}
    \end{bmatrix},
    \label{eq:manifoldmodel}
\end{equation}
where $\calE_{\rm hi}^1$, $\calE_{\rm lo}^1$, $\calE_{\rm lo}^2$ are mean-zero, symmetric-matrix valued perturbations. We see that when $\bm{\calE} = \begin{bmatrix}\mathbf{0}^{d\times d} & \mathbf{0}^{d\times d} & \mathbf{0}^{d\times d}\end{bmatrix}\t$ we indeed have $\bfS = \bfSigma$.  


\subsection{Fixed-$\Sigmalo$ simplification} %
\label{sec:fixSigmalo}
In this section we consider the regression problem specifically with the setup of \Cref{sec:example_s3,sec:example_s4},
\begin{equation*}
	\bfS = \begin{bmatrix}
		\Shi \\ \SloOne \\ \SloTwo \end{bmatrix} \sim \left(\bfSigma= \begin{bmatrix}
		\Sigmahi \\ \Sigmalo \\ \Sigmalo
	\end{bmatrix}, \Gamma_\bfS  = \E[\log_\bfSigma \bfS \otimes_\bfSigma \log_\bfSigma \bfS] \right),
\end{equation*}
with $\Shi$ and $\SloOne$ correlated and $\SloTwo \indep (\Shi, \SloOne)$. We motivate our development by the setting in which $\Shi$ and $\SloOne$ are sample covariance matrices constructed from $M_1$ coupled pairs of $(\Xhi, \Xlo)$ and $\SloTwo$ a sample covariance matrix constructed from an additional $M_2$ i.i.d.\ samples of $\Xlo$, as discussed in \Cref{sec:example_s3}. In instances when the total number of low-fidelity samples $M = M_1 + M_2$ is high relative to $d$, which may occur if sampling $\Xlo$ is cheap, the sample covariance matrix $\Slobar = \widehat{\Cov}[\{X_{\rm lo}^{(i)}\}_{i=1}^{M}]$ may be a good estimate of $\Sigmalo$ {on its own}, absent any multifidelity correction. Indeed, we have seen in practice that the estimate $\Sigmalohat$ resulting from solving \cref{eq:mdist_min} often does not differ greatly from $\Slobar$ when $M \gg d$. 

A reasonable and cost-effective approach to solving \cref{eq:mdist_min} in this setting is to fix $\Sigmalo = \Slobar$ in the squared Mahalanobis distance and obtain a simplified multifidelity estimator for $\Sigmahi$,
\begin{equation} 
	\Sigmahihat = \argmin_{\Sigmahi \in \bbP_d} \langle \log_{\bfSigma} \bfS,\; \Gamma_{\bfS,
 \bfI}\inv \log_{\bfSigma} \bfS \rangle  \quad \text{s.t. } \bfSigma = (\Sigmahi, \Slobar, \Slobar). 
 \label{eq:fixlo_three}
\end{equation} 

When $\Sigmalo$ is fixed in this manner, the objective function simplifies and we effectively solve 
\begin{equation} 
\begin{aligned}
\Sigmahihat = \argmin_{\Sigmahi \in \bbP_d} \left\langle \log_{\bfSigma_{1:2}}\bfS_{1:2}, \; \Gamma_{\bfS_{1:2}, \bfI}\inv \log_{\bfSigma_{1:2}}\bfS_{1:2} \right\rangle \quad \text{s.t. } \bfSigma_{1:2} = (\Sigmahi, \bar S_{\rm lo}), 
\end{aligned}
\label{eq:twomat_mdist}
\end{equation} 
where $\bfS_{1:2} = (\Shi, \SloOne)$ and $\Gamma_{\bfS_{1:2}, \bfI} = \E[\log_{\bfSigma_{1:2}}(\bfS_{1:2}) \otimes \log_{\bfSigma_{1:2}}(\bfS_{1:2})]$ is the upper ``block'' of $\Gamma_{\bfS, \bfI}$ corresponding to the variables $\Shi$ and $\SloOne$. Thus we do not need to include $\SloTwo$ in our optimization for $\Sigmahi$ when $\Sigmalo$ is fixed \textit{a priori}; rather, in the case that $\SloOne$ and $\SloTwo$ are SCMs, we only combine the samples of $\Xlo$ that would correspond to $\SloTwo$ with those involved in $\SloOne$ to construct $\bar S_{\rm lo} \approx \Sigmalo$. In the remainder of this section we thus use $\bfS$ to refer to the $\bbP_d^2$-valued random variable $\bfS = (\Shi, \SloOne) \equiv (\Shi, \Slo)$ with mean and covariance 
\begin{equation}
\bfS \sim \left( \bfE[\bfS] = (\Sigmahi, \Sigmalo) = \bfSigma, \; \Gamma_\bfS = \E\left[\log_\bfSigma \bfS \otimes \log_\bfSigma \bfS\right] \right)
\label{eq:twoMatrixRV}
\end{equation}  
and understand $\bar S_{\rm lo}$ to refer to a very good \textit{a priori} estimate of $\Sigmalo$. In writing \cref{eq:twoMatrixRV} we have taken $\Gamma_{\bfS} \equiv \Gamma_{\bfS, \bfI}$ and in the following will drop the dependence of Mahalanobis distance on $\langle \cdot, \cdot \rangle_\bfSigma$, as allowed by \cref{thm:ts_agnostic}. 

Beyond being computationally convenient, the simplification \cref{eq:twomat_mdist} is more analytically tractable than the full regression problem \cref{eq:mdist_min_ex}. For instance,  \cref{eq:twomat_mdist} has a closed-form expected minimum Mahalanobis distance in the case that $\Sigmalo$ is known exactly: 
\begin{proposition}
Suppose that $\Sigmalo$ is fixed at its \textit{true value} in \cref{eq:twomat_mdist}. 
The expected value of the corresponding minimum Mahalanobis distance is 
\begin{equation} 
\E_{(\Shi, \Slo)} \left[\argmin_{\Sigmahi \in \bbP_d} \left\langle \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Sigmalo(\Slo) \end{bmatrix}, \; \Gamma_{S} \inv \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_\Sigmalo(\Slo) \end{bmatrix}  \right\rangle \right] = \sdfrac{d(d+1)}{2}.
 \label{eq:fixlo_mdist}
\end{equation} 
\label{prop:expectedMdist}
\end{proposition}
$\frac{d(d+1)}{2}$ is the number of degrees of freedom in a $d \times d$ symmetric matrix and arises as the expected minimum of the fixed-$\Sigmalo$ Mahalanobis distance \cref{eq:fixlo_mdist} because the optimal value of $\Sigmahi$ causes all individual inner products in the Mahalanobis distance \cref{eq:twomat_mdist} to cancel except for the term $\langle \log_\Sigmalo \Slo, \; \Gamma_{\rm lo}\inv \log_\Sigmalo \Slo \rangle$, equal to the Mahalanobis distance between $\Slo$ and its marginal distribution. As noted in \cite{pennec2006intrinsic}, the expected value of the Mahalanobis distance between any random object and its distribution is the number of degrees of freedom in that object, which gives us $\frac{d(d+1)}{2}$ for $\Slo$. 

Knowing the expected minimum of \cref{eq:twomat_mdist} can be useful when implementing regularization schemes, as we will discuss in \Cref{sec:regularization}. %
Furthermore, the \textit{minimizer} of \cref{eq:twomat_mdist} satisfies a nonlinear equation which can be interpreted as a control-variate estimator of $\Sigmahi$ in the affine-invariant geometry for $\bbP_d$. We make this result explicit and discuss consequent connections in \Cref{sec:mfGeneralGeom}.

\subsection{Multifidelity estimation in general geometries} 
\label{sec:mfGeneralGeom}
In this section we unify the multifidelity covariance estimators of \cite{maurais2023logEuclidean}, which employ the Euclidean and log-Euclidean geometries for $\bbP_d$, with our regression estimator \cref{eq:mdist_min}, formulated using the affine-invariant geometry, and discuss broader implications for multifidelity estimation of covariance matrices. Our discussion centers on a striking result arising from the fixed-$\Sigmalo$ simplification \cref{eq:twomat_mdist} of the regression estimator: 
the solution to the Mahalanobis distance minimization problem in this setting satisfies a \textit{nonlinear control variate equation}.

\begin{proposition}
Consider the regression problem 
\begin{equation} 
\begin{aligned}
	\hat{\Sigma}_{\rm hi} &= \argmin_{\Sigmahi \in \bbP_d} \langle \log_{\bfSigma} S,\; \Gamma_{\bfS }\inv \log_{\bfSigma} S \rangle  \quad \text{s.t. } \Sigma = (\Sigmahi, \bar{S}_{\rm lo}) \\
	&= \argmin_{\Sigmahi \in \bbP_d} \left\langle \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_{\bar{S}_{\rm lo}}(\Slo) \end{bmatrix}, \; \Gamma_{\bfS} \inv \begin{bmatrix} \log_\Sigmahi(\Shi) \\ \log_{\bar{S}_{\rm lo}}(\Slo) \end{bmatrix}  \right\rangle.  
\end{aligned}
\label{eq:twomat_reg}
\end{equation}     
$\hat{\Sigma}_{\rm hi}$ satisfies 
\begin{equation}
\log_{\hat\Sigma_{\rm hi}}(\Shi) = \Gamma_{\rm lo, hi} \Gamma_{\rm lo}\inv \log_{\bar{S}_{\rm lo}}(\Slo),
\label{eq:regest_nleq}
\end{equation} 
where $\Gamma_{\rm lo} = \E[\log_\Sigmalo (\Slo) \otimes \log_\Sigmalo (\Slo)]$ and $\Gamma_{\rm lo, hi} = \E[\log_\Sigmahi (\Shi) \otimes \log_\Sigmalo(\Slo)]$.
\label{prop:nleq}
\end{proposition}

The linear operator $\Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv$ is identifiable as the {optimal gain} between $\log_\Sigmahi(\Shi)$ and $\log_\Sigmalo(\Slo)$ and appears, e.g., in the context of vector-valued control variates \cite{rubinstein1985efficiency} and Kalman-type filtering schemes \cite{kalmanfilter, evensenbook}. %
\Cref{prop:nleq} reveals a satisfying connection: control-variate type multifidelity estimators can be viewed as a {special case} of the Riemannian multifidelity regression framework we develop here. %

 
\subsubsection{Interpretation of fixed-$\Sigmalo$ estimator as control variates}
If we fix $\Sigmalo$ at $\Slobar$ and minimize squared Mahalanobis distance over $\Sigmahi$ alone \cref{eq:fixlo_three}, we obtain an estimate $\Sigmahihat \equiv \Sigmahihat^{\rm MRMF}$ satisfying 
\begin{equation}
\log_{\Sigmahihat^{\rm MRMF}}(\Shi) = \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \log_{\Slobar}(\Slo), 
\label{eq:reg_as_cv}
\end{equation}
where $\Gamma_{\rm lo, hi}  = \E[\log_\Sigmahi(\Shi) \otimes \log_\Sigmalo(\Slo)]$ is the Riemannian cross-covariance between $\Shi$ and $\Slo$ and $\Gamma_{\rm lo} = \E[\log_\Sigmalo(\Slo) \otimes \log_\Sigmalo(\Slo)]$ is the Riemannian auto-covariance of $\Slo$. %
As discussed in \Cref{sec:intrinsicStatistics},  the Riemannian logarithm $\log_A B = A^{\frac{1}{2}}\log(A^{-\frac{1}{2}} B A^{\frac{1}{2}}) A^{\frac{1}{2}}$ can be interpreted as a ``difference'' between $A, B \in \bbP_d$. %
In \eqref{eq:reg_as_cv} we see that the ``difference'' between our Mahalanobis distance-minimizing estimate of $\bfE[\Shi] = \Sigmahi$ and our sample of $\Shi$ is equal to the ``difference'' between $\Slobar \approx \Sigmalo = \bfE[\Slo]$ and our sample of $\Slo$, multiplied by the {optimal gain} $\Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv$. This relationship has the form of an optimal control variate equation analogous to those employed in \cite{maurais2023logEuclidean}:

\paragraph{Euclidean control variate estimator}
The Euclidean multifidelity (EMF) covariance estimator of \cite{maurais2023logEuclidean} is given in the form
\begin{equation}
\Sigmahihat^{\rm EMF} = \Shi + \alpha(\Slobar - \Slo), \quad \alpha \in \R 
\label{eq:sigmaLCV}
\end{equation} 
where we have specialized to the bifidelity case and $\Shi$, $\Slo$, and $\Slobar$ are as in \Cref{sec:fixSigmalo}. The optimal scalar value for $\alpha$ is $\frac{\trace{\Psi_{\rm lo, hi}}}{\trace{\Psi_{\rm lo}}}$ \cite{maurais2023logEuclidean},
where $\Psi_{\rm lo, hi} = \E[(\Shi - \Sigmahi) \otimes (\Slo - \Sigmalo)]$ and $\Psi_{\rm lo} = \E[(\Slo - \Sigmalo) \otimes (\Slo - \Sigmalo)]$ are Euclidean covariances. More generally, if we allow $\alpha$ to be {linear operator}-valued, the optimal LCV estimator satisfies 
\begin{equation}
\Sigmahihat^{\rm EMF} - \Shi = \Psi_{\rm lo, hi}\Psi_{\rm lo}\inv (\Slobar - \Slo).
\label{eq:LCV_rearr}
\end{equation}
\Cref{eq:LCV_rearr} has the same form as \cref{eq:reg_as_cv}: the difference (computed via subtraction) between $\Sigmahihat^{\rm EMF}$ and $\Shi$ is equal to the difference between $\Slobar$ and $\Slo$ scaled by $\Psi_{\rm lo, hi}\Psi_{\rm lo}\inv$, 
the Euclidean analogue of $\Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv$. 

\paragraph{Log-linear control variate estimator}
As a positive-definiteness-preserving alternative to \cref{eq:sigmaLCV}, the authors \cite{maurais2023logEuclidean} propose the log-Euclidean multifidelity (LEMF) estimator,
\begin{equation}
\log \Sigmahihat^{\rm LEMF} = \log \Shi + \alpha(\log\Slobar - \log\Slo), 
\label{eq:LEMF}
\end{equation}
which is a linear control variate estimator in the log-Euclidean geometry for $\bbP_d$ \cite{arsigny2006log}. If we seek to minimize the \textit{log-Euclidean} MSE,  $\E[||\log\Sigmahihat^{\rm LEMF} - \Sigmahi ||_{\rm F}^2]$, and once again allow $\alpha$ to be a linear operator, then the resulting optimal log-Euclidean control variate estimator satisfies  
\begin{equation}
	\log \Sigmahihat^{\rm LEMF} - \log \Shi = \Phi_{\rm lo, hi}\Phi_{\rm lo}\inv (\log \Slobar - \log \Slo),
	\label{eq:le_est}
\end{equation}
where $\Phi_{\rm lo, hi}$ and $\Phi_{\rm lo}$ are the {log-Euclidean} covariances
\[ 
\Phi_{\rm lo, hi} = \E[(\log \Shi - \log \Sigmahi) \otimes (\log \Slo - \log \Sigmalo)], \quad  \Phi_{\rm lo} = \E[(\log \Slo - \log \Sigmalo) \otimes (\log \Slo - \log \Sigmalo) ].  
\]
The LEMF estimator \eqref{eq:le_est} has the same form as the fixed-$\Sigmalo$ regression estimator \eqref{eq:reg_as_cv} and the LCV estimator \eqref{eq:LCV_rearr}. 
Indeed, each of the three estimators takes the form of a control variate equation in a different geometry for $\bbP_d$:

\begin{align}
	\Sigmahihat^{\rm EMF} - \Shi &= \Psi_{\rm lo, hi}\Psi_{\rm lo}\inv (\Slobar - \Slo) & \text{Euclidean geometry} \label{eq:cv_euclidean}\\
	\log \Sigmahihat^{\rm LEMF} - \log \Shi &= \Phi_{\rm lo, hi}\Phi_{\rm lo}\inv (\log \Slobar - \log \Slo) & \text{Log-Euclidean geometry} \label{eq:cv_logEuclidean} \\ 
 	\log_{\Sigmahihat^{\rm MRMF}}(\Shi) &= \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \log_{\Slobar}(\Slo) & \text{Affine-invariant geometry} \label{eq:cv_affinvar}
\end{align}
Furthermore, the control variate estimators \cref{eq:cv_affinvar,eq:cv_euclidean,eq:cv_logEuclidean} have the form of best linear unbiased estimators (BLUEs) on the tangent spaces corresponding to their respective geometries, as we state in the following theorem. 
\begin{theorem}
    Suppose that $\Sigmalo$ is known and we are given a random variable pair $(\Shi, \Slo)$
    such that $(\Shi, \Slo)$ is an unbiased estimator of $(\Sigmahi, \Sigmalo)$ in the sense of \cite{smith2005covariance} under the (a) Euclidean, (b) log-Euclidean, or (c) affine-invariant geometry for $\bbP_d$. That is, (a) $\E[\Shi - \Sigmahi] = \bf 0$ and $\E[\Slo - \Sigmalo] = \bf 0$, (b) $\E[\log \Shi - \log\Sigmahi] = \bf 0$ and $\E[\log \Slo - \log\Sigmalo] = \bf 0$, or (c) $\E[\log_\Sigmahi \Shi] = \bf 0$ and $\E[\log_\Sigmalo \Slo] = \bf 0$. 
    The following implications hold:
    \vskip 0.2cm
    \begin{enumerate}[label=\Roman*., left=0.05cm]
       \item If (a), then the best unbiased linear estimator of $\Sigmahi$ on $\bbH_d$ equipped with the Frobenius metric satisfies 
        \[
        \hat\Sigma_{\rm hi}^{\rm EMF} - \Shi = \Psi_{\rm lo, hi}\Psi_{\rm lo}\inv (\Sigmalo - \Slo).
        \]
        \item If (b), then the best unbiased linear estimator of $\Sigmahi$ on $\bbP_d$ equipped with the log-Euclidean geometry and metric satisfies
        \[
        \log \hat\Sigma_{\rm hi}^{\rm LEMF} - \log \Shi = \Phi_{\rm lo, hi}\Phi_{\rm lo}\inv (\log \Sigmalo - \log \Slo).
        \]
        \item If (c), then the best unbiased linear-on-tangent-space estimator of $\Sigmahi$ on $\bbP_d$ equipped with the affine-invariant geometry satisfies
        \[
        \log_{\hat\Sigma_{\rm hi}^{\rm MRMF}}(\Shi) = \Gamma_{\rm lo, hi}\Gamma_{\rm lo}\inv \log_{\Sigmalo}(\Slo).
        \]
    \end{enumerate}
\label{prop:BLUEs}
\end{theorem}

\subsubsection{Generality of the regression framework}%
As we obtained \cref{eq:cv_affinvar} by applying the simplifying assumption that $\Sigmalo$ is (approximately) known in \cref{eq:mdist_min}, we could have also obtained \cref{eq:cv_euclidean,eq:cv_logEuclidean} by simplifying analogous regression estimators formulated following the structure of \Cref{sec:formulation} but with the Euclidean or log-Euclidean instead of the affine-invariant geometry for $\bbP_d$. Indeed, because the Euclidean and log-Euclidean geometries both feature vector-space structure, the Mahalanobis distance minimization \cref{eq:mdist_min}, which under the affine-invariant geometry defines a nonlinear least-squares problem, would become a {linear} least squares problem, either for $\Sigma_0, \dots, \Sigma_L$ or $\log\Sigma_0, \dots, \log\Sigma_L$, possessing a closed-form solution analogous to that of multilevel scalar BLUEs in \cite{schaden2020multilevel, schaden2021asymptotic}. One could consider a number of other geometries for $\bbP_d$ as well, including Bures-Wasserstein \cite{MalagoEtAl2018, bhatia2019bures} and log-Cholesky \cite{Lin2019}. %
Choice of geometry within the context of multifidelity covariance estimation should depend on a number of factors, including computational complexity, availability of Riemannian logarithmic and exponential maps, preservation of positive-definiteness, and desired interpretation. We discuss implications of this generality for multifidelity estimation more broadly in \Cref{sec:conclusion}.
 
\section{Computational approaches} %
\label{sec:computation}
We now discuss the practicalities of solving 
\begin{equation*}
	(\hat\Sigma_0, \dots, \hat\Sigma_L) = \argmin_{\Sigma_0,\dots, \Sigma_L \in \bbP_d} \langle \log_{\bfSigma} \bfS,\; \Gamma_\bfS\inv \log_{\bfSigma} \bfS \rangle_{\bfSigma} \quad \text{s.t. } \bfSigma = \mu_\bfS(\Sigma_0, \dots, \Sigma_L)
\end{equation*}
in order to estimate the high-fidelity covariance matrix $\Sigma_0$ and (as an added bonus) low-fidelity covariance matrices $\Sigma_1, \dots, \Sigma_L$, where $\bfS$ and $\mu_\bfS(\Sigma_0, \dots, \Sigma_L)$ are as in \cref{eq:S_rv}. As made possible by \cref{thm:ts_agnostic}, in this section we work exclusively with the equivalent problem formulated in terms of the inner- and outer-products of $\rmT_\bfI \bbP_d^{N}$, 
\begin{equation}
	(\hat\Sigma_0, \dots, \hat\Sigma_L) = \argmin_{\Sigma_0,\dots, \Sigma_L \in \bbP_d} \langle \log_{\bfSigma} \bfS,\; \Gamma_{\bfS, \bfI}\inv \log_{\bfSigma} \bfS \rangle \quad \text{s.t. } \bfSigma = \mu_\bfS(\Sigma_0, \dots, \Sigma_L)
	\label{eq:mdist_min_I}.
\end{equation}

\subsection{Regularization in the intrinsic metric}
\label{sec:regularization}
We have noticed empirically that computing the Mahalanobis distance objective can be numerically unstable even when preconditioning is employed. %
A helpful tool for addressing this issue is regularization in the intrinsic metric; instead of solving \cref{eq:mdist_min_I} as written, we solve a \textit{penalized version}
\begin{multline}
(\hat\Sigma_0, \dots, \hat\Sigma_L) = \argmin_{\Sigma_0,\dots, \Sigma_L \in \bbP_d} \langle \log_{\bfSigma} \bfS,\; \Gamma_{\bfS, \bfI}\inv \log_{\bfSigma} \bfS \rangle  + \sum_{\ell = 1}^L \lambda_\ell ||\log(\Sigma_\ell)||^2_{\rm F} \\ \text{s.t. } \bfSigma = \mu_\bfS(\Sigma_0, \dots, \Sigma_L) 
\label{eq:mdist_min_reg}
\end{multline}
where $\lambda_1, \dots, \lambda_L > 0$ are positive regularization parameters. The terms $||\log(\Sigma_\ell) ||_{\rm F}^2 = d^2(\Sigma_\ell, I)$ correspond to the intrinsic distances between $(\Sigma_0, \dots, \Sigma_L)$ and the identity matrix and in general help control the conditioning of $\hat\Sigma_0, \dots, \hat\Sigma_L$, as the intrinsic distance between any non-positive-definite matrix and the identity is infinite. %

\subsubsection{Regularization parameter selection}
As with any penalized estimator, the regularization parameters in \cref{eq:mdist_min_reg} should be tuned to balance data fitting, encapsulated in the Mahalanobis distance term, with regularity, accounted for in the intrinsic distance terms. While we leave development of regularization parameter selection methods for the general estimator \cref{eq:mdist_min_reg} to future work, in the specific case of the fixed-$\Sigmalo$ simplification (\Cref{sec:fixSigmalo}) we have found a useful heuristic. The penalized regression problem in this setting reads 
\begin{equation} 
\Sigmahihat = \argmin_{\Sigmahi \in \bbP_d} \langle \log_\bfSigma \bfS, \; \Gamma_\bfS\inv \log_\bfSigma \bfS \rangle + \lambda_{\rm hi} ||\log (\Sigmahi) ||_{\rm F}^2 , \quad \text{s.t. } \bfSigma = \mu_\bfS(\Sigmahi, \Slobar) 
\label{eq:fixlo_penalized}
\end{equation} 
In testing our estimator on simple examples of varying dimension and sweeping over a wide range of regularization parameters, we found that the choice of $\lambda_{\rm hi}$ minimizing MSE in the intrinsic metric closely corresponded to that yielding a mean squared Mahalanobis distance of $\frac{d(d+1)}{2}$, as demonstrated in \cref{fig:regParSelect}. 
% Figure environment removed
We saw in \Cref{sec:fixSigmalo} that when $\Sigmalo$ is known exactly, the analytical minimum of \eqref{eq:twomat_reg} solved over $\bbP_d$ without regularization has expectation $\frac{d(d+1)}{2}$. Thus it appears that the best choice of regularization parameter in \cref{eq:fixlo_penalized} with $\Sigmalohat$ fixed at $\Slobar \approx \Sigmalo$ is that which ensures that the statistics of our computed solution match the statistics of the theoretical solution given in \eqref{eq:regest_nleq}. 

\subsection{Square root parameterization} %
Solving \cref{eq:mdist_min_I} directly requires optimization over the manifold-valued variables $\Sigma_0, \dots, \Sigma_L \in \bbP_d$,
to which standard gradient-based methods are not directly applicable. Although there do exist methods and software packages for manifold optimization, e.g., \cite{absil2009optimization, boumal2014manopt, sra2015conic}, we choose to circumvent their machinery by reformulating the problem in terms of matrix square roots. 
Instead of solving \cref{eq:mdist_min_I}, we solve 
\begin{equation}
	(\hat B_0, \dots, \hat B_L) \in \argmin_{B_0, \dots, B_L \in \bbH_d} \left\langle\log_{\bfSigma} \bfS,\, \Gamma\inv_{\bfS, \bfI}\log_\bfSigma \bfS \right\rangle \quad \text{s.t. } \bfSigma = \mu_\bfS(B_0^2, \dots, B_L^2)
	\label{eq:sqrtOpt},
\end{equation}
optimizing over the {matrix square roots} $(B_0, \dots, B_L) \equiv \left(\Sigma_0^{1/2}, \dots, \Sigma_L^{1/2} \right)$ which inhabit the Euclidean vector space $\bbH_d$. The formulation \eqref{eq:sqrtOpt} lends itself to unconstrained gradient descent methods because, given a starting point consisting of $L+1$ symmetric matrices, as long as the descent directions are computed such that they lie in $\bbH_d^L$ (see, e.g., \cite{matrixcookbook, minka2000old}), the result of optimization will also be in $\bbH_d^L$.

A slight subtlety of the square root formulation \cref{eq:sqrtOpt} is that it does not guarantee that the resulting $(\hat\Sigma_0, \dots, \hat\Sigma_L) = (\hat B_0^2, \dots, \hat B_L^2)$ will be strictly positive definite; rather it is only necessary that they be positive semidefinite. Strict positive definiteness and increased numerical stability can be enforced by adding regularization as in \Cref{sec:regularization}. 

\section{Numerical results}
\label{sec:numerics}
In this section we demonstrate the performance of our multifidelity covariance estimator \cref{eq:mdist_min} in a forward uncertainty quantification setting (\Cref{sec:simpleGaussian}) and in a downstream machine-learning task known as metric learning (\Cref{sec:metricLearning}).
\subsection{Simple Gaussian example}
\label{sec:simpleGaussian}
The first test problem we consider is that of estimating the covariance of a high-fidelity four-dimensional Gaussian random variable $\Xhi \sim \calN(\mathbf{0}, \Sigmahi)$ by incorporating samples of a low-fidelity random variable related to $\Xhi$ by
\[
\Xlo = \Xhi + \varepsilon,
\]
where $\quad \varepsilon \sim \calN(\mathbf{0}, \sigma^2 I)$ is independent of $\Xhi$. $\Xhi$ and $\Xlo$ are jointly Gaussian with 
\[
\begin{bmatrix}
    \Xhi \\ \Xlo 
\end{bmatrix} 
\sim \calN \left(\mathbf{0}, \; \begin{bmatrix}\Sigmahi & \Sigmahi \\ \Sigmahi & \Sigmahi + \sigma^2 I\end{bmatrix} \right). 
\]
We set $\sigma^2 = 0.7$ and obtain $\Sigmahi$ from the Wishart ensemble in $d = 4$ dimensions, i.e., $\Sigmahi = A\t A$ where the entries of $A \in \R^{4 \times 4}$ were sampled i.i.d.\ from the standard normal distribution. 

We (artificially) impose costs $c_{\rm hi} = 1$ to sample $\Xhi$ and $c_{\rm lo} = 10^{-2}$ to sample $X_{\rm lo}$ and vary the total sampling budget $B$ in the interval $[6, 206]$. For each budget value we compute a regularized fixed-$\Sigmalo$ multifidelity regression estimator \cref{eq:fixlo_three} and EMF and LEMF control variate estimators using the optimal sample-allocation corresponding to the Euclidean estimator; see \cite{maurais2023logEuclidean} for details. We additionally compute equivalent-cost single-fidelity estimators using high-fidelity samples alone and low-fidelity samples alone for comparison.
% Figure environment removed
For each value of the budget $B$ we pre-compute the covariance operator $\Gamma_{\bf S, I}$ using 1000 pilot samples. We additionally pre-compute the regularization parameters $\lambda_{\rm hi}$ in \cref{eq:fixlo_penalized} \textit{admissibly} by testing 18 values of $\lambda_{\rm hi}$ logarithmically spaced over $[10^{-3}, 10^2]$ and choosing the one corresponding most closely to an average minimum Mahalanobis distance of $\frac{d(d+1)}{2} = 10$ as computed over 32 trials. A plot of the selected regularization parameters and the resulting mean Mahalanobis distance in the ensuing trials for each value of $B$ can be seen in \cref{fig:diagnostics_gaussian}. 

In \cref{fig:se_gaussian,fig:SE_hists,fig:SE_hists_intrinsic} we examine the squared error behavior of our regression estimator $\Sigmahihat^{\rm MRMF}$ \cref{eq:fixlo_three}, the LEMF and EMF estimators $\Sigmahihat^{\rm LEMF}$ \cref{eq:LEMF} and $\Sigmahihat^{\rm EMF}$ \cref{eq:sigmaLCV} of \cite{maurais2023logEuclidean}, and equivalent-cost low-fidelity- and high-fidelity-only estimators $\Sigmahihat^{\rm LF}$ and $\Sigmahihat^{\rm HF}$, in the Frobenius and intrinsic metrics over 2000 repeated trials. In both metrics we see that $\Sigmahihat^{\rm MRMF}$ outperforms $\Sigmahihat^{\rm HF}$ at all budgets and generally has an edge on $\Sigmahihat^{\rm LEMF}$ and $\Sigmahihat^{\rm EMF}$ as well.
% Figure environment removed
While the performances of $\Sigmahihat^{\rm MRMF}$ and $\Sigmahihat^{\rm EMF}$ are similar in the Frobenius metric, $\Sigmahihat^{\rm EMF}$ does noticeably worse in the intrinsic metric because it frequently loses definiteness and the intrinsic distance between $\Sigmahi$ and an indefinite matrix is infinite. The rates at which $\Sigmahihat^{\rm EMF}$ loses definiteness are shown in the right panel of \cref{fig:diagnostics_gaussian}; at the four lowest budgets they exceed 10\%. 

Performances of $\Sigmahihat^{\rm MRMF}$ and $\Sigmahihat^{\rm LEMF}$ are similar in both metrics except for lower values of budget $B$, at which we have noticed that $\Sigmahihat^{\rm LEMF}$ can become unstable. Indeed, while the median squared Frobenius error of $\Sigmahihat^{\rm LEMF}$ looks reasonable in \cref{fig:se_gaussian}, the \textit{mean} squared error in the Frobenius metric at the lowest budget was quite high, on the order of $10^8$, due to a few extreme outliers. Likewise, in \cref{fig:SE_hists_intrinsic} we see that at the lowest budget the squared error distribution of $\Sigmahihat^{\rm LEMF}$ is shifted significantly upward from that of $\Sigmahihat^{\rm MRMF}$.

While the low-fidelity estimator $\Sigmahihat^{\rm LF}$ does out-perform the other estimators at the lowest budgets, its error stagnates as the budget is increased due to the presence of bias. By contrast, the squared errors of the multifidelity estimators decrease with increasing budget and quickly fall below that of $\Sigmahihat^{\rm LF}$ to such an extent that their histograms have almost no common support. 
 




% Figure environment removed 

% Figure environment removed 






\subsection{Metric learning with the surface quasi-geostrophic equation}
\label{sec:metricLearning}
In this second example we demonstrate the utility of our multifidelity covariance regression estimator applied within the geometric mean metric learning framework of \cite{zadeh2016geometric}. 

\subsubsection{Geometric mean metric learning}
Broadly speaking, the goal of metric learning is to obtain a distance measure over Euclidean space such that machine-learning tasks, including clustering and classification, are easier in the new metric for a given dataset \cite{xing2002distance, weinberger2009distance, kulis2013metric, bellet2013survey}. Supposing, for example, that we have a dataset containing points belonging to $K \geq 2$ distinct classes, an effective learned metric $d(\cdot, \cdot)$ on this space should place points from the same class close together while placing those from different classes far apart. If we consider only metrics which take the form of a Mahalanobis distance,
\[
d_A(\bfy, \bfy') = \sqrt{(\bfy - \bfy')\t A(\bfy - \bfy')}, 
\]
where $\bfy, \bfy' \in \R^d$ and $A \in \bbP_d$, the task of metric learning reduces to the task of obtaining a suitable symmetric positive definite ``metric matrix'' $A$. 
In \cite{zadeh2016geometric}, the authors propose a novel family of objective functions for the semi-supervised Mahalanobis metric learning problem. The optimal metric matrices admit closed form expressions as points on geodesics of $\bbP_d$ and are, up to scaling by constant factors, 
\begin{equation}
A_{\rm GMML} = T^{-\frac{1}{2}}(T^{\frac{1}{2}} D T^{\frac{1}{2}})^t T^{-\frac{1}{2}},
\label{eq:GMML}
\end{equation} 
where $t\in [0,1]$ and $T$ and $D$ are the \textit{similarity matrix} and \textit{dissimilarity matrix,}
\[
T = \E_{\mathrm{class}(\bfy) = \mathrm{class}(\bfy')}\left[\left(\bfy - \bfy'\right)\left(\bfy - \bfy'\right)\t \right], \quad D = \E_{\mathrm{class}(\bfy) \neq \mathrm{class}(\bfy')}\left[\left(\bfy - \bfy'\right)\left(\bfy - \bfy'\right)\t \right].
\]
In the case that the dataset is drawn from an equal mixture of two classes $(K = 2)$, $T$ and $D$ can be written %
\begin{equation}
T = \Gamma_0 + \Gamma_1, \quad D = T + (\bfm_0 - \bfm_1)(\bfm_0 - \bfm_1)\t,
\label{eq:TandD_covs}
\end{equation} 
with $\Gamma_i = \Cov[\bfy \mid \mathrm{class}(\bfy) = i]$ and $\bfm_i = \E[\bfy \mid \mathrm{class}(\bfy) = i]$, $i \in \{0, 1\}$. We see from the formulations in \cref{eq:TandD_covs} that our ability to learn the metric matrix \eqref{eq:GMML} is strongly dependent on our ability to learn the covariance matrices $\Gamma_0$ and $\Gamma_1 \in \bbP_d$. 

\subsubsection{Surface quasi-geostrophic equation}
In this example we consider a metric learning problem in which our data are observations of solutions to a surface quasi-geostrophic (SQG) equation \cite{HeldEtAl1985} with parameters drawn according to a two-class mixture distribution. %
The SQG equation describes the evolution of the buoyancy $b(\bfx, t)$ over a periodic spatial domain $\calX = [-\pi, \pi] \times [-\pi, \pi]$ and is given by 
\begin{equation}
    \partial_t b(\bfx, t; \bftheta) + J(\psi, b) = 0 \, ,
\label{eq:SQG}
\end{equation}
where $\bfx = (x_1,x_2)$ is the spatial coordinate, $\psi$ is the streamfunction, $J(\psi, b)$ is the Jacobian determinant
\begin{equation*}
    J(\psi, b) = \left( \frac{\partial \psi}{\partial x_1} \right)\left(\frac{\partial b}{\partial x_2}  \right) - \left( \frac{\partial b}{\partial x_1}  \right)\left(\frac{\partial \psi}{\partial x_2}  \right) 
\end{equation*}
and $\bftheta \in \R^5$ are parameters of the dynamics and initial condition. We specify the initial condition as 
\[
    b_0(\bfx;\bftheta) = -\frac{1}{(2\pi/ |\theta_5|)^2} \exp\left( -x_1^2 - \exp(2\theta_1) x_2^2 \right),
\]
the contours of which form ellipses parameterized by $\theta_1$ and $\theta_5$. The remaining parameters $\theta_2$, $\theta_3$, and $\theta_4$ govern the dynamics \cref{eq:SQG}. The parameters $\bftheta$ are drawn from an equal mixture of $\pi_0\sim \calN(\bfmu_0, C)$ and $\pi_1 \sim (\bfmu_1, C)$, which differ only in the mean of the log aspect-ratio $\theta_1$; see \cref{app:sqg} for details. We sample the solution to \eqref{eq:SQG} at nine equally spaced points in the domain $\calX$ to obtain observations $\bfy \in \R^9$. Our goal in the metric learning setting is to be able to distinguish samples of $\bfy \mid \bftheta \sim \pi_0$ from samples of $\bfy \mid \bftheta \sim \pi_1$. 

\subsubsection{Multifidelity metric learning}
The metric \cref{eq:GMML} can be learned by estimating $\Gamma_i = \Cov[\bfy \mid \bftheta \sim \pi_i]$ and $m_i = \Cov[\bfy \mid \bftheta \sim \pi_i]$, $i \in \{0,1\}$ from samples of $\bfy \mid \bftheta \sim \pi_0$ and $\bfy \mid \bftheta \sim \pi_1$. We cast this metric learning problem in the multifidelity setting as follows: Let $\bfy_{\rm hi}$ correspond to realizations of the observable when the SQG equation \cref{eq:SQG} is solved numerically over 256 grid points in each coordinate direction, and $\bfy_{\rm lo}$ correspond to observations when the SQG equation \cref{eq:SQG} is solved numerically over just 16 grid points in each direction. In both cases we compute the solution to time $T = 24$ with a time step of $\Delta t = 0.05$, so we associate the costs of sampling $\bfy_{\rm hi}$ and $\bfy_{\rm lo}$ with the number of grid points in the solver; $c_{\rm hi} = 256^2 = 65,536$ and $c_{\rm lo} = 16^2 = 256$. $\bfy_{\rm hi}$ is thus 256 times more expensive to sample than $\bfy_{\rm lo}$. 

Our goal is to learn the covariance matrices $\Gamma_0$ and $\Gamma_1$, and subsequently the metric matrix $A_{\rm GMML}$, by taking advantage of the multifidelity structure in this problem. We allocate a computational budget of $B = 17c_{\rm hi}$ to learning each of $\Gamma_0$ and $\Gamma_1$ and apply a manifold regression multifidelity (MRMF) estimator to each, with the numbers of high- and low-fidelity samples involved determined according the optimal allocation given in \cite{maurais2023logEuclidean}. 
For comparison we also consider the log-Euclidean multifidelity (LEMF) and Euclidean multifidelity (EMF) estimators of \cite{maurais2023logEuclidean} with the same sample allocation, and equivalent cost estimators using only high-fidelity samples $\bfy_{\rm hi} \mid \bftheta \sim \pi_i$ or only low-fidelity samples $\bfy_{\rm lo} \mid \bftheta \sim \pi_i$. The specific values of the sample allocations for each class $i \in \{0,1\}$ and each type of estimator can be seen in \cref{tab:samp_alloc}. We use the estimates we obtain of $\Gamma_0$ and $\Gamma_1$ to construct an estimate of $A_{\rm GMML}$. 
\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule  
     &  \multicolumn{2}{c}{Class 0} & \multicolumn{2}{c}{Class 1} \\ \cmidrule(lr){2-3}\cmidrule(lr){4-5}
     & $N_{\rm hi}$ & $N_{\rm lo}$ & $N_{\rm hi}$ & $N_{\rm lo}$ \\ 
     \midrule 
    High-fidelity alone & 17 & 0 & 17 & 0 \\ 
    Low-fidelity alone & 0 & 4352 & 0 & 4352\\ 
    Multifidelity & 15 & 512 & 14 & 768 \\ 
    \bottomrule  
    \end{tabular}
    \caption{SQG metric learning: Sample allocations for single- and multi-fidelity estimators of $\Gamma_0$ and $\Gamma_1$. Each allocation requires the same computational budget, and the multifidelity allocations differ between classes due to differing values of generalized correlation determining the allocations according to \cite{maurais2023logEuclidean}.}
    \label{tab:samp_alloc}
    \vskip -0.4cm
\end{table}
\subsubsection{Results}
Prior to applying our MRMF estimator and the LEMF and EMF estimators of \cite{maurais2023logEuclidean} to the tasks of estimating $\Gamma_0$ and $\Gamma_1$, we simulate 12,000 high-fidelity and 12,000 low-fidelity pilot samples of each of $\bfy \mid \bftheta \sim \pi_0$ and $\bfy \mid \bftheta \sim \pi_1$ in order to estimate the required hyperparameters: generalized correlations and variances for the LEMF and EMF estimators, and $\Gamma_\bfS$ for the MRMF estimator. We additionally compute a reference estimate of $A_{\rm GMML}$ with these samples, which we use to approximate the error in the estimators we consider. 

Using the sample-allocations in \cref{tab:samp_alloc}, we compute estimates $\hat \Gamma_i^{\rm HF}$, $\hat \Gamma_i^{\rm LF}$, $\hat \Gamma_i^{\rm EMF}$, $\hat \Gamma_i^{\rm LEMF}$, and $\hat \Gamma_i^{\rm MRMF}$ using high-fidelity samples alone, low-fidelity samples alone, the LEMF estimator \cref{eq:LEMF}, the EMF estimator \cref{eq:sigmaLCV}, and the MRMF estimator \cref{eq:fixlo_penalized}, respectively, $i \in \{0, 1\}$. We specifically employ the regularized, fixed-$\Sigmalo$ version of the regression estimator \cref{eq:fixlo_penalized} and in this example choose optimal values of the regularization parameters via a coarse direct search. We combine these estimates of $\Gamma_0$ and $\Gamma_1$ with estimates of $\bfm_0$ and $\bfm_1$, obtained using multifidelity Monte Carlo \cite{bpkwmg} for the multifidelity covariance estimators and standard (single-fidelity) Monte Carlo for the single-fidelity covariance estimators, to obtain ensuing estimates of $A$, denoted $\hat A^{\rm HF}$, $\hat A^{\rm LF}$, $\hat A^{\rm EMF}$, $\hat A^{\rm LEMF}$, and $\hat A^{\rm MRMF}$. Motivated by Figure 3 in \cite{zadeh2016geometric}, we set $t = 0.1$ in \cref{eq:GMML} for our experiments. The results presented in this section reflect performance of each estimator of $A$ over 2000 repeated trials; for results detailing performance in estimating each of $\Gamma_0$ and $\Gamma_1$ individually see \cref{app:sqg:addResults}. %
% Figure environment removed
% Figure environment removed  
\paragraph{Efficacy in Estimating $A_{\rm GMML}$}
In \cref{fig:sqg_mse_HF} we show distributions of the squared error of each of $\hat A^{\rm LF}$, $\hat A^{\rm LEMF}$, and $\hat A^{\rm MRMF}$ in comparison to the squared error distribution of $\hat A^{\rm HF}$. We do not show results for $\hat A^{\rm EMF}$ because one of $\hat\Gamma_0^{\rm EMF}$ or $\hat\Gamma_1^{\rm EMF}$ was \textit{indefinite in 94\% of trials} (a known liability of the EMF estimator; see \cite{maurais2023logEuclidean} for further discussion), precluding meaningful computation of the geodesic \cref{eq:GMML} defining $A_{\rm GMML}$. We see from \cref{fig:sqg_mse_HF} that $\hat A^{\rm LF}$, $\hat A^{\rm LEMF}$, and $\hat A^{\rm MRMF}$ all provide significant decreases in Frobenius MSE relative to the baseline approach of estimating $A_{\rm GMML}$ with high-fidelity samples alone and that $\hat A^{\rm MRMF}$ enjoys the best performance by a sizeable margin. Interestingly, use of $\hat A^{\rm LEMF}$ causes an \textit{increase} in intrinsic MSE relative to $\hat A^{\rm HF}$. We notice a similar phenomenon in the results of \Cref{sec:simpleGaussian} for low budgets, and posit that it may result from amplification of error by the matrix exponential. In \cref{fig:sqg_mse_reg} we further compare the squared error distributions of $\hat A^{\rm LF}$ and $\hat A^{\rm LEMF}$ to that of $\hat A^{\rm MRMF}$ and demonstrate that use of the regression estimator \cref{eq:fixlo_penalized} results in substantial decreases in squared error even in comparison to use of low-fidelity samples alone or the LEMF estimator of \cite{maurais2023logEuclidean}. In particular, there is very little overlap between the supports of the squared error distributions of $\hat A^{\rm LF}$ and those of $\hat A^{\rm MRMF}$ in \cref{fig:sqg_mse_reg}. 

In light of this observation, we note that although $\hat A^{\rm LF}$ features low variance (in a generalized sense) due to the large numbers of samples involved in computing $\hat\Gamma_0^{\rm LF}$ and $\hat\Gamma_1^{\rm LF}$, it has high bias due to the coarseness of the discretization generating $\bfy_{\rm lo}$. The multifidelity methods, by contrast, use low-fidelity samples to reduce variance while retaining a small number of high-fidelity samples to counteract bias, thus achieving an overall lower MSE in this example.

% Figure environment removed
\paragraph{Downstream performance quantified by mean relative error}
One way of quantifying the goodness of an estimate of $A_{\rm GMML}$ is to examine the mean relative error between the norms induced by $A_{\rm GMML}$ and those by the estimate \cite{zadeh2016geometric}. For $A \in \bbP_d$ we denote the norm corresponding to the distance $d_{A}(\cdot, \cdot)$ by $\| \cdot \|_A = d(\cdot, 0)$, i.e., for $\bfy \in \R^d$ we have $\| \bfy \|_A = \sqrt{\bfy\t A \bfy}$. The mean relative error (MRE) associated with an estimate $\hat A$ of $A_{\rm GMML}$ is given by 
\begin{equation} 
\mathrm{MRE}(\hat A) = \E_\bfy\left[ \frac{\left| \| \bfy\|_{\hat A} - \| \bfy \|_{A_{\rm GMML}} \right|}{\| \bfy \|_{A_{\rm GMML}}} \right].
\label{eq:MRE_def}
\end{equation} 
For each estimator $\hat A \in \{\hat A^{\rm HF}, \hat A^{\rm LF}, \hat A^{\rm LEMF},\hat A^{\rm MRMF} \}$ we estimate $\mathrm{MRE}(\hat A)$ by approximating \cref{eq:MRE_def} with a Monte Carlo estimate over a test set of observations $\{\bfy^{(i)}\}_{i=1}^{5000}$,
\begin{equation}
\mathrm{MRE}(\hat A) \approx \frac{1}{5000}\sum_{i=1}^{5000} \frac{\left| \| \bfy^{(i)}\|_{\hat A} - \| \bfy^{(i)} \|_{A_{\rm GMML}} \right|}{\| \bfy^{(i)} \|_{A_{\rm GMML}}},
\label{eq:MRE_mc}
\end{equation}
where the parameters generating the test observations $\{\bfy^{(i)}\}_{i=1}^{5000}$ are sampled from an equal mixture of $\pi_0$ and $\pi_1$. We compute \cref{eq:MRE_mc} for 50 realizations of each $\hat A \in \{\hat A^{\rm HF}, \hat A^{\rm LF}, \hat A^{\rm LEMF},\hat A^{\rm MRMF} \}$ and visualize the resulting values of empirical MRE in \cref{fig:mre}.
As can be seen, use of any of $\hat A^{\rm LF}$, $\hat A^{\rm LEMF}$, or $\hat A^{\rm MRMF}$ results in a substantial decrease in MRE over $\hat A^{\rm HF}$, with $\hat A^{\rm MRMF}$ providing the steepest decrease: the average MRE of $\hat A^{\rm MRMF}$ is 53\% lower than that of $\hat A^{\rm HF}$ and additionally 25\% lower than that of $\hat A^{\rm LF}$ and 6.9\% lower than that of $\hat A^{\rm LEMF}$.

\section{Conclusions}
\label{sec:conclusion}
We have introduced a manifold regression multifidelity (MRMF) estimator of covariance matrices, formulated as the solution to a regression problem on the manifold of SPD matrices. The estimator maintains positive definiteness by construction, %
 provides significant decreases in squared estimation error relative to single-fidelity and other multifidelity covariance estimators, and can benefit downstream tasks such as metric learning. Furthermore, our multifidelity regression framework \textit{encompasses} existing multifidelity covariance estimators based on control variates \cite{maurais2023logEuclidean}, and suggests a general approach to multifidelity estimation of objects residing on Riemannian manifolds. 

Herein we specifically focused on estimation of covariance matrices, and in doing so employed the affine-invariant geometry for SPD matrices; using this geometry enabled us to exploit appealing theoretical properties of the resulting Mahalanobis distance and demonstrate the viability of our multifidelity regression approach in the absence of vector space structure. More broadly, however, the Riemannian multifidelity regression framework we lay out in \Cref{sec:formulation} is applicable to estimation of \textit{any} object residing on a nonlinear manifold, with covariance matrices being just one use case. To generalize our estimator in this way, one would adapt the definitions of mean and covariance from \cite{pennec2006intrinsic} to the particular manifold of interest and substitute them into the random variable formulation \cref{eq:S_rv} and Mahalanobis distance minimization problem \cref{eq:mdist_min} given here. Objects which reside on Riemannian manifolds and may be good candidates for multifidelity estimation include rotation matrices \cite{moakher2002means}, elementwise positive matrices \cite{gillis2014NMF}, and probability measures \cite{amari2016information, villani2009optimal, hanApproximateControlVariates2023}. %




\bibliographystyle{siamplain}
\bibliography{references}
\makeatletter\@input{yy.tex}\makeatother
\end{document}
