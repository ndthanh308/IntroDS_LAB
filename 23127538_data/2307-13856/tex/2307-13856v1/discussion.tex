Following we discuss the design choices made in NAFNet and the Baseline network that constrain the performance of the network against adversarial attacks, despite employing adequate defense techniques.

\subsection{Analyzing Intermediate networks}
\label{subsec:discuss:intermediate}
First, we study the \emph{Intermediate network} to ascertain if the spectral artifacts introduced by NAFNet in its reconstructed images were due to replacing a non-linear activation function with a \emph{Simple Gate}.
This is because the channel-wise multiplication would best explain the color mixing artifact and the inherent wrong subsampling during this operation and would account for the accentuated aliasing artifacts. 
Further to understand the influence of the non-linear activation, we also train the  Intermediate network with ReLU activation, referred to as \emph{Intermediate + ReLU}.

We report the findings on the Intermediate networks in Table~\ref{tab:intermediate}.
Here we observe that the Intermediate network performs marginally worse than even NAFNet, especially under adversarial attacks. 
Additionally, in Figure~\ref{fig:cospgd_attack}, we visualize the images reconstructed by the Intermediate network.
Firstly, the clean images~(unperturbed) have not been deblurred significantly.
Secondly, even under mild adversarial attacks, the quality of the reconstructed images is abysmal.
We observe severe checkboard patterns, aliasing, and color mixing in all images reconstructed by the Intermediate network under adversarial attack.
Thus, to better understand the performance of the Intermediate network in comparison to the Baseline network and NAFNet, we perform significantly weaker adversarial attacks. 
To this effect, we use the CosPGD attack but with $\epsilon\approx\frac{2}{255}$, and consider attack iterations $\in$ \{1, 3, 5\}.
We again use $\alpha=0.01$.

\input{lower_eps}
We report the performance of the Intermediate networks in Table~\ref{tab:lower_eps}. 
Interestingly, we observe that after one adversarial attack iteration, the Intermediate network is significantly outperforming both the Baseline network and NAFNet.
However, the Intermediate network is unable to retain this superior performance, and its performance significantly drops as we increase the attack strength~(attack iterations).
Additionally, in Figure~\ref{fig:weak_cospgd} we observe the introduction of the same spectral artifacts for the Intermediate network as those observed in Figure
\ref{fig:cospgd_attack} and Figure~\ref{fig:pgd_attack} (please refer to Section~\ref{appendix:results}).
\input{weak_cospgd}
The intensity of the spectral artifacts increases as we increase the attack strength.
This phenomenon is similar to the performance of NAFNet, which performs admirably on clean samples and under weak adversarial attacks but begins to perform significantly worse as the attack strength increases.
This indicates that even smoothed activation functions in the NAFNet architecture instead of Simple Gate produce strong spectral artifacts in the reconstructed images.

This is in striking contrast to using a non-smooth non-linear activation function, ReLU.
Interestingly, we observe that \emph{Intermediate+ReLU} is significantly more robust, and the degradation in its performance with attack strength is significantly lower than all considered networks, including Restormer. 
In Figures~\ref{fig:cospgd_attack},~\ref{fig:pgd_attack}~\&\ref{fig:weak_cospgd} we observe that the images reconstructed by \emph{Intermediate+ReLU}, while blurry, have significantly fewer artifacts for reasonable values of $\epsilon$.

\input{relu_higher_eps}
Under adversarial attacks, the reconstructed images do not have spectral artifacts similar to \emph{Intermediate network} or NAFNet, but more similar to Restormer and the Baseline.
It is only at severely higher $\epsilon\approx\frac{20}{255}$ that spectral artifacts similar to those produced by \emph{Intermediate network} appear in the reconstructed images from \emph{Intermediate+ReLU}.
Thus, the smoothening of feature maps by the conjunction of Simplified Channel Attention and GeLU, and Simple Gate could be attributed to the introduction of some peculiar spectral artifacts and loss in robustness.%like those produced by NAFNet and Intermediate network .
Using a non-smoothed non-linear activation function like ReLU appears to be an effective mitigation technique.

Additionally, as reported in Table~\ref{tab:adv_perf} we observe the adversarial robustness of both the \emph{Intermediate network} and \emph{Intermediate+ReLU} significantly increases after FGSM training, and is comparable to Restormer.
This significant improvement in adversarial performance is also visible at lower $\epsilon$ attacks, please refer to Table~\ref{tab:lower_eps} and visually shown in Figure~\ref{fig:weak_cospgd}.
Thus, as observed before, adversarial training is a fix to reduce artifacts, even for the \emph{Intermediate network}.


\subsection{Superiority of Restormer}
\label{subsec:discuss:restormer}
In their work, \cite{chen2022simple} attempt to reduce model complexity while retaining the performance of the Restormer.
However, as shown in our work this significantly degrades the generalization ability of the consequent models. 
As larger models tend to have a better trade-off between robustness and accuracy \cite{hendrycks2019benchmarking, hoffmann2021towards}, the reduced model capacity in the Baseline and NAFNet could contribute to the reduced robustness. 
While reducing model complexity is certainly important and desirable, to maintain robustness it requires a more careful and systematic pruning of networks \cite{ye2019adversarial,NEURIPS2020_e3a72c79,hoffmann2021towards} than simply dropping components. 
Apart from the model's complexity in terms of the number of parameters, the attention mechanism itself could be crucial for robustness.

While the Restormer uses a multi-headed self-attention mechanism, both the Baseline network and NAFNet use variants of channel-attention (NAFNet uses the simplified channel-attention proposed by \cite{chen2022simple}).
As shown by \cite{NEURIPS2021_e19347e1}, the self-attention module of vision Transformers significantly aids the Transformer based models to improve their robustness.
Additionally, it helps the model better utilize defense strategies such as additional training, distillation, etc.
A similar phenomenon is observed in Table~\ref{tab:adv_perf}, as Restormer, a vision transformer based model with a multi-headed self-attention module is able to better utilize adversarial training compared to the Baseline network and NAFNet.

\paragraph{Limitations. } Adversarial training and design choices like the use of smoothed or non-smoothed activation functions against using Simple Gates certainly have a significant impact on the performance of the considered image restoration models.
However, these still is a considerable gap in the clean performance of the considered models.
While the fixes work in increasing adversarial robustness and removal of spectral artifacts the images are far from ideal restoration.
As observed, the restored images after the fixes are significantly blurry.
This is a limitation of this work, as this work was focused on removal of spectral artifacts and better adversarial robustness.

This work is a step towards finding a fix and not an absolute fix.
Exploring methods other than adversarial training for increasing adversarial robustness and removal of spectral artifacts could be an interesting future work direction.