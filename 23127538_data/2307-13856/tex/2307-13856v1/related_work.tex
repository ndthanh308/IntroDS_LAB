\paragraph{Transformers for Image Restoration} 
The past decade saw significant improvements in  image restoration, largely owing to the adoption of  deep networks trained on large datasets of clean and degraded images. 
While earlier restoration networks largely adopted CNN-based architectures, subsequent works also explored the use of attention mechanisms inside CNNs~\cite{zhou2020cross,niu2020single,Suin_2020_CVPR}. 
We refer \cite{su2022survey} for a  detailed survey on deep learning approaches to restoration. More recently, vision Transformers \cite{liu2021swin,dosovitskiy2021an} are increasingly adopted for several restoration tasks. 
While~\cite{liang2021swinir,zamir2022restormer,wang2022uformer,conde2022swin2sr,pmlr-v202-xiao23a} adopt Transformers for generic restoration tasks, a few works focus on specific restoration tasks by including such as deblurring~\cite{tsai2022stripformer}, deraining~\cite{liang2022drt}, dehazing~\cite{guo2022image,song2023vision}, removing degradations due to adverse weather conditions~\cite{valanarasu2022transweather}.  
These networks typically employ  encoder-decoder-based architectures with Transformer blocks combined with convolutions.
%\VG{Do we need to write about Restormer, NAFNet \cite{zamir2022restormer,chen2022simple} in a little more detail?}

\paragraph{Adversarial Robustness of Image Restoration. } While the adversarial robustness of deep networks for image recognition is extensively studied, a few works also study the robustness of image restoration networks to adversarial attacks.
\cite{estimators_robustness,Choi_2020_ACCV,Yue21RobustSR}  evaluate adversarial robustness of deep learning-based image super-resolution. 
While \cite{Choi_2020_ACCV} propose adversarial regularization, \cite{Yue21RobustSR} propose frequency domain adversarial example detection, combined with random frequency masking to improve robustness.
\cite{ga2022deblurring} evaluate adversarial robustness of deblurring networks with and without the knowledge of the blur operator, and introduce targeted attacks on restoration.
In \cite{image-to-image}, the adversarial robustness of image-to-image translation models is studied, including restoration tasks, and adversarial training and different transformation-based defenses are evaluated.
Yan et al.~\cite{ijcai2022p211} investigate the robustness of image denoising to zero-mean adversarial perturbations and propose training with clean and adversarial samples to improve robustness.
Yu et al.~\cite{yu2022towards} investigate adversarial robustness of deep learning-based rain removal, and study the effect of architecture and training choices on robustness. Yet, these works do not focus on the more recent Transformer based restoration networks.
With the notable exception of \cite{agnihotri2023cospgd}, where they simply benchmark the adversarial performance of the image restoration networks recently proposed by \cite{chen2022simple}.

\paragraph{Robustness of Transformers \& other modern architectures. }
Recently, Vision Transformers (ViTs) \cite{dosovitskiy2021an,liu2021swin} have been successfully applied to  image recognition, outperforming the older ResNets. Follow-up works modified training schemes and architectures leading to more performant CNN architectures such as ConvNext \cite{liu2022convnet}, and hybrid models combining components of ViTs and CNNs \cite{ali2021xcit}. 
Following the introduction of these novel architectures, several works examined  the robustness properties of these models.
\cite{shao2021adversarial,bhojanapalli2021understanding,shao2022on,paul2022vision} suggest Transformers have better adversarial robustness than CNNs. 
However, \cite{Mahmood_2021_ICCV} shows that vision Transformers are also as vulnerable as CNNs under strong attacks.
\cite{NEURIPS2021_e19347e1} show that CNNs can achieve similar adversarial robustness as Transformers when trained using similar training recipes, yet Transformers still outperform CNNs on out-of-distribution generalization.
\cite{tang2021robustart} benchmark for robustness dependent on the network architecture. 
They find that Transformers are best suited against adversarial attacks while being extremely vulnerable to common corruptions~\cite{hendrycks2019benchmarking} and system noise.
Conversely, CNNs are more robust against common corruptions and system noise while being weakest against adversarial attacks. 
Further, they show that MLP-Mixers are not the best and also not the worst for both cases.

In their work, \cite{eccv2022_cnn_vs_Transformer} benchmark the robustness of state-of-the-art Transformers and CNN architectures and show that CNNs using ConvNext  architecture can be at least as robust as  Transformers for image recognition. 
Meanwhile \cite{croce2022interplay} analyzes the effect of different architectural components such as patches, convolution, activation, and attention, and demonstrates that ConvNexts have better adversarial robustness than ResNets. \cite{xie2020smooth} observe that smooth activation functions improve adversarial training as they enable better gradient updates to compute harder adversarial examples. Subsequent works  \cite{NEURIPS2021_e19347e1,croce2022interplay} also confirm improvement in robustness when GELU~\cite{gelu} activation functions are used in adversarial training.
While \cite{NEURIPS2021_e19347e1} attribute significant robustness gains in Transformers to the self-attention mechanism, \cite{wang2023can} identify other architectural components, including, the use of patches, larger kernels, reducing activation and normalization layers which when incorporated into CNNs lead to out of distribution robustness at least on par with Transformers without the use of attention.

In contrast, our work focuses on the investigation of the robustness of several recent Transformer based restoration models and shows interesting effects of adversarial attacks that can be attributed to different building modules of such models.