Following, we describe the attack framework used and the defense strategy used to combat the vulnerabilities of the architectures exposed by the adversarial attacks.

\subsection{Attack Framework} 
\label{subsec:method:attack}

Let $\mathbf{x}$ denote the ground-truth image, which is corrupted by a possibly non-linear degradation operator $\mathbf{A}$, resulting in an observation $\mathbf{y}^{\mathrm{clean}}$, which can be expressed as
\begin{equation}
    \mathbf{y}^{\mathrm{clean}} = \mathbf{A}(\mathbf{x}).
    \label{eq:measurement}
\end{equation}
Let  $\net$ be a (Transformer-based) neural network parameterized by $\theta$ trained to recover $\mathbf{x}$ from $\mathbf{y}^{\mathrm{clean}}$. 
In this work, we are interested in studying the stability of $\net$ to adversarial attacks that aim to degrade its performance through visually imperceptible changes to the inputs \cite{fgsm, pgd}. 
We evaluate the robustness to attacks using additive perturbations $\delta$ with $\ell_p$-norm constraints.
We generate the adversarial perturbations based on two powerful attack methods CosPGD \cite{agnihotri2023cospgd} developed for dense prediction tasks, and PGD attack \cite{pgd}, both of which we detail in the following.
The objective of the attack is to maximize the deviation of the network output from the ground truth as measured by a loss function $L$, subject to $\ell_p$ norm constraints on the perturbation:
\begin{equation}
 \underset{\delta}{\mathrm{maximize}}~L(\net(\mathbf{y}^{\mathrm{clean}} + \delta) ,~ \mathbf{x} )~ \text{  s.t.  }  \norm{\delta}_p \leq \epsilon.
 \end{equation}
 
\paragraph{PGD. } PGD is an iterative adversarial attack, where each sample is perturbed for a %iteratively, given a 
fixed amount of attack iterations (steps) with the intention of maximizing the loss further with each attack step.
 A single attack step in the PGD attack \cite{pgd} is given as follows,
%\begin{gather}
\begin{align}
    \mathbf{y}^{\mathrm{adv}_{t+1}} &= \mathbf{y}^{\mathrm{adv}_t}+\alpha \cdot \mathrm{sign}\nabla_{\mathbf{y}^{\mathrm{adv}_t}}L(\net(\mathbf{y}^{\mathrm{adv}_t}), \mathbf{x})\\
    \delta &= \phi^{\epsilon}(\mathbf{y}^{\mathrm{adv}_{t+1}} - \mathbf{y}^{\mathrm{clean}})\nonumber\\
    \mathbf{y}^{\mathrm{adv}_{t+1}} &= \phi^{r}(\mathbf{y}^{\mathrm{clean}}+ \delta)\nonumber
    \label{eq:pgd}
\end{align}

where the adversarial example $\mathbf{y}^{\mathrm{adv}_{t+1}}$ at step $t+1$,  is updated using the adversarial example from the previous step $\mathbf{y}^{\mathrm{adv}_{t}}$, $\nabla$ represents the gradient operation, $\alpha$ is the step size for the perturbation, $\phi^{\epsilon}$ is denotes projection onto the appropriate $\ell_p$-norm ball of radius $\epsilon$, depending on the  $\ell_p$ norm constraints on $\delta$, and 
$\phi^{r}$ clips the adversarial example to lie in the valid intensity range of images (between [0, 1]).
Prior works evaluating the adversarial robustness of image restoration networks consider $L$ to be the reconstruction loss (MSE loss) to obtain adversarial examples  maximizing  the reconstruction error.
%
%
\paragraph{CosPGD. } Instead of directly utilizing the averaged pixel-wise losses in PGD attack steps, \cite{agnihotri2023cospgd} propose to weigh the pixel-wise losses using the cosine similarity between the network output and the ground truth (both scaled by softmax), to reduce the importance of the pixels which already have a large error in the previous iterations, and enable the attack to focus on the pixels with low error. 
For the task of restoration (a regression task),
CosPGD attack steps for an untargeted attack are given as:
\begin{align}
\mathbf{x}^{\mathrm{adv}_{t}} &= \net(\mathbf{y}^{\mathrm{adv}_{t}})\\
L_{\mathrm{cos}}&=\sum\mathrm{cossim}(\Psi(\mathbf{x}^{\mathrm{adv}_{t}}), \Psi(\mathbf{x}))\odot L(\mathbf{x}^{\mathrm{adv}_{t}}, \mathbf{x})\nonumber\\
\mathbf{y}^{\mathrm{adv}_{t+1}} &= \mathbf{y}^{\mathrm{adv}_{t}} + \alpha \cdot \mathrm{sign}\nabla_{\mathbf{y}^{\mathrm{adv}_{t}}}L_{\mathrm{cos}}\nonumber\\
\delta &= \phi^{\epsilon}(\mathbf{y}^{\mathrm{adv}_{t+1}} - \mathbf{y}^{\mathrm{clean}})\nonumber\\
    \mathbf{y}^{\mathrm{adv}_{t+1}} &= \phi^{r}(\mathbf{y}^{\mathrm{clean}}+ \delta),\nonumber
\end{align}
where $\Psi$ is the softmax function, $\odot$ denotes  point-wise multiplication,  and the cosine similarity (cossim) is given by
\begin{equation}
\label{eq:cossim}
    \mathrm{cossim}(\overrightarrow{\mathbf{u}},\overrightarrow{\mathbf{v}})=\dfrac{\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{v}}}
{||\overrightarrow{\mathbf{u}}|| \cdot  ||\overrightarrow{\mathbf{v}}|| }
\end{equation}
\cite{agnihotri2023cospgd} demonstrate that this approach results in a stronger attack for pixel-wise regression tasks than a PGD attack.
We use both PGD and CosPGD in our robustness evaluation.
\subsection{Architectures: from Restormer to NAFNet}
\label{subsec:method:arch}

We evaluate the adversarial robustness of  \emph{Restormer} \cite{zamir2022restormer}, a Transformer based architecture for image restoration  and two architectures introduced in \cite{chen2022simple} by  modifying the Restormer architecture. 
Restormer \cite{zamir2022restormer} has a UNet  \cite{ronneberger2015u} like encoder-decoder architecture, using multi-head channel-wise attention modules, gated linear units \cite{dauphin2017language} and  depth-wise convolutions in the feed-forward network. 
This network achieved state-of-the-art performance in image restoration at the time of its publication. 
The authors in \cite{chen2022simple} investigate whether it is possible to retain the performance of Restormer, with a simplified architecture. 
After a thorough ablation study, they propose a simplified \emph{Baseline} network that improved upon the SOTA performance. 
The Baseline network utilizes GELU activations \cite{gelu} and replaces multi-headed self-attention in \cite{zamir2022restormer} with a channel attention module \cite{hu2018squeeze}. 
Without loss in i.i.d. performance, they further simplify this architecture by removing activation functions altogether, replacing GELU with a \emph{simple gate} which performs element-wise product of feature maps, and replacing the channel attention by a \emph{simplified channel attention} without activation functions. 
The resulting network is referred to as a Nonlinear Activation-Free Network (NAFNet).
In contrast to \cite{chen2022simple} who focus on performance with clean inputs, we analyze the adversarial robustness of these networks, which also allows us to evaluate the effect of different activation functions and attention mechanisms on the robustness of restoration transformers.
In Figure \ref{fig:teaser}, we observe that NAFNet has significantly different artifacts in the reconstructed images compared to Restormer and the Baseline network.
One might simply hypothesize that these strange artifacts which appear to be the cumulative effect of aliasing and color mixing are due to the use of `Simple Gate' in place of a non-linear activation function like GELU. 
To confirm this hypothesis we additionally consider an \emph{Intermediate network}, from \cite{chen2022simple}.
In this \emph{Intermediate network} we replace the \emph{channel attention} in the baseline network with the \emph{simplified channel attention} but retain the GELU activation.
Additionally, to better understand the role of non-linear activation functions in this context, we consider an architecture the same as the \emph{Intermediate network} but with ReLU activations instead of GELU.
In Figure~\ref{fig:arch_blocks}, we modify the visualization by \cite{chen2022simple}, to present the repeating blocks of all the considered architectures in our work.

\subsection{Defenses}
As discussed in Section~\ref{sec:intro}, we observe in Figure~\ref{fig:teaser} that all considered architectures are vulnerable to adversarial attacks.
Prior work \cite{fgsm, pgd, gu2022segpgd} has shown that adversarial training is an effective defense against adversarial attacks.
Thus we use adversarial training as a defense strategy.
\paragraph{Adversarial Training. } We use the FGSM attack as proposed by \cite{fgsm} to generate adversarial samples during training.
Adversarial training can be hypothesized as a min-max problem, where we try to find perturbations for the samples such that the loss is maximized while training the network on these samples to minimize the loss of the model over training iterations. 
PGD attack is essentially a multi-step extension of FGSM attack, and thus the loss that FGSM attack attempts to maximize remains the same.
Additionally, the attack step of FGSM is also the same as described in Section~\ref{subsec:method:attack}, with one notable difference being that in the case of an FGSM attack, the attack step size $\alpha$ is equal to the permissible perturbation size of $\epsilon$.

While training, to avoid overfitting to adversarial samples, and enable the model to make reasonable reconstructions on unperturbed samples we use the training regime similar to \cite{gu2022segpgd} and use only 50\% of the sample in the training batch to generate perturbed adversarial samples and use the other 50\% samples unperturbed.
Thus, the effective learning objective is as described by Equation~\ref{eq:adv_train}.
\begin{equation}
 \underset{\theta}{\mathrm{minimize}}~\sum_i L(\net(\mathbf{y}^{\mathrm{clean}_i}) ,~ \mathbf{x}_i)+ \sum_j L(\net(\mathbf{y}^{\mathrm{adv}_j}) ,~ \mathbf{x}_j)
\label{eq:adv_train}
 \end{equation}
where the indices $i$ and $j$ correspond to the examples from the clean and adversarial batch splits, and  FGSM adversarial examples are generated as:
\begin{equation}
\mathbf{y}^{\mathrm{adv}_{j}} = \phi^{r}(\mathbf{y}^{\mathrm{clean}_j}+ \phi^{\epsilon}( \epsilon \cdot \mathrm{sign}\nabla_{\mathbf{y}_j}L(\net(\mathbf{y}_j), \mathbf{x}_j)))
\label{eq:fgsm}
\end{equation}

