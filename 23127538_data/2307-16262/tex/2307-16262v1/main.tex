\documentclass[times,twocolumn,preprint, longtitle]{elsarticle}
\usepackage{medima}
\usepackage{framed,multirow}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{footnote}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{array}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{subcaption}
\makesavenoteenv{tabular}
\usepackage{caption, subcaption}
\usepackage[figuresleft]{rotating}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage[acronym]{glossaries}
\makeglossaries
\acrodef{GI}{Gastrointestinal}
\acrodef{CRC}{Colorectal Cancer}
\acrodef{CADx}{Computer-Aided Diagnosis}
\acrodef{CNN}{Convolutional Neural Network}
\acrodef{DNN}{Deep Neural Network}
\acrodef{DL}{Deep Learning}
\acrodef{ML}{Machine Learning}
\acrodef{RNN}{Residual Neural Network}
\acrodef{GAN}{Genearative Adversarial Network}
\acrodef{Medico}{Multimedia  for  Medicine  Task}
\acrodef{AI}{Artificial Intelligence}
\acrodef{FPS}{Frame Per Second}
\acrodef{CGAN}{Conditional Generative Adversarial Network}
\acrodef{CADe}{Computer Aided Detection}
\acrodef{BLI}{Blue laser imaging}
\acrodef{COD}{Camouflaged Object Detection}
\acrodef{GRU}{Gated Recurrent Neural Network}
\acrodef{FICE}{Flexible spectral Imaging Color Enhancement}
\acrodef{DSC}{Dice coefficient}
\acrodef{FPS}{Frames per second}
\acrodef{mIoU}{mean Intersection over union}
\acrodef{SOTA}{state-of-the-art}
\definecolor{newcolor}{rgb}{.8,.349,.1}

\journal{Medical Image Analysis}

\begin{document}
%\verso{D. Jha \textit{et~al.}}
\begin{frontmatter}
%\title{Comparative validation of polyp and instrument segmentation methods in colonoscopy: results from Medico 2020 polyp segmentation and  MedAI 2021 transparency challenge}

\title{An objective validation of polyp and instrument segmentation  methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges}

\author[1]{Debesh Jha}
\cortext[cor1]{Corresponding author}
\ead{debesh.jha@northwestern.edu}
\author[4]{Vanshali Sharma}
\author[5]{Debapriya Banik}
\author[7]{Debayan Bhattacharya}
\author[5]{Kaushiki Roy}
\author[2]{Steven A. Hicks}
\author[1]{Nikhil Kumar Tomar}
\author[2]{Vajira Thambawita}
\author[11]{Adrian Krenzer}
\author[17]{Ge-Peng Ji}
\author[15]{Sahadev Poudel}
\author[19]{George Batchkala}
\author[26]{Saruar Alam}
\author[9]{Awadelrahman M. A. Ahmed}
\author[21]{Quoc-Huy Trinh}
\author[20]{Zeshan Khan}
\author[12]{Tien-Phat Nguyen}
\author[23]{Shruti Shrestha}
\author[24]{Sabari Nathan}
\author[25]{Jeonghwan Gwak}
\author[1]{Ritika K. Jha}
\author[1]{Zheyuan Zhang}
\author[7]{Alexander Schlaefer}
\author[5]{Debotosh Bhattacharjee}
\author[4]{M.K. Bhuyan}
\author[4]{Pradip K. Das}
\author[21]{Sravanthi Parsa}
\author[16]{Sharib Ali}
\author[2,3]{Michael A. Riegler}
\author[2,3,@]{P{\aa}l Halvorsen}
\author[1,@]{Ulas Bagci}
\author[27,@]{Thomas De Lange}

\address[1]{Machine \& Hybrid Intelligence Lab, Department of Radiology, Northwestern University, Chicago, USA}
\address[2]{Department of Holistic System, SimulaMet, Oslo, Norway}
\address[3]{Oslo Metropolitan University, Oslo, Norway}
\address[5]{Jadavpur University, Kolkata, India}
\address[4]{Indian Institute of Technology, Guwahati, India}
\address[7]{Institute of Medical Technology and Intelligent Systems, Technische Universität Hamburg, Germany}
\address[8]{Jaeyong Kang, Information Systems Technology and Design, Singapore University of Technology and Design, Singapore}
\address[9]{University of Oslo, Norway}
\address[11]{Julius-Maximilian University of Würzburg, Germany}
\address[12]{Faculty of Information Technology, University of Science, VNU-HCM, Vietnam}
\address[13]{Vietnam National University, Ho Chi Minh City, Vietnam}
\address[14]{Department of Colorectal Surgery, the Second Affiliated Hospital of Zhejiang University School of Medicine, Zhejiang, China} 
\address[15]{Department of IT Convergence Engineering, Gachon University, Seongnam 13120, South Korea}
\address[16]{School of Computing, University of Leeds, LS2 9JT, Leeds, United Kingdom}
\address[17]{Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE}
\address[18]{School of Computer Science, Wuhan University, Hubei, China}
\address[19]{Department of Engineering Science, University of Oxford, Oxford, UK}
\address[20] {National University of Computer and Emerging Sciences, Karachi Campus, Pakistan} 
\address[21]{Swedish Medical Center, Seattle, USA}
\address[22] {Vietnam National University, Ho Chi Minh City, Vietnam}
\address[23] {NepAL Applied Mathematics and Informatics Institute for Research (NAAMII), Kathmandu, Nepal} 
\address[24] {Couger Inc, Tokyo, Japan}
\address[25] {Department of Software, Korea National University of Transportation, Chungju-si, South Korea}
\address[26]{University of Bergen, Bergen, Norway} 
\address[27]{Sahlgrenska University Hospital, Sweden} 
\address[@]{joint senior authors}


\received{x xxxx xxxx}
\finalform{x xxxx xxxx}
\accepted{x xxxx xxxx}
\availableonline{x xxxx xxxx}
\communicated{xxxx xxxx}

\begin{abstract}
Automatic analysis of colonoscopy images has been an active field of research motivated by the importance of early detection of precancerous polyps. However, detecting polyps during the live examination can be challenging due to various factors such as variation of skills and experience among the endoscopists,  lack of attentiveness, and fatigue leading to a high polyp miss-rate. Therefore, there is a need for an automated system that can flag missed polyps during the examination and improve patient care. Deep learning has emerged as a promising solution to this challenge as it can assist endoscopists in detecting and classifying overlooked polyps and abnormalities in real time, improving the accuracy of diagnosis and enhancing treatment. In addition to the algorithm's accuracy, transparency and interpretability are crucial to explaining the whys and hows of the algorithm's prediction. Further, conclusions based on incorrect decisions may be fatal, especially in medicine. Despite these pitfalls,  most algorithms are developed in private data, closed source, or proprietary software, and methods lack reproducibility. Therefore, to promote the development of efficient and transparent methods, we have organized the \textit{{``Medico automatic polyp segmentation (Medico 2020)''}} and \textit{``MedAI: Transparency in Medical Image Segmentation (MedAI 2021)"} competitions. The Medico 2020 challenge pulled submissions from 17 different teams, while the MedAI 2021 challenge gathered submissions from 17  teams. We present a comprehensive summary and analyze each contribution, highlight the strength of the best-performing methods, and discuss the possibility of clinical translations of such methods into the clinic. Our analysis revealed that the participants improved dice coefficient metrics from 0.8607 in 2020 to 0.8993 in 2021 despite adding diverse and challenging frames (containing irregular, smaller, sessile, or flat polyps), which are frequently missed during a routine clinical examination. For the instrument segmentation task, the best team obtained a mean Intersection over union metric of 0.9364. For the transparency task, a multi-disciplinary team, including expert gastroenterologists, accessed each submission and evaluated the team based on open-source practices, failure case analysis, ablation studies, usability and understandability of evaluations to gain a deeper understanding of the models’ credibility for clinical deployment. The best team obtained a final transparency score of 21 out of 28. Through the comprehensive analysis of the challenge, we not only highlight the advancements in polyp and surgical instrument segmentation but also encourage qualitative evaluation for building more transparent and understandable AI-based colonoscopy systems. Moreover, we discuss the need for multi-center and out-of-distribution testing to address the current limitations of the methods with the ultimate goal of reducing the cancer burden and improving patient care.  


\end{abstract}

\begin{keyword}
\vspace{-15pt}
\KWD Colonoscopy  \sep polyp segmentation  \sep Transparency \sep polyp challenge \sep computer-aided diagnosis  \sep medicine
%\sep instrument segmentation 
 %\sep Efficient processing
\end{keyword}
\end{frontmatter}


\section{Introduction}
\label{section:introduction}
Cancer is a significant global health problem worldwide and is the second most common cause of mortality in the United States. According to the recent 2023 estimates, there will be approximately  1,958,310 new cancer incidences and 609,820 cancer deaths in the United States~\citep{siegel2023cancer}. Among various types of cancer, the highest number of deaths occur from lung, prostate, and colorectum in men and lung, breast, and colorectum cancer in women. As colorectal cancer is prevalent among both men and women, it is the second leading cause of cancer related death overall. One of the key indicators of colon cancer is the development of polyps in the colon and rectum. The 5-year survival rate for colon cancer stands at 68\%, and 44\% for stomach cancer~\citep{asplund2018survival}. If these polyps are detected and removed early, it can significantly increase the chance of survival. In fact, studies have shown that early detection and removal of polyps can increase survival rates to as high as 100\%~\citep{levin2008screening}. Thus, regular screening is crucial for early detection of these polyps, as it allows for earlier diagnosis and prompt treatment. 


Endoscopic procedures, such as colonoscopy, are considered the gold standard for detecting and treating GI abnormalities (such as polyps) and cancer~\citep{moriyama2015advanced}. However, manual screening for polyps is susceptible to error and is also time-consuming. That's why there has been a push to develop \ac{CADe} and \ac{CADx} systems that can be integrated into the clinical workflow~\citep{riegler2016eir} and potentially contribute to the prevention of colorectal cancer. In the past, traditional machine learning-based \ac{CADx} systems~\citep{inproceedings2,inproceedings3} were popular.  With the recent advancement in the hardware capabilities, such as powerful GPUs and the emergence of deep learning~\citep{article}, the research has shifted towards deep learning-based \ac{CADx} systems~\citep{fan2020pranet,jha2019resunet++}. These algorithms have shown superior performance compared to traditional \ac{CADx} solutions.


% Figure environment removed

However, despite their superior performance, deep learning-based \ac{CADx} systems are still considered a ``black box'', meaning their inner workings are not fully understood or there is a lack of transparency in understanding the predictions made by the model. Because of the complexity of multiple layers and interconnected nodes in the convolutional neural network, it is challenging to interpret the decision or understand the features contributing to the outcome. For these systems to be widely adopted in clinical settings, they must be rigorously evaluated on benchmark datasets. They must demonstrate the ability to handle patient and recording device variability, provide explainability and robustness and process data in real-time. Only by carefully evaluating these systems, we can ensure the reliability and effectiveness of detecting and diagnosing cancer and its precursors (such as polyps) in a clinical setting.


In this paper, we present a comprehensive analysis of the results of the two prominent challenges in the field of automatic polyp segmentation, namely, \textit{{``Medico automatic polyp segmentation (Medico 2020)~\footnote{\url{https://multimediaeval.github.io/editions/2020/tasks/medico/}}''}} challenge and the \textit{``MedAI: Transparency in Medical Image Segmentation (MedAI 2021)''~\footnote{\url{https://www.nora.ai/competition/image-segmentation.html}}} challenge. These challenges were designed to foster the development of \ac{CADx} solutions on the same datasets, with a focus on transparency, explainability, robustness, speed, and generalization, aiming to evaluate the relevance of such algorithms in clinical workflows. The challenges provided  posed four distinct tasks:

% TODO: A critical part that is missing is why instrument segmentation - reviewers will pick on that! No motivation provided for this part of the challenge MedAI2021

\begin{itemize}
    \item Accurate polyp segmentation task to develop state-of-the-art algorithms for early detection and treatment of colon cancer (Medico 2020, MedAI 2021).
    \item Algorithm efficiency task to develop methods with the least frames-per-second (FPS) on predetermined hardware (Medico 2020).
    \item Surgical instruments segmentation task to enable tracking and localization of essential tools in endoscopy and help to improve targeted biopsies and surgeries in complex GI tract organs (MedAI 2021).
    \item Transparency task to evaluate the proposed system from a transparency point of view (for example, explanations of the training procedure, amount of data used and model's predictions interpretation) (MedAI 2021).
\end{itemize}

These tasks were focused on the development of \ac{SOTA} algorithms for polyp, instrument and medical image segmentation in a variety of settings, including performance evaluation, resource utilization (efficiency), and transparency. By analyzing the results of these challenges, we can better understand the field's current state, identify the strength and weaknesses of different methods and find the most effective method for our problem.  It is also useful to identify the research gap and areas for future innovation in the field of polyp, instrument and medical image segmentation. Figure~\ref{GIChallengeoverviewfigure} provides an overview of both challenges along with the total number of images used for training and testing in each task. Ground truth samples with their corresponding original images are also presented for the segmentation tasks. In addition, task-specific metrics are presented (for example, FPS for ``Algorithm efficiency''). 

In short, the main contributions are the following: (i) We present a comprehensive and detailed analysis of all participant results; (ii) we provide an overview and comparative analysis of the developed methods; (iii) we obtain and discuss new insights into the current state of AI in the field of GI including open challenges and future directions; and (iv) finally, we provide a detailed discussion of issues such as generalizability issues, multi-center and out-of-distribution testing in context to current limitations of computer-aided diagnosis systems.

%NOTE: CADe and CADx are just detection and diagnostic systems where you have to identify benign or malignant lesion - here your challenge is not identification or characterisation - its  a computer-aided polyp segmentation! You could use CADx but some reviewers might oppose it especially if they are from clinical background - you could write CADx system for polyp segmentation




\section{Challenge description}
%Here, we briefly describe both the challenge and tasks. 
\subsection{{Medico 2020 Automatic Polyp Segmentation Challenge}}
The \textit{``Medico Automatic Polyp Segmentation challenge "} was an international benchmarking challenge hosted through the MediaEval platform. This challenge aimed to benchmark automated polyp segmentation algorithms using the same dataset and to develop methods that can detect difficult-to-detect polyps (such as flat, sessile, and small or diminutive polyps). Researchers from medical image analysis, machine learning, multimedia, and computer vision were invited to submit their results for this challenge, which included two tasks. 

\subsubsection{Task Description} 

The participants were invited to submit their solutions for the following tasks. 

\paragraph{\textbf{a) Automatic Polyp Segmentation Task}:} 
In this task, the participants were asked to develop innovative algorithms for segmenting polyps in colonoscopic images. The focus was on developing efficient systems that could accurately segment the maximum polyp area in a frame while being fast enough for practical use in a clinical setting. This task addresses the need for robust \ac{CADx} solutions for colonoscopy.

To participate in the challenge, participants were required to train their segmentation models on an available training dataset. Once the test dataset was released, participants could test their models and submit their predicted segmentation maps to the organizers in a zip file with the name of each segmentation map image matching the colonoscopy image in the test dataset. 

\textbf{b) Algorithmic Efficiency Task}: 

CADx systems for polyp segmentation that operate in real-time can provide valuable feedback to clinicians during colonoscopy screenings, potentially reducing the risk of missing polyps. However, real-time deep learning-based CADx solutions often have fewer parameters and may therefore have lower segmentation accuracy compared to more computationally intensive CADx solutions. In order to address this trade-off between accuracy and speed, the efficiency task of the challenge was designed to encourage the development of lightweight segmentation models that are both accurate and fast.

To participate in this task, participants were asked to submit docker images of their proposed algorithms. These algorithms were then evaluated on a dedicated Nvidia GeForce GTX 1080 graphics card, and the results were used to rank the teams. A \acf{mIoU} threshold was set for considering a solution to be a valid efficient segmentation solution, and teams were ranked according to their \ac{FPS}. By focusing on developing efficient CAD solutions, this task aimed to foster the creation of real-time systems that can provide valuable feedback to clinicians while maintaining high accuracy. A detailed description of the challenge, tasks, and evaluation metrics can be found in~\citep{jha2020medico}. 

\subsection{\textbf{MedAI: Transparency in Medical Image Segmentation Challenge}}

MedAI: Transparency in Medical Image Segmentation challenge (MedAI 2021) was held for the first time at the Nordic AI Meet~\footnote{\url{https://nordicaimeet.com}} 2021 that focused on medical image segmentation and transparency in \ac{ML} based \ac{CADx} systems. This challenge proposed three tasks to address specific GI image segmentation challenges, including two separate segmentation scenarios and one scenario on transparent ML systems. The latter task emphasized the need for explainable and interpretable \ac{ML} algorithms in the field of medical image analysis.

To participate in this challenge, participants were provided with a training dataset to use for training their \ac{ML} models. These models were then tested on a concealed test dataset, allowing participants to evaluate their performance. The focus on transparency underscores the importance of developing \ac{ML} algorithms that provide not only accurate and efficient results but also provide interpretable and explainable predictions. By addressing these specific challenges, this challenge aimed to foster the development of innovative and effective \ac{CADx} solutions for GI endoscopy.

%(e.g., availability of the code, reporting of the training procedure and amount of data,  use of explainable AI methods, etc.).

\subsubsection{Task Description}

We present three tasks: the polyp segmentation task, the instrument segmentation task, and the transparency task. Each task targets a different requirement within automatic findings segmentation in \ac{GI} image analysis. 

\textbf{a) Automatic Polyp Segmentation Task}: In this task, participants were invited to submit segmentation masks of polyps from colonoscopic images of the GI tract. They were provided with a training dataset to develop their models, and a hidden test dataset was later released to them without the ground truth segmentation masks. Participants were required to submit a zip file containing their predicted masks in the same resolution as the input images, with the filenames of each mask matching the corresponding input image and using the ``.png" file format. The objective of this task was similar to Medico 2020. By using a hidden test dataset, the results of this task were reliable and provided a valuable benchmark for the field.


\textbf{b) Automatic Instrument Segmentation Task}: The instrument segmentation task required the development of algorithms that could generate segmentation masks for GI instruments during live endoscopy procedures. This task aimed to create segmentation models that enable tracking and localization of essential tools in endoscopy that could aid surgeons during surgeries (such as polypectomies) by providing a precise and dense map of the instrument. Like the polyp segmentation task, participants were given a training dataset to develop their models. The submission procedure for this task was similar to that of the polyp segmentation task, with participants required to submit zip files containing their predicted masks in the same resolution as the input images and with filenames matching the corresponding input images. %By focusing on the segmentation of instruments during live endoscopy procedures, this task aimed to encourage the development of innovative solutions that could enhance the accuracy and efficiency of these procedures. 

% Figure environment removed

\textbf{c) Transparency Task}:
The transparency task focused on the importance of transparent research in medical artificial intelligence (AI). The main goal of this task was to evaluate systems from a transparency perspective, which included detailing the training procedures of the algorithms, the dataset used for training, the interpretation of the model's predictions, the use of explainable AI methods, etc. To participate in this task, researchers were encouraged to perform ablation studies, conduct a thorough failure analysis of their proposed algorithms, and share their code in a GitHub repository with clear steps for reproducing the results. In addition, participants were required to submit a one-page document summarizing their findings from the transparency task. By promoting transparency in AI research, this task aimed to foster the development of reliable, interpretable, and trustworthy algorithms for use in medical image segmentation. A detailed description of the challenge, tasks, and evaluation metrics can also be found in~\citep{medaireview}. 

\section{Related Work}
Polyp detection and segmentation using \ac{ML} has been an active field of research for over a decade but have been previously limited by hand-crafted features ~\citep{bernal2012towards, hwang2007polyp,Shin.2018}. Previous methods had limitations in sub-optimal performance, poor generalization to unseen images, and complexity that limited real-world applicability. However, in the recent 5-6 years with the success of Convolutional Neural Networks (CNNs), polyp segmentation task has seen tremendous performance boost, including the winning model in the MICCAI challenge~\citep{bernal2017comparative}. The widespread use of CNNs, particularly the U-Net ~\citep{ronneberger2015u} and its variants have been successfully applied on several polyp segmentation datasets and discussed in challenge reports. In addition, recent advances in CNN architectures for polyp segmentation have focused on improving convolution operations~\citep{alam2020automatic}, adding attention blocks~\citep{jha2019resunet++, oktay2018attention}, incorporating feature aggregation blocks(~\citep{mahmud2021polypsegnet}) and using self-supervised learning techniques~\citep{bhattacharya2021self}. These modifications and learning strategies have proven effective in improving the accuracy and reliability of polyp segmentation using CNNs.

Apart from the contributions of individual research groups, several challenges~\citep{bernal2017comparative,ali2021deep} have been organized to improve the detection and classification of GI abnormalities from either single image frames or videos. These challenges have limitations such as small datasets~\citep{ali2020endoscopy} or datasets that are not used consistently across different challenges (for example, EndoVis2015 challenge on Early Barrett's cancer detection\footnote{\url{https://endovissub-barrett.grand-challenge.org}}). Additionally, the algorithms proposed in these challenges are often not publicly available, making it difficult to reproduce and build upon them. Hence, there is a need for benchmarking datasets and for making the algorithms proposed in these challenges reproducible to facilitate progress in this field.

\input{tables/Table_overview_challenges.tex}

Table~\ref{tab:challengeoverview} provides an overview of GI challenges held in the past eight years, including the imaging modalities used.  Several challenges have been organized in the past eight years to compare and improve computer vision classification methods and benchmark GI datasets. In 2015, Bernal et al.~\citep{bernal2017comparative} organized the ``Automatic Polyp Detection in colonoscopy videos'' challenge. Likewise, they organized the GIANA challenge in 2017 and 2018\footnote{\url{https://giana.grand-challenge.org/}} focused on colonoscopy data and included tasks such as detection of lesions in WCE, polyp detection, and polyp segmentation. The EndoCV2020 challenge\footnote{\url{https://endocv.grand-challenge.org}} included a sub-challenge on ``Endoscopy disease detection (EDD2020)" with multi-organ and multi-modal endoscopy data, but only 386 annotated frames and 5 class categories were included. Recent challenges attempted to address generalisability in polyp detection and segmentation~\citep{ali2022assessing} with both single frames and sequence colonoscopy datasets. They demonstrated how variability in images can affect algorithm performances. Altogether, these challenges have led to many algorithmic innovations in the detection and classification of GI abnormalities.



% Note/Comment: Most reviewers knows endocv2021 - they may think the information is not upto date - I recommend using all information and develop from there - truth to be told but see the brighter side of what is done here!

%Overall, data science challenges in this field of research have been useful in benchmarking and generating datasets which is encouraging and positive in the development of innovative algorithms for detecting abnormalities in GI endoscopy. However, there is still a need for more comprehensive benchmarking of various datasets in GI endoscopy under several task categories that have not been addressed in other challenges. Furthermore, most of these datasets are not publicly available, making it difficult to reproduce results and develop new algorithms based on them. 



Additionally, past challenges have not emphasized on the explainability and reliability of deep learning model predictions. Most challenges also do not focus on open source codes for research and development making it difficult for proposed algorithms to be adopted in clinical settings due to a lack of transparency. Moreover, the reported methods are not reproducible which hinders further algorithmic advancement. Thus, we lose track of what are best practices and where we are heading in this field. Through our challenges in Medico 2020 and MedAI 2021, we address reproducibility and open science which are the two most important aspects that can enable experienced and new ML scientists to build upon and advance the field.

Medico 2020~\citep{jha2020medico} focused on promoting new algorithmic innovations in polyp segmentation and assessing algorithmic efficiency, while MedAI 2021 ~\citep{medaireview} emphasizes innovations in both polyp segmentation and instrument segmentation. In addition, the MedAI 2021 challenge also introduces a transparency task that encourages and validates reproducible research and a focus on the explainability of model predictions. Through these two challenges, we aimed to address some of the key challenges in GI endoscopy, including benchmarking of datasets, reproducibility of algorithms, and explainability of model predictions. In this paper, we comprehensively analyze the outcomes of both challenges. 

\section{Challenge datasets and evaluation metrics}

% Figure environment removed

\subsection{Medico 2020 dataset}
The dataset contains 1,000 polyp images and their corresponding ground truth mask taken from Kvasir-SEG~\citep{jha2020kvasir}. The datasets were collected from real routine clinical examinations at Vestre Viken Health Trust (VV) in Norway by expert gastroenterologists. The VV is the collaboration of the four hospitals that provide healthcare services to 470,000 people. The resolution of images varies from $332\times487$ to $1920\times1072$ pixels. Some of the images contain green thumbnails in the lower-left corner of the images showing the position marking from the ScopeGuide (Olympus). The training dataset can be downloaded from \footnote{\url{https://datasets.simula.no/kvasir-seg/}}. The test dataset contains unique polyp dataset from the same distribution (collected from the same center).  It can be downloaded from \footnote{\url{https://drive.google.com/file/d/1uP2W2g0iCCS3T6Cf7TPmNdSX4gayOrv2}}. Some samples are shown in Figure~\ref{fig:polyp20_21}.


\subsection{MedAI Transparency challenge 2021 dataset}
For the MedAI transparency challenge as well, we use Kvasir-SEG~\citep{jha2020kvasir} as the training dataset. The development dataset for the polyp segmentation task can be downloaded from \footnote{\url{https://datasets.simula.no/kvasir-seg/}} whereas the development dataset for the instrument segmentation task can be downloaded from \footnote{\url{https://datasets.simula.no/kvasir-instrument/}}. Some sample images are shown in Figure~\ref{fig:polyp20_21} and Figure~\ref{fig:inst21}. Figure~\ref{fig:datadistributuion} shows the data distribution of the train and test datasets used in Medico 2020 and MedAI 2021. We have categorized the datasets into ``small", ``medium'' and ``large'' according to the size of regions of interest and plotted the height versus width of each data point. This is to visualize the dimension of each data point and observe the diversity and complexity of the dataset used in the study. The information about the size categories and the dataset's dimensions are crucial for assessing the performance, robustness, and generalizability of the proposed algorithms. 


%\section{Evaluation Metrics}
\subsection{Metrics for polyp and instrument segmentation tasks}
We used \ac{mIoU} as an evaluation metric for the polyp and instrument segmentation tasks. If the teams had the same mIoU values, they are further evaluated based on the higher value of the \ac{DSC}. We also recommend calculating other standard evaluation metrics such as Precision (Pre), pixel accuracy (Acc.), Recall, (Rec), F2-Score (F2), and \ac{FPS} for a comprehensive evaluation. 


\subsection{Metrics for efficiency tasks}
Efficiency is essential during colonoscopy as it directly impacts the models' feasibility and practicality in real-world scenarios. For example, in a clinical setting, endoscopists may need to analyze a large number of frames in real-time during routine colonoscopy and lag in the analysis could lead to suboptimal results. Therefore, we strongly recommend calculating processing speed in terms of \ac{FPS} as an evaluation metric for the polyp segmentation tasks.

\subsection{Metrics for transparency tasks}
The transparency task aims to assess the transparency and understandability of algorithms for medical AI by utilizing a qualitative approach in the evaluation metrics. The evaluation team, comprising multiple experts from diverse fields evaluated the submissions based on various attributes such as the availability of code, the depth of evaluation, reproducibility, and the implementation of explainable AI techniques. In addition, participants are expected to provide detailed information about their solution, including rigorous failure analysis, thorough ablation studies, and a comprehensive GitHub repository with clear reproducibility steps. The transparency evaluation is divided into three categories: open source, model evaluation, and clinical evaluation, with corresponding scores outlined in Table~\ref{tab:transscore}. Ultimately, this task aims to promote the development of more transparent and interpretable medical AI systems.



\section{Participating Research Teams}
\subsection{Methods used in Medico 2020}
In Table~\ref{table:challenge_summary_medico} we have provided summary of all the teams who participated in ``Medico 2020” challenge. It can be seen from Table~\ref{table:challenge_summary_medico}  that most of the teams participated in only one subtask (Task 1) whereas 9 teams participated in both Task 1 and 2 of the challenge.


\begin{table} [!t]
\caption{Summary information of participating teams in Medico 2020. Here, `$\surd$'  = Team participated, `--' = No participation, \textbf{Task 1 =} Polyp segmentation task and \textbf{Task 2 =} Algorithm efficiency task}
\label{table:challenge_summary_medico}
\centering
\begin{tabular}{p{0.7cm}|p{3.5cm}|p{1cm}p{1cm}}
\toprule
\textbf{Chal.} & \centering \textbf{Team Name} & \textbf{Task 1} &\textbf{Task 2}\\
\midrule

\multirow{17}{*}{\rotatebox{90}{\textbf{Medico 2020}}}
              & FAST-NU-DS &$\surd$  &  $\surd$   \\ %\hline
              & AI-TCE & $\surd$  & -- \\ %\hline
              & ML-MMIVSARUAR & $\surd$ &--    \\ %\hline
              & UiO-Zero  & $\surd$ &--\\ %\hline
              & HBKU\_UNITN\_SIMULA & $\surd$ &--  \\ %\hline
              & AI-JMU & $\surd$ &$\surd$  \\ %\hline
              & SBS & $\surd$ & $\surd$  \\ %\hline
              & AMI Lab & $\surd$ &$\surd$ \\ %\hline
              & UNITRK & $\surd$& $\surd$  \\ %\hline
              & MedSeg\_JU & $\surd$ &-- \\ %\hline
              & IIAI-Med & $\surd$ &-- \\ %\hline
              & HGV-HCMUS & $\surd$ & $\surd$   \\ %\hline
              & GeorgeBatch & $\surd$& $\surd$ \\ 
              & PRML2020GU & $\surd$ & $\surd$  \\ %\hline
              & VT & $\surd$ &  -- \\ %\hline
              & IRIS-NSYSU & $\surd$ & -- \\ %\hline
              & NKT & $\surd$& $\surd$  \\ %\hline
              \bottomrule
\end{tabular}
\end{table}


\textbf{FAST-NU-DS:} 
Team FAST-NU-DS~\citep{ali2020depth} explored the advantage of using depth-wise separable convolution in the atrous convolution of the ResUNet++\citep{jha2019resunet++} architecture. Modifications were made to get the lightweight image segmentation. Deep atrous spatial pyramid pooling (ASPP) was also implemented on the ResUNet++ architecture. The purpose of this architectural design was to provide good performance on the image segmentation evaluation metrics as well as in terms of inference time. The implementation of depth-wise separable convolution resulted in less number of parameters and giga-floating point operations (GFLOPs). To get the lightweight model architecture the convolution layer in the atrous bridge was replaced with depth-wise separable convolution and the atrous bridge was also replaced with a deep atrous bridge.  The comparison of modification in model architecture was made against UNet~\citep{ronneberger2015u} and ResUNet++. All models were trained on custom mean intersection over union loss.


\textbf{AI-TCE:} 
Team AI-TCE~\citep{nathan2020efficient} proposed an efficient supervision network that uses EfficientNet~\citep{tan2019efficientnet} and an attention Unit. The proposed network had the properties of an encoder-decoder structure with supervision layers. An EfficientNet-B4 was used as a pre-trained architecture in the encoder block. The decoder block combined dense block and Concurrent Spatial and Channel Attention (CSCA) block. Both the encoder and decoder were connected by Convolution Block Attention Module (CBAM). All the outputs of the decoder layer were supervised, i.e., individual decoder output was taken and upsampled with the output layer and supervised by the loss function. Also, all upsampled outputs were concatenated and fed into CBAM. In the upsampling, the convolution transpose layer was used. 


\textbf{ML-MMIV SARUAR}: Team ML-MMIV SARUAR~\citep{alam2020automatic} used the U-Net with pre-trained ResNet50 on the ImageNet dataset as the encoder for the polyp segmentation task. The use of a pre-trained encoder helped the model to converge easily. The input image was fed into the pre-trained ResNet50 encoder, consisting of a series of residual blocks as their main component. These residual blocks helped the encoder extract the important features from the input image, which were then passed to the decoder. Skip connections between the encoder and decoder branch help the model to get all the low-level semantic information from the encoder, which allowed the decoder to generate the desired feature maps.


\textbf{UiO-Zero}: Team UiO-Zero~\citep{ahmed2020generative}  used the generative adversarial networks framework for solving the automatic segmentation problem. Perceiving the problem as an image-to-image translation task, conditional generative adversarial networks were utilized to generate masks conditioned by the images as inputs. The polyp segmentation GAN-based model consists of two networks, namely a generator and discriminator, that were based on convolution neural networks. A generator takes the images as input and tries to produce realistic-looking masks conditioned by this input and a discriminator, which was basically a classifier that had access to the ground truth masks and tried to classify whether the generated masks was real or not. To stabilize the training, the images were concatenated with the masks (generated or real) before being fed to the discriminator.


\textbf{HBKU\_UNITN\_SIMULA}: {HBKU\_UNITN\_SIMULA}~\citep{trinh2020hcmus} team proposed methods combining the Residual module, Inception module, Adaptive CNN with U-Net model, and PraNet for semantic segmentation of various types of polyps in endoscopic images. The team submitted five different runs considering five different solutions. In the first approach, a simple U-Net architecture was adopted to parse masks of polyps. Second, the regular ReLU was replaced with Leaky ReLU to deal with dead neurons. Third, to further boost the result, an Inception module was introduced to extract better features. Fourth, a pre-trained model with the Resnet50 backbone was used to build ResUNet, yielding better obtained results. Last, PraNet was employed for polyp segmentation in colonoscopy images.

\textbf{AI-JMU:} Team AI-JMU~\citep{krenzer2020bigger} explored various image segmentation models, specifically the Cascade Mask R-CNN and Mask R-CNN with ResNet as well as the ResNeSt architectures was used as the backbone. Additionally, the team investigated the effect of varying the depth of both the ResNet and ResNeSt architectures. Depths of 50, 101, and 200 were evaluated for the ResNeSt model, and depths of 50 and 101 for the ResNet model.

\textbf{SBS}: Team SBS~\citep{shrestha2020ensemble} exploited ResNet 34~\citep{he2016deep} and EfficientNet-B2~\citep{tan2019efficientnet} backbones in the U-Net~\citep{ronneberger2015u}. The team introduced two different models: Single Model and Ensemble Model. The ResNet-34 was used in the single model. The weights saved after the training phase was loaded in the network, and test data were fed to get the predicted polyp masks. However, in the case of the ensemble model, both ResNet-34 and EfficientNetB2 were used to predict the masks. Then the individual prediction was ensembled using bitwise multiplication between the two predicted masks. The ensemble model provided better evaluation results as compared to the single model, as when multiple algorithms were ensembled predictive power increases and error rate decreases.

\textbf{AMI Lab:} Team AMI Lab~\citep{kang2020kd} utilized the knowledge distillation technique to improve ResUNet++~~\citep{jha2019resunet++}, which performs well on automatic polyp segmentation. First, the data augmentation module was used to generate augmented images for the input. Second, the obtained augmented images were fed to both the student model and the teacher model. Third, the distillation loss between the outputs of student and teacher models was calculated. Similarly, the loss between the output of the student model and the ground truth label was computed to train the student model.

\textbf{UNITRK:} Team UNITRK~\citep{khadka2020transfer} employed the UNet model pre-trained on the brain MRI dataset. The notion of knowledge transfer has been the key motivating factor to choose a simple pre-trained model. The model was fine-tuned with the polyp dataset. The fine-tuning of the pre-trained model helped to converge faster without the requirement of a large number of training examples. The additive soft attention mechanism was integrated with the pre-trained UNet architecture. The key benefit of this attention UNet structure in comparison to multi-stage CNNs was that it does not require training of multiple models to deal with object localization and thus reduces the number of model parameters. It helps to focus on relevant regions in the input images.


\textbf{MedSeg\_JU}: Team MedSeg\_JU~\citep{banik2020deep} proposed an approach for polyp segmentation based on deep conditional adversarial learning. The proposed framework consists of two interdependent modules: a generator network and a discriminator network. The generator was an encoder-decoder network responsible to predict the polyp mask while the discriminator enforces the segmentation to be as similar to the ground truth segmented mask.  The training process of the network alternates between training the generator and the discriminator, with the generator trained to produce a predicted synthetic mask by freezing the discriminator and the discriminator trained while freezing the generator. 


\textbf{IIAI-Med}: Team IIAI-Med team~\citep{ji2020automatic} presented a novel deep neural network, called the Parallel Reverse Attention Network (PraNet), for the task of automatic polyp segmentation at MediaEval 2020. The network first aggregated features in high-level layers using a parallel partial decoder (PPD). This combined feature was then used to generate a global map as the initial guidance area for the following components. Additionally, the network mines boundary cues using a reverse attention (RA) module which establishes the relationship between areas and boundary cues. Thanks to the recurrent cooperation mechanism between areas and boundaries, the PraNet was able to calibrate misaligned predictions, improving segmentation accuracy and achieving real-time efficiency (nearly 30fps). The code and results are available at https://github.com/GewelsJI/MediaEval2020-IIAI-Med.


\textbf{HGV-HCMUS:} Team HGV-HCMUS~\citep{nguyen2020hcmus} proposed two different approaches leveraging the advantages of either ResUNet++ or PraNet model to efficiently segment polyps in colonoscopy images, with modifications on the network structure, parameters, and training strategies to tackle various observed characteristics of the given dataset. For the first approach, PraNet was used, which is a parallel reverse attention network that helps to analyze and use the relationship between areas and boundary cues for accurate polyp segmentation. The PraNet with Training Signal Annealing strategy was used to improve segmentation accuracy and effectively train from scratch on the given small dataset.  For the second approach, ResUNet++ was used, which takes advantage of residual blocks, squeeze and excitation blocks, atrous spatial pyramid pooling, and attention blocks. The input path was modified and integrates a guided mask layer to the original structure for better segmentation accuracy. 


\textbf{GS-CDT}: Team GS-CDT~\citep{batchkala2020real}  used the standard U-Net architecture for the binary segmentation task, and experiments were conducted using the intersection-over-union loss (IoU loss) instead of the commonly used binary cross-entropy (BCE) loss. They also experiment with a combination of both losses in the training process. The motivation behind this approach was to strike a balance between accuracy and speed for using automated systems during colon cancer surveillance and surgical removal of polyps. This balance is considered while experimenting with other parameters like loss function and data augmentation to boost performance. The reported outcomes show that using IoU loss result in enhanced segmentation performance, with a nearly 3\% improvement on the \ac{DSC} metric while maintaining real-time performance (more than 200 FPS). The code and results are available at https://github.com/GeorgeBatch/kvasir-seg.


\textbf{PRML2020GU:} An overview of the approach proposed by team PRML2020GU \citep{poudel2020automatic} is shown in Figure~\ref{bestmethod2020}. The team employed an EfficientNetB3 as an encoder backbone with a U-Net decoder and leveraged the concept of U-Net++ of redesigning the skip connections to use multi-scale semantic details. The densely connected skip connections to the decoder side enable flexible multi-scale feature fusion both horizontally and vertically at the same resolution. Besides, the proposed method is powered by deep supervision, where all the outputs after deep supervision is averaged, and the final mask is generated. Further, channel-spatial attention enables significantly better performance and fast convergence. Moreover, integrating the channel and spatial attention modules restrains irrelevant features and allows only useful spatial details.
% Figure environment removed

%The model is trained for 35 epochs using the Adam optimizer with a learning rate of 1e-4 and a batch size of 12, and is implemented on a system with an Intel Xeon Processor, 128 GB RAM and NVIDIA Quadro P5000 GPU of 16GB.

\textbf{VT:} Team VT~\citep{thambawita2020pyramid} proposed a simple but efficient idea of using an augmentation method called pyramid focus-augmentation (PYRA) that uses grids in a pyramid-like manner (large to small) for polyp segmentation. The method has two main steps: data augmentation with PYRA using pre-defined grid sizes followed by training of a DL model with the resulting augmented data. PYRA can be used to improve the performance of segmentation tasks when there is a small dataset to train the DL models or if the number of positive findings is small. The method shows a large benefit in the medical diagnosis use case by focusing the clinician’s attention on regions with findings step-by-step.


\textbf{IRISNSYSU:} Team IRISNSYSU~\citep{maxwell2020temporal} proposed a local region model with attentive temporal-spatial pathways for automatically learning various target structures. The attentive spatial pathway highlights the salient region to generate bounding boxes and ignores irrelevant regions in an input image. The proposed attention mechanism allows efficient object localization, and the overall predictive performance is increased because there are fewer false positives for the object detection task for medical images with manual annotations.


\textbf{NKT:} Team NKT~\citep{tomar2021automatic} proposed a full convolution network following an encoder-decoder approach. It combines the strength of residual learning and the attention mechanism of the squeeze and excitation (SE) network. The encoding network consists of 4 encoder blocks with 32, 64, 128, and 256 filters. The decoding network also consists of 4 decoder blocks with 128, 64, 32, and 16 filters. Both the encoder and decoder block consists of a residual block as their core component. The residual block helps in building deep neural networks by solving the vanishing gradient and exploding gradient problem.

Additionally, in Table~\ref{table:challenge_summary2020}, we provide an elaborate summary of all the research teams who participated in the ``Medico 2020" challenge. It gives a detailed overview of the algorithms, backbone, loss function, data augmentation, and optimizer used by the different participating teams. 

\input{tables/Summary_Team_Contributions}
\begin{table} [!t]
\caption{Summary information of participating teams in MedAI 2021. Here, `$\surd$'  = Team participated, `--' = No participation,  \textbf{Task 1 =} Polyp segmentation task, \textbf{Task 2 =} Instrument segmentation task, and \textbf{Task 3 =} Transparency task.}
\label{table:challenge_summary_MedAI}
\centering
\begin{tabular}{p{0.7cm}|p{3.2cm}|p{1cm}p{1cm}p{1cm}}
\toprule
\textbf{Chal.} & \centering \textbf{Team Name} & \textbf{Task 1}  &\textbf{Task 2} &\textbf{Task 3}\\
\midrule
\multirow{16}{*}{\rotatebox{90}{\textbf{MedAI 2021}}}
              & The Segmentors & $\surd$   & $\surd$ &$\surd$\\ %\hline
              & The Arctic & $\surd$    & $\surd$ &$\surd$ \\ %\hline
              & mTEC & $\surd$ &$\surd$ &$\surd$ \\ %\hline
              & MedSeg\_JU  &-- &  $\surd$ &--\\ %\hline
              & MAHUNM & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & IIAI-CV\&Med & $\surd$   &--  &$\surd$  \\ %\hline
              & NYCity & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & PRML & $\surd$ &   $\surd$ &$\surd$ \\ %\hline
              & leen & $\surd$ &   $\surd$ &$\surd$ \\ %\hline
              & CV\&Med IIAI & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & Polypixel & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & agaldran & $\surd$ &   $\surd$ &$\surd$ \\ %\hline
              & TeamAIKitchen & $\surd$ &$\surd$  & $\surd$  \\ %\hline
              & CamAI& $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & OXGastroVision & $\surd$ &  $\surd$ &$\surd$ \\
              & Vyobotics & $\surd$  & -- & --\\ %\hline
              & NAAMII & $\surd$ &  $\surd$ & --\\ %\hline     
\bottomrule
\end{tabular}
\end{table}

\subsection{Methods used in MedAI 2021}
In this subsection, we briefly summarize the methods used by the participating teams in the MedAI 2021 challenge. The challenge has three subtasks: polyp segmentation task, instrument segmentation task, and transparency task. In Table~\ref{table:challenge_summary_MedAI}, we present the research teams who have participated in each of these three tasks. It can be seen from this table that most of the teams participated in all three tasks except for four teams which participated in either one or two of the sub-tasks. Most participating teams have used the same architecture in their submission for both subtasks. However, two teams, namely \textit{Vyobotics}~\citep{Rauniyar2021} and  \textit{MedSeg\_JU}~\citep{Banik2021} have participated in only one of the subtasks. The team \textit{Vyobotics}~\citep{Rauniyar2021} has participated in the polyp segmentation task whereas the team \textit{MedSeg\_JU}~\citep{Banik2021} has participated in the instrument segmentation task.


%\subsubsection{Methods for polyp segmentation}

\textbf{The Segmentors}: Team Segmentors~\citep{Mirza2021}%Yeung2021
proposed solution is a UNet-based algorithm designed for segmenting polyps in images taken from endoscopies. The primary focus of this approach was to achieve high segmentation metrics on the supplied test dataset, which was a crucial requirement for accurate and reliable polyp segmentation. To this end, they experimented with data augmentation and model tuning to achieve satisfactory results on the test sets.

%presented a deep learning pipeline that is specifically developed to accurately segment colorectal polyps and various instruments used during endoscopic procedures. To improve transparency and interpretability, the pipeline leveraged the Attention U-Net architecture, which enables visualization of the attention coefficients to identify the most salient regions of the input images. This allowed for a better understanding of the model's decision-making process and facilitated the identification of potential errors. To further improve performance, the pipeline incorporated transfer learning using a pre-trained encoder. Additionally, test-time augmentation, softmax averaging, softmax thresholding and connected component labeling were used to further refine predictions and boost performance.

%\textbf{The Arctic}: The team Arctic~\citep{Somani2021}%Chou2021
%proposed a novel Dual Model Filtering (DMF) strategy, which effectively removed false positive predictions in negative samples through the use of a metrics-based threshold setting. To better adapt to high-resolution input with various distributions, the PVTv2~\citep{wang2022pvt} backbone was embedded into the SINetV2~\citep{fan2021concealed} framework. The SINetV2 framework with Camouflaged Object Detection (COD) was used for better identification ability, as polyp segmentation is a downstream task. Additionally, extensive experiments have been conducted to study the effectiveness of DMF, and it was found that the method performs well under different data distributions, making it a favorable solution for problems where the training dataset had a different distribution of negative samples compared to the testing dataset.

\textbf{The Arctic}: Team Arctic~\citep{Somani2021}
utilized a unique hybrid optimization technique that combined the power of DeepLabV3+~\citep{chen2018encoder} and ResNet101~\citep{he2016deep} to address the specific challenges of GI image segmentation effectively. In order to ensure the accuracy of their results, the team employed a 5-fold cross-validation approach, with a learning rate of 0.0001 and a batch size of 12. Additionally, towards transparency, they proposed a method of rendering feature attention maps to visualize the attention of the network on individual pixels within the image.

\textbf{mTEC}: Team mTEC~\citep{bhattacharya_betz_eggert_schlaefer_2021} introduced a new architecture called Dual Parallel Reverse Attention Edge Network (DPRA-EdgeNet) for joint segmentation of polyp masks and polyp edge masks. This architecture utilizes the reverse attention module from PraNet~\citep{fan2020pranet} to perform the segmentation tasks. The team implemented two parallel decoder blocks, with one focused on extracting features for polyp segmentation and the other focused on extracting features for polyp edge segmentation. The polyp mask decoder leverages the features from the edge decoder block to improve the accuracy of the segmentation. Additionally, the team employed deep supervision of both edge and polyp features to stabilize the optimization process of the model.


\textbf{MedSeg\_JU}: Team MedSeg\_JU~\citep{Banik2021} proposed EM-Net, encoder-decoder-based architecture inspired by the M-Net~\citep{7950555} architecture. In their approach, the encoder branch of the network utilized  EfficientNet-B3~\citep{DBLP:journals/corr/abs-1905-11946} as its backbone. The network also employed a multi-scale input method, where the input image was downsampled at rates of 2, 4, and 8 at each level of the encoder branch, providing a multi-level receptive field. The decoder branch was a mirror structure of the encoder, where upsampling was used to increase the size of the feature maps at each level. Skip connections were used to enhance the flow of spatial information lost during downsampling. The final feature maps underwent point-wise convolution and sigmoid activation and were then upsampled to provide deep supervision and a local pixel-level prediction map for each scale of the input image. These maps were then fused to generate the final segmentation mask. 


\textbf{MAHUNM}: Team MAHUNM~\citep{Haithami2021} presented an approach for enhancing the segmentation capabilities of DeeplabV3 by incorporating Gated Recurrent Neural Network (GRU). In their approach, the team replaced the 1-by-1 convolution in DeeplabV3 with GRU after the Atrous Spatial Pyramid Pooling (ASSP) layer to combine input feature maps. While the convolution and GRU had sharable parameters, the latter had gates that enabled or disabled the contribution of each input feature map. The experimental evaluation conducted on unseen test sets demonstrated that using GRU instead of convolution produced better segmentation results.


\textbf{leen}: Team leen~\citep{ahmed2021explainable} utilized the generative adversarial networks (GANs) framework to produce corresponding masks that locate the polyps or instruments on GI polyp images. To ensure transparency and explainability of their models, the team leen adopted the layer-wise relevance propagation (LRP) approach~\citep{bach2015pixel}, which is one of the most widely used methods in explainable artificial intelligence. This approach generated relevant maps that display the contribution of each pixel of the input image in the final decision of the model.


\textbf{NYCity}: Team NYCity~\citep{Chen2021}  presented a novel multi-model ensemble framework for medical image segmentation. The team first collected a set of state-of-the-art models in this field and further improved them through a series of architecture refinement moves and a set of specific training skills. By integrating those fine-tuned models into a more powerful ensemble framework, they were able to achieve improved performance. The proposed multi-model ensemble framework was tested on polyp and instrument datasets and experiment results have shown that it performed satisfactorily.




\textbf{PRML}: Team PRML~\citep{Poudel2021} introduced Ef-UNet, a segmentation model that is composed of two main components. First, a U-Net encoder that utilizes EfficientNet~\citep{DBLP:journals/corr/abs-1905-11946} as a backbone, which allows the generation of different semantic details in multiple stages. Second, a decoder integrates spatial information from different stages to generate a final precise segmentation mask. Using EfficientNet as the encoder backbone provides Ef-UNet with the ability to efficiently extract high-level features from the input images while the decoder component effectively integrates these features to produce accurate segmentation results.




\textbf{OXGastroVision}: Team OXGastroVision~\citep{ali2021iterative} presented a novel solution that utilizes two state-of-the-art deep learning models, namely the iterative FANet~\citep{tomar2022fanet} architecture and DDANet~\citep{tomar2021ddanet}. The FANet is based on a feedback attention network that allows rectifying predictions iteratively. It consists of four encoder and four decoder layers. Similarly, DDANet is based on a dual decoder attention network with one shared encoder at each layer. While the iterative mechanism in the full FANet architecture can lead to larger computational time, DDANet has real-time performance (70 FPS) but sub-optimal output. To overcome these limitations, the team proposes to use the segmentation maps from the DDANet output as input for the FANet iterative network for pruning. This approach aims to achieve a balance between computational efficiency and segmentation accuracy.





\textbf{CV\&Med IIAI}: Team CV\&Med IIAI~\citep{Chou2021}
proposed a novel dual model filtering (DMF) strategy, which effectively removed false positive predictions in negative samples through the use of a metrics-based threshold setting. To better adapt to high-resolution input with various distributions, the PVTv2~\citep{wang2022pvt} backbone was embedded into the SINetV2~\citep{fan2021concealed} framework. The SINetV2 framework with camouflaged object detection (COD) was used for better identification ability, as polyp segmentation is a downstream task. Additionally, extensive experiments have been conducted to study the effectiveness of DMF, and it was found that the method performs well under different data distributions, making it a favorable solution for problems where the training dataset had a different distribution of negative samples compared to the testing dataset.


\textbf{Polypixel}: Team Polypixel~\citep{Tzavara2021} presented a study in which they used both pretrained and non-pretrained segmentation models for the polyp and instrument segmentation task. The team trained and validated both models on the dataset. The model architectures were retrieved from a Python library, ``Segmentation Models"~\url{https://github.com/qubvel/segmentation_models}, that contained different CNN architectures. This library offered models with both untrained and pre-trained weights, which were trained on the ImageNet dataset. To find the optimal fit for their datasets, they experimented and tested their results using EfficientNet, MobileNet, SE-ResNet, Inception, ResNet, and VGG. They achieved the best results with EfficientNetB1 for the polyp segmentation task. 




\textbf{agaldran}: Team agaldran~\citep{galdran2021polyp} utilized a double encoder-decoder structure for polyp and instrument segmentation, which consists of two U-Net like structures arranged sequentially as shown in Figure~\ref{fig:bestmethodmedAI2021}. The first encoder-decoder network processes the original image and produces output that is fed into the second encoder-decoder network. According to the authors, this setup allows the first network to highlight the important features of the image for segmentation while the second network further improves the predictions of the first network. To train their models, the team employed a 4-fold cross-validation approach, training with four separate models and used temperature sharpening on the ensemble to produce the final segmentation maps. 


% Figure environment removed


\textbf{TeamAIKitchen}: Team TeamAIKitchen~\citep{Keprate2021} presented a methodology for developing, fine-tuning, and analyzing a U-Net-based model for generating segmentation masks for the polyp segmentation task. The evaluation using the unseen testing dataset resulted in an IOU of 0.29 and a DSC of 0.41 for the polyp segmentation task. 



\textbf{CamAI}: Team CamAI~\citep{Yeung2021} presented a deep learning pipeline that is specifically developed to accurately segment colorectal polyps and various instruments used during endoscopic procedures. To improve transparency and interpretability, the pipeline leveraged the Attention U-Net architecture, which enables visualization of the attention coefficients to identify the most salient regions of the input images. This allowed for a better understanding of the model's decision-making process and facilitated the identification of potential errors. To further improve performance, the pipeline incorporated transfer learning using a pre-trained encoder. Additionally, test-time augmentation, softmax averaging, softmax thresholding and connected component labeling were used to further refine predictions and boost performance.



\textbf{IIAI-CV\&Med}: Team IIAI-CV\&Med~\citep{Dong2021} 
developed an ensemble of three sub-models, namely Polyp-PVT \citep{dong2021polyp}, Sinv2-PVT, and Transfuse-PVT. The official Polyp-PVT, as designed for polyp segmentation, was adopted without modification and achieved state-of-the-art segmentation capability and generalization performance. Transfuse, also designed for polyp segmentation, was improved by replacing the transformer part with the pyramid vision transformer (PVT)~\citep{wang2022pvt} to enhance its performance. The official Sinv2~\citep{fan2021concealed}, which proposes an end-to-end network for searching and recognizing concealed objects, was employed and its original backbone of Res2Net was replaced with a stronger PVT transformer~\citep{wang2022pvt} to extract more meaningful features. 




\input{tables/medai2021summary}

\textbf{Vyobotics}: Team Vyobotics~\citep{Rauniyar2021}  presented a solution based on dual decoder attention network (DDANet)~\citep{tomar2021ddanet}, a deep learning model that has been specifically designed to achieve decent performance and real-time speed. The team performed data augmentation and trained a smaller network. This smaller network has a lower number of trainable parameters, which resulted in lower GPU training time. The ultimate goal of this approach was to achieve decent evaluation metrics while maintaining a decent FPS speed, which is crucial for real-time applications.

\textbf{NAAMII}: The team participated in polyp and instrument segmentation tasks. They employed U2-Net as the base network. They added a separate learnable CNN network on the decoder part of the U2Net to regress the HoG features of the input images. The output from each decoder block was fed into the HoG regressor and learned the parameters to predict the HoG correctly. They jointly minimized Mean Squared Error (MSE) loss for HoG features and CrossEntropy loss for Segmentation. However, they only submitted their method description to the organizer and did not publish it as a research paper.



\section{Results}
In this section, we present a summary of the evaluated results obtained on the test dataset by all the participating teams in the two challenges: ``Medico 2020” and ``MedAI 2021”. Each challenge consists of tasks with a specific focus and evaluation metrics. There were two tasks for Medico 2020 challenge, namely \textit{polyp segmentation} and \textit{algorithm efficiency} tasks. In the MedAI 2021, there were three tasks, namely \textit{polyp segmentation}, \textit{surgical instrument segmentation} and \textit{transparency task}. The teams were evaluated based on standard evaluation metrics such as mIoU, DSC, Rec, Pre, Acc, F1, F2, and FPS. We gave more emphasis was given to mIoU, DSC and FPS. We have highlighted the best and the second-best scores in boldface and red color, respectively, for all the tasks in the two challenges.




\subsection{Medico 2020 Polyp segmentation results}
\input{tables/Summary_Team_Results}
\input{tables/Summary_Team_Efficiency}

% Figure environment removed

In Table~\ref{table:medicochallengeresults} and Table~\ref{table:Algorithm_efficiency_task}, we provide the results for the \textit{polyp segmentation} task and \textit{algorithm efficiency} task for the challenge ``Medico 2020''. A total number of 17 teams participated in the first task and 9 teams participated in the second task. The teams were ranked based on the mIoU metric for the first task, and for the second task, the teams were ranked based on the FPS metric. It can be observed from Table~\ref{table:medicochallengeresults} that Team ``PRML2020GU'' outperforms other participating teams in the polyp segmentation task. It achieves a mIoU of 0.7897, DSC of 0.8607, recall of 0.9031, precision of 0.8673, accuracy of  0.9546 and F2 of 0.8748. Team ``AI-TCE'' was the second best performing team with mIoU of 0.7770 and ``IIAI-Med'' was the third best performing team. The best performing team ``PRML2020GU'' used an encoder-decoder structure with EfficientNet as the backbone and a U-Net decoder with channel-spatial attention with deep supervision. This architecture had an improvement of 1.24\% and 1.31\% over the mIoU and DSC achieved by the Team ``AI-TCE'' which used PraNet and Res2Net backbone. 


For the second task, as in Table~\ref{table:Algorithm_efficiency_task}, team ``PRML2020GU'' has poor speed performance with a processing speed of only 2.25 fps which is not desirable for a real-time efficient model. An interesting observation is that Team ``GeorgeBatch'' outperforms other participating teams in the algorithm efficiency task with a processing speed of 196.79 fps as can be observed from Table~\ref{table:Algorithm_efficiency_task} and Figure~\ref{fig:fps}. However, it is worth noting that the team obtained a low mIoU of 0.6351 for the polyp segmentation task as evident from Table~\ref{table:medicochallengeresults}.  Despite the two teams, ``PRML2020GU''' and ``GeorgeBatch'', achieving the highest evaluation metric values, there is a trade-off between these performance metrics. Low FPS cannot be used for real-time medical processing applications, and low overlap evaluation metrics cannot generate precise segmentation masks. To provide further insight, we have included the qualitative results obtained by all the research teams who participated in Medico 2020 challenge in Figure~\ref{fig:medico2020results}. We can see that none of the teams came close to the ground truth mask. Achieving a balance between these metrics is crucial for developing an efficient polyp segmentation model. 

 

\subsection{MedAI 2021 polyp and instrument segmentation challenge results}
\input{tables/Table_medai1.tex}
\input{tables/Table_medai2.tex}


% Figure environment removed

% Figure environment removed

In Tables~\ref{table:medai} and~\ref{table:medai2}, we tabulated the evaluation results of all the participating teams in MedAI 2021 for the two tasks, namely polyp segmentation and instrument segmentation. A total of 15 teams participated in both tasks of the MedAI2021 challenge. However, two teams, namely ``Vyobotics~\citep{Rauniyar2021}” and ``MedSeg\_JU~\citep{Banik2021}” participated in only one task. Almost all the teams have used the same architecture for both tasks. The participating teams for the two tasks were ranked based on the mIoU metric.  From Table~\ref{table:medai}, it can be observed that team ``agaldran” outperforms other teams in the polyp segmentation task with mIoU of 0.8522,  DSC of 0.8965, accuracy of 0.9791, recall of 0.9009 and precision of 0.9242. Team ``IIAI-CV\&Med” also showed good performance and was ranked $2^{nd}$ in the polyp segmentation task with a DSC of 0.8927, a very small difference from the best-performing team. In Figure~\ref{fig:medaipolypchallenge2011}, we present the qualitative results of the participating teams for the polyp segmentation task of MedAI~2021. None of the methods performed well on this challenging image, emphasizing the need for more robust polyp segmentation methods. However, in the overall test set, the predicted segmentation masks from most of the team performed well on regular polyps. Overall, the qualitative masks produced by teams ``agaldran”  and ``IIAI-CV\&Med'' were better as compared to the other teams. 


We present the results of the instrument segmentation task in Table~\ref{table:medai2}. From the table, it can be observed that the same team, ``agaldran” also outperforms other participating teams in the instrument segmentation task with a high mIoU of 0.9364 and DSC of 0.9635.  Team ``NYCity” was ranked $2^{nd}$ in this task with a mIoU of 0.9326 and DSC of 0.9586. However, Team ``NYCity” obtained the highest recall of 0.9712, which signifies it has low false negative (FN) regions in the predicted segmentation mask compared to team ``agaldran”. Another interesting observation is the team ``agaldran” also achieved higher metric values for the instrument segmentation task as compared to the polyp segmentation task, as instrument segmentation is relatively easier than polyp extraction due to the greater variability of the latter regarding color and appearance. In Figure~\ref{fig:medAI_ins_2021}, we also present the qualitative results of the research teams who participated in the instrument segmentation challenge of MedAI2021. From the qualitative results, it can be observed that the ground truth prediction made by team ``agaldran” is also superior to the other team.  Therefore, it can be concluded from the obtained evaluation metrics for the two tasks that team ``agaldran” proposed a more robust algorithm and was accurately able to segment polyp and instrument at high accuracy.

 


% Figure environment removed




\begin{table*} [!t]
\caption{Evaluation of the \textbf{`Transparency tasks'} for MedAI 2021 Challenge. For this task, a team of experts accessed the submission based on several criteria and provided a score based on the availability and quality of the source code (for e.g., open access, public availability, and documentation for reproducibility), model evaluation (for e.g., failure analysis, ablation study, explainability, and metrics used) and qualitative evaluation from clinical experts (e.g, usefulness and understandability of the results).}
\label{tab:transscore}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|ccc|cccc|ll|c|}
\toprule
%\rowcolor[HTML]{93C47D} 
\multicolumn{1}{|c|}{}  & \multicolumn{3}{c}{\textbf{Open Source}}     & \multicolumn{4}{|c}{\textbf{Model Evaluation}}              & \multicolumn{2}{|c|}{\textbf{Doctor Evaluation}}       &       \\ \cline{2-10}

\multicolumn{1}{|c|}{\multirow{-2}{*}{\textbf{Team Name}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Publicly \\ available \\ (0 or 1) \end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Code \\ Quality \\ (+1-3) \end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}Readme \\ (+1-3)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Failure \\ Analysis \\ (+1-3)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Ablation \\ Study \\ (+1-3)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Explainability \\ (+1-3) \end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}Metrics \\ Used \\ (+1-3)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Usefulness \\ (+1-3)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Understandable \\ (+1-5)\end{tabular}}} & \multirow{-2}{*}{\textbf{Final Score}} \\ \hline
agaldran                                                                           & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{2}                                                    & 3                      & \multicolumn{1}{c|}{3}                                                        & \multicolumn{1}{c|}{3}                                                      & \multicolumn{1}{c|}{3}                                                      & 1                            & \multicolumn{1}{c|}{2}                                                  & \multicolumn{1}{c|}{3}                                                      & \cellcolor[HTML]{57BB8A}21                                     \\ \hline
CamAI                                                                              & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{1}                                                    & 1                      & \multicolumn{1}{c|}{2}                                                        & \multicolumn{1}{c|}{1}                                                      & \multicolumn{1}{c|}{2}                                                      & 1                            & \multicolumn{1}{c|}{2}                                                  & \multicolumn{1}{c|}{5}                                                      & \cellcolor[HTML]{87CFAC}16                                     \\ \hline
CV\&Med IIAI                                                                       & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{1}                                                    & 0                      & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{FBECEB}3                                      \\ \hline
IIAI-CV\&Med                                                                       & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{1}                                                    & 2                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{c|}{1}                                                  & \multicolumn{1}{c|}{4}                                                      & \cellcolor[HTML]{C1E6D4}10                                     \\ \hline
leen                                                                               & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{1}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{2}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{FBFEFC}4                                      \\ \hline
MAHUNM                                                                             & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{1}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{FBECEB}3                                      \\ \hline
mTEC                                                                               & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{1}                                                    & 3                      & \multicolumn{1}{c|}{3}                                                        & \multicolumn{1}{c|}{1}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{c|}{3}                                                  & \multicolumn{1}{c|}{4}                                                      & \cellcolor[HTML]{7ECBA5}17                                     \\ \hline
NAAMII                                                                             & \multicolumn{1}{l|}{}                                                         & \multicolumn{1}{l|}{}                                                     & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}                                                         & \multicolumn{1}{l|}{}                                                       & \multicolumn{1}{l|}{}                                                       &                              & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{E67C73}0                                      \\ \hline
NYCity                                                                             & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{EDA19B}1                                      \\ \hline
OXGastroVision                                                                     & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{2}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{FBECEB}3                                      \\ \hline
Polypixel                                                                          & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{1}                                                    & 2                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{F1FAF5}5                                      \\ \hline
PRML                                                                               & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{1}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{F4C6C3}2                                      \\ \hline
TeamAIKitchen                                                                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{1}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{F4C6C3}2                                      \\ \hline
The Arctic                                                                         & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{2}                                                    & 1                      & \multicolumn{1}{c|}{1}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{3}                                                      & 1                            & \multicolumn{1}{c|}{1}                                                  & \multicolumn{1}{c|}{3}                                                      & \cellcolor[HTML]{A4DBC0}13                                     \\ \hline
The Segmentors                                                                     & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                    & 0                      & \multicolumn{1}{c|}{0}                                                        & \multicolumn{1}{c|}{0}                                                      & \multicolumn{1}{c|}{0}                                                      & 1                            & \multicolumn{1}{l|}{}                                                   &                                                                             & \cellcolor[HTML]{FBFEFC}4                                      \\ \hline
Vyobotics & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}& \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{} &  & \cellcolor[HTML]{E67C73}0 \\ \hline
{MedSeg\_JU} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}& \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{} &  & \cellcolor[HTML]{E67C73}0 \\
\bottomrule
\end{tabular}
}

\end{table*}


% Figure environment removed


A detailed score distribution under different criteria is shown in Table~\ref{tab:transscore}, which was part of our \textbf{\textit{Task 3 (transparency task)}}. Here, submissions that did not participate in task 3 are left blank. We have evaluated transparency tasks using a more quantitative approach compared to polyp and instrument segmentation. A multi-disciplinary team assessed each submission and evaluated the transparency and understandability of the proposed solutions. Each team was scored based on the three criteria: open source code, model evaluation and clinical evaluation. The open source code was evaluated based on the presence of a publicly available repository, code quality and quality of the readme file. The model evaluation included failure analysis, ablation study, explainability of the method, and metrics used.  Evaluation by clinical experts  consider the usefulness of the method and its interpretability. With these three criteria, we aim to measure the transparency of the provided solutions. We present the results in Table~\ref{tab:transscore}. Team ``agaldran'' outperformed other competitors with a final score of 21 out of 25. Similarly,  ``mTEC'' obtained a score of 17 out of 25 and was ranked $2^{nd}$. Likewise, team ``CamAI" obtained a score of 16 out of 25 and was ranked third in the transparency task. There were also efforts from team such as ``The Arctic" which obtained a score of 13 out of 25 and IIAI-CV\&Med that obtained a score of 10 out of 25. These scores show their effort to provide a transparent solution to the polyp and instrument segmentation tasks. We provide the final ranking and task-wise scores in Figure~\ref{fig:medaipolypchallenge2021}. Notably, team ``agaldran'' outperforms others in all three tasks and overall challenge and emerged as the winner of the MedAI challenge. Overall, mTec secured the second position. Following closely behind, CamAI showcased the third-best solution. 


Figure~\ref{fig:avg_score} illustrates the plot of DSC reported by each team in their submissions in the two challenges with three different tasks. It can be observed that the \textit{polyp segmentation task} from 2020 to 2021 gained improvement with a larger number of submissions achieving a DSC of more than 0.80 and the best performing team with a DSC of around 0.90. Similar progress can be observed in Figure~\ref{fig:avg_graph} where an overall DSC increased by 2.33\%  when an average score is computed over all participating teams' individual best DSC in the 2021 polyp segmentation challenge. We further compared all segmentation metrics, including DSC, recall, precision, mIoU score, accuracy, and F2 score, as shown in Figure~\ref{fig:comparison_graph}. Notably, all teams' accuracy is consistently high, with a score close to 0.90. Similarly, the different scores of evaluation metrics are consistent for instrument segmentation tasks in the MedAI challenge. However, there is a high variation in the mIoU between the different teams in the polyp segmentation tasks of Medico 2020 and MedAI 2021 challenges. 


These values pertain to the best score corresponding to a particular metric the individual team obtained in different executions. It is to be noted that each team was given the opportunity to submit five different submissions, and the best results for the best submission are reported in the Tables here. From here, it can be observed that most teams in MedAI 2021 challenge reported overall high scores in terms of various segmentation metrics when compared to Medico 2020 outcomes, thus, highlighting the improved performance trends in automated systems over time. Furthermore, it can also be visualized that unlike the high variations shown by teams' scores in the polyp segmentation task, better performance and smaller deviations in scores are reported in the instrument segmentation task. The high variations in the polyp segmentation results also show that polyp segmentation is more challenging because of the presence of variations in the size, structure and appearance of the polyps and the presence of the artifacts and lighting conditions deteriorate the algorithm's performance. 


\section{Discussions}
The rapid advancement in the AI-based techniques that support CADe and CADx systems has resulted in the introduction of numerous algorithms in the domain of medical image analysis, including colonoscopy. To assess the performance of these algorithms, it is important to benchmark on the particular set of datasets. It enables the comparison and analysis of different techniques and assists in identifying challenging cases that need to be targeted using improved methodologies. This also includes cases that are misled by the presence of artifacts and occlusion by surgical instruments~\citep{ali2020objective}. Besides developing and analyzing AI-based algorithms, it is crucial to include explainability and interpretability to infuse trust and reliance during the adoption of automated systems in clinical settings~\citep{npjdm:Ali22}. Therefore, the challenges discussed in this paper not only focus on lesion and instrument segmentation but also emphasize the importance of transparency in medical image analysis. This section covers the findings and limitations of the two challenges, Medico 2020 and 2021.


The findings from Medico 2020 and MedAI 2021 challenges provide valuable insight and trends for the current biomedical image analysis challenge. All the participants in both challenges used deep learning-based frameworks for segmentation tasks. It can be observed that most of the deep learning frameworks submitted for the challenge used Adam optimizer for optimizing their network. However, a handful of teams used other optimizers such as stochastic gradient descent (SGD) and RMSProp. We also observed that most of the deep learning frameworks used by participants in both challenges used the encoder-decoder framework as the backbone network. Additionally, most of the teams used data augmentation to boost the number of training samples prior to training their frameworks to improve the performance of their architecture. For the MedAI 2021 challenge, excluding two teams, all the others participated in both polyp and instrument segmentation tasks and used the same framework.   

\subsection{Medico 2020 challenge methods} %Polyp segmentation
Most of the methods reported in the Medico 2020 challenge focus on encoder-decoder architecture. Other networks used include GAN and R-CNN. The overview of the methods is provided in Table~\ref{table:challenge_summary2020}. For more detailed architectural information, we have also included the backbone and algorithm used by each team. Further, we also report the nature of the algorithm and the choice basis of evaluation, such as mIoU, DSC or FPS. Additionally, we provide information about the augmentation and hyperparameters such as loss function and optimizers. It is noteworthy that all the top three methods used the encoder-decoder architecture and out of 17, only three teams adopted some other architectures. Comparative analysis shows that the highest-scoring encoder-decoder network outperforms the GAN-based approach by a significant margin of 35.17\% in and mIoU and 29.89\% in dice score. Similarly, compared to the R-CNN-inspired networks (team ``IRIS-NSYSU''), the best approach (team ``PRML2020GU'') achieves an improvement of 28.63\% in mIoU score and 21.91\% dice score.



\subsection{MedAI 2021 challenge methods}
The summary of the different approaches adopted by the participating teams of the MedAI2021 Challenge is presented in Table~\ref{table:challenge_summary2021}. To provide a brief overview of the general techniques adopted by the different teams, they can be categorized based on the nature of the approach followed, such as ensemble models, encoder-decoder based architectures, CNN, and hybrid CNN models. Almost all the teams presented the same model for both the tasks proposed in the challenge.  In the polyp segmentation task, most teams explored ensemble modeling, encoder-decoder networks, or a combination of both. Another criterion of categorization could be CNN or transformed-based approaches. It is observed that the top-ranked team (agaldran) utilized two encoder-decoder networks and reported a mIoU score of 85.22\%. Contrary to the Medico 2020 polyp segmentation challenge, where GAN-based methods failed to perform well, in this challenge, the second-leading team in polyp and instrument segmentation tasks (IIAI-CV\&Med) adopted the GAN framework to generate segmented polyp masks. This approach is observed to be a close competitor to the best performer, with a slight difference of 0.90\% in accuracy and 0.38\% in the mIoU score. Also, this GAN-based approach performed better than the best method in terms of dice score and recall. The next close competitor in the list is a CNN and transformer based ensemble model.

In the MedAI2021 instrument challenge, participants mainly focused on either ensemble models or encoder-decoder networks similar to the polyp segmentation task. As the majority of the teams utilized the same model in this task that they proposed for the polyp segmentation problem, the categorization of overall methods remains the same as that of the first task described above. The top rank is secured by ``agaldran'', with encoder-decoder architecture, pyramid network as the decoder, and Resnext101 as the pre-trained decoder.  The second-ranked model by team "NYCity" is the CNN and transformer based ensemble model, which achieved only a slight difference in the scores from the leading model. From the challenge, it can be observed that most of the team were reluctant to share their method (refer Table~\ref{tab:transscore}). Additionally, the quality of the code submitted by most of the team was was not satisfactory.  Most of the participants did not put much effort into the readme file. Additionally, most of the teams neglected the failure analysis, ablation study and explainability in their submission. Moreover, based on the doctor's evaluation, only the solution provided by a few teams (for example, mTEC, CamAI,agaldran, and IIAI-CV\&Med) was useful and understandable. 


%NOTE: This is good! 
% For reference see here: https://www.sciencedirect.com/science/article/pii/S1361841521000487?via%3Dihub#sec0023

\subsubsection{Analysis of the failed cases}
We have analyzed the regular and failing cases in polyp and surgical tool segmentation to highlight the limitations of the current methods so that these cases can be considered during further algorithm development. Figure~\ref{fig:medico2020results} and Figures~\ref{fig:medaipolypchallenge2011} shows the example of instances where the models fail for most cases. From the results on the test dataset, it was observed that most of the algorithms failed on diminutive and flat polyps located in the left colon. These are the challenging classes in the colon and require effective detection and diagnosis system. Similarly, although most of the methods performed well on the diagnostic and therapeutic surgical tool, there were issues with the images having caps and forceps. Similarly, the performance on the challenging images for polyps and instruments (see Figure~\ref{fig:medico2020results}, Figures~\ref{fig:medaipolypchallenge2011} and~\ref{fig:medAI_ins_2021}) demonstrate that we can not solely rely on image-based metrics. Therefore, investigating the cause for misclassification for each sample in the dataset and failure analysis will be critical to focus for future research. 


\subsubsection{Trust, safety, and interpretability of methods}
Integrating CADe or CADx in healthcare necessitates addressing factors such as trust, safety, and interpretability to ensure its adoption in clinical settings. The high variations in the curated datasets used to train such models and the actual scenarios in which they are adopted create a high chance of biases, impacting the generalizability of the method. Such bias ultimately makes it challenging to infuse trust while adopting CADe or CADx tools and questions the safety of patients. We addressed this issue by incorporating a transparency task in the MedAI2021 challenge. 

The main aim of this task was to emphasize the need for interoperability, reproducibility and explainability in the AI-based submissions and shed light on the potential bias and wrong decisions that could have resulted from model and algorithmic bias.  Our dataset contained polyp cases with varied shapes and sizes. We included samples with artifacts to make them closer to real clinical settings. Further, the inclusion of frames containing surgical instruments supported the cases of occluded endoluminal elements or polyps that could arise in general. Some of the methods adopted by the participating teams include the submission of intermediate heatmaps using approaches like LRP and a detailed ablation study in support of the predictions obtained. By promoting transparency and addressing potential biases, the challenge aimed to foster trust in the presented solution and ensure safety in adopting such methods in the clinic.



\subsection{Transparency}
One of the main aim of the MedAI 2021 challenge was to promote transparency in polyp and surgical instrument segmentation. In addition to the metric-based evaluation, we also evaluated submissions based on the transparency of their work. We encouraged participants to share how their models were trained, how datasets were used, and the insight on the interpretation of the model prediction. We allowed the participants to submit, considering the transparency and left them to decide what to deliver for the task. We gave some suggestions, such as providing rigorous analysis of the failing cases and a detailed GitHub repository with clear steps to run the code for reproducibility. We encouraged the participants to list package dependencies, provide the code for the architecture (guiding through building, compiling, and training of the proposed architecture), provide documentation for the code, and share trained model weights in a standardized format.
Additionally, we encouraged participants to include the code for model evaluation and provide repository licensing information to enable others to use the code and the trained model responsibly. Moreover, we suggested that the participants explain model predictions using intermediate heatmaps and statistical analysis and provide a detailed ablation study to show the contributing blocks in the network. Although heatmaps might not be the best choice, other alternatives, such as SHapley Additive exPlanations (SHAP), should be explored. 

The submissions of the transparency task were evaluated using a more qualitative approach. A multi-disciplinary team accessed each submission and evaluated the transparency of the proposed solution. Then, we provided a report on the transparency of their submission and highlighted the details about which parts were good and which required more clarity to achieve transparency. Using this approach, we hope that the participant will be more responsible in making their submission in a public repository and including transparency parameters in their future work. 

\subsubsection{Limitation of the Medico 2020 and MedAI 2021}
In our study, we aimed to standardize the challenge of polyp and instrument segmentation by providing the same test sets and evaluation metrics to all participants. To achieve this, we introduced variable polyp cases, including polyps with different sizes, noisy frames with artifacts, blurry images, and occlusion. We also added regular frames to the test set to ensure that participants drew the ground truth manually and did not cheat. However, our study has some limitations. Although we used datasets collected from four medical centers in Norway, these images are from a single country, limiting the ethnicity variance. Therefore, there is a need for a more diverse dataset that includes multiple ethnicities and countries. Moreover, the current models should be tested on multi-center datasets to assess their generalization ability.

In our challenge, there was no online leaderboard due to the policy of Mediaeval. Therefore, we calculated the predictions submitted by each team manually. Each team had limitations of 5 submissions for each task, which restricted further optimization opportunities. Although we have also introduced normal findings from the gastrointestinal tract to trick the participants and models, our challenge only used still frames and did not incorporate video sequence datasets. Most of the images are only from white light imaging. Although our dataset was annotated by one annotator and checked by two gastroenterologists, there is still a possibility of bias in the labels. In the instrument challenge, we had more images from the stomach class than surgical instruments such as surgical forceps or snares due to the lack of availability of datasets. Finally, despite including diverse cases in the polyp and instrument segmentation challenge, we still had limited flat and sessile polyps, frequently missed during routine colonoscopy examinations. Incorporating multi-center data, video sequences and addressing label biases will lead to more comprehensive and reliable evaluations of AI-based colonoscopy systems.


\subsubsection{Future steps and strategies}
In our study, we aimed to promote transparency and interpretability in machine learning models for the GI setting. However, more work is needed to understand how decisions are made and identify potential biases or errors in a quantitative manner to build trust in such systems in a clinical setting. To achieve this, we plan to test the best-performing algorithms on large-scale datasets to observe their scalability. We will organize a competition on multi-center datasets, including images from various modalities such as Flexible spectral Imaging Color Enhancement (FICE) and Blue laser imaging (BLI), and challenging cases. We will also consider metrics that weight speed, accuracy, and robustness for better objective assessments and introduce more distance-based metrics such as Hausdorff distance and normalized surface distance for improved fairness.

Furthermore, we will do a reliability test on the best-performing methods on easy, medium, and challenging real-world cases. We will emphasize more transparent decision-making methods and visualize interpretability results while focusing on clinical relevance rated by expert clinicians instead of just one objective metric. To achieve this, we have already started collecting large-scale datasets and plan to build a tool if the algorithms are robust enough and verified by our gastroenterologists. As a next step, we have organized Medical Visual Question Answering for GI Task - MEDVQA-GI challenge in 2023~\footnote{https://www.imageclef.org/2023/medical/vqa}. 


\section{Conclusion}
\label{section:conclusion}
Our study aimed to provide a comprehensive analysis of the methods used by participants in the Medico 2020 and MedAI 2021 competitions for different medical image analysis tasks. We designed the tasks and datasets to demonstrate that the best-performing approaches were relatively robust and efficient for automatic polyp and instrument segmentation. We evaluated the challenge based on several standard metrics. In MedAI 2021, we also used a quantitative approach, where a multi-disciplinary team, including gastroenterologists, accessed each submission and evaluated the usefulness and understandability of their results. Through the qualitative results, we found that more generalizable and transparent methods are needed to be integrated into real-world clinical settings. During the ``performance task'' and ``algorithm efficiency'' tasks, we observed a trade-off between accuracy and inference time when tested across unseen still frames. For the instrument segmentation challenge, we observed that all most all teams performed relatively well as a segmenting instrument is easy compared to polyp segmentation. From the transparency task, we observed that more effort is required from the community to enhance the transparency of their work. Overall, we also observed that several teams demonstrated the use of data augmentation and optimization techniques to improve performance on specific tasks. Our study highlights the need for multi-center dataset collection from larger and more diverse populations, including experts from various clinics worldwide. More competitions should be held to achieve the goal of generalization in polyp segmentation. Further research should investigate multiple polyp classes that typically fail in clinical settings, multi-center clinical trials, and the emphasis on real-time systems. Additionally, research on transparency and interpretability could help build more clinically relevant and trustworthy systems.


\section*{Acknowledgment}
D. Jha is supported by the NIH funding: R01-CA246704 and R01-CA240639. V. Sharma is supported by the INSPIRE fellowship (IF190362), DST, Govt. of India. D. Bhattacharya is funded partially by the i$^3$ initiative of the Hamburg University of Technology and by the Free and Hanseatic City of Hamburg (Interdisciplinary Graduate School “Innovative Technologies in Cancer Diagnostics and Therapy”). K. Roy is thankful to DST Inspire Ph.D fellowship (IF170366).

\section*{Authors contribution}
D. Jha conceptualized, initiated, and coordinated the work. He also led the data collection, curation, and annotation processes for Medico 2020 and evaluated the Medico 2020 Challenge. S. Hicks and M.A. Riegler initiated the MedAI 2021 Challenge, organized the data collection together with D. Jha and conducted all evaluations for the challenges, organized the reviews and coordinated with all the authors. V. Sharma analyzed the results and prepared most graphs for technical validation along with N. K. Tomar. She also wrote a part of the results and discussion. D. Banik, D. Bhattacharya and  K. Roy wrote part of the introduction, related work, and participants' methods and provided subsequent feedback on the method's tables. M.A. Riegler and P. Halvorsen facilitated the data and organization for both competitions. Our gastroenterologists, T. De Lange and S. Parasa, reviewed the annotations and provided the required feedback during dataset preparation and evaluation. Challenge participants provided the method details for Medico 2020. All authors read the manuscript, provided substantial input, and agreed to the submission.

\bibliographystyle{vendor/model2-names.bst}\biboptions{authoryear}
\bibliography{refs}
\end{document}