
%Figure~\ref{bestmethod2020} shows a broad overview of our proposed method. 
%EfficientNet uses the MobileNet inverted block (MB)~\citep{sandler2018mobilenetv2} with squeeze and excitation network~\citep{hu2018squeeze}, and a combination of these components works as the good feature extractor module. We keep a network different level depending upon the size of feature map. We reduce the size of feature maps by 2 at each level. The size of the spatial feature map at the MB block is $7\times7$, which indicates that the feature maps are down-sampled by five times and is halved according to the previous level. At different levels, each node concatenates the feature maps from its previous node of the same level and the upsampled feature maps of the next level, enabling aggregation of multi-scale features. Next, the concatenated features are passed through the channel-spatial network at each node. On the decoder side, a transposed convolution is used for upsampling the feature maps. Similarly, we upscale the outputs of the decoder block to $224\times224$ depending upon the current feature map size and apply a 1x1 convolution with 1 kernel and a sigmoid function. Then, all the outputs (after deep supervision) are averaged and a final result is generated. We performed experiments on five different settings as explained below:

% \begin{itemize}
% \item Method 1 uses the UNet++~\citep{zhou2018unet++} as a baseline model (skipping deep supervision) and with the EfficientNetB0 as an encoder backbone.
% \item Method 2 extends Method 1 with the addition of Deep supervision horizontally. The feature maps are upsampled to the input image size at each decoder level, and the sigmoid activation function was applied accordingly.
% \item Method 3 employs EfficientNetB1 with the same settings as Method 2.
% \item Method 4 employs EfficientNetB2 with the same settings as Method 3.
% \item Method 5 employs EfficientNetB3 with the addition of channel-spatial block at each node to restrict the irrelevant features.
% \end{itemize}




%The authors compared our model with two of the approaches below: Single model: We used ResNet-34 as our first model. The weights saved after the training phase were loaded in the network and test data were fed to get the predicted polyp masks. Ensemble model: We used two models, ResNet-34 and EfficientNet- B2, to predict our masks. Then we ensembled the predictions by using bit wise multiplication between the two predicted masks. We used ensemble learning as our approach. A single algorithm is often a weak learner, and it might learn a particular type of pattern from the training data. When we ensemble multiple algorithms (i.e. Multiple weak learners), they become strong learners as their predictive power increases and error rate decreases. We used ensemble learning as our approach. A single algorithm is often a weak learner, and it might learn a particular type of pattern from the training data. When we ensemble multiple algorithms (i.e. Multiple weak learners), they become strong learners as their predictive power increases and error rate decreases.




%Our proposed polyp segmentation method uses a deep convolutional network to learn a connection between the input polyps’ images. The overall block diagram of the proposed architecture is shown in Figure 1, which mainly consist of five layers (1) Convolutional layer, (2) Efficient layer~\citep{tan2019efficientnet}, (3) Encoder block with efficientNet B4 (4) Decoder block a combination of dense block and Con-current Spatial and Channel Attention\citep{roy2018concurrent} blocks (5) Convolution Block Attention Module~\citep{woo2018cbam}. In this architecture, the input image pixels $332\times487$ is resized to $384\times 256$ and divided by 255 passed into the convolutional layer. The proposed network inspired by the multilevel hyper vision Net~\citep{sabarinathan2019hyper} and had properties of an encoder, decoder structure with supervision layers. Encoder block used efficient Net B4 as the backbone of the architecture. Decoder block consists of a combination of dense block~\citep{huang2017densely} and Concurrent Spatial and Channel Attention Block (CSCA). Both the encoder and decoder connected by Convolution block attention CBAM block. All the encoder output are supervision, i.e., take individual decoder output and upsampled with output layer and supervised by the loss function. All upsampled results are concatenated and fed into CBAM. Totally six results obtained from our proposed architecture. In the upsampling, we have used a convolution transpose layer. The sum of categorical and dice loss used in training.


%\textcolor{red}{\textbf{HGV-HCMUS:} %-HGV
%In our proposed methods, we offer an architecture that uses the Efficient Net for the encoder block. Moreover, we propose using the low-scale feature to get a better mask of the output and improve the model’s performance, which is Multiscale Efficient U-Net. This architecture includes three main blocks Multiscale Block (MC Block), EfficientNet Encoder block (EEN Block), and Decoder Block. Initially, the input with the shape (w, h,c) passes through the MC Block; this block includes 3 Max Pooling layers, 2 Convolution 2D layers, and 1 Batch Normalization layer. MC layers to create the new low-scale feature for the model. The parameter of the layers can be changed to adapt to the feature representation of images. From the output of the MC block, there are two types of features: low-scale and large-scale features. Then these features pass the encoder block created by the Efficient Encoder Block, with high-scale features, and the map to the decoder block while low-scale features pass the encoder block. Using MultiScale BLock, the model can learn the low-scale feature to improve the performance and adapt to the boundary problem. Their main Motivation is from dataset analysis. We review that the size of the dataset is small, which is the reason why if using some traditional finetune methods, it can be overfitting, that is the reason why we adapt multiscale block to deal with the problem and help perform better. }


%First, input images are augmented by the data augmentation module. Second, augmented images are used as the input of both the student model and the teacher model. Third, the distillation loss between the output of the student model and the output of the teacher model and the student loss between the output of the student model and ground-truth label is calculated to train the student model. 
%The results show that KD-ResUNet++ outperforms ResUNet++ in terms of Jaccard index, Dice similarity coefficient, and recall.
%We trained our proposed model using the Kvasir-SEG~\citep{jha2020kvasir} dataset, which consists of 1,000 polyp images and their corresponding ground truth masks annotated by expert endoscopists. The dataset is split into 88\% for learning the weights and 12\% for validating the model during the training step. Before the training step, we augment input images using our data augmentation module and converted the images to the size of $256\times256$ pixels. The validation set is only normalized. The learning rate is set to 0.001. We use Adam as our optimizer. In our experiment, our proposed model called KD-ResUNet++ outperforms ResUNet++ in terms of Jaccard index, Dice similarity coefficient, and recall.




%The approach used for polyps segmentation is the encoder/decoder architecture. The architecture used resunet++ as the baseline model and modifications were made to get the lightweight image segmentation. The purpose was to get an architecture that provides good performance on the image segmentation evaluation metrics as well as perform well in term of terms of inference time. To check the performance in inference time two metrics were used. These metrics were number of parameters and giga-floating point operations (GFLOPs). The performance evaluation metrics were mean intersection over union, dice co-efficients. To get the lightweight model architecture, changes were made to atous bridge in resunet++ architecture. The convolution layer in atrous bridge were replaced with depthwise separable convolution. The implementation of depth-wise separable convolution resulted in less number of parameters and GFLOPs. The atrous bridge was also replaced with deep atrous bridge. The comparison of modification in model architecture were made against UNet and ResUnet++. All models were trained on custom mean intersection over union loss.


 
%All models are trained using a stochastic gradient descent optimization algorithm with a learning rate of 0.00025 and a batch size of 16. The training process is conducted for a maximum of 80000 iterations with the model's parameters being saved every 300 iterations. The checkpoint with the lowest validation loss is selected as the final model for evaluation. Furthermore data augmentation techniques such as random horizontal flipping vertical flipping and random resizing while maintaining aspect ratio are employed to reduce the generalization error. 
%The main motivation of using the ResNeSt backbone was incorporating a split attention block that modifies the structure of the ResNet backbone allowing for attention to be shared across feature-map groups. This modification leads to improved performance on the task of polyp segmentation. Finally, the team has demonstrated that the ReStNeSt101 backbone combined with the Cascade Mask R-CNN structure is the best segmentation algorithm among all models.


%The approach was focused on the accuracy and time. The resunet++ is one of the best approaches used for the image segmentation for the accuracy aspects. The network is modified to get the similar detection accuracy with lesser resources and time. The approach is a modification in the resunet++ by using depth-wise separable convolution for the reduction in the model size. The approach resulted in 69.48\% mean interaction over union in the dataset of Kvasir-SEG with the model size of 3,047,265 parameters and 6,070,057 GFLOPs.



\begin{comment}

The ``Medico automatic polyp segmentation challenge\footnote{\url{https://multimediaeval.github.io/editions/2020/tasks/medico/}}'' is an international benchmarking challenge hosted through Mediaeval\footnote{\url{http://www.multimediaeval.org/}} platform. The ``Medico automatic polyp segmentation challenge'' aimed to benchmark automated polyp segmentation algorithms on the same dataset. The challenge also aims to develop a method that can detect challenging polyps (for example, flat polyps, sessile polyps and small or diminutive polyps). In this challenge, we invited multimedia and computer vision researchers to submit the results on two tasks; (i) polyp segmentation task and (ii) algorithm efficiency tasks. In the first task, the participants were asked to design and submit the best automated methods to automatically segment polyps. In the second task, participants were asked to submit the results taking speed into account. The evaluation metrics for the first task and second task was \gls{miou} and \gls{fps} respectively. 

Table~\ref{table:medicochallengeresults} shows the results of the ``polyp segmentation task (task i: required)'' of the 17 participating teams in the challenge. Table~\ref{table:Algorithm_efficiency_task} shows the results of the ``algorithm efficiency task (task ii: optional)'' from the 9 participating teams in the challenge. Team ``PRML2020GU" obtained the highest \gls{miou} of 0.7897 and highest \gls{dsc} of 0.8607. Similarly, team ``GeorgeBatch''obtained the highest processing speed of 196.89 \gls{fps}. However, we consider only the teams with a minimum of \gls{miou} of 0.70 and \gls{fps} of 30. Therefore, the algorithm proposed by team ``HCMUS'' was considered as the best solution for the algorithm efficiency task. 

\textbf{IIAI-Med}: IIAI-Med team present a novel deep neural network, termed Parallel Reverse Attention Network (PraNet), for the task of automatic polyp segmentation at MediaEval 2020. Specifically, we first aggregate the features in high-level layers using a parallel partial decoder (PPD). Based on the combined feature, we then generate a global map as the initial guidance area for the following components. In addition, we mine the boundary cues using the reverse attention (RA) module, which is able to establish the relationship between areas and boundary cues. Thanks to the recurrent cooperation mechanism between areas and boundaries, our PraNet is capable of calibrating misaligned predictions, improving the segmentation accuracy and achieving real-time efficiency (nearly 30fps) on a single NVIDIA GeForce GTX 1080 GPU. Code and results are available at https://github.com/GewelsJI/MediaEval2020-IIAI-Med.

\textbf{GS-CDT}: Classically, binary cross-entropy (BCE) is used for binary segmentation tasks. We preserved the standard U-Net architecture, but used intersection-over-union loss (IoU Loss) and experimented with a combination of BCE and IoU losses. Training setup and parameters:(no augmentation for the version submitted to the challenge: v1). The main motivation behind the approach was A trade-off between accuracy and speed is still vital for the use of automated systems during CRC surveillance and surgical removal of polyps. We decided to favour inference/prediction speed when choosing the architecture (standard U-Net) while experimenting with other parameters like loss function and augmentation to boost the performance. Our results demonstrate that IoU loss results in an improved segmentation performance (nearly 3\% improvement on Dice) for real-time polyp segmentation. Our model was the fastest among the models submitted to the challenge (>200 FPS), however the performance needs to be significantly improved (mIoU < 0.7) in order to achieve higher speed-accuracy trade-off. The link is available at https://github.com/GeorgeBatch/kvasir-seg.

\textbf{MedSeg\_JU}: Polyp segmentation is a significant problem for the early diagnosis of CRC. So, a great deal of research has been dedicated to automatic polyp segmentation. In the last few years, deep learning techniques based on Generative Adversarial Networks (GANs) have further revolutionized the state-of-the-art techniques in the computer vision domain. Recently, a significant improvement has been seen in the field of medical image analysis. So, motivated from the insights of GAN where the training is based on learning the data distribution through adversarial method. Hence, the segmentation network introduced for the prediction of synthetic mask should have the same data representation as the original image. In this study, we have proposed an approach for polyp segmentation based on deep conditional adversarial learning technique, as shown in fig 1. The proposed framework has two interdependent modules, a generator network (G) and the discriminator network (D). The generator network is a Fully Convolutional Network (FCN) employed to predict the polyp mask while the discriminator enforces the segmentation to be as similar as the real segmented mask (ground truth). The size of the training images ranges from to pixels. So, we have resized the training images into a fixed size of before they are fed into our proposed network to speed up the training process. To enhance the size of the training dataset, we have applied three general transformation approaches such as rotation (), flipping (vertical and horizontal), and translation (-5 to 30). The training process of the proposed network alternates between 2 steps: (1) The generator is trained to produce a predicted synthetic mask by freezing the discriminator, and (2) the discriminator is trained while freezing the generator. To optimize the generator network, we have used Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions weighted with an empirical weighing factor to minimize the training instability where is chosen to be 0.5. Similarly, we have used BCE as a loss function for the discriminator network. We have trained each of the generator and the discriminator networks for 35 epochs. We have used the Adam optimizer with a learning rate of 1e-4 and a batch size of 12. Our proposed model is implemented on a system with Intel Xeon Processor, 128 GB RAM powered by NVIDIA Quadro P5000 GPU of 16GB.

    
\end{comment}
 %% Figure environment removed

%The EAD2019 challenge~\citep{ali2020objective} released over 2,192 still endoscopy frames featuring multi-organ and multi-center data to classify 6 different artefact classes, and a comprehensive analysis of the methods evaluated in this challenge revealed the need for more quantifiable metrics and clinical applicability tests with current DL approaches. 

% as the current datasets are limited in terms of the number of images, modality, and organ. Furthermore, most of these datasets are not publicly available, making it difficult to reproduce results and develop new algorithms based on them. 

% due to 
% to make it difficult or impossible to reproduce results. Reproducibility and open science are two important aspects that enable experienced and new ML scientists to build upon and advance the field.
\par


\acf{GI} cancer is the third most prevalent form of cancer in the United States~\citep{siegel2020colorectal}.  It poses a significant threat and contributes to substantial morbidity and mortality rates worldwide.   It comprises of


%Detection and removal of precancerous lesions provide the opportunity top prevent cancer and improve the survival rate to almost 100\%(Levin et al.,2008). 

%Early diagnosis and treatment can be facilitated by regular screening of patients at average risks before the disease becomes symptomatic. Screening of high prevalence areas of infection, such as stomach and the large bowel (CRC), is particularly important to prevent cancer through early detection.

Integrating CADe or CADx in healthcare necessitates addressing factors such as trust, safety, and interpretability to ensure its adoption in clinical settings. The high variations in the curated datasets used to train such models and the actual scenarios in which they are adopted create a high chance of biases, impacting the generalizability of the method. Such bias ultimately makes it challenging to infuse trust while adopting CADe or CADx tools and questions the safety of patients. We addressed this issue by incorporating a transparency task in the MedAI2021 challenge. The main aim of this task was to emphasize the need for interpretability and explainability in the AI-based submissions and shed light on the potential bias and wrong decisions that could have resulted from model and algorithmic bias. Some of the methods adopted by the participating teams include the submission of intermediate heatmaps using approaches like LRP and a detailed ablation study in support of the predictions obtained.   


%Consequently, the superior performance of "agaldran" in both tasks highlights the effectiveness of their algorithm in handling such variability and producing accurate segmentations.


% TODO: I would expect slightly more context to importance reflecting on the challenge conducted - See some papers from Lena and group on metrics

%By addressing the limitations and identifying areas for improvement, these challenges contribute to advancing the field of biomedical image analysis and promoting the development of more reliable and effective automated systems for clinical applications.


% % Figure environment removed

% \subsection{Medical implications of the challenge}
% Colorectal cancer is one of the most commonly occurring cancer and has a very high mortality rate. Polyp segmentation can have substantial medical implications in diagnosis, treatment planning, and surgery. A computer aided diagnosis system can help locate precancerous tissue and improve patient care. Moreover, it may significantly reduce the clinical overload and fatigue of gastroenterologists and nurses who take part in the examination. The colonoscopy time can be utilized more efficiently. For the patients, it may reduce the overall cost, encouraging them to take the test more often. If people are encouraged to take the test regularly, colorectal cancer may be diagnosed at an early stage which may increase overall survivability. By accurately characterizing and segmenting the polyp, therapeutic interventions such as biopsy may be done for surgical removal. Moreover, the gastrointestinal surgical tool segmentation algorithm may assist in improving pre-operative planning, allowing physicians to do more precise segmentation. It may reduce the risk of tissue damage during surgery. Moreover, it may reduce the overall time and resources during surgery, leading to better patient outcomes. 


%Important links:
%https://ceur-ws.org/Vol-2882/
%https://multimediaeval.github.io/editions/2020/tasks/medico/
%https://www.nora.ai/competition/image-segmentation.html
%https://ceur-ws.org/Vol-2882/paper1.pdf
%https://journals.uio.no/NMI/issue/view/787
%https://ceur-ws.org/Vol-2882/

%https://arxiv.org/pdf/2202.12031.pdf
%https://www.sciencedirect.com/science/article/pii/S1361841521000530
%https://arxiv.org/pdf/2110.10965.pdf
%https://www.sciencedirect.com/science/article/pii/S136184152030284X